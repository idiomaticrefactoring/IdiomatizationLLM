repo_name,file_html,class_name,me_name,me_code,old_code,new_code
fairseq,https://github.com/pytorch/fairseq/tree/master/examples/speech_text_joint_to_text/scripts/g2p_encode.py,,dup_pho$95,"def dup_pho(sent, dup_v_num, dup_c_num):
    """"""
    duplicate phoneme defined as cmudict
    http://www.speech.cs.cmu.edu/cgi-bin/cmudict
    """"""
    if dup_v_num == 1 and dup_c_num == 1:
        return sent
    ns = []
    for p in sent:
        ns.append(p)
        if re.search(r""\d$"", p):
            for i in range(1, dup_v_num):
                ns.append(f""{p}-{i}P"")
        elif re.search(r""\w"", p):
            for i in range(1, dup_c_num):
                ns.append(f""{p}-{i}P"")
    return ns",dup_v_num == 1 and dup_c_num == 1,dup_v_num == 1 == dup_c_num
nicotine-plus,https://github.com/nicotine-plus/nicotine-plus/tree/master/pynicotine/transfers.py,Transfers,_file_request_download$1030,"def _file_request_download(self, msg, i):

        log.add_transfer(""Received file upload request %(request)s for file %(filename)s from user %(user)s"", {
            ""request"": msg.req,
            ""filename"": i.filename,
            ""user"": i.user
        })

        incompletedir = self.config.sections[""transfers""][""incompletedir""]
        needupdate = True

        if i.conn is None and i.size is not None:
            i.conn = msg.conn
            i.req = None

            if i in self.transfer_request_times:
                del self.transfer_request_times[i]

            if not incompletedir:
                if i.path:
                    incompletedir = i.path
                else:
                    incompletedir = self.get_default_download_folder(i.user)

            try:
                if not os.access(incompletedir, os.F_OK):
                    os.makedirs(incompletedir)
                if not os.access(incompletedir, os.R_OK | os.W_OK | os.X_OK):
                    raise OSError(""Download directory %s Permissions error.\nDir Permissions: %s"" %
                                  (incompletedir, oct(os.stat(incompletedir)[stat.ST_MODE] & 0o777)))

            except OSError as error:
                log.add(_(""OS error: %s""), error)
                self.download_folder_error(i, error)

            else:
                file_handle = None
                try:
                    from hashlib import md5
                    md5sum = md5()
                    md5sum.update((i.filename + i.user).encode('utf-8'))

                    base_name = clean_file(i.filename.replace('/', '\\').split('\\')[-1])
                    incomplete_name = os.path.join(incompletedir, ""INCOMPLETE"" + md5sum.hexdigest() + base_name)
                    file_handle = open(incomplete_name, 'ab+')

                    if self.config.sections[""transfers""][""lock""]:
                        try:
                            import fcntl
                            try:
                                fcntl.lockf(file_handle, fcntl.LOCK_EX | fcntl.LOCK_NB)
                            except IOError as error:
                                log.add(_(""Can't get an exclusive lock on file - I/O error: %s""), error)
                        except ImportError:
                            pass

                    file_handle.seek(0, 2)
                    offset = file_handle.tell()

                except IOError as error:
                    log.add(_(""Download I/O error: %s""), error)

                    self.abort_transfer(i)
                    i.status = ""Local file error""

                else:
                    i.file = file_handle
                    i.lastbytes = offset
                    i.place = 0

                    self.core.statistics.append_stat_value(""started_downloads"", 1)
                    self.core.pluginhandler.download_started_notification(i.user, i.filename, incomplete_name)

                    if i.size > offset:
                        i.status = ""Transferring""
                        i.legacy_attempt = False
                        self.queue.append(slskmessages.DownloadFile(i.conn, file_handle))
                        self.queue.append(slskmessages.FileOffset(i.conn, i.size, offset))

                        log.add_download(
                            _(""Download started: user %(user)s, file %(file)s""), {
                                ""user"": i.user,
                                ""file"": ""%s"" % file_handle.name
                            }
                        )
                    else:
                        self.download_finished(file_handle, i)
                        needupdate = False

            if self.downloadsview:
                self.downloadsview.new_transfer_notification()

                if needupdate:
                    self.downloadsview.update(i)

        else:
            log.add_transfer(""Download error formally known as 'Unknown file request': %(req)s (%(user)s: %(file)s)"", {
                'req': str(vars(msg)),
                'user': i.user,
                'file': i.filename
            })

            self.queue.append(slskmessages.ConnClose(msg.conn))",i.conn is None and i.size is not None,i.conn is None is not i.size
COCO-WholeBody,https://github.com/jin-s13/COCO-WholeBody/tree/master/evaluation/myeval_body.py,MYeval_body,evaluateImg$199,"def evaluateImg(self, imgId, catId, aRng, maxDet):
        '''
        perform evaluation for single category and image
        :return: dict (single image results)
        '''
        p = self.params
        if p.useCats:
            gt = self._gts[imgId,catId]
            dt = self._dts[imgId,catId]
        else:
            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]
            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]
        if len(gt) == 0 and len(dt) ==0:
            return None

        for g in gt:
            if g['ignore'] or (g['area']<aRng[0] or g['area']>aRng[1]):
                g['_ignore'] = 1
            else:
                g['_ignore'] = 0

        # sort dt highest score first, sort gt ignore last
        gtind = np.argsort([g['_ignore'] for g in gt], kind='mergesort')
        gt = [gt[i] for i in gtind]
        dtind = np.argsort([-d['score'] for d in dt], kind='mergesort')
        dt = [dt[i] for i in dtind[0:maxDet]]
        iscrowd = [int(o['iscrowd']) for o in gt]
        # load computed ious
        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]

        T = len(p.iouThrs)
        G = len(gt)
        D = len(dt)
        gtm  = np.zeros((T,G))
        dtm  = np.zeros((T,D))
        gtIg = np.array([g['_ignore'] for g in gt])
        dtIg = np.zeros((T,D))
        if not len(ious)==0:
            for tind, t in enumerate(p.iouThrs):
                for dind, d in enumerate(dt):
                    # information about best match so far (m=-1 -> unmatched)
                    iou = min([t,1-1e-10])
                    m   = -1
                    for gind, g in enumerate(gt):
                        # if this gt already matched, and not a crowd, continue
                        if gtm[tind,gind]>0 and not iscrowd[gind]:
                            continue
                        # if dt matched to reg gt, and on ignore gt, stop
                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:
                            break
                        # continue to next gt unless better match made
                        if ious[dind,gind] < iou:
                            continue
                        # if match successful and best so far, store appropriately
                        iou=ious[dind,gind]
                        m=gind
                    # if match made store id of match for both dt and gt
                    if m ==-1:
                        continue
                    dtIg[tind,dind] = gtIg[m]
                    dtm[tind,dind]  = gt[m]['id']
                    gtm[tind,m]     = d['id']
        # set unmatched detections outside of area range to ignore
        a = np.array([d['area']<aRng[0] or d['area']>aRng[1] for d in dt]).reshape((1, len(dt)))
        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))
        # store results for given image and category
        return {
                'image_id':     imgId,
                'category_id':  catId,
                'aRng':         aRng,
                'maxDet':       maxDet,
                'dtIds':        [d['id'] for d in dt],
                'gtIds':        [g['id'] for g in gt],
                'dtMatches':    dtm,
                'gtMatches':    gtm,
                'dtScores':     [d['score'] for d in dt],
                'gtIgnore':     gtIg,
                'dtIgnore':     dtIg,
            }",len(gt) == 0 and len(dt) == 0,len(gt) == 0 == len(dt)
HarvestText,https://github.com/blmoistawinde/HarvestText/tree/master/harvesttext/resources.py,,get_jieba_dict$127,"def get_jieba_dict(min_freq=0, max_freq=float('inf'), with_pos=False, use_proxy=False, proxies=None):
    """"""
    获得jieba自带的中文词语词频词典
    
    :params min_freq: 选取词语需要的最小词频
    :params max_freq: 选取词语允许的最大词频
    :params with_pos: 返回结果是否包括词性信息
    :return if not with_pos, dict of {wd: freq}, else, dict of {(wd, pos): freq} 
    """"""
    from .download_utils import RemoteFileMetadata, check_download_resource
    remote = RemoteFileMetadata(
        filename='jieba_dict.txt',
        url='https://github.com/blmoistawinde/HarvestText/releases/download/V0.8/jieba_dict.txt',
        checksum='7197c3211ddd98962b036cdf40324d1ea2bfaa12bd028e68faa70111a88e12a8')
    file_path = check_download_resource(remote, use_proxy, proxies)
    ret = defaultdict(int)
    with open(file_path, ""r"", encoding=""utf-8"") as f:
        for line in f:
            if len(line.strip().split()) == 3:
                wd, freq, pos = line.strip().split()
                freq = int(freq)
            if freq > min_freq and freq < max_freq:
                if not with_pos:
                    ret[wd] = freq
                else:
                    ret[(wd, pos)] = freq
    return ret",freq > min_freq and freq < max_freq,min_freq < freq < max_freq
rasa,https://github.com/RasaHQ/rasa/tree/master/tests/core/policies/test_ted_policy.py,TestTEDPolicy,test_gen_batch$317,"def test_gen_batch(
        self, trained_policy: TEDPolicy, default_domain: Domain, stories_path: Path
    ):
        training_trackers = tests.core.test_policies.train_trackers(
            default_domain, stories_path, augmentation_factor=0
        )
        precomputations = None
        training_data, label_ids, entity_tags = trained_policy._featurize_for_training(
            training_trackers, default_domain, precomputations
        )

        _, all_labels = trained_policy._create_label_data(
            default_domain, precomputations
        )
        model_data = trained_policy._create_model_data(
            training_data, label_ids, entity_tags, all_labels
        )
        batch_size = 2
        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=False, batch_strategy=""sequence""
        )
        iterator = iter(data_generator)
        # model data keys were sorted, so the order is alphabetical
        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # batch and dialogue dimensions are NOT combined for masks
        assert (
            batch_slots_mask.shape[0] == batch_size
            and batch_intent_mask.shape[0] == batch_size
            and batch_entities_mask.shape[0] == batch_size
            and batch_action_name_mask.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )

        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=True, batch_strategy=""balanced""
        )
        iterator = iter(data_generator)

        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )",batch_label_ids.shape[0] == batch_size and batch_dialogue_length.shape[0] == batch_size,batch_label_ids.shape[0] == batch_size == batch_dialogue_length.shape[0]
rasa,https://github.com/RasaHQ/rasa/tree/master/tests/core/policies/test_ted_policy.py,TestTEDPolicy,test_gen_batch$317,"def test_gen_batch(
        self, trained_policy: TEDPolicy, default_domain: Domain, stories_path: Path
    ):
        training_trackers = tests.core.test_policies.train_trackers(
            default_domain, stories_path, augmentation_factor=0
        )
        precomputations = None
        training_data, label_ids, entity_tags = trained_policy._featurize_for_training(
            training_trackers, default_domain, precomputations
        )

        _, all_labels = trained_policy._create_label_data(
            default_domain, precomputations
        )
        model_data = trained_policy._create_model_data(
            training_data, label_ids, entity_tags, all_labels
        )
        batch_size = 2
        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=False, batch_strategy=""sequence""
        )
        iterator = iter(data_generator)
        # model data keys were sorted, so the order is alphabetical
        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # batch and dialogue dimensions are NOT combined for masks
        assert (
            batch_slots_mask.shape[0] == batch_size
            and batch_intent_mask.shape[0] == batch_size
            and batch_entities_mask.shape[0] == batch_size
            and batch_action_name_mask.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )

        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=True, batch_strategy=""balanced""
        )
        iterator = iter(data_generator)

        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )",batch_slots_mask.shape[0] == batch_size and batch_intent_mask.shape[0] == batch_size and (batch_entities_mask.shape[0] == batch_size) and (batch_action_name_mask.shape[0] == batch_size),batch_slots_mask.shape[0] == batch_size == batch_intent_mask.shape[0] and batch_entities_mask.shape[0] == batch_size == batch_action_name_mask.shape[0]
rasa,https://github.com/RasaHQ/rasa/tree/master/tests/core/policies/test_ted_policy.py,TestTEDPolicy,test_gen_batch$317,"def test_gen_batch(
        self, trained_policy: TEDPolicy, default_domain: Domain, stories_path: Path
    ):
        training_trackers = tests.core.test_policies.train_trackers(
            default_domain, stories_path, augmentation_factor=0
        )
        precomputations = None
        training_data, label_ids, entity_tags = trained_policy._featurize_for_training(
            training_trackers, default_domain, precomputations
        )

        _, all_labels = trained_policy._create_label_data(
            default_domain, precomputations
        )
        model_data = trained_policy._create_model_data(
            training_data, label_ids, entity_tags, all_labels
        )
        batch_size = 2
        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=False, batch_strategy=""sequence""
        )
        iterator = iter(data_generator)
        # model data keys were sorted, so the order is alphabetical
        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # batch and dialogue dimensions are NOT combined for masks
        assert (
            batch_slots_mask.shape[0] == batch_size
            and batch_intent_mask.shape[0] == batch_size
            and batch_entities_mask.shape[0] == batch_size
            and batch_action_name_mask.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )

        data_generator = RasaBatchDataGenerator(
            model_data, batch_size=batch_size, shuffle=True, batch_strategy=""balanced""
        )
        iterator = iter(data_generator)

        (
            (
                batch_action_name_mask,
                _,
                _,
                batch_action_name_sentence_shape,
                batch_dialogue_length,
                batch_entities_mask,
                _,
                _,
                batch_entities_sentence_shape,
                batch_intent_mask,
                _,
                _,
                batch_intent_sentence_shape,
                batch_label_ids,
                batch_slots_mask,
                _,
                _,
                batch_slots_sentence_shape,
            ),
            _,
        ) = next(iterator)

        assert (
            batch_label_ids.shape[0] == batch_size
            and batch_dialogue_length.shape[0] == batch_size
        )
        # some features might be ""fake"" so there sequence is `0`
        seq_len = max(
            [
                batch_intent_sentence_shape[1],
                batch_action_name_sentence_shape[1],
                batch_entities_sentence_shape[1],
                batch_slots_sentence_shape[1],
            ]
        )
        assert (
            batch_intent_sentence_shape[1] == seq_len
            or batch_intent_sentence_shape[1] == 0
        )
        assert (
            batch_action_name_sentence_shape[1] == seq_len
            or batch_action_name_sentence_shape[1] == 0
        )
        assert (
            batch_entities_sentence_shape[1] == seq_len
            or batch_entities_sentence_shape[1] == 0
        )
        assert (
            batch_slots_sentence_shape[1] == seq_len
            or batch_slots_sentence_shape[1] == 0
        )",batch_label_ids.shape[0] == batch_size and batch_dialogue_length.shape[0] == batch_size,batch_label_ids.shape[0] == batch_size == batch_dialogue_length.shape[0]
PyBoy,https://github.com/Baekalfen/PyBoy/tree/master/pyboy/plugins/debug.py,BaseDebugWindow,mark_tile$426,"def mark_tile(self, x, y, color, height, width, grid):
        tw = width # Tile width
        th = height # Tile height
        if grid:
            xx = x - (x%tw)
            yy = y - (y%th)
        else:
            xx = x
            yy = y
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx < self.width:
                self.buf0[yy + i][xx] = color
        for i in range(tw):
            if 0 <= (yy) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy][xx + i] = color
        for i in range(tw):
            if 0 <= (yy + th - 1) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy + th - 1][xx + i] = color
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx + tw - 1 < self.width:
                self.buf0[yy + i][xx + tw - 1] = color",0 <= yy + i < self.height and 0 <= xx < self.width,self.height > yy + i >= 0 <= xx < self.width
PyBoy,https://github.com/Baekalfen/PyBoy/tree/master/pyboy/plugins/debug.py,BaseDebugWindow,mark_tile$426,"def mark_tile(self, x, y, color, height, width, grid):
        tw = width # Tile width
        th = height # Tile height
        if grid:
            xx = x - (x%tw)
            yy = y - (y%th)
        else:
            xx = x
            yy = y
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx < self.width:
                self.buf0[yy + i][xx] = color
        for i in range(tw):
            if 0 <= (yy) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy][xx + i] = color
        for i in range(tw):
            if 0 <= (yy + th - 1) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy + th - 1][xx + i] = color
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx + tw - 1 < self.width:
                self.buf0[yy + i][xx + tw - 1] = color",0 <= yy < self.height and 0 <= xx + i < self.width,self.height > yy >= 0 <= xx + i < self.width
PyBoy,https://github.com/Baekalfen/PyBoy/tree/master/pyboy/plugins/debug.py,BaseDebugWindow,mark_tile$426,"def mark_tile(self, x, y, color, height, width, grid):
        tw = width # Tile width
        th = height # Tile height
        if grid:
            xx = x - (x%tw)
            yy = y - (y%th)
        else:
            xx = x
            yy = y
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx < self.width:
                self.buf0[yy + i][xx] = color
        for i in range(tw):
            if 0 <= (yy) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy][xx + i] = color
        for i in range(tw):
            if 0 <= (yy + th - 1) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy + th - 1][xx + i] = color
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx + tw - 1 < self.width:
                self.buf0[yy + i][xx + tw - 1] = color",0 <= yy + th - 1 < self.height and 0 <= xx + i < self.width,self.height > yy + th - 1 >= 0 <= xx + i < self.width
PyBoy,https://github.com/Baekalfen/PyBoy/tree/master/pyboy/plugins/debug.py,BaseDebugWindow,mark_tile$426,"def mark_tile(self, x, y, color, height, width, grid):
        tw = width # Tile width
        th = height # Tile height
        if grid:
            xx = x - (x%tw)
            yy = y - (y%th)
        else:
            xx = x
            yy = y
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx < self.width:
                self.buf0[yy + i][xx] = color
        for i in range(tw):
            if 0 <= (yy) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy][xx + i] = color
        for i in range(tw):
            if 0 <= (yy + th - 1) < self.height and 0 <= xx + i < self.width:
                self.buf0[yy + th - 1][xx + i] = color
        for i in range(th):
            if 0 <= (yy + i) < self.height and 0 <= xx + tw - 1 < self.width:
                self.buf0[yy + i][xx + tw - 1] = color",0 <= yy + i < self.height and 0 <= xx + tw - 1 < self.width,self.height > yy + i >= 0 <= xx + tw - 1 < self.width
dumpmon,https://github.com/jordan-wright/dumpmon/tree/master/lib/helper.py,,build_tweet$41,"def build_tweet(paste):
    '''
    build_tweet(url, paste) - Determines if the paste is interesting and, if so, builds and returns the tweet accordingly

    '''
    tweet = None
    if paste.match():
        tweet = paste.url
        if paste.type == 'db_dump':
            if paste.num_emails > 0:
                tweet += ' Emails: ' + str(paste.num_emails)
            if paste.num_hashes > 0:
                tweet += ' Hashes: ' + str(paste.num_hashes)
            if paste.num_hashes > 0 and paste.num_emails > 0:
                tweet += ' E/H: ' + str(round(
                    paste.num_emails / float(paste.num_hashes), 2))
            tweet += ' Keywords: ' + str(paste.db_keywords)
        elif paste.type == 'google_api':
            tweet += ' Found possible Google API key(s)'
        elif paste.type in ['cisco', 'juniper']:
            tweet += ' Possible ' + paste.type + ' configuration'
        elif paste.type == 'ssh_private':
            tweet += ' Possible SSH private key'
        elif paste.type == 'honeypot':
            tweet += ' Dionaea Honeypot Log'
        elif paste.type == 'pgp_private':
            tweet += ' Found possible PGP Private Key'
        tweet += ' #infoleak'
    if paste.num_emails > 0:
        print(paste.emails)
    return tweet",paste.num_hashes > 0 and paste.num_emails > 0,paste.num_hashes > 0 < paste.num_emails
gym-minigrid,https://github.com/maximecb/gym-minigrid/tree/master/gym_minigrid/minigrid.py,MiniGridEnv,dir_vec$980,"def dir_vec(self):
        """"""
        Get the direction vector for the agent, pointing in the direction
        of forward movement.
        """"""

        assert self.agent_dir >= 0 and self.agent_dir < 4
        return DIR_TO_VEC[self.agent_dir]",self.agent_dir >= 0 and self.agent_dir < 4,0 <= self.agent_dir < 4
Pyro4,https://github.com/irmen/Pyro4/tree/master/src/Pyro4/core.py,Daemon,__init__$1110,"def __init__(self, host=None, port=0, unixsocket=None, nathost=None, natport=None, interface=DaemonObject, connected_socket=None):
        if connected_socket:
            nathost = natport = None
        else:
            if host is None:
                host = config.HOST
            if nathost is None:
                nathost = config.NATHOST
            if natport is None and nathost is not None:
                natport = config.NATPORT
            if nathost and unixsocket:
                raise ValueError(""cannot use nathost together with unixsocket"")
            if (nathost is None) ^ (natport is None):
                raise ValueError(""must provide natport with nathost"")
        self.__mustshutdown = threading.Event()
        self.__mustshutdown.set()
        self.__loopstopped = threading.Event()
        self.__loopstopped.set()
        if connected_socket:
            from Pyro4.socketserver.existingconnectionserver import SocketServer_ExistingConnection
            self.transportServer = SocketServer_ExistingConnection()
            self.transportServer.init(self, connected_socket)
        else:
            if config.SERVERTYPE == ""thread"":
                from Pyro4.socketserver.threadpoolserver import SocketServer_Threadpool
                self.transportServer = SocketServer_Threadpool()
            elif config.SERVERTYPE == ""multiplex"":
                from Pyro4.socketserver.multiplexserver import SocketServer_Multiplex
                self.transportServer = SocketServer_Multiplex()
            else:
                raise errors.PyroError(""invalid server type '%s'"" % config.SERVERTYPE)
            self.transportServer.init(self, host, port, unixsocket)
        #: The location (str of the form ``host:portnumber``) on which the Daemon is listening
        self.locationStr = self.transportServer.locationStr
        log.debug(""daemon created on %s - %s (pid %d)"", self.locationStr, socketutil.family_str(self.transportServer.sock), os.getpid())
        natport_for_loc = natport
        if natport == 0:
            # expose internal port number as NAT port as well. (don't use port because it could be 0 and will be chosen by the OS)
            natport_for_loc = int(self.locationStr.split("":"")[1])
        #: The NAT-location (str of the form ``nathost:natportnumber``) on which the Daemon is exposed for use with NAT-routing
        self.natLocationStr = ""%s:%d"" % (nathost, natport_for_loc) if nathost else None
        if self.natLocationStr:
            log.debug(""NAT address is %s"", self.natLocationStr)
        pyroObject = interface(self)
        pyroObject._pyroId = constants.DAEMON_NAME
        #: Dictionary from Pyro object id to the actual Pyro object registered by this id
        self.objectsById = {pyroObject._pyroId: pyroObject}
        # assert that the configured serializers are available, and remember their ids:
        self.__serializer_ids = {util.get_serializer(ser_name).serializer_id for ser_name in config.SERIALIZERS_ACCEPTED}
        log.debug(""accepted serializers: %s"" % config.SERIALIZERS_ACCEPTED)
        log.debug(""pyro protocol version: %d  pickle version: %d"" % (constants.PROTOCOL_VERSION, config.PICKLE_PROTOCOL_VERSION))
        self.__pyroHmacKey = None
        self._pyroInstances = {}   # pyro objects for instance_mode=single (singletons, just one per daemon)
        self.streaming_responses = {}   # stream_id -> (client, creation_timestamp, linger_timestamp, stream)
        self.housekeeper_lock = threading.Lock()
        self.create_single_instance_lock = threading.Lock()
        self.__mustshutdown.clear()",natport is None and nathost is not None,natport is None is not nathost
metaworld,https://github.com/rlworkgroup/metaworld/tree/master/metaworld/envs/mujoco/sawyer_xyz/v1/sawyer_disassemble_peg.py,SawyerNutDisassembleEnv,compute_reward$104,"def compute_reward(self, actions, obs):

        graspPos = obs[3:6]
        objPos = graspPos

        rightFinger, leftFinger = self._get_site_pos('rightEndEffector'), self._get_site_pos('leftEndEffector')
        fingerCOM  =  (rightFinger + leftFinger)/2

        heightTarget = self.heightTarget
        placingGoal = self._target_pos

        reachDist = np.linalg.norm(graspPos - fingerCOM)
        reachDistxy = np.linalg.norm(graspPos[:-1] - fingerCOM[:-1])
        zDist = np.abs(fingerCOM[-1] - self.init_fingerCOM[-1])

        placingDist = np.linalg.norm(objPos - placingGoal)

        def reachReward():
            reachRew = -reachDist
            if reachDistxy < 0.04:
                reachRew = -reachDist
            else:
                reachRew =  -reachDistxy - 2*zDist

            # incentive to close fingers when reachDist is small
            if reachDist < 0.04:
                reachRew = -reachDist + max(actions[-1],0)/50
            return reachRew, reachDist

        def pickCompletionCriteria():
            tolerance = 0.01
            if objPos[2] >= (heightTarget- tolerance) and reachDist < 0.04:
                return True
            else:
                return False

        if pickCompletionCriteria():
            self.pickCompleted = True

        def objDropped():
            return (objPos[2] < (self.objHeight + 0.005)) and (placingDist >0.02) and (reachDist > 0.02)

        def orig_pickReward():
            hScale = 100
            if self.pickCompleted and not(objDropped()):
                return hScale*heightTarget
            elif (reachDist < 0.04) and (objPos[2]> (self.objHeight + 0.005)) :
                return hScale* min(heightTarget, objPos[2])
            else:
                return 0

        def placeRewardMove():
            c1 = 1000
            c2 = 0.01
            c3 = 0.001

            placeRew = 1000*(self.maxPlacingDist - placingDist) + c1*(np.exp(-(placingDist**2)/c2) + np.exp(-(placingDist**2)/c3))
            placeRew = max(placeRew,0)
            cond = self.pickCompleted and (reachDist < 0.03) and not(objDropped())
            if cond:
                return [placeRew, placingDist]
            else:
                return [0 , placingDist]


        reachRew, reachDist = reachReward()
        pickRew = orig_pickReward()

        peg_pos = self.sim.model.body_pos[self.model.body_name2id('peg')]
        nut_pos = self.get_body_com('RoundNut')
        if abs(nut_pos[0] - peg_pos[0]) > 0.05 or \
                abs(nut_pos[1] - peg_pos[1]) > 0.05:
            placingDist = 0
            reachRew = 0
            reachDist = 0
            pickRew = heightTarget*100

        placeRew , placingDist = placeRewardMove()
        assert ((placeRew >=0) and (pickRew>=0))
        reward = reachRew + pickRew + placeRew
        success = (abs(nut_pos[0] - peg_pos[0]) > 0.05 or abs(nut_pos[1] - peg_pos[1]) > 0.05) or placingDist < 0.02

        return [reward, reachRew, reachDist, pickRew, placeRew, placingDist, float(success)]",placeRew >= 0 and pickRew >= 0,placeRew >= 0 <= pickRew
metaworld,https://github.com/rlworkgroup/metaworld/tree/master/metaworld/envs/mujoco/sawyer_xyz/v1/sawyer_disassemble_peg.py,SawyerNutDisassembleEnv,compute_reward$104,"def compute_reward(self, actions, obs):

        graspPos = obs[3:6]
        objPos = graspPos

        rightFinger, leftFinger = self._get_site_pos('rightEndEffector'), self._get_site_pos('leftEndEffector')
        fingerCOM  =  (rightFinger + leftFinger)/2

        heightTarget = self.heightTarget
        placingGoal = self._target_pos

        reachDist = np.linalg.norm(graspPos - fingerCOM)
        reachDistxy = np.linalg.norm(graspPos[:-1] - fingerCOM[:-1])
        zDist = np.abs(fingerCOM[-1] - self.init_fingerCOM[-1])

        placingDist = np.linalg.norm(objPos - placingGoal)

        def reachReward():
            reachRew = -reachDist
            if reachDistxy < 0.04:
                reachRew = -reachDist
            else:
                reachRew =  -reachDistxy - 2*zDist

            # incentive to close fingers when reachDist is small
            if reachDist < 0.04:
                reachRew = -reachDist + max(actions[-1],0)/50
            return reachRew, reachDist

        def pickCompletionCriteria():
            tolerance = 0.01
            if objPos[2] >= (heightTarget- tolerance) and reachDist < 0.04:
                return True
            else:
                return False

        if pickCompletionCriteria():
            self.pickCompleted = True

        def objDropped():
            return (objPos[2] < (self.objHeight + 0.005)) and (placingDist >0.02) and (reachDist > 0.02)

        def orig_pickReward():
            hScale = 100
            if self.pickCompleted and not(objDropped()):
                return hScale*heightTarget
            elif (reachDist < 0.04) and (objPos[2]> (self.objHeight + 0.005)) :
                return hScale* min(heightTarget, objPos[2])
            else:
                return 0

        def placeRewardMove():
            c1 = 1000
            c2 = 0.01
            c3 = 0.001

            placeRew = 1000*(self.maxPlacingDist - placingDist) + c1*(np.exp(-(placingDist**2)/c2) + np.exp(-(placingDist**2)/c3))
            placeRew = max(placeRew,0)
            cond = self.pickCompleted and (reachDist < 0.03) and not(objDropped())
            if cond:
                return [placeRew, placingDist]
            else:
                return [0 , placingDist]


        reachRew, reachDist = reachReward()
        pickRew = orig_pickReward()

        peg_pos = self.sim.model.body_pos[self.model.body_name2id('peg')]
        nut_pos = self.get_body_com('RoundNut')
        if abs(nut_pos[0] - peg_pos[0]) > 0.05 or \
                abs(nut_pos[1] - peg_pos[1]) > 0.05:
            placingDist = 0
            reachRew = 0
            reachDist = 0
            pickRew = heightTarget*100

        placeRew , placingDist = placeRewardMove()
        assert ((placeRew >=0) and (pickRew>=0))
        reward = reachRew + pickRew + placeRew
        success = (abs(nut_pos[0] - peg_pos[0]) > 0.05 or abs(nut_pos[1] - peg_pos[1]) > 0.05) or placingDist < 0.02

        return [reward, reachRew, reachDist, pickRew, placeRew, placingDist, float(success)]",objPos[2] < self.objHeight + 0.005 and placingDist > 0.02 and (reachDist > 0.02),objPos[2] < self.objHeight + 0.005 and placingDist > 0.02 < reachDist
palladium,https://github.com/ottogroup/palladium/tree/master/palladium/persistence.py,Database,read$447,"def read(self, version=None):
        use_active_model = version is None

        with session_scope(self.session) as session:
            query = session.query(self.DBModel)
            if not version:
                version = self._active_version
            dbmodel = query.filter_by(version=version).first()

            if dbmodel is not None:
                query2 = session.query(self.DBModelChunk).filter_by(
                    model_version=dbmodel.version
                    ).order_by('id').yield_per(4)
                fileobj = io.BytesIO()
                for chunk in query2:
                    fileobj.write(chunk.blob)
                fileobj.seek(0)
                return pickle.load(gzip.GzipFile(fileobj=fileobj, mode='rb'))

        if use_active_model and dbmodel is None and version is not None:
            raise LookupError(
                ""Activated model not available. Maybe it was deleted."")

        raise LookupError(""No model available"")",use_active_model and dbmodel is None and (version is not None),dbmodel is None is not version and use_active_model
brave,https://github.com/bbc/brave/tree/master/brave/inputs/input.py,Input,summarise$31,"def summarise(self, for_config_file=False):
        s = super().summarise(for_config_file)

        if not for_config_file:
            if hasattr(self, 'pipeline'):
                position = int(str(self.pipeline.query_position(Gst.Format.TIME).cur))
                if position is not None and position is not -1:
                    s['position'] = position
                s['duration'] = int(str(self.pipeline.query_duration(Gst.Format.TIME).duration))

                has_connection_speed, _, _ = self.pipeline.lookup('connection-speed')
                if has_connection_speed:
                    s['connection_speed'] = self.pipeline.get_property('connection-speed')
                has_buffer_size, _, _ = self.pipeline.lookup('buffer-size')
                if has_buffer_size:
                    s['buffer_size'] = self.pipeline.get_property('buffer-size')
                has_buffer_duration, _, _ = self.pipeline.lookup('buffer-duration')
                if has_buffer_duration:
                    buffer_duration = self.pipeline.get_property('buffer-duration')
                    if buffer_duration != -1:
                        s['buffer_duration'] = buffer_duration

                # playbin will respond with duration=-1 when not known.
                if (s['duration'] == -1):
                    s.pop('duration', None)

            if hasattr(self, 'get_input_cap_props'):
                cap_props = self.get_input_cap_props()
                if cap_props:
                    s = {**s, **cap_props}

        return s",position is not None and position is not -1,None is not position is not -1
dynaconf,https://github.com/rochacbruno/dynaconf/tree/master/dynaconf/vendor_src/click/types.py,Choice,convert$166,"def convert(self, value, param, ctx):
        # Match through normalization and case sensitivity
        # first do token_normalize_func, then lowercase
        # preserve original `value` to produce an accurate message in
        # `self.fail`
        normed_value = value
        normed_choices = {choice: choice for choice in self.choices}

        if ctx is not None and ctx.token_normalize_func is not None:
            normed_value = ctx.token_normalize_func(value)
            normed_choices = {
                ctx.token_normalize_func(normed_choice): original
                for normed_choice, original in normed_choices.items()
            }

        if not self.case_sensitive:
            normed_value = normed_value.casefold()
            normed_choices = {
                normed_choice.casefold(): original
                for normed_choice, original in normed_choices.items()
            }

        if normed_value in normed_choices:
            return normed_choices[normed_value]

        self.fail(
            f""invalid choice: {value}. (choose from {', '.join(self.choices)})"",
            param,
            ctx,
        )",ctx is not None and ctx.token_normalize_func is not None,ctx is not None is not ctx.token_normalize_func
checkov,https://github.com/bridgecrewio/checkov/tree/master/checkov/cloudformation/graph_builder/graph_components/blocks.py,CloudformationBlock,_should_add_previous_breadcrumbs$83,"def _should_add_previous_breadcrumbs(change_origin_id: Optional[int],
                                         previous_breadcrumbs: List[BreadcrumbMetadata],
                                         attribute_at_dest: Optional[str]):
        return (
            change_origin_id is not None
            and attribute_at_dest is not None
            and (not previous_breadcrumbs or previous_breadcrumbs[-1].vertex_id != change_origin_id)
        )",change_origin_id is not None and attribute_at_dest is not None and (not previous_breadcrumbs or previous_breadcrumbs[-1].vertex_id != change_origin_id),change_origin_id is not None is not attribute_at_dest and (not previous_breadcrumbs)
moto,https://github.com/spulec/moto/tree/master/moto/sagemaker/models.py,FakeTrialComponent,response_object$2107,"def response_object(self):
        response_object = self.gen_response_object()
        return {
            k: v for k, v in response_object.items() if v is not None and v != [None]
        }",v is not None and v != [None],None is not v != [None]
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/adapters/model_mixin.py,EmbeddingAdaptersMixin,add_embeddings$142,"def add_embeddings(self, name, tokenizer, reference_embedding=None, reference_tokenizer=None, embedding_dim=None):
        """"""
        Add a new embedding to the model. If a reference embedding and reference tokenizer are provided tokens in the
        present in both tokenizers are initialized to the embedding in the reference_embedding.

        Args:
            name: the name of the embedding
            tokenizer: the tokenizer determining the vocab of the embedding
            reference_embedding:
                the reference embedding to use for initializing the embeddings of tokens present in the newly created
                embedding
            reference_tokenizer: the tokenizer providing the vocab for the reference embedding
            embedding_dim: the dimension of the embeddings (if None the hidden_size from the config is used)

        """"""
        if name in self.loaded_embeddings:
            raise ValueError(""An embedding with the name {} already exists"".format(name))
        if embedding_dim is None:
            embedding_dim = self.config.hidden_size
        embedding = nn.Embedding(len(tokenizer), embedding_dim)
        # Use same initialization as base Transformer model
        embedding.weight.data.normal_(mean=0.0, std=0.02)
        if embedding.padding_idx is not None:
            embedding.weight.data[embedding.padding_idx].zero_()
        embedding.requires_grad_(False)
        if (reference_embedding is not None and reference_tokenizer is None) or (
            reference_tokenizer is not None and reference_embedding is None
        ):
            raise KeyError(
                ""Reference embedding and reference tokenizer are required to use initialize embeddings from reference""
                "" embedding""
            )
        if reference_embedding is not None and reference_tokenizer is not None:
            tokens = set(tokenizer.get_vocab().keys()) & set(reference_tokenizer.get_vocab().keys())
            reference_vocab = reference_tokenizer.get_vocab()
            vocab = tokenizer.get_vocab()
            for t in tokens:
                idx_reference = reference_vocab[t]
                idx = vocab[t]
                embedding.weight[idx] = (
                    self.loaded_embeddings[reference_embedding].weight[idx_reference].detach().clone()
                )
        embedding.train(False)
        self.loaded_embeddings[name] = embedding
        self.set_active_embeddings(name)",reference_embedding is not None and reference_tokenizer is not None,reference_embedding is not None is not reference_tokenizer
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/adapters/model_mixin.py,EmbeddingAdaptersMixin,add_embeddings$142,"def add_embeddings(self, name, tokenizer, reference_embedding=None, reference_tokenizer=None, embedding_dim=None):
        """"""
        Add a new embedding to the model. If a reference embedding and reference tokenizer are provided tokens in the
        present in both tokenizers are initialized to the embedding in the reference_embedding.

        Args:
            name: the name of the embedding
            tokenizer: the tokenizer determining the vocab of the embedding
            reference_embedding:
                the reference embedding to use for initializing the embeddings of tokens present in the newly created
                embedding
            reference_tokenizer: the tokenizer providing the vocab for the reference embedding
            embedding_dim: the dimension of the embeddings (if None the hidden_size from the config is used)

        """"""
        if name in self.loaded_embeddings:
            raise ValueError(""An embedding with the name {} already exists"".format(name))
        if embedding_dim is None:
            embedding_dim = self.config.hidden_size
        embedding = nn.Embedding(len(tokenizer), embedding_dim)
        # Use same initialization as base Transformer model
        embedding.weight.data.normal_(mean=0.0, std=0.02)
        if embedding.padding_idx is not None:
            embedding.weight.data[embedding.padding_idx].zero_()
        embedding.requires_grad_(False)
        if (reference_embedding is not None and reference_tokenizer is None) or (
            reference_tokenizer is not None and reference_embedding is None
        ):
            raise KeyError(
                ""Reference embedding and reference tokenizer are required to use initialize embeddings from reference""
                "" embedding""
            )
        if reference_embedding is not None and reference_tokenizer is not None:
            tokens = set(tokenizer.get_vocab().keys()) & set(reference_tokenizer.get_vocab().keys())
            reference_vocab = reference_tokenizer.get_vocab()
            vocab = tokenizer.get_vocab()
            for t in tokens:
                idx_reference = reference_vocab[t]
                idx = vocab[t]
                embedding.weight[idx] = (
                    self.loaded_embeddings[reference_embedding].weight[idx_reference].detach().clone()
                )
        embedding.train(False)
        self.loaded_embeddings[name] = embedding
        self.set_active_embeddings(name)",reference_embedding is not None and reference_tokenizer is None,reference_embedding is not None is reference_tokenizer
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/adapters/model_mixin.py,EmbeddingAdaptersMixin,add_embeddings$142,"def add_embeddings(self, name, tokenizer, reference_embedding=None, reference_tokenizer=None, embedding_dim=None):
        """"""
        Add a new embedding to the model. If a reference embedding and reference tokenizer are provided tokens in the
        present in both tokenizers are initialized to the embedding in the reference_embedding.

        Args:
            name: the name of the embedding
            tokenizer: the tokenizer determining the vocab of the embedding
            reference_embedding:
                the reference embedding to use for initializing the embeddings of tokens present in the newly created
                embedding
            reference_tokenizer: the tokenizer providing the vocab for the reference embedding
            embedding_dim: the dimension of the embeddings (if None the hidden_size from the config is used)

        """"""
        if name in self.loaded_embeddings:
            raise ValueError(""An embedding with the name {} already exists"".format(name))
        if embedding_dim is None:
            embedding_dim = self.config.hidden_size
        embedding = nn.Embedding(len(tokenizer), embedding_dim)
        # Use same initialization as base Transformer model
        embedding.weight.data.normal_(mean=0.0, std=0.02)
        if embedding.padding_idx is not None:
            embedding.weight.data[embedding.padding_idx].zero_()
        embedding.requires_grad_(False)
        if (reference_embedding is not None and reference_tokenizer is None) or (
            reference_tokenizer is not None and reference_embedding is None
        ):
            raise KeyError(
                ""Reference embedding and reference tokenizer are required to use initialize embeddings from reference""
                "" embedding""
            )
        if reference_embedding is not None and reference_tokenizer is not None:
            tokens = set(tokenizer.get_vocab().keys()) & set(reference_tokenizer.get_vocab().keys())
            reference_vocab = reference_tokenizer.get_vocab()
            vocab = tokenizer.get_vocab()
            for t in tokens:
                idx_reference = reference_vocab[t]
                idx = vocab[t]
                embedding.weight[idx] = (
                    self.loaded_embeddings[reference_embedding].weight[idx_reference].detach().clone()
                )
        embedding.train(False)
        self.loaded_embeddings[name] = embedding
        self.set_active_embeddings(name)",reference_tokenizer is not None and reference_embedding is None,reference_tokenizer is not None is reference_embedding
angr,https://github.com/angr/angr/tree/master/angr/procedures/libc/memcpy.py,memcpy,run$9,"def run(self, dst_addr, src_addr, limit):
        if not self.state.solver.symbolic(limit):
            # not symbolic so we just take the value
            conditional_size = self.state.solver.eval(limit)
        else:
            # constraints on the limit are added during the store
            max_memcpy_size = self.state.libc.max_memcpy_size
            max_limit = self.state.solver.max_int(limit)
            min_limit = self.state.solver.min_int(limit)
            conditional_size = min(max_memcpy_size, max(min_limit, max_limit))
            if max_limit > max_memcpy_size and conditional_size < max_limit:
                l.warning(""memcpy upper bound of %#x outside limit, limiting to %#x instead"",
                          max_limit, conditional_size)

        l.debug(""Memcpy running with conditional_size %#x"", conditional_size)

        if conditional_size > 0:
            src_mem = self.state.memory.load(src_addr, conditional_size, endness='Iend_BE')
            if ABSTRACT_MEMORY in self.state.options:
                self.state.memory.store(dst_addr, src_mem, size=conditional_size, endness='Iend_BE')
            else:
                self.state.memory.store(dst_addr, src_mem, size=limit, endness='Iend_BE')


        return dst_addr",max_limit > max_memcpy_size and conditional_size < max_limit,conditional_size < max_limit > max_memcpy_size
InstaPy,https://github.com/InstaPy/InstaPy/tree/master/instapy/like_util.py,,get_links_for_username$430,"def get_links_for_username(
    browser,
    username,
    person,
    amount,
    logger,
    logfolder,
    randomize=False,
    media=None,
    taggedImages=False,
):
    """"""
    Fetches the number of links specified by amount and returns a list of links
    """"""

    if media is None:
        # All known media types
        media = MEDIA_ALL_TYPES
    elif media == MEDIA_PHOTO:
        # Include posts with multiple images in it
        media = [MEDIA_PHOTO, MEDIA_CAROUSEL]
    else:
        # Make it an array to use it in the following part
        media = [media]

    logger.info(""Getting {} image list..."".format(person))

    user_link = ""https://www.instagram.com/{}/"".format(person)
    if taggedImages:
        user_link = user_link + ""tagged/""

    # if private user, we can get links only if we following
    following_status, _ = get_following_status(
        browser, ""profile"", username, person, None, logger, logfolder
    )

    # Check URL of the webpage, if it already is user's profile page,
    # then do not navigate to it again
    web_address_navigator(browser, user_link)

    if not is_page_available(browser, logger):
        logger.error(
            ""Instagram error: The link you followed may be broken, or the ""
            ""page may have been removed...""
        )
        return False

    # if following_status is None:
    #    browser.wait_for_valid_connection(browser, username, logger)

    # if following_status == 'Follow':
    #    browser.wait_for_valid_authorization(browser, username, logger)

    is_private = is_private_profile(browser, logger, following_status == ""Following"")

    if (
        is_private is None
        or (is_private is True and following_status not in [""Following"", True])
        or (following_status == ""Blocked"")
    ):
        logger.info(
            ""This user is private and we are not following. '{}':'{}'"".format(
                is_private, following_status
            )
        )
        return False

    # Get links
    links = []
    main_elem = browser.find_element(By.TAG_NAME, ""article"")
    posts_count = get_number_of_posts(browser)
    attempt = 0

    if posts_count is not None and amount > posts_count:
        logger.info(
            ""You have requested to get {} posts from {}'s profile page but""
            "" there only {} posts available :D"".format(amount, person, posts_count)
        )
        amount = posts_count

    while len(links) < amount:
        initial_links = links
        browser.execute_script(""window.scrollTo(0, document.body.scrollHeight);"")
        # update server calls after a scroll request
        update_activity(browser, state=None)
        sleep(0.66)

        # using `extend`  or `+=` results reference stay alive which affects
        # previous assignment (can use `copy()` for it)
        main_elem = browser.find_element(By.TAG_NAME, ""article"")
        links = links + get_links(browser, person, logger, media, main_elem)
        links = sorted(set(links), key=links.index)

        if len(links) == len(initial_links):
            if attempt >= 7:
                logger.info(
                    ""There are possibly less posts than {} in {}'s profile ""
                    ""page!"".format(amount, person)
                )
                break
            else:
                attempt += 1
        else:
            attempt = 0

    if randomize is True:
        random.shuffle(links)

    return links[:amount]",posts_count is not None and amount > posts_count,None is not posts_count < amount
nnUNet,https://github.com/MIC-DKFZ/nnUNet/tree/master/nnunet/evaluation/model_selection/figure_out_what_to_submit.py,,main$47,"def main():
    import argparse
    parser = argparse.ArgumentParser(usage=""This is intended to identify the best model based on the five fold ""
                                           ""cross-validation. Running this script requires all models to have been run ""
                                           ""already. This script will summarize the results of the five folds of all ""
                                           ""models in one json each for easy interpretability"")

    parser.add_argument(""-m"", '--models', nargs=""+"", required=False, default=['2d', '3d_lowres', '3d_fullres',
                                                                              '3d_cascade_fullres'])
    parser.add_argument(""-t"", '--task_ids', nargs=""+"", required=True)

    parser.add_argument(""-tr"", type=str, required=False, default=default_trainer,
                        help=""nnUNetTrainer class. Default: %s"" % default_trainer)
    parser.add_argument(""-ctr"", type=str, required=False, default=default_cascade_trainer,
                        help=""nnUNetTrainer class for cascade model. Default: %s"" % default_cascade_trainer)
    parser.add_argument(""-pl"", type=str, required=False, default=default_plans_identifier,
                        help=""plans name, Default: %s"" % default_plans_identifier)
    parser.add_argument('-f', '--folds', nargs='+', default=(0, 1, 2, 3, 4), help=""Use this if you have non-standard ""
                                                                                  ""folds. Experienced users only."")
    parser.add_argument('--disable_ensembling', required=False, default=False, action='store_true',
                        help='Set this flag to disable the use of ensembling. This will find the best single '
                             'configuration for each task.')
    parser.add_argument(""--disable_postprocessing"", required=False, default=False, action=""store_true"",
                        help=""Set this flag if you want to disable the use of postprocessing"")

    args = parser.parse_args()
    tasks = [int(i) for i in args.task_ids]

    models = args.models
    tr = args.tr
    trc = args.ctr
    pl = args.pl
    disable_ensembling = args.disable_ensembling
    disable_postprocessing = args.disable_postprocessing
    folds = tuple(int(i) for i in args.folds)

    validation_folder = ""validation_raw""

    # this script now acts independently from the summary jsons. That was unnecessary
    id_task_mapping = {}

    for t in tasks:
        # first collect pure model performance
        results = {}
        all_results = {}
        valid_models = []
        for m in models:
            if m == ""3d_cascade_fullres"":
                trainer = trc
            else:
                trainer = tr

            if t not in id_task_mapping.keys():
                task_name = find_task_name(get_output_folder_name(m), t)
                id_task_mapping[t] = task_name

            output_folder = get_output_folder_name(m, id_task_mapping[t], trainer, pl)
            if not isdir(output_folder):
                raise RuntimeError(""Output folder for model %s is missing, expected: %s"" % (m, output_folder))

            if disable_postprocessing:
                # we need to collect the predicted niftis from the 5-fold cv and evaluate them against the ground truth
                cv_niftis_folder = join(output_folder, 'cv_niftis_raw')

                if not isfile(join(cv_niftis_folder, 'summary.json')):
                    print(t, m, ': collecting niftis from 5-fold cv')
                    if isdir(cv_niftis_folder):
                        shutil.rmtree(cv_niftis_folder)

                    collect_cv_niftis(output_folder, cv_niftis_folder, validation_folder, folds)

                    niftis_gt = subfiles(join(output_folder, ""gt_niftis""), suffix='.nii.gz', join=False)
                    niftis_cv = subfiles(cv_niftis_folder, suffix='.nii.gz', join=False)
                    if not all([i in niftis_gt for i in niftis_cv]):
                        raise AssertionError(""It does not seem like you trained all the folds! Train "" \
                                             ""all folds first! There are %d gt niftis in %s but only "" \
                                             ""%d predicted niftis in %s"" % (len(niftis_gt), niftis_gt,
                                                                            len(niftis_cv), niftis_cv))

                    # load a summary file so that we can know what class labels to expect
                    summary_fold0 = load_json(join(output_folder, ""fold_%d"" % folds[0], validation_folder,
                                                   ""summary.json""))['results']['mean']
                    # read classes from summary.json
                    classes = tuple((int(i) for i in summary_fold0.keys()))

                    # evaluate the cv niftis
                    print(t, m, ': evaluating 5-fold cv results')
                    evaluate_folder(join(output_folder, ""gt_niftis""), cv_niftis_folder, classes)

            else:
                postprocessing_json = join(output_folder, ""postprocessing.json"")
                cv_niftis_folder = join(output_folder, ""cv_niftis_raw"")

                # we need cv_niftis_postprocessed to know the single model performance. And we need the
                # postprocessing_json. If either of those is missing, rerun consolidate_folds
                if not isfile(postprocessing_json) or not isdir(cv_niftis_folder):
                    print(""running missing postprocessing for %s and model %s"" % (id_task_mapping[t], m))
                    consolidate_folds(output_folder, folds=folds)

                assert isfile(postprocessing_json), ""Postprocessing json missing, expected: %s"" % postprocessing_json
                assert isdir(cv_niftis_folder), ""Folder with niftis from CV missing, expected: %s"" % cv_niftis_folder

            # obtain mean foreground dice
            summary_file = join(cv_niftis_folder, ""summary.json"")
            results[m] = get_mean_foreground_dice(summary_file)
            foreground_mean(summary_file)
            all_results[m] = load_json(summary_file)['results']['mean']
            valid_models.append(m)

        if not disable_ensembling:
            # now run ensembling and add ensembling to results
            print(""\nI will now ensemble combinations of the following models:\n"", valid_models)
            if len(valid_models) > 1:
                for m1, m2 in combinations(valid_models, 2):

                    trainer_m1 = trc if m1 == ""3d_cascade_fullres"" else tr
                    trainer_m2 = trc if m2 == ""3d_cascade_fullres"" else tr

                    ensemble_name = ""ensemble_"" + m1 + ""__"" + trainer_m1 + ""__"" + pl + ""--"" + m2 + ""__"" + trainer_m2 + ""__"" + pl
                    output_folder_base = join(network_training_output_dir, ""ensembles"", id_task_mapping[t], ensemble_name)
                    maybe_mkdir_p(output_folder_base)

                    network1_folder = get_output_folder_name(m1, id_task_mapping[t], trainer_m1, pl)
                    network2_folder = get_output_folder_name(m2, id_task_mapping[t], trainer_m2, pl)

                    print(""ensembling"", network1_folder, network2_folder)
                    ensemble(network1_folder, network2_folder, output_folder_base, id_task_mapping[t], validation_folder, folds, allow_ensembling=not disable_postprocessing)
                    # ensembling will automatically do postprocessingget_foreground_mean

                    # now get result of ensemble
                    results[ensemble_name] = get_mean_foreground_dice(join(output_folder_base, ""ensembled_raw"", ""summary.json""))
                    summary_file = join(output_folder_base, ""ensembled_raw"", ""summary.json"")
                    foreground_mean(summary_file)
                    all_results[ensemble_name] = load_json(summary_file)['results']['mean']

        # now print all mean foreground dice and highlight the best
        foreground_dices = list(results.values())
        best = np.max(foreground_dices)
        for k, v in results.items():
            print(k, v)

        predict_str = """"
        best_model = None
        for k, v in results.items():
            if v == best:
                print(""%s submit model %s"" % (id_task_mapping[t], k), v)
                best_model = k
                print(""\nHere is how you should predict test cases. Run in sequential order and replace all input and output folder names with your personalized ones\n"")
                if k.startswith(""ensemble""):
                    tmp = k[len(""ensemble_""):]
                    model1, model2 = tmp.split(""--"")
                    m1, t1, pl1 = model1.split(""__"")
                    m2, t2, pl2 = model2.split(""__"")
                    predict_str += ""nnUNet_predict -i FOLDER_WITH_TEST_CASES -o OUTPUT_FOLDER_MODEL1 -tr "" + tr + "" -ctr "" + trc + "" -m "" + m1 + "" -p "" + pl + "" -t "" + \
                                   id_task_mapping[t] + "" -z\n""
                    predict_str += ""nnUNet_predict -i FOLDER_WITH_TEST_CASES -o OUTPUT_FOLDER_MODEL2 -tr "" + tr + "" -ctr "" + trc + "" -m "" + m2 + "" -p "" + pl + "" -t "" + \
                                   id_task_mapping[t] + "" -z\n""

                    if not disable_postprocessing:
                        predict_str += ""nnUNet_ensemble -f OUTPUT_FOLDER_MODEL1 OUTPUT_FOLDER_MODEL2 -o OUTPUT_FOLDER -pp "" + join(network_training_output_dir, ""ensembles"", id_task_mapping[t], k, ""postprocessing.json"") + ""\n""
                    else:
                        predict_str += ""nnUNet_ensemble -f OUTPUT_FOLDER_MODEL1 OUTPUT_FOLDER_MODEL2 -o OUTPUT_FOLDER\n""
                else:
                    predict_str += ""nnUNet_predict -i FOLDER_WITH_TEST_CASES -o OUTPUT_FOLDER_MODEL1 -tr "" + tr + "" -ctr "" + trc + "" -m "" + k + "" -p "" + pl + "" -t "" + \
                                   id_task_mapping[t] + ""\n""
                print(predict_str)

        summary_folder = join(network_training_output_dir, ""ensembles"", id_task_mapping[t])
        maybe_mkdir_p(summary_folder)
        with open(join(summary_folder, ""prediction_commands.txt""), 'w') as f:
            f.write(predict_str)

        num_classes = len([i for i in all_results[best_model].keys() if i != 'mean' and i != '0'])
        with open(join(summary_folder, ""summary.csv""), 'w') as f:
            f.write(""model"")
            for c in range(1, num_classes + 1):
                f.write("",class%d"" % c)
            f.write("",average"")
            f.write(""\n"")
            for m in all_results.keys():
                f.write(m)
                for c in range(1, num_classes + 1):
                    f.write("",%01.4f"" % all_results[m][str(c)][""Dice""])
                f.write("",%01.4f"" % all_results[m]['mean'][""Dice""])
                f.write(""\n"")",i != 'mean' and i != '0','mean' != i != '0'
lingvo,https://github.com/tensorflow/lingvo/tree/master/lingvo/base_runner.py,BaseRunner,_LoopEnqueue$526,"def _LoopEnqueue(self, op, session_override=None):
    """"""Runs the enqueue op in a loop. Used by the Trainer and TrainerTpu.""""""
    if py_utils.IsEagerMode():
      raise ValueError('_LoopEnqueue is not supported in eager mode.')
    p = self.params
    sess = session_override or self._GetSession()

    with tf.container(self._container_id), sess:
      if self._initialize_tables is not None:
        sess.run(self._initialize_tables)
      for task in self._model.tasks:
        task.input.Initialize(sess)
      local_enqueue_steps = 0

      # Global enqueue steps measures how many global steps have data enqueued
      # for already. We use this to terminate; note that the enqueue op may
      # hang in session.run if we do not terminate with this check.
      global_enqueue_steps = None

      tf.logging.info('params.train.max_steps: %d, enqueue_max_steps: %d',
                      p.train.max_steps, p.train.enqueue_max_steps)
      while True:
        if self._dequeue_thread_complete:
          tf.logging.info(
              'LoopEnqueue done since consuming thread is done.')
          return

        global_step = sess.run(py_utils.GetGlobalStep())
        if global_enqueue_steps is None:
          global_enqueue_steps = global_step
        if local_enqueue_steps % 1000 == 0 or self._verbose_enqueue_logging:
          tf.logging.info(
              'Current global_enqueue_steps: %d, '
              'local_enqueue_steps: %d, global_step: %d', global_enqueue_steps,
              local_enqueue_steps, global_step)

        if py_utils.use_tpu():
          global_steps_with_available_data = int(global_enqueue_steps //
                                                 p.train.tpu_steps_per_loop *
                                                 p.train.tpu_steps_per_loop)
          # In TPU Training, the training thread in TrainerTpu is responsible
          # for checking early stop via _ShouldEarlyStop.
          check_early_stop = False
        else:
          global_steps_with_available_data = global_enqueue_steps
          check_early_stop = True

        if (self._ShouldStop(sess, global_steps_with_available_data,
                             check_early_stop) or
            self._ShouldStop(sess, global_step, check_early_stop)):
          tf.logging.info('Enqueue loop: Done. ShouldStop is True. Sleeping')
          time.sleep(15)
          continue
        if (p.train.enqueue_max_steps > 0 and
            local_enqueue_steps >= p.train.enqueue_max_steps):
          tf.logging.info('Enqueue loop: Done. train.enqueue_max_steps '
                          'reached. Sleeping.')
          time.sleep(15)
          continue
        local_enqueue_steps += 1

        # There are tpu_infeed_parallelism parallel threads enqueuing.
        # We account for all of them when updating global_enqueue_steps.
        global_enqueue_steps += p.input.tpu_infeed_parallelism

        # Input data stats generated during training are collected and logged in
        # in input generators. The merged summary op for input data stats merges
        # all the scalar summaries for the stats logged from the input
        # generators. If merged scalar summaries for input data stats are
        # available write them to the training directory along with processing
        # the TPU infeed op.
        if self._merged_input_data_summary_op is not None:
          summary_str, _ = sess.run([self._merged_input_data_summary_op, op])
          self._WriteInputDataStatSummaries(summary_str, global_enqueue_steps)
        else:
          sess.run([op])",p.train.enqueue_max_steps > 0 and local_enqueue_steps >= p.train.enqueue_max_steps,local_enqueue_steps >= p.train.enqueue_max_steps > 0
pyowm,https://github.com/csparpa/pyowm/tree/master/pyowm/weatherapi25/weather.py,Weather,__init__$64,"def __init__(self, reference_time, sunset_time, sunrise_time, clouds, rain,
                 snow, wind, humidity, pressure, temperature, status,
                 detailed_status, weather_code, weather_icon_name,
                 visibility_distance, dewpoint, humidex, heat_index,
                 utc_offset=None, uvi=None, precipitation_probability=None):
        if reference_time < 0:
            raise ValueError(""'reference_time' must be greater than 0"")
        self.ref_time = reference_time

        if sunrise_time is not None and sunset_time < 0:
            sunset_time = None
        self.sset_time = sunset_time

        if sunrise_time is not None and sunrise_time < 0:
            sunrise_time = None
        self.srise_time = sunrise_time

        if clouds < 0:
            raise ValueError(""'clouds' must be greater than 0"")
        self.clouds = clouds

        self.rain = rain
        self.snow = snow
        self.wnd = wind

        if humidity < 0:
            raise ValueError(""'humidity' must be greatear than 0"")
        self.humidity = humidity

        self.pressure = pressure
        self.temp = temperature
        self.status = status
        self.detailed_status = detailed_status
        self.weather_code = weather_code
        self.weather_icon_name = weather_icon_name

        if visibility_distance is not None and visibility_distance < 0:
            raise ValueError(""'visibility_distance' must be greater than 0"")
        self.visibility_distance = visibility_distance

        self.dewpoint = dewpoint

        if humidex is not None and humidex < 0:
            raise ValueError(""'humidex' must be greater than 0"")
        self.humidex = humidex

        if heat_index is not None and heat_index < 0:
            raise ValueError(""'heat index' must be grater than 0"")
        self.heat_index = heat_index

        if utc_offset is not None:
            assert isinstance(utc_offset, int), ""utc_offset must be an integer""
        self.utc_offset = utc_offset

        if uvi is not None and uvi < 0:
            raise ValueError(""'uvi' must be grater than or equal to 0"")
        self.uvi = uvi

        if precipitation_probability is not None and \
           (precipitation_probability < 0.0 or precipitation_probability > 1.0):
            raise ValueError(""'precipitation_probability' must be between "" \
                             ""0.0 and 1.0"")
        self.precipitation_probability = precipitation_probability",sunrise_time is not None and sunrise_time < 0,None is not sunrise_time < 0
pyowm,https://github.com/csparpa/pyowm/tree/master/pyowm/weatherapi25/weather.py,Weather,__init__$64,"def __init__(self, reference_time, sunset_time, sunrise_time, clouds, rain,
                 snow, wind, humidity, pressure, temperature, status,
                 detailed_status, weather_code, weather_icon_name,
                 visibility_distance, dewpoint, humidex, heat_index,
                 utc_offset=None, uvi=None, precipitation_probability=None):
        if reference_time < 0:
            raise ValueError(""'reference_time' must be greater than 0"")
        self.ref_time = reference_time

        if sunrise_time is not None and sunset_time < 0:
            sunset_time = None
        self.sset_time = sunset_time

        if sunrise_time is not None and sunrise_time < 0:
            sunrise_time = None
        self.srise_time = sunrise_time

        if clouds < 0:
            raise ValueError(""'clouds' must be greater than 0"")
        self.clouds = clouds

        self.rain = rain
        self.snow = snow
        self.wnd = wind

        if humidity < 0:
            raise ValueError(""'humidity' must be greatear than 0"")
        self.humidity = humidity

        self.pressure = pressure
        self.temp = temperature
        self.status = status
        self.detailed_status = detailed_status
        self.weather_code = weather_code
        self.weather_icon_name = weather_icon_name

        if visibility_distance is not None and visibility_distance < 0:
            raise ValueError(""'visibility_distance' must be greater than 0"")
        self.visibility_distance = visibility_distance

        self.dewpoint = dewpoint

        if humidex is not None and humidex < 0:
            raise ValueError(""'humidex' must be greater than 0"")
        self.humidex = humidex

        if heat_index is not None and heat_index < 0:
            raise ValueError(""'heat index' must be grater than 0"")
        self.heat_index = heat_index

        if utc_offset is not None:
            assert isinstance(utc_offset, int), ""utc_offset must be an integer""
        self.utc_offset = utc_offset

        if uvi is not None and uvi < 0:
            raise ValueError(""'uvi' must be grater than or equal to 0"")
        self.uvi = uvi

        if precipitation_probability is not None and \
           (precipitation_probability < 0.0 or precipitation_probability > 1.0):
            raise ValueError(""'precipitation_probability' must be between "" \
                             ""0.0 and 1.0"")
        self.precipitation_probability = precipitation_probability",visibility_distance is not None and visibility_distance < 0,None is not visibility_distance < 0
pyowm,https://github.com/csparpa/pyowm/tree/master/pyowm/weatherapi25/weather.py,Weather,__init__$64,"def __init__(self, reference_time, sunset_time, sunrise_time, clouds, rain,
                 snow, wind, humidity, pressure, temperature, status,
                 detailed_status, weather_code, weather_icon_name,
                 visibility_distance, dewpoint, humidex, heat_index,
                 utc_offset=None, uvi=None, precipitation_probability=None):
        if reference_time < 0:
            raise ValueError(""'reference_time' must be greater than 0"")
        self.ref_time = reference_time

        if sunrise_time is not None and sunset_time < 0:
            sunset_time = None
        self.sset_time = sunset_time

        if sunrise_time is not None and sunrise_time < 0:
            sunrise_time = None
        self.srise_time = sunrise_time

        if clouds < 0:
            raise ValueError(""'clouds' must be greater than 0"")
        self.clouds = clouds

        self.rain = rain
        self.snow = snow
        self.wnd = wind

        if humidity < 0:
            raise ValueError(""'humidity' must be greatear than 0"")
        self.humidity = humidity

        self.pressure = pressure
        self.temp = temperature
        self.status = status
        self.detailed_status = detailed_status
        self.weather_code = weather_code
        self.weather_icon_name = weather_icon_name

        if visibility_distance is not None and visibility_distance < 0:
            raise ValueError(""'visibility_distance' must be greater than 0"")
        self.visibility_distance = visibility_distance

        self.dewpoint = dewpoint

        if humidex is not None and humidex < 0:
            raise ValueError(""'humidex' must be greater than 0"")
        self.humidex = humidex

        if heat_index is not None and heat_index < 0:
            raise ValueError(""'heat index' must be grater than 0"")
        self.heat_index = heat_index

        if utc_offset is not None:
            assert isinstance(utc_offset, int), ""utc_offset must be an integer""
        self.utc_offset = utc_offset

        if uvi is not None and uvi < 0:
            raise ValueError(""'uvi' must be grater than or equal to 0"")
        self.uvi = uvi

        if precipitation_probability is not None and \
           (precipitation_probability < 0.0 or precipitation_probability > 1.0):
            raise ValueError(""'precipitation_probability' must be between "" \
                             ""0.0 and 1.0"")
        self.precipitation_probability = precipitation_probability",humidex is not None and humidex < 0,None is not humidex < 0
pyowm,https://github.com/csparpa/pyowm/tree/master/pyowm/weatherapi25/weather.py,Weather,__init__$64,"def __init__(self, reference_time, sunset_time, sunrise_time, clouds, rain,
                 snow, wind, humidity, pressure, temperature, status,
                 detailed_status, weather_code, weather_icon_name,
                 visibility_distance, dewpoint, humidex, heat_index,
                 utc_offset=None, uvi=None, precipitation_probability=None):
        if reference_time < 0:
            raise ValueError(""'reference_time' must be greater than 0"")
        self.ref_time = reference_time

        if sunrise_time is not None and sunset_time < 0:
            sunset_time = None
        self.sset_time = sunset_time

        if sunrise_time is not None and sunrise_time < 0:
            sunrise_time = None
        self.srise_time = sunrise_time

        if clouds < 0:
            raise ValueError(""'clouds' must be greater than 0"")
        self.clouds = clouds

        self.rain = rain
        self.snow = snow
        self.wnd = wind

        if humidity < 0:
            raise ValueError(""'humidity' must be greatear than 0"")
        self.humidity = humidity

        self.pressure = pressure
        self.temp = temperature
        self.status = status
        self.detailed_status = detailed_status
        self.weather_code = weather_code
        self.weather_icon_name = weather_icon_name

        if visibility_distance is not None and visibility_distance < 0:
            raise ValueError(""'visibility_distance' must be greater than 0"")
        self.visibility_distance = visibility_distance

        self.dewpoint = dewpoint

        if humidex is not None and humidex < 0:
            raise ValueError(""'humidex' must be greater than 0"")
        self.humidex = humidex

        if heat_index is not None and heat_index < 0:
            raise ValueError(""'heat index' must be grater than 0"")
        self.heat_index = heat_index

        if utc_offset is not None:
            assert isinstance(utc_offset, int), ""utc_offset must be an integer""
        self.utc_offset = utc_offset

        if uvi is not None and uvi < 0:
            raise ValueError(""'uvi' must be grater than or equal to 0"")
        self.uvi = uvi

        if precipitation_probability is not None and \
           (precipitation_probability < 0.0 or precipitation_probability > 1.0):
            raise ValueError(""'precipitation_probability' must be between "" \
                             ""0.0 and 1.0"")
        self.precipitation_probability = precipitation_probability",heat_index is not None and heat_index < 0,None is not heat_index < 0
pyowm,https://github.com/csparpa/pyowm/tree/master/pyowm/weatherapi25/weather.py,Weather,__init__$64,"def __init__(self, reference_time, sunset_time, sunrise_time, clouds, rain,
                 snow, wind, humidity, pressure, temperature, status,
                 detailed_status, weather_code, weather_icon_name,
                 visibility_distance, dewpoint, humidex, heat_index,
                 utc_offset=None, uvi=None, precipitation_probability=None):
        if reference_time < 0:
            raise ValueError(""'reference_time' must be greater than 0"")
        self.ref_time = reference_time

        if sunrise_time is not None and sunset_time < 0:
            sunset_time = None
        self.sset_time = sunset_time

        if sunrise_time is not None and sunrise_time < 0:
            sunrise_time = None
        self.srise_time = sunrise_time

        if clouds < 0:
            raise ValueError(""'clouds' must be greater than 0"")
        self.clouds = clouds

        self.rain = rain
        self.snow = snow
        self.wnd = wind

        if humidity < 0:
            raise ValueError(""'humidity' must be greatear than 0"")
        self.humidity = humidity

        self.pressure = pressure
        self.temp = temperature
        self.status = status
        self.detailed_status = detailed_status
        self.weather_code = weather_code
        self.weather_icon_name = weather_icon_name

        if visibility_distance is not None and visibility_distance < 0:
            raise ValueError(""'visibility_distance' must be greater than 0"")
        self.visibility_distance = visibility_distance

        self.dewpoint = dewpoint

        if humidex is not None and humidex < 0:
            raise ValueError(""'humidex' must be greater than 0"")
        self.humidex = humidex

        if heat_index is not None and heat_index < 0:
            raise ValueError(""'heat index' must be grater than 0"")
        self.heat_index = heat_index

        if utc_offset is not None:
            assert isinstance(utc_offset, int), ""utc_offset must be an integer""
        self.utc_offset = utc_offset

        if uvi is not None and uvi < 0:
            raise ValueError(""'uvi' must be grater than or equal to 0"")
        self.uvi = uvi

        if precipitation_probability is not None and \
           (precipitation_probability < 0.0 or precipitation_probability > 1.0):
            raise ValueError(""'precipitation_probability' must be between "" \
                             ""0.0 and 1.0"")
        self.precipitation_probability = precipitation_probability",uvi is not None and uvi < 0,None is not uvi < 0
sentry,https://github.com/getsentry/sentry/tree/master/src/sentry/tasks/integrations/migrate_repo.py,,migrate_repo$13,"def migrate_repo(repo_id: int, integration_id: int, organization_id: int) -> None:
    integration = Integration.objects.get(id=integration_id)
    installation = integration.get_installation(organization_id=organization_id)
    repo = Repository.objects.get(id=repo_id)
    if installation.has_repo_access(repo):
        # This probably shouldn't happen, but log it just in case.
        if repo.integration_id is not None and repo.integration_id != integration_id:
            logger.info(
                ""repo.migration.integration-change"",
                extra={
                    ""integration_id"": integration_id,
                    ""old_integration_id"": repo.integration_id,
                    ""organization_id"": organization_id,
                    ""repo_id"": repo.id,
                },
            )

        repo.integration_id = integration_id
        repo.provider = f""integrations:{integration.provider}""
        # Check against disabled specifically -- don't want to accidentally un-delete repos.
        original_status = repo.status
        if repo.status == ObjectStatus.DISABLED:
            repo.status = ObjectStatus.VISIBLE
        repo.save()
        logger.info(
            ""repo.migrated"",
            extra={
                ""integration_id"": integration_id,
                ""organization_id"": organization_id,
                ""repo_id"": repo.id,
                ""original_status"": original_status,
            },
        )

        from sentry.mediators.plugins import Migrator

        Migrator.run(
            integration=integration, organization=Organization.objects.get(id=organization_id)
        )",repo.integration_id is not None and repo.integration_id != integration_id,None is not repo.integration_id != integration_id
DSB2017,https://github.com/lfz/DSB2017/tree/master/training/detector/data.py,,augment$130,"def augment(sample, target, bboxes, coord, ifflip = True, ifrotate=True, ifswap = True):
    #                     angle1 = np.random.rand()*180
    if ifrotate:
        validrot = False
        counter = 0
        while not validrot:
            newtarget = np.copy(target)
            angle1 = np.random.rand()*180
            size = np.array(sample.shape[2:4]).astype('float')
            rotmat = np.array([[np.cos(angle1/180*np.pi),-np.sin(angle1/180*np.pi)],[np.sin(angle1/180*np.pi),np.cos(angle1/180*np.pi)]])
            newtarget[1:3] = np.dot(rotmat,target[1:3]-size/2)+size/2
            if np.all(newtarget[:3]>target[3]) and np.all(newtarget[:3]< np.array(sample.shape[1:4])-newtarget[3]):
                validrot = True
                target = newtarget
                sample = rotate(sample,angle1,axes=(2,3),reshape=False)
                coord = rotate(coord,angle1,axes=(2,3),reshape=False)
                for box in bboxes:
                    box[1:3] = np.dot(rotmat,box[1:3]-size/2)+size/2
            else:
                counter += 1
                if counter ==3:
                    break
    if ifswap:
        if sample.shape[1]==sample.shape[2] and sample.shape[1]==sample.shape[3]:
            axisorder = np.random.permutation(3)
            sample = np.transpose(sample,np.concatenate([[0],axisorder+1]))
            coord = np.transpose(coord,np.concatenate([[0],axisorder+1]))
            target[:3] = target[:3][axisorder]
            bboxes[:,:3] = bboxes[:,:3][:,axisorder]
            
    if ifflip:
#         flipid = np.array([np.random.randint(2),np.random.randint(2),np.random.randint(2)])*2-1
        flipid = np.array([1,np.random.randint(2),np.random.randint(2)])*2-1
        sample = np.ascontiguousarray(sample[:,::flipid[0],::flipid[1],::flipid[2]])
        coord = np.ascontiguousarray(coord[:,::flipid[0],::flipid[1],::flipid[2]])
        for ax in range(3):
            if flipid[ax]==-1:
                target[ax] = np.array(sample.shape[ax+1])-target[ax]
                bboxes[:,ax]= np.array(sample.shape[ax+1])-bboxes[:,ax]
    return sample, target, bboxes, coord",sample.shape[1] == sample.shape[2] and sample.shape[1] == sample.shape[3],sample.shape[2] == sample.shape[1] == sample.shape[3]
demon,https://github.com/lmb-freiburg/demon/tree/master/python/depthmotionnet/dataset_tools/helpers.py,,safe_crop_array2d$106,"def safe_crop_array2d(arr, box, fill_value):
    """"""crops an array and adds a border if necessary
    
    arr: numpy.ndarray with 2 dims

    box: 4 tuple
        (x0,y0,x1,y1) tuple. x is the column and y is the row!

    fill_value: scalar

    Returns the cropped array
    """"""
    x0, y0, x1, y1 = box
    if x0 >=0 and y0 >= 0 and x1 < arr.shape[1] and y1 < arr.shape[0]:
        return arr[y0:y1,x0:x1]
    else:
        crop_width = x1-x0
        crop_height = y1-y0
        tmp = np.full((crop_height, crop_width), fill_value, dtype=arr.dtype)
        safe_box = (
            max(0,min(x0,arr.shape[1]-1)),
            max(0,min(y0,arr.shape[0]-1)),
            max(0,min(x1,arr.shape[1])),
            max(0,min(y1,arr.shape[0])),
            )
        x = -x0 if x0 < 0 else 0
        y = -y0 if y0 < 0 else 0
        safe_width = safe_box[2]-safe_box[0]
        safe_height = safe_box[3]-safe_box[1]
        tmp[y:y+safe_height,x:x+safe_width] = arr[safe_box[1]:safe_box[3],safe_box[0]:safe_box[2]]
        return tmp",x0 >= 0 and y0 >= 0 and (x1 < arr.shape[1]) and (y1 < arr.shape[0]),x0 >= 0 <= y0 and x1 < arr.shape[1] and (y1 < arr.shape[0])
LXC-Web-Panel,https://github.com/lxc-webpanel/LXC-Web-Panel/tree/master//lwp.py,,edit$141,"def edit(container=None):
    '''
    edit containers page and actions if form post request
    '''

    if 'logged_in' in session:
        host_memory = lwp.host_memory_usage()
        if request.method == 'POST':
            cfg = lwp.get_container_settings(container)
            ip_regex = '(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?).(25[0-5]' \
                       '|2[0-4][0-9]|[01]?[0-9][0-9]?).(25[0-5]|2[0-4]' \
                       '[0-9]|[01]?[0-9][0-9]?).(25[0-5]|2[0-4][0-9]|[01]' \
                       '?[0-9][0-9]?)(/(3[0-2]|[12]?[0-9]))?'
            info = lxc.info(container)

            form = {}
            form['type'] = request.form['type']
            form['link'] = request.form['link']
            try:
                form['flags'] = request.form['flags']
            except KeyError:
                form['flags'] = 'down'
            form['hwaddr'] = request.form['hwaddress']
            form['rootfs'] = request.form['rootfs']
            form['utsname'] = request.form['hostname']
            form['ipv4'] = request.form['ipaddress']
            form['memlimit'] = request.form['memlimit']
            form['swlimit'] = request.form['swlimit']
            form['cpus'] = request.form['cpus']
            form['shares'] = request.form['cpushares']
            try:
                form['autostart'] = request.form['autostart']
            except KeyError:
                form['autostart'] = False

            if form['utsname'] != cfg['utsname'] and \
                    re.match('(?!^containers$)|^(([a-zA-Z0-9]|[a-zA-Z0-9]'
                             '[a-zA-Z0-9\-]*[a-zA-Z0-9])\.)*([A-Za-z0-9]|'
                             '[A-Za-z0-9][A-Za-z0-9\-]*[A-Za-z0-9])$',
                             form['utsname']):
                lwp.push_config_value('lxc.utsname', form['utsname'],
                                      container=container)
                flash(u'Hostname updated for %s!' % container, 'success')

            if form['flags'] != cfg['flags'] and \
                    re.match('^(up|down)$', form['flags']):
                lwp.push_config_value('lxc.network.flags', form['flags'],
                                      container=container)
                flash(u'Network flag updated for %s!' % container, 'success')

            if form['type'] != cfg['type'] and \
                    re.match('^\w+$', form['type']):
                lwp.push_config_value('lxc.network.type', form['type'],
                                      container=container)
                flash(u'Link type updated for %s!' % container, 'success')

            if form['link'] != cfg['link'] and \
                    re.match('^[a-zA-Z0-9_-]+$', form['link']):
                lwp.push_config_value('lxc.network.link', form['link'],
                                      container=container)
                flash(u'Link name updated for %s!' % container, 'success')

            if form['hwaddr'] != cfg['hwaddr'] and \
                    re.match('^([a-fA-F0-9]{2}[:|\-]?){6}$', form['hwaddr']):
                lwp.push_config_value('lxc.network.hwaddr', form['hwaddr'],
                                      container=container)
                flash(u'Hardware address updated for %s!' % container,
                      'success')

            if (not form['ipv4'] and form['ipv4'] != cfg['ipv4']) or \
                    (form['ipv4'] != cfg['ipv4'] and
                     re.match('^%s$' % ip_regex, form['ipv4'])):
                lwp.push_config_value('lxc.network.ipv4', form['ipv4'],
                                      container=container)
                flash(u'IP address updated for %s!' % container, 'success')

            if form['memlimit'] != cfg['memlimit'] and \
                    form['memlimit'].isdigit() and \
                    int(form['memlimit']) <= int(host_memory['total']):
                if int(form['memlimit']) == int(host_memory['total']):
                    form['memlimit'] = ''

                if form['memlimit'] != cfg['memlimit']:
                    lwp.push_config_value('lxc.cgroup.memory.limit_in_bytes',
                                          form['memlimit'],
                                          container=container)
                    if info[""state""].lower() != 'stopped':
                        lxc.cgroup(container,
                                   'lxc.cgroup.memory.limit_in_bytes',
                                   form['memlimit'])
                    flash(u'Memory limit updated for %s!' % container,
                          'success')

            if form['swlimit'] != cfg['swlimit'] and \
                    form['swlimit'].isdigit() and \
                    int(form['swlimit']) <= int(host_memory['total'] * 2):
                if int(form['swlimit']) == int(host_memory['total'] * 2):
                    form['swlimit'] = ''

                if form['swlimit'].isdigit():
                    form['swlimit'] = int(form['swlimit'])

                if form['memlimit'].isdigit():
                    form['memlimit'] = int(form['memlimit'])

                if (form['memlimit'] == '' and form['swlimit'] != '') or \
                        (form['memlimit'] > form['swlimit'] and
                         form['swlimit'] != ''):
                    flash(u'Can\'t assign swap memory lower than'
                          ' the memory limit', 'warning')

                elif form['swlimit'] != cfg['swlimit'] and \
                        form['memlimit'] <= form['swlimit']:
                    lwp.push_config_value(
                        'lxc.cgroup.memory.memsw.limit_in_bytes',
                        form['swlimit'], container=container)

                    if info[""state""].lower() != 'stopped':
                        lxc.cgroup(container,
                                   'lxc.cgroup.memory.memsw.limit_in_bytes',
                                   form['swlimit'])
                    flash(u'Swap limit updated for %s!' % container, 'success')

            if (not form['cpus'] and form['cpus'] != cfg['cpus']) or \
                    (form['cpus'] != cfg['cpus'] and
                     re.match('^[0-9,-]+$', form['cpus'])):
                lwp.push_config_value('lxc.cgroup.cpuset.cpus', form['cpus'],
                                      container=container)

                if info[""state""].lower() != 'stopped':
                        lxc.cgroup(container, 'lxc.cgroup.cpuset.cpus',
                                   form['cpus'])
                flash(u'CPUs updated for %s!' % container, 'success')

            if (not form['shares'] and form['shares'] != cfg['shares']) or \
                    (form['shares'] != cfg['shares'] and
                     re.match('^[0-9]+$', form['shares'])):
                lwp.push_config_value('lxc.cgroup.cpu.shares', form['shares'],
                                      container=container)
                if info[""state""].lower() != 'stopped':
                        lxc.cgroup(container, 'lxc.cgroup.cpu.shares',
                                   form['shares'])
                flash(u'CPU shares updated for %s!' % container, 'success')

            if form['rootfs'] != cfg['rootfs'] and \
                    re.match('^[a-zA-Z0-9_/\-\.]+', form['rootfs']):
                lwp.push_config_value('lxc.rootfs', form['rootfs'],
                                      container=container)
                flash(u'Rootfs updated!' % container, 'success')

            auto = lwp.ls_auto()
            if form['autostart'] == 'True' and \
                    not ('%s.conf' % container) in auto:
                try:
                    os.symlink('/var/lib/lxc/%s/config' % container,
                               '/etc/lxc/auto/%s.conf' % container)
                    flash(u'Autostart enabled for %s' % container, 'success')
                except OSError:
                    flash(u'Unable to create symlink \'/etc/lxc/auto/%s.conf\''
                          % container, 'error')
            elif not form['autostart'] and ('%s.conf' % container) in auto:
                try:
                    os.remove('/etc/lxc/auto/%s.conf' % container)
                    flash(u'Autostart disabled for %s' % container, 'success')
                except OSError:
                    flash(u'Unable to remove symlink', 'error')

        info = lxc.info(container)
        status = info['state']
        pid = info['pid']

        infos = {'status': status,
                 'pid': pid,
                 'memusg': lwp.memory_usage(container)}
        return render_template('edit.html', containers=lxc.ls(),
                               container=container, infos=infos,
                               settings=lwp.get_container_settings(container),
                               host_memory=host_memory)
    return render_template('login.html')",form['memlimit'] == '' and form['swlimit'] != '',form['memlimit'] == '' != form['swlimit']
LXC-Web-Panel,https://github.com/lxc-webpanel/LXC-Web-Panel/tree/master//lwp.py,,edit$141,"def edit(container=None):
    '''
    edit containers page and actions if form post request
    '''

    if 'logged_in' in session:
        host_memory = lwp.host_memory_usage()
        if request.method == 'POST':
            cfg = lwp.get_container_settings(container)
            ip_regex = '(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?).(25[0-5]' \
                       '|2[0-4][0-9]|[01]?[0-9][0-9]?).(25[0-5]|2[0-4]' \
                       '[0-9]|[01]?[0-9][0-9]?).(25[0-5]|2[0-4][0-9]|[01]' \
                       '?[0-9][0-9]?)(/(3[0-2]|[12]?[0-9]))?'
            info = lxc.info(container)

            form = {}
            form['type'] = request.form['type']
            form['link'] = request.form['link']
            try:
                form['flags'] = request.form['flags']
            except KeyError:
                form['flags'] = 'down'
            form['hwaddr'] = request.form['hwaddress']
            form['rootfs'] = request.form['rootfs']
            form['utsname'] = request.form['hostname']
            form['ipv4'] = request.form['ipaddress']
            form['memlimit'] = request.form['memlimit']
            form['swlimit'] = request.form['swlimit']
            form['cpus'] = request.form['cpus']
            form['shares'] = request.form['cpushares']
            try:
                form['autostart'] = request.form['autostart']
            except KeyError:
                form['autostart'] = False

            if form['utsname'] != cfg['utsname'] and \
                    re.match('(?!^containers$)|^(([a-zA-Z0-9]|[a-zA-Z0-9]'
                             '[a-zA-Z0-9\-]*[a-zA-Z0-9])\.)*([A-Za-z0-9]|'
                             '[A-Za-z0-9][A-Za-z0-9\-]*[A-Za-z0-9])$',
                             form['utsname']):
                lwp.push_config_value('lxc.utsname', form['utsname'],
                                      container=container)
                flash(u'Hostname updated for %s!' % container, 'success')

            if form['flags'] != cfg['flags'] and \
                    re.match('^(up|down)$', form['flags']):
                lwp.push_config_value('lxc.network.flags', form['flags'],
                                      container=container)
                flash(u'Network flag updated for %s!' % container, 'success')

            if form['type'] != cfg['type'] and \
                    re.match('^\w+$', form['type']):
                lwp.push_config_value('lxc.network.type', form['type'],
                                      container=container)
                flash(u'Link type updated for %s!' % container, 'success')

            if form['link'] != cfg['link'] and \
                    re.match('^[a-zA-Z0-9_-]+$', form['link']):
                lwp.push_config_value('lxc.network.link', form['link'],
                                      container=container)
                flash(u'Link name updated for %s!' % container, 'success')

            if form['hwaddr'] != cfg['hwaddr'] and \
                    re.match('^([a-fA-F0-9]{2}[:|\-]?){6}$', form['hwaddr']):
                lwp.push_config_value('lxc.network.hwaddr', form['hwaddr'],
                                      container=container)
                flash(u'Hardware address updated for %s!' % container,
                      'success')

            if (not form['ipv4'] and form['ipv4'] != cfg['ipv4']) or \
                    (form['ipv4'] != cfg['ipv4'] and
                     re.match('^%s$' % ip_regex, form['ipv4'])):
                lwp.push_config_value('lxc.network.ipv4', form['ipv4'],
                                      container=container)
                flash(u'IP address updated for %s!' % container, 'success')

            if form['memlimit'] != cfg['memlimit'] and \
                    form['memlimit'].isdigit() and \
                    int(form['memlimit']) <= int(host_memory['total']):
                if int(form['memlimit']) == int(host_memory['total']):
                    form['memlimit'] = ''

                if form['memlimit'] != cfg['memlimit']:
                    lwp.push_config_value('lxc.cgroup.memory.limit_in_bytes',
                                          form['memlimit'],
                                          container=container)
                    if info[""state""].lower() != 'stopped':
                        lxc.cgroup(container,
                                   'lxc.cgroup.memory.limit_in_bytes',
                                   form['memlimit'])
                    flash(u'Memory limit updated for %s!' % container,
                          'success')

            if form['swlimit'] != cfg['swlimit'] and \
                    form['swlimit'].isdigit() and \
                    int(form['swlimit']) <= int(host_memory['total'] * 2):
                if int(form['swlimit']) == int(host_memory['total'] * 2):
                    form['swlimit'] = ''

                if form['swlimit'].isdigit():
                    form['swlimit'] = int(form['swlimit'])

                if form['memlimit'].isdigit():
                    form['memlimit'] = int(form['memlimit'])

                if (form['memlimit'] == '' and form['swlimit'] != '') or \
                        (form['memlimit'] > form['swlimit'] and
                         form['swlimit'] != ''):
                    flash(u'Can\'t assign swap memory lower than'
                          ' the memory limit', 'warning')

                elif form['swlimit'] != cfg['swlimit'] and \
                        form['memlimit'] <= form['swlimit']:
                    lwp.push_config_value(
                        'lxc.cgroup.memory.memsw.limit_in_bytes',
                        form['swlimit'], container=container)

                    if info[""state""].lower() != 'stopped':
                        lxc.cgroup(container,
                                   'lxc.cgroup.memory.memsw.limit_in_bytes',
                                   form['swlimit'])
                    flash(u'Swap limit updated for %s!' % container, 'success')

            if (not form['cpus'] and form['cpus'] != cfg['cpus']) or \
                    (form['cpus'] != cfg['cpus'] and
                     re.match('^[0-9,-]+$', form['cpus'])):
                lwp.push_config_value('lxc.cgroup.cpuset.cpus', form['cpus'],
                                      container=container)

                if info[""state""].lower() != 'stopped':
                        lxc.cgroup(container, 'lxc.cgroup.cpuset.cpus',
                                   form['cpus'])
                flash(u'CPUs updated for %s!' % container, 'success')

            if (not form['shares'] and form['shares'] != cfg['shares']) or \
                    (form['shares'] != cfg['shares'] and
                     re.match('^[0-9]+$', form['shares'])):
                lwp.push_config_value('lxc.cgroup.cpu.shares', form['shares'],
                                      container=container)
                if info[""state""].lower() != 'stopped':
                        lxc.cgroup(container, 'lxc.cgroup.cpu.shares',
                                   form['shares'])
                flash(u'CPU shares updated for %s!' % container, 'success')

            if form['rootfs'] != cfg['rootfs'] and \
                    re.match('^[a-zA-Z0-9_/\-\.]+', form['rootfs']):
                lwp.push_config_value('lxc.rootfs', form['rootfs'],
                                      container=container)
                flash(u'Rootfs updated!' % container, 'success')

            auto = lwp.ls_auto()
            if form['autostart'] == 'True' and \
                    not ('%s.conf' % container) in auto:
                try:
                    os.symlink('/var/lib/lxc/%s/config' % container,
                               '/etc/lxc/auto/%s.conf' % container)
                    flash(u'Autostart enabled for %s' % container, 'success')
                except OSError:
                    flash(u'Unable to create symlink \'/etc/lxc/auto/%s.conf\''
                          % container, 'error')
            elif not form['autostart'] and ('%s.conf' % container) in auto:
                try:
                    os.remove('/etc/lxc/auto/%s.conf' % container)
                    flash(u'Autostart disabled for %s' % container, 'success')
                except OSError:
                    flash(u'Unable to remove symlink', 'error')

        info = lxc.info(container)
        status = info['state']
        pid = info['pid']

        infos = {'status': status,
                 'pid': pid,
                 'memusg': lwp.memory_usage(container)}
        return render_template('edit.html', containers=lxc.ls(),
                               container=container, infos=infos,
                               settings=lwp.get_container_settings(container),
                               host_memory=host_memory)
    return render_template('login.html')",form['memlimit'] > form['swlimit'] and form['swlimit'] != '',form['memlimit'] > form['swlimit'] != ''
LXC-Web-Panel,https://github.com/lxc-webpanel/LXC-Web-Panel/tree/master//lwp.py,,edit$141,"def edit(container=None):
    '''
    edit containers page and actions if form post request
    '''

    if 'logged_in' in session:
        host_memory = lwp.host_memory_usage()
        if request.method == 'POST':
            cfg = lwp.get_container_settings(container)
            ip_regex = '(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?).(25[0-5]' \
                       '|2[0-4][0-9]|[01]?[0-9][0-9]?).(25[0-5]|2[0-4]' \
                       '[0-9]|[01]?[0-9][0-9]?).(25[0-5]|2[0-4][0-9]|[01]' \
                       '?[0-9][0-9]?)(/(3[0-2]|[12]?[0-9]))?'
            info = lxc.info(container)

            form = {}
            form['type'] = request.form['type']
            form['link'] = request.form['link']
            try:
                form['flags'] = request.form['flags']
            except KeyError:
                form['flags'] = 'down'
            form['hwaddr'] = request.form['hwaddress']
            form['rootfs'] = request.form['rootfs']
            form['utsname'] = request.form['hostname']
            form['ipv4'] = request.form['ipaddress']
            form['memlimit'] = request.form['memlimit']
            form['swlimit'] = request.form['swlimit']
            form['cpus'] = request.form['cpus']
            form['shares'] = request.form['cpushares']
            try:
                form['autostart'] = request.form['autostart']
            except KeyError:
                form['autostart'] = False

            if form['utsname'] != cfg['utsname'] and \
                    re.match('(?!^containers$)|^(([a-zA-Z0-9]|[a-zA-Z0-9]'
                             '[a-zA-Z0-9\-]*[a-zA-Z0-9])\.)*([A-Za-z0-9]|'
                             '[A-Za-z0-9][A-Za-z0-9\-]*[A-Za-z0-9])$',
                             form['utsname']):
                lwp.push_config_value('lxc.utsname', form['utsname'],
                                      container=container)
                flash(u'Hostname updated for %s!' % container, 'success')

            if form['flags'] != cfg['flags'] and \
                    re.match('^(up|down)$', form['flags']):
                lwp.push_config_value('lxc.network.flags', form['flags'],
                                      container=container)
                flash(u'Network flag updated for %s!' % container, 'success')

            if form['type'] != cfg['type'] and \
                    re.match('^\w+$', form['type']):
                lwp.push_config_value('lxc.network.type', form['type'],
                                      container=container)
                flash(u'Link type updated for %s!' % container, 'success')

            if form['link'] != cfg['link'] and \
                    re.match('^[a-zA-Z0-9_-]+$', form['link']):
                lwp.push_config_value('lxc.network.link', form['link'],
                                      container=container)
                flash(u'Link name updated for %s!' % container, 'success')

            if form['hwaddr'] != cfg['hwaddr'] and \
                    re.match('^([a-fA-F0-9]{2}[:|\-]?){6}$', form['hwaddr']):
                lwp.push_config_value('lxc.network.hwaddr', form['hwaddr'],
                                      container=container)
                flash(u'Hardware address updated for %s!' % container,
                      'success')

            if (not form['ipv4'] and form['ipv4'] != cfg['ipv4']) or \
                    (form['ipv4'] != cfg['ipv4'] and
                     re.match('^%s$' % ip_regex, form['ipv4'])):
                lwp.push_config_value('lxc.network.ipv4', form['ipv4'],
                                      container=container)
                flash(u'IP address updated for %s!' % container, 'success')

            if form['memlimit'] != cfg['memlimit'] and \
                    form['memlimit'].isdigit() and \
                    int(form['memlimit']) <= int(host_memory['total']):
                if int(form['memlimit']) == int(host_memory['total']):
                    form['memlimit'] = ''

                if form['memlimit'] != cfg['memlimit']:
                    lwp.push_config_value('lxc.cgroup.memory.limit_in_bytes',
                                          form['memlimit'],
                                          container=container)
                    if info[""state""].lower() != 'stopped':
                        lxc.cgroup(container,
                                   'lxc.cgroup.memory.limit_in_bytes',
                                   form['memlimit'])
                    flash(u'Memory limit updated for %s!' % container,
                          'success')

            if form['swlimit'] != cfg['swlimit'] and \
                    form['swlimit'].isdigit() and \
                    int(form['swlimit']) <= int(host_memory['total'] * 2):
                if int(form['swlimit']) == int(host_memory['total'] * 2):
                    form['swlimit'] = ''

                if form['swlimit'].isdigit():
                    form['swlimit'] = int(form['swlimit'])

                if form['memlimit'].isdigit():
                    form['memlimit'] = int(form['memlimit'])

                if (form['memlimit'] == '' and form['swlimit'] != '') or \
                        (form['memlimit'] > form['swlimit'] and
                         form['swlimit'] != ''):
                    flash(u'Can\'t assign swap memory lower than'
                          ' the memory limit', 'warning')

                elif form['swlimit'] != cfg['swlimit'] and \
                        form['memlimit'] <= form['swlimit']:
                    lwp.push_config_value(
                        'lxc.cgroup.memory.memsw.limit_in_bytes',
                        form['swlimit'], container=container)

                    if info[""state""].lower() != 'stopped':
                        lxc.cgroup(container,
                                   'lxc.cgroup.memory.memsw.limit_in_bytes',
                                   form['swlimit'])
                    flash(u'Swap limit updated for %s!' % container, 'success')

            if (not form['cpus'] and form['cpus'] != cfg['cpus']) or \
                    (form['cpus'] != cfg['cpus'] and
                     re.match('^[0-9,-]+$', form['cpus'])):
                lwp.push_config_value('lxc.cgroup.cpuset.cpus', form['cpus'],
                                      container=container)

                if info[""state""].lower() != 'stopped':
                        lxc.cgroup(container, 'lxc.cgroup.cpuset.cpus',
                                   form['cpus'])
                flash(u'CPUs updated for %s!' % container, 'success')

            if (not form['shares'] and form['shares'] != cfg['shares']) or \
                    (form['shares'] != cfg['shares'] and
                     re.match('^[0-9]+$', form['shares'])):
                lwp.push_config_value('lxc.cgroup.cpu.shares', form['shares'],
                                      container=container)
                if info[""state""].lower() != 'stopped':
                        lxc.cgroup(container, 'lxc.cgroup.cpu.shares',
                                   form['shares'])
                flash(u'CPU shares updated for %s!' % container, 'success')

            if form['rootfs'] != cfg['rootfs'] and \
                    re.match('^[a-zA-Z0-9_/\-\.]+', form['rootfs']):
                lwp.push_config_value('lxc.rootfs', form['rootfs'],
                                      container=container)
                flash(u'Rootfs updated!' % container, 'success')

            auto = lwp.ls_auto()
            if form['autostart'] == 'True' and \
                    not ('%s.conf' % container) in auto:
                try:
                    os.symlink('/var/lib/lxc/%s/config' % container,
                               '/etc/lxc/auto/%s.conf' % container)
                    flash(u'Autostart enabled for %s' % container, 'success')
                except OSError:
                    flash(u'Unable to create symlink \'/etc/lxc/auto/%s.conf\''
                          % container, 'error')
            elif not form['autostart'] and ('%s.conf' % container) in auto:
                try:
                    os.remove('/etc/lxc/auto/%s.conf' % container)
                    flash(u'Autostart disabled for %s' % container, 'success')
                except OSError:
                    flash(u'Unable to remove symlink', 'error')

        info = lxc.info(container)
        status = info['state']
        pid = info['pid']

        infos = {'status': status,
                 'pid': pid,
                 'memusg': lwp.memory_usage(container)}
        return render_template('edit.html', containers=lxc.ls(),
                               container=container, infos=infos,
                               settings=lwp.get_container_settings(container),
                               host_memory=host_memory)
    return render_template('login.html')",form['swlimit'] != cfg['swlimit'] and form['memlimit'] <= form['swlimit'],form['memlimit'] <= form['swlimit'] != cfg['swlimit']
gym,https://github.com/openai/gym/tree/master/gym/wrappers/time_limit.py,TimeLimit,__init__$5,"def __init__(self, env, max_episode_steps=None):
        super().__init__(env)
        if max_episode_steps is None and self.env.spec is not None:
            max_episode_steps = env.spec.max_episode_steps
        if self.env.spec is not None:
            self.env.spec.max_episode_steps = max_episode_steps
        self._max_episode_steps = max_episode_steps
        self._elapsed_steps = None",max_episode_steps is None and self.env.spec is not None,max_episode_steps is None is not self.env.spec
pianoplayer,https://github.com/marcomusy/pianoplayer/tree/master/pianoplayer/scorereader.py,,reader$42,"def reader(sf, beam=0):

    noteseq = []

    if hasattr(sf, 'parts'):
        if len(sf.parts) <= beam:
            return []
        strm = sf.parts[beam].flat
    elif hasattr(sf, 'elements'):
        if len(sf.elements)==1 and beam==1:
            strm = sf[0]
        else:
            if len(sf) <= beam:
                return []
            strm = sf[beam]
    else:
        strm = sf.flat

    print('Reading beam', beam, 'with', len(strm), 'objects in stream.')

    chordID = 0
    noteID = 0
    for n in strm.getElementsByClass(""GeneralNote""):

        if n.duration.quarterLength==0: continue

        if hasattr(n, 'tie'): # address bug https://github.com/marcomusy/pianoplayer/issues/29
            if n.tie and (n.tie.type == 'continue' or n.tie.type=='stop'): continue

        if n.isNote:
            if len(noteseq) and n.offset == noteseq[-1].time:
                # print ""doppia nota"", n.name
                continue
            an        = INote()
            an.noteID = noteID
            an.note21 = n
            an.isChord= False
            an.name   = n.name
            an.octave = n.octave
            an.measure= n.measureNumber
            an.x      = keypos(n)
            an.pitch  = n.pitch.midi
            an.time   = n.offset
            an.duration = n.duration.quarterLength
            an.isBlack= False
            pc = n.pitch.pitchClass
            an.isBlack = False
            if pc in [1, 3, 6, 8, 10]:
                an.isBlack = True
            if n.lyrics:
                an.fingering = n.lyric

            an.fingering = get_finger_music21(n)
            noteseq.append(an)
            noteID += 1

        elif n.isChord:

            if n.tie and (n.tie.type=='continue' or n.tie.type=='stop'): continue
            sfasam = 0.05 # sfasa leggermente le note dell'accordo

            for j, cn in enumerate(n.pitches):
                an = INote()
                an.chordID  = chordID
                an.noteID = noteID
                an.isChord = True
                an.pitch = cn.midi
                an.note21  = cn
                an.name    = cn.name
                an.chordnr = j
                an.NinChord = len(n.pitches)
                an.octave  = cn.octave
                an.measure = n.measureNumber
                an.x       = keypos(cn)
                an.time    = n.offset-sfasam*(len(n.pitches)-j-1)
                an.duration= n.duration.quarterLength+sfasam*(an.NinChord-1)
                if hasattr(cn, 'pitch'):
                    pc = cn.pitch.pitchClass
                else:
                    pc = cn.pitchClass
                if pc in [1, 3, 6, 8, 10]:
                    an.isBlack = True
                else:
                    an.isBlack = False
                an.fingering = get_finger_music21(n, j)
                noteID += 1
                noteseq.append(an)
            chordID += 1

    if len(noteseq) < 2:
        print(""Beam is empty."")
        return []
    return noteseq",len(sf.elements) == 1 and beam == 1,len(sf.elements) == 1 == beam
NiftyNet,https://github.com/NifTK/NiftyNet/tree/master/niftynet/utilities/download.py,ConfigStoreCache,get_download_params$351,"def get_download_params(self, example_id):
        """"""
        Returns the local configuration file for this example_id
        """"""
        config_filename = self.get_local_path(example_id)

        parser = NiftyNetLaunchConfig()
        parser.read(config_filename)
        if parser.has_section('config'):
            config_section = dict(parser.items('config'))
        else:
            config_section = {}

        other_sections = {}
        for section in parser.sections():
            if section != 'config' and section != 'DEFAULT':
                other_sections[section] = dict(parser.items(section))
        return config_section, other_sections",section != 'config' and section != 'DEFAULT','config' != section != 'DEFAULT'
textual,https://github.com/willmcgugan/textual/tree/master/src/textual/widget.py,Widget,scroll_home$1425,"def scroll_home(
        self,
        *,
        animate: bool = True,
        speed: float | None = None,
        duration: float | None = None,
        easing: EasingFunction | str | None = None,
    ) -> bool:
        """"""Scroll to home position.

        Args:
            animate (bool, optional): Animate scroll. Defaults to True.
            speed (float | None, optional): Speed of scroll if animate is True. Or None to use duration.
            duration (float | None, optional): Duration of animation, if animate is True and speed is None.
            easing (EasingFunction | str | None, optional): An easing method for the scrolling animation. Defaults to ""None"",
                which will result in Textual choosing the configured default scrolling easing function.

        Returns:
            bool: True if any scrolling was done.
        """"""
        if speed is None and duration is None:
            duration = 1.0
        return self.scroll_to(
            0, 0, animate=animate, speed=speed, duration=duration, easing=easing
        )",speed is None and duration is None,speed is None is duration
trankit,https://github.com/nlp-uoregon/trankit/tree/master/trankit/adapter_transformers/modeling_bert.py,BertModel,forward$669,"def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        adapter_names=None,
    ):
        r""""""
    Return:
        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:
        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):
            Sequence of hidden-states at the output of the last layer of the model.
        pooler_output (:obj:`torch.FloatTensor`: of shape :obj:`(batch_size, hidden_size)`):
            Last layer hidden-state of the first token of the sequence (classification token)
            further processed by a Linear layer and a Tanh activation function. The Linear
            layer weights are trained from the next sentence prediction (classification)
            objective during pre-training.

            This output is usually *not* a good summary
            of the semantic content of the input, you're often better with averaging or pooling
            the sequence of hidden-states for the whole input sequence.
        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):
            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
            of shape :obj:`(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):
            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape
            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.

    Examples::

        from transformers import BertModel, BertTokenizer
        import torch

        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        model = BertModel.from_pretrained('bert-base-uncased')

        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True)).unsqueeze(0)  # Batch size 1
        outputs = model(input_ids)

        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple

        """"""
        # override the default active adapters with those passed in the method call
        adapter_names = adapter_names or self.active_adapters
        # some warnings if we don't use available adapters
        if not adapter_names and self.has_adapters():
            logger.warning(""There are adapters available but none are passed to model.forward"")

        if input_ids is not None and inputs_embeds is not None:
            raise ValueError(""You cannot specify both input_ids and inputs_embeds at the same time"")
        elif input_ids is not None:
            input_shape = input_ids.size()
        elif inputs_embeds is not None:
            input_shape = inputs_embeds.size()[:-1]
        else:
            raise ValueError(""You have to specify either input_ids or inputs_embeds"")

        device = input_ids.device if input_ids is not None else inputs_embeds.device

        if attention_mask is None:
            attention_mask = torch.ones(input_shape, device=device)
        if token_type_ids is None:
            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)

        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]
        # ourselves in which case we just need to make it broadcastable to all heads.
        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)

        # If a 2D ou 3D attention mask is provided for the cross-attention
        # we need to make broadcastabe to [batch_size, num_heads, seq_length, seq_length]
        if self.config.is_decoder and encoder_hidden_states is not None:
            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
            if encoder_attention_mask is None:
                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)
            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)
        else:
            encoder_extended_attention_mask = None

        # Prepare head mask if needed
        # 1.0 in head_mask indicate we keep the head
        # attention_probs has shape bsz x n_heads x N x N
        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]
        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]
        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)

        embedding_output = self.embeddings(
            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds
        )

        # TODO: Currently no fusion over invertible adapters, takes only very first language adapter position
        if adapter_names is not None and len(adapter_names) > 0:
            adapter_names = parse_adapter_names(adapter_names)

            if adapter_names[0][0] in self.invertible_lang_adapters:
                embedding_output = self.invertible_lang_adapters[adapter_names[0][0]](embedding_output, rev=False)

        encoder_outputs = self.encoder(
            embedding_output,
            attention_mask=extended_attention_mask,
            head_mask=head_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_extended_attention_mask,
            adapter_names=adapter_names,
        )
        sequence_output = encoder_outputs[0]
        pooled_output = self.pooler(sequence_output)

        outputs = (sequence_output, pooled_output,) + encoder_outputs[
            1:
        ]  # add hidden_states and attentions if they are here

        return outputs",input_ids is not None and inputs_embeds is not None,input_ids is not None is not inputs_embeds
transformers,https://github.com/huggingface/transformers/tree/master/src/transformers/models/deberta/modeling_tf_deberta.py,TFDebertaEmbeddings,call$767,"def call(
        self,
        input_ids: tf.Tensor = None,
        position_ids: tf.Tensor = None,
        token_type_ids: tf.Tensor = None,
        inputs_embeds: tf.Tensor = None,
        mask: tf.Tensor = None,
        training: bool = False,
    ) -> tf.Tensor:
        """"""
        Applies embedding based on inputs tensor.

        Returns:
            final_embeddings (`tf.Tensor`): output embedding tensor.
        """"""
        if input_ids is None and inputs_embeds is None:
            raise ValueError(""Need to provide either `input_ids` or `input_embeds`."")

        if input_ids is not None:
            # Note: tf.gather, on which the embedding layer is based, won't check positive out of bound
            # indices on GPU, returning zeros instead. This is a dangerous silent behavior.
            tf.debugging.assert_less(
                input_ids,
                tf.cast(self.vocab_size, dtype=input_ids.dtype),
                message=(
                    ""input_ids must be smaller than the embedding layer's input dimension (got""
                    f"" {tf.math.reduce_max(input_ids)} >= {self.vocab_size})""
                ),
            )
            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)

        input_shape = shape_list(inputs_embeds)[:-1]

        if token_type_ids is None:
            token_type_ids = tf.fill(dims=input_shape, value=0)

        if position_ids is None:
            position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)

        final_embeddings = inputs_embeds
        if self.position_biased_input:
            position_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)
            final_embeddings += position_embeds
        if self.type_vocab_size > 0:
            token_type_embeds = tf.gather(params=self.token_type_embeddings, indices=token_type_ids)
            final_embeddings += token_type_embeds

        if self.embedding_size != self.hidden_size:
            final_embeddings = self.embed_proj(final_embeddings)

        final_embeddings = self.LayerNorm(final_embeddings)

        if mask is not None:
            if len(shape_list(mask)) != len(shape_list(final_embeddings)):
                if len(shape_list(mask)) == 4:
                    mask = tf.squeeze(tf.squeeze(mask, axis=1), axis=1)
                mask = tf.cast(tf.expand_dims(mask, axis=2), tf.float32)

            final_embeddings = final_embeddings * mask

        final_embeddings = self.dropout(final_embeddings, training=training)

        return final_embeddings",input_ids is None and inputs_embeds is None,input_ids is None is inputs_embeds
tinydb,https://github.com/msiemens/tinydb/tree/master/tinydb/table.py,Table,upsert$490,"def upsert(self, document: Mapping, cond: Optional[QueryLike] = None) -> List[int]:
        """"""
        Update documents, if they exist, insert them otherwise.

        Note: This will update *all* documents matching the query. Document
        argument can be a tinydb.table.Document object if you want to specify a
        doc_id.

        :param document: the document to insert or the fields to update
        :param cond: which document to look for, optional if you've passed a
        Document with a doc_id
        :returns: a list containing the updated documents' IDs
        """"""

        # Extract doc_id
        if isinstance(document, Document) and hasattr(document, 'doc_id'):
            doc_ids: Optional[List[int]] = [document.doc_id]
        else:
            doc_ids = None

        # Make sure we can actually find a matching document
        if doc_ids is None and cond is None:
            raise ValueError(""If you don't specify a search query, you must ""
                             ""specify a doc_id. Hint: use a table.Document ""
                             ""object."")

        # Perform the update operation
        try:
            updated_docs: Optional[List[int]] = self.update(document, cond, doc_ids)
        except KeyError:
            # This happens when a doc_id is specified, but it's missing
            updated_docs = None

        # If documents have been updated: return their IDs
        if updated_docs:
            return updated_docs

        # There are no documents that match the specified query -> insert the
        # data as a new document
        return [self.insert(document)]",doc_ids is None and cond is None,doc_ids is None is cond
PaddleX,https://github.com/PaddlePaddle/PaddleX/tree/master/paddlex/paddleseg/core/train.py,,train$50,"def train(model,
          train_dataset,
          val_dataset=None,
          optimizer=None,
          save_dir='output',
          iters=10000,
          batch_size=2,
          resume_model=None,
          save_interval=1000,
          log_iters=10,
          num_workers=0,
          use_vdl=False,
          losses=None,
          keep_checkpoint_max=5):
    """"""
    Launch training.

    Args:
        model（nn.Layer): A sementic segmentation model.
        train_dataset (paddle.io.Dataset): Used to read and process training datasets.
        val_dataset (paddle.io.Dataset, optional): Used to read and process validation datasets.
        optimizer (paddle.optimizer.Optimizer): The optimizer.
        save_dir (str, optional): The directory for saving the model snapshot. Default: 'output'.
        iters (int, optional): How may iters to train the model. Defualt: 10000.
        batch_size (int, optional): Mini batch size of one gpu or cpu. Default: 2.
        resume_model (str, optional): The path of resume model.
        save_interval (int, optional): How many iters to save a model snapshot once during training. Default: 1000.
        log_iters (int, optional): Display logging information at every log_iters. Default: 10.
        num_workers (int, optional): Num workers for data loader. Default: 0.
        use_vdl (bool, optional): Whether to record the data to VisualDL during training. Default: False.
        losses (dict): A dict including 'types' and 'coef'. The length of coef should equal to 1 or len(losses['types']).
            The 'types' item is a list of object of paddleseg.models.losses while the 'coef' item is a list of the relevant coefficient.
        keep_checkpoint_max (int, optional): Maximum number of checkpoints to save. Default: 5.
    """"""
    model.train()
    nranks = paddle.distributed.ParallelEnv().nranks
    local_rank = paddle.distributed.ParallelEnv().local_rank

    start_iter = 0
    if resume_model is not None:
        start_iter = resume(model, optimizer, resume_model)

    if not os.path.isdir(save_dir):
        if os.path.exists(save_dir):
            os.remove(save_dir)
        os.makedirs(save_dir)

    if nranks > 1:
        # Initialize parallel environment if not done.
        if not paddle.distributed.parallel.parallel_helper._is_parallel_ctx_initialized(
        ):
            paddle.distributed.init_parallel_env()
            ddp_model = paddle.DataParallel(model)
        else:
            ddp_model = paddle.DataParallel(model)

    batch_sampler = paddle.io.DistributedBatchSampler(
        train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)

    loader = paddle.io.DataLoader(
        train_dataset,
        batch_sampler=batch_sampler,
        num_workers=num_workers,
        return_list=True, )

    if use_vdl:
        from visualdl import LogWriter
        log_writer = LogWriter(save_dir)

    avg_loss = 0.0
    avg_loss_list = []
    iters_per_epoch = len(batch_sampler)
    best_mean_iou = -1.0
    best_model_iter = -1
    reader_cost_averager = TimeAverager()
    batch_cost_averager = TimeAverager()
    save_models = deque()
    batch_start = time.time()

    iter = start_iter
    while iter < iters:
        for data in loader:
            iter += 1
            if iter > iters:
                break
            reader_cost_averager.record(time.time() - batch_start)
            images = data[0]
            labels = data[1].astype('int64')
            edges = None
            if len(data) == 3:
                edges = data[2].astype('int64')

            if nranks > 1:
                logits_list = ddp_model(images)
            else:
                logits_list = model(images)
            loss_list = loss_computation(
                logits_list=logits_list,
                labels=labels,
                losses=losses,
                edges=edges)
            loss = sum(loss_list)
            loss.backward()

            optimizer.step()
            lr = optimizer.get_lr()
            if isinstance(optimizer._learning_rate,
                          paddle.optimizer.lr.LRScheduler):
                optimizer._learning_rate.step()
            model.clear_gradients()
            avg_loss += loss.numpy()[0]
            if not avg_loss_list:
                avg_loss_list = [l.numpy() for l in loss_list]
            else:
                for i in range(len(loss_list)):
                    avg_loss_list[i] += loss_list[i].numpy()
            batch_cost_averager.record(
                time.time() - batch_start, num_samples=batch_size)

            if (iter) % log_iters == 0 and local_rank == 0:
                avg_loss /= log_iters
                avg_loss_list = [l[0] / log_iters for l in avg_loss_list]
                remain_iters = iters - iter
                avg_train_batch_cost = batch_cost_averager.get_average()
                avg_train_reader_cost = reader_cost_averager.get_average()
                eta = calculate_eta(remain_iters, avg_train_batch_cost)
                logger.info(
                    ""[TRAIN] epoch: {}, iter: {}/{}, loss: {:.4f}, lr: {:.6f}, batch_cost: {:.4f}, reader_cost: {:.5f}, ips: {:.4f} samples/sec | ETA {}""
                    .format((iter - 1
                             ) // iters_per_epoch + 1, iter, iters, avg_loss,
                            lr, avg_train_batch_cost, avg_train_reader_cost,
                            batch_cost_averager.get_ips_average(), eta))
                if use_vdl:
                    log_writer.add_scalar('Train/loss', avg_loss, iter)
                    # Record all losses if there are more than 2 losses.
                    if len(avg_loss_list) > 1:
                        avg_loss_dict = {}
                        for i, value in enumerate(avg_loss_list):
                            avg_loss_dict['loss_' + str(i)] = value
                        for key, value in avg_loss_dict.items():
                            log_tag = 'Train/' + key
                            log_writer.add_scalar(log_tag, value, iter)

                    log_writer.add_scalar('Train/lr', lr, iter)
                    log_writer.add_scalar('Train/batch_cost',
                                          avg_train_batch_cost, iter)
                    log_writer.add_scalar('Train/reader_cost',
                                          avg_train_reader_cost, iter)
                avg_loss = 0.0
                avg_loss_list = []
                reader_cost_averager.reset()
                batch_cost_averager.reset()

            if (iter % save_interval == 0 or
                    iter == iters) and (val_dataset is not None):
                num_workers = 1 if num_workers > 0 else 0
                mean_iou, acc, _, _, _ = evaluate(
                    model, val_dataset, num_workers=num_workers)
                model.train()

            if (iter % save_interval == 0 or
                    iter == iters) and local_rank == 0:
                current_save_dir = os.path.join(save_dir,
                                                ""iter_{}"".format(iter))
                if not os.path.isdir(current_save_dir):
                    os.makedirs(current_save_dir)
                paddle.save(model.state_dict(),
                            os.path.join(current_save_dir, 'model.pdparams'))
                paddle.save(optimizer.state_dict(),
                            os.path.join(current_save_dir, 'model.pdopt'))
                save_models.append(current_save_dir)
                if len(save_models) > keep_checkpoint_max > 0:
                    model_to_remove = save_models.popleft()
                    shutil.rmtree(model_to_remove)

                if val_dataset is not None:
                    if mean_iou > best_mean_iou:
                        best_mean_iou = mean_iou
                        best_model_iter = iter
                        best_model_dir = os.path.join(save_dir, ""best_model"")
                        paddle.save(
                            model.state_dict(),
                            os.path.join(best_model_dir, 'model.pdparams'))
                    logger.info(
                        '[EVAL] The model with the best validation mIoU ({:.4f}) was saved at iter {}.'
                        .format(best_mean_iou, best_model_iter))

                    if use_vdl:
                        log_writer.add_scalar('Evaluate/mIoU', mean_iou, iter)
                        log_writer.add_scalar('Evaluate/Acc', acc, iter)
            batch_start = time.time()

    # Calculate flops.
    if local_rank == 0:

        def count_syncbn(m, x, y):
            x = x[0]
            nelements = x.numel()
            m.total_ops += int(2 * nelements)

        _, c, h, w = images.shape
        flops = paddle.flops(
            model, [1, c, h, w],
            custom_ops={paddle.nn.SyncBatchNorm: count_syncbn})

    # Sleep for half a second to let dataloader release resources.
    time.sleep(0.5)
    if use_vdl:
        log_writer.close()",iter % log_iters == 0 and local_rank == 0,iter % log_iters == 0 == local_rank
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/algorithms/amplitude_estimators/ae_utils.py,,_derivative_circ_dist$115,"def _derivative_circ_dist(x, p):
    """"""Derivative of circumferential distance function.

    Args:
        x (float): first angle
        p (float): second angle

    Returns:
        float: The derivative.
    """"""
    # pylint: disable=chained-comparison,misplaced-comparison-constant
    t = p - x
    if t < -0.5 or (0 < t and t < 0.5):
        return -1
    if t > 0.5 or (-0.5 < t and t < 0):
        return 1
    return 0",0 < t and t < 0.5,0 < t < 0.5
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/algorithms/amplitude_estimators/ae_utils.py,,_derivative_circ_dist$115,"def _derivative_circ_dist(x, p):
    """"""Derivative of circumferential distance function.

    Args:
        x (float): first angle
        p (float): second angle

    Returns:
        float: The derivative.
    """"""
    # pylint: disable=chained-comparison,misplaced-comparison-constant
    t = p - x
    if t < -0.5 or (0 < t and t < 0.5):
        return -1
    if t > 0.5 or (-0.5 < t and t < 0):
        return 1
    return 0",#NAME?,#NAME?
tf-coreml,https://github.com/tf-coreml/tf-coreml/tree/master/tfcoreml/_layers.py,,resize_bilinear$936,"def resize_bilinear(op, context):

  input_name = compat.as_str_any(op.inputs[0].name)
  output_name = compat.as_str_any(op.outputs[0].name)

  if op.inputs[1].name in context.consts:
    output_spatial_sizes = context.consts[op.inputs[1].name]
  else:
    output_spatial_sizes = context.session.run(
      op.inputs[1].name, feed_dict=context.input_feed_dict)

  shape = context.shape_dict[input_name]

  assert (len(shape) == 4), ('Resize Bilinear: input must be 4-D shape. Input shape = {}'.
                             format(str(shape)))

  if op.get_attr('align_corners'):
    mode = 'STRICT_ALIGN_ENDPOINTS_MODE'
  else:
    mode = 'UPSAMPLE_MODE'

  if mode == 'UPSAMPLE_MODE' and (output_spatial_sizes[0] % shape[1] == 0) and (output_spatial_sizes[1] % shape[2] == 0):
      upsample_factor_height = output_spatial_sizes[0] // shape[1]
      upsample_factor_width = output_spatial_sizes[1] // shape[2]
      context.builder.add_upsample(output_name, upsample_factor_height,
                                    upsample_factor_width, input_name, output_name, mode='BILINEAR')
  else:
    context.builder.add_resize_bilinear(output_name, input_name, output_name,
                                      target_height=output_spatial_sizes[0], target_width=output_spatial_sizes[1],
                                      mode=mode)
    context.builder.spec.specificationVersion = 3

  context.translated[output_name] = True",mode == 'UPSAMPLE_MODE' and output_spatial_sizes[0] % shape[1] == 0 and (output_spatial_sizes[1] % shape[2] == 0),mode == 'UPSAMPLE_MODE' and output_spatial_sizes[0] % shape[1] == 0 == output_spatial_sizes[1] % shape[2]
conda,https://github.com/conda/conda/tree/master/conda/core/package_cache_data.py,ProgressiveFetchExtract,make_actions_for_record$472,"def make_actions_for_record(pref_or_spec):
        assert pref_or_spec is not None
        # returns a cache_action and extract_action

        # if the pref or spec has an md5 value
        # look in all caches for package cache record that is
        #   (1) already extracted, and
        #   (2) matches the md5
        # If one exists, no actions are needed.
        sha256 = pref_or_spec.get(""sha256"")
        size = pref_or_spec.get(""size"")
        md5 = pref_or_spec.get(""md5"")
        legacy_bz2_size = pref_or_spec.get(""legacy_bz2_size"")
        legacy_bz2_md5 = pref_or_spec.get(""legacy_bz2_md5"")

        def pcrec_matches(pcrec):
            matches = True
            # sha256 is overkill for things that are already in the package cache.
            #     It's just a quick match.
            # if sha256 is not None and pcrec.sha256 is not None:
            #     matches = sha256 == pcrec.sha256
            if size is not None and pcrec.get('size') is not None:
                matches = pcrec.size in (size, legacy_bz2_size)
            if matches and md5 is not None and pcrec.get('md5') is not None:
                matches = pcrec.md5 in (md5, legacy_bz2_md5)
            return matches

        extracted_pcrec = next((
            pcrec for pcrec in concat(PackageCacheData(pkgs_dir).query(pref_or_spec)
                                      for pkgs_dir in context.pkgs_dirs)
            if pcrec.is_extracted
        ), None)
        if extracted_pcrec and pcrec_matches(extracted_pcrec) and extracted_pcrec.get('url'):
            return None, None

        # there is no extracted dist that can work, so now we look for tarballs that
        #   aren't extracted
        # first we look in all writable caches, and if we find a match, we extract in place
        # otherwise, if we find a match in a non-writable cache, we link it to the first writable
        #   cache, and then extract
        pcrec_from_writable_cache = next(
            (pcrec for pcrec in concat(
                pcache.query(pref_or_spec) for pcache in PackageCacheData.writable_caches()
            ) if pcrec.is_fetched),
            None
        )
        if (pcrec_from_writable_cache and pcrec_matches(pcrec_from_writable_cache) and
                pcrec_from_writable_cache.get('url')):
            # extract in place
            extract_axn = ExtractPackageAction(
                source_full_path=pcrec_from_writable_cache.package_tarball_full_path,
                target_pkgs_dir=dirname(pcrec_from_writable_cache.package_tarball_full_path),
                target_extracted_dirname=basename(pcrec_from_writable_cache.extracted_package_dir),
                record_or_spec=pcrec_from_writable_cache,
                sha256=pcrec_from_writable_cache.sha256 or sha256,
                size=pcrec_from_writable_cache.size or size,
                md5=pcrec_from_writable_cache.md5 or md5,
            )
            return None, extract_axn

        pcrec_from_read_only_cache = next((
            pcrec for pcrec in concat(pcache.query(pref_or_spec)
                                      for pcache in PackageCacheData.read_only_caches())
            if pcrec.is_fetched
        ), None)

        first_writable_cache = PackageCacheData.first_writable()
        if pcrec_from_read_only_cache and pcrec_matches(pcrec_from_read_only_cache):
            # we found a tarball, but it's in a read-only package cache
            # we need to link the tarball into the first writable package cache,
            #   and then extract
            cache_axn = CacheUrlAction(
                url=path_to_url(pcrec_from_read_only_cache.package_tarball_full_path),
                target_pkgs_dir=first_writable_cache.pkgs_dir,
                target_package_basename=pcrec_from_read_only_cache.fn,
                sha256=pcrec_from_read_only_cache.get(""sha256"") or sha256,
                size=pcrec_from_read_only_cache.get(""size"") or size,
                md5=pcrec_from_read_only_cache.get(""md5"") or md5,
            )
            trgt_extracted_dirname = strip_pkg_extension(pcrec_from_read_only_cache.fn)[0]
            extract_axn = ExtractPackageAction(
                source_full_path=cache_axn.target_full_path,
                target_pkgs_dir=first_writable_cache.pkgs_dir,
                target_extracted_dirname=trgt_extracted_dirname,
                record_or_spec=pcrec_from_read_only_cache,
                sha256=pcrec_from_read_only_cache.get(""sha256"") or sha256,
                size=pcrec_from_read_only_cache.get(""size"") or size,
                md5=pcrec_from_read_only_cache.get(""md5"") or md5,
            )
            return cache_axn, extract_axn

        # if we got here, we couldn't find a matching package in the caches
        #   we'll have to download one; fetch and extract
        url = pref_or_spec.get('url')
        assert url

        cache_axn = CacheUrlAction(
            url=url,
            target_pkgs_dir=first_writable_cache.pkgs_dir,
            target_package_basename=pref_or_spec.fn,
            sha256=sha256,
            size=size,
            md5=md5,
        )
        extract_axn = ExtractPackageAction(
            source_full_path=cache_axn.target_full_path,
            target_pkgs_dir=first_writable_cache.pkgs_dir,
            target_extracted_dirname=strip_pkg_extension(pref_or_spec.fn)[0],
            record_or_spec=pref_or_spec,
            sha256=sha256,
            size=size,
            md5=md5,
        )
        return cache_axn, extract_axn",size is not None and pcrec.get('size') is not None,size is not None is not pcrec.get('size')
conda,https://github.com/conda/conda/tree/master/conda/core/package_cache_data.py,ProgressiveFetchExtract,make_actions_for_record$472,"def make_actions_for_record(pref_or_spec):
        assert pref_or_spec is not None
        # returns a cache_action and extract_action

        # if the pref or spec has an md5 value
        # look in all caches for package cache record that is
        #   (1) already extracted, and
        #   (2) matches the md5
        # If one exists, no actions are needed.
        sha256 = pref_or_spec.get(""sha256"")
        size = pref_or_spec.get(""size"")
        md5 = pref_or_spec.get(""md5"")
        legacy_bz2_size = pref_or_spec.get(""legacy_bz2_size"")
        legacy_bz2_md5 = pref_or_spec.get(""legacy_bz2_md5"")

        def pcrec_matches(pcrec):
            matches = True
            # sha256 is overkill for things that are already in the package cache.
            #     It's just a quick match.
            # if sha256 is not None and pcrec.sha256 is not None:
            #     matches = sha256 == pcrec.sha256
            if size is not None and pcrec.get('size') is not None:
                matches = pcrec.size in (size, legacy_bz2_size)
            if matches and md5 is not None and pcrec.get('md5') is not None:
                matches = pcrec.md5 in (md5, legacy_bz2_md5)
            return matches

        extracted_pcrec = next((
            pcrec for pcrec in concat(PackageCacheData(pkgs_dir).query(pref_or_spec)
                                      for pkgs_dir in context.pkgs_dirs)
            if pcrec.is_extracted
        ), None)
        if extracted_pcrec and pcrec_matches(extracted_pcrec) and extracted_pcrec.get('url'):
            return None, None

        # there is no extracted dist that can work, so now we look for tarballs that
        #   aren't extracted
        # first we look in all writable caches, and if we find a match, we extract in place
        # otherwise, if we find a match in a non-writable cache, we link it to the first writable
        #   cache, and then extract
        pcrec_from_writable_cache = next(
            (pcrec for pcrec in concat(
                pcache.query(pref_or_spec) for pcache in PackageCacheData.writable_caches()
            ) if pcrec.is_fetched),
            None
        )
        if (pcrec_from_writable_cache and pcrec_matches(pcrec_from_writable_cache) and
                pcrec_from_writable_cache.get('url')):
            # extract in place
            extract_axn = ExtractPackageAction(
                source_full_path=pcrec_from_writable_cache.package_tarball_full_path,
                target_pkgs_dir=dirname(pcrec_from_writable_cache.package_tarball_full_path),
                target_extracted_dirname=basename(pcrec_from_writable_cache.extracted_package_dir),
                record_or_spec=pcrec_from_writable_cache,
                sha256=pcrec_from_writable_cache.sha256 or sha256,
                size=pcrec_from_writable_cache.size or size,
                md5=pcrec_from_writable_cache.md5 or md5,
            )
            return None, extract_axn

        pcrec_from_read_only_cache = next((
            pcrec for pcrec in concat(pcache.query(pref_or_spec)
                                      for pcache in PackageCacheData.read_only_caches())
            if pcrec.is_fetched
        ), None)

        first_writable_cache = PackageCacheData.first_writable()
        if pcrec_from_read_only_cache and pcrec_matches(pcrec_from_read_only_cache):
            # we found a tarball, but it's in a read-only package cache
            # we need to link the tarball into the first writable package cache,
            #   and then extract
            cache_axn = CacheUrlAction(
                url=path_to_url(pcrec_from_read_only_cache.package_tarball_full_path),
                target_pkgs_dir=first_writable_cache.pkgs_dir,
                target_package_basename=pcrec_from_read_only_cache.fn,
                sha256=pcrec_from_read_only_cache.get(""sha256"") or sha256,
                size=pcrec_from_read_only_cache.get(""size"") or size,
                md5=pcrec_from_read_only_cache.get(""md5"") or md5,
            )
            trgt_extracted_dirname = strip_pkg_extension(pcrec_from_read_only_cache.fn)[0]
            extract_axn = ExtractPackageAction(
                source_full_path=cache_axn.target_full_path,
                target_pkgs_dir=first_writable_cache.pkgs_dir,
                target_extracted_dirname=trgt_extracted_dirname,
                record_or_spec=pcrec_from_read_only_cache,
                sha256=pcrec_from_read_only_cache.get(""sha256"") or sha256,
                size=pcrec_from_read_only_cache.get(""size"") or size,
                md5=pcrec_from_read_only_cache.get(""md5"") or md5,
            )
            return cache_axn, extract_axn

        # if we got here, we couldn't find a matching package in the caches
        #   we'll have to download one; fetch and extract
        url = pref_or_spec.get('url')
        assert url

        cache_axn = CacheUrlAction(
            url=url,
            target_pkgs_dir=first_writable_cache.pkgs_dir,
            target_package_basename=pref_or_spec.fn,
            sha256=sha256,
            size=size,
            md5=md5,
        )
        extract_axn = ExtractPackageAction(
            source_full_path=cache_axn.target_full_path,
            target_pkgs_dir=first_writable_cache.pkgs_dir,
            target_extracted_dirname=strip_pkg_extension(pref_or_spec.fn)[0],
            record_or_spec=pref_or_spec,
            sha256=sha256,
            size=size,
            md5=md5,
        )
        return cache_axn, extract_axn",matches and md5 is not None and (pcrec.get('md5') is not None),md5 is not None is not pcrec.get('md5') and matches
SlowFast,https://github.com/facebookresearch/SlowFast/tree/master/slowfast/utils/benchmark.py,,benchmark_data_loading$20,"def benchmark_data_loading(cfg):
    """"""
    Benchmark the speed of data loading in PySlowFast.
    Args:

        cfg (CfgNode): configs. Details can be found in
            slowfast/config/defaults.py
    """"""
    # Set up environment.
    setup_environment()
    # Set random seed from configs.
    np.random.seed(cfg.RNG_SEED)
    torch.manual_seed(cfg.RNG_SEED)

    # Setup logging format.
    logging.setup_logging(cfg.OUTPUT_DIR)

    # Print config.
    logger.info(""Benchmark data loading with config:"")
    logger.info(pprint.pformat(cfg))

    timer = Timer()
    dataloader = loader.construct_loader(cfg, ""train"")
    logger.info(
        ""Initialize loader using {:.2f} seconds."".format(timer.seconds())
    )
    # Total batch size across different machines.
    batch_size = cfg.TRAIN.BATCH_SIZE * cfg.NUM_SHARDS
    log_period = cfg.BENCHMARK.LOG_PERIOD
    epoch_times = []
    # Test for a few epochs.
    for cur_epoch in range(cfg.BENCHMARK.NUM_EPOCHS):
        timer = Timer()
        timer_epoch = Timer()
        iter_times = []
        if cfg.BENCHMARK.SHUFFLE:
            loader.shuffle_dataset(dataloader, cur_epoch)
        for cur_iter, _ in enumerate(tqdm.tqdm(dataloader)):
            if cur_iter > 0 and cur_iter % log_period == 0:
                iter_times.append(timer.seconds())
                ram_usage, ram_total = misc.cpu_mem_usage()
                logger.info(
                    ""Epoch {}: {} iters ({} videos) in {:.2f} seconds. ""
                    ""RAM Usage: {:.2f}/{:.2f} GB."".format(
                        cur_epoch,
                        log_period,
                        log_period * batch_size,
                        iter_times[-1],
                        ram_usage,
                        ram_total,
                    )
                )
                timer.reset()
        epoch_times.append(timer_epoch.seconds())
        ram_usage, ram_total = misc.cpu_mem_usage()
        logger.info(
            ""Epoch {}: in total {} iters ({} videos) in {:.2f} seconds. ""
            ""RAM Usage: {:.2f}/{:.2f} GB."".format(
                cur_epoch,
                len(dataloader),
                len(dataloader) * batch_size,
                epoch_times[-1],
                ram_usage,
                ram_total,
            )
        )
        logger.info(
            ""Epoch {}: on average every {} iters ({} videos) take {:.2f}/{:.2f} ""
            ""(avg/std) seconds."".format(
                cur_epoch,
                log_period,
                log_period * batch_size,
                np.mean(iter_times),
                np.std(iter_times),
            )
        )
    logger.info(
        ""On average every epoch ({} videos) takes {:.2f}/{:.2f} ""
        ""(avg/std) seconds."".format(
            len(dataloader) * batch_size,
            np.mean(epoch_times),
            np.std(epoch_times),
        )
    )",cur_iter > 0 and cur_iter % log_period == 0,cur_iter > 0 == cur_iter % log_period
Det3D,https://github.com/poodarchu/Det3D/tree/master/det3d/datasets/utils/kitti_object_eval_python/kitti_common.py,,kitti_result_line$205,"def kitti_result_line(result_dict, precision=4):
    prec_float = ""{"" + "":.{}f"".format(precision) + ""}""
    res_line = []
    all_field_default = OrderedDict(
        [
            (""name"", None),
            (""truncated"", -1),
            (""occluded"", -1),
            (""alpha"", -10),
            (""bbox"", None),
            (""dimensions"", [-1, -1, -1]),
            (""location"", [-1000, -1000, -1000]),
            (""rotation_y"", -10),
            (""score"", None),
        ]
    )
    res_dict = [(key, None) for key, val in all_field_default.items()]
    res_dict = OrderedDict(res_dict)
    for key, val in result_dict.items():
        if all_field_default[key] is None and val is None:
            raise ValueError(""you must specify a value for {}"".format(key))
        res_dict[key] = val

    for key, val in res_dict.items():
        if key == ""name"":
            res_line.append(val)
        elif key in [""truncated"", ""alpha"", ""rotation_y"", ""score""]:
            if val is None:
                res_line.append(str(all_field_default[key]))
            else:
                res_line.append(prec_float.format(val))
        elif key == ""occluded"":
            if val is None:
                res_line.append(str(all_field_default[key]))
            else:
                res_line.append(""{}"".format(val))
        elif key in [""bbox"", ""dimensions"", ""location""]:
            if val is None:
                res_line += [str(v) for v in all_field_default[key]]
            else:
                res_line += [prec_float.format(v) for v in val]
        else:
            raise ValueError(""unknown key. supported key:{}"".format(res_dict.keys()))
    return "" "".join(res_line)",all_field_default[key] is None and val is None,all_field_default[key] is None is val
deepdrive,https://github.com/deepdrive/deepdrive/tree/master/sim/gym_env.py,DeepDriveEnv,is_stuck$739,"def is_stuck(self, obz):
        start_is_stuck = time.time()

        # TODO: Get this from the game instead
        ret = False
        if 'TEST_END_OF_EPISODE' in os.environ and self.step_num >= 9:
            log.warn('TEST_END_OF_EPISODE is set triggering end of episode'
                     ' via is_stuck!')
            ret = True
        elif obz is None:
            log.debug('obz is None, not checking if stuck')
            ret = False
        elif obz['speed'] < 100:  # cm/s
            log.debug('speed less than 1m/s, checking if stuck')
            self.steps_crawling += 1
            if obz['throttle'] > 0 and obz['brake'] == 0 and obz['handbrake'] == 0:
                self.steps_crawling_with_throttle_on += 1
                log.debug('crawling detected num steps crawling is %d', self.steps_crawling_with_throttle_on)
            else:
                log.debug('not stuck, throttle %f, brake %f, handbrake %f',
                          obz['throttle'], obz['brake'], obz['handbrake'])

            time_crawling = time.time() - self.last_not_stuck_time

            # This was to detect legitimate stops, but we will have real
            # collision detection before the need to stop
            # portion_crawling = self.steps_crawling_with_throttle_on / max(1, self.steps_crawling)

            if self.steps_crawling_with_throttle_on > 40 and time_crawling > 5:
                log.warn('No progress made while throttle on - '
                         'assuming stuck and ending episode. '
                         'steps crawling: %r, '
                         'steps crawling with throttle on: %r, '
                         'time crawling: %r',
                         self.steps_crawling,
                         self.steps_crawling_with_throttle_on, time_crawling)
                self.set_forward_progress()
                self.episode_return.got_stuck = True
                ret = True
        else:
            log.debug('speed greater than 1m/s, not stuck')
            self.set_forward_progress()
        log.debug('is stuck took %fs', time.time() - start_is_stuck)

        if ret:
            log.info('episode finished, detected we were stuck')

        return ret",obz['throttle'] > 0 and obz['brake'] == 0 and (obz['handbrake'] == 0),obz['throttle'] > 0 == obz['brake'] and obz['handbrake'] == 0
BruteSploit,https://github.com/screetsec/BruteSploit/tree/master/tools/dirsearch/lib/connection/Requester.py,Requester,__init__$43,"def __init__(self, url, cookie=None, useragent=None, maxPool=1, maxRetries=5, timeout=30, ip=None, proxy=None,
                 redirect=False, requestByHostname=False):
        # if no backslash, append one
        if not url.endswith('/'):
            url = url + '/'
        parsed = urllib.parse.urlparse(url)
        self.basePath = parsed.path

        # if not protocol specified, set http by default
        if parsed.scheme != 'http' and parsed.scheme != 'https':
            parsed = urllib.parse.urlparse('http://' + url)
            self.basePath = parsed.path
        self.protocol = parsed.scheme
        if self.protocol != 'http' and self.protocol != 'https':
            self.protocol = 'http'
        self.host = parsed.netloc.split(':')[0]

        # resolve DNS to decrease overhead
        if ip is not None:
            self.ip = ip
        else:
            try:
                self.ip = socket.gethostbyname(self.host)
            except socket.gaierror:
                raise RequestException({'message': ""Couldn't resolve DNS""})
        self.headers['Host'] = self.host

        # If no port specified, set default (80, 443)
        try:
            self.port = parsed.netloc.split(':')[1]
        except IndexError:
            self.port = (443 if self.protocol == 'https' else 80)

        # Set cookie and user-agent headers
        if cookie is not None:
            self.setHeader('Cookie', cookie)
        if useragent is not None:
            self.setHeader('User-agent', useragent)
        self.maxRetries = maxRetries
        self.maxPool = maxPool
        self.timeout = timeout
        self.pool = None
        self.proxy = proxy
        self.redirect = redirect
        self.randomAgents = None
        self.requestByHostname = requestByHostname",parsed.scheme != 'http' and parsed.scheme != 'https','http' != parsed.scheme != 'https'
BruteSploit,https://github.com/screetsec/BruteSploit/tree/master/tools/dirsearch/lib/connection/Requester.py,Requester,__init__$43,"def __init__(self, url, cookie=None, useragent=None, maxPool=1, maxRetries=5, timeout=30, ip=None, proxy=None,
                 redirect=False, requestByHostname=False):
        # if no backslash, append one
        if not url.endswith('/'):
            url = url + '/'
        parsed = urllib.parse.urlparse(url)
        self.basePath = parsed.path

        # if not protocol specified, set http by default
        if parsed.scheme != 'http' and parsed.scheme != 'https':
            parsed = urllib.parse.urlparse('http://' + url)
            self.basePath = parsed.path
        self.protocol = parsed.scheme
        if self.protocol != 'http' and self.protocol != 'https':
            self.protocol = 'http'
        self.host = parsed.netloc.split(':')[0]

        # resolve DNS to decrease overhead
        if ip is not None:
            self.ip = ip
        else:
            try:
                self.ip = socket.gethostbyname(self.host)
            except socket.gaierror:
                raise RequestException({'message': ""Couldn't resolve DNS""})
        self.headers['Host'] = self.host

        # If no port specified, set default (80, 443)
        try:
            self.port = parsed.netloc.split(':')[1]
        except IndexError:
            self.port = (443 if self.protocol == 'https' else 80)

        # Set cookie and user-agent headers
        if cookie is not None:
            self.setHeader('Cookie', cookie)
        if useragent is not None:
            self.setHeader('User-agent', useragent)
        self.maxRetries = maxRetries
        self.maxPool = maxPool
        self.timeout = timeout
        self.pool = None
        self.proxy = proxy
        self.redirect = redirect
        self.randomAgents = None
        self.requestByHostname = requestByHostname",self.protocol != 'http' and self.protocol != 'https','http' != self.protocol != 'https'
aliyun-openapi-python-sdk,https://github.com/aliyun/aliyun-openapi-python-sdk/tree/master/aliyun-python-sdk-dybaseapi/aliyunsdkdybaseapi/mns/mns_tool.py,ValidatorBase,retnumber_validate$81,"def retnumber_validate(req):
        ValidatorBase.type_validate(req.ret_number, types.IntType, req_id=req.request_id)
        if (req.ret_number != -1 and req.ret_number <= 0 ):
            raise MNSClientParameterException(""HeaderInvalid"", ""Bad value: '%s', x-mns-number should larger than 0."" % req.ret_number, req.request_id)",req.ret_number != -1 and req.ret_number <= 0,-1 != req.ret_number <= 0
python-diskcache,https://github.com/grantjenks/python-diskcache/tree/master/diskcache/core.py,Cache,peek$1605,"def peek(
        self,
        prefix=None,
        default=(None, None),
        side='front',
        expire_time=False,
        tag=False,
        retry=False,
    ):
        """"""Peek at key and value item pair from `side` of queue in cache.

        When prefix is None, integer keys are used. Otherwise, string keys are
        used in the format ""prefix-integer"". Integer starts at 500 trillion.

        If queue is empty, return default.

        Defaults to peeking at key and value item pairs from front of queue.
        Set side to 'back' to pull from back of queue. Side must be one of
        'front' or 'back'.

        Expired items are deleted from cache. Operation is atomic. Concurrent
        operations will be serialized.

        Raises :exc:`Timeout` error when database timeout occurs and `retry` is
        `False` (default).

        See also `Cache.pull` and `Cache.push`.

        >>> cache = Cache()
        >>> for letter in 'abc':
        ...     print(cache.push(letter))
        500000000000000
        500000000000001
        500000000000002
        >>> key, value = cache.peek()
        >>> print(key)
        500000000000000
        >>> value
        'a'
        >>> key, value = cache.peek(side='back')
        >>> print(key)
        500000000000002
        >>> value
        'c'

        :param str prefix: key prefix (default None, key is integer)
        :param default: value to return if key is missing
            (default (None, None))
        :param str side: either 'front' or 'back' (default 'front')
        :param bool expire_time: if True, return expire_time in tuple
            (default False)
        :param bool tag: if True, return tag in tuple (default False)
        :param bool retry: retry if database timeout occurs (default False)
        :return: key and value item pair or default if queue is empty
        :raises Timeout: if database timeout occurs

        """"""
        # Caution: Nearly identical code exists in Cache.pull
        if prefix is None:
            min_key = 0
            max_key = 999999999999999
        else:
            min_key = prefix + '-000000000000000'
            max_key = prefix + '-999999999999999'

        order = {'front': 'ASC', 'back': 'DESC'}
        select = (
            'SELECT rowid, key, expire_time, tag, mode, filename, value'
            ' FROM Cache WHERE ? < key AND key < ? AND raw = 1'
            ' ORDER BY key %s LIMIT 1'
        ) % order[side]

        if expire_time and tag:
            default = default, None, None
        elif expire_time or tag:
            default = default, None

        while True:
            while True:
                with self._transact(retry) as (sql, cleanup):
                    rows = sql(select, (min_key, max_key)).fetchall()

                    if not rows:
                        return default

                    (
                        (rowid, key, db_expire, db_tag, mode, name, db_value),
                    ) = rows

                    if db_expire is not None and db_expire < time.time():
                        sql('DELETE FROM Cache WHERE rowid = ?', (rowid,))
                        cleanup(name)
                    else:
                        break

            try:
                value = self._disk.fetch(mode, name, db_value, False)
            except IOError:
                # Key was deleted before we could retrieve result.
                continue
            finally:
                if name is not None:
                    self._disk.remove(name)
            break

        if expire_time and tag:
            return (key, value), db_expire, db_tag
        elif expire_time:
            return (key, value), db_expire
        elif tag:
            return (key, value), db_tag
        else:
            return key, value",db_expire is not None and db_expire < time.time(),None is not db_expire < time.time()
pyrender,https://github.com/mmatl/pyrender/tree/master/pyrender/camera.py,OrthographicCamera,get_projection_matrix$283,"def get_projection_matrix(self, width=None, height=None):
        """"""Return the OpenGL projection matrix for this camera.

        Parameters
        ----------
        width : int
            Width of the current viewport, in pixels.
            Unused in this function.
        height : int
            Height of the current viewport, in pixels.
            Unused in this function.
        """"""
        xmag = self.xmag
        ymag = self.ymag

        # If screen width/height defined, rescale xmag
        if width is not None and height is not None:
            xmag = width / height * ymag

        n = self.znear
        f = self.zfar
        P = np.zeros((4,4))
        P[0][0] = 1.0 / xmag
        P[1][1] = 1.0 / ymag
        P[2][2] = 2.0 / (n - f)
        P[2][3] = (f + n) / (n - f)
        P[3][3] = 1.0
        return P",width is not None and height is not None,width is not None is not height
exbert,https://github.com/bhoov/exbert/tree/master/server/transformers/src/transformers/tokenization_utils.py,PreTrainedTokenizer,add_tokens$1127,"def add_tokens(self, new_tokens):
        """"""
        Add a list of new tokens to the tokenizer class. If the new tokens are not in the
        vocabulary, they are added to it with indices starting from length of the current vocabulary.

        Args:
            new_tokens: string or list of string. Each string is a token to add. Tokens are only added if they are not
            already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).

        Returns:
            Number of tokens added to the vocabulary.

        Examples::

            # Let's see how to increase the vocabulary of Bert model and tokenizer
            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
            model = BertModel.from_pretrained('bert-base-uncased')

            num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])
            print('We have added', num_added_toks, 'tokens')
            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.
        """"""
        if not new_tokens:
            return 0

        if not isinstance(new_tokens, list):
            new_tokens = [new_tokens]

        to_add_tokens = []
        for token in new_tokens:
            assert isinstance(token, str)
            if self.init_kwargs.get(""do_lower_case"", False) and token not in self.all_special_tokens:
                token = token.lower()
            if (
                token != self.unk_token
                and self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token)
                and token not in to_add_tokens
            ):
                to_add_tokens.append(token)
                logger.info(""Adding %s to the vocabulary"", token)

        added_tok_encoder = dict((tok, len(self) + i) for i, tok in enumerate(to_add_tokens))
        added_tok_decoder = {v: k for k, v in added_tok_encoder.items()}
        self.added_tokens_encoder.update(added_tok_encoder)
        self.unique_added_tokens_encoder = set(self.added_tokens_encoder.keys()).union(set(self.all_special_tokens))
        self.added_tokens_decoder.update(added_tok_decoder)

        return len(to_add_tokens)",token != self.unk_token and self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token) and (token not in to_add_tokens),self.unk_token != token not in to_add_tokens and self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token)
taurus,https://github.com/Blazemeter/taurus/tree/master/tests/unit/modules/jmeter/test_ScenarioBuilder.py,TestScenarioBuilder,get_plugin_json_extractor_config$34,"def get_plugin_json_extractor_config(xml_tree):
        cfg = {}
        block_name = ""com.atlantbh.jmeter.plugins.jsonutils.jsonpathextractor.JSONPathExtractor""
        blocks = xml_tree.findall("".//%s"" % block_name)
        for block in blocks:
            varname = block.find("".//stringProp[@name='VAR']"")
            jsonpath = block.find("".//stringProp[@name='JSONPATH']"")
            default = block.find("".//stringProp[@name='DEFAULT']"")
            subject = block.find("".//stringProp[@name='SUBJECT']"")
            from_variable = block.find("".//stringProp[@name='VARIABLE']"")
            varname = varname.text
            jsonpath = jsonpath.text
            if default is not None:
                default = default.text
            if (subject is not None) and subject.text == ""VAR"" and (from_variable is not None):
                from_variable = from_variable.text
            else:
                from_variable = None
            cfg[varname] = {""jsonpath"": jsonpath, ""default"": default, ""from_variable"": from_variable}
        return cfg",subject is not None and subject.text == 'VAR' and (from_variable is not None),subject is not None is not from_variable and subject.text == 'VAR'
hypothesis,https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/src/hypothesis/vendor/pretty.py,,_seq_pprinter_factory$519,"def _seq_pprinter_factory(start, end, basetype):
    """"""Factory that returns a pprint function useful for sequences.

    Used by the default pprint for tuples, dicts, and lists.

    """"""

    def inner(obj, p, cycle):
        typ = type(obj)
        if (
            basetype is not None
            and typ is not basetype
            and typ.__repr__ != basetype.__repr__
        ):
            # If the subclass provides its own repr, use it instead.
            return p.text(typ.__repr__(obj))

        if cycle:
            return p.text(start + ""..."" + end)
        step = len(start)
        p.begin_group(step, start)
        for idx, x in p._enumerate(obj):
            if idx:
                p.text("","")
                p.breakable()
            p.pretty(x)
        if len(obj) == 1 and type(obj) is tuple:
            # Special case for 1-item tuples.
            p.text("","")
        p.end_group(step, end)

    return inner",basetype is not None and typ is not basetype and (typ.__repr__ != basetype.__repr__),None is not basetype is not typ and typ.__repr__ != basetype.__repr__
sphinx,https://github.com/sphinx-doc/sphinx/tree/master/sphinx/domains/c.py,ASTAssignmentExpr,describe_signature$583,"def describe_signature(self, signode: TextElement, mode: str,
                           env: ""BuildEnvironment"", symbol: ""Symbol"") -> None:
        self.exprs[0].describe_signature(signode, mode, env, symbol)
        for i in range(1, len(self.exprs)):
            signode += addnodes.desc_sig_space()
            op = self.ops[i - 1]
            if ord(op[0]) >= ord('a') and ord(op[0]) <= ord('z'):
                signode += addnodes.desc_sig_keyword(op, op)
            else:
                signode += addnodes.desc_sig_operator(op, op)
            signode += addnodes.desc_sig_space()
            self.exprs[i].describe_signature(signode, mode, env, symbol)",ord(op[0]) >= ord('a') and ord(op[0]) <= ord('z'),ord('a') <= ord(op[0]) <= ord('z')
R-Drop,https://github.com/dropreg/R-Drop/tree/master/fairseq_src/fairseq/sequence_generator.py,SequenceGenerator,_generate$184,"def _generate(
        self,
        sample: Dict[str, Dict[str, Tensor]],
        prefix_tokens: Optional[Tensor] = None,
        constraints: Optional[Tensor] = None,
        bos_token: Optional[int] = None,
    ):
        incremental_states = torch.jit.annotate(
            List[Dict[str, Dict[str, Optional[Tensor]]]],
            [
                torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {})
                for i in range(self.model.models_size)
            ],
        )
        net_input = sample[""net_input""]

        if ""src_tokens"" in net_input:
            src_tokens = net_input[""src_tokens""]
            # length of the source text being the character length except EndOfSentence and pad
            src_lengths = (
                (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)
            )
        elif ""source"" in net_input:
            src_tokens = net_input[""source""]
            src_lengths = (
                net_input[""padding_mask""].size(-1) - net_input[""padding_mask""].sum(-1)
                if net_input[""padding_mask""] is not None
                else torch.tensor(src_tokens.size(-1)).to(src_tokens)
            )
        else:
            raise Exception(""expected src_tokens or source in net input"")

        # bsz: total number of sentences in beam
        # Note that src_tokens may have more than 2 dimenions (i.e. audio features)
        bsz, src_len = src_tokens.size()[:2]
        beam_size = self.beam_size

        if constraints is not None and not self.search.supports_constraints:
            raise NotImplementedError(
                ""Target-side constraints were provided, but search method doesn't support them""
            )

        # Initialize constraints, when active
        self.search.init_constraints(constraints, beam_size)

        max_len: int = -1
        if self.match_source_len:
            max_len = src_lengths.max().item()
        else:
            max_len = min(
                int(self.max_len_a * src_len + self.max_len_b),
                # exclude the EOS marker
                self.model.max_decoder_positions() - 1,
            )
        assert (
            self.min_len <= max_len
        ), ""min_len cannot be larger than max_len, please adjust these!""
        # compute the encoder output for each beam
        encoder_outs = self.model.forward_encoder(net_input)

        # placeholder of indices for bsz * beam_size to hold tokens and accumulative scores
        new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)
        new_order = new_order.to(src_tokens.device).long()
        encoder_outs = self.model.reorder_encoder_out(encoder_outs, new_order)
        # ensure encoder_outs is a List.
        assert encoder_outs is not None

        # initialize buffers
        scores = (
            torch.zeros(bsz * beam_size, max_len + 1).to(src_tokens).float()
        )  # +1 for eos; pad is never chosen for scoring
        tokens = (
            torch.zeros(bsz * beam_size, max_len + 2)
            .to(src_tokens)
            .long()
            .fill_(self.pad)
        )  # +2 for eos and pad
        tokens[:, 0] = self.eos if bos_token is None else bos_token
        attn: Optional[Tensor] = None

        # A list that indicates candidates that should be ignored.
        # For example, suppose we're sampling and have already finalized 2/5
        # samples. Then cands_to_ignore would mark 2 positions as being ignored,
        # so that we only finalize the remaining 3 samples.
        cands_to_ignore = (
            torch.zeros(bsz, beam_size).to(src_tokens).eq(-1)
        )  # forward and backward-compatible False mask

        # list of completed sentences
        finalized = torch.jit.annotate(
            List[List[Dict[str, Tensor]]],
            [torch.jit.annotate(List[Dict[str, Tensor]], []) for i in range(bsz)],
        )  # contains lists of dictionaries of infomation about the hypothesis being finalized at each step

        finished = [
            False for i in range(bsz)
        ]  # a boolean array indicating if the sentence at the index is finished or not
        num_remaining_sent = bsz  # number of sentences remaining

        # number of candidate hypos per step
        cand_size = 2 * beam_size  # 2 x beam size in case half are EOS

        # offset arrays for converting between different indexing schemes
        bbsz_offsets = (
            (torch.arange(0, bsz) * beam_size)
            .unsqueeze(1)
            .type_as(tokens)
            .to(src_tokens.device)
        )
        cand_offsets = torch.arange(0, cand_size).type_as(tokens).to(src_tokens.device)

        reorder_state: Optional[Tensor] = None
        batch_idxs: Optional[Tensor] = None

        original_batch_idxs: Optional[Tensor] = None
        if ""id"" in sample and isinstance(sample[""id""], Tensor):
            original_batch_idxs = sample[""id""]
        else:
            original_batch_idxs = torch.arange(0, bsz).type_as(tokens)

        for step in range(max_len + 1):  # one extra step for EOS marker
            # reorder decoder internal states based on the prev choice of beams
            if reorder_state is not None:
                if batch_idxs is not None:
                    # update beam indices to take into account removed sentences
                    corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(
                        batch_idxs
                    )
                    reorder_state.view(-1, beam_size).add_(
                        corr.unsqueeze(-1) * beam_size
                    )
                    original_batch_idxs = original_batch_idxs[batch_idxs]
                self.model.reorder_incremental_state(incremental_states, reorder_state)
                encoder_outs = self.model.reorder_encoder_out(
                    encoder_outs, reorder_state
                )

            lprobs, avg_attn_scores = self.model.forward_decoder(
                tokens[:, : step + 1],
                encoder_outs,
                incremental_states,
                self.temperature,
            )

            if self.lm_model is not None:
                lm_out = self.lm_model(tokens[:, : step + 1])
                probs = self.lm_model.get_normalized_probs(
                    lm_out, log_probs=True, sample=None
                )
                probs = probs[:, -1, :] * self.lm_weight
                lprobs += probs

            lprobs[lprobs != lprobs] = torch.tensor(-math.inf).to(lprobs)

            lprobs[:, self.pad] = -math.inf  # never select pad
            lprobs[:, self.unk] -= self.unk_penalty  # apply unk penalty

            # handle max length constraint
            if step >= max_len:
                lprobs[:, : self.eos] = -math.inf
                lprobs[:, self.eos + 1 :] = -math.inf

            # handle prefix tokens (possibly with different lengths)
            if (
                prefix_tokens is not None
                and step < prefix_tokens.size(1)
                and step < max_len
            ):
                lprobs, tokens, scores = self._prefix_tokens(
                    step, lprobs, scores, tokens, prefix_tokens, beam_size
                )
            elif step < self.min_len:
                # minimum length constraint (does not apply if using prefix_tokens)
                lprobs[:, self.eos] = -math.inf

            # Record attention scores, only support avg_attn_scores is a Tensor
            if avg_attn_scores is not None:
                if attn is None:
                    attn = torch.empty(
                        bsz * beam_size, avg_attn_scores.size(1), max_len + 2
                    ).to(scores)
                attn[:, :, step + 1].copy_(avg_attn_scores)

            scores = scores.type_as(lprobs)
            eos_bbsz_idx = torch.empty(0).to(
                tokens
            )  # indices of hypothesis ending with eos (finished sentences)
            eos_scores = torch.empty(0).to(
                scores
            )  # scores of hypothesis ending with eos (finished sentences)

            if self.should_set_src_lengths:
                self.search.set_src_lengths(src_lengths)

            if self.repeat_ngram_blocker is not None:
                lprobs = self.repeat_ngram_blocker(
                    tokens, lprobs, bsz, beam_size, step
                )

            # Shape: (batch, cand_size)
            cand_scores, cand_indices, cand_beams = self.search.step(
                step,
                lprobs.view(bsz, -1, self.vocab_size),
                scores.view(bsz, beam_size, -1)[:, :, :step],
                tokens[:, : step + 1],
                original_batch_idxs,
            )

            # cand_bbsz_idx contains beam indices for the top candidate
            # hypotheses, with a range of values: [0, bsz*beam_size),
            # and dimensions: [bsz, cand_size]
            cand_bbsz_idx = cand_beams.add(bbsz_offsets)

            # finalize hypotheses that end in eos
            # Shape of eos_mask: (batch size, beam size)
            eos_mask = cand_indices.eq(self.eos) & cand_scores.ne(-math.inf)
            eos_mask[:, :beam_size][cands_to_ignore] = torch.tensor(0).to(eos_mask)

            # only consider eos when it's among the top beam_size indices
            # Now we know what beam item(s) to finish
            # Shape: 1d list of absolute-numbered
            eos_bbsz_idx = torch.masked_select(
                cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size]
            )

            finalized_sents: List[int] = []
            if eos_bbsz_idx.numel() > 0:
                eos_scores = torch.masked_select(
                    cand_scores[:, :beam_size], mask=eos_mask[:, :beam_size]
                )

                finalized_sents = self.finalize_hypos(
                    step,
                    eos_bbsz_idx,
                    eos_scores,
                    tokens,
                    scores,
                    finalized,
                    finished,
                    beam_size,
                    attn,
                    src_lengths,
                    max_len,
                )
                num_remaining_sent -= len(finalized_sents)

            assert num_remaining_sent >= 0
            if num_remaining_sent == 0:
                break
            if self.search.stop_on_max_len and step >= max_len:
                break
            assert step < max_len, f""{step} < {max_len}""

            # Remove finalized sentences (ones for which {beam_size}
            # finished hypotheses have been generated) from the batch.
            if len(finalized_sents) > 0:
                new_bsz = bsz - len(finalized_sents)

                # construct batch_idxs which holds indices of batches to keep for the next pass
                batch_mask = torch.ones(
                    bsz, dtype=torch.bool, device=cand_indices.device
                )
                batch_mask[finalized_sents] = False
                # TODO replace `nonzero(as_tuple=False)` after TorchScript supports it
                batch_idxs = torch.arange(
                    bsz, device=cand_indices.device
                ).masked_select(batch_mask)

                # Choose the subset of the hypothesized constraints that will continue
                self.search.prune_sentences(batch_idxs)

                eos_mask = eos_mask[batch_idxs]
                cand_beams = cand_beams[batch_idxs]
                bbsz_offsets.resize_(new_bsz, 1)
                cand_bbsz_idx = cand_beams.add(bbsz_offsets)
                cand_scores = cand_scores[batch_idxs]
                cand_indices = cand_indices[batch_idxs]

                if prefix_tokens is not None:
                    prefix_tokens = prefix_tokens[batch_idxs]
                src_lengths = src_lengths[batch_idxs]
                cands_to_ignore = cands_to_ignore[batch_idxs]

                scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)
                tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)
                if attn is not None:
                    attn = attn.view(bsz, -1)[batch_idxs].view(
                        new_bsz * beam_size, attn.size(1), -1
                    )
                bsz = new_bsz
            else:
                batch_idxs = None

            # Set active_mask so that values > cand_size indicate eos hypos
            # and values < cand_size indicate candidate active hypos.
            # After, the min values per row are the top candidate active hypos

            # Rewrite the operator since the element wise or is not supported in torchscript.

            eos_mask[:, :beam_size] = ~((~cands_to_ignore) & (~eos_mask[:, :beam_size]))
            active_mask = torch.add(
                eos_mask.type_as(cand_offsets) * cand_size,
                cand_offsets[: eos_mask.size(1)],
            )

            # get the top beam_size active hypotheses, which are just
            # the hypos with the smallest values in active_mask.
            # {active_hypos} indicates which {beam_size} hypotheses
            # from the list of {2 * beam_size} candidates were
            # selected. Shapes: (batch size, beam size)
            new_cands_to_ignore, active_hypos = torch.topk(
                active_mask, k=beam_size, dim=1, largest=False
            )

            # update cands_to_ignore to ignore any finalized hypos.
            cands_to_ignore = new_cands_to_ignore.ge(cand_size)[:, :beam_size]
            # Make sure there is at least one active item for each sentence in the batch.
            assert (~cands_to_ignore).any(dim=1).all()

            # update cands_to_ignore to ignore any finalized hypos

            # {active_bbsz_idx} denotes which beam number is continued for each new hypothesis (a beam
            # can be selected more than once).
            active_bbsz_idx = torch.gather(cand_bbsz_idx, dim=1, index=active_hypos)
            active_scores = torch.gather(cand_scores, dim=1, index=active_hypos)

            active_bbsz_idx = active_bbsz_idx.view(-1)
            active_scores = active_scores.view(-1)

            # copy tokens and scores for active hypotheses

            # Set the tokens for each beam (can select the same row more than once)
            tokens[:, : step + 1] = torch.index_select(
                tokens[:, : step + 1], dim=0, index=active_bbsz_idx
            )
            # Select the next token for each of them
            tokens.view(bsz, beam_size, -1)[:, :, step + 1] = torch.gather(
                cand_indices, dim=1, index=active_hypos
            )
            if step > 0:
                scores[:, :step] = torch.index_select(
                    scores[:, :step], dim=0, index=active_bbsz_idx
                )
            scores.view(bsz, beam_size, -1)[:, :, step] = torch.gather(
                cand_scores, dim=1, index=active_hypos
            )

            # Update constraints based on which candidates were selected for the next beam
            self.search.update_constraints(active_hypos)

            # copy attention for active hypotheses
            if attn is not None:
                attn[:, :, : step + 2] = torch.index_select(
                    attn[:, :, : step + 2], dim=0, index=active_bbsz_idx
                )

            # reorder incremental state in decoder
            reorder_state = active_bbsz_idx

        # sort by score descending
        for sent in range(len(finalized)):
            scores = torch.tensor(
                [float(elem[""score""].item()) for elem in finalized[sent]]
            )
            _, sorted_scores_indices = torch.sort(scores, descending=True)
            finalized[sent] = [finalized[sent][ssi] for ssi in sorted_scores_indices]
            finalized[sent] = torch.jit.annotate(
                List[Dict[str, Tensor]], finalized[sent]
            )
        return finalized",prefix_tokens is not None and step < prefix_tokens.size(1) and (step < max_len),prefix_tokens is not None and prefix_tokens.size(1) > step < max_len
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/base/models/items.py,Item,clean_per_order$712,"def clean_per_order(min_per_order, max_per_order):
        if min_per_order is not None and max_per_order is not None:
            if min_per_order > max_per_order:
                raise ValidationError(_('The maximum number per order can not be lower than the minimum number per '
                                        'order.'))",min_per_order is not None and max_per_order is not None,min_per_order is not None is not max_per_order
napari,https://github.com/napari/napari/tree/master/napari/layers/shapes/_shapes_models/rectangle.py,Rectangle,data$53,"def data(self, data):
        data = np.array(data).astype(float)

        if len(self.dims_order) != data.shape[1]:
            self._dims_order = list(range(data.shape[1]))

        if len(data) == 2 and data.shape[1] == 2:
            data = find_corners(data)

        if len(data) != 4:
            print(data)
            raise ValueError(
                trans._(
                    ""Data shape does not match a rectangle. Rectangle expects four corner vertices, {number} provided."",
                    deferred=True,
                    number=len(data),
                )
            )

        self._data = data
        self._update_displayed_data()",len(data) == 2 and data.shape[1] == 2,len(data) == 2 == data.shape[1]
torchdistill,https://github.com/yoshitomo-matsubara/torchdistill/tree/master/torchdistill/losses/custom.py,GeneralizedCustomLoss,__str__$60,"def __str__(self):
        desc = 'Loss = '
        tuple_list = [(self.org_loss_factor, 'OrgLoss')] \
            if self.org_loss_factor is not None and self.org_loss_factor != 0 else list()
        tuple_list.extend([(factor, criterion) for criterion, factor in self.term_dict.values()])
        desc += ' + '.join(['{} * {}'.format(factor, criterion) for factor, criterion in tuple_list])
        return desc",self.org_loss_factor is not None and self.org_loss_factor != 0,None is not self.org_loss_factor != 0
aws-iot-device-sdk-python,https://github.com/aws/aws-iot-device-sdk-python/tree/master/AWSIoTPythonSDK/core/protocol/internal/queues.py,OfflineRequestQueue,__init__$29,"def __init__(self, max_size, drop_behavior=DropBehaviorTypes.DROP_NEWEST):
        if not isinstance(max_size, int) or not isinstance(drop_behavior, int):
            self._logger.error(""init: MaximumSize/DropBehavior must be integer."")
            raise TypeError(""MaximumSize/DropBehavior must be integer."")
        if drop_behavior != DropBehaviorTypes.DROP_OLDEST and drop_behavior != DropBehaviorTypes.DROP_NEWEST:
            self._logger.error(""init: Drop behavior not supported."")
            raise ValueError(""Drop behavior not supported."")

        list.__init__([])
        self._drop_behavior = drop_behavior
        # When self._maximumSize > 0, queue is limited
        # When self._maximumSize == 0, queue is disabled
        # When self._maximumSize < 0. queue is infinite
        self._max_size = max_size",drop_behavior != DropBehaviorTypes.DROP_OLDEST and drop_behavior != DropBehaviorTypes.DROP_NEWEST,DropBehaviorTypes.DROP_OLDEST != drop_behavior != DropBehaviorTypes.DROP_NEWEST
go-explore,https://github.com/uber-research/go-explore/tree/master/robustified/goexplore_py/goexplore.py,Explore,run_explorer$750,"def run_explorer(self, explorer, start_cell=None, max_steps=-1):
        trajectory = []
        while True:
            if ((max_steps > 0 and len(trajectory) >= max_steps)):
                break
            action = explorer.get_action(ENV)
            state, reward, done, _ = self.step(action)
            self.frames_true += 1
            self.frames_compute += 1
            trajectory.append(
                TrajectoryElement(
                    # initial_pos_info,
                    self.get_pos_info(),
                    action, reward, done,
                    self.get_real_cell(),
                )
            )
            if done:
                break
        return trajectory",max_steps > 0 and len(trajectory) >= max_steps,len(trajectory) >= max_steps > 0
python-bitcoinlib,https://github.com/petertodd/python-bitcoinlib/tree/master/bitcoin/core/script.py,,IsLowDERSignature$868,"def IsLowDERSignature(sig):
    """"""
    Loosely correlates with IsLowDERSignature() from script/interpreter.cpp
    Verifies that the S value in a DER signature is the lowest possible value.
    Used by BIP62 malleability fixes.
    """"""
    length_r = sig[3]
    if isinstance(length_r, str):
        length_r = int(struct.unpack('B', length_r)[0])
    length_s = sig[5 + length_r]
    if isinstance(length_s, str):
        length_s = int(struct.unpack('B', length_s)[0])
    s_val = list(struct.unpack(str(length_s) + 'B', sig[6 + length_r:6 + length_r + length_s]))

    # If the S value is above the order of the curve divided by two, its
    # complement modulo the order could have been used instead, which is
    # one byte shorter when encoded correctly.
    max_mod_half_order = [
      0x7f,0xff,0xff,0xff,0xff,0xff,0xff,0xff,
      0xff,0xff,0xff,0xff,0xff,0xff,0xff,0xff,
      0x5d,0x57,0x6e,0x73,0x57,0xa4,0x50,0x1d,
      0xdf,0xe9,0x2f,0x46,0x68,0x1b,0x20,0xa0]

    return CompareBigEndian(s_val, [0]) > 0 and \
      CompareBigEndian(s_val, max_mod_half_order) <= 0","CompareBigEndian(s_val, [0]) > 0 and CompareBigEndian(s_val, max_mod_half_order) <= 0","CompareBigEndian(s_val, [0]) > 0 >= CompareBigEndian(s_val, max_mod_half_order)"
R-Drop,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/models/reformer/modeling_reformer.py,LSHSelfAttention,forward$359,"def forward(
        self,
        hidden_states,
        attention_mask=None,
        head_mask=None,
        num_hashes=None,
        buckets=None,
        past_buckets_states=None,
        use_cache=False,
        output_attentions=False,
        **kwargs
    ):
        sequence_length = hidden_states.shape[1]
        batch_size = hidden_states.shape[0]

        # num hashes can optionally be overwritten by user
        num_hashes = num_hashes if num_hashes is not None else self.num_hashes

        do_cached_attention = use_cache and past_buckets_states[1] is not None

        # check if cache shall be used and that hidden states are already cached
        if do_cached_attention:
            assert (
                sequence_length == 1
            ), f""At the moment, auto-regressive language generation is only possible one word at a time. Make sure that input sequence length {sequence_length} equals 1, when `past_buckets_states` is passed.""
            past_buckets = past_buckets_states[0]
            past_states = past_buckets_states[1]

            # get query vector
            query_vectors = self.query_key(hidden_states)
            query_vectors = self._split_hidden_size_dim(
                query_vectors, self.num_attention_heads, self.attention_head_size
            )

            if past_buckets is not None:
                key_value_hidden_states, sorted_bucket_idx, buckets = self._get_relevant_hid_states_and_buckets(
                    query_vectors=query_vectors,
                    attention_mask=attention_mask,
                    num_hashes=num_hashes,
                    hidden_states=hidden_states,
                    past_states=past_states,
                    past_buckets=past_buckets,
                )

                query_key_vectors = self._query_per_attn_head(key_value_hidden_states)
                value_vectors = self._value_per_attn_head(key_value_hidden_states)

                # split key & value vectors by num hashes to apply
                # self attention on each separately
                query_key_vectors = self._split_seq_length_dim_to(
                    query_key_vectors,
                    num_hashes,
                    -1,
                    self.num_attention_heads,
                    self.attention_head_size,
                )
                value_vectors = self._split_seq_length_dim_to(
                    value_vectors,
                    num_hashes,
                    -1,
                    self.num_attention_heads,
                    self.attention_head_size,
                )
                # repeat query vectors across hash dimension
                query_vectors = query_vectors.unsqueeze(2).repeat(1, 1, num_hashes, 1, 1)
            else:
                key_value_hidden_states = torch.cat([past_states, hidden_states], dim=1)

                query_key_vectors = self.query_key(key_value_hidden_states)
                value_vectors = self.value(key_value_hidden_states)

        else:
            # project hidden_states to query_key and value
            query_vectors = None
            query_key_vectors = self.query_key(hidden_states)
            value_vectors = self.value(hidden_states)

        # if query key is not already split
        if not do_cached_attention or past_buckets is None:
            query_key_vectors = self._split_hidden_size_dim(
                query_key_vectors, self.num_attention_heads, self.attention_head_size
            )
            value_vectors = self._split_hidden_size_dim(
                value_vectors, self.num_attention_heads, self.attention_head_size
            )

        # cache buckets for next incremental decoding
        if do_cached_attention and past_buckets is None and key_value_hidden_states.shape[1] >= self.chunk_length:
            buckets = self._hash_vectors(query_key_vectors, num_hashes, attention_mask)

        # free memory
        del hidden_states

        assert (
            query_key_vectors.shape[-1] == self.attention_head_size
        ), f""last dim of query_key_vectors is {query_key_vectors.shape[-1]} but should be {self.attention_head_size}.""
        assert (
            value_vectors.shape[-1] == self.attention_head_size
        ), f""last dim of value_vectors is {value_vectors.shape[-1]} but should be {self.attention_head_size}.""

        do_standard_self_attention = (sequence_length <= self.chunk_length) or (
            use_cache and past_buckets_states[1] is not None
        )
        # LSH attention only makes sense if chunked attention should be performed
        if not do_standard_self_attention:
            # set `num_buckets` on the fly, recommended way to do it
            if self.num_buckets is None:
                self._set_num_buckets(sequence_length)

            # use cached buckets for backprop only
            if buckets is None:
                # hash query key vectors into buckets
                buckets = self._hash_vectors(query_key_vectors, num_hashes, attention_mask)
            else:
                # make sure buckets has correct shape for LSH attention
                buckets = buckets.view(batch_size, self.num_attention_heads, num_hashes * sequence_length)

            assert (
                int(buckets.shape[-1]) == num_hashes * sequence_length
            ), f""last dim of buckets is {buckets.shape[-1]}, but should be {num_hashes * sequence_length}""

            sorted_bucket_idx, undo_sorted_bucket_idx = self._get_sorted_bucket_idx_and_undo_sorted_bucket_idx(
                sequence_length, buckets, num_hashes
            )

            # make sure bucket idx is not longer then sequence length
            sorted_bucket_idx_per_hash = sorted_bucket_idx % sequence_length

            # cluster query key value vectors according to hashed buckets
            query_key_vectors = self._gather_by_expansion(query_key_vectors, sorted_bucket_idx_per_hash, num_hashes)
            value_vectors = self._gather_by_expansion(value_vectors, sorted_bucket_idx_per_hash, num_hashes)
            query_key_vectors = self._split_seq_length_dim_to(
                query_key_vectors,
                -1,
                self.chunk_length,
                self.num_attention_heads,
                self.attention_head_size,
            )
            value_vectors = self._split_seq_length_dim_to(
                value_vectors,
                -1,
                self.chunk_length,
                self.num_attention_heads,
                self.attention_head_size,
            )

            if self.chunk_length is None:
                assert (
                    self.num_chunks_before == 0 and self.num_chunks_after == 0
                ), ""If `config.chunk_length` is `None`, make sure `config.num_chunks_after` and `config.num_chunks_before` are set to 0.""
        elif do_cached_attention and past_buckets is not None:
            # use max sequence length
            sorted_bucket_idx_per_hash = sorted_bucket_idx
        else:
            # get sequence length indices
            sorted_bucket_idx_per_hash = torch.arange(sequence_length, device=query_key_vectors.device).repeat(
                batch_size, self.num_attention_heads, 1
            )

        # scale key vectors
        key_vectors = self._len_and_dim_norm(query_key_vectors)

        # set query_vectors to query key vectors if LSH self attention
        query_vectors = query_vectors if query_vectors is not None else query_key_vectors

        # free memory
        del query_key_vectors

        # get attention probs
        out_vectors, logits, attention_probs = self._attend(
            query_vectors=query_vectors,
            key_vectors=key_vectors,
            value_vectors=value_vectors,
            sorted_bucket_idx_per_hash=sorted_bucket_idx_per_hash,
            attention_mask=attention_mask,
            head_mask=head_mask,
            do_standard_self_attention=do_standard_self_attention,
            do_cached_attention=do_cached_attention,
        )

        # free memory
        del key_vectors, value_vectors

        # re-order out_vectors and logits
        if not do_standard_self_attention:
            # sort clusters back to correct ordering
            out_vectors, logits = ReverseSort.apply(out_vectors, logits, sorted_bucket_idx, undo_sorted_bucket_idx)

        if not do_standard_self_attention or (do_cached_attention and past_buckets is not None):
            # sum up all hash rounds
            if num_hashes > 1:
                out_vectors = self._split_seq_length_dim_to(
                    out_vectors,
                    num_hashes,
                    sequence_length,
                    self.num_attention_heads,
                    self.attention_head_size,
                )
                logits = self._split_seq_length_dim_to(
                    logits,
                    num_hashes,
                    sequence_length,
                    self.num_attention_heads,
                    self.attention_head_size,
                ).unsqueeze(-1)

                probs_vectors = torch.exp(logits - torch.logsumexp(logits, dim=2, keepdim=True))
                out_vectors = torch.sum(out_vectors * probs_vectors, dim=2)
                # free memory
                del probs_vectors

            # free memory
            del logits

        assert out_vectors.shape == (
            batch_size,
            self.num_attention_heads,
            sequence_length,
            self.attention_head_size,
        ), ""out_vectors have be of shape `[batch_size, config.num_attention_heads, sequence_length, config.attention_head_size]`.""

        out_vectors = self._merge_hidden_size_dims(out_vectors, self.num_attention_heads, self.attention_head_size)

        if output_attentions is False:
            attention_probs = ()

        if buckets is not None:
            buckets = buckets.view(batch_size, self.num_attention_heads, num_hashes, -1)

        return LSHSelfAttentionOutput(hidden_states=out_vectors, attention_probs=attention_probs, buckets=buckets)",self.num_chunks_before == 0 and self.num_chunks_after == 0,self.num_chunks_before == 0 == self.num_chunks_after
bCNC,https://github.com/vlachoudis/bCNC/tree/master/bCNC/lib/svg_elements.py,Matrix,pre_scale$2741,"def pre_scale(self, sx=1.0, sy=None, x=0.0, y=0.0):
        if sy is None:
            sy = sx
        if x is None:
            x = 0.0
        if y is None:
            y = 0.0
        if x == 0 and y == 0:
            self.pre_cat(Matrix.scale(sx, sy))
        else:
            self.pre_translate(x, y)
            self.pre_scale(sx, sy)
            self.pre_translate(-x, -y)",x == 0 and y == 0,x == 0 == y
SMAC3,https://github.com/automl/SMAC3/tree/master/smac/runhistory/runhistory.py,RunHistory,add$191,"def add(
        self,
        config: Configuration,
        cost: float,
        time: float,
        status: StatusType,
        instance_id: typing.Optional[str] = None,
        seed: typing.Optional[int] = None,
        budget: float = 0.0,
        starttime: float = 0.0,
        endtime: float = 0.0,
        additional_info: typing.Optional[typing.Dict] = None,
        origin: DataOrigin = DataOrigin.INTERNAL,
        force_update: bool = False,
    ) -> None:
        """"""Adds a data of a new target algorithm (TA) run;
        it will update data if the same key values are used
        (config, instance_id, seed)

        Parameters
        ----------
            config : dict (or other type -- depending on config space module)
                Parameter configuration
            cost: float
                Cost of TA run (will be minimized)
            time: float
                Runtime of TA run
            status: str
                Status in {SUCCESS, TIMEOUT, CRASHED, ABORT, MEMOUT}
            instance_id: str
                String representing an instance (default: None)
            seed: int
                Random seed used by TA (default: None)
            budget: float
                budget (cutoff) used in intensifier to limit TA (default: 0)
            starttime: float
                starting timestamp of TA evaluation
            endtime: float
                ending timestamp of TA evaluation
            additional_info: dict
                Additional run infos (could include further returned
                information from TA or fields such as start time and host_id)
            origin: DataOrigin
                Defines how data will be used.
            force_update: bool (default: False)
                Forces the addition of a config to the history
        """"""

        if config is None:
            raise TypeError('Configuration to add to the runhistory must not be None')
        elif not isinstance(config, Configuration):
            raise TypeError(
                'Configuration to add to the runhistory is not of type Configuration, but %s' % type(config)
            )

        # Get the config id
        config_id_tmp = self.config_ids.get(config)
        if config_id_tmp is None:
            self._n_id += 1
            self.config_ids[config] = self._n_id
            config_id = typing.cast(int, self.config_ids.get(config))
            self.ids_config[self._n_id] = config
        else:
            config_id = typing.cast(int, config_id_tmp)

        k = RunKey(config_id, instance_id, seed, budget)
        v = RunValue(cost, time, status, starttime, endtime, additional_info)
        # Construct keys and values for the data dictionary
        for key, value in (
            ('config', config.get_dictionary()),
            ('config_id', config_id),
            ('instance_id', instance_id),
            ('seed', seed),
            ('budget', budget),
            ('cost', cost),
            ('time', time),
            ('status', status),
            ('starttime', starttime),
            ('endtime', endtime),
            ('additional_info', additional_info),
            ('origin', config.origin),
        ):
            self._check_json_serializable(key, value, EnumEncoder, k, v)

        # Each runkey is supposed to be used only once. Repeated tries to add
        # the same runkey will be ignored silently if not capped.
        if self.overwrite_existing_runs or force_update or self.data.get(k) is None:
            self._add(k, v, status, origin)
        elif status != StatusType.CAPPED and self.data[k].status == StatusType.CAPPED:
            # overwrite capped runs with uncapped runs
            self._add(k, v, status, origin)
        elif status == StatusType.CAPPED and self.data[k].status == StatusType.CAPPED and cost > self.data[k].cost:
            # overwrite if censored with a larger cutoff
            self._add(k, v, status, origin)",status != StatusType.CAPPED and self.data[k].status == StatusType.CAPPED,status != StatusType.CAPPED == self.data[k].status
SMAC3,https://github.com/automl/SMAC3/tree/master/smac/runhistory/runhistory.py,RunHistory,add$191,"def add(
        self,
        config: Configuration,
        cost: float,
        time: float,
        status: StatusType,
        instance_id: typing.Optional[str] = None,
        seed: typing.Optional[int] = None,
        budget: float = 0.0,
        starttime: float = 0.0,
        endtime: float = 0.0,
        additional_info: typing.Optional[typing.Dict] = None,
        origin: DataOrigin = DataOrigin.INTERNAL,
        force_update: bool = False,
    ) -> None:
        """"""Adds a data of a new target algorithm (TA) run;
        it will update data if the same key values are used
        (config, instance_id, seed)

        Parameters
        ----------
            config : dict (or other type -- depending on config space module)
                Parameter configuration
            cost: float
                Cost of TA run (will be minimized)
            time: float
                Runtime of TA run
            status: str
                Status in {SUCCESS, TIMEOUT, CRASHED, ABORT, MEMOUT}
            instance_id: str
                String representing an instance (default: None)
            seed: int
                Random seed used by TA (default: None)
            budget: float
                budget (cutoff) used in intensifier to limit TA (default: 0)
            starttime: float
                starting timestamp of TA evaluation
            endtime: float
                ending timestamp of TA evaluation
            additional_info: dict
                Additional run infos (could include further returned
                information from TA or fields such as start time and host_id)
            origin: DataOrigin
                Defines how data will be used.
            force_update: bool (default: False)
                Forces the addition of a config to the history
        """"""

        if config is None:
            raise TypeError('Configuration to add to the runhistory must not be None')
        elif not isinstance(config, Configuration):
            raise TypeError(
                'Configuration to add to the runhistory is not of type Configuration, but %s' % type(config)
            )

        # Get the config id
        config_id_tmp = self.config_ids.get(config)
        if config_id_tmp is None:
            self._n_id += 1
            self.config_ids[config] = self._n_id
            config_id = typing.cast(int, self.config_ids.get(config))
            self.ids_config[self._n_id] = config
        else:
            config_id = typing.cast(int, config_id_tmp)

        k = RunKey(config_id, instance_id, seed, budget)
        v = RunValue(cost, time, status, starttime, endtime, additional_info)
        # Construct keys and values for the data dictionary
        for key, value in (
            ('config', config.get_dictionary()),
            ('config_id', config_id),
            ('instance_id', instance_id),
            ('seed', seed),
            ('budget', budget),
            ('cost', cost),
            ('time', time),
            ('status', status),
            ('starttime', starttime),
            ('endtime', endtime),
            ('additional_info', additional_info),
            ('origin', config.origin),
        ):
            self._check_json_serializable(key, value, EnumEncoder, k, v)

        # Each runkey is supposed to be used only once. Repeated tries to add
        # the same runkey will be ignored silently if not capped.
        if self.overwrite_existing_runs or force_update or self.data.get(k) is None:
            self._add(k, v, status, origin)
        elif status != StatusType.CAPPED and self.data[k].status == StatusType.CAPPED:
            # overwrite capped runs with uncapped runs
            self._add(k, v, status, origin)
        elif status == StatusType.CAPPED and self.data[k].status == StatusType.CAPPED and cost > self.data[k].cost:
            # overwrite if censored with a larger cutoff
            self._add(k, v, status, origin)",status == StatusType.CAPPED and self.data[k].status == StatusType.CAPPED and (cost > self.data[k].cost),status == StatusType.CAPPED == self.data[k].status and cost > self.data[k].cost
autogluon,https://github.com/awslabs/autogluon/tree/master/tabular/src/autogluon/tabular/predictor/predictor.py,TabularPredictor,_validate_hyperparameter_tune_kwargs$2837,"def _validate_hyperparameter_tune_kwargs(self, hyperparameter_tune_kwargs, time_limit=None):
        """"""
        Returns True if hyperparameter_tune_kwargs is None or can construct a valid scheduler.
        Returns False if hyperparameter_tune_kwargs results in an invalid scheduler.
        """"""
        if hyperparameter_tune_kwargs is None:
            return True

        scheduler_cls, scheduler_params = scheduler_factory(hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,
                                                            time_out=time_limit,
                                                            nthreads_per_trial='auto', ngpus_per_trial='auto')

        if scheduler_params.get('dist_ip_addrs', None):
            logger.warning(
                'Warning: dist_ip_addrs does not currently work for Tabular. Distributed instances will not be utilized.')

        if scheduler_params['num_trials'] == 1:
            logger.warning(
                'Warning: Specified num_trials == 1 for hyperparameter tuning, disabling HPO. This can occur if time_limit was not specified in `fit()`.')
            return False

        scheduler_ngpus = scheduler_params['resource'].get('num_gpus', 0)
        if scheduler_ngpus is not None and isinstance(scheduler_ngpus, int) and scheduler_ngpus > 1:
            logger.warning(
                f""Warning: TabularPredictor currently doesn't use >1 GPU per training run. Detected {scheduler_ngpus} GPUs."")

        return True","scheduler_ngpus is not None and isinstance(scheduler_ngpus, int) and (scheduler_ngpus > 1)","None is not scheduler_ngpus > 1 and isinstance(scheduler_ngpus, int)"
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/incubate/fleet/base/role_maker.py,UserDefinedRoleMaker,__init__$1242,"def __init__(
        self,
        current_id=0,
        role=Role.WORKER,
        worker_num=0,
        server_endpoints=None,
    ):
        super().__init__()

        if not isinstance(server_endpoints, list):
            raise TypeError(""server_endpoints must be as string list"")
        elif len(server_endpoints) <= 0:
            raise ValueError(
                ""the length of server_endpoints list must be greater than 0""
            )
        elif len(server_endpoints) != len(set(server_endpoints)):
            raise ValueError(""server_endpoints can't have duplicate elements"")
        else:
            for server_endpoint in server_endpoints:
                if not isinstance(server_endpoint, str):
                    raise TypeError(
                        ""every element in server_endpoints list must be as string""
                    )
            self._server_endpoints = server_endpoints

        if role != Role.WORKER and role != Role.SERVER:
            raise TypeError(""role must be as Role"")
        else:
            self._role = role

        if not isinstance(current_id, int):
            raise TypeError(""current_id must be as int"")
        else:
            if current_id < 0:
                raise ValueError(
                    ""current_id must be greater than or equal to 0""
                )
            elif self._role == Role.SERVER and current_id >= len(
                server_endpoints
            ):
                raise ValueError(
                    ""if role is Role.SERVER, current_id must be less than or equal to len(server_endpoints) - 1""
                )
            self._current_id = current_id

        if not isinstance(worker_num, int):
            raise TypeError(""worker_num must be as int"")
        else:
            if worker_num <= 0:
                raise ValueError(""worker_num must be greater than 0"")
            self._worker_num = worker_num",role != Role.WORKER and role != Role.SERVER,Role.WORKER != role != Role.SERVER
mutagen,https://github.com/quodlibet/mutagen/tree/master/mutagen/_tools/_util.py,,split_escape$16,"def split_escape(string, sep, maxsplit=None, escape_char=""\\""):
    """"""Like unicode/str/bytes.split but allows for the separator to be escaped

    If passed unicode/str/bytes will only return list of unicode/str/bytes.
    """"""

    assert len(sep) == 1
    assert len(escape_char) == 1

    if isinstance(string, bytes):
        if isinstance(escape_char, str):
            escape_char = escape_char.encode(""ascii"")
        iter_ = iterbytes
    else:
        iter_ = iter

    if maxsplit is None:
        maxsplit = len(string)

    empty = string[:0]
    result = []
    current = empty
    escaped = False
    for char in iter_(string):
        if escaped:
            if char != escape_char and char != sep:
                current += escape_char
            current += char
            escaped = False
        else:
            if char == escape_char:
                escaped = True
            elif char == sep and len(result) < maxsplit:
                result.append(current)
                current = empty
            else:
                current += char
    result.append(current)
    return result",char != escape_char and char != sep,escape_char != char != sep
sparseml,https://github.com/neuralmagic/sparseml/tree/master/src/sparseml/onnx/utils/data.py,DataLoader,__next__$293,"def __next__(
        self,
    ) -> Tuple[Dict[str, numpy.ndarray], Union[None, Dict[str, numpy.ndarray]]]:
        if not self.infinite and self._step_count >= self._max_steps:
            _LOGGER.debug(""reached in of dataset, raising StopIteration"")
            raise StopIteration()

        self._step_count += 1
        data_batcher = NumpyArrayBatcher()
        label_batcher = NumpyArrayBatcher()
        num_resets = 0

        while len(data_batcher) < self._batch_size:
            try:
                _LOGGER.debug(""including data in batch at index {}"".format(self._index))
                dat, lab = self._labeled_data[self._index]

                if lab is None and len(label_batcher) > 0:
                    raise ValueError(
                        (
                            ""data has no label at index {}, but other data had labels""
                        ).format(self._index)
                    )
                elif (
                    lab is not None
                    and len(label_batcher) == 0
                    and len(data_batcher) > 0
                ):
                    raise ValueError(
                        (
                            ""data has label at index {}, ""
                            ""but other data did not have labels""
                        ).format(self._index)
                    )
                elif lab is not None:
                    label_batcher.append(lab)

                data_batcher.append(dat)
            except Exception as err:
                logging.error(
                    (
                        ""DataLoader: Error while adding file ""
                        ""to batch for index {}: {}""
                    ).format(self._index, err)
                )

            if self._index >= len(self._labeled_data) - 1:
                _LOGGER.debug(""resetting index to loop data again"")
                self._index = 0
                num_resets += 1

                if num_resets > self._batch_size // len(self._labeled_data) + 2:
                    # make sure we're not in an infinite loop because none of the
                    # data was loadable
                    raise ValueError(
                        ""could not create a batch from the files, ""
                        ""not enough were loadable to fill the batch size""
                    )
            else:
                self._index += 1

        batch_data = data_batcher.stack()
        _LOGGER.debug(""created batch data of size {}"".format(len(batch_data)))
        batch_label = label_batcher.stack() if len(label_batcher) > 0 else None

        if batch_label:
            _LOGGER.debug(""created batch labels of size {}"".format(len(batch_label)))

        return batch_data, batch_label",lab is not None and len(label_batcher) == 0 and (len(data_batcher) > 0),lab is not None and len(label_batcher) == 0 < len(data_batcher)
yamllint,https://github.com/adrienverge/yamllint/tree/master/yamllint/rules/key_duplicates.py,,check$73,"def check(conf, token, prev, next, nextnext, context):
    if 'stack' not in context:
        context['stack'] = []

    if isinstance(token, (yaml.BlockMappingStartToken,
                          yaml.FlowMappingStartToken)):
        context['stack'].append(Parent(MAP))
    elif isinstance(token, (yaml.BlockSequenceStartToken,
                            yaml.FlowSequenceStartToken)):
        context['stack'].append(Parent(SEQ))
    elif isinstance(token, (yaml.BlockEndToken,
                            yaml.FlowMappingEndToken,
                            yaml.FlowSequenceEndToken)):
        if len(context['stack']) > 0:
            context['stack'].pop()
    elif (isinstance(token, yaml.KeyToken) and
          isinstance(next, yaml.ScalarToken)):
        # This check is done because KeyTokens can be found inside flow
        # sequences... strange, but allowed.
        if len(context['stack']) > 0 and context['stack'][-1].type == MAP:
            if (next.value in context['stack'][-1].keys and
                    # `<<` is ""merge key"", see http://yaml.org/type/merge.html
                    next.value != '<<'):
                yield LintProblem(
                    next.start_mark.line + 1, next.start_mark.column + 1,
                    'duplication of key ""%s"" in mapping' % next.value)
            else:
                context['stack'][-1].keys.append(next.value)",next.value in context['stack'][-1].keys and next.value != '<<','<<' != next.value in context['stack'][-1].keys
salt,https://github.com/saltstack/salt/tree/master/salt/utils/dns.py,,_lookup_drill$324,"def _lookup_drill(name, rdtype, timeout=None, servers=None, secure=None):
    """"""
    Use drill to lookup addresses
    :param name: Name of record to search
    :param rdtype: DNS record type
    :param timeout: command return timeout
    :param servers: [] of servers to use
    :return: [] of records or False if error
    """"""
    cmd = ""drill ""
    if secure:
        cmd += ""-D -o ad ""
    cmd += ""{} {} "".format(rdtype, name)
    if servers:
        cmd += """".join([""@{} "".format(srv) for srv in servers])
    cmd = __salt__[""cmd.run_all""](
        cmd, timeout=timeout, python_shell=False, output_loglevel=""quiet""
    )

    if cmd[""retcode""] != 0:
        log.warning(""drill returned (%s): %s"", cmd[""retcode""], cmd[""stderr""])
        return False

    lookup_res = iter(cmd[""stdout""].splitlines())
    validated = False
    res = []
    try:
        line = """"
        while ""ANSWER SECTION"" not in line:
            line = next(lookup_res)
        while True:
            line = next(lookup_res)
            line = line.strip()
            if not line or line.startswith("";;""):
                break

            l_type, l_rec = line.split(None, 4)[-2:]
            if l_type == ""CNAME"" and rdtype != ""CNAME"":
                continue
            elif l_type == ""RRSIG"":
                validated = True
                continue
            elif l_type != rdtype:
                raise ValueError(""Invalid DNS type {}"".format(rdtype))

            res.append(_data_clean(l_rec))

    except StopIteration:
        pass

    if res and secure and not validated:
        return False
    else:
        return res",l_type == 'CNAME' and rdtype != 'CNAME',l_type == 'CNAME' != rdtype
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/gprof2dot.py,Profile,call_ratios$456,"def call_ratios(self, event):
        # Aggregate for incoming calls
        cycle_totals = {}
        for cycle in self.cycles:
            cycle_totals[cycle] = 0.0
        function_totals = {}
        for function in compat_itervalues(self.functions):
            function_totals[function] = 0.0

        # Pass 1:  function_total gets the sum of call[event] for all
        #          incoming arrows.  Same for cycle_total for all arrows
        #          that are coming into the *cycle* but are not part of it.
        for function in compat_itervalues(self.functions):
            for call in compat_itervalues(function.calls):
                if call.callee_id != function.id:
                    callee = self.functions[call.callee_id]
                    if event in call.events:
                        function_totals[callee] += call[event]
                        if (
                            callee.cycle is not None
                            and callee.cycle is not function.cycle
                        ):
                            cycle_totals[callee.cycle] += call[event]
                    else:
                        sys.stderr.write(
                            ""call_ratios: No data for ""
                            + function.name
                            + "" call to ""
                            + callee.name
                            + ""\n""
                        )

        # Pass 2:  Compute the ratios.  Each call[event] is scaled by the
        #          function_total of the callee.  Calls into cycles use the
        #          cycle_total, but not calls within cycles.
        for function in compat_itervalues(self.functions):
            for call in compat_itervalues(function.calls):
                assert call.ratio is None
                if call.callee_id != function.id:
                    callee = self.functions[call.callee_id]
                    if event in call.events:
                        if (
                            callee.cycle is not None
                            and callee.cycle is not function.cycle
                        ):
                            total = cycle_totals[callee.cycle]
                        else:
                            total = function_totals[callee]
                        call.ratio = ratio(call[event], total)
                    else:
                        # Warnings here would only repeat those issued above.
                        call.ratio = 0.0",callee.cycle is not None and callee.cycle is not function.cycle,None is not callee.cycle is not function.cycle
pupil,https://github.com/pupil-labs/pupil/tree/master/pupil_src/shared_modules/gprof2dot.py,Profile,call_ratios$456,"def call_ratios(self, event):
        # Aggregate for incoming calls
        cycle_totals = {}
        for cycle in self.cycles:
            cycle_totals[cycle] = 0.0
        function_totals = {}
        for function in compat_itervalues(self.functions):
            function_totals[function] = 0.0

        # Pass 1:  function_total gets the sum of call[event] for all
        #          incoming arrows.  Same for cycle_total for all arrows
        #          that are coming into the *cycle* but are not part of it.
        for function in compat_itervalues(self.functions):
            for call in compat_itervalues(function.calls):
                if call.callee_id != function.id:
                    callee = self.functions[call.callee_id]
                    if event in call.events:
                        function_totals[callee] += call[event]
                        if (
                            callee.cycle is not None
                            and callee.cycle is not function.cycle
                        ):
                            cycle_totals[callee.cycle] += call[event]
                    else:
                        sys.stderr.write(
                            ""call_ratios: No data for ""
                            + function.name
                            + "" call to ""
                            + callee.name
                            + ""\n""
                        )

        # Pass 2:  Compute the ratios.  Each call[event] is scaled by the
        #          function_total of the callee.  Calls into cycles use the
        #          cycle_total, but not calls within cycles.
        for function in compat_itervalues(self.functions):
            for call in compat_itervalues(function.calls):
                assert call.ratio is None
                if call.callee_id != function.id:
                    callee = self.functions[call.callee_id]
                    if event in call.events:
                        if (
                            callee.cycle is not None
                            and callee.cycle is not function.cycle
                        ):
                            total = cycle_totals[callee.cycle]
                        else:
                            total = function_totals[callee]
                        call.ratio = ratio(call[event], total)
                    else:
                        # Warnings here would only repeat those issued above.
                        call.ratio = 0.0",callee.cycle is not None and callee.cycle is not function.cycle,None is not callee.cycle is not function.cycle
nbdime,https://github.com/jupyter/nbdime/tree/master/nbdime/nbdiffapp.py,,_handle_diff$45,"def _handle_diff(base, remote, output, args):
    """"""Handles diffs of files, either as filenames or file-like objects""""""
    # Check that if args are filenames they either exist, or are
    # explicitly marked as missing (added/removed):
    for fn in (base, remote):
        if (isinstance(fn, str) and not os.path.exists(fn) and
                fn != EXPLICIT_MISSING_FILE):
            print(""Missing file {}"".format(fn))
            return 1
    # Both files cannot be missing
    assert not (base == EXPLICIT_MISSING_FILE and remote == EXPLICIT_MISSING_FILE), (
        'cannot diff %r against %r' % (base, remote))

    # Perform actual work:
    a = read_notebook(base, on_null='empty')
    b = read_notebook(remote, on_null='empty')

    d = diff_notebooks(a, b)

    # Output as JSON to file, or print to stdout:
    if output:
        with open(output, ""w"") as df:
            # Compact version:
            #json.dump(d, df)
            # Verbose version:
            json.dump(d, df, indent=2, separators=("","", "": ""))
    else:
        # This printer is to keep the unit tests passing,
        # some tests capture output with capsys which doesn't
        # pick up on sys.stdout.write()
        class Printer:
            def write(self, text):
                print(text, end="""")
        # This sets up what to ignore:
        config = prettyprint_config_from_args(args, out=Printer())
        # Separate out filenames:
        base_name = base if isinstance(base, str) else base.name
        remote_name = remote if isinstance(remote, str) else remote.name
        pretty_print_notebook_diff(base_name, remote_name, a, d, config)

    return 0",base == EXPLICIT_MISSING_FILE and remote == EXPLICIT_MISSING_FILE,base == EXPLICIT_MISSING_FILE == remote
zmirror,https://github.com/aploium/zmirror/tree/master/tests/base_class.py,ZmirrorTestBase,reload_zmirror$56,"def reload_zmirror(self, configs_dict=None):
        self.del_temp_var()

        import config
        importlib.reload(config)

        test_config_names = (name for name in dir(self.C) if name[:2] != '__' and name[-2:] != '__')
        for config_name in test_config_names:
            config_value = getattr(self.C, config_name)
            setattr(config, config_name, config_value)

        if configs_dict is not None:
            for config_name, config_value in configs_dict.items():
                setattr(config, config_name, config_value)

        import zmirror.cache_system as cache_system
        import zmirror.zmirror as zmirror
        importlib.reload(cache_system)
        importlib.reload(zmirror)

        zmirror.app.config['TESTING'] = True

        # 处理有端口号的测试, 在 del_temp_var() 中回滚
        if hasattr(self.C, ""my_host_port""):
            port = getattr(self.C, ""my_host_port"", None)
            my_host_name = getattr(self.C, ""my_host_name"", ""127.0.0.1"")
            if port is not None:
                self.C.my_host_name_no_port = my_host_name
                self.C.my_host_name = self.C.my_host_name_no_port + "":"" + str(port)
            else:
                self.C.my_host_name_no_port = my_host_name
        elif hasattr(self.C, ""my_host_name""):
            self.C.my_host_name_no_port = self.C.my_host_name

        self.client = zmirror.app.test_client()  # type: FlaskClient
        self.app = zmirror.app  # type: Flask
        self.zmirror = zmirror",name[:2] != '__' and name[-2:] != '__',name[:2] != '__' != name[-2:]
diffvg,https://github.com/BachiLi/diffvg/tree/master/apps/generative_models/sketch_rnn.py,,train$357,"def train(args):
    th.manual_seed(0)
    np.random.seed(0)

    dataset = data.QuickDrawDataset(args.dataset)
    dataloader = DataLoader(
        dataset, batch_size=args.bs, num_workers=4, shuffle=True,
        pin_memory=False)

    val_dataset = [s for idx, s in enumerate(dataset) if idx < 8]
    val_dataloader = DataLoader(
        val_dataset, batch_size=8, num_workers=4, shuffle=False,
        pin_memory=False)

    model_params = {
        ""zdim"": args.zdim,
        ""num_gaussians"": args.num_gaussians,
        ""encoder_dim"": args.encoder_dim,
        ""decoder_dim"": args.decoder_dim,
    }
    model = SketchRNN(**model_params)
    model.train()

    device = ""cpu""
    if th.cuda.is_available():
        device = ""cuda""
        LOG.info(""Using CUDA"")

    interface = Interface(model, lr=args.lr, lr_decay=args.lr_decay,
                          kl_decay=args.kl_decay, kl_weight=args.kl_weight,
                          sampling_temperature=args.sampling_temperature,
                          device=device)

    chkpt = OUTPUT_BASELINE
    env_name = ""sketch_rnn""

    # Resume from checkpoint, if any
    checkpointer = ttools.Checkpointer(
        chkpt, model, meta=model_params,
        optimizers=interface.optimizers(),
        schedulers=interface.schedulers)
    extras, meta = checkpointer.load_latest()
    epoch = extras[""epoch""] if extras and ""epoch"" in extras.keys() else 0

    if meta is not None and meta != model_params:
        LOG.info(""Checkpoint's metaparams differ ""
                 ""from CLI, aborting: %s and %s"", meta, model_params)

    trainer = ttools.Trainer(interface)

    # Add callbacks
    losses = [""loss"", ""kl_loss"", ""recons_loss""]
    training_debug = [""lr"", ""kl_weight""]
    trainer.add_callback(ttools.callbacks.ProgressBarCallback(
        keys=losses, val_keys=None))
    trainer.add_callback(ttools.callbacks.VisdomLoggingCallback(
        keys=losses, val_keys=None, env=env_name, port=args.port))
    trainer.add_callback(ttools.callbacks.VisdomLoggingCallback(
        keys=training_debug, smoothing=0, val_keys=None, env=env_name,
        port=args.port))
    trainer.add_callback(ttools.callbacks.CheckpointingCallback(
        checkpointer, max_files=2, interval=600, max_epochs=10))
    trainer.add_callback(
        ttools.callbacks.LRSchedulerCallback(interface.schedulers))

    trainer.add_callback(SketchRNNCallback(
        env=env_name, win=""samples"", port=args.port, frequency=args.freq))

    # Start training
    trainer.train(dataloader, starting_epoch=epoch,
                  val_dataloader=val_dataloader,
                  num_epochs=args.num_epochs)",meta is not None and meta != model_params,None is not meta != model_params
forseti-security,https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/scanner/audit/retention_rules_engine.py,,bucket_conditions_guarantee_min$446,"def bucket_conditions_guarantee_min(conditions, min_retention):
    """"""Check if other conditions can guarantee minimum retention.

    Args:
        conditions (dict): the condition dict of the bucket
        min_retention (int): the value of minimum retention.
    Returns:
        bool: True: min is guaranteed even if age is too small.
    """"""
    age = conditions.get('age')
    if age is not None and age >= min_retention:
        return True
    # if createdBefore is old enough, it's OK.
    if 'createdBefore' in conditions:
        created_before = conditions['createdBefore']
        dt_cfg = dt.get_datetime_from_string(created_before,
                                             '%Y-%m-%d')
        dt_now = dt.get_utc_now_datetime()
        day_diff = (dt_now - dt_cfg).days
        if day_diff >= min_retention:
            return True

    # if number of new version is larger than 0, OK.
    if conditions.get('numNewerVersions', 0) >= 1:
        return True

    return False",age is not None and age >= min_retention,None is not age >= min_retention
core,https://github.com/home-assistant/core/tree/master/homeassistant/components/yamaha_musiccast/media_player.py,MusicCastMediaPlayer,is_part_of_group$640,"def is_part_of_group(self, group_server) -> bool:
        """"""Return True if the given server is the server of self's group.""""""
        return group_server != self and (
            (
                self.ip_address in group_server.coordinator.data.group_client_list
                and self.coordinator.data.group_id
                == group_server.coordinator.data.group_id
                and self.ip_address != group_server.ip_address
                and self.source == ATTR_MC_LINK
            )
            or (
                self.ip_address == group_server.ip_address
                and self.source == ATTR_MAIN_SYNC
            )
        )",self.ip_address in group_server.coordinator.data.group_client_list and self.coordinator.data.group_id == group_server.coordinator.data.group_id and (self.ip_address != group_server.ip_address) and (self.source == ATTR_MC_LINK),group_server.ip_address != self.ip_address in group_server.coordinator.data.group_client_list and self.coordinator.data.group_id == group_server.coordinator.data.group_id and (self.source == ATTR_MC_LINK)
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/chainer_/chainercv2/models/resdropresnet_cifar.py,CIFARResDropResNet,__init__$97,"def __init__(self,
                 channels,
                 init_block_channels,
                 bottleneck,
                 life_probs,
                 in_channels=3,
                 in_size=(32, 32),
                 classes=10):
        super(CIFARResDropResNet, self).__init__()
        self.in_size = in_size
        self.classes = classes

        with self.init_scope():
            self.features = SimpleSequential()
            with self.features.init_scope():
                setattr(self.features, ""init_block"", conv3x3_block(
                    in_channels=in_channels,
                    out_channels=init_block_channels))
                in_channels = init_block_channels
                k = 0
                for i, channels_per_stage in enumerate(channels):
                    stage = SimpleSequential()
                    with stage.init_scope():
                        for j, out_channels in enumerate(channels_per_stage):
                            stride = 2 if (j == 0) and (i != 0) else 1
                            setattr(stage, ""unit{}"".format(j + 1), ResDropResUnit(
                                in_channels=in_channels,
                                out_channels=out_channels,
                                stride=stride,
                                bottleneck=bottleneck,
                                life_prob=life_probs[k]))
                            in_channels = out_channels
                            k += 1
                    setattr(self.features, ""stage{}"".format(i + 1), stage)
                setattr(self.features, ""final_pool"", partial(
                    F.average_pooling_2d,
                    ksize=8,
                    stride=1))

            self.output = SimpleSequential()
            with self.output.init_scope():
                setattr(self.output, ""flatten"", partial(
                    F.reshape,
                    shape=(-1, in_channels)))
                setattr(self.output, ""fc"", L.Linear(
                    in_size=in_channels,
                    out_size=classes))",j == 0 and i != 0,j == 0 != i
python-for-absolute-beginners-course,https://github.com/talkpython/python-for-absolute-beginners-course/tree/master/code/06-organizing-code-with-functions/rocks-game/rpsgame.py,,play_game$16,"def play_game(player_1, player_2):
    rounds = 3
    wins_p1 = 0
    wins_p2 = 0

    rolls = ['rock', 'paper', 'scissors']

    while wins_p1 < rounds and wins_p2 < rounds:
        roll1 = get_roll(player_1, rolls)
        roll2 = random.choice(rolls)

        if not roll1:
            print(""Try again!"")
            continue

        print(f""{player_1} roll {roll1}"")
        print(f""{player_2} rolls {roll2}"")

        winner = check_for_winning_throw(player_1, player_2, roll1, roll2)

        if winner is None:
            print(""This round was a tie!"")
        else:
            print(f'{winner} takes the round!')
            if winner == player_1:
                wins_p1 += 1
            elif winner == player_2:
                wins_p2 += 1

        print(f""Score is {player_1}: {wins_p1} and {player_2}: {wins_p2}."")
        print()

    if wins_p1 >= rounds:
        overall_winner = player_1
    else:
        overall_winner = player_2

    print(f""{overall_winner} wins the game!"")",wins_p1 < rounds and wins_p2 < rounds,wins_p1 < rounds > wins_p2
autogluon,https://github.com/awslabs/autogluon/tree/master/tabular/tests/conftest.py,ModelFitHelper,fit_dataset$230,"def fit_dataset(train_data, model, label, fit_args, sample_size=None):
        if sample_size is not None and sample_size < len(train_data):
            train_data = train_data.sample(n=sample_size, random_state=0)
        X = train_data.drop(columns=[label])
        y = train_data[label]

        problem_type = infer_problem_type(y)
        label_cleaner = LabelCleaner.construct(problem_type=problem_type, y=y)
        y = label_cleaner.transform(y)
        feature_generator = AutoMLPipelineFeatureGenerator()
        X = feature_generator.fit_transform(X, y)

        X, X_val, y, y_val = generate_train_test_split(X, y, problem_type=problem_type, test_size=0.2, random_state=0)

        model.fit(X=X, y=y, X_val=X_val, y_val=y_val, **fit_args)
        return model, label_cleaner, feature_generator",sample_size is not None and sample_size < len(train_data),None is not sample_size < len(train_data)
PyHive,https://github.com/dropbox/PyHive/tree/master/TCLIService/ttypes.py,TTypeDesc,write$1150,"def write(self, oprot):
        if oprot._fast_encode is not None and self.thrift_spec is not None:
            oprot.trans.write(oprot._fast_encode(self, (self.__class__, self.thrift_spec)))
            return
        oprot.writeStructBegin('TTypeDesc')
        if self.types is not None:
            oprot.writeFieldBegin('types', TType.LIST, 1)
            oprot.writeListBegin(TType.STRUCT, len(self.types))
            for iter33 in self.types:
                iter33.write(oprot)
            oprot.writeListEnd()
            oprot.writeFieldEnd()
        oprot.writeFieldStop()
        oprot.writeStructEnd()",oprot._fast_encode is not None and self.thrift_spec is not None,oprot._fast_encode is not None is not self.thrift_spec
intelmq,https://github.com/certtools/intelmq/tree/master/intelmq/lib/pipeline.py,PipelineFactory,create$32,"def create(logger, broker=None, direction=None, queues=None, pipeline_args=None, load_balance=False, is_multithreaded=False):
        """"""
        direction: ""source"" or ""destination"", optional, needed for queues
        queues: needs direction to be set, calls set_queues
        bot: Bot instance
        """"""
        if pipeline_args is None:
            pipeline_args = {}

        if direction not in [None, ""source"", ""destination""]:
            raise exceptions.InvalidArgument(""direction"", got=direction,
                                             expected=[""destination"", ""source""])

        if 'load_balance' not in pipeline_args:
            pipeline_args['load_balance'] = load_balance

        if direction == 'source' and 'source_pipeline_broker' in pipeline_args:
            broker = pipeline_args['source_pipeline_broker'].title()
        if direction == 'destination' and 'destination_pipeline_broker' in pipeline_args:
            broker = pipeline_args['destination_pipeline_broker'].title()
        elif (pipeline_args.get('source_pipeline_broker', None) == pipeline_args.get('destination_pipeline_broker', None) and
              pipeline_args.get('source_pipeline_broker', None) is not None):
            broker = pipeline_args['source_pipeline_broker'].title()
        else:
            if broker is not None:
                broker = broker.title()
            else:
                broker = ""Redis""
        pipe = getattr(intelmq.lib.pipeline, broker)(logger=logger, pipeline_args=pipeline_args, load_balance=load_balance, is_multithreaded=is_multithreaded)
        if queues and not direction:
            raise ValueError(""Parameter 'direction' must be given when using ""
                             ""the queues parameter."")
        elif queues:
            pipe.set_queues(queues, direction)

        return pipe","pipeline_args.get('source_pipeline_broker', None) == pipeline_args.get('destination_pipeline_broker', None) and pipeline_args.get('source_pipeline_broker', None) is not None","pipeline_args.get('destination_pipeline_broker', None) == pipeline_args.get('source_pipeline_broker', None) is not None"
arcade,https://github.com/pythonarcade/arcade/tree/master/arcade/experimental/bloom_multilayer_defender.py,MyGame,on_update$298,"def on_update(self, delta_time):
        """""" Movement and game logic """"""

        # Calculate speed based on the keys pressed
        if self.up_pressed and not self.down_pressed:
            self.player_sprite.accelerate_up()
        elif self.down_pressed and not self.up_pressed:
            self.player_sprite.accelerate_down()

        if self.left_pressed and not self.right_pressed:
            self.player_sprite.accelerate_left()
        elif self.right_pressed and not self.left_pressed:
            self.player_sprite.accelerate_right()

        # Call update to move the sprite
        self.player_list.update()
        self.bullet_sprite_list.update()

        for bullet in self.bullet_sprite_list:
            enemy_hit_list = arcade.check_for_collision_with_list(bullet, self.enemy_sprite_list)
            for enemy in enemy_hit_list:
                enemy.remove_from_sprite_lists()
                for i in range(10):
                    particle = Particle(4, 4, arcade.color.RED)
                    while particle.change_y == 0 and particle.change_x == 0:
                        particle.change_y = random.randrange(-2, 3)
                        particle.change_x = random.randrange(-2, 3)
                    particle.center_x = enemy.center_x
                    particle.center_y = enemy.center_y
                    self.bullet_sprite_list.append(particle)

        # Scroll left
        left_boundary = self.view_left + VIEWPORT_MARGIN
        if self.player_sprite.left < left_boundary:
            self.view_left -= left_boundary - self.player_sprite.left

        # Scroll right
        right_boundary = self.view_left + SCREEN_WIDTH - VIEWPORT_MARGIN
        if self.player_sprite.right > right_boundary:
            self.view_left += self.player_sprite.right - right_boundary

        # Scroll up
        self.view_bottom = DEFAULT_BOTTOM_VIEWPORT
        top_boundary = self.view_bottom + SCREEN_HEIGHT - TOP_VIEWPORT_MARGIN
        if self.player_sprite.top > top_boundary:
            self.view_bottom += self.player_sprite.top - top_boundary

        self.view_left = int(self.view_left)
        self.view_bottom = int(self.view_bottom)",particle.change_y == 0 and particle.change_x == 0,particle.change_y == 0 == particle.change_x
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/adapters/pose_estimation_associative_embedding.py,AssociativeEmbeddingDecoder,refine$274,"def refine(heatmap, tag, keypoints, pose_tag=None):
        K, H, W = heatmap.shape
        if len(tag.shape) == 3:
            tag = tag[..., None]

        if pose_tag is not None:
            prev_tag = pose_tag
        else:
            tags = []
            for i in range(K):
                if keypoints[i, 2] > 0:
                    x, y = keypoints[i][:2].astype(int)
                    tags.append(tag[i, y, x])
            prev_tag = np.mean(tags, axis=0)

        for i, (_heatmap, _tag) in enumerate(zip(heatmap, tag)):
            if keypoints[i, 2] > 0:
                continue
            # Get position with the closest tag value to the pose tag.
            diff = np.abs(_tag[..., 0] - prev_tag) + 0.5
            diff = diff.astype(np.int32).astype(_heatmap.dtype)
            diff -= _heatmap
            idx = diff.argmin()
            y, x = np.divmod(idx, _heatmap.shape[-1])
            # Corresponding keypoint detection score.
            val = _heatmap[y, x]
            if val > 0:
                keypoints[i, :3] = x, y, val
                if 1 < x < W - 1 and 1 < y < H - 1:
                    diff = np.array([
                        _heatmap[y, x + 1] - _heatmap[y, x - 1],
                        _heatmap[y + 1, x] - _heatmap[y - 1, x]
                    ])
                    keypoints[i, :2] += np.sign(diff) * .25

        return keypoints",1 < x < W - 1 and 1 < y < H - 1,W - 1 > x > 1 < y < H - 1
Parakeet,https://github.com/PaddlePaddle/Parakeet/tree/master/parakeet/frontend/tone_sandhi.py,ToneSandhi,_three_sandhi$166,"def _three_sandhi(self, word: str, finals: List[str]) -> List[str]:
        if len(word) == 2 and self._all_tone_three(finals):
            finals[0] = finals[0][:-1] + ""2""
        elif len(word) == 3:
            word_list = self._split_word(word)
            if self._all_tone_three(finals):
                #  disyllabic + monosyllabic, e.g. 蒙古/包
                if len(word_list[0]) == 2:
                    finals[0] = finals[0][:-1] + ""2""
                    finals[1] = finals[1][:-1] + ""2""
                #  monosyllabic + disyllabic, e.g. 纸/老虎
                elif len(word_list[0]) == 1:
                    finals[1] = finals[1][:-1] + ""2""
            else:
                finals_list = [
                    finals[:len(word_list[0])], finals[len(word_list[0]):]
                ]
                if len(finals_list) == 2:
                    for i, sub in enumerate(finals_list):
                        # e.g. 所有/人
                        if self._all_tone_three(sub) and len(sub) == 2:
                            finals_list[i][0] = finals_list[i][0][:-1] + ""2""
                        # e.g. 好/喜欢
                        elif i == 1 and not self._all_tone_three(sub) and finals_list[i][0][-1] == ""3"" and \
                                finals_list[0][-1][-1] == ""3"":

                            finals_list[0][-1] = finals_list[0][-1][:-1] + ""2""
                        finals = sum(finals_list, [])
        # split idiom into two words who's length is 2
        elif len(word) == 4:
            finals_list = [finals[:2], finals[2:]]
            finals = []
            for sub in finals_list:
                if self._all_tone_three(sub):
                    sub[0] = sub[0][:-1] + ""2""
                finals += sub

        return finals",i == 1 and (not self._all_tone_three(sub)) and (finals_list[i][0][-1] == '3') and (finals_list[0][-1][-1] == '3'),i == 1 and finals_list[i][0][-1] == '3' == finals_list[0][-1][-1] and (not self._all_tone_three(sub))
dnspython,https://github.com/rthalley/dnspython/tree/master/dns/name.py,,_validate_labels$266,"def _validate_labels(labels):
    """"""Check for empty labels in the middle of a label sequence,
    labels that are too long, and for too many labels.

    Raises ``dns.name.NameTooLong`` if the name as a whole is too long.

    Raises ``dns.name.EmptyLabel`` if a label is empty (i.e. the root
    label) and appears in a position other than the end of the label
    sequence

    """"""

    l = len(labels)
    total = 0
    i = -1
    j = 0
    for label in labels:
        ll = len(label)
        total += ll + 1
        if ll > 63:
            raise LabelTooLong
        if i < 0 and label == b'':
            i = j
        j += 1
    if total > 255:
        raise NameTooLong
    if i >= 0 and i != l - 1:
        raise EmptyLabel",i >= 0 and i != l - 1,0 <= i != l - 1
tartube,https://github.com/axcore/tartube/tree/master/tartube/downloads.py,JSONFetcher,read_child_process$9611,"def read_child_process(self):

        """"""Called by self.do_fetch().

        Reads from the child process STDOUT and STDERR, in the correct order.

        For this JSONFetcher object, the order doesn't matter very much: we
        are expecting data in either STDOUT or STDERR.

        Return values:

            True if either STDOUT or STDERR were read, None if both queues were
                empty

        """"""

        # mini_list is in the form [time, pipe_type, data]
        try:
            mini_list = self.queue.get_nowait()

        except:
            # Nothing left to read
            return None

        # Import the main application (for convenience)
        app_obj = self.download_manager_obj.app_obj

        # Failsafe check
        if not mini_list \
        or (mini_list[1] != 'stdout' and mini_list[1] != 'stderr'):

            # Just in case...
            GObject.timeout_add(
                0,
                self.download_manager_obj.app_obj.system_error,
                315,
                'Malformed STDOUT or STDERR data',
            )

        # STDOUT or STDERR has been read
        data = mini_list[2].rstrip()
        # Convert bytes to string
        data = data.decode(utils.get_encoding(), 'replace')

        # STDOUT
        if mini_list[1] == 'stdout':

            if data[:1] == '{':

                # Broadcasting livestream detected; create a new media.Video
                #   object
                GObject.timeout_add(
                    0,
                    app_obj.create_livestream_from_download,
                    self.container_obj,
                    2,                      # Livestream has started
                    self.video_name,
                    self.video_source,
                    self.video_descrip,
                    self.video_upload_time,
                )

        # STDERR
        else:

            live_data_dict = utils.extract_livestream_data(data)
            if live_data_dict:

                # Waiting livestream detected; create a new media.Video object
                GObject.timeout_add(
                    0,
                    app_obj.create_livestream_from_download,
                    self.container_obj,
                    1,                  # Livestream waiting to start
                    self.video_name,
                    self.video_source,
                    self.video_descrip,
                    self.video_upload_time,
                    live_data_dict,
                )

        # Either (or both) of STDOUT and STDERR were non-empty
        self.queue.task_done()
        return True",mini_list[1] != 'stdout' and mini_list[1] != 'stderr','stdout' != mini_list[1] != 'stderr'
ReAgent,https://github.com/facebookresearch/ReAgent/tree/master/reagent/ope/test/multiclass_bandits.py,MultiClassPolicy,_query$194,"def _query(self, context: int) -> Tuple[Action, ActionDistribution]:
        dist = self._action_distributions[context]
        if len(dist.shape) > 1 and dist.shape[0] == 1:
            dist = dist[0]
        if dist.shape[0] < len(self.action_space):
            dist = torch.cat(
                (dist, torch.zeros([len(self.action_space) - dist.shape[0]]))
            )
        dist = dist * self._exploitation_prob + self._exploration_prob
        action = torch.multinomial(dist, 1).item()
        return Action(action), ActionDistribution(dist)",len(dist.shape) > 1 and dist.shape[0] == 1,len(dist.shape) > 1 == dist.shape[0]
st2,https://github.com/StackStorm/st2/tree/master/st2common/st2common/util/action_db.py,,update_liveaction_status$207,"def update_liveaction_status(
    status=None,
    result=None,
    context=None,
    end_timestamp=None,
    liveaction_id=None,
    runner_info=None,
    liveaction_db=None,
    publish=True,
):
    """"""
    Update the status of the specified LiveAction to the value provided in
    new_status.

    The LiveAction may be specified using either liveaction_id, or as an
    liveaction_db instance.
    """"""

    if (liveaction_id is None) and (liveaction_db is None):
        raise ValueError(
            ""Must specify an liveaction_id or an liveaction_db when ""
            ""calling update_LiveAction_status""
        )

    if liveaction_db is None:
        liveaction_db = get_liveaction_by_id(liveaction_id)

    if status not in LIVEACTION_STATUSES:
        raise ValueError(
            'Attempting to set status for LiveAction ""%s"" '
            'to unknown status string. Unknown status is ""%s""' % (liveaction_db, status)
        )

    if (
        result
        and cfg.CONF.system.validate_output_schema
        and status == LIVEACTION_STATUS_SUCCEEDED
    ):
        action_db = get_action_by_ref(liveaction_db.action)
        runner_db = get_runnertype_by_name(action_db.runner_type[""name""])
        result, status = output_schema.validate_output(
            runner_db.output_schema,
            action_db.output_schema,
            result,
            status,
            runner_db.output_key,
        )

    # If liveaction_db status is set then we need to decrement the counter
    # because it is transitioning to a new state
    if liveaction_db.status:
        get_driver().dec_counter(""action.executions.%s"" % (liveaction_db.status))

    # If status is provided then we need to increment the timer because the action
    # is transitioning into this new state
    if status:
        get_driver().inc_counter(""action.executions.%s"" % (status))

    extra = {""liveaction_db"": liveaction_db}
    LOG.debug(
        'Updating ActionExection: ""%s"" with status=""%s""',
        liveaction_db.id,
        status,
        extra=extra,
    )

    # If liveaction is already canceled, then do not allow status to be updated.
    if (
        liveaction_db.status == LIVEACTION_STATUS_CANCELED
        and status != LIVEACTION_STATUS_CANCELED
    ):
        LOG.info(
            'Unable to update ActionExecution ""%s"" with status=""%s"". '
            ""ActionExecution is already canceled."",
            liveaction_db.id,
            status,
            extra=extra,
        )
        return liveaction_db

    old_status = liveaction_db.status
    liveaction_db.status = status

    if result:
        liveaction_db.result = result

    if context:
        liveaction_db.context.update(context)

    if end_timestamp:
        liveaction_db.end_timestamp = end_timestamp

    if runner_info:
        liveaction_db.runner_info = runner_info

    # TODO: This is not efficient. Perform direct partial update and only update
    # manipulated fields
    liveaction_db = LiveAction.add_or_update(liveaction_db)

    LOG.debug(""Updated status for LiveAction object."", extra=extra)

    if publish and status != old_status:
        LiveAction.publish_status(liveaction_db)
        LOG.debug(""Published status for LiveAction object."", extra=extra)

    return liveaction_db",liveaction_id is None and liveaction_db is None,liveaction_id is None is liveaction_db
st2,https://github.com/StackStorm/st2/tree/master/st2common/st2common/util/action_db.py,,update_liveaction_status$207,"def update_liveaction_status(
    status=None,
    result=None,
    context=None,
    end_timestamp=None,
    liveaction_id=None,
    runner_info=None,
    liveaction_db=None,
    publish=True,
):
    """"""
    Update the status of the specified LiveAction to the value provided in
    new_status.

    The LiveAction may be specified using either liveaction_id, or as an
    liveaction_db instance.
    """"""

    if (liveaction_id is None) and (liveaction_db is None):
        raise ValueError(
            ""Must specify an liveaction_id or an liveaction_db when ""
            ""calling update_LiveAction_status""
        )

    if liveaction_db is None:
        liveaction_db = get_liveaction_by_id(liveaction_id)

    if status not in LIVEACTION_STATUSES:
        raise ValueError(
            'Attempting to set status for LiveAction ""%s"" '
            'to unknown status string. Unknown status is ""%s""' % (liveaction_db, status)
        )

    if (
        result
        and cfg.CONF.system.validate_output_schema
        and status == LIVEACTION_STATUS_SUCCEEDED
    ):
        action_db = get_action_by_ref(liveaction_db.action)
        runner_db = get_runnertype_by_name(action_db.runner_type[""name""])
        result, status = output_schema.validate_output(
            runner_db.output_schema,
            action_db.output_schema,
            result,
            status,
            runner_db.output_key,
        )

    # If liveaction_db status is set then we need to decrement the counter
    # because it is transitioning to a new state
    if liveaction_db.status:
        get_driver().dec_counter(""action.executions.%s"" % (liveaction_db.status))

    # If status is provided then we need to increment the timer because the action
    # is transitioning into this new state
    if status:
        get_driver().inc_counter(""action.executions.%s"" % (status))

    extra = {""liveaction_db"": liveaction_db}
    LOG.debug(
        'Updating ActionExection: ""%s"" with status=""%s""',
        liveaction_db.id,
        status,
        extra=extra,
    )

    # If liveaction is already canceled, then do not allow status to be updated.
    if (
        liveaction_db.status == LIVEACTION_STATUS_CANCELED
        and status != LIVEACTION_STATUS_CANCELED
    ):
        LOG.info(
            'Unable to update ActionExecution ""%s"" with status=""%s"". '
            ""ActionExecution is already canceled."",
            liveaction_db.id,
            status,
            extra=extra,
        )
        return liveaction_db

    old_status = liveaction_db.status
    liveaction_db.status = status

    if result:
        liveaction_db.result = result

    if context:
        liveaction_db.context.update(context)

    if end_timestamp:
        liveaction_db.end_timestamp = end_timestamp

    if runner_info:
        liveaction_db.runner_info = runner_info

    # TODO: This is not efficient. Perform direct partial update and only update
    # manipulated fields
    liveaction_db = LiveAction.add_or_update(liveaction_db)

    LOG.debug(""Updated status for LiveAction object."", extra=extra)

    if publish and status != old_status:
        LiveAction.publish_status(liveaction_db)
        LOG.debug(""Published status for LiveAction object."", extra=extra)

    return liveaction_db",liveaction_db.status == LIVEACTION_STATUS_CANCELED and status != LIVEACTION_STATUS_CANCELED,liveaction_db.status == LIVEACTION_STATUS_CANCELED != status
dm_control,https://github.com/deepmind/dm_control/tree/master/dm_control/composer/initializers/tcp_initializer.py,ToolCenterPointInitializer,_has_relevant_collisions$89,"def _has_relevant_collisions(self, physics):
    mjcf_root = self._arm.mjcf_model.root_model
    all_geoms = mjcf_root.find_all('geom')
    free_body_geoms = set()
    for body in mjcf_root.worldbody.get_children('body'):
      if mjcf.get_freejoint(body):
        free_body_geoms.update(body.find_all('geom'))

    arm_model = self._arm.mjcf_model
    hand_model = None
    if self._hand is not None:
      hand_model = self._hand.mjcf_model

    def is_robot(geom):
      return geom.root is arm_model or geom.root is hand_model

    def is_external_body_without_freejoint(geom):
      return not (is_robot(geom) or geom in free_body_geoms)

    for contact in physics.data.contact:
      geom_1 = all_geoms[contact.geom1]
      geom_2 = all_geoms[contact.geom2]
      if contact.dist > 0:
        # Ignore ""contacts"" with positive distance (i.e. not actually touching).
        continue
      if (
          # Include arm-arm and arm-hand self-collisions (but not hand-hand).
          (geom_1.root is arm_model and geom_2.root is arm_model) or
          (geom_1.root is arm_model and geom_2.root is hand_model) or
          (geom_1.root is hand_model and geom_2.root is arm_model) or
          # Include collisions between the arm or hand and an external body
          # provided that the external body does not have a freejoint.
          (is_robot(geom_1) and is_external_body_without_freejoint(geom_2)) or
          (is_external_body_without_freejoint(geom_1) and is_robot(geom_2))):
        return True
    return False",geom_1.root is arm_model and geom_2.root is arm_model,geom_1.root is arm_model is geom_2.root
django-hvad,https://github.com/KristianOellegaard/django-hvad/tree/master/hvad/models.py,TranslatableModel,_check_ordering$397,"def _check_ordering(cls):
        if not cls._meta.ordering:
            return []

        if not isinstance(cls._meta.ordering, (list, tuple)):
            return [checks.Error(""'ordering' must be a tuple or list."",
                                 hint=None, obj=cls, id='models.E014')]

        fields = [f for f in cls._meta.ordering if f != '?']
        fields = [f[1:] if f.startswith('-') else f for f in fields]
        fields = set(f for f in fields if f not in ('_order', 'pk') and '__' not in f)

        valid_fields = set(chain.from_iterable(
            (f.name, f.attname)
            for f in cls._meta.fields
        ))
        valid_tfields = set(chain.from_iterable(
            (f.name, f.attname)
            for f in cls._meta.translations_model._meta.fields
            if f.name not in ('master', 'language_code')
        ))

        return [checks.Error(""'ordering' refers to the non-existent field '%s' --hvad."" % field,
                             hint=None, obj=cls, id='models.E015')
                for field in fields - valid_fields - valid_tfields]","f not in ('_order', 'pk') and '__' not in f","'__' not in f not in ('_order', 'pk')"
linearmodels,https://github.com/bashtage/linearmodels/tree/master/linearmodels/iv/data.py,IVData,__init__$58,"def __init__(
        self,
        x: Optional[""IVDataLike""],
        var_name: str = ""x"",
        nobs: Optional[int] = None,
        convert_dummies: bool = True,
        drop_first: bool = True,
    ):

        if isinstance(x, IVData):
            self.__dict__.update(copy.deepcopy(x.__dict__))
            return
        if x is None and nobs is not None:
            x = np.empty((nobs, 0))
        elif x is None:
            raise ValueError(""nobs required when x is None"")

        self.original = x
        assert x is not None
        xndim = x.ndim
        if xndim > 2:
            raise ValueError(dim_err.format(var_name, xndim))

        if isinstance(x, np.ndarray):
            x = x.astype(dtype=np.float64)
            if xndim == 1:
                x = x.reshape((x.shape[0], -1))

            self._ndarray = x.astype(np.float64)
            index = list(range(x.shape[0]))
            if x.shape[1] == 1:
                cols = [var_name]
            else:
                cols = [var_name + "".{0}"".format(i) for i in range(x.shape[1])]
            self._pandas = pd.DataFrame(x, index=index, columns=cols)
            self._row_labels = index
            self._col_labels = cols

        elif isinstance(x, (pd.Series, pd.DataFrame)):
            if isinstance(x, pd.Series):
                name = var_name if not x.name else x.name
                x = pd.DataFrame({name: x})
            copied = False
            columns = list(x.columns)
            if len(set(columns)) != len(columns):
                raise ValueError(
                    ""DataFrame contains duplicate column names. ""
                    ""All column names must be distinct""
                )
            all_numeric = True
            for col in x:
                c = x[col]
                if (
                    is_string_dtype(c.dtype)
                    and c.map(lambda v: isinstance(v, str)).all()
                ):
                    c = c.astype(""category"")
                    if not copied:
                        x = x.copy()
                        copied = True
                    x[col] = c
                dt = c.dtype
                all_numeric = all_numeric and is_numeric_dtype(dt)
                if not (is_numeric_dtype(dt) or is_categorical_dtype(dt)):
                    raise ValueError(
                        ""Only numeric, string  or categorical "" ""data permitted""
                    )

            if convert_dummies:
                x = expand_categoricals(x, drop_first)

            self._pandas = x
            self._ndarray = np.asarray(self._pandas)
            if all_numeric or convert_dummies:
                self._ndarray = self._ndarray.astype(np.float64)
            self._row_labels = list(x.axes[0])
            self._col_labels = list(x.axes[1])

        else:
            try:
                import xarray as xr
            except ImportError:
                raise TypeError(type_err)
            if isinstance(x, xr.DataArray):
                if x.ndim == 1:
                    x = xr.concat([x], dim=var_name)
                    assert isinstance(x, xr.DataArray)
                    x = x.transpose()

                index = list(x.coords[x.dims[0]].values)
                xr_col_values = x.coords[x.dims[1]].values
                xr_cols = list(xr_col_values)
                if is_numeric_dtype(xr_col_values.dtype):
                    xr_cols = [var_name + "".{0}"".format(i) for i in range(x.shape[1])]
                self._ndarray = x.values.astype(np.float64)
                self._pandas = pd.DataFrame(self._ndarray, columns=xr_cols, index=index)
                self._row_labels = index
                self._col_labels = xr_cols
            else:
                raise TypeError(type_err)

        if nobs is not None:
            if self._ndarray.shape[0] != nobs:
                msg = ""Array required to have {nobs} obs, has "" ""{act}"".format(
                    nobs=nobs, act=self._ndarray.shape[0]
                )
                raise ValueError(msg)",x is None and nobs is not None,x is None is not nobs
proxy.py,https://github.com/abhinavsingh/proxy.py/tree/master/proxy/http/handler.py,HttpProtocolHandler,_encryption_enabled$308,"def _encryption_enabled(self) -> bool:
        return self.flags.keyfile is not None and \
            self.flags.certfile is not None",self.flags.keyfile is not None and self.flags.certfile is not None,self.flags.keyfile is not None is not self.flags.certfile
edx-platform,https://github.com/edx/edx-platform/tree/master/scripts/xsslint/xsslint/linters.py,JavaScriptLinter,_is_jquery_argument_safe$512,"def _is_jquery_argument_safe(self, argument):
        """"""
        Check the argument sent to a jQuery DOM insertion function (e.g.
        append()) to check if it is safe.

        Safe arguments include:
        - the argument can end with "".el"", "".$el"" (with no concatenation)
        - the argument can be a single variable ending in ""El"" or starting with
            ""$"". For example, ""testEl"" or ""$test"".
        - the argument can be a single string literal with no HTML tags
        - the argument can be a call to $() with the first argument a string
            literal with a single HTML tag.  For example, "".append($('<br/>'))""
            or "".append($('<br/>'))"".
        - the argument can be a call to HtmlUtils.xxx(html).toString()

        Arguments:
            argument: The argument sent to the jQuery function (e.g.
            append(argument)).

        Returns:
            True if the argument is safe, and False otherwise.

        """"""
        match_variable_name = re.search(""[_$a-zA-Z]+[_$a-zA-Z0-9]*"", argument)
        if match_variable_name is not None and match_variable_name.group() == argument:
            if argument.endswith('El') or argument.startswith('$'):
                return True
        elif argument.startswith('""') or argument.startswith(""'""):
            # a single literal string with no HTML is ok
            # 1. it gets rid of false negatives for non-jquery calls (e.g. graph.append(""g""))
            # 2. JQuery will treat this as a plain text string and will escape any & if needed.
            string = ParseString(argument, 0, len(argument))
            if string.string == argument and ""<"" not in argument:
                return True
        elif argument.startswith('$('):
            # match on JQuery calls with single string and single HTML tag
            # Examples:
            #    $(""<span>"")
            #    $(""<div/>"")
            #    $(""<div/>"", {...})
            match = re.search(r""""""\$\(\s*['""]<[a-zA-Z0-9]+\s*[/]?>['""]\s*[,)]"""""", argument)
            if match is not None:
                return True
        elif self._is_jquery_argument_safe_html_utils_call(argument):
            return True
        # check rules that shouldn't use concatenation
        elif ""+"" not in argument:
            if argument.endswith('.el') or argument.endswith('.$el'):
                return True
        return False",string.string == argument and '<' not in argument,'<' not in argument == string.string
tvm,https://github.com/apache/tvm/tree/master/tests/python/contrib/test_bnns/test_dense.py,,_get_expected_codegen$62,"def _get_expected_codegen(shape, weight_shape, units, dtype, has_bias=False, has_gelu=False):
    output_shape = (shape[0], units)
    name = ""nn.dense""
    if has_bias is True:
        name = ""bnns.dense_bias""
    if has_bias is True and has_gelu is True:
        name = ""bnns.dense_bias_gelu""

    node = {
        ""op"": ""kernel"",
        ""name"": name,
        ""inputs"": [],
        ""attrs"": {
            ""num_outputs"": ""1"",
            ""out_dtype"": [[""float32""]],
            ""shape"": [[list(output_shape)]],
            ""dtype"": [[dtype]],
            ""units"": [[str(units)]],
        },
    }

    inputs = [
        {""op"": ""input"", ""name"": """", ""attrs"": {""shape"": [[list(shape)]], ""dtype"": [[str(dtype)]]}},
        {
            ""op"": ""const"",
            ""name"": """",
            ""attrs"": {""shape"": [[list(weight_shape)]], ""dtype"": [[str(dtype)]]},
        },
    ]

    if has_bias:
        inputs.append(
            {
                ""op"": ""const"",
                ""name"": """",
                ""attrs"": {""shape"": [[[weight_shape[0]]]], ""dtype"": [[""float32""]]},
            }
        )

    input_idx = 0
    for _ in range(len(inputs)):
        node[""inputs""].append([input_idx, 0, 0])
        input_idx += 1
    node[""attrs""][""num_inputs""] = str(len(inputs))
    inputs.append(node)
    return inputs",has_bias is True and has_gelu is True,has_bias is True is has_gelu
mmaction2,https://github.com/open-mmlab/mmaction2/tree/master/demo/webcam_demo_spatiotemporal_det.py,ClipHelper,__init__$340,"def __init__(self,
                 config,
                 display_height=0,
                 display_width=0,
                 input_video=0,
                 predict_stepsize=40,
                 output_fps=25,
                 clip_vis_length=8,
                 out_filename=None,
                 show=True,
                 stdet_input_shortside=256):
        # stdet sampling strategy
        val_pipeline = config.data.val.pipeline
        sampler = [x for x in val_pipeline
                   if x['type'] == 'SampleAVAFrames'][0]
        clip_len, frame_interval = sampler['clip_len'], sampler[
            'frame_interval']
        self.window_size = clip_len * frame_interval

        # asserts
        assert (out_filename or show), \
            'out_filename and show cannot both be None'
        assert clip_len % 2 == 0, 'We would like to have an even clip_len'
        assert clip_vis_length <= predict_stepsize
        assert 0 < predict_stepsize <= self.window_size

        # source params
        try:
            self.cap = cv2.VideoCapture(int(input_video))
            self.webcam = True
        except ValueError:
            self.cap = cv2.VideoCapture(input_video)
            self.webcam = False
        assert self.cap.isOpened()

        # stdet input preprocessing params
        h = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        w = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.stdet_input_size = mmcv.rescale_size(
            (w, h), (stdet_input_shortside, np.Inf))
        img_norm_cfg = config['img_norm_cfg']
        if 'to_rgb' not in img_norm_cfg and 'to_bgr' in img_norm_cfg:
            to_bgr = img_norm_cfg.pop('to_bgr')
            img_norm_cfg['to_rgb'] = to_bgr
        img_norm_cfg['mean'] = np.array(img_norm_cfg['mean'])
        img_norm_cfg['std'] = np.array(img_norm_cfg['std'])
        self.img_norm_cfg = img_norm_cfg

        # task init params
        self.clip_vis_length = clip_vis_length
        self.predict_stepsize = predict_stepsize
        self.buffer_size = self.window_size - self.predict_stepsize
        frame_start = self.window_size // 2 - (clip_len // 2) * frame_interval
        self.frames_inds = [
            frame_start + frame_interval * i for i in range(clip_len)
        ]
        self.buffer = []
        self.processed_buffer = []

        # output/display params
        if display_height > 0 and display_width > 0:
            self.display_size = (display_width, display_height)
        elif display_height > 0 or display_width > 0:
            self.display_size = mmcv.rescale_size(
                (w, h), (np.Inf, max(display_height, display_width)))
        else:
            self.display_size = (w, h)
        self.ratio = tuple(
            n / o for n, o in zip(self.stdet_input_size, self.display_size))
        if output_fps <= 0:
            self.output_fps = int(self.cap.get(cv2.CAP_PROP_FPS))
        else:
            self.output_fps = output_fps
        self.show = show
        self.video_writer = None
        if out_filename is not None:
            self.video_writer = self.get_output_video_writer(out_filename)
        display_start_idx = self.window_size // 2 - self.predict_stepsize // 2
        self.display_inds = [
            display_start_idx + i for i in range(self.predict_stepsize)
        ]

        # display multi-theading params
        self.display_id = -1  # task.id for display queue
        self.display_queue = {}
        self.display_lock = threading.Lock()
        self.output_lock = threading.Lock()

        # read multi-theading params
        self.read_id = -1  # task.id for read queue
        self.read_id_lock = threading.Lock()
        self.read_queue = queue.Queue()
        self.read_lock = threading.Lock()
        self.not_end = True  # cap.read() flag

        # program state
        self.stopped = False

        atexit.register(self.clean)",display_height > 0 and display_width > 0,display_height > 0 < display_width
distributed,https://github.com/dask/distributed/tree/master/distributed/utils.py,,validate_key$855,"def validate_key(k):
    """"""Validate a key as received on a stream.""""""
    typ = type(k)
    if typ is not str and typ is not bytes:
        raise TypeError(f""Unexpected key type {typ} (value: {k!r})"")",typ is not str and typ is not bytes,str is not typ is not bytes
pygorithm,https://github.com/OmkarPathak/pygorithm/tree/master/pygorithm/data_structures/heap.py,Heap,favorite$74,"def favorite(self, parent):
        """"""
        Determines which child has the highest priority by 3 cases
        """"""
        left = self.left_child_idx(parent)
        right = self.right_child_idx(parent)

        # case 1: both nodes exist
        if left <= self.rear and right <= self.rear:
            if self.queue[left] <= self.queue[right]:
                return left
            else:
                return right
        # case 2: only left exists
        elif left <= self.rear:
            return left
        # case 3: no children (if left doesn't exist, neither can the right)
        else:
            return None",left <= self.rear and right <= self.rear,left <= self.rear >= right
jellylanguage,https://github.com/DennisMitchell/jellylanguage/tree/master/jelly/interpreter.py,,div$112,"def div(dividend, divisor, floor = False):
	if divisor == 0:
		return dividend * inf
	if divisor == inf:
		return 0
	if floor or (type(dividend) == int and type(divisor) == int and not dividend % divisor):
		return int(dividend // divisor)
	return dividend / divisor",type(dividend) == int and type(divisor) == int and (not dividend % divisor),type(dividend) == int == type(divisor) and (not dividend % divisor)
minimalRL,https://github.com/seungeunrho/minimalRL/tree/master//sac.py,,main$131,"def main():
    env = gym.make('Pendulum-v0')
    memory = ReplayBuffer()
    q1, q2, q1_target, q2_target = QNet(lr_q), QNet(lr_q), QNet(lr_q), QNet(lr_q)
    pi = PolicyNet(lr_pi)

    q1_target.load_state_dict(q1.state_dict())
    q2_target.load_state_dict(q2.state_dict())

    score = 0.0
    print_interval = 20

    for n_epi in range(10000):
        s = env.reset()
        done = False

        while not done:
            a, log_prob= pi(torch.from_numpy(s).float())
            s_prime, r, done, info = env.step([2.0*a.item()])
            memory.put((s, a.item(), r/10.0, s_prime, done))
            score +=r
            s = s_prime
                
        if memory.size()>1000:
            for i in range(20):
                mini_batch = memory.sample(batch_size)
                td_target = calc_target(pi, q1_target, q2_target, mini_batch)
                q1.train_net(td_target, mini_batch)
                q2.train_net(td_target, mini_batch)
                entropy = pi.train_net(q1, q2, mini_batch)
                q1.soft_update(q1_target)
                q2.soft_update(q2_target)
                
        if n_epi%print_interval==0 and n_epi!=0:
            print(""# of episode :{}, avg score : {:.1f} alpha:{:.4f}"".format(n_epi, score/print_interval, pi.log_alpha.exp()))
            score = 0.0

    env.close()",n_epi % print_interval == 0 and n_epi != 0,n_epi % print_interval == 0 != n_epi
ansible,https://github.com/ansible/ansible/tree/master/lib/ansible/module_utils/basic.py,AnsibleModule,set_context_if_different$751,"def set_context_if_different(self, path, context, changed, diff=None):

        if not self.selinux_enabled():
            return changed

        if self.check_file_absent_if_check_mode(path):
            return True

        cur_context = self.selinux_context(path)
        new_context = list(cur_context)
        # Iterate over the current context instead of the
        # argument context, which may have selevel.

        (is_special_se, sp_context) = self.is_special_selinux_path(path)
        if is_special_se:
            new_context = sp_context
        else:
            for i in range(len(cur_context)):
                if len(context) > i:
                    if context[i] is not None and context[i] != cur_context[i]:
                        new_context[i] = context[i]
                    elif context[i] is None:
                        new_context[i] = cur_context[i]

        if cur_context != new_context:
            if diff is not None:
                if 'before' not in diff:
                    diff['before'] = {}
                diff['before']['secontext'] = cur_context
                if 'after' not in diff:
                    diff['after'] = {}
                diff['after']['secontext'] = new_context

            try:
                if self.check_mode:
                    return True
                rc = selinux.lsetfilecon(to_native(path), ':'.join(new_context))
            except OSError as e:
                self.fail_json(path=path, msg='invalid selinux context: %s' % to_native(e),
                               new_context=new_context, cur_context=cur_context, input_was=context)
            if rc != 0:
                self.fail_json(path=path, msg='set selinux context failed')
            changed = True
        return changed",context[i] is not None and context[i] != cur_context[i],None is not context[i] != cur_context[i]
simpletransformers,https://github.com/ThilinaRajapakse/simpletransformers/tree/master/simpletransformers/retrieval/retrieval_model.py,RetrievalModel,train$343,"def train(
        self,
        train_dataset,
        output_dir,
        show_running_loss=True,
        eval_data=None,
        additional_eval_passages=None,
        verbose=True,
        **kwargs,
    ):
        """"""
        Trains the model on train_dataset.

        Utility function to be used by the train_model() method. Not intended to be used directly.
        """"""

        context_model = self.context_encoder
        query_model = self.query_encoder
        args = self.args

        tb_writer = SummaryWriter(logdir=args.tensorboard_dir)
        train_sampler = RandomSampler(train_dataset)
        train_dataloader = DataLoader(
            train_dataset,
            sampler=train_sampler,
            batch_size=args.train_batch_size,
            num_workers=self.args.dataloader_num_workers,
        )

        if args.max_steps > 0:
            t_total = args.max_steps
            args.num_train_epochs = (
                args.max_steps
                // (len(train_dataloader) // args.gradient_accumulation_steps)
                + 1
            )
        else:
            t_total = (
                len(train_dataloader)
                // args.gradient_accumulation_steps
                * args.num_train_epochs
            )

        optimizer_grouped_parameters = self.get_optimizer_parameters(
            context_model, query_model, args
        )

        warmup_steps = math.ceil(t_total * args.warmup_ratio)
        args.warmup_steps = (
            warmup_steps if args.warmup_steps == 0 else args.warmup_steps
        )

        if args.optimizer == ""AdamW"":
            optimizer = AdamW(
                optimizer_grouped_parameters,
                lr=args.learning_rate,
                eps=args.adam_epsilon,
                betas=args.adam_betas,
            )
        elif args.optimizer == ""Adafactor"":
            optimizer = Adafactor(
                optimizer_grouped_parameters,
                lr=args.learning_rate,
                eps=args.adafactor_eps,
                clip_threshold=args.adafactor_clip_threshold,
                decay_rate=args.adafactor_decay_rate,
                beta1=args.adafactor_beta1,
                weight_decay=args.weight_decay,
                scale_parameter=args.adafactor_scale_parameter,
                relative_step=args.adafactor_relative_step,
                warmup_init=args.adafactor_warmup_init,
            )
        else:
            raise ValueError(
                ""{} is not a valid optimizer class. Please use one of ('AdamW', 'Adafactor') instead."".format(
                    args.optimizer
                )
            )

        scheduler = self.get_scheduler(optimizer, args, t_total)

        criterion = torch.nn.NLLLoss(reduction=""mean"")

        if (
            args.model_name
            and os.path.isfile(os.path.join(args.model_name, ""optimizer.pt""))
            and os.path.isfile(os.path.join(args.model_name, ""scheduler.pt""))
        ):
            # Load in optimizer and scheduler states
            optimizer.load_state_dict(
                torch.load(os.path.join(args.model_name, ""optimizer.pt""))
            )
            scheduler.load_state_dict(
                torch.load(os.path.join(args.model_name, ""scheduler.pt""))
            )

        if args.n_gpu > 1:
            context_model = torch.nn.DataParallel(context_model)
            query_model = torch.nn.DataParallel(query_model)

        logger.info("" Training started"")

        global_step = 0
        training_progress_scores = None
        tr_loss, logging_loss = 0.0, 0.0
        context_model.zero_grad()
        query_model.zero_grad()
        train_iterator = trange(
            int(args.num_train_epochs), desc=""Epoch"", disable=args.silent, mininterval=0
        )
        epoch_number = 0
        best_eval_metric = None
        early_stopping_counter = 0
        steps_trained_in_current_epoch = 0
        epochs_trained = 0

        if args.model_name and os.path.exists(args.model_name):
            try:
                # set global_step to gobal_step of last saved checkpoint from model path
                checkpoint_suffix = args.model_name.split(""/"")[-1].split(""-"")
                if len(checkpoint_suffix) > 2:
                    checkpoint_suffix = checkpoint_suffix[1]
                else:
                    checkpoint_suffix = checkpoint_suffix[-1]
                global_step = int(checkpoint_suffix)
                epochs_trained = global_step // (
                    len(train_dataloader) // args.gradient_accumulation_steps
                )
                steps_trained_in_current_epoch = global_step % (
                    len(train_dataloader) // args.gradient_accumulation_steps
                )

                logger.info(
                    ""   Continuing training from checkpoint, will skip to saved global_step""
                )
                logger.info(""   Continuing training from epoch %d"", epochs_trained)
                logger.info(""   Continuing training from global step %d"", global_step)
                logger.info(
                    ""   Will skip the first %d steps in the current epoch"",
                    steps_trained_in_current_epoch,
                )
            except ValueError:
                logger.info(""   Starting fine-tuning."")

        if args.evaluate_during_training:
            training_progress_scores = self._create_training_progress_scores(**kwargs)

        if args.wandb_project:
            wandb.init(
                project=args.wandb_project,
                config={**asdict(args)},
                **args.wandb_kwargs,
            )
            wandb.run._label(repo=""simpletransformers"")
            wandb.watch(context_model)
            wandb.watch(query_model)

        if args.fp16:
            from torch.cuda import amp

            scaler = amp.GradScaler()

        for current_epoch in train_iterator:
            if args.train_context_encoder:
                context_model.train()
            else:
                context_model.eval()
            if args.train_query_encoder:
                query_model.train()
            else:
                query_model.eval()
            if epochs_trained > 0:
                epochs_trained -= 1
                continue
            train_iterator.set_description(
                f""Epoch {epoch_number + 1} of {args.num_train_epochs}""
            )
            batch_iterator = tqdm(
                train_dataloader,
                desc=f""Running Epoch {epoch_number} of {args.num_train_epochs}"",
                disable=args.silent,
                mininterval=0,
            )
            for step, batch in enumerate(batch_iterator):
                if steps_trained_in_current_epoch > 0:
                    steps_trained_in_current_epoch -= 1
                    continue
                # batch = tuple(t.to(device) for t in batch)

                context_inputs, query_inputs, labels = self._get_inputs_dict(batch)
                if args.fp16:
                    with amp.autocast():
                        loss, *_, correct_count = self._calculate_loss(
                            context_model,
                            query_model,
                            context_inputs,
                            query_inputs,
                            labels,
                            criterion,
                        )
                else:
                    loss, *_, correct_count = self._calculate_loss(
                        context_model,
                        query_model,
                        context_inputs,
                        query_inputs,
                        labels,
                        criterion,
                    )

                if args.n_gpu > 1:
                    loss = loss.mean()

                current_loss = loss.item()

                if show_running_loss:
                    batch_iterator.set_description(
                        f""Epochs {epoch_number}/{args.num_train_epochs}. Running Loss: {current_loss:9.4f} Correct count: {correct_count}""
                    )

                if args.gradient_accumulation_steps > 1:
                    loss = loss / args.gradient_accumulation_steps

                if args.fp16:
                    scaler.scale(loss).backward()
                else:
                    loss.backward()

                tr_loss += loss.item()
                if (step + 1) % args.gradient_accumulation_steps == 0:
                    if args.fp16:
                        scaler.unscale_(optimizer)
                    if args.optimizer == ""AdamW"":
                        torch.nn.utils.clip_grad_norm_(
                            context_model.parameters(), args.max_grad_norm
                        )
                        torch.nn.utils.clip_grad_norm_(
                            query_model.parameters(), args.max_grad_norm
                        )

                    if args.fp16:
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        optimizer.step()
                    scheduler.step()  # Update learning rate schedule
                    context_model.zero_grad()
                    query_model.zero_grad()
                    global_step += 1

                    if args.logging_steps > 0 and global_step % args.logging_steps == 0:
                        # Log metrics
                        tb_writer.add_scalar(
                            ""lr"", scheduler.get_last_lr()[0], global_step
                        )
                        tb_writer.add_scalar(
                            ""loss"",
                            (tr_loss - logging_loss) / args.logging_steps,
                            global_step,
                        )
                        logging_loss = tr_loss
                        if args.wandb_project or self.is_sweeping:
                            wandb.log(
                                {
                                    ""Training loss"": current_loss,
                                    ""lr"": scheduler.get_last_lr()[0],
                                    ""global_step"": global_step,
                                }
                            )

                    if args.save_steps > 0 and global_step % args.save_steps == 0:
                        # Save model checkpoint
                        output_dir_current = os.path.join(
                            output_dir, ""checkpoint-{}"".format(global_step)
                        )

                        self.save_model(
                            output_dir_current,
                            optimizer,
                            scheduler,
                            context_model=context_model,
                            query_model=query_model,
                        )

                    if args.evaluate_during_training and (
                        args.evaluate_during_training_steps > 0
                        and global_step % args.evaluate_during_training_steps == 0
                    ):
                        # Only evaluate when single GPU otherwise metrics may not average well
                        results, *_ = self.eval_model(
                            eval_data,
                            additional_passages=additional_eval_passages,
                            verbose=verbose and args.evaluate_during_training_verbose,
                            silent=args.evaluate_during_training_silent,
                            **kwargs,
                        )
                        for key, value in results.items():
                            try:
                                tb_writer.add_scalar(
                                    ""eval_{}"".format(key), value, global_step
                                )
                            except (NotImplementedError, AssertionError):
                                pass

                        output_dir_current = os.path.join(
                            output_dir, ""checkpoint-{}"".format(global_step)
                        )

                        if args.save_eval_checkpoints:
                            self.save_model(
                                output_dir_current,
                                optimizer,
                                scheduler,
                                context_model=context_model,
                                query_model=query_model,
                                results=results,
                            )

                        training_progress_scores[""global_step""].append(global_step)
                        training_progress_scores[""train_loss""].append(current_loss)
                        for key in results:
                            training_progress_scores[key].append(results[key])
                        report = pd.DataFrame(training_progress_scores)
                        report.to_csv(
                            os.path.join(
                                args.output_dir, ""training_progress_scores.csv""
                            ),
                            index=False,
                        )

                        if args.wandb_project or self.is_sweeping:
                            wandb.log(self._get_last_metrics(training_progress_scores))

                        if not best_eval_metric:
                            best_eval_metric = results[args.early_stopping_metric]
                            if args.save_best_model:
                                self.save_model(
                                    args.best_model_dir,
                                    optimizer,
                                    scheduler,
                                    context_model=context_model,
                                    query_model=query_model,
                                    results=results,
                                )
                        if best_eval_metric and args.early_stopping_metric_minimize:
                            if (
                                results[args.early_stopping_metric] - best_eval_metric
                                < args.early_stopping_delta
                            ):
                                best_eval_metric = results[args.early_stopping_metric]
                                if args.save_best_model:
                                    self.save_model(
                                        args.best_model_dir,
                                        optimizer,
                                        scheduler,
                                        context_model=context_model,
                                        query_model=query_model,
                                        results=results,
                                    )
                                early_stopping_counter = 0
                            else:
                                if args.use_early_stopping:
                                    if (
                                        early_stopping_counter
                                        < args.early_stopping_patience
                                    ):
                                        early_stopping_counter += 1
                                        if verbose:
                                            logger.info(
                                                f"" No improvement in {args.early_stopping_metric}""
                                            )
                                            logger.info(
                                                f"" Current step: {early_stopping_counter}""
                                            )
                                            logger.info(
                                                f"" Early stopping patience: {args.early_stopping_patience}""
                                            )
                                    else:
                                        if verbose:
                                            logger.info(
                                                f"" Patience of {args.early_stopping_patience} steps reached""
                                            )
                                            logger.info("" Training terminated."")
                                            train_iterator.close()
                                        return (
                                            global_step,
                                            tr_loss / global_step
                                            if not self.args.evaluate_during_training
                                            else training_progress_scores,
                                        )
                        else:
                            if (
                                results[args.early_stopping_metric] - best_eval_metric
                                > args.early_stopping_delta
                            ):
                                best_eval_metric = results[args.early_stopping_metric]
                                if args.save_best_model:
                                    self.save_model(
                                        args.best_model_dir,
                                        optimizer,
                                        scheduler,
                                        context_model=context_model,
                                        query_model=query_model,
                                        results=results,
                                    )
                                early_stopping_counter = 0
                            else:
                                if args.use_early_stopping:
                                    if (
                                        early_stopping_counter
                                        < args.early_stopping_patience
                                    ):
                                        early_stopping_counter += 1
                                        if verbose:
                                            logger.info(
                                                f"" No improvement in {args.early_stopping_metric}""
                                            )
                                            logger.info(
                                                f"" Current step: {early_stopping_counter}""
                                            )
                                            logger.info(
                                                f"" Early stopping patience: {args.early_stopping_patience}""
                                            )
                                    else:
                                        if verbose:
                                            logger.info(
                                                f"" Patience of {args.early_stopping_patience} steps reached""
                                            )
                                            logger.info("" Training terminated."")
                                            train_iterator.close()
                                        return (
                                            global_step,
                                            tr_loss / global_step
                                            if not self.args.evaluate_during_training
                                            else training_progress_scores,
                                        )
                        context_model.train()
                        query_model.train()

            epoch_number += 1
            output_dir_current = os.path.join(
                output_dir, ""checkpoint-{}-epoch-{}"".format(global_step, epoch_number)
            )

            if args.save_model_every_epoch or args.evaluate_during_training:
                os.makedirs(output_dir_current, exist_ok=True)

            if args.save_model_every_epoch:
                self.save_model(
                    output_dir_current,
                    optimizer,
                    scheduler,
                    context_model=context_model,
                    query_model=query_model,
                )

            if args.evaluate_during_training and args.evaluate_each_epoch:
                results, *_ = self.eval_model(
                    eval_data,
                    additional_passages=additional_eval_passages,
                    verbose=verbose and args.evaluate_during_training_verbose,
                    silent=args.evaluate_during_training_silent,
                    **kwargs,
                )

                if args.save_eval_checkpoints:
                    self.save_model(
                        output_dir_current, optimizer, scheduler, results=results
                    )

                training_progress_scores[""global_step""].append(global_step)
                training_progress_scores[""train_loss""].append(current_loss)
                for key in results:
                    training_progress_scores[key].append(results[key])
                report = pd.DataFrame(training_progress_scores)
                report.to_csv(
                    os.path.join(args.output_dir, ""training_progress_scores.csv""),
                    index=False,
                )

                if args.wandb_project or self.is_sweeping:
                    wandb.log(self._get_last_metrics(training_progress_scores))

                if not best_eval_metric:
                    best_eval_metric = results[args.early_stopping_metric]
                    if args.save_best_model:
                        self.save_model(
                            args.best_model_dir,
                            optimizer,
                            scheduler,
                            context_model=context_model,
                            query_model=query_model,
                            results=results,
                        )
                if best_eval_metric and args.early_stopping_metric_minimize:
                    if (
                        results[args.early_stopping_metric] - best_eval_metric
                        < args.early_stopping_delta
                    ):
                        best_eval_metric = results[args.early_stopping_metric]
                        if args.save_best_model:
                            self.save_model(
                                args.best_model_dir,
                                optimizer,
                                scheduler,
                                context_model=context_model,
                                query_model=query_model,
                                results=results,
                            )
                        early_stopping_counter = 0
                    else:
                        if (
                            args.use_early_stopping
                            and args.early_stopping_consider_epochs
                        ):
                            if early_stopping_counter < args.early_stopping_patience:
                                early_stopping_counter += 1
                                if verbose:
                                    logger.info(
                                        f"" No improvement in {args.early_stopping_metric}""
                                    )
                                    logger.info(
                                        f"" Current step: {early_stopping_counter}""
                                    )
                                    logger.info(
                                        f"" Early stopping patience: {args.early_stopping_patience}""
                                    )
                            else:
                                if verbose:
                                    logger.info(
                                        f"" Patience of {args.early_stopping_patience} steps reached""
                                    )
                                    logger.info("" Training terminated."")
                                    train_iterator.close()
                                return (
                                    global_step,
                                    tr_loss / global_step
                                    if not self.args.evaluate_during_training
                                    else training_progress_scores,
                                )
                else:
                    if (
                        results[args.early_stopping_metric] - best_eval_metric
                        > args.early_stopping_delta
                    ):
                        best_eval_metric = results[args.early_stopping_metric]
                        if args.save_best_model:
                            self.save_model(
                                args.best_model_dir,
                                optimizer,
                                scheduler,
                                context_model=context_model,
                                query_model=query_model,
                                results=results,
                            )
                        early_stopping_counter = 0
                    else:
                        if (
                            args.use_early_stopping
                            and args.early_stopping_consider_epochs
                        ):
                            if early_stopping_counter < args.early_stopping_patience:
                                early_stopping_counter += 1
                                if verbose:
                                    logger.info(
                                        f"" No improvement in {args.early_stopping_metric}""
                                    )
                                    logger.info(
                                        f"" Current step: {early_stopping_counter}""
                                    )
                                    logger.info(
                                        f"" Early stopping patience: {args.early_stopping_patience}""
                                    )
                            else:
                                if verbose:
                                    logger.info(
                                        f"" Patience of {args.early_stopping_patience} steps reached""
                                    )
                                    logger.info("" Training terminated."")
                                    train_iterator.close()
                                return (
                                    global_step,
                                    tr_loss / global_step
                                    if not self.args.evaluate_during_training
                                    else training_progress_scores,
                                )

        return (
            global_step,
            tr_loss / global_step
            if not self.args.evaluate_during_training
            else training_progress_scores,
        )",args.logging_steps > 0 and global_step % args.logging_steps == 0,args.logging_steps > 0 == global_step % args.logging_steps
simpletransformers,https://github.com/ThilinaRajapakse/simpletransformers/tree/master/simpletransformers/retrieval/retrieval_model.py,RetrievalModel,train$343,"def train(
        self,
        train_dataset,
        output_dir,
        show_running_loss=True,
        eval_data=None,
        additional_eval_passages=None,
        verbose=True,
        **kwargs,
    ):
        """"""
        Trains the model on train_dataset.

        Utility function to be used by the train_model() method. Not intended to be used directly.
        """"""

        context_model = self.context_encoder
        query_model = self.query_encoder
        args = self.args

        tb_writer = SummaryWriter(logdir=args.tensorboard_dir)
        train_sampler = RandomSampler(train_dataset)
        train_dataloader = DataLoader(
            train_dataset,
            sampler=train_sampler,
            batch_size=args.train_batch_size,
            num_workers=self.args.dataloader_num_workers,
        )

        if args.max_steps > 0:
            t_total = args.max_steps
            args.num_train_epochs = (
                args.max_steps
                // (len(train_dataloader) // args.gradient_accumulation_steps)
                + 1
            )
        else:
            t_total = (
                len(train_dataloader)
                // args.gradient_accumulation_steps
                * args.num_train_epochs
            )

        optimizer_grouped_parameters = self.get_optimizer_parameters(
            context_model, query_model, args
        )

        warmup_steps = math.ceil(t_total * args.warmup_ratio)
        args.warmup_steps = (
            warmup_steps if args.warmup_steps == 0 else args.warmup_steps
        )

        if args.optimizer == ""AdamW"":
            optimizer = AdamW(
                optimizer_grouped_parameters,
                lr=args.learning_rate,
                eps=args.adam_epsilon,
                betas=args.adam_betas,
            )
        elif args.optimizer == ""Adafactor"":
            optimizer = Adafactor(
                optimizer_grouped_parameters,
                lr=args.learning_rate,
                eps=args.adafactor_eps,
                clip_threshold=args.adafactor_clip_threshold,
                decay_rate=args.adafactor_decay_rate,
                beta1=args.adafactor_beta1,
                weight_decay=args.weight_decay,
                scale_parameter=args.adafactor_scale_parameter,
                relative_step=args.adafactor_relative_step,
                warmup_init=args.adafactor_warmup_init,
            )
        else:
            raise ValueError(
                ""{} is not a valid optimizer class. Please use one of ('AdamW', 'Adafactor') instead."".format(
                    args.optimizer
                )
            )

        scheduler = self.get_scheduler(optimizer, args, t_total)

        criterion = torch.nn.NLLLoss(reduction=""mean"")

        if (
            args.model_name
            and os.path.isfile(os.path.join(args.model_name, ""optimizer.pt""))
            and os.path.isfile(os.path.join(args.model_name, ""scheduler.pt""))
        ):
            # Load in optimizer and scheduler states
            optimizer.load_state_dict(
                torch.load(os.path.join(args.model_name, ""optimizer.pt""))
            )
            scheduler.load_state_dict(
                torch.load(os.path.join(args.model_name, ""scheduler.pt""))
            )

        if args.n_gpu > 1:
            context_model = torch.nn.DataParallel(context_model)
            query_model = torch.nn.DataParallel(query_model)

        logger.info("" Training started"")

        global_step = 0
        training_progress_scores = None
        tr_loss, logging_loss = 0.0, 0.0
        context_model.zero_grad()
        query_model.zero_grad()
        train_iterator = trange(
            int(args.num_train_epochs), desc=""Epoch"", disable=args.silent, mininterval=0
        )
        epoch_number = 0
        best_eval_metric = None
        early_stopping_counter = 0
        steps_trained_in_current_epoch = 0
        epochs_trained = 0

        if args.model_name and os.path.exists(args.model_name):
            try:
                # set global_step to gobal_step of last saved checkpoint from model path
                checkpoint_suffix = args.model_name.split(""/"")[-1].split(""-"")
                if len(checkpoint_suffix) > 2:
                    checkpoint_suffix = checkpoint_suffix[1]
                else:
                    checkpoint_suffix = checkpoint_suffix[-1]
                global_step = int(checkpoint_suffix)
                epochs_trained = global_step // (
                    len(train_dataloader) // args.gradient_accumulation_steps
                )
                steps_trained_in_current_epoch = global_step % (
                    len(train_dataloader) // args.gradient_accumulation_steps
                )

                logger.info(
                    ""   Continuing training from checkpoint, will skip to saved global_step""
                )
                logger.info(""   Continuing training from epoch %d"", epochs_trained)
                logger.info(""   Continuing training from global step %d"", global_step)
                logger.info(
                    ""   Will skip the first %d steps in the current epoch"",
                    steps_trained_in_current_epoch,
                )
            except ValueError:
                logger.info(""   Starting fine-tuning."")

        if args.evaluate_during_training:
            training_progress_scores = self._create_training_progress_scores(**kwargs)

        if args.wandb_project:
            wandb.init(
                project=args.wandb_project,
                config={**asdict(args)},
                **args.wandb_kwargs,
            )
            wandb.run._label(repo=""simpletransformers"")
            wandb.watch(context_model)
            wandb.watch(query_model)

        if args.fp16:
            from torch.cuda import amp

            scaler = amp.GradScaler()

        for current_epoch in train_iterator:
            if args.train_context_encoder:
                context_model.train()
            else:
                context_model.eval()
            if args.train_query_encoder:
                query_model.train()
            else:
                query_model.eval()
            if epochs_trained > 0:
                epochs_trained -= 1
                continue
            train_iterator.set_description(
                f""Epoch {epoch_number + 1} of {args.num_train_epochs}""
            )
            batch_iterator = tqdm(
                train_dataloader,
                desc=f""Running Epoch {epoch_number} of {args.num_train_epochs}"",
                disable=args.silent,
                mininterval=0,
            )
            for step, batch in enumerate(batch_iterator):
                if steps_trained_in_current_epoch > 0:
                    steps_trained_in_current_epoch -= 1
                    continue
                # batch = tuple(t.to(device) for t in batch)

                context_inputs, query_inputs, labels = self._get_inputs_dict(batch)
                if args.fp16:
                    with amp.autocast():
                        loss, *_, correct_count = self._calculate_loss(
                            context_model,
                            query_model,
                            context_inputs,
                            query_inputs,
                            labels,
                            criterion,
                        )
                else:
                    loss, *_, correct_count = self._calculate_loss(
                        context_model,
                        query_model,
                        context_inputs,
                        query_inputs,
                        labels,
                        criterion,
                    )

                if args.n_gpu > 1:
                    loss = loss.mean()

                current_loss = loss.item()

                if show_running_loss:
                    batch_iterator.set_description(
                        f""Epochs {epoch_number}/{args.num_train_epochs}. Running Loss: {current_loss:9.4f} Correct count: {correct_count}""
                    )

                if args.gradient_accumulation_steps > 1:
                    loss = loss / args.gradient_accumulation_steps

                if args.fp16:
                    scaler.scale(loss).backward()
                else:
                    loss.backward()

                tr_loss += loss.item()
                if (step + 1) % args.gradient_accumulation_steps == 0:
                    if args.fp16:
                        scaler.unscale_(optimizer)
                    if args.optimizer == ""AdamW"":
                        torch.nn.utils.clip_grad_norm_(
                            context_model.parameters(), args.max_grad_norm
                        )
                        torch.nn.utils.clip_grad_norm_(
                            query_model.parameters(), args.max_grad_norm
                        )

                    if args.fp16:
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        optimizer.step()
                    scheduler.step()  # Update learning rate schedule
                    context_model.zero_grad()
                    query_model.zero_grad()
                    global_step += 1

                    if args.logging_steps > 0 and global_step % args.logging_steps == 0:
                        # Log metrics
                        tb_writer.add_scalar(
                            ""lr"", scheduler.get_last_lr()[0], global_step
                        )
                        tb_writer.add_scalar(
                            ""loss"",
                            (tr_loss - logging_loss) / args.logging_steps,
                            global_step,
                        )
                        logging_loss = tr_loss
                        if args.wandb_project or self.is_sweeping:
                            wandb.log(
                                {
                                    ""Training loss"": current_loss,
                                    ""lr"": scheduler.get_last_lr()[0],
                                    ""global_step"": global_step,
                                }
                            )

                    if args.save_steps > 0 and global_step % args.save_steps == 0:
                        # Save model checkpoint
                        output_dir_current = os.path.join(
                            output_dir, ""checkpoint-{}"".format(global_step)
                        )

                        self.save_model(
                            output_dir_current,
                            optimizer,
                            scheduler,
                            context_model=context_model,
                            query_model=query_model,
                        )

                    if args.evaluate_during_training and (
                        args.evaluate_during_training_steps > 0
                        and global_step % args.evaluate_during_training_steps == 0
                    ):
                        # Only evaluate when single GPU otherwise metrics may not average well
                        results, *_ = self.eval_model(
                            eval_data,
                            additional_passages=additional_eval_passages,
                            verbose=verbose and args.evaluate_during_training_verbose,
                            silent=args.evaluate_during_training_silent,
                            **kwargs,
                        )
                        for key, value in results.items():
                            try:
                                tb_writer.add_scalar(
                                    ""eval_{}"".format(key), value, global_step
                                )
                            except (NotImplementedError, AssertionError):
                                pass

                        output_dir_current = os.path.join(
                            output_dir, ""checkpoint-{}"".format(global_step)
                        )

                        if args.save_eval_checkpoints:
                            self.save_model(
                                output_dir_current,
                                optimizer,
                                scheduler,
                                context_model=context_model,
                                query_model=query_model,
                                results=results,
                            )

                        training_progress_scores[""global_step""].append(global_step)
                        training_progress_scores[""train_loss""].append(current_loss)
                        for key in results:
                            training_progress_scores[key].append(results[key])
                        report = pd.DataFrame(training_progress_scores)
                        report.to_csv(
                            os.path.join(
                                args.output_dir, ""training_progress_scores.csv""
                            ),
                            index=False,
                        )

                        if args.wandb_project or self.is_sweeping:
                            wandb.log(self._get_last_metrics(training_progress_scores))

                        if not best_eval_metric:
                            best_eval_metric = results[args.early_stopping_metric]
                            if args.save_best_model:
                                self.save_model(
                                    args.best_model_dir,
                                    optimizer,
                                    scheduler,
                                    context_model=context_model,
                                    query_model=query_model,
                                    results=results,
                                )
                        if best_eval_metric and args.early_stopping_metric_minimize:
                            if (
                                results[args.early_stopping_metric] - best_eval_metric
                                < args.early_stopping_delta
                            ):
                                best_eval_metric = results[args.early_stopping_metric]
                                if args.save_best_model:
                                    self.save_model(
                                        args.best_model_dir,
                                        optimizer,
                                        scheduler,
                                        context_model=context_model,
                                        query_model=query_model,
                                        results=results,
                                    )
                                early_stopping_counter = 0
                            else:
                                if args.use_early_stopping:
                                    if (
                                        early_stopping_counter
                                        < args.early_stopping_patience
                                    ):
                                        early_stopping_counter += 1
                                        if verbose:
                                            logger.info(
                                                f"" No improvement in {args.early_stopping_metric}""
                                            )
                                            logger.info(
                                                f"" Current step: {early_stopping_counter}""
                                            )
                                            logger.info(
                                                f"" Early stopping patience: {args.early_stopping_patience}""
                                            )
                                    else:
                                        if verbose:
                                            logger.info(
                                                f"" Patience of {args.early_stopping_patience} steps reached""
                                            )
                                            logger.info("" Training terminated."")
                                            train_iterator.close()
                                        return (
                                            global_step,
                                            tr_loss / global_step
                                            if not self.args.evaluate_during_training
                                            else training_progress_scores,
                                        )
                        else:
                            if (
                                results[args.early_stopping_metric] - best_eval_metric
                                > args.early_stopping_delta
                            ):
                                best_eval_metric = results[args.early_stopping_metric]
                                if args.save_best_model:
                                    self.save_model(
                                        args.best_model_dir,
                                        optimizer,
                                        scheduler,
                                        context_model=context_model,
                                        query_model=query_model,
                                        results=results,
                                    )
                                early_stopping_counter = 0
                            else:
                                if args.use_early_stopping:
                                    if (
                                        early_stopping_counter
                                        < args.early_stopping_patience
                                    ):
                                        early_stopping_counter += 1
                                        if verbose:
                                            logger.info(
                                                f"" No improvement in {args.early_stopping_metric}""
                                            )
                                            logger.info(
                                                f"" Current step: {early_stopping_counter}""
                                            )
                                            logger.info(
                                                f"" Early stopping patience: {args.early_stopping_patience}""
                                            )
                                    else:
                                        if verbose:
                                            logger.info(
                                                f"" Patience of {args.early_stopping_patience} steps reached""
                                            )
                                            logger.info("" Training terminated."")
                                            train_iterator.close()
                                        return (
                                            global_step,
                                            tr_loss / global_step
                                            if not self.args.evaluate_during_training
                                            else training_progress_scores,
                                        )
                        context_model.train()
                        query_model.train()

            epoch_number += 1
            output_dir_current = os.path.join(
                output_dir, ""checkpoint-{}-epoch-{}"".format(global_step, epoch_number)
            )

            if args.save_model_every_epoch or args.evaluate_during_training:
                os.makedirs(output_dir_current, exist_ok=True)

            if args.save_model_every_epoch:
                self.save_model(
                    output_dir_current,
                    optimizer,
                    scheduler,
                    context_model=context_model,
                    query_model=query_model,
                )

            if args.evaluate_during_training and args.evaluate_each_epoch:
                results, *_ = self.eval_model(
                    eval_data,
                    additional_passages=additional_eval_passages,
                    verbose=verbose and args.evaluate_during_training_verbose,
                    silent=args.evaluate_during_training_silent,
                    **kwargs,
                )

                if args.save_eval_checkpoints:
                    self.save_model(
                        output_dir_current, optimizer, scheduler, results=results
                    )

                training_progress_scores[""global_step""].append(global_step)
                training_progress_scores[""train_loss""].append(current_loss)
                for key in results:
                    training_progress_scores[key].append(results[key])
                report = pd.DataFrame(training_progress_scores)
                report.to_csv(
                    os.path.join(args.output_dir, ""training_progress_scores.csv""),
                    index=False,
                )

                if args.wandb_project or self.is_sweeping:
                    wandb.log(self._get_last_metrics(training_progress_scores))

                if not best_eval_metric:
                    best_eval_metric = results[args.early_stopping_metric]
                    if args.save_best_model:
                        self.save_model(
                            args.best_model_dir,
                            optimizer,
                            scheduler,
                            context_model=context_model,
                            query_model=query_model,
                            results=results,
                        )
                if best_eval_metric and args.early_stopping_metric_minimize:
                    if (
                        results[args.early_stopping_metric] - best_eval_metric
                        < args.early_stopping_delta
                    ):
                        best_eval_metric = results[args.early_stopping_metric]
                        if args.save_best_model:
                            self.save_model(
                                args.best_model_dir,
                                optimizer,
                                scheduler,
                                context_model=context_model,
                                query_model=query_model,
                                results=results,
                            )
                        early_stopping_counter = 0
                    else:
                        if (
                            args.use_early_stopping
                            and args.early_stopping_consider_epochs
                        ):
                            if early_stopping_counter < args.early_stopping_patience:
                                early_stopping_counter += 1
                                if verbose:
                                    logger.info(
                                        f"" No improvement in {args.early_stopping_metric}""
                                    )
                                    logger.info(
                                        f"" Current step: {early_stopping_counter}""
                                    )
                                    logger.info(
                                        f"" Early stopping patience: {args.early_stopping_patience}""
                                    )
                            else:
                                if verbose:
                                    logger.info(
                                        f"" Patience of {args.early_stopping_patience} steps reached""
                                    )
                                    logger.info("" Training terminated."")
                                    train_iterator.close()
                                return (
                                    global_step,
                                    tr_loss / global_step
                                    if not self.args.evaluate_during_training
                                    else training_progress_scores,
                                )
                else:
                    if (
                        results[args.early_stopping_metric] - best_eval_metric
                        > args.early_stopping_delta
                    ):
                        best_eval_metric = results[args.early_stopping_metric]
                        if args.save_best_model:
                            self.save_model(
                                args.best_model_dir,
                                optimizer,
                                scheduler,
                                context_model=context_model,
                                query_model=query_model,
                                results=results,
                            )
                        early_stopping_counter = 0
                    else:
                        if (
                            args.use_early_stopping
                            and args.early_stopping_consider_epochs
                        ):
                            if early_stopping_counter < args.early_stopping_patience:
                                early_stopping_counter += 1
                                if verbose:
                                    logger.info(
                                        f"" No improvement in {args.early_stopping_metric}""
                                    )
                                    logger.info(
                                        f"" Current step: {early_stopping_counter}""
                                    )
                                    logger.info(
                                        f"" Early stopping patience: {args.early_stopping_patience}""
                                    )
                            else:
                                if verbose:
                                    logger.info(
                                        f"" Patience of {args.early_stopping_patience} steps reached""
                                    )
                                    logger.info("" Training terminated."")
                                    train_iterator.close()
                                return (
                                    global_step,
                                    tr_loss / global_step
                                    if not self.args.evaluate_during_training
                                    else training_progress_scores,
                                )

        return (
            global_step,
            tr_loss / global_step
            if not self.args.evaluate_during_training
            else training_progress_scores,
        )",args.save_steps > 0 and global_step % args.save_steps == 0,args.save_steps > 0 == global_step % args.save_steps
simpletransformers,https://github.com/ThilinaRajapakse/simpletransformers/tree/master/simpletransformers/retrieval/retrieval_model.py,RetrievalModel,train$343,"def train(
        self,
        train_dataset,
        output_dir,
        show_running_loss=True,
        eval_data=None,
        additional_eval_passages=None,
        verbose=True,
        **kwargs,
    ):
        """"""
        Trains the model on train_dataset.

        Utility function to be used by the train_model() method. Not intended to be used directly.
        """"""

        context_model = self.context_encoder
        query_model = self.query_encoder
        args = self.args

        tb_writer = SummaryWriter(logdir=args.tensorboard_dir)
        train_sampler = RandomSampler(train_dataset)
        train_dataloader = DataLoader(
            train_dataset,
            sampler=train_sampler,
            batch_size=args.train_batch_size,
            num_workers=self.args.dataloader_num_workers,
        )

        if args.max_steps > 0:
            t_total = args.max_steps
            args.num_train_epochs = (
                args.max_steps
                // (len(train_dataloader) // args.gradient_accumulation_steps)
                + 1
            )
        else:
            t_total = (
                len(train_dataloader)
                // args.gradient_accumulation_steps
                * args.num_train_epochs
            )

        optimizer_grouped_parameters = self.get_optimizer_parameters(
            context_model, query_model, args
        )

        warmup_steps = math.ceil(t_total * args.warmup_ratio)
        args.warmup_steps = (
            warmup_steps if args.warmup_steps == 0 else args.warmup_steps
        )

        if args.optimizer == ""AdamW"":
            optimizer = AdamW(
                optimizer_grouped_parameters,
                lr=args.learning_rate,
                eps=args.adam_epsilon,
                betas=args.adam_betas,
            )
        elif args.optimizer == ""Adafactor"":
            optimizer = Adafactor(
                optimizer_grouped_parameters,
                lr=args.learning_rate,
                eps=args.adafactor_eps,
                clip_threshold=args.adafactor_clip_threshold,
                decay_rate=args.adafactor_decay_rate,
                beta1=args.adafactor_beta1,
                weight_decay=args.weight_decay,
                scale_parameter=args.adafactor_scale_parameter,
                relative_step=args.adafactor_relative_step,
                warmup_init=args.adafactor_warmup_init,
            )
        else:
            raise ValueError(
                ""{} is not a valid optimizer class. Please use one of ('AdamW', 'Adafactor') instead."".format(
                    args.optimizer
                )
            )

        scheduler = self.get_scheduler(optimizer, args, t_total)

        criterion = torch.nn.NLLLoss(reduction=""mean"")

        if (
            args.model_name
            and os.path.isfile(os.path.join(args.model_name, ""optimizer.pt""))
            and os.path.isfile(os.path.join(args.model_name, ""scheduler.pt""))
        ):
            # Load in optimizer and scheduler states
            optimizer.load_state_dict(
                torch.load(os.path.join(args.model_name, ""optimizer.pt""))
            )
            scheduler.load_state_dict(
                torch.load(os.path.join(args.model_name, ""scheduler.pt""))
            )

        if args.n_gpu > 1:
            context_model = torch.nn.DataParallel(context_model)
            query_model = torch.nn.DataParallel(query_model)

        logger.info("" Training started"")

        global_step = 0
        training_progress_scores = None
        tr_loss, logging_loss = 0.0, 0.0
        context_model.zero_grad()
        query_model.zero_grad()
        train_iterator = trange(
            int(args.num_train_epochs), desc=""Epoch"", disable=args.silent, mininterval=0
        )
        epoch_number = 0
        best_eval_metric = None
        early_stopping_counter = 0
        steps_trained_in_current_epoch = 0
        epochs_trained = 0

        if args.model_name and os.path.exists(args.model_name):
            try:
                # set global_step to gobal_step of last saved checkpoint from model path
                checkpoint_suffix = args.model_name.split(""/"")[-1].split(""-"")
                if len(checkpoint_suffix) > 2:
                    checkpoint_suffix = checkpoint_suffix[1]
                else:
                    checkpoint_suffix = checkpoint_suffix[-1]
                global_step = int(checkpoint_suffix)
                epochs_trained = global_step // (
                    len(train_dataloader) // args.gradient_accumulation_steps
                )
                steps_trained_in_current_epoch = global_step % (
                    len(train_dataloader) // args.gradient_accumulation_steps
                )

                logger.info(
                    ""   Continuing training from checkpoint, will skip to saved global_step""
                )
                logger.info(""   Continuing training from epoch %d"", epochs_trained)
                logger.info(""   Continuing training from global step %d"", global_step)
                logger.info(
                    ""   Will skip the first %d steps in the current epoch"",
                    steps_trained_in_current_epoch,
                )
            except ValueError:
                logger.info(""   Starting fine-tuning."")

        if args.evaluate_during_training:
            training_progress_scores = self._create_training_progress_scores(**kwargs)

        if args.wandb_project:
            wandb.init(
                project=args.wandb_project,
                config={**asdict(args)},
                **args.wandb_kwargs,
            )
            wandb.run._label(repo=""simpletransformers"")
            wandb.watch(context_model)
            wandb.watch(query_model)

        if args.fp16:
            from torch.cuda import amp

            scaler = amp.GradScaler()

        for current_epoch in train_iterator:
            if args.train_context_encoder:
                context_model.train()
            else:
                context_model.eval()
            if args.train_query_encoder:
                query_model.train()
            else:
                query_model.eval()
            if epochs_trained > 0:
                epochs_trained -= 1
                continue
            train_iterator.set_description(
                f""Epoch {epoch_number + 1} of {args.num_train_epochs}""
            )
            batch_iterator = tqdm(
                train_dataloader,
                desc=f""Running Epoch {epoch_number} of {args.num_train_epochs}"",
                disable=args.silent,
                mininterval=0,
            )
            for step, batch in enumerate(batch_iterator):
                if steps_trained_in_current_epoch > 0:
                    steps_trained_in_current_epoch -= 1
                    continue
                # batch = tuple(t.to(device) for t in batch)

                context_inputs, query_inputs, labels = self._get_inputs_dict(batch)
                if args.fp16:
                    with amp.autocast():
                        loss, *_, correct_count = self._calculate_loss(
                            context_model,
                            query_model,
                            context_inputs,
                            query_inputs,
                            labels,
                            criterion,
                        )
                else:
                    loss, *_, correct_count = self._calculate_loss(
                        context_model,
                        query_model,
                        context_inputs,
                        query_inputs,
                        labels,
                        criterion,
                    )

                if args.n_gpu > 1:
                    loss = loss.mean()

                current_loss = loss.item()

                if show_running_loss:
                    batch_iterator.set_description(
                        f""Epochs {epoch_number}/{args.num_train_epochs}. Running Loss: {current_loss:9.4f} Correct count: {correct_count}""
                    )

                if args.gradient_accumulation_steps > 1:
                    loss = loss / args.gradient_accumulation_steps

                if args.fp16:
                    scaler.scale(loss).backward()
                else:
                    loss.backward()

                tr_loss += loss.item()
                if (step + 1) % args.gradient_accumulation_steps == 0:
                    if args.fp16:
                        scaler.unscale_(optimizer)
                    if args.optimizer == ""AdamW"":
                        torch.nn.utils.clip_grad_norm_(
                            context_model.parameters(), args.max_grad_norm
                        )
                        torch.nn.utils.clip_grad_norm_(
                            query_model.parameters(), args.max_grad_norm
                        )

                    if args.fp16:
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        optimizer.step()
                    scheduler.step()  # Update learning rate schedule
                    context_model.zero_grad()
                    query_model.zero_grad()
                    global_step += 1

                    if args.logging_steps > 0 and global_step % args.logging_steps == 0:
                        # Log metrics
                        tb_writer.add_scalar(
                            ""lr"", scheduler.get_last_lr()[0], global_step
                        )
                        tb_writer.add_scalar(
                            ""loss"",
                            (tr_loss - logging_loss) / args.logging_steps,
                            global_step,
                        )
                        logging_loss = tr_loss
                        if args.wandb_project or self.is_sweeping:
                            wandb.log(
                                {
                                    ""Training loss"": current_loss,
                                    ""lr"": scheduler.get_last_lr()[0],
                                    ""global_step"": global_step,
                                }
                            )

                    if args.save_steps > 0 and global_step % args.save_steps == 0:
                        # Save model checkpoint
                        output_dir_current = os.path.join(
                            output_dir, ""checkpoint-{}"".format(global_step)
                        )

                        self.save_model(
                            output_dir_current,
                            optimizer,
                            scheduler,
                            context_model=context_model,
                            query_model=query_model,
                        )

                    if args.evaluate_during_training and (
                        args.evaluate_during_training_steps > 0
                        and global_step % args.evaluate_during_training_steps == 0
                    ):
                        # Only evaluate when single GPU otherwise metrics may not average well
                        results, *_ = self.eval_model(
                            eval_data,
                            additional_passages=additional_eval_passages,
                            verbose=verbose and args.evaluate_during_training_verbose,
                            silent=args.evaluate_during_training_silent,
                            **kwargs,
                        )
                        for key, value in results.items():
                            try:
                                tb_writer.add_scalar(
                                    ""eval_{}"".format(key), value, global_step
                                )
                            except (NotImplementedError, AssertionError):
                                pass

                        output_dir_current = os.path.join(
                            output_dir, ""checkpoint-{}"".format(global_step)
                        )

                        if args.save_eval_checkpoints:
                            self.save_model(
                                output_dir_current,
                                optimizer,
                                scheduler,
                                context_model=context_model,
                                query_model=query_model,
                                results=results,
                            )

                        training_progress_scores[""global_step""].append(global_step)
                        training_progress_scores[""train_loss""].append(current_loss)
                        for key in results:
                            training_progress_scores[key].append(results[key])
                        report = pd.DataFrame(training_progress_scores)
                        report.to_csv(
                            os.path.join(
                                args.output_dir, ""training_progress_scores.csv""
                            ),
                            index=False,
                        )

                        if args.wandb_project or self.is_sweeping:
                            wandb.log(self._get_last_metrics(training_progress_scores))

                        if not best_eval_metric:
                            best_eval_metric = results[args.early_stopping_metric]
                            if args.save_best_model:
                                self.save_model(
                                    args.best_model_dir,
                                    optimizer,
                                    scheduler,
                                    context_model=context_model,
                                    query_model=query_model,
                                    results=results,
                                )
                        if best_eval_metric and args.early_stopping_metric_minimize:
                            if (
                                results[args.early_stopping_metric] - best_eval_metric
                                < args.early_stopping_delta
                            ):
                                best_eval_metric = results[args.early_stopping_metric]
                                if args.save_best_model:
                                    self.save_model(
                                        args.best_model_dir,
                                        optimizer,
                                        scheduler,
                                        context_model=context_model,
                                        query_model=query_model,
                                        results=results,
                                    )
                                early_stopping_counter = 0
                            else:
                                if args.use_early_stopping:
                                    if (
                                        early_stopping_counter
                                        < args.early_stopping_patience
                                    ):
                                        early_stopping_counter += 1
                                        if verbose:
                                            logger.info(
                                                f"" No improvement in {args.early_stopping_metric}""
                                            )
                                            logger.info(
                                                f"" Current step: {early_stopping_counter}""
                                            )
                                            logger.info(
                                                f"" Early stopping patience: {args.early_stopping_patience}""
                                            )
                                    else:
                                        if verbose:
                                            logger.info(
                                                f"" Patience of {args.early_stopping_patience} steps reached""
                                            )
                                            logger.info("" Training terminated."")
                                            train_iterator.close()
                                        return (
                                            global_step,
                                            tr_loss / global_step
                                            if not self.args.evaluate_during_training
                                            else training_progress_scores,
                                        )
                        else:
                            if (
                                results[args.early_stopping_metric] - best_eval_metric
                                > args.early_stopping_delta
                            ):
                                best_eval_metric = results[args.early_stopping_metric]
                                if args.save_best_model:
                                    self.save_model(
                                        args.best_model_dir,
                                        optimizer,
                                        scheduler,
                                        context_model=context_model,
                                        query_model=query_model,
                                        results=results,
                                    )
                                early_stopping_counter = 0
                            else:
                                if args.use_early_stopping:
                                    if (
                                        early_stopping_counter
                                        < args.early_stopping_patience
                                    ):
                                        early_stopping_counter += 1
                                        if verbose:
                                            logger.info(
                                                f"" No improvement in {args.early_stopping_metric}""
                                            )
                                            logger.info(
                                                f"" Current step: {early_stopping_counter}""
                                            )
                                            logger.info(
                                                f"" Early stopping patience: {args.early_stopping_patience}""
                                            )
                                    else:
                                        if verbose:
                                            logger.info(
                                                f"" Patience of {args.early_stopping_patience} steps reached""
                                            )
                                            logger.info("" Training terminated."")
                                            train_iterator.close()
                                        return (
                                            global_step,
                                            tr_loss / global_step
                                            if not self.args.evaluate_during_training
                                            else training_progress_scores,
                                        )
                        context_model.train()
                        query_model.train()

            epoch_number += 1
            output_dir_current = os.path.join(
                output_dir, ""checkpoint-{}-epoch-{}"".format(global_step, epoch_number)
            )

            if args.save_model_every_epoch or args.evaluate_during_training:
                os.makedirs(output_dir_current, exist_ok=True)

            if args.save_model_every_epoch:
                self.save_model(
                    output_dir_current,
                    optimizer,
                    scheduler,
                    context_model=context_model,
                    query_model=query_model,
                )

            if args.evaluate_during_training and args.evaluate_each_epoch:
                results, *_ = self.eval_model(
                    eval_data,
                    additional_passages=additional_eval_passages,
                    verbose=verbose and args.evaluate_during_training_verbose,
                    silent=args.evaluate_during_training_silent,
                    **kwargs,
                )

                if args.save_eval_checkpoints:
                    self.save_model(
                        output_dir_current, optimizer, scheduler, results=results
                    )

                training_progress_scores[""global_step""].append(global_step)
                training_progress_scores[""train_loss""].append(current_loss)
                for key in results:
                    training_progress_scores[key].append(results[key])
                report = pd.DataFrame(training_progress_scores)
                report.to_csv(
                    os.path.join(args.output_dir, ""training_progress_scores.csv""),
                    index=False,
                )

                if args.wandb_project or self.is_sweeping:
                    wandb.log(self._get_last_metrics(training_progress_scores))

                if not best_eval_metric:
                    best_eval_metric = results[args.early_stopping_metric]
                    if args.save_best_model:
                        self.save_model(
                            args.best_model_dir,
                            optimizer,
                            scheduler,
                            context_model=context_model,
                            query_model=query_model,
                            results=results,
                        )
                if best_eval_metric and args.early_stopping_metric_minimize:
                    if (
                        results[args.early_stopping_metric] - best_eval_metric
                        < args.early_stopping_delta
                    ):
                        best_eval_metric = results[args.early_stopping_metric]
                        if args.save_best_model:
                            self.save_model(
                                args.best_model_dir,
                                optimizer,
                                scheduler,
                                context_model=context_model,
                                query_model=query_model,
                                results=results,
                            )
                        early_stopping_counter = 0
                    else:
                        if (
                            args.use_early_stopping
                            and args.early_stopping_consider_epochs
                        ):
                            if early_stopping_counter < args.early_stopping_patience:
                                early_stopping_counter += 1
                                if verbose:
                                    logger.info(
                                        f"" No improvement in {args.early_stopping_metric}""
                                    )
                                    logger.info(
                                        f"" Current step: {early_stopping_counter}""
                                    )
                                    logger.info(
                                        f"" Early stopping patience: {args.early_stopping_patience}""
                                    )
                            else:
                                if verbose:
                                    logger.info(
                                        f"" Patience of {args.early_stopping_patience} steps reached""
                                    )
                                    logger.info("" Training terminated."")
                                    train_iterator.close()
                                return (
                                    global_step,
                                    tr_loss / global_step
                                    if not self.args.evaluate_during_training
                                    else training_progress_scores,
                                )
                else:
                    if (
                        results[args.early_stopping_metric] - best_eval_metric
                        > args.early_stopping_delta
                    ):
                        best_eval_metric = results[args.early_stopping_metric]
                        if args.save_best_model:
                            self.save_model(
                                args.best_model_dir,
                                optimizer,
                                scheduler,
                                context_model=context_model,
                                query_model=query_model,
                                results=results,
                            )
                        early_stopping_counter = 0
                    else:
                        if (
                            args.use_early_stopping
                            and args.early_stopping_consider_epochs
                        ):
                            if early_stopping_counter < args.early_stopping_patience:
                                early_stopping_counter += 1
                                if verbose:
                                    logger.info(
                                        f"" No improvement in {args.early_stopping_metric}""
                                    )
                                    logger.info(
                                        f"" Current step: {early_stopping_counter}""
                                    )
                                    logger.info(
                                        f"" Early stopping patience: {args.early_stopping_patience}""
                                    )
                            else:
                                if verbose:
                                    logger.info(
                                        f"" Patience of {args.early_stopping_patience} steps reached""
                                    )
                                    logger.info("" Training terminated."")
                                    train_iterator.close()
                                return (
                                    global_step,
                                    tr_loss / global_step
                                    if not self.args.evaluate_during_training
                                    else training_progress_scores,
                                )

        return (
            global_step,
            tr_loss / global_step
            if not self.args.evaluate_during_training
            else training_progress_scores,
        )",args.evaluate_during_training_steps > 0 and global_step % args.evaluate_during_training_steps == 0,args.evaluate_during_training_steps > 0 == global_step % args.evaluate_during_training_steps
ssd.pytorch,https://github.com/amdegroot/ssd.pytorch/tree/master//train.py,,train$71,"def train():
    if args.dataset == 'COCO':
        if args.dataset_root == VOC_ROOT:
            if not os.path.exists(COCO_ROOT):
                parser.error('Must specify dataset_root if specifying dataset')
            print(""WARNING: Using default COCO dataset_root because "" +
                  ""--dataset_root was not specified."")
            args.dataset_root = COCO_ROOT
        cfg = coco
        dataset = COCODetection(root=args.dataset_root,
                                transform=SSDAugmentation(cfg['min_dim'],
                                                          MEANS))
    elif args.dataset == 'VOC':
        if args.dataset_root == COCO_ROOT:
            parser.error('Must specify dataset if specifying dataset_root')
        cfg = voc
        dataset = VOCDetection(root=args.dataset_root,
                               transform=SSDAugmentation(cfg['min_dim'],
                                                         MEANS))

    if args.visdom:
        import visdom
        viz = visdom.Visdom()

    ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])
    net = ssd_net

    if args.cuda:
        net = torch.nn.DataParallel(ssd_net)
        cudnn.benchmark = True

    if args.resume:
        print('Resuming training, loading {}...'.format(args.resume))
        ssd_net.load_weights(args.resume)
    else:
        vgg_weights = torch.load(args.save_folder + args.basenet)
        print('Loading base network...')
        ssd_net.vgg.load_state_dict(vgg_weights)

    if args.cuda:
        net = net.cuda()

    if not args.resume:
        print('Initializing weights...')
        # initialize newly added layers' weights with xavier method
        ssd_net.extras.apply(weights_init)
        ssd_net.loc.apply(weights_init)
        ssd_net.conf.apply(weights_init)

    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum,
                          weight_decay=args.weight_decay)
    criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,
                             False, args.cuda)

    net.train()
    # loss counters
    loc_loss = 0
    conf_loss = 0
    epoch = 0
    print('Loading the dataset...')

    epoch_size = len(dataset) // args.batch_size
    print('Training SSD on:', dataset.name)
    print('Using the specified args:')
    print(args)

    step_index = 0

    if args.visdom:
        vis_title = 'SSD.PyTorch on ' + dataset.name
        vis_legend = ['Loc Loss', 'Conf Loss', 'Total Loss']
        iter_plot = create_vis_plot('Iteration', 'Loss', vis_title, vis_legend)
        epoch_plot = create_vis_plot('Epoch', 'Loss', vis_title, vis_legend)

    data_loader = data.DataLoader(dataset, args.batch_size,
                                  num_workers=args.num_workers,
                                  shuffle=True, collate_fn=detection_collate,
                                  pin_memory=True)
    # create batch iterator
    batch_iterator = iter(data_loader)
    for iteration in range(args.start_iter, cfg['max_iter']):
        if args.visdom and iteration != 0 and (iteration % epoch_size == 0):
            update_vis_plot(epoch, loc_loss, conf_loss, epoch_plot, None,
                            'append', epoch_size)
            # reset epoch loss counters
            loc_loss = 0
            conf_loss = 0
            epoch += 1

        if iteration in cfg['lr_steps']:
            step_index += 1
            adjust_learning_rate(optimizer, args.gamma, step_index)

        # load train data
        images, targets = next(batch_iterator)

        if args.cuda:
            images = Variable(images.cuda())
            targets = [Variable(ann.cuda(), volatile=True) for ann in targets]
        else:
            images = Variable(images)
            targets = [Variable(ann, volatile=True) for ann in targets]
        # forward
        t0 = time.time()
        out = net(images)
        # backprop
        optimizer.zero_grad()
        loss_l, loss_c = criterion(out, targets)
        loss = loss_l + loss_c
        loss.backward()
        optimizer.step()
        t1 = time.time()
        loc_loss += loss_l.data[0]
        conf_loss += loss_c.data[0]

        if iteration % 10 == 0:
            print('timer: %.4f sec.' % (t1 - t0))
            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data[0]), end=' ')

        if args.visdom:
            update_vis_plot(iteration, loss_l.data[0], loss_c.data[0],
                            iter_plot, epoch_plot, 'append')

        if iteration != 0 and iteration % 5000 == 0:
            print('Saving state, iter:', iteration)
            torch.save(ssd_net.state_dict(), 'weights/ssd300_COCO_' +
                       repr(iteration) + '.pth')
    torch.save(ssd_net.state_dict(),
               args.save_folder + '' + args.dataset + '.pth')",args.visdom and iteration != 0 and (iteration % epoch_size == 0),iteration != 0 == iteration % epoch_size and args.visdom
ssd.pytorch,https://github.com/amdegroot/ssd.pytorch/tree/master//train.py,,train$71,"def train():
    if args.dataset == 'COCO':
        if args.dataset_root == VOC_ROOT:
            if not os.path.exists(COCO_ROOT):
                parser.error('Must specify dataset_root if specifying dataset')
            print(""WARNING: Using default COCO dataset_root because "" +
                  ""--dataset_root was not specified."")
            args.dataset_root = COCO_ROOT
        cfg = coco
        dataset = COCODetection(root=args.dataset_root,
                                transform=SSDAugmentation(cfg['min_dim'],
                                                          MEANS))
    elif args.dataset == 'VOC':
        if args.dataset_root == COCO_ROOT:
            parser.error('Must specify dataset if specifying dataset_root')
        cfg = voc
        dataset = VOCDetection(root=args.dataset_root,
                               transform=SSDAugmentation(cfg['min_dim'],
                                                         MEANS))

    if args.visdom:
        import visdom
        viz = visdom.Visdom()

    ssd_net = build_ssd('train', cfg['min_dim'], cfg['num_classes'])
    net = ssd_net

    if args.cuda:
        net = torch.nn.DataParallel(ssd_net)
        cudnn.benchmark = True

    if args.resume:
        print('Resuming training, loading {}...'.format(args.resume))
        ssd_net.load_weights(args.resume)
    else:
        vgg_weights = torch.load(args.save_folder + args.basenet)
        print('Loading base network...')
        ssd_net.vgg.load_state_dict(vgg_weights)

    if args.cuda:
        net = net.cuda()

    if not args.resume:
        print('Initializing weights...')
        # initialize newly added layers' weights with xavier method
        ssd_net.extras.apply(weights_init)
        ssd_net.loc.apply(weights_init)
        ssd_net.conf.apply(weights_init)

    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum,
                          weight_decay=args.weight_decay)
    criterion = MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5,
                             False, args.cuda)

    net.train()
    # loss counters
    loc_loss = 0
    conf_loss = 0
    epoch = 0
    print('Loading the dataset...')

    epoch_size = len(dataset) // args.batch_size
    print('Training SSD on:', dataset.name)
    print('Using the specified args:')
    print(args)

    step_index = 0

    if args.visdom:
        vis_title = 'SSD.PyTorch on ' + dataset.name
        vis_legend = ['Loc Loss', 'Conf Loss', 'Total Loss']
        iter_plot = create_vis_plot('Iteration', 'Loss', vis_title, vis_legend)
        epoch_plot = create_vis_plot('Epoch', 'Loss', vis_title, vis_legend)

    data_loader = data.DataLoader(dataset, args.batch_size,
                                  num_workers=args.num_workers,
                                  shuffle=True, collate_fn=detection_collate,
                                  pin_memory=True)
    # create batch iterator
    batch_iterator = iter(data_loader)
    for iteration in range(args.start_iter, cfg['max_iter']):
        if args.visdom and iteration != 0 and (iteration % epoch_size == 0):
            update_vis_plot(epoch, loc_loss, conf_loss, epoch_plot, None,
                            'append', epoch_size)
            # reset epoch loss counters
            loc_loss = 0
            conf_loss = 0
            epoch += 1

        if iteration in cfg['lr_steps']:
            step_index += 1
            adjust_learning_rate(optimizer, args.gamma, step_index)

        # load train data
        images, targets = next(batch_iterator)

        if args.cuda:
            images = Variable(images.cuda())
            targets = [Variable(ann.cuda(), volatile=True) for ann in targets]
        else:
            images = Variable(images)
            targets = [Variable(ann, volatile=True) for ann in targets]
        # forward
        t0 = time.time()
        out = net(images)
        # backprop
        optimizer.zero_grad()
        loss_l, loss_c = criterion(out, targets)
        loss = loss_l + loss_c
        loss.backward()
        optimizer.step()
        t1 = time.time()
        loc_loss += loss_l.data[0]
        conf_loss += loss_c.data[0]

        if iteration % 10 == 0:
            print('timer: %.4f sec.' % (t1 - t0))
            print('iter ' + repr(iteration) + ' || Loss: %.4f ||' % (loss.data[0]), end=' ')

        if args.visdom:
            update_vis_plot(iteration, loss_l.data[0], loss_c.data[0],
                            iter_plot, epoch_plot, 'append')

        if iteration != 0 and iteration % 5000 == 0:
            print('Saving state, iter:', iteration)
            torch.save(ssd_net.state_dict(), 'weights/ssd300_COCO_' +
                       repr(iteration) + '.pth')
    torch.save(ssd_net.state_dict(),
               args.save_folder + '' + args.dataset + '.pth')",iteration != 0 and iteration % 5000 == 0,iteration != 0 == iteration % 5000
OWOD,https://github.com/JosephKJ/OWOD/tree/master/projects/DensePose/densepose/utils/dbhelper.py,_FieldEntryRangePredicate,__call__$80,"def __call__(self, entry):
            return (entry[self.name] >= self.type(self.vmin)) and (
                entry[self.name] <= self.type(self.vmax)
            )",entry[self.name] >= self.type(self.vmin) and entry[self.name] <= self.type(self.vmax),self.type(self.vmin) <= entry[self.name] <= self.type(self.vmax)
GhIDA,https://github.com/Cisco-Talos/GhIDA/tree/master/ghida_plugin/idaxml.py,XmlExporter,check_char$353,"def check_char(self, ch):
        """"""
        Replaces a special XML character with an entity string.

        Args:
            ch: String containing the character to check.

        Returns:
            String containing either the character or the entity
            substition string.
        """"""
        if ((ord(ch) < 0x20) and (ord(ch) != 0x09 and
                                  ord(ch) != 0x0A and ord(ch) != 0x0D)):
            return ''
        elif ch == '&':
            return '&amp;'
        elif ch == '<':
            return ""&lt;""
        elif ch == '>':
            return ""&gt;""
        elif ch == '\'':
            return ""&apos;""
        elif ch == '""':
            return ""&quot;""
        elif ch == '\x7F':
            return ''
        elif ord(ch) > 0x7F:
            return '&#x' + format(ord(ch), ""x"") + "";""
        return ch",ord(ch) != 9 and ord(ch) != 10 and (ord(ch) != 13),9 != ord(ch) != 10 and ord(ch) != 13
fedlearner,https://github.com/bytedance/fedlearner/tree/master/fedlearner/data_join/data_portal_job_manager.py,DataPortalJobManager,_is_wanted_date$342,"def _is_wanted_date(self, cur_date_str):
        cur_date = convert_to_datetime(cur_date_str)
        if cur_date != INVALID_DATETIME:
            if self._start_date != INVALID_DATETIME and \
                cur_date < self._start_date:
                return False
            if self._end_date != INVALID_DATETIME and \
                cur_date >= self._end_date:
                return False
        return True",self._start_date != INVALID_DATETIME and cur_date < self._start_date,cur_date < self._start_date != INVALID_DATETIME
fedlearner,https://github.com/bytedance/fedlearner/tree/master/fedlearner/data_join/data_portal_job_manager.py,DataPortalJobManager,_is_wanted_date$342,"def _is_wanted_date(self, cur_date_str):
        cur_date = convert_to_datetime(cur_date_str)
        if cur_date != INVALID_DATETIME:
            if self._start_date != INVALID_DATETIME and \
                cur_date < self._start_date:
                return False
            if self._end_date != INVALID_DATETIME and \
                cur_date >= self._end_date:
                return False
        return True",self._end_date != INVALID_DATETIME and cur_date >= self._end_date,cur_date >= self._end_date != INVALID_DATETIME
pycma,https://github.com/CMA-ES/pycma/tree/master/cma/constraints_handler.py,BoundTransform,inverse$273,"def inverse(self, x, copy_if_changed=True):
        """"""inverse transform of ``x`` from the bounded domain.

        """"""
        if self.bounds is None or (self.bounds[0] is None and
                                   self.bounds[1] is None):
            return x
        return np.asarray(self.bounds_tf.inverse(x, copy_if_changed))",self.bounds[0] is None and self.bounds[1] is None,self.bounds[0] is None is self.bounds[1]
CenterNet-better,https://github.com/FateScript/CenterNet-better/tree/master/dl_lib/engine/hooks.py,EvalHook,after_step$318,"def after_step(self):
        next_iter = self.trainer.iter + 1
        is_final = next_iter == self.trainer.max_iter
        if is_final or (self._period > 0 and next_iter % self._period == 0):
            results = self._func()

            if results:
                assert isinstance(
                    results, dict
                ), ""Eval function must return a dict. Got {} instead."".format(results)

                flattened_results = flatten_results_dict(results)
                for k, v in flattened_results.items():
                    try:
                        v = float(v)
                    except Exception:
                        raise ValueError(
                            ""[EvalHook] eval_function should return a nested dict of float. ""
                            ""Got '{}: {}' instead."".format(k, v)
                        )
                self.trainer.storage.put_scalars(**flattened_results, smoothing_hint=False)

            # Evaluation may take different time among workers.
            # A barrier make them start the next iteration together.
            comm.synchronize()",self._period > 0 and next_iter % self._period == 0,self._period > 0 == next_iter % self._period
lattice,https://github.com/tensorflow/lattice/tree/master/tensorflow_lattice/python/pwl_calibration_sonnet_module.py,PWLCalibration,__call__$277,"def __call__(self, inputs):
    """"""Standard Sonnet __call__() method..

    Args:
      inputs: Either input tensor or list of 2 elements: input tensor and
        `is_missing` tensor.

    Returns:
      Calibrated input tensor.

    Raises:
      ValueError: If `is_missing` tensor specified incorrectly.
    """"""
    self._create_parameters_once(inputs)
    is_missing = None
    if isinstance(inputs, list):
      # Only 2 element lists are allowed. When such list is given - second
      # element represents 'is_missing' tensor encoded as float value.
      if not self.impute_missing:
        raise ValueError(""Multiple inputs for PWLCalibration module assume ""
                         ""regular input tensor and 'is_missing' tensor, but ""
                         ""this instance of a layer is not configured to handle ""
                         ""missing value. See 'impute_missing' parameter."")
      if len(inputs) > 2:
        raise ValueError(""Multiple inputs for PWLCalibration module assume ""
                         ""normal input tensor and 'is_missing' tensor, but more""
                         "" than 2 tensors given. 'inputs': "" + str(inputs))
      if len(inputs) == 2:
        inputs, is_missing = inputs
        if is_missing.shape.as_list() != inputs.shape.as_list():
          raise ValueError(
              ""is_missing shape %s does not match inputs shape %s for ""
              ""PWLCalibration module"" %
              (str(is_missing.shape), str(inputs.shape)))
      else:
        [inputs] = inputs
    if len(inputs.shape) != 2 or (inputs.shape[1] != self.units and
                                  inputs.shape[1] != 1):
      raise ValueError(""Shape of input tensor for PWLCalibration module must""
                       "" be [-1, units] or [-1, 1]. It is: "" +
                       str(inputs.shape))

    if self._interpolation_keypoints.dtype != inputs.dtype:
      raise ValueError(""dtype(%s) of input to PWLCalibration module does not ""
                       ""correspond to dtype(%s) of keypoints. You can enforce ""
                       ""dtype of keypoints by passing keypoints ""
                       ""in such format which by default will be converted into ""
                       ""the desired one."" %
                       (inputs.dtype, self._interpolation_keypoints.dtype))
    # Here is calibration. Everything else is handling of missing.
    if inputs.shape[1] > 1:
      # Add dimension to multi dim input to get shape [batch_size, units, 1].
      # Interpolation will have shape [batch_size, units, weights].
      inputs_to_calibration = tf.expand_dims(inputs, -1)
    else:
      inputs_to_calibration = inputs
    interpolation_weights = pwl_calibration_lib.compute_interpolation_weights(
        inputs_to_calibration, self._interpolation_keypoints, self._lengths)
    if self.is_cyclic:
      # Need to add such last height to make all heights to sum up to 0.0 in
      # order to make calibrator cyclic.
      bias_and_heights = tf.concat(
          [self.kernel, -tf.reduce_sum(self.kernel[1:], axis=0, keepdims=True)],
          axis=0)
    else:
      bias_and_heights = self.kernel

    # bias_and_heights has shape [weight, units].
    if inputs.shape[1] > 1:
      # Multi dim input has interpolation shape [batch_size, units, weights].
      result = tf.reduce_sum(
          interpolation_weights * tf.transpose(bias_and_heights), axis=-1)
    else:
      # Single dim input has interpolation shape [batch_size, weights].
      result = tf.matmul(interpolation_weights, bias_and_heights)

    if self.impute_missing:
      if is_missing is None:
        if self.missing_input_value is None:
          raise ValueError(""PWLCalibration layer is configured to impute ""
                           ""missing but no 'missing_input_value' specified and ""
                           ""'is_missing' tensor is not given."")
        assert self._missing_input_value_tensor is not None
        is_missing = tf.cast(
            tf.equal(inputs, self._missing_input_value_tensor),
            dtype=self.dtype)
      result = is_missing * self.missing_output + (1.0 - is_missing) * result
    return result",inputs.shape[1] != self.units and inputs.shape[1] != 1,self.units != inputs.shape[1] != 1
pybaseball,https://github.com/jldbc/pybaseball/tree/master/pybaseball/utils.py,,get_first_season$87,"def get_first_season(team: str, include_equivalents: bool = True) -> Optional[int]:
    if not include_equivalents:
        return first_season_map[team]
    
    oldest = first_season_map[team] or date.today().year
    
    equivalents = [x for x in team_equivalents if team in x]

    if not equivalents:
        return oldest

    for equivalent in equivalents[0]:
        equivalent_first = first_season_map[equivalent]
        if equivalent_first is not None and equivalent_first < oldest:
            oldest = equivalent_first
    
    return oldest",equivalent_first is not None and equivalent_first < oldest,None is not equivalent_first < oldest
model-analysis,https://github.com/tensorflow/model-analysis/tree/master/tensorflow_model_analysis/api/model_eval_lib.py,,_is_legacy_eval$82,"def _is_legacy_eval(
    config_version: Optional[int],
    eval_shared_model: Optional[types.MaybeMultipleEvalSharedModels],
    eval_config: Optional[config_pb2.EvalConfig]):
  """"""Returns True if legacy evaluation is being used.

  A legacy evaluation is an evalution that uses only a single EvalSharedModel,
  has no tags (or uses ""eval"" as its tag), and does not specify an eval_config
  The legacy evaluation is based on using add_metrics_callbacks to create a
  modified version of the graph saved with an EvalSavedModel. The newer version
  of evaluation supports both add_metrics_callbacks as well as metrics defined
  in MetricsSpecs inside of EvalConfig. The newer version works with both ""eval""
  and serving models and also supports multi-model evaluation. This function is
  used by code to support backwards compatibility for callers that have not
  updated to use the new EvalConfig.

  Args:
    config_version: Optionally, An explicit version of the config determined
      elsewhere. This is used to handle cases where the provided eval_config was
      generated internally, and thus not a reliable indicator of user intent.
    eval_shared_model: Optionally, the model to be evaluated.
    eval_config: Optionally, an EvalConfig specifying v2 config.

  Returns:
    Whether the user inputs should trigger a legacy evaluation.
  """"""
  return ((config_version is not None and config_version == 1) or
          (eval_shared_model and not isinstance(eval_shared_model, dict) and
           not isinstance(eval_shared_model, list) and
           (not eval_shared_model.model_loader.tags or
            eval_constants.EVAL_TAG in eval_shared_model.model_loader.tags) and
           not eval_config))",config_version is not None and config_version == 1,None is not config_version == 1
exbert,https://github.com/bhoov/exbert/tree/master/server/transformers/src/transformers/tokenization_transfo_xl.py,TransfoXLTokenizer,count_sents$162,"def count_sents(self, sents, verbose=False):
        """"""
            sents : a list of sentences, each a list of tokenized symbols
        """"""
        if verbose:
            logger.info(""counting {} sents ..."".format(len(sents)))
        for idx, symbols in enumerate(sents):
            if verbose and idx > 0 and idx % 500000 == 0:
                logger.info(""    line {}"".format(idx))
            self.counter.update(symbols)",verbose and idx > 0 and (idx % 500000 == 0),idx > 0 == idx % 500000 and verbose
PyBitmessage,https://github.com/Bitmessage/PyBitmessage/tree/master/src/helper_startup.py,,adjustHalfOpenConnectionsLimit$266,"def adjustHalfOpenConnectionsLimit():
    """"""Check and satisfy half-open connections limit (mainly XP and Vista)""""""
    if config.safeGet(
            'bitmessagesettings', 'socksproxytype', 'none') != 'none':
        state.maximumNumberOfHalfOpenConnections = 4
        return

    is_limited = False
    try:
        if sys.platform[0:3] == ""win"":
            # Some XP and Vista systems can only have 10 outgoing
            # connections at a time.
            VER_THIS = StrictVersion(platform.version())
            is_limited = (
                StrictVersion(""5.1.2600"") <= VER_THIS
                and StrictVersion(""6.0.6000"") >= VER_THIS
            )
    except ValueError:
        pass

    state.maximumNumberOfHalfOpenConnections = 9 if is_limited else 64",StrictVersion('5.1.2600') <= VER_THIS and StrictVersion('6.0.6000') >= VER_THIS,StrictVersion('5.1.2600') <= VER_THIS <= StrictVersion('6.0.6000')
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-airflow/dagster_airflow_tests/test_fixtures.py,,dagster_airflow_python_operator_pipeline$83,"def dagster_airflow_python_operator_pipeline():
    """"""This is a test fixture for running Dagster pipelines as Airflow DAGs.

    Usage:
        from dagster_airflow_tests.test_fixtures import dagster_airflow_python_operator_pipeline

        def test_airflow(dagster_airflow_python_operator_pipeline):
            results = dagster_airflow_python_operator_pipeline(
                pipeline_name='test_pipeline',
                recon_repo=reconstructable(define_pipeline),
                environment_yaml=['environments/test_*.yaml']
            )
            assert len(results) == 3
    """"""
    from dagster_airflow.factory import make_airflow_dag_for_recon_repo
    from dagster_airflow.vendor.python_operator import PythonOperator

    def _pipeline_fn(
        recon_repo,
        pipeline_name,
        run_config=None,
        environment_yaml=None,
        op_kwargs=None,
        mode=None,
        execution_date=timezone.utcnow(),
    ):
        if run_config is None and environment_yaml is not None:
            run_config = load_yaml_from_glob_list(environment_yaml)

        dag, tasks = make_airflow_dag_for_recon_repo(
            recon_repo, pipeline_name, run_config, mode=mode, op_kwargs=op_kwargs
        )
        assert isinstance(dag, DAG)

        for task in tasks:
            assert isinstance(task, PythonOperator)

        return execute_tasks_in_dag(
            dag, tasks, run_id=make_new_run_id(), execution_date=execution_date
        )

    return _pipeline_fn",run_config is None and environment_yaml is not None,run_config is None is not environment_yaml
hachoir,https://github.com/vstinner/hachoir/tree/master/hachoir/field/generic_field_set.py,GenericFieldSet,replaceField$445,"def replaceField(self, name, new_fields):
        # TODO: Check in self and not self.field
        # Problem is that ""generator is already executing""
        if name not in self._fields:
            raise ParserError(
                ""Unable to replace %s: field doesn't exist!"" % name)
        assert 1 <= len(new_fields)
        old_field = self[name]
        total_size = sum((field.size for field in new_fields))
        if old_field.size != total_size:
            raise ParserError(""Unable to replace %s: ""
                              ""new field(s) hasn't same size (%u bits instead of %u bits)!""
                              % (name, total_size, old_field.size))
        field = new_fields[0]
        if field._name.endswith(""[]""):
            self.setUniqueFieldName(field)
        field._address = old_field.address
        if field.name != name and field.name in self._fields:
            raise ParserError(
                ""Unable to replace %s: name \""%s\"" is already used!""
                % (name, field.name))
        self._fields.replace(name, field.name, field)
        self.raiseEvent(""field-replaced"", old_field, field)
        if 1 < len(new_fields):
            index = self._fields.index(new_fields[0].name) + 1
            address = field.address + field.size
            for field in new_fields[1:]:
                if field._name.endswith(""[]""):
                    self.setUniqueFieldName(field)
                field._address = address
                if field.name in self._fields:
                    raise ParserError(
                        ""Unable to replace %s: name \""%s\"" is already used!""
                        % (name, field.name))
                self._fields.insert(index, field.name, field)
                self.raiseEvent(""field-inserted"", index, field)
                index += 1
                address += field.size",field.name != name and field.name in self._fields,name != field.name in self._fields
PyHive,https://github.com/dropbox/PyHive/tree/master/TCLIService/ttypes.py,TRow,read$1929,"def read(self, iprot):
        if iprot._fast_decode is not None and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None:
            iprot._fast_decode(self, iprot, (self.__class__, self.thrift_spec))
            return
        iprot.readStructBegin()
        while True:
            (fname, ftype, fid) = iprot.readFieldBegin()
            if ftype == TType.STOP:
                break
            if fid == 1:
                if ftype == TType.LIST:
                    self.colVals = []
                    (_etype44, _size41) = iprot.readListBegin()
                    for _i45 in range(_size41):
                        _elem46 = TColumnValue()
                        _elem46.read(iprot)
                        self.colVals.append(_elem46)
                    iprot.readListEnd()
                else:
                    iprot.skip(ftype)
            else:
                iprot.skip(ftype)
            iprot.readFieldEnd()
        iprot.readStructEnd()","iprot._fast_decode is not None and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None)","iprot._fast_decode is not None is not self.thrift_spec and isinstance(iprot.trans, TTransport.CReadableTransport)"
dnspython,https://github.com/rthalley/dnspython/tree/master/dns/resolver.py,,_getaddrinfo$1329,"def _getaddrinfo(host=None, service=None, family=socket.AF_UNSPEC, socktype=0,
                 proto=0, flags=0):
    if flags & socket.AI_NUMERICHOST != 0:
        # Short circuit directly into the system's getaddrinfo().  We're
        # not adding any value in this case, and this avoids infinite loops
        # because dns.query.* needs to call getaddrinfo() for IPv6 scoping
        # reasons.  We will also do this short circuit below if we
        # discover that the host is an address literal.
        return _original_getaddrinfo(host, service, family, socktype, proto,
                                     flags)
    if flags & (socket.AI_ADDRCONFIG | socket.AI_V4MAPPED) != 0:
        # Not implemented.  We raise a gaierror as opposed to a
        # NotImplementedError as it helps callers handle errors more
        # appropriately.  [Issue #316]
        #
        # We raise EAI_FAIL as opposed to EAI_SYSTEM because there is
        # no EAI_SYSTEM on Windows [Issue #416].  We didn't go for
        # EAI_BADFLAGS as the flags aren't bad, we just don't
        # implement them.
        raise socket.gaierror(socket.EAI_FAIL,
                              'Non-recoverable failure in name resolution')
    if host is None and service is None:
        raise socket.gaierror(socket.EAI_NONAME, 'Name or service not known')
    v6addrs = []
    v4addrs = []
    canonical_name = None  # pylint: disable=redefined-outer-name
    # Is host None or an address literal?  If so, use the system's
    # getaddrinfo().
    if host is None:
        return _original_getaddrinfo(host, service, family, socktype,
                                     proto, flags)
    try:
        # We don't care about the result of af_for_address(), we're just
        # calling it so it raises an exception if host is not an IPv4 or
        # IPv6 address.
        dns.inet.af_for_address(host)
        return _original_getaddrinfo(host, service, family, socktype,
                                     proto, flags)
    except Exception:
        pass
    # Something needs resolution!
    try:
        if family == socket.AF_INET6 or family == socket.AF_UNSPEC:
            v6 = _resolver.resolve(host, dns.rdatatype.AAAA,
                                   raise_on_no_answer=False)
            # Note that setting host ensures we query the same name
            # for A as we did for AAAA.
            host = v6.qname
            canonical_name = v6.canonical_name.to_text(True)
            if v6.rrset is not None:
                for rdata in v6.rrset:
                    v6addrs.append(rdata.address)
        if family == socket.AF_INET or family == socket.AF_UNSPEC:
            v4 = _resolver.resolve(host, dns.rdatatype.A,
                                   raise_on_no_answer=False)
            host = v4.qname
            canonical_name = v4.canonical_name.to_text(True)
            if v4.rrset is not None:
                for rdata in v4.rrset:
                    v4addrs.append(rdata.address)
    except dns.resolver.NXDOMAIN:
        raise socket.gaierror(socket.EAI_NONAME, 'Name or service not known')
    except Exception:
        # We raise EAI_AGAIN here as the failure may be temporary
        # (e.g. a timeout) and EAI_SYSTEM isn't defined on Windows.
        # [Issue #416]
        raise socket.gaierror(socket.EAI_AGAIN,
                              'Temporary failure in name resolution')
    port = None
    try:
        # Is it a port literal?
        if service is None:
            port = 0
        else:
            port = int(service)
    except Exception:
        if flags & socket.AI_NUMERICSERV == 0:
            try:
                port = socket.getservbyname(service)
            except Exception:
                pass
    if port is None:
        raise socket.gaierror(socket.EAI_NONAME, 'Name or service not known')
    tuples = []
    if socktype == 0:
        socktypes = [socket.SOCK_DGRAM, socket.SOCK_STREAM]
    else:
        socktypes = [socktype]
    if flags & socket.AI_CANONNAME != 0:
        cname = canonical_name
    else:
        cname = ''
    if family == socket.AF_INET6 or family == socket.AF_UNSPEC:
        for addr in v6addrs:
            for socktype in socktypes:
                for proto in _protocols_for_socktype[socktype]:
                    tuples.append((socket.AF_INET6, socktype, proto,
                                   cname, (addr, port, 0, 0)))
    if family == socket.AF_INET or family == socket.AF_UNSPEC:
        for addr in v4addrs:
            for socktype in socktypes:
                for proto in _protocols_for_socktype[socktype]:
                    tuples.append((socket.AF_INET, socktype, proto,
                                   cname, (addr, port)))
    if len(tuples) == 0:
        raise socket.gaierror(socket.EAI_NONAME, 'Name or service not known')
    return tuples",host is None and service is None,host is None is service
netzob,https://github.com/netzob/netzob/tree/master/netzob/src/netzob/Inference/Vocabulary/FormatOperations/FieldSplitAligned/FieldSplitAligned.py,FieldSplitAligned,_getFieldValuesWithTag$608,"def _getFieldValuesWithTag(self, field, semanticTagsForEachMessage, tag):
        if field is None:
            raise TypeError(""Field cannot be None"")
        if semanticTagsForEachMessage is None:
            raise TypeError(""SemanticTagsForEachMessage cannot be None"")
        if tag is None:
            raise TypeError(""Tag cannot be None"")

        values = []

        # Retrieve value of each message in current field tagged with requested tag
        for message, tagsInMessage in list(semanticTagsForEachMessage.items()):
            initial = None
            end = None

            for tagIndex in sorted(tagsInMessage.keys()):
                tagName = tagsInMessage[tagIndex]
                if initial is None and tagName == tag:
                    initial = tagIndex
                elif initial is not None and tagName != tag:
                    end = tagIndex
                    break

            if initial is not None and end is None:
                end = sorted(tagsInMessage.keys())[-1] + 1
            if initial is not None and end is not None:
                values.append(message.getStringData()[initial:end])

                for i in range(initial, end):
                    del tagsInMessage[i]

        if b"""" not in values and len(
                list(semanticTagsForEachMessage.keys())) > len(values):
            values.append(b"""")

        return values",initial is not None and end is None,initial is not None is end
netzob,https://github.com/netzob/netzob/tree/master/netzob/src/netzob/Inference/Vocabulary/FormatOperations/FieldSplitAligned/FieldSplitAligned.py,FieldSplitAligned,_getFieldValuesWithTag$608,"def _getFieldValuesWithTag(self, field, semanticTagsForEachMessage, tag):
        if field is None:
            raise TypeError(""Field cannot be None"")
        if semanticTagsForEachMessage is None:
            raise TypeError(""SemanticTagsForEachMessage cannot be None"")
        if tag is None:
            raise TypeError(""Tag cannot be None"")

        values = []

        # Retrieve value of each message in current field tagged with requested tag
        for message, tagsInMessage in list(semanticTagsForEachMessage.items()):
            initial = None
            end = None

            for tagIndex in sorted(tagsInMessage.keys()):
                tagName = tagsInMessage[tagIndex]
                if initial is None and tagName == tag:
                    initial = tagIndex
                elif initial is not None and tagName != tag:
                    end = tagIndex
                    break

            if initial is not None and end is None:
                end = sorted(tagsInMessage.keys())[-1] + 1
            if initial is not None and end is not None:
                values.append(message.getStringData()[initial:end])

                for i in range(initial, end):
                    del tagsInMessage[i]

        if b"""" not in values and len(
                list(semanticTagsForEachMessage.keys())) > len(values):
            values.append(b"""")

        return values",initial is not None and end is not None,initial is not None is not end
Hypernets,https://github.com/DataCanvasIO/Hypernets/tree/master/hypernets/pipeline/base.py,ColumnTransformer,__init__$117,"def __init__(self, remainder='drop', sparse_threshold=0.3, n_jobs=None, transformer_weights=None, space=None,
                 name=None, **hyperparams):
        if remainder is not None and remainder != 'drop':
            hyperparams['remainder'] = remainder
        if sparse_threshold is not None and sparse_threshold != 0.3:
            hyperparams['sparse_threshold'] = sparse_threshold
        if n_jobs is not None:
            hyperparams['n_jobs'] = n_jobs
        if transformer_weights is not None:
            hyperparams['transformer_weights'] = transformer_weights

        ComposeTransformer.__init__(self, space, name, **hyperparams)",remainder is not None and remainder != 'drop',None is not remainder != 'drop'
Hypernets,https://github.com/DataCanvasIO/Hypernets/tree/master/hypernets/pipeline/base.py,ColumnTransformer,__init__$117,"def __init__(self, remainder='drop', sparse_threshold=0.3, n_jobs=None, transformer_weights=None, space=None,
                 name=None, **hyperparams):
        if remainder is not None and remainder != 'drop':
            hyperparams['remainder'] = remainder
        if sparse_threshold is not None and sparse_threshold != 0.3:
            hyperparams['sparse_threshold'] = sparse_threshold
        if n_jobs is not None:
            hyperparams['n_jobs'] = n_jobs
        if transformer_weights is not None:
            hyperparams['transformer_weights'] = transformer_weights

        ComposeTransformer.__init__(self, space, name, **hyperparams)",sparse_threshold is not None and sparse_threshold != 0.3,None is not sparse_threshold != 0.3
SlowFast,https://github.com/facebookresearch/SlowFast/tree/master/slowfast/utils/ava_evaluation/np_box_list.py,BoxList,__init__$39,"def __init__(self, data):
        """"""Constructs box collection.

        Args:
          data: a numpy array of shape [N, 4] representing box coordinates

        Raises:
          ValueError: if bbox data is not a numpy array
          ValueError: if invalid dimensions for bbox data
        """"""
        if not isinstance(data, np.ndarray):
            raise ValueError(""data must be a numpy array."")
        if len(data.shape) != 2 or data.shape[1] != 4:
            raise ValueError(""Invalid dimensions for box data."")
        if data.dtype != np.float32 and data.dtype != np.float64:
            raise ValueError(
                ""Invalid data type for box data: float is required.""
            )
        if not self._is_valid_boxes(data):
            raise ValueError(
                ""Invalid box data. data must be a numpy array of ""
                ""N*[y_min, x_min, y_max, x_max]""
            )
        self.data = {""boxes"": data}",data.dtype != np.float32 and data.dtype != np.float64,np.float32 != data.dtype != np.float64
scipy,https://github.com/scipy/scipy/tree/master/scipy/ndimage/_morphology.py,,grey_opening$1372,"def grey_opening(input, size=None, footprint=None, structure=None,
                 output=None, mode=""reflect"", cval=0.0, origin=0):
    """"""
    Multidimensional grayscale opening.

    A grayscale opening consists in the succession of a grayscale erosion,
    and a grayscale dilation.

    Parameters
    ----------
    input : array_like
        Array over which the grayscale opening is to be computed.
    size : tuple of ints
        Shape of a flat and full structuring element used for the grayscale
        opening. Optional if `footprint` or `structure` is provided.
    footprint : array of ints, optional
        Positions of non-infinite elements of a flat structuring element
        used for the grayscale opening.
    structure : array of ints, optional
        Structuring element used for the grayscale opening. `structure`
        may be a non-flat structuring element.
    output : array, optional
        An array used for storing the output of the opening may be provided.
    mode : {'reflect', 'constant', 'nearest', 'mirror', 'wrap'}, optional
        The `mode` parameter determines how the array borders are
        handled, where `cval` is the value when mode is equal to
        'constant'. Default is 'reflect'
    cval : scalar, optional
        Value to fill past edges of input if `mode` is 'constant'. Default
        is 0.0.
    origin : scalar, optional
        The `origin` parameter controls the placement of the filter.
        Default 0

    Returns
    -------
    grey_opening : ndarray
        Result of the grayscale opening of `input` with `structure`.

    See also
    --------
    binary_opening, grey_dilation, grey_erosion, grey_closing
    generate_binary_structure

    Notes
    -----
    The action of a grayscale opening with a flat structuring element amounts
    to smoothen high local maxima, whereas binary opening erases small objects.

    References
    ----------
    .. [1] https://en.wikipedia.org/wiki/Mathematical_morphology

    Examples
    --------
    >>> from scipy import ndimage
    >>> a = np.arange(36).reshape((6,6))
    >>> a[3, 3] = 50
    >>> a
    array([[ 0,  1,  2,  3,  4,  5],
           [ 6,  7,  8,  9, 10, 11],
           [12, 13, 14, 15, 16, 17],
           [18, 19, 20, 50, 22, 23],
           [24, 25, 26, 27, 28, 29],
           [30, 31, 32, 33, 34, 35]])
    >>> ndimage.grey_opening(a, size=(3,3))
    array([[ 0,  1,  2,  3,  4,  4],
           [ 6,  7,  8,  9, 10, 10],
           [12, 13, 14, 15, 16, 16],
           [18, 19, 20, 22, 22, 22],
           [24, 25, 26, 27, 28, 28],
           [24, 25, 26, 27, 28, 28]])
    >>> # Note that the local maximum a[3,3] has disappeared

    """"""
    if (size is not None) and (footprint is not None):
        warnings.warn(""ignoring size because footprint is set"", UserWarning, stacklevel=2)
    tmp = grey_erosion(input, size, footprint, structure, None, mode,
                       cval, origin)
    return grey_dilation(tmp, size, footprint, structure, output, mode,
                         cval, origin)",size is not None and footprint is not None,size is not None is not footprint
decaNLP,https://github.com/salesforce/decaNLP/tree/master/text/torchtext/datasets/generic.py,WikiSQL,__init__$514,"def __init__(self, path, field, query_as_question=False, subsample=None, **kwargs):
        fields = [(x, field) for x in self.fields]
        FIELD = data.Field(batch_first=True, use_vocab=False, sequential=False, 
            lower=False, numerical=True, eos_token=field.eos_token, init_token=field.init_token)
        fields.append(('wikisql_id', FIELD))


        cache_name = os.path.join(os.path.dirname(path), '.cache', 'query_as_question' if query_as_question else 'query_as_context', os.path.basename(path), str(subsample))
        if os.path.exists(cache_name):
            print(f'Loading cached data from {cache_name}')
            examples, all_answers = torch.load(cache_name)
        else:

            expanded_path = os.path.expanduser(path)
            table_path = os.path.splitext(expanded_path)
            table_path = table_path[0] + '.tables' + table_path[1]
           
            with open(table_path) as tables_file:
                tables = [json.loads(line) for line in tables_file]
                id_to_tables = {x['id']: x for x in tables}

            all_answers = []
            examples = []
            with open(expanded_path) as example_file:
                for idx, line in enumerate(example_file):
                    entry = json.loads(line)
                    human_query = entry['question']
                    table = id_to_tables[entry['table_id']]
                    sql = entry['sql']
                    header = table['header']
                    answer = repr(Query.from_dict(sql, header))
                    context = (f'The table has columns {"", "".join(table[""header""])} ' +
                               f'and key words {"", "".join(Query.agg_ops[1:] + Query.cond_ops + Query.syms)}')
                    if query_as_question:
                        question = human_query
                    else:
                        question = 'What is the translation from English to SQL?'
                        context += f'-- {human_query}'  
                    context_question = get_context_question(context, question) 
                    ex = data.Example.fromlist([context, question, answer, CONTEXT_SPECIAL, QUESTION_SPECIAL, context_question, idx], fields)
                    examples.append(ex)
                    all_answers.append({'sql': sql, 'header': header, 'answer': answer, 'table': table})
                    if subsample is not None and len(examples) > subsample:
                        break

            os.makedirs(os.path.dirname(cache_name), exist_ok=True)
            print(f'Caching data to {cache_name}')
            torch.save((examples, all_answers), cache_name)

        super(WikiSQL, self).__init__(examples, fields, **kwargs)
        self.all_answers = all_answers",subsample is not None and len(examples) > subsample,None is not subsample < len(examples)
python-dotenv,https://github.com/theskumar/python-dotenv/tree/master/src/dotenv/main.py,,dotenv_values$335,"def dotenv_values(
    dotenv_path: Union[str, _PathLike, None] = None,
    stream: Optional[IO[str]] = None,
    verbose: bool = False,
    interpolate: bool = True,
    encoding: Optional[str] = ""utf-8"",
) -> Dict[str, Optional[str]]:
    """"""
    Parse a .env file and return its content as a dict.

    - *dotenv_path*: absolute or relative path to .env file.
    - *stream*: `StringIO` object with .env content, used if `dotenv_path` is `None`.
    - *verbose*: whether to output a warning the .env file is missing. Defaults to
      `False`.
      in `.env` file.  Defaults to `False`.
    - *encoding*: encoding to be used to read the file.

    If both `dotenv_path` and `stream`, `find_dotenv()` is used to find the .env file.
    """"""
    if dotenv_path is None and stream is None:
        dotenv_path = find_dotenv()

    return DotEnv(
        dotenv_path=dotenv_path,
        stream=stream,
        verbose=verbose,
        interpolate=interpolate,
        override=True,
        encoding=encoding,
    ).dict()",dotenv_path is None and stream is None,dotenv_path is None is stream
text_renderer,https://github.com/Sanster/text_renderer/tree/master/libs/config.py,,check_fraction$15,"def check_fraction(cfg, name):
    """"""
    Check whether sum of all fractions in cfg equal to 1
    :param cfg: noise/line cfg
    """"""
    if not cfg.enable:
        return

    sum = 0
    for k, v in cfg.items():
        if k not in ['enable', 'fraction']:
            if v.enable:
                sum += v.fraction

    if sum != 0 and sum != 1:
        print('Sum of %s enabled item\'s fraction not equal to 1' % name)
        exit(-1)",sum != 0 and sum != 1,0 != sum != 1
Machine-Learning-with-Python,https://github.com/devAmoghS/Machine-Learning-with-Python/tree/master/k_means_clustering/utils.py,,bottom_up_cluster$113,"def bottom_up_cluster(inputs, distance_agg=min):
    # start with every input leaf cluster
    clusters = [input for input in inputs]

    # as long as we have more than one cluster left...
    while len(clusters) > 1:
        # find the two closest clusters
        c1, c2 = min([(cluster1, cluster2)
                      for i, cluster1 in enumerate(clusters)
                      for cluster2 in clusters[:i]],
                     key=lambda p: cluster_distance(p[0], p[1], distance_agg))

        # remove them from the list of clusters
        clusters = [c for c in clusters if c != c1 and c != c2]

        # merge them, using merge _order = # of cluster left
        merged_cluster = (len(clusters), [c1, c2])

        # add their merge
        clusters.append(merged_cluster)

    # when there is only one cluster left, return it
    return clusters[0]",c != c1 and c != c2,c1 != c != c2
quay,https://github.com/quay/quay/tree/master/data/logs_model/table_logs_model.py,TableLogsModel,lookup_logs$41,"def lookup_logs(
        self,
        start_datetime,
        end_datetime,
        performer_name=None,
        repository_name=None,
        namespace_name=None,
        filter_kinds=None,
        page_token=None,
        max_page_count=None,
    ):
        if filter_kinds is not None:
            assert all(isinstance(kind_name, str) for kind_name in filter_kinds)

        assert start_datetime is not None
        assert end_datetime is not None

        repository = None
        if repository_name and namespace_name:
            repository = model.repository.get_repository(namespace_name, repository_name)
            assert repository

        performer = None
        if performer_name:
            performer = model.user.get_user(performer_name)
            assert performer

        def get_logs(m, page_token):
            logs_query = model.log.get_logs_query(
                start_datetime,
                end_datetime,
                performer=performer,
                repository=repository,
                namespace=namespace_name,
                ignore=filter_kinds,
                model=m,
            )

            logs, next_page_token = model.modelutil.paginate(
                logs_query,
                m,
                descending=True,
                page_token=page_token,
                limit=20,
                max_page=max_page_count,
                sort_field_name=""datetime"",
            )

            return logs, next_page_token

        TOKEN_TABLE_ID = ""tti""
        table_index = 0
        logs = []
        next_page_token = page_token or None

        # Skip empty pages (empty table)
        while len(logs) == 0 and table_index < len(LOG_MODELS) - 1:
            table_specified = (
                next_page_token is not None and next_page_token.get(TOKEN_TABLE_ID) is not None
            )
            if table_specified:
                table_index = next_page_token.get(TOKEN_TABLE_ID)

            logs_result, next_page_token = get_logs(LOG_MODELS[table_index], next_page_token)
            logs.extend(logs_result)

            if next_page_token is None and table_index < len(LOG_MODELS) - 1:
                next_page_token = {TOKEN_TABLE_ID: table_index + 1}

        return LogEntriesPage([Log.for_logentry(log) for log in logs], next_page_token)",next_page_token is not None and next_page_token.get(TOKEN_TABLE_ID) is not None,next_page_token is not None is not next_page_token.get(TOKEN_TABLE_ID)
LIFT,https://github.com/cvlab-epfl/LIFT/tree/master/python-code/Utils/lasagne_tools.py,TransformerLayer,__init__$625,"def __init__(self, incoming, localization_network, out_height, out_width,
                 **kwargs):
        super(TransformerLayer, self).__init__(
            [incoming, localization_network], **kwargs)
        self.out_height = out_height
        self.out_width = out_width

        input_shp, loc_shp = self.input_shapes

        if loc_shp[-1] != 6 or (len(loc_shp) != 2 and len(loc_shp) != 3):
            raise ValueError(""The localization network must have ""
                             ""output shape: (batch_size, 6)"")
        if len(input_shp) != 4:
            raise ValueError(""The input network must have a 4-dimensional ""
                             ""output shape: (batch_size, num_input_channels, ""
                             ""input_rows, input_columns)."")",len(loc_shp) != 2 and len(loc_shp) != 3,2 != len(loc_shp) != 3
scikit-multiflow,https://github.com/scikit-multiflow/scikit-multiflow/tree/master/src/skmultiflow/trees/hoeffding_tree_regressor.py,HoeffdingTreeRegressor,_attempt_to_split$433,"def _attempt_to_split(self, node, parent, parent_idx: int):
        """"""Attempt to split a node.

        If the samples seen so far are not from the same class then:

        1. Find split candidates and select the top 2.
        2. Compute the Hoeffding bound.
        3. If the difference between the top 2 split candidates is larger than the Hoeffding bound:
           3.1 Replace the leaf node by a split node.
           3.2 Add a new leaf node on each branch of the new split node.
           3.3 Update tree's metrics

        Optional: Disable poor attribute. Depends on the tree's configuration.

        Parameters
        ----------
        node:
            The node to evaluate.
        parent: SplitNode
            The node's parent.
        parent_idx: int
            Parent node's branch index.

        """"""
        split_criterion = VarianceReductionSplitCriterion()
        best_split_suggestions = node.get_best_split_suggestions(split_criterion, self)
        best_split_suggestions.sort(key=attrgetter('merit'))
        should_split = False
        if len(best_split_suggestions) < 2:
            should_split = len(best_split_suggestions) > 0
        else:
            hoeffding_bound = self._hoeffding_bound(
                split_criterion.get_range_of_merit(node.stats), self.split_confidence,
                node.total_weight)
            best_suggestion = best_split_suggestions[-1]
            second_best_suggestion = best_split_suggestions[-2]
            if best_suggestion.merit > 0.0 and \
                    (second_best_suggestion.merit / best_suggestion.merit < 1 - hoeffding_bound
                        or hoeffding_bound < self.tie_threshold):
                should_split = True
            if self.remove_poor_atts:
                poor_atts = set()
                best_ratio = second_best_suggestion.merit / best_suggestion.merit

                # Add any poor attribute to set
                for i in range(len(best_split_suggestions)):
                    if best_split_suggestions[i].split_test is not None:
                        split_atts = best_split_suggestions[i].split_test.\
                            get_atts_test_depends_on()
                        if len(split_atts) == 1:
                            if (best_split_suggestions[i].merit / best_suggestion.merit
                                    < best_ratio - 2 * hoeffding_bound):
                                poor_atts.add(int(split_atts[0]))
                for poor_att in poor_atts:
                    node.disable_attribute(poor_att)
        if should_split:
            split_decision = best_split_suggestions[-1]
            if split_decision.split_test is None:
                # Preprune - null wins
                self._deactivate_learning_node(node, parent, parent_idx)
            else:
                new_split = self._new_split_node(split_decision.split_test, node.stats)
                for i in range(split_decision.num_splits()):
                    new_child = self._new_learning_node(
                        split_decision.resulting_stats_from_split(i), node)
                    new_split.set_child(i, new_child)
                self._active_leaf_node_cnt -= 1
                self._decision_node_cnt += 1
                self._active_leaf_node_cnt += split_decision.num_splits()
                if parent is None:
                    self._tree_root = new_split
                else:
                    parent.set_child(parent_idx, new_split)
            # Manage memory
            self._enforce_tracker_limit()
        elif len(best_split_suggestions) >= 2 and best_split_suggestions[-1].merit > 0 and \
                best_split_suggestions[-2].merit > 0:
            last_check_ratio = best_split_suggestions[-2].merit / best_split_suggestions[-1].merit
            last_check_sdr = best_split_suggestions[-1].merit

            node.manage_memory(split_criterion, last_check_ratio, last_check_sdr, hoeffding_bound)",len(best_split_suggestions) >= 2 and best_split_suggestions[-1].merit > 0 and (best_split_suggestions[-2].merit > 0),len(best_split_suggestions) >= 2 and best_split_suggestions[-1].merit > 0 < best_split_suggestions[-2].merit
swift,https://github.com/openstack/swift/tree/master/swift/obj/reconstructor.py,ObjectReconstructor,_iter_nodes_for_frag$862,"def _iter_nodes_for_frag(self, policy, partition, node):
        """"""
        Generate a priority list of nodes that can sync to the given node.

        The primary node is always the highest priority, after that we'll use
        handoffs.

        To avoid conflicts placing frags we'll skip through the handoffs and
        only yield back those that are offset equal to the given primary
        node index.

        Nodes returned from this iterator will have 'backend_index' set.
        """"""
        node['backend_index'] = policy.get_backend_index(node['index'])
        yield node
        count = 0
        for handoff_node in policy.object_ring.get_more_nodes(partition):
            handoff_backend_index = policy.get_backend_index(
                handoff_node['handoff_index'])
            if handoff_backend_index == node['backend_index']:
                if (self.rebuild_handoff_node_count >= 0 and
                        count >= self.rebuild_handoff_node_count):
                    break
                handoff_node['backend_index'] = handoff_backend_index
                yield handoff_node
                count += 1",self.rebuild_handoff_node_count >= 0 and count >= self.rebuild_handoff_node_count,count >= self.rebuild_handoff_node_count >= 0
nematus,https://github.com/EdinburghNLP/nematus/tree/master/nematus/layers.py,GRUStep,__init__$187,"def __init__(self, 
                 input_size, 
                 state_size,
                 batch_size,
                 use_layer_norm=False,
                 legacy_bias_type=LegacyBiasType.NEMATUS_COMPAT_FALSE,
                 dropout_input=None,
                 dropout_state=None):
        init = tf.concat([initializers.ortho_weight(state_size),
                          initializers.ortho_weight(state_size)],
                         axis=1)
        self.state_to_gates = tf.compat.v1.get_variable('state_to_gates',
                                              initializer=init)
        if input_size > 0:
            init = tf.concat([initializers.norm_weight(input_size, state_size),
                              initializers.norm_weight(input_size, state_size)],
                             axis=1)
            self.input_to_gates = tf.compat.v1.get_variable('input_to_gates',
                                                  initializer=init)

        if input_size == 0 and legacy_bias_type == LegacyBiasType.NEMATUS_COMPAT_FALSE:
            self.gates_bias = None
        else:
            self.gates_bias = tf.compat.v1.get_variable('gates_bias', [2*state_size],
                                          initializer=tf.zeros_initializer)

        init = initializers.ortho_weight(state_size)
        self.state_to_proposal = tf.compat.v1.get_variable('state_to_proposal',
                                                 initializer=init)
        if input_size > 0:
            init = initializers.norm_weight(input_size, state_size)
            self.input_to_proposal = tf.compat.v1.get_variable('input_to_proposal',
                                                     initializer=init)

        if input_size == 0 and legacy_bias_type == LegacyBiasType.NEMATUS_COMPAT_FALSE:
            self.proposal_bias = None
        else:
            self.proposal_bias = tf.compat.v1.get_variable('proposal_bias', [state_size],
                                             initializer=tf.zeros_initializer)

        self.legacy_bias_type = legacy_bias_type
        self.use_layer_norm = use_layer_norm

        self.gates_state_norm = None
        self.proposal_state_norm = None
        self.gates_x_norm = None
        self.proposal_x_norm = None
        if self.use_layer_norm is not None and self.use_layer_norm is not False:
            with tf.compat.v1.variable_scope('gates_state_norm'):
                self.gates_state_norm = self.use_layer_norm(2*state_size)
            with tf.compat.v1.variable_scope('proposal_state_norm'):
                self.proposal_state_norm = self.use_layer_norm(state_size)
            if input_size > 0:
                with tf.compat.v1.variable_scope('gates_x_norm'):
                    self.gates_x_norm = self.use_layer_norm(2*state_size)
                with tf.compat.v1.variable_scope('proposal_x_norm'):
                    self.proposal_x_norm = self.use_layer_norm(state_size)

        # Create dropout masks for input values (reused at every timestep).
        if dropout_input == None:
            self.dropout_mask_input_to_gates = None
            self.dropout_mask_input_to_proposal = None
        else:
            ones = tf.ones([batch_size, input_size])
            self.dropout_mask_input_to_gates = dropout_input(ones)
            self.dropout_mask_input_to_proposal = dropout_input(ones)

        # Create dropout masks for state values (reused at every timestep).
        if dropout_state == None:
            self.dropout_mask_state_to_gates = None
            self.dropout_mask_state_to_proposal = None
        else:
            ones = tf.ones([batch_size, state_size])
            self.dropout_mask_state_to_gates = dropout_state(ones)
            self.dropout_mask_state_to_proposal = dropout_state(ones)",self.use_layer_norm is not None and self.use_layer_norm is not False,None is not self.use_layer_norm is not False
surreal,https://github.com/SurrealAI/surreal/tree/master/surreal/env/atari_wrappers.py,EpisodicLifeEnv,_step$70,"def _step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.was_real_done = done
        # check current lives, make loss of life terminal,
        # then update lives to handle bonus lives
        lives = self.env.unwrapped.ale.lives()
        if lives < self.lives and lives > 0:
            # for Qbert somtimes we stay in lives == 0 condtion for a few frames
            # so its important to keep lives > 0, so that we only reset once
            # the environment advertises done.
            done = True
        self.lives = lives
        return obs, reward, done, info",lives < self.lives and lives > 0,self.lives > lives > 0
veusz,https://github.com/veusz/veusz/tree/master/veusz/widgets/axis.py,Axis,computePlottedRange$518,"def computePlottedRange(self, force=False, overriderange=None):
        """"""Convert the range requested into a plotted range.""""""

        if self.docchangeset == self.document.changeset and not force:
            return

        s = self.settings
        if overriderange is None:
            self.plottedrange = [s.min, s.max]
        else:
            self.plottedrange = overriderange

        # match the scale of this axis to another
        matched = False
        if s.match != '':
            # locate widget we're matching
            # this is ensured to be an Axis
            try:
                widget = s.get('match').getReferredWidget()
            except utils.InvalidType:
                widget = None

            # this looks valid + sanity checks
            if (widget is not None and widget != self and
                widget.settings.match == ''):
                # update if out of date
                if widget.docchangeset != self.document.changeset:
                    widget.computePlottedRange()
                # copy the range
                self.plottedrange = list(widget.plottedrange)
                matched = True

        # automatic lookup of minimum
        if not matched and overriderange is None:
            if s.min == 'Auto':
                self.plottedrange[0] = self.autorange[0]
            if s.max == 'Auto':
                self.plottedrange[1] = self.autorange[1]

        # yuck, but sometimes it's true
        # tweak range to make sure things don't blow up further down the
        # line
        if ( abs(self.plottedrange[0] - self.plottedrange[1]) <
             ( abs(self.plottedrange[0]) + abs(self.plottedrange[1]) )*1e-12 ):
            self.plottedrange[1] = (
                self.plottedrange[0] + max(1., self.plottedrange[0]*0.1) )

        # handle axis values round the wrong way
        invertaxis = self.plottedrange[0] > self.plottedrange[1]
        if invertaxis:
            self.plottedrange = self.plottedrange[::-1]

        # make sure log axes don't blow up
        if s.log:
            if self.plottedrange[0] < 1e-99:
                self.plottedrange[0] = 1e-99
            if self.plottedrange[1] < 1e-99:
                self.plottedrange[1] = 1e-99
            if self.plottedrange[0] == self.plottedrange[1]:
                self.plottedrange[1] = self.plottedrange[0]*2

        s.get('autoRange').adjustPlottedRange(
            self.plottedrange, s.min=='Auto', s.max=='Auto', s.log, self.document)

        self.computeTicks()

        # invert bounds if axis was inverted
        if invertaxis:
            self.plottedrange = self.plottedrange[::-1]

        self.docchangeset = self.document.changeset",widget is not None and widget != self and (widget.settings.match == ''),None is not widget != self and widget.settings.match == ''
aws-data-wrangler,https://github.com/awslabs/aws-data-wrangler/tree/master/awswrangler/quicksight/_create.py,,create_athena_dataset$198,"def create_athena_dataset(
    name: str,
    database: Optional[str] = None,
    table: Optional[str] = None,
    sql: Optional[str] = None,
    sql_name: str = ""CustomSQL"",
    data_source_name: Optional[str] = None,
    data_source_arn: Optional[str] = None,
    import_mode: str = ""DIRECT_QUERY"",
    allowed_to_use: Optional[List[str]] = None,
    allowed_to_manage: Optional[List[str]] = None,
    logical_table_alias: str = ""LogicalTable"",
    rename_columns: Optional[Dict[str, str]] = None,
    cast_columns_types: Optional[Dict[str, str]] = None,
    tag_columns: Optional[Dict[str, List[Dict[str, Any]]]] = None,
    tags: Optional[Dict[str, str]] = None,
    account_id: Optional[str] = None,
    boto3_session: Optional[boto3.Session] = None,
    namespace: str = ""default"",
) -> str:
    """"""Create a QuickSight dataset.

    Note
    ----
    You will not be able to see the the dataset in the console
    if you not pass your username to one of the ``allowed_*`` arguments.

    Note
    ----
    You must pass ``database``/``table`` OR ``sql`` argument.

    Note
    ----
    You must pass ``data_source_name`` OR ``data_source_arn`` argument.

    Parameters
    ----------
    name : str
        Dataset name.
    database : str
        Athena's database name.
    table : str
        Athena's table name.
    sql : str
        Use a SQL query to define your table.
    sql_name : str
        Query name.
    data_source_name : str, optional
        QuickSight data source name.
    data_source_arn : str, optional
        QuickSight data source ARN.
    import_mode : str
        Indicates whether you want to import the data into SPICE.
        'SPICE'|'DIRECT_QUERY'
    tags : Dict[str, str], optional
        Key/Value collection to put on the Cluster.
        e.g. {""foo"": ""boo"", ""bar"": ""xoo""}
    allowed_to_use : optional
        List of usernames that will be allowed to see and use the data source.
        e.g. [""john"", ""Mary""]
    allowed_to_manage : optional
        List of usernames that will be allowed to see, use, update and delete the data source.
        e.g. [""Mary""]
    logical_table_alias : str
        A display name for the logical table.
    rename_columns : Dict[str, str], optional
        Dictionary to map column renames. e.g. {""old_name"": ""new_name"", ""old_name2"": ""new_name2""}
    cast_columns_types : Dict[str, str], optional
        Dictionary to map column casts. e.g. {""col_name"": ""STRING"", ""col_name2"": ""DECIMAL""}
        Valid types: 'STRING'|'INTEGER'|'DECIMAL'|'DATETIME'
    tag_columns : Dict[str, List[Dict[str, Any]]], optional
        Dictionary to map column tags.
        e.g. {""col_name"": [{ ""ColumnGeographicRole"": ""CITY"" }],
              ""col_name2"": [{ ""ColumnDescription"": { ""Text"": ""description"" }}]}
        Valid geospatial roles: 'COUNTRY'|'STATE'|'COUNTY'|'CITY'|'POSTCODE'|'LONGITUDE'|'LATITUDE'
    account_id : str, optional
        If None, the account ID will be inferred from your boto3 session.
    boto3_session : boto3.Session(), optional
        Boto3 Session. The default boto3 session will be used if boto3_session receive None.
    namespace : str
        The namespace. Currently, you should set this to default.

    Returns
    -------
    str
        Dataset ID.

    Examples
    --------
    >>> import awswrangler as wr
    >>> dataset_id = wr.quicksight.create_athena_dataset(
    ...     name=""..."",
    ...     database=""...""
    ...     table=""...""
    ...     data_source_name=""...""
    ...     allowed_to_manage=[""Mary""]
    ... )
    """"""
    if (data_source_name is None) and (data_source_arn is None):
        raise exceptions.InvalidArgument(""You must pass a not None data_source_name or data_source_arn argument."")
    if ((database is None) and (table is None)) and (sql is None):
        raise exceptions.InvalidArgument(""You must pass database/table OR sql argument."")
    if (database is not None) and (sql is not None):
        raise exceptions.InvalidArgument(
            ""If you provide sql argument, please include the database name inside the sql statement.""
            ""Do NOT pass in with database argument.""
        )
    session: boto3.Session = _utils.ensure_session(session=boto3_session)
    client: boto3.client = _utils.client(service_name=""quicksight"", session=session)
    if account_id is None:
        account_id = sts.get_account_id(boto3_session=session)
    if (data_source_arn is None) and (data_source_name is not None):
        data_source_arn = get_data_source_arn(name=data_source_name, account_id=account_id, boto3_session=session)
    if sql is not None:
        physical_table: Dict[str, Dict[str, Any]] = {
            ""CustomSql"": {
                ""DataSourceArn"": data_source_arn,
                ""Name"": sql_name,
                ""SqlQuery"": sql,
                ""Columns"": extract_athena_query_columns(
                    sql=sql,
                    data_source_arn=data_source_arn,  # type: ignore
                    account_id=account_id,
                    boto3_session=session,
                ),
            }
        }
    else:
        physical_table = {
            ""RelationalTable"": {
                ""DataSourceArn"": data_source_arn,
                ""Schema"": database,
                ""Name"": table,
                ""InputColumns"": extract_athena_table_columns(
                    database=database,  # type: ignore
                    table=table,  # type: ignore
                    boto3_session=session,
                ),
            }
        }
    table_uuid: str = uuid.uuid4().hex
    dataset_id: str = uuid.uuid4().hex
    args: Dict[str, Any] = {
        ""AwsAccountId"": account_id,
        ""DataSetId"": dataset_id,
        ""Name"": name,
        ""ImportMode"": import_mode,
        ""PhysicalTableMap"": {table_uuid: physical_table},
        ""LogicalTableMap"": {table_uuid: {""Alias"": logical_table_alias, ""Source"": {""PhysicalTableId"": table_uuid}}},
    }
    trans: List[Dict[str, Dict[str, Any]]] = _generate_transformations(
        rename_columns=rename_columns, cast_columns_types=cast_columns_types, tag_columns=tag_columns
    )
    if trans:
        args[""LogicalTableMap""][table_uuid][""DataTransforms""] = trans
    permissions: List[Dict[str, Union[str, List[str]]]] = _generate_permissions(
        resource=""dataset"",
        account_id=account_id,
        boto3_session=session,
        allowed_to_use=allowed_to_use,
        allowed_to_manage=allowed_to_manage,
        namespace=namespace,
    )
    if permissions:
        args[""Permissions""] = permissions
    if tags is not None:
        _tags: List[Dict[str, str]] = [{""Key"": k, ""Value"": v} for k, v in tags.items()]
        args[""Tags""] = _tags
    client.create_data_set(**args)
    return dataset_id",data_source_name is None and data_source_arn is None,data_source_name is None is data_source_arn
aws-data-wrangler,https://github.com/awslabs/aws-data-wrangler/tree/master/awswrangler/quicksight/_create.py,,create_athena_dataset$198,"def create_athena_dataset(
    name: str,
    database: Optional[str] = None,
    table: Optional[str] = None,
    sql: Optional[str] = None,
    sql_name: str = ""CustomSQL"",
    data_source_name: Optional[str] = None,
    data_source_arn: Optional[str] = None,
    import_mode: str = ""DIRECT_QUERY"",
    allowed_to_use: Optional[List[str]] = None,
    allowed_to_manage: Optional[List[str]] = None,
    logical_table_alias: str = ""LogicalTable"",
    rename_columns: Optional[Dict[str, str]] = None,
    cast_columns_types: Optional[Dict[str, str]] = None,
    tag_columns: Optional[Dict[str, List[Dict[str, Any]]]] = None,
    tags: Optional[Dict[str, str]] = None,
    account_id: Optional[str] = None,
    boto3_session: Optional[boto3.Session] = None,
    namespace: str = ""default"",
) -> str:
    """"""Create a QuickSight dataset.

    Note
    ----
    You will not be able to see the the dataset in the console
    if you not pass your username to one of the ``allowed_*`` arguments.

    Note
    ----
    You must pass ``database``/``table`` OR ``sql`` argument.

    Note
    ----
    You must pass ``data_source_name`` OR ``data_source_arn`` argument.

    Parameters
    ----------
    name : str
        Dataset name.
    database : str
        Athena's database name.
    table : str
        Athena's table name.
    sql : str
        Use a SQL query to define your table.
    sql_name : str
        Query name.
    data_source_name : str, optional
        QuickSight data source name.
    data_source_arn : str, optional
        QuickSight data source ARN.
    import_mode : str
        Indicates whether you want to import the data into SPICE.
        'SPICE'|'DIRECT_QUERY'
    tags : Dict[str, str], optional
        Key/Value collection to put on the Cluster.
        e.g. {""foo"": ""boo"", ""bar"": ""xoo""}
    allowed_to_use : optional
        List of usernames that will be allowed to see and use the data source.
        e.g. [""john"", ""Mary""]
    allowed_to_manage : optional
        List of usernames that will be allowed to see, use, update and delete the data source.
        e.g. [""Mary""]
    logical_table_alias : str
        A display name for the logical table.
    rename_columns : Dict[str, str], optional
        Dictionary to map column renames. e.g. {""old_name"": ""new_name"", ""old_name2"": ""new_name2""}
    cast_columns_types : Dict[str, str], optional
        Dictionary to map column casts. e.g. {""col_name"": ""STRING"", ""col_name2"": ""DECIMAL""}
        Valid types: 'STRING'|'INTEGER'|'DECIMAL'|'DATETIME'
    tag_columns : Dict[str, List[Dict[str, Any]]], optional
        Dictionary to map column tags.
        e.g. {""col_name"": [{ ""ColumnGeographicRole"": ""CITY"" }],
              ""col_name2"": [{ ""ColumnDescription"": { ""Text"": ""description"" }}]}
        Valid geospatial roles: 'COUNTRY'|'STATE'|'COUNTY'|'CITY'|'POSTCODE'|'LONGITUDE'|'LATITUDE'
    account_id : str, optional
        If None, the account ID will be inferred from your boto3 session.
    boto3_session : boto3.Session(), optional
        Boto3 Session. The default boto3 session will be used if boto3_session receive None.
    namespace : str
        The namespace. Currently, you should set this to default.

    Returns
    -------
    str
        Dataset ID.

    Examples
    --------
    >>> import awswrangler as wr
    >>> dataset_id = wr.quicksight.create_athena_dataset(
    ...     name=""..."",
    ...     database=""...""
    ...     table=""...""
    ...     data_source_name=""...""
    ...     allowed_to_manage=[""Mary""]
    ... )
    """"""
    if (data_source_name is None) and (data_source_arn is None):
        raise exceptions.InvalidArgument(""You must pass a not None data_source_name or data_source_arn argument."")
    if ((database is None) and (table is None)) and (sql is None):
        raise exceptions.InvalidArgument(""You must pass database/table OR sql argument."")
    if (database is not None) and (sql is not None):
        raise exceptions.InvalidArgument(
            ""If you provide sql argument, please include the database name inside the sql statement.""
            ""Do NOT pass in with database argument.""
        )
    session: boto3.Session = _utils.ensure_session(session=boto3_session)
    client: boto3.client = _utils.client(service_name=""quicksight"", session=session)
    if account_id is None:
        account_id = sts.get_account_id(boto3_session=session)
    if (data_source_arn is None) and (data_source_name is not None):
        data_source_arn = get_data_source_arn(name=data_source_name, account_id=account_id, boto3_session=session)
    if sql is not None:
        physical_table: Dict[str, Dict[str, Any]] = {
            ""CustomSql"": {
                ""DataSourceArn"": data_source_arn,
                ""Name"": sql_name,
                ""SqlQuery"": sql,
                ""Columns"": extract_athena_query_columns(
                    sql=sql,
                    data_source_arn=data_source_arn,  # type: ignore
                    account_id=account_id,
                    boto3_session=session,
                ),
            }
        }
    else:
        physical_table = {
            ""RelationalTable"": {
                ""DataSourceArn"": data_source_arn,
                ""Schema"": database,
                ""Name"": table,
                ""InputColumns"": extract_athena_table_columns(
                    database=database,  # type: ignore
                    table=table,  # type: ignore
                    boto3_session=session,
                ),
            }
        }
    table_uuid: str = uuid.uuid4().hex
    dataset_id: str = uuid.uuid4().hex
    args: Dict[str, Any] = {
        ""AwsAccountId"": account_id,
        ""DataSetId"": dataset_id,
        ""Name"": name,
        ""ImportMode"": import_mode,
        ""PhysicalTableMap"": {table_uuid: physical_table},
        ""LogicalTableMap"": {table_uuid: {""Alias"": logical_table_alias, ""Source"": {""PhysicalTableId"": table_uuid}}},
    }
    trans: List[Dict[str, Dict[str, Any]]] = _generate_transformations(
        rename_columns=rename_columns, cast_columns_types=cast_columns_types, tag_columns=tag_columns
    )
    if trans:
        args[""LogicalTableMap""][table_uuid][""DataTransforms""] = trans
    permissions: List[Dict[str, Union[str, List[str]]]] = _generate_permissions(
        resource=""dataset"",
        account_id=account_id,
        boto3_session=session,
        allowed_to_use=allowed_to_use,
        allowed_to_manage=allowed_to_manage,
        namespace=namespace,
    )
    if permissions:
        args[""Permissions""] = permissions
    if tags is not None:
        _tags: List[Dict[str, str]] = [{""Key"": k, ""Value"": v} for k, v in tags.items()]
        args[""Tags""] = _tags
    client.create_data_set(**args)
    return dataset_id",database is not None and sql is not None,database is not None is not sql
aws-data-wrangler,https://github.com/awslabs/aws-data-wrangler/tree/master/awswrangler/quicksight/_create.py,,create_athena_dataset$198,"def create_athena_dataset(
    name: str,
    database: Optional[str] = None,
    table: Optional[str] = None,
    sql: Optional[str] = None,
    sql_name: str = ""CustomSQL"",
    data_source_name: Optional[str] = None,
    data_source_arn: Optional[str] = None,
    import_mode: str = ""DIRECT_QUERY"",
    allowed_to_use: Optional[List[str]] = None,
    allowed_to_manage: Optional[List[str]] = None,
    logical_table_alias: str = ""LogicalTable"",
    rename_columns: Optional[Dict[str, str]] = None,
    cast_columns_types: Optional[Dict[str, str]] = None,
    tag_columns: Optional[Dict[str, List[Dict[str, Any]]]] = None,
    tags: Optional[Dict[str, str]] = None,
    account_id: Optional[str] = None,
    boto3_session: Optional[boto3.Session] = None,
    namespace: str = ""default"",
) -> str:
    """"""Create a QuickSight dataset.

    Note
    ----
    You will not be able to see the the dataset in the console
    if you not pass your username to one of the ``allowed_*`` arguments.

    Note
    ----
    You must pass ``database``/``table`` OR ``sql`` argument.

    Note
    ----
    You must pass ``data_source_name`` OR ``data_source_arn`` argument.

    Parameters
    ----------
    name : str
        Dataset name.
    database : str
        Athena's database name.
    table : str
        Athena's table name.
    sql : str
        Use a SQL query to define your table.
    sql_name : str
        Query name.
    data_source_name : str, optional
        QuickSight data source name.
    data_source_arn : str, optional
        QuickSight data source ARN.
    import_mode : str
        Indicates whether you want to import the data into SPICE.
        'SPICE'|'DIRECT_QUERY'
    tags : Dict[str, str], optional
        Key/Value collection to put on the Cluster.
        e.g. {""foo"": ""boo"", ""bar"": ""xoo""}
    allowed_to_use : optional
        List of usernames that will be allowed to see and use the data source.
        e.g. [""john"", ""Mary""]
    allowed_to_manage : optional
        List of usernames that will be allowed to see, use, update and delete the data source.
        e.g. [""Mary""]
    logical_table_alias : str
        A display name for the logical table.
    rename_columns : Dict[str, str], optional
        Dictionary to map column renames. e.g. {""old_name"": ""new_name"", ""old_name2"": ""new_name2""}
    cast_columns_types : Dict[str, str], optional
        Dictionary to map column casts. e.g. {""col_name"": ""STRING"", ""col_name2"": ""DECIMAL""}
        Valid types: 'STRING'|'INTEGER'|'DECIMAL'|'DATETIME'
    tag_columns : Dict[str, List[Dict[str, Any]]], optional
        Dictionary to map column tags.
        e.g. {""col_name"": [{ ""ColumnGeographicRole"": ""CITY"" }],
              ""col_name2"": [{ ""ColumnDescription"": { ""Text"": ""description"" }}]}
        Valid geospatial roles: 'COUNTRY'|'STATE'|'COUNTY'|'CITY'|'POSTCODE'|'LONGITUDE'|'LATITUDE'
    account_id : str, optional
        If None, the account ID will be inferred from your boto3 session.
    boto3_session : boto3.Session(), optional
        Boto3 Session. The default boto3 session will be used if boto3_session receive None.
    namespace : str
        The namespace. Currently, you should set this to default.

    Returns
    -------
    str
        Dataset ID.

    Examples
    --------
    >>> import awswrangler as wr
    >>> dataset_id = wr.quicksight.create_athena_dataset(
    ...     name=""..."",
    ...     database=""...""
    ...     table=""...""
    ...     data_source_name=""...""
    ...     allowed_to_manage=[""Mary""]
    ... )
    """"""
    if (data_source_name is None) and (data_source_arn is None):
        raise exceptions.InvalidArgument(""You must pass a not None data_source_name or data_source_arn argument."")
    if ((database is None) and (table is None)) and (sql is None):
        raise exceptions.InvalidArgument(""You must pass database/table OR sql argument."")
    if (database is not None) and (sql is not None):
        raise exceptions.InvalidArgument(
            ""If you provide sql argument, please include the database name inside the sql statement.""
            ""Do NOT pass in with database argument.""
        )
    session: boto3.Session = _utils.ensure_session(session=boto3_session)
    client: boto3.client = _utils.client(service_name=""quicksight"", session=session)
    if account_id is None:
        account_id = sts.get_account_id(boto3_session=session)
    if (data_source_arn is None) and (data_source_name is not None):
        data_source_arn = get_data_source_arn(name=data_source_name, account_id=account_id, boto3_session=session)
    if sql is not None:
        physical_table: Dict[str, Dict[str, Any]] = {
            ""CustomSql"": {
                ""DataSourceArn"": data_source_arn,
                ""Name"": sql_name,
                ""SqlQuery"": sql,
                ""Columns"": extract_athena_query_columns(
                    sql=sql,
                    data_source_arn=data_source_arn,  # type: ignore
                    account_id=account_id,
                    boto3_session=session,
                ),
            }
        }
    else:
        physical_table = {
            ""RelationalTable"": {
                ""DataSourceArn"": data_source_arn,
                ""Schema"": database,
                ""Name"": table,
                ""InputColumns"": extract_athena_table_columns(
                    database=database,  # type: ignore
                    table=table,  # type: ignore
                    boto3_session=session,
                ),
            }
        }
    table_uuid: str = uuid.uuid4().hex
    dataset_id: str = uuid.uuid4().hex
    args: Dict[str, Any] = {
        ""AwsAccountId"": account_id,
        ""DataSetId"": dataset_id,
        ""Name"": name,
        ""ImportMode"": import_mode,
        ""PhysicalTableMap"": {table_uuid: physical_table},
        ""LogicalTableMap"": {table_uuid: {""Alias"": logical_table_alias, ""Source"": {""PhysicalTableId"": table_uuid}}},
    }
    trans: List[Dict[str, Dict[str, Any]]] = _generate_transformations(
        rename_columns=rename_columns, cast_columns_types=cast_columns_types, tag_columns=tag_columns
    )
    if trans:
        args[""LogicalTableMap""][table_uuid][""DataTransforms""] = trans
    permissions: List[Dict[str, Union[str, List[str]]]] = _generate_permissions(
        resource=""dataset"",
        account_id=account_id,
        boto3_session=session,
        allowed_to_use=allowed_to_use,
        allowed_to_manage=allowed_to_manage,
        namespace=namespace,
    )
    if permissions:
        args[""Permissions""] = permissions
    if tags is not None:
        _tags: List[Dict[str, str]] = [{""Key"": k, ""Value"": v} for k, v in tags.items()]
        args[""Tags""] = _tags
    client.create_data_set(**args)
    return dataset_id",data_source_arn is None and data_source_name is not None,data_source_arn is None is not data_source_name
aws-data-wrangler,https://github.com/awslabs/aws-data-wrangler/tree/master/awswrangler/quicksight/_create.py,,create_athena_dataset$198,"def create_athena_dataset(
    name: str,
    database: Optional[str] = None,
    table: Optional[str] = None,
    sql: Optional[str] = None,
    sql_name: str = ""CustomSQL"",
    data_source_name: Optional[str] = None,
    data_source_arn: Optional[str] = None,
    import_mode: str = ""DIRECT_QUERY"",
    allowed_to_use: Optional[List[str]] = None,
    allowed_to_manage: Optional[List[str]] = None,
    logical_table_alias: str = ""LogicalTable"",
    rename_columns: Optional[Dict[str, str]] = None,
    cast_columns_types: Optional[Dict[str, str]] = None,
    tag_columns: Optional[Dict[str, List[Dict[str, Any]]]] = None,
    tags: Optional[Dict[str, str]] = None,
    account_id: Optional[str] = None,
    boto3_session: Optional[boto3.Session] = None,
    namespace: str = ""default"",
) -> str:
    """"""Create a QuickSight dataset.

    Note
    ----
    You will not be able to see the the dataset in the console
    if you not pass your username to one of the ``allowed_*`` arguments.

    Note
    ----
    You must pass ``database``/``table`` OR ``sql`` argument.

    Note
    ----
    You must pass ``data_source_name`` OR ``data_source_arn`` argument.

    Parameters
    ----------
    name : str
        Dataset name.
    database : str
        Athena's database name.
    table : str
        Athena's table name.
    sql : str
        Use a SQL query to define your table.
    sql_name : str
        Query name.
    data_source_name : str, optional
        QuickSight data source name.
    data_source_arn : str, optional
        QuickSight data source ARN.
    import_mode : str
        Indicates whether you want to import the data into SPICE.
        'SPICE'|'DIRECT_QUERY'
    tags : Dict[str, str], optional
        Key/Value collection to put on the Cluster.
        e.g. {""foo"": ""boo"", ""bar"": ""xoo""}
    allowed_to_use : optional
        List of usernames that will be allowed to see and use the data source.
        e.g. [""john"", ""Mary""]
    allowed_to_manage : optional
        List of usernames that will be allowed to see, use, update and delete the data source.
        e.g. [""Mary""]
    logical_table_alias : str
        A display name for the logical table.
    rename_columns : Dict[str, str], optional
        Dictionary to map column renames. e.g. {""old_name"": ""new_name"", ""old_name2"": ""new_name2""}
    cast_columns_types : Dict[str, str], optional
        Dictionary to map column casts. e.g. {""col_name"": ""STRING"", ""col_name2"": ""DECIMAL""}
        Valid types: 'STRING'|'INTEGER'|'DECIMAL'|'DATETIME'
    tag_columns : Dict[str, List[Dict[str, Any]]], optional
        Dictionary to map column tags.
        e.g. {""col_name"": [{ ""ColumnGeographicRole"": ""CITY"" }],
              ""col_name2"": [{ ""ColumnDescription"": { ""Text"": ""description"" }}]}
        Valid geospatial roles: 'COUNTRY'|'STATE'|'COUNTY'|'CITY'|'POSTCODE'|'LONGITUDE'|'LATITUDE'
    account_id : str, optional
        If None, the account ID will be inferred from your boto3 session.
    boto3_session : boto3.Session(), optional
        Boto3 Session. The default boto3 session will be used if boto3_session receive None.
    namespace : str
        The namespace. Currently, you should set this to default.

    Returns
    -------
    str
        Dataset ID.

    Examples
    --------
    >>> import awswrangler as wr
    >>> dataset_id = wr.quicksight.create_athena_dataset(
    ...     name=""..."",
    ...     database=""...""
    ...     table=""...""
    ...     data_source_name=""...""
    ...     allowed_to_manage=[""Mary""]
    ... )
    """"""
    if (data_source_name is None) and (data_source_arn is None):
        raise exceptions.InvalidArgument(""You must pass a not None data_source_name or data_source_arn argument."")
    if ((database is None) and (table is None)) and (sql is None):
        raise exceptions.InvalidArgument(""You must pass database/table OR sql argument."")
    if (database is not None) and (sql is not None):
        raise exceptions.InvalidArgument(
            ""If you provide sql argument, please include the database name inside the sql statement.""
            ""Do NOT pass in with database argument.""
        )
    session: boto3.Session = _utils.ensure_session(session=boto3_session)
    client: boto3.client = _utils.client(service_name=""quicksight"", session=session)
    if account_id is None:
        account_id = sts.get_account_id(boto3_session=session)
    if (data_source_arn is None) and (data_source_name is not None):
        data_source_arn = get_data_source_arn(name=data_source_name, account_id=account_id, boto3_session=session)
    if sql is not None:
        physical_table: Dict[str, Dict[str, Any]] = {
            ""CustomSql"": {
                ""DataSourceArn"": data_source_arn,
                ""Name"": sql_name,
                ""SqlQuery"": sql,
                ""Columns"": extract_athena_query_columns(
                    sql=sql,
                    data_source_arn=data_source_arn,  # type: ignore
                    account_id=account_id,
                    boto3_session=session,
                ),
            }
        }
    else:
        physical_table = {
            ""RelationalTable"": {
                ""DataSourceArn"": data_source_arn,
                ""Schema"": database,
                ""Name"": table,
                ""InputColumns"": extract_athena_table_columns(
                    database=database,  # type: ignore
                    table=table,  # type: ignore
                    boto3_session=session,
                ),
            }
        }
    table_uuid: str = uuid.uuid4().hex
    dataset_id: str = uuid.uuid4().hex
    args: Dict[str, Any] = {
        ""AwsAccountId"": account_id,
        ""DataSetId"": dataset_id,
        ""Name"": name,
        ""ImportMode"": import_mode,
        ""PhysicalTableMap"": {table_uuid: physical_table},
        ""LogicalTableMap"": {table_uuid: {""Alias"": logical_table_alias, ""Source"": {""PhysicalTableId"": table_uuid}}},
    }
    trans: List[Dict[str, Dict[str, Any]]] = _generate_transformations(
        rename_columns=rename_columns, cast_columns_types=cast_columns_types, tag_columns=tag_columns
    )
    if trans:
        args[""LogicalTableMap""][table_uuid][""DataTransforms""] = trans
    permissions: List[Dict[str, Union[str, List[str]]]] = _generate_permissions(
        resource=""dataset"",
        account_id=account_id,
        boto3_session=session,
        allowed_to_use=allowed_to_use,
        allowed_to_manage=allowed_to_manage,
        namespace=namespace,
    )
    if permissions:
        args[""Permissions""] = permissions
    if tags is not None:
        _tags: List[Dict[str, str]] = [{""Key"": k, ""Value"": v} for k, v in tags.items()]
        args[""Tags""] = _tags
    client.create_data_set(**args)
    return dataset_id",database is None and table is None,database is None is table
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/text/datasets/imikolov.py,Imikolov,_load_anno$147,"def _load_anno(self):
        self.data = []
        with tarfile.open(self.data_file) as tf:
            filename = './simple-examples/data/ptb.{}.txt'.format(self.mode)
            f = tf.extractfile(filename)

            UNK = self.word_idx['<unk>']
            for l in f:
                if self.data_type == 'NGRAM':
                    assert self.window_size > -1, 'Invalid gram length'
                    l = ['<s>'] + l.strip().split() + ['<e>']
                    if len(l) >= self.window_size:
                        l = [self.word_idx.get(w, UNK) for w in l]
                        for i in range(self.window_size, len(l) + 1):
                            self.data.append(tuple(l[i - self.window_size : i]))
                elif self.data_type == 'SEQ':
                    l = l.strip().split()
                    l = [self.word_idx.get(w, UNK) for w in l]
                    src_seq = [self.word_idx['<s>']] + l
                    trg_seq = l + [self.word_idx['<e>']]
                    if self.window_size > 0 and len(src_seq) > self.window_size:
                        continue
                    self.data.append((src_seq, trg_seq))
                else:
                    assert False, 'Unknow data type'",self.window_size > 0 and len(src_seq) > self.window_size,len(src_seq) > self.window_size > 0
pvlib-python,https://github.com/pvlib/pvlib-python/tree/master/pvlib/solarposition.py,,get_solarposition$34,"def get_solarposition(time, latitude, longitude,
                      altitude=None, pressure=None,
                      method='nrel_numpy',
                      temperature=12, **kwargs):
    """"""
    A convenience wrapper for the solar position calculators.

    Parameters
    ----------
    time : pandas.DatetimeIndex
        Must be localized or UTC will be assumed.

    latitude : float
        Latitude in decimal degrees. Positive north of equator, negative
        to south.

    longitude : float
        Longitude in decimal degrees. Positive east of prime meridian,
        negative to west.

    altitude : None or float, default None
        If None, computed from pressure. Assumed to be 0 m
        if pressure is also None.

    pressure : None or float, default None
        If None, computed from altitude. Assumed to be 101325 Pa
        if altitude is also None.

    method : string, default 'nrel_numpy'
        'nrel_numpy' uses an implementation of the NREL SPA algorithm
        described in [1] (default, recommended): :py:func:`spa_python`

        'nrel_numba' uses an implementation of the NREL SPA algorithm
        described in [1], but also compiles the code first:
        :py:func:`spa_python`

        'pyephem' uses the PyEphem package: :py:func:`pyephem`

        'ephemeris' uses the pvlib ephemeris code: :py:func:`ephemeris`

        'nrel_c' uses the NREL SPA C code [3]: :py:func:`spa_c`

    temperature : float, default 12
        Degrees C.

    kwargs
        Other keywords are passed to the solar position function
        specified by the ``method`` argument.

    References
    ----------
    .. [1] I. Reda and A. Andreas, Solar position algorithm for solar radiation
       applications. Solar Energy, vol. 76, no. 5, pp. 577-589, 2004.

    .. [2] I. Reda and A. Andreas, Corrigendum to Solar position algorithm for
       solar radiation applications. Solar Energy, vol. 81, no. 6, p. 838,
       2007.

    .. [3] NREL SPA code: http://rredc.nrel.gov/solar/codesandalgorithms/spa/
    """"""

    if altitude is None and pressure is None:
        altitude = 0.
        pressure = 101325.
    elif altitude is None:
        altitude = atmosphere.pres2alt(pressure)
    elif pressure is None:
        pressure = atmosphere.alt2pres(altitude)

    method = method.lower()
    if isinstance(time, dt.datetime):
        time = pd.DatetimeIndex([time, ])

    if method == 'nrel_c':
        ephem_df = spa_c(time, latitude, longitude, pressure, temperature,
                         **kwargs)
    elif method == 'nrel_numba':
        ephem_df = spa_python(time, latitude, longitude, altitude,
                              pressure, temperature,
                              how='numba', **kwargs)
    elif method == 'nrel_numpy':
        ephem_df = spa_python(time, latitude, longitude, altitude,
                              pressure, temperature,
                              how='numpy', **kwargs)
    elif method == 'pyephem':
        ephem_df = pyephem(time, latitude, longitude,
                           altitude=altitude,
                           pressure=pressure,
                           temperature=temperature, **kwargs)
    elif method == 'ephemeris':
        ephem_df = ephemeris(time, latitude, longitude, pressure, temperature,
                             **kwargs)
    else:
        raise ValueError('Invalid solar position method')

    return ephem_df",altitude is None and pressure is None,altitude is None is pressure
second.pytorch,https://github.com/traveller59/second.pytorch/tree/master/second/utils/check.py,,shape_mergeable$6,"def shape_mergeable(x, expected_shape):
    mergeable = True
    if is_array_like(x) and is_array_like(expected_shape):
        x = np.array(x)
        if len(x.shape) == len(expected_shape):
            for s, s_ex in zip(x.shape, expected_shape):
                if s_ex is not None and s != s_ex:
                    mergeable = False
                    break
    return mergeable",s_ex is not None and s != s_ex,None is not s_ex != s
vega,https://github.com/huawei-noah/vega/tree/master/vega/modules/arch/double_channels_arch.py,Conv2dDoubleChannelArchitecture,fit_weights$38,"def fit_weights(module, x):
        """"""Fit weight.""""""
        inputs = x[0] if isinstance(x, tuple) else x
        for name, weight in module.get_weights().items():
            if weight is None:
                continue
            in_channels_axis = 1 if is_torch_backend() else 2
            out_channels_axis = 0 if is_torch_backend() else 3
            if 'BatchNorm' in name:
                out_channels_diff = int(module.out_channels) - int(weight.shape[0])
                if out_channels_diff == 0:
                    continue
                padding = [0, out_channels_diff]
            else:
                groups = module.groups
                if groups == module.in_channels and module.out_channels < groups:
                    module.out_channels = groups
                in_channels_diff = int(inputs.shape[1]) - int(weight.shape[in_channels_axis] * module.groups)
                out_channels_diff = int(module.out_channels) - int(weight.shape[out_channels_axis])
                if in_channels_diff == 0 and out_channels_diff == 0:
                    continue
                padding = [0, 0, 0, 0, 0, 0, 0, 0]
                if groups == 1:
                    if in_channels_diff != 0:
                        padding[5] = in_channels_diff
                        module.in_channels += in_channels_diff
                else:
                    if in_channels_diff > 0:
                        in_channels_group_diff = int(in_channels_diff / groups)
                        padding[5] = in_channels_group_diff
                    elif in_channels_diff < 0:
                        module.groups = int(abs(in_channels_diff) / weight.shape[in_channels_axis])
                    module.in_channels += in_channels_diff
                if out_channels_diff != 0:
                    padding[-1] = out_channels_diff
            module.set_weights(name, ops.pad(weight, padding))
        return None",groups == module.in_channels and module.out_channels < groups,module.out_channels < groups == module.in_channels
vega,https://github.com/huawei-noah/vega/tree/master/vega/modules/arch/double_channels_arch.py,Conv2dDoubleChannelArchitecture,fit_weights$38,"def fit_weights(module, x):
        """"""Fit weight.""""""
        inputs = x[0] if isinstance(x, tuple) else x
        for name, weight in module.get_weights().items():
            if weight is None:
                continue
            in_channels_axis = 1 if is_torch_backend() else 2
            out_channels_axis = 0 if is_torch_backend() else 3
            if 'BatchNorm' in name:
                out_channels_diff = int(module.out_channels) - int(weight.shape[0])
                if out_channels_diff == 0:
                    continue
                padding = [0, out_channels_diff]
            else:
                groups = module.groups
                if groups == module.in_channels and module.out_channels < groups:
                    module.out_channels = groups
                in_channels_diff = int(inputs.shape[1]) - int(weight.shape[in_channels_axis] * module.groups)
                out_channels_diff = int(module.out_channels) - int(weight.shape[out_channels_axis])
                if in_channels_diff == 0 and out_channels_diff == 0:
                    continue
                padding = [0, 0, 0, 0, 0, 0, 0, 0]
                if groups == 1:
                    if in_channels_diff != 0:
                        padding[5] = in_channels_diff
                        module.in_channels += in_channels_diff
                else:
                    if in_channels_diff > 0:
                        in_channels_group_diff = int(in_channels_diff / groups)
                        padding[5] = in_channels_group_diff
                    elif in_channels_diff < 0:
                        module.groups = int(abs(in_channels_diff) / weight.shape[in_channels_axis])
                    module.in_channels += in_channels_diff
                if out_channels_diff != 0:
                    padding[-1] = out_channels_diff
            module.set_weights(name, ops.pad(weight, padding))
        return None",in_channels_diff == 0 and out_channels_diff == 0,in_channels_diff == 0 == out_channels_diff
borb,https://github.com/jorisschellekens/borb/tree/master/borb/pdf/canvas/layout/text/chunks_of_text.py,Span,__init__$34,"def __init__(
        self,
        chunks_of_text: typing.List[ChunkOfText] = [],
        vertical_alignment: Alignment = Alignment.TOP,
        horizontal_alignment: Alignment = Alignment.LEFT,
        border_top: bool = False,
        border_right: bool = False,
        border_bottom: bool = False,
        border_left: bool = False,
        border_color: Color = HexColor(""000000""),
        border_width: Decimal = Decimal(1),
        padding_top: Decimal = Decimal(0),
        padding_right: Decimal = Decimal(0),
        padding_bottom: Decimal = Decimal(0),
        padding_left: Decimal = Decimal(0),
        margin_top: typing.Optional[Decimal] = None,
        margin_right: typing.Optional[Decimal] = None,
        margin_bottom: typing.Optional[Decimal] = None,
        margin_left: typing.Optional[Decimal] = None,
        fixed_leading: typing.Optional[Decimal] = None,
        multiplied_leading: typing.Optional[Decimal] = None,
        background_color: typing.Optional[Color] = None,
        parent: typing.Optional[""LayoutElement""] = None,  # type: ignore [name-defined]
    ):

        # background color
        self._background_color: typing.Optional[Color] = background_color

        # borders
        self._border_color: Color = border_color
        self._border_width: Decimal = border_width
        self._border_top: bool = border_top
        self._border_right: bool = border_right
        self._border_bottom: bool = border_bottom
        self._border_left: bool = border_left

        # alignment
        self._horizontal_alignment = horizontal_alignment
        self._vertical_alignment = vertical_alignment

        # padding
        self._padding_top: Decimal = padding_top
        self._padding_right: Decimal = padding_right
        self._padding_bottom: Decimal = padding_bottom
        self._padding_left: Decimal = padding_left

        # margin
        self._margin_top: typing.Optional[Decimal] = margin_top
        self._margin_right: typing.Optional[Decimal] = margin_right
        self._margin_bottom: typing.Optional[Decimal] = margin_bottom
        self._margin_left: typing.Optional[Decimal] = margin_left

        # leading
        self._font_size: typing.Optional[Decimal] = None
        if fixed_leading is None and multiplied_leading is None:
            multiplied_leading = Decimal(1.2)
        self._fixed_leading: typing.Optional[Decimal] = fixed_leading
        self._multiplied_leading: typing.Optional[Decimal] = multiplied_leading

        # store chunks
        self._chunks_of_text: typing.List[ChunkOfText] = []
        for c in chunks_of_text:
            self.add(c)",fixed_leading is None and multiplied_leading is None,fixed_leading is None is multiplied_leading
IntelOwl,https://github.com/intelowlproject/IntelOwl/tree/master/api_app/analyzers_manager/observable_analyzers/honeydb.py,HoneyDB,set_params$18,"def set_params(self, params):
        self.analysis_type = params.get(""honeydb_analysis"", ""all"")
        self.endpoints = [
            ""scan_twitter"",
            ""ip_query"",
            ""ip_history"",
            ""internet_scanner"",
            ""ip_info"",
        ]
        if self.analysis_type not in self.endpoints and self.analysis_type != ""all"":
            raise AnalyzerConfigurationException(
                f""analysis_type is not valid: {self.analysis_type}""
            )

        # set secrets
        self.__api_key = self._secrets[""api_key_name""]
        self.__api_id = self._secrets[""api_id_name""]
        self.headers = {
            ""X-HoneyDb-ApiKey"": self.__api_key,
            ""X-HoneyDb-ApiId"": self.__api_id,
        }
        self.result = {}",self.analysis_type not in self.endpoints and self.analysis_type != 'all','all' != self.analysis_type not in self.endpoints
fastMRI,https://github.com/facebookresearch/fastMRI/tree/master/fastmri/data/transforms.py,,center_crop$138,"def center_crop(data: torch.Tensor, shape: Tuple[int, int]) -> torch.Tensor:
    """"""
    Apply a center crop to the input real image or batch of real images.

    Args:
        data: The input tensor to be center cropped. It should
            have at least 2 dimensions and the cropping is applied along the
            last two dimensions.
        shape: The output shape. The shape should be smaller
            than the corresponding dimensions of data.

    Returns:
        The center cropped image.
    """"""
    if not (0 < shape[0] <= data.shape[-2] and 0 < shape[1] <= data.shape[-1]):
        raise ValueError(""Invalid shapes."")

    w_from = (data.shape[-2] - shape[0]) // 2
    h_from = (data.shape[-1] - shape[1]) // 2
    w_to = w_from + shape[0]
    h_to = h_from + shape[1]

    return data[..., w_from:w_to, h_from:h_to]",0 < shape[0] <= data.shape[-2] and 0 < shape[1] <= data.shape[-1],data.shape[-2] >= shape[0] > 0 < shape[1] <= data.shape[-1]
angr,https://github.com/angr/angr/tree/master/angr/analyses/reaching_definitions/engine_ail.py,SimEngineRDAIL,_ail_handle_Return$243,"def _ail_handle_Return(self, stmt: ailment.Stmt.Return):  # pylint:disable=unused-argument

        codeloc = self._codeloc()
        size = self.project.arch.bits // 8

        cc = None
        if self.state.analysis.subject.type == SubjectType.Function:
            cc = self.state.analysis.subject.content.calling_convention
            # import ipdb; ipdb.set_trace()

        if cc is None:
            # fall back to the default calling convention
            cc_cls = DEFAULT_CC.get(self.project.arch.name, None)
            if cc_cls is None:
                l.warning(""Unknown default calling convention for architecture %s."", self.project.arch.name)
                cc = None
            else:
                cc = cc_cls(self.project.arch)

        if cc is not None:
            # callee-saved args
            for reg in self.arch.register_list:
                if (reg.general_purpose
                        and reg.name not in cc.CALLER_SAVED_REGS
                        and reg.name not in cc.ARG_REGS
                        and reg.vex_offset not in {self.arch.sp_offset, self.arch.bp_offset, self.arch.ip_offset, }
                        and (isinstance(cc.RETURN_VAL, SimRegArg) and reg.name != cc.RETURN_VAL.reg_name)
                ):
                    self.state.add_use(Register(reg.vex_offset, reg.size), codeloc)

        if stmt.ret_exprs:
            # Handle return expressions
            for ret_expr in stmt.ret_exprs:
                self._expr(ret_expr)
            return

        # No return expressions are available.
        # consume registers that are potentially useful

        # return value
        if cc is not None and cc.ret_val is not None:
            if isinstance(cc.ret_val, SimRegArg):
                offset = cc.ret_val._fix_offset(None, size, arch=self.project.arch)
                self.state.add_use(Register(offset, size), codeloc)
            else:
                l.error(""Cannot handle CC with non-register return value location"")

        # base pointer
        # TODO: Check if the stack base pointer is used as a stack base pointer in this function or not
        self.state.add_use(Register(self.project.arch.bp_offset, self.project.arch.bits // 8), codeloc)",cc is not None and cc.ret_val is not None,cc is not None is not cc.ret_val
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/Utils/Board.py,Board,simulateUnmove$222,"def simulateUnmove(self, board1, move):
        moved = []
        new = []
        dead = []

        cord0, cord1 = move.cords
        flag = FLAG(move.move)

        # null move or SITTUYINCHESS in place promotion
        if cord0 == cord1 and flag != DROP and flag != QUEEN_PROMOTION:
            return moved, new, dead

        if self.variant == ATOMICCHESS and (board1[cord1] or
                                            flag == ENPASSANT):
            piece = board1[cord0].piece
            cord = self.getHoldingCord(self.color, piece)
            moved.append((self[cord], cord))
            self[cord].opacity = 1
            dead.append(self[cord])
        elif not (self.variant == FISCHERRANDOMCHESS and flag in
                  (QUEEN_CASTLE, KING_CASTLE)):
            moved.append((self[cord1], cord1))

        kcastle = flag == KING_CASTLE or (
            self.variant == SCHESS and (
                (self[move.cord1] is not None and self[move.cord1].piece == KING and cord0.x - cord1.x == -2) or
                (flag in (HAWK_GATE_AT_ROOK, ELEPHANT_GATE_AT_ROOK) and cord0.x - cord1.x > 0)))
        qcastle = flag == QUEEN_CASTLE or (
            self.variant == SCHESS and (
                (self[move.cord1] is not None and self[move.cord1].piece == KING and cord0.x - cord1.x == 2) or
                (flag in (HAWK_GATE_AT_ROOK, ELEPHANT_GATE_AT_ROOK) and cord0.x - cord1.x < 0)))

        # capture
        if board1[cord1] and not (flag in (QUEEN_CASTLE, KING_CASTLE, HAWK_GATE_AT_ROOK, ELEPHANT_GATE_AT_ROOK)):
            piece = PAWN if self.variant == CRAZYHOUSECHESS and board1[
                cord1].promoted else board1[cord1].piece
            cord = self.getHoldingCord(1 - self.color, piece)
            moved.append((self[cord], cord))
            self[cord].opacity = 1
            # add all captured pieces to ""new"" list to enforce repainting them after a possible reordering
            new = self.getHoldingPieces(self.color)
            dead.append(self[cord])

            if self.variant == ATOMICCHESS:
                from pychess.Variants.atomic import cordsAround
                for acord in cordsAround(cord1):
                    piece = board1[acord]
                    if piece and piece.piece != PAWN and acord != cord0:
                        piece.opacity = 0
                        cord = self.getHoldingCord(1 - piece.color,
                                                   piece.piece)
                        moved.append((self[cord], cord))
                        self[cord].opacity = 1
                        dead.append(self[cord])

        if kcastle or qcastle:
            side = 0 if qcastle else 1
            if FILE(cord0.x) == 3 and self.board.variant in (
                    WILDCASTLECHESS, WILDCASTLESHUFFLECHESS):
                side = 0 if side == 1 else 1
            rook = self.board.fin_rooks[board1.color][side]
            moved.append((self[Cord(rook)], Cord(rook)))

        elif flag in PROMOTIONS:
            newPiece = board1[cord0]
            moved.append((newPiece, cord1))
            new.append(newPiece)

        elif flag == ENPASSANT:
            cord = self.getHoldingCord(1 - self.color, PAWN)
            moved.append((self[cord], cord))
            self[cord].opacity = 1
            # add all captured pieces to ""new"" list to enforce repainting them after a possible reordering
            new = self.getHoldingPieces(self.color)
            dead.append(self[cord])

        elif flag in GATINGS:
            # add all gated pieces to ""new"" list to enforce repainting them after a possible reordering
            new = self.getHoldingPieces(1 - self.color)

        return moved, new, dead",cord0 == cord1 and flag != DROP and (flag != QUEEN_PROMOTION),cord0 == cord1 and DROP != flag != QUEEN_PROMOTION
angr,https://github.com/angr/angr/tree/master/angr/analyses/congruency_check.py,CongruencyCheck,run$186,"def run(self, depth=None):
        """"""
        Checks that the paths in the specified path group stay the same over the next
        `depth` bytes.

        The path group should have a ""left"" and a ""right"" stash, each with a single
        path.
        """"""
        #pg_history = [ ]
        if len(self.simgr.right) != 1 or len(self.simgr.left) != 1:
            self._report_incongruency(""Single path in pg.left and pg.right required."")
            return False

        if ""UNICORN"" in self.simgr.one_right.options and depth is not None:
            self.simgr.one_right.unicorn.max_steps = depth

        if ""UNICORN"" in self.simgr.one_left.options and depth is not None:
            self.simgr.one_left.unicorn.max_steps = depth

        l.debug(""Performing initial path comparison."")
        if not self.compare_paths(self.simgr.left[0], self.simgr.right[0]):
            self._report_incongruency(""Initial path comparison check failed."")
            return False

        while len(self.simgr.left) > 0 and len(self.simgr.right) > 0:
            if depth is not None:
                self._update_progress(100. * float(self.simgr.one_left.history.block_count) / depth)

            if len(self.simgr.deadended) != 0:
                self._report_incongruency(""Unexpected deadended paths before step."")
                return False
            if len(self.simgr.right) == 0 and len(self.simgr.left) == 0:
                l.debug(""All done!"")
                return True
            if len(self.simgr.right) != 1 or len(self.simgr.left) != 1:
                self._report_incongruency(""Different numbers of paths in left and right stash.."")
                return False

            # do a step
            l.debug(
                ""Stepping right path with weighted length %d/%d"",
                self.simgr.right[0].history.block_count,
                depth
            )
            self.prev_pg = self.simgr.copy() #pylint:disable=unused-variable
            self.simgr.step(stash='right')
            CongruencyCheck._sync_steps(self.simgr)

            if len(self.simgr.errored) != 0:
                self._report_incongruency(""Unexpected errored paths."")
                return False

            try:
                if not self.compare_path_group(self.simgr) and self._validate_incongruency():
                    self._report_incongruency(""Path group comparison failed."")
                    return False
            except AngrIncongruencyError:
                if self._validate_incongruency():
                    raise

            if depth is not None:
                self.simgr.drop(stash='left', filter_func=lambda p: p.history.block_count >= depth)
                self.simgr.drop(stash='right', filter_func=lambda p: p.history.block_count >= depth)

            self.simgr.right.sort(key=lambda p: p.addr)
            self.simgr.left.sort(key=lambda p: p.addr)
            self.simgr.stashed_right[:] = self.simgr.stashed_right[::-1]
            self.simgr.stashed_left[:] = self.simgr.stashed_left[::-1]
            self.simgr.move('stashed_right', 'right')
            self.simgr.move('stashed_left', 'left')

            if len(self.simgr.left) > 1:
                self.simgr.split(from_stash='left', limit=1, to_stash='stashed_left')
                self.simgr.split(from_stash='right', limit=1, to_stash='stashed_right')",len(self.simgr.left) > 0 and len(self.simgr.right) > 0,len(self.simgr.left) > 0 < len(self.simgr.right)
angr,https://github.com/angr/angr/tree/master/angr/analyses/congruency_check.py,CongruencyCheck,run$186,"def run(self, depth=None):
        """"""
        Checks that the paths in the specified path group stay the same over the next
        `depth` bytes.

        The path group should have a ""left"" and a ""right"" stash, each with a single
        path.
        """"""
        #pg_history = [ ]
        if len(self.simgr.right) != 1 or len(self.simgr.left) != 1:
            self._report_incongruency(""Single path in pg.left and pg.right required."")
            return False

        if ""UNICORN"" in self.simgr.one_right.options and depth is not None:
            self.simgr.one_right.unicorn.max_steps = depth

        if ""UNICORN"" in self.simgr.one_left.options and depth is not None:
            self.simgr.one_left.unicorn.max_steps = depth

        l.debug(""Performing initial path comparison."")
        if not self.compare_paths(self.simgr.left[0], self.simgr.right[0]):
            self._report_incongruency(""Initial path comparison check failed."")
            return False

        while len(self.simgr.left) > 0 and len(self.simgr.right) > 0:
            if depth is not None:
                self._update_progress(100. * float(self.simgr.one_left.history.block_count) / depth)

            if len(self.simgr.deadended) != 0:
                self._report_incongruency(""Unexpected deadended paths before step."")
                return False
            if len(self.simgr.right) == 0 and len(self.simgr.left) == 0:
                l.debug(""All done!"")
                return True
            if len(self.simgr.right) != 1 or len(self.simgr.left) != 1:
                self._report_incongruency(""Different numbers of paths in left and right stash.."")
                return False

            # do a step
            l.debug(
                ""Stepping right path with weighted length %d/%d"",
                self.simgr.right[0].history.block_count,
                depth
            )
            self.prev_pg = self.simgr.copy() #pylint:disable=unused-variable
            self.simgr.step(stash='right')
            CongruencyCheck._sync_steps(self.simgr)

            if len(self.simgr.errored) != 0:
                self._report_incongruency(""Unexpected errored paths."")
                return False

            try:
                if not self.compare_path_group(self.simgr) and self._validate_incongruency():
                    self._report_incongruency(""Path group comparison failed."")
                    return False
            except AngrIncongruencyError:
                if self._validate_incongruency():
                    raise

            if depth is not None:
                self.simgr.drop(stash='left', filter_func=lambda p: p.history.block_count >= depth)
                self.simgr.drop(stash='right', filter_func=lambda p: p.history.block_count >= depth)

            self.simgr.right.sort(key=lambda p: p.addr)
            self.simgr.left.sort(key=lambda p: p.addr)
            self.simgr.stashed_right[:] = self.simgr.stashed_right[::-1]
            self.simgr.stashed_left[:] = self.simgr.stashed_left[::-1]
            self.simgr.move('stashed_right', 'right')
            self.simgr.move('stashed_left', 'left')

            if len(self.simgr.left) > 1:
                self.simgr.split(from_stash='left', limit=1, to_stash='stashed_left')
                self.simgr.split(from_stash='right', limit=1, to_stash='stashed_right')",len(self.simgr.right) == 0 and len(self.simgr.left) == 0,len(self.simgr.right) == 0 == len(self.simgr.left)
rpaframework,https://github.com/robocorp/rpaframework/tree/master/packages/main/src/RPA/Desktop/Windows.py,Windows,get_element_rich_text$1132,"def get_element_rich_text(self, locator: str) -> Any:
        """"""Get value of element `rich text` attribute.

        :param locator: element locator
        :return: `rich_text` value if found, else False

        Example:

        .. code-block:: robotframework

            ${text}  Get Element Rich Text  CalculatorResults

        """"""
        element = self.get_element(locator)
        if element is not False and ""rich_text"" in element:
            return element[""rich_text""]
        elif element is False:
            self.logger.info(""Did not find element with locator: %s"", locator)
            return False
        else:
            self.logger.info(
                ""Element for locator %s does not have 'rich_text' attribute"", locator
            )
            return False",element is not False and 'rich_text' in element,'rich_text' in element is not False
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/tensorflow2/tf2cv/models/deeplabv3.py,DeepLabv3,__init__$198,"def __init__(self,
                 backbone,
                 backbone_out_channels=2048,
                 aux=False,
                 fixed_size=True,
                 in_channels=3,
                 in_size=(480, 480),
                 classes=21,
                 data_format=""channels_last"",
                 **kwargs):
        super(DeepLabv3, self).__init__(**kwargs)
        assert (in_channels > 0)
        assert ((in_size[0] % 8 == 0) and (in_size[1] % 8 == 0))
        self.in_size = in_size
        self.classes = classes
        self.aux = aux
        self.fixed_size = fixed_size
        self.data_format = data_format

        self.backbone = backbone
        pool_out_size = (self.in_size[0] // 8, self.in_size[1] // 8) if fixed_size else None
        self.pool = AtrousSpatialPyramidPooling(
            in_channels=backbone_out_channels,
            upscale_out_size=pool_out_size,
            data_format=data_format,
            name=""pool"")
        pool_out_channels = backbone_out_channels // 8
        self.final_block = DeepLabv3FinalBlock(
            in_channels=pool_out_channels,
            out_channels=classes,
            bottleneck_factor=1,
            data_format=data_format,
            name=""final_block"")
        if self.aux:
            aux_out_channels = backbone_out_channels // 2
            self.aux_block = DeepLabv3FinalBlock(
                in_channels=aux_out_channels,
                out_channels=classes,
                bottleneck_factor=4,
                data_format=data_format,
                name=""aux_block"")",in_size[0] % 8 == 0 and in_size[1] % 8 == 0,in_size[0] % 8 == 0 == in_size[1] % 8
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/ir/inference/test_trt_convert_prelu.py,TrtConvertPreluTest,sample_predictor_configs$150,"def sample_predictor_configs(
        self, program_config
    ) -> (paddle_infer.Config, List[int], float):
        def generate_dynamic_shape(attrs):
            if self.dim1 == 0:
                self.dynamic_shape.min_input_shape = {
                    ""input_data"": [1],
                }
                self.dynamic_shape.max_input_shape = {
                    ""input_data"": [4],
                }
                self.dynamic_shape.opt_input_shape = {
                    ""input_data"": [2],
                }
            else:
                if self.dim2 == 0 and self.dim3 == 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3],
                    }
                elif self.dim2 != 0 and self.dim3 != 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1, 1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 3, 16, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3, 16, 32],
                    }
                elif self.dim3 == 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 3, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3, 16],
                    }

        def clear_dynamic_shape():
            self.dynamic_shape.max_input_shape = {}
            self.dynamic_shape.min_input_shape = {}
            self.dynamic_shape.opt_input_shape = {}

        attrs = [
            program_config.ops[i].attrs for i in range(len(program_config.ops))
        ]

        def generate_trt_nodes_num(attrs, dynamic_shape):
            if (
                not dynamic_shape
                and self.dim1 == 0
                and self.dim2 == 0
                and self.dim3 == 0
            ):
                return 0, 3
            return 1, 2

        # for static_shape
        clear_dynamic_shape()
        self.trt_param.precision = paddle_infer.PrecisionType.Float32
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, False
        ), 1e-5
        self.trt_param.precision = paddle_infer.PrecisionType.Half
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, False
        ), (1e-3, 1e-3)

        # for dynamic_shape
        generate_dynamic_shape(attrs)
        self.trt_param.precision = paddle_infer.PrecisionType.Float32
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, True
        ), 1e-5
        self.trt_param.precision = paddle_infer.PrecisionType.Half
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, True
        ), (1e-3, 1e-3)",not dynamic_shape and self.dim1 == 0 and (self.dim2 == 0) and (self.dim3 == 0),self.dim1 == 0 == self.dim2 and self.dim3 == 0 and (not dynamic_shape)
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/ir/inference/test_trt_convert_prelu.py,TrtConvertPreluTest,sample_predictor_configs$150,"def sample_predictor_configs(
        self, program_config
    ) -> (paddle_infer.Config, List[int], float):
        def generate_dynamic_shape(attrs):
            if self.dim1 == 0:
                self.dynamic_shape.min_input_shape = {
                    ""input_data"": [1],
                }
                self.dynamic_shape.max_input_shape = {
                    ""input_data"": [4],
                }
                self.dynamic_shape.opt_input_shape = {
                    ""input_data"": [2],
                }
            else:
                if self.dim2 == 0 and self.dim3 == 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3],
                    }
                elif self.dim2 != 0 and self.dim3 != 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1, 1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 3, 16, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3, 16, 32],
                    }
                elif self.dim3 == 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 3, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3, 16],
                    }

        def clear_dynamic_shape():
            self.dynamic_shape.max_input_shape = {}
            self.dynamic_shape.min_input_shape = {}
            self.dynamic_shape.opt_input_shape = {}

        attrs = [
            program_config.ops[i].attrs for i in range(len(program_config.ops))
        ]

        def generate_trt_nodes_num(attrs, dynamic_shape):
            if (
                not dynamic_shape
                and self.dim1 == 0
                and self.dim2 == 0
                and self.dim3 == 0
            ):
                return 0, 3
            return 1, 2

        # for static_shape
        clear_dynamic_shape()
        self.trt_param.precision = paddle_infer.PrecisionType.Float32
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, False
        ), 1e-5
        self.trt_param.precision = paddle_infer.PrecisionType.Half
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, False
        ), (1e-3, 1e-3)

        # for dynamic_shape
        generate_dynamic_shape(attrs)
        self.trt_param.precision = paddle_infer.PrecisionType.Float32
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, True
        ), 1e-5
        self.trt_param.precision = paddle_infer.PrecisionType.Half
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, True
        ), (1e-3, 1e-3)",self.dim2 == 0 and self.dim3 == 0,self.dim2 == 0 == self.dim3
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/ir/inference/test_trt_convert_prelu.py,TrtConvertPreluTest,sample_predictor_configs$150,"def sample_predictor_configs(
        self, program_config
    ) -> (paddle_infer.Config, List[int], float):
        def generate_dynamic_shape(attrs):
            if self.dim1 == 0:
                self.dynamic_shape.min_input_shape = {
                    ""input_data"": [1],
                }
                self.dynamic_shape.max_input_shape = {
                    ""input_data"": [4],
                }
                self.dynamic_shape.opt_input_shape = {
                    ""input_data"": [2],
                }
            else:
                if self.dim2 == 0 and self.dim3 == 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3],
                    }
                elif self.dim2 != 0 and self.dim3 != 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1, 1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 3, 16, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3, 16, 32],
                    }
                elif self.dim3 == 0:
                    self.dynamic_shape.min_input_shape = {
                        ""input_data"": [1, 1, 1],
                    }
                    self.dynamic_shape.max_input_shape = {
                        ""input_data"": [4, 3, 32],
                    }
                    self.dynamic_shape.opt_input_shape = {
                        ""input_data"": [2, 3, 16],
                    }

        def clear_dynamic_shape():
            self.dynamic_shape.max_input_shape = {}
            self.dynamic_shape.min_input_shape = {}
            self.dynamic_shape.opt_input_shape = {}

        attrs = [
            program_config.ops[i].attrs for i in range(len(program_config.ops))
        ]

        def generate_trt_nodes_num(attrs, dynamic_shape):
            if (
                not dynamic_shape
                and self.dim1 == 0
                and self.dim2 == 0
                and self.dim3 == 0
            ):
                return 0, 3
            return 1, 2

        # for static_shape
        clear_dynamic_shape()
        self.trt_param.precision = paddle_infer.PrecisionType.Float32
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, False
        ), 1e-5
        self.trt_param.precision = paddle_infer.PrecisionType.Half
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, False
        ), (1e-3, 1e-3)

        # for dynamic_shape
        generate_dynamic_shape(attrs)
        self.trt_param.precision = paddle_infer.PrecisionType.Float32
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, True
        ), 1e-5
        self.trt_param.precision = paddle_infer.PrecisionType.Half
        yield self.create_inference_config(), generate_trt_nodes_num(
            attrs, True
        ), (1e-3, 1e-3)",self.dim2 != 0 and self.dim3 != 0,self.dim2 != 0 != self.dim3
botorch,https://github.com/pytorch/botorch/tree/master/botorch/models/gp_regression_fidelity.py,FixedNoiseMultiFidelityGP,__init__$182,"def __init__(
        self,
        train_X: Tensor,
        train_Y: Tensor,
        train_Yvar: Tensor,
        iteration_fidelity: Optional[int] = None,
        data_fidelity: Optional[int] = None,
        linear_truncated: bool = True,
        nu: float = 2.5,
        outcome_transform: Optional[OutcomeTransform] = None,
        input_transform: Optional[InputTransform] = None,
    ) -> None:
        r""""""
        Args:
            train_X: A `batch_shape x n x (d + s)` tensor of training features,
                where `s` is the dimension of the fidelity parameters (either one
                or two).
            train_Y: A `batch_shape x n x m` tensor of training observations.
            train_Yvar: A `batch_shape x n x m` tensor of observed measurement noise.
            iteration_fidelity: The column index for the training iteration fidelity
                parameter (optional).
            data_fidelity: The column index for the downsampling fidelity parameter
                (optional).
            linear_truncated: If True, use a `LinearTruncatedFidelityKernel` instead
                of the default kernel.
            nu: The smoothness parameter for the Matern kernel: either 1/2, 3/2, or
                5/2. Only used when `linear_truncated=True`.
            outcome_transform: An outcome transform that is applied to the
                training data during instantiation and to the posterior during
                inference (that is, the `Posterior` obtained by calling
                `.posterior` on the model will be on the original scale).
            input_transform: An input transform that is applied in the model's
                forward pass.
        """"""
        if iteration_fidelity is None and data_fidelity is None:
            raise UnsupportedError(
                ""FixedNoiseMultiFidelityGP requires at least one fidelity parameter.""
            )
        with torch.no_grad():
            transformed_X = self.transform_inputs(
                X=train_X, input_transform=input_transform
            )
        self._set_dimensions(train_X=transformed_X, train_Y=train_Y)
        covar_module, subset_batch_dict = _setup_multifidelity_covar_module(
            dim=transformed_X.size(-1),
            aug_batch_shape=self._aug_batch_shape,
            iteration_fidelity=iteration_fidelity,
            data_fidelity=data_fidelity,
            linear_truncated=linear_truncated,
            nu=nu,
        )
        super().__init__(
            train_X=train_X,
            train_Y=train_Y,
            train_Yvar=train_Yvar,
            covar_module=covar_module,
            outcome_transform=outcome_transform,
            input_transform=input_transform,
        )
        self._subset_batch_dict = {
            ""likelihood.noise_covar.raw_noise"": -2,
            ""mean_module.raw_constant"": -1,
            ""covar_module.raw_outputscale"": -1,
            **subset_batch_dict,
        }
        self.to(train_X)",iteration_fidelity is None and data_fidelity is None,iteration_fidelity is None is data_fidelity
PaddleClas,https://github.com/PaddlePaddle/PaddleClas/tree/master/ppcls/arch/backbone/model_zoo/res2net.py,Res2Net,__init__$159,"def __init__(self, layers=50, scales=4, width=26, class_num=1000):
        super(Res2Net, self).__init__()

        self.layers = layers
        self.scales = scales
        self.width = width
        basic_width = self.width * self.scales
        supported_layers = [50, 101, 152, 200]
        assert layers in supported_layers, \
            ""supported layers are {} but input layer is {}"".format(
                supported_layers, layers)

        if layers == 50:
            depth = [3, 4, 6, 3]
        elif layers == 101:
            depth = [3, 4, 23, 3]
        elif layers == 152:
            depth = [3, 8, 36, 3]
        elif layers == 200:
            depth = [3, 12, 48, 3]
        num_channels = [64, 256, 512, 1024]
        num_channels2 = [256, 512, 1024, 2048]
        num_filters = [basic_width * t for t in [1, 2, 4, 8]]

        self.conv1 = ConvBNLayer(
            num_channels=3,
            num_filters=64,
            filter_size=7,
            stride=2,
            act='relu',
            name=""conv1"")
        self.pool2d_max = MaxPool2D(kernel_size=3, stride=2, padding=1)

        self.block_list = []
        for block in range(len(depth)):
            shortcut = False
            for i in range(depth[block]):
                if layers in [101, 152] and block == 2:
                    if i == 0:
                        conv_name = ""res"" + str(block + 2) + ""a""
                    else:
                        conv_name = ""res"" + str(block + 2) + ""b"" + str(i)
                else:
                    conv_name = ""res"" + str(block + 2) + chr(97 + i)
                bottleneck_block = self.add_sublayer(
                    'bb_%d_%d' % (block, i),
                    BottleneckBlock(
                        num_channels1=num_channels[block]
                        if i == 0 else num_channels2[block],
                        num_channels2=num_channels2[block],
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1,
                        scales=scales,
                        shortcut=shortcut,
                        if_first=block == i == 0,
                        name=conv_name))
                self.block_list.append(bottleneck_block)
                shortcut = True

        self.pool2d_avg = AdaptiveAvgPool2D(1)

        self.pool2d_avg_channels = num_channels[-1] * 2

        stdv = 1.0 / math.sqrt(self.pool2d_avg_channels * 1.0)

        self.out = Linear(
            self.pool2d_avg_channels,
            class_num,
            weight_attr=ParamAttr(
                initializer=Uniform(-stdv, stdv), name=""fc_weights""),
            bias_attr=ParamAttr(name=""fc_offset""))",i == 0 and block != 0,i == 0 != block
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/fields/definitions.py,Integer,_deserialize$218,"def _deserialize(self, value, attr, data, **kwargs):
        value = super()._deserialize(value, attr, data)

        enum = self.metadata.get(""enum"")
        if enum and value not in enum:
            raise self.make_error(""enum"", value=value, enum=enum)

        maximum = self.metadata.get(""maximum"")
        if maximum is not None and value > maximum:
            raise self.make_error(""maximum"", value=value, maximum=maximum)

        minimum = self.metadata.get(""minimum"")
        if minimum is not None and value < minimum:
            raise self.make_error(""minimum"", value=value, minimum=minimum)

        exclusive_maximum = self.metadata.get(""exclusiveMaximum"")
        if exclusive_maximum is not None and value >= exclusive_maximum:
            raise self.make_error(
                ""exclusiveMaximum"", value=value, exclusiveMaximum=exclusive_maximum
            )

        exclusive_minimum = self.metadata.get(""exclusiveMinimum"")
        if exclusive_minimum is not None and value <= exclusive_minimum:
            raise self.make_error(
                ""exclusiveMinimum"", value=value, exclusiveMinimum=exclusive_minimum
            )

        multiple_of = self.metadata.get(""multipleOf"")
        if multiple_of is not None and value % multiple_of != 0:
            raise self.make_error(""multipleOf"", value=value, multipleOf=multiple_of)

        return value",maximum is not None and value > maximum,None is not maximum < value
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/fields/definitions.py,Integer,_deserialize$218,"def _deserialize(self, value, attr, data, **kwargs):
        value = super()._deserialize(value, attr, data)

        enum = self.metadata.get(""enum"")
        if enum and value not in enum:
            raise self.make_error(""enum"", value=value, enum=enum)

        maximum = self.metadata.get(""maximum"")
        if maximum is not None and value > maximum:
            raise self.make_error(""maximum"", value=value, maximum=maximum)

        minimum = self.metadata.get(""minimum"")
        if minimum is not None and value < minimum:
            raise self.make_error(""minimum"", value=value, minimum=minimum)

        exclusive_maximum = self.metadata.get(""exclusiveMaximum"")
        if exclusive_maximum is not None and value >= exclusive_maximum:
            raise self.make_error(
                ""exclusiveMaximum"", value=value, exclusiveMaximum=exclusive_maximum
            )

        exclusive_minimum = self.metadata.get(""exclusiveMinimum"")
        if exclusive_minimum is not None and value <= exclusive_minimum:
            raise self.make_error(
                ""exclusiveMinimum"", value=value, exclusiveMinimum=exclusive_minimum
            )

        multiple_of = self.metadata.get(""multipleOf"")
        if multiple_of is not None and value % multiple_of != 0:
            raise self.make_error(""multipleOf"", value=value, multipleOf=multiple_of)

        return value",minimum is not None and value < minimum,None is not minimum > value
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/fields/definitions.py,Integer,_deserialize$218,"def _deserialize(self, value, attr, data, **kwargs):
        value = super()._deserialize(value, attr, data)

        enum = self.metadata.get(""enum"")
        if enum and value not in enum:
            raise self.make_error(""enum"", value=value, enum=enum)

        maximum = self.metadata.get(""maximum"")
        if maximum is not None and value > maximum:
            raise self.make_error(""maximum"", value=value, maximum=maximum)

        minimum = self.metadata.get(""minimum"")
        if minimum is not None and value < minimum:
            raise self.make_error(""minimum"", value=value, minimum=minimum)

        exclusive_maximum = self.metadata.get(""exclusiveMaximum"")
        if exclusive_maximum is not None and value >= exclusive_maximum:
            raise self.make_error(
                ""exclusiveMaximum"", value=value, exclusiveMaximum=exclusive_maximum
            )

        exclusive_minimum = self.metadata.get(""exclusiveMinimum"")
        if exclusive_minimum is not None and value <= exclusive_minimum:
            raise self.make_error(
                ""exclusiveMinimum"", value=value, exclusiveMinimum=exclusive_minimum
            )

        multiple_of = self.metadata.get(""multipleOf"")
        if multiple_of is not None and value % multiple_of != 0:
            raise self.make_error(""multipleOf"", value=value, multipleOf=multiple_of)

        return value",exclusive_maximum is not None and value >= exclusive_maximum,None is not exclusive_maximum <= value
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/gui/fields/definitions.py,Integer,_deserialize$218,"def _deserialize(self, value, attr, data, **kwargs):
        value = super()._deserialize(value, attr, data)

        enum = self.metadata.get(""enum"")
        if enum and value not in enum:
            raise self.make_error(""enum"", value=value, enum=enum)

        maximum = self.metadata.get(""maximum"")
        if maximum is not None and value > maximum:
            raise self.make_error(""maximum"", value=value, maximum=maximum)

        minimum = self.metadata.get(""minimum"")
        if minimum is not None and value < minimum:
            raise self.make_error(""minimum"", value=value, minimum=minimum)

        exclusive_maximum = self.metadata.get(""exclusiveMaximum"")
        if exclusive_maximum is not None and value >= exclusive_maximum:
            raise self.make_error(
                ""exclusiveMaximum"", value=value, exclusiveMaximum=exclusive_maximum
            )

        exclusive_minimum = self.metadata.get(""exclusiveMinimum"")
        if exclusive_minimum is not None and value <= exclusive_minimum:
            raise self.make_error(
                ""exclusiveMinimum"", value=value, exclusiveMinimum=exclusive_minimum
            )

        multiple_of = self.metadata.get(""multipleOf"")
        if multiple_of is not None and value % multiple_of != 0:
            raise self.make_error(""multipleOf"", value=value, multipleOf=multiple_of)

        return value",exclusive_minimum is not None and value <= exclusive_minimum,None is not exclusive_minimum >= value
rllab,https://github.com/rll/rllab/tree/master/rllab/algos/cma_es_lib.py,,is_feasible$528,"def is_feasible(x, f):
    """"""default to check feasibility, see also ``cma_default_options``""""""
    return f is not None and f is not np.NaN",f is not None and f is not np.NaN,None is not f is not np.NaN
programmingbitcoin,https://github.com/jimmysong/programmingbitcoin/tree/master/code-ch06/ecc.py,Point,__init__$139,"def __init__(self, x, y, a, b):
        self.a = a
        self.b = b
        self.x = x
        self.y = y
        # x being None and y being None represents the point at infinity
        # Check for that here since the equation below won't make sense
        # with None values for both.
        if self.x is None and self.y is None:
            return
        # make sure that the elliptic curve equation is satisfied
        # y**2 == x**3 + a*x + b
        if self.y**2 != self.x**3 + a * x + b:
            # if not, throw a ValueError
            raise ValueError('({}, {}) is not on the curve'.format(x, y))",self.x is None and self.y is None,self.x is None is self.y
this-word-does-not-exist,https://github.com/turtlesoupy/this-word-does-not-exist/tree/master/title_maker_pro/urban_dictionary_scraper.py,,fetch_all_letter_word_url$138,"def fetch_all_letter_word_url(session, letter, limit=None):
    first_ip = fetch_letter_page(session, letter)

    if not first_ip.num_pages:
        raise RuntimeError(f""First page of {letter} lacks total number of pages!"")

    all_definitions = OrderedDict((d.title, d) for d in first_ip.definition_urls)
    for i in range(2, first_ip.num_pages + 1):
        ip = fetch_letter_page(session, letter, page=i)
        all_definitions.update((d.title, d) for d in ip.definition_urls)

        if limit is not None and i > limit:
            break

    return all_definitions",limit is not None and i > limit,None is not limit < i
mmf,https://github.com/facebookresearch/mmf/tree/master/tests/trainers/lightning/test_checkpoint.py,TestLightningCheckpoint,_assert_same$53,"def _assert_same(self, obj1, obj2, same=True):
        if same:
            if hasattr(obj1, ""mean"") and obj1.dtype == torch.float:
                self.assertAlmostEquals(obj1.mean().item(), obj2.mean().item(), 2)
            elif hasattr(obj1, ""item""):
                self.assertEqual(obj1.item(), obj2.item())
            elif type(obj1) is dict and type(obj2) is dict:
                self._assert_same_dict(obj1, obj2)
            else:
                self.assertEqual(obj1, obj2)
        else:
            if hasattr(obj1, ""mean"") and obj1.dtype == torch.float:
                self.assertNotEqual(obj1.mean().item(), obj2.mean().item())
            elif hasattr(obj1, ""item""):
                self.assertNotEqual(obj1.item(), obj2.item())
            elif type(obj1) is dict and type(obj2) is dict:
                self._assert_same_dict(obj1, obj2, same=False)
            else:
                self.assertNotEqual(obj1, obj2)",type(obj1) is dict and type(obj2) is dict,type(obj1) is dict is type(obj2)
mmf,https://github.com/facebookresearch/mmf/tree/master/tests/trainers/lightning/test_checkpoint.py,TestLightningCheckpoint,_assert_same$53,"def _assert_same(self, obj1, obj2, same=True):
        if same:
            if hasattr(obj1, ""mean"") and obj1.dtype == torch.float:
                self.assertAlmostEquals(obj1.mean().item(), obj2.mean().item(), 2)
            elif hasattr(obj1, ""item""):
                self.assertEqual(obj1.item(), obj2.item())
            elif type(obj1) is dict and type(obj2) is dict:
                self._assert_same_dict(obj1, obj2)
            else:
                self.assertEqual(obj1, obj2)
        else:
            if hasattr(obj1, ""mean"") and obj1.dtype == torch.float:
                self.assertNotEqual(obj1.mean().item(), obj2.mean().item())
            elif hasattr(obj1, ""item""):
                self.assertNotEqual(obj1.item(), obj2.item())
            elif type(obj1) is dict and type(obj2) is dict:
                self._assert_same_dict(obj1, obj2, same=False)
            else:
                self.assertNotEqual(obj1, obj2)",type(obj1) is dict and type(obj2) is dict,type(obj1) is dict is type(obj2)
hypothesis,https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/tests/cover/test_numerics.py,,test_fuzz_floats_bounds$39,"def test_fuzz_floats_bounds(data):
    bound = none() | floats(allow_nan=False)
    low, high = data.draw(tuples(bound, bound), label=""low, high"")
    if low is not None and high is not None and low > high:
        low, high = high, low
    exmin = low is not None and low != inf and data.draw(booleans(), label=""ex_min"")
    exmax = high is not None and high != -inf and data.draw(booleans(), label=""ex_max"")
    try:
        val = data.draw(
            floats(low, high, exclude_min=exmin, exclude_max=exmax), label=""value""
        )
        assume(val)  # positive/negative zero is an issue
    except (InvalidArgument, HypothesisDeprecationWarning):
        assert (
            (exmin and exmax and low == next_down(high))
            or (low == high and (exmin or exmax))
            or (
                low == high == 0
                and copysign(1.0, low) == 1
                and copysign(1.0, high) == -1
            )
        )
        reject()  # no floats in required range
    if low is not None:
        assert low <= val
    if high is not None:
        assert val <= high
    if exmin:
        assert low != val
    if exmax:
        assert high != val",low is not None and high is not None and (low > high),low is not None is not high < low
hypothesis,https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/tests/cover/test_numerics.py,,test_fuzz_floats_bounds$39,"def test_fuzz_floats_bounds(data):
    bound = none() | floats(allow_nan=False)
    low, high = data.draw(tuples(bound, bound), label=""low, high"")
    if low is not None and high is not None and low > high:
        low, high = high, low
    exmin = low is not None and low != inf and data.draw(booleans(), label=""ex_min"")
    exmax = high is not None and high != -inf and data.draw(booleans(), label=""ex_max"")
    try:
        val = data.draw(
            floats(low, high, exclude_min=exmin, exclude_max=exmax), label=""value""
        )
        assume(val)  # positive/negative zero is an issue
    except (InvalidArgument, HypothesisDeprecationWarning):
        assert (
            (exmin and exmax and low == next_down(high))
            or (low == high and (exmin or exmax))
            or (
                low == high == 0
                and copysign(1.0, low) == 1
                and copysign(1.0, high) == -1
            )
        )
        reject()  # no floats in required range
    if low is not None:
        assert low <= val
    if high is not None:
        assert val <= high
    if exmin:
        assert low != val
    if exmax:
        assert high != val","low is not None and low != inf and data.draw(booleans(), label='ex_min')","None is not low != inf and data.draw(booleans(), label='ex_min')"
hypothesis,https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/tests/cover/test_numerics.py,,test_fuzz_floats_bounds$39,"def test_fuzz_floats_bounds(data):
    bound = none() | floats(allow_nan=False)
    low, high = data.draw(tuples(bound, bound), label=""low, high"")
    if low is not None and high is not None and low > high:
        low, high = high, low
    exmin = low is not None and low != inf and data.draw(booleans(), label=""ex_min"")
    exmax = high is not None and high != -inf and data.draw(booleans(), label=""ex_max"")
    try:
        val = data.draw(
            floats(low, high, exclude_min=exmin, exclude_max=exmax), label=""value""
        )
        assume(val)  # positive/negative zero is an issue
    except (InvalidArgument, HypothesisDeprecationWarning):
        assert (
            (exmin and exmax and low == next_down(high))
            or (low == high and (exmin or exmax))
            or (
                low == high == 0
                and copysign(1.0, low) == 1
                and copysign(1.0, high) == -1
            )
        )
        reject()  # no floats in required range
    if low is not None:
        assert low <= val
    if high is not None:
        assert val <= high
    if exmin:
        assert low != val
    if exmax:
        assert high != val","high is not None and high != -inf and data.draw(booleans(), label='ex_max')","None is not high != -inf and data.draw(booleans(), label='ex_max')"
attn2d,https://github.com/elbayadm/attn2d/tree/master/fairseq/data/denoising_dataset.py,DenoisingDataset,__getitem__$157,"def __getitem__(self, index):
        with data_utils.numpy_seed(self.seed, self.epoch, index):
            tokens = self.dataset[index]
            assert tokens[-1] == self.eos
            source, target = tokens, tokens.clone()

            if self.permute_sentence_ratio > 0.0:
                source = self.permute_sentences(source, self.permute_sentence_ratio)

            if self.mask_ratio > 0:
                source = self.add_whole_word_mask(source, self.mask_ratio)

            if self.insert_ratio > 0:
                source = self.add_insertion_noise(source, self.insert_ratio)

            if self.rotate_ratio > 0.0 and np.random.random() < self.rotate_ratio:
                source = self.add_rolling_noise(source)

        assert (source >= 0).all()
        assert (source[1:-1] >= 1).all()
        assert (source <= len(self.vocab)).all()
        assert source[0] == self.vocab.bos()
        assert source[-1] == self.eos
        return {
            'id': index,
            'source': source,
            'target': target,
        }",self.rotate_ratio > 0.0 and np.random.random() < self.rotate_ratio,np.random.random() < self.rotate_ratio > 0.0
python-twitter,https://github.com/bear/python-twitter/tree/master/twitter/api.py,Api,PostUpdate$1014,"def PostUpdate(self,
                   status,
                   media=None,
                   media_additional_owners=None,
                   media_category=None,
                   in_reply_to_status_id=None,
                   auto_populate_reply_metadata=False,
                   exclude_reply_user_ids=None,
                   latitude=None,
                   longitude=None,
                   place_id=None,
                   display_coordinates=False,
                   trim_user=False,
                   verify_status_length=True,
                   attachment_url=None):
        """"""Post a twitter status message from the authenticated user.

        https://developer.twitter.com/en/docs/tweets/post-and-engage/api-reference/post-statuses-update

        Args:
            status (str):
                The message text to be posted. Must be less than or equal to
                CHARACTER_LIMIT characters.
            media (int, str, fp, optional):
                A media ID, URL, local file, or file-like object (something with
                a read() method), or a list of any combination of the above.
            media_additional_owners (list, optional):
                A list of user ids representing Twitter users that should be able
                to use the uploaded media in their tweets. If you pass a list of
                media, then additional_owners will apply to each object. If you
                need more granular control, please use the UploadMedia* methods.
            media_category (str, optional):
                Only for use with the AdsAPI. See
                https://dev.twitter.com/ads/creative/promoted-video-overview if
                this applies to your application.
            in_reply_to_status_id (int, optional):
                The ID of an existing status that the status to be posted is
                in reply to.  This implicitly sets the in_reply_to_user_id
                attribute of the resulting status to the user ID of the
                message being replied to.  Invalid/missing status IDs will be
                ignored.
            auto_populate_reply_metadata (bool, optional):
                Automatically include the @usernames of the users mentioned or
                participating in the tweet to which this tweet is in reply.
            exclude_reply_user_ids (list, optional):
                Remove given user_ids (*not* @usernames) from the tweet's
                automatically generated reply metadata.
            attachment_url (str, optional):
                URL to an attachment resource: one to four photos, a GIF,
                video, Quote Tweet, or DM deep link. If not specified and
                media parameter is not None, we will attach the first media
                object as the attachment URL. If a bad URL is passed, Twitter
                will raise an error.
            latitude (float, optional):
                Latitude coordinate of the tweet in degrees. Will only work
                in conjunction with longitude argument. Both longitude and
                latitude will be ignored by twitter if the user has a false
                geo_enabled setting.
            longitude (float, optional):
                Longitude coordinate of the tweet in degrees. Will only work
                in conjunction with latitude argument. Both longitude and
                latitude will be ignored by twitter if the user has a false
                geo_enabled setting.
            place_id (int, optional):
                A place in the world. These IDs can be retrieved from
                GET geo/reverse_geocode.
            display_coordinates (bool, optional):
                Whether or not to put a pin on the exact coordinates a tweet
                has been sent from.
            trim_user (bool, optional):
                If True the returned payload will only contain the user IDs,
                otherwise the payload will contain the full user data item.
            verify_status_length (bool, optional):
                If True, api throws a hard error that the status is over
                CHARACTER_LIMIT characters. If False, Api will attempt to post
                the status.
        Returns:
            (twitter.Status) A twitter.Status instance representing the
            message posted.
        """"""
        url = '%s/statuses/update.json' % self.base_url

        if isinstance(status, str) or self._input_encoding is None:
            u_status = status
        else:
            u_status = str(status, self._input_encoding)

        if verify_status_length and calc_expected_status_length(u_status) > CHARACTER_LIMIT:
            raise TwitterError(""Text must be less than or equal to CHARACTER_LIMIT characters."")

        if auto_populate_reply_metadata and not in_reply_to_status_id:
            raise TwitterError(""If auto_populate_reply_metadata is True, you must set in_reply_to_status_id"")

        parameters = {
            'status': u_status,
            'in_reply_to_status_id': in_reply_to_status_id,
            'auto_populate_reply_metadata': auto_populate_reply_metadata,
            'place_id': place_id,
            'display_coordinates': display_coordinates,
            'trim_user': trim_user,
            'exclude_reply_user_ids': ','.join([str(u) for u in exclude_reply_user_ids or []]),
        }

        if attachment_url:
            parameters['attachment_url'] = attachment_url

        if media:
            chunked_types = ['video/mp4', 'video/quicktime', 'image/gif']
            media_ids = []
            if isinstance(media, (int, long)):
                media_ids.append(media)

            elif isinstance(media, list):
                for media_file in media:

                    # If you want to pass just a media ID, it should be an int
                    if isinstance(media_file, (int, long)):
                        media_ids.append(media_file)
                        continue

                    _, _, file_size, media_type = parse_media_file(media_file)
                    if (media_type == 'image/gif' or media_type == 'video/mp4') and len(media) > 1:
                        raise TwitterError(
                            'You cannot post more than 1 GIF or 1 video in a single status.')
                    if file_size > self.chunk_size or media_type in chunked_types:
                        media_id = self.UploadMediaChunked(
                            media=media_file,
                            additional_owners=media_additional_owners,
                            media_category=media_category)
                    else:
                        media_id = self.UploadMediaSimple(
                            media=media_file,
                            additional_owners=media_additional_owners,
                            media_category=media_category)
                    media_ids.append(media_id)
            else:
                _, _, file_size, media_type = parse_media_file(media)
                if file_size > self.chunk_size or media_type in chunked_types:
                    media_ids.append(self.UploadMediaChunked(
                        media, media_additional_owners, media_category=media_category
                    ))
                else:
                    media_ids.append(self.UploadMediaSimple(
                        media, media_additional_owners, media_category=media_category
                    ))
            parameters['media_ids'] = ','.join([str(mid) for mid in media_ids])

        if latitude is not None and longitude is not None:
            parameters['lat'] = str(latitude)
            parameters['long'] = str(longitude)

        resp = self._RequestUrl(url, 'POST', data=parameters)
        data = self._ParseAndCheckTwitter(resp.content.decode('utf-8'))

        return Status.NewFromJsonDict(data)",latitude is not None and longitude is not None,latitude is not None is not longitude
TracKit,https://github.com/researchmm/TracKit/tree/master/lib/tracker/online.py,ONLINE,generate_init_samples$323,"def generate_init_samples(self, im: torch.Tensor) -> TensorList:
        """"""Perform data augmentation to generate initial training samples.""""""

        if getattr(self.p, 'border_mode', 'replicate') == 'inside':
            # Get new sample size if forced inside the image
            im_sz = torch.Tensor([im.shape[2], im.shape[3]])
            sample_sz = self.target_scale * self.img_sample_sz
            shrink_factor = (sample_sz.float() / im_sz).max().clamp(1)
            sample_sz = (sample_sz.float() / shrink_factor)
            self.init_sample_scale = (sample_sz / self.img_sample_sz).prod().sqrt()
            tl = self.pos - (sample_sz - 1) / 2
            br = self.pos + sample_sz / 2 + 1
            global_shift = - ((-tl).clamp(0) - (br - im_sz).clamp(0)) / self.init_sample_scale
        else:
            self.init_sample_scale = self.target_scale
            global_shift = torch.zeros(2)

        self.init_sample_pos = self.pos.round()

        # Compute augmentation size
        aug_expansion_factor = getattr(self.p, 'augmentation_expansion_factor', None)
        aug_expansion_sz = self.img_sample_sz.clone()
        aug_output_sz = None
        if aug_expansion_factor is not None and aug_expansion_factor != 1:
            aug_expansion_sz = (self.img_sample_sz * aug_expansion_factor).long()
            aug_expansion_sz += (aug_expansion_sz - self.img_sample_sz.long()) % 2
            aug_expansion_sz = aug_expansion_sz.float()
            aug_output_sz = self.img_sample_sz.long().tolist()

        # Random shift for each sample
        get_rand_shift = lambda: None
        random_shift_factor = getattr(self.p, 'random_shift_factor', 0)
        if random_shift_factor > 0:
            get_rand_shift = lambda: ((torch.rand(2) - 0.5) * self.img_sample_sz * random_shift_factor + global_shift).long().tolist()

        # Always put identity transformation first, since it is the unaugmented sample that is always used
        self.transforms = [augmentation.Identity(aug_output_sz, global_shift.long().tolist())]

        augs = self.p.augmentation if getattr(self.p, 'use_augmentation', True) else {}

        # Add all augmentations
        if 'shift' in augs:
            self.transforms.extend([augmentation.Translation(shift, aug_output_sz, global_shift.long().tolist()) for shift in augs['shift']])
        if 'relativeshift' in augs:
            get_absolute = lambda shift: (torch.Tensor(shift) * self.img_sample_sz/2).long().tolist()
            self.transforms.extend([augmentation.Translation(get_absolute(shift), aug_output_sz, global_shift.long().tolist()) for shift in augs['relativeshift']])
        if 'fliplr' in augs and augs['fliplr']:
            self.transforms.append(augmentation.FlipHorizontal(aug_output_sz, get_rand_shift()))
        if 'blur' in augs:
            self.transforms.extend([augmentation.Blur(sigma, aug_output_sz, get_rand_shift()) for sigma in augs['blur']])
        if 'scale' in augs:
            self.transforms.extend([augmentation.Scale(scale_factor, aug_output_sz, get_rand_shift()) for scale_factor in augs['scale']])
        if 'rotate' in augs:
            self.transforms.extend([augmentation.Rotate(angle, aug_output_sz, get_rand_shift()) for angle in augs['rotate']])

        # Extract augmented image patches
        im_patches = sample_patch_transformed(im, self.init_sample_pos, self.init_sample_scale, aug_expansion_sz, self.transforms)

        # Extract initial backbone features
        with torch.no_grad():
            init_backbone_feat = self.net.extract_backbone(im_patches)

        return init_backbone_feat",aug_expansion_factor is not None and aug_expansion_factor != 1,None is not aug_expansion_factor != 1
espnet,https://github.com/espnet/espnet/tree/master/espnet2/tts/espnet_model.py,ESPnetTTSModel,inference$223,"def inference(
        self,
        text: torch.Tensor,
        speech: Optional[torch.Tensor] = None,
        spembs: Optional[torch.Tensor] = None,
        sids: Optional[torch.Tensor] = None,
        lids: Optional[torch.Tensor] = None,
        durations: Optional[torch.Tensor] = None,
        pitch: Optional[torch.Tensor] = None,
        energy: Optional[torch.Tensor] = None,
        **decode_config,
    ) -> Dict[str, torch.Tensor]:
        """"""Caclualte features and return them as a dict.

        Args:
            text (Tensor): Text index tensor (T_text).
            speech (Tensor): Speech waveform tensor (T_wav).
            spembs (Optional[Tensor]): Speaker embedding tensor (D,).
            sids (Optional[Tensor]): Speaker ID tensor (1,).
            lids (Optional[Tensor]): Language ID tensor (1,).
            durations (Optional[Tensor): Duration tensor.
            pitch (Optional[Tensor): Pitch tensor.
            energy (Optional[Tensor): Energy tensor.

        Returns:
            Dict[str, Tensor]: Dict of outputs.

        """"""
        input_dict = dict(text=text)
        if decode_config[""use_teacher_forcing""] or getattr(self.tts, ""use_gst"", False):
            if speech is None:
                raise RuntimeError(""missing required argument: 'speech'"")
            if self.feats_extract is not None:
                feats = self.feats_extract(speech[None])[0][0]
            else:
                # Use precalculated feats (feats_type != raw case)
                feats = speech
            if self.normalize is not None:
                feats = self.normalize(feats[None])[0][0]
            input_dict.update(feats=feats)
            if self.tts.require_raw_speech:
                input_dict.update(speech=speech)

        if decode_config[""use_teacher_forcing""]:
            if durations is not None:
                input_dict.update(durations=durations)

            if self.pitch_extract is not None:
                pitch = self.pitch_extract(
                    speech[None],
                    feats_lengths=torch.LongTensor([len(feats)]),
                    durations=durations[None],
                )[0][0]
            if self.pitch_normalize is not None:
                pitch = self.pitch_normalize(pitch[None])[0][0]
            if pitch is not None:
                input_dict.update(pitch=pitch)

            if self.energy_extract is not None:
                energy = self.energy_extract(
                    speech[None],
                    feats_lengths=torch.LongTensor([len(feats)]),
                    durations=durations[None],
                )[0][0]
            if self.energy_normalize is not None:
                energy = self.energy_normalize(energy[None])[0][0]
            if energy is not None:
                input_dict.update(energy=energy)

        if spembs is not None:
            input_dict.update(spembs=spembs)
        if sids is not None:
            input_dict.update(sids=sids)
        if lids is not None:
            input_dict.update(lids=lids)

        output_dict = self.tts.inference(**input_dict, **decode_config)

        if self.normalize is not None and output_dict.get(""feat_gen"") is not None:
            # NOTE: normalize.inverse is in-place operation
            feat_gen_denorm = self.normalize.inverse(
                output_dict[""feat_gen""].clone()[None]
            )[0][0]
            output_dict.update(feat_gen_denorm=feat_gen_denorm)

        return output_dict",self.normalize is not None and output_dict.get('feat_gen') is not None,self.normalize is not None is not output_dict.get('feat_gen')
CenterPoint,https://github.com/tianweiy/CenterPoint/tree/master/det3d/torchie/trainer/checkpoint.py,,load_state_dict$67,"def load_state_dict(module, state_dict, strict=False, logger=None):
    """"""Load state_dict into a module
    """"""
    unexpected_keys = []
    shape_mismatch_pairs = []

    own_state = module.state_dict()

    spconv_keys = find_all_spconv_keys(module)

    for name, param in state_dict.items():

        if name in spconv_keys and name in own_state and own_state[name].shape != param.shape:
            # from https://github.com/acivgin1/OpenPCDet/blob/8fc1a5d57bcb418d71d5118fb3df4b58d4ea0244/pcdet/models/detectors/detector3d_template.py
            # with different spconv versions, we need to adapt weight shapes for spconv blocks
            # adapt spconv weights from version 1.x to version 2.x if you used weights from spconv 1.x

            param_native = param.transpose(-1, -2)  # (k1, k2, k3, c_in, c_out) to (k1, k2, k3, c_out, c_in)
            if param_native.shape == own_state[name].shape:
                param = param_native.contiguous()
            else:
                assert param.shape.__len__() == 5, 'currently only spconv 3D is supported'
                param_implicit = param.permute(4, 0, 1, 2, 3)  # (k1, k2, k3, c_in, c_out) to (c_out, k1, k2, k3, c_in)
                if param_implicit.shape == own_state[name].shape:
                    param = param_implicit.contiguous()


        # a hacky fixed to load a new voxelnet 
        if name not in own_state:
            unexpected_keys.append(name)
            continue
        if isinstance(param, torch.nn.Parameter):
            # backwards compatibility for serialized parameters
            param = param.data
        if param.size() != own_state[name].size():
            shape_mismatch_pairs.append([name, own_state[name].size(), param.size()])
            continue
        own_state[name].copy_(param)

    all_missing_keys = set(own_state.keys()) - set(state_dict.keys())
    # ignore ""num_batches_tracked"" of BN layers
    missing_keys = [key for key in all_missing_keys if ""num_batches_tracked"" not in key]

    err_msg = []
    if unexpected_keys:
        err_msg.append(
            ""unexpected key in source state_dict: {}\n"".format(
                "", "".join(unexpected_keys)
            )
        )
    if missing_keys:
        err_msg.append(
            ""missing keys in source state_dict: {}\n"".format("", "".join(missing_keys))
        )
    if shape_mismatch_pairs:
        mismatch_info = ""these keys have mismatched shape:\n""
        header = [""key"", ""expected shape"", ""loaded shape""]
        table_data = [header] + shape_mismatch_pairs
        table = AsciiTable(table_data)
        err_msg.append(mismatch_info + table.table)

    rank, _ = get_dist_info()
    if len(err_msg) > 0 and rank == 0:
        err_msg.insert(0, ""The model and loaded state dict do not match exactly\n"")
        err_msg = ""\n"".join(err_msg)
        if strict:
            raise RuntimeError(err_msg)
        elif logger is not None:
            logger.warning(err_msg)
        else:
            print(err_msg)",len(err_msg) > 0 and rank == 0,len(err_msg) > 0 == rank
Text-Pastry,https://github.com/duydao/Text-Pastry/tree/master//text_pastry.py,TextPastryDateRangeCommand,run$1257,"def run(self, edit, text, date=None, step_size=""day"", count=None, date_format=None, last_day_of_month=False, repeat=None):
        # support repeats
        repeat = int(repeat) - 1 if repeat else len(self.view.sel()) - 1
        if len(self.view.sel()) == 1 and repeat:
            TextPastryTools.duplicate(self.view, edit, self.view.sel()[0], repeat)
        selection_count = len(self.view.sel())
        match = re.search('^\\d+$', text)
        if count is None and match:
            count = int(match.group(0))
        match = re.search('^([\\d]{1,2}[\\.-/][\\d]{1,2}[\\.-/][\\d]{1,4}) ?(.+)?$', text)
        if match:
            date = self.parse_date(match.group(1))
            fmt = match.group(2)
            if fmt and '%' in fmt:
                date_format = fmt
        if date is None:
            # try to see if text is a date
            if text:
                date = self.parse_date(text)
        if date is None:
            s = None
            # check if selection is a date
            if selection_count == 1:
                s = self.view.substr(self.view.sel()[0]).strip()
            if s:
                date = self.parse_date(s)
        if date is None:
            # use today as date
            date = datetime.datetime.now()
            date = date + datetime.timedelta(hours=-date.hour, minutes=-date.minute, seconds=-date.second, microseconds=-date.microsecond)
        if count is None:
            # use selection count if no count was given/found
            count = selection_count
        else:
            # set boundaries of count
            if count > selection_count and selection_count > 1:
                count = selection_count
        if date_format is None:
            date_format = global_settings('date_format', '%x')
        # generate item list
        items = [self.date(date, step_size, x, last_day_of_month).strftime(date_format) for x in range(count)]
        # create one text entry if single selection
        if selection_count == 1:
            newline = '\r\n' if self.view.line_endings() == 'windows' else '\n'
            items = [newline.join(items)]
        for idx, region in enumerate(self.view.sel()):
            self.view.replace(edit, region, items[idx])",count > selection_count and selection_count > 1,count > selection_count > 1
liquidctl,https://github.com/liquidctl/liquidctl/tree/master/liquidctl/driver/kraken3.py,KrakenZ3,_find_winusb_device$582,"def _find_winusb_device(self, vid, pid, serial):
        winusb_devices = self.bulk_device.list_usb_devices(
            deviceinterface=True, present=True, findparent=True
        )
        for device in winusb_devices:
            if (
                device.path.find(vid + ""&"" + pid) != -1
                and device.parent
                and device.parent.find(serial) != -1
            ):
                self.bulk_device.init_winusb_device_with_path(device.path)
                return True
        return False",device.path.find(vid + '&' + pid) != -1 and device.parent and (device.parent.find(serial) != -1),device.path.find(vid + '&' + pid) != -1 != device.parent.find(serial) and device.parent
snakeware,https://github.com/joshiemoore/snakeware/tree/master/snakewm/wm.py,SnakeWM,run$165,"def run(self):
        clock = pygame.time.Clock()
        running = True

        while running:
            delta = clock.tick(60) / 1000.0

            pressed = pygame.key.get_pressed()

            for event in pygame.event.get():
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_LSUPER:
                        if self.APPMENU is None:
                            # open app menu
                            self.APPMENU = AppMenuPanel(
                                self.MANAGER,
                                (0, 0),
                                ""apps"",
                                self.APPS,
                                self.appmenu_load,
                            )
                        else:
                            # close app menu
                            self.APPMENU.destroy()
                            self.APPMENU = None

                    if pressed[pygame.K_LALT]:
                        if event.key == pygame.K_ESCAPE:
                            running = False
                            pygame.quit()
                            exit()
                        elif event.key == pygame.K_p:
                            # toggle paint mode
                            self.PAINT = not self.PAINT
                            self.BRUSH_SURF.fill((0, 0, 0, 0))
                        elif event.key == pygame.K_d:
                            # toggle dynamic background
                            if self.DYNBG is None and self.DYNBG_MENU is None:
                                self.DYNBG_MENU = SnakeBGMenu(self.MANAGER)
                            elif self.DYNBG is not None:
                                del self.DYNBG
                                self.DYNBG = None

                elif event.type == pygame.MOUSEBUTTONDOWN:
                    if self.PAINT:
                        if event.button == 4:
                            # mouse wheel up
                            if pressed[pygame.K_LALT]:
                                self.PAINT_COLOR = (self.PAINT_COLOR + 1) % len(
                                    self.PAINT_COLOR_LIST
                                )
                            elif pressed[pygame.K_LCTRL]:
                                self.PAINT_SHAPE = (
                                    self.PAINT_SHAPE + 1
                                ) % self.NUM_SHAPES
                            else:
                                self.PAINT_RADIUS += 2
                        elif event.button == 5:
                            # mouse wheel down
                            if pressed[pygame.K_LALT]:
                                self.PAINT_COLOR = (self.PAINT_COLOR - 1) % len(
                                    self.PAINT_COLOR_LIST
                                )
                            elif pressed[pygame.K_LCTRL]:
                                self.PAINT_SHAPE = (
                                    self.PAINT_SHAPE - 1
                                ) % self.NUM_SHAPES
                            else:
                                self.PAINT_RADIUS -= 2
                            if self.PAINT_RADIUS < 2:
                                self.PAINT_RADIUS = 2
                elif event.type == pygame.USEREVENT:
                    if event.user_type == ""window_selected"":
                        # focus selected window
                        if self.FOCUS is not None:
                            self.FOCUS.unfocus()
                        self.FOCUS = event.ui_element
                        self.FOCUS.focus()
                    elif event.user_type == pygame_gui.UI_COLOUR_PICKER_COLOUR_PICKED:
                        if event.ui_object_id == ""#desktop_colour_picker"":
                            # set desktop background color - no alpha channel
                            self.set_bg_color(event.colour[:-1])
                    elif event.user_type == pygame_gui.UI_FILE_DIALOG_PATH_PICKED:
                        if event.ui_object_id == ""#background_picker"":
                            self.set_bg_image(event.text)
                    elif event.user_type == pygame_gui.UI_BUTTON_PRESSED:
                        if ""#bgmenu"" in event.ui_object_id:
                            if ""close_button"" in event.ui_object_id:
                                self.DYNBG_MENU.kill()
                                del self.DYNBG_MENU
                                self.DYNBG_MENU = None
                            elif not ""title_bar"" in event.ui_object_id:
                                selected_bg = event.ui_object_id.split(""."")[1]
                                self.DYNBG = SnakeBG(selected_bg, TESTMODE)
                                self.DYNBG_MENU.kill()
                                del self.DYNBG_MENU
                                self.DYNBG_MENU = None

                                self.PAINT = False

                self.MANAGER.process_events(event)

            self.MANAGER.update(delta)

            # blit paintbrush/dynbg layer
            if self.DYNBG is not None:
                # update dynamic background
                self.DYNBG.draw(self.BRUSH_SURF)
                self.SCREEN.blit(self.BG, (0, 0))
                self.SCREEN.blit(self.BRUSH_SURF, (0, 0))
            elif self.PAINT:
                mpos = pygame.mouse.get_pos()

                # default drawing the brush to the temporary brush layer
                draw_surf = self.BRUSH_SURF

                if pygame.mouse.get_pressed()[0]:
                    # paint to the actual background
                    draw_surf = self.BG

                if self.PAINT_SHAPE == 0:
                    # circle
                    pygame.draw.circle(
                        draw_surf,
                        self.PAINT_COLOR_LIST[self.PAINT_COLOR],
                        mpos,
                        self.PAINT_RADIUS,
                    )
                elif self.PAINT_SHAPE == 1:
                    # square
                    pygame.draw.rect(
                        draw_surf,
                        self.PAINT_COLOR_LIST[self.PAINT_COLOR],
                        pygame.Rect(
                            (mpos[0] - self.PAINT_RADIUS, mpos[1] - self.PAINT_RADIUS),
                            (self.PAINT_RADIUS * 2, self.PAINT_RADIUS * 2),
                        ),
                    )
                elif self.PAINT_SHAPE == 2:
                    # triangle
                    pygame.draw.polygon(
                        draw_surf,
                        self.PAINT_COLOR_LIST[self.PAINT_COLOR],
                        (
                            (mpos[0] - self.PAINT_RADIUS, mpos[1] + self.PAINT_RADIUS),
                            (mpos[0] + self.PAINT_RADIUS, mpos[1] + self.PAINT_RADIUS),
                            (mpos[0], mpos[1] - self.PAINT_RADIUS),
                        ),
                    )

                self.SCREEN.blit(self.BG, (0, 0))
                self.SCREEN.blit(self.BRUSH_SURF, (0, 0))
                self.BRUSH_SURF.fill((0, 0, 0, 0))
            else:
                # not in paint mode, just blit background
                self.SCREEN.blit(self.BG, (0, 0))

            self.MANAGER.draw_ui(self.SCREEN)
            pygame.display.update()",self.DYNBG is None and self.DYNBG_MENU is None,self.DYNBG is None is self.DYNBG_MENU
awx,https://github.com/ansible/awx/tree/master/awx/main/scheduler/dag_simple.py,SimpleDAG,has_cycle$169,"def has_cycle(self):
        node_objs = [node['node_object'] for node in self.get_root_nodes()]
        node_objs_visited = set([])
        path = set([])
        stack = node_objs
        res = False

        if len(self.nodes) != 0 and len(node_objs) == 0:
            return True

        while stack:
            node_obj = stack.pop()

            children = [node['node_object'] for node in self.get_children(node_obj)]
            children_to_add = list(filter(lambda node_obj: node_obj not in node_objs_visited, children))

            if children_to_add:
                if node_obj in path:
                    res = True
                    break
                path.add(node_obj)
                stack.append(node_obj)
                stack.extend(children_to_add)
            else:
                node_objs_visited.add(node_obj)
                path.discard(node_obj)
        return res",len(self.nodes) != 0 and len(node_objs) == 0,len(self.nodes) != 0 == len(node_objs)
tartube,https://github.com/axcore/tartube/tree/master/tartube/mainapp.py,TartubeApp,mark_video_missing$16340,"def mark_video_missing(self, video_obj, missing_flag, \
    no_update_index_flag=False, no_update_catalogue_flag=False, \
    no_sort_flag=False):

        """"""Can be called by anything.

        Marks a video object as missing or not missing. (A video is missing if
        it has been downloaded from a channel/playlist by the user, but has
        since been removed from that channel/playlist by its creator).

        The video object's .missing_flag IV is updated.

        Args:

            video_obj (media.Video): The media.Video object to mark

            missing_flag (bool): True to mark the video as missing, False to
                mark it as not missing

            no_update_index_flag (bool): True if the Video Index should not be
                updated, because the calling function wants to do that itself

            no_update_catalogue_flag (bool): True if rows in the Video
                Catalogue should not be updated, because the calling function
                wants to redraw the whole catalogue itself

            no_sort_flag (bool): True if the parent container's .child_list
                should not be sorted, because the calling function wants to do
                that itself

        """"""

        # (List of Video Index rows to update, at the end of this function)
        update_list = [self.fixed_missing_folder]
        if not no_update_index_flag:
            update_list.append(video_obj.parent_obj)
            update_list.append(self.fixed_all_folder)
            update_list.append(self.fixed_recent_folder)
            if video_obj.bookmark_flag:
                update_list.append(self.fixed_bookmark_folder)
            if video_obj.fav_flag:
                update_list.append(self.fixed_fav_folder)
            if video_obj.live_mode:
                update_list.append(self.fixed_live_folder)
            if video_obj.new_flag:
                update_list.append(self.fixed_new_folder)
            if video_obj.waiting_flag:
                update_list.append(self.fixed_waiting_folder)

        # Mark the video as missing or not missing
        if not isinstance(video_obj, media.Video):
            return self.system_error(
                161,
                'Mark video as missing request failed sanity check',
            )

        elif not missing_flag:

            # Mark video as not missing
            if not video_obj.missing_flag:

                # Already marked
                return

            else:

                # Update the video object's IVs
                video_obj.set_missing_flag(False)
                # Update the parent object
                video_obj.parent_obj.dec_missing_count()

                # Remove this video from the private 'Missing Videos' folder
                #   (the folder's count IVs are automatically updated)
                self.fixed_missing_folder.del_child(video_obj)
                # Update the Video Catalogue, if that folder is the visible one
                #    (deleting the row, if the 'Missing Videos' folder is
                #   visible)
                if not no_update_catalogue_flag:

                    if self.main_win_obj.video_index_current_dbid is not None \
                    and self.main_win_obj.video_index_current_dbid \
                    == self.fixed_missing_folder.dbid:
                        self.main_win_obj.video_catalogue_delete_video(
                            video_obj,
                        )

                    else:
                        GObject.timeout_add(
                            0,
                            self.main_win_obj.video_catalogue_update_video,
                            video_obj,
                        )

                # Update other private folders
                self.fixed_all_folder.dec_missing_count()
                self.fixed_missing_folder.dec_missing_count()
                if video_obj.bookmark_flag:
                    self.fixed_bookmark_folder.dec_missing_count()
                if video_obj.fav_flag:
                    self.fixed_fav_folder.dec_missing_count()
                if video_obj.live_mode:
                    self.fixed_live_folder.dec_missing_count()
                if video_obj.missing_flag:
                    self.fixed_missing_folder.dec_missing_count()
                if video_obj.new_flag:
                    self.fixed_new_folder.dec_missing_count()
                if video_obj in self.fixed_recent_folder.child_list:
                    self.fixed_recent_folder.dec_missing_count()
                if video_obj.waiting_flag:
                    self.fixed_waiting_folder.dec_missing_count()

        else:

            # Mark video as missing (but not if the video is not marked as
            #   downloaded)
            if video_obj.missing_flag or not video_obj.dl_flag:

                # Already marked, or not elligible
                return

            else:

                # Update the video object's IVs
                video_obj.set_missing_flag(True)
                # Update the parent object
                video_obj.parent_obj.inc_missing_count()

                # Add this video to the private 'Missing Videos' folder
                self.fixed_missing_folder.add_child(
                    self,
                    video_obj,
                    no_sort_flag,
                )

                self.fixed_missing_folder.inc_missing_count()
                if video_obj.bookmark_flag:
                    self.fixed_missing_folder.inc_bookmark_count()
                if video_obj.dl_flag:
                    self.fixed_missing_folder.inc_dl_count()
                if video_obj.fav_flag:
                    self.fixed_missing_folder.inc_fav_count()
                if video_obj.live_mode:
                    self.fixed_missing_folder.inc_live_count()
                if video_obj.missing_flag:
                    self.fixed_missing_folder.inc_missing_count()
                if video_obj.new_flag:
                    self.fixed_missing_folder.inc_new_count()
                if video_obj.waiting_flag:
                    self.fixed_missing_folder.inc_waiting_count()

                # Update the Video Catalogue, if that folder is the visible one
                if not no_update_catalogue_flag:
                    GObject.timeout_add(
                        0,
                        self.main_win_obj.video_catalogue_update_video,
                        video_obj,
                    )

                # Update other private folders
                self.fixed_all_folder.inc_missing_count()
                if video_obj.bookmark_flag:
                    self.fixed_bookmark_folder.inc_missing_count()
                if video_obj.fav_flag:
                    self.fixed_fav_folder.inc_missing_count()
                if video_obj.live_mode:
                    self.fixed_live_folder.inc_missing_count()
                if video_obj.missing_flag:
                    self.fixed_missing_folder.inc_missing_count()
                if video_obj.new_flag:
                    self.fixed_new_folder.inc_missing_count()
                if video_obj in self.fixed_recent_folder.child_list:
                    self.fixed_recent_folder.inc_missing_count()
                if video_obj.waiting_flag:
                    self.fixed_waiting_folder.inc_missing_count()

        # Update rows in the Video Index
        for container_obj in update_list:
            GObject.timeout_add(
                0,
                self.main_win_obj.video_index_update_row_text,
                container_obj,
            )",self.main_win_obj.video_index_current_dbid is not None and self.main_win_obj.video_index_current_dbid == self.fixed_missing_folder.dbid,None is not self.main_win_obj.video_index_current_dbid == self.fixed_missing_folder.dbid
discord.py,https://github.com/Rapptz/discord.py/tree/master/discord/ext/commands/context.py,Context,valid$252,"def valid(self) -> bool:
        """""":class:`bool`: Checks if the invocation context is valid to be invoked with.""""""
        return self.prefix is not None and self.command is not None",self.prefix is not None and self.command is not None,self.prefix is not None is not self.command
Imooc-Algorithm-PythonEdition,https://github.com/ShiveryMoon/Imooc-Algorithm-PythonEdition/tree/master/9/用拓扑排序求DAG的最短路径.py,DAGShortestPath,showPath$82,"def showPath(self,w):
        assert w >= 0 and w < len(self.vertDict)
        assert self.hasPathTo(w), 'There is no path!'
        pathlist=self.shortestPath(w)
        print(pathlist)",w >= 0 and w < len(self.vertDict),0 <= w < len(self.vertDict)
aws-data-wrangler,https://github.com/awslabs/aws-data-wrangler/tree/master/awswrangler/quicksight/_describe.py,,describe_dashboard$14,"def describe_dashboard(
    name: Optional[str] = None,
    dashboard_id: Optional[str] = None,
    account_id: Optional[str] = None,
    boto3_session: Optional[boto3.Session] = None,
) -> Dict[str, Any]:
    """"""Describe a QuickSight dashboard by name or ID.

    Note
    ----
    You must pass a not None ``name`` or ``dashboard_id`` argument.

    Parameters
    ----------
    name : str, optional
        Dashboard name.
    dashboard_id : str, optional
        Dashboard ID.
    account_id : str, optional
        If None, the account ID will be inferred from your boto3 session.
    boto3_session : boto3.Session(), optional
        Boto3 Session. The default boto3 session will be used if boto3_session receive None.

    Returns
    -------
    Dict[str, Any]
        Dashboad Description.

    Examples
    --------
    >>> import awswrangler as wr
    >>> description = wr.quicksight.describe_dashboard(name=""my-dashboard"")
    """"""
    if (name is None) and (dashboard_id is None):
        raise exceptions.InvalidArgument(""You must pass a not None name or dashboard_id argument."")
    session: boto3.Session = _utils.ensure_session(session=boto3_session)
    if account_id is None:
        account_id = sts.get_account_id(boto3_session=session)
    if (dashboard_id is None) and (name is not None):
        dashboard_id = get_dashboard_id(name=name, account_id=account_id, boto3_session=session)
    client: boto3.client = _utils.client(service_name=""quicksight"", session=session)
    return cast(
        Dict[str, Any], client.describe_dashboard(AwsAccountId=account_id, DashboardId=dashboard_id)[""Dashboard""]
    )",name is None and dashboard_id is None,name is None is dashboard_id
aws-data-wrangler,https://github.com/awslabs/aws-data-wrangler/tree/master/awswrangler/quicksight/_describe.py,,describe_dashboard$14,"def describe_dashboard(
    name: Optional[str] = None,
    dashboard_id: Optional[str] = None,
    account_id: Optional[str] = None,
    boto3_session: Optional[boto3.Session] = None,
) -> Dict[str, Any]:
    """"""Describe a QuickSight dashboard by name or ID.

    Note
    ----
    You must pass a not None ``name`` or ``dashboard_id`` argument.

    Parameters
    ----------
    name : str, optional
        Dashboard name.
    dashboard_id : str, optional
        Dashboard ID.
    account_id : str, optional
        If None, the account ID will be inferred from your boto3 session.
    boto3_session : boto3.Session(), optional
        Boto3 Session. The default boto3 session will be used if boto3_session receive None.

    Returns
    -------
    Dict[str, Any]
        Dashboad Description.

    Examples
    --------
    >>> import awswrangler as wr
    >>> description = wr.quicksight.describe_dashboard(name=""my-dashboard"")
    """"""
    if (name is None) and (dashboard_id is None):
        raise exceptions.InvalidArgument(""You must pass a not None name or dashboard_id argument."")
    session: boto3.Session = _utils.ensure_session(session=boto3_session)
    if account_id is None:
        account_id = sts.get_account_id(boto3_session=session)
    if (dashboard_id is None) and (name is not None):
        dashboard_id = get_dashboard_id(name=name, account_id=account_id, boto3_session=session)
    client: boto3.client = _utils.client(service_name=""quicksight"", session=session)
    return cast(
        Dict[str, Any], client.describe_dashboard(AwsAccountId=account_id, DashboardId=dashboard_id)[""Dashboard""]
    )",dashboard_id is None and name is not None,dashboard_id is None is not name
LSP,https://github.com/sublimelsp/LSP/tree/master/plugin/core/sessions.py,Session,has_capability$1001,"def has_capability(self, capability: str) -> bool:
        value = self.get_capability(capability)
        return value is not False and value is not None",value is not False and value is not None,False is not value is not None
saleor,https://github.com/saleor/saleor/tree/master/saleor/graphql/shipping/mutations/channels.py,ShippingMethodChannelListingUpdate,clean_input$137,"def clean_input(cls, data, shipping_method, errors):
        cleaned_input = data.get(""add_channels"")
        cls.clean_add_channels(shipping_method, cleaned_input)
        channel_listing_to_update = cls.get_shipping_method_channel_listing_to_update(
            shipping_method.id, cleaned_input
        )
        for channel_input in cleaned_input:
            channel_id = channel_input.get(""channel_id"")
            price_amount = channel_input.pop(""price"", None)
            if price_amount is not None:
                try:
                    validate_price_precision(
                        price_amount, channel_input[""channel""].currency_code
                    )
                    validate_decimal_max_value(price_amount)
                    channel_input[""price_amount""] = price_amount
                except ValidationError as error:
                    error.code = ShippingErrorCode.INVALID.value
                    error.params = {
                        ""channels"": [channel_id],
                    }
                    errors[""price""].append(error)
            else:
                if channel_id not in channel_listing_to_update:
                    errors[""price""].append(
                        ValidationError(
                            ""This field is required."",
                            code=ShippingErrorCode.REQUIRED,
                            params={""channels"": [channel_id]},
                        )
                    )

            min_price = None
            max_price = None
            if ""minimum_order_price"" in channel_input:
                min_price = channel_input.pop(""minimum_order_price"")
                channel_input[""minimum_order_price_amount""] = min_price
            if min_price is not None:
                try:
                    validate_price_precision(
                        min_price, channel_input[""channel""].currency_code
                    )
                    validate_decimal_max_value(min_price)
                except ValidationError as error:
                    error.code = ShippingErrorCode.INVALID.value
                    error.params = {
                        ""channels"": [channel_id],
                    }
                    errors[""minimum_order_price""].append(error)

            if ""maximum_order_price"" in channel_input:
                max_price = channel_input.pop(""maximum_order_price"")
                channel_input[""maximum_order_price_amount""] = max_price
            if max_price is not None:
                try:
                    validate_price_precision(
                        max_price, channel_input[""channel""].currency_code
                    )
                    validate_decimal_max_value(max_price)
                except ValidationError as error:
                    error.code = ShippingErrorCode.INVALID.value
                    error.params = {
                        ""channels"": [channel_id],
                    }
                    errors[""maximum_order_price""].append(error)

            if (
                min_price is not None
                and max_price is not None
                and max_price <= min_price
            ):
                errors[""maximum_order_price""].append(
                    ValidationError(
                        (
                            ""Maximum order price should be larger than ""
                            ""the minimum order price.""
                        ),
                        code=ShippingErrorCode.MAX_LESS_THAN_MIN,
                        params={""channels"": [channel_id]},
                    )
                )

        return data",min_price is not None and max_price is not None and (max_price <= min_price),min_price is not None is not max_price <= min_price
atomic,https://github.com/projectatomic/atomic/tree/master/Atomic/rpm_host_install.py,RPMHostInstall,rm_add_files_to_host$89,"def rm_add_files_to_host(old_installed_files_checksum, exports, prefix=""/"", files_template=None, values=None, rename_files=None, use_links=True):
        # if any file was installed on the host delete it
        if old_installed_files_checksum:
            for path, checksum in old_installed_files_checksum.items():
                new_checksum = RPMHostInstall.file_checksum(path)
                if new_checksum != checksum:
                    # Do not delete the file if it was modified.
                    util.write_out(""Will not delete %s as it was manually modified."" % path, lf=""\n"")
                    continue
                try:
                    os.remove(path)
                except OSError:
                    pass

        if not exports:
            return []

        templates_set = set(files_template or [])

        # if there is a directory hostfs/ under exports, copy these files to the host file system.
        hostfs = os.path.join(exports, ""hostfs"")
        new_installed_files_checksum = {}
        if not os.path.exists(hostfs):
            return new_installed_files_checksum

        selinux_hnd = None
        try:
            if os.getuid() == 0 and selinux.is_selinux_enabled() != 0:
                selinux_hnd = selinux.selabel_open(selinux.SELABEL_CTX_FILE, None, 0)

            for root, dirs, files in os.walk(hostfs):
                rel_root_path = os.path.relpath(root, hostfs)
                if not os.path.exists(os.path.join(prefix, rel_root_path)):
                    os.makedirs(os.path.join(prefix, rel_root_path))
                for f in dirs + files:
                    src_file = os.path.join(root, f)
                    dest_path = os.path.join(prefix, rel_root_path, f)
                    rel_dest_path = os.path.join(""/"", rel_root_path, f)

                    # If rename_files is set, rename the destination file
                    if rename_files:
                        rel_dest_path = RPMHostInstall._do_rename_path(rel_dest_path, rename_files)
                        dest_path = os.path.join(prefix or ""/"", os.path.relpath(rel_dest_path, ""/""))

                    if os.path.exists(dest_path):
                        if os.path.isfile(dest_path):
                            util.write_out(""File %s already exists"" % os.path.normpath(dest_path), lf=""\n"")
                        continue

                    if not os.path.exists(os.path.dirname(dest_path)):
                        os.makedirs(os.path.dirname(dest_path))

                    created = False
                    if rel_dest_path in templates_set:
                        with open(src_file, 'r') as src_file_obj:
                            data = src_file_obj.read()

                        if selinux_hnd is not None:
                            ctx = selinux.selabel_lookup_raw(selinux_hnd, str(dest_path), os.stat(src_file).st_mode)
                            selinux.setfscreatecon_raw(ctx[1])

                        util.write_template(src_file, data, values or {}, dest_path)
                        shutil.copymode(src_file, dest_path)
                        created = True
                    else:
                        try_hardlink = use_links and RPMHostInstall._should_use_hard_link(dest_path)
                        created = RPMHostInstall._copyfile(selinux_hnd, src_file, dest_path, try_hardlink=try_hardlink)

                    if created:
                        new_installed_files_checksum[rel_dest_path] = RPMHostInstall.file_checksum(dest_path)
        finally:
            if selinux_hnd is not None:
                selinux.setfscreatecon_raw(None)

        return new_installed_files_checksum",os.getuid() == 0 and selinux.is_selinux_enabled() != 0,os.getuid() == 0 != selinux.is_selinux_enabled()
sympy,https://github.com/sympy/sympy/tree/master/sympy/functions/elementary/hyperbolic.py,atanh,eval$1400,"def eval(cls, arg):
        from sympy.functions.elementary.trigonometric import atan
        arg = sympify(arg)

        if arg.is_Number:
            if arg is S.NaN:
                return S.NaN
            elif arg.is_zero:
                return S.Zero
            elif arg is S.One:
                return S.Infinity
            elif arg is S.NegativeOne:
                return S.NegativeInfinity
            elif arg is S.Infinity:
                return -S.ImaginaryUnit * atan(arg)
            elif arg is S.NegativeInfinity:
                return S.ImaginaryUnit * atan(-arg)
            elif arg.is_negative:
                return -cls(-arg)
        else:
            if arg is S.ComplexInfinity:
                from sympy.calculus.accumulationbounds import AccumBounds
                return S.ImaginaryUnit*AccumBounds(-S.Pi/2, S.Pi/2)

            i_coeff = arg.as_coefficient(S.ImaginaryUnit)

            if i_coeff is not None:
                return S.ImaginaryUnit * atan(i_coeff)
            else:
                if arg.could_extract_minus_sign():
                    return -cls(-arg)

        if arg.is_zero:
            return S.Zero

        if isinstance(arg, tanh) and arg.args[0].is_number:
            z = arg.args[0]
            if z.is_real:
                return z
            r, i = match_real_imag(z)
            if r is not None and i is not None:
                f = floor(2*i/pi)
                even = f.is_even
                m = z - I*f*pi/2
                if even is True:
                    return m
                elif even is False:
                    return m - I*pi/2",r is not None and i is not None,r is not None is not i
pylearn2,https://github.com/lisa-lab/pylearn2/tree/master/pylearn2/format/target_format.py,,_validate_labels$290,"def _validate_labels(labels, ndim):
    """"""
    Validate that the passed label is in a right data type, and convert
    it into the desired shape.

    Parameters
    ----------
    labels : array_like, 1-dimensional (or 2-dimensional (nlabels, 1))
        The integer labels to use to construct the one hot matrix.
    ndim : int
        Number of dimensions the label have.

    Returns
    -------
    labels : ndarray, (nlabels, ) or (nlabels, )
        The resulting label vector.
    """"""
    labels = np.asarray(labels)
    if labels.dtype.kind not in ('u', 'i'):
        raise ValueError(""labels must have int or uint dtype"")
    if ndim == 1 and labels.ndim != 1:
        if labels.ndim == 2 and labels.shape[1] == 1:
            labels = labels.squeeze()
        else:
            raise ValueError(""labels must be 1-dimensional"")
    elif ndim == 2 and labels.ndim != 2:
        raise ValueError(""labels must be 2-dimensional, no ragged ""
                         ""lists-of-lists"")
    return labels",ndim == 1 and labels.ndim != 1,ndim == 1 != labels.ndim
pylearn2,https://github.com/lisa-lab/pylearn2/tree/master/pylearn2/format/target_format.py,,_validate_labels$290,"def _validate_labels(labels, ndim):
    """"""
    Validate that the passed label is in a right data type, and convert
    it into the desired shape.

    Parameters
    ----------
    labels : array_like, 1-dimensional (or 2-dimensional (nlabels, 1))
        The integer labels to use to construct the one hot matrix.
    ndim : int
        Number of dimensions the label have.

    Returns
    -------
    labels : ndarray, (nlabels, ) or (nlabels, )
        The resulting label vector.
    """"""
    labels = np.asarray(labels)
    if labels.dtype.kind not in ('u', 'i'):
        raise ValueError(""labels must have int or uint dtype"")
    if ndim == 1 and labels.ndim != 1:
        if labels.ndim == 2 and labels.shape[1] == 1:
            labels = labels.squeeze()
        else:
            raise ValueError(""labels must be 1-dimensional"")
    elif ndim == 2 and labels.ndim != 2:
        raise ValueError(""labels must be 2-dimensional, no ragged ""
                         ""lists-of-lists"")
    return labels",ndim == 2 and labels.ndim != 2,ndim == 2 != labels.ndim
ctci-solutions,https://github.com/w-hat/ctci-solutions/tree/master/ch-10-sorting-and-searching/03-search-in-rotated-array.py,,rotated_search$3,"def rotated_search(array, item, leftix=0, rightix=None):
  if rightix is None:
    rightix = len(array)
  if rightix <= leftix:
    return None
  middleix = (rightix + leftix) / 2
  left, middle = array[leftix], array[middleix]
  if item == middle:
    return middleix
  if item == left:
    return leftix
  if left > middle:
    if middle < item and item < left:
      return rotated_search(array, item, middleix+1, rightix)
    else:
      return rotated_search(array, item, leftix+1, middleix)
  elif left < item and item < middle:
    return rotated_search(array, item, leftix+1, middleix)
  else:
    return rotated_search(array, item, middleix+1, rightix)",middle < item and item < left,middle < item < left
ctci-solutions,https://github.com/w-hat/ctci-solutions/tree/master/ch-10-sorting-and-searching/03-search-in-rotated-array.py,,rotated_search$3,"def rotated_search(array, item, leftix=0, rightix=None):
  if rightix is None:
    rightix = len(array)
  if rightix <= leftix:
    return None
  middleix = (rightix + leftix) / 2
  left, middle = array[leftix], array[middleix]
  if item == middle:
    return middleix
  if item == left:
    return leftix
  if left > middle:
    if middle < item and item < left:
      return rotated_search(array, item, middleix+1, rightix)
    else:
      return rotated_search(array, item, leftix+1, middleix)
  elif left < item and item < middle:
    return rotated_search(array, item, leftix+1, middleix)
  else:
    return rotated_search(array, item, middleix+1, rightix)",left < item and item < middle,left < item < middle
fastHan,https://github.com/fastnlp/fastHan/tree/master/fastHan/model/model.py,CharModel,_generate_embedding$62,"def _generate_embedding(self,feats,word_lens,seq_len,pos):
        device=feats.device
        new_feats=[]
        batch_size=feats.size()[0]
        sentence_length=feats.size()[1]
        if self.use_average==False:
            for i in range(batch_size):
                new_feats.append(torch.index_select(feats[i],0,word_lens[i]))
            new_feats=torch.stack(new_feats,0)
        else:
            for i in range(batch_size):
                feats_for_one_sample=[]
                for j in range(word_lens.size()[1]):
                    if word_lens[i][j]==0 and j!=0:
                        feats_for_one_word=torch.zeros(feats.size()[-1]).to(device)
                    else:
                        if j==word_lens.size()[1]-1 or word_lens[i][j+1]==0:
                            index=range(word_lens[i][j],seq_len[i])
                        else:
                            index=range(word_lens[i][j],word_lens[i][j+1])
                        
                        index=torch.tensor(index)
                        index=index.to(device)
                        feats_for_one_word=torch.index_select(feats[i],0,index)
                        feats_for_one_word=torch.mean(feats_for_one_word,dim=0)
                        feats_for_one_word=feats_for_one_word.to(device)
                    feats_for_one_sample.append(feats_for_one_word)
                feats_for_one_sample=torch.stack(feats_for_one_sample,dim=0)
                new_feats.append(feats_for_one_sample)
            new_feats=torch.stack(new_feats,0)
        if self.use_pos_embedding:
            pos_feats=self.pos_embedding(pos)
            new_feats=new_feats+pos_feats
        return new_feats",word_lens[i][j] == 0 and j != 0,word_lens[i][j] == 0 != j
PolarMask,https://github.com/xieenze/PolarMask/tree/master/mmdet/models/anchor_heads/fcos_instance_head_miou_mskctness.py,FCOS_Instance_Head_MIOU_MSKCTNESS,get_bboxes_single$415,"def get_bboxes_single(self,
                          cls_scores,
                          bbox_preds,
                          mask_preds,
                          centernesses,
                          mlvl_points,
                          img_shape,
                          scale_factor,
                          cfg,
                          rescale=False):
        assert len(cls_scores) == len(bbox_preds) == len(mlvl_points)
        mlvl_bboxes = []
        mlvl_scores = []
        mlvl_masks = []
        mlvl_centerness = []
        for cls_score, bbox_pred, mask_pred, centerness, points in zip(
                cls_scores, bbox_preds, mask_preds, centernesses, mlvl_points):
            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]
            scores = cls_score.permute(1, 2, 0).reshape(
                -1, self.cls_out_channels).sigmoid()

            centerness = centerness.permute(1, 2, 0).reshape(-1).sigmoid()
            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)
            mask_pred = mask_pred.permute(1, 2, 0).reshape(-1, 36)
            nms_pre = cfg.get('nms_pre', -1)
            if nms_pre > 0 and scores.shape[0] > nms_pre:
                max_scores, _ = (scores * centerness[:, None]).max(dim=1)
                _, topk_inds = max_scores.topk(nms_pre)
                points = points[topk_inds, :]
                bbox_pred = bbox_pred[topk_inds, :]
                mask_pred = mask_pred[topk_inds, :]
                scores = scores[topk_inds, :]
                centerness = centerness[topk_inds]
            bboxes = distance2bbox(points, bbox_pred, max_shape=img_shape)
            masks = distance2mask(points, mask_pred, self.angles, max_shape=img_shape)
            
            mlvl_bboxes.append(bboxes)
            mlvl_scores.append(scores)
            mlvl_centerness.append(centerness)
            mlvl_masks.append(masks)

        mlvl_bboxes = torch.cat(mlvl_bboxes)
        mlvl_masks = torch.cat(mlvl_masks)
        if rescale:
            _mlvl_bboxes = mlvl_bboxes/ mlvl_bboxes.new_tensor(scale_factor)
            try:
                scale_factor = torch.Tensor(scale_factor)[:2].cuda().unsqueeze(1).repeat(1, 36)
                _mlvl_masks = mlvl_masks / scale_factor
            except:
                _mlvl_masks = mlvl_masks / mlvl_masks.new_tensor(scale_factor)

        mlvl_scores = torch.cat(mlvl_scores)
        padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)
        mlvl_scores = torch.cat([padding, mlvl_scores], dim=1)
        mlvl_centerness = torch.cat(mlvl_centerness)



        centerness_factor = 0.5 #mask centerness is smaller than origin centerness, so add a constant is important or the score will be too low.
        '''1 mask->min bbox->nms, performance same to origin box'''
        # a = _mlvl_masks
        # _mlvl_bboxes = torch.stack([a[:, 0].min(1)[0],a[:, 1].min(1)[0],a[:, 0].max(1)[0],a[:, 1].max(1)[0]],-1)
        # det_bboxes, det_labels, det_masks = multiclass_nms_with_mask(
        #     _mlvl_bboxes,
        #     mlvl_scores,
        #     _mlvl_masks,
        #     cfg.score_thr,
        #     cfg.nms,
        #     cfg.max_per_img,
        #     score_factors=mlvl_centerness + centerness_factor)

        '''2 origin bbox->nms, performance same to origin box'''
        det_bboxes, det_labels, det_masks = multiclass_nms_with_mask(
            _mlvl_bboxes,
            mlvl_scores,
            _mlvl_masks,
            cfg.score_thr,
            cfg.nms,
            cfg.max_per_img,
            score_factors=mlvl_centerness + centerness_factor)

        return det_bboxes, det_labels, det_masks",nms_pre > 0 and scores.shape[0] > nms_pre,scores.shape[0] > nms_pre > 0
astropy,https://github.com/astropy/astropy/tree/master/astropy/time/tests/test_basic.py,,test_len_size$1555,"def test_len_size():
    """"""Check length of Time objects and that scalar ones do not have one.""""""
    t = Time(np.arange(50000, 50010), format=""mjd"", scale=""utc"")
    assert len(t) == 10 and t.size == 10
    t1 = Time(np.arange(50000, 50010).reshape(2, 5), format=""mjd"", scale=""utc"")
    assert len(t1) == 2 and t1.size == 10
    # Can have length 1 or length 0 arrays.
    t2 = t[:1]
    assert len(t2) == 1 and t2.size == 1
    t3 = t[:0]
    assert len(t3) == 0 and t3.size == 0
    # But cannot get length from scalar.
    t4 = t[0]
    with pytest.raises(TypeError) as err:
        len(t4)
    # Ensure we're not just getting the old error of
    # ""object of type 'float' has no len()"".
    assert ""Time"" in str(err.value)",len(t) == 10 and t.size == 10,len(t) == 10 == t.size
astropy,https://github.com/astropy/astropy/tree/master/astropy/time/tests/test_basic.py,,test_len_size$1555,"def test_len_size():
    """"""Check length of Time objects and that scalar ones do not have one.""""""
    t = Time(np.arange(50000, 50010), format=""mjd"", scale=""utc"")
    assert len(t) == 10 and t.size == 10
    t1 = Time(np.arange(50000, 50010).reshape(2, 5), format=""mjd"", scale=""utc"")
    assert len(t1) == 2 and t1.size == 10
    # Can have length 1 or length 0 arrays.
    t2 = t[:1]
    assert len(t2) == 1 and t2.size == 1
    t3 = t[:0]
    assert len(t3) == 0 and t3.size == 0
    # But cannot get length from scalar.
    t4 = t[0]
    with pytest.raises(TypeError) as err:
        len(t4)
    # Ensure we're not just getting the old error of
    # ""object of type 'float' has no len()"".
    assert ""Time"" in str(err.value)",len(t2) == 1 and t2.size == 1,len(t2) == 1 == t2.size
astropy,https://github.com/astropy/astropy/tree/master/astropy/time/tests/test_basic.py,,test_len_size$1555,"def test_len_size():
    """"""Check length of Time objects and that scalar ones do not have one.""""""
    t = Time(np.arange(50000, 50010), format=""mjd"", scale=""utc"")
    assert len(t) == 10 and t.size == 10
    t1 = Time(np.arange(50000, 50010).reshape(2, 5), format=""mjd"", scale=""utc"")
    assert len(t1) == 2 and t1.size == 10
    # Can have length 1 or length 0 arrays.
    t2 = t[:1]
    assert len(t2) == 1 and t2.size == 1
    t3 = t[:0]
    assert len(t3) == 0 and t3.size == 0
    # But cannot get length from scalar.
    t4 = t[0]
    with pytest.raises(TypeError) as err:
        len(t4)
    # Ensure we're not just getting the old error of
    # ""object of type 'float' has no len()"".
    assert ""Time"" in str(err.value)",len(t3) == 0 and t3.size == 0,len(t3) == 0 == t3.size
python-spider,https://github.com/Jack-Cherish/python-spider/tree/master/2020/bilibili/xml2ass.py,,WriteCommentAcfunPositioned$319,"def WriteCommentAcfunPositioned(f, c, width, height, styleid):
    AcfunPlayerSize = (560, 400)
    ZoomFactor = GetZoomFactor(AcfunPlayerSize, (width, height))

    def GetPosition(InputPos, isHeight):
        isHeight = int(isHeight)  # True -> 1
        return AcfunPlayerSize[isHeight]*ZoomFactor[0]*InputPos*0.001+ZoomFactor[isHeight+1]

    def GetTransformStyles(x=None, y=None, scale_x=None, scale_y=None, rotate_z=None, rotate_y=None, color=None, alpha=None):
        styles = []
        if x is not None and y is not None:
            styles.append('\\pos(%s, %s)' % (x, y))
        if scale_x is not None:
            styles.append('\\fscx%s' % scale_x)
        if scale_y is not None:
            styles.append('\\fscy%s' % scale_y)
        if rotate_z is not None and rotate_y is not None:
            assert x is not None
            assert y is not None
            styles.append('\\frx%s\\fry%s\\frz%s\\fax%s\\fay%s' % ConvertFlashRotation(rotate_y, rotate_z, (x-ZoomFactor[1])/(width-ZoomFactor[1]*2), (y-ZoomFactor[2])/(height-ZoomFactor[2]*2)))
        if color is not None:
            styles.append('\\c&H%02X%02X%02X&' % (color & 0xff, (color >> 8) & 0xff, (color >> 16) & 0xff))
            if color == 0x000000:
                styles.append('\\3c&HFFFFFF&')
        if alpha is not None:
            alpha = 255-round(alpha*255)
            styles.append('\\alpha&H%02X' % alpha)
        return styles

    def FlushCommentLine(f, text, styles, start_time, end_time, styleid):
        if end_time > start_time:
            f.write('Dialogue: -1,%(start)s,%(end)s,%(styleid)s,,0,0,0,,{%(styles)s}%(text)s\n' % {'start': ConvertTimestamp(start_time), 'end': ConvertTimestamp(end_time), 'styles': ''.join(styles), 'text': text, 'styleid': styleid})

    try:
        comment_args = c[3]
        text = ASSEscape(str(comment_args['n']).replace('\r', '\n').replace('\r', '\n'))
        common_styles = []
        anchor = {0: 7, 1: 8, 2: 9, 3: 4, 4: 5, 5: 6, 6: 1, 7: 2, 8: 3}.get(comment_args.get('c', 0), 7)
        if anchor != 7:
            common_styles.append('\\an%s' % anchor)
        font = comment_args.get('w')
        if font:
            font = dict(font)
            fontface = font.get('f')
            if fontface:
                common_styles.append('\\fn%s' % ASSEscape(str(fontface)))
            fontbold = bool(font.get('b'))
            if fontbold:
                common_styles.append('\\b1')
        common_styles.append('\\fs%s' % round(c[6]*ZoomFactor[0]))
        isborder = bool(comment_args.get('b', True))
        if not isborder:
            common_styles.append('\\bord0')
        to_pos = dict(comment_args.get('p', {'x': 0, 'y': 0}))
        to_x = round(GetPosition(int(to_pos.get('x', 0)), False))
        to_y = round(GetPosition(int(to_pos.get('y', 0)), True))
        to_scale_x = round(float(comment_args.get('e', 1.0))*100)
        to_scale_y = round(float(comment_args.get('f', 1.0))*100)
        to_rotate_z = float(comment_args.get('r', 0.0))
        to_rotate_y = float(comment_args.get('k', 0.0))
        to_color = c[5]
        to_alpha = float(comment_args.get('a', 1.0))
        from_time = float(comment_args.get('t', 0.0))
        action_time = float(comment_args.get('l', 3.0))
        actions = list(comment_args.get('z', []))
        transform_styles = GetTransformStyles(to_x, to_y, to_scale_x, to_scale_y, to_rotate_z, to_rotate_y, to_color, to_alpha)
        FlushCommentLine(f, text, common_styles+transform_styles, c[0]+from_time, c[0]+from_time+action_time, styleid)
        for action in actions:
            action = dict(action)
            from_x, from_y = to_x, to_y
            from_scale_x, from_scale_y = to_scale_x, to_scale_y
            from_rotate_z, from_rotate_y = to_rotate_z, to_rotate_y
            from_color, from_alpha = to_color, to_alpha
            from_time += action_time
            action_time = float(action.get('l', 0.0))
            action_styles = []
            if 'x' in action:
                to_x = round(GetPosition(int(action['x']), False))
            if 'y' in action:
                to_y = round(GetPosition(int(action['y']), True))
            if 'f' in action:
                to_scale_x = round(float(action['f'])*100)
                action_styles.append('\\fscx%s' % to_scale_x)
            if 'g' in action:
                to_scale_y = round(float(action['g'])*100)
                action_styles.append('\\fscy%s' % to_scale_y)
            if 'c' in action:
                to_color = int(action['c'])
                action_styles.append('\\c&H%02X%02X%02X&' % (to_color & 0xff, (to_color >> 8) & 0xff, (to_color >> 16) & 0xff))
            if 't' in action:
                to_alpha = float(action['t'])
                action_styles.append('\\alpha&H%02X' % (255-round(to_alpha*255)))
            if 'd' in action:
                to_rotate_z = float(action['d'])
            if 'e' in action:
                to_rotate_y = float(action['e'])
            if ('x' in action) or ('y' in action):
                transform_styles = GetTransformStyles(None, None, from_scale_x, from_scale_y, None, None, from_color, from_alpha)
                transform_styles.append('\\move(%s, %s, %s, %s)' % (from_x, from_y, to_x, to_y))
                action_styles.append('\\frx%s\\fry%s\\frz%s\\fax%s\\fay%s' % ConvertFlashRotation(to_rotate_y, to_rotate_z, (to_x-ZoomFactor[1])/(width-ZoomFactor[1]*2), (to_y-ZoomFactor[2])/(width-ZoomFactor[2]*2)))
            elif ('d' in action) or ('e' in action):
                action_styles.append('\\frx%s\\fry%s\\frz%s\\fax%s\\fay%s' % ConvertFlashRotation(to_rotate_y, to_rotate_z, (to_x-ZoomFactor[1])/(width-ZoomFactor[1]*2), (to_y-ZoomFactor[2])/(width-ZoomFactor[2]*2)))
            else:
                transform_styles = GetTransformStyles(from_x, from_y, from_scale_x, from_scale_y, from_rotate_z, from_rotate_y, from_color, from_alpha)
            if action_styles:
                transform_styles.append('\\t(%s)' % (''.join(action_styles)))
            FlushCommentLine(f, text, common_styles+transform_styles, c[0]+from_time, c[0]+from_time+action_time, styleid)
    except (IndexError, ValueError) as e:
        logging.warning(_('Invalid comment: %r') % c[3])",x is not None and y is not None,x is not None is not y
python-spider,https://github.com/Jack-Cherish/python-spider/tree/master/2020/bilibili/xml2ass.py,,WriteCommentAcfunPositioned$319,"def WriteCommentAcfunPositioned(f, c, width, height, styleid):
    AcfunPlayerSize = (560, 400)
    ZoomFactor = GetZoomFactor(AcfunPlayerSize, (width, height))

    def GetPosition(InputPos, isHeight):
        isHeight = int(isHeight)  # True -> 1
        return AcfunPlayerSize[isHeight]*ZoomFactor[0]*InputPos*0.001+ZoomFactor[isHeight+1]

    def GetTransformStyles(x=None, y=None, scale_x=None, scale_y=None, rotate_z=None, rotate_y=None, color=None, alpha=None):
        styles = []
        if x is not None and y is not None:
            styles.append('\\pos(%s, %s)' % (x, y))
        if scale_x is not None:
            styles.append('\\fscx%s' % scale_x)
        if scale_y is not None:
            styles.append('\\fscy%s' % scale_y)
        if rotate_z is not None and rotate_y is not None:
            assert x is not None
            assert y is not None
            styles.append('\\frx%s\\fry%s\\frz%s\\fax%s\\fay%s' % ConvertFlashRotation(rotate_y, rotate_z, (x-ZoomFactor[1])/(width-ZoomFactor[1]*2), (y-ZoomFactor[2])/(height-ZoomFactor[2]*2)))
        if color is not None:
            styles.append('\\c&H%02X%02X%02X&' % (color & 0xff, (color >> 8) & 0xff, (color >> 16) & 0xff))
            if color == 0x000000:
                styles.append('\\3c&HFFFFFF&')
        if alpha is not None:
            alpha = 255-round(alpha*255)
            styles.append('\\alpha&H%02X' % alpha)
        return styles

    def FlushCommentLine(f, text, styles, start_time, end_time, styleid):
        if end_time > start_time:
            f.write('Dialogue: -1,%(start)s,%(end)s,%(styleid)s,,0,0,0,,{%(styles)s}%(text)s\n' % {'start': ConvertTimestamp(start_time), 'end': ConvertTimestamp(end_time), 'styles': ''.join(styles), 'text': text, 'styleid': styleid})

    try:
        comment_args = c[3]
        text = ASSEscape(str(comment_args['n']).replace('\r', '\n').replace('\r', '\n'))
        common_styles = []
        anchor = {0: 7, 1: 8, 2: 9, 3: 4, 4: 5, 5: 6, 6: 1, 7: 2, 8: 3}.get(comment_args.get('c', 0), 7)
        if anchor != 7:
            common_styles.append('\\an%s' % anchor)
        font = comment_args.get('w')
        if font:
            font = dict(font)
            fontface = font.get('f')
            if fontface:
                common_styles.append('\\fn%s' % ASSEscape(str(fontface)))
            fontbold = bool(font.get('b'))
            if fontbold:
                common_styles.append('\\b1')
        common_styles.append('\\fs%s' % round(c[6]*ZoomFactor[0]))
        isborder = bool(comment_args.get('b', True))
        if not isborder:
            common_styles.append('\\bord0')
        to_pos = dict(comment_args.get('p', {'x': 0, 'y': 0}))
        to_x = round(GetPosition(int(to_pos.get('x', 0)), False))
        to_y = round(GetPosition(int(to_pos.get('y', 0)), True))
        to_scale_x = round(float(comment_args.get('e', 1.0))*100)
        to_scale_y = round(float(comment_args.get('f', 1.0))*100)
        to_rotate_z = float(comment_args.get('r', 0.0))
        to_rotate_y = float(comment_args.get('k', 0.0))
        to_color = c[5]
        to_alpha = float(comment_args.get('a', 1.0))
        from_time = float(comment_args.get('t', 0.0))
        action_time = float(comment_args.get('l', 3.0))
        actions = list(comment_args.get('z', []))
        transform_styles = GetTransformStyles(to_x, to_y, to_scale_x, to_scale_y, to_rotate_z, to_rotate_y, to_color, to_alpha)
        FlushCommentLine(f, text, common_styles+transform_styles, c[0]+from_time, c[0]+from_time+action_time, styleid)
        for action in actions:
            action = dict(action)
            from_x, from_y = to_x, to_y
            from_scale_x, from_scale_y = to_scale_x, to_scale_y
            from_rotate_z, from_rotate_y = to_rotate_z, to_rotate_y
            from_color, from_alpha = to_color, to_alpha
            from_time += action_time
            action_time = float(action.get('l', 0.0))
            action_styles = []
            if 'x' in action:
                to_x = round(GetPosition(int(action['x']), False))
            if 'y' in action:
                to_y = round(GetPosition(int(action['y']), True))
            if 'f' in action:
                to_scale_x = round(float(action['f'])*100)
                action_styles.append('\\fscx%s' % to_scale_x)
            if 'g' in action:
                to_scale_y = round(float(action['g'])*100)
                action_styles.append('\\fscy%s' % to_scale_y)
            if 'c' in action:
                to_color = int(action['c'])
                action_styles.append('\\c&H%02X%02X%02X&' % (to_color & 0xff, (to_color >> 8) & 0xff, (to_color >> 16) & 0xff))
            if 't' in action:
                to_alpha = float(action['t'])
                action_styles.append('\\alpha&H%02X' % (255-round(to_alpha*255)))
            if 'd' in action:
                to_rotate_z = float(action['d'])
            if 'e' in action:
                to_rotate_y = float(action['e'])
            if ('x' in action) or ('y' in action):
                transform_styles = GetTransformStyles(None, None, from_scale_x, from_scale_y, None, None, from_color, from_alpha)
                transform_styles.append('\\move(%s, %s, %s, %s)' % (from_x, from_y, to_x, to_y))
                action_styles.append('\\frx%s\\fry%s\\frz%s\\fax%s\\fay%s' % ConvertFlashRotation(to_rotate_y, to_rotate_z, (to_x-ZoomFactor[1])/(width-ZoomFactor[1]*2), (to_y-ZoomFactor[2])/(width-ZoomFactor[2]*2)))
            elif ('d' in action) or ('e' in action):
                action_styles.append('\\frx%s\\fry%s\\frz%s\\fax%s\\fay%s' % ConvertFlashRotation(to_rotate_y, to_rotate_z, (to_x-ZoomFactor[1])/(width-ZoomFactor[1]*2), (to_y-ZoomFactor[2])/(width-ZoomFactor[2]*2)))
            else:
                transform_styles = GetTransformStyles(from_x, from_y, from_scale_x, from_scale_y, from_rotate_z, from_rotate_y, from_color, from_alpha)
            if action_styles:
                transform_styles.append('\\t(%s)' % (''.join(action_styles)))
            FlushCommentLine(f, text, common_styles+transform_styles, c[0]+from_time, c[0]+from_time+action_time, styleid)
    except (IndexError, ValueError) as e:
        logging.warning(_('Invalid comment: %r') % c[3])",rotate_z is not None and rotate_y is not None,rotate_z is not None is not rotate_y
SpanBERT,https://github.com/facebookresearch/SpanBERT/tree/master/pretraining/fairseq/tokenizer.py,Tokenizer,binarize$75,"def binarize(filename, dict, consumer, tokenize=tokenize_line,
                            append_eos=True, reverse_order=False,
                            offset=0, end=-1):
        nseq, ntok = 0, 0
        replaced = Counter()
        def replaced_consumer(word, idx):
            if idx == dict.unk() and word != dict.unk_word:
                replaced.update([word])
        with open(filename, 'r') as f:
            f.seek(offset)
            # next(f) breaks f.tell(), hence readline() must be used
            line = safe_readline(f)
            while line:
                if end > 0 and f.tell() > end:
                    break
                ids = Tokenizer.tokenize(
                    line=line,
                    dict=dict,
                    tokenize=tokenize,
                    add_if_not_exist=False,
                    consumer=replaced_consumer,
                    reverse_order=reverse_order,
                )
                nseq += 1
                ntok += len(ids)
                consumer(ids)
                line = f.readline()
        return {'nseq': nseq, 'nunk': sum(replaced.values()), 'ntok': ntok, 'replaced': replaced}",end > 0 and f.tell() > end,f.tell() > end > 0
tvm,https://github.com/apache/tvm/tree/master/python/tvm/topi/x86/conv2d.py,,schedule_conv2d_NCHWc$243,"def schedule_conv2d_NCHWc(cfg, outs):
    """"""Create schedule for tensors""""""
    outs = [outs] if isinstance(outs, te.tensor.Tensor) else outs
    s = te.create_schedule([x.op for x in outs])

    def _callback(op):
        if ""conv2d_NCHWc"" in op.tag:
            conv_out = op.output(0)
            kernel_vec = conv_out.op.input_tensors[1]
            data_vec = conv_out.op.input_tensors[0]

            args = [s, cfg, data_vec, kernel_vec, conv_out, outs[0]]
            (
                _,
                _,
                kh,
                kw,
                _,
                _,
            ) = get_const_tuple(kernel_vec.shape)
            if kh == 1 and kw == 1:
                conv2d_avx_1x1._schedule_conv_NCHWc(*args)
            else:
                conv2d_avx_common._schedule_conv_NCHWc(*args)

    traverse_inline(s, outs[0].op, _callback)
    return s",kh == 1 and kw == 1,kh == 1 == kw
azure-cli,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/cosmosdb/custom.py,,cli_cosmosdb_sql_role_assignment_create$2241,"def cli_cosmosdb_sql_role_assignment_create(client,
                                            resource_group_name,
                                            account_name,
                                            scope,
                                            principal_id,
                                            role_assignment_id=None,
                                            role_definition_name=None,
                                            role_definition_id=None,
                                            no_wait=False):
    """"""Creates an Azure Cosmos DB Sql Role Assignment""""""

    if role_definition_id is not None and role_definition_name is not None:
        raise CLIError('Can only provide one out of role_definition_id and role_definition_name.')

    if role_definition_id is None and role_definition_name is None:
        raise CLIError('Providing one out of role_definition_id and role_definition_name is required.')

    if role_definition_name is not None:
        role_definition_id = get_associated_role_definition_id(client, resource_group_name, account_name, role_definition_name)

    sql_role_assignment_create_update_parameters = SqlRoleAssignmentCreateUpdateParameters(
        role_definition_id=role_definition_id,
        scope=scope,
        principal_id=principal_id)

    return sdk_no_wait(no_wait, client.begin_create_update_sql_role_assignment, role_assignment_id, resource_group_name, account_name, sql_role_assignment_create_update_parameters)",role_definition_id is not None and role_definition_name is not None,role_definition_id is not None is not role_definition_name
azure-cli,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/cosmosdb/custom.py,,cli_cosmosdb_sql_role_assignment_create$2241,"def cli_cosmosdb_sql_role_assignment_create(client,
                                            resource_group_name,
                                            account_name,
                                            scope,
                                            principal_id,
                                            role_assignment_id=None,
                                            role_definition_name=None,
                                            role_definition_id=None,
                                            no_wait=False):
    """"""Creates an Azure Cosmos DB Sql Role Assignment""""""

    if role_definition_id is not None and role_definition_name is not None:
        raise CLIError('Can only provide one out of role_definition_id and role_definition_name.')

    if role_definition_id is None and role_definition_name is None:
        raise CLIError('Providing one out of role_definition_id and role_definition_name is required.')

    if role_definition_name is not None:
        role_definition_id = get_associated_role_definition_id(client, resource_group_name, account_name, role_definition_name)

    sql_role_assignment_create_update_parameters = SqlRoleAssignmentCreateUpdateParameters(
        role_definition_id=role_definition_id,
        scope=scope,
        principal_id=principal_id)

    return sdk_no_wait(no_wait, client.begin_create_update_sql_role_assignment, role_assignment_id, resource_group_name, account_name, sql_role_assignment_create_update_parameters)",role_definition_id is None and role_definition_name is None,role_definition_id is None is role_definition_name
EGVSR,https://github.com/Thmen/EGVSR/tree/master/codes/data/unpaired_lmdb_dataset.py,UnpairedLMDBDataset,crop_sequence$94,"def crop_sequence(self, frms):
        csz = self.crop_size

        h, w = frms.shape[-2:]
        assert (csz <= h) and (csz <= w), \
            'the crop size is larger than the image size'

        # crop
        top = random.randint(0, h - csz)
        left = random.randint(0, w - csz)
        pats = frms[..., top: top + csz, left: left + csz]

        return pats",csz <= h and csz <= w,h >= csz <= w
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/layers/detection.py,,multi_box_head$2027,"def multi_box_head(
    inputs,
    image,
    base_size,
    num_classes,
    aspect_ratios,
    min_ratio=None,
    max_ratio=None,
    min_sizes=None,
    max_sizes=None,
    steps=None,
    step_w=None,
    step_h=None,
    offset=0.5,
    variance=[0.1, 0.1, 0.2, 0.2],
    flip=True,
    clip=False,
    kernel_size=1,
    pad=0,
    stride=1,
    name=None,
    min_max_aspect_ratios_order=False,
):
    """"""
        :api_attr: Static Graph

    Base on SSD ((Single Shot MultiBox Detector) algorithm, generate prior boxes,
    regression location and classification confidence on multiple input feature
    maps, then output the concatenate results. The details of this algorithm,
    please refer the section 2.2 of SSD paper `SSD: Single Shot MultiBox Detector
    <https://arxiv.org/abs/1512.02325>`_ .

    Args:
       inputs (list(Variable)|tuple(Variable)): The list of input variables,
           the format of all Variables are 4-D Tensor, layout is NCHW.
           Data type should be float32 or float64.
       image (Variable): The input image, layout is NCHW. Data type should be
           the same as inputs.
       base_size(int): the base_size is input image size. When len(inputs) > 2
           and `min_size` and `max_size` are None, the `min_size` and `max_size`
           are calculated by `baze_size`, 'min_ratio' and `max_ratio`. The
           formula is as follows:

              ..  code-block:: text

                  min_sizes = []
                  max_sizes = []
                  step = int(math.floor(((max_ratio - min_ratio)) / (num_layer - 2)))
                  for ratio in range(min_ratio, max_ratio + 1, step):
                      min_sizes.append(base_size * ratio / 100.)
                      max_sizes.append(base_size * (ratio + step) / 100.)
                      min_sizes = [base_size * .10] + min_sizes
                      max_sizes = [base_size * .20] + max_sizes

       num_classes(int): The number of classes.
       aspect_ratios(list(float) | tuple(float)): the aspect ratios of generated
           prior boxes. The length of input and aspect_ratios must be equal.
       min_ratio(int): the min ratio of generated prior boxes.
       max_ratio(int): the max ratio of generated prior boxes.
       min_sizes(list|tuple|None): If `len(inputs) <=2`,
            min_sizes must be set up, and the length of min_sizes
            should equal to the length of inputs. Default: None.
       max_sizes(list|tuple|None): If `len(inputs) <=2`,
            max_sizes must be set up, and the length of min_sizes
            should equal to the length of inputs. Default: None.
       steps(list|tuple): If step_w and step_h are the same,
            step_w and step_h can be replaced by steps.
       step_w(list|tuple): Prior boxes step
            across width. If step_w[i] == 0.0, the prior boxes step
            across width of the inputs[i] will be automatically
            calculated. Default: None.
       step_h(list|tuple): Prior boxes step across height, If
            step_h[i] == 0.0, the prior boxes step across height of
            the inputs[i] will be automatically calculated. Default: None.
       offset(float): Prior boxes center offset. Default: 0.5
       variance(list|tuple): the variances to be encoded in prior boxes.
            Default:[0.1, 0.1, 0.2, 0.2].
       flip(bool): Whether to flip aspect ratios. Default:False.
       clip(bool): Whether to clip out-of-boundary boxes. Default: False.
       kernel_size(int): The kernel size of conv2d. Default: 1.
       pad(int|list|tuple): The padding of conv2d. Default:0.
       stride(int|list|tuple): The stride of conv2d. Default:1,
       name(str): The default value is None.  Normally there is no need
           for user to set this property.  For more information, please
           refer to :ref:`api_guide_Name`.
       min_max_aspect_ratios_order(bool): If set True, the output prior box is
            in order of [min, max, aspect_ratios], which is consistent with
            Caffe. Please note, this order affects the weights order of
            convolution layer followed by and does not affect the final
            detection results. Default: False.

    Returns:
        tuple: A tuple with four Variables. (mbox_loc, mbox_conf, boxes, variances)

        mbox_loc (Variable): The predicted boxes' location of the inputs. The
        layout is [N, num_priors, 4], where N is batch size, ``num_priors``
        is the number of prior boxes. Data type is the same as input.

        mbox_conf (Variable): The predicted boxes' confidence of the inputs.
        The layout is [N, num_priors, C], where ``N`` and ``num_priors``
        has the same meaning as above. C is the number of Classes.
        Data type is the same as input.

        boxes (Variable): the output prior boxes. The layout is [num_priors, 4].
        The meaning of num_priors is the same as above.
        Data type is the same as input.

        variances (Variable): the expanded variances for prior boxes.
        The layout is [num_priors, 4]. Data type is the same as input.

    Examples 1: set min_ratio and max_ratio:
        .. code-block:: python

          import paddle
          paddle.enable_static()

          images = paddle.static.data(name='data', shape=[None, 3, 300, 300], dtype='float32')
          conv1 = paddle.static.data(name='conv1', shape=[None, 512, 19, 19], dtype='float32')
          conv2 = paddle.static.data(name='conv2', shape=[None, 1024, 10, 10], dtype='float32')
          conv3 = paddle.static.data(name='conv3', shape=[None, 512, 5, 5], dtype='float32')
          conv4 = paddle.static.data(name='conv4', shape=[None, 256, 3, 3], dtype='float32')
          conv5 = paddle.static.data(name='conv5', shape=[None, 256, 2, 2], dtype='float32')
          conv6 = paddle.static.data(name='conv6', shape=[None, 128, 1, 1], dtype='float32')

          mbox_locs, mbox_confs, box, var = paddle.static.nn.multi_box_head(
            inputs=[conv1, conv2, conv3, conv4, conv5, conv6],
            image=images,
            num_classes=21,
            min_ratio=20,
            max_ratio=90,
            aspect_ratios=[[2.], [2., 3.], [2., 3.], [2., 3.], [2.], [2.]],
            base_size=300,
            offset=0.5,
            flip=True,
            clip=True)

    Examples 2: set min_sizes and max_sizes:
        .. code-block:: python

          import paddle
          paddle.enable_static()

          images = paddle.static.data(name='data', shape=[None, 3, 300, 300], dtype='float32')
          conv1 = paddle.static.data(name='conv1', shape=[None, 512, 19, 19], dtype='float32')
          conv2 = paddle.static.data(name='conv2', shape=[None, 1024, 10, 10], dtype='float32')
          conv3 = paddle.static.data(name='conv3', shape=[None, 512, 5, 5], dtype='float32')
          conv4 = paddle.static.data(name='conv4', shape=[None, 256, 3, 3], dtype='float32')
          conv5 = paddle.static.data(name='conv5', shape=[None, 256, 2, 2], dtype='float32')
          conv6 = paddle.static.data(name='conv6', shape=[None, 128, 1, 1], dtype='float32')

          mbox_locs, mbox_confs, box, var = paddle.static.nn.multi_box_head(
            inputs=[conv1, conv2, conv3, conv4, conv5, conv6],
            image=images,
            num_classes=21,
            min_sizes=[60.0, 105.0, 150.0, 195.0, 240.0, 285.0],
            max_sizes=[[], 150.0, 195.0, 240.0, 285.0, 300.0],
            aspect_ratios=[[2.], [2., 3.], [2., 3.], [2., 3.], [2.], [2.]],
            base_size=300,
            offset=0.5,
            flip=True,
            clip=True)

    """"""

    def _reshape_with_axis_(input, axis=1):
        # Note : axis!=0 in current references to this func
        # if axis == 0:
        #     x = paddle.flatten(input, 0, -1)
        #     x = paddle.unsqueeze(x, 0)
        #     return x
        # else:
        x = paddle.flatten(input, axis, -1)
        x = paddle.flatten(x, 0, axis - 1)
        return x

    def _is_list_or_tuple_(data):
        return isinstance(data, list) or isinstance(data, tuple)

    def _is_list_or_tuple_and_equal(data, length, err_info):
        if not (_is_list_or_tuple_(data) and len(data) == length):
            raise ValueError(err_info)

    if not _is_list_or_tuple_(inputs):
        raise ValueError('inputs should be a list or tuple.')

    num_layer = len(inputs)

    if num_layer <= 2:
        assert min_sizes is not None and max_sizes is not None
        assert len(min_sizes) == num_layer and len(max_sizes) == num_layer
    elif min_sizes is None and max_sizes is None:
        min_sizes = []
        max_sizes = []
        step = int(math.floor(((max_ratio - min_ratio)) / (num_layer - 2)))
        for ratio in range(min_ratio, max_ratio + 1, step):
            min_sizes.append(base_size * ratio / 100.0)
            max_sizes.append(base_size * (ratio + step) / 100.0)
        min_sizes = [base_size * 0.10] + min_sizes
        max_sizes = [base_size * 0.20] + max_sizes

    if aspect_ratios:
        _is_list_or_tuple_and_equal(
            aspect_ratios,
            num_layer,
            'aspect_ratios should be list or tuple, and the length of inputs '
            'and aspect_ratios should be the same.',
        )
    if step_h is not None:
        _is_list_or_tuple_and_equal(
            step_h,
            num_layer,
            'step_h should be list or tuple, and the length of inputs and '
            'step_h should be the same.',
        )
    if step_w is not None:
        _is_list_or_tuple_and_equal(
            step_w,
            num_layer,
            'step_w should be list or tuple, and the length of inputs and '
            'step_w should be the same.',
        )
    if steps is not None:
        _is_list_or_tuple_and_equal(
            steps,
            num_layer,
            'steps should be list or tuple, and the length of inputs and '
            'step_w should be the same.',
        )
        step_w = steps
        step_h = steps

    mbox_locs = []
    mbox_confs = []
    box_results = []
    var_results = []
    for i, input in enumerate(inputs):
        min_size = min_sizes[i]
        max_size = max_sizes[i]

        if not _is_list_or_tuple_(min_size):
            min_size = [min_size]
        if not _is_list_or_tuple_(max_size):
            max_size = [max_size]

        aspect_ratio = []
        if aspect_ratios is not None:
            aspect_ratio = aspect_ratios[i]
            if not _is_list_or_tuple_(aspect_ratio):
                aspect_ratio = [aspect_ratio]
        step = [step_w[i] if step_w else 0.0, step_h[i] if step_w else 0.0]

        box, var = prior_box(
            input,
            image,
            min_size,
            max_size,
            aspect_ratio,
            variance,
            flip,
            clip,
            step,
            offset,
            None,
            min_max_aspect_ratios_order,
        )

        box_results.append(box)
        var_results.append(var)

        num_boxes = box.shape[2]

        # get loc
        num_loc_output = num_boxes * 4
        mbox_loc = nn.conv2d(
            input=input,
            num_filters=num_loc_output,
            filter_size=kernel_size,
            padding=pad,
            stride=stride,
        )

        mbox_loc = paddle.transpose(mbox_loc, perm=[0, 2, 3, 1])
        mbox_loc_flatten = paddle.flatten(mbox_loc, 1, -1)
        mbox_locs.append(mbox_loc_flatten)

        # get conf
        num_conf_output = num_boxes * num_classes
        conf_loc = nn.conv2d(
            input=input,
            num_filters=num_conf_output,
            filter_size=kernel_size,
            padding=pad,
            stride=stride,
        )

        conf_loc = paddle.transpose(conf_loc, perm=[0, 2, 3, 1])
        conf_loc_flatten = paddle.flatten(conf_loc, 1, -1)
        mbox_confs.append(conf_loc_flatten)

    if len(box_results) == 1:
        box = box_results[0]
        var = var_results[0]
        mbox_locs_concat = mbox_locs[0]
        mbox_confs_concat = mbox_confs[0]
    else:
        reshaped_boxes = []
        reshaped_vars = []
        for i in range(len(box_results)):
            reshaped_boxes.append(_reshape_with_axis_(box_results[i], axis=3))
            reshaped_vars.append(_reshape_with_axis_(var_results[i], axis=3))

        box = tensor.concat(reshaped_boxes)
        var = tensor.concat(reshaped_vars)
        mbox_locs_concat = tensor.concat(mbox_locs, axis=1)
        mbox_locs_concat = paddle.reshape(mbox_locs_concat, shape=[0, -1, 4])
        mbox_confs_concat = tensor.concat(mbox_confs, axis=1)
        mbox_confs_concat = paddle.reshape(
            mbox_confs_concat, shape=[0, -1, num_classes]
        )

    box.stop_gradient = True
    var.stop_gradient = True
    return mbox_locs_concat, mbox_confs_concat, box, var",min_sizes is not None and max_sizes is not None,min_sizes is not None is not max_sizes
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/layers/detection.py,,multi_box_head$2027,"def multi_box_head(
    inputs,
    image,
    base_size,
    num_classes,
    aspect_ratios,
    min_ratio=None,
    max_ratio=None,
    min_sizes=None,
    max_sizes=None,
    steps=None,
    step_w=None,
    step_h=None,
    offset=0.5,
    variance=[0.1, 0.1, 0.2, 0.2],
    flip=True,
    clip=False,
    kernel_size=1,
    pad=0,
    stride=1,
    name=None,
    min_max_aspect_ratios_order=False,
):
    """"""
        :api_attr: Static Graph

    Base on SSD ((Single Shot MultiBox Detector) algorithm, generate prior boxes,
    regression location and classification confidence on multiple input feature
    maps, then output the concatenate results. The details of this algorithm,
    please refer the section 2.2 of SSD paper `SSD: Single Shot MultiBox Detector
    <https://arxiv.org/abs/1512.02325>`_ .

    Args:
       inputs (list(Variable)|tuple(Variable)): The list of input variables,
           the format of all Variables are 4-D Tensor, layout is NCHW.
           Data type should be float32 or float64.
       image (Variable): The input image, layout is NCHW. Data type should be
           the same as inputs.
       base_size(int): the base_size is input image size. When len(inputs) > 2
           and `min_size` and `max_size` are None, the `min_size` and `max_size`
           are calculated by `baze_size`, 'min_ratio' and `max_ratio`. The
           formula is as follows:

              ..  code-block:: text

                  min_sizes = []
                  max_sizes = []
                  step = int(math.floor(((max_ratio - min_ratio)) / (num_layer - 2)))
                  for ratio in range(min_ratio, max_ratio + 1, step):
                      min_sizes.append(base_size * ratio / 100.)
                      max_sizes.append(base_size * (ratio + step) / 100.)
                      min_sizes = [base_size * .10] + min_sizes
                      max_sizes = [base_size * .20] + max_sizes

       num_classes(int): The number of classes.
       aspect_ratios(list(float) | tuple(float)): the aspect ratios of generated
           prior boxes. The length of input and aspect_ratios must be equal.
       min_ratio(int): the min ratio of generated prior boxes.
       max_ratio(int): the max ratio of generated prior boxes.
       min_sizes(list|tuple|None): If `len(inputs) <=2`,
            min_sizes must be set up, and the length of min_sizes
            should equal to the length of inputs. Default: None.
       max_sizes(list|tuple|None): If `len(inputs) <=2`,
            max_sizes must be set up, and the length of min_sizes
            should equal to the length of inputs. Default: None.
       steps(list|tuple): If step_w and step_h are the same,
            step_w and step_h can be replaced by steps.
       step_w(list|tuple): Prior boxes step
            across width. If step_w[i] == 0.0, the prior boxes step
            across width of the inputs[i] will be automatically
            calculated. Default: None.
       step_h(list|tuple): Prior boxes step across height, If
            step_h[i] == 0.0, the prior boxes step across height of
            the inputs[i] will be automatically calculated. Default: None.
       offset(float): Prior boxes center offset. Default: 0.5
       variance(list|tuple): the variances to be encoded in prior boxes.
            Default:[0.1, 0.1, 0.2, 0.2].
       flip(bool): Whether to flip aspect ratios. Default:False.
       clip(bool): Whether to clip out-of-boundary boxes. Default: False.
       kernel_size(int): The kernel size of conv2d. Default: 1.
       pad(int|list|tuple): The padding of conv2d. Default:0.
       stride(int|list|tuple): The stride of conv2d. Default:1,
       name(str): The default value is None.  Normally there is no need
           for user to set this property.  For more information, please
           refer to :ref:`api_guide_Name`.
       min_max_aspect_ratios_order(bool): If set True, the output prior box is
            in order of [min, max, aspect_ratios], which is consistent with
            Caffe. Please note, this order affects the weights order of
            convolution layer followed by and does not affect the final
            detection results. Default: False.

    Returns:
        tuple: A tuple with four Variables. (mbox_loc, mbox_conf, boxes, variances)

        mbox_loc (Variable): The predicted boxes' location of the inputs. The
        layout is [N, num_priors, 4], where N is batch size, ``num_priors``
        is the number of prior boxes. Data type is the same as input.

        mbox_conf (Variable): The predicted boxes' confidence of the inputs.
        The layout is [N, num_priors, C], where ``N`` and ``num_priors``
        has the same meaning as above. C is the number of Classes.
        Data type is the same as input.

        boxes (Variable): the output prior boxes. The layout is [num_priors, 4].
        The meaning of num_priors is the same as above.
        Data type is the same as input.

        variances (Variable): the expanded variances for prior boxes.
        The layout is [num_priors, 4]. Data type is the same as input.

    Examples 1: set min_ratio and max_ratio:
        .. code-block:: python

          import paddle
          paddle.enable_static()

          images = paddle.static.data(name='data', shape=[None, 3, 300, 300], dtype='float32')
          conv1 = paddle.static.data(name='conv1', shape=[None, 512, 19, 19], dtype='float32')
          conv2 = paddle.static.data(name='conv2', shape=[None, 1024, 10, 10], dtype='float32')
          conv3 = paddle.static.data(name='conv3', shape=[None, 512, 5, 5], dtype='float32')
          conv4 = paddle.static.data(name='conv4', shape=[None, 256, 3, 3], dtype='float32')
          conv5 = paddle.static.data(name='conv5', shape=[None, 256, 2, 2], dtype='float32')
          conv6 = paddle.static.data(name='conv6', shape=[None, 128, 1, 1], dtype='float32')

          mbox_locs, mbox_confs, box, var = paddle.static.nn.multi_box_head(
            inputs=[conv1, conv2, conv3, conv4, conv5, conv6],
            image=images,
            num_classes=21,
            min_ratio=20,
            max_ratio=90,
            aspect_ratios=[[2.], [2., 3.], [2., 3.], [2., 3.], [2.], [2.]],
            base_size=300,
            offset=0.5,
            flip=True,
            clip=True)

    Examples 2: set min_sizes and max_sizes:
        .. code-block:: python

          import paddle
          paddle.enable_static()

          images = paddle.static.data(name='data', shape=[None, 3, 300, 300], dtype='float32')
          conv1 = paddle.static.data(name='conv1', shape=[None, 512, 19, 19], dtype='float32')
          conv2 = paddle.static.data(name='conv2', shape=[None, 1024, 10, 10], dtype='float32')
          conv3 = paddle.static.data(name='conv3', shape=[None, 512, 5, 5], dtype='float32')
          conv4 = paddle.static.data(name='conv4', shape=[None, 256, 3, 3], dtype='float32')
          conv5 = paddle.static.data(name='conv5', shape=[None, 256, 2, 2], dtype='float32')
          conv6 = paddle.static.data(name='conv6', shape=[None, 128, 1, 1], dtype='float32')

          mbox_locs, mbox_confs, box, var = paddle.static.nn.multi_box_head(
            inputs=[conv1, conv2, conv3, conv4, conv5, conv6],
            image=images,
            num_classes=21,
            min_sizes=[60.0, 105.0, 150.0, 195.0, 240.0, 285.0],
            max_sizes=[[], 150.0, 195.0, 240.0, 285.0, 300.0],
            aspect_ratios=[[2.], [2., 3.], [2., 3.], [2., 3.], [2.], [2.]],
            base_size=300,
            offset=0.5,
            flip=True,
            clip=True)

    """"""

    def _reshape_with_axis_(input, axis=1):
        # Note : axis!=0 in current references to this func
        # if axis == 0:
        #     x = paddle.flatten(input, 0, -1)
        #     x = paddle.unsqueeze(x, 0)
        #     return x
        # else:
        x = paddle.flatten(input, axis, -1)
        x = paddle.flatten(x, 0, axis - 1)
        return x

    def _is_list_or_tuple_(data):
        return isinstance(data, list) or isinstance(data, tuple)

    def _is_list_or_tuple_and_equal(data, length, err_info):
        if not (_is_list_or_tuple_(data) and len(data) == length):
            raise ValueError(err_info)

    if not _is_list_or_tuple_(inputs):
        raise ValueError('inputs should be a list or tuple.')

    num_layer = len(inputs)

    if num_layer <= 2:
        assert min_sizes is not None and max_sizes is not None
        assert len(min_sizes) == num_layer and len(max_sizes) == num_layer
    elif min_sizes is None and max_sizes is None:
        min_sizes = []
        max_sizes = []
        step = int(math.floor(((max_ratio - min_ratio)) / (num_layer - 2)))
        for ratio in range(min_ratio, max_ratio + 1, step):
            min_sizes.append(base_size * ratio / 100.0)
            max_sizes.append(base_size * (ratio + step) / 100.0)
        min_sizes = [base_size * 0.10] + min_sizes
        max_sizes = [base_size * 0.20] + max_sizes

    if aspect_ratios:
        _is_list_or_tuple_and_equal(
            aspect_ratios,
            num_layer,
            'aspect_ratios should be list or tuple, and the length of inputs '
            'and aspect_ratios should be the same.',
        )
    if step_h is not None:
        _is_list_or_tuple_and_equal(
            step_h,
            num_layer,
            'step_h should be list or tuple, and the length of inputs and '
            'step_h should be the same.',
        )
    if step_w is not None:
        _is_list_or_tuple_and_equal(
            step_w,
            num_layer,
            'step_w should be list or tuple, and the length of inputs and '
            'step_w should be the same.',
        )
    if steps is not None:
        _is_list_or_tuple_and_equal(
            steps,
            num_layer,
            'steps should be list or tuple, and the length of inputs and '
            'step_w should be the same.',
        )
        step_w = steps
        step_h = steps

    mbox_locs = []
    mbox_confs = []
    box_results = []
    var_results = []
    for i, input in enumerate(inputs):
        min_size = min_sizes[i]
        max_size = max_sizes[i]

        if not _is_list_or_tuple_(min_size):
            min_size = [min_size]
        if not _is_list_or_tuple_(max_size):
            max_size = [max_size]

        aspect_ratio = []
        if aspect_ratios is not None:
            aspect_ratio = aspect_ratios[i]
            if not _is_list_or_tuple_(aspect_ratio):
                aspect_ratio = [aspect_ratio]
        step = [step_w[i] if step_w else 0.0, step_h[i] if step_w else 0.0]

        box, var = prior_box(
            input,
            image,
            min_size,
            max_size,
            aspect_ratio,
            variance,
            flip,
            clip,
            step,
            offset,
            None,
            min_max_aspect_ratios_order,
        )

        box_results.append(box)
        var_results.append(var)

        num_boxes = box.shape[2]

        # get loc
        num_loc_output = num_boxes * 4
        mbox_loc = nn.conv2d(
            input=input,
            num_filters=num_loc_output,
            filter_size=kernel_size,
            padding=pad,
            stride=stride,
        )

        mbox_loc = paddle.transpose(mbox_loc, perm=[0, 2, 3, 1])
        mbox_loc_flatten = paddle.flatten(mbox_loc, 1, -1)
        mbox_locs.append(mbox_loc_flatten)

        # get conf
        num_conf_output = num_boxes * num_classes
        conf_loc = nn.conv2d(
            input=input,
            num_filters=num_conf_output,
            filter_size=kernel_size,
            padding=pad,
            stride=stride,
        )

        conf_loc = paddle.transpose(conf_loc, perm=[0, 2, 3, 1])
        conf_loc_flatten = paddle.flatten(conf_loc, 1, -1)
        mbox_confs.append(conf_loc_flatten)

    if len(box_results) == 1:
        box = box_results[0]
        var = var_results[0]
        mbox_locs_concat = mbox_locs[0]
        mbox_confs_concat = mbox_confs[0]
    else:
        reshaped_boxes = []
        reshaped_vars = []
        for i in range(len(box_results)):
            reshaped_boxes.append(_reshape_with_axis_(box_results[i], axis=3))
            reshaped_vars.append(_reshape_with_axis_(var_results[i], axis=3))

        box = tensor.concat(reshaped_boxes)
        var = tensor.concat(reshaped_vars)
        mbox_locs_concat = tensor.concat(mbox_locs, axis=1)
        mbox_locs_concat = paddle.reshape(mbox_locs_concat, shape=[0, -1, 4])
        mbox_confs_concat = tensor.concat(mbox_confs, axis=1)
        mbox_confs_concat = paddle.reshape(
            mbox_confs_concat, shape=[0, -1, num_classes]
        )

    box.stop_gradient = True
    var.stop_gradient = True
    return mbox_locs_concat, mbox_confs_concat, box, var",len(min_sizes) == num_layer and len(max_sizes) == num_layer,len(min_sizes) == num_layer == len(max_sizes)
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/layers/detection.py,,multi_box_head$2027,"def multi_box_head(
    inputs,
    image,
    base_size,
    num_classes,
    aspect_ratios,
    min_ratio=None,
    max_ratio=None,
    min_sizes=None,
    max_sizes=None,
    steps=None,
    step_w=None,
    step_h=None,
    offset=0.5,
    variance=[0.1, 0.1, 0.2, 0.2],
    flip=True,
    clip=False,
    kernel_size=1,
    pad=0,
    stride=1,
    name=None,
    min_max_aspect_ratios_order=False,
):
    """"""
        :api_attr: Static Graph

    Base on SSD ((Single Shot MultiBox Detector) algorithm, generate prior boxes,
    regression location and classification confidence on multiple input feature
    maps, then output the concatenate results. The details of this algorithm,
    please refer the section 2.2 of SSD paper `SSD: Single Shot MultiBox Detector
    <https://arxiv.org/abs/1512.02325>`_ .

    Args:
       inputs (list(Variable)|tuple(Variable)): The list of input variables,
           the format of all Variables are 4-D Tensor, layout is NCHW.
           Data type should be float32 or float64.
       image (Variable): The input image, layout is NCHW. Data type should be
           the same as inputs.
       base_size(int): the base_size is input image size. When len(inputs) > 2
           and `min_size` and `max_size` are None, the `min_size` and `max_size`
           are calculated by `baze_size`, 'min_ratio' and `max_ratio`. The
           formula is as follows:

              ..  code-block:: text

                  min_sizes = []
                  max_sizes = []
                  step = int(math.floor(((max_ratio - min_ratio)) / (num_layer - 2)))
                  for ratio in range(min_ratio, max_ratio + 1, step):
                      min_sizes.append(base_size * ratio / 100.)
                      max_sizes.append(base_size * (ratio + step) / 100.)
                      min_sizes = [base_size * .10] + min_sizes
                      max_sizes = [base_size * .20] + max_sizes

       num_classes(int): The number of classes.
       aspect_ratios(list(float) | tuple(float)): the aspect ratios of generated
           prior boxes. The length of input and aspect_ratios must be equal.
       min_ratio(int): the min ratio of generated prior boxes.
       max_ratio(int): the max ratio of generated prior boxes.
       min_sizes(list|tuple|None): If `len(inputs) <=2`,
            min_sizes must be set up, and the length of min_sizes
            should equal to the length of inputs. Default: None.
       max_sizes(list|tuple|None): If `len(inputs) <=2`,
            max_sizes must be set up, and the length of min_sizes
            should equal to the length of inputs. Default: None.
       steps(list|tuple): If step_w and step_h are the same,
            step_w and step_h can be replaced by steps.
       step_w(list|tuple): Prior boxes step
            across width. If step_w[i] == 0.0, the prior boxes step
            across width of the inputs[i] will be automatically
            calculated. Default: None.
       step_h(list|tuple): Prior boxes step across height, If
            step_h[i] == 0.0, the prior boxes step across height of
            the inputs[i] will be automatically calculated. Default: None.
       offset(float): Prior boxes center offset. Default: 0.5
       variance(list|tuple): the variances to be encoded in prior boxes.
            Default:[0.1, 0.1, 0.2, 0.2].
       flip(bool): Whether to flip aspect ratios. Default:False.
       clip(bool): Whether to clip out-of-boundary boxes. Default: False.
       kernel_size(int): The kernel size of conv2d. Default: 1.
       pad(int|list|tuple): The padding of conv2d. Default:0.
       stride(int|list|tuple): The stride of conv2d. Default:1,
       name(str): The default value is None.  Normally there is no need
           for user to set this property.  For more information, please
           refer to :ref:`api_guide_Name`.
       min_max_aspect_ratios_order(bool): If set True, the output prior box is
            in order of [min, max, aspect_ratios], which is consistent with
            Caffe. Please note, this order affects the weights order of
            convolution layer followed by and does not affect the final
            detection results. Default: False.

    Returns:
        tuple: A tuple with four Variables. (mbox_loc, mbox_conf, boxes, variances)

        mbox_loc (Variable): The predicted boxes' location of the inputs. The
        layout is [N, num_priors, 4], where N is batch size, ``num_priors``
        is the number of prior boxes. Data type is the same as input.

        mbox_conf (Variable): The predicted boxes' confidence of the inputs.
        The layout is [N, num_priors, C], where ``N`` and ``num_priors``
        has the same meaning as above. C is the number of Classes.
        Data type is the same as input.

        boxes (Variable): the output prior boxes. The layout is [num_priors, 4].
        The meaning of num_priors is the same as above.
        Data type is the same as input.

        variances (Variable): the expanded variances for prior boxes.
        The layout is [num_priors, 4]. Data type is the same as input.

    Examples 1: set min_ratio and max_ratio:
        .. code-block:: python

          import paddle
          paddle.enable_static()

          images = paddle.static.data(name='data', shape=[None, 3, 300, 300], dtype='float32')
          conv1 = paddle.static.data(name='conv1', shape=[None, 512, 19, 19], dtype='float32')
          conv2 = paddle.static.data(name='conv2', shape=[None, 1024, 10, 10], dtype='float32')
          conv3 = paddle.static.data(name='conv3', shape=[None, 512, 5, 5], dtype='float32')
          conv4 = paddle.static.data(name='conv4', shape=[None, 256, 3, 3], dtype='float32')
          conv5 = paddle.static.data(name='conv5', shape=[None, 256, 2, 2], dtype='float32')
          conv6 = paddle.static.data(name='conv6', shape=[None, 128, 1, 1], dtype='float32')

          mbox_locs, mbox_confs, box, var = paddle.static.nn.multi_box_head(
            inputs=[conv1, conv2, conv3, conv4, conv5, conv6],
            image=images,
            num_classes=21,
            min_ratio=20,
            max_ratio=90,
            aspect_ratios=[[2.], [2., 3.], [2., 3.], [2., 3.], [2.], [2.]],
            base_size=300,
            offset=0.5,
            flip=True,
            clip=True)

    Examples 2: set min_sizes and max_sizes:
        .. code-block:: python

          import paddle
          paddle.enable_static()

          images = paddle.static.data(name='data', shape=[None, 3, 300, 300], dtype='float32')
          conv1 = paddle.static.data(name='conv1', shape=[None, 512, 19, 19], dtype='float32')
          conv2 = paddle.static.data(name='conv2', shape=[None, 1024, 10, 10], dtype='float32')
          conv3 = paddle.static.data(name='conv3', shape=[None, 512, 5, 5], dtype='float32')
          conv4 = paddle.static.data(name='conv4', shape=[None, 256, 3, 3], dtype='float32')
          conv5 = paddle.static.data(name='conv5', shape=[None, 256, 2, 2], dtype='float32')
          conv6 = paddle.static.data(name='conv6', shape=[None, 128, 1, 1], dtype='float32')

          mbox_locs, mbox_confs, box, var = paddle.static.nn.multi_box_head(
            inputs=[conv1, conv2, conv3, conv4, conv5, conv6],
            image=images,
            num_classes=21,
            min_sizes=[60.0, 105.0, 150.0, 195.0, 240.0, 285.0],
            max_sizes=[[], 150.0, 195.0, 240.0, 285.0, 300.0],
            aspect_ratios=[[2.], [2., 3.], [2., 3.], [2., 3.], [2.], [2.]],
            base_size=300,
            offset=0.5,
            flip=True,
            clip=True)

    """"""

    def _reshape_with_axis_(input, axis=1):
        # Note : axis!=0 in current references to this func
        # if axis == 0:
        #     x = paddle.flatten(input, 0, -1)
        #     x = paddle.unsqueeze(x, 0)
        #     return x
        # else:
        x = paddle.flatten(input, axis, -1)
        x = paddle.flatten(x, 0, axis - 1)
        return x

    def _is_list_or_tuple_(data):
        return isinstance(data, list) or isinstance(data, tuple)

    def _is_list_or_tuple_and_equal(data, length, err_info):
        if not (_is_list_or_tuple_(data) and len(data) == length):
            raise ValueError(err_info)

    if not _is_list_or_tuple_(inputs):
        raise ValueError('inputs should be a list or tuple.')

    num_layer = len(inputs)

    if num_layer <= 2:
        assert min_sizes is not None and max_sizes is not None
        assert len(min_sizes) == num_layer and len(max_sizes) == num_layer
    elif min_sizes is None and max_sizes is None:
        min_sizes = []
        max_sizes = []
        step = int(math.floor(((max_ratio - min_ratio)) / (num_layer - 2)))
        for ratio in range(min_ratio, max_ratio + 1, step):
            min_sizes.append(base_size * ratio / 100.0)
            max_sizes.append(base_size * (ratio + step) / 100.0)
        min_sizes = [base_size * 0.10] + min_sizes
        max_sizes = [base_size * 0.20] + max_sizes

    if aspect_ratios:
        _is_list_or_tuple_and_equal(
            aspect_ratios,
            num_layer,
            'aspect_ratios should be list or tuple, and the length of inputs '
            'and aspect_ratios should be the same.',
        )
    if step_h is not None:
        _is_list_or_tuple_and_equal(
            step_h,
            num_layer,
            'step_h should be list or tuple, and the length of inputs and '
            'step_h should be the same.',
        )
    if step_w is not None:
        _is_list_or_tuple_and_equal(
            step_w,
            num_layer,
            'step_w should be list or tuple, and the length of inputs and '
            'step_w should be the same.',
        )
    if steps is not None:
        _is_list_or_tuple_and_equal(
            steps,
            num_layer,
            'steps should be list or tuple, and the length of inputs and '
            'step_w should be the same.',
        )
        step_w = steps
        step_h = steps

    mbox_locs = []
    mbox_confs = []
    box_results = []
    var_results = []
    for i, input in enumerate(inputs):
        min_size = min_sizes[i]
        max_size = max_sizes[i]

        if not _is_list_or_tuple_(min_size):
            min_size = [min_size]
        if not _is_list_or_tuple_(max_size):
            max_size = [max_size]

        aspect_ratio = []
        if aspect_ratios is not None:
            aspect_ratio = aspect_ratios[i]
            if not _is_list_or_tuple_(aspect_ratio):
                aspect_ratio = [aspect_ratio]
        step = [step_w[i] if step_w else 0.0, step_h[i] if step_w else 0.0]

        box, var = prior_box(
            input,
            image,
            min_size,
            max_size,
            aspect_ratio,
            variance,
            flip,
            clip,
            step,
            offset,
            None,
            min_max_aspect_ratios_order,
        )

        box_results.append(box)
        var_results.append(var)

        num_boxes = box.shape[2]

        # get loc
        num_loc_output = num_boxes * 4
        mbox_loc = nn.conv2d(
            input=input,
            num_filters=num_loc_output,
            filter_size=kernel_size,
            padding=pad,
            stride=stride,
        )

        mbox_loc = paddle.transpose(mbox_loc, perm=[0, 2, 3, 1])
        mbox_loc_flatten = paddle.flatten(mbox_loc, 1, -1)
        mbox_locs.append(mbox_loc_flatten)

        # get conf
        num_conf_output = num_boxes * num_classes
        conf_loc = nn.conv2d(
            input=input,
            num_filters=num_conf_output,
            filter_size=kernel_size,
            padding=pad,
            stride=stride,
        )

        conf_loc = paddle.transpose(conf_loc, perm=[0, 2, 3, 1])
        conf_loc_flatten = paddle.flatten(conf_loc, 1, -1)
        mbox_confs.append(conf_loc_flatten)

    if len(box_results) == 1:
        box = box_results[0]
        var = var_results[0]
        mbox_locs_concat = mbox_locs[0]
        mbox_confs_concat = mbox_confs[0]
    else:
        reshaped_boxes = []
        reshaped_vars = []
        for i in range(len(box_results)):
            reshaped_boxes.append(_reshape_with_axis_(box_results[i], axis=3))
            reshaped_vars.append(_reshape_with_axis_(var_results[i], axis=3))

        box = tensor.concat(reshaped_boxes)
        var = tensor.concat(reshaped_vars)
        mbox_locs_concat = tensor.concat(mbox_locs, axis=1)
        mbox_locs_concat = paddle.reshape(mbox_locs_concat, shape=[0, -1, 4])
        mbox_confs_concat = tensor.concat(mbox_confs, axis=1)
        mbox_confs_concat = paddle.reshape(
            mbox_confs_concat, shape=[0, -1, num_classes]
        )

    box.stop_gradient = True
    var.stop_gradient = True
    return mbox_locs_concat, mbox_confs_concat, box, var",min_sizes is None and max_sizes is None,min_sizes is None is max_sizes
noto-emoji,https://github.com/googlefonts/noto-emoji/tree/master//generate_emoji_name_data.py,,generate_names$276,"def generate_names(
    src_dir, dst_dir, skip_limit=20, omit_groups=None, pretty_print=False,
    verbose=False):
  srcdir = tool_utils.resolve_path(src_dir)
  if not path.isdir(srcdir):
    print('%s is not a directory' % src_dir, file=sys.stderr)
    return

  if omit_groups:
    unknown_groups = set(omit_groups) - set(unicode_data.get_emoji_groups())
    if unknown_groups:
      print('did not recognize %d group%s: %s' % (
          len(unknown_groups), '' if len(unknown_groups) == 1 else 's',
          ', '.join('""%s""' % g for g in omit_groups if g in unknown_groups)), file=sys.stderr)
      print('valid groups are:\n  %s' % (
          '\n  '.join(g for g in unicode_data.get_emoji_groups())), file=sys.stderr)
      return
    print('omitting %d group%s: %s' % (
        len(omit_groups), '' if len(omit_groups) == 1 else 's',
        ', '.join('""%s""' % g for g in omit_groups)))
  else:
    # might be None
    print('keeping all groups')
    omit_groups = []

  # make sure the destination exists
  dstdir = tool_utils.ensure_dir_exists(
      tool_utils.resolve_path(dst_dir))

  # _get_image_data returns canonical cp sequences
  print('src dir:', srcdir)
  seq_to_file = generate_emoji_html._get_image_data(srcdir, 'png', 'emoji_u')
  print('seq to file has %d sequences' % len(seq_to_file))

  # Aliases add non-gendered versions using gendered images for the most part.
  # But when we display the images, we don't distinguish genders in the
  # naming, we rely on the images-- so these look redundant. So we
  # intentionally don't generate images for these.
  # However, the alias file also includes the flag aliases, which we do want,
  # and it also fails to exclude the unknown flag pua (since it doesn't
  # map to anything), so we need to adjust for this.
  canonical_aliases = generate_emoji_html._get_canonical_aliases()

  aliases = set([
      cps for cps in canonical_aliases.keys()
      if not unicode_data.is_regional_indicator_seq(cps)])
  aliases.add((0xfe82b,))  # unknown flag PUA
  excluded = aliases | generate_emoji_html._get_canonical_excluded()

  # The flag aliases have distinct names, so we _do_ want to show them
  # multiple times.
  to_add = {}
  for seq in canonical_aliases:
    if unicode_data.is_regional_indicator_seq(seq):
      replace_seq = canonical_aliases[seq]
      if seq in seq_to_file:
        print('warning, alias %s has file %s' % (
            unicode_data.regional_indicator_seq_to_string(seq),
            seq_to_file[seq]))
        continue
      replace_file = seq_to_file.get(replace_seq)
      if replace_file:
        to_add[seq] = replace_file
  seq_to_file.update(to_add)

  data = []
  last_skipped_group = None
  skipcount = 0
  for group in unicode_data.get_emoji_groups():
    if group in omit_groups:
      continue
    name_data = []
    for seq in unicode_data.get_emoji_in_group(group):
      if seq in excluded:
        continue
      seq_file = seq_to_file.get(seq, None)
      if seq_file is None:
        skipcount += 1
        if verbose:
          if group != last_skipped_group:
            print('group %s' % group)
            last_skipped_group = group
          print('  %s (%s)' % (
              unicode_data.seq_to_string(seq),
              ', '.join(unicode_data.name(cp, 'x') for cp in seq)))
        if skip_limit >= 0 and skipcount > skip_limit:
          raise Exception('skipped too many items')
      else:
        name_data.append(_name_data(seq, seq_file))
    data.append({'category': group, 'emojis': name_data})

  outfile = path.join(dstdir, 'data.json')
  with open(outfile, 'w') as f:
    indent = 2 if pretty_print else None
    separators = None if pretty_print else (',', ':')
    json.dump(data, f, indent=indent, separators=separators)
  print('wrote %s' % outfile)",skip_limit >= 0 and skipcount > skip_limit,skipcount > skip_limit >= 0
graph-rcnn.pytorch,https://github.com/jwyang/graph-rcnn.pytorch/tree/master/lib/scene_parser/rcnn/layers/misc.py,,interpolate$76,"def interpolate(
    input, size=None, scale_factor=None, mode=""nearest"", align_corners=None
):
    if input.numel() > 0:
        return torch.nn.functional.interpolate(
            input, size, scale_factor, mode, align_corners
        )

    def _check_size_scale_factor(dim):
        if size is None and scale_factor is None:
            raise ValueError(""either size or scale_factor should be defined"")
        if size is not None and scale_factor is not None:
            raise ValueError(""only one of size or scale_factor should be defined"")
        if (
            scale_factor is not None
            and isinstance(scale_factor, tuple)
            and len(scale_factor) != dim
        ):
            raise ValueError(
                ""scale_factor shape must match input shape. ""
                ""Input is {}D, scale_factor size is {}"".format(dim, len(scale_factor))
            )

    def _output_size(dim):
        _check_size_scale_factor(dim)
        if size is not None:
            return size
        scale_factors = _ntuple(dim)(scale_factor)
        # math.floor might return float in py2.7
        return [
            int(math.floor(input.size(i + 2) * scale_factors[i])) for i in range(dim)
        ]

    output_shape = tuple(_output_size(2))
    output_shape = input.shape[:-2] + output_shape
    return _NewEmptyTensorOp.apply(input, output_shape)",size is None and scale_factor is None,size is None is scale_factor
graph-rcnn.pytorch,https://github.com/jwyang/graph-rcnn.pytorch/tree/master/lib/scene_parser/rcnn/layers/misc.py,,interpolate$76,"def interpolate(
    input, size=None, scale_factor=None, mode=""nearest"", align_corners=None
):
    if input.numel() > 0:
        return torch.nn.functional.interpolate(
            input, size, scale_factor, mode, align_corners
        )

    def _check_size_scale_factor(dim):
        if size is None and scale_factor is None:
            raise ValueError(""either size or scale_factor should be defined"")
        if size is not None and scale_factor is not None:
            raise ValueError(""only one of size or scale_factor should be defined"")
        if (
            scale_factor is not None
            and isinstance(scale_factor, tuple)
            and len(scale_factor) != dim
        ):
            raise ValueError(
                ""scale_factor shape must match input shape. ""
                ""Input is {}D, scale_factor size is {}"".format(dim, len(scale_factor))
            )

    def _output_size(dim):
        _check_size_scale_factor(dim)
        if size is not None:
            return size
        scale_factors = _ntuple(dim)(scale_factor)
        # math.floor might return float in py2.7
        return [
            int(math.floor(input.size(i + 2) * scale_factors[i])) for i in range(dim)
        ]

    output_shape = tuple(_output_size(2))
    output_shape = input.shape[:-2] + output_shape
    return _NewEmptyTensorOp.apply(input, output_shape)",size is not None and scale_factor is not None,size is not None is not scale_factor
netzob,https://github.com/netzob/netzob/tree/master/netzob/src/netzob/Inference/Vocabulary/CorrelationFinder.py,CorrelationFinder,execute$112,"def execute(self, symbol):
        """"""
        :param symbol: the symbol in which we are looking for correlations
        :type symbol: :class:`netzob.Model.Vocabulary.AbstractField.AbstractField`
        """"""

        (attributeValues_headers,
         attributeValues) = self._generateAttributeValuesForSymbol(symbol)
        symbolResults = []

        # MINE computation of each field's combination
        for i, values_x in enumerate(attributeValues[:-1]):
            for j, values_y in enumerate(attributeValues[i + 1:]):
                mine = MINE(alpha=0.6, c=15)
                mine.compute_score(
                    numpy.array(values_x), numpy.array(values_y))
                mic = round(mine.mic(), 2)
                if mic > float(self.minMic):
                    # We add the relation to the results
                    (x_fields, x_attribute) = attributeValues_headers[i]
                    (y_fields, y_attribute) = attributeValues_headers[j]
                    # The relation should not apply on the same field
                    if len(x_fields) == 1 and len(y_fields) == 1 and x_fields[
                            0].id == y_fields[0].id:
                        continue
                    pearson = numpy.corrcoef(values_x, values_y)[0, 1]
                    if not numpy.isnan(pearson):
                        pearson = round(pearson, 2)
                    relation_type = self._findRelationType(x_attribute,
                                                           y_attribute)
                    self._debug_mine_stats(mine)
                    self._logger.debug(""Correlation found between '"" + str(
                        x_fields) + "":"" + x_attribute + ""' and '"" + str(
                            y_fields) + "":"" + y_attribute + ""'"")
                    self._logger.debug(""  MIC score: "" + str(mic))
                    self._logger.debug(""  Pearson score: "" + str(pearson))
                    id_relation = str(uuid.uuid4())
                    symbolResults.append({
                        'id': id_relation,
                        ""relation_type"": relation_type,
                        'x_fields': x_fields,
                        'x_attribute': x_attribute,
                        'y_fields': y_fields,
                        'y_attribute': y_attribute,
                        'mic': mic,
                        'pearson': pearson
                    })
        return symbolResults",len(x_fields) == 1 and len(y_fields) == 1 and (x_fields[0].id == y_fields[0].id),len(x_fields) == 1 == len(y_fields) and x_fields[0].id == y_fields[0].id
mars,https://github.com/mars-project/mars/tree/master/mars/tensor/statistics/cov.py,,cov$28,"def cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None, aweights=None):
    """"""
    Estimate a covariance matrix, given data and weights.

    Covariance indicates the level to which two variables vary together.
    If we examine N-dimensional samples, :math:`X = [x_1, x_2, ... x_N]^T`,
    then the covariance matrix element :math:`C_{ij}` is the covariance of
    :math:`x_i` and :math:`x_j`. The element :math:`C_{ii}` is the variance
    of :math:`x_i`.

    See the notes for an outline of the algorithm.

    Parameters
    ----------
    m : array_like
        A 1-D or 2-D array containing multiple variables and observations.
        Each row of `m` represents a variable, and each column a single
        observation of all those variables. Also see `rowvar` below.
    y : array_like, optional
        An additional set of variables and observations. `y` has the same form
        as that of `m`.
    rowvar : bool, optional
        If `rowvar` is True (default), then each row represents a
        variable, with observations in the columns. Otherwise, the relationship
        is transposed: each column represents a variable, while the rows
        contain observations.
    bias : bool, optional
        Default normalization (False) is by ``(N - 1)``, where ``N`` is the
        number of observations given (unbiased estimate). If `bias` is True,
        then normalization is by ``N``. These values can be overridden by using
        the keyword ``ddof`` in numpy versions >= 1.5.
    ddof : int, optional
        If not ``None`` the default value implied by `bias` is overridden.
        Note that ``ddof=1`` will return the unbiased estimate, even if both
        `fweights` and `aweights` are specified, and ``ddof=0`` will return
        the simple average. See the notes for the details. The default value
        is ``None``.
    fweights : array_like, int, optional
        1-D tensor of integer freguency weights; the number of times each
        observation vector should be repeated.
    aweights : array_like, optional
        1-D tensor of observation vector weights. These relative weights are
        typically large for observations considered ""important"" and smaller for
        observations considered less ""important"". If ``ddof=0`` the array of
        weights can be used to assign probabilities to observation vectors.

    Returns
    -------
    out : Tensor
        The covariance matrix of the variables.

    See Also
    --------
    corrcoef : Normalized covariance matrix

    Notes
    -----
    Assume that the observations are in the columns of the observation
    array `m` and let ``f = fweights`` and ``a = aweights`` for brevity. The
    steps to compute the weighted covariance are as follows::

        >>> w = f * a
        >>> v1 = mt.sum(w)
        >>> v2 = mt.sum(w * a)
        >>> m -= mt.sum(m * w, axis=1, keepdims=True) / v1
        >>> cov = mt.dot(m * w, m.T) * v1 / (v1**2 - ddof * v2)

    Note that when ``a == 1``, the normalization factor
    ``v1 / (v1**2 - ddof * v2)`` goes over to ``1 / (np.sum(f) - ddof)``
    as it should.

    Examples
    --------
    Consider two variables, :math:`x_0` and :math:`x_1`, which
    correlate perfectly, but in opposite directions:

    >>> import mars.tensor as mt

    >>> x = mt.array([[0, 2], [1, 1], [2, 0]]).T
    >>> x.execute()
    array([[0, 1, 2],
           [2, 1, 0]])

    Note how :math:`x_0` increases while :math:`x_1` decreases. The covariance
    matrix shows this clearly:

    >>> mt.cov(x).execute()
    array([[ 1., -1.],
           [-1.,  1.]])

    Note that element :math:`C_{0,1}`, which shows the correlation between
    :math:`x_0` and :math:`x_1`, is negative.

    Further, note how `x` and `y` are combined:

    >>> x = [-2.1, -1,  4.3]
    >>> y = [3,  1.1,  0.12]
    >>> X = mt.stack((x, y), axis=0)
    >>> print(mt.cov(X).execute())
    [[ 11.71        -4.286     ]
     [ -4.286        2.14413333]]
    >>> print(mt.cov(x, y).execute())
    [[ 11.71        -4.286     ]
     [ -4.286        2.14413333]]
    >>> print(mt.cov(x).execute())
    11.71

    """"""
    from ..merge import vstack
    from ..linalg import dot

    if ddof is not None and ddof != int(ddof):
        raise ValueError(""ddof must be integer"")

    m = astensor(m)
    if m.ndim > 2:
        raise ValueError(""m has more than 2 dimensions"")

    if y is None:
        dtype = np.result_type(m.dtype, np.float64)
    else:
        y = astensor(y)
        if y.ndim > 2:
            raise ValueError(""y has more than 2 dimensions"")
        dtype = np.result_type(m.dtype, y.dtype, np.float64)

    X = array(m, ndmin=2, dtype=dtype)
    if not rowvar and X.shape[0] != 1:
        X = X.T
    if y is not None:
        y = array(y, copy=False, ndmin=2, dtype=dtype)
        if not rowvar and y.shape[0] != 1:
            y = y.T
        X = vstack((X, y))

    if ddof is None:
        if bias == 0:
            ddof = 1
        else:
            ddof = 0

    # Get the product of frequencies and weights
    w = None
    if fweights is not None:
        fweights = astensor(fweights, dtype=float)
        if fweights.ndim > 1:
            raise RuntimeError(""cannot handle multidimensional fweights"")
        if fweights.shape[0] != X.shape[1]:
            raise RuntimeError(""incompatible numbers of samples and fweights"")
        if any(fweights < 0):
            raise ValueError(""fweights cannot be negative"")
        w = fweights
    if aweights is not None:
        aweights = astensor(aweights, dtype=float)
        if aweights.ndim > 1:
            raise RuntimeError(""cannot handle multidimensional aweights"")
        if aweights.shape[0] != X.shape[1]:
            raise RuntimeError(""incompatible numbers of samples and aweights"")
        if any(aweights < 0):
            raise ValueError(""aweights cannot be negative"")
        if w is None:
            w = aweights
        else:
            w *= aweights

    avg, w_sum = average(X, axis=1, weights=w, returned=True)
    w_sum = w_sum[0]

    # Determine the normalization
    if w is None:
        fact = X.shape[1] - ddof
    elif ddof == 0:
        fact = w_sum
    elif aweights is None:
        fact = w_sum - ddof
    else:
        fact = w_sum - ddof * sum(w * aweights) / w_sum

    X -= avg[:, None]
    if w is None:
        X_T = X.T
    else:
        X_T = (X * w).T
    c = dot(X, X_T.conj())
    if isinstance(fact, Tensor):
        fact = where(fact <= 0, 0.0, fact)
        fact = fact.astype(float)
    else:
        if fact <= 0:
            warnings.warn(
                ""Degrees of freedom <= 0 for slice"", RuntimeWarning, stacklevel=2
            )
            fact = 0.0
        fact = np.float64(fact)
    c = c * (1.0 / fact)
    return squeeze(c)",ddof is not None and ddof != int(ddof),None is not ddof != int(ddof)
azure-cli,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/appconfig/_validators.py,,validate_export_profile$275,"def validate_export_profile(namespace):
    if namespace.profile == ImportExportProfiles.KVSET:
        if namespace.destination != 'file':
            raise InvalidArgumentValueError(""The profile '{}' only supports exporting to a file."".format(ImportExportProfiles.KVSET))
        if namespace.format_ != 'json':
            raise InvalidArgumentValueError(""The profile '{}' only supports exporting in the JSON format"".format(ImportExportProfiles.KVSET))
        if namespace.prefix is not None and namespace.prefix != '':
            raise __construct_kvset_invalid_argument_error(is_exporting=True, argument='prefix')
        if namespace.dest_label is not None:
            raise __construct_kvset_invalid_argument_error(is_exporting=True, argument='dest-label')
        if namespace.resolve_keyvault:
            raise __construct_kvset_invalid_argument_error(is_exporting=True, argument='resolve-keyvault')
        if namespace.separator is not None:
            raise __construct_kvset_invalid_argument_error(is_exporting=True, argument='separator')",namespace.prefix is not None and namespace.prefix != '',None is not namespace.prefix != ''
cesium,https://github.com/cesium-ml/cesium/tree/master/cesium/features/num_alias.py,,num_alias$5,"def num_alias(lomb_model):
    """""" Here we check for ""1-day"" aliases in ASAS / Deboss sources.""""""
    alias = [{'per':1.,
              'p_low':0.92,
              'p_high':1.08,
              'alpha_1':8.191855,
              'alpha_2':-7.976243},
             {'per':0.5,
              'p_low':0.48,
              'p_high':0.52,
              'alpha_1':2.438913,
              'alpha_2':0.9837243},
             {'per':0.3333333333,
              'p_low':0.325,
              'p_high':0.342,
              'alpha_1':2.95749,
              'alpha_2':-4.285432},
             {'per':0.25,
              'p_low':0.245,
              'p_high':0.255,
              'alpha_1':1.347657,
              'alpha_2':2.326338}]

    count = 0
    for freq_dict in lomb_model['freq_fits']:
        period = 1. / freq_dict['freq']
        for a in alias:
            cutoff = (a['alpha_1'] / np.power(np.abs(period - a['per']), 0.25)
                      + a['alpha_2'])
            if (period >= a['p_low'] and period <= a['p_high'] and
                freq_dict['signif'] < cutoff):
                count += 1  # this frequency has a ""1 day"" alias (or 0.5 or 0.33
                break  # only need to do this once per period, if an alias is found.
    return count",period >= a['p_low'] and period <= a['p_high'] and (freq_dict['signif'] < cutoff),a['p_low'] <= period <= a['p_high'] and freq_dict['signif'] < cutoff
Mathics,https://github.com/mathics/Mathics/tree/master/mathics/builtin/arithfns/basic.py,Plus,format_plus$285,"def format_plus(self, items, evaluation):
        ""Plus[items__]""

        def negate(item):
            if item.has_form(""Times"", 1, None):
                if isinstance(item.leaves[0], Number):
                    neg = -item.leaves[0]
                    if neg.sameQ(Integer1):
                        if len(item.leaves) == 1:
                            return neg
                        else:
                            return Expression(""Times"", *item.leaves[1:])
                    else:
                        return Expression(""Times"", neg, *item.leaves[1:])
                else:
                    return Expression(""Times"", -1, *item.leaves)
            elif isinstance(item, Number):
                return -item.to_sympy()
            else:
                return Expression(""Times"", -1, item)

        def is_negative(value):
            if isinstance(value, Complex):
                real, imag = value.to_sympy().as_real_imag()
                if real <= 0 and imag <= 0:
                    return True
            elif isinstance(value, Number) and value.to_sympy() < 0:
                return True
            return False

        items = items.get_sequence()
        values = [Expression(""HoldForm"", item) for item in items[:1]]
        ops = []
        for item in items[1:]:
            if (
                item.has_form(""Times"", 1, None) and is_negative(item.leaves[0])
            ) or is_negative(item):
                item = negate(item)
                op = ""-""
            else:
                op = ""+""
            values.append(Expression(""HoldForm"", item))
            ops.append(String(op))
        return Expression(
            ""Infix"",
            Expression(""List"", *values),
            Expression(""List"", *ops),
            310,
            Symbol(""Left""),
        )",real <= 0 and imag <= 0,real <= 0 >= imag
sqlite-utils,https://github.com/simonw/sqlite-utils/tree/master/sqlite_utils/db.py,Database,add_foreign_keys$837,"def add_foreign_keys(self, foreign_keys: Iterable[Tuple[str, str, str, str]]):
        """"""
        See :ref:`python_api_add_foreign_keys`.

        ``foreign_keys`` should be a list of  ``(table, column, other_table, other_column)``
        tuples, see :ref:`python_api_add_foreign_keys`.
        """"""
        # foreign_keys is a list of explicit 4-tuples
        assert all(
            len(fk) == 4 and isinstance(fk, (list, tuple)) for fk in foreign_keys
        ), ""foreign_keys must be a list of 4-tuples, (table, column, other_table, other_column)""

        foreign_keys_to_create = []

        # Verify that all tables and columns exist
        for table, column, other_table, other_column in foreign_keys:
            if not self[table].exists():
                raise AlterError(""No such table: {}"".format(table))
            table_obj = self[table]
            if not isinstance(table_obj, Table):
                raise AlterError(""Must be a table, not a view: {}"".format(table))
            table_obj = cast(Table, table_obj)
            if column not in table_obj.columns_dict:
                raise AlterError(""No such column: {} in {}"".format(column, table))
            if not self[other_table].exists():
                raise AlterError(""No such other_table: {}"".format(other_table))
            if (
                other_column != ""rowid""
                and other_column not in self[other_table].columns_dict
            ):
                raise AlterError(
                    ""No such other_column: {} in {}"".format(other_column, other_table)
                )
            # We will silently skip foreign keys that exist already
            if not any(
                fk
                for fk in table_obj.foreign_keys
                if fk.column == column
                and fk.other_table == other_table
                and fk.other_column == other_column
            ):
                foreign_keys_to_create.append(
                    (table, column, other_table, other_column)
                )

        # Construct SQL for use with ""UPDATE sqlite_master SET sql = ? WHERE name = ?""
        table_sql: Dict[str, str] = {}
        for table, column, other_table, other_column in foreign_keys_to_create:
            old_sql = table_sql.get(table, self[table].schema)
            extra_sql = "",\n   FOREIGN KEY([{column}]) REFERENCES [{other_table}]([{other_column}])\n"".format(
                column=column, other_table=other_table, other_column=other_column
            )
            # Stick that bit in at the very end just before the closing ')'
            last_paren = old_sql.rindex("")"")
            new_sql = old_sql[:last_paren].strip() + extra_sql + old_sql[last_paren:]
            table_sql[table] = new_sql

        # And execute it all within a single transaction
        with self.conn:
            cursor = self.conn.cursor()
            schema_version = cursor.execute(""PRAGMA schema_version"").fetchone()[0]
            cursor.execute(""PRAGMA writable_schema = 1"")
            for table_name, new_sql in table_sql.items():
                cursor.execute(
                    ""UPDATE sqlite_master SET sql = ? WHERE name = ?"",
                    (new_sql, table_name),
                )
            cursor.execute(""PRAGMA schema_version = %d"" % (schema_version + 1))
            cursor.execute(""PRAGMA writable_schema = 0"")
        # Have to VACUUM outside the transaction to ensure .foreign_keys property
        # can see the newly created foreign key.
        self.vacuum()",other_column != 'rowid' and other_column not in self[other_table].columns_dict,'rowid' != other_column not in self[other_table].columns_dict
tensorpack,https://github.com/tensorpack/tensorpack/tree/master/tensorpack/train/base.py,TrainLoop,config$41,"def config(self, steps_per_epoch, starting_epoch, max_epoch):
        """"""
        Configure the loop given the settings.
        """"""
        self.starting_epoch = int(starting_epoch)
        self.max_epoch = int(max_epoch)
        self.steps_per_epoch = int(steps_per_epoch)
        # Allow empty epoch (no steps), if we want to run the callbacks only.
        assert self.steps_per_epoch >= 0 and self.max_epoch >= 0

        self._epoch_num = starting_epoch - 1",self.steps_per_epoch >= 0 and self.max_epoch >= 0,self.steps_per_epoch >= 0 <= self.max_epoch
cvpods,https://github.com/Megvii-BaseDetection/cvpods/tree/master/tools/benchmark.py,,if_main_my$124,"if __name__ == ""__main__"":
    parser = default_argument_parser()
    parser.add_argument(""--task"", choices=[""train"", ""eval"", ""data""], required=True)
    args = parser.parse_args()
    assert not args.eval_only

    if args.task == ""data"":
        f = benchmark_data
    elif args.task == ""train"":
        """"""
        Note: training speed may not be representative.
        The training cost of a R-CNN model varies with the content of the data
        and the quality of the model.
        """"""
        f = benchmark_train
    elif args.task == ""eval"":
        f = benchmark_eval
        # only benchmark single-GPU inference.
        assert args.num_gpus == 1 and args.num_machines == 1
    launch(f, args.num_gpus, args.num_machines, args.machine_rank, args.dist_url, args=(args,))",args.num_gpus == 1 and args.num_machines == 1,args.num_gpus == 1 == args.num_machines
tensorflow-DeepFM,https://github.com/ChenglongChen/tensorflow-DeepFM/tree/master//DeepFM.py,DeepFM,training_termination$327,"def training_termination(self, valid_result):
        if len(valid_result) > 5:
            if self.greater_is_better:
                if valid_result[-1] < valid_result[-2] and \
                    valid_result[-2] < valid_result[-3] and \
                    valid_result[-3] < valid_result[-4] and \
                    valid_result[-4] < valid_result[-5]:
                    return True
            else:
                if valid_result[-1] > valid_result[-2] and \
                    valid_result[-2] > valid_result[-3] and \
                    valid_result[-3] > valid_result[-4] and \
                    valid_result[-4] > valid_result[-5]:
                    return True
        return False",valid_result[-1] < valid_result[-2] and valid_result[-2] < valid_result[-3] and (valid_result[-3] < valid_result[-4]) and (valid_result[-4] < valid_result[-5]),valid_result[-1] < valid_result[-2] < valid_result[-3] < valid_result[-4] < valid_result[-5]
tensorflow-DeepFM,https://github.com/ChenglongChen/tensorflow-DeepFM/tree/master//DeepFM.py,DeepFM,training_termination$327,"def training_termination(self, valid_result):
        if len(valid_result) > 5:
            if self.greater_is_better:
                if valid_result[-1] < valid_result[-2] and \
                    valid_result[-2] < valid_result[-3] and \
                    valid_result[-3] < valid_result[-4] and \
                    valid_result[-4] < valid_result[-5]:
                    return True
            else:
                if valid_result[-1] > valid_result[-2] and \
                    valid_result[-2] > valid_result[-3] and \
                    valid_result[-3] > valid_result[-4] and \
                    valid_result[-4] > valid_result[-5]:
                    return True
        return False",valid_result[-1] > valid_result[-2] and valid_result[-2] > valid_result[-3] and (valid_result[-3] > valid_result[-4]) and (valid_result[-4] > valid_result[-5]),valid_result[-1] > valid_result[-2] > valid_result[-3] > valid_result[-4] > valid_result[-5]
chia-blockchain,https://github.com/Chia-Network/chia-blockchain/tree/master/chia/full_node/full_node_store.py,FullNodeStore,new_finished_sub_slot$252,"def new_finished_sub_slot(
        self,
        eos: EndOfSubSlotBundle,
        blocks: BlockchainInterface,
        peak: Optional[BlockRecord],
        peak_full_block: Optional[FullBlock],
    ) -> Optional[List[timelord_protocol.NewInfusionPointVDF]]:
        """"""
        Returns false if not added. Returns a list if added. The list contains all infusion points that depended
        on this sub slot
        """"""
        assert len(self.finished_sub_slots) >= 1
        assert (peak is None) == (peak_full_block is None)

        last_slot, _, last_slot_iters = self.finished_sub_slots[-1]

        cc_challenge: bytes32 = (
            last_slot.challenge_chain.get_hash() if last_slot is not None else self.constants.GENESIS_CHALLENGE
        )
        rc_challenge: bytes32 = (
            last_slot.reward_chain.get_hash() if last_slot is not None else self.constants.GENESIS_CHALLENGE
        )
        icc_challenge: Optional[bytes32] = None
        icc_iters: Optional[uint64] = None

        # Skip if already present
        for slot, _, _ in self.finished_sub_slots:
            if slot == eos:
                return []

        if eos.challenge_chain.challenge_chain_end_of_slot_vdf.challenge != cc_challenge:
            # This slot does not append to our next slot
            # This prevent other peers from appending fake VDFs to our cache
            return None

        if peak is None:
            sub_slot_iters = self.constants.SUB_SLOT_ITERS_STARTING
        else:
            sub_slot_iters = peak.sub_slot_iters

        total_iters = uint128(last_slot_iters + sub_slot_iters)

        if peak is not None and peak.total_iters > last_slot_iters:
            # Peak is in this slot

            # Note: Adding an end of subslot does not lock the blockchain, for performance reasons. Only the
            # timelord_lock is used. Therefore, it's possible that we add a new peak at the same time as seeing
            # the finished subslot, and the peak is not fully added yet, so it looks like we still need the subslot.
            # In that case, we will exit here and let the new_peak code add the subslot.
            if total_iters < peak.total_iters:
                return None

            rc_challenge = eos.reward_chain.end_of_slot_vdf.challenge
            cc_start_element = peak.challenge_vdf_output
            iters = uint64(total_iters - peak.total_iters)
            if peak.reward_infusion_new_challenge != rc_challenge:
                # We don't have this challenge hash yet
                if rc_challenge not in self.future_eos_cache:
                    self.future_eos_cache[rc_challenge] = []
                self.future_eos_cache[rc_challenge].append(eos)
                self.future_cache_key_times[rc_challenge] = int(time.time())
                log.info(f""Don't have challenge hash {rc_challenge}, caching EOS"")
                return None

            if peak.deficit == self.constants.MIN_BLOCKS_PER_CHALLENGE_BLOCK:
                icc_start_element = None
            elif peak.deficit == self.constants.MIN_BLOCKS_PER_CHALLENGE_BLOCK - 1:
                icc_start_element = ClassgroupElement.get_default_element()
            else:
                icc_start_element = peak.infused_challenge_vdf_output

            if peak.deficit < self.constants.MIN_BLOCKS_PER_CHALLENGE_BLOCK:
                curr = peak
                while not curr.first_in_sub_slot and not curr.is_challenge_block(self.constants):
                    curr = blocks.block_record(curr.prev_hash)
                if curr.is_challenge_block(self.constants):
                    icc_challenge = curr.challenge_block_info_hash
                    icc_iters = uint64(total_iters - curr.total_iters)
                else:
                    assert curr.finished_infused_challenge_slot_hashes is not None
                    icc_challenge = curr.finished_infused_challenge_slot_hashes[-1]
                    icc_iters = sub_slot_iters
                assert icc_challenge is not None

            if can_finish_sub_and_full_epoch(
                self.constants,
                blocks,
                peak.height,
                peak.prev_hash,
                peak.deficit,
                peak.sub_epoch_summary_included is not None,
            )[0]:
                assert peak_full_block is not None
                ses: Optional[SubEpochSummary] = next_sub_epoch_summary(
                    self.constants, blocks, peak.required_iters, peak_full_block, True
                )
                if ses is not None:
                    if eos.challenge_chain.subepoch_summary_hash != ses.get_hash():
                        log.warning(f""SES not correct {ses.get_hash(), eos.challenge_chain}"")
                        return None
                else:
                    if eos.challenge_chain.subepoch_summary_hash is not None:
                        log.warning(""SES not correct, should be None"")
                        return None
        else:
            # This is on an empty slot
            cc_start_element = ClassgroupElement.get_default_element()
            icc_start_element = ClassgroupElement.get_default_element()
            iters = sub_slot_iters
            icc_iters = sub_slot_iters

            # The icc should only be present if the previous slot had an icc too, and not deficit 0 (just finished slot)
            icc_challenge = (
                last_slot.infused_challenge_chain.get_hash()
                if last_slot is not None
                and last_slot.infused_challenge_chain is not None
                and last_slot.reward_chain.deficit != self.constants.MIN_BLOCKS_PER_CHALLENGE_BLOCK
                else None
            )

        # Validate cc VDF
        partial_cc_vdf_info = VDFInfo(
            cc_challenge,
            iters,
            eos.challenge_chain.challenge_chain_end_of_slot_vdf.output,
        )
        # The EOS will have the whole sub-slot iters, but the proof is only the delta, from the last peak
        if eos.challenge_chain.challenge_chain_end_of_slot_vdf != dataclasses.replace(
            partial_cc_vdf_info,
            number_of_iterations=sub_slot_iters,
        ):
            return None
        if (
            not eos.proofs.challenge_chain_slot_proof.normalized_to_identity
            and not eos.proofs.challenge_chain_slot_proof.is_valid(
                self.constants,
                cc_start_element,
                partial_cc_vdf_info,
            )
        ):
            return None
        if (
            eos.proofs.challenge_chain_slot_proof.normalized_to_identity
            and not eos.proofs.challenge_chain_slot_proof.is_valid(
                self.constants,
                ClassgroupElement.get_default_element(),
                eos.challenge_chain.challenge_chain_end_of_slot_vdf,
            )
        ):
            return None

        # Validate reward chain VDF
        if not eos.proofs.reward_chain_slot_proof.is_valid(
            self.constants,
            ClassgroupElement.get_default_element(),
            eos.reward_chain.end_of_slot_vdf,
            VDFInfo(rc_challenge, iters, eos.reward_chain.end_of_slot_vdf.output),
        ):
            return None

        if icc_challenge is not None:
            assert icc_start_element is not None
            assert icc_iters is not None
            assert eos.infused_challenge_chain is not None
            assert eos.infused_challenge_chain is not None
            assert eos.proofs.infused_challenge_chain_slot_proof is not None

            partial_icc_vdf_info = VDFInfo(
                icc_challenge,
                iters,
                eos.infused_challenge_chain.infused_challenge_chain_end_of_slot_vdf.output,
            )
            # The EOS will have the whole sub-slot iters, but the proof is only the delta, from the last peak
            if eos.infused_challenge_chain.infused_challenge_chain_end_of_slot_vdf != dataclasses.replace(
                partial_icc_vdf_info,
                number_of_iterations=icc_iters,
            ):
                return None
            if (
                not eos.proofs.infused_challenge_chain_slot_proof.normalized_to_identity
                and not eos.proofs.infused_challenge_chain_slot_proof.is_valid(
                    self.constants, icc_start_element, partial_icc_vdf_info
                )
            ):
                return None
            if (
                eos.proofs.infused_challenge_chain_slot_proof.normalized_to_identity
                and not eos.proofs.infused_challenge_chain_slot_proof.is_valid(
                    self.constants,
                    ClassgroupElement.get_default_element(),
                    eos.infused_challenge_chain.infused_challenge_chain_end_of_slot_vdf,
                )
            ):
                return None
        else:
            # This is the first sub slot and it's empty, therefore there is no ICC
            if eos.infused_challenge_chain is not None or eos.proofs.infused_challenge_chain_slot_proof is not None:
                return None

        self.finished_sub_slots.append((eos, [None] * self.constants.NUM_SPS_SUB_SLOT, total_iters))

        new_cc_hash = eos.challenge_chain.get_hash()
        self.recent_eos.put(new_cc_hash, (eos, time.time()))

        new_ips: List[timelord_protocol.NewInfusionPointVDF] = []
        for ip in self.future_ip_cache.get(eos.reward_chain.get_hash(), []):
            new_ips.append(ip)

        return new_ips",last_slot is not None and last_slot.infused_challenge_chain is not None and (last_slot.reward_chain.deficit != self.constants.MIN_BLOCKS_PER_CHALLENGE_BLOCK),last_slot is not None is not last_slot.infused_challenge_chain and last_slot.reward_chain.deficit != self.constants.MIN_BLOCKS_PER_CHALLENGE_BLOCK
curator,https://github.com/elastic/curator/tree/master/curator/indexlist.py,IndexList,filter_period$1054,"def filter_period(
        self, period_type='relative', source='name', range_from=None, range_to=None,
        date_from=None, date_to=None, date_from_format=None, date_to_format=None,
        timestring=None, unit=None, field=None, stats_result='min_value',
        intersect=False, week_starts_on='sunday', epoch=None, exclude=False,
        ):
        """"""
        Match `indices` with ages within a given period.

        :arg period_type: Can be either ``absolute`` or ``relative``.  Default is
            ``relative``.  ``date_from`` and ``date_to`` are required when using
            ``period_type='absolute'``. ``range_from`` and ``range_to`` are
            required with ``period_type='relative'``.
        :arg source: Source of index age. Can be one of 'name', 'creation_date',
            or 'field_stats'
        :arg range_from: How many ``unit`` (s) in the past/future is the origin?
        :arg range_to: How many ``unit`` (s) in the past/future is the end point?
        :arg date_from: The simplified date for the start of the range
        :arg date_to: The simplified date for the end of the range.  If this value
            is the same as ``date_from``, the full value of ``unit`` will be
            extrapolated for the range.  For example, if ``unit`` is ``months``,
            and ``date_from`` and ``date_to`` are both ``2017.01``, then the entire
            month of January 2017 will be the absolute date range.
        :arg date_from_format: The strftime string used to parse ``date_from``
        :arg date_to_format: The strftime string used to parse ``date_to``
        :arg timestring: An strftime string to match the datestamp in an index
            name. Only used for index filtering by ``name``.
        :arg unit: One of ``hours``, ``days``, ``weeks``, ``months``, or
            ``years``.
        :arg field: A timestamp field name.  Only used for ``field_stats`` based
            calculations.
        :arg stats_result: Either `min_value` or `max_value`.  Only used in
            conjunction with ``source='field_stats'`` to choose whether to
            reference the minimum or maximum result value.
        :arg intersect: Only used when ``source='field_stats'``.
            If `True`, only indices where both `min_value` and `max_value` are
            within the period will be selected. If `False`, it will use whichever
            you specified.  Default is `False` to preserve expected behavior.
        :arg week_starts_on: Either ``sunday`` or ``monday``. Default is
            ``sunday``
        :arg epoch: An epoch timestamp used to establish a point of reference
            for calculations. If not provided, the current time will be used.
        :arg exclude: If `exclude` is `True`, this filter will remove matching
            indices from `indices`. If `exclude` is `False`, then only matching
            indices will be kept in `indices`.
            Default is `False`
        """"""

        self.loggit.debug('Filtering indices by period')
        if period_type not in ['absolute', 'relative']:
            raise ValueError(
                'Unacceptable value: {0} -- ""period_type"" must be either ""absolute"" or '
                '""relative"".'.format(period_type)
            )
        if period_type == 'relative':
            func = utils.date_range
            args = [unit, range_from, range_to, epoch]
            kwgs = { 'week_starts_on': week_starts_on }
            if type(range_from) != type(int()) or type(range_to) != type(int()):
                raise exceptions.ConfigurationError(
                    '""range_from"" and ""range_to"" must be integer values')
        else:
            func = utils.absolute_date_range
            args = [unit, date_from, date_to]
            kwgs = { 'date_from_format': date_from_format, 'date_to_format': date_to_format }
            for reqd in [date_from, date_to, date_from_format, date_to_format]:
                if not reqd:
                    raise exceptions.ConfigurationError(
                        'Must provide ""date_from"", ""date_to"", ""date_from_format"", and '
                        '""date_to_format"" with absolute period_type'
                    )
        try:
            start, end = func(*args, **kwgs)
        except Exception as e:
            utils.report_failure(e)

        self._calculate_ages(
            source=source, timestring=timestring, field=field,
            stats_result=stats_result
        )
        for index in self.working_list():
            try:
                if source == 'field_stats' and intersect:
                    min_age = int(self.index_info[index]['age']['min_value'])
                    max_age = int(self.index_info[index]['age']['max_value'])
                    msg = (
                        'Index ""{0}"", timestamp field ""{1}"", min_value ({2}), '
                        'max_value ({3}), period start: ""{4}"", period '
                        'end, ""{5}""'.format(
                            index,
                            field,
                            min_age,
                            max_age,
                            start,
                            end
                        )
                    )
                    # Because time adds to epoch, smaller numbers are actually older
                    # timestamps.
                    inrange = ((min_age >= start) and (max_age <= end))
                else:
                    age = int(self.index_info[index]['age'][self.age_keyfield])
                    msg = (
                        'Index ""{0}"" age ({1}), period start: ""{2}"", period '
                        'end, ""{3}""'.format(
                            index,
                            age,
                            start,
                            end
                        )
                    )
                    # Because time adds to epoch, smaller numbers are actually older
                    # timestamps.
                    inrange = ((age >= start) and (age <= end))
                self.__excludify(inrange, exclude, index, msg)
            except KeyError:
                self.loggit.debug(
                    'Index ""{0}"" does not meet provided criteria. '
                    'Removing from list.'.format(index))
                self.indices.remove(index)",age >= start and age <= end,start <= age <= end
PyHive,https://github.com/dropbox/PyHive/tree/master/TCLIService/ttypes.py,TTypeQualifierValue,write$462,"def write(self, oprot):
        if oprot._fast_encode is not None and self.thrift_spec is not None:
            oprot.trans.write(oprot._fast_encode(self, (self.__class__, self.thrift_spec)))
            return
        oprot.writeStructBegin('TTypeQualifierValue')
        if self.i32Value is not None:
            oprot.writeFieldBegin('i32Value', TType.I32, 1)
            oprot.writeI32(self.i32Value)
            oprot.writeFieldEnd()
        if self.stringValue is not None:
            oprot.writeFieldBegin('stringValue', TType.STRING, 2)
            oprot.writeString(self.stringValue.encode('utf-8') if sys.version_info[0] == 2 else self.stringValue)
            oprot.writeFieldEnd()
        oprot.writeFieldStop()
        oprot.writeStructEnd()",oprot._fast_encode is not None and self.thrift_spec is not None,oprot._fast_encode is not None is not self.thrift_spec
BeRoot,https://github.com/AlessandroZ/BeRoot/tree/master/Windows/templates/MS16-075/webclient/secretsdump.py,,if_main_my$2299,"if __name__ == '__main__':
    # Init the example's logger theme
    # logger.init()
    # Explicitly changing the stdout encoding format
    if sys.stdout.encoding is None:
        # Output is redirected to a file
        sys.stdout = codecs.getwriter('utf8')(sys.stdout)

    print(version.BANNER)

    parser = argparse.ArgumentParser(add_help=True,
                                     description=""Performs various techniques to dump secrets from the remote machine without executing any agent there."")

    parser.add_argument('target', action='store',
                        help='[[domain/]username[:password]@]<targetName or address> or LOCAL (if you want to parse local files)')
    parser.add_argument('-debug', action='store_true', help='Turn DEBUG output ON')
    parser.add_argument('-system', action='store', help='SYSTEM hive to parse')
    parser.add_argument('-security', action='store', help='SECURITY hive to parse')
    parser.add_argument('-sam', action='store', help='SAM hive to parse')
    parser.add_argument('-ntds', action='store', help='NTDS.DIT file to parse')
    parser.add_argument('-resumefile', action='store',
                        help='resume file name to resume NTDS.DIT session dump (only available to DRSUAPI approach). This file will also be used to keep updating the session\'s state')
    parser.add_argument('-outputfile', action='store',
                        help='base output filename. Extensions will be added for sam, secrets, cached and ntds')
    parser.add_argument('-use-vss', action='store_true', default=False,
                        help='Use the VSS method insead of default DRSUAPI')
    group = parser.add_argument_group('display options')
    group.add_argument('-just-dc-user', action='store', metavar='USERNAME',
                       help='Extract only NTDS.DIT data for the user specified. Only available for DRSUAPI approach. Implies also -just-dc switch')
    group.add_argument('-just-dc', action='store_true', default=False,
                       help='Extract only NTDS.DIT data (NTLM hashes and Kerberos keys)')
    group.add_argument('-just-dc-ntlm', action='store_true', default=False,
                       help='Extract only NTDS.DIT data (NTLM hashes only)')
    group.add_argument('-pwd-last-set', action='store_true', default=False,
                       help='Shows pwdLastSet attribute for each NTDS.DIT account. Doesn\'t apply to -outputfile data')
    group.add_argument('-history', action='store_true', help='Dump password history')
    group = parser.add_argument_group('authentication')

    group.add_argument('-hashes', action=""store"", metavar=""LMHASH:NTHASH"", help='NTLM hashes, format is LMHASH:NTHASH')
    group.add_argument('-no-pass', action=""store_true"", help='don\'t ask for password (useful for -k)')
    group.add_argument('-k', action=""store_true"",
                       help='Use Kerberos authentication. Grabs credentials from ccache file (KRB5CCNAME) based on target parameters. If valid credentials cannot be found, it will use the ones specified in the command line')
    group.add_argument('-aesKey', action=""store"", metavar=""hex key"",
                       help='AES key to use for Kerberos Authentication (128 or 256 bits)')
    group.add_argument('-dc-ip', action='store', metavar=""ip address"",
                       help='IP Address of the domain controller. If ommited it use the domain part (FQDN) specified in the target parameter')

    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(1)

    options = parser.parse_args()

    # if options.debug is True:
    #     logging.getLogger().setLevel(logging.DEBUG)
    # else:
    #     logging.getLogger().setLevel(logging.INFO)

    import re

    domain, username, password, address = re.compile('(?:(?:([^/@:]*)/)?([^@:]*)(?::([^@]*))?@)?(.*)').match(
        options.target).groups('')

    # In case the password contains '@'
    if '@' in address:
        password = password + '@' + address.rpartition('@')[0]
        address = address.rpartition('@')[2]

    if options.just_dc_user is not None:
        if options.use_vss is True:
            logging.error('-just-dc-user switch is not supported in VSS mode')
            sys.exit(1)
        elif options.resumefile is not None:
            logging.error('resuming a previous NTDS.DIT dump session not compatible with -just-dc-user switch')
            sys.exit(1)
        elif address.upper() == 'LOCAL' and username == '':
            logging.error('-just-dc-user not compatible in LOCAL mode')
            sys.exit(1)
        else:
            # Having this switch on implies not asking for anything else.
            options.just_dc = True

    if options.use_vss is True and options.resumefile is not None:
        logging.error('resuming a previous NTDS.DIT dump session is not supported in VSS mode')
        sys.exit(1)

    if address.upper() == 'LOCAL' and username == '' and options.resumefile is not None:
        logging.error('resuming a previous NTDS.DIT dump session is not supported in LOCAL mode')
        sys.exit(1)

    if address.upper() == 'LOCAL' and username == '':
        if options.system is None:
            logging.error('SYSTEM hive is always required for local parsing, check help')
            sys.exit(1)
    else:

        if domain is None:
            domain = ''

        if password == '' and username != '' and options.hashes is None and options.no_pass is False and options.aesKey is None:
            from getpass import getpass

            password = getpass(""Password:"")

        if options.aesKey is not None:
            options.k = True

    dumper = DumpSecrets(address, username, password, domain, options)
    try:
        dumper.dump()
    except Exception as e:
        logging.error(e)",password == '' and username != '' and (options.hashes is None) and (options.no_pass is False) and (options.aesKey is None),password == '' != username and options.no_pass is False and (options.hashes is None is options.aesKey)
capa,https://github.com/mandiant/capa/tree/master/capa/ida/plugin/proxy.py,CapaExplorerRangeProxyModel,lessThan$29,"def lessThan(self, left, right):
        """"""return True if left item is less than right item, else False

        @param left: QModelIndex of left
        @param right: QModelIndex of right
        """"""
        ldata = left.internalPointer().data(left.column())
        rdata = right.internalPointer().data(right.column())

        if (
            ldata
            and rdata
            and left.column() == CapaExplorerDataModel.COLUMN_INDEX_VIRTUAL_ADDRESS
            and left.column() == right.column()
        ):
            # convert virtual address before compare
            return int(ldata, 16) < int(rdata, 16)
        else:
            # compare as lowercase
            return ldata.lower() < rdata.lower()",ldata and rdata and (left.column() == CapaExplorerDataModel.COLUMN_INDEX_VIRTUAL_ADDRESS) and (left.column() == right.column()),CapaExplorerDataModel.COLUMN_INDEX_VIRTUAL_ADDRESS == left.column() == right.column() and ldata and rdata
horovod,https://github.com/horovod/horovod/tree/master/examples/adasum/adasum_small_model.py,,train$38,"def train(args):
    hvd.init()

    net = Net(args.mode)
    optimizer = torch.optim.SGD(
        net.parameters(),
        lr=args.learning_rate,
    )

    num_steps = 50

    np.random.seed(1 + hvd.rank())
    torch.manual_seed(1234)

    prev_zero = False

    for step in range(1, num_steps + 1):
        features = torch.Tensor(np.random.rand(1) * 2 * args.x_max - args.x_max)
        if args.mode == ""square"": 
            labels = sq(features)  
        else: 
            labels = qu(features)
        optimizer.zero_grad()
        outputs = net(features)
        loss = torch.nn.MSELoss()(outputs, labels)
        loss.backward()

        if args.op == ""Average"":
            net.param.grad.data = hvd.allreduce(net.param.grad.data, op=hvd.Average)
        elif args.op == ""Adasum"":
            net.param.grad.data = hvd.allreduce(net.param.grad.data, op=hvd.Adasum)

        optimizer.step()

        #Uncomment below lines to see how loss and gradients change with Adasum
        #if hvd.rank() == 0:
        #    print(step, loss.item(), net.param.grad.data[0].item(), net.param.grad.data[1].item())

        if net.param.grad.data[0].item() == 0 and net.param.grad.data[1].item() == 0:
            if prev_zero:
                break
            else:
                prev_zero = True
        else:
            prev_zero = False


    if step == num_steps:
        step = 100

    if hvd.rank() == 0:
            print(args.x_max, args.op, args.learning_rate, hvd.size(), step)",net.param.grad.data[0].item() == 0 and net.param.grad.data[1].item() == 0,net.param.grad.data[0].item() == 0 == net.param.grad.data[1].item()
exploitable,https://github.com/jfoote/exploitable/tree/master/exploitable/lib/classifier.py,Classification,__cmp__$121,"def __cmp__(self, other):
        if other is None:
            return 1

        if not issubclass(type(other), type(self)):
            raise TypeError(""cannot compare type {} to type {}"".format(type(other), type(self)))

        if len(self.tags) == 0 or len(other.tags) == 0:
            return len(self.tags) - len(other.tags)

        i = 0
        while i < len(self.tags) and i < len(other.tags):
            result = cmp(self.tags[i], other.tags[i])
            if result:
                return result
            i += 1
        return result",i < len(self.tags) and i < len(other.tags),len(self.tags) > i < len(other.tags)
PyTorch-RL,https://github.com/Khrylx/PyTorch-RL/tree/master/examples/trpo_gym.py,,main_loop$103,"def main_loop():
    for i_iter in range(args.max_iter_num):
        """"""generate multiple trajectories that reach the minimum batch_size""""""
        batch, log = agent.collect_samples(args.min_batch_size, render=args.render)
        t0 = time.time()
        update_params(batch)
        t1 = time.time()
        """"""evaluate with determinstic action (remove noise for exploration)""""""
        _, log_eval = agent.collect_samples(args.eval_batch_size, mean_action=True)
        t2 = time.time()

        if i_iter % args.log_interval == 0:
            print('{}\tT_sample {:.4f}\tT_update {:.4f}\tT_eval {:.4f}\ttrain_R_min {:.2f}\ttrain_R_max {:.2f}\ttrain_R_avg {:.2f}\teval_R_avg {:.2f}'.format(
                i_iter, log['sample_time'], t1-t0, t2-t1, log['min_reward'], log['max_reward'], log['avg_reward'], log_eval['avg_reward']))

        if args.save_model_interval > 0 and (i_iter+1) % args.save_model_interval == 0:
            to_device(torch.device('cpu'), policy_net, value_net)
            pickle.dump((policy_net, value_net, running_state),
                        open(os.path.join(assets_dir(), 'learned_models/{}_trpo.p'.format(args.env_name)), 'wb'))
            to_device(device, policy_net, value_net)

        """"""clean up gpu memory""""""
        torch.cuda.empty_cache()",args.save_model_interval > 0 and (i_iter + 1) % args.save_model_interval == 0,args.save_model_interval > 0 == (i_iter + 1) % args.save_model_interval
fvcore,https://github.com/facebookresearch/fvcore/tree/master/fvcore/transforms/transform.py,CropTransform,inverse$736,"def inverse(self) -> Transform:
        assert (
            self.orig_w is not None and self.orig_h is not None
        ), ""orig_w, orig_h are required for CropTransform to be invertible!""
        pad_x1 = self.orig_w - self.x0 - self.w
        pad_y1 = self.orig_h - self.y0 - self.h
        return PadTransform(
            self.x0, self.y0, pad_x1, pad_y1, orig_w=self.w, orig_h=self.h
        )",self.orig_w is not None and self.orig_h is not None,self.orig_w is not None is not self.orig_h
RsaCtfTool,https://github.com/Ganapati/RsaCtfTool/tree/master/attacks/single_key/mersenne_primes.py,Attack,attack$16,"def attack(self, publickey, cipher=[], progress=True):
        """"""Run tests against mersenne primes""""""
        with timeout(self.timeout):
            try:
                p = q = None
                mersenne_tab = [
                    2,
                    3,
                    5,
                    7,
                    13,
                    17,
                    19,
                    31,
                    61,
                    89,
                    107,
                    127,
                    521,
                    607,
                    1279,
                    2203,
                    2281,
                    3217,
                    4253,
                    4423,
                    9689,
                    9941,
                    11213,
                    19937,
                    21701,
                    23209,
                    44497,
                    86243,
                    110503,
                    132049,
                    216091,
                    756839,
                    859433,
                    1257787,
                    1398269,
                    2976221,
                    3021377,
                    6972593,
                    13466917,
                    20336011,
                    24036583,
                    25964951,
                    30402457,
                    32582657,
                    37156667,
                    42643801,
                    43112609,
                    57885161,
                    74207281,
                    77232917,
                    82589933,
                ]
                i = getpubkeysz(publickey.n)
                for mersenne_prime in tqdm(mersenne_tab, disable=(not progress)):
                    if mersenne_prime <= i:
                        m = (1 << mersenne_prime) - 1
                        if publickey.n % m == 0:
                            p = m
                            q = publickey.n // p
                            break
                    else:
                        break
                if p is not None and q is not None:
                    priv_key = PrivateKey(
                        int(p), int(q), int(publickey.e), int(publickey.n)
                    )
                    return (priv_key, None)
                return (None, None)
            except TimeoutError:
                return (None, None)",p is not None and q is not None,p is not None is not q
ansible-modules-extras,https://github.com/ansible/ansible-modules-extras/tree/master/cloud/amazon/ec2_eni.py,,wait_for_eni$286,"def wait_for_eni(eni, status):

    while True:
        time.sleep(3)
        eni.update()
        # If the status is detached we just need attachment to disappear
        if eni.attachment is None:
            if status == ""detached"":
                break
        else:
            if status == ""attached"" and eni.attachment.status == ""attached"":
                break",status == 'attached' and eni.attachment.status == 'attached',status == 'attached' == eni.attachment.status
retopoflow,https://github.com/CGCookie/retopoflow/tree/master/retopoflow/rftool_polypen/polypen.py,PolyPen,draw_postpixel$749,"def draw_postpixel(self):
        # TODO: put all logic into set_next_state(), such as vertex snapping, edge splitting, etc.

        #if self.rfcontext.nav or self.mode != 'main': return
        if not self.actions.using_onlymods('insert'): return
        hit_pos = self.actions.hit_pos
        if not hit_pos: return

        self.set_next_state()

        bgl.glEnable(bgl.GL_BLEND)
        CC_DRAW.stipple(pattern=[4,4])
        CC_DRAW.point_size(8)
        CC_DRAW.line_width(2)

        if self.next_state == 'knife selected edge':
            bmv1,bmv2 = self.insert_edge.verts
            faces = self.insert_edge.link_faces
            if faces:
                for f in faces:
                    lco = []
                    for v0,v1 in iter_pairs(f.verts, True):
                        lco.append(v0.co)
                        if (v0 == bmv1 and v1 == bmv2) or (v0 == bmv2 and v1 == bmv1):
                            lco.append(hit_pos)
                    self.draw_lines(lco)
            else:
                self.draw_lines([bmv1.co, hit_pos])
                self.draw_lines([bmv2.co, hit_pos])

        elif self.next_state == 'new vertex':
            p0 = hit_pos
            e1,d = self.rfcontext.nearest2D_edge(edges=self.vis_edges)
            if e1:
                bmv1,bmv2 = e1.verts
                if d is not None and d < self.rfcontext.drawing.scale(options['polypen insert dist']):
                    f = next(iter(e1.link_faces), None)
                    if f:
                        lco = []
                        for v0,v1 in iter_pairs(f.verts, True):
                            lco.append(v0.co)
                            if (v0 == bmv1 and v1 == bmv2) or (v0 == bmv2 and v1 == bmv1):
                                lco.append(p0)
                        self.draw_lines(lco)
                    else:
                        self.draw_lines([bmv1.co, hit_pos])
                        self.draw_lines([bmv2.co, hit_pos])
                else:
                    self.draw_lines([hit_pos])
            else:
                self.draw_lines([hit_pos])

        elif self.next_state in {'vert-edge', 'vert-edge-vert'}:
            sel_verts = self.sel_verts
            bmv0 = next(iter(sel_verts))
            if self.nearest_vert:
                p0 = self.nearest_vert.co
            elif self.next_state == 'vert-edge':
                p0 = hit_pos
                e1,d = self.rfcontext.nearest2D_edge(edges=self.vis_edges)
                if e1:
                    bmv1,bmv2 = e1.verts
                    if d is not None and d < self.rfcontext.drawing.scale(options['polypen insert dist']):
                        f = next(iter(e1.link_faces), None)
                        if f:
                            lco = []
                            for v0,v1 in iter_pairs(f.verts, True):
                                lco.append(v0.co)
                                if (v0 == bmv1 and v1 == bmv2) or (v0 == bmv2 and v1 == bmv1):
                                    lco.append(p0)
                            self.draw_lines(lco)
                        else:
                            self.draw_lines([bmv1.co, p0])
                            self.draw_lines([bmv2.co, p0])
            elif self.next_state == 'vert-edge-vert':
                p0 = hit_pos
            else:
                return
            self.draw_lines([bmv0.co, p0])

        elif self.actions.shift and not self.actions.ctrl:
            if self.next_state in ['edge-face', 'edge-quad', 'edge-quad-snap', 'tri-quad']:
                nearest_vert,_ = self.rfcontext.nearest2D_vert(verts=self.sel_verts, max_dist=options['polypen merge dist'])
                if nearest_vert:
                    self.draw_lines([nearest_vert.co, hit_pos])

        elif not self.actions.shift and self.actions.ctrl:
            if self.next_state == 'edge-face':
                e0,_ = self.rfcontext.nearest2D_edge(edges=self.sel_edges) #next(iter(self.sel_edges))
                if not e0: return
                e1,d = self.rfcontext.nearest2D_edge(edges=self.vis_edges)
                if e1 and d < self.rfcontext.drawing.scale(options['polypen insert dist']) and e0 == e1:
                    bmv1,bmv2 = e1.verts
                    p0 = hit_pos
                    f = next(iter(e1.link_faces), None)
                    if f:
                        lco = []
                        for v0,v1 in iter_pairs(f.verts, True):
                            lco.append(v0.co)
                            if (v0 == bmv1 and v1 == bmv2) or (v0 == bmv2 and v1 == bmv1):
                                lco.append(p0)
                        self.draw_lines(lco)
                    else:
                        self.draw_lines([bmv1.co, hit_pos])
                        self.draw_lines([bmv2.co, hit_pos])
                else:
                    # self.draw_lines([hit_pos])
                    bmv1,bmv2 = e0.verts
                    if self.nearest_vert and not self.nearest_vert.select:
                        p0 = self.nearest_vert.co
                    else:
                        p0 = hit_pos
                    self.draw_lines([p0, bmv1.co, bmv2.co])

            elif self.next_state == 'edge-quad':
                # a Desmos construction of how this works: https://www.desmos.com/geometry/bmmx206thi
                xy0, xy1, xy2, xy3 = self._get_edge_quad_verts()
                if xy0 is None: return
                co0 = self.rfcontext.raycast_sources_Point2D(xy0)[0]
                co1 = self.rfcontext.raycast_sources_Point2D(xy1)[0]
                co2 = self.rfcontext.raycast_sources_Point2D(xy2)[0]
                co3 = self.rfcontext.raycast_sources_Point2D(xy3)[0]
                self.draw_lines([co1, co2, co3, co0])

            elif self.next_state == 'edge-quad-snap':
                e0,_ = self.rfcontext.nearest2D_edge(edges=self.sel_edges)
                e1 = self.nearest_edge
                if not e0 or not e1: return
                bmv0,bmv1 = e0.verts
                bmv2,bmv3 = e1.verts
                p0,p1 = self.rfcontext.Point_to_Point2D(bmv0.co),self.rfcontext.Point_to_Point2D(bmv1.co)
                p2,p3 = self.rfcontext.Point_to_Point2D(bmv2.co),self.rfcontext.Point_to_Point2D(bmv3.co)
                if intersect2d_segment_segment(p1, p2, p3, p0): bmv2,bmv3 = bmv3,bmv2
                # if e0.vector2D(self.rfcontext.Point_to_Point2D).dot(e1.vector2D(self.rfcontext.Point_to_Point2D)) > 0:
                #     bmv2,bmv3 = bmv3,bmv2
                self.draw_lines([bmv0.co, bmv1.co, bmv2.co, bmv3.co])

            elif self.next_state == 'tri-quad':
                if self.nearest_vert and not self.nearest_vert.select:
                    p0 = self.nearest_vert.co
                else:
                    p0 = hit_pos
                e1,_ = self.rfcontext.nearest2D_edge(edges=self.sel_edges)
                if not e1: return
                bmv1,bmv2 = e1.verts
                f = next(iter(e1.link_faces), None)
                if not f: return
                lco = []
                for v0,v1 in iter_pairs(f.verts, True):
                    lco.append(v0.co)
                    if (v0 == bmv1 and v1 == bmv2) or (v0 == bmv2 and v1 == bmv1):
                        lco.append(p0)
                self.draw_lines(lco)",d is not None and d < self.rfcontext.drawing.scale(options['polypen insert dist']),None is not d < self.rfcontext.drawing.scale(options['polypen insert dist'])
retopoflow,https://github.com/CGCookie/retopoflow/tree/master/retopoflow/rftool_polypen/polypen.py,PolyPen,draw_postpixel$749,"def draw_postpixel(self):
        # TODO: put all logic into set_next_state(), such as vertex snapping, edge splitting, etc.

        #if self.rfcontext.nav or self.mode != 'main': return
        if not self.actions.using_onlymods('insert'): return
        hit_pos = self.actions.hit_pos
        if not hit_pos: return

        self.set_next_state()

        bgl.glEnable(bgl.GL_BLEND)
        CC_DRAW.stipple(pattern=[4,4])
        CC_DRAW.point_size(8)
        CC_DRAW.line_width(2)

        if self.next_state == 'knife selected edge':
            bmv1,bmv2 = self.insert_edge.verts
            faces = self.insert_edge.link_faces
            if faces:
                for f in faces:
                    lco = []
                    for v0,v1 in iter_pairs(f.verts, True):
                        lco.append(v0.co)
                        if (v0 == bmv1 and v1 == bmv2) or (v0 == bmv2 and v1 == bmv1):
                            lco.append(hit_pos)
                    self.draw_lines(lco)
            else:
                self.draw_lines([bmv1.co, hit_pos])
                self.draw_lines([bmv2.co, hit_pos])

        elif self.next_state == 'new vertex':
            p0 = hit_pos
            e1,d = self.rfcontext.nearest2D_edge(edges=self.vis_edges)
            if e1:
                bmv1,bmv2 = e1.verts
                if d is not None and d < self.rfcontext.drawing.scale(options['polypen insert dist']):
                    f = next(iter(e1.link_faces), None)
                    if f:
                        lco = []
                        for v0,v1 in iter_pairs(f.verts, True):
                            lco.append(v0.co)
                            if (v0 == bmv1 and v1 == bmv2) or (v0 == bmv2 and v1 == bmv1):
                                lco.append(p0)
                        self.draw_lines(lco)
                    else:
                        self.draw_lines([bmv1.co, hit_pos])
                        self.draw_lines([bmv2.co, hit_pos])
                else:
                    self.draw_lines([hit_pos])
            else:
                self.draw_lines([hit_pos])

        elif self.next_state in {'vert-edge', 'vert-edge-vert'}:
            sel_verts = self.sel_verts
            bmv0 = next(iter(sel_verts))
            if self.nearest_vert:
                p0 = self.nearest_vert.co
            elif self.next_state == 'vert-edge':
                p0 = hit_pos
                e1,d = self.rfcontext.nearest2D_edge(edges=self.vis_edges)
                if e1:
                    bmv1,bmv2 = e1.verts
                    if d is not None and d < self.rfcontext.drawing.scale(options['polypen insert dist']):
                        f = next(iter(e1.link_faces), None)
                        if f:
                            lco = []
                            for v0,v1 in iter_pairs(f.verts, True):
                                lco.append(v0.co)
                                if (v0 == bmv1 and v1 == bmv2) or (v0 == bmv2 and v1 == bmv1):
                                    lco.append(p0)
                            self.draw_lines(lco)
                        else:
                            self.draw_lines([bmv1.co, p0])
                            self.draw_lines([bmv2.co, p0])
            elif self.next_state == 'vert-edge-vert':
                p0 = hit_pos
            else:
                return
            self.draw_lines([bmv0.co, p0])

        elif self.actions.shift and not self.actions.ctrl:
            if self.next_state in ['edge-face', 'edge-quad', 'edge-quad-snap', 'tri-quad']:
                nearest_vert,_ = self.rfcontext.nearest2D_vert(verts=self.sel_verts, max_dist=options['polypen merge dist'])
                if nearest_vert:
                    self.draw_lines([nearest_vert.co, hit_pos])

        elif not self.actions.shift and self.actions.ctrl:
            if self.next_state == 'edge-face':
                e0,_ = self.rfcontext.nearest2D_edge(edges=self.sel_edges) #next(iter(self.sel_edges))
                if not e0: return
                e1,d = self.rfcontext.nearest2D_edge(edges=self.vis_edges)
                if e1 and d < self.rfcontext.drawing.scale(options['polypen insert dist']) and e0 == e1:
                    bmv1,bmv2 = e1.verts
                    p0 = hit_pos
                    f = next(iter(e1.link_faces), None)
                    if f:
                        lco = []
                        for v0,v1 in iter_pairs(f.verts, True):
                            lco.append(v0.co)
                            if (v0 == bmv1 and v1 == bmv2) or (v0 == bmv2 and v1 == bmv1):
                                lco.append(p0)
                        self.draw_lines(lco)
                    else:
                        self.draw_lines([bmv1.co, hit_pos])
                        self.draw_lines([bmv2.co, hit_pos])
                else:
                    # self.draw_lines([hit_pos])
                    bmv1,bmv2 = e0.verts
                    if self.nearest_vert and not self.nearest_vert.select:
                        p0 = self.nearest_vert.co
                    else:
                        p0 = hit_pos
                    self.draw_lines([p0, bmv1.co, bmv2.co])

            elif self.next_state == 'edge-quad':
                # a Desmos construction of how this works: https://www.desmos.com/geometry/bmmx206thi
                xy0, xy1, xy2, xy3 = self._get_edge_quad_verts()
                if xy0 is None: return
                co0 = self.rfcontext.raycast_sources_Point2D(xy0)[0]
                co1 = self.rfcontext.raycast_sources_Point2D(xy1)[0]
                co2 = self.rfcontext.raycast_sources_Point2D(xy2)[0]
                co3 = self.rfcontext.raycast_sources_Point2D(xy3)[0]
                self.draw_lines([co1, co2, co3, co0])

            elif self.next_state == 'edge-quad-snap':
                e0,_ = self.rfcontext.nearest2D_edge(edges=self.sel_edges)
                e1 = self.nearest_edge
                if not e0 or not e1: return
                bmv0,bmv1 = e0.verts
                bmv2,bmv3 = e1.verts
                p0,p1 = self.rfcontext.Point_to_Point2D(bmv0.co),self.rfcontext.Point_to_Point2D(bmv1.co)
                p2,p3 = self.rfcontext.Point_to_Point2D(bmv2.co),self.rfcontext.Point_to_Point2D(bmv3.co)
                if intersect2d_segment_segment(p1, p2, p3, p0): bmv2,bmv3 = bmv3,bmv2
                # if e0.vector2D(self.rfcontext.Point_to_Point2D).dot(e1.vector2D(self.rfcontext.Point_to_Point2D)) > 0:
                #     bmv2,bmv3 = bmv3,bmv2
                self.draw_lines([bmv0.co, bmv1.co, bmv2.co, bmv3.co])

            elif self.next_state == 'tri-quad':
                if self.nearest_vert and not self.nearest_vert.select:
                    p0 = self.nearest_vert.co
                else:
                    p0 = hit_pos
                e1,_ = self.rfcontext.nearest2D_edge(edges=self.sel_edges)
                if not e1: return
                bmv1,bmv2 = e1.verts
                f = next(iter(e1.link_faces), None)
                if not f: return
                lco = []
                for v0,v1 in iter_pairs(f.verts, True):
                    lco.append(v0.co)
                    if (v0 == bmv1 and v1 == bmv2) or (v0 == bmv2 and v1 == bmv1):
                        lco.append(p0)
                self.draw_lines(lco)",d is not None and d < self.rfcontext.drawing.scale(options['polypen insert dist']),None is not d < self.rfcontext.drawing.scale(options['polypen insert dist'])
unilm,https://github.com/microsoft/unilm/tree/master/xtune/src/transformers/modeling_xlm.py,XLMModel,forward$399,"def forward(
        self,
        input_ids=None,
        attention_mask=None,
        langs=None,
        token_type_ids=None,
        position_ids=None,
        lengths=None,
        cache=None,
        head_mask=None,
        inputs_embeds=None,
    ):
        r""""""
    Return:
        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.XLMConfig`) and inputs:
        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):
            Sequence of hidden-states at the output of the last layer of the model.
        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):
            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
            of shape :obj:`(batch_size, sequence_length, hidden_size)`.

            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):
            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape
            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.

            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
            heads.

    Examples::

        from transformers import XLMTokenizer, XLMModel
        import torch

        tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')
        model = XLMModel.from_pretrained('xlm-mlm-en-2048')
        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True)).unsqueeze(0)  # Batch size 1
        outputs = model(input_ids)
        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple

        """"""
        if input_ids is not None:
            bs, slen = input_ids.size()
        else:
            bs, slen = inputs_embeds.size()[:-1]

        if lengths is None:
            if input_ids is not None:
                lengths = (input_ids != self.pad_index).sum(dim=1).long()
            else:
                lengths = torch.LongTensor([slen] * bs)
        # mask = input_ids != self.pad_index

        # check inputs
        assert lengths.size(0) == bs
        assert lengths.max().item() <= slen
        # input_ids = input_ids.transpose(0, 1)  # batch size as dimension 0
        # assert (src_enc is None) == (src_len is None)
        # if src_enc is not None:
        #     assert self.is_decoder
        #     assert src_enc.size(0) == bs

        # generate masks
        mask, attn_mask = get_masks(slen, lengths, self.causal, padding_mask=attention_mask)
        # if self.is_decoder and src_enc is not None:
        #     src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]

        device = input_ids.device if input_ids is not None else inputs_embeds.device

        # position_ids
        if position_ids is None:
            position_ids = torch.arange(slen, dtype=torch.long, device=device)
            position_ids = position_ids.unsqueeze(0).expand((bs, slen))
        else:
            assert position_ids.size() == (bs, slen)  # (slen, bs)
            # position_ids = position_ids.transpose(0, 1)

        # langs
        if langs is not None:
            assert langs.size() == (bs, slen)  # (slen, bs)
            # langs = langs.transpose(0, 1)

        # Prepare head mask if needed
        # 1.0 in head_mask indicate we keep the head
        # attention_probs has shape bsz x n_heads x N x N
        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]
        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x qlen x klen]
        if head_mask is not None:
            if head_mask.dim() == 1:
                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)
                head_mask = head_mask.expand(self.n_layers, -1, -1, -1, -1)
            elif head_mask.dim() == 2:
                head_mask = (
                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)
                )  # We can specify head_mask for each layer
            head_mask = head_mask.to(
                dtype=next(self.parameters()).dtype
            )  # switch to fload if need + fp16 compatibility
        else:
            head_mask = [None] * self.n_layers

        # do not recompute cached elements
        if cache is not None and input_ids is not None:
            _slen = slen - cache[""slen""]
            input_ids = input_ids[:, -_slen:]
            position_ids = position_ids[:, -_slen:]
            if langs is not None:
                langs = langs[:, -_slen:]
            mask = mask[:, -_slen:]
            attn_mask = attn_mask[:, -_slen:]

        # embeddings
        if inputs_embeds is None:
            inputs_embeds = self.embeddings(input_ids)

        tensor = inputs_embeds + self.position_embeddings(position_ids).expand_as(inputs_embeds)
        if langs is not None and self.use_lang_emb and self.n_langs > 1:
            tensor = tensor + self.lang_embeddings(langs)
        if token_type_ids is not None:
            tensor = tensor + self.embeddings(token_type_ids)
        tensor = self.layer_norm_emb(tensor)
        tensor = F.dropout(tensor, p=self.dropout, training=self.training)
        tensor *= mask.unsqueeze(-1).to(tensor.dtype)

        # transformer layers
        hidden_states = ()
        attentions = ()
        for i in range(self.n_layers):
            if self.output_hidden_states:
                hidden_states = hidden_states + (tensor,)

            # self attention
            attn_outputs = self.attentions[i](tensor, attn_mask, cache=cache, head_mask=head_mask[i])
            attn = attn_outputs[0]
            if self.output_attentions:
                attentions = attentions + (attn_outputs[1],)
            attn = F.dropout(attn, p=self.dropout, training=self.training)
            tensor = tensor + attn
            tensor = self.layer_norm1[i](tensor)

            # encoder attention (for decoder only)
            # if self.is_decoder and src_enc is not None:
            #     attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)
            #     attn = F.dropout(attn, p=self.dropout, training=self.training)
            #     tensor = tensor + attn
            #     tensor = self.layer_norm15[i](tensor)

            # FFN
            tensor = tensor + self.ffns[i](tensor)
            tensor = self.layer_norm2[i](tensor)
            tensor *= mask.unsqueeze(-1).to(tensor.dtype)

        # Add last hidden state
        if self.output_hidden_states:
            hidden_states = hidden_states + (tensor,)

        # update cache length
        if cache is not None:
            cache[""slen""] += tensor.size(1)

        # move back sequence length to dimension 0
        # tensor = tensor.transpose(0, 1)

        outputs = (tensor,)
        if self.output_hidden_states:
            outputs = outputs + (hidden_states,)
        if self.output_attentions:
            outputs = outputs + (attentions,)
        return outputs",cache is not None and input_ids is not None,cache is not None is not input_ids
webpy,https://github.com/webpy/webpy/tree/master/web/template.py,Parser,read_expr$245,"def read_expr(self, text, escape=True):
        """"""Reads a python expression from the text and returns the expression and remaining text.

        expr -> simple_expr | paren_expr
        simple_expr -> id extended_expr
        extended_expr -> attr_access | paren_expr extended_expr | ''
        attr_access -> dot id extended_expr
        paren_expr -> [ tokens ] | ( tokens ) | { tokens }

            >>> read_expr = Parser().read_expr
            >>> read_expr(""name"")
            ($name, '')
            >>> read_expr(""a.b and c"")
            ($a.b, ' and c')
            >>> read_expr(""a. b"")
            ($a, '. b')
            >>> read_expr(""name</h1>"")
            ($name, '</h1>')
            >>> read_expr(""(limit)ing"")
            ($(limit), 'ing')
            >>> read_expr('a[1, 2][:3].f(1+2, ""weird string[)."", 3 + 4) done.')
            ($a[1, 2][:3].f(1+2, ""weird string[)."", 3 + 4), ' done.')
        """"""

        def simple_expr():
            identifier()
            extended_expr()

        def identifier():
            next(tokens)

        def extended_expr():
            lookahead = tokens.lookahead()
            if lookahead is None:
                return
            elif lookahead.value == ""."":
                attr_access()
            elif lookahead.value in parens:
                paren_expr()
                extended_expr()
            else:
                return

        def attr_access():
            from token import NAME  # python token constants

            if tokens.lookahead2().type == NAME:
                next(tokens)  # consume dot
                identifier()
                extended_expr()

        def paren_expr():
            begin = next(tokens).value
            end = parens[begin]
            while True:
                if tokens.lookahead().value in parens:
                    paren_expr()
                else:
                    t = next(tokens)
                    if t.value == end:
                        break
            return

        parens = {""("": "")"", ""["": ""]"", ""{"": ""}""}

        def get_tokens(text):
            """"""tokenize text using python tokenizer.
            Python tokenizer ignores spaces, but they might be important in some cases.
            This function introduces dummy space tokens when it identifies any ignored space.
            Each token is a storage object containing type, value, begin and end.
            """"""
            i = iter([text])
            readline = lambda: next(i)
            end = None
            for t in tokenize.generate_tokens(readline):
                t = storage(type=t[0], value=t[1], begin=t[2], end=t[3])
                if end is not None and end != t.begin:
                    _, x1 = end
                    _, x2 = t.begin
                    yield storage(type=-1, value=text[x1:x2], begin=end, end=t.begin)
                end = t.end
                yield t

        class BetterIter:
            """"""Iterator like object with 2 support for 2 look aheads.""""""

            def __init__(self, items):
                self.iteritems = iter(items)
                self.items = []
                self.position = 0
                self.current_item = None

            def lookahead(self):
                if len(self.items) <= self.position:
                    self.items.append(self._next())
                return self.items[self.position]

            def _next(self):
                try:
                    return next(self.iteritems)
                except StopIteration:
                    return None

            def lookahead2(self):
                if len(self.items) <= self.position + 1:
                    self.items.append(self._next())
                return self.items[self.position + 1]

            def __next__(self):
                self.current_item = self.lookahead()
                self.position += 1
                return self.current_item

        tokens = BetterIter(get_tokens(text))

        if tokens.lookahead().value in parens:
            paren_expr()
        else:
            simple_expr()
        row, col = tokens.current_item.end
        return ExpressionNode(text[:col], escape=escape), text[col:]",end is not None and end != t.begin,None is not end != t.begin
strawberryfields,https://github.com/XanaduAI/strawberryfields/tree/master/strawberryfields/program.py,Program,compile$633,"def compile(self, *, device=None, compiler=None, **kwargs):
        """"""Compile the program given a Strawberry Fields photonic compiler, or
        hardware device specification.

        The compilation process can involve up to three stages:

        1. **Validation:** Validates properties of the program, including number of modes and
           allowed operations, making sure all the :doc:`/introduction/ops` used are accepted by the
           compiler.

        2. **Decomposition:** Once the program has been validated, decomposition are performed,
           transforming certain gates into sequences of simpler gates.

        3. **General compilation:** Finally, the compiler might specify bespoke compilation logic
           for transforming the  quantum circuit into an equivalent circuit which can be executed
           by the target device.

        **Example:**

        The ``gbs`` compile target will
        compile a circuit consisting of Gaussian operations and Fock measurements
        into canonical Gaussian boson sampling form.

        >>> prog2 = prog.compile(compiler=""gbs"")

        For a hardware device a :class:`~.Device` object, and optionally a specified compile strategy,
        must be supplied. If no compile strategy is supplied the default compiler from the device
        specification is used.

        >>> eng = sf.RemoteEngine(""X8"")
        >>> device = eng.device_spec
        >>> prog2 = prog.compile(device=device, compiler=""Xcov"")

        Args:
            device (~strawberryfields.Device): device specification object to use for
                program compilation
            compiler (str, ~strawberryfields.compilers.Compiler): Compiler name or compile strategy
                to use. If a device is specified, this overrides the compile strategy specified by
                the hardware :class:`~.Device`.

        Keyword Args:
            optimize (bool): If True, try to optimize the program by merging and canceling gates.
                The default is False.
            warn_connected (bool): If True, the user is warned if the quantum circuit is not weakly
                connected. The default is True.
            shots (int): Number of times the program measurement evaluation is repeated. Passed
                along to the compiled program's ``run_options``.

        Returns:
            Program: compiled program
        """"""
        # pylint: disable=too-many-branches
        if device is None and compiler is None:
            raise ValueError(""Either one or both of 'device' and 'compiler' must be specified"")

        def _get_compiler(compiler_or_name):
            if compiler_or_name in compiler_db:
                return compiler_db[compiler_or_name]()

            if isinstance(compiler_or_name, Compiler):
                return compiler_or_name

            raise ValueError(f""Unknown compiler '{compiler_or_name}'."")

        if device is not None:
            target = device.target

            if compiler is None:
                # get the default compiler from the device spec
                compiler = compiler_db[device.default_compiler]()
            else:
                compiler = _get_compiler(compiler)

            if device.modes is not None:
                # check that the number of modes is correct, if device.modes is provided
                # as an integer, or that the number of measurements is within the allowed
                # limits for each measurement type, if `device.modes` is a dictionary
                self.assert_modes(device)

            # if a device layout exist and the device default compiler is the same as
            # the requested compiler, initialize the circuit in the compiler class
            if device.layout and device.default_compiler == compiler.short_name:
                compiler.init_circuit(device.layout)
        else:
            compiler = _get_compiler(compiler)
            target = compiler.short_name

        seq = compiler.decompose(self.circuit)

        if kwargs.get(""warn_connected"", True):
            DAG = pu.list_to_DAG(seq)
            temp = nx.algorithms.components.number_weakly_connected_components(DAG)
            if temp > 1:
                warnings.warn(""The circuit consists of {} disconnected components."".format(temp))

        # run optimizations
        if kwargs.get(""optimize"", False):
            seq = pu.optimize_circuit(seq)

        compiled = self._linked_copy()

        seq = compiler.compile(seq, self.register)

        # create the compiled Program
        compiled.circuit = seq
        compiled._target = target  # pylint: disable=protected-access
        compiled._compile_info = (device, compiler.short_name)  # pylint: disable=protected-access

        # parameters are updated if necessary due to compiler changes
        compiler.update_params(compiled, device)

        # Get run options of compiled program.
        run_options = {k: kwargs[k] for k in ALLOWED_RUN_OPTIONS if k in kwargs}
        compiled.run_options.update(run_options)

        # set backend options of the program
        backend_options = {k: kwargs[k] for k in kwargs if k not in ALLOWED_RUN_OPTIONS}
        compiled.backend_options.update(backend_options)

        if kwargs.get(""realistic_loss"", False):
            try:
                compiler.add_loss(compiled, device)
            except NotImplementedError:
                warnings.warn(f""Compiler {compiler} does not support adding realistic loss."")

        # if device spec has allowed gate parameters, validate the applied gate parameters
        if device and device.gate_parameters:
            if not device.layout:
                raise ValueError(
                    ""Gate parameters cannot be validated. Device specification is missing a ""
                    ""circuit layout.""
                )

            pu.validate_gate_parameters(compiled)

        return compiled",device is None and compiler is None,device is None is compiler
geemap,https://github.com/giswqs/geemap/tree/master/geemap/toolbar.py,,sankee_gui$2606,"def sankee_gui(m=None):

    import sankee

    widget_width = ""250px""
    padding = ""0px 0px 0px 5px""  # upper, right, bottom, left

    toolbar_button = widgets.ToggleButton(
        value=False,
        tooltip=""Toolbar"",
        icon=""random"",
        layout=widgets.Layout(width=""28px"", height=""28px"", padding=""0px 0px 0px 4px""),
    )

    close_button = widgets.ToggleButton(
        value=False,
        tooltip=""Close the tool"",
        icon=""times"",
        button_style=""primary"",
        layout=widgets.Layout(height=""28px"", width=""28px"", padding=""0px 0px 0px 4px""),
    )

    region = widgets.Dropdown(
        options=[""User-drawn ROI""],
        value=""User-drawn ROI"",
        description=""Region:"",
        layout=widgets.Layout(width=widget_width, padding=padding),
        style={""description_width"": ""initial""},
    )

    def region_changed(change):
        if change[""new""] == ""Las Vegas"":
            if m is not None:
                las_vegas = ee.Geometry.Polygon(
                    [
                        [
                            [-115.01184401606046, 36.24170785506492],
                            [-114.98849806879484, 36.29928186470082],
                            [-115.25628981684171, 36.35238941394592],
                            [-115.34692702387296, 36.310348922031565],
                            [-115.37988600824796, 36.160811202271944],
                            [-115.30298171137296, 36.03653336474891],
                            [-115.25628981684171, 36.05207884201088],
                            [-115.26590285395109, 36.226199908103695],
                            [-115.19174513910734, 36.25499793268206],
                        ]
                    ]
                )
                m.addLayer(las_vegas, {}, ""Las Vegas"")
                m.centerObject(las_vegas, 10)

    region.observe(region_changed, ""value"")

    dataset = widgets.Dropdown(
        options=[
            ""NLCD - National Land Cover Database"",
            ""MCD12Q1 - MODIS Global Land Cover"",
            ""CGLS - Copernicus Global Land Cover"",
            ""LCMS - Land Change Monitoring System"",
        ],
        value=""NLCD - National Land Cover Database"",
        description=""Dataset:"",
        layout=widgets.Layout(width=widget_width, padding=padding),
        style={""description_width"": ""initial""},
    )

    NLCD_options = [""2001"", ""2004"", ""2006"", ""2008"", ""2011"", ""2013"", ""2016""]
    MODIS_options = [str(y) for y in range(2001, 2020)]
    CGLS_options = [str(y) for y in range(2015, 2020)]
    LCMS_options = [str(y) for y in range(1985, 2021)]

    before = widgets.Dropdown(
        options=NLCD_options,
        value=""2001"",
        description=""Before:"",
        layout=widgets.Layout(width=""123px"", padding=padding),
        style={""description_width"": ""initial""},
    )

    after = widgets.Dropdown(
        options=NLCD_options,
        value=""2016"",
        description=""After:"",
        layout=widgets.Layout(width=""123px"", padding=padding),
        style={""description_width"": ""initial""},
    )

    def dataset_changed(change):
        if change[""new""] == ""NLCD - National Land Cover Database"":
            before.options = NLCD_options
            after.options = NLCD_options
            before.value = NLCD_options[0]
            after.value = NLCD_options[-1]
        elif change[""new""] == ""MCD12Q1 - MODIS Global Land Cover"":
            before.options = MODIS_options
            after.options = MODIS_options
            before.value = MODIS_options[0]
            after.value = MODIS_options[-1]
        elif change[""new""] == ""CGLS - Copernicus Global Land Cover"":
            before.options = CGLS_options
            after.options = CGLS_options
            before.value = CGLS_options[0]
            after.value = CGLS_options[-1]
        elif change[""new""] == ""LCMS - Land Change Monitoring System"":
            before.options = LCMS_options
            after.options = LCMS_options
            before.value = LCMS_options[0]
            after.value = LCMS_options[-1]

    dataset.observe(dataset_changed, ""value"")

    dataset_template = {
        ""NLCD - National Land Cover Database"": sankee.datasets.NLCD2016,
        ""MCD12Q1 - MODIS Global Land Cover"": sankee.datasets.MODIS_LC_TYPE1,
        ""CGLS - Copernicus Global Land Cover"": sankee.datasets.CGLS_LC100,
        ""LCMS - Land Change Monitoring System"": sankee.datasets.LCMS_LC,
    }

    band_name = {
        ""NLCD - National Land Cover Database"": ""landcover"",
        ""MCD12Q1 - MODIS Global Land Cover"": ""LC_Type1"",
        ""CGLS - Copernicus Global Land Cover"": ""discrete_classification"",
        ""LCMS - Land Change Monitoring System"": ""Land_Cover"",
    }

    samples = widgets.IntText(
        value=1000,
        description=""Samples:"",
        placeholder=""The number of samples points to randomly generate for characterizing all images"",
        style={""description_width"": ""initial""},
        layout=widgets.Layout(width=""133px"", padding=padding),
    )

    classes = widgets.IntText(
        value=6,
        description=""Classes:"",
        style={""description_width"": ""initial""},
        layout=widgets.Layout(width=""113px"", padding=padding),
    )

    title = widgets.Text(
        value=""Land Cover Change"",
        description=""Title:"",
        style={""description_width"": ""initial""},
        layout=widgets.Layout(width=widget_width, padding=padding),
    )

    buttons = widgets.ToggleButtons(
        value=None,
        options=[""Apply"", ""Reset"", ""Close""],
        tooltips=[""Apply"", ""Reset"", ""Close""],
        button_style=""primary"",
    )
    buttons.style.button_width = ""80px""

    output = widgets.Output(layout=widgets.Layout(padding=padding))

    toolbar_widget = widgets.VBox()
    toolbar_widget.children = [toolbar_button]
    toolbar_header = widgets.HBox()
    toolbar_header.children = [close_button, toolbar_button]
    toolbar_footer = widgets.VBox()
    toolbar_footer.children = [
        region,
        dataset,
        widgets.HBox([before, after]),
        widgets.HBox([samples, classes]),
        title,
        buttons,
        output,
    ]

    toolbar_event = ipyevents.Event(
        source=toolbar_widget, watched_events=[""mouseenter"", ""mouseleave""]
    )

    if m is not None:
        if ""Las Vegas"" not in m.ee_vector_layer_names:
            region.options = [""User-drawn ROI"", ""Las Vegas""] + m.ee_vector_layer_names
        else:
            region.options = [""User-drawn ROI""] + m.ee_vector_layer_names

        plot_close_btn = widgets.Button(
            tooltip=""Close the plot"",
            icon=""times"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def plot_close_btn_clicked(b):
            plot_widget.children = []

        plot_close_btn.on_click(plot_close_btn_clicked)

        plot_reset_btn = widgets.Button(
            tooltip=""Reset the plot"",
            icon=""home"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def plot_reset_btn_clicked(b):

            m.sankee_plot.update_layout(
                width=600,
                height=250,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        plot_reset_btn.on_click(plot_reset_btn_clicked)

        plot_fullscreen_btn = widgets.Button(
            tooltip=""Fullscreen the plot"",
            icon=""arrows-alt"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def plot_fullscreen_btn_clicked(b):

            m.sankee_plot.update_layout(
                width=1030,
                height=int(m.layout.height[:-2]) - 60,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        plot_fullscreen_btn.on_click(plot_fullscreen_btn_clicked)

        width_btn = widgets.Button(
            tooltip=""Change plot width"",
            icon=""arrows-h"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def width_btn_clicked(b):
            m.sankee_plot.update_layout(
                width=1030,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        width_btn.on_click(width_btn_clicked)

        height_btn = widgets.Button(
            tooltip=""Change plot height"",
            icon=""arrows-v"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def height_btn_clicked(b):
            m.sankee_plot.update_layout(
                height=int(m.layout.height[:-2]) - 60,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        height_btn.on_click(height_btn_clicked)

        width_slider = widgets.IntSlider(
            value=600,
            min=400,
            max=1030,
            step=10,
            description="""",
            readout=False,
            continuous_update=False,
            layout=widgets.Layout(width=""100px"", padding=padding),
            style={""description_width"": ""initial""},
        )

        width_slider_label = widgets.Label(
            layout=widgets.Layout(padding=""0px 10px 0px 0px"")
        )
        widgets.jslink((width_slider, ""value""), (width_slider_label, ""value""))

        def width_changed(change):
            if change[""new""]:

                m.sankee_plot.update_layout(
                    width=width_slider.value,
                    margin=dict(l=10, r=10, b=10, t=50, pad=5),
                )
                with plot_output:
                    plot_output.clear_output()
                    display(m.sankee_plot)

        width_slider.observe(width_changed, ""value"")

        height_slider = widgets.IntSlider(
            value=250,
            min=200,
            max=int(m.layout.height[:-2]) - 60,
            step=10,
            description="""",
            readout=False,
            continuous_update=False,
            layout=widgets.Layout(width=""100px"", padding=padding),
            style={""description_width"": ""initial""},
        )

        height_slider_label = widgets.Label()
        widgets.jslink((height_slider, ""value""), (height_slider_label, ""value""))

        def height_changed(change):
            if change[""new""]:

                m.sankee_plot.update_layout(
                    height=height_slider.value,
                    margin=dict(l=10, r=10, b=10, t=50, pad=5),
                )
                with plot_output:
                    plot_output.clear_output()
                    display(m.sankee_plot)

        height_slider.observe(height_changed, ""value"")

        plot_output = widgets.Output()

        plot_widget = widgets.VBox([plot_output])

        sankee_control = ipyleaflet.WidgetControl(
            widget=plot_widget, position=""bottomright""
        )
        m.add_control(sankee_control)
        m.sankee_control = sankee_control

    def handle_toolbar_event(event):

        if event[""type""] == ""mouseenter"":
            toolbar_widget.children = [toolbar_header, toolbar_footer]
        elif event[""type""] == ""mouseleave"":
            if not toolbar_button.value:
                toolbar_widget.children = [toolbar_button]
                toolbar_button.value = False
                close_button.value = False

    toolbar_event.on_dom_event(handle_toolbar_event)

    def toolbar_btn_click(change):
        if change[""new""]:
            close_button.value = False
            toolbar_widget.children = [toolbar_header, toolbar_footer]
        else:
            if not close_button.value:
                toolbar_widget.children = [toolbar_button]

    toolbar_button.observe(toolbar_btn_click, ""value"")

    def close_btn_click(change):
        if change[""new""]:
            toolbar_button.value = False
            if m is not None:
                m.toolbar_reset()
                if m.tool_control is not None and m.tool_control in m.controls:
                    m.remove_control(m.tool_control)
                    m.tool_control = None
                if m.sankee_control is not None and m.sankee_control in m.controls:
                    m.remove_control(m.sankee_control)
                    m.sankee_control = None
            toolbar_widget.close()

    close_button.observe(close_btn_click, ""value"")

    def button_clicked(change):
        if change[""new""] == ""Apply"":
            with output:
                output.clear_output()
                plot_output.clear_output()
                print(""Running ..."")

            if m is not None:
                exclude_classes = []

                if ""NLCD"" in dataset.value:
                    before_img = ee.Image(f""USGS/NLCD/NLCD{before.value}"")
                    after_img = ee.Image(f""USGS/NLCD/NLCD{after.value}"")
                    vis_params = {}
                elif ""MODIS"" in dataset.value:
                    before_img = ee.Image(f""MODIS/006/MCD12Q1/{before.value}_01_01"")
                    after_img = ee.Image(f""MODIS/006/MCD12Q1/{after.value}_01_01"")
                    vis_params = {
                        ""min"": 1.0,
                        ""max"": 17.0,
                        ""palette"": [
                            ""05450a"",
                            ""086a10"",
                            ""54a708"",
                            ""78d203"",
                            ""009900"",
                            ""c6b044"",
                            ""dcd159"",
                            ""dade48"",
                            ""fbff13"",
                            ""b6ff05"",
                            ""27ff87"",
                            ""c24f44"",
                            ""a5a5a5"",
                            ""ff6d4c"",
                            ""69fff8"",
                            ""f9ffa4"",
                            ""1c0dff"",
                        ],
                    }
                elif ""CGLS"" in dataset.value:
                    before_img = ee.Image(
                        f""COPERNICUS/Landcover/100m/Proba-V-C3/Global/{before.value}""
                    )
                    after_img = ee.Image(
                        f""COPERNICUS/Landcover/100m/Proba-V-C3/Global/{after.value}""
                    )
                    vis_params = {}
                elif ""LCMS"" in dataset.value:
                    before_img = ee.Image(
                        f""USFS/GTAC/LCMS/v2020-5/LCMS_CONUS_v2020-5_{before.value}""
                    )
                    after_img = ee.Image(
                        f""USFS/GTAC/LCMS/v2020-5/LCMS_CONUS_v2020-5_{after.value}""
                    )
                    vis_params = {}
                    # LCMS Land Cover class 15 is a no data mask and should be excluded
                    exclude_classes.append(15)

                img_list = [before_img, after_img]
                label_list = [before.value, after.value]

                image1 = before_img.select(band_name[dataset.value])
                image2 = after_img.select(band_name[dataset.value])

                if region.value != ""User-drawn ROI"" or (
                    region.value == ""User-drawn ROI"" and m.user_roi is not None
                ):

                    if region.value == ""User-drawn ROI"":
                        geom = m.user_roi
                        image1 = image1.clip(geom)
                        image2 = image2.clip(geom)
                    else:
                        roi_object = m.ee_layer_dict[region.value][""ee_object""]
                        if region.value == ""Las Vegas"":
                            m.centerObject(roi_object, 10)
                        if isinstance(roi_object, ee.Geometry):
                            geom = roi_object
                            image1 = image1.clip(geom)
                            image2 = image2.clip(geom)
                        else:
                            roi_object = ee.FeatureCollection(roi_object)
                            image1 = image1.clipToCollection(roi_object)
                            image2 = image2.clipToCollection(roi_object)
                            geom = roi_object.geometry()

                    if len(title.value) > 0:
                        plot_title = title.value
                    else:
                        plot_title = None
                    m.default_style = {""cursor"": ""wait""}
                    plot = sankee.sankify(
                        img_list,
                        geom,
                        label_list,
                        dataset_template[dataset.value],
                        max_classes=classes.value,
                        n=int(samples.value),
                        title=plot_title,
                        exclude=exclude_classes,
                    )

                    output.clear_output()
                    plot_output.clear_output()
                    with plot_output:
                        plot.update_layout(
                            width=600,
                            height=250,
                            margin=dict(l=10, r=10, b=10, t=50, pad=5),
                        )
                        plot_widget.children = [
                            widgets.HBox(
                                [
                                    plot_close_btn,
                                    plot_reset_btn,
                                    plot_fullscreen_btn,
                                    width_btn,
                                    width_slider,
                                    width_slider_label,
                                    height_btn,
                                    height_slider,
                                    height_slider_label,
                                ]
                            ),
                            plot_output,
                        ]
                        display(plot)

                    m.sankee_plot = plot
                    m.addLayer(image1, vis_params, before.value)
                    m.addLayer(image2, vis_params, after.value)
                    m.default_style = {""cursor"": ""default""}

                else:
                    with output:
                        output.clear_output()
                        print(""Draw a polygon on the map."")

        elif change[""new""] == ""Reset"":
            output.clear_output()
            plot_output.clear_output()
            plot_widget.children = []

        elif change[""new""] == ""Close"":
            if m is not None:
                m.toolbar_reset()
                if m.tool_control is not None and m.tool_control in m.controls:
                    m.remove_control(m.tool_control)
                    m.tool_control = None
                if m.sankee_control is not None and m.sankee_control in m.controls:
                    m.remove_control(m.sankee_control)
                    m.sankee_control = None
            toolbar_widget.close()

        buttons.value = None

    buttons.observe(button_clicked, ""value"")

    toolbar_button.value = True
    if m is not None:
        toolbar_control = ipyleaflet.WidgetControl(
            widget=toolbar_widget, position=""topright""
        )

        if toolbar_control not in m.controls:
            m.add_control(toolbar_control)
            m.tool_control = toolbar_control
    else:
        return toolbar_widget",m.tool_control is not None and m.tool_control in m.controls,None is not m.tool_control in m.controls
geemap,https://github.com/giswqs/geemap/tree/master/geemap/toolbar.py,,sankee_gui$2606,"def sankee_gui(m=None):

    import sankee

    widget_width = ""250px""
    padding = ""0px 0px 0px 5px""  # upper, right, bottom, left

    toolbar_button = widgets.ToggleButton(
        value=False,
        tooltip=""Toolbar"",
        icon=""random"",
        layout=widgets.Layout(width=""28px"", height=""28px"", padding=""0px 0px 0px 4px""),
    )

    close_button = widgets.ToggleButton(
        value=False,
        tooltip=""Close the tool"",
        icon=""times"",
        button_style=""primary"",
        layout=widgets.Layout(height=""28px"", width=""28px"", padding=""0px 0px 0px 4px""),
    )

    region = widgets.Dropdown(
        options=[""User-drawn ROI""],
        value=""User-drawn ROI"",
        description=""Region:"",
        layout=widgets.Layout(width=widget_width, padding=padding),
        style={""description_width"": ""initial""},
    )

    def region_changed(change):
        if change[""new""] == ""Las Vegas"":
            if m is not None:
                las_vegas = ee.Geometry.Polygon(
                    [
                        [
                            [-115.01184401606046, 36.24170785506492],
                            [-114.98849806879484, 36.29928186470082],
                            [-115.25628981684171, 36.35238941394592],
                            [-115.34692702387296, 36.310348922031565],
                            [-115.37988600824796, 36.160811202271944],
                            [-115.30298171137296, 36.03653336474891],
                            [-115.25628981684171, 36.05207884201088],
                            [-115.26590285395109, 36.226199908103695],
                            [-115.19174513910734, 36.25499793268206],
                        ]
                    ]
                )
                m.addLayer(las_vegas, {}, ""Las Vegas"")
                m.centerObject(las_vegas, 10)

    region.observe(region_changed, ""value"")

    dataset = widgets.Dropdown(
        options=[
            ""NLCD - National Land Cover Database"",
            ""MCD12Q1 - MODIS Global Land Cover"",
            ""CGLS - Copernicus Global Land Cover"",
            ""LCMS - Land Change Monitoring System"",
        ],
        value=""NLCD - National Land Cover Database"",
        description=""Dataset:"",
        layout=widgets.Layout(width=widget_width, padding=padding),
        style={""description_width"": ""initial""},
    )

    NLCD_options = [""2001"", ""2004"", ""2006"", ""2008"", ""2011"", ""2013"", ""2016""]
    MODIS_options = [str(y) for y in range(2001, 2020)]
    CGLS_options = [str(y) for y in range(2015, 2020)]
    LCMS_options = [str(y) for y in range(1985, 2021)]

    before = widgets.Dropdown(
        options=NLCD_options,
        value=""2001"",
        description=""Before:"",
        layout=widgets.Layout(width=""123px"", padding=padding),
        style={""description_width"": ""initial""},
    )

    after = widgets.Dropdown(
        options=NLCD_options,
        value=""2016"",
        description=""After:"",
        layout=widgets.Layout(width=""123px"", padding=padding),
        style={""description_width"": ""initial""},
    )

    def dataset_changed(change):
        if change[""new""] == ""NLCD - National Land Cover Database"":
            before.options = NLCD_options
            after.options = NLCD_options
            before.value = NLCD_options[0]
            after.value = NLCD_options[-1]
        elif change[""new""] == ""MCD12Q1 - MODIS Global Land Cover"":
            before.options = MODIS_options
            after.options = MODIS_options
            before.value = MODIS_options[0]
            after.value = MODIS_options[-1]
        elif change[""new""] == ""CGLS - Copernicus Global Land Cover"":
            before.options = CGLS_options
            after.options = CGLS_options
            before.value = CGLS_options[0]
            after.value = CGLS_options[-1]
        elif change[""new""] == ""LCMS - Land Change Monitoring System"":
            before.options = LCMS_options
            after.options = LCMS_options
            before.value = LCMS_options[0]
            after.value = LCMS_options[-1]

    dataset.observe(dataset_changed, ""value"")

    dataset_template = {
        ""NLCD - National Land Cover Database"": sankee.datasets.NLCD2016,
        ""MCD12Q1 - MODIS Global Land Cover"": sankee.datasets.MODIS_LC_TYPE1,
        ""CGLS - Copernicus Global Land Cover"": sankee.datasets.CGLS_LC100,
        ""LCMS - Land Change Monitoring System"": sankee.datasets.LCMS_LC,
    }

    band_name = {
        ""NLCD - National Land Cover Database"": ""landcover"",
        ""MCD12Q1 - MODIS Global Land Cover"": ""LC_Type1"",
        ""CGLS - Copernicus Global Land Cover"": ""discrete_classification"",
        ""LCMS - Land Change Monitoring System"": ""Land_Cover"",
    }

    samples = widgets.IntText(
        value=1000,
        description=""Samples:"",
        placeholder=""The number of samples points to randomly generate for characterizing all images"",
        style={""description_width"": ""initial""},
        layout=widgets.Layout(width=""133px"", padding=padding),
    )

    classes = widgets.IntText(
        value=6,
        description=""Classes:"",
        style={""description_width"": ""initial""},
        layout=widgets.Layout(width=""113px"", padding=padding),
    )

    title = widgets.Text(
        value=""Land Cover Change"",
        description=""Title:"",
        style={""description_width"": ""initial""},
        layout=widgets.Layout(width=widget_width, padding=padding),
    )

    buttons = widgets.ToggleButtons(
        value=None,
        options=[""Apply"", ""Reset"", ""Close""],
        tooltips=[""Apply"", ""Reset"", ""Close""],
        button_style=""primary"",
    )
    buttons.style.button_width = ""80px""

    output = widgets.Output(layout=widgets.Layout(padding=padding))

    toolbar_widget = widgets.VBox()
    toolbar_widget.children = [toolbar_button]
    toolbar_header = widgets.HBox()
    toolbar_header.children = [close_button, toolbar_button]
    toolbar_footer = widgets.VBox()
    toolbar_footer.children = [
        region,
        dataset,
        widgets.HBox([before, after]),
        widgets.HBox([samples, classes]),
        title,
        buttons,
        output,
    ]

    toolbar_event = ipyevents.Event(
        source=toolbar_widget, watched_events=[""mouseenter"", ""mouseleave""]
    )

    if m is not None:
        if ""Las Vegas"" not in m.ee_vector_layer_names:
            region.options = [""User-drawn ROI"", ""Las Vegas""] + m.ee_vector_layer_names
        else:
            region.options = [""User-drawn ROI""] + m.ee_vector_layer_names

        plot_close_btn = widgets.Button(
            tooltip=""Close the plot"",
            icon=""times"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def plot_close_btn_clicked(b):
            plot_widget.children = []

        plot_close_btn.on_click(plot_close_btn_clicked)

        plot_reset_btn = widgets.Button(
            tooltip=""Reset the plot"",
            icon=""home"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def plot_reset_btn_clicked(b):

            m.sankee_plot.update_layout(
                width=600,
                height=250,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        plot_reset_btn.on_click(plot_reset_btn_clicked)

        plot_fullscreen_btn = widgets.Button(
            tooltip=""Fullscreen the plot"",
            icon=""arrows-alt"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def plot_fullscreen_btn_clicked(b):

            m.sankee_plot.update_layout(
                width=1030,
                height=int(m.layout.height[:-2]) - 60,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        plot_fullscreen_btn.on_click(plot_fullscreen_btn_clicked)

        width_btn = widgets.Button(
            tooltip=""Change plot width"",
            icon=""arrows-h"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def width_btn_clicked(b):
            m.sankee_plot.update_layout(
                width=1030,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        width_btn.on_click(width_btn_clicked)

        height_btn = widgets.Button(
            tooltip=""Change plot height"",
            icon=""arrows-v"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def height_btn_clicked(b):
            m.sankee_plot.update_layout(
                height=int(m.layout.height[:-2]) - 60,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        height_btn.on_click(height_btn_clicked)

        width_slider = widgets.IntSlider(
            value=600,
            min=400,
            max=1030,
            step=10,
            description="""",
            readout=False,
            continuous_update=False,
            layout=widgets.Layout(width=""100px"", padding=padding),
            style={""description_width"": ""initial""},
        )

        width_slider_label = widgets.Label(
            layout=widgets.Layout(padding=""0px 10px 0px 0px"")
        )
        widgets.jslink((width_slider, ""value""), (width_slider_label, ""value""))

        def width_changed(change):
            if change[""new""]:

                m.sankee_plot.update_layout(
                    width=width_slider.value,
                    margin=dict(l=10, r=10, b=10, t=50, pad=5),
                )
                with plot_output:
                    plot_output.clear_output()
                    display(m.sankee_plot)

        width_slider.observe(width_changed, ""value"")

        height_slider = widgets.IntSlider(
            value=250,
            min=200,
            max=int(m.layout.height[:-2]) - 60,
            step=10,
            description="""",
            readout=False,
            continuous_update=False,
            layout=widgets.Layout(width=""100px"", padding=padding),
            style={""description_width"": ""initial""},
        )

        height_slider_label = widgets.Label()
        widgets.jslink((height_slider, ""value""), (height_slider_label, ""value""))

        def height_changed(change):
            if change[""new""]:

                m.sankee_plot.update_layout(
                    height=height_slider.value,
                    margin=dict(l=10, r=10, b=10, t=50, pad=5),
                )
                with plot_output:
                    plot_output.clear_output()
                    display(m.sankee_plot)

        height_slider.observe(height_changed, ""value"")

        plot_output = widgets.Output()

        plot_widget = widgets.VBox([plot_output])

        sankee_control = ipyleaflet.WidgetControl(
            widget=plot_widget, position=""bottomright""
        )
        m.add_control(sankee_control)
        m.sankee_control = sankee_control

    def handle_toolbar_event(event):

        if event[""type""] == ""mouseenter"":
            toolbar_widget.children = [toolbar_header, toolbar_footer]
        elif event[""type""] == ""mouseleave"":
            if not toolbar_button.value:
                toolbar_widget.children = [toolbar_button]
                toolbar_button.value = False
                close_button.value = False

    toolbar_event.on_dom_event(handle_toolbar_event)

    def toolbar_btn_click(change):
        if change[""new""]:
            close_button.value = False
            toolbar_widget.children = [toolbar_header, toolbar_footer]
        else:
            if not close_button.value:
                toolbar_widget.children = [toolbar_button]

    toolbar_button.observe(toolbar_btn_click, ""value"")

    def close_btn_click(change):
        if change[""new""]:
            toolbar_button.value = False
            if m is not None:
                m.toolbar_reset()
                if m.tool_control is not None and m.tool_control in m.controls:
                    m.remove_control(m.tool_control)
                    m.tool_control = None
                if m.sankee_control is not None and m.sankee_control in m.controls:
                    m.remove_control(m.sankee_control)
                    m.sankee_control = None
            toolbar_widget.close()

    close_button.observe(close_btn_click, ""value"")

    def button_clicked(change):
        if change[""new""] == ""Apply"":
            with output:
                output.clear_output()
                plot_output.clear_output()
                print(""Running ..."")

            if m is not None:
                exclude_classes = []

                if ""NLCD"" in dataset.value:
                    before_img = ee.Image(f""USGS/NLCD/NLCD{before.value}"")
                    after_img = ee.Image(f""USGS/NLCD/NLCD{after.value}"")
                    vis_params = {}
                elif ""MODIS"" in dataset.value:
                    before_img = ee.Image(f""MODIS/006/MCD12Q1/{before.value}_01_01"")
                    after_img = ee.Image(f""MODIS/006/MCD12Q1/{after.value}_01_01"")
                    vis_params = {
                        ""min"": 1.0,
                        ""max"": 17.0,
                        ""palette"": [
                            ""05450a"",
                            ""086a10"",
                            ""54a708"",
                            ""78d203"",
                            ""009900"",
                            ""c6b044"",
                            ""dcd159"",
                            ""dade48"",
                            ""fbff13"",
                            ""b6ff05"",
                            ""27ff87"",
                            ""c24f44"",
                            ""a5a5a5"",
                            ""ff6d4c"",
                            ""69fff8"",
                            ""f9ffa4"",
                            ""1c0dff"",
                        ],
                    }
                elif ""CGLS"" in dataset.value:
                    before_img = ee.Image(
                        f""COPERNICUS/Landcover/100m/Proba-V-C3/Global/{before.value}""
                    )
                    after_img = ee.Image(
                        f""COPERNICUS/Landcover/100m/Proba-V-C3/Global/{after.value}""
                    )
                    vis_params = {}
                elif ""LCMS"" in dataset.value:
                    before_img = ee.Image(
                        f""USFS/GTAC/LCMS/v2020-5/LCMS_CONUS_v2020-5_{before.value}""
                    )
                    after_img = ee.Image(
                        f""USFS/GTAC/LCMS/v2020-5/LCMS_CONUS_v2020-5_{after.value}""
                    )
                    vis_params = {}
                    # LCMS Land Cover class 15 is a no data mask and should be excluded
                    exclude_classes.append(15)

                img_list = [before_img, after_img]
                label_list = [before.value, after.value]

                image1 = before_img.select(band_name[dataset.value])
                image2 = after_img.select(band_name[dataset.value])

                if region.value != ""User-drawn ROI"" or (
                    region.value == ""User-drawn ROI"" and m.user_roi is not None
                ):

                    if region.value == ""User-drawn ROI"":
                        geom = m.user_roi
                        image1 = image1.clip(geom)
                        image2 = image2.clip(geom)
                    else:
                        roi_object = m.ee_layer_dict[region.value][""ee_object""]
                        if region.value == ""Las Vegas"":
                            m.centerObject(roi_object, 10)
                        if isinstance(roi_object, ee.Geometry):
                            geom = roi_object
                            image1 = image1.clip(geom)
                            image2 = image2.clip(geom)
                        else:
                            roi_object = ee.FeatureCollection(roi_object)
                            image1 = image1.clipToCollection(roi_object)
                            image2 = image2.clipToCollection(roi_object)
                            geom = roi_object.geometry()

                    if len(title.value) > 0:
                        plot_title = title.value
                    else:
                        plot_title = None
                    m.default_style = {""cursor"": ""wait""}
                    plot = sankee.sankify(
                        img_list,
                        geom,
                        label_list,
                        dataset_template[dataset.value],
                        max_classes=classes.value,
                        n=int(samples.value),
                        title=plot_title,
                        exclude=exclude_classes,
                    )

                    output.clear_output()
                    plot_output.clear_output()
                    with plot_output:
                        plot.update_layout(
                            width=600,
                            height=250,
                            margin=dict(l=10, r=10, b=10, t=50, pad=5),
                        )
                        plot_widget.children = [
                            widgets.HBox(
                                [
                                    plot_close_btn,
                                    plot_reset_btn,
                                    plot_fullscreen_btn,
                                    width_btn,
                                    width_slider,
                                    width_slider_label,
                                    height_btn,
                                    height_slider,
                                    height_slider_label,
                                ]
                            ),
                            plot_output,
                        ]
                        display(plot)

                    m.sankee_plot = plot
                    m.addLayer(image1, vis_params, before.value)
                    m.addLayer(image2, vis_params, after.value)
                    m.default_style = {""cursor"": ""default""}

                else:
                    with output:
                        output.clear_output()
                        print(""Draw a polygon on the map."")

        elif change[""new""] == ""Reset"":
            output.clear_output()
            plot_output.clear_output()
            plot_widget.children = []

        elif change[""new""] == ""Close"":
            if m is not None:
                m.toolbar_reset()
                if m.tool_control is not None and m.tool_control in m.controls:
                    m.remove_control(m.tool_control)
                    m.tool_control = None
                if m.sankee_control is not None and m.sankee_control in m.controls:
                    m.remove_control(m.sankee_control)
                    m.sankee_control = None
            toolbar_widget.close()

        buttons.value = None

    buttons.observe(button_clicked, ""value"")

    toolbar_button.value = True
    if m is not None:
        toolbar_control = ipyleaflet.WidgetControl(
            widget=toolbar_widget, position=""topright""
        )

        if toolbar_control not in m.controls:
            m.add_control(toolbar_control)
            m.tool_control = toolbar_control
    else:
        return toolbar_widget",m.sankee_control is not None and m.sankee_control in m.controls,None is not m.sankee_control in m.controls
geemap,https://github.com/giswqs/geemap/tree/master/geemap/toolbar.py,,sankee_gui$2606,"def sankee_gui(m=None):

    import sankee

    widget_width = ""250px""
    padding = ""0px 0px 0px 5px""  # upper, right, bottom, left

    toolbar_button = widgets.ToggleButton(
        value=False,
        tooltip=""Toolbar"",
        icon=""random"",
        layout=widgets.Layout(width=""28px"", height=""28px"", padding=""0px 0px 0px 4px""),
    )

    close_button = widgets.ToggleButton(
        value=False,
        tooltip=""Close the tool"",
        icon=""times"",
        button_style=""primary"",
        layout=widgets.Layout(height=""28px"", width=""28px"", padding=""0px 0px 0px 4px""),
    )

    region = widgets.Dropdown(
        options=[""User-drawn ROI""],
        value=""User-drawn ROI"",
        description=""Region:"",
        layout=widgets.Layout(width=widget_width, padding=padding),
        style={""description_width"": ""initial""},
    )

    def region_changed(change):
        if change[""new""] == ""Las Vegas"":
            if m is not None:
                las_vegas = ee.Geometry.Polygon(
                    [
                        [
                            [-115.01184401606046, 36.24170785506492],
                            [-114.98849806879484, 36.29928186470082],
                            [-115.25628981684171, 36.35238941394592],
                            [-115.34692702387296, 36.310348922031565],
                            [-115.37988600824796, 36.160811202271944],
                            [-115.30298171137296, 36.03653336474891],
                            [-115.25628981684171, 36.05207884201088],
                            [-115.26590285395109, 36.226199908103695],
                            [-115.19174513910734, 36.25499793268206],
                        ]
                    ]
                )
                m.addLayer(las_vegas, {}, ""Las Vegas"")
                m.centerObject(las_vegas, 10)

    region.observe(region_changed, ""value"")

    dataset = widgets.Dropdown(
        options=[
            ""NLCD - National Land Cover Database"",
            ""MCD12Q1 - MODIS Global Land Cover"",
            ""CGLS - Copernicus Global Land Cover"",
            ""LCMS - Land Change Monitoring System"",
        ],
        value=""NLCD - National Land Cover Database"",
        description=""Dataset:"",
        layout=widgets.Layout(width=widget_width, padding=padding),
        style={""description_width"": ""initial""},
    )

    NLCD_options = [""2001"", ""2004"", ""2006"", ""2008"", ""2011"", ""2013"", ""2016""]
    MODIS_options = [str(y) for y in range(2001, 2020)]
    CGLS_options = [str(y) for y in range(2015, 2020)]
    LCMS_options = [str(y) for y in range(1985, 2021)]

    before = widgets.Dropdown(
        options=NLCD_options,
        value=""2001"",
        description=""Before:"",
        layout=widgets.Layout(width=""123px"", padding=padding),
        style={""description_width"": ""initial""},
    )

    after = widgets.Dropdown(
        options=NLCD_options,
        value=""2016"",
        description=""After:"",
        layout=widgets.Layout(width=""123px"", padding=padding),
        style={""description_width"": ""initial""},
    )

    def dataset_changed(change):
        if change[""new""] == ""NLCD - National Land Cover Database"":
            before.options = NLCD_options
            after.options = NLCD_options
            before.value = NLCD_options[0]
            after.value = NLCD_options[-1]
        elif change[""new""] == ""MCD12Q1 - MODIS Global Land Cover"":
            before.options = MODIS_options
            after.options = MODIS_options
            before.value = MODIS_options[0]
            after.value = MODIS_options[-1]
        elif change[""new""] == ""CGLS - Copernicus Global Land Cover"":
            before.options = CGLS_options
            after.options = CGLS_options
            before.value = CGLS_options[0]
            after.value = CGLS_options[-1]
        elif change[""new""] == ""LCMS - Land Change Monitoring System"":
            before.options = LCMS_options
            after.options = LCMS_options
            before.value = LCMS_options[0]
            after.value = LCMS_options[-1]

    dataset.observe(dataset_changed, ""value"")

    dataset_template = {
        ""NLCD - National Land Cover Database"": sankee.datasets.NLCD2016,
        ""MCD12Q1 - MODIS Global Land Cover"": sankee.datasets.MODIS_LC_TYPE1,
        ""CGLS - Copernicus Global Land Cover"": sankee.datasets.CGLS_LC100,
        ""LCMS - Land Change Monitoring System"": sankee.datasets.LCMS_LC,
    }

    band_name = {
        ""NLCD - National Land Cover Database"": ""landcover"",
        ""MCD12Q1 - MODIS Global Land Cover"": ""LC_Type1"",
        ""CGLS - Copernicus Global Land Cover"": ""discrete_classification"",
        ""LCMS - Land Change Monitoring System"": ""Land_Cover"",
    }

    samples = widgets.IntText(
        value=1000,
        description=""Samples:"",
        placeholder=""The number of samples points to randomly generate for characterizing all images"",
        style={""description_width"": ""initial""},
        layout=widgets.Layout(width=""133px"", padding=padding),
    )

    classes = widgets.IntText(
        value=6,
        description=""Classes:"",
        style={""description_width"": ""initial""},
        layout=widgets.Layout(width=""113px"", padding=padding),
    )

    title = widgets.Text(
        value=""Land Cover Change"",
        description=""Title:"",
        style={""description_width"": ""initial""},
        layout=widgets.Layout(width=widget_width, padding=padding),
    )

    buttons = widgets.ToggleButtons(
        value=None,
        options=[""Apply"", ""Reset"", ""Close""],
        tooltips=[""Apply"", ""Reset"", ""Close""],
        button_style=""primary"",
    )
    buttons.style.button_width = ""80px""

    output = widgets.Output(layout=widgets.Layout(padding=padding))

    toolbar_widget = widgets.VBox()
    toolbar_widget.children = [toolbar_button]
    toolbar_header = widgets.HBox()
    toolbar_header.children = [close_button, toolbar_button]
    toolbar_footer = widgets.VBox()
    toolbar_footer.children = [
        region,
        dataset,
        widgets.HBox([before, after]),
        widgets.HBox([samples, classes]),
        title,
        buttons,
        output,
    ]

    toolbar_event = ipyevents.Event(
        source=toolbar_widget, watched_events=[""mouseenter"", ""mouseleave""]
    )

    if m is not None:
        if ""Las Vegas"" not in m.ee_vector_layer_names:
            region.options = [""User-drawn ROI"", ""Las Vegas""] + m.ee_vector_layer_names
        else:
            region.options = [""User-drawn ROI""] + m.ee_vector_layer_names

        plot_close_btn = widgets.Button(
            tooltip=""Close the plot"",
            icon=""times"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def plot_close_btn_clicked(b):
            plot_widget.children = []

        plot_close_btn.on_click(plot_close_btn_clicked)

        plot_reset_btn = widgets.Button(
            tooltip=""Reset the plot"",
            icon=""home"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def plot_reset_btn_clicked(b):

            m.sankee_plot.update_layout(
                width=600,
                height=250,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        plot_reset_btn.on_click(plot_reset_btn_clicked)

        plot_fullscreen_btn = widgets.Button(
            tooltip=""Fullscreen the plot"",
            icon=""arrows-alt"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def plot_fullscreen_btn_clicked(b):

            m.sankee_plot.update_layout(
                width=1030,
                height=int(m.layout.height[:-2]) - 60,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        plot_fullscreen_btn.on_click(plot_fullscreen_btn_clicked)

        width_btn = widgets.Button(
            tooltip=""Change plot width"",
            icon=""arrows-h"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def width_btn_clicked(b):
            m.sankee_plot.update_layout(
                width=1030,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        width_btn.on_click(width_btn_clicked)

        height_btn = widgets.Button(
            tooltip=""Change plot height"",
            icon=""arrows-v"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def height_btn_clicked(b):
            m.sankee_plot.update_layout(
                height=int(m.layout.height[:-2]) - 60,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        height_btn.on_click(height_btn_clicked)

        width_slider = widgets.IntSlider(
            value=600,
            min=400,
            max=1030,
            step=10,
            description="""",
            readout=False,
            continuous_update=False,
            layout=widgets.Layout(width=""100px"", padding=padding),
            style={""description_width"": ""initial""},
        )

        width_slider_label = widgets.Label(
            layout=widgets.Layout(padding=""0px 10px 0px 0px"")
        )
        widgets.jslink((width_slider, ""value""), (width_slider_label, ""value""))

        def width_changed(change):
            if change[""new""]:

                m.sankee_plot.update_layout(
                    width=width_slider.value,
                    margin=dict(l=10, r=10, b=10, t=50, pad=5),
                )
                with plot_output:
                    plot_output.clear_output()
                    display(m.sankee_plot)

        width_slider.observe(width_changed, ""value"")

        height_slider = widgets.IntSlider(
            value=250,
            min=200,
            max=int(m.layout.height[:-2]) - 60,
            step=10,
            description="""",
            readout=False,
            continuous_update=False,
            layout=widgets.Layout(width=""100px"", padding=padding),
            style={""description_width"": ""initial""},
        )

        height_slider_label = widgets.Label()
        widgets.jslink((height_slider, ""value""), (height_slider_label, ""value""))

        def height_changed(change):
            if change[""new""]:

                m.sankee_plot.update_layout(
                    height=height_slider.value,
                    margin=dict(l=10, r=10, b=10, t=50, pad=5),
                )
                with plot_output:
                    plot_output.clear_output()
                    display(m.sankee_plot)

        height_slider.observe(height_changed, ""value"")

        plot_output = widgets.Output()

        plot_widget = widgets.VBox([plot_output])

        sankee_control = ipyleaflet.WidgetControl(
            widget=plot_widget, position=""bottomright""
        )
        m.add_control(sankee_control)
        m.sankee_control = sankee_control

    def handle_toolbar_event(event):

        if event[""type""] == ""mouseenter"":
            toolbar_widget.children = [toolbar_header, toolbar_footer]
        elif event[""type""] == ""mouseleave"":
            if not toolbar_button.value:
                toolbar_widget.children = [toolbar_button]
                toolbar_button.value = False
                close_button.value = False

    toolbar_event.on_dom_event(handle_toolbar_event)

    def toolbar_btn_click(change):
        if change[""new""]:
            close_button.value = False
            toolbar_widget.children = [toolbar_header, toolbar_footer]
        else:
            if not close_button.value:
                toolbar_widget.children = [toolbar_button]

    toolbar_button.observe(toolbar_btn_click, ""value"")

    def close_btn_click(change):
        if change[""new""]:
            toolbar_button.value = False
            if m is not None:
                m.toolbar_reset()
                if m.tool_control is not None and m.tool_control in m.controls:
                    m.remove_control(m.tool_control)
                    m.tool_control = None
                if m.sankee_control is not None and m.sankee_control in m.controls:
                    m.remove_control(m.sankee_control)
                    m.sankee_control = None
            toolbar_widget.close()

    close_button.observe(close_btn_click, ""value"")

    def button_clicked(change):
        if change[""new""] == ""Apply"":
            with output:
                output.clear_output()
                plot_output.clear_output()
                print(""Running ..."")

            if m is not None:
                exclude_classes = []

                if ""NLCD"" in dataset.value:
                    before_img = ee.Image(f""USGS/NLCD/NLCD{before.value}"")
                    after_img = ee.Image(f""USGS/NLCD/NLCD{after.value}"")
                    vis_params = {}
                elif ""MODIS"" in dataset.value:
                    before_img = ee.Image(f""MODIS/006/MCD12Q1/{before.value}_01_01"")
                    after_img = ee.Image(f""MODIS/006/MCD12Q1/{after.value}_01_01"")
                    vis_params = {
                        ""min"": 1.0,
                        ""max"": 17.0,
                        ""palette"": [
                            ""05450a"",
                            ""086a10"",
                            ""54a708"",
                            ""78d203"",
                            ""009900"",
                            ""c6b044"",
                            ""dcd159"",
                            ""dade48"",
                            ""fbff13"",
                            ""b6ff05"",
                            ""27ff87"",
                            ""c24f44"",
                            ""a5a5a5"",
                            ""ff6d4c"",
                            ""69fff8"",
                            ""f9ffa4"",
                            ""1c0dff"",
                        ],
                    }
                elif ""CGLS"" in dataset.value:
                    before_img = ee.Image(
                        f""COPERNICUS/Landcover/100m/Proba-V-C3/Global/{before.value}""
                    )
                    after_img = ee.Image(
                        f""COPERNICUS/Landcover/100m/Proba-V-C3/Global/{after.value}""
                    )
                    vis_params = {}
                elif ""LCMS"" in dataset.value:
                    before_img = ee.Image(
                        f""USFS/GTAC/LCMS/v2020-5/LCMS_CONUS_v2020-5_{before.value}""
                    )
                    after_img = ee.Image(
                        f""USFS/GTAC/LCMS/v2020-5/LCMS_CONUS_v2020-5_{after.value}""
                    )
                    vis_params = {}
                    # LCMS Land Cover class 15 is a no data mask and should be excluded
                    exclude_classes.append(15)

                img_list = [before_img, after_img]
                label_list = [before.value, after.value]

                image1 = before_img.select(band_name[dataset.value])
                image2 = after_img.select(band_name[dataset.value])

                if region.value != ""User-drawn ROI"" or (
                    region.value == ""User-drawn ROI"" and m.user_roi is not None
                ):

                    if region.value == ""User-drawn ROI"":
                        geom = m.user_roi
                        image1 = image1.clip(geom)
                        image2 = image2.clip(geom)
                    else:
                        roi_object = m.ee_layer_dict[region.value][""ee_object""]
                        if region.value == ""Las Vegas"":
                            m.centerObject(roi_object, 10)
                        if isinstance(roi_object, ee.Geometry):
                            geom = roi_object
                            image1 = image1.clip(geom)
                            image2 = image2.clip(geom)
                        else:
                            roi_object = ee.FeatureCollection(roi_object)
                            image1 = image1.clipToCollection(roi_object)
                            image2 = image2.clipToCollection(roi_object)
                            geom = roi_object.geometry()

                    if len(title.value) > 0:
                        plot_title = title.value
                    else:
                        plot_title = None
                    m.default_style = {""cursor"": ""wait""}
                    plot = sankee.sankify(
                        img_list,
                        geom,
                        label_list,
                        dataset_template[dataset.value],
                        max_classes=classes.value,
                        n=int(samples.value),
                        title=plot_title,
                        exclude=exclude_classes,
                    )

                    output.clear_output()
                    plot_output.clear_output()
                    with plot_output:
                        plot.update_layout(
                            width=600,
                            height=250,
                            margin=dict(l=10, r=10, b=10, t=50, pad=5),
                        )
                        plot_widget.children = [
                            widgets.HBox(
                                [
                                    plot_close_btn,
                                    plot_reset_btn,
                                    plot_fullscreen_btn,
                                    width_btn,
                                    width_slider,
                                    width_slider_label,
                                    height_btn,
                                    height_slider,
                                    height_slider_label,
                                ]
                            ),
                            plot_output,
                        ]
                        display(plot)

                    m.sankee_plot = plot
                    m.addLayer(image1, vis_params, before.value)
                    m.addLayer(image2, vis_params, after.value)
                    m.default_style = {""cursor"": ""default""}

                else:
                    with output:
                        output.clear_output()
                        print(""Draw a polygon on the map."")

        elif change[""new""] == ""Reset"":
            output.clear_output()
            plot_output.clear_output()
            plot_widget.children = []

        elif change[""new""] == ""Close"":
            if m is not None:
                m.toolbar_reset()
                if m.tool_control is not None and m.tool_control in m.controls:
                    m.remove_control(m.tool_control)
                    m.tool_control = None
                if m.sankee_control is not None and m.sankee_control in m.controls:
                    m.remove_control(m.sankee_control)
                    m.sankee_control = None
            toolbar_widget.close()

        buttons.value = None

    buttons.observe(button_clicked, ""value"")

    toolbar_button.value = True
    if m is not None:
        toolbar_control = ipyleaflet.WidgetControl(
            widget=toolbar_widget, position=""topright""
        )

        if toolbar_control not in m.controls:
            m.add_control(toolbar_control)
            m.tool_control = toolbar_control
    else:
        return toolbar_widget",m.tool_control is not None and m.tool_control in m.controls,None is not m.tool_control in m.controls
geemap,https://github.com/giswqs/geemap/tree/master/geemap/toolbar.py,,sankee_gui$2606,"def sankee_gui(m=None):

    import sankee

    widget_width = ""250px""
    padding = ""0px 0px 0px 5px""  # upper, right, bottom, left

    toolbar_button = widgets.ToggleButton(
        value=False,
        tooltip=""Toolbar"",
        icon=""random"",
        layout=widgets.Layout(width=""28px"", height=""28px"", padding=""0px 0px 0px 4px""),
    )

    close_button = widgets.ToggleButton(
        value=False,
        tooltip=""Close the tool"",
        icon=""times"",
        button_style=""primary"",
        layout=widgets.Layout(height=""28px"", width=""28px"", padding=""0px 0px 0px 4px""),
    )

    region = widgets.Dropdown(
        options=[""User-drawn ROI""],
        value=""User-drawn ROI"",
        description=""Region:"",
        layout=widgets.Layout(width=widget_width, padding=padding),
        style={""description_width"": ""initial""},
    )

    def region_changed(change):
        if change[""new""] == ""Las Vegas"":
            if m is not None:
                las_vegas = ee.Geometry.Polygon(
                    [
                        [
                            [-115.01184401606046, 36.24170785506492],
                            [-114.98849806879484, 36.29928186470082],
                            [-115.25628981684171, 36.35238941394592],
                            [-115.34692702387296, 36.310348922031565],
                            [-115.37988600824796, 36.160811202271944],
                            [-115.30298171137296, 36.03653336474891],
                            [-115.25628981684171, 36.05207884201088],
                            [-115.26590285395109, 36.226199908103695],
                            [-115.19174513910734, 36.25499793268206],
                        ]
                    ]
                )
                m.addLayer(las_vegas, {}, ""Las Vegas"")
                m.centerObject(las_vegas, 10)

    region.observe(region_changed, ""value"")

    dataset = widgets.Dropdown(
        options=[
            ""NLCD - National Land Cover Database"",
            ""MCD12Q1 - MODIS Global Land Cover"",
            ""CGLS - Copernicus Global Land Cover"",
            ""LCMS - Land Change Monitoring System"",
        ],
        value=""NLCD - National Land Cover Database"",
        description=""Dataset:"",
        layout=widgets.Layout(width=widget_width, padding=padding),
        style={""description_width"": ""initial""},
    )

    NLCD_options = [""2001"", ""2004"", ""2006"", ""2008"", ""2011"", ""2013"", ""2016""]
    MODIS_options = [str(y) for y in range(2001, 2020)]
    CGLS_options = [str(y) for y in range(2015, 2020)]
    LCMS_options = [str(y) for y in range(1985, 2021)]

    before = widgets.Dropdown(
        options=NLCD_options,
        value=""2001"",
        description=""Before:"",
        layout=widgets.Layout(width=""123px"", padding=padding),
        style={""description_width"": ""initial""},
    )

    after = widgets.Dropdown(
        options=NLCD_options,
        value=""2016"",
        description=""After:"",
        layout=widgets.Layout(width=""123px"", padding=padding),
        style={""description_width"": ""initial""},
    )

    def dataset_changed(change):
        if change[""new""] == ""NLCD - National Land Cover Database"":
            before.options = NLCD_options
            after.options = NLCD_options
            before.value = NLCD_options[0]
            after.value = NLCD_options[-1]
        elif change[""new""] == ""MCD12Q1 - MODIS Global Land Cover"":
            before.options = MODIS_options
            after.options = MODIS_options
            before.value = MODIS_options[0]
            after.value = MODIS_options[-1]
        elif change[""new""] == ""CGLS - Copernicus Global Land Cover"":
            before.options = CGLS_options
            after.options = CGLS_options
            before.value = CGLS_options[0]
            after.value = CGLS_options[-1]
        elif change[""new""] == ""LCMS - Land Change Monitoring System"":
            before.options = LCMS_options
            after.options = LCMS_options
            before.value = LCMS_options[0]
            after.value = LCMS_options[-1]

    dataset.observe(dataset_changed, ""value"")

    dataset_template = {
        ""NLCD - National Land Cover Database"": sankee.datasets.NLCD2016,
        ""MCD12Q1 - MODIS Global Land Cover"": sankee.datasets.MODIS_LC_TYPE1,
        ""CGLS - Copernicus Global Land Cover"": sankee.datasets.CGLS_LC100,
        ""LCMS - Land Change Monitoring System"": sankee.datasets.LCMS_LC,
    }

    band_name = {
        ""NLCD - National Land Cover Database"": ""landcover"",
        ""MCD12Q1 - MODIS Global Land Cover"": ""LC_Type1"",
        ""CGLS - Copernicus Global Land Cover"": ""discrete_classification"",
        ""LCMS - Land Change Monitoring System"": ""Land_Cover"",
    }

    samples = widgets.IntText(
        value=1000,
        description=""Samples:"",
        placeholder=""The number of samples points to randomly generate for characterizing all images"",
        style={""description_width"": ""initial""},
        layout=widgets.Layout(width=""133px"", padding=padding),
    )

    classes = widgets.IntText(
        value=6,
        description=""Classes:"",
        style={""description_width"": ""initial""},
        layout=widgets.Layout(width=""113px"", padding=padding),
    )

    title = widgets.Text(
        value=""Land Cover Change"",
        description=""Title:"",
        style={""description_width"": ""initial""},
        layout=widgets.Layout(width=widget_width, padding=padding),
    )

    buttons = widgets.ToggleButtons(
        value=None,
        options=[""Apply"", ""Reset"", ""Close""],
        tooltips=[""Apply"", ""Reset"", ""Close""],
        button_style=""primary"",
    )
    buttons.style.button_width = ""80px""

    output = widgets.Output(layout=widgets.Layout(padding=padding))

    toolbar_widget = widgets.VBox()
    toolbar_widget.children = [toolbar_button]
    toolbar_header = widgets.HBox()
    toolbar_header.children = [close_button, toolbar_button]
    toolbar_footer = widgets.VBox()
    toolbar_footer.children = [
        region,
        dataset,
        widgets.HBox([before, after]),
        widgets.HBox([samples, classes]),
        title,
        buttons,
        output,
    ]

    toolbar_event = ipyevents.Event(
        source=toolbar_widget, watched_events=[""mouseenter"", ""mouseleave""]
    )

    if m is not None:
        if ""Las Vegas"" not in m.ee_vector_layer_names:
            region.options = [""User-drawn ROI"", ""Las Vegas""] + m.ee_vector_layer_names
        else:
            region.options = [""User-drawn ROI""] + m.ee_vector_layer_names

        plot_close_btn = widgets.Button(
            tooltip=""Close the plot"",
            icon=""times"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def plot_close_btn_clicked(b):
            plot_widget.children = []

        plot_close_btn.on_click(plot_close_btn_clicked)

        plot_reset_btn = widgets.Button(
            tooltip=""Reset the plot"",
            icon=""home"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def plot_reset_btn_clicked(b):

            m.sankee_plot.update_layout(
                width=600,
                height=250,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        plot_reset_btn.on_click(plot_reset_btn_clicked)

        plot_fullscreen_btn = widgets.Button(
            tooltip=""Fullscreen the plot"",
            icon=""arrows-alt"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def plot_fullscreen_btn_clicked(b):

            m.sankee_plot.update_layout(
                width=1030,
                height=int(m.layout.height[:-2]) - 60,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        plot_fullscreen_btn.on_click(plot_fullscreen_btn_clicked)

        width_btn = widgets.Button(
            tooltip=""Change plot width"",
            icon=""arrows-h"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def width_btn_clicked(b):
            m.sankee_plot.update_layout(
                width=1030,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        width_btn.on_click(width_btn_clicked)

        height_btn = widgets.Button(
            tooltip=""Change plot height"",
            icon=""arrows-v"",
            layout=widgets.Layout(
                height=""28px"", width=""28px"", padding=""0px 0px 0px 0px""
            ),
        )

        def height_btn_clicked(b):
            m.sankee_plot.update_layout(
                height=int(m.layout.height[:-2]) - 60,
                margin=dict(l=10, r=10, b=10, t=50, pad=5),
            )
            with plot_output:
                plot_output.clear_output()
                display(m.sankee_plot)

        height_btn.on_click(height_btn_clicked)

        width_slider = widgets.IntSlider(
            value=600,
            min=400,
            max=1030,
            step=10,
            description="""",
            readout=False,
            continuous_update=False,
            layout=widgets.Layout(width=""100px"", padding=padding),
            style={""description_width"": ""initial""},
        )

        width_slider_label = widgets.Label(
            layout=widgets.Layout(padding=""0px 10px 0px 0px"")
        )
        widgets.jslink((width_slider, ""value""), (width_slider_label, ""value""))

        def width_changed(change):
            if change[""new""]:

                m.sankee_plot.update_layout(
                    width=width_slider.value,
                    margin=dict(l=10, r=10, b=10, t=50, pad=5),
                )
                with plot_output:
                    plot_output.clear_output()
                    display(m.sankee_plot)

        width_slider.observe(width_changed, ""value"")

        height_slider = widgets.IntSlider(
            value=250,
            min=200,
            max=int(m.layout.height[:-2]) - 60,
            step=10,
            description="""",
            readout=False,
            continuous_update=False,
            layout=widgets.Layout(width=""100px"", padding=padding),
            style={""description_width"": ""initial""},
        )

        height_slider_label = widgets.Label()
        widgets.jslink((height_slider, ""value""), (height_slider_label, ""value""))

        def height_changed(change):
            if change[""new""]:

                m.sankee_plot.update_layout(
                    height=height_slider.value,
                    margin=dict(l=10, r=10, b=10, t=50, pad=5),
                )
                with plot_output:
                    plot_output.clear_output()
                    display(m.sankee_plot)

        height_slider.observe(height_changed, ""value"")

        plot_output = widgets.Output()

        plot_widget = widgets.VBox([plot_output])

        sankee_control = ipyleaflet.WidgetControl(
            widget=plot_widget, position=""bottomright""
        )
        m.add_control(sankee_control)
        m.sankee_control = sankee_control

    def handle_toolbar_event(event):

        if event[""type""] == ""mouseenter"":
            toolbar_widget.children = [toolbar_header, toolbar_footer]
        elif event[""type""] == ""mouseleave"":
            if not toolbar_button.value:
                toolbar_widget.children = [toolbar_button]
                toolbar_button.value = False
                close_button.value = False

    toolbar_event.on_dom_event(handle_toolbar_event)

    def toolbar_btn_click(change):
        if change[""new""]:
            close_button.value = False
            toolbar_widget.children = [toolbar_header, toolbar_footer]
        else:
            if not close_button.value:
                toolbar_widget.children = [toolbar_button]

    toolbar_button.observe(toolbar_btn_click, ""value"")

    def close_btn_click(change):
        if change[""new""]:
            toolbar_button.value = False
            if m is not None:
                m.toolbar_reset()
                if m.tool_control is not None and m.tool_control in m.controls:
                    m.remove_control(m.tool_control)
                    m.tool_control = None
                if m.sankee_control is not None and m.sankee_control in m.controls:
                    m.remove_control(m.sankee_control)
                    m.sankee_control = None
            toolbar_widget.close()

    close_button.observe(close_btn_click, ""value"")

    def button_clicked(change):
        if change[""new""] == ""Apply"":
            with output:
                output.clear_output()
                plot_output.clear_output()
                print(""Running ..."")

            if m is not None:
                exclude_classes = []

                if ""NLCD"" in dataset.value:
                    before_img = ee.Image(f""USGS/NLCD/NLCD{before.value}"")
                    after_img = ee.Image(f""USGS/NLCD/NLCD{after.value}"")
                    vis_params = {}
                elif ""MODIS"" in dataset.value:
                    before_img = ee.Image(f""MODIS/006/MCD12Q1/{before.value}_01_01"")
                    after_img = ee.Image(f""MODIS/006/MCD12Q1/{after.value}_01_01"")
                    vis_params = {
                        ""min"": 1.0,
                        ""max"": 17.0,
                        ""palette"": [
                            ""05450a"",
                            ""086a10"",
                            ""54a708"",
                            ""78d203"",
                            ""009900"",
                            ""c6b044"",
                            ""dcd159"",
                            ""dade48"",
                            ""fbff13"",
                            ""b6ff05"",
                            ""27ff87"",
                            ""c24f44"",
                            ""a5a5a5"",
                            ""ff6d4c"",
                            ""69fff8"",
                            ""f9ffa4"",
                            ""1c0dff"",
                        ],
                    }
                elif ""CGLS"" in dataset.value:
                    before_img = ee.Image(
                        f""COPERNICUS/Landcover/100m/Proba-V-C3/Global/{before.value}""
                    )
                    after_img = ee.Image(
                        f""COPERNICUS/Landcover/100m/Proba-V-C3/Global/{after.value}""
                    )
                    vis_params = {}
                elif ""LCMS"" in dataset.value:
                    before_img = ee.Image(
                        f""USFS/GTAC/LCMS/v2020-5/LCMS_CONUS_v2020-5_{before.value}""
                    )
                    after_img = ee.Image(
                        f""USFS/GTAC/LCMS/v2020-5/LCMS_CONUS_v2020-5_{after.value}""
                    )
                    vis_params = {}
                    # LCMS Land Cover class 15 is a no data mask and should be excluded
                    exclude_classes.append(15)

                img_list = [before_img, after_img]
                label_list = [before.value, after.value]

                image1 = before_img.select(band_name[dataset.value])
                image2 = after_img.select(band_name[dataset.value])

                if region.value != ""User-drawn ROI"" or (
                    region.value == ""User-drawn ROI"" and m.user_roi is not None
                ):

                    if region.value == ""User-drawn ROI"":
                        geom = m.user_roi
                        image1 = image1.clip(geom)
                        image2 = image2.clip(geom)
                    else:
                        roi_object = m.ee_layer_dict[region.value][""ee_object""]
                        if region.value == ""Las Vegas"":
                            m.centerObject(roi_object, 10)
                        if isinstance(roi_object, ee.Geometry):
                            geom = roi_object
                            image1 = image1.clip(geom)
                            image2 = image2.clip(geom)
                        else:
                            roi_object = ee.FeatureCollection(roi_object)
                            image1 = image1.clipToCollection(roi_object)
                            image2 = image2.clipToCollection(roi_object)
                            geom = roi_object.geometry()

                    if len(title.value) > 0:
                        plot_title = title.value
                    else:
                        plot_title = None
                    m.default_style = {""cursor"": ""wait""}
                    plot = sankee.sankify(
                        img_list,
                        geom,
                        label_list,
                        dataset_template[dataset.value],
                        max_classes=classes.value,
                        n=int(samples.value),
                        title=plot_title,
                        exclude=exclude_classes,
                    )

                    output.clear_output()
                    plot_output.clear_output()
                    with plot_output:
                        plot.update_layout(
                            width=600,
                            height=250,
                            margin=dict(l=10, r=10, b=10, t=50, pad=5),
                        )
                        plot_widget.children = [
                            widgets.HBox(
                                [
                                    plot_close_btn,
                                    plot_reset_btn,
                                    plot_fullscreen_btn,
                                    width_btn,
                                    width_slider,
                                    width_slider_label,
                                    height_btn,
                                    height_slider,
                                    height_slider_label,
                                ]
                            ),
                            plot_output,
                        ]
                        display(plot)

                    m.sankee_plot = plot
                    m.addLayer(image1, vis_params, before.value)
                    m.addLayer(image2, vis_params, after.value)
                    m.default_style = {""cursor"": ""default""}

                else:
                    with output:
                        output.clear_output()
                        print(""Draw a polygon on the map."")

        elif change[""new""] == ""Reset"":
            output.clear_output()
            plot_output.clear_output()
            plot_widget.children = []

        elif change[""new""] == ""Close"":
            if m is not None:
                m.toolbar_reset()
                if m.tool_control is not None and m.tool_control in m.controls:
                    m.remove_control(m.tool_control)
                    m.tool_control = None
                if m.sankee_control is not None and m.sankee_control in m.controls:
                    m.remove_control(m.sankee_control)
                    m.sankee_control = None
            toolbar_widget.close()

        buttons.value = None

    buttons.observe(button_clicked, ""value"")

    toolbar_button.value = True
    if m is not None:
        toolbar_control = ipyleaflet.WidgetControl(
            widget=toolbar_widget, position=""topright""
        )

        if toolbar_control not in m.controls:
            m.add_control(toolbar_control)
            m.tool_control = toolbar_control
    else:
        return toolbar_widget",m.sankee_control is not None and m.sankee_control in m.controls,None is not m.sankee_control in m.controls
PyGaze,https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/libeyelogic.py,EyeLogicTracker,calibrate$391,"def calibrate(self):
        #self.screen.clear()
        #self.screen.draw_text(
        #    text=""Calibrate EyeTracker"",
        #    fontsize=20)
        #self.disp.fill(self.screen)
        #self.disp.show()

        if (not self._recording.is_set()):
            resultTracking = self.api.requestTracking(0)
            if (resultTracking != ELApi.ReturnStart.SUCCESS):
                raise Exception(""unable to start eye tracker"")

        resultCalibrate = self.api.calibrate(0)
        if (resultCalibrate != ELApi.ReturnCalibrate.SUCCESS):
            self.api.unrequestTracking()
            self.errorbeep.play()
            raise Exception(""Calibration failed = {}"".format(errorstringCalibrate(resultCalibrate)))
        self._calibrated.set()

        # NOISE CALIBRATION
        self.screen.clear()
        self.screen.draw_text(
            text=""Noise calibration. Please look at the dot, and press any key to start."",
            fontsize=20, \
            pos=(int(self.dispsize[0]/2),int(self.dispsize[1]*0.3)))
        x = int(float(self.dispsize[0]) / 2.0)
        y = int(float(self.dispsize[1]) / 2.0)
        self.screen.draw_fixation(fixtype=""dot"", pos=(x,y))
        self.disp.fill(self.screen)
        self.disp.show()
        self.kb.get_key(keylist=None, timeout=None, flush=True)

        # wait for a bit, to allow participant to fixate
        clock.pause(500)

        # get distance to screen
        screendist = 0
        i = 0
        while screendist == 0 and i < self.maxtries:
            i = i+1
            self.sampleLock.acquire()
            if (self.lastSample is not None):
                if self.eye_used != 1 and self.lastSample.eyePositionLeftZ != ELInvalidValue:
                    screendist = self.lastSample.eyePositionLeftZ / 10.0 # eyePositionZ is in mm; screendist is in cm
                elif self.eye_used != 0 and self.lastSample.eyePositionRightZ != ELInvalidValue:
                    screendist = self.lastSample.eyePositionRightZ / 10.0
            self.sampleLock.release()
            clock.pause(int(self.sampleTime))
        if i >= self.maxtries:
            self.api.unrequestTracking()
            self.errorbeep.play()
            raise Exception(""unable to receive gaze data for noise calibration"")

        # get samples
        sl = [self.sample()] # samplelist, prefilled with 1 sample to prevent sl[-1] from producing an error; first sample will be ignored for RMS calculation
        t0 = clock.get_time() # starting time
        while clock.get_time() - t0 < 1000:
            s = self.sample() # sample
            if s[0] != -1 and s[1] != -1 and s[0] != ELInvalidValue and s[1] != ELInvalidValue:
                sl.append(s)
            clock.pause(int(self.sampleTime))
        if (len(sl) < 2):
            if (not self._recording.is_set()):
                self.api.unrequestTracking()
            return False

        # calculate RMS noise
        Xvar = []
        Yvar = []
        Xmean = 0.
        Ymean = 0.
        for i in range(2,len(sl)):
            Xvar.append((sl[i][0]-sl[i-1][0])**2)
            Yvar.append((sl[i][1]-sl[i-1][1])**2)
            Xmean += sl[i][0]
            Ymean += sl[i][1]
        XRMS = (sum(Xvar) / len(Xvar))**0.5
        YRMS = (sum(Yvar) / len(Yvar))**0.5
        Xmean = Xmean / (len(sl)-2)
        Ymean = Ymean / (len(sl)-2)
        self.pxdsttresh = (XRMS, YRMS)

        # calculate pixels per cm
        pixpercm = (self.dispsize[0]/float(self.screensize[0]) + self.dispsize[1]/float(self.screensize[1])) / 2

        # get accuracy
        accuracyPxX = abs( Xmean - x )
        accuracyPxY = abs( Ymean - y )
        self.accuracy = ( pix2deg(screendist, accuracyPxX, pixpercm), \
                          pix2deg(screendist, accuracyPxY, pixpercm) )

        # calculate thresholds based on tracker settings
        self.pxfixtresh = deg2pix(screendist, self.fixtresh, pixpercm)
        self.pxaccuracy = (accuracyPxX, accuracyPxY )
        self.pxspdtresh = deg2pix(screendist, self.spdtresh/1000.0, pixpercm) # in pixels per millisecond
        self.pxacctresh = deg2pix(screendist, self.accthresh/1000.0, pixpercm) # in pixels per millisecond**2

        ## log
        self.log(""pygaze calibration"")
        self.log(""accuracy (degrees) = X={}, Y={}"".format( \
            self.accuracy[0], self.accuracy[1] ))
        self.log(""accuracy (in pixels) = X={}, Y={}"".format( \
            self.pxaccuracy[0], self.pxaccuracy[1]))
        self.log(""precision (RMS noise in pixels) = X={}, Y={}"".format( \
            self.pxdsttresh[0], self.pxdsttresh[1]))
        self.log(""distance between participant and display = {} cm"".format(screendist))
        self.log(""fixation threshold = {} pixels"".format(self.pxfixtresh))
        self.log(""speed threshold = {} pixels/ms"".format(self.pxspdtresh))
        self.log(""acceleration threshold = {} pixels/ms**2"".format(self.pxacctresh))
        
        if (not self._recording.is_set()):
            self.api.unrequestTracking()
        return True",s[0] != -1 and s[1] != -1 and (s[0] != ELInvalidValue) and (s[1] != ELInvalidValue),s[1] != -1 != s[0] != ELInvalidValue != s[1]
DPT,https://github.com/isl-org/DPT/tree/master/util/io.py,,write_pfm$63,"def write_pfm(path, image, scale=1):
    """"""Write pfm file.

    Args:
        path (str): pathto file
        image (array): data
        scale (int, optional): Scale. Defaults to 1.
    """"""

    with open(path, ""wb"") as file:
        color = None

        if image.dtype.name != ""float32"":
            raise Exception(""Image dtype must be float32."")

        image = np.flipud(image)

        if len(image.shape) == 3 and image.shape[2] == 3:  # color image
            color = True
        elif (
            len(image.shape) == 2 or len(image.shape) == 3 and image.shape[2] == 1
        ):  # greyscale
            color = False
        else:
            raise Exception(""Image must have H x W x 3, H x W x 1 or H x W dimensions."")

        file.write(""PF\n"" if color else ""Pf\n"".encode())
        file.write(""%d %d\n"".encode() % (image.shape[1], image.shape[0]))

        endian = image.dtype.byteorder

        if endian == ""<"" or endian == ""="" and sys.byteorder == ""little"":
            scale = -scale

        file.write(""%f\n"".encode() % scale)

        image.tofile(file)",len(image.shape) == 3 and image.shape[2] == 3,len(image.shape) == 3 == image.shape[2]
hypothesis,https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/src/hypothesis/strategies/_internal/numbers.py,IntegersStrategy,filter$96,"def filter(self, condition):
        kwargs, pred = get_integer_predicate_bounds(condition)

        start, end = self.start, self.end
        if ""min_value"" in kwargs:
            start = max(kwargs[""min_value""], -math.inf if start is None else start)
        if ""max_value"" in kwargs:
            end = min(kwargs[""max_value""], math.inf if end is None else end)

        if start != self.start or end != self.end:
            if start is not None and end is not None and start > end:
                return nothing()
            self = type(self)(start, end)
        if pred is None:
            return self
        return super().filter(pred)",start is not None and end is not None and (start > end),None is not start is not end > end
napari,https://github.com/napari/napari/tree/master/napari/utils/events/containers/_typed.py,TypedMutableSequence,index$192,"def index(self, value: _L, start: int = 0, stop: int = None) -> int:
        """"""Return first index of value.

        Parameters
        ----------
        value : Any
            A value to lookup.  If `type(value)` is in the lookups functions
            provided for this class, then values in the list will be searched
            using the corresponding lookup converter function.
        start : int, optional
            The starting index to search, by default 0
        stop : int, optional
            The ending index to search, by default None

        Returns
        -------
        int
            The index of the value

        Raises
        ------
        ValueError
            If the value is not present
        """"""
        if start is not None and start < 0:
            start = max(len(self) + start, 0)
        if stop is not None and stop < 0:
            stop += len(self)

        convert = self._lookup.get(type(value), _noop)

        for i in self._iter_indices(start, stop):
            v = convert(self[i])
            if v is value or v == value:
                return i

        raise ValueError(
            trans._(
                ""{value!r} is not in list"",
                deferred=True,
                value=value,
            )
        )",start is not None and start < 0,None is not start < 0
napari,https://github.com/napari/napari/tree/master/napari/utils/events/containers/_typed.py,TypedMutableSequence,index$192,"def index(self, value: _L, start: int = 0, stop: int = None) -> int:
        """"""Return first index of value.

        Parameters
        ----------
        value : Any
            A value to lookup.  If `type(value)` is in the lookups functions
            provided for this class, then values in the list will be searched
            using the corresponding lookup converter function.
        start : int, optional
            The starting index to search, by default 0
        stop : int, optional
            The ending index to search, by default None

        Returns
        -------
        int
            The index of the value

        Raises
        ------
        ValueError
            If the value is not present
        """"""
        if start is not None and start < 0:
            start = max(len(self) + start, 0)
        if stop is not None and stop < 0:
            stop += len(self)

        convert = self._lookup.get(type(value), _noop)

        for i in self._iter_indices(start, stop):
            v = convert(self[i])
            if v is value or v == value:
                return i

        raise ValueError(
            trans._(
                ""{value!r} is not in list"",
                deferred=True,
                value=value,
            )
        )",stop is not None and stop < 0,None is not stop < 0
airflow,https://github.com/apache/airflow/tree/master/airflow/models/dag.py,DAG,create_dagrun$2272,"def create_dagrun(
        self,
        state: DagRunState,
        execution_date: Optional[datetime] = None,
        run_id: Optional[str] = None,
        start_date: Optional[datetime] = None,
        external_trigger: Optional[bool] = False,
        conf: Optional[dict] = None,
        run_type: Optional[DagRunType] = None,
        session=None,
        dag_hash: Optional[str] = None,
        creating_job_id: Optional[int] = None,
        data_interval: Optional[Tuple[datetime, datetime]] = None,
    ):
        """"""
        Creates a dag run from this dag including the tasks associated with this dag.
        Returns the dag run.

        :param run_id: defines the run id for this dag run
        :type run_id: str
        :param run_type: type of DagRun
        :type run_type: airflow.utils.types.DagRunType
        :param execution_date: the execution date of this dag run
        :type execution_date: datetime.datetime
        :param state: the state of the dag run
        :type state: airflow.utils.state.DagRunState
        :param start_date: the date this dag run should be evaluated
        :type start_date: datetime
        :param external_trigger: whether this dag run is externally triggered
        :type external_trigger: bool
        :param conf: Dict containing configuration/parameters to pass to the DAG
        :type conf: dict
        :param creating_job_id: id of the job creating this DagRun
        :type creating_job_id: int
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dag_hash: Hash of Serialized DAG
        :type dag_hash: str
        :param data_interval: Data interval of the DagRun
        :type data_interval: tuple[datetime, datetime] | None
        """"""
        if run_id:  # Infer run_type from run_id if needed.
            if not isinstance(run_id, str):
                raise ValueError(f""`run_id` expected to be a str is {type(run_id)}"")
            if not run_type:
                run_type = DagRunType.from_run_id(run_id)
        elif run_type and execution_date is not None:  # Generate run_id from run_type and execution_date.
            if not isinstance(run_type, DagRunType):
                raise ValueError(f""`run_type` expected to be a DagRunType is {type(run_type)}"")
            run_id = DagRun.generate_run_id(run_type, execution_date)
        else:
            raise AirflowException(
                ""Creating DagRun needs either `run_id` or both `run_type` and `execution_date`""
            )

        logical_date = timezone.coerce_datetime(execution_date)
        if data_interval is None and logical_date is not None:
            warnings.warn(
                ""Calling `DAG.create_dagrun()` without an explicit data interval is deprecated"",
                DeprecationWarning,
                stacklevel=3,
            )
            if run_type == DagRunType.MANUAL:
                data_interval = self.timetable.infer_manual_data_interval(run_after=logical_date)
            else:
                data_interval = self.infer_automated_data_interval(logical_date)

        # create a copy of params before validating
        copied_params = copy.deepcopy(self.params)
        copied_params.update(conf or {})
        copied_params.validate()

        run = DagRun(
            dag_id=self.dag_id,
            run_id=run_id,
            execution_date=logical_date,
            start_date=start_date,
            external_trigger=external_trigger,
            conf=conf,
            state=state,
            run_type=run_type,
            dag_hash=dag_hash,
            creating_job_id=creating_job_id,
            data_interval=data_interval,
        )
        session.add(run)
        session.flush()

        run.dag = self

        # create the associated task instances
        # state is None at the moment of creation
        run.verify_integrity(session=session)

        return run",data_interval is None and logical_date is not None,data_interval is None is not logical_date
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/lxmert/modeling_tf_lxmert.py,TFLxmertForPreTraining,call$1290,"def call(
        self,
        input_ids=None,
        visual_feats=None,
        visual_pos=None,
        attention_mask=None,
        visual_attention_mask=None,
        token_type_ids=None,
        inputs_embeds=None,
        masked_lm_labels=None,
        obj_labels=None,
        matched_label=None,
        ans=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        training=False,
    ):
        r""""""
        masked_lm_labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,
            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the
            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`
        obj_labels: (`Dict[Str: Tuple[tf.Tensor, tf.Tensor]]`, *optional*, defaults to `None`):
            each key is named after each one of the visual losses and each element of the tuple is of the shape
            `(batch_size, num_features)` and `(batch_size, num_features, visual_feature_dim)` for each the label id and
            the label score respectively
        matched_label (`tf.Tensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the whether or not the text input matches the image (classification) loss. Input
            should be a sequence pair (see `input_ids` docstring) Indices should be in `[0, 1]`:

            - 0 indicates that the sentence does not match the image,
            - 1 indicates that the sentence does match the image.
        ans (`Torch.Tensor` of shape `(batch_size)`, *optional*, defaults to `None`):
            a one hot representation hof the correct answer *optional*

        Returns:
        """"""

        lxmert_output = self.lxmert(
            input_ids,
            visual_feats,
            visual_pos,
            attention_mask,
            visual_attention_mask,
            token_type_ids,
            inputs_embeds,
            output_attentions,
            output_hidden_states,
            return_dict,
            training,
        )

        lang_output, visual_output, pooled_output = (
            lxmert_output[0],
            lxmert_output[1],
            lxmert_output[2],
        )
        lang_prediction_scores, cross_relationship_score = self.cls(lang_output, pooled_output)
        if self.task_qa:
            answer_score = self.answer_head(pooled_output)
        else:
            answer_score = pooled_output[0][0]

        total_loss = (
            None
            if (masked_lm_labels is None and matched_label is None and obj_labels is None and ans is None)
            else tf.constant(0.0)
        )
        losses = ()
        if masked_lm_labels is not None and self.task_mask_lm:
            masked_lm_loss = self.loss_fcts[""ce""](
                tf.reshape(masked_lm_labels, [-1]),
                tf.reshape(lang_prediction_scores, [-1, self.config.vocab_size]),
            )
            total_loss += masked_lm_loss
            losses += (masked_lm_loss,)
        if matched_label is not None and self.task_matched:
            matched_loss = self.loss_fcts[""ce""](
                tf.reshape(matched_label, [-1]),
                tf.reshape(cross_relationship_score, [-1, 2]),
            )
            total_loss += matched_loss
            losses += (matched_loss,)
        if obj_labels is not None and self.task_obj_predict:
            total_visn_loss = 0.0
            visn_prediction_scores_dict = self.obj_predict_head(visual_output)
            for key, key_info in self.visual_losses.items():
                label, mask_conf = obj_labels[key]
                output_dim = key_info[""num""]
                loss_fct_name = key_info[""loss""]
                label_shape = key_info[""shape""]
                weight = self.visual_loss_normalizer
                visn_loss_fct = self.loss_fcts[loss_fct_name]
                visn_prediction_scores = visn_prediction_scores_dict[key]
                visn_loss = visn_loss_fct(
                    tf.reshape(label, label_shape),
                    tf.reshape(visn_prediction_scores, [-1, output_dim]),
                )

                if visn_loss.ndim > 1:  # Regression Losses
                    visn_loss = tf.reduce_mean(visn_loss)
                visn_loss = tf.reduce_mean(visn_loss * tf.cast(tf.reshape(mask_conf, [-1]), visn_loss.dtype)) * weight
                total_visn_loss += visn_loss
                losses += (visn_loss,)
            total_loss += total_visn_loss
        if ans is not None and self.task_qa:
            answer_loss = self.loss_fcts[""ce""](
                tf.reshape(ans, [-1]), tf.reshape(answer_score, [-1, self.num_qa_labels])
            )
            # exclude ""*2"" here to match the effect of QA losses.
            # Previous: (loss *0) for 6 epochs, (loss *2) for 6 epochs.   (Used 10 instead of 6 in EMNLP paper)
            # Now     : (loss *1) for 12 epochs
            #
            # * 2       # Multiply by 2 because > half of the data will not have label
            total_loss += answer_loss
            losses += (answer_loss,)
        # return total_loss, tf.stack(losses)[tf.new_axis, ...], answer_score.detach()

        if not return_dict:
            output = (
                lang_prediction_scores,
                cross_relationship_score,
                answer_score,
            ) + lxmert_output[3:]
            return ((total_loss,) + output) if total_loss is not None else output

        return TFLxmertForPreTrainingOutput(
            loss=total_loss,
            prediction_logits=lang_prediction_scores,
            cross_relationship_score=cross_relationship_score,
            question_answering_score=answer_score,
            language_hidden_states=lxmert_output.language_hidden_states,
            vision_hidden_states=lxmert_output.vision_hidden_states,
            language_attentions=lxmert_output.language_attentions,
            vision_attentions=lxmert_output.vision_attentions,
            cross_encoder_attentions=lxmert_output.cross_encoder_attentions,
        )",masked_lm_labels is None and matched_label is None and (obj_labels is None) and (ans is None),masked_lm_labels is None is matched_label and obj_labels is None is ans
tf-coreml,https://github.com/tf-coreml/tf-coreml/tree/master/tfcoreml/_shape_sensitive_layers.py,,_add_reduce$316,"def _add_reduce(op, context, mode):

  input_name = compat.as_str_any(op.inputs[0].name)
  output_name = compat.as_str_any(op.outputs[0].name)
  if op.inputs[1].name in context.consts:
    axis_ind = context.consts[op.inputs[1].name]
  else:
    axis_ind = context.session(op.inputs[1].name, feed_dict=context.input_feed_dict)

  input_shape = context.shape_dict[input_name]
  output_shape = context.shape_dict[output_name]

  # skip if output shape is same as input shape: in that case its a dummy operation
  if input_shape == output_shape:
    skip(op, context)
    return

  if context.use_dfs_shape_infer:
    status = interpret_shape(input_name, context)
  else:
    status = False

  # convert axis_ind into a list
  if axis_ind is None:
    axis = 'CHW'
    context.builder.add_reduce(output_name, input_name, output_name, axis, mode)
    context.translated[output_name] = True
    return
  elif isinstance(axis_ind, int) or isinstance(axis_ind, np.int32) or isinstance(axis_ind, np.int):
    axis_ind = (len(input_shape) + axis_ind) if axis_ind < 0 else axis_ind
    axis_ind = [axis_ind]
  elif isinstance(axis_ind, np.ndarray):
    axis_ind = axis_ind.tolist()

  # Determine reduction axis labels
  axis = ''

  if status:
    labeled_shape = context.dim_labels[input_name]
    for i in axis_ind:
      if input_shape[i] != 1:
        axis += labeled_shape[i]
    axis = ''.join(sorted(axis))
  else:
    # check for all cases: len(input_shape) = 1,2,3,4
    if len(input_shape) == len(axis_ind):
      axis = 'CHW'
    elif len(input_shape) == 1:
      axis = 'C'
    elif len(input_shape) == 2:
      if len(axis_ind) == 1 and axis_ind[0] == 1: axis = 'C'
      if len(axis_ind) == 1 and axis_ind[0] == 0: axis = 'W' # TODO - Handle it more robustly. Only works for stylenet. (W,C)--->(1,C)
    elif len(input_shape) == 3:
      for ind in [['H', 'W', 'C'][i] for i in axis_ind]:
        axis += ind
      axis = ''.join(sorted(axis))
    elif len(input_shape) == 4:
      for ind in [['B', 'H', 'W', 'C'][i] for i in axis_ind]:
        axis += ind
      axis = ''.join(sorted(axis))
      if len(axis) > 1 and axis[0] == 'B':
        axis = axis[1:]

  if len(axis) == 0:
    raise NotImplementedError(
        'Reduce axis %s for input shape %s, output shape %s,  not handled currently' %(str(axis_ind), str(input_shape), str(output_shape)))
  else:
    axis.replace('B','S')
    assert axis in ['S', 'C', 'H', 'W', 'CHW', 'HW'], (
        'Axis value %s not supported. '
        'Reduction supported along C, H, W, HW, CHW dimensions only.' % axis)

  # The simple case; reduction along non sequence axis
  if axis != 'S':
    context.builder.add_reduce(output_name, input_name, output_name, axis, mode)
  # Need to permute, reduce and then permute back
  else:
    context.builder.add_permute(
      output_name + '_swap_Seq_C', (1, 0, 2, 3), input_name, output_name + '_swap_Seq_C')
    context.builder.add_reduce(
        output_name + '_pre_permute', output_name + '_swap_Seq_C',
        output_name + '_pre_permute', 'C', mode)
    context.builder.add_permute(
        output_name, (1, 0, 2, 3), output_name + '_pre_permute', output_name)
  context.translated[output_name] = True",len(axis_ind) == 1 and axis_ind[0] == 1,len(axis_ind) == 1 == axis_ind[0]
pint,https://github.com/hgrecco/pint/tree/master/pint/quantity.py,Quantity,_imul_div$1171,"def _imul_div(self, other, magnitude_op, units_op=None):
        """"""Perform multiplication or division operation in-place and return the
        result.

        Parameters
        ----------
        other : pint.Quantity or any type accepted by :func:`_to_magnitude`
            object to be multiplied/divided with self
        magnitude_op : function
            operator function to perform on the magnitudes
            (e.g. operator.mul)
        units_op : function or None
            operator function to perform on the units; if None,
            *magnitude_op* is used (Default value = None)

        Returns
        -------

        """"""
        if units_op is None:
            units_op = magnitude_op

        offset_units_self = self._get_non_multiplicative_units()
        no_offset_units_self = len(offset_units_self)

        if not self._check(other):

            if not self._ok_for_muldiv(no_offset_units_self):
                raise OffsetUnitCalculusError(self._units, getattr(other, ""units"", """"))
            if len(offset_units_self) == 1:
                if self._units[offset_units_self[0]] != 1 or magnitude_op not in [
                    operator.mul,
                    operator.imul,
                ]:
                    raise OffsetUnitCalculusError(
                        self._units, getattr(other, ""units"", """")
                    )
            try:
                other_magnitude = _to_magnitude(
                    other, self.force_ndarray, self.force_ndarray_like
                )
            except PintTypeError:
                raise
            except TypeError:
                return NotImplemented
            self._magnitude = magnitude_op(self._magnitude, other_magnitude)
            self._units = units_op(self._units, self.UnitsContainer())
            return self

        if isinstance(other, self._REGISTRY.Unit):
            other = 1 * other

        if not self._ok_for_muldiv(no_offset_units_self):
            raise OffsetUnitCalculusError(self._units, other._units)
        elif no_offset_units_self == 1 and len(self._units) == 1:
            self.ito_root_units()

        no_offset_units_other = len(other._get_non_multiplicative_units())

        if not other._ok_for_muldiv(no_offset_units_other):
            raise OffsetUnitCalculusError(self._units, other._units)
        elif no_offset_units_other == 1 and len(other._units) == 1:
            other.ito_root_units()

        self._magnitude = magnitude_op(self._magnitude, other._magnitude)
        self._units = units_op(self._units, other._units)

        return self",no_offset_units_self == 1 and len(self._units) == 1,no_offset_units_self == 1 == len(self._units)
pint,https://github.com/hgrecco/pint/tree/master/pint/quantity.py,Quantity,_imul_div$1171,"def _imul_div(self, other, magnitude_op, units_op=None):
        """"""Perform multiplication or division operation in-place and return the
        result.

        Parameters
        ----------
        other : pint.Quantity or any type accepted by :func:`_to_magnitude`
            object to be multiplied/divided with self
        magnitude_op : function
            operator function to perform on the magnitudes
            (e.g. operator.mul)
        units_op : function or None
            operator function to perform on the units; if None,
            *magnitude_op* is used (Default value = None)

        Returns
        -------

        """"""
        if units_op is None:
            units_op = magnitude_op

        offset_units_self = self._get_non_multiplicative_units()
        no_offset_units_self = len(offset_units_self)

        if not self._check(other):

            if not self._ok_for_muldiv(no_offset_units_self):
                raise OffsetUnitCalculusError(self._units, getattr(other, ""units"", """"))
            if len(offset_units_self) == 1:
                if self._units[offset_units_self[0]] != 1 or magnitude_op not in [
                    operator.mul,
                    operator.imul,
                ]:
                    raise OffsetUnitCalculusError(
                        self._units, getattr(other, ""units"", """")
                    )
            try:
                other_magnitude = _to_magnitude(
                    other, self.force_ndarray, self.force_ndarray_like
                )
            except PintTypeError:
                raise
            except TypeError:
                return NotImplemented
            self._magnitude = magnitude_op(self._magnitude, other_magnitude)
            self._units = units_op(self._units, self.UnitsContainer())
            return self

        if isinstance(other, self._REGISTRY.Unit):
            other = 1 * other

        if not self._ok_for_muldiv(no_offset_units_self):
            raise OffsetUnitCalculusError(self._units, other._units)
        elif no_offset_units_self == 1 and len(self._units) == 1:
            self.ito_root_units()

        no_offset_units_other = len(other._get_non_multiplicative_units())

        if not other._ok_for_muldiv(no_offset_units_other):
            raise OffsetUnitCalculusError(self._units, other._units)
        elif no_offset_units_other == 1 and len(other._units) == 1:
            other.ito_root_units()

        self._magnitude = magnitude_op(self._magnitude, other._magnitude)
        self._units = units_op(self._units, other._units)

        return self",no_offset_units_other == 1 and len(other._units) == 1,no_offset_units_other == 1 == len(other._units)
geopandas,https://github.com/geopandas/geopandas/tree/master/geopandas/plotting.py,,_plot_point_collection$250,"def _plot_point_collection(
    ax,
    geoms,
    values=None,
    color=None,
    cmap=None,
    vmin=None,
    vmax=None,
    marker=""o"",
    markersize=None,
    **kwargs,
):
    """"""
    Plots a collection of Point and MultiPoint geometries to `ax`

    Parameters
    ----------
    ax : matplotlib.axes.Axes
        where shapes will be plotted
    geoms : sequence of `N` Points or MultiPoints

    values : a sequence of `N` values, optional
        Values mapped to colors using vmin, vmax, and cmap.
        Cannot be specified together with `color`.
    markersize : scalar or array-like, optional
        Size of the markers. Note that under the hood ``scatter`` is
        used, so the specified value will be proportional to the
        area of the marker (size in points^2).

    Returns
    -------
    collection : matplotlib.collections.Collection that was plotted
    """"""
    if values is not None and color is not None:
        raise ValueError(""Can only specify one of 'values' and 'color' kwargs"")

    geoms, multiindex = _flatten_multi_geoms(geoms)
    # values are expanded below as kwargs[""c""]

    x = [p.x if not p.is_empty else None for p in geoms]
    y = [p.y if not p.is_empty else None for p in geoms]

    # matplotlib 1.4 does not support c=None, and < 2.0 does not support s=None
    if values is not None:
        kwargs[""c""] = values
    if markersize is not None:
        kwargs[""s""] = markersize

    # Add to kwargs for easier checking below.
    if color is not None:
        kwargs[""color""] = color
    if marker is not None:
        kwargs[""marker""] = marker
    _expand_kwargs(kwargs, multiindex)

    if ""norm"" not in kwargs:
        collection = ax.scatter(x, y, vmin=vmin, vmax=vmax, cmap=cmap, **kwargs)
    else:
        collection = ax.scatter(x, y, cmap=cmap, **kwargs)

    return collection",values is not None and color is not None,values is not None is not color
docassemble,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/server.py,,install_zip_package$3202,"def install_zip_package(packagename, file_number):
    #logmessage(""install_zip_package: "" + packagename + "" "" + str(file_number))
    existing_package = db.session.execute(select(Package).filter_by(name=packagename).order_by(Package.id.desc()).with_for_update()).scalar()
    if existing_package is None:
        package_auth = PackageAuth(user_id=current_user.id)
        package_entry = Package(name=packagename, package_auth=package_auth, upload=file_number, active=True, type='zip', version=1)
        db.session.add(package_auth)
        db.session.add(package_entry)
    else:
        if existing_package.type == 'zip' and existing_package.upload is not None and existing_package.upload != file_number:
            SavedFile(existing_package.upload).delete()
        existing_package.package_auth.user_id = current_user.id
        existing_package.package_auth.authtype = 'owner'
        existing_package.upload = file_number
        existing_package.active = True
        existing_package.limitation = None
        existing_package.giturl = None
        existing_package.gitbranch = None
        existing_package.type = 'zip'
        existing_package.version += 1
    db.session.commit()
    return",existing_package.type == 'zip' and existing_package.upload is not None and (existing_package.upload != file_number),existing_package.type == 'zip' and None is not existing_package.upload != file_number
scipy,https://github.com/scipy/scipy/tree/master/scipy/special/tests/test_hyp2f1.py,TestHyp2f1,test_a_b_negative_int$390,"def test_a_b_negative_int(self, hyp2f1_test_case):
        a, b, c, z, expected, rtol = hyp2f1_test_case
        assert a == int(a) and a < 0 or b == int(b) and b < 0  # Tests the test
        assert_allclose(hyp2f1(a, b, c, z), expected, rtol=rtol)",a == int(a) and a < 0,int(a) == a < 0
scipy,https://github.com/scipy/scipy/tree/master/scipy/special/tests/test_hyp2f1.py,TestHyp2f1,test_a_b_negative_int$390,"def test_a_b_negative_int(self, hyp2f1_test_case):
        a, b, c, z, expected, rtol = hyp2f1_test_case
        assert a == int(a) and a < 0 or b == int(b) and b < 0  # Tests the test
        assert_allclose(hyp2f1(a, b, c, z), expected, rtol=rtol)",b == int(b) and b < 0,int(b) == b < 0
aioredis-py,https://github.com/aio-libs/aioredis-py/tree/master/aioredis/client.py,Redis,zrangebyscore$3268,"def zrangebyscore(
        self,
        name: KeyT,
        min: ZScoreBoundT,
        max: ZScoreBoundT,
        start: Optional[int] = None,
        num: Optional[int] = None,
        withscores: bool = False,
        score_cast_func: Union[Type, Callable] = float,
    ) -> Awaitable:
        """"""
        Return a range of values from the sorted set ``name`` with scores
        between ``min`` and ``max``.

        If ``start`` and ``num`` are specified, then return a slice
        of the range.

        ``withscores`` indicates to return the scores along with the values.
        The return type is a list of (value, score) pairs

        `score_cast_func`` a callable used to cast the score return value
        """"""
        if (start is not None and num is None) or (num is not None and start is None):
            raise DataError(""``start`` and ``num`` must both be specified"")
        pieces: List[EncodableT] = [""ZRANGEBYSCORE"", name, min, max]
        if start is not None and num is not None:
            pieces.extend([b""LIMIT"", start, num])
        if withscores:
            pieces.append(b""WITHSCORES"")
        options = {""withscores"": withscores, ""score_cast_func"": score_cast_func}
        return self.execute_command(*pieces, **options)",start is not None and num is not None,start is not None is not num
aioredis-py,https://github.com/aio-libs/aioredis-py/tree/master/aioredis/client.py,Redis,zrangebyscore$3268,"def zrangebyscore(
        self,
        name: KeyT,
        min: ZScoreBoundT,
        max: ZScoreBoundT,
        start: Optional[int] = None,
        num: Optional[int] = None,
        withscores: bool = False,
        score_cast_func: Union[Type, Callable] = float,
    ) -> Awaitable:
        """"""
        Return a range of values from the sorted set ``name`` with scores
        between ``min`` and ``max``.

        If ``start`` and ``num`` are specified, then return a slice
        of the range.

        ``withscores`` indicates to return the scores along with the values.
        The return type is a list of (value, score) pairs

        `score_cast_func`` a callable used to cast the score return value
        """"""
        if (start is not None and num is None) or (num is not None and start is None):
            raise DataError(""``start`` and ``num`` must both be specified"")
        pieces: List[EncodableT] = [""ZRANGEBYSCORE"", name, min, max]
        if start is not None and num is not None:
            pieces.extend([b""LIMIT"", start, num])
        if withscores:
            pieces.append(b""WITHSCORES"")
        options = {""withscores"": withscores, ""score_cast_func"": score_cast_func}
        return self.execute_command(*pieces, **options)",start is not None and num is None,start is not None is num
aioredis-py,https://github.com/aio-libs/aioredis-py/tree/master/aioredis/client.py,Redis,zrangebyscore$3268,"def zrangebyscore(
        self,
        name: KeyT,
        min: ZScoreBoundT,
        max: ZScoreBoundT,
        start: Optional[int] = None,
        num: Optional[int] = None,
        withscores: bool = False,
        score_cast_func: Union[Type, Callable] = float,
    ) -> Awaitable:
        """"""
        Return a range of values from the sorted set ``name`` with scores
        between ``min`` and ``max``.

        If ``start`` and ``num`` are specified, then return a slice
        of the range.

        ``withscores`` indicates to return the scores along with the values.
        The return type is a list of (value, score) pairs

        `score_cast_func`` a callable used to cast the score return value
        """"""
        if (start is not None and num is None) or (num is not None and start is None):
            raise DataError(""``start`` and ``num`` must both be specified"")
        pieces: List[EncodableT] = [""ZRANGEBYSCORE"", name, min, max]
        if start is not None and num is not None:
            pieces.extend([b""LIMIT"", start, num])
        if withscores:
            pieces.append(b""WITHSCORES"")
        options = {""withscores"": withscores, ""score_cast_func"": score_cast_func}
        return self.execute_command(*pieces, **options)",num is not None and start is None,num is not None is start
torch-light,https://github.com/ne7ermore/torch-light/tree/master/yolo-v3/layer.py,BasicPred,build_targets$95,"def build_targets(pred_boxes, pred_conf, pred_cls, target, anchors, num_anchors, num_classes, grid_size, ignore_thres, img_dim):
            """"""
            target - [bsz, max_obj, 5]
            """"""
            nB = target.size(0)
            nA = num_anchors
            nC = num_classes
            nG = grid_size
            mask = torch.zeros(nB, nA, nG, nG)
            conf_mask = torch.ones(nB, nA, nG, nG)
            tx = torch.zeros(nB, nA, nG, nG)
            ty = torch.zeros(nB, nA, nG, nG)
            tw = torch.zeros(nB, nA, nG, nG)
            th = torch.zeros(nB, nA, nG, nG)
            tconf = torch.ByteTensor(nB, nA, nG, nG).fill_(0)
            tcls = torch.ByteTensor(nB, nA, nG, nG, nC).fill_(0)

            nGT = 0
            nCorrect = 0
            for b in range(nB):
                for t in range(target.shape[1]):
                    if target[b, t].sum() == 0:
                        # pad
                        continue
                    nGT += 1
                    # Convert to position relative to box
                    gx = target[b, t, 1] * nG
                    gy = target[b, t, 2] * nG
                    gw = target[b, t, 3] * nG
                    gh = target[b, t, 4] * nG
                    # Get grid box indices
                    gi = int(gx)
                    gj = int(gy)
                    # Get shape of gt box
                    gt_box = torch.FloatTensor(
                        np.array([0, 0, gw, gh])).unsqueeze(0)
                    # Get shape of anchor box
                    anchor_shapes = torch.FloatTensor(np.concatenate(
                        (np.zeros((len(anchors), 2)), np.array(anchors)), 1))

                    # Calculate iou between gt and anchor shapes
                    # 1 on 3
                    anch_ious = bbox_iou(gt_box, anchor_shapes)
                    # Where the overlap is larger than threshold set mask to zero (ignore)
                    conf_mask[b, anch_ious > ignore_thres, gj, gi] = 0
                    # Find the best matching anchor box

                    best_n = np.argmax(anch_ious)
                    # Get ground truth box
                    gt_box = torch.FloatTensor(
                        np.array([gx, gy, gw, gh])).unsqueeze(0)
                    # Get the best prediction
                    pred_box = pred_boxes[b, best_n, gj, gi].unsqueeze(0)
                    # Masks
                    mask[b, best_n, gj, gi] = 1
                    conf_mask[b, best_n, gj, gi] = 1
                    # Coordinates
                    tx[b, best_n, gj, gi] = gx - gi
                    ty[b, best_n, gj, gi] = gy - gj
                    # Width and height
                    tw[b, best_n, gj, gi] = math.log(
                        gw / anchors[best_n][0] + 1e-16)
                    th[b, best_n, gj, gi] = math.log(
                        gh / anchors[best_n][1] + 1e-16)
                    # One-hot encoding of label
                    target_label = int(target[b, t, 0])
                    tcls[b, best_n, gj, gi, target_label] = 1
                    tconf[b, best_n, gj, gi] = 1

                    # Calculate iou between ground truth and best matching prediction
                    iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)
                    pred_label = torch.argmax(pred_cls[b, best_n, gj, gi])
                    score = pred_conf[b, best_n, gj, gi]
                    if iou > 0.5 and pred_label == target_label and score > 0.5:
                        nCorrect += 1

            return nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls",iou > 0.5 < score and pred_label == target_label and (score > 0.5),iou > 0.5 < score > 0.5 and pred_label == target_label
diff-match-patch,https://github.com/google/diff-match-patch/tree/master/python3/diff_match_patch.py,diff_match_patch,patch_make$1407,"def patch_make(self, a, b=None, c=None):
    """"""Compute a list of patches to turn text1 into text2.
    Use diffs if provided, otherwise compute it ourselves.
    There are four ways to call this function, depending on what data is
    available to the caller:
    Method 1:
    a = text1, b = text2
    Method 2:
    a = diffs
    Method 3 (optimal):
    a = text1, b = diffs
    Method 4 (deprecated, use method 3):
    a = text1, b = text2, c = diffs

    Args:
      a: text1 (methods 1,3,4) or Array of diff tuples for text1 to
          text2 (method 2).
      b: text2 (methods 1,4) or Array of diff tuples for text1 to
          text2 (method 3) or undefined (method 2).
      c: Array of diff tuples for text1 to text2 (method 4) or
          undefined (methods 1,2,3).

    Returns:
      Array of Patch objects.
    """"""
    text1 = None
    diffs = None
    if isinstance(a, str) and isinstance(b, str) and c is None:
      # Method 1: text1, text2
      # Compute diffs from text1 and text2.
      text1 = a
      diffs = self.diff_main(text1, b, True)
      if len(diffs) > 2:
        self.diff_cleanupSemantic(diffs)
        self.diff_cleanupEfficiency(diffs)
    elif isinstance(a, list) and b is None and c is None:
      # Method 2: diffs
      # Compute text1 from diffs.
      diffs = a
      text1 = self.diff_text1(diffs)
    elif isinstance(a, str) and isinstance(b, list) and c is None:
      # Method 3: text1, diffs
      text1 = a
      diffs = b
    elif (isinstance(a, str) and isinstance(b, str) and
          isinstance(c, list)):
      # Method 4: text1, text2, diffs
      # text2 is not used.
      text1 = a
      diffs = c
    else:
      raise ValueError(""Unknown call format to patch_make."")

    if not diffs:
      return []  # Get rid of the None case.
    patches = []
    patch = patch_obj()
    char_count1 = 0  # Number of characters into the text1 string.
    char_count2 = 0  # Number of characters into the text2 string.
    prepatch_text = text1  # Recreate the patches to determine context info.
    postpatch_text = text1
    for x in range(len(diffs)):
      (diff_type, diff_text) = diffs[x]
      if len(patch.diffs) == 0 and diff_type != self.DIFF_EQUAL:
        # A new patch starts here.
        patch.start1 = char_count1
        patch.start2 = char_count2
      if diff_type == self.DIFF_INSERT:
        # Insertion
        patch.diffs.append(diffs[x])
        patch.length2 += len(diff_text)
        postpatch_text = (postpatch_text[:char_count2] + diff_text +
                          postpatch_text[char_count2:])
      elif diff_type == self.DIFF_DELETE:
        # Deletion.
        patch.length1 += len(diff_text)
        patch.diffs.append(diffs[x])
        postpatch_text = (postpatch_text[:char_count2] +
                          postpatch_text[char_count2 + len(diff_text):])
      elif (diff_type == self.DIFF_EQUAL and
            len(diff_text) <= 2 * self.Patch_Margin and
            len(patch.diffs) != 0 and len(diffs) != x + 1):
        # Small equality inside a patch.
        patch.diffs.append(diffs[x])
        patch.length1 += len(diff_text)
        patch.length2 += len(diff_text)

      if (diff_type == self.DIFF_EQUAL and
          len(diff_text) >= 2 * self.Patch_Margin):
        # Time for a new patch.
        if len(patch.diffs) != 0:
          self.patch_addContext(patch, prepatch_text)
          patches.append(patch)
          patch = patch_obj()
          # Unlike Unidiff, our patch lists have a rolling context.
          # https://github.com/google/diff-match-patch/wiki/Unidiff
          # Update prepatch text & pos to reflect the application of the
          # just completed patch.
          prepatch_text = postpatch_text
          char_count1 = char_count2

      # Update the current character count.
      if diff_type != self.DIFF_INSERT:
        char_count1 += len(diff_text)
      if diff_type != self.DIFF_DELETE:
        char_count2 += len(diff_text)

    # Pick up the leftover patch if not empty.
    if len(patch.diffs) != 0:
      self.patch_addContext(patch, prepatch_text)
      patches.append(patch)
    return patches","isinstance(a, list) and b is None and (c is None)","b is None is c and isinstance(a, list)"
aws-parallelcluster,https://github.com/aws/aws-parallelcluster/tree/master/api/client/src/pcluster_client/model/list_image_log_streams_response_content.py,ListImageLogStreamsResponseContent,_from_openapi_data$111,"def _from_openapi_data(cls, log_streams, *args, **kwargs):  # noqa: E501
        """"""ListImageLogStreamsResponseContent - a model defined in OpenAPI

        Args:
            log_streams ([LogStream]):

        Keyword Args:
            _check_type (bool): if True, values for parameters in openapi_types
                                will be type checked and a TypeError will be
                                raised if the wrong type is input.
                                Defaults to True
            _path_to_item (tuple/list): This is a list of keys or values to
                                drill down to the model in received_data
                                when deserializing a response
            _spec_property_naming (bool): True if the variable names in the input data
                                are serialized names, as specified in the OpenAPI document.
                                False if the variable names in the input data
                                are pythonic names, e.g. snake case (default)
            _configuration (Configuration): the instance to use when
                                deserializing a file_type parameter.
                                If passed, type conversion is attempted
                                If omitted no type conversion is done.
            _visited_composed_classes (tuple): This stores a tuple of
                                classes that we have traveled through so that
                                if we see that class again we will not use its
                                discriminator again.
                                When traveling through a discriminator, the
                                composed schema that is
                                is traveled through is added to this set.
                                For example if Animal has a discriminator
                                petType and we pass in ""Dog"", and the class Dog
                                allOf includes Animal, we move through Animal
                                once using the discriminator, and pick Dog.
                                Then in Dog, we will make an instance of the
                                Animal class but this time we won't travel
                                through its discriminator because we passed in
                                _visited_composed_classes = (Animal,)
            next_token (str): Token to use for paginated requests.. [optional]  # noqa: E501
        """"""

        _check_type = kwargs.pop('_check_type', True)
        _spec_property_naming = kwargs.pop('_spec_property_naming', True)
        _path_to_item = kwargs.pop('_path_to_item', ())
        _configuration = kwargs.pop('_configuration', None)
        _visited_composed_classes = kwargs.pop('_visited_composed_classes', ())

        self = super(OpenApiModel, cls).__new__(cls)

        if args:
            for arg in args:
                if isinstance(arg, dict):
                    kwargs.update(arg)
                else:
                    raise ApiTypeError(
                        ""Invalid positional arguments=%s passed to %s. Remove those invalid positional arguments."" % (
                            args,
                            self.__class__.__name__,
                        ),
                        path_to_item=_path_to_item,
                        valid_classes=(self.__class__,),
                    )

        self._data_store = {}
        self._check_type = _check_type
        self._spec_property_naming = _spec_property_naming
        self._path_to_item = _path_to_item
        self._configuration = _configuration
        self._visited_composed_classes = _visited_composed_classes + (self.__class__,)

        self.log_streams = log_streams
        for var_name, var_value in kwargs.items():
            if var_name not in self.attribute_map and \
                        self._configuration is not None and \
                        self._configuration.discard_unknown_keys and \
                        self.additional_properties_type is None:
                # discard variable.
                continue
            setattr(self, var_name, var_value)
        return self",var_name not in self.attribute_map and self._configuration is not None and self._configuration.discard_unknown_keys and (self.additional_properties_type is None),var_name not in self.attribute_map and self._configuration is not None is self.additional_properties_type and self._configuration.discard_unknown_keys
angr,https://github.com/angr/angr/tree/master/angr/analyses/reaching_definitions/engine_vex.py,SimEngineRDVEX,_handle_Xor$684,"def _handle_Xor(self, expr):
        expr0, expr1 = self._expr(expr.args[0]), self._expr(expr.args[1])
        bits = expr.result_size(self.tyenv)

        r = None
        expr0_v = expr0.one_value()
        expr1_v = expr1.one_value()

        if expr0_v is None and expr1_v is None:
            # we do not support xor between two real multivalues
            r = MultiValues(offset_to_values={0: {self.state.top(bits)}})
        elif expr0_v is None and expr1_v is not None:
            # bitwise-xor a single value with a multivalue
            if len(expr0.values) == 1 and 0 in expr0.values:
                vs = {v ^ expr1_v for v in expr0.values[0]}
                r = MultiValues(offset_to_values={0: vs})
        elif expr0_v is not None and expr1_v is None:
            # bitwise-xor a single value to a multivalue
            if len(expr1.values) == 1 and 0 in expr1.values:
                vs = {v ^ expr0_v for v in expr1.values[0]}
                r = MultiValues(offset_to_values={0: vs})
        else:
            if expr0_v.concrete and expr1_v.concrete:
                # bitwise-xor two single values together
                r = MultiValues(offset_to_values={0: {expr0_v ^ expr1_v}})

        if r is None:
            r = MultiValues(offset_to_values={0: {self.state.top(bits)}})

        return r",expr0_v is None and expr1_v is None,expr0_v is None is expr1_v
angr,https://github.com/angr/angr/tree/master/angr/analyses/reaching_definitions/engine_vex.py,SimEngineRDVEX,_handle_Xor$684,"def _handle_Xor(self, expr):
        expr0, expr1 = self._expr(expr.args[0]), self._expr(expr.args[1])
        bits = expr.result_size(self.tyenv)

        r = None
        expr0_v = expr0.one_value()
        expr1_v = expr1.one_value()

        if expr0_v is None and expr1_v is None:
            # we do not support xor between two real multivalues
            r = MultiValues(offset_to_values={0: {self.state.top(bits)}})
        elif expr0_v is None and expr1_v is not None:
            # bitwise-xor a single value with a multivalue
            if len(expr0.values) == 1 and 0 in expr0.values:
                vs = {v ^ expr1_v for v in expr0.values[0]}
                r = MultiValues(offset_to_values={0: vs})
        elif expr0_v is not None and expr1_v is None:
            # bitwise-xor a single value to a multivalue
            if len(expr1.values) == 1 and 0 in expr1.values:
                vs = {v ^ expr0_v for v in expr1.values[0]}
                r = MultiValues(offset_to_values={0: vs})
        else:
            if expr0_v.concrete and expr1_v.concrete:
                # bitwise-xor two single values together
                r = MultiValues(offset_to_values={0: {expr0_v ^ expr1_v}})

        if r is None:
            r = MultiValues(offset_to_values={0: {self.state.top(bits)}})

        return r",expr0_v is None and expr1_v is not None,expr0_v is None is not expr1_v
angr,https://github.com/angr/angr/tree/master/angr/analyses/reaching_definitions/engine_vex.py,SimEngineRDVEX,_handle_Xor$684,"def _handle_Xor(self, expr):
        expr0, expr1 = self._expr(expr.args[0]), self._expr(expr.args[1])
        bits = expr.result_size(self.tyenv)

        r = None
        expr0_v = expr0.one_value()
        expr1_v = expr1.one_value()

        if expr0_v is None and expr1_v is None:
            # we do not support xor between two real multivalues
            r = MultiValues(offset_to_values={0: {self.state.top(bits)}})
        elif expr0_v is None and expr1_v is not None:
            # bitwise-xor a single value with a multivalue
            if len(expr0.values) == 1 and 0 in expr0.values:
                vs = {v ^ expr1_v for v in expr0.values[0]}
                r = MultiValues(offset_to_values={0: vs})
        elif expr0_v is not None and expr1_v is None:
            # bitwise-xor a single value to a multivalue
            if len(expr1.values) == 1 and 0 in expr1.values:
                vs = {v ^ expr0_v for v in expr1.values[0]}
                r = MultiValues(offset_to_values={0: vs})
        else:
            if expr0_v.concrete and expr1_v.concrete:
                # bitwise-xor two single values together
                r = MultiValues(offset_to_values={0: {expr0_v ^ expr1_v}})

        if r is None:
            r = MultiValues(offset_to_values={0: {self.state.top(bits)}})

        return r",expr0_v is not None and expr1_v is None,expr0_v is not None is expr1_v
vectorbt,https://github.com/polakowo/vectorbt/tree/master/vectorbt/signals/factory.py,SignalFactory,from_choice_func$165,"def from_choice_func(
            self,
            entry_choice_func: tp.Optional[tp.ChoiceFunc] = None,
            exit_choice_func: tp.Optional[tp.ChoiceFunc] = None,
            generate_func: tp.Callable = generate_nb,
            generate_ex_func: tp.Callable = generate_ex_nb,
            generate_enex_func: tp.Callable = generate_enex_nb,
            cache_func: tp.Callable = None,
            entry_settings: tp.KwargsLike = None,
            exit_settings: tp.KwargsLike = None,
            cache_settings: tp.KwargsLike = None,
            numba_loop: bool = False,
            **kwargs) -> tp.Type[IndicatorBase]:
        """"""Build signal generator class around entry and exit choice functions.

        A choice function is simply a function that returns indices of signals.
        There are two types of it: entry choice function and exit choice function.
        Each choice function takes broadcast time series, broadcast in-place output time series,
        broadcast parameter arrays, and other arguments, and returns an array of indices
        corresponding to chosen signals. See `vectorbt.signals.nb.generate_nb`.

        Args:
            entry_choice_func (callable): `choice_func_nb` that returns indices of entries.

                Defaults to `vectorbt.signals.nb.first_choice_nb` for `FactoryMode.Chain`.
            exit_choice_func (callable): `choice_func_nb` that returns indices of exits.
            generate_func (callable): Entry generation function.

                Defaults to `vectorbt.signals.nb.generate_nb`.
            generate_ex_func (callable): Exit generation function.

                Defaults to `vectorbt.signals.nb.generate_ex_nb`.
            generate_enex_func (callable): Entry and exit generation function.

                Defaults to `vectorbt.signals.nb.generate_enex_nb`.
            cache_func (callable): A caching function to preprocess data beforehand.

                All returned objects will be passed as last arguments to choice functions.
            entry_settings (dict): Settings dict for `entry_choice_func`.
            exit_settings (dict): Settings dict for `exit_choice_func`.
            cache_settings (dict): Settings dict for `cache_func`.
            numba_loop (bool): Whether to loop using Numba.

                Set to True when iterating large number of times over small input.
            **kwargs: Keyword arguments passed to `IndicatorFactory.from_custom_func`.

        !!! note
            Choice functions should be Numba-compiled.

            Which inputs, parameters and arguments to pass to each function should be
            explicitly indicated in the function's settings dict. By default, nothing is passed.

            Passing keyword arguments directly to the choice functions is not supported.
            Use `pass_kwargs` in a settings dict to pass keyword arguments as positional.

        Settings dict of each function can have the following keys:

        Attributes:
            pass_inputs (list of str): Input names to pass to the choice function.

                Defaults to []. Order matters. Each name must be in `input_names`.
            pass_in_outputs (list of str): In-place output names to pass to the choice function.

                Defaults to []. Order matters. Each name must be in `in_output_names`.
            pass_params (list of str): Parameter names to pass to the choice function.

                Defaults to []. Order matters. Each name must be in `param_names`.
            pass_kwargs (dict, list of str or list of tuple): Keyword arguments from `kwargs` dict to
                pass as positional arguments to the choice function.

                Defaults to []. Order matters.

                If any element is a tuple, should contain the name and the default value.
                If any element is a string, the default value is None.

                Built-in keys include:

                * `input_shape`: Input shape if no input time series passed.
                    Default is provided by the pipeline if `pass_input_shape` is True.
                * `wait`: Number of ticks to wait before placing signals.
                    Default is 1.
                * `until_next`: Whether to place signals up to the next entry signal.
                    Default is True.

                    Applied in `generate_ex_func` only.
                * `skip_until_exit`: Whether to skip processing entry signals until the next exit.
                    Default is False.

                    Applied in `generate_ex_func` only.
                * `pick_first`: Whether to stop as soon as the first exit signal is found.
                    Default is False with `FactoryMode.Entries`, otherwise is True.
                * `temp_idx_arr`: Empty integer array used to temporarily store indices.
                    Default is an automatically generated array of shape `input_shape[0]`.

                    You can also pass `temp_idx_arr1`, `temp_idx_arr2`, etc. to generate multiple.
                * `flex_2d`: See `vectorbt.base.reshape_fns.flex_select_auto_nb`.
                    Default is provided by the pipeline if `pass_flex_2d` is True.
            pass_cache (bool): Whether to pass cache from `cache_func` to the choice function.

                Defaults to False. Cache is passed unpacked.

        The following arguments can be passed to `run` and `run_combs` methods:

        Args:
            *args: Should be used instead of `entry_args` with `FactoryMode.Entries` and instead of
                `exit_args` with `FactoryMode.Exits` and `FactoryMode.Chain` with default `entry_choice_func`.
            entry_args (tuple): Arguments passed to the entry choice function.
            exit_args (tuple): Arguments passed to the exit choice function.
            cache_args (tuple): Arguments passed to the cache function.
            entry_kwargs (tuple): Settings for the entry choice function. Also contains arguments
                passed as positional if in `pass_kwargs`.
            exit_kwargs (tuple): Settings for the exit choice function. Also contains arguments
                passed as positional if in `pass_kwargs`.
            cache_kwargs (tuple): Settings for the cache function. Also contains arguments
                passed as positional if in `pass_kwargs`.
            return_cache (bool): Whether to return only cache.
            use_cache (any): Cache to use.
            **kwargs: Should be used instead of `entry_kwargs` with `FactoryMode.Entries` and instead of
                `exit_kwargs` with `FactoryMode.Exits` and `FactoryMode.Chain` with default `entry_choice_func`.

        For more arguments, see `vectorbt.indicators.factory.run_pipeline`.

        Usage:
            * The simplest signal indicator that places True at the very first index:

            ```pycon
            >>> from numba import njit
            >>> import vectorbt as vbt
            >>> import numpy as np

            >>> @njit
            ... def entry_choice_func(from_i, to_i, col):
            ...     return np.array([from_i])

            >>> @njit
            ... def exit_choice_func(from_i, to_i, col):
            ...     return np.array([from_i])

            >>> MySignals = vbt.SignalFactory().from_choice_func(
            ...     entry_choice_func=entry_choice_func,
            ...     exit_choice_func=exit_choice_func,
            ...     entry_kwargs=dict(wait=1),
            ...     exit_kwargs=dict(wait=1)
            ... )

            >>> my_sig = MySignals.run(input_shape=(3, 3))
            >>> my_sig.entries
                   0      1      2
            0   True   True   True
            1  False  False  False
            2   True   True   True
            >>> my_sig.exits
                   0      1      2
            0  False  False  False
            1   True   True   True
            2  False  False  False
            ```

            * Take the first entry and place an exit after waiting `n` ticks. Find the next entry and repeat.
            Test three different `n` values.

            ```pycon
            >>> from numba import njit
            >>> from vectorbt.signals.factory import SignalFactory

            >>> @njit
            ... def wait_choice_nb(from_i, to_i, col, n, temp_idx_arr):
            ...     temp_idx_arr[0] = from_i + n  # index of next exit
            ...     if temp_idx_arr[0] < to_i:
            ...         return temp_idx_arr[:1]
            ...     return temp_idx_arr[:0]  # must return array anyway

            >>> # Build signal generator
            >>> MySignals = SignalFactory(
            ...     mode='chain',
            ...     param_names=['n']
            ... ).from_choice_func(
            ...     exit_choice_func=wait_choice_nb,
            ...     exit_settings=dict(
            ...         pass_params=['n'],
            ...         pass_kwargs=['temp_idx_arr']  # built-in kwarg
            ...     )
            ... )

            >>> # Run signal generator
            >>> entries = [True, True, True, True, True]
            >>> my_sig = MySignals.run(entries, [0, 1, 2])

            >>> my_sig.entries  # input entries
            custom_n     0     1     2
            0         True  True  True
            1         True  True  True
            2         True  True  True
            3         True  True  True
            4         True  True  True

            >>> my_sig.new_entries  # output entries
            custom_n      0      1      2
            0          True   True   True
            1         False  False  False
            2          True  False  False
            3         False   True  False
            4          True  False   True

            >>> my_sig.exits  # output exits
            custom_n      0      1      2
            0         False  False  False
            1          True  False  False
            2         False   True  False
            3          True  False   True
            4         False  False  False
            ```

            * To combine multiple iterative signals, you would need to create a custom choice function.
            Here is an example of combining two random generators using ""OR"" rule (the first signal wins):

            ```pycon
            >>> from numba import njit
            >>> from collections import namedtuple
            >>> from vectorbt.indicators.configs import flex_elem_param_config
            >>> from vectorbt.signals.factory import SignalFactory
            >>> from vectorbt.signals.nb import rand_by_prob_choice_nb

            >>> # Enum to distinguish random generators
            >>> RandType = namedtuple('RandType', ['R1', 'R2'])(0, 1)

            >>> # Define exit choice function
            >>> @njit
            ... def rand_exit_choice_nb(from_i, to_i, col, rand_type, prob1,
            ...                         prob2, temp_idx_arr1, temp_idx_arr2, flex_2d):
            ...     idxs1 = rand_by_prob_choice_nb(from_i, to_i, col, prob1, True, temp_idx_arr1, flex_2d)
            ...     if len(idxs1) > 0:
            ...         to_i = idxs1[0]  # no need to go beyond first the first found signal
            ...     idxs2 = rand_by_prob_choice_nb(from_i, to_i, col, prob2, True, temp_idx_arr2, flex_2d)
            ...     if len(idxs2) > 0:
            ...         rand_type[idxs2[0], col] = RandType.R2
            ...         return idxs2
            ...     if len(idxs1) > 0:
            ...         rand_type[idxs1[0], col] = RandType.R1
            ...         return idxs1
            ...     return temp_idx_arr1[:0]

            >>> # Build signal generator
            >>> MySignals = SignalFactory(
            ...     mode='chain',
            ...     in_output_names=['rand_type'],
            ...     param_names=['prob1', 'prob2'],
            ...     attr_settings=dict(
            ...         rand_type=dict(dtype=RandType)  # creates rand_type_readable
            ...     )
            ... ).from_choice_func(
            ...     exit_choice_func=rand_exit_choice_nb,
            ...     exit_settings=dict(
            ...         pass_in_outputs=['rand_type'],
            ...         pass_params=['prob1', 'prob2'],
            ...         pass_kwargs=['temp_idx_arr1', 'temp_idx_arr2', 'flex_2d']
            ...     ),
            ...     param_settings=dict(
            ...         prob1=flex_elem_param_config,  # param per frame/row/col/element
            ...         prob2=flex_elem_param_config
            ...     ),
            ...     pass_flex_2d=True,
            ...     rand_type=-1  # fill with this value
            ... )

            >>> # Run signal generator
            >>> entries = [True, True, True, True, True]
            >>> my_sig = MySignals.run(entries, [0., 1.], [0., 1.], param_product=True)

            >>> my_sig.new_entries
            custom_prob1           0.0           1.0
            custom_prob2    0.0    1.0    0.0    1.0
            0              True   True   True   True
            1             False  False  False  False
            2             False   True   True   True
            3             False  False  False  False
            4             False   True   True   True

            >>> my_sig.exits
            custom_prob1           0.0           1.0
            custom_prob2    0.0    1.0    0.0    1.0
            0             False  False  False  False
            1             False   True   True   True
            2             False  False  False  False
            3             False   True   True   True
            4             False  False  False  False

            >>> my_sig.rand_type_readable
            custom_prob1     0.0     1.0
            custom_prob2 0.0 1.0 0.0 1.0
            0
            1                 R2  R1  R1
            2
            3                 R2  R1  R1
            4
            ```
        """"""

        mode = self.mode
        input_names = self.input_names
        param_names = self.param_names
        in_output_names = self.in_output_names

        if mode == FactoryMode.Entries:
            require_input_shape = True
            checks.assert_not_none(entry_choice_func)
            checks.assert_numba_func(entry_choice_func)
            if exit_choice_func is not None:
                raise ValueError(""exit_choice_func cannot be used with FactoryMode.Entries"")
        elif mode == FactoryMode.Exits:
            require_input_shape = False
            if entry_choice_func is not None:
                raise ValueError(""entry_choice_func cannot be used with FactoryMode.Exits"")
            checks.assert_not_none(exit_choice_func)
            checks.assert_numba_func(exit_choice_func)
        elif mode == FactoryMode.Both:
            require_input_shape = True
            checks.assert_not_none(entry_choice_func)
            checks.assert_numba_func(entry_choice_func)
            checks.assert_not_none(exit_choice_func)
            checks.assert_numba_func(exit_choice_func)
        else:
            require_input_shape = False
            if entry_choice_func is None:
                entry_choice_func = first_choice_nb
            if entry_settings is None:
                entry_settings = {}
            entry_settings = merge_dicts(dict(
                pass_inputs=['entries']
            ), entry_settings)
            checks.assert_not_none(entry_choice_func)
            checks.assert_numba_func(entry_choice_func)
            checks.assert_not_none(exit_choice_func)
            checks.assert_numba_func(exit_choice_func)
        require_input_shape = kwargs.pop('require_input_shape', require_input_shape)

        if entry_settings is None:
            entry_settings = {}
        if exit_settings is None:
            exit_settings = {}
        if cache_settings is None:
            cache_settings = {}

        valid_keys = [
            'pass_inputs',
            'pass_in_outputs',
            'pass_params',
            'pass_kwargs',
            'pass_cache'
        ]
        checks.assert_dict_valid(entry_settings, valid_keys)
        checks.assert_dict_valid(exit_settings, valid_keys)
        checks.assert_dict_valid(cache_settings, valid_keys)

        # Get input names for each function
        def _get_func_names(func_settings: tp.Kwargs, setting: str, all_names: tp.Sequence[str]) -> tp.List[str]:
            func_input_names = func_settings.get(setting, None)
            if func_input_names is None:
                return []
            else:
                for name in func_input_names:
                    checks.assert_in(name, all_names)
            return func_input_names

        entry_input_names = _get_func_names(entry_settings, 'pass_inputs', input_names)
        exit_input_names = _get_func_names(exit_settings, 'pass_inputs', input_names)
        cache_input_names = _get_func_names(cache_settings, 'pass_inputs', input_names)

        entry_in_output_names = _get_func_names(entry_settings, 'pass_in_outputs', in_output_names)
        exit_in_output_names = _get_func_names(exit_settings, 'pass_in_outputs', in_output_names)
        cache_in_output_names = _get_func_names(cache_settings, 'pass_in_outputs', in_output_names)

        entry_param_names = _get_func_names(entry_settings, 'pass_params', param_names)
        exit_param_names = _get_func_names(exit_settings, 'pass_params', param_names)
        cache_param_names = _get_func_names(cache_settings, 'pass_params', param_names)

        # Build a function that selects a parameter tuple
        if mode == FactoryMode.Entries:
            _0 = ""i""
            _0 += "", shape""
            _0 += "", entry_pick_first""
            _0 += "", entry_input_tuple""
            if len(entry_in_output_names) > 0:
                _0 += "", entry_in_output_tuples""
            if len(entry_param_names) > 0:
                _0 += "", entry_param_tuples""
            _0 += "", entry_args""
            _1 = ""shape""
            _1 += "", entry_pick_first""
            _1 += "", entry_choice_func""
            _1 += "", *entry_input_tuple""
            if len(entry_in_output_names) > 0:
                _1 += "", *entry_in_output_tuples[i]""
            if len(entry_param_names) > 0:
                _1 += "", *entry_param_tuples[i]""
            _1 += "", *entry_args""
            func_str = ""def apply_func({0}):\n   return generate_func({1})"".format(_0, _1)
            scope = {
                'generate_func': generate_func,
                'entry_choice_func': entry_choice_func
            }
            filename = inspect.getfile(lambda: None)
            code = compile(func_str, filename, 'single')
            exec(code, scope)
            apply_func = scope['apply_func']
            if numba_loop:
                apply_func = njit(apply_func)
                apply_and_concat_func = combine_fns.apply_and_concat_one_nb
            else:
                apply_and_concat_func = combine_fns.apply_and_concat_one

        elif mode == FactoryMode.Exits:
            _0 = ""i""
            _0 += "", entries""
            _0 += "", exit_wait""
            _0 += "", until_next""
            _0 += "", skip_until_exit""
            _0 += "", exit_pick_first""
            _0 += "", exit_input_tuple""
            if len(exit_in_output_names) > 0:
                _0 += "", exit_in_output_tuples""
            if len(exit_param_names) > 0:
                _0 += "", exit_param_tuples""
            _0 += "", exit_args""
            _1 = ""entries""
            _1 += "", exit_wait""
            _1 += "", until_next""
            _1 += "", skip_until_exit""
            _1 += "", exit_pick_first""
            _1 += "", exit_choice_func""
            _1 += "", *exit_input_tuple""
            if len(exit_in_output_names) > 0:
                _1 += "", *exit_in_output_tuples[i]""
            if len(exit_param_names) > 0:
                _1 += "", *exit_param_tuples[i]""
            _1 += "", *exit_args""
            func_str = ""def apply_func({0}):\n   return generate_ex_func({1})"".format(_0, _1)
            scope = {
                'generate_ex_func': generate_ex_func,
                'exit_choice_func': exit_choice_func
            }
            filename = inspect.getfile(lambda: None)
            code = compile(func_str, filename, 'single')
            exec(code, scope)
            apply_func = scope['apply_func']
            if numba_loop:
                apply_func = njit(apply_func)
                apply_and_concat_func = combine_fns.apply_and_concat_one_nb
            else:
                apply_and_concat_func = combine_fns.apply_and_concat_one

        else:
            _0 = ""i""
            _0 += "", shape""
            _0 += "", entry_wait""
            _0 += "", exit_wait""
            _0 += "", entry_pick_first""
            _0 += "", exit_pick_first""
            _0 += "", entry_input_tuple""
            _0 += "", exit_input_tuple""
            if len(entry_in_output_names) > 0:
                _0 += "", entry_in_output_tuples""
            if len(exit_in_output_names) > 0:
                _0 += "", exit_in_output_tuples""
            if len(entry_param_names) > 0:
                _0 += "", entry_param_tuples""
            if len(exit_param_names) > 0:
                _0 += "", exit_param_tuples""
            _0 += "", entry_args""
            _0 += "", exit_args""
            _1 = ""shape""
            _1 += "", entry_wait""
            _1 += "", exit_wait""
            _1 += "", entry_pick_first""
            _1 += "", exit_pick_first""
            _1 += "", entry_choice_func""
            _1 += "", (*entry_input_tuple""
            if len(entry_in_output_names) > 0:
                _1 += "", *entry_in_output_tuples[i]""
            if len(entry_param_names) > 0:
                _1 += "", *entry_param_tuples[i]""
            _1 += "", *entry_args)""
            _1 += "", exit_choice_func""
            _1 += "", (*exit_input_tuple""
            if len(exit_in_output_names) > 0:
                _1 += "", *exit_in_output_tuples[i]""
            if len(exit_param_names) > 0:
                _1 += "", *exit_param_tuples[i]""
            _1 += "", *exit_args)""
            func_str = ""def apply_func({0}):\n   return generate_enex_func({1})"".format(_0, _1)
            scope = {
                'generate_enex_func': generate_enex_func,
                'entry_choice_func': entry_choice_func,
                'exit_choice_func': exit_choice_func
            }
            filename = inspect.getfile(lambda: None)
            code = compile(func_str, filename, 'single')
            exec(code, scope)
            apply_func = scope['apply_func']
            if numba_loop:
                apply_func = njit(apply_func)
                apply_and_concat_func = combine_fns.apply_and_concat_multiple_nb
            else:
                apply_and_concat_func = combine_fns.apply_and_concat_multiple

        def custom_func(input_list: tp.List[tp.AnyArray],
                        in_output_list: tp.List[tp.List[tp.AnyArray]],
                        param_list: tp.List[tp.List[tp.Param]],
                        *args,
                        input_shape: tp.Optional[tp.Shape] = None,
                        flex_2d: tp.Optional[bool] = None,
                        entry_args: tp.Optional[tp.Args] = None,
                        exit_args: tp.Optional[tp.Args] = None,
                        cache_args: tp.Optional[tp.Args] = None,
                        entry_kwargs: tp.KwargsLike = None,
                        exit_kwargs: tp.KwargsLike = None,
                        cache_kwargs: tp.KwargsLike = None,
                        return_cache: bool = False,
                        use_cache: tp.Optional[CacheOutputT] = None,
                        **_kwargs) -> tp.Union[CacheOutputT, tp.Array2d, tp.List[tp.Array2d]]:
            # Get arguments
            if len(input_list) == 0:
                if input_shape is None:
                    raise ValueError(""Pass input_shape if no input time series were passed"")
            else:
                input_shape = input_list[0].shape

            if entry_args is None:
                entry_args = ()
            if exit_args is None:
                exit_args = ()
            if cache_args is None:
                cache_args = ()
            if mode == FactoryMode.Entries:
                if len(entry_args) > 0:
                    raise ValueError(""Use *args instead of entry_args with FactoryMode.Entries"")
                entry_args = args
            elif mode == FactoryMode.Exits or (mode == FactoryMode.Chain and entry_choice_func == first_choice_nb):
                if len(exit_args) > 0:
                    raise ValueError(""Use *args instead of exit_args ""
                                     ""with FactoryMode.Exits or FactoryMode.Chain"")
                exit_args = args
            else:
                if len(args) > 0:
                    raise ValueError(""*args cannot be used with FactoryMode.Both"")

            if entry_kwargs is None:
                entry_kwargs = {}
            if exit_kwargs is None:
                exit_kwargs = {}
            if cache_kwargs is None:
                cache_kwargs = {}
            if mode == FactoryMode.Entries:
                if len(entry_kwargs) > 0:
                    raise ValueError(""Use **kwargs instead of entry_kwargs with FactoryMode.Entries"")
                entry_kwargs = _kwargs
            elif mode == FactoryMode.Exits or (mode == FactoryMode.Chain and entry_choice_func == first_choice_nb):
                if len(exit_kwargs) > 0:
                    raise ValueError(""Use **kwargs instead of exit_kwargs ""
                                     ""with FactoryMode.Exits or FactoryMode.Chain"")
                exit_kwargs = _kwargs
            else:
                if len(_kwargs) > 0:
                    raise ValueError(""*args cannot be used with FactoryMode.Both"")

            kwargs_defaults = dict(
                input_shape=input_shape,
                wait=1,
                until_next=True,
                skip_until_exit=False,
                pick_first=True,
                flex_2d=flex_2d,
            )
            if mode == FactoryMode.Entries:
                kwargs_defaults['pick_first'] = False
            entry_kwargs = merge_dicts(kwargs_defaults, entry_kwargs)
            exit_kwargs = merge_dicts(kwargs_defaults, exit_kwargs)
            cache_kwargs = merge_dicts(kwargs_defaults, cache_kwargs)
            entry_wait = entry_kwargs['wait']
            exit_wait = exit_kwargs['wait']
            entry_pick_first = entry_kwargs['pick_first']
            exit_pick_first = exit_kwargs['pick_first']
            until_next = exit_kwargs['until_next']
            skip_until_exit = exit_kwargs['skip_until_exit']

            # Distribute arguments across functions
            entry_input_tuple = ()
            exit_input_tuple = ()
            cache_input_tuple = ()
            for input_name in entry_input_names:
                entry_input_tuple += (input_list[input_names.index(input_name)],)
            for input_name in exit_input_names:
                exit_input_tuple += (input_list[input_names.index(input_name)],)
            for input_name in cache_input_names:
                cache_input_tuple += (input_list[input_names.index(input_name)],)

            entry_in_output_list = []
            exit_in_output_list = []
            cache_in_output_list = []
            for in_output_name in entry_in_output_names:
                entry_in_output_list.append(in_output_list[in_output_names.index(in_output_name)])
            for in_output_name in exit_in_output_names:
                exit_in_output_list.append(in_output_list[in_output_names.index(in_output_name)])
            for in_output_name in cache_in_output_names:
                cache_in_output_list.append(in_output_list[in_output_names.index(in_output_name)])

            entry_param_list = []
            exit_param_list = []
            cache_param_list = []
            for param_name in entry_param_names:
                entry_param_list.append(param_list[param_names.index(param_name)])
            for param_name in exit_param_names:
                exit_param_list.append(param_list[param_names.index(param_name)])
            for param_name in cache_param_names:
                cache_param_list.append(param_list[param_names.index(param_name)])

            n_params = len(param_list[0]) if len(param_list) > 0 else 1
            entry_in_output_tuples = list(zip(*entry_in_output_list))
            exit_in_output_tuples = list(zip(*exit_in_output_list))
            entry_param_tuples = list(zip(*entry_param_list))
            exit_param_tuples = list(zip(*exit_param_list))

            def _build_more_args(func_settings: tp.Kwargs, func_kwargs: tp.Kwargs) -> tp.Args:
                pass_kwargs = func_settings.get('pass_kwargs', [])
                if isinstance(pass_kwargs, dict):
                    pass_kwargs = list(pass_kwargs.items())
                more_args = ()
                for key in pass_kwargs:
                    value = None
                    if isinstance(key, tuple):
                        key, value = key
                    else:
                        if key.startswith('temp_idx_arr'):
                            value = np.empty((input_shape[0],), dtype=np.int_)
                    value = func_kwargs.get(key, value)
                    more_args += (value,)
                return more_args

            entry_more_args = _build_more_args(entry_settings, entry_kwargs)
            exit_more_args = _build_more_args(exit_settings, exit_kwargs)
            cache_more_args = _build_more_args(cache_settings, cache_kwargs)

            # Caching
            cache = use_cache
            if cache is None and cache_func is not None:
                _cache_in_output_list = cache_in_output_list
                _cache_param_list = cache_param_list
                if checks.is_numba_func(cache_func):
                    if len(_cache_in_output_list) > 0:
                        _cache_in_output_list = [to_typed_list(in_outputs) for in_outputs in _cache_in_output_list]
                    if len(_cache_param_list) > 0:
                        _cache_param_list = [to_typed_list(params) for params in _cache_param_list]

                cache = cache_func(
                    *cache_input_tuple,
                    *_cache_in_output_list,
                    *_cache_param_list,
                    *cache_args,
                    *cache_more_args
                )
            if return_cache:
                return cache
            if cache is None:
                cache = ()
            if not isinstance(cache, tuple):
                cache = (cache,)

            entry_cache = ()
            exit_cache = ()
            if entry_settings.get('pass_cache', False):
                entry_cache = cache
            if exit_settings.get('pass_cache', False):
                exit_cache = cache

            # Apply and concatenate
            if mode == FactoryMode.Entries:
                if len(entry_in_output_names) > 0:
                    if numba_loop:
                        _entry_in_output_tuples = (to_typed_list(entry_in_output_tuples),)
                    else:
                        _entry_in_output_tuples = (entry_in_output_tuples,)
                else:
                    _entry_in_output_tuples = ()
                if len(entry_param_names) > 0:
                    if numba_loop:
                        _entry_param_tuples = (to_typed_list(entry_param_tuples),)
                    else:
                        _entry_param_tuples = (entry_param_tuples,)
                else:
                    _entry_param_tuples = ()

                return apply_and_concat_func(
                    n_params,
                    apply_func,
                    input_shape,
                    entry_pick_first,
                    entry_input_tuple,
                    *_entry_in_output_tuples,
                    *_entry_param_tuples,
                    entry_args + entry_more_args + entry_cache
                )

            elif mode == FactoryMode.Exits:
                if len(exit_in_output_names) > 0:
                    if numba_loop:
                        _exit_in_output_tuples = (to_typed_list(exit_in_output_tuples),)
                    else:
                        _exit_in_output_tuples = (exit_in_output_tuples,)
                else:
                    _exit_in_output_tuples = ()
                if len(exit_param_names) > 0:
                    if numba_loop:
                        _exit_param_tuples = (to_typed_list(exit_param_tuples),)
                    else:
                        _exit_param_tuples = (exit_param_tuples,)
                else:
       ",cache is None and cache_func is not None,cache is None is not cache_func
Python-For-Kids,https://github.com/mytechnotalent/Python-For-Kids/tree/master/Part_7_Unittest/0013_number_guessing_game_repl/unittest.py,TestCase,assertAlmostEqual$89,"def assertAlmostEqual(self, x, y, places=None, msg='', delta=None):
        """"""
        Method to handle assert almost equal logic

        Params:
            x: ANY
            y: ANY
            places: NoneType, optional
            msg: str, optional
            delta: NoneType, optional
        """"""
        if x == y:
            return
        if delta is not None and places is not None:
            raise TypeError('specify delta or places not both')
        if delta is not None:
            if abs(x - y) <= delta:
                return
            if not msg:
                msg = '%r != %r within %r delta' % (x, y, delta)
        else:
            if places is None:
                places = 7
            if round(abs(y-x), places) == 0:
                return
            if not msg:
                msg = '%r != %r within %r places' % (x, y, places)
        assert False, msg",delta is not None and places is not None,delta is not None is not places
mmaction2,https://github.com/open-mmlab/mmaction2/tree/master/mmaction/core/evaluation/ava_evaluation/per_image_evaluation.py,PerImageEvaluation,_compute_tp_fp$106,"def _compute_tp_fp(self,
                       detected_boxes,
                       detected_scores,
                       detected_class_labels,
                       groundtruth_boxes,
                       groundtruth_class_labels,
                       detected_masks=None,
                       groundtruth_masks=None):
        """"""Labels true/false positives of detections of an image across all
        classes.

        Args:
            detected_boxes: A float numpy array of shape [N, 4], representing N
                regions of detected object regions.
                Each row is of the format [y_min, x_min, y_max, x_max]
            detected_scores: A float numpy array of shape [N, 1], representing
                the confidence scores of the detected N object instances.
            detected_class_labels: A integer numpy array of shape [N, 1],
                repreneting the class labels of the detected N object
                instances.
            groundtruth_boxes: A float numpy array of shape [M, 4],
                representing M regions of object instances in ground truth
            groundtruth_class_labels: An integer numpy array of shape [M, 1],
                representing M class labels of object instances in ground truth
            detected_masks: (optional) A np.uint8 numpy array of shape
                [N, height, width]. If not None, the scores will be computed
                based on masks.
            groundtruth_masks: (optional) A np.uint8 numpy array of shape
                [M, height, width].

        Returns:
            result_scores: A list of float numpy arrays. Each numpy array is of
                shape [K, 1], representing K scores detected with object class
                label c
            result_tp_fp_labels: A list of boolean numpy array. Each numpy
                array is of shape [K, 1], representing K True/False positive
                label of object instances detected with class label c

        Raises:
            ValueError: If detected masks is not None but groundtruth masks are
                None, or the other way around.
        """"""
        if detected_masks is not None and groundtruth_masks is None:
            raise ValueError(
                'Detected masks is available but groundtruth masks is not.')
        if detected_masks is None and groundtruth_masks is not None:
            raise ValueError(
                'Groundtruth masks is available but detected masks is not.')

        result_scores = []
        result_tp_fp_labels = []
        for i in range(self.num_groundtruth_classes):
            (gt_boxes_at_ith_class, gt_masks_at_ith_class,
             detected_boxes_at_ith_class, detected_scores_at_ith_class,
             detected_masks_at_ith_class) = self._get_ith_class_arrays(
                 detected_boxes, detected_scores, detected_masks,
                 detected_class_labels, groundtruth_boxes, groundtruth_masks,
                 groundtruth_class_labels, i)
            scores, tp_fp_labels = self._compute_tp_fp_for_single_class(
                detected_boxes=detected_boxes_at_ith_class,
                detected_scores=detected_scores_at_ith_class,
                groundtruth_boxes=gt_boxes_at_ith_class,
                detected_masks=detected_masks_at_ith_class,
                groundtruth_masks=gt_masks_at_ith_class,
            )
            result_scores.append(scores)
            result_tp_fp_labels.append(tp_fp_labels)
        return result_scores, result_tp_fp_labels",detected_masks is not None and groundtruth_masks is None,detected_masks is not None is groundtruth_masks
mmaction2,https://github.com/open-mmlab/mmaction2/tree/master/mmaction/core/evaluation/ava_evaluation/per_image_evaluation.py,PerImageEvaluation,_compute_tp_fp$106,"def _compute_tp_fp(self,
                       detected_boxes,
                       detected_scores,
                       detected_class_labels,
                       groundtruth_boxes,
                       groundtruth_class_labels,
                       detected_masks=None,
                       groundtruth_masks=None):
        """"""Labels true/false positives of detections of an image across all
        classes.

        Args:
            detected_boxes: A float numpy array of shape [N, 4], representing N
                regions of detected object regions.
                Each row is of the format [y_min, x_min, y_max, x_max]
            detected_scores: A float numpy array of shape [N, 1], representing
                the confidence scores of the detected N object instances.
            detected_class_labels: A integer numpy array of shape [N, 1],
                repreneting the class labels of the detected N object
                instances.
            groundtruth_boxes: A float numpy array of shape [M, 4],
                representing M regions of object instances in ground truth
            groundtruth_class_labels: An integer numpy array of shape [M, 1],
                representing M class labels of object instances in ground truth
            detected_masks: (optional) A np.uint8 numpy array of shape
                [N, height, width]. If not None, the scores will be computed
                based on masks.
            groundtruth_masks: (optional) A np.uint8 numpy array of shape
                [M, height, width].

        Returns:
            result_scores: A list of float numpy arrays. Each numpy array is of
                shape [K, 1], representing K scores detected with object class
                label c
            result_tp_fp_labels: A list of boolean numpy array. Each numpy
                array is of shape [K, 1], representing K True/False positive
                label of object instances detected with class label c

        Raises:
            ValueError: If detected masks is not None but groundtruth masks are
                None, or the other way around.
        """"""
        if detected_masks is not None and groundtruth_masks is None:
            raise ValueError(
                'Detected masks is available but groundtruth masks is not.')
        if detected_masks is None and groundtruth_masks is not None:
            raise ValueError(
                'Groundtruth masks is available but detected masks is not.')

        result_scores = []
        result_tp_fp_labels = []
        for i in range(self.num_groundtruth_classes):
            (gt_boxes_at_ith_class, gt_masks_at_ith_class,
             detected_boxes_at_ith_class, detected_scores_at_ith_class,
             detected_masks_at_ith_class) = self._get_ith_class_arrays(
                 detected_boxes, detected_scores, detected_masks,
                 detected_class_labels, groundtruth_boxes, groundtruth_masks,
                 groundtruth_class_labels, i)
            scores, tp_fp_labels = self._compute_tp_fp_for_single_class(
                detected_boxes=detected_boxes_at_ith_class,
                detected_scores=detected_scores_at_ith_class,
                groundtruth_boxes=gt_boxes_at_ith_class,
                detected_masks=detected_masks_at_ith_class,
                groundtruth_masks=gt_masks_at_ith_class,
            )
            result_scores.append(scores)
            result_tp_fp_labels.append(tp_fp_labels)
        return result_scores, result_tp_fp_labels",detected_masks is None and groundtruth_masks is not None,detected_masks is None is not groundtruth_masks
DALEX,https://github.com/ModelOriented/DALEX/tree/master/python/dalex/dalex/_explainer/utils.py,,create_surrogate_model$63,"def create_surrogate_model(explainer, type, max_vars, max_depth, **kwargs):
    # utility function for model_surrogate(type=('tree', 'linear'))

    y_hat = explainer.predict(explainer.data) if explainer.y_hat is None else explainer.y_hat

    # init a proper model
    if explainer.model_type == 'regression':
        if type == 'tree':
            from sklearn.tree import DecisionTreeRegressor
            surrogate_model = DecisionTreeRegressor(max_depth=max_depth, **kwargs)
        elif type == 'linear':
            from sklearn.linear_model import LinearRegression
            surrogate_model = LinearRegression(**kwargs)
        else:
            raise TypeError(""'type' parameter must be one of {'tree', 'linear'}"")
    elif explainer.model_type == 'classification':
        y_hat = y_hat.round().astype(int)  # modify prob to classes
        if type == 'tree':
            from sklearn.tree import DecisionTreeClassifier
            surrogate_model = DecisionTreeClassifier(max_depth=max_depth, **kwargs)
        elif type == 'linear':
            from sklearn.linear_model import LogisticRegression
            surrogate_model = LogisticRegression(**kwargs)
        else:
            raise TypeError(""'type' parameter must be one of {'tree', 'linear'}"")
    else:
        raise ValueError(""'explainer.model_type' must be 'regression' or 'classification'"")

    # find the most important variables
    if max_vars is not None and max_vars < len(explainer.data.columns):
        # use model native feature importance if possible
        if hasattr(explainer.model, ""feature_importances_""):  # scikit, xgboost, lightgbm
            # take last max_vars of the variables sorted from least to most important
            vi = explainer.model.feature_importances_
            variables = explainer.data.columns[np.argsort(vi)][-max_vars:]
        else:
            # calculate model_parts and take max_vars most important variables
            vi = explainer.model_parts(N=500, B=15)
            variables = vi.result.variable[~vi.result.variable.isin(['_baseline_', '_full_model_'])].tail(max_vars)
        X = explainer.data.loc[:, variables]
    else:
        variables = explainer.data.columns
        X = explainer.data

    surrogate_model.fit(X, y_hat)

    # add additional attributes to the surrogate model object
    surrogate_model.feature_names = variables.tolist()
    if hasattr(surrogate_model, 'classes_'):
        surrogate_model.class_names = surrogate_model.classes_.astype(str).tolist()
    else:
        surrogate_model.class_names = None

    from .object import Explainer
    surrogate_explainer = Explainer(surrogate_model, X, y_hat, model_type=explainer.model_type, verbose=False)
    surrogate_model_performance = surrogate_explainer.model_performance()
    surrogate_model.performance = surrogate_model_performance.result

    # add the plot method to the surrogate model object
    if type == 'tree':
        # it uses feature_names and class_names if needed
        surrogate_model.plot = types.MethodType(plot_tree_custom, surrogate_model)

    # TODO: add plot method for the linear model

    return surrogate_model",max_vars is not None and max_vars < len(explainer.data.columns),None is not max_vars < len(explainer.data.columns)
ALiPy,https://github.com/NUAA-AL/ALiPy/tree/master/alipy/query_strategy/multi_label.py,_LabelRankingModel_MatlabVer,__init__$65,"def __init__(self, init_X=None, init_y=None):
        self._init_flag = False
        if init_X is not None and init_y is not None:
            assert len(init_X) == len(init_y)
            assert len(np.shape(init_y)) == 2
            self._init_X = np.asarray(init_X)
            self._init_y = np.asarray(init_y)

            if len(np.nonzero(self._init_y == 2.0)[0]) == 0:
                self._init_y = np.hstack((self._init_y, 2 * np.ones((self._init_y.shape[0], 1))))
                # B, V, AB, AV, Anum, trounds, costs, norm_up, step_size0, num_sub, lmbda, avg_begin, avg_size, n_repeat, \
                # max_query = self.init_model_train(self._init_X, self._init_y)
            self._init_flag = True",init_X is not None and init_y is not None,init_X is not None is not init_y
nonebot,https://github.com/nonebot/nonebot/tree/master/nonebot/command/argfilter/validators.py,,between_inclusive$88,"def between_inclusive(start=None, end=None, message=None) -> Filter_T:
    """"""
    Validate any comparable object to ensure it's between
    `start` and `end` inclusively.
    """"""

    def validate(value):
        if start is not None and value < start:
            _raise_failure(message)
        if end is not None and end < value:
            _raise_failure(message)
        return value

    return validate",start is not None and value < start,None is not start > value
nonebot,https://github.com/nonebot/nonebot/tree/master/nonebot/command/argfilter/validators.py,,between_inclusive$88,"def between_inclusive(start=None, end=None, message=None) -> Filter_T:
    """"""
    Validate any comparable object to ensure it's between
    `start` and `end` inclusively.
    """"""

    def validate(value):
        if start is not None and value < start:
            _raise_failure(message)
        if end is not None and end < value:
            _raise_failure(message)
        return value

    return validate",end is not None and end < value,None is not end < value
pytorch-center-loss,https://github.com/KaiyangZhou/pytorch-center-loss/tree/master//main.py,,main$48,"def main():
    torch.manual_seed(args.seed)
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    use_gpu = torch.cuda.is_available()
    if args.use_cpu: use_gpu = False

    sys.stdout = Logger(osp.join(args.save_dir, 'log_' + args.dataset + '.txt'))

    if use_gpu:
        print(""Currently using GPU: {}"".format(args.gpu))
        cudnn.benchmark = True
        torch.cuda.manual_seed_all(args.seed)
    else:
        print(""Currently using CPU"")

    print(""Creating dataset: {}"".format(args.dataset))
    dataset = datasets.create(
        name=args.dataset, batch_size=args.batch_size, use_gpu=use_gpu,
        num_workers=args.workers,
    )

    trainloader, testloader = dataset.trainloader, dataset.testloader

    print(""Creating model: {}"".format(args.model))
    model = models.create(name=args.model, num_classes=dataset.num_classes)

    if use_gpu:
        model = nn.DataParallel(model).cuda()

    criterion_xent = nn.CrossEntropyLoss()
    criterion_cent = CenterLoss(num_classes=dataset.num_classes, feat_dim=2, use_gpu=use_gpu)
    optimizer_model = torch.optim.SGD(model.parameters(), lr=args.lr_model, weight_decay=5e-04, momentum=0.9)
    optimizer_centloss = torch.optim.SGD(criterion_cent.parameters(), lr=args.lr_cent)

    if args.stepsize > 0:
        scheduler = lr_scheduler.StepLR(optimizer_model, step_size=args.stepsize, gamma=args.gamma)

    start_time = time.time()

    for epoch in range(args.max_epoch):
        print(""==> Epoch {}/{}"".format(epoch+1, args.max_epoch))
        train(model, criterion_xent, criterion_cent,
              optimizer_model, optimizer_centloss,
              trainloader, use_gpu, dataset.num_classes, epoch)

        if args.stepsize > 0: scheduler.step()

        if args.eval_freq > 0 and (epoch+1) % args.eval_freq == 0 or (epoch+1) == args.max_epoch:
            print(""==> Test"")
            acc, err = test(model, testloader, use_gpu, dataset.num_classes, epoch)
            print(""Accuracy (%): {}\t Error rate (%): {}"".format(acc, err))

    elapsed = round(time.time() - start_time)
    elapsed = str(datetime.timedelta(seconds=elapsed))
    print(""Finished. Total elapsed time (h:m:s): {}"".format(elapsed))",args.eval_freq > 0 and (epoch + 1) % args.eval_freq == 0,args.eval_freq > 0 == (epoch + 1) % args.eval_freq
rlkit,https://github.com/rail-berkeley/rlkit/tree/master/rlkit/envs/pearl_envs/rand_param_envs/walker2d_rand_params.py,Walker2DRandParamsEnv,_step$11,"def _step(self, a):
        # import ipdb; ipdb.set_trace()
        # posbefore = self.model.data.qpos[0, 0]
        posbefore = self.sim.data.qpos[0]
        self.do_simulation(a, self.frame_skip)
        # posafter, height, ang = self.model.data.qpos[0:3, 0]
        posafter, height, ang = self.sim.data.qpos[0:3]
        alive_bonus = 1.0
        reward = ((posafter - posbefore) / self.dt)
        reward += alive_bonus
        reward -= 1e-3 * np.square(a).sum()
        done = not (height > 0.8 and height < 2.0 and
                    ang > -1.0 and ang < 1.0)
        ob = self._get_obs()
        return ob, reward, done, {}",height > 0.8 and height < 2.0 and (ang > -1.0) and (ang < 1.0),0.8 < height < 2.0 and -1.0 < ang < 1.0
dm-haiku,https://github.com/deepmind/dm-haiku/tree/master/haiku/_src/stateful.py,,difference$274,"def difference(before: InternalState, after: InternalState) -> InternalState:
  """"""Returns an InternalState object with unchanged items set to ``None``.

  Note that to determine what values have changed we compare them by identity
  not by value. This is only reasonable to do if `difference` is used to compare
  state *inside* a JAX transform (e.g. comparing the arguments passed into JIT
  with the values that you are about to return from it).

  This function never produces false negatives (e.g. we will never incorrectly
  say that a piece of state is unchanged when it has), however it may produce
  false positives. One well known case is if a value is traced by an inner JAX
  transform but unchanged, the identity of the Python object will differ from
  the value passed into the outer function, but the value will not have changed.
  In this case `difference` will say that the value has changed. For example if
  the following change happened inside a function whose state was being diffed
  we would defensively say that ``u`` had changed value even though it had only
  changed Python identity:

  >>> u = hk.get_state(""u"", [], init=jnp.ones)
  >>> u, _ = jax.jit(lambda a: a, a ** 2)(u)
  >>> hk.set_state(""u"", u)

  Args:
    before: state before.
    after: state after.

  Returns:
    The difference between before and after, with any values that have the same
    identity before and after set to `None`.
  """"""

  def if_changed(is_new, box_a, box_b):
    if box_a.value is None or is_new(box_a.value, box_b.value):
      return box_b.value
    else:
      return None

  # params
  is_new_param = lambda a, b: a is not b
  params_before, params_after = box_and_fill_missing(before.params,
                                                     after.params)
  params_after = jax.tree_util.tree_map(
      functools.partial(if_changed, is_new_param), params_before, params_after)

  # state
  def is_new_state(a: base.StatePair, b: base.StatePair):
    return a.initial is not b.initial or a.current is not b.current

  state_before, state_after = box_and_fill_missing(before.state, after.state)
  state_after = jax.tree_util.tree_map(
      functools.partial(if_changed, is_new_state), state_before, state_after)

  # rng
  def is_new_rng(a: Optional[base.PRNGSequenceState],
                 b: Optional[base.PRNGSequenceState]):
    if a is None:
      return True
    assert len(a) == 2 and len(b) == 2
    return a[0] is not b[0] or a[1] is not b[1]

  rng = after.rng if is_new_rng(before.rng, after.rng) else None

  return InternalState(params_after, state_after, rng)",len(a) == 2 and len(b) == 2,len(a) == 2 == len(b)
metaworld,https://github.com/rlworkgroup/metaworld/tree/master/metaworld/envs/mujoco/sawyer_xyz/v1/sawyer_assembly_peg.py,SawyerNutAssemblyEnv,compute_reward$105,"def compute_reward(self, actions, obs):
        graspPos = obs[3:6]
        objPos = self.get_body_com('RoundNut')

        rightFinger, leftFinger = self._get_site_pos('rightEndEffector'), self._get_site_pos('leftEndEffector')
        fingerCOM  =  (rightFinger + leftFinger)/2

        heightTarget = self.heightTarget
        placingGoal = self._target_pos

        reachDist = np.linalg.norm(graspPos - fingerCOM)

        placingDist = np.linalg.norm(objPos[:2] - placingGoal[:2])
        placingDistFinal = np.abs(objPos[-1] - self.objHeight)

        def reachReward():
            reachRew = -reachDist
            reachDistxy = np.linalg.norm(graspPos[:-1] - fingerCOM[:-1])
            zRew = np.linalg.norm(fingerCOM[-1] - self.init_fingerCOM[-1])
            if reachDistxy < 0.04:
                reachRew = -reachDist
            else:
                reachRew =  -reachDistxy - zRew

            # incentive to close fingers when reachDist is small
            if reachDist < 0.04:
                reachRew = -reachDist + max(actions[-1],0)/50
            return reachRew, reachDist

        def pickCompletionCriteria():
            tolerance = 0.01
            if objPos[2] >= (heightTarget - tolerance) and reachDist < 0.03:
                return True
            else:
                return False

        if pickCompletionCriteria():
            self.pickCompleted = True

        def objDropped():
            return (objPos[2] < (self.objHeight + 0.005)) and (placingDist >0.02) and (reachDist > 0.02)

        def placeCompletionCriteria():
            if abs(objPos[0] - placingGoal[0]) < 0.03 and \
                abs(objPos[1] - placingGoal[1]) < 0.03:
                return True
            else:
                return False

        if placeCompletionCriteria():
            self.placeCompleted = True
        else:
            self.placeCompleted = False

        def orig_pickReward():
            hScale = 100
            if self.placeCompleted or (self.pickCompleted and not(objDropped())):
                return hScale*heightTarget
            elif (reachDist < 0.04) and (objPos[2]> (self.objHeight + 0.005)) :
                return hScale* min(heightTarget, objPos[2])
            else:
                return 0

        def placeRewardMove():
            c1 = 1000 ; c2 = 0.01 ; c3 = 0.001
            placeRew = 1000*(self.maxPlacingDist - placingDist) + c1*(np.exp(-(placingDist**2)/c2) + np.exp(-(placingDist**2)/c3))
            if self.placeCompleted:
                c4 = 2000; c5 = 0.003; c6 = 0.0003
                placeRew += 2000*(heightTarget - placingDistFinal) + c4*(np.exp(-(placingDistFinal**2)/c5) + np.exp(-(placingDistFinal**2)/c6))
            placeRew = max(placeRew,0)
            cond = self.placeCompleted or (self.pickCompleted and (reachDist < 0.04) and not(objDropped()))
            if cond:
                return [placeRew, placingDist, placingDistFinal]
            else:
                return [0, placingDist, placingDistFinal]

        reachRew, reachDist = reachReward()
        pickRew = orig_pickReward()
        placeRew , placingDist, placingDistFinal = placeRewardMove()
        assert ((placeRew >=0) and (pickRew>=0))
        reward = reachRew + pickRew + placeRew
        success = (abs(objPos[0] - placingGoal[0]) < 0.03 and abs(objPos[1] - placingGoal[1]) < 0.03 and placingDistFinal <= 0.04)
        return [reward, reachRew, reachDist, pickRew, placeRew, placingDist, placingDistFinal, success]",placeRew >= 0 and pickRew >= 0,placeRew >= 0 <= pickRew
metaworld,https://github.com/rlworkgroup/metaworld/tree/master/metaworld/envs/mujoco/sawyer_xyz/v1/sawyer_assembly_peg.py,SawyerNutAssemblyEnv,compute_reward$105,"def compute_reward(self, actions, obs):
        graspPos = obs[3:6]
        objPos = self.get_body_com('RoundNut')

        rightFinger, leftFinger = self._get_site_pos('rightEndEffector'), self._get_site_pos('leftEndEffector')
        fingerCOM  =  (rightFinger + leftFinger)/2

        heightTarget = self.heightTarget
        placingGoal = self._target_pos

        reachDist = np.linalg.norm(graspPos - fingerCOM)

        placingDist = np.linalg.norm(objPos[:2] - placingGoal[:2])
        placingDistFinal = np.abs(objPos[-1] - self.objHeight)

        def reachReward():
            reachRew = -reachDist
            reachDistxy = np.linalg.norm(graspPos[:-1] - fingerCOM[:-1])
            zRew = np.linalg.norm(fingerCOM[-1] - self.init_fingerCOM[-1])
            if reachDistxy < 0.04:
                reachRew = -reachDist
            else:
                reachRew =  -reachDistxy - zRew

            # incentive to close fingers when reachDist is small
            if reachDist < 0.04:
                reachRew = -reachDist + max(actions[-1],0)/50
            return reachRew, reachDist

        def pickCompletionCriteria():
            tolerance = 0.01
            if objPos[2] >= (heightTarget - tolerance) and reachDist < 0.03:
                return True
            else:
                return False

        if pickCompletionCriteria():
            self.pickCompleted = True

        def objDropped():
            return (objPos[2] < (self.objHeight + 0.005)) and (placingDist >0.02) and (reachDist > 0.02)

        def placeCompletionCriteria():
            if abs(objPos[0] - placingGoal[0]) < 0.03 and \
                abs(objPos[1] - placingGoal[1]) < 0.03:
                return True
            else:
                return False

        if placeCompletionCriteria():
            self.placeCompleted = True
        else:
            self.placeCompleted = False

        def orig_pickReward():
            hScale = 100
            if self.placeCompleted or (self.pickCompleted and not(objDropped())):
                return hScale*heightTarget
            elif (reachDist < 0.04) and (objPos[2]> (self.objHeight + 0.005)) :
                return hScale* min(heightTarget, objPos[2])
            else:
                return 0

        def placeRewardMove():
            c1 = 1000 ; c2 = 0.01 ; c3 = 0.001
            placeRew = 1000*(self.maxPlacingDist - placingDist) + c1*(np.exp(-(placingDist**2)/c2) + np.exp(-(placingDist**2)/c3))
            if self.placeCompleted:
                c4 = 2000; c5 = 0.003; c6 = 0.0003
                placeRew += 2000*(heightTarget - placingDistFinal) + c4*(np.exp(-(placingDistFinal**2)/c5) + np.exp(-(placingDistFinal**2)/c6))
            placeRew = max(placeRew,0)
            cond = self.placeCompleted or (self.pickCompleted and (reachDist < 0.04) and not(objDropped()))
            if cond:
                return [placeRew, placingDist, placingDistFinal]
            else:
                return [0, placingDist, placingDistFinal]

        reachRew, reachDist = reachReward()
        pickRew = orig_pickReward()
        placeRew , placingDist, placingDistFinal = placeRewardMove()
        assert ((placeRew >=0) and (pickRew>=0))
        reward = reachRew + pickRew + placeRew
        success = (abs(objPos[0] - placingGoal[0]) < 0.03 and abs(objPos[1] - placingGoal[1]) < 0.03 and placingDistFinal <= 0.04)
        return [reward, reachRew, reachDist, pickRew, placeRew, placingDist, placingDistFinal, success]",abs(objPos[0] - placingGoal[0]) < 0.03 and abs(objPos[1] - placingGoal[1]) < 0.03 and (placingDistFinal <= 0.04),abs(objPos[0] - placingGoal[0]) < 0.03 > abs(objPos[1] - placingGoal[1]) and placingDistFinal <= 0.04
metaworld,https://github.com/rlworkgroup/metaworld/tree/master/metaworld/envs/mujoco/sawyer_xyz/v1/sawyer_assembly_peg.py,SawyerNutAssemblyEnv,compute_reward$105,"def compute_reward(self, actions, obs):
        graspPos = obs[3:6]
        objPos = self.get_body_com('RoundNut')

        rightFinger, leftFinger = self._get_site_pos('rightEndEffector'), self._get_site_pos('leftEndEffector')
        fingerCOM  =  (rightFinger + leftFinger)/2

        heightTarget = self.heightTarget
        placingGoal = self._target_pos

        reachDist = np.linalg.norm(graspPos - fingerCOM)

        placingDist = np.linalg.norm(objPos[:2] - placingGoal[:2])
        placingDistFinal = np.abs(objPos[-1] - self.objHeight)

        def reachReward():
            reachRew = -reachDist
            reachDistxy = np.linalg.norm(graspPos[:-1] - fingerCOM[:-1])
            zRew = np.linalg.norm(fingerCOM[-1] - self.init_fingerCOM[-1])
            if reachDistxy < 0.04:
                reachRew = -reachDist
            else:
                reachRew =  -reachDistxy - zRew

            # incentive to close fingers when reachDist is small
            if reachDist < 0.04:
                reachRew = -reachDist + max(actions[-1],0)/50
            return reachRew, reachDist

        def pickCompletionCriteria():
            tolerance = 0.01
            if objPos[2] >= (heightTarget - tolerance) and reachDist < 0.03:
                return True
            else:
                return False

        if pickCompletionCriteria():
            self.pickCompleted = True

        def objDropped():
            return (objPos[2] < (self.objHeight + 0.005)) and (placingDist >0.02) and (reachDist > 0.02)

        def placeCompletionCriteria():
            if abs(objPos[0] - placingGoal[0]) < 0.03 and \
                abs(objPos[1] - placingGoal[1]) < 0.03:
                return True
            else:
                return False

        if placeCompletionCriteria():
            self.placeCompleted = True
        else:
            self.placeCompleted = False

        def orig_pickReward():
            hScale = 100
            if self.placeCompleted or (self.pickCompleted and not(objDropped())):
                return hScale*heightTarget
            elif (reachDist < 0.04) and (objPos[2]> (self.objHeight + 0.005)) :
                return hScale* min(heightTarget, objPos[2])
            else:
                return 0

        def placeRewardMove():
            c1 = 1000 ; c2 = 0.01 ; c3 = 0.001
            placeRew = 1000*(self.maxPlacingDist - placingDist) + c1*(np.exp(-(placingDist**2)/c2) + np.exp(-(placingDist**2)/c3))
            if self.placeCompleted:
                c4 = 2000; c5 = 0.003; c6 = 0.0003
                placeRew += 2000*(heightTarget - placingDistFinal) + c4*(np.exp(-(placingDistFinal**2)/c5) + np.exp(-(placingDistFinal**2)/c6))
            placeRew = max(placeRew,0)
            cond = self.placeCompleted or (self.pickCompleted and (reachDist < 0.04) and not(objDropped()))
            if cond:
                return [placeRew, placingDist, placingDistFinal]
            else:
                return [0, placingDist, placingDistFinal]

        reachRew, reachDist = reachReward()
        pickRew = orig_pickReward()
        placeRew , placingDist, placingDistFinal = placeRewardMove()
        assert ((placeRew >=0) and (pickRew>=0))
        reward = reachRew + pickRew + placeRew
        success = (abs(objPos[0] - placingGoal[0]) < 0.03 and abs(objPos[1] - placingGoal[1]) < 0.03 and placingDistFinal <= 0.04)
        return [reward, reachRew, reachDist, pickRew, placeRew, placingDist, placingDistFinal, success]",objPos[2] < self.objHeight + 0.005 and placingDist > 0.02 and (reachDist > 0.02),objPos[2] < self.objHeight + 0.005 and placingDist > 0.02 < reachDist
metaworld,https://github.com/rlworkgroup/metaworld/tree/master/metaworld/envs/mujoco/sawyer_xyz/v1/sawyer_assembly_peg.py,SawyerNutAssemblyEnv,compute_reward$105,"def compute_reward(self, actions, obs):
        graspPos = obs[3:6]
        objPos = self.get_body_com('RoundNut')

        rightFinger, leftFinger = self._get_site_pos('rightEndEffector'), self._get_site_pos('leftEndEffector')
        fingerCOM  =  (rightFinger + leftFinger)/2

        heightTarget = self.heightTarget
        placingGoal = self._target_pos

        reachDist = np.linalg.norm(graspPos - fingerCOM)

        placingDist = np.linalg.norm(objPos[:2] - placingGoal[:2])
        placingDistFinal = np.abs(objPos[-1] - self.objHeight)

        def reachReward():
            reachRew = -reachDist
            reachDistxy = np.linalg.norm(graspPos[:-1] - fingerCOM[:-1])
            zRew = np.linalg.norm(fingerCOM[-1] - self.init_fingerCOM[-1])
            if reachDistxy < 0.04:
                reachRew = -reachDist
            else:
                reachRew =  -reachDistxy - zRew

            # incentive to close fingers when reachDist is small
            if reachDist < 0.04:
                reachRew = -reachDist + max(actions[-1],0)/50
            return reachRew, reachDist

        def pickCompletionCriteria():
            tolerance = 0.01
            if objPos[2] >= (heightTarget - tolerance) and reachDist < 0.03:
                return True
            else:
                return False

        if pickCompletionCriteria():
            self.pickCompleted = True

        def objDropped():
            return (objPos[2] < (self.objHeight + 0.005)) and (placingDist >0.02) and (reachDist > 0.02)

        def placeCompletionCriteria():
            if abs(objPos[0] - placingGoal[0]) < 0.03 and \
                abs(objPos[1] - placingGoal[1]) < 0.03:
                return True
            else:
                return False

        if placeCompletionCriteria():
            self.placeCompleted = True
        else:
            self.placeCompleted = False

        def orig_pickReward():
            hScale = 100
            if self.placeCompleted or (self.pickCompleted and not(objDropped())):
                return hScale*heightTarget
            elif (reachDist < 0.04) and (objPos[2]> (self.objHeight + 0.005)) :
                return hScale* min(heightTarget, objPos[2])
            else:
                return 0

        def placeRewardMove():
            c1 = 1000 ; c2 = 0.01 ; c3 = 0.001
            placeRew = 1000*(self.maxPlacingDist - placingDist) + c1*(np.exp(-(placingDist**2)/c2) + np.exp(-(placingDist**2)/c3))
            if self.placeCompleted:
                c4 = 2000; c5 = 0.003; c6 = 0.0003
                placeRew += 2000*(heightTarget - placingDistFinal) + c4*(np.exp(-(placingDistFinal**2)/c5) + np.exp(-(placingDistFinal**2)/c6))
            placeRew = max(placeRew,0)
            cond = self.placeCompleted or (self.pickCompleted and (reachDist < 0.04) and not(objDropped()))
            if cond:
                return [placeRew, placingDist, placingDistFinal]
            else:
                return [0, placingDist, placingDistFinal]

        reachRew, reachDist = reachReward()
        pickRew = orig_pickReward()
        placeRew , placingDist, placingDistFinal = placeRewardMove()
        assert ((placeRew >=0) and (pickRew>=0))
        reward = reachRew + pickRew + placeRew
        success = (abs(objPos[0] - placingGoal[0]) < 0.03 and abs(objPos[1] - placingGoal[1]) < 0.03 and placingDistFinal <= 0.04)
        return [reward, reachRew, reachDist, pickRew, placeRew, placingDist, placingDistFinal, success]",abs(objPos[0] - placingGoal[0]) < 0.03 and abs(objPos[1] - placingGoal[1]) < 0.03,abs(objPos[0] - placingGoal[0]) < 0.03 > abs(objPos[1] - placingGoal[1])
binance-public-data,https://github.com/binance/binance-public-data/tree/master/python/download-trade.py,,download_monthly_trades$19,"def download_monthly_trades(trading_type, symbols, num_symbols, years, months, start_date, end_date, folder, checksum):
  current = 0
  date_range = None

  if start_date and end_date:
    date_range = start_date + "" "" + end_date

  if not start_date:
    start_date = START_DATE
  else:
    start_date = convert_to_date_object(start_date)

  if not end_date:
    end_date = END_DATE
  else:
    end_date = convert_to_date_object(end_date)

  print(""Found {} symbols"".format(num_symbols))

  for symbol in symbols:
    print(""[{}/{}] - start download monthly {} trades "".format(current+1, num_symbols, symbol))
    for year in years:
      for month in months:
        current_date = convert_to_date_object('{}-{}-01'.format(year, month))
        if current_date >= start_date and current_date <= end_date:
          path = get_path(trading_type, ""trades"", ""monthly"", symbol)
          file_name = ""{}-trades-{}-{}.zip"".format(symbol.upper(), year, '{:02d}'.format(month))
          download_file(path, file_name, date_range, folder)

          if checksum == 1:
            checksum_path = get_path(trading_type, ""trades"", ""monthly"", symbol)
            checksum_file_name = ""{}-trades-{}-{}.zip.CHECKSUM"".format(symbol.upper(), year, '{:02d}'.format(month))
            download_file(checksum_path, checksum_file_name, date_range, folder)
    
    current += 1",current_date >= start_date and current_date <= end_date,start_date <= current_date <= end_date
gluon-nlp,https://github.com/dmlc/gluon-nlp/tree/master/scripts/question_answering/run_squad.py,SquadDatasetProcessor,process_sample$212,"def process_sample(self, feature: SquadFeature):
        """"""Process the data to the following format.

        Note that we mask all the special tokens except the CLS token. The reason for not masking
        the CLS token is that if the question is not answerable, we will set the start and end to
        be 0.


        Merged:      <CLS> Question <SEP> Context <SEP>
        Segment IDs:  0       0       0      1      1
        Mask:         0       1       1      0      1

        Here, we need to emphasize that when mask = 1, the data are actually not masked!

        Parameters
        ----------
        feature
            Tokenized SQuAD feature

        Returns
        -------
        ret
            Divide the feature into multiple chunks and extract the feature which contains
            the following:
            - data
                The data that concatenates the query and the context + special tokens
            - valid_length
                The valid_length of the data
            - segment_ids
                We assign the query part as segment 0 and the context part as segment 1.
            - masks
                We mask all the special tokens. 1 --> not masked, 0 --> masked.
            - is_impossible
                Whether the provided context is impossible to answer or not.
            - gt_start
                The ground-truth start location of the span
            - gt_end
                The ground-truth end location of the span
            - chunk_start
                The start of the chunk
            - chunk_length
                The length of the chunk
        """"""
        ret = []
        truncated_query_ids = feature.query_token_ids[:self._max_query_length]
        chunks = feature.get_chunks(
            doc_stride=self._doc_stride,
            max_chunk_length=self._max_seq_length - len(truncated_query_ids) - 3)
        for chunk in chunks:
            data = np.array([self.cls_id] + truncated_query_ids + [self.sep_id] +
                            feature.context_token_ids[chunk.start:(chunk.start + chunk.length)] +
                            [self.sep_id], dtype=np.int32)
            valid_length = len(data)
            segment_ids = np.array([0] + [0] * len(truncated_query_ids) +
                                   [0] + [1] * chunk.length + [1], dtype=np.int32)
            masks = np.array([0] + [1] * len(truncated_query_ids) + [1] + [0] * chunk.length + [1],
                             dtype=np.int32)
            context_offset = len(truncated_query_ids) + 2
            if chunk.gt_start_pos is None and chunk.gt_end_pos is None:
                start_pos = 0
                end_pos = 0
            else:
                # Here, we increase the start and end because we put query before context
                start_pos = chunk.gt_start_pos + context_offset
                end_pos = chunk.gt_end_pos + context_offset
            chunk_feature = ChunkFeature(qas_id=feature.qas_id,
                                         data=data,
                                         valid_length=valid_length,
                                         segment_ids=segment_ids,
                                         masks=masks,
                                         is_impossible=chunk.is_impossible,
                                         gt_start=start_pos,
                                         gt_end=end_pos,
                                         context_offset=context_offset,
                                         chunk_start=chunk.start,
                                         chunk_length=chunk.length)
            ret.append(chunk_feature)
        return ret",chunk.gt_start_pos is None and chunk.gt_end_pos is None,chunk.gt_start_pos is None is chunk.gt_end_pos
tvm,https://github.com/apache/tvm/tree/master/tests/python/unittest/test_tir_transform_compact_buffer_region.py,,compacted_padding_pattern_func$409,"def compacted_padding_pattern_func(a: T.handle, c: T.handle) -> None:
    A = T.match_buffer(a, [16, 16], dtype=""float32"")
    C = T.match_buffer(c, [20, 20], dtype=""float32"")
    with T.block():
        B = T.alloc_buffer([16, 16], dtype=""float32"")
        for i, j in T.grid(16, 16):
            with T.block():
                B[i, j] = A[i, j]
        for i, j in T.grid(20, 20):
            with T.block():
                C[i, j] = T.if_then_else(
                    2 <= i and i < 18 and 2 <= j and j < 18, B[i - 2, j - 2], 0.0, dtype=""float32""
                )",2 <= i and i < 18 and (2 <= j) and (j < 18),j < 18 > i >= 2 <= j
PaddleDetection,https://github.com/PaddlePaddle/PaddleDetection/tree/master/static/ppdet/utils/voc_eval.py,,prune_zero_padding$113,"def prune_zero_padding(gt_box, gt_label, difficult=None):
    valid_cnt = 0
    for i in range(len(gt_box)):
        if gt_box[i, 0] == 0 and gt_box[i, 1] == 0 and \
                gt_box[i, 2] == 0 and gt_box[i, 3] == 0:
            break
        valid_cnt += 1
    return (gt_box[:valid_cnt], gt_label[:valid_cnt], difficult[:valid_cnt]
            if difficult is not None else None)","gt_box[i, 0] == 0 and gt_box[i, 1] == 0 and (gt_box[i, 2] == 0) and (gt_box[i, 3] == 0)","gt_box[i, 0] == 0 == gt_box[i, 1] and gt_box[i, 2] == 0 == gt_box[i, 3]"
mmtracking,https://github.com/open-mmlab/mmtracking/tree/master/mmtrack/models/losses/l2_loss.py,L2Loss,update_weight$77,"def update_weight(self, pred, target, weight, avg_factor):
        """"""Update the weight according to targets.""""""
        if weight is None:
            weight = target.new_ones(target.size())

        invalid_inds = weight <= 0
        target[invalid_inds] = -1
        pos_inds = target == 1
        neg_inds = target == 0

        if self.pos_margin > 0:
            pred[pos_inds] -= self.pos_margin
        if self.neg_margin > 0:
            pred[neg_inds] -= self.neg_margin
        pred = torch.clamp(pred, min=0, max=1)

        num_pos = int((target == 1).sum())
        num_neg = int((target == 0).sum())
        if self.neg_pos_ub > 0 and num_neg / (num_pos +
                                              1e-6) > self.neg_pos_ub:
            num_neg = num_pos * self.neg_pos_ub
            neg_idx = torch.nonzero(target == 0, as_tuple=False)

            if self.hard_mining:
                costs = l2_loss(
                    pred, target, reduction='none')[neg_idx[:, 0],
                                                    neg_idx[:, 1]].detach()
                neg_idx = neg_idx[costs.topk(num_neg)[1], :]
            else:
                neg_idx = self.random_choice(neg_idx, num_neg)

            new_neg_inds = neg_inds.new_zeros(neg_inds.size()).bool()
            new_neg_inds[neg_idx[:, 0], neg_idx[:, 1]] = True

            invalid_neg_inds = torch.logical_xor(neg_inds, new_neg_inds)
            weight[invalid_neg_inds] = 0

        avg_factor = (weight > 0).sum()
        return pred, weight, avg_factor",self.neg_pos_ub > 0 and num_neg / (num_pos + 1e-06) > self.neg_pos_ub,num_neg / (num_pos + 1e-06) > self.neg_pos_ub > 0
nlg-eval,https://github.com/Maluuba/nlg-eval/tree/master/nlgeval/pycocoevalcap/cider/cider_scorer.py,CiderScorer,sim$135,"def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):
            '''
            Compute the cosine similarity of two vectors.
            :param vec_hyp: array of dictionary for vector corresponding to hypothesis
            :param vec_ref: array of dictionary for vector corresponding to reference
            :param norm_hyp: array of float for vector corresponding to hypothesis
            :param norm_ref: array of float for vector corresponding to reference
            :param length_hyp: int containing length of hypothesis
            :param length_ref: int containing length of reference
            :return: array of score for each n-grams cosine similarity
            '''
            delta = float(length_hyp - length_ref)
            # measure consine similarity
            val = np.array([0.0 for _ in range(self.n)])
            for n in range(self.n):
                # ngram
                for (ngram,count) in six.iteritems(vec_hyp[n]):
                    # vrama91 : added clipping
                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]

                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):
                    val[n] /= (norm_hyp[n]*norm_ref[n])

                assert(not math.isnan(val[n]))
                # vrama91: added a length based gaussian penalty
                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))
            return val",norm_hyp[n] != 0 != norm_ref[n] and norm_ref[n] != 0,norm_hyp[n] != 0 != norm_ref[n] != 0
R-Drop,https://github.com/dropreg/R-Drop/tree/master/fairseq_src/fairseq/models/fairseq_model.py,BaseFairseqModel,prepare_for_onnx_export_$211,"def prepare_for_onnx_export_(self, **kwargs):
        """"""Make model exportable via ONNX trace.""""""
        seen = set()

        def apply_prepare_for_onnx_export_(module):
            if (
                module != self
                and hasattr(module, ""prepare_for_onnx_export_"")
                and module not in seen
            ):
                seen.add(module)
                module.prepare_for_onnx_export_(**kwargs)

        self.apply(apply_prepare_for_onnx_export_)","module != self and hasattr(module, 'prepare_for_onnx_export_') and (module not in seen)","self != module not in seen and hasattr(module, 'prepare_for_onnx_export_')"
novelWriter,https://github.com/vkbo/novelWriter/tree/master/novelwriter/gui/projtree.py,GuiProjectTree,_addTreeItem$946,"def _addTreeItem(self, nwItem, nHandle=None):
        """"""Create a QTreeWidgetItem from an NWItem and add it to the
        project tree.
        """"""
        tHandle = nwItem.itemHandle
        pHandle = nwItem.itemParent
        newItem = QTreeWidgetItem([""""]*4)

        newItem.setText(self.C_NAME, """")
        newItem.setText(self.C_COUNT, ""0"")
        newItem.setText(self.C_EXPORT, """")
        newItem.setText(self.C_STATUS, """")

        newItem.setTextAlignment(self.C_NAME, Qt.AlignLeft)
        newItem.setTextAlignment(self.C_COUNT, Qt.AlignRight)
        newItem.setTextAlignment(self.C_EXPORT, Qt.AlignLeft)
        newItem.setTextAlignment(self.C_STATUS, Qt.AlignLeft)

        newItem.setData(self.C_NAME, Qt.UserRole, tHandle)
        newItem.setData(self.C_COUNT, Qt.UserRole, 0)

        self._treeMap[tHandle] = newItem
        if pHandle is None:
            if nwItem.itemType == nwItemType.ROOT:
                self.addTopLevelItem(newItem)
                self.theParent.mainMenu.setAvailableRoot()
            elif nwItem.itemType == nwItemType.TRASH:
                self.addTopLevelItem(newItem)
            else:
                self.theParent.makeAlert(self.tr(
                    ""There is nowhere to add item with name '{0}'.""
                ).format(nwItem.itemName), nwAlert.ERROR)
                del self._treeMap[tHandle]
                return None

        else:
            byIndex = -1
            if nHandle is not None and nHandle in self._treeMap:
                try:
                    byIndex = self._treeMap[pHandle].indexOfChild(self._treeMap[nHandle])
                except Exception:
                    logger.error(""Failed to get index of item with handle '%s'"", nHandle)
            if byIndex >= 0:
                self._treeMap[pHandle].insertChild(byIndex+1, newItem)
            else:
                self._treeMap[pHandle].addChild(newItem)
            self.propagateCount(tHandle, nwItem.wordCount)

        self.setTreeItemValues(tHandle)
        newItem.setExpanded(nwItem.isExpanded)

        self._setTreeChanged(True)

        return newItem",nHandle is not None and nHandle in self._treeMap,None is not nHandle in self._treeMap
SpanBERT,https://github.com/facebookresearch/SpanBERT/tree/master/pretraining/fairseq/optim/bert_adam.py,BertAdam,__init__$89,"def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',
                 betas=(0.9, 0.999), e=1e-6, weight_decay=0.01,
                 max_grad_norm=1.0):
        b1 = betas[0]
        b2 = betas[1]
        if lr is not required and lr < 0.0:
            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))
        if schedule not in SCHEDULES:
            raise ValueError(""Invalid schedule parameter: {}"".format(schedule))
        if not 0.0 <= warmup < 1.0 and not warmup == -1:
            raise ValueError(""Invalid warmup: {} - should be in [0.0, 1.0[ or -1"".format(warmup))
        if not 0.0 <= b1 < 1.0:
            raise ValueError(""Invalid b1 parameter: {} - should be in [0.0, 1.0["".format(b1))
        if not 0.0 <= b2 < 1.0:
            raise ValueError(""Invalid b2 parameter: {} - should be in [0.0, 1.0["".format(b2))
        if not e >= 0.0:
            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(e))
        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,
                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,
                        max_grad_norm=max_grad_norm)
        super(BertAdam, self).__init__(params, defaults)",lr is not required and lr < 0.0,required is not lr < 0.0
scanpy,https://github.com/theislab/scanpy/tree/master/scanpy/tools/_rank_genes_groups.py,_RankGenes,t_test$197,"def t_test(self, method):
        from scipy import stats

        self._basic_stats()

        for group_index, mask in enumerate(self.groups_masks):
            if self.ireference is not None and group_index == self.ireference:
                continue

            mean_group = self.means[group_index]
            var_group = self.vars[group_index]
            ns_group = np.count_nonzero(mask)

            if self.ireference is not None:
                mean_rest = self.means[self.ireference]
                var_rest = self.vars[self.ireference]
                ns_other = np.count_nonzero(self.groups_masks[self.ireference])
            else:
                mean_rest = self.means_rest[group_index]
                var_rest = self.vars_rest[group_index]
                ns_other = self.X.shape[0] - ns_group

            if method == 't-test':
                ns_rest = ns_other
            elif method == 't-test_overestim_var':
                # hack for overestimating the variance for small groups
                ns_rest = ns_group
            else:
                raise ValueError('Method does not exist.')

            # TODO: Come up with better solution. Mask unexpressed genes?
            # See https://github.com/scipy/scipy/issues/10269
            with np.errstate(invalid=""ignore""):
                scores, pvals = stats.ttest_ind_from_stats(
                    mean1=mean_group,
                    std1=np.sqrt(var_group),
                    nobs1=ns_group,
                    mean2=mean_rest,
                    std2=np.sqrt(var_rest),
                    nobs2=ns_rest,
                    equal_var=False,  # Welch's
                )

            # I think it's only nan when means are the same and vars are 0
            scores[np.isnan(scores)] = 0
            # This also has to happen for Benjamini Hochberg
            pvals[np.isnan(pvals)] = 1

            yield group_index, scores, pvals",self.ireference is not None and group_index == self.ireference,None is not self.ireference == group_index
SparseR-CNN,https://github.com/PeizeSun/SparseR-CNN/tree/master/detectron2/export/shared.py,,_fuse_once$964,"def _fuse_once(predict_net):
        ssa, blob_versions = core.get_ssa(predict_net)
        consumer_map = get_consumer_map(ssa)
        versioned_external_output = [
            (name, blob_versions[name]) for name in predict_net.external_output
        ]

        for op_id, op in enumerate(predict_net.op):
            if op.type in _COPY_OPS:
                fw_copy_versioned_output = ssa[op_id][1][0]
                consumer_ids = [x[0] for x in consumer_map[fw_copy_versioned_output]]
                reverse_op_type = _COPY_OPS[1 - _COPY_OPS.index(op.type)]

                is_fusable = (
                    len(consumer_ids) > 0
                    and fw_copy_versioned_output not in versioned_external_output
                    and all(
                        predict_net.op[_op_id].type == reverse_op_type
                        and ssa[_op_id][1][0] not in versioned_external_output
                        for _op_id in consumer_ids
                    )
                )

                if is_fusable:
                    for rv_copy_op_id in consumer_ids:
                        # making each NextOp uses ""a"" directly and removing Copy ops
                        rs_copy_versioned_output = ssa[rv_copy_op_id][1][0]
                        next_op_id, inp_id = consumer_map[rs_copy_versioned_output][0]
                        predict_net.op[next_op_id].input[inp_id] = op.input[0]
                    # remove CopyOps
                    new_ops = [
                        op
                        for i, op in enumerate(predict_net.op)
                        if i != op_id and i not in consumer_ids
                    ]
                    del predict_net.op[:]
                    predict_net.op.extend(new_ops)
                    return True

        return False",i != op_id and i not in consumer_ids,op_id != i not in consumer_ids
lifter,https://github.com/agateblue/lifter/tree/master/lifter/lookups.py,value_range,lookup$131,"def lookup(self, value):
        return value >= self.start and value <= self.end",value >= self.start and value <= self.end,self.start <= value <= self.end
proplot,https://github.com/lukelbd/proplot/tree/master/proplot/demos.py,,_draw_bars$422,"def _draw_bars(
    cmaps, *, source, unknown='User', include=None, ignore=None,
    length=4.0, width=0.2, N=None, rasterized=None,
):
    """"""
    Draw colorbars for ""colormaps"" and ""color cycles"". This is called by
    `show_cycles` and `show_cmaps`.
    """"""
    # Categorize the input names
    table = {unknown: []} if unknown else {}
    table.update({cat: [None] * len(names) for cat, names in source.items()})
    for cmap in cmaps:
        cat = None
        name = cmap.name or '_no_name'
        name = name.lower()
        for opt, names in source.items():
            names = list(map(str.lower, names))
            if name in names:
                i, cat = names.index(name), opt
        if cat:
            table[cat][i] = cmap
        elif unknown:
            table[unknown].append(cmap)

    # Filter out certain categories
    options = set(map(str.lower, source))
    if ignore is None:
        ignore = ('matlab', 'gnuplot', 'gist', 'other')
    if isinstance(include, str):
        include = (include,)
    if isinstance(ignore, str):
        ignore = (ignore,)
    if include is None:
        include = options - set(map(str.lower, ignore))
    else:
        include = set(map(str.lower, include))
    if any(cat not in options and cat != unknown for cat in include):
        raise ValueError(
            f'Invalid categories {include!r}. Options are: '
            + ', '.join(map(repr, source)) + '.'
        )
    for cat in tuple(table):
        table[cat][:] = [cmap for cmap in table[cat] if cmap is not None]
        if not table[cat] or cat.lower() not in include and cat != unknown:
            del table[cat]

    # Draw figure
    # Allocate two colorbar widths for each title of sections
    naxs = 2 * len(table) + sum(map(len, table.values()))
    fig, axs = ui.subplots(
        refwidth=length, refheight=width,
        nrows=naxs, share=False, hspace='2pt', top='-1em',
    )
    i = -1
    nheads = nbars = 0  # for deciding which axes to plot in
    for cat, cmaps in table.items():
        nheads += 1
        for j, cmap in enumerate(cmaps):
            i += 1
            if j + nheads + nbars > naxs:
                break
            if j == 0:  # allocate this axes for title
                i += 2
                for ax in axs[i - 2:i]:
                    ax.set_visible(False)
            ax = axs[i]
            if N is not None:
                cmap = cmap.copy(N=N)
            label = cmap.name
            label = re.sub(r'\A_*', '', label)
            label = re.sub(r'(_copy)*\Z', '', label)
            ax.colorbar(
                cmap, loc='fill', orientation='horizontal',
                locator='null', linewidth=0, rasterized=rasterized,
            )
            ax.text(
                0 - (rc['axes.labelpad'] / 72) / length, 0.45, label,
                ha='right', va='center', transform='axes',
            )
            if j == 0:
                ax.set_title(cat, weight='bold')
        nbars += len(cmaps)

    return fig, axs",cat not in options and cat != unknown,unknown != cat not in options
node-gyp,https://github.com/nodejs/node-gyp/tree/master/gyp/pylib/gyp/generator/make.py,MakefileWriter,WriteTarget$1571,"def WriteTarget(
        self, spec, configs, deps, link_deps, bundle_deps, extra_outputs, part_of_all
    ):
        """"""Write Makefile code to produce the final target of the gyp spec.

        spec, configs: input from gyp.
        deps, link_deps: dependency lists; see ComputeDeps()
        extra_outputs: any extra outputs that our target should depend on
        part_of_all: flag indicating this target is part of 'all'
        """"""

        self.WriteLn(""### Rules for final target."")

        if extra_outputs:
            self.WriteDependencyOnExtraOutputs(self.output_binary, extra_outputs)
            self.WriteMakeRule(
                extra_outputs,
                deps,
                comment=(""Preserve order dependency of "" ""special output on deps.""),
                order_only=True,
            )

        target_postbuilds = {}
        if self.type != ""none"":
            for configname in sorted(configs.keys()):
                config = configs[configname]
                if self.flavor == ""mac"":
                    ldflags = self.xcode_settings.GetLdflags(
                        configname,
                        generator_default_variables[""PRODUCT_DIR""],
                        lambda p: Sourceify(self.Absolutify(p)),
                        arch=config.get(""xcode_configuration_platform""),
                    )

                    # TARGET_POSTBUILDS_$(BUILDTYPE) is added to postbuilds later on.
                    gyp_to_build = gyp.common.InvertRelativePath(self.path)
                    target_postbuild = self.xcode_settings.AddImplicitPostbuilds(
                        configname,
                        QuoteSpaces(
                            os.path.normpath(os.path.join(gyp_to_build, self.output))
                        ),
                        QuoteSpaces(
                            os.path.normpath(
                                os.path.join(gyp_to_build, self.output_binary)
                            )
                        ),
                    )
                    if target_postbuild:
                        target_postbuilds[configname] = target_postbuild
                else:
                    ldflags = config.get(""ldflags"", [])
                    # Compute an rpath for this output if needed.
                    if any(dep.endswith("".so"") or "".so."" in dep for dep in deps):
                        # We want to get the literal string ""$ORIGIN""
                        # into the link command, so we need lots of escaping.
                        ldflags.append(r""-Wl,-rpath=\$$ORIGIN/"")
                        ldflags.append(r""-Wl,-rpath-link=\$(builddir)/"")
                library_dirs = config.get(""library_dirs"", [])
                ldflags += [(""-L%s"" % library_dir) for library_dir in library_dirs]
                self.WriteList(ldflags, ""LDFLAGS_%s"" % configname)
                if self.flavor == ""mac"":
                    self.WriteList(
                        self.xcode_settings.GetLibtoolflags(configname),
                        ""LIBTOOLFLAGS_%s"" % configname,
                    )
            libraries = spec.get(""libraries"")
            if libraries:
                # Remove duplicate entries
                libraries = gyp.common.uniquer(libraries)
                if self.flavor == ""mac"":
                    libraries = self.xcode_settings.AdjustLibraries(libraries)
            self.WriteList(libraries, ""LIBS"")
            self.WriteLn(
                ""%s: GYP_LDFLAGS := $(LDFLAGS_$(BUILDTYPE))""
                % QuoteSpaces(self.output_binary)
            )
            self.WriteLn(""%s: LIBS := $(LIBS)"" % QuoteSpaces(self.output_binary))

            if self.flavor == ""mac"":
                self.WriteLn(
                    ""%s: GYP_LIBTOOLFLAGS := $(LIBTOOLFLAGS_$(BUILDTYPE))""
                    % QuoteSpaces(self.output_binary)
                )

        # Postbuild actions. Like actions, but implicitly depend on the target's
        # output.
        postbuilds = []
        if self.flavor == ""mac"":
            if target_postbuilds:
                postbuilds.append(""$(TARGET_POSTBUILDS_$(BUILDTYPE))"")
            postbuilds.extend(gyp.xcode_emulation.GetSpecPostbuildCommands(spec))

        if postbuilds:
            # Envvars may be referenced by TARGET_POSTBUILDS_$(BUILDTYPE),
            # so we must output its definition first, since we declare variables
            # using "":="".
            self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())

            for configname in target_postbuilds:
                self.WriteLn(
                    ""%s: TARGET_POSTBUILDS_%s := %s""
                    % (
                        QuoteSpaces(self.output),
                        configname,
                        gyp.common.EncodePOSIXShellList(target_postbuilds[configname]),
                    )
                )

            # Postbuilds expect to be run in the gyp file's directory, so insert an
            # implicit postbuild to cd to there.
            postbuilds.insert(0, gyp.common.EncodePOSIXShellList([""cd"", self.path]))
            for i, postbuild in enumerate(postbuilds):
                if not postbuild.startswith(""$""):
                    postbuilds[i] = EscapeShellArgument(postbuild)
            self.WriteLn(""%s: builddir := $(abs_builddir)"" % QuoteSpaces(self.output))
            self.WriteLn(
                ""%s: POSTBUILDS := %s""
                % (QuoteSpaces(self.output), "" "".join(postbuilds))
            )

        # A bundle directory depends on its dependencies such as bundle resources
        # and bundle binary. When all dependencies have been built, the bundle
        # needs to be packaged.
        if self.is_mac_bundle:
            # If the framework doesn't contain a binary, then nothing depends
            # on the actions -- make the framework depend on them directly too.
            self.WriteDependencyOnExtraOutputs(self.output, extra_outputs)

            # Bundle dependencies. Note that the code below adds actions to this
            # target, so if you move these two lines, move the lines below as well.
            self.WriteList([QuoteSpaces(dep) for dep in bundle_deps], ""BUNDLE_DEPS"")
            self.WriteLn(""%s: $(BUNDLE_DEPS)"" % QuoteSpaces(self.output))

            # After the framework is built, package it. Needs to happen before
            # postbuilds, since postbuilds depend on this.
            if self.type in (""shared_library"", ""loadable_module""):
                self.WriteLn(
                    ""\t@$(call do_cmd,mac_package_framework,,,%s)""
                    % self.xcode_settings.GetFrameworkVersion()
                )

            # Bundle postbuilds can depend on the whole bundle, so run them after
            # the bundle is packaged, not already after the bundle binary is done.
            if postbuilds:
                self.WriteLn(""\t@$(call do_postbuilds)"")
            postbuilds = []  # Don't write postbuilds for target's output.

            # Needed by test/mac/gyptest-rebuild.py.
            self.WriteLn(""\t@true  # No-op, used by tests"")

            # Since this target depends on binary and resources which are in
            # nested subfolders, the framework directory will be older than
            # its dependencies usually. To prevent this rule from executing
            # on every build (expensive, especially with postbuilds), expliclity
            # update the time on the framework directory.
            self.WriteLn(""\t@touch -c %s"" % QuoteSpaces(self.output))

        if postbuilds:
            assert not self.is_mac_bundle, (
                ""Postbuilds for bundles should be done ""
                ""on the bundle, not the binary (target '%s')"" % self.target
            )
            assert ""product_dir"" not in spec, (
                ""Postbuilds do not work with "" ""custom product_dir""
            )

        if self.type == ""executable"":
            self.WriteLn(
                ""%s: LD_INPUTS := %s""
                % (
                    QuoteSpaces(self.output_binary),
                    "" "".join(QuoteSpaces(dep) for dep in link_deps),
                )
            )
            if self.toolset == ""host"" and self.flavor == ""android"":
                self.WriteDoCmd(
                    [self.output_binary],
                    link_deps,
                    ""link_host"",
                    part_of_all,
                    postbuilds=postbuilds,
                )
            else:
                self.WriteDoCmd(
                    [self.output_binary],
                    link_deps,
                    ""link"",
                    part_of_all,
                    postbuilds=postbuilds,
                )

        elif self.type == ""static_library"":
            for link_dep in link_deps:
                assert "" "" not in link_dep, (
                    ""Spaces in alink input filenames not supported (%s)"" % link_dep
                )
            if (
                self.flavor not in (""mac"", ""openbsd"", ""netbsd"", ""win"")
                and not self.is_standalone_static_library
            ):
                self.WriteDoCmd(
                    [self.output_binary],
                    link_deps,
                    ""alink_thin"",
                    part_of_all,
                    postbuilds=postbuilds,
                )
            else:
                self.WriteDoCmd(
                    [self.output_binary],
                    link_deps,
                    ""alink"",
                    part_of_all,
                    postbuilds=postbuilds,
                )
        elif self.type == ""shared_library"":
            self.WriteLn(
                ""%s: LD_INPUTS := %s""
                % (
                    QuoteSpaces(self.output_binary),
                    "" "".join(QuoteSpaces(dep) for dep in link_deps),
                )
            )
            self.WriteDoCmd(
                [self.output_binary],
                link_deps,
                ""solink"",
                part_of_all,
                postbuilds=postbuilds,
            )
        elif self.type == ""loadable_module"":
            for link_dep in link_deps:
                assert "" "" not in link_dep, (
                    ""Spaces in module input filenames not supported (%s)"" % link_dep
                )
            if self.toolset == ""host"" and self.flavor == ""android"":
                self.WriteDoCmd(
                    [self.output_binary],
                    link_deps,
                    ""solink_module_host"",
                    part_of_all,
                    postbuilds=postbuilds,
                )
            else:
                self.WriteDoCmd(
                    [self.output_binary],
                    link_deps,
                    ""solink_module"",
                    part_of_all,
                    postbuilds=postbuilds,
                )
        elif self.type == ""none"":
            # Write a stamp line.
            self.WriteDoCmd(
                [self.output_binary], deps, ""touch"", part_of_all, postbuilds=postbuilds
            )
        else:
            print(""WARNING: no output for"", self.type, self.target)

        # Add an alias for each target (if there are any outputs).
        # Installable target aliases are created below.
        if (self.output and self.output != self.target) and (
            self.type not in self._INSTALLABLE_TARGETS
        ):
            self.WriteMakeRule(
                [self.target], [self.output], comment=""Add target alias"", phony=True
            )
            if part_of_all:
                self.WriteMakeRule(
                    [""all""],
                    [self.target],
                    comment='Add target alias to ""all"" target.',
                    phony=True,
                )

        # Add special-case rules for our installable targets.
        # 1) They need to install to the build dir or ""product"" dir.
        # 2) They get shortcuts for building (e.g. ""make chrome"").
        # 3) They are part of ""make all"".
        if self.type in self._INSTALLABLE_TARGETS or self.is_standalone_static_library:
            if self.type == ""shared_library"":
                file_desc = ""shared library""
            elif self.type == ""static_library"":
                file_desc = ""static library""
            else:
                file_desc = ""executable""
            install_path = self._InstallableTargetInstallPath()
            installable_deps = [self.output]
            if (
                self.flavor == ""mac""
                and ""product_dir"" not in spec
                and self.toolset == ""target""
            ):
                # On mac, products are created in install_path immediately.
                assert install_path == self.output, ""{} != {}"".format(
                    install_path,
                    self.output,
                )

            # Point the target alias to the final binary output.
            self.WriteMakeRule(
                [self.target], [install_path], comment=""Add target alias"", phony=True
            )
            if install_path != self.output:
                assert not self.is_mac_bundle  # See comment a few lines above.
                self.WriteDoCmd(
                    [install_path],
                    [self.output],
                    ""copy"",
                    comment=""Copy this to the %s output path."" % file_desc,
                    part_of_all=part_of_all,
                )
                installable_deps.append(install_path)
            if self.output != self.alias and self.alias != self.target:
                self.WriteMakeRule(
                    [self.alias],
                    installable_deps,
                    comment=""Short alias for building this %s."" % file_desc,
                    phony=True,
                )
            if part_of_all:
                self.WriteMakeRule(
                    [""all""],
                    [install_path],
                    comment='Add %s to ""all"" target.' % file_desc,
                    phony=True,
                )",self.output != self.alias and self.alias != self.target,self.output != self.alias != self.target
PyHive,https://github.com/dropbox/PyHive/tree/master/TCLIService/TCLIService.py,GetOperationStatus_args,read$2914,"def read(self, iprot):
        if iprot._fast_decode is not None and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None:
            iprot._fast_decode(self, iprot, (self.__class__, self.thrift_spec))
            return
        iprot.readStructBegin()
        while True:
            (fname, ftype, fid) = iprot.readFieldBegin()
            if ftype == TType.STOP:
                break
            if fid == 1:
                if ftype == TType.STRUCT:
                    self.req = TGetOperationStatusReq()
                    self.req.read(iprot)
                else:
                    iprot.skip(ftype)
            else:
                iprot.skip(ftype)
            iprot.readFieldEnd()
        iprot.readStructEnd()","iprot._fast_decode is not None and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None)","iprot._fast_decode is not None is not self.thrift_spec and isinstance(iprot.trans, TTransport.CReadableTransport)"
qutebrowser,https://github.com/qutebrowser/qutebrowser/tree/master/tests/end2end/fixtures/quteprocess.py,QuteProc,_process_line$478,"def _process_line(self, log_line):
        """"""Check if the line matches any initial lines we're interested in.""""""
        start_okay_message = (
            ""load status for <qutebrowser.browser.* tab_id=0 ""
            ""url='about:blank'>: LoadStatus.success"")

        if (log_line.category == 'ipc' and
                log_line.message.startswith(""Listening as "")):
            self._ipc_socket = log_line.message.split(' ', maxsplit=2)[2]
        elif (log_line.category == 'webview' and
              testutils.pattern_match(pattern=start_okay_message,
                                      value=log_line.message)):
            log_line.waited_for = True
            self.ready.emit()
        elif (log_line.category == 'init' and
              log_line.module == 'standarddir' and
              log_line.function == 'init' and
              log_line.message.startswith('Base directory:')):
            self.basedir = log_line.message.split(':', maxsplit=1)[1].strip()
        elif self._is_error_logline(log_line):
            self.got_error.emit()",log_line.category == 'init' and log_line.module == 'standarddir' and (log_line.function == 'init') and log_line.message.startswith('Base directory:'),log_line.category == 'init' == log_line.function and log_line.module == 'standarddir' and log_line.message.startswith('Base directory:')
marathon-lb,https://github.com/mesosphere/marathon-lb/tree/master//marathon_lb.py,,get_apps$1624,"def get_apps(marathon, apps=[]):
    if len(apps) == 0:
        apps = marathon.list()

    logger.debug(""got apps %s"", [app[""id""] for app in apps])

    marathon_apps = []
    # This process requires 2 passes: the first is to gather apps belonging
    # to a deployment group.
    processed_apps = []
    deployment_groups = {}

    for app in apps:
        deployment_group = None
        if 'HAPROXY_DEPLOYMENT_GROUP' in app['labels']:
            deployment_group = app['labels']['HAPROXY_DEPLOYMENT_GROUP']
            # mutate the app id to match deployment group
            if deployment_group[0] != '/':
                deployment_group = '/' + deployment_group
            app['id'] = deployment_group
        else:
            processed_apps.append(app)
            continue
        if deployment_group in deployment_groups:
            # merge the groups, with the oldest taking precedence
            prev = deployment_groups[deployment_group]
            cur = app

            # If for some reason neither label is set correctly, then it's a
            # crapshoot. Most likely, whichever one is unset was not deployed
            # with ZDD, so we should prefer the one with a date set.
            cur_date = datetime.datetime.min
            prev_date = datetime.datetime.min
            if 'HAPROXY_DEPLOYMENT_STARTED_AT' in prev['labels']:
                prev_date = dateutil.parser.parse(
                    prev['labels']['HAPROXY_DEPLOYMENT_STARTED_AT'])

            if 'HAPROXY_DEPLOYMENT_STARTED_AT' in cur['labels']:
                cur_date = dateutil.parser.parse(
                    cur['labels']['HAPROXY_DEPLOYMENT_STARTED_AT'])

            old = new = None
            if prev_date < cur_date:
                old = prev
                new = cur
            else:
                new = prev
                old = cur

            if 'HAPROXY_DEPLOYMENT_NEW_INSTANCES' in new['labels']:
                if int(new['labels']['HAPROXY_DEPLOYMENT_NEW_INSTANCES'] != 0):
                    new_scale_time = dateutil.parser.parse(
                        new['versionInfo']['lastScalingAt'])
                    old_scale_time = dateutil.parser.parse(
                        old['versionInfo']['lastScalingAt'])
                    if old_scale_time > new_scale_time:
                        temp = old
                        old = new
                        new = temp

                target_instances = \
                    int(new['labels']['HAPROXY_DEPLOYMENT_TARGET_INSTANCES'])

            else:
                target_instances = 1

            # Mark N tasks from old app as draining, where N is the
            # number of instances in the new app.  Sort the old tasks so that
            # order is deterministic (i.e. so that we always drain the same
            # tasks).
            old_tasks = sorted(old['tasks'], key=lambda task: task['id'])

            healthy_new_instances = 0
            if 'healthChecks' in app and len(app['healthChecks']) > 0:
                for task in new['tasks']:
                    if 'healthCheckResults' not in task:
                        continue
                    alive = False
                    for result in task['healthCheckResults']:
                        if result['alive']:
                            alive = True
                    if alive:
                        healthy_new_instances += 1
            else:
                healthy_new_instances = new['instances']

            maximum_drainable = \
                max(0, (healthy_new_instances + old['instances']) -
                    target_instances)

            for i in range(0, min(len(old_tasks),
                                  healthy_new_instances,
                                  maximum_drainable)):
                old_tasks[i]['draining'] = True

            # merge tasks from new app into old app
            merged = old
            old_tasks.extend(new['tasks'])
            merged['tasks'] = old_tasks

            deployment_groups[deployment_group] = merged
        else:
            deployment_groups[deployment_group] = app

    processed_apps.extend(deployment_groups.values())

    # Reset the service port assigner.  This forces the port assigner to
    # re-assign ports for IP-per-task applications.  The upshot is that
    # the service port for a particular app may change dynamically, but
    # the service port will be deterministic and identical across all
    # instances of the marathon-lb.
    SERVICE_PORT_ASSIGNER.reset()

    for app in processed_apps:
        appId = app['id']
        if appId[1:] == os.environ.get(""FRAMEWORK_NAME""):
            continue

        marathon_app = MarathonApp(marathon, appId, app)

        if 'HAPROXY_GROUP' in marathon_app.app['labels']:
            marathon_app.groups = \
                marathon_app.app['labels']['HAPROXY_GROUP'].split(',')
        marathon_apps.append(marathon_app)

        service_ports = SERVICE_PORT_ASSIGNER.get_service_ports(app)
        for i, servicePort in enumerate(service_ports):
            if servicePort is None:
                logger.warning(""Skipping undefined service port"")
                continue

            service = MarathonService(appId, servicePort,
                                      get_health_check(app, i),
                                      marathon.strict_mode())

            for key_unformatted in label_keys:
                key = key_unformatted.format(i)
                if key in marathon_app.app['labels']:
                    func = label_keys[key_unformatted]
                    func(service,
                         key_unformatted,
                         marathon_app.app['labels'][key])

            # https://github.com/mesosphere/marathon-lb/issues/198
            # Marathon app manifest which defines healthChecks is
            # defined for a specific given service port identified
            # by either a port or portIndex.
            # (Marathon itself will prefer port before portIndex
            # https://mesosphere.github.io/marathon/docs/health-checks.html)
            #
            # We want to be able to instruct HAProxy
            # to use health check defined for service port B
            # in marathon to indicate service port A is healthy
            # or not for service port A in HAProxy.
            #
            # This is done by specifying a label:
            #  HAPROXY_{n}_BACKEND_HEALTHCHECK_PORT_INDEX
            #
            # TODO(norangshol) Refactor and supply MarathonService
            # TODO(norangshol) with its labels and do this in its constructor?
            if service.healthCheck is None \
                    and service.healthcheck_port_index is not None:
                service.healthCheck = \
                    get_health_check(app, service.healthcheck_port_index)
                if service.healthCheck:
                    healthProto = service.healthCheck['protocol']
                    if healthProto in ['HTTP', 'HTTPS', 'MESOS_HTTP',
                                       'MESOS_HTTPS']:
                        service.mode = 'http'

            marathon_app.services[servicePort] = service

        for task in app['tasks']:
            # Marathon 0.7.6 bug workaround
            if not task['host']:
                logger.warning(""Ignoring Marathon task without host "" +
                               task['id'])
                continue

            # 'state' will not be present in test cases.
            # Should always be present in an actual cluster
            if 'state' in task and task['state'] in EXCLUDED_TASK_STATES:
                logger.warning(""Ignoring non-running task "" + task['id'] +
                               "" with state "" + task['state'])
                continue

            if marathon.health_check() and 'healthChecks' in app and \
               len(app['healthChecks']) > 0:
                alive = False
                if 'healthCheckResults' not in task:
                    # use previously cached result, if it exists
                    if not healthCheckResultCache.get(task['id'], False):
                        continue
                else:
                    for result in task['healthCheckResults']:
                        if result['alive']:
                            alive = True
                    healthCheckResultCache.set(task['id'], alive)
                    if not alive:
                        continue

            task_ip, task_ports = get_task_ip_and_ports(app, task)
            if task_ip is None:
                logger.warning(""Task has no resolvable IP address - skip"")
                continue

            draining = task.get('draining', False)

            # if different versions of app have different number of ports,
            # try to match as many ports as possible
            for task_port, service_port in zip(task_ports, service_ports):
                service = marathon_app.services.get(service_port, None)
                if service:
                    service.groups = marathon_app.groups
                    service.add_backend(task['host'],
                                        task_ip,
                                        task_port,
                                        draining)

    # Convert into a list for easier consumption
    apps_list = []
    for marathon_app in marathon_apps:
        for service in list(marathon_app.services.values()):
            if service.backends:
                apps_list.append(service)

    return apps_list",service.healthCheck is None and service.healthcheck_port_index is not None,service.healthCheck is None is not service.healthcheck_port_index
Open3D-ML,https://github.com/isl-org/Open3D-ML/tree/master/ml3d/tf/utils/pointnet/pointnet2_modules.py,_PointnetSAModuleBase,call$17,"def call(self, xyz, features=None, new_xyz=None, training=True):
        r""""""
        :param xyz: (B, N, 3) tensor of the xyz coordinates of the features
        :param features: (B, N, C) tensor of the descriptors of the the features
        :param new_xyz:
        :return:
            new_xyz: (B, npoint, 3) tensor of the new features' xyz
            new_features: (B, npoint, \sum_k(mlps[k][-1])) tensor of the new_features descriptors
        """"""
        new_features_list = []

        if new_xyz is None and self.npoint is not None:
            sampling = tf.expand_dims(pointnet2_utils.furthest_point_sample(
                xyz, self.npoint),
                                      axis=-1)
            new_xyz = tf.gather_nd(xyz, sampling, batch_dims=1)

        for i in range(len(self.groupers)):
            new_features = self.groupers[i](xyz, new_xyz,
                                            features)  # (B, C, npoint, nsample)

            new_features = self.mlps[i](
                new_features,
                training=training)  # (B, mlp[-1], npoint, nsample)
            if self.pool_method == 'max_pool':
                new_features = tf.nn.max_pool2d(new_features,
                                                kernel_size=[
                                                    1, new_features.size(3)
                                                ])  # (B, mlp[-1], npoint, 1)
            elif self.pool_method == 'avg_pool':
                new_features = tf.nn.avg_pool2d(new_features,
                                                kernel_size=[
                                                    1, new_features.size(3)
                                                ])  # (B, mlp[-1], npoint, 1)
            else:
                raise NotImplementedError

            new_features = tf.squeeze(new_features,
                                      axis=-1)  # (B, mlp[-1], npoint)
            new_features_list.append(new_features)

        return new_xyz, tf.concat(new_features_list, axis=1)",new_xyz is None and self.npoint is not None,new_xyz is None is not self.npoint
khard,https://github.com/scheibler/khard/tree/master/khard/carddav_object.py,YAMLEditable,_format_date_object$928,"def _format_date_object(date: Optional[Date], localize: bool) -> str:
        if not date:
            return """"
        if isinstance(date, str):
            return date
        if date.year == 1900 and date.month != 0 and date.day != 0 \
                and date.hour == 0 and date.minute == 0 and date.second == 0:
            return date.strftime(""--%m-%d"")
        tz = date.tzname()
        if (tz and tz[3:]) or (date.hour != 0 or date.minute != 0
                               or date.second != 0):
            if localize:
                return date.strftime(locale.nl_langinfo(locale.D_T_FMT))
            utc_offset = -time.timezone / 60 / 60
            return date.strftime(""%FT%T+{:02}:00"".format(int(utc_offset)))
        if localize:
            return date.strftime(locale.nl_langinfo(locale.D_FMT))
        return date.strftime(""%F"")",date.year == 1900 and date.month != 0 and (date.day != 0) and (date.hour == 0) and (date.minute == 0) and (date.second == 0),date.year == 1900 and date.second == 0 and (date.month != 0 != date.day) and (date.hour == 0 == date.minute)
ARL,https://github.com/TophantTechnology/ARL/tree/master/app/utils/IPy.py,IPint,overlaps$656,"def overlaps(self, item):
        """"""Check if two IP address ranges overlap.

        Returns 0 if the two ranges don't overlap, 1 if the given
        range overlaps at the end and -1 if it does at the beginning.

        >>> IP('192.168.0.0/23').overlaps('192.168.1.0/24')
        1
        >>> IP('192.168.0.0/23').overlaps('192.168.1.255')
        1
        >>> IP('192.168.0.0/23').overlaps('192.168.2.0')
        0
        >>> IP('192.168.1.0/24').overlaps('192.168.0.0/23')
        -1
        """"""

        if not isinstance(item, IP):
            item = IP(item)
        if item.ip >= self.ip and item.ip < self.ip + self.len():
            return 1
        elif self.ip >= item.ip and self.ip < item.ip + item.len():
            return -1
        else:
            return 0",item.ip >= self.ip and item.ip < self.ip + self.len(),self.ip <= item.ip < self.ip + self.len()
ARL,https://github.com/TophantTechnology/ARL/tree/master/app/utils/IPy.py,IPint,overlaps$656,"def overlaps(self, item):
        """"""Check if two IP address ranges overlap.

        Returns 0 if the two ranges don't overlap, 1 if the given
        range overlaps at the end and -1 if it does at the beginning.

        >>> IP('192.168.0.0/23').overlaps('192.168.1.0/24')
        1
        >>> IP('192.168.0.0/23').overlaps('192.168.1.255')
        1
        >>> IP('192.168.0.0/23').overlaps('192.168.2.0')
        0
        >>> IP('192.168.1.0/24').overlaps('192.168.0.0/23')
        -1
        """"""

        if not isinstance(item, IP):
            item = IP(item)
        if item.ip >= self.ip and item.ip < self.ip + self.len():
            return 1
        elif self.ip >= item.ip and self.ip < item.ip + item.len():
            return -1
        else:
            return 0",self.ip >= item.ip and self.ip < item.ip + item.len(),item.ip <= self.ip < item.ip + item.len()
httplib2,https://github.com/httplib2/httplib2/tree/master/python3/httplib2/socks.py,socksocket,__negotiatesocks5$251,"def __negotiatesocks5(self, destaddr, destport):
        """"""__negotiatesocks5(self,destaddr,destport)
        Negotiates a connection through a SOCKS5 server.
        """"""
        # First we'll send the authentication packages we support.
        if (self.__proxy[4] != None) and (self.__proxy[5] != None):
            # The username/password details were supplied to the
            # setproxy method so we support the USERNAME/PASSWORD
            # authentication (in addition to the standard none).
            self.sendall(struct.pack(""BBBB"", 0x05, 0x02, 0x00, 0x02))
        else:
            # No username/password were entered, therefore we
            # only support connections with no authentication.
            self.sendall(struct.pack(""BBB"", 0x05, 0x01, 0x00))
        # We'll receive the server's response to determine which
        # method was selected
        chosenauth = self.__recvall(2)
        if chosenauth[0:1] != chr(0x05).encode():
            self.close()
            raise GeneralProxyError((1, _generalerrors[1]))
        # Check the chosen authentication method
        if chosenauth[1:2] == chr(0x00).encode():
            # No authentication is required
            pass
        elif chosenauth[1:2] == chr(0x02).encode():
            # Okay, we need to perform a basic username/password
            # authentication.
            packet = bytearray()
            packet.append(0x01)
            packet.append(len(self.__proxy[4]))
            packet.extend(self.__proxy[4])
            packet.append(len(self.__proxy[5]))
            packet.extend(self.__proxy[5])
            self.sendall(packet)
            authstat = self.__recvall(2)
            if authstat[0:1] != chr(0x01).encode():
                # Bad response
                self.close()
                raise GeneralProxyError((1, _generalerrors[1]))
            if authstat[1:2] != chr(0x00).encode():
                # Authentication failed
                self.close()
                raise Socks5AuthError((3, _socks5autherrors[3]))
            # Authentication succeeded
        else:
            # Reaching here is always bad
            self.close()
            if chosenauth[1] == chr(0xFF).encode():
                raise Socks5AuthError((2, _socks5autherrors[2]))
            else:
                raise GeneralProxyError((1, _generalerrors[1]))
        # Now we can request the actual connection
        req = struct.pack(""BBB"", 0x05, 0x01, 0x00)
        # If the given destination address is an IP address, we'll
        # use the IPv4 address request even if remote resolving was specified.
        try:
            ipaddr = socket.inet_aton(destaddr)
            req = req + chr(0x01).encode() + ipaddr
        except socket.error:
            # Well it's not an IP number,  so it's probably a DNS name.
            if self.__proxy[3]:
                # Resolve remotely
                ipaddr = None
                req = (
                    req
                    + chr(0x03).encode()
                    + chr(len(destaddr)).encode()
                    + destaddr.encode()
                )
            else:
                # Resolve locally
                ipaddr = socket.inet_aton(socket.gethostbyname(destaddr))
                req = req + chr(0x01).encode() + ipaddr
        req = req + struct.pack("">H"", destport)
        self.sendall(req)
        # Get the response
        resp = self.__recvall(4)
        if resp[0:1] != chr(0x05).encode():
            self.close()
            raise GeneralProxyError((1, _generalerrors[1]))
        elif resp[1:2] != chr(0x00).encode():
            # Connection failed
            self.close()
            if ord(resp[1:2]) <= 8:
                raise Socks5Error((ord(resp[1:2]), _socks5errors[ord(resp[1:2])]))
            else:
                raise Socks5Error((9, _socks5errors[9]))
        # Get the bound address/port
        elif resp[3:4] == chr(0x01).encode():
            boundaddr = self.__recvall(4)
        elif resp[3:4] == chr(0x03).encode():
            resp = resp + self.recv(1)
            boundaddr = self.__recvall(ord(resp[4:5]))
        else:
            self.close()
            raise GeneralProxyError((1, _generalerrors[1]))
        boundport = struct.unpack("">H"", self.__recvall(2))[0]
        self.__proxysockname = (boundaddr, boundport)
        if ipaddr != None:
            self.__proxypeername = (socket.inet_ntoa(ipaddr), destport)
        else:
            self.__proxypeername = (destaddr, destport)",self.__proxy[4] != None and self.__proxy[5] != None,self.__proxy[4] != None != self.__proxy[5]
Sprytile,https://github.com/Sprytile/Sprytile/tree/master//sprytile_modal.py,VIEW3D_OP_SprytileModalTool,get_face_tiledata$107,"def get_face_tiledata(bmesh, face):
        grid_id_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_INDEX)
        tile_id_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_TILE_ID)
        if grid_id_layer is None or tile_id_layer is None:
            return None, None, None, None, None

        grid_id = face[grid_id_layer]
        tile_packed_id = face[tile_id_layer]

        width = 1
        width_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_SEL_WIDTH)
        if width_layer is not None:
            width = face[width_layer]
            if width is None:
                width = 1

        height = 1
        height_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_SEL_HEIGHT)
        if height_layer is not None:
            height = face[height_layer]
            if height is None:
                height = 1

        origin = -1
        origin_layer = bmesh.faces.layers.int.get(UvDataLayers.GRID_SEL_ORIGIN)
        if origin_layer is not None:
            origin = face[origin_layer]
            if origin is None:
                origin = -1

        # For backwards compatibility. Origin/width/height
        # did not exist before 0.4.2
        if origin == 0 and height == 0 and width == 0:
            origin = tile_packed_id
        height = max(1, height)
        width = max(1, width)

        # print(""get tile data - grid:{0}, tile_id:{1}, w:{2}, h:{3}, o:{4}""
        #       .format(grid_id, tile_packed_id, width, height, origin))
        return grid_id, tile_packed_id, width, height, origin",origin == 0 and height == 0 and (width == 0),origin == 0 == height and width == 0
R-Drop,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/generation_beam_search.py,BeamSearchScorer,process$199,"def process(
        self,
        input_ids: torch.LongTensor,
        next_scores: torch.FloatTensor,
        next_tokens: torch.LongTensor,
        next_indices: torch.LongTensor,
        pad_token_id: Optional[int] = None,
        eos_token_id: Optional[int] = None,
    ) -> Tuple[torch.Tensor]:
        cur_len = input_ids.shape[-1]
        batch_size = len(self._beam_hyps)
        assert batch_size == (input_ids.shape[0] // self.group_size)

        device = input_ids.device
        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)
        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)
        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)

        for batch_idx, beam_hyp in enumerate(self._beam_hyps):
            if self._done[batch_idx]:
                assert (
                    len(beam_hyp) >= self.num_beams
                ), f""Batch can only be done if at least {self.num_beams} beams have been generated""
                assert (
                    eos_token_id is not None and pad_token_id is not None
                ), ""generated beams >= num_beams -> eos_token_id and pad_token have to be defined""
                # pad the batch
                next_beam_scores[batch_idx, :] = 0
                next_beam_tokens[batch_idx, :] = pad_token_id
                next_beam_indices[batch_idx, :] = 0
                continue

            # next tokens for this sentence
            beam_idx = 0
            for beam_token_rank, (next_token, next_score, next_index) in enumerate(
                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])
            ):
                batch_beam_idx = batch_idx * self.group_size + next_index
                # add to generated hypotheses if end of sentence
                if (eos_token_id is not None) and (next_token.item() == eos_token_id):
                    # if beam_token does not belong to top num_beams tokens, it should not be added
                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size
                    if is_beam_token_worse_than_top_num_beams:
                        continue
                    beam_hyp.add(
                        input_ids[batch_beam_idx].clone(),
                        next_score.item(),
                    )
                else:
                    # add next predicted token since it is not eos_token
                    next_beam_scores[batch_idx, beam_idx] = next_score
                    next_beam_tokens[batch_idx, beam_idx] = next_token
                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx
                    beam_idx += 1

                # once the beam for next step is full, don't add more tokens to it.
                if beam_idx == self.group_size:
                    break

            if beam_idx < self.group_size:
                raise ValueError(
                    f""At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id: {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.""
                )

            # Check if we are done so that we can save a pad step if all(done)
            self._done[batch_idx] = self._done[batch_idx] or beam_hyp.is_done(
                next_scores[batch_idx].max().item(), cur_len
            )

        return UserDict(
            {
                ""next_beam_scores"": next_beam_scores.view(-1),
                ""next_beam_tokens"": next_beam_tokens.view(-1),
                ""next_beam_indices"": next_beam_indices.view(-1),
            }
        )",eos_token_id is not None and pad_token_id is not None,eos_token_id is not None is not pad_token_id
R-Drop,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/generation_beam_search.py,BeamSearchScorer,process$199,"def process(
        self,
        input_ids: torch.LongTensor,
        next_scores: torch.FloatTensor,
        next_tokens: torch.LongTensor,
        next_indices: torch.LongTensor,
        pad_token_id: Optional[int] = None,
        eos_token_id: Optional[int] = None,
    ) -> Tuple[torch.Tensor]:
        cur_len = input_ids.shape[-1]
        batch_size = len(self._beam_hyps)
        assert batch_size == (input_ids.shape[0] // self.group_size)

        device = input_ids.device
        next_beam_scores = torch.zeros((batch_size, self.group_size), dtype=next_scores.dtype, device=device)
        next_beam_tokens = torch.zeros((batch_size, self.group_size), dtype=next_tokens.dtype, device=device)
        next_beam_indices = torch.zeros((batch_size, self.group_size), dtype=next_indices.dtype, device=device)

        for batch_idx, beam_hyp in enumerate(self._beam_hyps):
            if self._done[batch_idx]:
                assert (
                    len(beam_hyp) >= self.num_beams
                ), f""Batch can only be done if at least {self.num_beams} beams have been generated""
                assert (
                    eos_token_id is not None and pad_token_id is not None
                ), ""generated beams >= num_beams -> eos_token_id and pad_token have to be defined""
                # pad the batch
                next_beam_scores[batch_idx, :] = 0
                next_beam_tokens[batch_idx, :] = pad_token_id
                next_beam_indices[batch_idx, :] = 0
                continue

            # next tokens for this sentence
            beam_idx = 0
            for beam_token_rank, (next_token, next_score, next_index) in enumerate(
                zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])
            ):
                batch_beam_idx = batch_idx * self.group_size + next_index
                # add to generated hypotheses if end of sentence
                if (eos_token_id is not None) and (next_token.item() == eos_token_id):
                    # if beam_token does not belong to top num_beams tokens, it should not be added
                    is_beam_token_worse_than_top_num_beams = beam_token_rank >= self.group_size
                    if is_beam_token_worse_than_top_num_beams:
                        continue
                    beam_hyp.add(
                        input_ids[batch_beam_idx].clone(),
                        next_score.item(),
                    )
                else:
                    # add next predicted token since it is not eos_token
                    next_beam_scores[batch_idx, beam_idx] = next_score
                    next_beam_tokens[batch_idx, beam_idx] = next_token
                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx
                    beam_idx += 1

                # once the beam for next step is full, don't add more tokens to it.
                if beam_idx == self.group_size:
                    break

            if beam_idx < self.group_size:
                raise ValueError(
                    f""At most {self.group_size} tokens in {next_tokens[batch_idx]} can be equal to `eos_token_id: {eos_token_id}`. Make sure {next_tokens[batch_idx]} are corrected.""
                )

            # Check if we are done so that we can save a pad step if all(done)
            self._done[batch_idx] = self._done[batch_idx] or beam_hyp.is_done(
                next_scores[batch_idx].max().item(), cur_len
            )

        return UserDict(
            {
                ""next_beam_scores"": next_beam_scores.view(-1),
                ""next_beam_tokens"": next_beam_tokens.view(-1),
                ""next_beam_indices"": next_beam_indices.view(-1),
            }
        )",eos_token_id is not None and next_token.item() == eos_token_id,None is not eos_token_id == next_token.item()
PINCE,https://github.com/korcankaraokcu/PINCE/tree/master//PINCE.py,ReferencedStringsWidgetForm,__init__$5219,"def __init__(self, parent=None):
        super().__init__()
        self.setupUi(self)
        GuiUtils.fill_value_combobox(self.comboBox_ValueType, type_defs.VALUE_INDEX.INDEX_STRING_UTF8)
        self.parent = lambda: parent
        global instances
        instances.append(self)
        GuiUtils.center_to_parent(self)
        self.setWindowFlags(Qt.Window)
        self.tableWidget_References.setColumnWidth(REF_STR_ADDR_COL, 150)
        self.tableWidget_References.setColumnWidth(REF_STR_COUNT_COL, 80)
        self.splitter.setStretchFactor(0, 1)
        self.listWidget_Referrers.resize(400, self.listWidget_Referrers.height())
        self.hex_len = 16 if GDB_Engine.inferior_arch == type_defs.INFERIOR_ARCH.ARCH_64 else 8
        str_dict, jmp_dict, call_dict = GDB_Engine.get_dissect_code_data()
        str_dict_len, jmp_dict_len, call_dict_len = len(str_dict), len(jmp_dict), len(call_dict)
        str_dict.close()
        jmp_dict.close()
        call_dict.close()
        if str_dict_len == 0 and jmp_dict_len == 0 and call_dict_len == 0:
            confirm_dialog = InputDialogForm(item_list=[(""You need to dissect code first\nProceed?"",)])
            if confirm_dialog.exec_():
                dissect_code_dialog = DissectCodeDialogForm()
                dissect_code_dialog.scan_finished_signal.connect(dissect_code_dialog.accept)
                dissect_code_dialog.exec_()
        self.refresh_table()
        self.tableWidget_References.sortByColumn(REF_STR_ADDR_COL, Qt.AscendingOrder)
        self.tableWidget_References.selectionModel().currentChanged.connect(self.tableWidget_References_current_changed)
        self.listWidget_Referrers.itemDoubleClicked.connect(self.listWidget_Referrers_item_double_clicked)
        self.tableWidget_References.itemDoubleClicked.connect(self.tableWidget_References_item_double_clicked)
        self.tableWidget_References.contextMenuEvent = self.tableWidget_References_context_menu_event
        self.listWidget_Referrers.contextMenuEvent = self.listWidget_Referrers_context_menu_event
        self.pushButton_Search.clicked.connect(self.refresh_table)
        self.comboBox_ValueType.currentIndexChanged.connect(self.refresh_table)
        self.shortcut_search = QShortcut(QKeySequence(""Return""), self)
        self.shortcut_search.activated.connect(self.refresh_table)",str_dict_len == 0 and jmp_dict_len == 0 and (call_dict_len == 0),str_dict_len == 0 == jmp_dict_len and call_dict_len == 0
angrop,https://github.com/angr/angrop/tree/master/angrop/rop_utils.py,,get_reg_name$123,"def get_reg_name(arch, reg_offset):
    """"""
    :param reg_offset: Tries to find the name of a register given the offset in the registers.
    :return: The register name
    """"""
    # todo does this make sense
    if reg_offset is None:
        raise RegNotFoundException(""register offset is None"")

    original_offset = reg_offset
    while reg_offset >= 0 and reg_offset >= original_offset - arch.bytes:
        if reg_offset in arch.register_names:
            return arch.register_names[reg_offset]
        else:
            reg_offset -= 1
    raise RegNotFoundException(""register %s not found"" % str(original_offset))",reg_offset >= 0 and reg_offset >= original_offset - arch.bytes,0 <= reg_offset >= original_offset - arch.bytes
GenSMBIOS,https://github.com/corpnewt/GenSMBIOS/tree/master/Scripts/utils.py,Utils,check_path$102,"def check_path(self, path):
        # Let's loop until we either get a working path, or no changes
        test_path = path
        last_path = None
        while True:
            # Bail if we've looped at least once and the path didn't change
            if last_path != None and last_path == test_path: return None
            last_path = test_path
            # Check if we stripped everything out
            if not len(test_path): return None
            # Check if we have a valid path
            if os.path.exists(test_path):
                return os.path.abspath(test_path)
            # Check for quotes
            if test_path[0] == test_path[-1] and test_path[0] in ('""',""'""):
                test_path = test_path[1:-1]
                continue
            # Check for a tilde and expand if needed
            if test_path[0] == ""~"":
                tilde_expanded = os.path.expanduser(test_path)
                if tilde_expanded != test_path:
                    # Got a change
                    test_path = tilde_expanded
                    continue
            # Let's check for spaces - strip from the left first, then the right
            if test_path[0] in ("" "",""\t""):
                test_path = test_path[1:]
                continue
            if test_path[-1] in ("" "",""\t""):
                test_path = test_path[:-1]
                continue
            # Maybe we have escapes to handle?
            test_path = ""\\"".join([x.replace(""\\"", """") for x in test_path.split(""\\\\"")])",last_path != None and last_path == test_path,None != last_path == test_path
GenSMBIOS,https://github.com/corpnewt/GenSMBIOS/tree/master/Scripts/utils.py,Utils,check_path$102,"def check_path(self, path):
        # Let's loop until we either get a working path, or no changes
        test_path = path
        last_path = None
        while True:
            # Bail if we've looped at least once and the path didn't change
            if last_path != None and last_path == test_path: return None
            last_path = test_path
            # Check if we stripped everything out
            if not len(test_path): return None
            # Check if we have a valid path
            if os.path.exists(test_path):
                return os.path.abspath(test_path)
            # Check for quotes
            if test_path[0] == test_path[-1] and test_path[0] in ('""',""'""):
                test_path = test_path[1:-1]
                continue
            # Check for a tilde and expand if needed
            if test_path[0] == ""~"":
                tilde_expanded = os.path.expanduser(test_path)
                if tilde_expanded != test_path:
                    # Got a change
                    test_path = tilde_expanded
                    continue
            # Let's check for spaces - strip from the left first, then the right
            if test_path[0] in ("" "",""\t""):
                test_path = test_path[1:]
                continue
            if test_path[-1] in ("" "",""\t""):
                test_path = test_path[:-1]
                continue
            # Maybe we have escapes to handle?
            test_path = ""\\"".join([x.replace(""\\"", """") for x in test_path.split(""\\\\"")])","test_path[0] == test_path[-1] and test_path[0] in ('""', ""'"")","test_path[-1] == test_path[0] in ('""', ""'"")"
jcvi,https://github.com/tanghaibao/jcvi/tree/master/jcvi/formats/gff.py,,note$2810,"def note(args):
    """"""
    %prog note gffile > tabfile

    Extract certain attribute field for each feature.
    """"""
    p = OptionParser(note.__doc__)
    p.add_option(
        ""--type"",
        default=None,
        help=""Only process certain types, multiple types allowed with comma"",
    )
    p.add_option(
        ""--attribute"",
        default=""Parent,Note"",
        help=""Attribute field to extract, multiple fields allowd with comma"",
    )
    p.add_option(""--AED"", type=""float"", help=""Only extract lines with AED score <="")
    p.add_option(
        ""--exoncount"",
        default=False,
        action=""store_true"",
        help=""Get the exon count for each mRNA feat"",
    )
    opts, args = p.parse_args(args)

    if len(args) != 1:
        sys.exit(not p.print_help())

    (gffile,) = args
    type = opts.type
    if type:
        type = type.split("","")

    g = make_index(gffile)
    exoncounts = {}
    if opts.exoncount:
        for feat in g.features_of_type(""mRNA""):
            nexons = 0
            for c in g.children(feat.id, 1):
                if c.featuretype != ""exon"":
                    continue
                nexons += 1
            exoncounts[feat.id] = nexons

    attrib = opts.attribute.split("","")

    gff = Gff(gffile)
    seen = set()
    AED = opts.AED
    for g in gff:
        if type and g.type not in type:
            continue
        if AED is not None and float(g.attributes[""_AED""][0]) > AED:
            continue
        keyval = [g.accn] + [
            "","".join(g.attributes[x]) for x in attrib if x in g.attributes
        ]
        if exoncounts:
            nexons = exoncounts.get(g.accn, 0)
            keyval.append(str(nexons))
        keyval = tuple(keyval)
        if keyval not in seen:
            print(""\t"".join(keyval))
            seen.add(keyval)",AED is not None and float(g.attributes['_AED'][0]) > AED,None is not AED < float(g.attributes['_AED'][0])
autogluon,https://github.com/awslabs/autogluon/tree/master/core/src/autogluon/core/utils/feature_selection.py,FeatureSelector,setup$467,"def setup(self, X: pd.DataFrame, y: pd.DataFrame, X_val: pd.DataFrame, y_val: pd.DataFrame, n_train_subsample: int, prune_threshold: float,
              **kwargs: dict) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series, pd.DataFrame, pd.Series, Union[float, str], List[str]]:
        """"""
        Modify training data, validation data, and model fit kwargs appropriately by subsampling, adding noise columns, replacing bagged
        models, and more.
        """"""
        # subsample training data
        min_fi_samples = kwargs.get('min_fi_samples', 10000)
        random_state = self.rng.integers(low=0, high=1e5)
        if n_train_subsample is not None and len(X) > n_train_subsample:
            logger.log(20, f""\tNumber of training samples {len(X)} is greater than {n_train_subsample}. Using {n_train_subsample} samples as training data."")
            drop_ratio = 1. - n_train_subsample / len(X)
            X_train, _, y_train, _ = generate_train_test_split(X=X, y=y, problem_type=self.problem_type, random_state=random_state, test_size=drop_ratio)
        else:
            X_train, y_train = X, y

        # replace bagged model with its child model for the proxy model if replace_bag=True (overrides subsampling if triggered)
        if n_train_subsample is None:
            trigger_replace_bag = kwargs.get('replace_bag', False) and self.is_bagged
        else:
            trigger_replace_bag = kwargs.get('replace_bag', True) and self.is_bagged and len(X) > n_train_subsample + min_fi_samples
        if trigger_replace_bag:
            logger.log(20, f""\tFeature selection model is bagged and replace_bag=True. Using a non-bagged version of the model for feature selection."")
            val_ratio = 1. - n_train_subsample / len(X) if n_train_subsample is not None else 0.25
            X_train, X_val, y_train, y_val = generate_train_test_split(X=X, y=y, problem_type=self.problem_type, random_state=random_state, test_size=val_ratio)
            self.is_bagged = False
            self.replace_bag = True

        # Be more lenient with feature importance computation shuffles for very high dimensional datasets for time's sake
        if len(X_train.columns) > 1000:
            self.max_n_shuffle = self.max_n_shuffle // 2

        # set prune_threshold and optionally modify feature_metadata
        noise_columns = []
        feature_metadata = deepcopy(kwargs.get('feature_metadata', None))
        if prune_threshold == 'none':
            prune_threshold = float('inf')
        elif prune_threshold == 'noise':
            X_train, noise_columns = add_noise_column(X=X_train, rng=self.rng)
            if feature_metadata is not None:
                for noise_column in noise_columns:
                    feature_metadata.type_map_raw[noise_column] = R_FLOAT
            if isinstance(X_val, pd.DataFrame):
                X_val, _ = add_noise_column(X=X_val, rng=self.rng, noise_columns=noise_columns)
        else:
            assert isinstance(prune_threshold, float), ""prune_threshold must be float, 'noise', or 'none'.""
        X_fi, y_fi = (X_train, y_train) if self.is_bagged else (X_val, y_val)
        return X_train, y_train, X_val, y_val, X_fi, y_fi, prune_threshold, noise_columns, feature_metadata",n_train_subsample is not None and len(X) > n_train_subsample,None is not n_train_subsample < len(X)
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagster-airflow/dagster_airflow_tests/test_fixtures.py,,dagster_airflow_docker_operator_pipeline$175,"def dagster_airflow_docker_operator_pipeline():
    """"""This is a test fixture for running Dagster pipelines as containerized Airflow DAGs.

    Usage:
        from dagster_airflow_tests.test_fixtures import dagster_airflow_docker_operator_pipeline

        def test_airflow(dagster_airflow_docker_operator_pipeline):
            results = dagster_airflow_docker_operator_pipeline(
                pipeline_name='test_pipeline',
                recon_repo=reconstructable(define_pipeline),
                environment_yaml=['environments/test_*.yaml'],
                image='myimage:latest'
            )
            assert len(results) == 3
    """"""
    from dagster_airflow.factory import make_airflow_dag_containerized_for_recon_repo
    from dagster_airflow.operators.docker_operator import DagsterDockerOperator

    def _pipeline_fn(
        recon_repo,
        pipeline_name,
        image,
        run_config=None,
        environment_yaml=None,
        op_kwargs=None,
        mode=None,
        execution_date=timezone.utcnow(),
    ):
        if run_config is None and environment_yaml is not None:
            run_config = load_yaml_from_glob_list(environment_yaml)

        op_kwargs = op_kwargs or {}
        op_kwargs[""network_mode""] = ""container:test-postgres-db-airflow""

        with postgres_instance() as instance:

            dag, tasks = make_airflow_dag_containerized_for_recon_repo(
                recon_repo=recon_repo,
                job_name=pipeline_name,
                image=image,
                mode=mode,
                run_config=run_config,
                op_kwargs=op_kwargs,
                instance=instance,
            )
            assert isinstance(dag, DAG)

            for task in tasks:
                assert isinstance(task, DagsterDockerOperator)

            return execute_tasks_in_dag(
                dag, tasks, run_id=make_new_run_id(), execution_date=execution_date
            )

    return _pipeline_fn",run_config is None and environment_yaml is not None,run_config is None is not environment_yaml
pdfminer,https://github.com/euske/pdfminer/tree/master/pdfminer/runlength.py,,rldecode$9,"def rldecode(data):
    r""""""
    RunLength decoder (Adobe version) implementation based on PDF Reference
    version 1.4 section 3.3.4:
        The RunLengthDecode filter decodes data that has been encoded in a
        simple byte-oriented format based on run length. The encoded data
        is a sequence of runs, where each run consists of a length byte
        followed by 1 to 128 bytes of data. If the length byte is in the
        range 0 to 127, the following length + 1 (1 to 128) bytes are
        copied literally during decompression. If length is in the range
        129 to 255, the following single byte is to be copied 257 - length
        (2 to 128) times during decompression. A length value of 128
        denotes EOD.
    >>> s = b'\x05123456\xfa7\x04abcde\x80junk'
    >>> rldecode(s)
    b'1234567777777abcde'
    """"""
    decoded = b''
    i = 0
    while i < len(data):
        #print('data[%d]=:%d:' % (i,ord(data[i])))
        length = data[i]
        if length == 128:
            break
        if length >= 0 and length < 128:
            run = data[i+1:(i+1)+(length+1)]
            #print('length=%d, run=%s' % (length+1,run))
            decoded += run
            i = (i+1) + (length+1)
        if length > 128:
            run = data[i+1:i+2]*(257-length)
            #print('length=%d, run=%s' % (257-length,run))
            decoded += run
            i = (i+1) + 1
    return decoded",length >= 0 and length < 128,0 <= length < 128
rasa,https://github.com/RasaHQ/rasa/tree/master/rasa/shared/nlu/training_data/formats/readerwriter.py,TrainingDataWriter,generate_entity_attributes$151,"def generate_entity_attributes(
        text: Text, entity: Dict[Text, Any], short_allowed: bool = True
    ) -> Text:
        """"""Generates text for the entity attributes.

        Args:
            text: The text that is annotated with the entity
            entity: Entity data
            short_allowed: If `True`, allow shorthand annotation with parenthesis

        Returns:
            The annotation text that should follow the given text
        """"""
        entity_text = text
        entity_type = entity.get(ENTITY_ATTRIBUTE_TYPE)
        entity_value = entity.get(ENTITY_ATTRIBUTE_VALUE)
        entity_role = entity.get(ENTITY_ATTRIBUTE_ROLE)
        entity_group = entity.get(ENTITY_ATTRIBUTE_GROUP)

        if entity_value and entity_value == entity_text:
            entity_value = None

        use_short_syntax = (
            short_allowed
            and entity_value is None
            and entity_role is None
            and entity_group is None
        )

        if use_short_syntax:
            return f""({entity_type})""
        else:
            entity_dict = OrderedDict(
                [
                    (ENTITY_ATTRIBUTE_TYPE, entity_type),
                    (ENTITY_ATTRIBUTE_ROLE, entity_role),
                    (ENTITY_ATTRIBUTE_GROUP, entity_group),
                    (ENTITY_ATTRIBUTE_VALUE, entity_value),
                ]
            )
            entity_dict = OrderedDict(
                [(k, v) for k, v in entity_dict.items() if v is not None]
            )

            return f""{json.dumps(entity_dict)}""",short_allowed and entity_value is None and (entity_role is None) and (entity_group is None),entity_value is None is entity_role and entity_group is None and short_allowed
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/reformer/modeling_reformer.py,ReformerModel,forward$2017,"def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        head_mask: Optional[torch.Tensor] = None,
        inputs_embeds: Optional[torch.Tensor] = None,
        num_hashes: Optional[int] = None,
        past_buckets_states: Optional[List[Tuple[torch.Tensor]]] = None,
        use_cache: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, ReformerModelOutput]:
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if input_ids is not None and inputs_embeds is not None:
            raise ValueError(""You cannot specify both input_ids and inputs_embeds at the same time"")
        elif input_ids is not None:
            input_shape = input_ids.size()  # noqa: F841
            device = input_ids.device
        elif inputs_embeds is not None:
            input_shape = inputs_embeds.size()[:-1]  # noqa: F841
            device = inputs_embeds.device
        else:
            raise ValueError(""You have to specify either input_ids or inputs_embeds"")

        assert (
            len(input_shape) == 2
        ), f""`input_ids` have be of shape `[batch_size, sequence_length]`, but got shape: {input_shape}""

        if past_buckets_states is not None:
            assert not self.training, ""`past_buckets_states` can only be used for inference, not for training`.""

        # prepare head mask
        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers, is_attention_chunked=True)

        # original sequence length for padding
        orig_sequence_length = input_shape[-1]

        # if needs padding
        least_common_mult_chunk_length = _get_least_common_mult_chunk_len(self.config)
        min_chunk_length = _get_min_chunk_len(self.config)

        must_pad_to_match_chunk_length = (
            input_shape[-1] % least_common_mult_chunk_length != 0
            and input_shape[-1] > min_chunk_length
            and past_buckets_states is None
        )

        if must_pad_to_match_chunk_length:
            padding_length = least_common_mult_chunk_length - input_shape[-1] % least_common_mult_chunk_length

            if self.training is True:
                raise ValueError(
                    f""If training, sequence length {input_shape[-1]} has to be a multiple of least common multiple ""
                    f""chunk_length {least_common_mult_chunk_length}. Please consider padding the input to a length ""
                    f""of {input_shape[-1] + padding_length}.""
                )

            # pad input
            input_ids, inputs_embeds, attention_mask, position_ids, input_shape = self._pad_to_mult_of_chunk_length(
                input_ids,
                inputs_embeds=inputs_embeds,
                attention_mask=attention_mask,
                position_ids=position_ids,
                input_shape=input_shape,
                padding_length=padding_length,
                padded_seq_length=least_common_mult_chunk_length,
                device=device,
            )

        # start index for position encoding depends on incremental decoding
        if past_buckets_states is not None:
            start_idx_pos_encodings = past_buckets_states[0][1].shape[1]
        else:
            start_idx_pos_encodings = 0

        embedding_output = self.embeddings(
            input_ids=input_ids,
            position_ids=position_ids,
            inputs_embeds=inputs_embeds,
            start_idx_pos_encodings=start_idx_pos_encodings,
        )

        encoder_outputs = self.encoder(
            hidden_states=embedding_output,
            head_mask=head_mask,
            attention_mask=attention_mask,
            num_hashes=num_hashes,
            past_buckets_states=past_buckets_states,
            use_cache=use_cache,
            orig_sequence_length=orig_sequence_length,
            output_hidden_states=output_hidden_states,
            output_attentions=output_attentions,
        )
        sequence_output = encoder_outputs.hidden_states

        # if padding was applied
        if must_pad_to_match_chunk_length:
            sequence_output = sequence_output[:, :orig_sequence_length]

        past_buckets_states = encoder_outputs.past_buckets_states if use_cache else None
        hidden_states = encoder_outputs.all_hidden_states if output_hidden_states else None
        attentions = encoder_outputs.all_attentions if output_attentions else None

        if not return_dict:
            return tuple(v for v in [sequence_output, past_buckets_states, hidden_states, attentions] if v is not None)
        return ReformerModelOutput(
            last_hidden_state=sequence_output,
            past_buckets_states=past_buckets_states,
            hidden_states=hidden_states,
            attentions=attentions,
        )",input_ids is not None and inputs_embeds is not None,input_ids is not None is not inputs_embeds
krill,https://github.com/p-e-w/krill/tree/master/krill/krill.py,,main$303,"def main():
    # Force UTF-8 encoding for stdout as we will be printing Unicode characters
    # which will fail with a UnicodeEncodeError if the encoding is not set,
    # e.g. because stdout is being piped.
    # See http://www.macfreek.nl/memory/Encoding_of_Python_stdout and
    # http://stackoverflow.com/a/4546129 for extensive discussions of the issue.
    if sys.stdout.encoding != ""UTF-8"":
        # For Python 2 and 3 compatibility
        prev_stdout = sys.stdout if sys.version_info[0] < 3 else sys.stdout.buffer
        sys.stdout = codecs.getwriter(""utf-8"")(prev_stdout)

    arg_parser = argparse.ArgumentParser(prog=""krill"", description=""Read and filter web feeds."")
    arg_parser.add_argument(""-s"", ""--sources"", nargs=""+"",
            help=""URLs to pull data from"", metavar=""URL"")
    arg_parser.add_argument(""-S"", ""--sources-file"",
            help=""file from which to load source URLs "" +
                 ""(OPML format assumed if filename ends with \"".opml\"")"", metavar=""FILE"")
    arg_parser.add_argument(""-f"", ""--filters"", nargs=""+"",
            help=""patterns used to select feed items to print"", metavar=""REGEX"")
    arg_parser.add_argument(""-F"", ""--filters-file"",
            help=""file from which to load filter patterns"", metavar=""FILE"")
    arg_parser.add_argument(""-u"", ""--update-interval"", default=300, type=int,
            help=""time between successive feed updates "" +
                 ""(default: 300 seconds, 0 for single pull only)"", metavar=""SECONDS"")
    args = arg_parser.parse_args()

    if args.sources is None and args.sources_file is None:
        arg_parser.error(""either a source URL (-s) or a sources file (-S) must be given"")

    Application(args).run()",args.sources is None and args.sources_file is None,args.sources is None is args.sources_file
astropy,https://github.com/astropy/astropy/tree/master/astropy/modeling/core.py,Model,coerce_units$2323,"def coerce_units(
        self,
        input_units=None,
        return_units=None,
        input_units_equivalencies=None,
        input_units_allow_dimensionless=False,
    ):
        """"""
        Attach units to this (unitless) model.

        Parameters
        ----------
        input_units : dict or tuple, optional
            Input units to attach.  If dict, each key is the name of a model input,
            and the value is the unit to attach.  If tuple, the elements are units
            to attach in order corresponding to `Model.inputs`.
        return_units : dict or tuple, optional
            Output units to attach.  If dict, each key is the name of a model output,
            and the value is the unit to attach.  If tuple, the elements are units
            to attach in order corresponding to `Model.outputs`.
        input_units_equivalencies : dict, optional
            Default equivalencies to apply to input values.  If set, this should be a
            dictionary where each key is a string that corresponds to one of the
            model inputs.
        input_units_allow_dimensionless : bool or dict, optional
            Allow dimensionless input. If this is True, input values to evaluate will
            gain the units specified in input_units. If this is a dictionary then it
            should map input name to a bool to allow dimensionless numbers for that
            input.

        Returns
        -------
        `CompoundModel`
            A `CompoundModel` composed of the current model plus
            `~astropy.modeling.mappings.UnitsMapping` model(s) that attach the units.

        Raises
        ------
        ValueError
            If the current model already has units.

        Examples
        --------

        Wrapping a unitless model to require and convert units:

        >>> from astropy.modeling.models import Polynomial1D
        >>> from astropy import units as u
        >>> poly = Polynomial1D(1, c0=1, c1=2)
        >>> model = poly.coerce_units((u.m,), (u.s,))
        >>> model(u.Quantity(10, u.m))  # doctest: +FLOAT_CMP
        <Quantity 21. s>
        >>> model(u.Quantity(1000, u.cm))  # doctest: +FLOAT_CMP
        <Quantity 21. s>
        >>> model(u.Quantity(10, u.cm))  # doctest: +FLOAT_CMP
        <Quantity 1.2 s>

        Wrapping a unitless model but still permitting unitless input:

        >>> from astropy.modeling.models import Polynomial1D
        >>> from astropy import units as u
        >>> poly = Polynomial1D(1, c0=1, c1=2)
        >>> model = poly.coerce_units((u.m,), (u.s,), input_units_allow_dimensionless=True)
        >>> model(u.Quantity(10, u.m))  # doctest: +FLOAT_CMP
        <Quantity 21. s>
        >>> model(10)  # doctest: +FLOAT_CMP
        <Quantity 21. s>
        """"""
        from .mappings import UnitsMapping

        result = self

        if input_units is not None:
            if self.input_units is not None:
                model_units = self.input_units
            else:
                model_units = {}

            for unit in [model_units.get(i) for i in self.inputs]:
                if unit is not None and unit != dimensionless_unscaled:
                    raise ValueError(
                        ""Cannot specify input_units for model with existing input units""
                    )

            if isinstance(input_units, dict):
                if input_units.keys() != set(self.inputs):
                    message = (
                        f""""""input_units keys ({"", "".join(input_units.keys())}) """"""
                        f""""""do not match model inputs ({"", "".join(self.inputs)})""""""
                    )
                    raise ValueError(message)
                input_units = [input_units[i] for i in self.inputs]

            if len(input_units) != self.n_inputs:
                message = (
                    ""input_units length does not match n_inputs: ""
                    f""expected {self.n_inputs}, received {len(input_units)}""
                )
                raise ValueError(message)

            mapping = tuple(
                (unit, model_units.get(i)) for i, unit in zip(self.inputs, input_units)
            )
            input_mapping = UnitsMapping(
                mapping,
                input_units_equivalencies=input_units_equivalencies,
                input_units_allow_dimensionless=input_units_allow_dimensionless,
            )
            input_mapping.inputs = self.inputs
            input_mapping.outputs = self.inputs
            result = input_mapping | result

        if return_units is not None:
            if self.return_units is not None:
                model_units = self.return_units
            else:
                model_units = {}

            for unit in [model_units.get(i) for i in self.outputs]:
                if unit is not None and unit != dimensionless_unscaled:
                    raise ValueError(
                        ""Cannot specify return_units for model ""
                        ""with existing output units""
                    )

            if isinstance(return_units, dict):
                if return_units.keys() != set(self.outputs):
                    message = (
                        f""""""return_units keys ({"", "".join(return_units.keys())}) """"""
                        f""""""do not match model outputs ({"", "".join(self.outputs)})""""""
                    )
                    raise ValueError(message)
                return_units = [return_units[i] for i in self.outputs]

            if len(return_units) != self.n_outputs:
                message = (
                    ""return_units length does not match n_outputs: ""
                    f""expected {self.n_outputs}, received {len(return_units)}""
                )
                raise ValueError(message)

            mapping = tuple(
                (model_units.get(i), unit)
                for i, unit in zip(self.outputs, return_units)
            )
            return_mapping = UnitsMapping(mapping)
            return_mapping.inputs = self.outputs
            return_mapping.outputs = self.outputs
            result = result | return_mapping

        return result",unit is not None and unit != dimensionless_unscaled,None is not unit != dimensionless_unscaled
astropy,https://github.com/astropy/astropy/tree/master/astropy/modeling/core.py,Model,coerce_units$2323,"def coerce_units(
        self,
        input_units=None,
        return_units=None,
        input_units_equivalencies=None,
        input_units_allow_dimensionless=False,
    ):
        """"""
        Attach units to this (unitless) model.

        Parameters
        ----------
        input_units : dict or tuple, optional
            Input units to attach.  If dict, each key is the name of a model input,
            and the value is the unit to attach.  If tuple, the elements are units
            to attach in order corresponding to `Model.inputs`.
        return_units : dict or tuple, optional
            Output units to attach.  If dict, each key is the name of a model output,
            and the value is the unit to attach.  If tuple, the elements are units
            to attach in order corresponding to `Model.outputs`.
        input_units_equivalencies : dict, optional
            Default equivalencies to apply to input values.  If set, this should be a
            dictionary where each key is a string that corresponds to one of the
            model inputs.
        input_units_allow_dimensionless : bool or dict, optional
            Allow dimensionless input. If this is True, input values to evaluate will
            gain the units specified in input_units. If this is a dictionary then it
            should map input name to a bool to allow dimensionless numbers for that
            input.

        Returns
        -------
        `CompoundModel`
            A `CompoundModel` composed of the current model plus
            `~astropy.modeling.mappings.UnitsMapping` model(s) that attach the units.

        Raises
        ------
        ValueError
            If the current model already has units.

        Examples
        --------

        Wrapping a unitless model to require and convert units:

        >>> from astropy.modeling.models import Polynomial1D
        >>> from astropy import units as u
        >>> poly = Polynomial1D(1, c0=1, c1=2)
        >>> model = poly.coerce_units((u.m,), (u.s,))
        >>> model(u.Quantity(10, u.m))  # doctest: +FLOAT_CMP
        <Quantity 21. s>
        >>> model(u.Quantity(1000, u.cm))  # doctest: +FLOAT_CMP
        <Quantity 21. s>
        >>> model(u.Quantity(10, u.cm))  # doctest: +FLOAT_CMP
        <Quantity 1.2 s>

        Wrapping a unitless model but still permitting unitless input:

        >>> from astropy.modeling.models import Polynomial1D
        >>> from astropy import units as u
        >>> poly = Polynomial1D(1, c0=1, c1=2)
        >>> model = poly.coerce_units((u.m,), (u.s,), input_units_allow_dimensionless=True)
        >>> model(u.Quantity(10, u.m))  # doctest: +FLOAT_CMP
        <Quantity 21. s>
        >>> model(10)  # doctest: +FLOAT_CMP
        <Quantity 21. s>
        """"""
        from .mappings import UnitsMapping

        result = self

        if input_units is not None:
            if self.input_units is not None:
                model_units = self.input_units
            else:
                model_units = {}

            for unit in [model_units.get(i) for i in self.inputs]:
                if unit is not None and unit != dimensionless_unscaled:
                    raise ValueError(
                        ""Cannot specify input_units for model with existing input units""
                    )

            if isinstance(input_units, dict):
                if input_units.keys() != set(self.inputs):
                    message = (
                        f""""""input_units keys ({"", "".join(input_units.keys())}) """"""
                        f""""""do not match model inputs ({"", "".join(self.inputs)})""""""
                    )
                    raise ValueError(message)
                input_units = [input_units[i] for i in self.inputs]

            if len(input_units) != self.n_inputs:
                message = (
                    ""input_units length does not match n_inputs: ""
                    f""expected {self.n_inputs}, received {len(input_units)}""
                )
                raise ValueError(message)

            mapping = tuple(
                (unit, model_units.get(i)) for i, unit in zip(self.inputs, input_units)
            )
            input_mapping = UnitsMapping(
                mapping,
                input_units_equivalencies=input_units_equivalencies,
                input_units_allow_dimensionless=input_units_allow_dimensionless,
            )
            input_mapping.inputs = self.inputs
            input_mapping.outputs = self.inputs
            result = input_mapping | result

        if return_units is not None:
            if self.return_units is not None:
                model_units = self.return_units
            else:
                model_units = {}

            for unit in [model_units.get(i) for i in self.outputs]:
                if unit is not None and unit != dimensionless_unscaled:
                    raise ValueError(
                        ""Cannot specify return_units for model ""
                        ""with existing output units""
                    )

            if isinstance(return_units, dict):
                if return_units.keys() != set(self.outputs):
                    message = (
                        f""""""return_units keys ({"", "".join(return_units.keys())}) """"""
                        f""""""do not match model outputs ({"", "".join(self.outputs)})""""""
                    )
                    raise ValueError(message)
                return_units = [return_units[i] for i in self.outputs]

            if len(return_units) != self.n_outputs:
                message = (
                    ""return_units length does not match n_outputs: ""
                    f""expected {self.n_outputs}, received {len(return_units)}""
                )
                raise ValueError(message)

            mapping = tuple(
                (model_units.get(i), unit)
                for i, unit in zip(self.outputs, return_units)
            )
            return_mapping = UnitsMapping(mapping)
            return_mapping.inputs = self.outputs
            return_mapping.outputs = self.outputs
            result = result | return_mapping

        return result",unit is not None and unit != dimensionless_unscaled,None is not unit != dimensionless_unscaled
AIF360,https://github.com/Trusted-AI/AIF360/tree/master/aif360/algorithms/postprocessing/calibrated_eq_odds_postprocessing.py,,weighted_cost$203,"def weighted_cost(fp_rate, fn_rate, cm, privileged):
    norm_const = float(fp_rate + fn_rate) if\
                      (fp_rate != 0 and fn_rate != 0) else 1
    return ((fp_rate / norm_const
            * cm.generalized_false_positive_rate(privileged=privileged)
            * (1 - cm.base_rate(privileged=privileged))) +
           (fn_rate / norm_const
            * cm.generalized_false_negative_rate(privileged=privileged)
            * cm.base_rate(privileged=privileged)))",fp_rate != 0 and fn_rate != 0,fp_rate != 0 != fn_rate
StereoVision,https://github.com/erget/StereoVision/tree/master/stereovision/blockmatchers.py,StereoBM,window_size$140,"def window_size(self, value):
        """"""Set private ``_window_size`` and reset ``_block_matcher``.""""""
        if (value > 4 and
            value < self.parameter_maxima[""window_size""] and
            value % 2):
            self._window_size = value
        else:
            raise InvalidWindowSizeError(""Window size must be an odd number ""
                                      ""between 0 and {}."".format(
                                      self.parameter_maxima[""window_size""] + 1))
        self._replace_bm()",value > 4 and value < self.parameter_maxima['window_size'] and value % 2,4 < value < self.parameter_maxima['window_size'] and value % 2
PGL,https://github.com/PaddlePaddle/PGL/tree/master/examples/metapath2vec/datasets/sampling.py,,deepwalk_sample$172,"def deepwalk_sample(graph, nodes, max_depth, alias_name=None,
                    events_name=None):
    """"""Implement of random walk.

    This function get random walks path for given nodes and depth.

    Args:
        nodes: Walk starting from nodes
        max_depth: Max walking depth

    Return:
        A list of walks.
    """"""
    walk = []
    # init
    for node in nodes:
        walk.append([node])

    cur_walk_ids = np.arange(0, len(nodes))
    cur_nodes = np.array(nodes, dtype=""uint64"")
    for l in range(max_depth):
        # select the walks not end
        cur_succs = graph.successor(cur_nodes)
        mask = [len(succ) > 0 for succ in cur_succs]

        if np.any(mask):
            cur_walk_ids = cur_walk_ids[mask]
            cur_nodes = cur_nodes[mask]
            cur_succs = cur_succs[mask]
        else:
            # stop when all nodes have no successor
            break

        if alias_name is not None and events_name is not None:
            sample_index = [
                alias_sample([1], graph.node_feat[alias_name][node],
                             graph.node_feat[events_name][node])[0]
                for node in cur_nodes
            ]
        else:
            outdegree = [len(cur_succ) for cur_succ in cur_succs]
            sample_index = np.floor(
                np.random.rand(cur_succs.shape[0]) * outdegree).astype(""int64"")

        nxt_cur_nodes = []
        for s, ind, walk_id in zip(cur_succs, sample_index, cur_walk_ids):
            walk[walk_id].append(s[ind])
            nxt_cur_nodes.append(s[ind])
        cur_nodes = np.array(nxt_cur_nodes, dtype=""uint64"")
    return walk",alias_name is not None and events_name is not None,alias_name is not None is not events_name
DeepLabCut,https://github.com/DeepLabCut/DeepLabCut/tree/master/deeplabcut/pose_estimation_tensorflow/datasets/pose_tensorpack.py,RandomCropping,get_transform$72,"def get_transform(self, img):
        hmax = self.hmax or img.shape[0]
        wmax = self.wmax or img.shape[1]
        hmin = min(self.hmin, img.shape[0])
        wmin = min(self.wmin, img.shape[1])
        hmax = min(hmax, img.shape[0])
        wmax = min(wmax, img.shape[1])
        h = self.rng.randint(hmin, hmax + 1)
        w = self.rng.randint(wmin, wmax + 1)
        diffh = img.shape[0] - h
        diffw = img.shape[1] - w
        assert diffh >= 0 and diffw >= 0
        y0 = 0 if diffh == 0 else self.rng.randint(diffh)
        x0 = 0 if diffw == 0 else self.rng.randint(diffw)
        crop_aug = CropTransform(y0, x0, h, w)

        return crop_aug",diffh >= 0 and diffw >= 0,diffh >= 0 <= diffw
salt,https://github.com/saltstack/salt/tree/master/salt/ext/tornado/test/simple_httpclient_test.py,SimpleHTTPClientTestMixin,test_connection_limit$165,"def test_connection_limit(self):
        with closing(self.create_client(max_clients=2)) as client:
            self.assertEqual(client.max_clients, 2)
            seen = []
            # Send 4 requests.  Two can be sent immediately, while the others
            # will be queued
            for i in range(4):
                client.fetch(self.get_url(""/trigger""),
                             lambda response, i=i: (seen.append(i), self.stop()))
            self.wait(condition=lambda: len(self.triggers) == 2)
            self.assertEqual(len(client.queue), 2)

            # Finish the first two requests and let the next two through
            self.triggers.popleft()()
            self.triggers.popleft()()
            self.wait(condition=lambda: (len(self.triggers) == 2 and
                                         len(seen) == 2))
            self.assertEqual(set(seen), set([0, 1]))
            self.assertEqual(len(client.queue), 0)

            # Finish all the pending requests
            self.triggers.popleft()()
            self.triggers.popleft()()
            self.wait(condition=lambda: len(seen) == 4)
            self.assertEqual(set(seen), set([0, 1, 2, 3]))
            self.assertEqual(len(self.triggers), 0)",len(self.triggers) == 2 and len(seen) == 2,len(self.triggers) == 2 == len(seen)
haystack,https://github.com/deepset-ai/haystack/tree/master/haystack/document_stores/memory.py,InMemoryDocumentStore,get_scores_torch$277,"def get_scores_torch(self, query_emb: np.ndarray, document_to_search: List[Document]) -> List[float]:
        """"""
        Calculate similarity scores between query embedding and a list of documents using torch.

        :param query_emb: Embedding of the query (e.g. gathered from DPR)
        :param document_to_search: List of documents to compare `query_emb` against.
        """"""
        query_emb = torch.tensor(query_emb, dtype=torch.float).to(self.main_device)
        if len(query_emb.shape) == 1:
            query_emb = query_emb.unsqueeze(dim=0)

        doc_embeds = np.array([doc.embedding for doc in document_to_search])
        doc_embeds = torch.as_tensor(doc_embeds, dtype=torch.float)
        if len(doc_embeds.shape) == 1 and doc_embeds.shape[0] == 1:
            doc_embeds = doc_embeds.unsqueeze(dim=0)
        elif len(doc_embeds.shape) == 1 and doc_embeds.shape[0] == 0:
            return []

        if self.similarity == ""cosine"":
            # cosine similarity is just a normed dot product
            query_emb_norm = torch.norm(query_emb, dim=1)
            query_emb = torch.div(query_emb, query_emb_norm)

            doc_embeds_norms = torch.norm(doc_embeds, dim=1)
            doc_embeds = torch.div(doc_embeds.T, doc_embeds_norms).T

        curr_pos = 0
        scores = []
        while curr_pos < len(doc_embeds):
            doc_embeds_slice = doc_embeds[curr_pos : curr_pos + self.scoring_batch_size]
            doc_embeds_slice = doc_embeds_slice.to(self.main_device)
            with torch.inference_mode():
                slice_scores = torch.matmul(doc_embeds_slice, query_emb.T).cpu()
                slice_scores = slice_scores.squeeze(dim=1)
                slice_scores = slice_scores.numpy().tolist()

            scores.extend(slice_scores)
            curr_pos += self.scoring_batch_size

        return scores",len(doc_embeds.shape) == 1 and doc_embeds.shape[0] == 1,len(doc_embeds.shape) == 1 == doc_embeds.shape[0]
python-progressbar,https://github.com/WoLpH/python-progressbar/tree/master/progressbar/bar.py,ProgressBar,update$776,"def update(self, value=None, force=False, **kwargs):
        'Updates the ProgressBar to a new value.'
        if self.start_time is None:
            self.start()
            return self.update(value, force=force, **kwargs)

        if value is not None and value is not base.UnknownLength and isinstance(
            value,
            int
        ):
            if self.max_value is base.UnknownLength:
                # Can't compare against unknown lengths so just update
                pass
            elif self.min_value > value:  # type: ignore
                raise ValueError(
                    'Value %s is too small. Should be between %s and %s'
                    % (value, self.min_value, self.max_value)
                )
            elif self.max_value < value:  # type: ignore
                if self.max_error:
                    raise ValueError(
                        'Value %s is too large. Should be between %s and %s'
                        % (value, self.min_value, self.max_value)
                    )
                else:
                    value = self.max_value

            self.previous_value = self.value
            self.value = value  # type: ignore

        # Save the updated values for dynamic messages
        variables_changed = False
        for key in kwargs:
            if key not in self.variables:
                raise TypeError(
                    'update() got an unexpected variable name as argument '
                    '{0!r}'.format(key)
                )
            elif self.variables[key] != kwargs[key]:
                self.variables[key] = kwargs[key]
                variables_changed = True

        if self._needs_update() or variables_changed or force:
            self.updates += 1
            ResizableMixin.update(self, value=value)
            ProgressBarBase.update(self, value=value)
            StdRedirectMixin.update(self, value=value)  # type: ignore

            # Only flush if something was actually written
            self.fd.flush()","value is not None and value is not base.UnknownLength and isinstance(value, int)","None is not value is not base.UnknownLength and isinstance(value, int)"
bleach,https://github.com/mozilla/bleach/tree/master/bleach/_vendor/html5lib/treebuilders/base.py,TreeBuilder,reconstructActiveFormattingElements$218,"def reconstructActiveFormattingElements(self):
        # Within this algorithm the order of steps described in the
        # specification is not quite the same as the order of steps in the
        # code. It should still do the same though.

        # Step 1: stop the algorithm when there's nothing to do.
        if not self.activeFormattingElements:
            return

        # Step 2 and step 3: we start with the last element. So i is -1.
        i = len(self.activeFormattingElements) - 1
        entry = self.activeFormattingElements[i]
        if entry == Marker or entry in self.openElements:
            return

        # Step 6
        while entry != Marker and entry not in self.openElements:
            if i == 0:
                # This will be reset to 0 below
                i = -1
                break
            i -= 1
            # Step 5: let entry be one earlier in the list.
            entry = self.activeFormattingElements[i]

        while True:
            # Step 7
            i += 1

            # Step 8
            entry = self.activeFormattingElements[i]
            clone = entry.cloneNode()  # Mainly to get a new copy of the attributes

            # Step 9
            element = self.insertElement({""type"": ""StartTag"",
                                          ""name"": clone.name,
                                          ""namespace"": clone.namespace,
                                          ""data"": clone.attributes})

            # Step 10
            self.activeFormattingElements[i] = element

            # Step 11
            if element == self.activeFormattingElements[-1]:
                break",entry != Marker and entry not in self.openElements,Marker != entry not in self.openElements
RenderPipeline,https://github.com/tobspr/RenderPipeline/tree/master/rplibs/yaml/yaml_py3/emitter.py,Emitter,write_folded$990,"def write_folded(self, text):
        hints = self.determine_block_hints(text)
        self.write_indicator('>'+hints, True)
        if hints[-1:] == '+':
            self.open_ended = True
        self.write_line_break()
        leading_space = True
        spaces = False
        breaks = True
        start = end = 0
        while end <= len(text):
            ch = None
            if end < len(text):
                ch = text[end]
            if breaks:
                if ch is None or ch not in '\n\x85\u2028\u2029':
                    if not leading_space and ch is not None and ch != ' '   \
                            and text[start] == '\n':
                        self.write_line_break()
                    leading_space = (ch == ' ')
                    for br in text[start:end]:
                        if br == '\n':
                            self.write_line_break()
                        else:
                            self.write_line_break(br)
                    if ch is not None:
                        self.write_indent()
                    start = end
            elif spaces:
                if ch != ' ':
                    if start+1 == end and self.column > self.best_width:
                        self.write_indent()
                    else:
                        data = text[start:end]
                        self.column += len(data)
                        if self.encoding:
                            data = data.encode(self.encoding)
                        self.stream.write(data)
                    start = end
            else:
                if ch is None or ch in ' \n\x85\u2028\u2029':
                    data = text[start:end]
                    self.column += len(data)
                    if self.encoding:
                        data = data.encode(self.encoding)
                    self.stream.write(data)
                    if ch is None:
                        self.write_line_break()
                    start = end
            if ch is not None:
                breaks = (ch in '\n\x85\u2028\u2029')
                spaces = (ch == ' ')
            end += 1",not leading_space and ch is not None and (ch != ' ') and (text[start] == '\n'),None is not ch != ' ' and text[start] == '\n' and (not leading_space)
AI_Sudoku,https://github.com/neeru1207/AI_Sudoku/tree/master//MainUI.py,SudokuUI,__draw_cursor$414,"def __draw_cursor(self):
        self.canvas.delete(""cursor"")
        if self.row >= 0 and self.col >= 0:
            x0 = 20 + self.col * 50 + 1
            y0 = 20 + self.row * 50 + 1
            x1 = 20 + (self.col + 1) * 50 - 1
            y1 = 20 + (self.row + 1) * 50 - 1
            self.canvas.create_rectangle(
                x0, y0, x1, y1,
                outline=""red"", tags=""cursor""
            )",self.row >= 0 and self.col >= 0,self.row >= 0 <= self.col
fiftyone,https://github.com/voxel51/fiftyone/tree/master/fiftyone/core/frame.py,Frames,_iter_frames$567,"def _iter_frames(self, offset=None):
        if offset is None:
            offset = -1

        if not self._in_db or self._delete_all:
            for frame_number in sorted(self._replacements.keys()):
                if frame_number >= offset:
                    yield self._replacements[frame_number]

            return

        if self._replacements:
            max_repl_fn = max(self._replacements.keys())
            repl_done = False
        else:
            max_repl_fn = -1
            repl_done = True

        results = self._iter_frames_db()

        try:
            d = next(results)
            db_done = False
        except StopIteration:
            d = None
            db_done = True

        frame_number = 1
        while True:
            if repl_done and db_done:
                break

            if frame_number >= offset:
                if not repl_done and frame_number in self._replacements:
                    yield self._replacements[frame_number]

                elif (
                    not db_done
                    and frame_number == d[""frame_number""]
                    and frame_number not in self._delete_frames
                ):
                    frame = self._make_frame(d)
                    self._set_replacement(frame)

                    yield frame

            frame_number += 1

            if not repl_done:
                repl_done = max_repl_fn < frame_number

            if not db_done:
                while d[""frame_number""] < frame_number:
                    try:
                        d = next(results)
                    except StopIteration:
                        db_done = True
                        break",not db_done and frame_number == d['frame_number'] and (frame_number not in self._delete_frames),d['frame_number'] == frame_number not in self._delete_frames and (not db_done)
koalas,https://github.com/databricks/koalas/tree/master/databricks/koalas/series.py,Series,clip$1987,"def clip(self, lower: Union[float, int] = None, upper: Union[float, int] = None) -> ""Series"":
        """"""
        Trim values at input threshold(s).

        Assigns values outside boundary to boundary values.

        Parameters
        ----------
        lower : float or int, default None
            Minimum threshold value. All values below this threshold will be set to it.
        upper : float or int, default None
            Maximum threshold value. All values above this threshold will be set to it.

        Returns
        -------
        Series
            Series with the values outside the clip boundaries replaced

        Examples
        --------
        >>> ks.Series([0, 2, 4]).clip(1, 3)
        0    1
        1    2
        2    3
        dtype: int64

        Notes
        -----
        One difference between this implementation and pandas is that running
        `pd.Series(['a', 'b']).clip(0, 1)` will crash with ""TypeError: '<=' not supported between
        instances of 'str' and 'int'"" while `ks.Series(['a', 'b']).clip(0, 1)` will output the
        original Series, simply ignoring the incompatible types.
        """"""
        if is_list_like(lower) or is_list_like(upper):
            raise ValueError(
                ""List-like value are not supported for 'lower' and 'upper' at the "" + ""moment""
            )

        if lower is None and upper is None:
            return self

        if isinstance(self.spark.data_type, NumericType):
            scol = self.spark.column
            if lower is not None:
                scol = F.when(scol < lower, lower).otherwise(scol)
            if upper is not None:
                scol = F.when(scol > upper, upper).otherwise(scol)
            return self._with_new_scol(scol, dtype=self.dtype)
        else:
            return self",lower is None and upper is None,lower is None is upper
LeaderF,https://github.com/Yggdroot/LeaderF/tree/master/autoload/leaderf/python/leaderf/manager.py,Manager,_toUp$1067,"def _toUp(self):
        if self._getInstance().getWinPos() == 'popup':
            lfCmd(""call win_execute(%d, 'norm! k')"" % (self._getInstance().getPopupWinId()))
            self._getInstance().refreshPopupStatusline()
            return

        adjust = False
        if self._getInstance().isReverseOrder() and self._getInstance().getCurrentPos()[0] == 1:
            adjust = True
            self._setResultContent()
            if self._cli.pattern and self._cli.isFuzzy \
                    and len(self._highlight_pos) < (len(self._getInstance().buffer) - self._help_length) // self._getUnit() \
                    and len(self._highlight_pos) < int(lfEval(""g:Lf_NumberOfHighlight"")):
                self._highlight_method()

        lfCmd(""norm! k"")

        if adjust:
            lfCmd(""norm! zt"")

        self._getInstance().setLineNumber()
        lfCmd(""setlocal cursorline!"")   # these two help to redraw the statusline,
        lfCmd(""setlocal cursorline!"")",self._cli.pattern and self._cli.isFuzzy and (len(self._highlight_pos) < (len(self._getInstance().buffer) - self._help_length) // self._getUnit()) and (len(self._highlight_pos) < int(lfEval('g:Lf_NumberOfHighlight'))),(len(self._getInstance().buffer) - self._help_length) // self._getUnit() > len(self._highlight_pos) < int(lfEval('g:Lf_NumberOfHighlight')) and self._cli.pattern and self._cli.isFuzzy
espresso,https://github.com/freewym/espresso/tree/master/examples/MMPT/mmpt/models/mmfusion.py,MMFusion,__init__$96,"def __init__(self, config, **kwargs):
        super().__init__()
        transformer_config = AutoConfig.from_pretrained(
            config.dataset.bert_name)
        self.hidden_size = transformer_config.hidden_size
        self.is_train = False
        if config.dataset.train_path is not None:
            self.is_train = True
        # 0 means no iso; 1-12 means iso up to that layer.
        self.num_hidden_layers = transformer_config.num_hidden_layers
        self.last_iso_layer = 0
        if config.dataset.num_iso_layer is not None:
            self.last_iso_layer = config.dataset.num_iso_layer - 1 + 1

        if config.model.mm_encoder_cls is not None:
            mm_encoder_cls = getattr(transformermodel, config.model.mm_encoder_cls)
            model_config = AutoConfig.from_pretrained(config.dataset.bert_name)
            model_config.max_video_len = config.dataset.max_video_len
            # TODO: a general way to add parameter for a model.
            model_config.use_seg_emb = config.model.use_seg_emb
            self.mm_encoder = mm_encoder_cls.from_pretrained(
                config.dataset.bert_name, config=model_config)
        elif config.model.video_encoder_cls is not None\
                and config.model.text_encoder_cls is not None:
            video_encoder_cls = getattr(transformermodel, config.model.video_encoder_cls)
            model_config = AutoConfig.from_pretrained(config.dataset.bert_name)
            model_config.max_video_len = config.dataset.max_video_len
            # TODO: make each model a set of config class.
            if hasattr(model_config, ""num_layers""):
                model_config.num_layers = config.model.num_hidden_video_layers
            else:
                model_config.num_hidden_layers = config.model.num_hidden_video_layers
            self.video_encoder = video_encoder_cls.from_pretrained(
                config.dataset.bert_name, config=model_config)
            # exact same NLP model from Huggingface.
            text_encoder_cls = getattr(transformermodel, config.model.text_encoder_cls)
            self.text_encoder = text_encoder_cls.from_pretrained(
                config.dataset.bert_name)
        else:
            raise ValueError(""the encoder must be either MM or two backbones."")",config.model.video_encoder_cls is not None and config.model.text_encoder_cls is not None,config.model.video_encoder_cls is not None is not config.model.text_encoder_cls
satellite,https://github.com/Blockstream/satellite/tree/master/blocksatcli/usb.py,,_dvbnet_single$198,"def _dvbnet_single(adapter, ifname, pid, ule, existing_dvbnet_interfaces):
    """"""Start DVB network interface

    Args:
        adapter                    : DVB adapter index
        ifname                     : DVB network interface name
        pid                        : PID to listen to
        ule                        : Whether to use ULE framing
        existing_dvbnet_interfaces : List of dvbnet interfaces already
                                     configured for the adapter

    """"""

    assert (pid >= 32 and pid <= 8190), ""PID not insider range 32 to 8190""

    if (ule):
        encapsulation = 'ULE'
    else:
        encapsulation = 'MPE'

    # Check if the interface already exists
    res = subprocess.call([""ip"", ""addr"", ""show"", ""dev"", ifname],
                          stdout=subprocess.DEVNULL,
                          stderr=subprocess.DEVNULL)
    os_interface_exists = (res == 0)
    matching_dvbnet_if = None

    # When the network interface exists in the OS, we also need to check if the
    # matching dvbnet device is configured according to what we want now
    if (os_interface_exists):
        print(""Network interface %s already exists"" % (ifname))

        for interface in existing_dvbnet_interfaces:
            if (interface['name'] == ifname):
                matching_dvbnet_if = interface
                break

    # Our indication that interface exists comes from ""ip addr show
    # dev"". However, it is possible that dvbnet does not have any interface
    # associated to an adapter, so check if we found anything:
    cfg_interface = False
    if (len(existing_dvbnet_interfaces) > 0
            and matching_dvbnet_if is not None):
        # Compare to desired configurations
        if (matching_dvbnet_if['pid'] != pid
                or matching_dvbnet_if['encapsulation'] != encapsulation):
            cfg_interface = True

        if (matching_dvbnet_if['pid'] != pid):
            print(""Current PID is %d. Set it to %d"" %
                  (matching_dvbnet_if['pid'], pid))

        if (matching_dvbnet_if['encapsulation'] != encapsulation):
            print(""Current encapsulation is %s. Set it to %s"" %
                  (matching_dvbnet_if['encapsulation'], encapsulation))
    else:
        cfg_interface = True

    # Create interface in case it doesn't exist or needs to be re-created
    if (cfg_interface):
        # If interface exists, but must be re-created, remove the existing one
        # first
        if (os_interface_exists):
            _rm_dvbnet_interface(adapter, ifname, verbose=False)

        ule_arg = ""-U"" if ule else """"

        if (runner.dry):
            print(""Create interface {}:"".format(ifname))

        runner.run([""dvbnet"", ""-a"", adapter, ""-p"",
                    str(pid), ule_arg],
                   root=True)
    else:
        print(""Network interface %s already configured correctly"" % (ifname))",pid >= 32 and pid <= 8190,32 <= pid <= 8190
andriller,https://github.com/den4uk/andriller/tree/master/andriller/decoders.py,WhatsAppCallsDecoder,call_type$340,"def call_type(from_me, duration):
        if from_me == 0 and duration == 0:
            return 'Missed'
        elif from_me == 0 and duration > 0:
            return 'Received'
        elif from_me == 1:
            return 'Dialled'
        else:
            return 'Unknown'",from_me == 0 and duration == 0,from_me == 0 == duration
andriller,https://github.com/den4uk/andriller/tree/master/andriller/decoders.py,WhatsAppCallsDecoder,call_type$340,"def call_type(from_me, duration):
        if from_me == 0 and duration == 0:
            return 'Missed'
        elif from_me == 0 and duration > 0:
            return 'Received'
        elif from_me == 1:
            return 'Dialled'
        else:
            return 'Unknown'",from_me == 0 and duration > 0,from_me == 0 < duration
pandas,https://github.com/pandas-dev/pandas/tree/master/pandas/io/parsers/base_parser.py,ParserBase,_should_parse_dates$251,"def _should_parse_dates(self, i: int) -> bool:
        if isinstance(self.parse_dates, bool):
            return self.parse_dates
        else:
            if self.index_names is not None:
                name = self.index_names[i]
            else:
                name = None
            j = i if self.index_col is None else self.index_col[i]

            if is_scalar(self.parse_dates):
                return (j == self.parse_dates) or (
                    name is not None and name == self.parse_dates
                )
            else:
                return (j in self.parse_dates) or (
                    name is not None and name in self.parse_dates
                )",name is not None and name == self.parse_dates,None is not name == self.parse_dates
pandas,https://github.com/pandas-dev/pandas/tree/master/pandas/io/parsers/base_parser.py,ParserBase,_should_parse_dates$251,"def _should_parse_dates(self, i: int) -> bool:
        if isinstance(self.parse_dates, bool):
            return self.parse_dates
        else:
            if self.index_names is not None:
                name = self.index_names[i]
            else:
                name = None
            j = i if self.index_col is None else self.index_col[i]

            if is_scalar(self.parse_dates):
                return (j == self.parse_dates) or (
                    name is not None and name == self.parse_dates
                )
            else:
                return (j in self.parse_dates) or (
                    name is not None and name in self.parse_dates
                )",name is not None and name in self.parse_dates,None is not name in self.parse_dates
DeepKE,https://github.com/zjunlp/DeepKE/tree/master/example/re/document/run.py,,train$17,"def train(args, model, train_features, dev_features, test_features):
    def logging(s, print_=True, log_=True):
        if print_:
            print(s)
        if log_ and args.log_dir != '':
            with open(args.log_dir, 'a+') as f_log:
                f_log.write(s + '\n')
    def finetune(features, optimizer, num_epoch, num_steps, model):
        cur_model = model.module if hasattr(model, 'module') else model
        device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
        if args.train_from_saved_model != '':
            best_score = torch.load(args.train_from_saved_model)[""best_f1""]
            epoch_delta = torch.load(args.train_from_saved_model)[""epoch""] + 1
        else:
            epoch_delta = 0
            best_score = -1
        train_dataloader = DataLoader(features, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn, drop_last=True)
        train_iterator = [epoch + epoch_delta for epoch in range(num_epoch)]
        total_steps = int(len(train_dataloader) * num_epoch // args.gradient_accumulation_steps)
        warmup_steps = int(total_steps * args.warmup_ratio)
        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)
        print(""Total steps: {}"".format(total_steps))
        print(""Warmup steps: {}"".format(warmup_steps))
        global_step = 0
        log_step = 100
        total_loss = 0
        


        #scaler = GradScaler()
        for epoch in train_iterator:
            start_time = time.time()
            optimizer.zero_grad()

            for step, batch in enumerate(train_dataloader):
                model.train()

                inputs = {'input_ids': batch[0].to(device),
                          'attention_mask': batch[1].to(device),
                          'labels': batch[2],
                          'entity_pos': batch[3],
                          'hts': batch[4],
                          }
                #with autocast():
                outputs = model(**inputs)
                loss = outputs[0] / args.gradient_accumulation_steps
                total_loss += loss.item()
                #    scaler.scale(loss).backward()
               

                loss.backward()

                if step % args.gradient_accumulation_steps == 0:
                    #scaler.unscale_(optimizer)
                    if args.max_grad_norm > 0:
                        # torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)
                        torch.nn.utils.clip_grad_norm_(cur_model.parameters(), args.max_grad_norm)
                    #scaler.step(optimizer)
                    #scaler.update()
                    #scheduler.step()
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()
                    global_step += 1
                    num_steps += 1
                    if global_step % log_step == 0:
                        cur_loss = total_loss / log_step
                        elapsed = time.time() - start_time
                        logging(
                            '| epoch {:2d} | step {:4d} | min/b {:5.2f} | lr {} | train loss {:5.3f}'.format(
                                epoch, global_step, elapsed / 60, scheduler.get_last_lr(), cur_loss * 1000))
                        total_loss = 0
                        start_time = time.time()

                        wandb.log({
                            ""train_loss"":cur_loss
                        })

                if (step + 1) == len(train_dataloader) - 1 or (args.evaluation_steps > 0 and num_steps % args.evaluation_steps == 0 and step % args.gradient_accumulation_steps == 0):
                # if step ==0:
                    logging('-' * 89)
                    eval_start_time = time.time()
                    dev_score, dev_output = evaluate(args, model, dev_features, tag=""dev"")

                    logging(
                        '| epoch {:3d} | time: {:5.2f}s | dev_result:{}'.format(epoch, time.time() - eval_start_time,
                                                                                dev_output))

                    wandb.log({
                            ""dev_result"":dev_output
                    })

                    logging('-' * 89)
                    if dev_score > best_score:
                        best_score = dev_score
                        logging(
                            '| epoch {:3d} | best_f1:{}'.format(epoch, best_score))

                        wandb.log({
                            ""best_f1"":best_score
                        })

                        if args.save_path != """":
                            torch.save({
                                'epoch': epoch,
                                'checkpoint': cur_model.state_dict(),
                                'best_f1': best_score,
                                'optimizer': optimizer.state_dict()
                            }, args.save_path
                            , _use_new_zipfile_serialization=False)
                            logging(
                                '| successfully save model at: {}'.format(args.save_path))
                            logging('-' * 89)
        return num_steps

    cur_model = model.module if hasattr(model, 'module') else model
    extract_layer = [""extractor"", ""bilinear""]
    bert_layer = ['bert_model']
    optimizer_grouped_parameters = [
        {""params"": [p for n, p in cur_model.named_parameters() if any(nd in n for nd in bert_layer)], ""lr"": args.bert_lr},
        {""params"": [p for n, p in cur_model.named_parameters() if any(nd in n for nd in extract_layer)], ""lr"": 1e-4},
        {""params"": [p for n, p in cur_model.named_parameters() if not any(nd in n for nd in extract_layer + bert_layer)]},
    ]

    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)
    if args.train_from_saved_model != '':
        optimizer.load_state_dict(torch.load(args.train_from_saved_model)[""optimizer""])
        print(""load saved optimizer from {}."".format(args.train_from_saved_model))
    

    num_steps = 0
    set_seed(args)
    model.zero_grad()
    finetune(train_features, optimizer, args.num_train_epochs, num_steps, model)",args.evaluation_steps > 0 and num_steps % args.evaluation_steps == 0 and (step % args.gradient_accumulation_steps == 0),args.evaluation_steps > 0 == num_steps % args.evaluation_steps and step % args.gradient_accumulation_steps == 0
littleballoffur,https://github.com/benedekrozemberczki/littleballoffur/tree/master/littleballoffur/exploration_sampling/communitystructureexpansionsampler.py,CommunityStructureExpansionSampler,_create_node_set$29,"def _create_node_set(self, graph, start_node):
        """"""
        Choosing a seed node.
        """"""
        if start_node is not None:
            if start_node >= 0 and start_node < self.backend.get_number_of_nodes(graph):
                self._sampled_nodes = set([start_node])
            else:
                raise ValueError(""Starting node index is out of range."")
        else:
            self._sampled_nodes = set(
                [random.choice(range(self.backend.get_number_of_nodes(graph)))]
            )",start_node >= 0 and start_node < self.backend.get_number_of_nodes(graph),0 <= start_node < self.backend.get_number_of_nodes(graph)
PyHive,https://github.com/dropbox/PyHive/tree/master/TCLIService/ttypes.py,TRenewDelegationTokenResp,write$7045,"def write(self, oprot):
        if oprot._fast_encode is not None and self.thrift_spec is not None:
            oprot.trans.write(oprot._fast_encode(self, (self.__class__, self.thrift_spec)))
            return
        oprot.writeStructBegin('TRenewDelegationTokenResp')
        if self.status is not None:
            oprot.writeFieldBegin('status', TType.STRUCT, 1)
            self.status.write(oprot)
            oprot.writeFieldEnd()
        oprot.writeFieldStop()
        oprot.writeStructEnd()",oprot._fast_encode is not None and self.thrift_spec is not None,oprot._fast_encode is not None is not self.thrift_spec
angr,https://github.com/angr/angr/tree/master/angr/analyses/vsa_ddg.py,VSA_DDG,__init__$37,"def __init__(self,
                 vfg=None,
                 start_addr=None,
                 interfunction_level=0,
                 context_sensitivity_level=2,
                 keep_data=False,

                 ):
        """"""
        Constructor.

        :param vfg:                 An already constructed VFG. If not specified, a new VFG will be created with other
                                    specified parameters. `vfg` and `start_addr` cannot both be unspecified.
        :param start_addr:          The address where to start the analysis (typically, a function's entry point).
        :param interfunction_level: See VFG analysis.
        :param context_sensitivity_level: See VFG analysis.
        :param keep_data:           Whether we keep set of addresses as edges in the graph, or just the cardinality of
                                    the sets, which can be used as a ""weight"".
        """"""

        # sanity check
        if vfg is None and start_addr is None:
            raise AngrDDGError('Argument vfg and start_addr cannot both be unspecified.')

        if vfg is not None:
            self._vfg = vfg
        else:
            self._vfg = self.project.analyses.VFG(function_start=start_addr,
                                             interfunction_level=interfunction_level,
                                             context_sensitivity_level=context_sensitivity_level)

        self.graph = networkx.DiGraph()
        self.keep_data = keep_data

        self._simproc_map = {}
        self._imarks = {}

        self._explore()",vfg is None and start_addr is None,vfg is None is start_addr
DataStructure_Algorithm_ZJU,https://github.com/CYBruce/DataStructure_Algorithm_ZJU/tree/master/树（上）/03-树2 List Leaves.py,,InOrderTravel$25,"def InOrderTravel(T):
    stack.append(T)
    while(len(stack)!=0):
        T = stack.pop(0)
        if (tree[T][0] == '-' and tree[T][1] == '-'):
            ans.append(T)
        if (tree[T][0] != '-'):
            stack.append(int(tree[T][0]))
        if(tree[T][1] != '-'):
            stack.append(int(tree[T][1]))",tree[T][0] == '-' and tree[T][1] == '-',tree[T][0] == '-' == tree[T][1]
oio-sds,https://github.com/open-io/oio-sds/tree/master/oio/crawler/meta2/filters/auto_sharding.py,AutomaticSharding,init$37,"def init(self):
        self.sharding_strategy_params = {k[9:]: v for k, v in self.conf.items()
                                         if k.startswith(""sharding_"")}
        self.sharding_strategy = self.sharding_strategy_params.pop(
            'strategy', None)
        self.sharding_db_size = int_value(
            self.sharding_strategy_params.pop('db_size', None),
            self.DEFAULT_SHARDING_DB_SIZE)
        self.shrinking_db_size = int_value(
            self.conf.get('shrinking_db_size', None),
            self.DEFAULT_SHRINKING_DB_SIZE)
        if (self.sharding_db_size > 0
                and self.shrinking_db_size >= self.sharding_db_size):
            raise ValueError(
                'The database size for sharding '
                'must be larger than the size for shrinking')

        kwargs = dict()
        kwargs['create_shard_timeout'] = self.sharding_strategy_params.pop(
            'create_shard_timeout', None)
        kwargs['save_writes_timeout'] = self.sharding_strategy_params.pop(
            'save_writes_timeout', None)
        self.step_timeout = int_value(
            self.sharding_strategy_params.pop('step_timeout', None),
            self.DEFAULT_STEP_TIMEOUT)

        self.api = self.app_env['api']
        self.container_sharding = ContainerSharding(
            self.conf, logger=self.logger,
            pool_manager=self.api.container.pool_manager, **kwargs)

        self.skipped = 0
        self.errors = 0
        self.possible_orphan_shards = 0
        self.cleaning_successes = 0
        self.cleaning_errors = 0
        self.sharding_in_progress = 0
        self.sharding_no_change = 0
        self.sharding_successes = 0
        self.sharding_errors = 0
        self.shrinking_no_change = 0
        self.shrinking_successes = 0
        self.shrinking_errors = 0",self.sharding_db_size > 0 and self.shrinking_db_size >= self.sharding_db_size,self.shrinking_db_size >= self.sharding_db_size > 0
WordOps,https://github.com/WordOps/WordOps/tree/master/wo/cli/plugins/stack_pref.py,,pre_stack$1954,"def pre_stack(self):
    """"""Inital server configuration and tweak""""""
    # remove old sysctl tweak
    if os.path.isfile('/etc/sysctl.d/60-ubuntu-nginx-web-server.conf'):
        WOFileUtils.rm(
            self, '/etc/sysctl.d/60-ubuntu-nginx-web-server.conf')
    # check if version.txt exist
    if os.path.exists('/var/lib/wo/version.txt'):
        with open('/var/lib/wo/version.txt',
                  mode='r', encoding='utf-8') as wo_ver:
            # check version written in version.txt
            wo_check = bool(wo_ver.read().strip() ==
                            '{0}'.format(WOVar.wo_version))
    else:
        wo_check = False
    if wo_check is False:
        # wo sysctl tweaks
        # check system type
        wo_arch = bool((os.uname()[4]) == 'x86_64')
        if os.path.isfile('/proc/1/environ'):
            # detect lxc containers
            wo_lxc = WOFileUtils.grepcheck(
                self, '/proc/1/environ', 'container=lxc')
            # detect wsl
            wo_wsl = WOFileUtils.grepcheck(
                self, '/proc/1/environ', 'wsl')
        else:
            wo_wsl = True
            wo_lxc = True

        if (wo_lxc is not True) and (wo_wsl is not True) and (wo_arch is True):
            data = dict()
            WOTemplate.deploy(
                self, '/etc/sysctl.d/60-wo-tweaks.conf',
                'sysctl.mustache', data, True)
            # use tcp_bbr congestion algorithm only on new kernels
            if (WOVar.wo_platform_codename == 'bionic' or
                WOVar.wo_platform_codename == 'focal' or
                WOVar.wo_platform_codename == 'buster' or
                WOVar.wo_platform_codename == 'jammy' or
                    WOVar.wo_platform_codename == 'bullseye'):
                try:
                    WOShellExec.cmd_exec(
                        self, 'modprobe tcp_bbr')
                    with open(
                        ""/etc/modules-load.d/bbr.conf"",
                            encoding='utf-8', mode='w') as bbr_file:
                        bbr_file.write('tcp_bbr')
                    with open(
                        ""/etc/sysctl.d/60-wo-tweaks.conf"",
                            encoding='utf-8', mode='a') as sysctl_file:
                        sysctl_file.write(
                            '\nnet.ipv4.tcp_congestion_control = bbr'
                            '\nnet.ipv4.tcp_notsent_lowat = 16384')
                except OSError as e:
                    Log.debug(self, str(e))
                    Log.warn(self, ""failed to tweak sysctl"")
            else:
                try:
                    WOShellExec.cmd_exec(
                        self, 'modprobe tcp_htcp')
                    with open(
                        ""/etc/modules-load.d/htcp.conf"",
                            encoding='utf-8', mode='w') as bbr_file:
                        bbr_file.write('tcp_htcp')
                    with open(
                        ""/etc/sysctl.d/60-wo-tweaks.conf"",
                            encoding='utf-8', mode='a') as sysctl_file:
                        sysctl_file.write(
                            '\nnet.ipv4.tcp_congestion_control = htcp')
                except OSError as e:
                    Log.debug(self, str(e))
                    Log.warn(self, ""failed to tweak sysctl"")

            # apply sysctl tweaks
            WOShellExec.cmd_exec(
                self, 'sysctl -eq -p /etc/sysctl.d/60-wo-tweaks.conf')

        # sysctl tweak service
        data = dict()
        if not os.path.isfile('/opt/wo-kernel.sh'):
            WOTemplate.deploy(self, '/opt/wo-kernel.sh',
                              'wo-kernel-script.mustache', data)
        WOFileUtils.chmod(self, '/opt/wo-kernel.sh', 0o700)
        if not os.path.isfile('/lib/systemd/system/wo-kernel.service'):
            WOTemplate.deploy(
                self, '/lib/systemd/system/wo-kernel.service',
                'wo-kernel-service.mustache', data)
            WOShellExec.cmd_exec(self, 'systemctl enable wo-kernel.service')
            WOService.start_service(self, 'wo-kernel')
        # open_files_limit tweak
        if not WOFileUtils.grepcheck(self,
                                     '/etc/security/limits.conf', '500000'):
            with open(""/etc/security/limits.conf"",
                      encoding='utf-8', mode='a') as limit_file:
                limit_file.write(
                    '*         hard    nofile      500000\n'
                    '*         soft    nofile      500000\n'
                    'root      hard    nofile      500000\n'
                    'root      soft    nofile      500000\n')
        # custom motd-news
        data = dict()
        # check if update-motd.d directory exist
        if os.path.isdir('/etc/update-motd.d/'):
            # render custom motd template
            WOTemplate.deploy(
                self, '/etc/update-motd.d/98-wo-update',
                'wo-update.mustache', data)
            WOFileUtils.chmod(
                self, ""/etc/update-motd.d/98-wo-update"", 0o755)
        with open('/var/lib/wo/version.txt',
                  mode='w', encoding='utf-8') as wo_ver:
            wo_ver.write('{0}'.format(WOVar.wo_version))",wo_lxc is not True and wo_wsl is not True and (wo_arch is True),wo_lxc is not True is not wo_wsl and wo_arch is True
angr,https://github.com/angr/angr/tree/master/angr/analyses/variable_recovery/engine_base.py,SimEngineVRBase,_store_to_global$347,"def _store_to_global(self, addr: int, data: RichR, size: int, stmt=None, offset: Optional[claripy.ast.BV]=None,
                         elem_size: Optional[claripy.ast.BV]=None):
        variable_manager = self.variable_manager['global']
        if stmt is None:
            existing_vars = variable_manager.find_variables_by_stmt(self.block.addr, self.stmt_idx, 'memory')
        else:
            existing_vars = variable_manager.find_variables_by_atom(self.block.addr, self.stmt_idx, stmt)

        if offset is None or elem_size is None:
            # trivial case
            abs_addr = addr
        elif offset.concrete and elem_size.concrete:
            abs_addr = addr + offset._model_concrete.value * elem_size._model_concrete.value
        else:
            abs_addr = None

        if not existing_vars:
            # special case for global variables: find existing variable by base address
            existing_vars = { (var, (offset, elem_size)) for var in variable_manager.get_global_variables(addr) }

        if not existing_vars:
            variable = SimMemoryVariable(addr, size,
                                         ident=variable_manager.next_variable_ident('global'),
                                         )
            variable_manager.set_variable('global', addr, variable)
            l.debug('Identified a new global variable %s at %#x.', variable, self.ins_addr)
            existing_vars = {(variable, (offset, elem_size))}
        else:
            variable, _ = next(iter(existing_vars))

        data_expr: claripy.ast.Base = data.data
        data_expr = self.state.annotate_with_variables(data_expr, [(0, variable)])

        if abs_addr is not None:
            self.state.global_region.store(addr,
                                           data_expr,
                                           endness=self.state.arch.memory_endness if stmt is None else stmt.endness)

        codeloc = CodeLocation(self.block.addr, self.stmt_idx, ins_addr=self.ins_addr)
        values = None
        if abs_addr is not None:
            try:
                values: MultiValues = self.state.global_region.load(
                    abs_addr,
                    size=size,
                    endness=self.state.arch.memory_endness if stmt is None else stmt.endness)
            except SimMemoryMissingError:
                pass

        if values is not None:
            for vs in values.values.values():
                for v in vs:
                    for var_offset, var in self.state.extract_variables(v):
                        variable_manager.write_to(var, var_offset, codeloc, atom=stmt)
        else:
            for var, var_offset in existing_vars:
                variable_manager.write_to(var, var_offset, codeloc, atom=stmt)

        # create type constraints
        if not self.state.typevars.has_type_variable_for(variable, codeloc):
            typevar = typevars.TypeVariable()
            self.state.typevars.add_type_variable(variable, codeloc, typevar)
        else:
            typevar = self.state.typevars.get_type_variable(variable, codeloc)

        if data.typevar is not None:
            self.state.add_type_constraint(
                typevars.Subtype(data.typevar, typevar)
            )

        if offset is not None and elem_size is not None:
            # it's an array!
            if offset.concrete and elem_size.concrete:
                concrete_offset = offset._model_concrete.value * elem_size._model_concrete.value
                store_typevar = typevars.DerivedTypeVariable(
                    typevars.DerivedTypeVariable(typevar, typevars.Store()),
                    typevars.HasField(size * self.state.arch.byte_width, concrete_offset)
                )
                self.state.add_type_constraint(
                    typevars.Existence(store_typevar)
                )
            else:
                # FIXME: This is a hack
                for i in range(0, 4):
                    concrete_offset = size * i
                    store_typevar = typevars.DerivedTypeVariable(
                        typevars.DerivedTypeVariable(typevar, typevars.Store()),
                        typevars.HasField(size * self.state.arch.byte_width, concrete_offset)
                    )
                    self.state.add_type_constraint(
                        typevars.Existence(store_typevar)
                    )",offset is not None and elem_size is not None,offset is not None is not elem_size
quay,https://github.com/quay/quay/tree/master/data/model/user.py,,increase_maximum_build_count$172,"def increase_maximum_build_count(user, maximum_queued_builds_count):
    """"""
    Increases the maximum number of allowed builds on the namespace, if greater than that already
    present.
    """"""
    if (
        user.maximum_queued_builds_count is not None
        and maximum_queued_builds_count > user.maximum_queued_builds_count
    ):
        user.maximum_queued_builds_count = maximum_queued_builds_count
        user.save()",user.maximum_queued_builds_count is not None and maximum_queued_builds_count > user.maximum_queued_builds_count,None is not user.maximum_queued_builds_count < maximum_queued_builds_count
PaddleVideo,https://github.com/PaddlePaddle/PaddleVideo/tree/master/paddlevideo/utils/multigrid/save_load_helper.py,,sub_to_normal_bn$7,"def sub_to_normal_bn(sd):
    """"""
    When save, Convert the Sub-BN paprameters to normal BN parameters in a state dict.
    There are two copies of BN layers in a Sub-BN implementation: `bn.bn` and
    `bn.split_bn`. `bn.split_bn` is used during training and
    ""compute_precise_bn"". Before saving or evaluation, its stats are copied to
    `bn.bn`. We rename `bn.bn` to `bn` and store it to be consistent with normal
    BN layers.
    Args:
        sd (OrderedDict): a dict of parameters which might contain Sub-BN
        parameters.
    Returns:
        new_sd (OrderedDict): a dict with Sub-BN parameters reshaped to
        normal parameters.
    """"""
    modifications = [
        (""bn.bn._mean"", ""bn._mean""),
        (""bn.bn._variance"", ""bn._variance""),
    ]
    to_remove = [""bn.bn."", "".split_bn.""]
    key_list = list(sd.keys())  #odict_keys to list
    for key in key_list:
        for before, after in modifications:
            if key.endswith(before):
                new_key = key.split(before)[0] + after
                sd[new_key] = sd.pop(key)

        for rm in to_remove:
            if rm in key and key in sd:
                del sd[key]",rm in key and key in sd,rm in key in sd
highway-env,https://github.com/eleurent/highway-env/tree/master/highway_env/envs/common/observation.py,OccupancyGridObservation,fill_road_layer_by_lanes$373,"def fill_road_layer_by_lanes(self, layer_index: int, lane_perception_distance: float = 100) -> None:
        """"""
        A layer to encode the onroad (1) / offroad (0) information

        Here, we iterate over lanes and regularly placed waypoints on these lanes to fill the corresponding cells.
        This approach is faster if the grid is large and the road network is small.

        :param layer_index: index of the layer in the grid
        :param lane_perception_distance: lanes are rendered +/- this distance from vehicle location
        """"""
        lane_waypoints_spacing = np.amin(self.grid_step)
        road = self.env.road

        for _from in road.network.graph.keys():
            for _to in road.network.graph[_from].keys():
                for lane in road.network.graph[_from][_to]:
                    origin, _ = lane.local_coordinates(self.observer_vehicle.position)
                    waypoints = np.arange(origin - lane_perception_distance,
                                            origin + lane_perception_distance,
                                            lane_waypoints_spacing).clip(0, lane.length)
                    for waypoint in waypoints:
                        cell = self.pos_to_index(lane.position(waypoint, 0))
                        if 0 <= cell[1] < self.grid.shape[-2] and 0 <= cell[0] < self.grid.shape[-1]:
                            self.grid[layer_index, cell[1], cell[0]] = 1",0 <= cell[1] < self.grid.shape[-2] and 0 <= cell[0] < self.grid.shape[-1],self.grid.shape[-2] > cell[1] >= 0 <= cell[0] < self.grid.shape[-1]
pony,https://github.com/ponyorm/pony/tree/master/pony/orm/core.py,Entity,find_updated_attributes$5385,"def find_updated_attributes(obj):
        entity = obj.__class__
        attrs_to_select = []
        attrs_to_select.extend(entity._pk_attrs_)
        discr = entity._discriminator_attr_
        if discr is not None and discr.pk_offset is None:
            attrs_to_select.append(discr)
        for attr in obj._attrs_with_bit_(obj._attrs_with_columns_, obj._rbits_):
            optimistic = attr.optimistic if attr.optimistic is not None else attr.converters[0].optimistic
            if optimistic:
                attrs_to_select.append(attr)

        optimistic_converters = []
        attr_offsets = {}
        select_list = [ 'ALL' ]
        for attr in attrs_to_select:
            optimistic_converters.extend(attr.converters)
            attr_offsets[attr] = offsets = []
            for columns in attr.columns:
                select_list.append([ 'COLUMN', None, columns])
                offsets.append(len(select_list) - 2)

        from_list = [ 'FROM', [ None, 'TABLE', entity._table_ ] ]
        pk_columns = entity._pk_columns_
        pk_converters = entity._pk_converters_
        criteria_list = [ [ converter.EQ, [ 'COLUMN', None, column ], [ 'PARAM', (i, None, None), converter ] ]
                          for i, (column, converter) in enumerate(izip(pk_columns, pk_converters)) ]
        sql_ast = [ 'SELECT', select_list, from_list, [ 'WHERE' ] + criteria_list ]
        database = entity._database_
        sql, adapter = database._ast2sql(sql_ast)
        arguments = adapter(obj._get_raw_pkval_())
        cursor = database._exec_sql(sql, arguments)
        row = cursor.fetchone()
        if row is None:
            return ""Object %s was deleted outside of current transaction"" % safe_repr(obj)

        real_entity_subclass, pkval, avdict = entity._parse_row_(row, attr_offsets)
        diff = []
        for attr, new_dbval in avdict.items():
            old_dbval = obj._dbvals_[attr]
            converter = attr.converters[0]
            if old_dbval != new_dbval and (
                    attr.reverse or not converter.dbvals_equal(old_dbval, new_dbval)):
                diff.append('%s (%r -> %r)' % (attr.name, old_dbval, new_dbval))

        return ""Object %s was updated outside of current transaction%s"" % (
            safe_repr(obj), ('. Changes: %s' % ', '.join(diff) if diff else ''))",discr is not None and discr.pk_offset is None,discr is not None is discr.pk_offset
bayespy,https://github.com/bayespy/bayespy/tree/master/bayespy/inference/vmp/nodes/gaussian_markov_chain.py,VaryingGaussianMarkovChain,_constructor$1331,"def _constructor(cls, mu, Lambda, B, S, v, n=None, **kwargs):
        """"""
        Constructs distribution and moments objects.

        Compute the dimensions of phi and u.

        The plates and dimensions of the parents should be:
        mu:     (...)                    and D-dimensional
        Lambda: (...)                    and D-dimensional
        B:      (...,D)                  and (D,K)-dimensional
        S:      (...,N-1)                and K-dimensional
        v:      (...,1,D) or (...,N-1,D) and 0-dimensional
        N:      ()                       and 0-dimensional (dummy parent)

        Check that the dimensionalities of the parents are proper.
        """"""

        mu = cls._ensure_moments(mu, GaussianMoments, ndim=1)
        Lambda = cls._ensure_moments(Lambda, WishartMoments, ndim=1)
        B = cls._ensure_moments(B, GaussianMoments, ndim=2)
        S = cls._ensure_moments(S, GaussianMoments, ndim=1)
        v = cls._ensure_moments(v, GammaMoments)

        (D, K) = B.dims[0]

        parent_moments = (
            GaussianMoments((D,)),
            WishartMoments((D,)),
            GaussianMoments((D, K)),
            GaussianMoments((K,)),
            GammaMoments()
        )

        # A dummy wrapper for the number of time instances.
        n_S = 1
        if len(S.plates) >= 1:
            n_S = S.plates[-1]
        n_v = 1
        if len(v.plates) >= 2:
            n_v = v.plates[-2]
        if n_v != n_S and n_v != 1 and n_S != 1:
            raise Exception(
                ""Plates of A and v are giving different number of time ""
                ""instances"")
        n_S = max(n_v, n_S)
        if n is None:
            if n_S == 1:
                raise Exception(
                    ""The number of time instances could not be determined ""
                    ""automatically. Give the number of time instances."")

            n = n_S + 1
        elif n_S != 1 and n_S+1 != n:
            raise Exception(
                ""The number of time instances must match the number of last ""
                ""plates of parents:"" ""%d != %d+1""
                % (n, n_S))

        D = mu.dims[0][0]
        K = B.dims[0][-1]
        M = n #N.get_moments()[0]

        # Check mu
        if mu.dims != ( (D,), (D,D) ):
            raise ValueError(""First parent has wrong dimensionality"")
        # Check Lambda
        if Lambda.dims != ( (D,D), () ):
            raise ValueError(""Second parent has wrong dimensionality"")
        # Check B
        if B.dims != ( (D,K), (D,K,D,K) ):
            raise ValueError(""Third parent has wrong dimensionality {0}. Should be {1}."".format(B.dims[0], (D,K)))
        if len(B.plates) == 0 or B.plates[-1] != D:
            raise ValueError(""Third parent should have a last plate ""
                             ""equal to the dimensionality of the ""
                             ""system."")
        if S.dims != ( (K,), (K,K) ):
            raise ValueError(""Fourth parent has wrong dimensionality %s, ""
                             ""should be %s""
                             % (S.dims, ( (K,), (K,K) )))
        if (len(S.plates) >= 1
            and S.plates[-1] != 1
            and S.plates[-1] != M-1):
            raise ValueError(""The last plate of the fourth ""
                             ""parent should have length equal to one or ""
                             ""N-1, where N is the number of time ""
                             ""instances."")
        # Check v
        if v.dims != ( (), () ):
            raise Exception(""Fifth parent has wrong dimensionality"")
        if len(v.plates) == 0 or v.plates[-1] != D:
            raise Exception(""Fifth parent should have a last plate ""
                            ""equal to the dimensionality of the ""
                            ""system."")
        if (len(v.plates) >= 2
            and v.plates[-2] != 1
            and v.plates[-2] != M-1):
            raise ValueError(""The second last plate of the fifth ""
                             ""parent should have length equal to one or ""
                             ""N-1 where N is the number of time ""
                             ""instances."")

        distribution = VaryingGaussianMarkovChainDistribution(M, D)
        moments = GaussianMarkovChainMoments(M, D)

        parents = [mu, Lambda, B, S, v]

        dims = ( (M,D), (M,D,D), (M-1,D,D) )

        return (parents,
                kwargs,
                dims,
                cls._total_plates(kwargs.get('plates'),
                                  distribution.plates_from_parent(0, mu.plates),
                                  distribution.plates_from_parent(1, Lambda.plates),
                                  distribution.plates_from_parent(2, B.plates),
                                  distribution.plates_from_parent(3, S.plates),
                                  distribution.plates_from_parent(4, v.plates)),
                distribution,
                moments,
                parent_moments)",n_v != n_S and n_v != 1 and (n_S != 1),n_S != n_v != 1 != n_S
bayespy,https://github.com/bayespy/bayespy/tree/master/bayespy/inference/vmp/nodes/gaussian_markov_chain.py,VaryingGaussianMarkovChain,_constructor$1331,"def _constructor(cls, mu, Lambda, B, S, v, n=None, **kwargs):
        """"""
        Constructs distribution and moments objects.

        Compute the dimensions of phi and u.

        The plates and dimensions of the parents should be:
        mu:     (...)                    and D-dimensional
        Lambda: (...)                    and D-dimensional
        B:      (...,D)                  and (D,K)-dimensional
        S:      (...,N-1)                and K-dimensional
        v:      (...,1,D) or (...,N-1,D) and 0-dimensional
        N:      ()                       and 0-dimensional (dummy parent)

        Check that the dimensionalities of the parents are proper.
        """"""

        mu = cls._ensure_moments(mu, GaussianMoments, ndim=1)
        Lambda = cls._ensure_moments(Lambda, WishartMoments, ndim=1)
        B = cls._ensure_moments(B, GaussianMoments, ndim=2)
        S = cls._ensure_moments(S, GaussianMoments, ndim=1)
        v = cls._ensure_moments(v, GammaMoments)

        (D, K) = B.dims[0]

        parent_moments = (
            GaussianMoments((D,)),
            WishartMoments((D,)),
            GaussianMoments((D, K)),
            GaussianMoments((K,)),
            GammaMoments()
        )

        # A dummy wrapper for the number of time instances.
        n_S = 1
        if len(S.plates) >= 1:
            n_S = S.plates[-1]
        n_v = 1
        if len(v.plates) >= 2:
            n_v = v.plates[-2]
        if n_v != n_S and n_v != 1 and n_S != 1:
            raise Exception(
                ""Plates of A and v are giving different number of time ""
                ""instances"")
        n_S = max(n_v, n_S)
        if n is None:
            if n_S == 1:
                raise Exception(
                    ""The number of time instances could not be determined ""
                    ""automatically. Give the number of time instances."")

            n = n_S + 1
        elif n_S != 1 and n_S+1 != n:
            raise Exception(
                ""The number of time instances must match the number of last ""
                ""plates of parents:"" ""%d != %d+1""
                % (n, n_S))

        D = mu.dims[0][0]
        K = B.dims[0][-1]
        M = n #N.get_moments()[0]

        # Check mu
        if mu.dims != ( (D,), (D,D) ):
            raise ValueError(""First parent has wrong dimensionality"")
        # Check Lambda
        if Lambda.dims != ( (D,D), () ):
            raise ValueError(""Second parent has wrong dimensionality"")
        # Check B
        if B.dims != ( (D,K), (D,K,D,K) ):
            raise ValueError(""Third parent has wrong dimensionality {0}. Should be {1}."".format(B.dims[0], (D,K)))
        if len(B.plates) == 0 or B.plates[-1] != D:
            raise ValueError(""Third parent should have a last plate ""
                             ""equal to the dimensionality of the ""
                             ""system."")
        if S.dims != ( (K,), (K,K) ):
            raise ValueError(""Fourth parent has wrong dimensionality %s, ""
                             ""should be %s""
                             % (S.dims, ( (K,), (K,K) )))
        if (len(S.plates) >= 1
            and S.plates[-1] != 1
            and S.plates[-1] != M-1):
            raise ValueError(""The last plate of the fourth ""
                             ""parent should have length equal to one or ""
                             ""N-1, where N is the number of time ""
                             ""instances."")
        # Check v
        if v.dims != ( (), () ):
            raise Exception(""Fifth parent has wrong dimensionality"")
        if len(v.plates) == 0 or v.plates[-1] != D:
            raise Exception(""Fifth parent should have a last plate ""
                            ""equal to the dimensionality of the ""
                            ""system."")
        if (len(v.plates) >= 2
            and v.plates[-2] != 1
            and v.plates[-2] != M-1):
            raise ValueError(""The second last plate of the fifth ""
                             ""parent should have length equal to one or ""
                             ""N-1 where N is the number of time ""
                             ""instances."")

        distribution = VaryingGaussianMarkovChainDistribution(M, D)
        moments = GaussianMarkovChainMoments(M, D)

        parents = [mu, Lambda, B, S, v]

        dims = ( (M,D), (M,D,D), (M-1,D,D) )

        return (parents,
                kwargs,
                dims,
                cls._total_plates(kwargs.get('plates'),
                                  distribution.plates_from_parent(0, mu.plates),
                                  distribution.plates_from_parent(1, Lambda.plates),
                                  distribution.plates_from_parent(2, B.plates),
                                  distribution.plates_from_parent(3, S.plates),
                                  distribution.plates_from_parent(4, v.plates)),
                distribution,
                moments,
                parent_moments)",len(S.plates) >= 1 and S.plates[-1] != 1 and (S.plates[-1] != M - 1),len(S.plates) >= 1 != S.plates[-1] != M - 1
bayespy,https://github.com/bayespy/bayespy/tree/master/bayespy/inference/vmp/nodes/gaussian_markov_chain.py,VaryingGaussianMarkovChain,_constructor$1331,"def _constructor(cls, mu, Lambda, B, S, v, n=None, **kwargs):
        """"""
        Constructs distribution and moments objects.

        Compute the dimensions of phi and u.

        The plates and dimensions of the parents should be:
        mu:     (...)                    and D-dimensional
        Lambda: (...)                    and D-dimensional
        B:      (...,D)                  and (D,K)-dimensional
        S:      (...,N-1)                and K-dimensional
        v:      (...,1,D) or (...,N-1,D) and 0-dimensional
        N:      ()                       and 0-dimensional (dummy parent)

        Check that the dimensionalities of the parents are proper.
        """"""

        mu = cls._ensure_moments(mu, GaussianMoments, ndim=1)
        Lambda = cls._ensure_moments(Lambda, WishartMoments, ndim=1)
        B = cls._ensure_moments(B, GaussianMoments, ndim=2)
        S = cls._ensure_moments(S, GaussianMoments, ndim=1)
        v = cls._ensure_moments(v, GammaMoments)

        (D, K) = B.dims[0]

        parent_moments = (
            GaussianMoments((D,)),
            WishartMoments((D,)),
            GaussianMoments((D, K)),
            GaussianMoments((K,)),
            GammaMoments()
        )

        # A dummy wrapper for the number of time instances.
        n_S = 1
        if len(S.plates) >= 1:
            n_S = S.plates[-1]
        n_v = 1
        if len(v.plates) >= 2:
            n_v = v.plates[-2]
        if n_v != n_S and n_v != 1 and n_S != 1:
            raise Exception(
                ""Plates of A and v are giving different number of time ""
                ""instances"")
        n_S = max(n_v, n_S)
        if n is None:
            if n_S == 1:
                raise Exception(
                    ""The number of time instances could not be determined ""
                    ""automatically. Give the number of time instances."")

            n = n_S + 1
        elif n_S != 1 and n_S+1 != n:
            raise Exception(
                ""The number of time instances must match the number of last ""
                ""plates of parents:"" ""%d != %d+1""
                % (n, n_S))

        D = mu.dims[0][0]
        K = B.dims[0][-1]
        M = n #N.get_moments()[0]

        # Check mu
        if mu.dims != ( (D,), (D,D) ):
            raise ValueError(""First parent has wrong dimensionality"")
        # Check Lambda
        if Lambda.dims != ( (D,D), () ):
            raise ValueError(""Second parent has wrong dimensionality"")
        # Check B
        if B.dims != ( (D,K), (D,K,D,K) ):
            raise ValueError(""Third parent has wrong dimensionality {0}. Should be {1}."".format(B.dims[0], (D,K)))
        if len(B.plates) == 0 or B.plates[-1] != D:
            raise ValueError(""Third parent should have a last plate ""
                             ""equal to the dimensionality of the ""
                             ""system."")
        if S.dims != ( (K,), (K,K) ):
            raise ValueError(""Fourth parent has wrong dimensionality %s, ""
                             ""should be %s""
                             % (S.dims, ( (K,), (K,K) )))
        if (len(S.plates) >= 1
            and S.plates[-1] != 1
            and S.plates[-1] != M-1):
            raise ValueError(""The last plate of the fourth ""
                             ""parent should have length equal to one or ""
                             ""N-1, where N is the number of time ""
                             ""instances."")
        # Check v
        if v.dims != ( (), () ):
            raise Exception(""Fifth parent has wrong dimensionality"")
        if len(v.plates) == 0 or v.plates[-1] != D:
            raise Exception(""Fifth parent should have a last plate ""
                            ""equal to the dimensionality of the ""
                            ""system."")
        if (len(v.plates) >= 2
            and v.plates[-2] != 1
            and v.plates[-2] != M-1):
            raise ValueError(""The second last plate of the fifth ""
                             ""parent should have length equal to one or ""
                             ""N-1 where N is the number of time ""
                             ""instances."")

        distribution = VaryingGaussianMarkovChainDistribution(M, D)
        moments = GaussianMarkovChainMoments(M, D)

        parents = [mu, Lambda, B, S, v]

        dims = ( (M,D), (M,D,D), (M-1,D,D) )

        return (parents,
                kwargs,
                dims,
                cls._total_plates(kwargs.get('plates'),
                                  distribution.plates_from_parent(0, mu.plates),
                                  distribution.plates_from_parent(1, Lambda.plates),
                                  distribution.plates_from_parent(2, B.plates),
                                  distribution.plates_from_parent(3, S.plates),
                                  distribution.plates_from_parent(4, v.plates)),
                distribution,
                moments,
                parent_moments)",len(v.plates) >= 2 and v.plates[-2] != 1 and (v.plates[-2] != M - 1),len(v.plates) >= 2 and 1 != v.plates[-2] != M - 1
lektor,https://github.com/lektor/lektor/tree/master/lektor/sourceobj.py,SourceObject,url_to$94,"def url_to(
        self,
        path,  # : Union[str, ""SourceObject"", ""SupportsUrlPath""]
        alt: Optional[str] = None,
        absolute: Optional[bool] = None,
        external: Optional[bool] = None,
        base_url: Optional[str] = None,
        resolve: Optional[bool] = None,
        strict_resolve: Optional[bool] = None,
    ) -> str:
        """"""Calculates the URL from the current source object to the given
        other source object.  Alternatively a path can also be provided
        instead of a source object.  If the path starts with a leading
        bang (``!``) then no resolving is performed.

        If a `base_url` is provided then it's used instead of the URL of
        the record itself.

        If path is a string and resolve=False is passed, then no attempt is
        made to resolve the path to a Lektor source object.

        If path is a string and strict_resolve=True is passed, then an exception
        is raised if the path can not be resolved to a Lektor source object.

        API CHANGE: It used to be (lektor <= 3.3.1) that if absolute was true-ish,
        then a url_path (URL path relative to the site's ``base_path`` was returned.
        This is changed so that now an absolute URL path is returned.
        """"""
        if base_url is None:
            base_url = self.url_path
        if absolute:
            # This sort of reproduces the old behaviour, where when
            # ``absolute`` was trueish, the ""absolute"" URL path
            # (relative to config.base_path) was returned, regardless
            # of the value of ``external``.
            external = False
        if resolve is None and strict_resolve:
            resolve = True

        if isinstance(path, SourceObject):
            # assert not isinstance(path, Asset)
            target = path
            if alt is not None and alt != target.alt:
                # NB: path.path includes page_num
                alt_target = self.pad.get(path.path, alt=alt, persist=False)
                if alt_target is not None:
                    target = alt_target
                # FIXME: issue warning or fail if cannot get correct alt?
            url_path = target.url_path
        elif hasattr(path, ""url_path""):  # e.g. Thumbnail
            assert path.url_path.startswith(""/"")
            url_path = path.url_path
        elif path[:1] == ""!"":
            # XXX: error if used with explicit alt?
            if resolve:
                raise RuntimeError(""Resolve=True is incompatible with '!' prefix."")
            url_path = posixpath.join(self.url_path, path[1:])
        elif resolve is not None and not resolve:
            # XXX: error if used with explicit alt?
            url_path = posixpath.join(self.url_path, path)
        else:
            with ignore_url_unaffecting_dependencies():
                return self._resolve_url(
                    path,
                    alt=alt,
                    absolute=absolute,
                    external=external,
                    base_url=base_url,
                    strict=strict_resolve,
                )

        return self.pad.make_url(url_path, base_url, absolute, external)",alt is not None and alt != target.alt,None is not alt != target.alt
autorandr,https://github.com/phillipberndt/autorandr/tree/master//autorandr.py,XrandrOutput,edid_equals$514,"def edid_equals(self, other):
        ""Compare to another XrandrOutput's edid and on/off-state, taking legacy autorandr behaviour (md5sum'ing) into account""
        if self.edid and other.edid:
            if len(self.edid) == 32 and len(other.edid) != 32 and not other.edid.startswith(XrandrOutput.EDID_UNAVAILABLE):
                return hashlib.md5(binascii.unhexlify(other.edid)).hexdigest() == self.edid
            if len(self.edid) != 32 and len(other.edid) == 32 and not self.edid.startswith(XrandrOutput.EDID_UNAVAILABLE):
                return hashlib.md5(binascii.unhexlify(self.edid)).hexdigest() == other.edid
            if ""*"" in self.edid:
                return match_asterisk(self.edid, other.edid) > 0
            elif ""*"" in other.edid:
                return match_asterisk(other.edid, self.edid) > 0
        return self.edid == other.edid",len(self.edid) == 32 and len(other.edid) != 32 and (not other.edid.startswith(XrandrOutput.EDID_UNAVAILABLE)),len(self.edid) == 32 != len(other.edid) and (not other.edid.startswith(XrandrOutput.EDID_UNAVAILABLE))
autorandr,https://github.com/phillipberndt/autorandr/tree/master//autorandr.py,XrandrOutput,edid_equals$514,"def edid_equals(self, other):
        ""Compare to another XrandrOutput's edid and on/off-state, taking legacy autorandr behaviour (md5sum'ing) into account""
        if self.edid and other.edid:
            if len(self.edid) == 32 and len(other.edid) != 32 and not other.edid.startswith(XrandrOutput.EDID_UNAVAILABLE):
                return hashlib.md5(binascii.unhexlify(other.edid)).hexdigest() == self.edid
            if len(self.edid) != 32 and len(other.edid) == 32 and not self.edid.startswith(XrandrOutput.EDID_UNAVAILABLE):
                return hashlib.md5(binascii.unhexlify(self.edid)).hexdigest() == other.edid
            if ""*"" in self.edid:
                return match_asterisk(self.edid, other.edid) > 0
            elif ""*"" in other.edid:
                return match_asterisk(other.edid, self.edid) > 0
        return self.edid == other.edid",len(self.edid) != 32 and len(other.edid) == 32 and (not self.edid.startswith(XrandrOutput.EDID_UNAVAILABLE)),len(self.edid) != 32 == len(other.edid) and (not self.edid.startswith(XrandrOutput.EDID_UNAVAILABLE))
nnabla,https://github.com/sony/nnabla/tree/master/python/src/nnabla/utils/audio_utils/pydub_utils.py,,auresize$110,"def auresize(audio_arr, size, channel_first=False):
    """"""
    Resize audio_arr by pydub module.
    Args:
        audio_arr (numpy.ndarray): Audio array to save.
            Audio shape is considered as (samples, channels) by default.
        size (tupple of int): Output shape. The order is (samples, channels).
        channel_first (bool):
            This argument specifies the shape of audio is whether (samples, channels) or (channels,
            samples).
            Default value is False, which means the audio_arr shape is (samples, channels)
    Returns:
         numpy.ndarray whose shape is (samples, channels) or (channels, samples)
    """"""

    audio = _auresize_before(audio_arr, size, channel_first)
    n_channel_num = size[1]
    n_sample_num = size[0]
    o_channel_num = audio.shape[1]
    o_sample_num = audio.shape[0]

    if o_channel_num != 1 and n_channel_num != 1:
        if o_channel_num != n_channel_num:
            raise ValueError(""pydub set_channels only supports mono-to-multi channel""
                             "" and multi-to-mono channel conversion"")

    audio_segment = get_audiosegment_from_nparray(audio)
    new_rate = math.floor(48000 * n_sample_num / o_sample_num + 1)
    audio_segment = audio_segment.set_frame_rate(new_rate)
    audio_segment = audio_segment.set_channels(n_channel_num)
    audio_segment = audio_segment.get_sample_slice(0, n_sample_num)

    resized = get_nparray_from_pydub(audio_segment)

    return resized",o_channel_num != 1 and n_channel_num != 1,o_channel_num != 1 != n_channel_num
model-analysis,https://github.com/tensorflow/model-analysis/tree/master/tensorflow_model_analysis/metrics/confusion_matrix_metrics.py,DiagnosticOddsRatio,result$2067,"def result(self, tp: float, tn: float, fp: float, fn: float) -> float:
    if fn > 0.0 and fp > 0.0 and tn > 0.0:
      return (tp / fn) / (fp / tn)
    else:
      return float('nan')",fn > 0.0 and fp > 0.0 and (tn > 0.0),fn > 0.0 < fp and tn > 0.0
Python-For-Kids,https://github.com/mytechnotalent/Python-For-Kids/tree/master/Part_7_Unittest/p_0005_wonka_chocolate_machine/unittest.py,TestCase,assertAlmostEqual$89,"def assertAlmostEqual(self, x, y, places=None, msg='', delta=None):
        """"""
        Method to handle assert almost equal logic

        Params:
            x: ANY
            y: ANY
            places: NoneType, optional
            msg: str, optional
            delta: NoneType, optional
        """"""
        if x == y:
            return
        if delta is not None and places is not None:
            raise TypeError('specify delta or places not both')
        if delta is not None:
            if abs(x - y) <= delta:
                return
            if not msg:
                msg = '%r != %r within %r delta' % (x, y, delta)
        else:
            if places is None:
                places = 7
            if round(abs(y-x), places) == 0:
                return
            if not msg:
                msg = '%r != %r within %r places' % (x, y, places)
        assert False, msg",delta is not None and places is not None,delta is not None is not places
udocker,https://github.com/indigo-dc/udocker/tree/master/udocker/engine/fakechroot.py,FakechrootEngine,_get_volume_bindings$93,"def _get_volume_bindings(self):
        """"""Get the volume bindings string for fakechroot run""""""
        host_volumes_list = []
        map_volumes_list = []
        map_volumes_dict = dict()
        for vol in self.opt[""vol""]:
            (host_path, cont_path) = Uvolume(vol).split()
            if not (host_path and cont_path):
                continue
            real_host_path = os.path.realpath(host_path)
            if (host_path == cont_path and
                    Config.conf['fakechroot_expand_symlinks'] is False):
                host_volumes_list.append(host_path)
            elif (host_path == cont_path and
                  host_path in Config.conf['sysdirs_list']):
                host_volumes_list.append(host_path)
            elif host_path == cont_path and not os.path.isdir(real_host_path):
                host_volumes_list.append(host_path)
            else:
                map_volumes_dict[cont_path] = real_host_path + '!' + cont_path
                if host_path != real_host_path or os.path.isdir(real_host_path):
                    self._recommend_expand_symlinks = True
        for cont_path in sorted(map_volumes_dict, reverse=True):
            map_volumes_list.append(map_volumes_dict[cont_path])
        return (':'.join(host_volumes_list), ':'.join(map_volumes_list))",host_path == cont_path and host_path in Config.conf['sysdirs_list'],cont_path == host_path in Config.conf['sysdirs_list']
3D-R2N2,https://github.com/chrischoy/3D-R2N2/tree/master/lib/read_mesh.py,,is_quad_smooth_uv$919,"def is_quad_smooth_uv(f):
    return len(f['vertex']) == 4 and f[""normal""] and SHADING == ""smooth"" and len(f['uv']) == 4",len(f['vertex']) == 4 and f['normal'] and (SHADING == 'smooth') and (len(f['uv']) == 4),len(f['vertex']) == 4 == len(f['uv']) and SHADING == 'smooth' and f['normal']
kivy,https://github.com/kivy/kivy/tree/master/examples/miscellaneous/shapecollisions.py,BaseShape,shape_collide$166,"def shape_collide(self, x, y, *args):
        '''Point to polygon collision through a list of points.'''

        # ignore if no polygon area is set
        poly = self.poly
        if not poly:
            return False

        n = self.poly_len
        inside = False
        p1x = poly[0]
        p1y = poly[1]

        # compare point pairs via PIP algo, too long, read
        # https://en.wikipedia.org/wiki/Point_in_polygon
        for i in range(0, n + 2, 2):
            p2x = poly[i % n]
            p2y = poly[(i + 1) % n]

            if y > min(p1y, p2y) and y <= max(p1y, p2y) and x <= max(p1x, p2x):
                if p1y != p2y:
                    xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x
                if p1x == p2x or x <= xinters:
                    inside = not inside

            p1x, p1y = p2x, p2y
        return inside","y > min(p1y, p2y) and y <= max(p1y, p2y) and (x <= max(p1x, p2x))","min(p1y, p2y) < y <= max(p1y, p2y) and x <= max(p1x, p2x)"
coding-interview-gym,https://github.com/partho-maple/coding-interview-gym/tree/master/leetcode.com/python/490_The_Maze.py,Solution,hasPath$4,"def hasPath(self, maze, start, destination):
        """"""
        :type maze: List[List[int]]
        :type start: List[int]
        :type destination: List[int]
        :rtype: bool
        """"""
        if not maze or not start or not destination:
            # bad input
            return False
        if start == destination:
            # input start and destination were the same
            return True

        q = deque([(start[0], start[1])])
        # using a deque is important when used as a queue
        # stack here would be DFS
        visited = set()
        # a set will provide constant time access, we will never have duplicates
        directions_to_go = [(0, 1), (0, -1), (1, 0), (-1, 0)]
        # we can always roll north, south, east, or west

        while q:
            current = q.popleft()
            # first in first out (swap to pop here with stack for DFS)
            if current[0] == destination[0] and current[1] == destination[1]:
                return True
            for direction in directions_to_go:
                # move in a direction
                x = current[0] + direction[0]
                y = current[1] + direction[1]
                while 0 <= x < len(maze) and 0 <= y < len(maze[0]) and maze[x][y] == 0:
                    # keep moving until ONE PAST where you can't move (roll) anymore
                    x += direction[0]
                    y += direction[1]
                # roll back one so that you're actually where you should be
                rolled_to_x = x - direction[0]
                rolled_to_y = y - direction[1]
                if (rolled_to_x, rolled_to_y) not in visited:
                    visited.add((rolled_to_x, rolled_to_y))
                    # add this position to be searched from as well
                    q.append((rolled_to_x, rolled_to_y))
        # if you're here no solution was found and everything has been visited
        return False",0 <= x < len(maze) and 0 <= y < len(maze[0]) and (maze[x][y] == 0),len(maze) > x >= 0 <= y < len(maze[0]) and maze[x][y] == 0
PaddleViT,https://github.com/BR-IDL/PaddleViT/tree/master/image_classification/MobileViT/mixup.py,Mixup,get_params$181,"def get_params(self):
        """"""Decide to use cutmix or regular mixup by sampling and
           sample lambda for mixup
        """"""
        lam = 1.
        use_cutmix = False
        use_mixup = np.random.rand() < self.mix_prob
        if use_mixup:
            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:
                use_cutmix = np.random.rand() < self.switch_prob
                alpha = self.cutmix_alpha if use_cutmix else self.mixup_alpha
                lam_mix = np.random.beta(alpha, alpha)
            elif self.mixup_alpha == 0. and self.cutmix_alpha > 0.:
                use_cutmix=True
                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)
            elif self.mixup_alpha > 0. and self.cutmix_alpha == 0.:
                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)
            else:
                raise ValueError('mixup_alpha and cutmix_alpha cannot be all 0')
            lam = float(lam_mix)
        return lam, use_cutmix",self.mixup_alpha > 0.0 and self.cutmix_alpha > 0.0,self.mixup_alpha > 0.0 < self.cutmix_alpha
PaddleViT,https://github.com/BR-IDL/PaddleViT/tree/master/image_classification/MobileViT/mixup.py,Mixup,get_params$181,"def get_params(self):
        """"""Decide to use cutmix or regular mixup by sampling and
           sample lambda for mixup
        """"""
        lam = 1.
        use_cutmix = False
        use_mixup = np.random.rand() < self.mix_prob
        if use_mixup:
            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:
                use_cutmix = np.random.rand() < self.switch_prob
                alpha = self.cutmix_alpha if use_cutmix else self.mixup_alpha
                lam_mix = np.random.beta(alpha, alpha)
            elif self.mixup_alpha == 0. and self.cutmix_alpha > 0.:
                use_cutmix=True
                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)
            elif self.mixup_alpha > 0. and self.cutmix_alpha == 0.:
                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)
            else:
                raise ValueError('mixup_alpha and cutmix_alpha cannot be all 0')
            lam = float(lam_mix)
        return lam, use_cutmix",self.mixup_alpha == 0.0 and self.cutmix_alpha > 0.0,self.mixup_alpha == 0.0 < self.cutmix_alpha
PaddleViT,https://github.com/BR-IDL/PaddleViT/tree/master/image_classification/MobileViT/mixup.py,Mixup,get_params$181,"def get_params(self):
        """"""Decide to use cutmix or regular mixup by sampling and
           sample lambda for mixup
        """"""
        lam = 1.
        use_cutmix = False
        use_mixup = np.random.rand() < self.mix_prob
        if use_mixup:
            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:
                use_cutmix = np.random.rand() < self.switch_prob
                alpha = self.cutmix_alpha if use_cutmix else self.mixup_alpha
                lam_mix = np.random.beta(alpha, alpha)
            elif self.mixup_alpha == 0. and self.cutmix_alpha > 0.:
                use_cutmix=True
                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)
            elif self.mixup_alpha > 0. and self.cutmix_alpha == 0.:
                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)
            else:
                raise ValueError('mixup_alpha and cutmix_alpha cannot be all 0')
            lam = float(lam_mix)
        return lam, use_cutmix",self.mixup_alpha > 0.0 and self.cutmix_alpha == 0.0,self.mixup_alpha > 0.0 == self.cutmix_alpha
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/select_sentences_tfidf.py,,select_pars$23,"def select_pars(qa_dct, docs_list, word_freqs, n_sents=100, n_context=3):
    question = qa_dct['title'][0]
    split_docs = [sentence_split(doc['text'][0], max_len=64) for doc in docs_list]
    q_ti_dct = tf_idf_vec(question, word_freqs['title'][0], word_freqs['title'][1])
    split_docs_pre = [
        (i, j, sen, tf_idf_vec(sen, word_freqs['doc'][0], word_freqs['doc'][1]))
        for i, doc in enumerate(split_docs)
        for j, sen in enumerate(doc)
    ]
    split_docs_sc = [
        (i, j, tf_idf_dist(q_ti_dct, dct))
        for k, (i, j, sen, dct) in enumerate(split_docs_pre)
        if len(sen.split()) >= 4 and sen not in [x[2] for x in split_docs_pre[:k]]
    ]
    split_docs_sort = sorted(split_docs_sc, key=lambda x: x[-1], reverse=True)[:n_sents]
    select_ids = sorted([(i, j) for i, j, _ in split_docs_sort])
    par_ids = []
    this_par = []
    last_seen = (-1, -1)
    for i, j in select_ids:
        if i > last_seen[0]:
            par_ids += [this_par]
            this_par = []
            for k in range(-n_context, n_context + 1):
                if j + k >= 0 and j + k < len(split_docs[i]):
                    this_par += [(i, j + k)]
                    last_seen = (i, j + k)
        else:
            if j - n_context > last_seen[1] + 1:
                par_ids += [this_par]
                this_par = []
            for k in range(-n_context, n_context + 1):
                if j + k > last_seen[1] and j + k >= 0 and j + k < len(split_docs[i]):
                    this_par += [(i, j + k)]
                    last_seen = (i, j + k)
    par_ids = par_ids[1:] + [this_par]
    extract_doc = ' <P> '.join(
        [''] + [' '.join([split_docs[i][j] for i, j in par]) for par in par_ids]
    ).strip()
    return extract_doc",j + k >= 0 and j + k < len(split_docs[i]),0 <= j + k < len(split_docs[i])
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/select_sentences_tfidf.py,,select_pars$23,"def select_pars(qa_dct, docs_list, word_freqs, n_sents=100, n_context=3):
    question = qa_dct['title'][0]
    split_docs = [sentence_split(doc['text'][0], max_len=64) for doc in docs_list]
    q_ti_dct = tf_idf_vec(question, word_freqs['title'][0], word_freqs['title'][1])
    split_docs_pre = [
        (i, j, sen, tf_idf_vec(sen, word_freqs['doc'][0], word_freqs['doc'][1]))
        for i, doc in enumerate(split_docs)
        for j, sen in enumerate(doc)
    ]
    split_docs_sc = [
        (i, j, tf_idf_dist(q_ti_dct, dct))
        for k, (i, j, sen, dct) in enumerate(split_docs_pre)
        if len(sen.split()) >= 4 and sen not in [x[2] for x in split_docs_pre[:k]]
    ]
    split_docs_sort = sorted(split_docs_sc, key=lambda x: x[-1], reverse=True)[:n_sents]
    select_ids = sorted([(i, j) for i, j, _ in split_docs_sort])
    par_ids = []
    this_par = []
    last_seen = (-1, -1)
    for i, j in select_ids:
        if i > last_seen[0]:
            par_ids += [this_par]
            this_par = []
            for k in range(-n_context, n_context + 1):
                if j + k >= 0 and j + k < len(split_docs[i]):
                    this_par += [(i, j + k)]
                    last_seen = (i, j + k)
        else:
            if j - n_context > last_seen[1] + 1:
                par_ids += [this_par]
                this_par = []
            for k in range(-n_context, n_context + 1):
                if j + k > last_seen[1] and j + k >= 0 and j + k < len(split_docs[i]):
                    this_par += [(i, j + k)]
                    last_seen = (i, j + k)
    par_ids = par_ids[1:] + [this_par]
    extract_doc = ' <P> '.join(
        [''] + [' '.join([split_docs[i][j] for i, j in par]) for par in par_ids]
    ).strip()
    return extract_doc",j + k > last_seen[1] and j + k >= 0 and (j + k < len(split_docs[i])),last_seen[1] < j + k >= 0 and j + k < len(split_docs[i])
Text-Pastry,https://github.com/duydao/Text-Pastry/tree/master//text_pastry.py,Overlay,get$1878,"def get(self, index):
        item = None
        if index >= 0 and index < len(self._items): item = self._items[index]
        return item",index >= 0 and index < len(self._items),0 <= index < len(self._items)
pandas,https://github.com/pandas-dev/pandas/tree/master/pandas/core/internals/array_manager.py,BaseArrayManager,apply_with_block$258,"def apply_with_block(
        self: T, f, align_keys=None, swap_axis: bool = True, **kwargs
    ) -> T:
        # switch axis to follow BlockManager logic
        if swap_axis and ""axis"" in kwargs and self.ndim == 2:
            kwargs[""axis""] = 1 if kwargs[""axis""] == 0 else 0

        align_keys = align_keys or []
        aligned_args = {k: kwargs[k] for k in align_keys}

        result_arrays = []

        for i, arr in enumerate(self.arrays):

            if aligned_args:
                for k, obj in aligned_args.items():
                    if isinstance(obj, (ABCSeries, ABCDataFrame)):
                        # The caller is responsible for ensuring that
                        #  obj.axes[-1].equals(self.items)
                        if obj.ndim == 1:
                            if self.ndim == 2:
                                kwargs[k] = obj.iloc[slice(i, i + 1)]._values
                            else:
                                kwargs[k] = obj.iloc[:]._values
                        else:
                            kwargs[k] = obj.iloc[:, [i]]._values
                    else:
                        # otherwise we have an ndarray
                        if obj.ndim == 2:
                            kwargs[k] = obj[[i]]

            if isinstance(arr.dtype, np.dtype) and not isinstance(arr, np.ndarray):
                # i.e. TimedeltaArray, DatetimeArray with tz=None. Need to
                #  convert for the Block constructors.
                arr = np.asarray(arr)

            if self.ndim == 2:
                arr = ensure_block_shape(arr, 2)
                block = new_block(arr, placement=slice(0, 1, 1), ndim=2)
            else:
                block = new_block(arr, placement=slice(0, len(self), 1), ndim=1)

            applied = getattr(block, f)(**kwargs)
            if isinstance(applied, list):
                applied = applied[0]
            arr = applied.values
            if self.ndim == 2 and arr.ndim == 2:
                # 2D for np.ndarray or DatetimeArray/TimedeltaArray
                assert len(arr) == 1
                # error: No overload variant of ""__getitem__"" of ""ExtensionArray""
                # matches argument type ""Tuple[int, slice]""
                arr = arr[0, :]  # type: ignore[call-overload]
            result_arrays.append(arr)

        return type(self)(result_arrays, self._axes)",self.ndim == 2 and arr.ndim == 2,self.ndim == 2 == arr.ndim
netflix-proxy,https://github.com/ab77/netflix-proxy/tree/master/auth/auth.py,Delete,POST$533,"def POST(self):
        auth_form = get_form()
        if not auth_form.validates():
            flash('error', 'form validation failed')
            return render.form(get_form(name='delete'))

        is_ipv4 = web.net.validipaddr(auth_form['ipaddr'].value)
        is_ipv6 = web.net.validip6addr(auth_form['ipaddr'].value)
        if is_ipv4 == False and is_ipv6 == False:
            flash('error', '%s is not a valid ipv4/6 address' % auth_form['ipaddr'].value)
            return render.form(get_form(name='delete'))
        
        web.debug('De-authorising ipaddr=%s' % auth_form['ipaddr'].value)
        web.header('Content-Type', 'text/html')
        db_result = db.delete('ipaddrs', where=""user_id=%s AND ipaddr='%s'"" % (session.user['ID'],
                                                                               auth_form['ipaddr'].value))
        web.debug('db.delete: %s' % db_result)
        if db_result == 0: db_result = 1
        for i in range(0, db_result):
            if is_ipv4: result = run_ipt_cmd(auth_form['ipaddr'].value, 'D')
            if is_ipv6: result = run_ipt6_cmd(auth_form['ipaddr'].value, 'D') 
            web.debug('iptables_update: %s' % [result])
        session.auth_ip_count -= 1
        flash('success', '%s de-authorized' % auth_form['ipaddr'].value)
        return render.form(get_form(name='delete'))",is_ipv4 == False and is_ipv6 == False,is_ipv4 == False == is_ipv6
Tencent2019_Finals_Rank1st,https://github.com/bettenW/Tencent2019_Finals_Rank1st/tree/master/gdy/extract_features.py,,history_static_log$311,"def history_static_log(train_df,test_df,f,flag):
    label='label'
    log=pd.read_pickle('preprocess_data/log_label_{}.pkl'.format(flag))
    if f!='aid':
        op_df=pd.read_csv('../data/final_map_bid_opt.out', sep='\t',names=['aid','op_time','op_type','objective','bid_type','bid']).sort_values(by=['aid','op_time'])
        op_df['day']=op_df['op_time']//1000000%100
        op_dfs=[]
        for day in range(10,25): 
            op_df1=op_df[op_df['day']<=day]
            op_df1=op_df1.drop_duplicates('aid',keep='last')
            op_df1['day']=day
            op_dfs.append(op_df1)
        op_df=pd.concat(op_dfs,0)
        ad_df =pd.read_pickle('preprocess_data/ad_static_feature.pkl')
        ad_df['good_id_advertiser']=ad_df['good_id']*1000000+ad_df['advertiser']
        log=log.merge(ad_df,on=['aid'],how='left')
        log=log.merge(op_df,on=['aid','day'],how='left')
    log['day']=log['day'].astype(int)
    log[f]=log[f].fillna(-1).astype(int)
    print(""history_static_log:"",f+'_'+label)
    #K-fold positive and negative num



    dic_all={}
    for item in tqdm(log[['day',f,'label_0','label_1']].values,total=len(log)):
        try:
            dic_all[(item[0],item[1])][0]+=item[2]
            dic_all[(item[0],item[1])][1]+=item[3]
        except:
            dic_all[(item[0],item[1])]=[0,0]
            dic_all[(item[0],item[1])][0]+=item[2]
            dic_all[(item[0],item[1])][1]+=item[3]
    print(""static done!"")              
    positive=[]
    negative=[]
    sequence=[]
    rate=[]
    for item in train_df[['day',f,'request_cont']].values:
        n=0
        p=0
        first=True
        temp=[]
        for day in range(int(item[0])-1,9,-1):
            if (day,item[1]) in dic_all:
                if first is True:
                    n+=dic_all[(day,item[1])][0]
                    p+=dic_all[(day,item[1])][1]
                    first=False
                k0,k1=dic_all[(day,item[1])]

        if p==0 and n==0:
            positive.append(-1)
            negative.append(-1)
            rate.append(0)
        else:
            positive.append(p)
            negative.append(n)
            rate.append(p/(p+n))
            
    train_df[f+'_'+'history'+'_positive_num']=positive
    train_df[f+'_'+'history'+'_negative_num']=negative
    train_df[f+'_'+'history'+'_rate']=rate

    
    print(""train done!"")
    #for test
    positive=[]
    negative=[]
    sequence=[]
    rate=[]
    log_rate=[]
    for item in test_df[['day',f,'request_cont']].values:
        n=0
        p=0
        first=True
        temp=[]
        for day in range(int(item[0])-1,9,-1):
            if (day,item[1]) in dic_all:
                if first is True:
                    n+=dic_all[(day,item[1])][0]
                    p+=dic_all[(day,item[1])][1]
                    first=False
                k0,k1=dic_all[(day,item[1])]

        sequence.append(' '.join(temp))
        if p==0 and n==0:
            positive.append(-1)
            negative.append(-1)
            rate.append(0)
            log_rate.append(0)
        else:
            positive.append(p)
            negative.append(n)
            rate.append(p/(p+n))
            log_rate.append(np.log(p+1)/np.log(p+n+1))
        
    test_df[f+'_'+'history'+'_positive_num']=positive
    test_df[f+'_'+'history'+'_negative_num']=negative  
    test_df[f+'_'+'history'+'_rate']=rate

    print(""test done!"")
    train_df[f+'_predict_imp']=train_df[f+'_'+'history'+'_rate']*train_df['request_cont']
    test_df[f+'_predict_imp']=test_df[f+'_'+'history'+'_rate']*test_df['request_cont']  
    del log
    gc.collect()
    print(f+'_'+label+'_positive_num')
    print(f+'_'+label+'_negative_num')
    print(f+'_'+label+'_rate')
    print('avg of positive num',np.mean(train_df[f+'_'+'history'+'_positive_num']),np.mean(test_df[f+'_'+'history'+'_positive_num']))
    print('avg of negative num',np.mean(train_df[f+'_'+'history'+'_negative_num']),np.mean(test_df[f+'_'+'history'+'_negative_num']))
    print('avg of rate',np.mean(train_df[f+'_'+'history'+'_rate']),np.mean(test_df[f+'_'+'history'+'_rate']))
    print('avg of predict imp',train_df[f+'_predict_imp'].mean(),test_df[f+'_predict_imp'].mean())",p == 0 and n == 0,p == 0 == n
Tencent2019_Finals_Rank1st,https://github.com/bettenW/Tencent2019_Finals_Rank1st/tree/master/gdy/extract_features.py,,history_static_log$311,"def history_static_log(train_df,test_df,f,flag):
    label='label'
    log=pd.read_pickle('preprocess_data/log_label_{}.pkl'.format(flag))
    if f!='aid':
        op_df=pd.read_csv('../data/final_map_bid_opt.out', sep='\t',names=['aid','op_time','op_type','objective','bid_type','bid']).sort_values(by=['aid','op_time'])
        op_df['day']=op_df['op_time']//1000000%100
        op_dfs=[]
        for day in range(10,25): 
            op_df1=op_df[op_df['day']<=day]
            op_df1=op_df1.drop_duplicates('aid',keep='last')
            op_df1['day']=day
            op_dfs.append(op_df1)
        op_df=pd.concat(op_dfs,0)
        ad_df =pd.read_pickle('preprocess_data/ad_static_feature.pkl')
        ad_df['good_id_advertiser']=ad_df['good_id']*1000000+ad_df['advertiser']
        log=log.merge(ad_df,on=['aid'],how='left')
        log=log.merge(op_df,on=['aid','day'],how='left')
    log['day']=log['day'].astype(int)
    log[f]=log[f].fillna(-1).astype(int)
    print(""history_static_log:"",f+'_'+label)
    #K-fold positive and negative num



    dic_all={}
    for item in tqdm(log[['day',f,'label_0','label_1']].values,total=len(log)):
        try:
            dic_all[(item[0],item[1])][0]+=item[2]
            dic_all[(item[0],item[1])][1]+=item[3]
        except:
            dic_all[(item[0],item[1])]=[0,0]
            dic_all[(item[0],item[1])][0]+=item[2]
            dic_all[(item[0],item[1])][1]+=item[3]
    print(""static done!"")              
    positive=[]
    negative=[]
    sequence=[]
    rate=[]
    for item in train_df[['day',f,'request_cont']].values:
        n=0
        p=0
        first=True
        temp=[]
        for day in range(int(item[0])-1,9,-1):
            if (day,item[1]) in dic_all:
                if first is True:
                    n+=dic_all[(day,item[1])][0]
                    p+=dic_all[(day,item[1])][1]
                    first=False
                k0,k1=dic_all[(day,item[1])]

        if p==0 and n==0:
            positive.append(-1)
            negative.append(-1)
            rate.append(0)
        else:
            positive.append(p)
            negative.append(n)
            rate.append(p/(p+n))
            
    train_df[f+'_'+'history'+'_positive_num']=positive
    train_df[f+'_'+'history'+'_negative_num']=negative
    train_df[f+'_'+'history'+'_rate']=rate

    
    print(""train done!"")
    #for test
    positive=[]
    negative=[]
    sequence=[]
    rate=[]
    log_rate=[]
    for item in test_df[['day',f,'request_cont']].values:
        n=0
        p=0
        first=True
        temp=[]
        for day in range(int(item[0])-1,9,-1):
            if (day,item[1]) in dic_all:
                if first is True:
                    n+=dic_all[(day,item[1])][0]
                    p+=dic_all[(day,item[1])][1]
                    first=False
                k0,k1=dic_all[(day,item[1])]

        sequence.append(' '.join(temp))
        if p==0 and n==0:
            positive.append(-1)
            negative.append(-1)
            rate.append(0)
            log_rate.append(0)
        else:
            positive.append(p)
            negative.append(n)
            rate.append(p/(p+n))
            log_rate.append(np.log(p+1)/np.log(p+n+1))
        
    test_df[f+'_'+'history'+'_positive_num']=positive
    test_df[f+'_'+'history'+'_negative_num']=negative  
    test_df[f+'_'+'history'+'_rate']=rate

    print(""test done!"")
    train_df[f+'_predict_imp']=train_df[f+'_'+'history'+'_rate']*train_df['request_cont']
    test_df[f+'_predict_imp']=test_df[f+'_'+'history'+'_rate']*test_df['request_cont']  
    del log
    gc.collect()
    print(f+'_'+label+'_positive_num')
    print(f+'_'+label+'_negative_num')
    print(f+'_'+label+'_rate')
    print('avg of positive num',np.mean(train_df[f+'_'+'history'+'_positive_num']),np.mean(test_df[f+'_'+'history'+'_positive_num']))
    print('avg of negative num',np.mean(train_df[f+'_'+'history'+'_negative_num']),np.mean(test_df[f+'_'+'history'+'_negative_num']))
    print('avg of rate',np.mean(train_df[f+'_'+'history'+'_rate']),np.mean(test_df[f+'_'+'history'+'_rate']))
    print('avg of predict imp',train_df[f+'_predict_imp'].mean(),test_df[f+'_predict_imp'].mean())",p == 0 and n == 0,p == 0 == n
gpodder,https://github.com/gpodder/gpodder/tree/master/share/gpodder/extensions/mpris-listener.py,,subsecond_difference$49,"def subsecond_difference(usec1, usec2):
    return usec1 is not None and usec2 is not None and abs(usec1 - usec2) < USECS_IN_SEC",usec1 is not None and usec2 is not None and (abs(usec1 - usec2) < USECS_IN_SEC),usec1 is not None is not usec2 and abs(usec1 - usec2) < USECS_IN_SEC
texar,https://github.com/asyml/texar/tree/master/texar/tf/utils/average_recorder.py,AverageRecorder,__init__$158,"def __init__(self, size=None):
        if size is not None and size <= 0:
            raise ValueError(""`size` must be > 0 or `None`."")
        self._size = size
        self._recorders = None
        self._default_metric_name = ""metric""
        self._record_type = None",size is not None and size <= 0,None is not size <= 0
Lenia,https://github.com/Chakazul/Lenia/tree/master/Python/LeniaND.py,Lenia,change_stat_axis$1973,"def change_stat_axis(self, axis1, axis2, d):
        if self.stats_mode == 0:
            self.stats_mode = 1
        while True:
            axis1 = (axis1 + d) % len(self.analyzer.STAT_HEADERS)
            if axis1 != axis2 and axis1 > 2: break
        return axis1",axis1 != axis2 and axis1 > 2,axis2 != axis1 > 2
strictyaml,https://github.com/crdoconnor/strictyaml/tree/master/strictyaml/ruamel/emitter.py,Emitter,write_folded$1491,"def write_folded(self, text):
        # type: (Any) -> None
        hints, _indent, _indicator = self.determine_block_hints(text)
        self.write_indicator(u"">"" + hints, True)
        if _indicator == u""+"":
            self.open_ended = True
        self.write_line_break()
        leading_space = True
        spaces = False
        breaks = True
        start = end = 0
        while end <= len(text):
            ch = None
            if end < len(text):
                ch = text[end]
            if breaks:
                if ch is None or ch not in u""\n\x85\u2028\u2029\a"":
                    if (
                        not leading_space
                        and ch is not None
                        and ch != u"" ""
                        and text[start] == u""\n""
                    ):
                        self.write_line_break()
                    leading_space = ch == u"" ""
                    for br in text[start:end]:
                        if br == u""\n"":
                            self.write_line_break()
                        else:
                            self.write_line_break(br)
                    if ch is not None:
                        self.write_indent()
                    start = end
            elif spaces:
                if ch != u"" "":
                    if start + 1 == end and self.column > self.best_width:
                        self.write_indent()
                    else:
                        data = text[start:end]
                        self.column += len(data)
                        if bool(self.encoding):
                            data = data.encode(self.encoding)
                        self.stream.write(data)
                    start = end
            else:
                if ch is None or ch in u"" \n\x85\u2028\u2029\a"":
                    data = text[start:end]
                    self.column += len(data)
                    if bool(self.encoding):
                        data = data.encode(self.encoding)
                    self.stream.write(data)
                    if ch == u""\a"":
                        if end < (len(text) - 1) and not text[end + 2].isspace():
                            self.write_line_break()
                            self.write_indent()
                            end += 2  # \a and the space that is inserted on the fold
                        else:
                            raise EmitterError(
                                ""unexcpected fold indicator \\a before space""
                            )
                    if ch is None:
                        self.write_line_break()
                    start = end
            if ch is not None:
                breaks = ch in u""\n\x85\u2028\u2029""
                spaces = ch == u"" ""
            end += 1",not leading_space and ch is not None and (ch != u' ') and (text[start] == u'\n'),None is not ch != u' ' and text[start] == u'\n' and (not leading_space)
bayespy,https://github.com/bayespy/bayespy/tree/master/bayespy/inference/vmp/nodes/CovarianceFunctions.py,,covfunc_delta$226,"def covfunc_delta(amplitude, x1, x2=None, gradient=False):

    # Make sure that amplitude is a scalar, not an array object
    amplitude = utils.array_to_scalar(amplitude)

    ## if gradient:
    ##     gradient_amplitude = gradient[0]
    ## else:
    ##     gradient_amplitude = []

    ## inputs = gp_preprocess_inputs(*inputs)

    # Compute distance and covariance matrix
    if x2 is None:
        x1 = gp_preprocess_inputs(x1)
        # Only variance vector asked
        #x = inputs[0]
        N = np.shape(x1)[0]
        K = np.ones(N) * amplitude**2

    else:
        (x1,x2) = gp_preprocess_inputs(x1,x2)
        # Full covariance matrix asked
        #x1 = inputs[0]
        #x2 = inputs[1]
        # Number of inputs x1
        N1 = np.shape(x1)[0]

        # x1 == x2?
        if x1 is x2:
            delta = True
            # Delta covariance
            #
            # FIXME: Broadcasting doesn't work with sparse matrices,
            # so must use scalar multiplication
            K = gp_cov_delta(N1) * amplitude**2
            #K = gp_cov_delta(N1).multiply(amplitude**2)
        else:
            delta = False
            # Number of inputs x2
            N2 = np.shape(x2)[0]
            # Zero covariance
            if N1 > 0 and N2 > 0:
                K = sp.csc_matrix((N1,N2))
            else:
                K = np.zeros((N1,N2))

    # Gradient w.r.t. amplitude
    if gradient:
        # FIXME: Broadcasting doesn't work with sparse matrices,
        # so must use scalar multiplication
        gradient_amplitude = K*(2/amplitude)
        print(""noise grad"", gradient_amplitude)
        return (K, (gradient_amplitude,))
    else:
        return K",N1 > 0 and N2 > 0,N1 > 0 < N2
oio-sds,https://github.com/open-io/oio-sds/tree/master/tests/functional/api/test_rdir.py,TestRdirClient,_assert_chunk_status$324,"def _assert_chunk_status(self, expected_entries, status,
                             max=0, incident=False):
        expected_status = dict()
        expected_status['chunk'] = {'total': len(expected_entries)}
        expected_status['container'] = dict()
        for entry in expected_entries:
            expected_status['container'][entry[0]]['total'] = \
                expected_status['container'].setdefault(
                    entry[0], dict()).get('total', 0) + 1
        if incident:
            expected_entries_rebuild = [entry for entry in expected_entries
                                        if entry[1][-1] == '0']
            expected_status['chunk']['to_rebuild'] = \
                len(expected_entries_rebuild)
            for entry in expected_entries_rebuild:
                expected_status['container'][entry[0]]['to_rebuild'] = \
                    expected_status['container'][entry[0]].get(
                        'to_rebuild', 0) + 1
        self.assertDictEqual(expected_status, status)
        nb_requests = 1
        if max > 0 and len(expected_entries) > 0:
            nb_requests = int(math.ceil(len(expected_entries)/float(max)))
        self.assertEqual(nb_requests, self.rdir._direct_request.call_count)
        self.rdir._direct_request.reset_mock()",max > 0 and len(expected_entries) > 0,max > 0 < len(expected_entries)
swift,https://github.com/openstack/swift/tree/master/swift/common/middleware/s3api/controllers/multi_upload.py,UploadsController,GET$261,"def GET(self, req):
        """"""
        Handles List Multipart Uploads
        """"""

        def separate_uploads(uploads, prefix, delimiter):
            """"""
            separate_uploads will separate uploads into non_delimited_uploads
            (a subset of uploads) and common_prefixes according to the
            specified delimiter. non_delimited_uploads is a list of uploads
            which exclude the delimiter. common_prefixes is a set of prefixes
            prior to the specified delimiter. Note that the prefix in the
            common_prefixes includes the delimiter itself.

            i.e. if '/' delimiter specified and then the uploads is consists of
            ['foo', 'foo/bar'], this function will return (['foo'], ['foo/']).

            :param uploads: A list of uploads dictionary
            :param prefix: A string of prefix reserved on the upload path.
                           (i.e. the delimiter must be searched behind the
                            prefix)
            :param delimiter: A string of delimiter to split the path in each
                              upload

            :return (non_delimited_uploads, common_prefixes)
            """"""
            if six.PY2:
                (prefix, delimiter) = utf8encode(prefix, delimiter)
            non_delimited_uploads = []
            common_prefixes = set()
            for upload in uploads:
                key = upload['key']
                end = key.find(delimiter, len(prefix))
                if end >= 0:
                    common_prefix = key[:end + len(delimiter)]
                    common_prefixes.add(common_prefix)
                else:
                    non_delimited_uploads.append(upload)
            return non_delimited_uploads, sorted(common_prefixes)

        encoding_type = get_param(req, 'encoding-type')
        if encoding_type is not None and encoding_type != 'url':
            err_msg = 'Invalid Encoding Method specified in Request'
            raise InvalidArgument('encoding-type', encoding_type, err_msg)

        keymarker = get_param(req, 'key-marker', '')
        uploadid = get_param(req, 'upload-id-marker', '')
        maxuploads = req.get_validated_param(
            'max-uploads', DEFAULT_MAX_UPLOADS, DEFAULT_MAX_UPLOADS)

        query = {
            'format': 'json',
            'marker': '',
        }

        if uploadid and keymarker:
            query.update({'marker': '%s/%s' % (keymarker, uploadid)})
        elif keymarker:
            query.update({'marker': '%s/~' % (keymarker)})
        if 'prefix' in req.params:
            query.update({'prefix': get_param(req, 'prefix')})

        container = req.container_name + MULTIUPLOAD_SUFFIX
        uploads = []
        prefixes = []

        def object_to_upload(object_info):
            obj, upid = object_info['name'].rsplit('/', 1)
            obj_dict = {'key': obj,
                        'upload_id': upid,
                        'last_modified': object_info['last_modified']}
            return obj_dict

        is_segment = re.compile('.*/[0-9]+$')

        while len(uploads) < maxuploads:
            try:
                resp = req.get_response(self.app, container=container,
                                        query=query)
                objects = json.loads(resp.body)
            except NoSuchBucket:
                # Assume NoSuchBucket as no uploads
                objects = []
            if not objects:
                break

            new_uploads = [object_to_upload(obj) for obj in objects
                           if not is_segment.match(obj.get('name', ''))]
            new_prefixes = []
            if 'delimiter' in req.params:
                prefix = get_param(req, 'prefix', '')
                delimiter = get_param(req, 'delimiter')
                new_uploads, new_prefixes = separate_uploads(
                    new_uploads, prefix, delimiter)
            uploads.extend(new_uploads)
            prefixes.extend(new_prefixes)
            if six.PY2:
                query['marker'] = objects[-1]['name'].encode('utf-8')
            else:
                query['marker'] = objects[-1]['name']

        truncated = len(uploads) >= maxuploads
        if len(uploads) > maxuploads:
            uploads = uploads[:maxuploads]

        nextkeymarker = ''
        nextuploadmarker = ''
        if len(uploads) > 1:
            nextuploadmarker = uploads[-1]['upload_id']
            nextkeymarker = uploads[-1]['key']

        result_elem = Element('ListMultipartUploadsResult')
        SubElement(result_elem, 'Bucket').text = req.container_name
        SubElement(result_elem, 'KeyMarker').text = keymarker
        SubElement(result_elem, 'UploadIdMarker').text = uploadid
        SubElement(result_elem, 'NextKeyMarker').text = nextkeymarker
        SubElement(result_elem, 'NextUploadIdMarker').text = nextuploadmarker
        if 'delimiter' in req.params:
            SubElement(result_elem, 'Delimiter').text = \
                get_param(req, 'delimiter')
        if 'prefix' in req.params:
            SubElement(result_elem, 'Prefix').text = get_param(req, 'prefix')
        SubElement(result_elem, 'MaxUploads').text = str(maxuploads)
        if encoding_type is not None:
            SubElement(result_elem, 'EncodingType').text = encoding_type
        SubElement(result_elem, 'IsTruncated').text = \
            'true' if truncated else 'false'

        # TODO: don't show uploads which are initiated before this bucket is
        # created.
        for u in uploads:
            upload_elem = SubElement(result_elem, 'Upload')
            name = u['key']
            if encoding_type == 'url':
                name = quote(name)
            SubElement(upload_elem, 'Key').text = name
            SubElement(upload_elem, 'UploadId').text = u['upload_id']
            initiator_elem = SubElement(upload_elem, 'Initiator')
            SubElement(initiator_elem, 'ID').text = req.user_id
            SubElement(initiator_elem, 'DisplayName').text = req.user_id
            owner_elem = SubElement(upload_elem, 'Owner')
            SubElement(owner_elem, 'ID').text = req.user_id
            SubElement(owner_elem, 'DisplayName').text = req.user_id
            SubElement(upload_elem, 'StorageClass').text = 'STANDARD'
            SubElement(upload_elem, 'Initiated').text = \
                S3Timestamp.from_isoformat(u['last_modified']).s3xmlformat

        for p in prefixes:
            elem = SubElement(result_elem, 'CommonPrefixes')
            SubElement(elem, 'Prefix').text = p

        body = tostring(result_elem)

        return HTTPOk(body=body, content_type='application/xml')",encoding_type is not None and encoding_type != 'url',None is not encoding_type != 'url'
text_classification,https://github.com/brightmart/text_classification/tree/master/a06_Seq2seqWithAttention/a1_seq2seq.py,,rnn_decoder_with_attention$23,"def rnn_decoder_with_attention(decoder_inputs, initial_state, cell, loop_function,attention_states,scope=None):#3D Tensor [batch_size x attn_length x attn_size]
    """"""RNN decoder for the sequence-to-sequence model.
    Args:
        decoder_inputs: A list of 2D Tensors [batch_size x input_size].it is decoder input.
        initial_state: 2D Tensor with shape [batch_size x cell.state_size].it is the encoded vector of input sentences, which represent 'thought vector'
        cell: core_rnn_cell.RNNCell defining the cell function and size.
        loop_function: If not None, this function will be applied to the i-th output
            in order to generate the i+1-st input, and decoder_inputs will be ignored,
            except for the first element (""GO"" symbol). This can be used for decoding,
            but also for training to emulate http://arxiv.org/abs/1506.03099.
            Signature -- loop_function(prev, i) = next
                * prev is a 2D Tensor of shape [batch_size x output_size],
                * i is an integer, the step number (when advanced control is needed),
                * next is a 2D Tensor of shape [batch_size x input_size].
        attention_states: 3D Tensor [batch_size x attn_length x attn_size].it is represent input X.
        scope: VariableScope for the created subgraph; defaults to ""rnn_decoder"".
    Returns:
        A tuple of the form (outputs, state), where:
        outputs: A list of the same length as decoder_inputs of 2D Tensors with
            shape [batch_size x output_size] containing generated outputs.
        state: The state of each cell at the final time-step.
            It is a 2D Tensor of shape [batch_size x cell.state_size].
            (Note that in some cases, like basic RNN cell or GRU cell, outputs and
            states can be the same. They are different for LSTM cells though.)
    """"""
    with tf.variable_scope(scope or ""rnn_decoder""):
        print(""rnn_decoder_with_attention started..."")
        state = initial_state  #[batch_size x cell.state_size].
        _, hidden_size = state.get_shape().as_list() #200
        attention_states_original=attention_states
        batch_size,sequence_length,_=attention_states.get_shape().as_list()
        outputs = []
        prev = None
        #################################################
        for i, inp in enumerate(decoder_inputs):#循环解码部分的输入。如sentence_length个[batch_size x input_size]
            # 如果是训练，使用训练数据的输入；如果是test, 将t时刻的输出作为t + 1 时刻的s输入
            if loop_function is not None and prev is not None:#测试的时候：如果loop_function不为空且前一个词的值不为空，那么使用前一个的值作为RNN的输入
                with tf.variable_scope(""loop_function"", reuse=True):
                    inp = loop_function(prev, i)
            if i > 0:
                tf.get_variable_scope().reuse_variables()
            ##ATTENTION#################################################################################################################################################
            # 1.get logits of attention for each encoder input. attention_states:[batch_size x attn_length x attn_size]; query=state:[batch_size x cell.state_size]
            query=state
            W_a = tf.get_variable(""W_a"", shape=[hidden_size, hidden_size],initializer=tf.random_normal_initializer(stddev=0.1))
            query=tf.matmul(query, W_a) #[batch_size,hidden_size]
            query=tf.expand_dims(query,axis=1) #[batch_size, 1, hidden_size]
            U_a = tf.get_variable(""U_a"", shape=[hidden_size, hidden_size],initializer=tf.random_normal_initializer(stddev=0.1))
            U_aa = tf.get_variable(""U_aa"", shape=[ hidden_size])
            attention_states=tf.reshape(attention_states,shape=(-1,hidden_size)) #[batch_size*sentence_length,hidden_size]
            attention_states=tf.matmul(attention_states, U_a) #[batch_size*sentence_length,hidden_size]
            #print(""batch_size"",batch_size,"" ;sequence_length:"",sequence_length,"" ;hidden_size:"",hidden_size) #print(""attention_states:"", attention_states) #(?, 200)
            attention_states=tf.reshape(attention_states,shape=(-1,sequence_length,hidden_size)) # TODO [batch_size,sentence_length,hidden_size]
            #query_expanded:            [batch_size,1,             hidden_size]
            #attention_states_reshaped: [batch_size,sentence_length,hidden_size]
            attention_logits=tf.nn.tanh(query+attention_states+U_aa) #[batch_size,sentence_length,hidden_size]. additive style

            # 2.get possibility of attention
            attention_logits=tf.reshape(attention_logits,shape=(-1,hidden_size)) #batch_size*sequence_length [batch_size*sentence_length,hidden_size]
            V_a = tf.get_variable(""V_a"", shape=[hidden_size,1],initializer=tf.random_normal_initializer(stddev=0.1)) #[hidden_size,1]
            attention_logits=tf.matmul(attention_logits,V_a) #最终需要的是[batch_size*sentence_length,1]<-----[batch_size*sentence_length,hidden_size],[hidden_size,1]
            attention_logits=tf.reshape(attention_logits,shape=(-1,sequence_length)) #attention_logits:[batch_size,sequence_length]
            ##########################################################################################################################################################
            #attention_logits=tf.reduce_sum(attention_logits,2)        #[batch_size x attn_length]
            attention_logits_max=tf.reduce_max(attention_logits,axis=1,keep_dims=True) #[batch_size x 1]
            # possibility distribution for each encoder input.it means how much attention or focus for each encoder input
            p_attention=tf.nn.softmax(attention_logits-attention_logits_max)#[batch_size x attn_length]

            # 3.get weighted sum of hidden state for each encoder input as attention state
            p_attention=tf.expand_dims(p_attention,axis=2)            #[batch_size x attn_length x 1]
            # attention_states:[batch_size x attn_length x attn_size]; p_attention:[batch_size x attn_length];
            attention_final=tf.multiply(attention_states_original,p_attention) #[batch_size x attn_length x attn_size]
            context_vector=tf.reduce_sum(attention_final,axis=1)     #[batch_size x attn_size]
            ############################################################################################################################################################
            #inp:[batch_size x input_size].it is decoder input;  attention_final:[batch_size x attn_size]
            output, state = cell(inp, state,context_vector)          #attention_final TODO 使用RNN走一步
            outputs.append(output) # 将输出添加到结果列表中
            if loop_function is not None:
                prev = output
    print(""rnn_decoder_with_attention ended..."")
    return outputs, state",loop_function is not None and prev is not None,loop_function is not None is not prev
second.pytorch,https://github.com/traveller59/second.pytorch/tree/master/second/pytorch/core/losses.py,BootstrappedSigmoidClassificationLoss,__init__$412,"def __init__(self, alpha, bootstrap_type='soft'):
    """"""Constructor.

    Args:
      alpha: a float32 scalar tensor between 0 and 1 representing interpolation
        weight
      bootstrap_type: set to either 'hard' or 'soft' (default)

    Raises:
      ValueError: if bootstrap_type is not either 'hard' or 'soft'
    """"""
    if bootstrap_type != 'hard' and bootstrap_type != 'soft':
      raise ValueError('Unrecognized bootstrap_type: must be one of '
                       '\'hard\' or \'soft.\'')
    self._alpha = alpha
    self._bootstrap_type = bootstrap_type",bootstrap_type != 'hard' and bootstrap_type != 'soft','hard' != bootstrap_type != 'soft'
PyHive,https://github.com/dropbox/PyHive/tree/master/TCLIService/TCLIService.py,GetLog_result,write$3962,"def write(self, oprot):
        if oprot._fast_encode is not None and self.thrift_spec is not None:
            oprot.trans.write(oprot._fast_encode(self, (self.__class__, self.thrift_spec)))
            return
        oprot.writeStructBegin('GetLog_result')
        if self.success is not None:
            oprot.writeFieldBegin('success', TType.STRUCT, 0)
            self.success.write(oprot)
            oprot.writeFieldEnd()
        oprot.writeFieldStop()
        oprot.writeStructEnd()",oprot._fast_encode is not None and self.thrift_spec is not None,oprot._fast_encode is not None is not self.thrift_spec
sopel,https://github.com/sopel-irc/sopel/tree/master/sopel/coretasks.py,,receive_cap_ls_reply$956,"def receive_cap_ls_reply(bot, trigger):
    if bot.server_capabilities:
        # We've already seen the results, so someone sent CAP LS from a plugin.
        # We're too late to do SASL, and we don't want to send CAP END before
        # the plugin has done what it needs to, so just return
        return

    for cap in trigger.split():
        c = cap.split('=')
        if len(c) == 2:
            batched_caps[c[0]] = c[1]
        else:
            batched_caps[c[0]] = None

    # Not the last in a multi-line reply. First two args are * and LS.
    if trigger.args[2] == '*':
        return

    LOGGER.info(
        ""Client capability negotiation list: %s"",
        ', '.join(batched_caps.keys()),
    )
    bot.server_capabilities = batched_caps

    # If some other plugin requests it, we don't need to add another request.
    # If some other plugin prohibits it, we shouldn't request it.
    core_caps = [
        'echo-message',
        'multi-prefix',
        'away-notify',
        'chghost',
        'cap-notify',
        'server-time',
        'userhost-in-names',
        'message-tags',
    ]
    for cap in core_caps:
        if cap not in bot._cap_reqs:
            bot._cap_reqs[cap] = [utils.CapReq('', 'coretasks')]

    def acct_warn(bot, cap):
        LOGGER.info(""Server does not support %s, or it conflicts with a custom ""
                    ""plugin. User account validation unavailable or limited."",
                    cap[1:])
        if bot.config.core.owner_account or bot.config.core.admin_accounts:
            LOGGER.warning(
                ""Owner or admin accounts are configured, but %s is not ""
                ""supported by the server. This may cause unexpected behavior."",
                cap[1:])
    auth_caps = ['account-notify', 'extended-join', 'account-tag']
    for cap in auth_caps:
        if cap not in bot._cap_reqs:
            bot._cap_reqs[cap] = [utils.CapReq('', 'coretasks', acct_warn)]

    for cap, reqs in bot._cap_reqs.items():
        # At this point, we know mandatory and prohibited don't co-exist, but
        # we need to call back for optionals if they're also prohibited
        prefix = ''
        for entry in reqs:
            if prefix == '-' and entry.prefix != '-':
                entry.failure(bot, entry.prefix + cap)
                continue
            if entry.prefix:
                prefix = entry.prefix

        # It's not required, or it's supported, so we can request it
        if prefix != '=' or cap in bot.server_capabilities:
            # REQs fail as a whole, so we send them one capability at a time
            bot.write(('CAP', 'REQ', entry.prefix + cap))
        # If it's required but not in server caps, we need to call all the
        # callbacks
        else:
            for entry in reqs:
                if entry.failure and entry.prefix == '=':
                    entry.failure(bot, entry.prefix + cap)

    # If we want to do SASL, we have to wait before we can send CAP END. So if
    # we are, wait on 903 (SASL successful) to send it.
    if bot.config.core.auth_method == 'sasl' or bot.config.core.server_auth_method == 'sasl':
        bot.write(('CAP', 'REQ', 'sasl'))
    else:
        bot.write(('CAP', 'END'))
        LOGGER.info(""End of client capability negotiation requests."")",prefix == '-' and entry.prefix != '-',prefix == '-' != entry.prefix
clize,https://github.com/epsy/clize/tree/master/clize/parser.py,MultiParameter,set_value$595,"def set_value(self, ba, val):
        """"""Adds passed argument to the collection returned
        by `get_collection`.""""""
        col = self.get_collection(ba)
        col.append(val)
        if self.min <= len(col):
            ba.unsatisfied.discard(self)
        if self.max is not None and self.max < len(col):
            raise errors.TooManyValues",self.max is not None and self.max < len(col),None is not self.max < len(col)
3d-vehicle-tracking,https://github.com/ucbdrive/3d-vehicle-tracking/tree/master/faster-rcnn.pytorch/lib/model/rpn/proposal_layer.py,_ProposalLayer,forward$45,"def forward(self, input):

        # Algorithm:
        #
        # for each (H, W) location i
        #   generate A anchor boxes centered on cell i
        #   apply predicted bbox deltas at cell i to each of the A anchors
        # clip predicted boxes to image
        # remove predicted boxes with either height or width < threshold
        # sort all (proposal, score) pairs by score from highest to lowest
        # take top pre_nms_topN proposals before NMS
        # apply NMS with threshold 0.7 to remaining proposals
        # take after_nms_topN proposals after NMS
        # return the top proposals (-> RoIs top, scores top)


        # the first set of _num_anchors channels are bg probs
        # the second set are the fg probs
        scores = input[0][:, self._num_anchors:, :, :]
        bbox_deltas = input[1]
        im_info = input[2]
        cfg_key = input[3]

        pre_nms_topN  = cfg[cfg_key].RPN_PRE_NMS_TOP_N
        post_nms_topN = cfg[cfg_key].RPN_POST_NMS_TOP_N
        nms_thresh    = cfg[cfg_key].RPN_NMS_THRESH
        min_size      = cfg[cfg_key].RPN_MIN_SIZE

        batch_size = bbox_deltas.size(0)

        feat_height, feat_width = scores.size(2), scores.size(3)
        shift_x = np.arange(0, feat_width) * self._feat_stride
        shift_y = np.arange(0, feat_height) * self._feat_stride
        shift_x, shift_y = np.meshgrid(shift_x, shift_y)
        shifts = torch.from_numpy(np.vstack((shift_x.ravel(), shift_y.ravel(),
                                  shift_x.ravel(), shift_y.ravel())).transpose())
        shifts = shifts.contiguous().type_as(scores).float()

        A = self._num_anchors
        K = shifts.size(0)

        self._anchors = self._anchors.type_as(scores)
        # anchors = self._anchors.view(1, A, 4) + shifts.view(1, K, 4).permute(1, 0, 2).contiguous()
        anchors = self._anchors.view(1, A, 4) + shifts.view(K, 1, 4)
        anchors = anchors.view(1, K * A, 4).expand(batch_size, K * A, 4)

        # Transpose and reshape predicted bbox transformations to get them
        # into the same order as the anchors:

        bbox_deltas = bbox_deltas.permute(0, 2, 3, 1).contiguous()
        bbox_deltas = bbox_deltas.view(batch_size, -1, 4)

        # Same story for the scores:
        scores = scores.permute(0, 2, 3, 1).contiguous()
        scores = scores.view(batch_size, -1)

        # Convert anchors into proposals via bbox transformations
        proposals = bbox_transform_inv(anchors, bbox_deltas, batch_size)

        # 2. clip predicted boxes to image
        proposals = clip_boxes(proposals, im_info, batch_size)
        # proposals = clip_boxes_batch(proposals, im_info, batch_size)

        # assign the score to 0 if it's non keep.
        # keep = self._filter_boxes(proposals, min_size * im_info[:, 2])

        # trim keep index to make it euqal over batch
        # keep_idx = torch.cat(tuple(keep_idx), 0)

        # scores_keep = scores.view(-1)[keep_idx].view(batch_size, trim_size)
        # proposals_keep = proposals.view(-1, 4)[keep_idx, :].contiguous().view(batch_size, trim_size, 4)

        # _, order = torch.sort(scores_keep, 1, True)

        scores_keep = scores
        proposals_keep = proposals
        _, order = torch.sort(scores_keep, 1, True)

        output = scores.new(batch_size, post_nms_topN, 5).zero_()
        for i in range(batch_size):
            # # 3. remove predicted boxes with either height or width < threshold
            # # (NOTE: convert min_size to input image scale stored in im_info[2])
            proposals_single = proposals_keep[i]
            scores_single = scores_keep[i]

            # # 4. sort all (proposal, score) pairs by score from highest to lowest
            # # 5. take top pre_nms_topN (e.g. 6000)
            order_single = order[i]

            if pre_nms_topN > 0 and pre_nms_topN < scores_keep.numel():
                order_single = order_single[:pre_nms_topN]

            proposals_single = proposals_single[order_single, :]
            scores_single = scores_single[order_single].view(-1,1)

            # 6. apply nms (e.g. threshold = 0.7)
            # 7. take after_nms_topN (e.g. 300)
            # 8. return the top proposals (-> RoIs top)
            keep_idx_i = nms(proposals_single, scores_single.squeeze(1), nms_thresh)
            keep_idx_i = keep_idx_i.long().view(-1)

            if post_nms_topN > 0:
                keep_idx_i = keep_idx_i[:post_nms_topN]
            proposals_single = proposals_single[keep_idx_i, :]
            scores_single = scores_single[keep_idx_i, :]

            # padding 0 at the end.
            num_proposal = proposals_single.size(0)
            output[i,:,0] = i
            output[i,:num_proposal,1:] = proposals_single

        return output",pre_nms_topN > 0 and pre_nms_topN < scores_keep.numel(),0 < pre_nms_topN < scores_keep.numel()
ffjord,https://github.com/rtqichen/ffjord/tree/master//train_tabular.py,,if_main_my$154,"if __name__ == '__main__':

    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    logger.info('Using {} GPUs.'.format(torch.cuda.device_count()))

    data = load_data(args.data)
    data.trn.x = torch.from_numpy(data.trn.x)
    data.val.x = torch.from_numpy(data.val.x)
    data.tst.x = torch.from_numpy(data.tst.x)

    args.dims = '-'.join([str(args.hdim_factor * data.n_dims)] * args.nhidden)

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, data.n_dims, regularization_fns).to(device)
    set_cnf_options(args, model)

    for k in model.state_dict().keys():
        logger.info(k)

    if args.resume is not None:
        checkpt = torch.load(args.resume)

        # Backwards compatibility with an older version of the code.
        # TODO: remove upon release.
        filtered_state_dict = {}
        for k, v in checkpt['state_dict'].items():
            if 'diffeq.diffeq' not in k:
                filtered_state_dict[k.replace('module.', '')] = v
        model.load_state_dict(filtered_state_dict)

    logger.info(model)
    logger.info(""Number of trainable parameters: {}"".format(count_parameters(model)))

    if not args.evaluate:
        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

        time_meter = utils.RunningAverageMeter(0.98)
        loss_meter = utils.RunningAverageMeter(0.98)
        nfef_meter = utils.RunningAverageMeter(0.98)
        nfeb_meter = utils.RunningAverageMeter(0.98)
        tt_meter = utils.RunningAverageMeter(0.98)

        best_loss = float('inf')
        itr = 0
        n_vals_without_improvement = 0
        end = time.time()
        model.train()
        while True:
            if args.early_stopping > 0 and n_vals_without_improvement > args.early_stopping:
                break

            for x in batch_iter(data.trn.x, shuffle=True):
                if args.early_stopping > 0 and n_vals_without_improvement > args.early_stopping:
                    break

                optimizer.zero_grad()

                x = cvt(x)
                loss = compute_loss(x, model)
                loss_meter.update(loss.item())

                if len(regularization_coeffs) > 0:
                    reg_states = get_regularization(model, regularization_coeffs)
                    reg_loss = sum(
                        reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                    )
                    loss = loss + reg_loss

                total_time = count_total_time(model)
                nfe_forward = count_nfe(model)

                loss.backward()
                optimizer.step()

                nfe_total = count_nfe(model)
                nfe_backward = nfe_total - nfe_forward
                nfef_meter.update(nfe_forward)
                nfeb_meter.update(nfe_backward)

                time_meter.update(time.time() - end)
                tt_meter.update(total_time)

                if itr % args.log_freq == 0:
                    log_message = (
                        'Iter {:06d} | Epoch {:.2f} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) | '
                        'NFE Forward {:.0f}({:.1f}) | NFE Backward {:.0f}({:.1f}) | CNF Time {:.4f}({:.4f})'.format(
                            itr,
                            float(itr) / (data.trn.x.shape[0] / float(args.batch_size)), time_meter.val, time_meter.avg,
                            loss_meter.val, loss_meter.avg, nfef_meter.val, nfef_meter.avg, nfeb_meter.val,
                            nfeb_meter.avg, tt_meter.val, tt_meter.avg
                        )
                    )
                    if len(regularization_coeffs) > 0:
                        log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)

                    logger.info(log_message)
                itr += 1
                end = time.time()

                # Validation loop.
                if itr % args.val_freq == 0:
                    model.eval()
                    start_time = time.time()
                    with torch.no_grad():
                        val_loss = utils.AverageMeter()
                        val_nfe = utils.AverageMeter()
                        for x in batch_iter(data.val.x, batch_size=test_batch_size):
                            x = cvt(x)
                            val_loss.update(compute_loss(x, model).item(), x.shape[0])
                            val_nfe.update(count_nfe(model))

                        if val_loss.avg < best_loss:
                            best_loss = val_loss.avg
                            utils.makedirs(args.save)
                            torch.save({
                                'args': args,
                                'state_dict': model.state_dict(),
                            }, os.path.join(args.save, 'checkpt.pth'))
                            n_vals_without_improvement = 0
                        else:
                            n_vals_without_improvement += 1
                        update_lr(optimizer, n_vals_without_improvement)

                        log_message = (
                            '[VAL] Iter {:06d} | Val Loss {:.6f} | NFE {:.0f} | '
                            'NoImproveEpochs {:02d}/{:02d}'.format(
                                itr, val_loss.avg, val_nfe.avg, n_vals_without_improvement, args.early_stopping
                            )
                        )
                        logger.info(log_message)
                    model.train()

        logger.info('Training has finished.')
        model = restore_model(model, os.path.join(args.save, 'checkpt.pth')).to(device)
        set_cnf_options(args, model)

    logger.info('Evaluating model on test set.')
    model.eval()

    override_divergence_fn(model, ""brute_force"")

    with torch.no_grad():
        test_loss = utils.AverageMeter()
        test_nfe = utils.AverageMeter()
        for itr, x in enumerate(batch_iter(data.tst.x, batch_size=test_batch_size)):
            x = cvt(x)
            test_loss.update(compute_loss(x, model).item(), x.shape[0])
            test_nfe.update(count_nfe(model))
            logger.info('Progress: {:.2f}%'.format(100. * itr / (data.tst.x.shape[0] / test_batch_size)))
        log_message = '[TEST] Iter {:06d} | Test Loss {:.6f} | NFE {:.0f}'.format(itr, test_loss.avg, test_nfe.avg)
        logger.info(log_message)",args.early_stopping > 0 and n_vals_without_improvement > args.early_stopping,n_vals_without_improvement > args.early_stopping > 0
ffjord,https://github.com/rtqichen/ffjord/tree/master//train_tabular.py,,if_main_my$154,"if __name__ == '__main__':

    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)

    logger.info('Using {} GPUs.'.format(torch.cuda.device_count()))

    data = load_data(args.data)
    data.trn.x = torch.from_numpy(data.trn.x)
    data.val.x = torch.from_numpy(data.val.x)
    data.tst.x = torch.from_numpy(data.tst.x)

    args.dims = '-'.join([str(args.hdim_factor * data.n_dims)] * args.nhidden)

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, data.n_dims, regularization_fns).to(device)
    set_cnf_options(args, model)

    for k in model.state_dict().keys():
        logger.info(k)

    if args.resume is not None:
        checkpt = torch.load(args.resume)

        # Backwards compatibility with an older version of the code.
        # TODO: remove upon release.
        filtered_state_dict = {}
        for k, v in checkpt['state_dict'].items():
            if 'diffeq.diffeq' not in k:
                filtered_state_dict[k.replace('module.', '')] = v
        model.load_state_dict(filtered_state_dict)

    logger.info(model)
    logger.info(""Number of trainable parameters: {}"".format(count_parameters(model)))

    if not args.evaluate:
        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

        time_meter = utils.RunningAverageMeter(0.98)
        loss_meter = utils.RunningAverageMeter(0.98)
        nfef_meter = utils.RunningAverageMeter(0.98)
        nfeb_meter = utils.RunningAverageMeter(0.98)
        tt_meter = utils.RunningAverageMeter(0.98)

        best_loss = float('inf')
        itr = 0
        n_vals_without_improvement = 0
        end = time.time()
        model.train()
        while True:
            if args.early_stopping > 0 and n_vals_without_improvement > args.early_stopping:
                break

            for x in batch_iter(data.trn.x, shuffle=True):
                if args.early_stopping > 0 and n_vals_without_improvement > args.early_stopping:
                    break

                optimizer.zero_grad()

                x = cvt(x)
                loss = compute_loss(x, model)
                loss_meter.update(loss.item())

                if len(regularization_coeffs) > 0:
                    reg_states = get_regularization(model, regularization_coeffs)
                    reg_loss = sum(
                        reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
                    )
                    loss = loss + reg_loss

                total_time = count_total_time(model)
                nfe_forward = count_nfe(model)

                loss.backward()
                optimizer.step()

                nfe_total = count_nfe(model)
                nfe_backward = nfe_total - nfe_forward
                nfef_meter.update(nfe_forward)
                nfeb_meter.update(nfe_backward)

                time_meter.update(time.time() - end)
                tt_meter.update(total_time)

                if itr % args.log_freq == 0:
                    log_message = (
                        'Iter {:06d} | Epoch {:.2f} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) | '
                        'NFE Forward {:.0f}({:.1f}) | NFE Backward {:.0f}({:.1f}) | CNF Time {:.4f}({:.4f})'.format(
                            itr,
                            float(itr) / (data.trn.x.shape[0] / float(args.batch_size)), time_meter.val, time_meter.avg,
                            loss_meter.val, loss_meter.avg, nfef_meter.val, nfef_meter.avg, nfeb_meter.val,
                            nfeb_meter.avg, tt_meter.val, tt_meter.avg
                        )
                    )
                    if len(regularization_coeffs) > 0:
                        log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)

                    logger.info(log_message)
                itr += 1
                end = time.time()

                # Validation loop.
                if itr % args.val_freq == 0:
                    model.eval()
                    start_time = time.time()
                    with torch.no_grad():
                        val_loss = utils.AverageMeter()
                        val_nfe = utils.AverageMeter()
                        for x in batch_iter(data.val.x, batch_size=test_batch_size):
                            x = cvt(x)
                            val_loss.update(compute_loss(x, model).item(), x.shape[0])
                            val_nfe.update(count_nfe(model))

                        if val_loss.avg < best_loss:
                            best_loss = val_loss.avg
                            utils.makedirs(args.save)
                            torch.save({
                                'args': args,
                                'state_dict': model.state_dict(),
                            }, os.path.join(args.save, 'checkpt.pth'))
                            n_vals_without_improvement = 0
                        else:
                            n_vals_without_improvement += 1
                        update_lr(optimizer, n_vals_without_improvement)

                        log_message = (
                            '[VAL] Iter {:06d} | Val Loss {:.6f} | NFE {:.0f} | '
                            'NoImproveEpochs {:02d}/{:02d}'.format(
                                itr, val_loss.avg, val_nfe.avg, n_vals_without_improvement, args.early_stopping
                            )
                        )
                        logger.info(log_message)
                    model.train()

        logger.info('Training has finished.')
        model = restore_model(model, os.path.join(args.save, 'checkpt.pth')).to(device)
        set_cnf_options(args, model)

    logger.info('Evaluating model on test set.')
    model.eval()

    override_divergence_fn(model, ""brute_force"")

    with torch.no_grad():
        test_loss = utils.AverageMeter()
        test_nfe = utils.AverageMeter()
        for itr, x in enumerate(batch_iter(data.tst.x, batch_size=test_batch_size)):
            x = cvt(x)
            test_loss.update(compute_loss(x, model).item(), x.shape[0])
            test_nfe.update(count_nfe(model))
            logger.info('Progress: {:.2f}%'.format(100. * itr / (data.tst.x.shape[0] / test_batch_size)))
        log_message = '[TEST] Iter {:06d} | Test Loss {:.6f} | NFE {:.0f}'.format(itr, test_loss.avg, test_nfe.avg)
        logger.info(log_message)",args.early_stopping > 0 and n_vals_without_improvement > args.early_stopping,n_vals_without_improvement > args.early_stopping > 0
oppia,https://github.com/oppia/oppia/tree/master/core/domain/exp_domain.py,Exploration,from_dict$1437,"def from_dict(
        cls,
        exploration_dict: ExplorationDict,
        exploration_version: int = 0,
        exploration_created_on: Optional[datetime.datetime] = None,
        exploration_last_updated: Optional[datetime.datetime] = None
    ) -> Exploration:
        """"""Return a Exploration domain object from a dict.

        Args:
            exploration_dict: dict. The dict representation of Exploration
                object.
            exploration_version: int. The version of the exploration.
            exploration_created_on: datetime.datetime. Date and time when the
                exploration is created.
            exploration_last_updated: datetime.datetime. Date and time when the
                exploration was last updated.

        Returns:
            Exploration. The corresponding Exploration domain object.

        Raises:
            Exception. Some parameter was used in a state but not declared
                in the Exploration dict.
        """"""
        # NOTE TO DEVELOPERS: It is absolutely ESSENTIAL this conversion to and
        # from an ExplorationModel/dictionary MUST be exhaustive and complete.
        exploration = cls.create_default_exploration(
            exploration_dict['id'],
            title=exploration_dict['title'],
            category=exploration_dict['category'],
            objective=exploration_dict['objective'],
            language_code=exploration_dict['language_code'])
        exploration.tags = exploration_dict['tags']
        exploration.blurb = exploration_dict['blurb']
        exploration.author_notes = exploration_dict['author_notes']
        exploration.auto_tts_enabled = exploration_dict['auto_tts_enabled']
        exploration.correctness_feedback_enabled = exploration_dict[
            'correctness_feedback_enabled']
        exploration.edits_allowed = exploration_dict['edits_allowed']

        exploration.param_specs = {
            ps_name: param_domain.ParamSpec.from_dict(ps_val) for
            (ps_name, ps_val) in exploration_dict['param_specs'].items()
        }

        exploration.states_schema_version = exploration_dict[
            'states_schema_version']
        init_state_name = exploration_dict['init_state_name']
        exploration.rename_state(exploration.init_state_name, init_state_name)
        exploration.add_states([
            state_name for state_name in exploration_dict['states']
            if state_name != init_state_name])

        for (state_name, sdict) in exploration_dict['states'].items():
            state = exploration.states[state_name]

            state.content = state_domain.SubtitledHtml(
                sdict['content']['content_id'], sdict['content']['html'])
            state.content.validate()

            state.param_changes = [param_domain.ParamChange(
                pc['name'], pc['generator_id'], pc['customization_args']
            ) for pc in sdict['param_changes']]

            for pc in state.param_changes:
                if pc.name not in exploration.param_specs:
                    raise Exception(
                        'Parameter %s was used in a state but not '
                        'declared in the exploration param_specs.' % pc.name)

            idict = sdict['interaction']
            interaction_answer_groups = [
                state_domain.AnswerGroup.from_dict(group)
                for group in idict['answer_groups']]

            default_outcome = (
                state_domain.Outcome.from_dict(idict['default_outcome'])
                if idict['default_outcome'] is not None else None)

            solution = (
                state_domain.Solution.from_dict(idict['id'], idict['solution'])
                if idict['solution'] is not None and idict['id'] is not None
                else None
            )

            customization_args = (
                state_domain.InteractionInstance.
                convert_customization_args_dict_to_customization_args(
                    idict['id'],
                    idict['customization_args']
                )
            )
            state.interaction = state_domain.InteractionInstance(
                idict['id'], customization_args,
                interaction_answer_groups, default_outcome,
                idict['confirmed_unclassified_answers'],
                [state_domain.Hint.from_dict(h) for h in idict['hints']],
                solution)

            state.recorded_voiceovers = (
                state_domain.RecordedVoiceovers.from_dict(
                    sdict['recorded_voiceovers']))

            state.written_translations = (
                state_domain.WrittenTranslations.from_dict(
                    sdict['written_translations']))

            state.next_content_id_index = sdict['next_content_id_index']

            state.linked_skill_id = sdict['linked_skill_id']

            state.solicit_answer_details = sdict['solicit_answer_details']

            state.card_is_checkpoint = sdict['card_is_checkpoint']

            exploration.states[state_name] = state

        exploration.param_changes = [
            param_domain.ParamChange.from_dict(pc)
            for pc in exploration_dict['param_changes']]

        exploration.version = exploration_version
        exploration.created_on = exploration_created_on
        exploration.last_updated = exploration_last_updated

        return exploration",idict['solution'] is not None and idict['id'] is not None,idict['solution'] is not None is not idict['id']
trezor-firmware,https://github.com/trezor/trezor-firmware/tree/master/core/src/apps/monero/xmr/bulletproof.py,,_ensure_dst_keyvect$1045,"def _ensure_dst_keyvect(dst=None, size: int | None = None):
    if dst is None:
        dst = KeyV(elems=size)
        return dst
    if size is not None and size != len(dst):
        dst.resize(size)
    return dst",size is not None and size != len(dst),None is not size != len(dst)
PythonStdioGames,https://github.com/asweigart/PythonStdioGames/tree/master/src/gamesbyexample/pygame_games/tetromino.py,,drawBox$444,"def drawBox(boxx, boxy, color, pixelx=None, pixely=None):
    # draw a single box (each tetromino piece has four boxes)
    # at xy coordinates on the board. Or, if pixelx & pixely
    # are specified, draw to the pixel coordinates stored in
    # pixelx & pixely (this is used for the ""Next"" piece).
    if color == BLANK:
        return
    if pixelx == None and pixely == None:
        pixelx, pixely = convertToPixelCoords(boxx, boxy)
    pygame.draw.rect(DISPLAYSURF, COLORS[color], (pixelx + 1, pixely + 1, BOXSIZE - 1, BOXSIZE - 1))
    pygame.draw.rect(DISPLAYSURF, LIGHTCOLORS[color], (pixelx + 1, pixely + 1, BOXSIZE - 4, BOXSIZE - 4))",pixelx == None and pixely == None,pixelx == None == pixely
SSR-Net,https://github.com/shamangary/SSR-Net/tree/master/training_and_testing/densenet.py,,DenseNetFCN$243,"def DenseNetFCN(input_shape, nb_dense_block=5, growth_rate=16, nb_layers_per_block=4,
                reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, init_conv_filters=48,
                include_top=True, weights=None, input_tensor=None, classes=1, activation='softmax',
                upsampling_conv=128, upsampling_type='deconv'):
    '''Instantiate the DenseNet FCN architecture.
        Note that when using TensorFlow,
        for best performance you should set
        `image_data_format='channels_last'` in your Keras config
        at ~/.keras/keras.json.
        # Arguments
            nb_dense_block: number of dense blocks to add to end (generally = 3)
            growth_rate: number of filters to add per dense block
            nb_layers_per_block: number of layers in each dense block.
                Can be a positive integer or a list.
                If positive integer, a set number of layers per dense block.
                If list, nb_layer is used as provided. Note that list size must
                be (nb_dense_block + 1)
            reduction: reduction factor of transition blocks.
                Note : reduction value is inverted to compute compression.
            dropout_rate: dropout rate
            init_conv_filters: number of layers in the initial convolution layer
            include_top: whether to include the fully-connected
                layer at the top of the network.
            weights: one of `None` (random initialization) or
                'cifar10' (pre-training on CIFAR-10)..
            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
                to use as image input for the model.
            input_shape: optional shape tuple, only to be specified
                if `include_top` is False (otherwise the input shape
                has to be `(32, 32, 3)` (with `channels_last` dim ordering)
                or `(3, 32, 32)` (with `channels_first` dim ordering).
                It should have exactly 3 inputs channels,
                and width and height should be no smaller than 8.
                E.g. `(200, 200, 3)` would be one valid value.
            classes: optional number of classes to classify images
                into, only to be specified if `include_top` is True, and
                if no `weights` argument is specified.
            activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.
                Note that if sigmoid is used, classes must be 1.
            upsampling_conv: number of convolutional layers in upsampling via subpixel convolution
            upsampling_type: Can be one of 'upsampling', 'deconv' and
                'subpixel'. Defines type of upsampling algorithm used.
            batchsize: Fixed batch size. This is a temporary requirement for
                computation of output shape in the case of Deconvolution2D layers.
                Parameter will be removed in next iteration of Keras, which infers
                output shape of deconvolution layers automatically.
        # Returns
            A Keras model instance.
    '''

    if weights not in {None}:
        raise ValueError('The `weights` argument should be '
                         '`None` (random initialization) as no '
                         'model weights are provided.')

    upsampling_type = upsampling_type.lower()

    if upsampling_type not in ['upsampling', 'deconv', 'subpixel']:
        raise ValueError('Parameter ""upsampling_type"" must be one of ""upsampling"", '
                         '""deconv"" or ""subpixel"".')

    if input_shape is None:
        raise ValueError('For fully convolutional models, input shape must be supplied.')

    if type(nb_layers_per_block) is not list and nb_dense_block < 1:
        raise ValueError('Number of dense layers per block must be greater than 1. Argument '
                         'value was %d.' % (nb_layers_per_block))

    if activation not in ['softmax', 'sigmoid']:
        raise ValueError('activation must be one of ""softmax"" or ""sigmoid""')

    if activation == 'sigmoid' and classes != 1:
        raise ValueError('sigmoid activation can only be used when classes = 1')

    # Determine proper input shape
    min_size = 2 ** nb_dense_block

    if K.image_data_format() == 'channels_first':
        if input_shape is not None:
            if ((input_shape[1] is not None and input_shape[1] < min_size) or
                    (input_shape[2] is not None and input_shape[2] < min_size)):
                raise ValueError('Input size must be at least ' +
                                 str(min_size) + 'x' + str(min_size) + ', got '
                                                                       '`input_shape=' + str(input_shape) + '`')
        else:
            input_shape = (classes, None, None)
    else:
        if input_shape is not None:
            if ((input_shape[0] is not None and input_shape[0] < min_size) or
                    (input_shape[1] is not None and input_shape[1] < min_size)):
                raise ValueError('Input size must be at least ' +
                                 str(min_size) + 'x' + str(min_size) + ', got '
                                                                       '`input_shape=' + str(input_shape) + '`')
        else:
            input_shape = (None, None, classes)

    if input_tensor is None:
        img_input = Input(shape=input_shape)
    else:
        if not K.is_keras_tensor(input_tensor):
            img_input = Input(tensor=input_tensor, shape=input_shape)
        else:
            img_input = input_tensor

    x = __create_fcn_dense_net(classes, img_input, include_top, nb_dense_block,
                               growth_rate, reduction, dropout_rate, weight_decay,
                               nb_layers_per_block, upsampling_conv, upsampling_type,
                               init_conv_filters, input_shape, activation)

    # Ensure that the model takes into account
    # any potential predecessors of `input_tensor`.
    if input_tensor is not None:
        inputs = get_source_inputs(input_tensor)
    else:
        inputs = img_input
    # Create model.
    model = Model(inputs, x, name='fcn-densenet')

    return model",input_shape[1] is not None and input_shape[1] < min_size,None is not input_shape[1] < min_size
SSR-Net,https://github.com/shamangary/SSR-Net/tree/master/training_and_testing/densenet.py,,DenseNetFCN$243,"def DenseNetFCN(input_shape, nb_dense_block=5, growth_rate=16, nb_layers_per_block=4,
                reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, init_conv_filters=48,
                include_top=True, weights=None, input_tensor=None, classes=1, activation='softmax',
                upsampling_conv=128, upsampling_type='deconv'):
    '''Instantiate the DenseNet FCN architecture.
        Note that when using TensorFlow,
        for best performance you should set
        `image_data_format='channels_last'` in your Keras config
        at ~/.keras/keras.json.
        # Arguments
            nb_dense_block: number of dense blocks to add to end (generally = 3)
            growth_rate: number of filters to add per dense block
            nb_layers_per_block: number of layers in each dense block.
                Can be a positive integer or a list.
                If positive integer, a set number of layers per dense block.
                If list, nb_layer is used as provided. Note that list size must
                be (nb_dense_block + 1)
            reduction: reduction factor of transition blocks.
                Note : reduction value is inverted to compute compression.
            dropout_rate: dropout rate
            init_conv_filters: number of layers in the initial convolution layer
            include_top: whether to include the fully-connected
                layer at the top of the network.
            weights: one of `None` (random initialization) or
                'cifar10' (pre-training on CIFAR-10)..
            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
                to use as image input for the model.
            input_shape: optional shape tuple, only to be specified
                if `include_top` is False (otherwise the input shape
                has to be `(32, 32, 3)` (with `channels_last` dim ordering)
                or `(3, 32, 32)` (with `channels_first` dim ordering).
                It should have exactly 3 inputs channels,
                and width and height should be no smaller than 8.
                E.g. `(200, 200, 3)` would be one valid value.
            classes: optional number of classes to classify images
                into, only to be specified if `include_top` is True, and
                if no `weights` argument is specified.
            activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.
                Note that if sigmoid is used, classes must be 1.
            upsampling_conv: number of convolutional layers in upsampling via subpixel convolution
            upsampling_type: Can be one of 'upsampling', 'deconv' and
                'subpixel'. Defines type of upsampling algorithm used.
            batchsize: Fixed batch size. This is a temporary requirement for
                computation of output shape in the case of Deconvolution2D layers.
                Parameter will be removed in next iteration of Keras, which infers
                output shape of deconvolution layers automatically.
        # Returns
            A Keras model instance.
    '''

    if weights not in {None}:
        raise ValueError('The `weights` argument should be '
                         '`None` (random initialization) as no '
                         'model weights are provided.')

    upsampling_type = upsampling_type.lower()

    if upsampling_type not in ['upsampling', 'deconv', 'subpixel']:
        raise ValueError('Parameter ""upsampling_type"" must be one of ""upsampling"", '
                         '""deconv"" or ""subpixel"".')

    if input_shape is None:
        raise ValueError('For fully convolutional models, input shape must be supplied.')

    if type(nb_layers_per_block) is not list and nb_dense_block < 1:
        raise ValueError('Number of dense layers per block must be greater than 1. Argument '
                         'value was %d.' % (nb_layers_per_block))

    if activation not in ['softmax', 'sigmoid']:
        raise ValueError('activation must be one of ""softmax"" or ""sigmoid""')

    if activation == 'sigmoid' and classes != 1:
        raise ValueError('sigmoid activation can only be used when classes = 1')

    # Determine proper input shape
    min_size = 2 ** nb_dense_block

    if K.image_data_format() == 'channels_first':
        if input_shape is not None:
            if ((input_shape[1] is not None and input_shape[1] < min_size) or
                    (input_shape[2] is not None and input_shape[2] < min_size)):
                raise ValueError('Input size must be at least ' +
                                 str(min_size) + 'x' + str(min_size) + ', got '
                                                                       '`input_shape=' + str(input_shape) + '`')
        else:
            input_shape = (classes, None, None)
    else:
        if input_shape is not None:
            if ((input_shape[0] is not None and input_shape[0] < min_size) or
                    (input_shape[1] is not None and input_shape[1] < min_size)):
                raise ValueError('Input size must be at least ' +
                                 str(min_size) + 'x' + str(min_size) + ', got '
                                                                       '`input_shape=' + str(input_shape) + '`')
        else:
            input_shape = (None, None, classes)

    if input_tensor is None:
        img_input = Input(shape=input_shape)
    else:
        if not K.is_keras_tensor(input_tensor):
            img_input = Input(tensor=input_tensor, shape=input_shape)
        else:
            img_input = input_tensor

    x = __create_fcn_dense_net(classes, img_input, include_top, nb_dense_block,
                               growth_rate, reduction, dropout_rate, weight_decay,
                               nb_layers_per_block, upsampling_conv, upsampling_type,
                               init_conv_filters, input_shape, activation)

    # Ensure that the model takes into account
    # any potential predecessors of `input_tensor`.
    if input_tensor is not None:
        inputs = get_source_inputs(input_tensor)
    else:
        inputs = img_input
    # Create model.
    model = Model(inputs, x, name='fcn-densenet')

    return model",input_shape[2] is not None and input_shape[2] < min_size,None is not input_shape[2] < min_size
SSR-Net,https://github.com/shamangary/SSR-Net/tree/master/training_and_testing/densenet.py,,DenseNetFCN$243,"def DenseNetFCN(input_shape, nb_dense_block=5, growth_rate=16, nb_layers_per_block=4,
                reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, init_conv_filters=48,
                include_top=True, weights=None, input_tensor=None, classes=1, activation='softmax',
                upsampling_conv=128, upsampling_type='deconv'):
    '''Instantiate the DenseNet FCN architecture.
        Note that when using TensorFlow,
        for best performance you should set
        `image_data_format='channels_last'` in your Keras config
        at ~/.keras/keras.json.
        # Arguments
            nb_dense_block: number of dense blocks to add to end (generally = 3)
            growth_rate: number of filters to add per dense block
            nb_layers_per_block: number of layers in each dense block.
                Can be a positive integer or a list.
                If positive integer, a set number of layers per dense block.
                If list, nb_layer is used as provided. Note that list size must
                be (nb_dense_block + 1)
            reduction: reduction factor of transition blocks.
                Note : reduction value is inverted to compute compression.
            dropout_rate: dropout rate
            init_conv_filters: number of layers in the initial convolution layer
            include_top: whether to include the fully-connected
                layer at the top of the network.
            weights: one of `None` (random initialization) or
                'cifar10' (pre-training on CIFAR-10)..
            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
                to use as image input for the model.
            input_shape: optional shape tuple, only to be specified
                if `include_top` is False (otherwise the input shape
                has to be `(32, 32, 3)` (with `channels_last` dim ordering)
                or `(3, 32, 32)` (with `channels_first` dim ordering).
                It should have exactly 3 inputs channels,
                and width and height should be no smaller than 8.
                E.g. `(200, 200, 3)` would be one valid value.
            classes: optional number of classes to classify images
                into, only to be specified if `include_top` is True, and
                if no `weights` argument is specified.
            activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.
                Note that if sigmoid is used, classes must be 1.
            upsampling_conv: number of convolutional layers in upsampling via subpixel convolution
            upsampling_type: Can be one of 'upsampling', 'deconv' and
                'subpixel'. Defines type of upsampling algorithm used.
            batchsize: Fixed batch size. This is a temporary requirement for
                computation of output shape in the case of Deconvolution2D layers.
                Parameter will be removed in next iteration of Keras, which infers
                output shape of deconvolution layers automatically.
        # Returns
            A Keras model instance.
    '''

    if weights not in {None}:
        raise ValueError('The `weights` argument should be '
                         '`None` (random initialization) as no '
                         'model weights are provided.')

    upsampling_type = upsampling_type.lower()

    if upsampling_type not in ['upsampling', 'deconv', 'subpixel']:
        raise ValueError('Parameter ""upsampling_type"" must be one of ""upsampling"", '
                         '""deconv"" or ""subpixel"".')

    if input_shape is None:
        raise ValueError('For fully convolutional models, input shape must be supplied.')

    if type(nb_layers_per_block) is not list and nb_dense_block < 1:
        raise ValueError('Number of dense layers per block must be greater than 1. Argument '
                         'value was %d.' % (nb_layers_per_block))

    if activation not in ['softmax', 'sigmoid']:
        raise ValueError('activation must be one of ""softmax"" or ""sigmoid""')

    if activation == 'sigmoid' and classes != 1:
        raise ValueError('sigmoid activation can only be used when classes = 1')

    # Determine proper input shape
    min_size = 2 ** nb_dense_block

    if K.image_data_format() == 'channels_first':
        if input_shape is not None:
            if ((input_shape[1] is not None and input_shape[1] < min_size) or
                    (input_shape[2] is not None and input_shape[2] < min_size)):
                raise ValueError('Input size must be at least ' +
                                 str(min_size) + 'x' + str(min_size) + ', got '
                                                                       '`input_shape=' + str(input_shape) + '`')
        else:
            input_shape = (classes, None, None)
    else:
        if input_shape is not None:
            if ((input_shape[0] is not None and input_shape[0] < min_size) or
                    (input_shape[1] is not None and input_shape[1] < min_size)):
                raise ValueError('Input size must be at least ' +
                                 str(min_size) + 'x' + str(min_size) + ', got '
                                                                       '`input_shape=' + str(input_shape) + '`')
        else:
            input_shape = (None, None, classes)

    if input_tensor is None:
        img_input = Input(shape=input_shape)
    else:
        if not K.is_keras_tensor(input_tensor):
            img_input = Input(tensor=input_tensor, shape=input_shape)
        else:
            img_input = input_tensor

    x = __create_fcn_dense_net(classes, img_input, include_top, nb_dense_block,
                               growth_rate, reduction, dropout_rate, weight_decay,
                               nb_layers_per_block, upsampling_conv, upsampling_type,
                               init_conv_filters, input_shape, activation)

    # Ensure that the model takes into account
    # any potential predecessors of `input_tensor`.
    if input_tensor is not None:
        inputs = get_source_inputs(input_tensor)
    else:
        inputs = img_input
    # Create model.
    model = Model(inputs, x, name='fcn-densenet')

    return model",input_shape[0] is not None and input_shape[0] < min_size,None is not input_shape[0] < min_size
SSR-Net,https://github.com/shamangary/SSR-Net/tree/master/training_and_testing/densenet.py,,DenseNetFCN$243,"def DenseNetFCN(input_shape, nb_dense_block=5, growth_rate=16, nb_layers_per_block=4,
                reduction=0.0, dropout_rate=0.0, weight_decay=1e-4, init_conv_filters=48,
                include_top=True, weights=None, input_tensor=None, classes=1, activation='softmax',
                upsampling_conv=128, upsampling_type='deconv'):
    '''Instantiate the DenseNet FCN architecture.
        Note that when using TensorFlow,
        for best performance you should set
        `image_data_format='channels_last'` in your Keras config
        at ~/.keras/keras.json.
        # Arguments
            nb_dense_block: number of dense blocks to add to end (generally = 3)
            growth_rate: number of filters to add per dense block
            nb_layers_per_block: number of layers in each dense block.
                Can be a positive integer or a list.
                If positive integer, a set number of layers per dense block.
                If list, nb_layer is used as provided. Note that list size must
                be (nb_dense_block + 1)
            reduction: reduction factor of transition blocks.
                Note : reduction value is inverted to compute compression.
            dropout_rate: dropout rate
            init_conv_filters: number of layers in the initial convolution layer
            include_top: whether to include the fully-connected
                layer at the top of the network.
            weights: one of `None` (random initialization) or
                'cifar10' (pre-training on CIFAR-10)..
            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)
                to use as image input for the model.
            input_shape: optional shape tuple, only to be specified
                if `include_top` is False (otherwise the input shape
                has to be `(32, 32, 3)` (with `channels_last` dim ordering)
                or `(3, 32, 32)` (with `channels_first` dim ordering).
                It should have exactly 3 inputs channels,
                and width and height should be no smaller than 8.
                E.g. `(200, 200, 3)` would be one valid value.
            classes: optional number of classes to classify images
                into, only to be specified if `include_top` is True, and
                if no `weights` argument is specified.
            activation: Type of activation at the top layer. Can be one of 'softmax' or 'sigmoid'.
                Note that if sigmoid is used, classes must be 1.
            upsampling_conv: number of convolutional layers in upsampling via subpixel convolution
            upsampling_type: Can be one of 'upsampling', 'deconv' and
                'subpixel'. Defines type of upsampling algorithm used.
            batchsize: Fixed batch size. This is a temporary requirement for
                computation of output shape in the case of Deconvolution2D layers.
                Parameter will be removed in next iteration of Keras, which infers
                output shape of deconvolution layers automatically.
        # Returns
            A Keras model instance.
    '''

    if weights not in {None}:
        raise ValueError('The `weights` argument should be '
                         '`None` (random initialization) as no '
                         'model weights are provided.')

    upsampling_type = upsampling_type.lower()

    if upsampling_type not in ['upsampling', 'deconv', 'subpixel']:
        raise ValueError('Parameter ""upsampling_type"" must be one of ""upsampling"", '
                         '""deconv"" or ""subpixel"".')

    if input_shape is None:
        raise ValueError('For fully convolutional models, input shape must be supplied.')

    if type(nb_layers_per_block) is not list and nb_dense_block < 1:
        raise ValueError('Number of dense layers per block must be greater than 1. Argument '
                         'value was %d.' % (nb_layers_per_block))

    if activation not in ['softmax', 'sigmoid']:
        raise ValueError('activation must be one of ""softmax"" or ""sigmoid""')

    if activation == 'sigmoid' and classes != 1:
        raise ValueError('sigmoid activation can only be used when classes = 1')

    # Determine proper input shape
    min_size = 2 ** nb_dense_block

    if K.image_data_format() == 'channels_first':
        if input_shape is not None:
            if ((input_shape[1] is not None and input_shape[1] < min_size) or
                    (input_shape[2] is not None and input_shape[2] < min_size)):
                raise ValueError('Input size must be at least ' +
                                 str(min_size) + 'x' + str(min_size) + ', got '
                                                                       '`input_shape=' + str(input_shape) + '`')
        else:
            input_shape = (classes, None, None)
    else:
        if input_shape is not None:
            if ((input_shape[0] is not None and input_shape[0] < min_size) or
                    (input_shape[1] is not None and input_shape[1] < min_size)):
                raise ValueError('Input size must be at least ' +
                                 str(min_size) + 'x' + str(min_size) + ', got '
                                                                       '`input_shape=' + str(input_shape) + '`')
        else:
            input_shape = (None, None, classes)

    if input_tensor is None:
        img_input = Input(shape=input_shape)
    else:
        if not K.is_keras_tensor(input_tensor):
            img_input = Input(tensor=input_tensor, shape=input_shape)
        else:
            img_input = input_tensor

    x = __create_fcn_dense_net(classes, img_input, include_top, nb_dense_block,
                               growth_rate, reduction, dropout_rate, weight_decay,
                               nb_layers_per_block, upsampling_conv, upsampling_type,
                               init_conv_filters, input_shape, activation)

    # Ensure that the model takes into account
    # any potential predecessors of `input_tensor`.
    if input_tensor is not None:
        inputs = get_source_inputs(input_tensor)
    else:
        inputs = img_input
    # Create model.
    model = Model(inputs, x, name='fcn-densenet')

    return model",input_shape[1] is not None and input_shape[1] < min_size,None is not input_shape[1] < min_size
primerpython,https://github.com/Helpsypoo/primerpython/tree/master/blender_scripts/video_scenes/scds.py,TextScene,surgery_routes$438,"def surgery_routes(self):
        #Continuation of inner_ear_intro

        cam_bobj, cam_swivel = cam_and_swivel(
            cam_location = [0, 0, 5],
            cam_rotation_euler = [0, 0, 0],
            cam_name = ""Camera Bobject"",
            swivel_location = [0.2, 0, 0.25],
            swivel_rotation_euler = [math.pi / 2, 0,  55 * math.pi / 180],
            swivel_name = 'Cam swivel',
            #control_sun = True
        )
        cam_swivel.add_to_blender(appear_time = -1)
        cam_bobj.ref_obj.children[0].data.clip_end = 200

        start_time = 15
        coch_time = 18.5
        vest_time = 20.5
        sup_time = 29.5
        temp_time = 32.5
        dehiscence_time = 39.5
        head_time = 48

        behind_time = 55

        r_inner_ear = bpy.data.objects['inner ear_from microCT']
        t_bone = bpy.data.objects['Temporal Bone 2 bone.outer']
        skin = bpy.data.objects['robertot']
        to_keep = [r_inner_ear, t_bone, skin]
        for obj in bpy.data.objects:
            if obj not in to_keep:
                obj.hide = True
                obj.hide_render = True

        slots = r_inner_ear.material_slots
        v_sys_mats = [
            slots[1].material,
            slots[2].material,
            slots[3].material,
            slots[4].material
        ]
        coch_mat = slots[0].material
        sup_mat = slots[2].material

        for mat in v_sys_mats + [coch_mat]:
            nodes = mat.node_tree.nodes
            mix = nodes['Mix Shader']
            mix.inputs[0].default_value = 0
            princ = nodes['Principled BSDF']
            color = princ.inputs[0]

            if mat == coch_mat:
                starting_color = list(color.default_value)
                color.keyframe_insert(
                    data_path = 'default_value',
                    frame = coch_time * FRAME_RATE
                )
                color.default_value = [0, 1, 0, 1]
                color.keyframe_insert(
                    data_path = 'default_value',
                    frame = coch_time * FRAME_RATE + OBJECT_APPEARANCE_TIME
                )

                color.keyframe_insert(
                    data_path = 'default_value',
                    frame = (vest_time - 0.5) * FRAME_RATE
                )
                color.default_value = starting_color
                color.keyframe_insert(
                    data_path = 'default_value',
                    frame = (vest_time - 0.5) * FRAME_RATE + OBJECT_APPEARANCE_TIME
                )

            if mat != coch_mat and mat != sup_mat:
                starting_color = list(color.default_value)
                color.keyframe_insert(
                    data_path = 'default_value',
                    frame = vest_time * FRAME_RATE
                )
                color.default_value = [0, 1, 0, 1]
                color.keyframe_insert(
                    data_path = 'default_value',
                    frame = vest_time * FRAME_RATE + OBJECT_APPEARANCE_TIME
                )

                color.keyframe_insert(
                    data_path = 'default_value',
                    frame = (sup_time - 0.5) * FRAME_RATE
                )
                color.default_value = starting_color
                color.keyframe_insert(
                    data_path = 'default_value',
                    frame = (sup_time - 0.5) * FRAME_RATE + OBJECT_APPEARANCE_TIME
                )

            if mat == sup_mat:
                starting_color = list(color.default_value)
                color.keyframe_insert(
                    data_path = 'default_value',
                    frame = vest_time * FRAME_RATE
                )
                color.default_value = [0, 1, 0, 1]
                color.keyframe_insert(
                    data_path = 'default_value',
                    frame = vest_time * FRAME_RATE + OBJECT_APPEARANCE_TIME
                )

                '''color.keyframe_insert(
                    data_path = 'default_value',
                    frame = (temp_time - 0.5) * FRAME_RATE
                )
                color.default_value = starting_color
                color.keyframe_insert(
                    data_path = 'default_value',
                    frame = (temp_time - 0.5) * FRAME_RATE + OBJECT_APPEARANCE_TIME
                )'''

        #Fade in temporal bone, then Dehiscence
        for slot in t_bone.material_slots:
            mat = slot.material
            nodes = mat.node_tree.nodes
            mix = nodes['Mix Shader']
            mix.inputs[0].default_value = 1
            #princ = nodes['Principled BSDF']
            #color = princ.inputs[0]

            mix.inputs[0].keyframe_insert(
                data_path = 'default_value',
                frame = (temp_time) * FRAME_RATE
            )
            mix.inputs[0].default_value = 0.8
            mix.inputs[0].keyframe_insert(
                data_path = 'default_value',
                frame = (temp_time) * FRAME_RATE + OBJECT_APPEARANCE_TIME
            )
            if mat == t_bone.material_slots[1].material: #Dehiscence
                mix.inputs[0].default_value = 0.4
                mix.inputs[0].keyframe_insert(
                    data_path = 'default_value',
                    frame = (dehiscence_time) * FRAME_RATE + OBJECT_APPEARANCE_TIME
                )

                mix2 = nodes['Mix Shader.001'].inputs[0]
                mix2.default_value = 0
                mix2.keyframe_insert(
                    data_path = 'default_value',
                    frame = dehiscence_time * FRAME_RATE
                )
                mix2.default_value = 1
                mix2.keyframe_insert(
                    data_path = 'default_value',
                    frame = dehiscence_time * FRAME_RATE + OBJECT_APPEARANCE_TIME
                )
                em = nodes['Emission']
                color = em.inputs[0]
                '''color.default_value = [1, 1, 1, 1]
                color.keyframe_insert(
                    data_path = 'default_value',
                    frame = turn_red_time * FRAME_RATE
                )'''
                color.default_value = [1, 0, 0, 1]
                '''color.keyframe_insert(
                    data_path = 'default_value',
                    frame = turn_red_time * FRAME_RATE + 2 * OBJECT_APPEARANCE_TIME
                )'''

        #Fade in head for context
        mat = skin.material_slots[0].material
        nodes = mat.node_tree.nodes
        mix = nodes['Mix Shader']
        mix.inputs[0].default_value = 1
        #princ = nodes['Principled BSDF']
        #color = princ.inputs[0]

        mix.inputs[0].keyframe_insert(
            data_path = 'default_value',
            frame = (head_time - 0.5) * FRAME_RATE
        )
        mix.inputs[0].default_value = 0.9
        mix.inputs[0].keyframe_insert(
            data_path = 'default_value',
            frame = (head_time - 0.5) * FRAME_RATE + OBJECT_APPEARANCE_TIME
        )

        #Zoom out for temporal bone
        cam_swivel.move_to(
            new_location = [0.2, 0, 1.25],
            start_time = temp_time - 0.5,
            end_time = temp_time + 2 * OBJECT_APPEARANCE_TIME / FRAME_RATE
        )
        cam_bobj.move_to(
            new_location = [0, 0, 24],
            start_time = temp_time - 0.5,
            end_time = temp_time + 2 * OBJECT_APPEARANCE_TIME / FRAME_RATE
        )

        #Zoom out for head context
        cam_swivel.move_to(
            new_location = [0, 5.1, -2],
            start_time = head_time - 0.5,
            end_time = head_time + 2 * OBJECT_APPEARANCE_TIME / FRAME_RATE
        )
        cam_bobj.move_to(
            new_location = [0, 0, 100],
            start_time = head_time - 0.5,
            end_time = head_time + 2 * OBJECT_APPEARANCE_TIME / FRAME_RATE
        )

        #swivel_rotation_euler = [75 * math.pi / 180, 0, 45 * math.pi / 180],

        #Go behind head for surgery view
        cam_swivel.move_to(
            new_location = [0, -1.9, 0.6],
            new_angle = [math.pi / 2, 0, - math.pi / 2],
            start_time = behind_time - 0.5,
            end_time = behind_time + 2 * OBJECT_APPEARANCE_TIME / FRAME_RATE
        )
        cam_bobj.move_to(
            new_location = [0, 0, 20],
            start_time = behind_time - 0.5,
            end_time = behind_time + 2 * OBJECT_APPEARANCE_TIME / FRAME_RATE
        )

        #Spinnnnnnn camera
        '''cam_swivel.spin(
            spin_rate = 0.11,
            start_time = start_time,
            axis = 2
        )'''",mat != coch_mat and mat != sup_mat,coch_mat != mat != sup_mat
madmom,https://github.com/CPJKU/madmom/tree/master/madmom/evaluation/tempo.py,,tempo_evaluation$57,"def tempo_evaluation(detections, annotations, tolerance=TOLERANCE):
    """"""
    Calculate the tempo P-Score, at least one and all tempi correct.

    Parameters
    ----------
    detections : list of tuples or numpy array
        Detected tempi (rows, first column) and their relative strengths
        (second column).
    annotations : list or numpy array
        Annotated tempi (rows, first column) and their relative strengths
        (second column).
    tolerance : float, optional
        Evaluation tolerance (max. allowed deviation).

    Returns
    -------
    pscore : float
        P-Score.
    at_least_one : bool
        At least one tempo correctly identified.
    all : bool
        All tempi correctly identified.

    Notes
    -----
    All given detections are evaluated against all annotations according to the
    relative strengths given. If no strengths are given, evenly distributed
    strengths are assumed. If the strengths do not sum to 1, they will be
    normalized.

    References
    ----------
    .. [1] M. McKinney, D. Moelants, M. Davies and A. Klapuri,
           ""Evaluation of audio beat tracking and music tempo extraction
           algorithms"",
           Journal of New Music Research, vol. 36, no. 1, 2007.

    """"""
    # neither detections nor annotations are given
    if len(detections) == 0 and len(annotations) == 0:
        # perfect result
        return 1., True, True
    # either detections or annotations are empty
    if len(detections) == 0 or len(annotations) == 0:
        # worst result
        return 0., False, False
    # tolerance must be greater than 0
    if float(tolerance) <= 0:
        raise ValueError('tolerance must be greater than 0')
    # make sure the annotations and detections have a float dtype
    detections = np.array(detections, dtype=float, ndmin=1)
    annotations = np.array(annotations, dtype=float, ndmin=1)
    # extract the detected tempi, ignore the strengths
    if detections.ndim == 2:
        detections = detections[:, 0]
    # extract the annotated tempi and strengths
    strengths = []
    if annotations.ndim == 2:
        # Note: extract the strength before using only the tempo annotations
        strengths = annotations[:, 1]
        annotations = annotations[:, 0]
    # strengths must sum up to 1
    strengths_sum = np.sum(strengths)
    if strengths_sum == 0:
        # uniformly distribute strengths
        warnings.warn('no annotated tempo strengths given, assuming a uniform '
                      'distribution')
        strengths = np.ones_like(annotations) / float(len(annotations))
    elif strengths_sum != 1:
        # normalize strengths
        warnings.warn('annotated tempo strengths do not sum to 1, normalizing')
        strengths /= float(strengths_sum)
    # test all detected tempi against all annotated tempi
    errors = np.abs(1 - (detections[:, np.newaxis] / annotations))
    # correctly identified annotation tempi
    correct = np.asarray(np.sum(errors <= tolerance, axis=0), bool)
    # the P-Score is the sum of the strengths of the correctly identified tempi
    pscore = np.sum(strengths[correct])
    # return the scores
    # TODO: also return the errors?
    return pscore, correct.any(), correct.all()",len(detections) == 0 and len(annotations) == 0,len(detections) == 0 == len(annotations)
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/perspectives/database/gamelist.py,GameList,load_games$130,"def load_games(self, direction=FIRST_PAGE):
        selection = self.get_selection()
        if selection is not None and self.preview_cid is not None and \
                selection.handler_is_connected(self.preview_cid):
            with GObject.signal_handler_block(selection, self.preview_cid):
                self.liststore.clear()
        else:
            self.liststore.clear()

        add = self.liststore.append

        self.records = []
        records, plys = self.persp.chessfile.get_records(direction)
        for i, rec in enumerate(records):
            game_id = rec[""Id""]
            offs = rec[""Offset""]
            wname = rec[""White""]
            bname = rec[""Black""]
            welo = rec[""WhiteElo""]
            belo = rec[""BlackElo""]
            result = rec[""Result""]
            result = ""½-½"" if result == DRAW else reprResult[result] if result else ""*""
            event = """" if rec[""Event""] is None else rec[""Event""].replace(""?"", """")
            site = """" if rec[""Site""] is None else rec[""Site""].replace(""?"", """")
            round_ = """" if rec[""Round""] is None else rec[""Round""].replace(""?"", """")
            date = """" if rec[""Date""] is None else rec[""Date""].replace("".??"", """").replace(""????."", """")

            try:
                ply = rec[""PlyCount""]
                length = str(int(ply) // 2) if ply else """"
            except ValueError:
                length = """"
            eco = rec[""ECO""]
            tc = rec[""TimeControl""]
            variant = rec[""Variant""]
            variant = variants[variant].cecp_name.capitalize() if variant else """"
            fen = rec[""FEN""]

            add([game_id, wname, welo, bname, belo, result, date, event, site,
                 round_, length, eco, tc, variant, fen])

            ply = plys.get(offs) if offs in plys else 0
            self.records.append((rec, ply))

        self.set_cursor(0)",selection is not None and self.preview_cid is not None and selection.handler_is_connected(self.preview_cid),selection is not None is not self.preview_cid and selection.handler_is_connected(self.preview_cid)
Mario-Level-1,https://github.com/justinmeister/Mario-Level-1/tree/master/data/components/mario.py,Mario,changing_to_fire$797,"def changing_to_fire(self):
        """"""Called when Mario is in a BIG_TO_FIRE state (i.e. when
        he obtains a fire flower""""""
        self.in_transition_state = True

        if self.facing_right:
            frames = [self.right_fire_frames[3],
                      self.right_big_green_frames[3],
                      self.right_big_red_frames[3],
                      self.right_big_black_frames[3]]
        else:
            frames = [self.left_fire_frames[3],
                      self.left_big_green_frames[3],
                      self.left_big_red_frames[3],
                      self.left_big_black_frames[3]]

        if self.fire_transition_timer == 0:
            self.fire_transition_timer = self.current_time
        elif (self.current_time - self.fire_transition_timer) > 65 and (self.current_time - self.fire_transition_timer) < 130:
            self.image = frames[0]
        elif (self.current_time - self.fire_transition_timer) < 195:
            self.image = frames[1]
        elif (self.current_time - self.fire_transition_timer) < 260:
            self.image = frames[2]
        elif (self.current_time - self.fire_transition_timer) < 325:
            self.image = frames[3]
        elif (self.current_time - self.fire_transition_timer) < 390:
            self.image = frames[0]
        elif (self.current_time - self.fire_transition_timer) < 455:
            self.image = frames[1]
        elif (self.current_time - self.fire_transition_timer) < 520:
            self.image = frames[2]
        elif (self.current_time - self.fire_transition_timer) < 585:
            self.image = frames[3]
        elif (self.current_time - self.fire_transition_timer) < 650:
            self.image = frames[0]
        elif (self.current_time - self.fire_transition_timer) < 715:
            self.image = frames[1]
        elif (self.current_time - self.fire_transition_timer) < 780:
            self.image = frames[2]
        elif (self.current_time - self.fire_transition_timer) < 845:
            self.image = frames[3]
        elif (self.current_time - self.fire_transition_timer) < 910:
            self.image = frames[0]
        elif (self.current_time - self.fire_transition_timer) < 975:
            self.image = frames[1]
        elif (self.current_time - self.fire_transition_timer) < 1040:
            self.image = frames[2]
            self.fire = True
            self.in_transition_state = False
            self.state = c.WALK
            self.transition_timer = 0",self.current_time - self.fire_transition_timer > 65 and self.current_time - self.fire_transition_timer < 130,65 < self.current_time - self.fire_transition_timer < 130
MB-Lab,https://github.com/animate1978/MB-Lab/tree/master//creation_tools_ops.py,,init_config$223,"def init_config():
    global config_content
    global loaded_project
    global blend_file_content_loaded
    # init collection
    c = bpy.data.collections.get('MB_LAB_Character')
    if c is not None and blend_file_content != None:
        for name in blend_file_content[1]:
            obj = algorithms.get_object_by_name(name)
            bpy.data.objects.remove(obj)
        bpy.data.collections.remove(c)
    # Init variables
    config_content = {""templates_list"": [], ""character_list"": [], ""data_directory"": """"}
    loaded_project = False
    blend_file_content_loaded = False",c is not None and blend_file_content != None,c is not None != blend_file_content
chainercv,https://github.com/chainer/chainercv/tree/master/tests/datasets_tests/online_products_tests/test_online_products_dataset.py,TestOnlineProductsDataset,test_online_products_dataset$23,"def test_online_products_dataset(self):
        assert_is_label_dataset(
            self.dataset, 22634, n_example=10)

        for _ in range(10):
            i = np.random.randint(0, len(self.dataset))
            _, _, super_label = self.dataset[i]

            assert isinstance(super_label, np.int32), \
                'label must be a numpy.int32.'
            assert super_label.ndim == 0, 'The ndim of label must be 0'
            assert (super_label >= 0 and
                    super_label < len(online_products_super_label_names)), \
                'The value of label must be in [0, n_class - 1].'",super_label >= 0 and super_label < len(online_products_super_label_names),0 <= super_label < len(online_products_super_label_names)
PaddleX,https://github.com/PaddlePaddle/PaddleX/tree/master/examples/meter_reader/reader_infer.py,MeterReader,locate_pointer$392,"def locate_pointer(self, line_pointers):
        """"""在线状预测结果中找到指针的中心位置

        参数：
            line_scales (list[np.array])：批量的指针线状预测结果。

        返回：
            scale_locations (list[list])：各图像中指针的中心位置。

        """"""
        batch_size = len(line_pointers)
        pointer_locations = list()
        for i in range(batch_size):
            line_pointer = line_pointers[i]
            find_start = False
            pointer_start = 0
            pointer_end = 0
            location = 0
            width = line_pointer.shape[0]
            for j in range(width - 1):
                if line_pointer[j] > 0 and line_pointer[j + 1] > 0:
                    if find_start == False:
                        pointer_start = j
                        find_start = True
                if find_start:
                    if line_pointer[j] == 0 and line_pointer[j + 1] == 0:
                        pointer_end = j - 1
                        location = (pointer_start + pointer_end) / 2
                        find_start = False
                        break
            pointer_locations.append(location)
        return pointer_locations",line_pointer[j] > 0 and line_pointer[j + 1] > 0,line_pointer[j] > 0 < line_pointer[j + 1]
PaddleX,https://github.com/PaddlePaddle/PaddleX/tree/master/examples/meter_reader/reader_infer.py,MeterReader,locate_pointer$392,"def locate_pointer(self, line_pointers):
        """"""在线状预测结果中找到指针的中心位置

        参数：
            line_scales (list[np.array])：批量的指针线状预测结果。

        返回：
            scale_locations (list[list])：各图像中指针的中心位置。

        """"""
        batch_size = len(line_pointers)
        pointer_locations = list()
        for i in range(batch_size):
            line_pointer = line_pointers[i]
            find_start = False
            pointer_start = 0
            pointer_end = 0
            location = 0
            width = line_pointer.shape[0]
            for j in range(width - 1):
                if line_pointer[j] > 0 and line_pointer[j + 1] > 0:
                    if find_start == False:
                        pointer_start = j
                        find_start = True
                if find_start:
                    if line_pointer[j] == 0 and line_pointer[j + 1] == 0:
                        pointer_end = j - 1
                        location = (pointer_start + pointer_end) / 2
                        find_start = False
                        break
            pointer_locations.append(location)
        return pointer_locations",line_pointer[j] == 0 and line_pointer[j + 1] == 0,line_pointer[j] == 0 == line_pointer[j + 1]
detectron2,https://github.com/facebookresearch/detectron2/tree/master/detectron2/data/build.py,,build_batch_data_loader$282,"def build_batch_data_loader(
    dataset,
    sampler,
    total_batch_size,
    *,
    aspect_ratio_grouping=False,
    num_workers=0,
    collate_fn=None,
):
    """"""
    Build a batched dataloader. The main differences from `torch.utils.data.DataLoader` are:
    1. support aspect ratio grouping options
    2. use no ""batch collation"", because this is common for detection training

    Args:
        dataset (torch.utils.data.Dataset): a pytorch map-style or iterable dataset.
        sampler (torch.utils.data.sampler.Sampler or None): a sampler that produces indices.
            Must be provided iff. ``dataset`` is a map-style dataset.
        total_batch_size, aspect_ratio_grouping, num_workers, collate_fn: see
            :func:`build_detection_train_loader`.

    Returns:
        iterable[list]. Length of each list is the batch size of the current
            GPU. Each element in the list comes from the dataset.
    """"""
    world_size = get_world_size()
    assert (
        total_batch_size > 0 and total_batch_size % world_size == 0
    ), ""Total batch size ({}) must be divisible by the number of gpus ({})."".format(
        total_batch_size, world_size
    )
    batch_size = total_batch_size // world_size

    if isinstance(dataset, torchdata.IterableDataset):
        assert sampler is None, ""sampler must be None if dataset is IterableDataset""
    else:
        dataset = ToIterableDataset(dataset, sampler)

    if aspect_ratio_grouping:
        data_loader = torchdata.DataLoader(
            dataset,
            num_workers=num_workers,
            collate_fn=operator.itemgetter(0),  # don't batch, but yield individual elements
            worker_init_fn=worker_init_reset_seed,
        )  # yield individual mapped dict
        data_loader = AspectRatioGroupedDataset(data_loader, batch_size)
        if collate_fn is None:
            return data_loader
        return MapDataset(data_loader, collate_fn)
    else:
        return torchdata.DataLoader(
            dataset,
            batch_size=batch_size,
            drop_last=True,
            num_workers=num_workers,
            collate_fn=trivial_batch_collator if collate_fn is None else collate_fn,
            worker_init_fn=worker_init_reset_seed,
        )",total_batch_size > 0 and total_batch_size % world_size == 0,total_batch_size > 0 == total_batch_size % world_size
model-analysis,https://github.com/tensorflow/model-analysis/tree/master/tensorflow_model_analysis/metrics/multi_label_confusion_matrix_plot.py,,_multi_label_confusion_matrix_plot$101,"def _multi_label_confusion_matrix_plot(
    thresholds: Optional[List[float]] = None,
    num_thresholds: Optional[int] = None,
    name: str = MULTI_LABEL_CONFUSION_MATRIX_PLOT_NAME,
    eval_config: Optional[config_pb2.EvalConfig] = None,
    model_name: str = '',
    output_name: str = '',
    example_weighted: bool = False,
) -> metric_types.MetricComputations:
  """"""Returns computations for multi-label confusion matrix at thresholds.""""""
  if num_thresholds is not None and thresholds is not None:
    raise ValueError(
        'only one of thresholds or num_thresholds can be set at a time')
  if num_thresholds is None and thresholds is None:
    thresholds = [0.5]
  if num_thresholds is not None:
    thresholds = [
        (i + 1) * 1.0 / (num_thresholds - 1) for i in range(num_thresholds - 2)
    ]
    thresholds = [-_EPSILON] + thresholds + [1.0 + _EPSILON]

  key = metric_types.PlotKey(
      name=name,
      model_name=model_name,
      output_name=output_name,
      example_weighted=example_weighted)
  return [
      metric_types.MetricComputation(
          keys=[key],
          preprocessor=None,
          combiner=_MultiLabelConfusionMatrixPlotCombiner(
              key=key,
              eval_config=eval_config,
              example_weighted=example_weighted,
              thresholds=thresholds))
  ]",num_thresholds is not None and thresholds is not None,num_thresholds is not None is not thresholds
model-analysis,https://github.com/tensorflow/model-analysis/tree/master/tensorflow_model_analysis/metrics/multi_label_confusion_matrix_plot.py,,_multi_label_confusion_matrix_plot$101,"def _multi_label_confusion_matrix_plot(
    thresholds: Optional[List[float]] = None,
    num_thresholds: Optional[int] = None,
    name: str = MULTI_LABEL_CONFUSION_MATRIX_PLOT_NAME,
    eval_config: Optional[config_pb2.EvalConfig] = None,
    model_name: str = '',
    output_name: str = '',
    example_weighted: bool = False,
) -> metric_types.MetricComputations:
  """"""Returns computations for multi-label confusion matrix at thresholds.""""""
  if num_thresholds is not None and thresholds is not None:
    raise ValueError(
        'only one of thresholds or num_thresholds can be set at a time')
  if num_thresholds is None and thresholds is None:
    thresholds = [0.5]
  if num_thresholds is not None:
    thresholds = [
        (i + 1) * 1.0 / (num_thresholds - 1) for i in range(num_thresholds - 2)
    ]
    thresholds = [-_EPSILON] + thresholds + [1.0 + _EPSILON]

  key = metric_types.PlotKey(
      name=name,
      model_name=model_name,
      output_name=output_name,
      example_weighted=example_weighted)
  return [
      metric_types.MetricComputation(
          keys=[key],
          preprocessor=None,
          combiner=_MultiLabelConfusionMatrixPlotCombiner(
              key=key,
              eval_config=eval_config,
              example_weighted=example_weighted,
              thresholds=thresholds))
  ]",num_thresholds is None and thresholds is None,num_thresholds is None is thresholds
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/esr_ea/esr_search.py,ESRSearch,search$280,"def search(self):
        """"""Search one random model.

        :return: current number of samples, and the model
        :rtype: int and class
        """"""
        if self.indiv_count > 0 and self.indiv_count % self.individual_num == 0:
            if np.sum(np.asarray(self.fit_state)) < self.individual_num:
                return
            else:
                self.update_fitness(self.fitness_pop)
                self.update_elitism(self.fitness_pop)
                self.save_results()
                self.reproduction()
                self.evolution_count += 1
                self.fitness_pop = [0] * self.individual_num
                self.fit_state = [0] * self.individual_num
        current_indiv = self.pop[self.indiv_count % self.individual_num]
        indiv_cfg = self.codec.decode(current_indiv)
        self.indiv_count += 1
        logging.info('model parameters:{}, model flops:{}'.format(current_indiv.parameter, current_indiv.flops))
        logging.info('model arch:{}'.format(current_indiv.active_net_list()))
        return self.indiv_count, indiv_cfg",self.indiv_count > 0 and self.indiv_count % self.individual_num == 0,self.indiv_count > 0 == self.indiv_count % self.individual_num
bonobo,https://github.com/python-bonobo/bonobo/tree/master/tests/config/test_services.py,,test_create_container_override$155,"def test_create_container_override():
    c = create_container({""http"": ""http"", ""fs"": ""fs""})
    assert len(c) == 2
    assert ""fs"" in c and c[""fs""] == ""fs""
    assert ""http"" in c and c[""http""] == ""http""",'fs' in c and c['fs'] == 'fs',c['fs'] == 'fs' in c
bonobo,https://github.com/python-bonobo/bonobo/tree/master/tests/config/test_services.py,,test_create_container_override$155,"def test_create_container_override():
    c = create_container({""http"": ""http"", ""fs"": ""fs""})
    assert len(c) == 2
    assert ""fs"" in c and c[""fs""] == ""fs""
    assert ""http"" in c and c[""http""] == ""http""",'http' in c and c['http'] == 'http',c['http'] == 'http' in c
search-plugins,https://github.com/qbittorrent/search-plugins/tree/master/nova3/engines/jackett.py,jackett,search$76,"def search(self, what, cat='all'):
        what = unquote(what)
        category = self.supported_categories[cat.lower()]

        # check for malformed configuration
        if 'malformed' in CONFIG_DATA:
            self.handle_error(""malformed configuration file"", what)
            return

        # check api_key
        if self.api_key == ""YOUR_API_KEY_HERE"":
            self.handle_error(""api key error"", what)
            return

        # prepare jackett url
        params = [
            ('apikey', self.api_key),
            ('q', what)
        ]
        if category is not None:
            params.append(('cat', ','.join(category)))
        params = urlencode(params)
        jacket_url = self.url + ""/api/v2.0/indexers/all/results/torznab/api?%s"" % params
        response = self.get_response(jacket_url)
        if response is None:
            self.handle_error(""connection error"", what)
            return

        # process search results
        response_xml = xml.etree.ElementTree.fromstring(response)
        for result in response_xml.find('channel').findall('item'):
            res = {}

            title = result.find('title')
            if title is not None:
                title = title.text
            else:
                continue

            tracker = result.find('jackettindexer')
            tracker = '' if tracker is None else tracker.text
            if CONFIG_DATA['tracker_first']:
                res['name'] = '[%s] %s' % (tracker, title)
            else:
                res['name'] = '%s [%s]' % (title, tracker)

            res['link'] = result.find(self.generate_xpath('magneturl'))
            if res['link'] is not None:
                res['link'] = res['link'].attrib['value']
            else:
                res['link'] = result.find('link')
                if res['link'] is not None:
                    res['link'] = res['link'].text
                else:
                    continue

            res['size'] = result.find('size')
            res['size'] = -1 if res['size'] is None else (res['size'].text + ' B')

            res['seeds'] = result.find(self.generate_xpath('seeders'))
            res['seeds'] = -1 if res['seeds'] is None else int(res['seeds'].attrib['value'])

            res['leech'] = result.find(self.generate_xpath('peers'))
            res['leech'] = -1 if res['leech'] is None else int(res['leech'].attrib['value'])

            if res['seeds'] != -1 and res['leech'] != -1:
                res['leech'] -= res['seeds']

            res['desc_link'] = result.find('comments')
            if res['desc_link'] is not None:
                res['desc_link'] = res['desc_link'].text
            else:
                res['desc_link'] = result.find('guid')
                res['desc_link'] = '' if res['desc_link'] is None else res['desc_link'].text

            # note: engine_url can't be changed, torrent download stops working
            res['engine_url'] = self.url

            prettyPrinter(self.escape_pipe(res))",res['seeds'] != -1 and res['leech'] != -1,res['seeds'] != -1 != res['leech']
keyboard,https://github.com/boppreh/keyboard/tree/master/keyboard/_darwinmouse.py,,release$115,"def release(button=LEFT):
    """""" Sends an up event for the specified button, using the provided constants """"""
    location = get_position()
    button_code, _, button_up, _ = _button_mapping[button]
    e = Quartz.CGEventCreateMouseEvent(
        None,
        button_up,
        location,
        button_code)

    if _last_click[""time""] is not None and _last_click[""time""] > datetime.datetime.now() - datetime.timedelta(microseconds=300000) and _last_click[""button""] == button and _last_click[""position""] == location:
        # Repeated Click
        Quartz.CGEventSetIntegerValueField(
            e,
            Quartz.kCGMouseEventClickState,
            _last_click[""click_count""])
    Quartz.CGEventPost(Quartz.kCGHIDEventTap, e)
    _button_state[button] = False",_last_click['time'] is not None and _last_click['time'] > datetime.datetime.now() - datetime.timedelta(microseconds=300000) and (_last_click['button'] == button) and (_last_click['position'] == location),None is not _last_click['time'] > datetime.datetime.now() - datetime.timedelta(microseconds=300000) and _last_click['button'] == button and (_last_click['position'] == location)
autogluon,https://github.com/awslabs/autogluon/tree/master/vision/src/autogluon/vision/predictor/predictor.py,ImagePredictor,_validate_kwargs$510,"def _validate_kwargs(self, kwargs):
        """"""validate and initialize default kwargs""""""

        valid_kwargs = {'holdout_frac', 'random_state', 'nthreads_per_trial', 'ngpus_per_trial', 'hyperparameter_tune_kwargs'}
        invalid_kwargs = []
        for key in kwargs:
            if key not in valid_kwargs:
                invalid_kwargs.append(key)
        if invalid_kwargs:
            raise KeyError(f'Invalid kwargs specified: {invalid_kwargs}. Valid kwargs names: {list(valid_kwargs)}')

        kwargs['holdout_frac'] = kwargs.get('holdout_frac', 0.1)
        if not (0 < kwargs['holdout_frac'] < 1.0):
            raise ValueError(f'Range error for `holdout_frac`, expected to be within range (0, 1), given {kwargs[""holdout_frac""]}')
        kwargs['random_state'] = kwargs.get('random_state', None)
        kwargs['nthreads_per_trial'] = kwargs.get('nthreads_per_trial', None)
        kwargs['ngpus_per_trial'] = kwargs.get('ngpus_per_trial', None)
        if kwargs['ngpus_per_trial'] is not None and kwargs['ngpus_per_trial'] > 0:
            detected_gpu = self._get_num_gpus_available()
            if detected_gpu < kwargs['ngpus_per_trial']:
                raise ValueError(f""Insufficient detected # gpus {detected_gpu} vs requested {kwargs['ngpus_per_trial']}"")
        # tune kwargs
        hpo_tune_args = kwargs.get('hyperparameter_tune_kwargs', {})
        hpo_tune_args['num_trials'] = hpo_tune_args.get('num_trials', 1)
        hpo_tune_args['searcher'] = hpo_tune_args.get('searcher', 'random')
        if not hpo_tune_args['searcher'] in ('random', 'grid'):
            raise ValueError(f""Invalid searcher: {hpo_tune_args['searcher']}, supported: ('random', 'grid')"")
        hpo_tune_args['scheduler'] = hpo_tune_args.get('scheduler', 'local')
        if not hpo_tune_args['scheduler'] in ('fifo', 'local'):
            raise ValueError(f""Invalid searcher: {hpo_tune_args['searcher']}, supported: ('fifo', 'local')"")
        hpo_tune_args['max_reward'] = hpo_tune_args.get('max_reward', None)
        if hpo_tune_args['max_reward'] is not None and hpo_tune_args['max_reward'] < 0:
            raise ValueError(f""Expected `max_reward` to be a positive float number between 0 and 1.0, given {hpo_tune_args['max_reward']}"")
        hpo_tune_args['scheduler_options'] = hpo_tune_args.get('scheduler_options', None)
        kwargs['hyperparameter_tune_kwargs'] = hpo_tune_args
        return kwargs",kwargs['ngpus_per_trial'] is not None and kwargs['ngpus_per_trial'] > 0,None is not kwargs['ngpus_per_trial'] > 0
autogluon,https://github.com/awslabs/autogluon/tree/master/vision/src/autogluon/vision/predictor/predictor.py,ImagePredictor,_validate_kwargs$510,"def _validate_kwargs(self, kwargs):
        """"""validate and initialize default kwargs""""""

        valid_kwargs = {'holdout_frac', 'random_state', 'nthreads_per_trial', 'ngpus_per_trial', 'hyperparameter_tune_kwargs'}
        invalid_kwargs = []
        for key in kwargs:
            if key not in valid_kwargs:
                invalid_kwargs.append(key)
        if invalid_kwargs:
            raise KeyError(f'Invalid kwargs specified: {invalid_kwargs}. Valid kwargs names: {list(valid_kwargs)}')

        kwargs['holdout_frac'] = kwargs.get('holdout_frac', 0.1)
        if not (0 < kwargs['holdout_frac'] < 1.0):
            raise ValueError(f'Range error for `holdout_frac`, expected to be within range (0, 1), given {kwargs[""holdout_frac""]}')
        kwargs['random_state'] = kwargs.get('random_state', None)
        kwargs['nthreads_per_trial'] = kwargs.get('nthreads_per_trial', None)
        kwargs['ngpus_per_trial'] = kwargs.get('ngpus_per_trial', None)
        if kwargs['ngpus_per_trial'] is not None and kwargs['ngpus_per_trial'] > 0:
            detected_gpu = self._get_num_gpus_available()
            if detected_gpu < kwargs['ngpus_per_trial']:
                raise ValueError(f""Insufficient detected # gpus {detected_gpu} vs requested {kwargs['ngpus_per_trial']}"")
        # tune kwargs
        hpo_tune_args = kwargs.get('hyperparameter_tune_kwargs', {})
        hpo_tune_args['num_trials'] = hpo_tune_args.get('num_trials', 1)
        hpo_tune_args['searcher'] = hpo_tune_args.get('searcher', 'random')
        if not hpo_tune_args['searcher'] in ('random', 'grid'):
            raise ValueError(f""Invalid searcher: {hpo_tune_args['searcher']}, supported: ('random', 'grid')"")
        hpo_tune_args['scheduler'] = hpo_tune_args.get('scheduler', 'local')
        if not hpo_tune_args['scheduler'] in ('fifo', 'local'):
            raise ValueError(f""Invalid searcher: {hpo_tune_args['searcher']}, supported: ('fifo', 'local')"")
        hpo_tune_args['max_reward'] = hpo_tune_args.get('max_reward', None)
        if hpo_tune_args['max_reward'] is not None and hpo_tune_args['max_reward'] < 0:
            raise ValueError(f""Expected `max_reward` to be a positive float number between 0 and 1.0, given {hpo_tune_args['max_reward']}"")
        hpo_tune_args['scheduler_options'] = hpo_tune_args.get('scheduler_options', None)
        kwargs['hyperparameter_tune_kwargs'] = hpo_tune_args
        return kwargs",hpo_tune_args['max_reward'] is not None and hpo_tune_args['max_reward'] < 0,None is not hpo_tune_args['max_reward'] < 0
mmdetection-to-tensorrt,https://github.com/grimoire/mmdetection-to-tensorrt/tree/master/mmdet2trt/models/dense_heads/corner_head.py,CornerHeadWraper,decode_heatmap$88,"def decode_heatmap(self,
                       tl_heat,
                       br_heat,
                       tl_off,
                       br_off,
                       tl_emb=None,
                       br_emb=None,
                       tl_centripetal_shift=None,
                       br_centripetal_shift=None,
                       img_meta=None,
                       k=100,
                       kernel=3,
                       distance_threshold=0.5,
                       num_dets=1000):
        with_embedding = tl_emb is not None and br_emb is not None
        with_centripetal_shift = (
            tl_centripetal_shift is not None
            and br_centripetal_shift is not None)
        assert with_embedding + with_centripetal_shift == 1
        batch, _, height, width = tl_heat.size()
        inp_h, inp_w, _ = img_meta['pad_shape']

        # perform nms on heatmaps
        tl_heat = self._local_maximum(tl_heat, kernel=kernel)
        br_heat = self._local_maximum(br_heat, kernel=kernel)

        tl_scores, tl_inds, tl_clses, tl_ys, tl_xs = self._topk(tl_heat, k=k)
        br_scores, br_inds, br_clses, br_ys, br_xs = self._topk(br_heat, k=k)

        # We use repeat instead of expand here because expand is a
        # shallow-copy function. Thus it could cause unexpected testing result
        # sometimes. Using expand will decrease about 10% mAP during testing
        # compared to repeat.
        tl_ys = tl_ys.view(batch, k, 1).repeat(1, 1, k)
        tl_xs = tl_xs.view(batch, k, 1).repeat(1, 1, k)
        br_ys = br_ys.view(batch, 1, k).repeat(1, k, 1)
        br_xs = br_xs.view(batch, 1, k).repeat(1, k, 1)

        tl_off = self._transpose_and_gather_feat(tl_off, tl_inds)
        tl_off = tl_off.view(batch, k, 1, 2)
        br_off = self._transpose_and_gather_feat(br_off, br_inds)
        br_off = br_off.view(batch, 1, k, 2)

        tl_xs = tl_xs + tl_off[..., 0]
        tl_ys = tl_ys + tl_off[..., 1]
        br_xs = br_xs + br_off[..., 0]
        br_ys = br_ys + br_off[..., 1]

        if with_centripetal_shift:
            tl_centripetal_shift = self._transpose_and_gather_feat(
                tl_centripetal_shift, tl_inds).view(batch, k, 1, 2).exp()
            br_centripetal_shift = self._transpose_and_gather_feat(
                br_centripetal_shift, br_inds).view(batch, 1, k, 2).exp()

            tl_ctxs = tl_xs + tl_centripetal_shift[..., 0]
            tl_ctys = tl_ys + tl_centripetal_shift[..., 1]
            br_ctxs = br_xs - br_centripetal_shift[..., 0]
            br_ctys = br_ys - br_centripetal_shift[..., 1]

        # all possible boxes based on top k corners (ignoring class)
        zero_tensor = torch.zeros([1], dtype=torch.int32)
        w_ratio = ((inp_w + zero_tensor).to(tl_heat.device)).float() / (
            (width + zero_tensor).to(tl_heat.device)).float()
        h_ratio = ((inp_h + zero_tensor).to(tl_heat.device)).float() / (
            (height + zero_tensor).to(tl_heat.device)).float()
        tl_xs *= w_ratio
        tl_ys *= h_ratio
        br_xs *= w_ratio
        br_ys *= h_ratio

        if with_centripetal_shift:
            tl_ctxs *= w_ratio
            tl_ctys *= h_ratio
            br_ctxs *= w_ratio
            br_ctys *= h_ratio

        x_off = img_meta['border'][2]
        y_off = img_meta['border'][0]

        tl_xs -= x_off
        tl_ys -= y_off
        br_xs -= x_off
        br_ys -= y_off

        tl_xs *= tl_xs.gt(0.0).type_as(tl_xs)
        tl_ys *= tl_ys.gt(0.0).type_as(tl_ys)
        br_xs *= br_xs.gt(0.0).type_as(br_xs)
        br_ys *= br_ys.gt(0.0).type_as(br_ys)

        bboxes = torch.stack((tl_xs, tl_ys, br_xs, br_ys), dim=3)
        area_bboxes = ((br_xs - tl_xs) * (br_ys - tl_ys)).abs()

        if with_centripetal_shift:
            tl_ctxs -= x_off
            tl_ctys -= y_off
            br_ctxs -= x_off
            br_ctys -= y_off

            tl_ctxs *= tl_ctxs.gt(0.0).type_as(tl_ctxs)
            tl_ctys *= tl_ctys.gt(0.0).type_as(tl_ctys)
            br_ctxs *= br_ctxs.gt(0.0).type_as(br_ctxs)
            br_ctys *= br_ctys.gt(0.0).type_as(br_ctys)

            ct_bboxes = torch.stack((tl_ctxs, tl_ctys, br_ctxs, br_ctys),
                                    dim=3)
            area_ct_bboxes = ((br_ctxs - tl_ctxs) * (br_ctys - tl_ctys)).abs()

            rcentral = torch.zeros_like(ct_bboxes)
            # magic nums from paper section 4.1
            mu = torch.ones_like(area_bboxes) / 2.4
            mu_mask = (area_bboxes > 3500).int().type_as(mu)
            # mu[area_bboxes > 3500] = 1 / 2.1  # large bbox have smaller mu
            mu = (1 / 2.1) * mu_mask + mu * (1. - mu_mask)

            bboxes_center_x = (bboxes[..., 0] + bboxes[..., 2]) / 2
            bboxes_center_y = (bboxes[..., 1] + bboxes[..., 3]) / 2
            rcentral0 = bboxes_center_x - mu * (bboxes[..., 2] -
                                                bboxes[..., 0]) / 2
            rcentral1 = bboxes_center_y - mu * (bboxes[..., 3] -
                                                bboxes[..., 1]) / 2
            rcentral2 = bboxes_center_x + mu * (bboxes[..., 2] -
                                                bboxes[..., 0]) / 2
            rcentral3 = bboxes_center_y + mu * (bboxes[..., 3] -
                                                bboxes[..., 1]) / 2
            rcentral = torch.stack(
                [rcentral0, rcentral1, rcentral2, rcentral3], dim=-1)
            area_rcentral = ((rcentral2 - rcentral0) *
                             (rcentral3 - rcentral1)).abs()
            dists = area_ct_bboxes / area_rcentral

            tl_ctx_inds = (ct_bboxes[..., 0] <= rcentral[..., 0]) | (
                ct_bboxes[..., 0] >= rcentral[..., 2])
            tl_cty_inds = (ct_bboxes[..., 1] <= rcentral[..., 1]) | (
                ct_bboxes[..., 1] >= rcentral[..., 3])
            br_ctx_inds = (ct_bboxes[..., 2] <= rcentral[..., 0]) | (
                ct_bboxes[..., 2] >= rcentral[..., 2])
            br_cty_inds = (ct_bboxes[..., 3] <= rcentral[..., 1]) | (
                ct_bboxes[..., 3] >= rcentral[..., 3])

        if with_embedding:
            tl_emb = self._transpose_and_gather_feat(tl_emb, tl_inds)
            tl_emb = tl_emb.view(batch, k, 1)
            br_emb = self._transpose_and_gather_feat(br_emb, br_inds)
            br_emb = br_emb.view(batch, 1, k)
            dists = torch.abs(tl_emb - br_emb)

        tl_scores = tl_scores.view(batch, k, 1).repeat(1, 1, k)
        br_scores = br_scores.view(batch, 1, k).repeat(1, k, 1)

        scores = (tl_scores + br_scores) / 2  # scores for all possible boxes

        # tl and br should have same class
        tl_clses = tl_clses.view(batch, k, 1).repeat(1, 1, k)
        br_clses = br_clses.view(batch, 1, k).repeat(1, k, 1)
        cls_inds = (tl_clses != br_clses)

        # reject boxes based on distances
        dist_inds = dists > distance_threshold

        # reject boxes based on widths and heights
        width_inds = (br_xs <= tl_xs)
        height_inds = (br_ys <= tl_ys)

        score_neg_mask = cls_inds | width_inds | height_inds | dist_inds
        if with_centripetal_shift:
            score_neg_mask = score_neg_mask | tl_ctx_inds | tl_cty_inds \
                | br_ctx_inds | br_cty_inds

        score_neg_mask = score_neg_mask.int().type_as(scores)
        scores = (1 - score_neg_mask) * scores + score_neg_mask * (-1)

        scores = scores.view(batch, -1)
        scores, inds = torch.topk(scores, num_dets)
        scores = scores.unsqueeze(2)

        bboxes = bboxes.view(batch, -1, 4)
        bboxes = self._gather_feat(bboxes, inds)

        clses = tl_clses.contiguous().view(batch, -1, 1)
        clses = self._gather_feat(clses, inds).float()

        return bboxes, scores, clses",tl_emb is not None and br_emb is not None,tl_emb is not None is not br_emb
mmdetection-to-tensorrt,https://github.com/grimoire/mmdetection-to-tensorrt/tree/master/mmdet2trt/models/dense_heads/corner_head.py,CornerHeadWraper,decode_heatmap$88,"def decode_heatmap(self,
                       tl_heat,
                       br_heat,
                       tl_off,
                       br_off,
                       tl_emb=None,
                       br_emb=None,
                       tl_centripetal_shift=None,
                       br_centripetal_shift=None,
                       img_meta=None,
                       k=100,
                       kernel=3,
                       distance_threshold=0.5,
                       num_dets=1000):
        with_embedding = tl_emb is not None and br_emb is not None
        with_centripetal_shift = (
            tl_centripetal_shift is not None
            and br_centripetal_shift is not None)
        assert with_embedding + with_centripetal_shift == 1
        batch, _, height, width = tl_heat.size()
        inp_h, inp_w, _ = img_meta['pad_shape']

        # perform nms on heatmaps
        tl_heat = self._local_maximum(tl_heat, kernel=kernel)
        br_heat = self._local_maximum(br_heat, kernel=kernel)

        tl_scores, tl_inds, tl_clses, tl_ys, tl_xs = self._topk(tl_heat, k=k)
        br_scores, br_inds, br_clses, br_ys, br_xs = self._topk(br_heat, k=k)

        # We use repeat instead of expand here because expand is a
        # shallow-copy function. Thus it could cause unexpected testing result
        # sometimes. Using expand will decrease about 10% mAP during testing
        # compared to repeat.
        tl_ys = tl_ys.view(batch, k, 1).repeat(1, 1, k)
        tl_xs = tl_xs.view(batch, k, 1).repeat(1, 1, k)
        br_ys = br_ys.view(batch, 1, k).repeat(1, k, 1)
        br_xs = br_xs.view(batch, 1, k).repeat(1, k, 1)

        tl_off = self._transpose_and_gather_feat(tl_off, tl_inds)
        tl_off = tl_off.view(batch, k, 1, 2)
        br_off = self._transpose_and_gather_feat(br_off, br_inds)
        br_off = br_off.view(batch, 1, k, 2)

        tl_xs = tl_xs + tl_off[..., 0]
        tl_ys = tl_ys + tl_off[..., 1]
        br_xs = br_xs + br_off[..., 0]
        br_ys = br_ys + br_off[..., 1]

        if with_centripetal_shift:
            tl_centripetal_shift = self._transpose_and_gather_feat(
                tl_centripetal_shift, tl_inds).view(batch, k, 1, 2).exp()
            br_centripetal_shift = self._transpose_and_gather_feat(
                br_centripetal_shift, br_inds).view(batch, 1, k, 2).exp()

            tl_ctxs = tl_xs + tl_centripetal_shift[..., 0]
            tl_ctys = tl_ys + tl_centripetal_shift[..., 1]
            br_ctxs = br_xs - br_centripetal_shift[..., 0]
            br_ctys = br_ys - br_centripetal_shift[..., 1]

        # all possible boxes based on top k corners (ignoring class)
        zero_tensor = torch.zeros([1], dtype=torch.int32)
        w_ratio = ((inp_w + zero_tensor).to(tl_heat.device)).float() / (
            (width + zero_tensor).to(tl_heat.device)).float()
        h_ratio = ((inp_h + zero_tensor).to(tl_heat.device)).float() / (
            (height + zero_tensor).to(tl_heat.device)).float()
        tl_xs *= w_ratio
        tl_ys *= h_ratio
        br_xs *= w_ratio
        br_ys *= h_ratio

        if with_centripetal_shift:
            tl_ctxs *= w_ratio
            tl_ctys *= h_ratio
            br_ctxs *= w_ratio
            br_ctys *= h_ratio

        x_off = img_meta['border'][2]
        y_off = img_meta['border'][0]

        tl_xs -= x_off
        tl_ys -= y_off
        br_xs -= x_off
        br_ys -= y_off

        tl_xs *= tl_xs.gt(0.0).type_as(tl_xs)
        tl_ys *= tl_ys.gt(0.0).type_as(tl_ys)
        br_xs *= br_xs.gt(0.0).type_as(br_xs)
        br_ys *= br_ys.gt(0.0).type_as(br_ys)

        bboxes = torch.stack((tl_xs, tl_ys, br_xs, br_ys), dim=3)
        area_bboxes = ((br_xs - tl_xs) * (br_ys - tl_ys)).abs()

        if with_centripetal_shift:
            tl_ctxs -= x_off
            tl_ctys -= y_off
            br_ctxs -= x_off
            br_ctys -= y_off

            tl_ctxs *= tl_ctxs.gt(0.0).type_as(tl_ctxs)
            tl_ctys *= tl_ctys.gt(0.0).type_as(tl_ctys)
            br_ctxs *= br_ctxs.gt(0.0).type_as(br_ctxs)
            br_ctys *= br_ctys.gt(0.0).type_as(br_ctys)

            ct_bboxes = torch.stack((tl_ctxs, tl_ctys, br_ctxs, br_ctys),
                                    dim=3)
            area_ct_bboxes = ((br_ctxs - tl_ctxs) * (br_ctys - tl_ctys)).abs()

            rcentral = torch.zeros_like(ct_bboxes)
            # magic nums from paper section 4.1
            mu = torch.ones_like(area_bboxes) / 2.4
            mu_mask = (area_bboxes > 3500).int().type_as(mu)
            # mu[area_bboxes > 3500] = 1 / 2.1  # large bbox have smaller mu
            mu = (1 / 2.1) * mu_mask + mu * (1. - mu_mask)

            bboxes_center_x = (bboxes[..., 0] + bboxes[..., 2]) / 2
            bboxes_center_y = (bboxes[..., 1] + bboxes[..., 3]) / 2
            rcentral0 = bboxes_center_x - mu * (bboxes[..., 2] -
                                                bboxes[..., 0]) / 2
            rcentral1 = bboxes_center_y - mu * (bboxes[..., 3] -
                                                bboxes[..., 1]) / 2
            rcentral2 = bboxes_center_x + mu * (bboxes[..., 2] -
                                                bboxes[..., 0]) / 2
            rcentral3 = bboxes_center_y + mu * (bboxes[..., 3] -
                                                bboxes[..., 1]) / 2
            rcentral = torch.stack(
                [rcentral0, rcentral1, rcentral2, rcentral3], dim=-1)
            area_rcentral = ((rcentral2 - rcentral0) *
                             (rcentral3 - rcentral1)).abs()
            dists = area_ct_bboxes / area_rcentral

            tl_ctx_inds = (ct_bboxes[..., 0] <= rcentral[..., 0]) | (
                ct_bboxes[..., 0] >= rcentral[..., 2])
            tl_cty_inds = (ct_bboxes[..., 1] <= rcentral[..., 1]) | (
                ct_bboxes[..., 1] >= rcentral[..., 3])
            br_ctx_inds = (ct_bboxes[..., 2] <= rcentral[..., 0]) | (
                ct_bboxes[..., 2] >= rcentral[..., 2])
            br_cty_inds = (ct_bboxes[..., 3] <= rcentral[..., 1]) | (
                ct_bboxes[..., 3] >= rcentral[..., 3])

        if with_embedding:
            tl_emb = self._transpose_and_gather_feat(tl_emb, tl_inds)
            tl_emb = tl_emb.view(batch, k, 1)
            br_emb = self._transpose_and_gather_feat(br_emb, br_inds)
            br_emb = br_emb.view(batch, 1, k)
            dists = torch.abs(tl_emb - br_emb)

        tl_scores = tl_scores.view(batch, k, 1).repeat(1, 1, k)
        br_scores = br_scores.view(batch, 1, k).repeat(1, k, 1)

        scores = (tl_scores + br_scores) / 2  # scores for all possible boxes

        # tl and br should have same class
        tl_clses = tl_clses.view(batch, k, 1).repeat(1, 1, k)
        br_clses = br_clses.view(batch, 1, k).repeat(1, k, 1)
        cls_inds = (tl_clses != br_clses)

        # reject boxes based on distances
        dist_inds = dists > distance_threshold

        # reject boxes based on widths and heights
        width_inds = (br_xs <= tl_xs)
        height_inds = (br_ys <= tl_ys)

        score_neg_mask = cls_inds | width_inds | height_inds | dist_inds
        if with_centripetal_shift:
            score_neg_mask = score_neg_mask | tl_ctx_inds | tl_cty_inds \
                | br_ctx_inds | br_cty_inds

        score_neg_mask = score_neg_mask.int().type_as(scores)
        scores = (1 - score_neg_mask) * scores + score_neg_mask * (-1)

        scores = scores.view(batch, -1)
        scores, inds = torch.topk(scores, num_dets)
        scores = scores.unsqueeze(2)

        bboxes = bboxes.view(batch, -1, 4)
        bboxes = self._gather_feat(bboxes, inds)

        clses = tl_clses.contiguous().view(batch, -1, 1)
        clses = self._gather_feat(clses, inds).float()

        return bboxes, scores, clses",tl_centripetal_shift is not None and br_centripetal_shift is not None,tl_centripetal_shift is not None is not br_centripetal_shift
scipy,https://github.com/scipy/scipy/tree/master/scipy/optimize/_minpack_py.py,,_check_func$22,"def _check_func(checker, argname, thefunc, x0, args, numinputs,
                output_shape=None):
    res = atleast_1d(thefunc(*((x0[:numinputs],) + args)))
    if (output_shape is not None) and (shape(res) != output_shape):
        if (output_shape[0] != 1):
            if len(output_shape) > 1:
                if output_shape[1] == 1:
                    return shape(res)
            msg = ""%s: there is a mismatch between the input and output "" \
                  ""shape of the '%s' argument"" % (checker, argname)
            func_name = getattr(thefunc, '__name__', None)
            if func_name:
                msg += "" '%s'."" % func_name
            else:
                msg += "".""
            msg += 'Shape should be %s but it is %s.' % (output_shape, shape(res))
            raise TypeError(msg)
    if issubdtype(res.dtype, inexact):
        dt = res.dtype
    else:
        dt = dtype(float)
    return shape(res), dt",output_shape is not None and shape(res) != output_shape,None is not output_shape != shape(res)
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/convbert/tokenization_convbert.py,BasicTokenizer,_is_chinese_char$422,"def _is_chinese_char(self, cp):
        """"""Checks whether CP is the codepoint of a CJK character.""""""
        # This defines a ""chinese character"" as anything in the CJK Unicode block:
        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
        #
        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
        # despite its name. The modern Korean Hangul alphabet is a different block,
        # as is Japanese Hiragana and Katakana. Those alphabets are used to write
        # space-separated words, so they are not treated specially and handled
        # like the all of the other languages.
        if (
            (cp >= 0x4E00 and cp <= 0x9FFF)
            or (cp >= 0x3400 and cp <= 0x4DBF)  #
            or (cp >= 0x20000 and cp <= 0x2A6DF)  #
            or (cp >= 0x2A700 and cp <= 0x2B73F)  #
            or (cp >= 0x2B740 and cp <= 0x2B81F)  #
            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #
            or (cp >= 0xF900 and cp <= 0xFAFF)
            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #
        ):  #
            return True

        return False",cp >= 19968 and cp <= 40959,19968 <= cp <= 40959
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/convbert/tokenization_convbert.py,BasicTokenizer,_is_chinese_char$422,"def _is_chinese_char(self, cp):
        """"""Checks whether CP is the codepoint of a CJK character.""""""
        # This defines a ""chinese character"" as anything in the CJK Unicode block:
        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
        #
        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
        # despite its name. The modern Korean Hangul alphabet is a different block,
        # as is Japanese Hiragana and Katakana. Those alphabets are used to write
        # space-separated words, so they are not treated specially and handled
        # like the all of the other languages.
        if (
            (cp >= 0x4E00 and cp <= 0x9FFF)
            or (cp >= 0x3400 and cp <= 0x4DBF)  #
            or (cp >= 0x20000 and cp <= 0x2A6DF)  #
            or (cp >= 0x2A700 and cp <= 0x2B73F)  #
            or (cp >= 0x2B740 and cp <= 0x2B81F)  #
            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #
            or (cp >= 0xF900 and cp <= 0xFAFF)
            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #
        ):  #
            return True

        return False",cp >= 13312 and cp <= 19903,13312 <= cp <= 19903
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/convbert/tokenization_convbert.py,BasicTokenizer,_is_chinese_char$422,"def _is_chinese_char(self, cp):
        """"""Checks whether CP is the codepoint of a CJK character.""""""
        # This defines a ""chinese character"" as anything in the CJK Unicode block:
        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
        #
        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
        # despite its name. The modern Korean Hangul alphabet is a different block,
        # as is Japanese Hiragana and Katakana. Those alphabets are used to write
        # space-separated words, so they are not treated specially and handled
        # like the all of the other languages.
        if (
            (cp >= 0x4E00 and cp <= 0x9FFF)
            or (cp >= 0x3400 and cp <= 0x4DBF)  #
            or (cp >= 0x20000 and cp <= 0x2A6DF)  #
            or (cp >= 0x2A700 and cp <= 0x2B73F)  #
            or (cp >= 0x2B740 and cp <= 0x2B81F)  #
            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #
            or (cp >= 0xF900 and cp <= 0xFAFF)
            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #
        ):  #
            return True

        return False",cp >= 131072 and cp <= 173791,131072 <= cp <= 173791
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/convbert/tokenization_convbert.py,BasicTokenizer,_is_chinese_char$422,"def _is_chinese_char(self, cp):
        """"""Checks whether CP is the codepoint of a CJK character.""""""
        # This defines a ""chinese character"" as anything in the CJK Unicode block:
        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
        #
        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
        # despite its name. The modern Korean Hangul alphabet is a different block,
        # as is Japanese Hiragana and Katakana. Those alphabets are used to write
        # space-separated words, so they are not treated specially and handled
        # like the all of the other languages.
        if (
            (cp >= 0x4E00 and cp <= 0x9FFF)
            or (cp >= 0x3400 and cp <= 0x4DBF)  #
            or (cp >= 0x20000 and cp <= 0x2A6DF)  #
            or (cp >= 0x2A700 and cp <= 0x2B73F)  #
            or (cp >= 0x2B740 and cp <= 0x2B81F)  #
            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #
            or (cp >= 0xF900 and cp <= 0xFAFF)
            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #
        ):  #
            return True

        return False",cp >= 173824 and cp <= 177983,173824 <= cp <= 177983
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/convbert/tokenization_convbert.py,BasicTokenizer,_is_chinese_char$422,"def _is_chinese_char(self, cp):
        """"""Checks whether CP is the codepoint of a CJK character.""""""
        # This defines a ""chinese character"" as anything in the CJK Unicode block:
        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
        #
        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
        # despite its name. The modern Korean Hangul alphabet is a different block,
        # as is Japanese Hiragana and Katakana. Those alphabets are used to write
        # space-separated words, so they are not treated specially and handled
        # like the all of the other languages.
        if (
            (cp >= 0x4E00 and cp <= 0x9FFF)
            or (cp >= 0x3400 and cp <= 0x4DBF)  #
            or (cp >= 0x20000 and cp <= 0x2A6DF)  #
            or (cp >= 0x2A700 and cp <= 0x2B73F)  #
            or (cp >= 0x2B740 and cp <= 0x2B81F)  #
            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #
            or (cp >= 0xF900 and cp <= 0xFAFF)
            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #
        ):  #
            return True

        return False",cp >= 177984 and cp <= 178207,177984 <= cp <= 178207
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/convbert/tokenization_convbert.py,BasicTokenizer,_is_chinese_char$422,"def _is_chinese_char(self, cp):
        """"""Checks whether CP is the codepoint of a CJK character.""""""
        # This defines a ""chinese character"" as anything in the CJK Unicode block:
        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
        #
        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
        # despite its name. The modern Korean Hangul alphabet is a different block,
        # as is Japanese Hiragana and Katakana. Those alphabets are used to write
        # space-separated words, so they are not treated specially and handled
        # like the all of the other languages.
        if (
            (cp >= 0x4E00 and cp <= 0x9FFF)
            or (cp >= 0x3400 and cp <= 0x4DBF)  #
            or (cp >= 0x20000 and cp <= 0x2A6DF)  #
            or (cp >= 0x2A700 and cp <= 0x2B73F)  #
            or (cp >= 0x2B740 and cp <= 0x2B81F)  #
            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #
            or (cp >= 0xF900 and cp <= 0xFAFF)
            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #
        ):  #
            return True

        return False",cp >= 178208 and cp <= 183983,178208 <= cp <= 183983
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/convbert/tokenization_convbert.py,BasicTokenizer,_is_chinese_char$422,"def _is_chinese_char(self, cp):
        """"""Checks whether CP is the codepoint of a CJK character.""""""
        # This defines a ""chinese character"" as anything in the CJK Unicode block:
        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
        #
        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
        # despite its name. The modern Korean Hangul alphabet is a different block,
        # as is Japanese Hiragana and Katakana. Those alphabets are used to write
        # space-separated words, so they are not treated specially and handled
        # like the all of the other languages.
        if (
            (cp >= 0x4E00 and cp <= 0x9FFF)
            or (cp >= 0x3400 and cp <= 0x4DBF)  #
            or (cp >= 0x20000 and cp <= 0x2A6DF)  #
            or (cp >= 0x2A700 and cp <= 0x2B73F)  #
            or (cp >= 0x2B740 and cp <= 0x2B81F)  #
            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #
            or (cp >= 0xF900 and cp <= 0xFAFF)
            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #
        ):  #
            return True

        return False",cp >= 63744 and cp <= 64255,63744 <= cp <= 64255
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/convbert/tokenization_convbert.py,BasicTokenizer,_is_chinese_char$422,"def _is_chinese_char(self, cp):
        """"""Checks whether CP is the codepoint of a CJK character.""""""
        # This defines a ""chinese character"" as anything in the CJK Unicode block:
        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
        #
        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
        # despite its name. The modern Korean Hangul alphabet is a different block,
        # as is Japanese Hiragana and Katakana. Those alphabets are used to write
        # space-separated words, so they are not treated specially and handled
        # like the all of the other languages.
        if (
            (cp >= 0x4E00 and cp <= 0x9FFF)
            or (cp >= 0x3400 and cp <= 0x4DBF)  #
            or (cp >= 0x20000 and cp <= 0x2A6DF)  #
            or (cp >= 0x2A700 and cp <= 0x2B73F)  #
            or (cp >= 0x2B740 and cp <= 0x2B81F)  #
            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #
            or (cp >= 0xF900 and cp <= 0xFAFF)
            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #
        ):  #
            return True

        return False",cp >= 194560 and cp <= 195103,194560 <= cp <= 195103
yt-dlp,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/wistia.py,WistiaBaseIE,_extract_media$33,"def _extract_media(self, embed_config):
        data = embed_config['media']
        video_id = data['hashedId']
        title = data['name']

        formats = []
        thumbnails = []
        for a in data['assets']:
            aurl = a.get('url')
            if not aurl:
                continue
            astatus = a.get('status')
            atype = a.get('type')
            if (astatus is not None and astatus != 2) or atype in ('preview', 'storyboard'):
                continue
            elif atype in ('still', 'still_image'):
                thumbnails.append({
                    'url': aurl,
                    'width': int_or_none(a.get('width')),
                    'height': int_or_none(a.get('height')),
                    'filesize': int_or_none(a.get('size')),
                })
            else:
                aext = a.get('ext')
                display_name = a.get('display_name')
                format_id = atype
                if atype and atype.endswith('_video') and display_name:
                    format_id = '%s-%s' % (atype[:-6], display_name)
                f = {
                    'format_id': format_id,
                    'url': aurl,
                    'tbr': int_or_none(a.get('bitrate')) or None,
                    'quality': 1 if atype == 'original' else None,
                }
                if display_name == 'Audio':
                    f.update({
                        'vcodec': 'none',
                    })
                else:
                    f.update({
                        'width': int_or_none(a.get('width')),
                        'height': int_or_none(a.get('height')),
                        'vcodec': a.get('codec'),
                    })
                if a.get('container') == 'm3u8' or aext == 'm3u8':
                    ts_f = f.copy()
                    ts_f.update({
                        'ext': 'ts',
                        'format_id': f['format_id'].replace('hls-', 'ts-'),
                        'url': f['url'].replace('.bin', '.ts'),
                    })
                    formats.append(ts_f)
                    f.update({
                        'ext': 'mp4',
                        'protocol': 'm3u8_native',
                    })
                else:
                    f.update({
                        'container': a.get('container'),
                        'ext': aext,
                        'filesize': int_or_none(a.get('size')),
                    })
                formats.append(f)

        self._sort_formats(formats)

        subtitles = {}
        for caption in data.get('captions', []):
            language = caption.get('language')
            if not language:
                continue
            subtitles[language] = [{
                'url': self._EMBED_BASE_URL + 'captions/' + video_id + '.vtt?language=' + language,
            }]

        return {
            'id': video_id,
            'title': title,
            'description': data.get('seoDescription'),
            'formats': formats,
            'thumbnails': thumbnails,
            'duration': float_or_none(data.get('duration')),
            'timestamp': int_or_none(data.get('createdAt')),
            'subtitles': subtitles,
        }",astatus is not None and astatus != 2,None is not astatus != 2
GraphGym,https://github.com/snap-stanford/GraphGym/tree/master/graphgym/loss.py,,compute_loss$9,"def compute_loss(pred, true):
    """"""
    Compute loss and prediction score

    Args:
        pred (torch.tensor): Unnormalized prediction
        true (torch.tensor): Grou

    Returns: Loss, normalized prediction score

    """"""
    bce_loss = nn.BCEWithLogitsLoss(reduction=cfg.model.size_average)
    mse_loss = nn.MSELoss(reduction=cfg.model.size_average)

    # default manipulation for pred and true
    # can be skipped if special loss computation is needed
    pred = pred.squeeze(-1) if pred.ndim > 1 else pred
    true = true.squeeze(-1) if true.ndim > 1 else true

    # Try to load customized loss
    for func in register.loss_dict.values():
        value = func(pred, true)
        if value is not None:
            return value

    if cfg.model.loss_fun == 'cross_entropy':
        # multiclass
        if pred.ndim > 1 and true.ndim == 1:
            pred = F.log_softmax(pred, dim=-1)
            return F.nll_loss(pred, true), pred
        # binary or multilabel
        else:
            true = true.float()
            return bce_loss(pred, true), torch.sigmoid(pred)
    elif cfg.model.loss_fun == 'mse':
        true = true.float()
        return mse_loss(pred, true), pred
    else:
        raise ValueError('Loss func {} not supported'.format(
            cfg.model.loss_fun))",pred.ndim > 1 and true.ndim == 1,pred.ndim > 1 == true.ndim
pywikibot,https://github.com/wikimedia/pywikibot/tree/master/pywikibot/scripts/generate_user_files.py,,get_site_and_lang$107,"def get_site_and_lang(default_family: Optional[str] = 'wikipedia',
                      default_lang: Optional[str] = 'en',
                      default_username: Optional[str] = None, force=False):
    """"""
    Ask the user for the family, site code and username.

    :param default_family: The default family which should be chosen.
    :param default_lang: The default site code which should be chosen,
        if the family supports it.
    :param default_username: The default username which should be chosen.
    :return: The family, site code and username
    :rtype: tuple of three str
    """"""
    known_families = sorted(pywikibot.config.family_files.keys())
    if default_family not in known_families:
        default_family = None
    fam = pywikibot.bot.input_list_choice(
        'Select family of sites we are working on, '
        'just enter the number or name',
        known_families,
        force=force,
        default=default_family)
    fam = pywikibot.family.Family.load(fam)
    if hasattr(fam, 'langs'):
        if hasattr(fam, 'languages_by_size'):
            by_size = [code for code in fam.languages_by_size
                       if code in fam.langs.keys()]
        else:
            by_size = []
        known_langs = by_size + sorted(
            set(fam.langs.keys()).difference(by_size))
    else:
        known_langs = []

    if not known_langs:
        pywikibot.output('There were no known site codes found in {}.'
                         .format(fam.name))
        default_lang = None
    elif len(known_langs) == 1:
        pywikibot.output('The only known site code: {}'.format(known_langs[0]))
        default_lang = known_langs[0]
    else:
        pywikibot.output('This is the list of known site oodes:')
        pywikibot.output(', '.join(known_langs))
        if default_lang not in known_langs:
            if default_lang != 'en' and 'en' in known_langs:
                default_lang = 'en'
            else:
                default_lang = None

    message = ""The site code of the site we're working on""
    mycode = None
    while not mycode:
        mycode = pywikibot.input(message, default=default_lang, force=force)
        if known_langs and mycode and mycode not in known_langs:
            if not pywikibot.input_yn(
                    fill('The site code {!r} is not in the list of known '
                         'sites. Do you want to continue?'.format(mycode)),
                    default=False, automatic_quit=False):
                mycode = None

    message = 'Username on {}:{}'.format(mycode, fam.name)
    username = pywikibot.input(message, default=default_username, force=force)
    # Escape ''s
    if username:
        username = username.replace(""'"", ""\\'"")
    return fam.name, mycode, username",default_lang != 'en' and 'en' in known_langs,default_lang != 'en' in known_langs
NeuroKit,https://github.com/neuropsychology/NeuroKit/tree/master/neurokit2/microstates/microstates_static.py,,_microstates_prevalence_plot$141,"def _microstates_prevalence_plot(microstates, lifetimes, out, ax_prop=None, ax_distrib=None):
    states = np.unique(microstates)

    # Plot
    if ax_prop is None and ax_distrib is None:
        fig, axes = plt.subplots(ncols=2)
        ax_prop = axes[0]
        ax_distrib = axes[1]
    else:
        fig = None

    for s in states:
        ax_prop.bar(s, out[str(s) + ""_Proportion""])
        ax_distrib.plot(lifetimes[s], label=str(s))

    plt.legend()
    ax_prop.set_title(""Proportion"")
    ax_distrib.set_title(""Lifetime Distribution"")

    return fig",ax_prop is None and ax_distrib is None,ax_prop is None is ax_distrib
PaddleViT,https://github.com/BR-IDL/PaddleViT/tree/master/image_classification/RepMLP/mixup.py,Mixup,get_params$181,"def get_params(self):
        """"""Decide to use cutmix or regular mixup by sampling and
           sample lambda for mixup
        """"""
        lam = 1.
        use_cutmix = False
        use_mixup = np.random.rand() < self.mix_prob
        if use_mixup:
            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:
                use_cutmix = np.random.rand() < self.switch_prob
                alpha = self.cutmix_alpha if use_cutmix else self.mixup_alpha
                lam_mix = np.random.beta(alpha, alpha)
            elif self.mixup_alpha == 0. and self.cutmix_alpha > 0.:
                use_cutmix=True
                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)
            elif self.mixup_alpha > 0. and self.cutmix_alpha == 0.:
                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)
            else:
                raise ValueError('mixup_alpha and cutmix_alpha cannot be all 0')
            lam = float(lam_mix)
        return lam, use_cutmix",self.mixup_alpha > 0.0 and self.cutmix_alpha > 0.0,self.mixup_alpha > 0.0 < self.cutmix_alpha
PaddleViT,https://github.com/BR-IDL/PaddleViT/tree/master/image_classification/RepMLP/mixup.py,Mixup,get_params$181,"def get_params(self):
        """"""Decide to use cutmix or regular mixup by sampling and
           sample lambda for mixup
        """"""
        lam = 1.
        use_cutmix = False
        use_mixup = np.random.rand() < self.mix_prob
        if use_mixup:
            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:
                use_cutmix = np.random.rand() < self.switch_prob
                alpha = self.cutmix_alpha if use_cutmix else self.mixup_alpha
                lam_mix = np.random.beta(alpha, alpha)
            elif self.mixup_alpha == 0. and self.cutmix_alpha > 0.:
                use_cutmix=True
                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)
            elif self.mixup_alpha > 0. and self.cutmix_alpha == 0.:
                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)
            else:
                raise ValueError('mixup_alpha and cutmix_alpha cannot be all 0')
            lam = float(lam_mix)
        return lam, use_cutmix",self.mixup_alpha == 0.0 and self.cutmix_alpha > 0.0,self.mixup_alpha == 0.0 < self.cutmix_alpha
PaddleViT,https://github.com/BR-IDL/PaddleViT/tree/master/image_classification/RepMLP/mixup.py,Mixup,get_params$181,"def get_params(self):
        """"""Decide to use cutmix or regular mixup by sampling and
           sample lambda for mixup
        """"""
        lam = 1.
        use_cutmix = False
        use_mixup = np.random.rand() < self.mix_prob
        if use_mixup:
            if self.mixup_alpha > 0. and self.cutmix_alpha > 0.:
                use_cutmix = np.random.rand() < self.switch_prob
                alpha = self.cutmix_alpha if use_cutmix else self.mixup_alpha
                lam_mix = np.random.beta(alpha, alpha)
            elif self.mixup_alpha == 0. and self.cutmix_alpha > 0.:
                use_cutmix=True
                lam_mix = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)
            elif self.mixup_alpha > 0. and self.cutmix_alpha == 0.:
                lam_mix = np.random.beta(self.mixup_alpha, self.mixup_alpha)
            else:
                raise ValueError('mixup_alpha and cutmix_alpha cannot be all 0')
            lam = float(lam_mix)
        return lam, use_cutmix",self.mixup_alpha > 0.0 and self.cutmix_alpha == 0.0,self.mixup_alpha > 0.0 == self.cutmix_alpha
pycord,https://github.com/Pycord-Development/pycord/tree/master/discord/webhook/sync.py,SyncWebhook,edit$812,"def edit(
        self,
        *,
        reason: str | None = None,
        name: str | None = MISSING,
        avatar: bytes | None = MISSING,
        channel: Snowflake | None = None,
        prefer_auth: bool = True,
    ) -> SyncWebhook:
        """"""Edits this Webhook.

        Parameters
        ----------
        name: Optional[:class:`str`]
            The webhook's new default name.
        avatar: Optional[:class:`bytes`]
            A :term:`py:bytes-like object` representing the webhook's new default avatar.
        channel: Optional[:class:`abc.Snowflake`]
            The webhook's new channel. This requires an authenticated webhook.
        reason: Optional[:class:`str`]
            The reason for editing this webhook. Shows up on the audit log.

            .. versionadded:: 1.4
        prefer_auth: :class:`bool`
            Whether to use the bot token over the webhook token
            if available. Defaults to ``True``.

        Returns
        -------
        :class:`SyncWebhook`
            The newly edited webhook.

        Raises
        ------
        HTTPException
            Editing the webhook failed.
        NotFound
            This webhook does not exist.
        InvalidArgument
            This webhook does not have a token associated with it, or
            it tried editing a channel without authentication.
        """"""
        if self.token is None and self.auth_token is None:
            raise InvalidArgument(
                ""This webhook does not have a token associated with it""
            )

        payload = {}
        if name is not MISSING:
            payload[""name""] = str(name) if name is not None else None

        if avatar is not MISSING:
            payload[""avatar""] = (
                utils._bytes_to_base64_data(avatar) if avatar is not None else None
            )

        adapter: WebhookAdapter = _get_webhook_adapter()

        data: WebhookPayload | None = None
        # If a channel is given, always use the authenticated endpoint
        if channel is not None:
            if self.auth_token is None:
                raise InvalidArgument(""Editing channel requires authenticated webhook"")

            payload[""channel_id""] = channel.id
            data = adapter.edit_webhook(
                self.id,
                self.auth_token,
                payload=payload,
                session=self.session,
                reason=reason,
            )

        if prefer_auth and self.auth_token:
            data = adapter.edit_webhook(
                self.id,
                self.auth_token,
                payload=payload,
                session=self.session,
                reason=reason,
            )
        elif self.token:
            data = adapter.edit_webhook_with_token(
                self.id,
                self.token,
                payload=payload,
                session=self.session,
                reason=reason,
            )

        if data is None:
            raise RuntimeError(""Unreachable code hit: data was not assigned"")

        return SyncWebhook(
            data=data, session=self.session, token=self.auth_token, state=self._state
        )",self.token is None and self.auth_token is None,self.token is None is self.auth_token
PaddleOCR,https://github.com/PaddlePaddle/PaddleOCR/tree/master/ppocr/modeling/backbones/det_resnet_vd_sast.py,ResNet_SAST,__init__$176,"def __init__(self, in_channels=3, layers=50, **kwargs):
        super(ResNet_SAST, self).__init__()

        self.layers = layers
        supported_layers = [18, 34, 50, 101, 152, 200]
        assert layers in supported_layers, \
            ""supported layers are {} but input layer is {}"".format(
                supported_layers, layers)

        if layers == 18:
            depth = [2, 2, 2, 2]
        elif layers == 34 or layers == 50:
            # depth = [3, 4, 6, 3]
            depth = [3, 4, 6, 3, 3]
        elif layers == 101:
            depth = [3, 4, 23, 3]
        elif layers == 152:
            depth = [3, 8, 36, 3]
        elif layers == 200:
            depth = [3, 12, 48, 3]
        # num_channels = [64, 256, 512,
        #                 1024] if layers >= 50 else [64, 64, 128, 256]
        # num_filters = [64, 128, 256, 512]
        num_channels = [64, 256, 512,
                        1024, 2048] if layers >= 50 else [64, 64, 128, 256]
        num_filters = [64, 128, 256, 512, 512]

        self.conv1_1 = ConvBNLayer(
            in_channels=in_channels,
            out_channels=32,
            kernel_size=3,
            stride=2,
            act='relu',
            name=""conv1_1"")
        self.conv1_2 = ConvBNLayer(
            in_channels=32,
            out_channels=32,
            kernel_size=3,
            stride=1,
            act='relu',
            name=""conv1_2"")
        self.conv1_3 = ConvBNLayer(
            in_channels=32,
            out_channels=64,
            kernel_size=3,
            stride=1,
            act='relu',
            name=""conv1_3"")
        self.pool2d_max = nn.MaxPool2D(kernel_size=3, stride=2, padding=1)

        self.stages = []
        self.out_channels = [3, 64]
        if layers >= 50:
            for block in range(len(depth)):
                block_list = []
                shortcut = False
                for i in range(depth[block]):
                    if layers in [101, 152] and block == 2:
                        if i == 0:
                            conv_name = ""res"" + str(block + 2) + ""a""
                        else:
                            conv_name = ""res"" + str(block + 2) + ""b"" + str(i)
                    else:
                        conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    bottleneck_block = self.add_sublayer(
                        'bb_%d_%d' % (block, i),
                        BottleneckBlock(
                            in_channels=num_channels[block]
                            if i == 0 else num_filters[block] * 4,
                            out_channels=num_filters[block],
                            stride=2 if i == 0 and block != 0 else 1,
                            shortcut=shortcut,
                            if_first=block == i == 0,
                            name=conv_name))
                    shortcut = True
                    block_list.append(bottleneck_block)
                self.out_channels.append(num_filters[block] * 4)
                self.stages.append(nn.Sequential(*block_list))
        else:
            for block in range(len(depth)):
                block_list = []
                shortcut = False
                for i in range(depth[block]):
                    conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    basic_block = self.add_sublayer(
                        'bb_%d_%d' % (block, i),
                        BasicBlock(
                            in_channels=num_channels[block]
                            if i == 0 else num_filters[block],
                            out_channels=num_filters[block],
                            stride=2 if i == 0 and block != 0 else 1,
                            shortcut=shortcut,
                            if_first=block == i == 0,
                            name=conv_name))
                    shortcut = True
                    block_list.append(basic_block)
                self.out_channels.append(num_filters[block])
                self.stages.append(nn.Sequential(*block_list))",i == 0 and block != 0,i == 0 != block
PaddleOCR,https://github.com/PaddlePaddle/PaddleOCR/tree/master/ppocr/modeling/backbones/det_resnet_vd_sast.py,ResNet_SAST,__init__$176,"def __init__(self, in_channels=3, layers=50, **kwargs):
        super(ResNet_SAST, self).__init__()

        self.layers = layers
        supported_layers = [18, 34, 50, 101, 152, 200]
        assert layers in supported_layers, \
            ""supported layers are {} but input layer is {}"".format(
                supported_layers, layers)

        if layers == 18:
            depth = [2, 2, 2, 2]
        elif layers == 34 or layers == 50:
            # depth = [3, 4, 6, 3]
            depth = [3, 4, 6, 3, 3]
        elif layers == 101:
            depth = [3, 4, 23, 3]
        elif layers == 152:
            depth = [3, 8, 36, 3]
        elif layers == 200:
            depth = [3, 12, 48, 3]
        # num_channels = [64, 256, 512,
        #                 1024] if layers >= 50 else [64, 64, 128, 256]
        # num_filters = [64, 128, 256, 512]
        num_channels = [64, 256, 512,
                        1024, 2048] if layers >= 50 else [64, 64, 128, 256]
        num_filters = [64, 128, 256, 512, 512]

        self.conv1_1 = ConvBNLayer(
            in_channels=in_channels,
            out_channels=32,
            kernel_size=3,
            stride=2,
            act='relu',
            name=""conv1_1"")
        self.conv1_2 = ConvBNLayer(
            in_channels=32,
            out_channels=32,
            kernel_size=3,
            stride=1,
            act='relu',
            name=""conv1_2"")
        self.conv1_3 = ConvBNLayer(
            in_channels=32,
            out_channels=64,
            kernel_size=3,
            stride=1,
            act='relu',
            name=""conv1_3"")
        self.pool2d_max = nn.MaxPool2D(kernel_size=3, stride=2, padding=1)

        self.stages = []
        self.out_channels = [3, 64]
        if layers >= 50:
            for block in range(len(depth)):
                block_list = []
                shortcut = False
                for i in range(depth[block]):
                    if layers in [101, 152] and block == 2:
                        if i == 0:
                            conv_name = ""res"" + str(block + 2) + ""a""
                        else:
                            conv_name = ""res"" + str(block + 2) + ""b"" + str(i)
                    else:
                        conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    bottleneck_block = self.add_sublayer(
                        'bb_%d_%d' % (block, i),
                        BottleneckBlock(
                            in_channels=num_channels[block]
                            if i == 0 else num_filters[block] * 4,
                            out_channels=num_filters[block],
                            stride=2 if i == 0 and block != 0 else 1,
                            shortcut=shortcut,
                            if_first=block == i == 0,
                            name=conv_name))
                    shortcut = True
                    block_list.append(bottleneck_block)
                self.out_channels.append(num_filters[block] * 4)
                self.stages.append(nn.Sequential(*block_list))
        else:
            for block in range(len(depth)):
                block_list = []
                shortcut = False
                for i in range(depth[block]):
                    conv_name = ""res"" + str(block + 2) + chr(97 + i)
                    basic_block = self.add_sublayer(
                        'bb_%d_%d' % (block, i),
                        BasicBlock(
                            in_channels=num_channels[block]
                            if i == 0 else num_filters[block],
                            out_channels=num_filters[block],
                            stride=2 if i == 0 and block != 0 else 1,
                            shortcut=shortcut,
                            if_first=block == i == 0,
                            name=conv_name))
                    shortcut = True
                    block_list.append(basic_block)
                self.out_channels.append(num_filters[block])
                self.stages.append(nn.Sequential(*block_list))",i == 0 and block != 0,i == 0 != block
lightning-flash,https://github.com/PyTorchLightning/lightning-flash/tree/master/flash/core/optimizers/lars.py,LARS,step$105,"def step(self, closure=None):
        """"""Performs a single optimization step.

        Args:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """"""
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        # exclude scaling for params with 0 weight decay
        for group in self.param_groups:
            weight_decay = group[""weight_decay""]
            momentum = group[""momentum""]
            dampening = group[""dampening""]
            nesterov = group[""nesterov""]

            for p in group[""params""]:
                if p.grad is None:
                    continue

                d_p = p.grad
                p_norm = torch.norm(p.data)
                g_norm = torch.norm(p.grad.data)

                # lars scaling + weight decay part
                if weight_decay != 0:
                    if p_norm != 0 and g_norm != 0:
                        lars_lr = p_norm / (g_norm + p_norm * weight_decay + self.eps)
                        lars_lr *= self.trust_coefficient

                        d_p = d_p.add(p, alpha=weight_decay)
                        d_p *= lars_lr

                # sgd part
                if momentum != 0:
                    param_state = self.state[p]
                    if ""momentum_buffer"" not in param_state:
                        buf = param_state[""momentum_buffer""] = torch.clone(d_p).detach()
                    else:
                        buf = param_state[""momentum_buffer""]
                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)
                    if nesterov:
                        d_p = d_p.add(buf, alpha=momentum)
                    else:
                        d_p = buf

                p.add_(d_p, alpha=-group[""lr""])

        return loss",p_norm != 0 and g_norm != 0,p_norm != 0 != g_norm
ssd_keras,https://github.com/pierluigiferrari/ssd_keras/tree/master/ssd_encoder_decoder/ssd_output_decoder.py,,decode_detections_fast$228,"def decode_detections_fast(y_pred,
                           confidence_thresh=0.5,
                           iou_threshold=0.45,
                           top_k='all',
                           input_coords='centroids',
                           normalize_coords=True,
                           img_height=None,
                           img_width=None,
                           border_pixels='half'):
    '''
    Convert model prediction output back to a format that contains only the positive box predictions
    (i.e. the same format that `enconde_y()` takes as input).

    Optionally performs confidence thresholding and greedy non-maximum suppression after the decoding stage.

    Note that the decoding procedure used here is not the same as the procedure used in the original Caffe implementation.
    For each box, the procedure used here assigns the box's highest confidence as its predicted class. Then it removes
    all boxes for which the highest confidence is the background class. This results in less work for the subsequent
    non-maximum suppression, because the vast majority of the predictions will be filtered out just by the fact that
    their highest confidence is for the background class. It is much more efficient than the procedure of the original
    implementation, but the results may also differ.

    Arguments:
        y_pred (array): The prediction output of the SSD model, expected to be a Numpy array
            of shape `(batch_size, #boxes, #classes + 4 + 4 + 4)`, where `#boxes` is the total number of
            boxes predicted by the model per image and the last axis contains
            `[one-hot vector for the classes, 4 predicted coordinate offsets, 4 anchor box coordinates, 4 variances]`.
        confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in any positive
            class required for a given box to be considered a positive prediction. A lower value will result
            in better recall, while a higher value will result in better precision. Do not use this parameter with the
            goal to combat the inevitably many duplicates that an SSD will produce, the subsequent non-maximum suppression
            stage will take care of those.
        iou_threshold (float, optional): `None` or a float in [0,1]. If `None`, no non-maximum suppression will be
            performed. If not `None`, greedy NMS will be performed after the confidence thresholding stage, meaning
            all boxes with a Jaccard similarity of greater than `iou_threshold` with a locally maximal box will be removed
            from the set of predictions, where 'maximal' refers to the box score.
        top_k (int, optional): 'all' or an integer with number of highest scoring predictions to be kept for each batch item
            after the non-maximum suppression stage. If 'all', all predictions left after the NMS stage will be kept.
        input_coords (str, optional): The box coordinate format that the model outputs. Can be either 'centroids'
            for the format `(cx, cy, w, h)` (box center coordinates, width, and height), 'minmax' for the format
            `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.
        normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])
            and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs
            relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.
            Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect
            coordinates. Requires `img_height` and `img_width` if set to `True`.
        img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.
        img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.
        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.
            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong
            to the boxes. If 'exclude', the border pixels do not belong to the boxes.
            If 'half', then one of each of the two horizontal and vertical borders belong
            to the boxex, but not the other.

    Returns:
        A python list of length `batch_size` where each list element represents the predicted boxes
        for one image and contains a Numpy array of shape `(boxes, 6)` where each row is a box prediction for
        a non-background class for the respective image in the format `[class_id, confidence, xmin, xmax, ymin, ymax]`.
    '''
    if normalize_coords and ((img_height is None) or (img_width is None)):
        raise ValueError(""If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`"".format(img_height, img_width))

    # 1: Convert the classes from one-hot encoding to their class ID
    y_pred_converted = np.copy(y_pred[:,:,-14:-8]) # Slice out the four offset predictions plus two elements whereto we'll write the class IDs and confidences in the next step
    y_pred_converted[:,:,0] = np.argmax(y_pred[:,:,:-12], axis=-1) # The indices of the highest confidence values in the one-hot class vectors are the class ID
    y_pred_converted[:,:,1] = np.amax(y_pred[:,:,:-12], axis=-1) # Store the confidence values themselves, too

    # 2: Convert the box coordinates from the predicted anchor box offsets to predicted absolute coordinates
    if input_coords == 'centroids':
        y_pred_converted[:,:,[4,5]] = np.exp(y_pred_converted[:,:,[4,5]] * y_pred[:,:,[-2,-1]]) # exp(ln(w(pred)/w(anchor)) / w_variance * w_variance) == w(pred) / w(anchor), exp(ln(h(pred)/h(anchor)) / h_variance * h_variance) == h(pred) / h(anchor)
        y_pred_converted[:,:,[4,5]] *= y_pred[:,:,[-6,-5]] # (w(pred) / w(anchor)) * w(anchor) == w(pred), (h(pred) / h(anchor)) * h(anchor) == h(pred)
        y_pred_converted[:,:,[2,3]] *= y_pred[:,:,[-4,-3]] * y_pred[:,:,[-6,-5]] # (delta_cx(pred) / w(anchor) / cx_variance) * cx_variance * w(anchor) == delta_cx(pred), (delta_cy(pred) / h(anchor) / cy_variance) * cy_variance * h(anchor) == delta_cy(pred)
        y_pred_converted[:,:,[2,3]] += y_pred[:,:,[-8,-7]] # delta_cx(pred) + cx(anchor) == cx(pred), delta_cy(pred) + cy(anchor) == cy(pred)
        y_pred_converted = convert_coordinates(y_pred_converted, start_index=-4, conversion='centroids2corners')
    elif input_coords == 'minmax':
        y_pred_converted[:,:,2:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively
        y_pred_converted[:,:,[2,3]] *= np.expand_dims(y_pred[:,:,-7] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)
        y_pred_converted[:,:,[4,5]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-6], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)
        y_pred_converted[:,:,2:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates
        y_pred_converted = convert_coordinates(y_pred_converted, start_index=-4, conversion='minmax2corners')
    elif input_coords == 'corners':
        y_pred_converted[:,:,2:] *= y_pred[:,:,-4:] # delta(pred) / size(anchor) / variance * variance == delta(pred) / size(anchor) for all four coordinates, where 'size' refers to w or h, respectively
        y_pred_converted[:,:,[2,4]] *= np.expand_dims(y_pred[:,:,-6] - y_pred[:,:,-8], axis=-1) # delta_xmin(pred) / w(anchor) * w(anchor) == delta_xmin(pred), delta_xmax(pred) / w(anchor) * w(anchor) == delta_xmax(pred)
        y_pred_converted[:,:,[3,5]] *= np.expand_dims(y_pred[:,:,-5] - y_pred[:,:,-7], axis=-1) # delta_ymin(pred) / h(anchor) * h(anchor) == delta_ymin(pred), delta_ymax(pred) / h(anchor) * h(anchor) == delta_ymax(pred)
        y_pred_converted[:,:,2:] += y_pred[:,:,-8:-4] # delta(pred) + anchor == pred for all four coordinates
    else:
        raise ValueError(""Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'."")

    # 3: If the model predicts normalized box coordinates and they are supposed to be converted back to absolute coordinates, do that
    if normalize_coords:
        y_pred_converted[:,:,[2,4]] *= img_width # Convert xmin, xmax back to absolute coordinates
        y_pred_converted[:,:,[3,5]] *= img_height # Convert ymin, ymax back to absolute coordinates

    # 4: Decode our huge `(batch, #boxes, 6)` tensor into a list of length `batch` where each list entry is an array containing only the positive predictions
    y_pred_decoded = []
    for batch_item in y_pred_converted: # For each image in the batch...
        boxes = batch_item[np.nonzero(batch_item[:,0])] # ...get all boxes that don't belong to the background class,...
        boxes = boxes[boxes[:,1] >= confidence_thresh] # ...then filter out those positive boxes for which the prediction confidence is too low and after that...
        if iou_threshold: # ...if an IoU threshold is set...
            boxes = _greedy_nms2(boxes, iou_threshold=iou_threshold, coords='corners', border_pixels=border_pixels) # ...perform NMS on the remaining boxes.
        if top_k != 'all' and boxes.shape[0] > top_k: # If we have more than `top_k` results left at this point...
            top_k_indices = np.argpartition(boxes[:,1], kth=boxes.shape[0]-top_k, axis=0)[boxes.shape[0]-top_k:] # ...get the indices of the `top_k` highest-scoring boxes...
            boxes = boxes[top_k_indices] # ...and keep only those boxes...
        y_pred_decoded.append(boxes) # ...and now that we're done, append the array of final predictions for this batch item to the output list

    return y_pred_decoded",top_k != 'all' and boxes.shape[0] > top_k,boxes.shape[0] > top_k != 'all'
kivy,https://github.com/kivy/kivy/tree/master/kivy/lang/parser.py,Parser,execute_directives$485,"def execute_directives(self):
        global __KV_INCLUDES__
        for ln, cmd in self.directives:
            cmd = cmd.strip()
            if __debug__:
                trace('Parser: got directive <%s>' % cmd)
            if cmd[:5] == 'kivy ':
                version = cmd[5:].strip()
                if len(version.split('.')) == 2:
                    version += '.0'
                require(version)
            elif cmd[:4] == 'set ':
                try:
                    name, value = cmd[4:].strip().split(' ', 1)
                except:
                    Logger.exception('')
                    raise ParserException(self, ln, 'Invalid directive syntax')
                try:
                    value = eval(value, global_idmap)
                except:
                    Logger.exception('')
                    raise ParserException(self, ln, 'Invalid value')
                global_idmap[name] = value
            elif cmd[:8] == 'include ':
                ref = cmd[8:].strip()
                force_load = False

                if ref[:6] == 'force ':
                    ref = ref[6:].strip()
                    force_load = True

                # if #:include [force] ""path with quotes around""
                if ref[0] == ref[-1] and ref[0] in ('""', ""'""):
                    c = ref[:3].count(ref[0])
                    ref = ref[c:-c] if c != 2 else ref

                if ref[-3:] != '.kv':
                    Logger.warning('Lang: {0} does not have a valid Kivy'
                                'Language extension (.kv)'.format(ref))
                    break
                if ref in __KV_INCLUDES__:
                    if not os.path.isfile(resource_find(ref) or ref):
                        raise ParserException(self, ln,
                                              'Invalid or unknown file: {0}'
                                              .format(ref))
                    if not force_load:
                        Logger.warning('Lang: {0} has already been included!'
                                    .format(ref))
                        continue
                    else:
                        Logger.debug('Lang: Reloading {0} '
                                     'because include was forced.'
                                     .format(ref))
                        kivy.lang.builder.Builder.unload_file(ref)
                        kivy.lang.builder.Builder.load_file(ref)
                        continue
                Logger.debug('Lang: Including file: {0}'.format(0))
                __KV_INCLUDES__.append(ref)
                kivy.lang.builder.Builder.load_file(ref)
            elif cmd[:7] == 'import ':
                package = cmd[7:].strip()
                z = package.split()
                if len(z) != 2:
                    raise ParserException(self, ln, 'Invalid import syntax')
                alias, package = z
                try:
                    if package not in sys.modules:
                        try:
                            mod = importlib.__import__(package)
                        except ImportError:
                            module_name = '.'.join(package.split('.')[:-1])
                            mod = importlib.__import__(module_name)
                        # resolve the whole thing
                        for part in package.split('.')[1:]:
                            mod = getattr(mod, part)
                    else:
                        mod = sys.modules[package]
                    global_idmap[alias] = mod
                except ImportError:
                    Logger.exception('')
                    raise ParserException(self, ln,
                                          'Unable to import package %r' %
                                          package)
            else:
                raise ParserException(self, ln, 'Unknown directive')","ref[0] == ref[-1] and ref[0] in ('""', ""'"")","ref[-1] == ref[0] in ('""', ""'"")"
d2l-en,https://github.com/d2l-ai/d2l-en/tree/master/d2l/tensorflow.py,,accuracy$1636,"def accuracy(y_hat, y):
    """"""Compute the number of correct predictions.

    Defined in :numref:`sec_utils`""""""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = d2l.argmax(y_hat, axis=1)
    cmp = d2l.astype(y_hat, y.dtype) == y
    return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))",len(y_hat.shape) > 1 and y_hat.shape[1] > 1,len(y_hat.shape) > 1 < y_hat.shape[1]
pywikibot,https://github.com/wikimedia/pywikibot/tree/master/pywikibot/data/api.py,APIGenerator,__iter__$2122,"def __iter__(self):
        """"""
        Submit request and iterate the response.

        Continues response as needed until limit (if defined) is reached.
        """"""
        offset = self.starting_offset
        n = 0
        while True:
            self.request[self.continue_name] = offset
            pywikibot.debug('{}: Request: {}'
                            .format(self.__class__.__name__, self.request),
                            _logger)
            data = self.request.submit()

            n_items = len(data[self.data_name])
            pywikibot.debug('{}: Retrieved {} items'
                            .format(self.__class__.__name__, n_items),
                            _logger)
            if n_items > 0:
                for item in data[self.data_name]:
                    yield item
                    n += 1
                    if self.limit is not None and n >= self.limit:
                        pywikibot.debug('%s: Stopped iterating due to '
                                        'exceeding item limit.' %
                                        self.__class__.__name__, _logger)
                        return
                offset += n_items
            else:
                pywikibot.debug('{}: Stopped iterating due to empty list in '
                                'response.'.format(self.__class__.__name__),
                                _logger)
                break",self.limit is not None and n >= self.limit,None is not self.limit <= n
R-Drop,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/models/rag/modeling_tf_rag.py,TFRagSequenceForGeneration,__init__$1391,"def __init__(
        self,
        config: Optional[PretrainedConfig] = None,
        question_encoder: Optional[TFPreTrainedModel] = None,
        generator: Optional[TFPreTrainedModel] = None,
        retriever: Optional = None,
        **kwargs,
    ):
        assert config is not None or (
            question_encoder is not None and generator is not None
        ), ""Either a configuration or an encoder and a generator has to be provided.""

        if config is None:
            config = RagConfig.from_question_encoder_generator_configs(
                question_encoder.config, generator.config, **kwargs
            )

        super().__init__(config)

        # instantiate model
        self.rag = TFRagModel(
            config=config,
            question_encoder=question_encoder,
            generator=generator,
            retriever=retriever,
            load_weight_prefix=self.load_weight_prefix,
            name=""rag"",
        )",question_encoder is not None and generator is not None,question_encoder is not None is not generator
great_expectations,https://github.com/great-expectations/great_expectations/tree/master/great_expectations/expectations/core/expect_select_column_values_to_be_unique_within_record.py,ExpectSelectColumnValuesToBeUniqueWithinRecord,_prescriptive_renderer$201,"def _prescriptive_renderer(
        cls,
        configuration: Optional[ExpectationConfiguration] = None,
        result: Optional[ExpectationValidationResult] = None,
        runtime_configuration: Optional[dict] = None,
        **kwargs,
    ):
        runtime_configuration = runtime_configuration or {}
        include_column_name = (
            False if runtime_configuration.get(""include_column_name"") is False else True
        )
        styling = runtime_configuration.get(""styling"")

        params = substitute_none_for_missing(
            configuration.kwargs,
            [
                ""column_list"",
                ""ignore_row_if"",
                ""row_condition"",
                ""condition_parser"",
                ""mostly"",
            ],
        )

        if params[""mostly""] is not None and params[""mostly""] < 1.0:
            params_with_json_schema[""mostly_pct""][""value""] = num_to_str(
                params[""mostly""] * 100, precision=15, no_scientific=True
            )
            template_str = ""Values must be unique across columns, at least $mostly_pct % of the time: ""
        else:
            template_str = ""Values must always be unique across columns: ""

        for idx in range(len(params[""column_list""]) - 1):
            template_str += f""$column_list_{str(idx)}, ""
            params[f""column_list_{str(idx)}""] = params[""column_list""][idx]

        last_idx = len(params[""column_list""]) - 1
        template_str += f""$column_list_{str(last_idx)}""
        params[f""column_list_{str(last_idx)}""] = params[""column_list""][last_idx]

        if params[""row_condition""] is not None:
            (
                conditional_template_str,
                conditional_params,
            ) = parse_row_condition_string_pandas_engine(params[""row_condition""])
            template_str = (
                conditional_template_str
                + "", then ""
                + template_str[0].lower()
                + template_str[1:]
            )
            params.update(conditional_params)
        return [
            RenderedStringTemplateContent(
                **{
                    ""content_block_type"": ""string_template"",
                    ""string_template"": {
                        ""template"": template_str,
                        ""params"": params,
                        ""styling"": styling,
                    },
                }
            )
        ]",params['mostly'] is not None and params['mostly'] < 1.0,None is not params['mostly'] < 1.0
tvm,https://github.com/apache/tvm/tree/master/python/tvm/relay/op/contrib/cutlass.py,,make_residual_block_pattern$102,"def make_residual_block_pattern(tensor_op_out, binary_op=""add"", with_act=""relu""):
    """"""Add pattern for residual blocks.""""""
    residual_input = wildcard()
    binary_out = is_op(binary_op)(tensor_op_out, residual_input) | is_op(binary_op)(
        residual_input, tensor_op_out
    )

    if with_act is not None and with_act == ""relu"":
        return is_op(""nn.relu"")(binary_out)

    return binary_out",with_act is not None and with_act == 'relu',None is not with_act == 'relu'
this-word-does-not-exist,https://github.com/turtlesoupy/this-word-does-not-exist/tree/master/title_maker_pro/urban_dictionary_scraper.py,,fetch_all_definitions$252,"def fetch_all_definitions(
    session, to_fetch, already_done=None, save_interval=1000, save_path=""all_words.pickle"", executor=None,
):
    already_done = already_done if already_done is not None else OrderedDict()
    fetch_list = list(to_fetch.values())
    pbar = tqdm(total=len(to_fetch) + len(already_done))
    pbar.update(len(already_done))

    mapper = executor.imap_unordered if executor else map
    for i, (word_url, word) in enumerate(mapper(partial(_fetch_word_lambda, session), fetch_list)):
        if word_url is None:
            logging.warning(f""Skipping due to upstream exception"")
        elif word_url.title not in already_done and word_url.title not in to_fetch:
            logging.error(f""Warning: {word_url.title} from {word_url.url} missing from fetch / done list"")
        else:
            already_done[word_url.title] = word
            del to_fetch[word_url.title]
        pbar.update()

        if i > 0 and i % save_interval == 0:
            with open(save_path, ""wb"") as f:
                pickle.dump(already_done, f, pickle.HIGHEST_PROTOCOL)
    pbar.close()
    return already_done",i > 0 and i % save_interval == 0,i > 0 == i % save_interval
hypothesis,https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/tests/cover/test_slices.py,,test_step_stays_within_bounds$42,"def test_step_stays_within_bounds(size):
    # indices -> (start, stop, step)
    # Stop is exclusive so we use -1 as the floor.
    # This uses the indices that slice produces to make this test more readable
    # due to how splice processes None being a little complex
    assert_all_examples(
        st.slices(size),
        lambda x: (
            x.indices(size)[0] + x.indices(size)[2] <= size
            and x.indices(size)[0] + x.indices(size)[2] >= -size
        )
        or x.start % size == x.stop % size,
    )",x.indices(size)[0] + x.indices(size)[2] <= size and x.indices(size)[0] + x.indices(size)[2] >= -size,size >= x.indices(size)[0] + x.indices(size)[2] >= -size
PaddleSlim,https://github.com/PaddlePaddle/PaddleSlim/tree/master/paddleslim/dygraph/prune/fpgm_pruner.py,FPGMFilterPruner,cal_mask$20,"def cal_mask(self, pruned_ratio, collection, num_head=-1):
        var_name = collection.master_name
        pruned_axis = collection.master_axis
        value = collection.values[var_name]
        groups = 1
        for _detail in collection.all_pruning_details():
            assert (isinstance(_detail.axis, int))
            if _detail.axis == 1:
                _groups = _detail.op.attr('groups')
                if _groups is not None and _groups > 1:
                    groups = _groups
                    break

        dist_sum_list = []
        for out_i in range(value.shape[0]):
            dist_sum = self.get_distance_sum(value, out_i)
            dist_sum_list.append(dist_sum)
        scores = np.array(dist_sum_list)

        if groups > 1:
            scores = scores.reshape([groups, -1])
            scores = np.mean(scores, axis=1)

        sorted_idx = scores.argsort()
        pruned_num = int(round(len(sorted_idx) * pruned_ratio))
        pruned_num = min(len(sorted_idx) - 1, pruned_num)
        pruned_idx = sorted_idx[:pruned_num]
        mask_shape = [value.shape[pruned_axis]]
        mask = np.ones(mask_shape, dtype=""int32"")
        if groups > 1:
            mask = mask.reshape([groups, -1])
        mask[pruned_idx] = 0
        return mask.reshape(mask_shape)",_groups is not None and _groups > 1,None is not _groups > 1
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-graphql/dagster_graphql/cli.py,,ui$183,"def ui(text, file, predefined, variables, remote, output, ephemeral_instance, **kwargs):
    query = None
    if text is not None and file is None and predefined is None:
        query = text.strip(""'\"" \n\t"")
    elif file is not None and text is None and predefined is None:
        query = file.read()
    elif predefined is not None and text is None and file is None:
        query = PREDEFINED_QUERIES[predefined]
    else:
        raise click.UsageError(
            ""Must select one and only one of text (-t), file (-f), or predefined (-p) ""
            ""to select GraphQL document to execute.""
        )

    if remote:
        res = execute_query_against_remote(remote, query, variables)
        print(res)  # pylint: disable=print-call
    else:
        instance = DagsterInstance.ephemeral() if ephemeral_instance else DagsterInstance.get()
        with get_workspace_process_context_from_kwargs(
            instance, version=__version__, read_only=False, kwargs=kwargs
        ) as workspace_process_context:
            execute_query_from_cli(
                workspace_process_context,
                query,
                variables,
                output,
            )",text is not None and file is None and (predefined is None),text is not None is file and predefined is None
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-graphql/dagster_graphql/cli.py,,ui$183,"def ui(text, file, predefined, variables, remote, output, ephemeral_instance, **kwargs):
    query = None
    if text is not None and file is None and predefined is None:
        query = text.strip(""'\"" \n\t"")
    elif file is not None and text is None and predefined is None:
        query = file.read()
    elif predefined is not None and text is None and file is None:
        query = PREDEFINED_QUERIES[predefined]
    else:
        raise click.UsageError(
            ""Must select one and only one of text (-t), file (-f), or predefined (-p) ""
            ""to select GraphQL document to execute.""
        )

    if remote:
        res = execute_query_against_remote(remote, query, variables)
        print(res)  # pylint: disable=print-call
    else:
        instance = DagsterInstance.ephemeral() if ephemeral_instance else DagsterInstance.get()
        with get_workspace_process_context_from_kwargs(
            instance, version=__version__, read_only=False, kwargs=kwargs
        ) as workspace_process_context:
            execute_query_from_cli(
                workspace_process_context,
                query,
                variables,
                output,
            )",file is not None and text is None and (predefined is None),file is not None is text and predefined is None
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/dagster-graphql/dagster_graphql/cli.py,,ui$183,"def ui(text, file, predefined, variables, remote, output, ephemeral_instance, **kwargs):
    query = None
    if text is not None and file is None and predefined is None:
        query = text.strip(""'\"" \n\t"")
    elif file is not None and text is None and predefined is None:
        query = file.read()
    elif predefined is not None and text is None and file is None:
        query = PREDEFINED_QUERIES[predefined]
    else:
        raise click.UsageError(
            ""Must select one and only one of text (-t), file (-f), or predefined (-p) ""
            ""to select GraphQL document to execute.""
        )

    if remote:
        res = execute_query_against_remote(remote, query, variables)
        print(res)  # pylint: disable=print-call
    else:
        instance = DagsterInstance.ephemeral() if ephemeral_instance else DagsterInstance.get()
        with get_workspace_process_context_from_kwargs(
            instance, version=__version__, read_only=False, kwargs=kwargs
        ) as workspace_process_context:
            execute_query_from_cli(
                workspace_process_context,
                query,
                variables,
                output,
            )",predefined is not None and text is None and (file is None),predefined is not None is text and file is None
flare-emu,https://github.com/mandiant/flare-emu/tree/master//flare_emu_radare.py,Radare2AnalysisHelper,getBlockEndInsnAddr$232,"def getBlockEndInsnAddr(self, addr, flowchart):
        try:
            bbs = self._getBasicBlocks(addr)
            bb = list(filter(lambda x: x['addr'] <= addr and (x['addr'] + x['size']) > addr, bbs))[0]
            addr = bb['addr']
            while addr < bb['addr'] + bb['size']:
                insn = self._getOpcode(addr)
                addr += insn['size']
            return insn['addr']
        except:
            return None",x['addr'] <= addr and x['addr'] + x['size'] > addr,x['addr'] <= addr < x['addr'] + x['size']
fairscale,https://github.com/facebookresearch/fairscale/tree/master/benchmarks/moe.py,,train$31,"def train(rank, world_size, benchmark_config, model_specs, args):
    logger = mp.log_to_stderr()
    logger.setLevel(logging.DEBUG if args.debug else logging.INFO)
    utils.init_random_seed(rank)

    init_method_pgroup = ""tcp://localhost:{}"".format(MPI_PORT)
    torch.distributed.init_process_group(
        backend=""nccl"", rank=rank, world_size=world_size, init_method=init_method_pgroup
    )
    logger.info(""train, rank={}"".format(rank))
    device = torch.device(""cuda"", rank) if torch.cuda.is_available() else torch.device(""cpu"")

    criterion = benchmark_config[""criterion""]

    model_config = utils.create_model_config(
        args, benchmark_config=benchmark_config, model_specs=model_specs, device=device
    )
    # vocab_size may change in create_model_config() due to input data
    vocab_size = model_specs[""vocab_size""]
    model = model_config[""model""]
    model.train()
    optimizer = model_config[""optimizer""]
    optimizer = optimizer(model.parameters())
    group = model.group if hasattr(model, ""group"") else None
    utils.log_number_of_parameters(model, logger)

    total_loss = 0.0
    word_counter = 0
    total_tokens = 0
    total_tokens_per_log_interval = 0
    bptt = 2

    total_elapsed = 0.0

    model = DDP(model, device_ids=[rank], output_device=rank, broadcast_buffers=False)
    lm_dataloader, _, _ = utils.get_data_loader(
        model_config[""dataset_info""], args, benchmark_config, model_specs, num_replicas=world_size, rank=rank
    )

    def get_batch(source):
        seq_len = len(source) - 1
        data = source[0:seq_len]
        target = source[1 : 1 + seq_len]
        return data, target

    for i, batch in enumerate(lm_dataloader):
        if i == 1:
            epoch_start_time = time.time()

        if args.max_batch and i > args.max_batch:
            break

        if i > 0:
            total_tokens += batch.numel()

        start_time = time.time()
        optimizer.zero_grad()
        source, target = get_batch(batch)
        source = source.to(device)
        target = target.to(device)
        try:
            output = model(source.to(device))
            loss = criterion(output.view(-1, vocab_size), target.view(-1))
            total_loss += loss.item()
            loss.backward()
            torch.nn.utils.clip_grad_value_(model.parameters(), model_specs[""clip_value""])
            optimizer.step()
        except Exception as e:
            raise RuntimeError(f""training failed on {torch.distributed.get_rank()}"") from e

        elapsed = time.time() - start_time
        total_elapsed += elapsed
        log_interval = 1
        total_tokens_per_log_interval += batch.numel()
        if i % log_interval == 0 and i > 0:
            cur_loss = total_loss / log_interval
            logger.debug(
                ""| batch {:5d} | wps {:5.2f} | loss {:5.2f} | ppl {:8.2f}"".format(
                    i, total_tokens_per_log_interval / elapsed, cur_loss, math.exp(cur_loss)
                )
            )
            total_tokens_per_log_interval = 0
            total_loss = 0

    wps = total_tokens / total_elapsed

    logger.debug(""rank {}, wps: {}"".format(rank, wps))
    logger.debug(
        ""Peak allocated bytes on cuda:{}: {:1d}"".format(
            dist.get_rank(), torch.cuda.memory_stats(dist.get_rank())[""allocated_bytes.all.peak""]
        )
    )",i % log_interval == 0 and i > 0,i % log_interval == 0 < i
opentelemetry-python,https://github.com/open-telemetry/opentelemetry-python/tree/master/exporter/opentelemetry-exporter-jaeger-thrift/src/opentelemetry/exporter/jaeger/thrift/gen/jaeger/ttypes.py,BatchSubmitResponse,read$786,"def read(self, iprot):
        if iprot._fast_decode is not None and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None:
            iprot._fast_decode(self, iprot, (self.__class__, self.thrift_spec))
            return
        iprot.readStructBegin()
        while True:
            (fname, ftype, fid) = iprot.readFieldBegin()
            if ftype == TType.STOP:
                break
            if fid == 1:
                if ftype == TType.BOOL:
                    self.ok = iprot.readBool()
                else:
                    iprot.skip(ftype)
            else:
                iprot.skip(ftype)
            iprot.readFieldEnd()
        iprot.readStructEnd()","iprot._fast_decode is not None and isinstance(iprot.trans, TTransport.CReadableTransport) and (self.thrift_spec is not None)","iprot._fast_decode is not None is not self.thrift_spec and isinstance(iprot.trans, TTransport.CReadableTransport)"
free-python-games,https://github.com/grantjenks/free-python-games/tree/master/freegames/cannon.py,,inside$30,"def inside(xy):
    """"""Return True if xy within screen.""""""
    return -200 < xy.x < 200 and -200 < xy.y < 200",-200 < xy.x < 200 and -200 < xy.y < 200,#NAME?
confidant,https://github.com/lyft/confidant/tree/master/confidant/routes/services.py,,get_service$139,"def get_service(id):
    '''
    Get a service object from the provided service ID.

    .. :quickref: Service; Get a service object from the provided service ID.

    **Example request**:

    .. sourcecode:: http

       GET /v1/services/example-development

    :param id: The service ID to get.
    :type id: str
    :query boolean metadata_only: If true, only fetch metadata for this
      service, and do not respond with decrypted credential pairs in the
      credential responses.

    **Example response**:

    .. sourcecode:: http

       HTTP/1.1 200 OK
       Content-Type: application/json

       {
         ""id"": ""example-development"",
         ""revision"": 1,
         ""enabled"": true,
         ""modified_date"": ""2019-12-16T23:16:11.413299+00:00"",
         ""modified_by"": ""rlane@example.com"",
         ""account"": null,
         ""credentials"": [
           {
             ""id"": ""abcd12345bf4f1cafe8e722d3860404"",
             ""name"": ""Example Credential"",
             ""credential_keys"": [""test_key""],
             ""credential_pairs"": {
               ""test_key"": ""test_value""
             },
             ""metadata"": {
               ""example_metadata_key"": ""example_value""
             },
             ""revision"": 1,
             ""enabled"": true,
             ""documentation"": ""Example documentation"",
             ""modified_date"": ""2019-12-16T23:16:11.413299+00:00"",
             ""modified_by"": ""rlane@example.com"",
             ""permissions"": {}
           },
           ...
         ],
         ""blind_credentials"": [],
         ""permissions"": {
           ""metadata"": true,
           ""get"": true,
           ""update"": true
         }
       }

    :resheader Content-Type: application/json
    :statuscode 200: Success
    :statuscode 403: Client does not have permissions to get the service ID
                     provided.
    '''
    permissions = {
        'metadata': False,
        'get': False,
        'update': False,
    }
    metadata_only = misc.get_boolean(request.args.get('metadata_only'))
    logged_in_user = authnz.get_logged_in_user()
    action = 'metadata' if metadata_only else 'get'
    permissions['metadata'] = acl_module_check(
        resource_type='service',
        action='metadata',
        resource_id=id,
    )
    permissions['get'] = acl_module_check(
        resource_type='service',
        action='get',
        resource_id=id,
    )
    if not permissions[action]:
        msg = ""{} does not have access to get service {}"".format(
            authnz.get_logged_in_user(),
            id
        )
        error_msg = {'error': msg, 'reference': id}
        return jsonify(error_msg), 403

    logger.info(
        'get_service called on id={} by user={} metadata_only={}'.format(
            id,
            logged_in_user,
            metadata_only,
        )
    )
    try:
        service = Service.get(id)
        if not authnz.service_in_account(service.account):
            logger.warning(
                'Authz failed for service {0} (wrong account).'.format(id)
            )
            msg = 'Authenticated user is not authorized.'
            return jsonify({'error': msg}), 401
    except DoesNotExist:
        return jsonify({}), 404
    if (service.data_type != 'service' and
            service.data_type != 'archive-service'):
        return jsonify({}), 404
    logger.debug('Authz succeeded for service {0}.'.format(id))
    try:
        credentials = credentialmanager.get_credentials(service.credentials)
    except KeyError:
        logger.exception('KeyError occurred in getting credentials')
        return jsonify({'error': 'Decryption error.'}), 500
    blind_credentials = credentialmanager.get_blind_credentials(
        service.blind_credentials,
    )
    # TODO: this check can be expensive, so we're gating only to user auth.
    # We should probably add an argument that opts in for permission hints,
    # rather than always checking them.
    if authnz.user_is_user_type('user'):
        combined_cred_ids = (
            list(service.credentials) + list(service.blind_credentials)
        )
        permissions['update'] = acl_module_check(
            resource_type='service',
            action='update',
            resource_id=id,
            kwargs={
                'credential_ids': combined_cred_ids,
            },
        )
    service_response = ServiceResponse.from_service_expanded(
        service,
        credentials=credentials,
        blind_credentials=blind_credentials,
        metadata_only=metadata_only,
    )
    service_response.permissions = permissions
    return service_expanded_response_schema.dumps(service_response)",service.data_type != 'service' and service.data_type != 'archive-service','service' != service.data_type != 'archive-service'
MetPy,https://github.com/Unidata/MetPy/tree/master/src/metpy/calc/tools.py,,_broadcast_to_axis$1249,"def _broadcast_to_axis(arr, axis, ndim):
    """"""Handle reshaping coordinate array to have proper dimensionality.

    This puts the values along the specified axis.
    """"""
    if arr.ndim == 1 and arr.ndim < ndim:
        new_shape = [1] * ndim
        new_shape[axis] = arr.size
        arr = arr.reshape(*new_shape)
    return arr",arr.ndim == 1 and arr.ndim < ndim,1 == arr.ndim < ndim
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/generation_flax_utils.py,FlaxGenerationMixin,generate$211,"def generate(
        self,
        input_ids: jnp.ndarray,
        max_length: Optional[int] = None,
        max_new_tokens: Optional[int] = None,
        pad_token_id: Optional[int] = None,
        bos_token_id: Optional[int] = None,
        eos_token_id: Optional[int] = None,
        decoder_start_token_id: Optional[int] = None,
        do_sample: Optional[bool] = None,
        prng_key: Optional[jnp.ndarray] = None,
        top_k: Optional[int] = None,
        top_p: Optional[float] = None,
        temperature: Optional[float] = None,
        num_beams: Optional[int] = None,
        no_repeat_ngram_size: Optional[int] = None,
        min_length: Optional[int] = None,
        forced_bos_token_id: Optional[int] = None,
        forced_eos_token_id: Optional[int] = None,
        length_penalty: Optional[float] = None,
        early_stopping: Optional[bool] = None,
        trace: bool = True,
        params: Optional[Dict[str, jnp.ndarray]] = None,
        **model_kwargs,
    ):
        r""""""
        Generates sequences of token ids for models with a language modeling head. The method supports the following
        generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:

            - *greedy decoding* by calling [`~generation_flax_utils.FlaxGenerationMixin._greedy_search`] if
              `num_beams=1` and `do_sample=False`.
            - *multinomial sampling* by calling [`~generation_flax_utils.FlaxGenerationMixin._sample`] if `num_beams=1`
              and `do_sample=True`.
            - *beam-search decoding* by calling [`~generation_utils.FlaxGenerationMixin._beam_search`] if `num_beams>1`
              and `do_sample=False`.

        <Tip warning={true}>

        Apart from `inputs`, all the arguments below will default to the value of the attribute of the same name as
        defined in the model's config (`config.json`) which in turn defaults to the
        [`~modeling_utils.PretrainedConfig`] of the model.

        </Tip>

        Most of these parameters are explained in more detail in [this blog
        post](https://huggingface.co/blog/how-to-generate).

        Parameters:
            input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):
                The sequence used as a prompt for the generation.
            max_length (`int`, *optional*, defaults to `model.config.max_length`):
                The maximum length the generated tokens can have. Corresponds to the length of the input prompt +
                `max_new_tokens`. In general, prefer the use of `max_new_tokens`, which ignores the number of tokens in
                the prompt.
            max_new_tokens (`int`, *optional*):
                The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.
            do_sample (`bool`, *optional*, defaults to `False`):
                Whether or not to use sampling ; use greedy decoding otherwise.
            temperature (`float`, *optional*, defaults to 1.0):
                The value used to module the next token probabilities.
            top_k (`int`, *optional*, defaults to 50):
                The number of highest probability vocabulary tokens to keep for top-k-filtering.
            top_p (`float`, *optional*, defaults to 1.0):
                If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher
                are kept for generation.
            pad_token_id (`int`, *optional*):
                The id of the *padding* token.
            bos_token_id (`int`, *optional*):
                The id of the *beginning-of-sequence* token.
            eos_token_id (`int`, *optional*):
                The id of the *end-of-sequence* token.
            num_beams (`int`, *optional*, defaults to 1):
                Number of beams for beam search. 1 means no beam search.
            decoder_start_token_id (`int`, *optional*):
                If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.
            trace (`bool`, *optional*, defaults to `True`):
                Whether to trace generation. Setting `trace=False` should only be used for debugging and will lead to a
                considerably slower runtime.
            params (`Dict[str, jnp.ndarray]`, *optional*):
                Optionally the model parameters can be passed. Can be useful for parallelized generation.
            model_kwargs:
                Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model
                is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
                should be prefixed with *decoder_*. Also accepts `encoder_outputs` to skip encoder part.

        Return:
            [`~utils.ModelOutput`].

        Examples:

        ```python
        >>> from transformers import AutoTokenizer, FlaxAutoModelForCausalLM

        >>> tokenizer = AutoTokenizer.from_pretrained(""distilgpt2"")
        >>> model = FlaxAutoModelForCausalLM.from_pretrained(""distilgpt2"")
        >>> input_context = ""The dog""
        >>> # encode input context
        >>> input_ids = tokenizer(input_context, return_tensors=""np"").input_ids
        >>> # generate candidates using sampling
        >>> outputs = model.generate(input_ids=input_ids, max_length=20, top_k=30, do_sample=True)
        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
        ```""""""
        # Validate the `.generate()` call
        self._validate_model_class()
        self._validate_model_kwargs(model_kwargs.copy())

        # set init values
        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id
        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id
        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id
        decoder_start_token_id = (
            decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id
        )
        prng_key = prng_key if prng_key is not None else jax.random.PRNGKey(0)

        if decoder_start_token_id is None and self.config.is_encoder_decoder:
            raise ValueError(""`decoder_start_token_id` has to be defined for encoder-decoder generation."")

        # decoder-only models should use left-padding for generation (can't be checked with `trace=True`)
        if not self.config.is_encoder_decoder and not trace:
            if pad_token_id is not None and jnp.sum(input_ids[:, -1] == pad_token_id) > 0:
                logger.warning(
                    ""A decoder-only architecture is being used, but right-padding was detected! For correct ""
                    ""generation results, please set `padding_side='left'` when initializing the tokenizer.""
                )

        if self.config.is_encoder_decoder:
            # add encoder_outputs to model_kwargs
            if model_kwargs.get(""encoder_outputs"") is None:
                model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(input_ids, params, model_kwargs)
            # prepare decoder_input_ids for generation
            input_ids = jnp.ones((input_ids.shape[0], 1), dtype=""i4"") * decoder_start_token_id

        # Prepare `max_length` depending on other stopping criteria.
        input_ids_seq_length = input_ids.shape[-1]
        if max_length is None and max_new_tokens is None:
            warnings.warn(
                ""Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to ""
                f""{self.config.max_length} (`self.config.max_length`). Controlling `max_length` via the config is ""
                ""deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend ""
                ""using `max_new_tokens` to control the maximum length of the generation."",
                UserWarning,
            )
        elif max_length is None and max_new_tokens is not None:
            max_length = max_new_tokens + input_ids_seq_length
        elif max_length is not None and max_new_tokens is not None:
            raise ValueError(
                ""Both `max_new_tokens` and `max_length` have been set but they serve the same purpose -- setting a""
                "" limit to the generated output length. Remove one of those arguments. Please refer to the""
                "" documentation for more information. ""
                ""(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)""
            )
        # default to config if still None
        max_length = max_length if max_length is not None else self.config.max_length
        min_length = min_length if min_length is not None else self.config.min_length

        if min_length is not None and min_length > max_length:
            raise ValueError(
                f""Unfeasable length constraints: the minimum length ({min_length}) is larger than the maximum ""
                f""length ({max_length})""
            )
        if input_ids_seq_length >= max_length:
            input_ids_string = ""decoder_input_ids"" if self.config.is_encoder_decoder else ""input_ids""
            logger.warning(
                f""Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to""
                f"" {max_length}. This can lead to unexpected behavior. You should consider increasing""
                ""`max_new_tokens`.""
            )

        do_sample = do_sample if do_sample is not None else self.config.do_sample
        num_beams = num_beams if num_beams is not None else self.config.num_beams

        if not do_sample and num_beams == 1:
            logits_processor = self._get_logits_processor(
                no_repeat_ngram_size, min_length, max_length, eos_token_id, forced_bos_token_id, forced_eos_token_id
            )
            return self._greedy_search(
                input_ids,
                max_length,
                pad_token_id,
                eos_token_id,
                logits_processor=logits_processor,
                trace=trace,
                params=params,
                model_kwargs=model_kwargs,
            )
        elif do_sample and num_beams == 1:
            logits_warper = self._get_logits_warper(top_k=top_k, top_p=top_p, temperature=temperature)
            logits_processor = self._get_logits_processor(
                no_repeat_ngram_size, min_length, max_length, eos_token_id, forced_bos_token_id, forced_eos_token_id
            )
            return self._sample(
                input_ids,
                max_length,
                pad_token_id,
                eos_token_id,
                prng_key,
                logits_warper=logits_warper,
                logits_processor=logits_processor,
                trace=trace,
                params=params,
                model_kwargs=model_kwargs,
            )
        elif not do_sample and num_beams > 1:
            # broadcast input_ids & encoder_outputs
            input_ids = self._expand_to_num_beams(input_ids, num_beams=num_beams)

            if ""encoder_outputs"" in model_kwargs:
                model_kwargs[""encoder_outputs""][""last_hidden_state""] = self._expand_to_num_beams(
                    model_kwargs[""encoder_outputs""][""last_hidden_state""], num_beams=num_beams
                )

            if ""attention_mask"" in model_kwargs:
                model_kwargs[""attention_mask""] = self._expand_to_num_beams(
                    model_kwargs[""attention_mask""], num_beams=num_beams
                )

            logits_processor = self._get_logits_processor(
                no_repeat_ngram_size, min_length, max_length, eos_token_id, forced_bos_token_id, forced_eos_token_id
            )

            return self._beam_search(
                input_ids,
                max_length,
                pad_token_id,
                eos_token_id,
                length_penalty=length_penalty,
                early_stopping=early_stopping,
                logits_processor=logits_processor,
                trace=trace,
                params=params,
                model_kwargs=model_kwargs,
            )
        else:
            raise NotImplementedError(""`Beam sampling is currently not implemented."")",max_length is None and max_new_tokens is None,max_length is None is max_new_tokens
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/generation_flax_utils.py,FlaxGenerationMixin,generate$211,"def generate(
        self,
        input_ids: jnp.ndarray,
        max_length: Optional[int] = None,
        max_new_tokens: Optional[int] = None,
        pad_token_id: Optional[int] = None,
        bos_token_id: Optional[int] = None,
        eos_token_id: Optional[int] = None,
        decoder_start_token_id: Optional[int] = None,
        do_sample: Optional[bool] = None,
        prng_key: Optional[jnp.ndarray] = None,
        top_k: Optional[int] = None,
        top_p: Optional[float] = None,
        temperature: Optional[float] = None,
        num_beams: Optional[int] = None,
        no_repeat_ngram_size: Optional[int] = None,
        min_length: Optional[int] = None,
        forced_bos_token_id: Optional[int] = None,
        forced_eos_token_id: Optional[int] = None,
        length_penalty: Optional[float] = None,
        early_stopping: Optional[bool] = None,
        trace: bool = True,
        params: Optional[Dict[str, jnp.ndarray]] = None,
        **model_kwargs,
    ):
        r""""""
        Generates sequences of token ids for models with a language modeling head. The method supports the following
        generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:

            - *greedy decoding* by calling [`~generation_flax_utils.FlaxGenerationMixin._greedy_search`] if
              `num_beams=1` and `do_sample=False`.
            - *multinomial sampling* by calling [`~generation_flax_utils.FlaxGenerationMixin._sample`] if `num_beams=1`
              and `do_sample=True`.
            - *beam-search decoding* by calling [`~generation_utils.FlaxGenerationMixin._beam_search`] if `num_beams>1`
              and `do_sample=False`.

        <Tip warning={true}>

        Apart from `inputs`, all the arguments below will default to the value of the attribute of the same name as
        defined in the model's config (`config.json`) which in turn defaults to the
        [`~modeling_utils.PretrainedConfig`] of the model.

        </Tip>

        Most of these parameters are explained in more detail in [this blog
        post](https://huggingface.co/blog/how-to-generate).

        Parameters:
            input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):
                The sequence used as a prompt for the generation.
            max_length (`int`, *optional*, defaults to `model.config.max_length`):
                The maximum length the generated tokens can have. Corresponds to the length of the input prompt +
                `max_new_tokens`. In general, prefer the use of `max_new_tokens`, which ignores the number of tokens in
                the prompt.
            max_new_tokens (`int`, *optional*):
                The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.
            do_sample (`bool`, *optional*, defaults to `False`):
                Whether or not to use sampling ; use greedy decoding otherwise.
            temperature (`float`, *optional*, defaults to 1.0):
                The value used to module the next token probabilities.
            top_k (`int`, *optional*, defaults to 50):
                The number of highest probability vocabulary tokens to keep for top-k-filtering.
            top_p (`float`, *optional*, defaults to 1.0):
                If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher
                are kept for generation.
            pad_token_id (`int`, *optional*):
                The id of the *padding* token.
            bos_token_id (`int`, *optional*):
                The id of the *beginning-of-sequence* token.
            eos_token_id (`int`, *optional*):
                The id of the *end-of-sequence* token.
            num_beams (`int`, *optional*, defaults to 1):
                Number of beams for beam search. 1 means no beam search.
            decoder_start_token_id (`int`, *optional*):
                If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.
            trace (`bool`, *optional*, defaults to `True`):
                Whether to trace generation. Setting `trace=False` should only be used for debugging and will lead to a
                considerably slower runtime.
            params (`Dict[str, jnp.ndarray]`, *optional*):
                Optionally the model parameters can be passed. Can be useful for parallelized generation.
            model_kwargs:
                Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model
                is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
                should be prefixed with *decoder_*. Also accepts `encoder_outputs` to skip encoder part.

        Return:
            [`~utils.ModelOutput`].

        Examples:

        ```python
        >>> from transformers import AutoTokenizer, FlaxAutoModelForCausalLM

        >>> tokenizer = AutoTokenizer.from_pretrained(""distilgpt2"")
        >>> model = FlaxAutoModelForCausalLM.from_pretrained(""distilgpt2"")
        >>> input_context = ""The dog""
        >>> # encode input context
        >>> input_ids = tokenizer(input_context, return_tensors=""np"").input_ids
        >>> # generate candidates using sampling
        >>> outputs = model.generate(input_ids=input_ids, max_length=20, top_k=30, do_sample=True)
        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
        ```""""""
        # Validate the `.generate()` call
        self._validate_model_class()
        self._validate_model_kwargs(model_kwargs.copy())

        # set init values
        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id
        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id
        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id
        decoder_start_token_id = (
            decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id
        )
        prng_key = prng_key if prng_key is not None else jax.random.PRNGKey(0)

        if decoder_start_token_id is None and self.config.is_encoder_decoder:
            raise ValueError(""`decoder_start_token_id` has to be defined for encoder-decoder generation."")

        # decoder-only models should use left-padding for generation (can't be checked with `trace=True`)
        if not self.config.is_encoder_decoder and not trace:
            if pad_token_id is not None and jnp.sum(input_ids[:, -1] == pad_token_id) > 0:
                logger.warning(
                    ""A decoder-only architecture is being used, but right-padding was detected! For correct ""
                    ""generation results, please set `padding_side='left'` when initializing the tokenizer.""
                )

        if self.config.is_encoder_decoder:
            # add encoder_outputs to model_kwargs
            if model_kwargs.get(""encoder_outputs"") is None:
                model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(input_ids, params, model_kwargs)
            # prepare decoder_input_ids for generation
            input_ids = jnp.ones((input_ids.shape[0], 1), dtype=""i4"") * decoder_start_token_id

        # Prepare `max_length` depending on other stopping criteria.
        input_ids_seq_length = input_ids.shape[-1]
        if max_length is None and max_new_tokens is None:
            warnings.warn(
                ""Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to ""
                f""{self.config.max_length} (`self.config.max_length`). Controlling `max_length` via the config is ""
                ""deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend ""
                ""using `max_new_tokens` to control the maximum length of the generation."",
                UserWarning,
            )
        elif max_length is None and max_new_tokens is not None:
            max_length = max_new_tokens + input_ids_seq_length
        elif max_length is not None and max_new_tokens is not None:
            raise ValueError(
                ""Both `max_new_tokens` and `max_length` have been set but they serve the same purpose -- setting a""
                "" limit to the generated output length. Remove one of those arguments. Please refer to the""
                "" documentation for more information. ""
                ""(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)""
            )
        # default to config if still None
        max_length = max_length if max_length is not None else self.config.max_length
        min_length = min_length if min_length is not None else self.config.min_length

        if min_length is not None and min_length > max_length:
            raise ValueError(
                f""Unfeasable length constraints: the minimum length ({min_length}) is larger than the maximum ""
                f""length ({max_length})""
            )
        if input_ids_seq_length >= max_length:
            input_ids_string = ""decoder_input_ids"" if self.config.is_encoder_decoder else ""input_ids""
            logger.warning(
                f""Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to""
                f"" {max_length}. This can lead to unexpected behavior. You should consider increasing""
                ""`max_new_tokens`.""
            )

        do_sample = do_sample if do_sample is not None else self.config.do_sample
        num_beams = num_beams if num_beams is not None else self.config.num_beams

        if not do_sample and num_beams == 1:
            logits_processor = self._get_logits_processor(
                no_repeat_ngram_size, min_length, max_length, eos_token_id, forced_bos_token_id, forced_eos_token_id
            )
            return self._greedy_search(
                input_ids,
                max_length,
                pad_token_id,
                eos_token_id,
                logits_processor=logits_processor,
                trace=trace,
                params=params,
                model_kwargs=model_kwargs,
            )
        elif do_sample and num_beams == 1:
            logits_warper = self._get_logits_warper(top_k=top_k, top_p=top_p, temperature=temperature)
            logits_processor = self._get_logits_processor(
                no_repeat_ngram_size, min_length, max_length, eos_token_id, forced_bos_token_id, forced_eos_token_id
            )
            return self._sample(
                input_ids,
                max_length,
                pad_token_id,
                eos_token_id,
                prng_key,
                logits_warper=logits_warper,
                logits_processor=logits_processor,
                trace=trace,
                params=params,
                model_kwargs=model_kwargs,
            )
        elif not do_sample and num_beams > 1:
            # broadcast input_ids & encoder_outputs
            input_ids = self._expand_to_num_beams(input_ids, num_beams=num_beams)

            if ""encoder_outputs"" in model_kwargs:
                model_kwargs[""encoder_outputs""][""last_hidden_state""] = self._expand_to_num_beams(
                    model_kwargs[""encoder_outputs""][""last_hidden_state""], num_beams=num_beams
                )

            if ""attention_mask"" in model_kwargs:
                model_kwargs[""attention_mask""] = self._expand_to_num_beams(
                    model_kwargs[""attention_mask""], num_beams=num_beams
                )

            logits_processor = self._get_logits_processor(
                no_repeat_ngram_size, min_length, max_length, eos_token_id, forced_bos_token_id, forced_eos_token_id
            )

            return self._beam_search(
                input_ids,
                max_length,
                pad_token_id,
                eos_token_id,
                length_penalty=length_penalty,
                early_stopping=early_stopping,
                logits_processor=logits_processor,
                trace=trace,
                params=params,
                model_kwargs=model_kwargs,
            )
        else:
            raise NotImplementedError(""`Beam sampling is currently not implemented."")",min_length is not None and min_length > max_length,None is not min_length > max_length
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/generation_flax_utils.py,FlaxGenerationMixin,generate$211,"def generate(
        self,
        input_ids: jnp.ndarray,
        max_length: Optional[int] = None,
        max_new_tokens: Optional[int] = None,
        pad_token_id: Optional[int] = None,
        bos_token_id: Optional[int] = None,
        eos_token_id: Optional[int] = None,
        decoder_start_token_id: Optional[int] = None,
        do_sample: Optional[bool] = None,
        prng_key: Optional[jnp.ndarray] = None,
        top_k: Optional[int] = None,
        top_p: Optional[float] = None,
        temperature: Optional[float] = None,
        num_beams: Optional[int] = None,
        no_repeat_ngram_size: Optional[int] = None,
        min_length: Optional[int] = None,
        forced_bos_token_id: Optional[int] = None,
        forced_eos_token_id: Optional[int] = None,
        length_penalty: Optional[float] = None,
        early_stopping: Optional[bool] = None,
        trace: bool = True,
        params: Optional[Dict[str, jnp.ndarray]] = None,
        **model_kwargs,
    ):
        r""""""
        Generates sequences of token ids for models with a language modeling head. The method supports the following
        generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:

            - *greedy decoding* by calling [`~generation_flax_utils.FlaxGenerationMixin._greedy_search`] if
              `num_beams=1` and `do_sample=False`.
            - *multinomial sampling* by calling [`~generation_flax_utils.FlaxGenerationMixin._sample`] if `num_beams=1`
              and `do_sample=True`.
            - *beam-search decoding* by calling [`~generation_utils.FlaxGenerationMixin._beam_search`] if `num_beams>1`
              and `do_sample=False`.

        <Tip warning={true}>

        Apart from `inputs`, all the arguments below will default to the value of the attribute of the same name as
        defined in the model's config (`config.json`) which in turn defaults to the
        [`~modeling_utils.PretrainedConfig`] of the model.

        </Tip>

        Most of these parameters are explained in more detail in [this blog
        post](https://huggingface.co/blog/how-to-generate).

        Parameters:
            input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):
                The sequence used as a prompt for the generation.
            max_length (`int`, *optional*, defaults to `model.config.max_length`):
                The maximum length the generated tokens can have. Corresponds to the length of the input prompt +
                `max_new_tokens`. In general, prefer the use of `max_new_tokens`, which ignores the number of tokens in
                the prompt.
            max_new_tokens (`int`, *optional*):
                The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.
            do_sample (`bool`, *optional*, defaults to `False`):
                Whether or not to use sampling ; use greedy decoding otherwise.
            temperature (`float`, *optional*, defaults to 1.0):
                The value used to module the next token probabilities.
            top_k (`int`, *optional*, defaults to 50):
                The number of highest probability vocabulary tokens to keep for top-k-filtering.
            top_p (`float`, *optional*, defaults to 1.0):
                If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher
                are kept for generation.
            pad_token_id (`int`, *optional*):
                The id of the *padding* token.
            bos_token_id (`int`, *optional*):
                The id of the *beginning-of-sequence* token.
            eos_token_id (`int`, *optional*):
                The id of the *end-of-sequence* token.
            num_beams (`int`, *optional*, defaults to 1):
                Number of beams for beam search. 1 means no beam search.
            decoder_start_token_id (`int`, *optional*):
                If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.
            trace (`bool`, *optional*, defaults to `True`):
                Whether to trace generation. Setting `trace=False` should only be used for debugging and will lead to a
                considerably slower runtime.
            params (`Dict[str, jnp.ndarray]`, *optional*):
                Optionally the model parameters can be passed. Can be useful for parallelized generation.
            model_kwargs:
                Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model
                is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
                should be prefixed with *decoder_*. Also accepts `encoder_outputs` to skip encoder part.

        Return:
            [`~utils.ModelOutput`].

        Examples:

        ```python
        >>> from transformers import AutoTokenizer, FlaxAutoModelForCausalLM

        >>> tokenizer = AutoTokenizer.from_pretrained(""distilgpt2"")
        >>> model = FlaxAutoModelForCausalLM.from_pretrained(""distilgpt2"")
        >>> input_context = ""The dog""
        >>> # encode input context
        >>> input_ids = tokenizer(input_context, return_tensors=""np"").input_ids
        >>> # generate candidates using sampling
        >>> outputs = model.generate(input_ids=input_ids, max_length=20, top_k=30, do_sample=True)
        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
        ```""""""
        # Validate the `.generate()` call
        self._validate_model_class()
        self._validate_model_kwargs(model_kwargs.copy())

        # set init values
        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id
        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id
        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id
        decoder_start_token_id = (
            decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id
        )
        prng_key = prng_key if prng_key is not None else jax.random.PRNGKey(0)

        if decoder_start_token_id is None and self.config.is_encoder_decoder:
            raise ValueError(""`decoder_start_token_id` has to be defined for encoder-decoder generation."")

        # decoder-only models should use left-padding for generation (can't be checked with `trace=True`)
        if not self.config.is_encoder_decoder and not trace:
            if pad_token_id is not None and jnp.sum(input_ids[:, -1] == pad_token_id) > 0:
                logger.warning(
                    ""A decoder-only architecture is being used, but right-padding was detected! For correct ""
                    ""generation results, please set `padding_side='left'` when initializing the tokenizer.""
                )

        if self.config.is_encoder_decoder:
            # add encoder_outputs to model_kwargs
            if model_kwargs.get(""encoder_outputs"") is None:
                model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(input_ids, params, model_kwargs)
            # prepare decoder_input_ids for generation
            input_ids = jnp.ones((input_ids.shape[0], 1), dtype=""i4"") * decoder_start_token_id

        # Prepare `max_length` depending on other stopping criteria.
        input_ids_seq_length = input_ids.shape[-1]
        if max_length is None and max_new_tokens is None:
            warnings.warn(
                ""Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to ""
                f""{self.config.max_length} (`self.config.max_length`). Controlling `max_length` via the config is ""
                ""deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend ""
                ""using `max_new_tokens` to control the maximum length of the generation."",
                UserWarning,
            )
        elif max_length is None and max_new_tokens is not None:
            max_length = max_new_tokens + input_ids_seq_length
        elif max_length is not None and max_new_tokens is not None:
            raise ValueError(
                ""Both `max_new_tokens` and `max_length` have been set but they serve the same purpose -- setting a""
                "" limit to the generated output length. Remove one of those arguments. Please refer to the""
                "" documentation for more information. ""
                ""(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)""
            )
        # default to config if still None
        max_length = max_length if max_length is not None else self.config.max_length
        min_length = min_length if min_length is not None else self.config.min_length

        if min_length is not None and min_length > max_length:
            raise ValueError(
                f""Unfeasable length constraints: the minimum length ({min_length}) is larger than the maximum ""
                f""length ({max_length})""
            )
        if input_ids_seq_length >= max_length:
            input_ids_string = ""decoder_input_ids"" if self.config.is_encoder_decoder else ""input_ids""
            logger.warning(
                f""Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to""
                f"" {max_length}. This can lead to unexpected behavior. You should consider increasing""
                ""`max_new_tokens`.""
            )

        do_sample = do_sample if do_sample is not None else self.config.do_sample
        num_beams = num_beams if num_beams is not None else self.config.num_beams

        if not do_sample and num_beams == 1:
            logits_processor = self._get_logits_processor(
                no_repeat_ngram_size, min_length, max_length, eos_token_id, forced_bos_token_id, forced_eos_token_id
            )
            return self._greedy_search(
                input_ids,
                max_length,
                pad_token_id,
                eos_token_id,
                logits_processor=logits_processor,
                trace=trace,
                params=params,
                model_kwargs=model_kwargs,
            )
        elif do_sample and num_beams == 1:
            logits_warper = self._get_logits_warper(top_k=top_k, top_p=top_p, temperature=temperature)
            logits_processor = self._get_logits_processor(
                no_repeat_ngram_size, min_length, max_length, eos_token_id, forced_bos_token_id, forced_eos_token_id
            )
            return self._sample(
                input_ids,
                max_length,
                pad_token_id,
                eos_token_id,
                prng_key,
                logits_warper=logits_warper,
                logits_processor=logits_processor,
                trace=trace,
                params=params,
                model_kwargs=model_kwargs,
            )
        elif not do_sample and num_beams > 1:
            # broadcast input_ids & encoder_outputs
            input_ids = self._expand_to_num_beams(input_ids, num_beams=num_beams)

            if ""encoder_outputs"" in model_kwargs:
                model_kwargs[""encoder_outputs""][""last_hidden_state""] = self._expand_to_num_beams(
                    model_kwargs[""encoder_outputs""][""last_hidden_state""], num_beams=num_beams
                )

            if ""attention_mask"" in model_kwargs:
                model_kwargs[""attention_mask""] = self._expand_to_num_beams(
                    model_kwargs[""attention_mask""], num_beams=num_beams
                )

            logits_processor = self._get_logits_processor(
                no_repeat_ngram_size, min_length, max_length, eos_token_id, forced_bos_token_id, forced_eos_token_id
            )

            return self._beam_search(
                input_ids,
                max_length,
                pad_token_id,
                eos_token_id,
                length_penalty=length_penalty,
                early_stopping=early_stopping,
                logits_processor=logits_processor,
                trace=trace,
                params=params,
                model_kwargs=model_kwargs,
            )
        else:
            raise NotImplementedError(""`Beam sampling is currently not implemented."")",max_length is None and max_new_tokens is not None,max_length is None is not max_new_tokens
adapter-transformers,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/generation_flax_utils.py,FlaxGenerationMixin,generate$211,"def generate(
        self,
        input_ids: jnp.ndarray,
        max_length: Optional[int] = None,
        max_new_tokens: Optional[int] = None,
        pad_token_id: Optional[int] = None,
        bos_token_id: Optional[int] = None,
        eos_token_id: Optional[int] = None,
        decoder_start_token_id: Optional[int] = None,
        do_sample: Optional[bool] = None,
        prng_key: Optional[jnp.ndarray] = None,
        top_k: Optional[int] = None,
        top_p: Optional[float] = None,
        temperature: Optional[float] = None,
        num_beams: Optional[int] = None,
        no_repeat_ngram_size: Optional[int] = None,
        min_length: Optional[int] = None,
        forced_bos_token_id: Optional[int] = None,
        forced_eos_token_id: Optional[int] = None,
        length_penalty: Optional[float] = None,
        early_stopping: Optional[bool] = None,
        trace: bool = True,
        params: Optional[Dict[str, jnp.ndarray]] = None,
        **model_kwargs,
    ):
        r""""""
        Generates sequences of token ids for models with a language modeling head. The method supports the following
        generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:

            - *greedy decoding* by calling [`~generation_flax_utils.FlaxGenerationMixin._greedy_search`] if
              `num_beams=1` and `do_sample=False`.
            - *multinomial sampling* by calling [`~generation_flax_utils.FlaxGenerationMixin._sample`] if `num_beams=1`
              and `do_sample=True`.
            - *beam-search decoding* by calling [`~generation_utils.FlaxGenerationMixin._beam_search`] if `num_beams>1`
              and `do_sample=False`.

        <Tip warning={true}>

        Apart from `inputs`, all the arguments below will default to the value of the attribute of the same name as
        defined in the model's config (`config.json`) which in turn defaults to the
        [`~modeling_utils.PretrainedConfig`] of the model.

        </Tip>

        Most of these parameters are explained in more detail in [this blog
        post](https://huggingface.co/blog/how-to-generate).

        Parameters:
            input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):
                The sequence used as a prompt for the generation.
            max_length (`int`, *optional*, defaults to `model.config.max_length`):
                The maximum length the generated tokens can have. Corresponds to the length of the input prompt +
                `max_new_tokens`. In general, prefer the use of `max_new_tokens`, which ignores the number of tokens in
                the prompt.
            max_new_tokens (`int`, *optional*):
                The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.
            do_sample (`bool`, *optional*, defaults to `False`):
                Whether or not to use sampling ; use greedy decoding otherwise.
            temperature (`float`, *optional*, defaults to 1.0):
                The value used to module the next token probabilities.
            top_k (`int`, *optional*, defaults to 50):
                The number of highest probability vocabulary tokens to keep for top-k-filtering.
            top_p (`float`, *optional*, defaults to 1.0):
                If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher
                are kept for generation.
            pad_token_id (`int`, *optional*):
                The id of the *padding* token.
            bos_token_id (`int`, *optional*):
                The id of the *beginning-of-sequence* token.
            eos_token_id (`int`, *optional*):
                The id of the *end-of-sequence* token.
            num_beams (`int`, *optional*, defaults to 1):
                Number of beams for beam search. 1 means no beam search.
            decoder_start_token_id (`int`, *optional*):
                If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.
            trace (`bool`, *optional*, defaults to `True`):
                Whether to trace generation. Setting `trace=False` should only be used for debugging and will lead to a
                considerably slower runtime.
            params (`Dict[str, jnp.ndarray]`, *optional*):
                Optionally the model parameters can be passed. Can be useful for parallelized generation.
            model_kwargs:
                Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model
                is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
                should be prefixed with *decoder_*. Also accepts `encoder_outputs` to skip encoder part.

        Return:
            [`~utils.ModelOutput`].

        Examples:

        ```python
        >>> from transformers import AutoTokenizer, FlaxAutoModelForCausalLM

        >>> tokenizer = AutoTokenizer.from_pretrained(""distilgpt2"")
        >>> model = FlaxAutoModelForCausalLM.from_pretrained(""distilgpt2"")
        >>> input_context = ""The dog""
        >>> # encode input context
        >>> input_ids = tokenizer(input_context, return_tensors=""np"").input_ids
        >>> # generate candidates using sampling
        >>> outputs = model.generate(input_ids=input_ids, max_length=20, top_k=30, do_sample=True)
        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
        ```""""""
        # Validate the `.generate()` call
        self._validate_model_class()
        self._validate_model_kwargs(model_kwargs.copy())

        # set init values
        bos_token_id = bos_token_id if bos_token_id is not None else self.config.bos_token_id
        pad_token_id = pad_token_id if pad_token_id is not None else self.config.pad_token_id
        eos_token_id = eos_token_id if eos_token_id is not None else self.config.eos_token_id
        decoder_start_token_id = (
            decoder_start_token_id if decoder_start_token_id is not None else self.config.decoder_start_token_id
        )
        prng_key = prng_key if prng_key is not None else jax.random.PRNGKey(0)

        if decoder_start_token_id is None and self.config.is_encoder_decoder:
            raise ValueError(""`decoder_start_token_id` has to be defined for encoder-decoder generation."")

        # decoder-only models should use left-padding for generation (can't be checked with `trace=True`)
        if not self.config.is_encoder_decoder and not trace:
            if pad_token_id is not None and jnp.sum(input_ids[:, -1] == pad_token_id) > 0:
                logger.warning(
                    ""A decoder-only architecture is being used, but right-padding was detected! For correct ""
                    ""generation results, please set `padding_side='left'` when initializing the tokenizer.""
                )

        if self.config.is_encoder_decoder:
            # add encoder_outputs to model_kwargs
            if model_kwargs.get(""encoder_outputs"") is None:
                model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(input_ids, params, model_kwargs)
            # prepare decoder_input_ids for generation
            input_ids = jnp.ones((input_ids.shape[0], 1), dtype=""i4"") * decoder_start_token_id

        # Prepare `max_length` depending on other stopping criteria.
        input_ids_seq_length = input_ids.shape[-1]
        if max_length is None and max_new_tokens is None:
            warnings.warn(
                ""Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to ""
                f""{self.config.max_length} (`self.config.max_length`). Controlling `max_length` via the config is ""
                ""deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend ""
                ""using `max_new_tokens` to control the maximum length of the generation."",
                UserWarning,
            )
        elif max_length is None and max_new_tokens is not None:
            max_length = max_new_tokens + input_ids_seq_length
        elif max_length is not None and max_new_tokens is not None:
            raise ValueError(
                ""Both `max_new_tokens` and `max_length` have been set but they serve the same purpose -- setting a""
                "" limit to the generated output length. Remove one of those arguments. Please refer to the""
                "" documentation for more information. ""
                ""(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)""
            )
        # default to config if still None
        max_length = max_length if max_length is not None else self.config.max_length
        min_length = min_length if min_length is not None else self.config.min_length

        if min_length is not None and min_length > max_length:
            raise ValueError(
                f""Unfeasable length constraints: the minimum length ({min_length}) is larger than the maximum ""
                f""length ({max_length})""
            )
        if input_ids_seq_length >= max_length:
            input_ids_string = ""decoder_input_ids"" if self.config.is_encoder_decoder else ""input_ids""
            logger.warning(
                f""Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to""
                f"" {max_length}. This can lead to unexpected behavior. You should consider increasing""
                ""`max_new_tokens`.""
            )

        do_sample = do_sample if do_sample is not None else self.config.do_sample
        num_beams = num_beams if num_beams is not None else self.config.num_beams

        if not do_sample and num_beams == 1:
            logits_processor = self._get_logits_processor(
                no_repeat_ngram_size, min_length, max_length, eos_token_id, forced_bos_token_id, forced_eos_token_id
            )
            return self._greedy_search(
                input_ids,
                max_length,
                pad_token_id,
                eos_token_id,
                logits_processor=logits_processor,
                trace=trace,
                params=params,
                model_kwargs=model_kwargs,
            )
        elif do_sample and num_beams == 1:
            logits_warper = self._get_logits_warper(top_k=top_k, top_p=top_p, temperature=temperature)
            logits_processor = self._get_logits_processor(
                no_repeat_ngram_size, min_length, max_length, eos_token_id, forced_bos_token_id, forced_eos_token_id
            )
            return self._sample(
                input_ids,
                max_length,
                pad_token_id,
                eos_token_id,
                prng_key,
                logits_warper=logits_warper,
                logits_processor=logits_processor,
                trace=trace,
                params=params,
                model_kwargs=model_kwargs,
            )
        elif not do_sample and num_beams > 1:
            # broadcast input_ids & encoder_outputs
            input_ids = self._expand_to_num_beams(input_ids, num_beams=num_beams)

            if ""encoder_outputs"" in model_kwargs:
                model_kwargs[""encoder_outputs""][""last_hidden_state""] = self._expand_to_num_beams(
                    model_kwargs[""encoder_outputs""][""last_hidden_state""], num_beams=num_beams
                )

            if ""attention_mask"" in model_kwargs:
                model_kwargs[""attention_mask""] = self._expand_to_num_beams(
                    model_kwargs[""attention_mask""], num_beams=num_beams
                )

            logits_processor = self._get_logits_processor(
                no_repeat_ngram_size, min_length, max_length, eos_token_id, forced_bos_token_id, forced_eos_token_id
            )

            return self._beam_search(
                input_ids,
                max_length,
                pad_token_id,
                eos_token_id,
                length_penalty=length_penalty,
                early_stopping=early_stopping,
                logits_processor=logits_processor,
                trace=trace,
                params=params,
                model_kwargs=model_kwargs,
            )
        else:
            raise NotImplementedError(""`Beam sampling is currently not implemented."")",max_length is not None and max_new_tokens is not None,max_length is not None is not max_new_tokens
pylearn2,https://github.com/lisa-lab/pylearn2/tree/master/pylearn2/scripts/papers/jia_huang_wkshp_11/extract_features.py,FeatureExtractor,__init__$157,"def __init__(self, batch_size, kmeans_path,
           save_path,  dataset_family, which_set,
           num_output_features,
           chunk_size = None, restrict = None, pool_mode = 'mean'):
        """"""
            batch_size:  the number of images to process simultaneously
                         this does not affect the final result, it is just for performance
                         larger values allow more parallel processing but require more memory
            kmeans_path: a path to a .pkl file containing a pylearn2.kmeans.KMeans instance
            save_path:   the base path to save to, should end in .npy
            dataset_family: extract_features.stl10, extract_features.cifar10, etc.
            which_set:     'train' or 'test'
            num_output_features: the number of randomly selected pooled features to extract per image
            chunk_size:   will build a design matrix of this many processed examples before serializing
                         them. if you use a chunk size of 10,000 on a dataset with 50,000 examples, and
                         a save_path of foo.npy, this will result in you obtaining foo_A.npy through
                         foo_E.npy.
                         Use a small chunk_size to avoid running out of memory.
            restrict:    a tuple of of (start,end) indices
                         restrict feature extraction to only these examples
                         the restrict option is used internally to implement the chunk_size option, so
                         you may not specify both for the same FeatureExtractor
            pool_mode:   'max' or 'mean'
        """"""

        if chunk_size is not None and restrict is not None:
            raise NotImplementedError(""Currently restrict is used internally to ""
                    ""implement chunk_size, so a client may not specify both"")

        self.batch_size = batch_size
        self.model_path = kmeans_path
        self.restrict = restrict
        self.pool_mode = pool_mode


        assert save_path is not None
        assert save_path.endswith('npy')

        assert pool_mode in [ 'mean', 'max' ]

        self.save_path = save_path
        self.which_set = which_set
        self.dataset_family = dataset_family
        self.chunk_size = chunk_size
        self.num_output_features = num_output_features",chunk_size is not None and restrict is not None,chunk_size is not None is not restrict
Blender-For-UnrealEngine-Addons,https://github.com/xavier150/Blender-For-UnrealEngine-Addons/tree/master/blender-for-unrealengine/bfu_ui.py,BFU_PT_BlenderForUnrealObject,draw$1186,"def draw(self, contex):
        scene = bpy.context.scene
        obj = bpy.context.object
        addon_prefs = GetAddonPrefs()
        layout = self.layout

        version = ""-1""
        for addon in addon_utils.modules():
            if addon.bl_info['name'] == ""Blender for UnrealEngine"":
                version = addon.bl_info.get('version', (-1, -1, -1))

        credit_box = layout.box()
        credit_box.label(text=ti('intro')+' Version: '+str(version))
        credit_box.operator(""object.bfu_open_documentation_page"", icon=""HELP"")

        row = layout.row(align=True)
        row.menu(
            'BFU_MT_ObjectGlobalPropertiesPresets',
            text='Global Properties Presets'
            )
        row.operator(
            'object.add_globalproperties_preset',
            text='',
            icon='ADD'
            )
        row.operator(
            'object.add_globalproperties_preset',
            text='',
            icon='REMOVE'
            ).remove_active = True
        layout.row().prop(scene, ""bfu_active_object_tab"", expand=True)

        if scene.bfu_active_object_tab == ""PROP"":
            bfu_ui_utils.LayoutSection(layout, ""bfu_object_properties_expanded"", ""Object Properties"")
            if scene.bfu_object_properties_expanded:

                if obj is None:
                    layout.row().label(text='No selected object.')
                else:

                    AssetType = layout.row()
                    AssetType.prop(obj, 'name', text="""", icon='OBJECT_DATA')
                    # Show asset type
                    AssetType.label(text='('+GetAssetType(obj)+')')

                    ExportType = layout.column()
                    ExportType.prop(obj, 'ExportEnum')

                    if obj.ExportEnum == ""export_recursive"":

                        folderNameProperty = layout.column()
                        folderNameProperty.prop(
                            obj,
                            'exportFolderName',
                            icon='FILE_FOLDER'
                            )

                        if obj.type == ""CAMERA"":
                            CameraProp = layout.column()
                            CameraProp.prop(obj, 'bfu_export_fbx_camera')

                        else:
                            ProxyProp = layout.column()
                            if GetExportAsProxy(obj):
                                ProxyProp.label(
                                        text=""The Armature was detected as a proxy.""
                                        )
                                proxy_child = GetExportProxyChild(obj)
                                if proxy_child:
                                    ProxyProp.label(
                                            text=""Proxy child: "" + proxy_child.name
                                            )
                                else:
                                    ProxyProp.label(text=""Proxy child not found"")

                            export_procedure_prop = layout.column()
                            export_procedure_prop.prop(obj, 'bfu_export_procedure')

                            if not GetExportAsProxy(obj):
                                AlembicProp = layout.column()
                                AlembicProp.prop(obj, 'ExportAsAlembic')
                                if obj.ExportAsAlembic:
                                    AlembicProp.label(
                                        text=""(Alembic animation are exported"" +
                                        "" with scene position.)""
                                        )
                                    AlembicProp.label(
                                        text=""(Use import script for"" +
                                        "" use the origin position.)""
                                        )
                                else:
                                    if addon_prefs.useGeneratedScripts:
                                        # Unreal python no longer support Skeletal mesh LODS import.
                                        if GetAssetType(obj) != ""SkeletalMesh"":
                                            LodProp = layout.column()
                                            LodProp.prop(obj, 'ExportAsLod')

                                    if obj.type == ""ARMATURE"":
                                        AssetType2 = layout.column()
                                        # Show asset type
                                        AssetType2.prop(obj, ""ForceStaticMesh"")
                                        if GetAssetType(obj) == ""SkeletalMesh"":
                                            AssetType2.prop(obj, 'exportDeformOnly')

                                # exportCustomName
                                exportCustomName = layout.row()
                                exportCustomName.prop(obj, ""bfu_use_custom_export_name"")
                                useCustomName = obj.bfu_use_custom_export_name
                                exportCustomNameText = exportCustomName.column()
                                exportCustomNameText.prop(obj, ""bfu_custom_export_name"")
                                exportCustomNameText.enabled = useCustomName

            bfu_ui_utils.LayoutSection(layout, ""bfu_object_lod_properties_expanded"", ""Lod"")
            if scene.bfu_object_lod_properties_expanded:
                if addon_prefs.useGeneratedScripts and obj is not None:
                    if obj.ExportEnum == ""export_recursive"":

                        # Lod selection
                        if not obj.ExportAsLod:
                            # Unreal python no longer support Skeletal mesh LODS import.
                            if (GetAssetType(obj) == ""StaticMesh""):
                                LodList = layout.column()
                                LodList.prop(obj, 'Ue4Lod1')
                                LodList.prop(obj, 'Ue4Lod2')
                                LodList.prop(obj, 'Ue4Lod3')
                                LodList.prop(obj, 'Ue4Lod4')
                                LodList.prop(obj, 'Ue4Lod5')

                        # StaticMesh prop
                        if GetAssetType(obj) == ""StaticMesh"":
                            if not obj.ExportAsLod:
                                StaticMeshLODGroup = layout.row()
                                StaticMeshLODGroup.prop(
                                    obj,
                                    'UseStaticMeshLODGroup',
                                    text="""")
                                SMLODGroupChild = StaticMeshLODGroup.column()
                                SMLODGroupChild.enabled = obj.UseStaticMeshLODGroup
                                SMLODGroupChild.prop(
                                    obj,
                                    'StaticMeshLODGroup'
                                    )

            bfu_ui_utils.LayoutSection(layout, ""bfu_object_collision_properties_expanded"", ""Collision"")
            if scene.bfu_object_collision_properties_expanded:
                if addon_prefs.useGeneratedScripts and obj is not None:
                    if obj.ExportEnum == ""export_recursive"":

                        # StaticMesh prop
                        if GetAssetType(obj) == ""StaticMesh"":
                            StaticMeshCollisionTraceFlag = layout.row()
                            StaticMeshCollisionTraceFlag.prop(
                                obj,
                                'CollisionTraceFlag'
                                )
                            if not obj.ExportAsLod:
                                AutoGenerateCollision = layout.row()
                                AutoGenerateCollision.prop(
                                    obj,
                                    'AutoGenerateCollision'
                                    )
                        # SkeletalMesh prop
                        if GetAssetType(obj) == ""SkeletalMesh"":
                            if not obj.ExportAsLod:
                                CreatePhysicsAsset = layout.row()
                                CreatePhysicsAsset.prop(obj, ""CreatePhysicsAsset"")

            bfu_ui_utils.LayoutSection(layout, ""bfu_object_material_properties_expanded"", ""Material"")
            if scene.bfu_object_material_properties_expanded:
                if addon_prefs.useGeneratedScripts and obj is not None:
                    if obj.ExportEnum == ""export_recursive"":

                        # MaterialSearchLocation
                        if not obj.ExportAsLod:
                            if (GetAssetType(obj) == ""StaticMesh"" or
                                    GetAssetType(obj) == ""SkeletalMesh"" or
                                    GetAssetType(obj) == ""Alembic""):
                                MaterialSearchLocation = layout.row()
                                MaterialSearchLocation.prop(
                                    obj, 'MaterialSearchLocation')

            bfu_ui_utils.LayoutSection(layout, ""bfu_object_vertex_color_properties_expanded"", ""Vertex color"")
            if scene.bfu_object_vertex_color_properties_expanded:
                if addon_prefs.useGeneratedScripts and obj is not None:
                    if obj.ExportEnum == ""export_recursive"":

                        # Vertex color
                        StaticMeshVertexColorImportOption = layout.column()
                        StaticMeshVertexColorImportOption.prop(obj, 'VertexColorImportOption')
                        if obj.VertexColorImportOption == ""OVERRIDE"":
                            StaticMeshVertexColorImportOptionColor = StaticMeshVertexColorImportOption.row()
                            StaticMeshVertexColorImportOptionColor.prop(obj, 'VertexOverrideColor')
                        if obj.VertexColorImportOption == ""REPLACE"":
                            StaticMeshVertexColorImportOptionIndex = StaticMeshVertexColorImportOption.row()
                            StaticMeshVertexColorImportOptionIndex.prop(obj, 'VertexColorToUse')
                            if obj.VertexColorToUse == ""CustomIndex"":
                                StaticMeshVertexColorImportOptionIndexCustom = StaticMeshVertexColorImportOption.row()
                                StaticMeshVertexColorImportOptionIndexCustom.prop(obj, 'VertexColorIndexToUse')

                            StaticMeshVertexColorFeedback = StaticMeshVertexColorImportOption.row()
                            if obj.type == ""MESH"":
                                vced = VertexColorExportData(obj)
                                if vced.export_type == ""REPLACE"":
                                    my_text = 'Vertex color nammed ""' + vced.name + '"" will be used.'
                                    StaticMeshVertexColorFeedback.label(text=my_text, icon='INFO')
                                else:
                                    my_text = 'No vertex color found at this index.'
                                    StaticMeshVertexColorFeedback.label(text=my_text, icon='ERROR')
                            else:
                                my_text = 'Vertex color property will be apply on the childrens.'
                                StaticMeshVertexColorFeedback.label(text=my_text, icon='INFO')

            bfu_ui_utils.LayoutSection(layout, ""bfu_object_light_map_properties_expanded"", ""Light map"")
            if scene.bfu_object_light_map_properties_expanded:
                if addon_prefs.useGeneratedScripts and obj is not None:
                    if obj.ExportEnum == ""export_recursive"":

                        # UV
                        if GetAssetType(obj) == ""StaticMesh"":
                            StaticMeshLightMapRes = layout.box()
                            StaticMeshLightMapRes.prop(obj, 'StaticMeshLightMapEnum')
                            if obj.StaticMeshLightMapEnum == ""CustomMap"":
                                CustomLightMap = StaticMeshLightMapRes.column()
                                CustomLightMap.prop(obj, 'customStaticMeshLightMapRes')
                            if obj.StaticMeshLightMapEnum == ""SurfaceArea"":
                                SurfaceAreaLightMap = StaticMeshLightMapRes.column()
                                SurfaceAreaLightMapButton = SurfaceAreaLightMap.row()
                                SurfaceAreaLightMapButton.operator(""object.computlightmap"", icon='TEXTURE')
                                SurfaceAreaLightMapButton.operator(""object.computalllightmap"", icon='TEXTURE')
                                SurfaceAreaLightMap.prop(obj, 'useStaticMeshLightMapWorldScale')
                                SurfaceAreaLightMap.prop(obj, 'staticMeshLightMapSurfaceScale')
                                SurfaceAreaLightMap.prop(obj, 'staticMeshLightMapRoundPowerOfTwo')
                            if obj.StaticMeshLightMapEnum != ""Default"":
                                CompuntedLightMap = str(GetCompuntedLightMap(obj))
                                StaticMeshLightMapRes.label(text='Compunted light map: ' + CompuntedLightMap)
                            GenerateLightmapUVs = layout.row()
                            GenerateLightmapUVs.prop(obj, 'GenerateLightmapUVs')

            bfu_ui_utils.LayoutSection(layout, ""bfu_object_advanced_properties_expanded"", ""Object advanced Properties"")
            if scene.bfu_object_advanced_properties_expanded:
                if obj is not None:
                    if obj.ExportEnum == ""export_recursive"":

                        transformProp = layout.column()
                        if GetAssetType(obj) != ""Alembic"" and GetAssetType(obj) != ""Camera"":
                            transformProp.prop(obj, ""MoveToCenterForExport"")
                            transformProp.prop(obj, ""RotateToZeroForExport"")
                            transformProp.prop(obj, ""AdditionalLocationForExport"")
                            transformProp.prop(obj, ""AdditionalRotationForExport"")
                            transformProp.prop(obj, 'exportGlobalScale')
                        elif GetAssetType(obj) == ""Camera"":
                            transformProp.prop(obj, ""AdditionalLocationForExport"")

                        AxisProperty = layout.column()
                        AxisProperty.prop(obj, 'exportAxisForward')
                        AxisProperty.prop(obj, 'exportAxisUp')
                        if GetAssetType(obj) == ""SkeletalMesh"":
                            BoneAxisProperty = layout.column()
                            BoneAxisProperty.prop(obj, 'exportPrimaryBaneAxis')
                            BoneAxisProperty.prop(obj, 'exporSecondaryBoneAxis')
                else:
                    layout.label(text='(No properties to show.)')

        if scene.bfu_active_object_tab == ""ANIM"":
            bfu_ui_utils.LayoutSection(layout, ""bfu_anim_properties_expanded"", ""Anim Properties"")
            if scene.bfu_anim_properties_expanded:
                if obj is not None:
                    if obj.ExportEnum == ""export_recursive"" and not obj.ExportAsLod:
                        if (GetAssetType(obj) == ""SkeletalMesh"" or
                                GetAssetType(obj) == ""Camera"" or
                                GetAssetType(obj) == ""Alembic""):

                            # Action time
                            if obj.type != ""CAMERA"" and obj.bfu_export_procedure != ""auto-rig-pro"":
                                ActionTimeProperty = layout.column()
                                ActionTimeProperty.prop(obj, 'AnimStartEndTimeEnum')
                                if obj.AnimStartEndTimeEnum == ""with_customframes"":
                                    OfsetTime = ActionTimeProperty.row()
                                    OfsetTime.prop(obj, 'AnimCustomStartTime')
                                    OfsetTime.prop(obj, 'AnimCustomEndTime')
                                if obj.AnimStartEndTimeEnum != ""with_customframes"":
                                    OfsetTime = ActionTimeProperty.row()
                                    OfsetTime.prop(obj, 'StartFramesOffset')
                                    OfsetTime.prop(obj, 'EndFramesOffset')

                            else:
                                layout.label(
                                    text=(
                                        ""Note: animation start/end use scene frames"" +
                                        "" with the camera for the sequencer."")
                                    )

                            if GetAssetType(obj) == ""SkeletalMesh"":
                                # Action list
                                ActionListProperty = layout.column()
                                ActionListProperty.prop(obj, 'exportActionEnum')
                                if obj.exportActionEnum == ""export_specific_list"":
                                    ActionListProperty.template_list(
                                        # type and unique id
                                        ""BFU_UL_ActionExportTarget"", """",
                                        # pointer to the CollectionProperty
                                        obj, ""exportActionList"",
                                        # pointer to the active identifier
                                        obj, ""active_ObjectAction"",
                                        maxrows=5,
                                        rows=5
                                    )
                                    ActionListProperty.operator(
                                        ""object.updateobjactionlist"",
                                        icon='RECOVER_LAST')
                                if obj.exportActionEnum == ""export_specific_prefix"":
                                    ActionListProperty.prop(obj, 'PrefixNameToExport')

                            # NLA
                            if GetAssetType(obj) == ""SkeletalMesh"":
                                NLAAnim = layout.row()
                                NLAAnim.prop(obj, 'ExportNLA')
                                NLAAnimChild = NLAAnim.column()
                                NLAAnimChild.enabled = obj.ExportNLA
                                NLAAnimChild.prop(obj, 'NLAAnimName')
                                if obj.bfu_export_procedure == ""auto-rig-pro"":
                                    NLAAnim.enabled = False
                                    NLAAnimChild.enabled = False

                            # Animation fbx properties
                            if (GetAssetType(obj) != ""Alembic""):
                                propsFbx = layout.row()
                                if obj.bfu_export_procedure != ""auto-rig-pro"":
                                    propsFbx.prop(obj, 'SampleAnimForExport')
                                propsFbx.prop(obj, 'SimplifyAnimForExport')

                            # Nomenclature
                            if GetAssetType(obj) == ""SkeletalMesh"":
                                export_anim_naming = layout.column()
                                export_anim_naming.prop(obj, 'bfu_anim_naming_type')
                                if obj.bfu_anim_naming_type == ""include_custom_name"":
                                    export_anim_naming_text = export_anim_naming.column()
                                    export_anim_naming_text.prop(obj, 'bfu_anim_naming_custom')

                            # Armature export action list feedback
                            if GetAssetType(obj) == ""SkeletalMesh"":
                                layout.label(
                                    text='Note: The Action with only one' +
                                    ' frame are exported like Pose.')
                                ArmaturePropertyInfo = (
                                    layout.row().box().split(factor=0.75)
                                    )
                                ActionNum = len(GetActionToExport(obj))
                                if obj.ExportNLA:
                                    ActionNum += 1
                                actionFeedback = (
                                    str(ActionNum) +
                                    "" Animation(s) will be exported with this object."")
                                ArmaturePropertyInfo.label(
                                    text=actionFeedback,
                                    icon='INFO')
                                ArmaturePropertyInfo.operator(""object.showobjaction"")

                        else:
                            layout.label(
                                text='(This assets is not a SkeletalMesh or Camera)')
                    else:
                        layout.label(text='(No properties to show.)')
                else:
                    layout.label(text='(No properties to show.)')

            bfu_ui_utils.LayoutSection(layout, ""bfu_anim_advanced_properties_expanded"", ""Animation advanced Properties"")
            if scene.bfu_anim_advanced_properties_expanded:
                if obj is not None:
                    if obj.ExportEnum == ""export_recursive"":

                        if GetAssetType(obj) != ""Alembic"":
                            transformProp = layout.column()
                            transformProp.prop(obj, ""MoveActionToCenterForExport"")
                            transformProp.prop(obj, ""RotateActionToZeroForExport"")

                            transformProp2 = layout.column()
                            transformProp2.prop(obj, ""MoveNLAToCenterForExport"")
                            transformProp2.prop(obj, ""RotateNLAToZeroForExport"")
                else:
                    layout.label(text='(No properties to show.)')

            bfu_ui_utils.LayoutSection(layout, ""bfu_skeleton_properties_expanded"", ""Skeleton"")
            if scene.bfu_skeleton_properties_expanded:
                if addon_prefs.useGeneratedScripts and obj is not None:
                    if obj.ExportEnum == ""export_recursive"":

                        # SkeletalMesh prop
                        if GetAssetType(obj) == ""SkeletalMesh"":
                            if not obj.ExportAsLod:

                                Ue4Skeleton = layout.column()
                                Ue4Skeleton.prop(obj, ""bfu_skeleton_search_mode"")
                                if obj.bfu_skeleton_search_mode == ""auto"":
                                    pass
                                if obj.bfu_skeleton_search_mode == ""custom_name"":
                                    Ue4Skeleton.prop(obj, ""bfu_target_skeleton_custom_name"")
                                if obj.bfu_skeleton_search_mode == ""custom_path_name"":
                                    Ue4Skeleton.prop(obj, ""bfu_target_skeleton_custom_path"")
                                    Ue4Skeleton.prop(obj, ""bfu_target_skeleton_custom_name"")
                                if obj.bfu_skeleton_search_mode == ""custom_reference"":
                                    Ue4Skeleton.prop(obj, ""bfu_target_skeleton_custom_ref"")

        if scene.bfu_active_object_tab == ""SCENE"":

            bfu_ui_utils.LayoutSection(layout, ""bfu_collection_properties_expanded"", ""Collection Properties"")
            if scene.bfu_collection_properties_expanded:
                collectionListProperty = layout.column()
                collectionListProperty.template_list(
                    # type and unique id
                    ""BFU_UL_CollectionExportTarget"", """",
                    # pointer to the CollectionProperty
                    scene, ""CollectionExportList"",
                    # pointer to the active identifier
                    scene, ""active_CollectionExportList"",
                    maxrows=5,
                    rows=5
                )
                collectionListProperty.operator(
                    ""object.updatecollectionlist"",
                    icon='RECOVER_LAST')

                if scene.active_CollectionExportList < len(scene.CollectionExportList):
                    col_name = scene.CollectionExportList[scene.active_CollectionExportList].name
                    if col_name in bpy.data.collections:
                        col = bpy.data.collections[col_name]
                        col_prop = layout
                        col_prop.prop(col, 'exportFolderName', icon='FILE_FOLDER')

                collectionPropertyInfo = layout.row().box().split(factor=0.75)
                collectionNum = len(GetCollectionToExport(scene))
                collectionFeedback = (
                    str(collectionNum) +
                    "" Collection(s) will be exported."")
                collectionPropertyInfo.label(text=collectionFeedback, icon='INFO')
                collectionPropertyInfo.operator(""object.showscenecollection"")
                layout.label(text='Note: The collection are exported like StaticMesh.')",GetAssetType(obj) != 'Alembic' and GetAssetType(obj) != 'Camera','Alembic' != GetAssetType(obj) != 'Camera'
imgaug,https://github.com/aleju/imgaug/tree/master/imgaug/augmenters/blend.py,BlendAlphaMask,_blend_coordinates$916,"def _blend_coordinates(cls, cbaoi, cbaoi_fg, cbaoi_bg, mask_image,
                           mode):
        coords = augm_utils.convert_cbaois_to_kpsois(cbaoi)
        coords_fg = augm_utils.convert_cbaois_to_kpsois(cbaoi_fg)
        coords_bg = augm_utils.convert_cbaois_to_kpsois(cbaoi_bg)

        coords = coords.to_xy_array()
        coords_fg = coords_fg.to_xy_array()
        coords_bg = coords_bg.to_xy_array()

        assert coords.shape == coords_fg.shape == coords_bg.shape, (
            ""Expected number of coordinates to not be changed by foreground ""
            ""or background branch in BlendAlphaMask. But input coordinates ""
            ""of shape %s were changed to %s (foreground) and %s ""
            ""(background). Make sure to not use any augmenters that affect ""
            ""the existence of coordinates."" % (
                coords.shape, coords_fg.shape, coords_bg.shape))

        h_img, w_img = mask_image.shape[0:2]

        if mode == cls._MODE_POINTWISE:
            # Augment pointwise, i.e. check for each point and its
            # xy-location the average mask value and pick based on that
            # either the point from the foreground or background branch.
            assert len(coords_fg) == len(coords_bg), (
                ""Got different numbers of coordinates before/after ""
                ""augmentation in BlendAlphaMask. The number of ""
                ""coordinates is currently not allowed to change for this ""
                ""augmenter. Input contained %d coordinates, foreground ""
                ""branch %d, backround branch %d."" % (
                    len(coords), len(coords_fg), len(coords_bg)))

            coords_aug = []
            subgen = zip(coords, coords_fg, coords_bg)
            for coord, coord_fg, coord_bg in subgen:
                x_int = int(np.round(coord[0]))
                y_int = int(np.round(coord[1]))
                if 0 <= y_int < h_img and 0 <= x_int < w_img:
                    alphas_i = mask_image[y_int, x_int, ...]
                    alpha = (
                        np.average(alphas_i) if alphas_i.size > 0 else 1.0)
                    if alpha > 0.5:
                        coords_aug.append(coord_fg)
                    else:
                        coords_aug.append(coord_bg)
                else:
                    coords_aug.append((x_int, y_int))
        else:
            # Augment with an either-or approach over all points, i.e.
            # based on the average of the whole mask, either all points
            # from the foreground or all points from the background branch
            # are used.
            # Note that we ensured above that _keypoint_mode must be
            # _MODE_EITHER_OR if it wasn't _MODE_POINTWISE.
            mask_image_avg = (
                np.average(mask_image) if mask_image.size > 0 else 1.0)
            if mask_image_avg > 0.5:
                coords_aug = coords_fg
            else:
                coords_aug = coords_bg

        kpsoi_aug = ia.KeypointsOnImage.from_xy_array(
            coords_aug, shape=cbaoi.shape)
        return augm_utils.invert_convert_cbaois_to_kpsois_(cbaoi, kpsoi_aug)",0 <= y_int < h_img and 0 <= x_int < w_img,h_img > y_int >= 0 <= x_int < w_img
vision,https://github.com/pytorch/vision/tree/master/test/test_transforms.py,,test_random_rotation$1844,"def test_random_rotation():

    with pytest.raises(ValueError):
        transforms.RandomRotation(-0.7)

    with pytest.raises(ValueError):
        transforms.RandomRotation([-0.7])

    with pytest.raises(ValueError):
        transforms.RandomRotation([-0.7, 0, 0.7])

    t = transforms.RandomRotation(0, fill=None)
    assert t.fill == 0

    t = transforms.RandomRotation(10)
    angle = t.get_params(t.degrees)
    assert angle > -10 and angle < 10

    t = transforms.RandomRotation((-10, 10))
    angle = t.get_params(t.degrees)
    assert -10 < angle < 10

    # Checking if RandomRotation can be printed as string
    t.__repr__()

    # assert changed type warning
    with pytest.warns(
        UserWarning,
        match=re.escape(
            ""Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. ""
            ""Please use InterpolationMode enum.""
        ),
    ):
        t = transforms.RandomRotation((-10, 10), interpolation=2)
        assert t.interpolation == transforms.InterpolationMode.BILINEAR",angle > -10 and angle < 10,#NAME?
edx-platform,https://github.com/edx/edx-platform/tree/master/lms/djangoapps/grades/tests/test_course_data.py,CourseDataTest,test_fill_course_data$43,"def test_fill_course_data(self, mock_get_blocks):
        """"""
        Tests to ensure that course data is fully filled with just a single input.
        """"""
        mock_get_blocks.return_value = self.one_true_structure
        for kwarg in self.expected_results:  # We iterate instead of ddt due to dependence on 'self'
            if kwarg == 'location':
                continue  # This property is purely output; it's never able to be used as input
            kwargs = {kwarg: self.expected_results[kwarg]}
            course_data = CourseData(self.user, **kwargs)
            for arg in self.expected_results:
                # No point validating the data we used as input, and c_b_s is input-only
                if arg != kwarg and arg != ""collected_block_structure"":  # lint-amnesty, pylint: disable=consider-using-in
                    expected = self.expected_results[arg]
                    actual = getattr(course_data, arg)
                    if arg == 'course':
                        assert expected.location == actual.location
                    else:
                        assert expected == actual",arg != kwarg and arg != 'collected_block_structure',kwarg != arg != 'collected_block_structure'
opencensus-python,https://github.com/census-instrumentation/opencensus-python/tree/master/contrib/opencensus-ext-jaeger/opencensus/ext/jaeger/trace_exporter/gen/jaeger/jaeger.py,submitBatches_result,write$1330,"def write(self, oprot):
        if oprot._fast_encode is not None and self.thrift_spec is not None:
            oprot.trans.write(
                oprot._fast_encode(self, (self.__class__, self.thrift_spec)))
            return
        oprot.writeStructBegin('submitBatches_result')
        if self.success is not None:
            oprot.writeFieldBegin('success', TType.LIST, 0)
            oprot.writeListBegin(TType.STRUCT, len(self.success))
            for iter55 in self.success:
                iter55.write(oprot)
            oprot.writeListEnd()
            oprot.writeFieldEnd()
        oprot.writeFieldStop()
        oprot.writeStructEnd()",oprot._fast_encode is not None and self.thrift_spec is not None,oprot._fast_encode is not None is not self.thrift_spec
transformers,https://github.com/huggingface/transformers/tree/master/src/transformers/tokenization_utils.py,PreTrainedTokenizer,_add_tokens$384,"def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_tokens: bool = False) -> int:
        """"""
        Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to
        it with indices starting from length of the current vocabulary.

        Args:
            new_tokens (`List[str]`or `List[tokenizers.AddedToken]`):
                Token(s) to add in vocabulary. A token is only added if it's not already in the vocabulary (tested by
                checking if the tokenizer assign the index of the `unk_token` to them).
            special_tokens (`bool`, *optional*, defaults to `False`):
                Whether or not the tokens should be added as special tokens.

        Returns:
            `int`: The number of tokens actually added to the vocabulary.

        Examples:

        ```python
        # Let's see how to increase the vocabulary of Bert model and tokenizer
        tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"")
        model = BertModel.from_pretrained(""bert-base-uncased"")

        num_added_toks = tokenizer.add_tokens([""new_tok1"", ""my_new-tok2""])
        print(""We have added"", num_added_toks, ""tokens"")
        # Note: resize_token_embeddings expects to receive the full size of the new vocabulary, i.e. the length of the tokenizer.
        model.resize_token_embeddings(len(tokenizer))
        ```""""""
        new_tokens = [str(tok) for tok in new_tokens]

        tokens_to_add = []
        for token in new_tokens:
            if not isinstance(token, str):
                raise TypeError(f""Token {token} is not a string but a {type(token)}."")
            if not special_tokens and hasattr(self, ""do_lower_case"") and self.do_lower_case:
                token = token.lower()
            if (
                token != self.unk_token
                and self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token)
                and token not in tokens_to_add
            ):
                tokens_to_add.append(token)
                if self.verbose:
                    logger.info(f""Adding {token} to the vocabulary"")

        added_tok_encoder = dict((tok, len(self) + i) for i, tok in enumerate(tokens_to_add))
        added_tok_decoder = {v: k for k, v in added_tok_encoder.items()}
        self.added_tokens_encoder.update(added_tok_encoder)
        self.added_tokens_decoder.update(added_tok_decoder)

        # Make sure we don't split on any special tokens (even they were already in the vocab before e.g. for Albert)
        if special_tokens:
            if len(new_tokens) == 1:
                _insert_one_token_to_ordered_list(self.unique_no_split_tokens, new_tokens[0])
            else:
                self.unique_no_split_tokens = sorted(set(self.unique_no_split_tokens).union(set(new_tokens)))
        else:
            # Or on the newly added tokens
            if len(tokens_to_add) == 1:
                _insert_one_token_to_ordered_list(self.unique_no_split_tokens, tokens_to_add[0])
            else:
                self.unique_no_split_tokens = sorted(set(self.unique_no_split_tokens).union(set(tokens_to_add)))
        self._create_trie(self.unique_no_split_tokens)

        return len(tokens_to_add)",token != self.unk_token and self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token) and (token not in tokens_to_add),self.unk_token != token not in tokens_to_add and self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token)
pynput,https://github.com/moses-palmer/pynput/tree/master/lib/pynput/keyboard/_xorg.py,Controller,_resolve_borrowing$371,"def _resolve_borrowing(self, key):
        """"""Tries to resolve a key by modifying the layout temporarily.

        A borrowed *keysym* does not exist on the keyboard, but is temporarily
        added to the layout.

        :param KeyCode key: The key to resolve.
        """"""
        keysym = self._key_to_keysym(key)
        if keysym is None:
            return None

        mapping = self._display.get_keyboard_mapping(8, 255 - 8)

        def i2kc(index):
            return index + 8

        def kc2i(keycode):
            return keycode - 8

        #: Finds a keycode and index by looking at already used keycodes
        def reuse():
            for _, (keycode, _, _) in self._borrows.items():
                keycodes = mapping[kc2i(keycode)]

                # Only the first four items are addressable by X
                for index in range(4):
                    if not keycodes[index]:
                        return keycode, index

        #: Finds a keycode and index by using a new keycode
        def borrow():
            for i, keycodes in enumerate(mapping):
                if not any(keycodes):
                    return i2kc(i), 0

        #: Finds a keycode and index by reusing an old, unused one
        def overwrite():
            for keysym, (keycode, index, count) in self._borrows.items():
                if count < 1:
                    del self._borrows[keysym]
                    return keycode, index

        #: Registers a keycode for a specific key and modifier state
        def register(dm, keycode, index):
            i = kc2i(keycode)

            # Check for use of empty mapping with a character that has upper
            # and lower forms
            lower = key.char.lower()
            upper = key.char.upper()
            if lower != upper and len(lower) == 1 and len(upper) == 1 and all(
                    m == Xlib.XK.NoSymbol
                    for m in mapping[i]):
                lower = self._key_to_keysym(KeyCode.from_char(lower))
                upper = self._key_to_keysym(KeyCode.from_char(upper))
                if lower:
                    mapping[i][0] = lower
                    self._borrows[lower] = (keycode, 0, 0)
                if upper:
                    mapping[i][1] = upper
                    self._borrows[upper] = (keycode, 1, 0)
            else:
                mapping[i][index] = keysym
                self._borrows[keysym] = (keycode, index, 0)
            dm.change_keyboard_mapping(keycode, mapping[i:i + 1])

        try:
            with display_manager(self._display) as dm, self._borrow_lock as _:
                # First try an already used keycode, then try a new one, and
                # fall back on reusing one that is not currently pressed
                register(dm, *(
                    reuse() or
                    borrow() or
                    overwrite()))
            return keysym

        except TypeError:
            return None",lower != upper and len(lower) == 1 and (len(upper) == 1) and all((m == Xlib.XK.NoSymbol for m in mapping[i])),lower != upper and len(lower) == 1 == len(upper) and all((m == Xlib.XK.NoSymbol for m in mapping[i]))
PyQt,https://github.com/PyQt5/PyQt/tree/master/Demo/Lib/FramelessWindow.py,FramelessWindow,mouseMoveEvent$238,"def mouseMoveEvent(self, event):
        """"""鼠标移动事件""""""
        super(FramelessWindow, self).mouseMoveEvent(event)
        pos = event.pos()
        xPos, yPos = pos.x(), pos.y()
        wm, hm = self.width() - self.Margins, self.height() - self.Margins
        if self.isMaximized() or self.isFullScreen():
            self.Direction = None
            self.setCursor(Qt.ArrowCursor)
            return
        if event.buttons() == Qt.LeftButton and self._pressed:
            self._resizeWidget(pos)
            return
        if xPos <= self.Margins and yPos <= self.Margins:
            # 左上角
            self.Direction = LeftTop
            self.setCursor(Qt.SizeFDiagCursor)
        elif wm <= xPos <= self.width() and hm <= yPos <= self.height():
            # 右下角
            self.Direction = RightBottom
            self.setCursor(Qt.SizeFDiagCursor)
        elif wm <= xPos and yPos <= self.Margins:
            # 右上角
            self.Direction = RightTop
            self.setCursor(Qt.SizeBDiagCursor)
        elif xPos <= self.Margins and hm <= yPos:
            # 左下角
            self.Direction = LeftBottom
            self.setCursor(Qt.SizeBDiagCursor)
        elif 0 <= xPos <= self.Margins and self.Margins <= yPos <= hm:
            # 左边
            self.Direction = Left
            self.setCursor(Qt.SizeHorCursor)
        elif wm <= xPos <= self.width() and self.Margins <= yPos <= hm:
            # 右边
            self.Direction = Right
            self.setCursor(Qt.SizeHorCursor)
        elif self.Margins <= xPos <= wm and 0 <= yPos <= self.Margins:
            # 上面
            self.Direction = Top
            self.setCursor(Qt.SizeVerCursor)
        elif self.Margins <= xPos <= wm and hm <= yPos <= self.height():
            # 下面
            self.Direction = Bottom
            self.setCursor(Qt.SizeVerCursor)",xPos <= self.Margins and yPos <= self.Margins,xPos <= self.Margins >= yPos
PyQt,https://github.com/PyQt5/PyQt/tree/master/Demo/Lib/FramelessWindow.py,FramelessWindow,mouseMoveEvent$238,"def mouseMoveEvent(self, event):
        """"""鼠标移动事件""""""
        super(FramelessWindow, self).mouseMoveEvent(event)
        pos = event.pos()
        xPos, yPos = pos.x(), pos.y()
        wm, hm = self.width() - self.Margins, self.height() - self.Margins
        if self.isMaximized() or self.isFullScreen():
            self.Direction = None
            self.setCursor(Qt.ArrowCursor)
            return
        if event.buttons() == Qt.LeftButton and self._pressed:
            self._resizeWidget(pos)
            return
        if xPos <= self.Margins and yPos <= self.Margins:
            # 左上角
            self.Direction = LeftTop
            self.setCursor(Qt.SizeFDiagCursor)
        elif wm <= xPos <= self.width() and hm <= yPos <= self.height():
            # 右下角
            self.Direction = RightBottom
            self.setCursor(Qt.SizeFDiagCursor)
        elif wm <= xPos and yPos <= self.Margins:
            # 右上角
            self.Direction = RightTop
            self.setCursor(Qt.SizeBDiagCursor)
        elif xPos <= self.Margins and hm <= yPos:
            # 左下角
            self.Direction = LeftBottom
            self.setCursor(Qt.SizeBDiagCursor)
        elif 0 <= xPos <= self.Margins and self.Margins <= yPos <= hm:
            # 左边
            self.Direction = Left
            self.setCursor(Qt.SizeHorCursor)
        elif wm <= xPos <= self.width() and self.Margins <= yPos <= hm:
            # 右边
            self.Direction = Right
            self.setCursor(Qt.SizeHorCursor)
        elif self.Margins <= xPos <= wm and 0 <= yPos <= self.Margins:
            # 上面
            self.Direction = Top
            self.setCursor(Qt.SizeVerCursor)
        elif self.Margins <= xPos <= wm and hm <= yPos <= self.height():
            # 下面
            self.Direction = Bottom
            self.setCursor(Qt.SizeVerCursor)",0 <= xPos <= self.Margins and self.Margins <= yPos <= hm,0 <= xPos <= self.Margins <= yPos <= hm
PyQt,https://github.com/PyQt5/PyQt/tree/master/Demo/Lib/FramelessWindow.py,FramelessWindow,mouseMoveEvent$238,"def mouseMoveEvent(self, event):
        """"""鼠标移动事件""""""
        super(FramelessWindow, self).mouseMoveEvent(event)
        pos = event.pos()
        xPos, yPos = pos.x(), pos.y()
        wm, hm = self.width() - self.Margins, self.height() - self.Margins
        if self.isMaximized() or self.isFullScreen():
            self.Direction = None
            self.setCursor(Qt.ArrowCursor)
            return
        if event.buttons() == Qt.LeftButton and self._pressed:
            self._resizeWidget(pos)
            return
        if xPos <= self.Margins and yPos <= self.Margins:
            # 左上角
            self.Direction = LeftTop
            self.setCursor(Qt.SizeFDiagCursor)
        elif wm <= xPos <= self.width() and hm <= yPos <= self.height():
            # 右下角
            self.Direction = RightBottom
            self.setCursor(Qt.SizeFDiagCursor)
        elif wm <= xPos and yPos <= self.Margins:
            # 右上角
            self.Direction = RightTop
            self.setCursor(Qt.SizeBDiagCursor)
        elif xPos <= self.Margins and hm <= yPos:
            # 左下角
            self.Direction = LeftBottom
            self.setCursor(Qt.SizeBDiagCursor)
        elif 0 <= xPos <= self.Margins and self.Margins <= yPos <= hm:
            # 左边
            self.Direction = Left
            self.setCursor(Qt.SizeHorCursor)
        elif wm <= xPos <= self.width() and self.Margins <= yPos <= hm:
            # 右边
            self.Direction = Right
            self.setCursor(Qt.SizeHorCursor)
        elif self.Margins <= xPos <= wm and 0 <= yPos <= self.Margins:
            # 上面
            self.Direction = Top
            self.setCursor(Qt.SizeVerCursor)
        elif self.Margins <= xPos <= wm and hm <= yPos <= self.height():
            # 下面
            self.Direction = Bottom
            self.setCursor(Qt.SizeVerCursor)",self.Margins <= xPos <= wm and 0 <= yPos <= self.Margins,0 <= yPos <= self.Margins <= xPos <= wm
gpodder,https://github.com/gpodder/gpodder/tree/master/src/gpodder/gtkui/main.py,gPodder,update_downloads_list$1161,"def update_downloads_list(self, can_call_cleanup=True):
        try:
            model = self.download_status_model

            downloading, synchronizing, failed, finished, queued, paused, others = 0, 0, 0, 0, 0, 0, 0
            total_speed, total_size, done_size = 0, 0, 0

            # Keep a list of all download tasks that we've seen
            download_tasks_seen = set()

            # Do not go through the list of the model is not (yet) available
            if model is None:
                model = ()

            for row in model:
                self.download_status_model.request_update(row.iter)

                task = row[self.download_status_model.C_TASK]
                speed, size, status, progress, activity = task.speed, task.total_size, task.status, task.progress, task.activity

                # Let the download task monitors know of changes
                for monitor in self.download_task_monitors:
                    monitor.task_updated(task)

                total_size += size
                done_size += size * progress

                download_tasks_seen.add(task)

                if (status in [download.DownloadTask.DOWNLOADING,
                               download.DownloadTask.CANCELLING,
                               download.DownloadTask.PAUSING] and
                        activity == download.DownloadTask.ACTIVITY_DOWNLOAD):
                    downloading += 1
                    total_speed += speed
                elif (status == download.DownloadTask.DOWNLOADING and
                        activity == download.DownloadTask.ACTIVITY_SYNCHRONIZE):
                    synchronizing += 1
                elif status == download.DownloadTask.FAILED:
                    failed += 1
                elif status == download.DownloadTask.DONE:
                    finished += 1
                elif status == download.DownloadTask.QUEUED:
                    queued += 1
                elif status == download.DownloadTask.PAUSED:
                    paused += 1
                else:
                    others += 1

            # Remember which tasks we have seen after this run
            self.download_tasks_seen = download_tasks_seen

            text = [_('Progress')]
            if downloading + synchronizing + failed + queued + paused > 0:
                s = []
                if downloading > 0:
                    s.append(N_('%(count)d active', '%(count)d active', downloading) % {'count': downloading})
                if synchronizing > 0:
                    s.append(N_('%(count)d active', '%(count)d active', synchronizing) % {'count': synchronizing})
                if failed > 0:
                    s.append(N_('%(count)d failed', '%(count)d failed', failed) % {'count': failed})
                if queued > 0:
                    s.append(N_('%(count)d queued', '%(count)d queued', queued) % {'count': queued})
                if paused > 0:
                    s.append(N_('%(count)d paused', '%(count)d paused', paused) % {'count': paused})
                text.append(' (' + ', '.join(s) + ')')
            self.labelDownloads.set_text(''.join(text))

            title = [self.default_title]

            # Accessing task.status_changed has the side effect of re-setting
            # the changed flag, but we only do it once here so that's okay
            channel_urls = [task.podcast_url for task in
                    self.download_tasks_seen if task.status_changed]
            episode_urls = [task.url for task in self.download_tasks_seen]

            if downloading > 0:
                title.append(N_('downloading %(count)d file',
                                'downloading %(count)d files',
                                downloading) % {'count': downloading})

                if total_size > 0:
                    percentage = 100.0 * done_size / total_size
                else:
                    percentage = 0.0
                self.set_download_progress(percentage / 100)
                total_speed = util.format_filesize(total_speed)
                title[1] += ' (%d%%, %s/s)' % (percentage, total_speed)
            if synchronizing > 0:
                title.append(N_('synchronizing %(count)d file',
                                'synchronizing %(count)d files',
                                synchronizing) % {'count': synchronizing})
            if queued > 0:
                title.append(N_('%(queued)d task queued',
                                '%(queued)d tasks queued',
                                queued) % {'queued': queued})
            if (downloading + synchronizing + queued) == 0 and self.things_adding_tasks == 0:
                self.set_download_progress(1.)
                self.downloads_finished(self.download_tasks_seen)
                gpodder.user_extensions.on_all_episodes_downloaded()
                logger.info('All tasks have finished.')

                # Remove finished episodes
                if self.config.ui.gtk.download_list.remove_finished and can_call_cleanup:
                    self.cleanup_downloads()

                # Stop updating the download list here
                self.download_list_update_enabled = False

            self.gPodder.set_title(' - '.join(title))

            self.update_episode_list_icons(episode_urls)
            self.play_or_download()
            if channel_urls:
                self.update_podcast_list_model(channel_urls)

            return self.download_list_update_enabled
        except Exception as e:
            logger.error('Exception happened while updating download list.', exc_info=True)
            self.show_message(
                '%s\n\n%s' % (_('Please report this problem and restart gPodder:'), html.escape(str(e))),
                _('Unhandled exception'), important=True)
            # We return False here, so the update loop won't be called again,
            # that's why we require the restart of gPodder in the message.
            return False",downloading + synchronizing + queued == 0 and self.things_adding_tasks == 0,downloading + synchronizing + queued == 0 == self.things_adding_tasks
Grokking-the-Coding-Interview-Patterns-for-Coding-Questions,https://github.com/cl2333/Grokking-the-Coding-Interview-Patterns-for-Coding-Questions/tree/master/17. Miscellaneous/Kth Smallest Number (hard).py,,find_Kth_smallest_number$63,"def find_Kth_smallest_number(nums, k):
  # to handle duplicates, we will keep track of previous smallest number and its index
  previousSmallestNum, previousSmallestIndex = -math.inf, -1
  currentSmallestNum, currentSmallestIndex = math.inf, -1
  for i in range(k):
    for j in range(len(nums)):
      if nums[j] > previousSmallestNum and nums[j] < currentSmallestNum:
        # found the next smallest number
        currentSmallestNum = nums[j]
        currentSmallestIndex = j
      elif nums[j] == previousSmallestNum and j > previousSmallestIndex:
        # found a number which is equal to the previous smallest number; since numbers can repeat,
        # we will consider 'nums[j]' only if it has a different index than previous smallest
        currentSmallestNum = nums[j]
        currentSmallestIndex = j
        break  # break here as we have found our definitive next smallest number

    # current smallest number becomes previous smallest number for the next iteration
    previousSmallestNum = currentSmallestNum
    previousSmallestIndex = currentSmallestIndex
    currentSmallestNum = math.inf

  return previousSmallestNum",nums[j] > previousSmallestNum and nums[j] < currentSmallestNum,previousSmallestNum < nums[j] < currentSmallestNum
maml_rl,https://github.com/cbfinn/maml_rl/tree/master/rllab/algos/cma_es_lib.py,CMAOptions,__call__$4656,"def __call__(self, key, default=None, loc=None):
        """"""evaluate and return the value of option `key` on the fly, or
        returns those options whose name or description contains `key`,
        case disregarded.

        Details
        -------
        Keys that contain `filename` are not evaluated.
        For ``loc==None``, `self` is used as environment
        but this does not define ``N``.

        :See: `eval()`, `evalall()`

        """"""
        try:
            val = self[key]
        except:
            return self.match(key)

        if loc is None:
            loc = self  # TODO: this hack is not so useful: popsize could be there, but N is missing
        try:
            if isinstance(val, str):
                val = val.split('#')[0].strip()  # remove comments
                if isinstance(val, str) and \
                        key.find('filename') < 0:
                        # and key.find('mindx') < 0:
                    val = eval(val, globals(), loc)
            # invoke default
            # TODO: val in ... fails with array type, because it is applied element wise!
            # elif val in (None,(),[],{}) and default is not None:
            elif val is None and default is not None:
                val = eval(str(default), globals(), loc)
        except:
            pass  # slighly optimistic: the previous is bug-free
        return val",val is None and default is not None,val is None is not default
PGPortfolio,https://github.com/ZhengyaoJiang/PGPortfolio/tree/master/pgportfolio/tdagent/algorithms/bk.py,BK,update$54,"def update(self, data, k, l, c):
        '''
        :param w: window sze
        :param c: correlation coefficient threshold
        '''
        T, N = data.shape
        m = -1
        histdata = np.zeros((T,N))

        if T <= k+1:
            return np.ones(N) / N

        if k==0 and l==0:
            histdata = data[:T,:]
            m = T
        else:
            for i in np.arange(k+1, T):
                #print 'i is %d k is %d T is %d\n' % (i,k,T)
                data2 = data[i-k-1:i,:] - data[T-k-1:T,:]
                #print data2

                if np.sqrt(np.trace(np.dot(data2,data2.T))) <= c/l:
                    m += 1
                    histdata[m,:] = data[i,:] #minus one to avoid out of bounds issue

        if m==-1:
            return np.ones(N) / N

        b = opt_weights(histdata[:m+1,:])
        #print b
        #print 'w is %d\t T is %d\n' % (w,T)
        return b",k == 0 and l == 0,k == 0 == l
brat,https://github.com/nlplab/brat/tree/master/server/src/search.py,,search_note$1455,"def search_note(collection, document, scope=""collection"",
                concordancing=""false"", context_length=50,
                text_match=""word"", match_case=""false"",
                category=None, type=None, text=DEFAULT_EMPTY_STRING):

    directory = collection

    # Interpret JSON booleans
    concordancing = _to_bool(concordancing)
    match_case = _to_bool(match_case)

    ann_objs = __doc_or_dir_to_annotations(directory, document, scope)

    restrict_types = []
    if type is not None and type != """":
        restrict_types.append(type)

    matches = search_anns_for_note(ann_objs, text, category,
                                   restrict_types=restrict_types,
                                   text_match=text_match,
                                   match_case=match_case)

    results = format_results(matches, concordancing, context_length)
    results['collection'] = directory

    return results",type is not None and type != '',None is not type != ''
coa_tools,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/addon_updater.py,Singleton_updater,check_for_update$1008,"def check_for_update(self, now=False):
        if self._verbose: print(""Checking for update function"")

        # clear the errors if any
        self._error = None
        self._error_msg = None

        # avoid running again in, just return past result if found
        # but if force now check, then still do it
        if self._update_ready != None and now == False:
            return (self._update_ready,self._update_version,self._update_link)

        if self._current_version == None:
            raise ValueError(""current_version not yet defined"")
        if self._repo == None:
            raise ValueError(""repo not yet defined"")
        if self._user == None:
            raise ValueError(""username not yet defined"")

        self.set_updater_json()  # self._json

        if now == False and self.past_interval_timestamp()==False:
            if self._verbose:
                print(""Aborting check for updated, check interval not reached"")
            return (False, None, None)

        # check if using tags or releases
        # note that if called the first time, this will pull tags from online
        if self._fake_install == True:
            if self._verbose:
                print(""fake_install = True, setting fake version as ready"")
            self._update_ready = True
            self._update_version = ""(999,999,999)""
            self._update_link = ""http://127.0.0.1""

            return (self._update_ready, self._update_version, self._update_link)

        # primary internet call
        self.get_tags()  # sets self._tags and self._tag_latest

        self._json[""last_check""] = str(datetime.now())
        self.save_updater_json()

        # can be () or ('master') in addition to branches, and version tag
        new_version = self.version_tuple_from_text(self.tag_latest)

        if len(self._tags)==0:
            self._update_ready = False
            self._update_version = None
            self._update_link = None
            return (False, None, None)
        elif self._include_branches == False:
            link = self._tags[0][""zipball_url""]  # potentially other sources
        else:
            n = len(self._include_branch_list)
            if len(self._tags)==n:
                # effectively means no tags found on repo
                # so provide the first one as default
                link = self._tags[0][""zipball_url""]  # potentially other sources
            else:
                link = self._tags[n][""zipball_url""]  # potentially other sources

        if new_version == ():
            self._update_ready = False
            self._update_version = None
            self._update_link = None
            return (False, None, None)
        elif str(new_version).lower() in self._include_branch_list:
            # handle situation where master/whichever branch is included
            # however, this code effectively is not triggered now
            # as new_version will only be tag names, not branch names
            if self._include_branch_autocheck == False:
                # don't offer update as ready,
                # but set the link for the default
                # branch for installing
                self._update_ready = True
                self._update_version = new_version
                self._update_link = link
                self.save_updater_json()
                return (True, new_version, link)
            else:
                raise ValueError(""include_branch_autocheck: NOT YET DEVELOPED"")
                # bypass releases and look at timestamp of last update
                # from a branch compared to now, see if commit values
                # match or not.

        else:
            # situation where branches not included

            if new_version > self._current_version:

                self._update_ready = True
                self._update_version = new_version
                self._update_link = link
                self.save_updater_json()
                return (True, new_version, link)

        # elif new_version != self._current_version:
        #   self._update_ready = False
        #   self._update_version = new_version
        #   self._update_link = link
        #   self.save_updater_json()
        #   return (True, new_version, link)

        # if no update, set ready to False from None
        self._update_ready = False
        self._update_version = None
        self._update_link = None
        return (False, None, None)",now == False and self.past_interval_timestamp() == False,now == False == self.past_interval_timestamp()
numpyro,https://github.com/pyro-ppl/numpyro/tree/master/numpyro/infer/hmc_gibbs.py,HMCECS,init$610,"def init(self, rng_key, num_warmup, init_params, model_args, model_kwargs):
        model_kwargs = {} if model_kwargs is None else model_kwargs.copy()
        rng_key, key_u = random.split(rng_key)
        # We use init strategy to get around ImproperUniform which does not have
        # sample method.
        self._prototype_trace = trace(
            substitute(seed(self.model, key_u), substitute_fn=init_to_sample)
        ).get_trace(*model_args, **model_kwargs)
        self._subsample_plate_sizes = {
            name: site[""args""]
            for name, site in self._prototype_trace.items()
            if site[""type""] == ""plate""
            and (site[""args""][1] is not None)
            and site[""args""][0] > site[""args""][1]
        }  # i.e. size > subsample_size
        self._gibbs_sites = list(self._subsample_plate_sizes.keys())
        assert self._gibbs_sites, ""Cannot detect any subsample statements in the model.""
        if self._proxy is not None:
            if any(
                {
                    name
                    for name, site in self._prototype_trace.items()
                    if site[""type""] == ""sample""
                    and (not site[""is_observed""])
                    and site[""fn""].support.is_discrete
                }
            ):
                raise RuntimeError(
                    ""Currently, the proxy does not support models with ""
                    ""discrete latent sites.""
                )
            proxy_fn, gibbs_init, self._gibbs_update = self._proxy(
                self._prototype_trace,
                self._subsample_plate_sizes,
                self.model,
                model_args,
                model_kwargs.copy(),
                num_blocks=self._num_blocks,
            )
            method = perturbed_method(self._subsample_plate_sizes, proxy_fn)
            self.inner_kernel._model = estimate_likelihood(
                self.inner_kernel._model, method
            )

            z_gibbs = {
                name: site[""value""]
                for name, site in self._prototype_trace.items()
                if name in self._gibbs_sites
            }
            rng_key, rng_state = random.split(rng_key)
            gibbs_state = gibbs_init(rng_state, z_gibbs)
        else:
            self._gibbs_update = partial(
                _block_update, self._subsample_plate_sizes, self._num_blocks
            )
            gibbs_state = ()

        model_kwargs[""_gibbs_state""] = gibbs_state
        state = super().init(rng_key, num_warmup, init_params, model_args, model_kwargs)
        return HMCECSState(
            state.z, state.hmc_state, state.rng_key, gibbs_state, jnp.zeros(())
        )",site['type'] == 'plate' and site['args'][1] is not None and (site['args'][0] > site['args'][1]),site['type'] == 'plate' and None is not site['args'][1] < site['args'][0]
pytext,https://github.com/facebookresearch/pytext/tree/master/pytext/data/xlm_dictionary.py,Dictionary,index_data$175,"def index_data(path, bin_path, dico):
        """"""
        Index sentences with a dictionary.
        """"""
        if bin_path is not None and PathManager.isfile(bin_path):
            print(""Loading data from %s ..."" % bin_path)
            data = torch.load(bin_path)
            assert dico == data[""dico""]
            return data

        positions = []
        sentences = []
        unk_words = {}

        # index sentences
        f = PathManager.open(path, ""r"", encoding=""utf-8"")
        for i, line in enumerate(f):
            if i % 1000000 == 0 and i > 0:
                print(i)
            s = line.rstrip().split()
            # skip empty sentences
            if len(s) == 0:
                print(""Empty sentence in line %i."" % i)
            # index sentence words
            count_unk = 0
            indexed = []
            for w in s:
                word_id = dico.index(w, no_unk=False)
                # if we find a special word which is not an unknown word,
                # skip the sentence
                if 0 <= word_id < 4 + SPECIAL_WORDS and word_id != 3:
                    logger.warning(
                        'Found unexpected special word ""%s"" (%i)!!' % (w, word_id)
                    )
                    continue
                assert word_id >= 0
                indexed.append(word_id)
                if word_id == dico.unk_index:
                    unk_words[w] = unk_words.get(w, 0) + 1
                    count_unk += 1
            # add sentence
            positions.append([len(sentences), len(sentences) + len(indexed)])
            sentences.extend(indexed)
            sentences.append(1)  # EOS index
        f.close()

        # tensorize data
        positions = np.int64(positions)
        if len(dico) < 1 << 16:
            sentences = np.uint16(sentences)
        elif len(dico) < 1 << 31:
            sentences = np.int32(sentences)
        else:
            raise Exception(""Dictionary is too big."")
        assert sentences.min() >= 0
        data = {
            ""dico"": dico,
            ""positions"": positions,
            ""sentences"": sentences,
            ""unk_words"": unk_words,
        }
        if bin_path is not None:
            print(""Saving the data to %s ..."" % bin_path)
            torch.save(data, bin_path, pickle_protocol=4)

        return data",i % 1000000 == 0 and i > 0,i % 1000000 == 0 < i
scipy,https://github.com/scipy/scipy/tree/master/scipy/stats/_hypotests.py,TukeyHSDResult,confidence_interval$1462,"def confidence_interval(self, confidence_level=.95):
        """"""Compute the confidence interval for the specified confidence level.

        Parameters
        ----------
        confidence_level : float, optional
            Confidence level for the computed confidence interval
            of the estimated proportion. Default is .95.

        Returns
        -------
        ci : ``ConfidenceInterval`` object
            The object has attributes ``low`` and ``high`` that hold the
            lower and upper bounds of the confidence intervals for each
            comparison. The high and low values are accessible for each
            comparison at index ``(i, j)`` between groups ``i`` and ``j``.

        References
        ----------
        .. [1] NIST/SEMATECH e-Handbook of Statistical Methods, ""7.4.7.1.
               Tukey's Method.""
               https://www.itl.nist.gov/div898/handbook/prc/section4/prc471.htm,
               28 November 2020.

        Examples
        --------
        >>> from scipy.stats import tukey_hsd
        >>> group0 = [24.5, 23.5, 26.4, 27.1, 29.9]
        >>> group1 = [28.4, 34.2, 29.5, 32.2, 30.1]
        >>> group2 = [26.1, 28.3, 24.3, 26.2, 27.8]
        >>> result = tukey_hsd(group0, group1, group2)
        >>> ci = result.confidence_interval()
        >>> ci.low
        array([[-3.649159, -8.249159, -3.909159],
               [ 0.950841, -3.649159,  0.690841],
               [-3.389159, -7.989159, -3.649159]])
        >>> ci.high
        array([[ 3.649159, -0.950841,  3.389159],
               [ 8.249159,  3.649159,  7.989159],
               [ 3.909159, -0.690841,  3.649159]])
        """"""
        # check to see if the supplied confidence level matches that of the
        # previously computed CI.
        if (self._ci is not None and self._ci_cl is not None and
                confidence_level == self._ci_cl):
            return self._ci

        if not 0 < confidence_level < 1:
            raise ValueError(""Confidence level must be between 0 and 1."")
        # determine the critical value of the studentized range using the
        # appropriate confidence level, number of treatments, and degrees
        # of freedom as determined by the number of data less the number of
        # treatments. (""Confidence limits for Tukey's method"")[1]. Note that
        # in the cases of unequal sample sizes there will be a criterion for
        # each group comparison.
        params = (confidence_level, self._nobs, self._ntreatments - self._nobs)
        srd = distributions.studentized_range.ppf(*params)
        # also called maximum critical value, the Tukey criterion is the
        # studentized range critical value * the square root of mean square
        # error over the sample size.
        tukey_criterion = srd * self._stand_err
        # the confidence levels are determined by the
        # `mean_differences` +- `tukey_criterion`
        upper_conf = self.statistic + tukey_criterion
        lower_conf = self.statistic - tukey_criterion
        self._ci = ConfidenceInterval(low=lower_conf, high=upper_conf)
        self._ci_cl = confidence_level
        return self._ci",self._ci is not None and self._ci_cl is not None and (confidence_level == self._ci_cl),self._ci is not None is not self._ci_cl == confidence_level
pyinstrument,https://github.com/joerick/pyinstrument/tree/master/test/test_stack_sampler.py,,test_multiple_contexts$89,"def test_multiple_contexts():
    sampler = stack_sampler.get_stack_sampler()

    counter_1 = SampleCounter()
    counter_2 = SampleCounter()

    context_1 = contextvars.copy_context()
    context_2 = contextvars.copy_context()

    assert sys.getprofile() is None
    assert len(sampler.subscribers) == 0
    context_1.run(sampler.subscribe, counter_1.sample, 0.001, True)
    context_2.run(sampler.subscribe, counter_2.sample, 0.001, True)

    assert sys.getprofile() is not None
    assert len(sampler.subscribers) == 2

    start = time.time()
    while time.time() < start + 1 and counter_1.count == 0 and counter_2.count == 0:
        do_nothing()

    assert counter_1.count > 0
    assert counter_2.count > 0

    assert sys.getprofile() is not None

    context_1.run(sampler.unsubscribe, counter_1.sample)
    context_2.run(sampler.unsubscribe, counter_2.sample)

    assert sys.getprofile() is None

    assert len(sampler.subscribers) == 0",time.time() < start + 1 and counter_1.count == 0 and (counter_2.count == 0),time.time() < start + 1 and counter_1.count == 0 == counter_2.count
YOWO,https://github.com/wei-tim/YOWO/tree/master/datasets/image.py,,draw_dense_reg$143,"def draw_dense_reg(regmap, heatmap, center, value, radius, is_offset=False):
    diameter = 2 * radius + 1
    gaussian = gaussian2D((diameter, diameter), sigma=diameter / 6)
    value = np.array(value, dtype=np.float32).reshape(-1, 1, 1)
    dim = value.shape[0]
    reg = np.ones((dim, diameter*2+1, diameter*2+1), dtype=np.float32) * value
    if is_offset and dim == 2:
        delta = np.arange(diameter*2+1) - radius
        reg[0] = reg[0] - delta.reshape(1, -1)
        reg[1] = reg[1] - delta.reshape(-1, 1)

    x, y = int(center[0]), int(center[1])

    height, width = heatmap.shape[0:2]

    left, right = min(x, radius), min(width - x, radius + 1)
    top, bottom = min(y, radius), min(height - y, radius + 1)

    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]
    masked_regmap = regmap[:, y - top:y + bottom, x - left:x + right]
    masked_gaussian = gaussian[radius - top:radius + bottom,
                               radius - left:radius + right]
    masked_reg = reg[:, radius - top:radius + bottom,
                        radius - left:radius + right]
    if min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0: # TODO debug
        idx = (masked_gaussian >= masked_heatmap).reshape(1, maskedgaussian.shape[0], masked_gaussian.shape[1])
        masked_regmap = (1-idx) * masked_regmap + idx * masked_reg
    regmap[:, y - top:y + bottom, x - left:x + right] = masked_regmap
    return regmap",min(masked_gaussian.shape) > 0 and min(masked_heatmap.shape) > 0,min(masked_gaussian.shape) > 0 < min(masked_heatmap.shape)
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/ec/history.py,,_parse_history_file$536,"def _parse_history_file(
    history: History,
    path: Path,
    query: Any,
    greptexts: List[str],
    limit: Optional[int],
    logger: Logger,
) -> List[Any]:
    entries: List[Any] = []
    line_no = 0
    # If we have greptexts we pre-filter the file using the extremely
    # fast GNU Grep
    # Revert lines from the log file to have the newer lines processed first
    cmd = ""tac %s"" % quote_shell_string(str(path))
    if greptexts:
        cmd += "" | egrep -i -e %s"" % quote_shell_string("".*"".join(greptexts))
    grep = subprocess.Popen(cmd, shell=True, close_fds=True, stdout=subprocess.PIPE)  # nosec
    if grep.stdout is None:
        raise Exception(""Huh? stdout vanished..."")

    for line in grep.stdout:
        line_no += 1
        if limit is not None and len(entries) > limit:
            grep.kill()
            grep.wait()
            break

        try:
            parts: List[Any] = line.decode(""utf-8"").rstrip(""\n"").split(""\t"")
            _convert_history_line(history, parts)
            values = [line_no] + parts
            if query.filter_row(values):
                entries.append(values)
        except Exception as e:
            logger.exception(""Invalid line '%r' in history file %s: %s"" % (line, path, e))

    return entries",limit is not None and len(entries) > limit,None is not limit < len(entries)
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_dist_base.py,,runtime_main$809,"def runtime_main(test_class):
    parser = argparse.ArgumentParser(description='Run dist test.')
    parser.add_argument(
        '--role', type=str, required=True, choices=['pserver', 'trainer']
    )
    parser.add_argument('--endpoints', type=str, required=False, default="""")
    parser.add_argument(
        '--update_method',
        type=str,
        default=""local"",
        choices=[
            ""pserver"",
            ""nccl2"",
            ""bkcl"",
            ""local"",
            ""nccl2_reduce_layer"",
            ""gloo"",
            ""hccl"",
            ""cncl"",
        ],
    )
    parser.add_argument('--trainer_id', type=int, required=False, default=0)
    parser.add_argument('--trainers', type=int, required=False, default=1)
    parser.add_argument('--nccl_comm_num', type=int, required=False, default=1)
    parser.add_argument('--enable_backward_deps', action='store_true')
    parser.add_argument('--use_hallreduce', action='store_true')
    parser.add_argument('--use_pipeline', action='store_true')
    parser.add_argument('--use_fleet_api', action='store_true')
    parser.add_argument('--use_fleet_api_20', action='store_true')
    parser.add_argument('--use_local_sgd', action='store_true')
    parser.add_argument('--diff_batch', action='store_true')
    parser.add_argument('--ut4grad_allreduce', action='store_true')
    parser.add_argument(
        '--hallreduce_inter_nranks', type=int, required=False, default=2
    )
    parser.add_argument(
        '--current_endpoint', type=str, required=False, default=""""
    )
    parser.add_argument('--sync_mode', action='store_true')
    parser.add_argument('--use_cuda', action='store_true')
    parser.add_argument('--use_cpu', action='store_true')
    parser.add_argument('--use_xpu', action='store_true')
    parser.add_argument('--use_dgc', action='store_true')
    parser.add_argument('--use_npu', action='store_true')
    parser.add_argument('--use_mlu', action='store_true')
    parser.add_argument('--accumulate_gradient', action='store_true')
    parser.add_argument('--find_unused_parameters', action='store_true')
    parser.add_argument('--use_reduce', action='store_true')
    parser.add_argument('--dc_asgd', action='store_true')
    parser.add_argument('--hogwild', action='store_true')
    parser.add_argument('--save_model', action='store_true')
    parser.add_argument(
        '--use_reader_alloc', action='store_true', required=False
    )
    parser.add_argument('--batch_size', required=False, type=int, default=2)
    parser.add_argument('--lr', required=False, type=float, default=0.001)
    parser.add_argument(
        '--batch_merge_repeat', required=False, type=int, default=1
    )
    parser.add_argument(
        '--nccl2_reduce_layer_local_run',
        required=False,
        type=bool,
        default=False,
    )
    parser.add_argument('--sync_batch_norm', action='store_true')
    parser.add_argument(
        '--fuse_all_reduce', required=False, type=ast.literal_eval, default=None
    )

    args = parser.parse_args()

    if args.update_method == 'gloo':
        paddle.set_device(""cpu"")

    model = test_class()
    if args.role == ""pserver"" and args.update_method == ""pserver"":
        model.run_pserver(args)
    elif args.use_fleet_api:
        model.run_use_fleet_api_trainer(args)
    elif args.use_fleet_api_20:
        model.run_use_fleet_api_20_trainer(args)
    elif args.use_pipeline:
        model.run_pipeline_trainer(args)
    else:
        model.run_trainer(args)",args.role == 'pserver' and args.update_method == 'pserver',args.role == 'pserver' == args.update_method
C-3-Framework,https://github.com/gjy3035/C-3-Framework/tree/master/misc/transforms.py,Scale,__call__$102,"def __call__(self, img, mask):
        if img.size != mask.size:
            print( img.size )
            print( mask.size )          
        assert img.size == mask.size
        w, h = img.size
        if (w <= h and w == self.size) or (h <= w and h == self.size):
            return img, mask
        if w < h:
            ow = self.size
            oh = int(self.size * h / w)
            return img.resize((ow, oh), Image.BILINEAR), mask.resize((ow, oh), Image.NEAREST)
        else:
            oh = self.size
            ow = int(self.size * w / h)
            return img.resize((ow, oh), Image.BILINEAR), mask.resize((ow, oh), Image.NEAREST)",w <= h and w == self.size,h >= w == self.size
C-3-Framework,https://github.com/gjy3035/C-3-Framework/tree/master/misc/transforms.py,Scale,__call__$102,"def __call__(self, img, mask):
        if img.size != mask.size:
            print( img.size )
            print( mask.size )          
        assert img.size == mask.size
        w, h = img.size
        if (w <= h and w == self.size) or (h <= w and h == self.size):
            return img, mask
        if w < h:
            ow = self.size
            oh = int(self.size * h / w)
            return img.resize((ow, oh), Image.BILINEAR), mask.resize((ow, oh), Image.NEAREST)
        else:
            oh = self.size
            ow = int(self.size * w / h)
            return img.resize((ow, oh), Image.BILINEAR), mask.resize((ow, oh), Image.NEAREST)",h <= w and h == self.size,w >= h == self.size
MotionPlanning,https://github.com/zhm-real/MotionPlanning/tree/master/CurvesGenerator/reeds_shepp.py,,LRSR$313,"def LRSR(x, y, phi):
    xi = x + math.sin(phi)
    eta = y - 1.0 - math.cos(phi)
    rho, theta = R(-eta, xi)

    if rho >= 2.0:
        t = theta
        u = 2.0 - rho
        v = M(t + 0.5 * PI - phi)
        if t >= 0.0 and u <= 0.0 and v <= 0.0:
            return True, t, u, v

    return False, 0.0, 0.0, 0.0",t >= 0.0 and u <= 0.0 and (v <= 0.0),t >= 0.0 >= u and v <= 0.0
