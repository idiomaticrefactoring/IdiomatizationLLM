repo_name,file_html,class_name,me_name,me_code,old_code,new_code
R-Drop,https://github.com/dropreg/R-Drop/tree/master/fairseq_src/scripts/compare_namespaces.py,,keys$12,"def keys(ns):
        ks = set()
        for k in dir(ns):
            if not k.startswith(""_""):
                ks.add(k)
        return ks","ks = set()
for k in dir(ns):
    if not k.startswith('_'):
        ks.add(k)",ks = {k for k in dir(ns) if not k.startswith('_')}
nilearn,https://github.com/nilearn/nilearn/tree/master/nilearn/datasets/func.py,,infer_subjects$2024,"def infer_subjects(urls):
        subjects = set()
        for url in urls:
            if 'sub-' in url:
                subjects.add(re.search(subject_regex, url).group(0)[:-1])
        return sorted(subjects)","subjects = set()
for url in urls:
    if 'sub-' in url:
        subjects.add(re.search(subject_regex, url).group(0)[:-1])","subjects = {re.search(subject_regex, url).group(0)[:-1] for url in urls if 'sub-' in url}"
Multilingual_Text_to_Speech,https://github.com/Tomiinek/Multilingual_Text_to_Speech/tree/master/dataset/dataset.py,TextToSpeechDataset,get_num_languages$185,"def get_num_languages(self):
        """"""Get number of unique languages in the dataset.""""""
        languages = set()
        for idx in range(len(self.items)):
            languages.add(self.items[idx]['language'])
        return len(languages)","languages = set()
for idx in range(len(self.items)):
    languages.add(self.items[idx]['language'])",languages = {self.items[idx]['language'] for idx in range(len(self.items))}
pandas-datareader,https://github.com/pydata/pandas-datareader/tree/master//versioneer.py,,do_setup$1754,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (
        EnvironmentError,
        configparser.NoSectionError,
        configparser.NoOptionError,
    ) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"", file=sys.stderr)
            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with open(cfg.versionfile_source, ""w"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(
            LONG
            % {
                ""DOLLAR"": ""$"",
                ""STYLE"": cfg.style,
                ""TAG_PREFIX"": cfg.tag_prefix,
                ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
            }
        )

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy, ""r"") as f:
                old = f.read()
        except EnvironmentError:
            old = """"
        if INIT_PY_SNIPPET not in old:
            print("" appending to %s"" % ipy)
            with open(ipy, ""a"") as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with open(manifest_in, ""r"") as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, ""a"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print(
            "" appending versionfile_source ('%s') to MANIFEST.in""
            % cfg.versionfile_source
        )
        with open(manifest_in, ""a"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-subst keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","simple_includes = set()
for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}
prjxray,https://github.com/SymbiFlow/prjxray/tree/master/utils/create_timing_worksheet_db.py,,build_wire_filter$482,"def build_wire_filter(wire_filter):
    wires_to_include = set()

    with OpenSafeFile(wire_filter) as f:
        for l in f:
            wire = l.strip()
            if not wire:
                continue
            wires_to_include.add(wire)

    def filter_net(net):
        wires_in_net = set()

        for node in net['nodes']:
            for wire in node['wires']:
                wires_in_net.add(wire['name'])

        return len(wires_in_net & wires_to_include) > 0

    return filter_net","wires_in_net = set()
for node in net['nodes']:
    for wire in node['wires']:
        wires_in_net.add(wire['name'])",wires_in_net = {wire['name'] for node in net['nodes'] for wire in node['wires']}
Amulet-Map-Editor,https://github.com/Amulet-Team/Amulet-Map-Editor/tree/master//versioneer.py,,do_setup$1753,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (
        EnvironmentError,
        configparser.NoSectionError,
        configparser.NoOptionError,
    ) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"", file=sys.stderr)
            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with open(cfg.versionfile_source, ""w"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(
            LONG
            % {
                ""DOLLAR"": ""$"",
                ""STYLE"": cfg.style,
                ""TAG_PREFIX"": cfg.tag_prefix,
                ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
            }
        )

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy, ""r"") as f:
                old = f.read()
        except EnvironmentError:
            old = """"
        if INIT_PY_SNIPPET not in old:
            print("" appending to %s"" % ipy)
            with open(ipy, ""a"") as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with open(manifest_in, ""r"") as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, ""a"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print(
            "" appending versionfile_source ('%s') to MANIFEST.in""
            % cfg.versionfile_source
        )
        with open(manifest_in, ""a"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-subst keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","simple_includes = set()
for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/fvqa/agents.py,SplitTeacher,_setup_data$161,"def _setup_data(self, questions_path, trainset_path, datatype, task_num):
        print('loading: ' + questions_path)
        with PathManager.open(questions_path) as questions_file:
            questions = json.load(questions_file)
        train_test_images = set()
        fn = os.path.join(trainset_path, '{}_list_{}.txt'.format(datatype, task_num))
        with PathManager.open(fn) as imageset:
            for line in imageset:
                train_test_images.add(line.strip())
        self.ques = [
            questions[k]
            for k in sorted(questions.keys())
            if questions[k]['img_file'] in train_test_images
        ]","train_test_images = set()
for line in imageset:
    train_test_images.add(line.strip())",train_test_images = {line.strip() for line in imageset}
SatanSword,https://github.com/Lucifer1993/SatanSword/tree/master/Evil_Eye/wappalyzer/Wappalyzer.py,Wappalyzer,analyze$255,"def analyze(self, webpage):
        """"""
        Return a list of applications that can be detected on the web page.
        """"""
        detected_apps = set()

        for app_name, app in self.apps.items():
            if self._has_app(app, webpage):
                detected_apps.add(app_name)

        detected_apps |= self._get_implied_apps(detected_apps)

        return detected_apps","detected_apps = set()
for (app_name, app) in self.apps.items():
    if self._has_app(app, webpage):
        detected_apps.add(app_name)","detected_apps = {app_name for (app_name, app) in self.apps.items() if self._has_app(app, webpage)}"
integrations-core,https://github.com/DataDog/integrations-core/tree/master/couch/tests/test_couchv2.py,,test_only_max_dbs_are_scanned$260,"def test_only_max_dbs_are_scanned(aggregator, gauges, number_db):
    config = deepcopy(common.NODE1)
    config[""max_dbs_per_check""] = number_db

    check = CouchDb(common.CHECK_NAME, {}, [config])
    check.check(config)

    metrics = []
    for metric_list in aggregator._metrics.values():
        metrics.extend(metric_list)

    db_tags = set()
    for m in metrics:
        for tag in m.tags:
            if tag.startswith('db:'):
                db_tags.add(tag)

    assert len(db_tags) == number_db","db_tags = set()
for m in metrics:
    for tag in m.tags:
        if tag.startswith('db:'):
            db_tags.add(tag)",db_tags = {tag for m in metrics for tag in m.tags if tag.startswith('db:')}
mlrun,https://github.com/mlrun/mlrun/tree/master/tests/api/api/feature_store/test_feature_sets.py,,test_feature_set_tagging_with_re_store$508,"def test_feature_set_tagging_with_re_store(db: Session, client: TestClient) -> None:
    project_name = f""prj-{uuid4().hex}""
    tests.api.api.utils.create_project(client, project_name)

    name = ""feature_set1""
    feature_set = _generate_feature_set(name)

    # Put a new object - verify it's created
    response = _store_and_assert_feature_set(
        client, project_name, name, ""tag1"", feature_set
    )
    uid = response[""metadata""][""uid""]

    # Put the same object with a different tag - this should result in just adding a tag
    response = _store_and_assert_feature_set(
        client, project_name, name, ""tag2"", feature_set
    )
    assert response[""metadata""][""uid""] == uid

    response = _list_and_assert_objects(
        client, ""feature_sets"", project_name, f""name={name}"", 2
    )[""feature_sets""]

    expected_tags = {""tag1"", ""tag2""}
    returned_tags = set()
    for feature_set_response in response:
        returned_tags.add(feature_set_response[""metadata""][""tag""])
    assert expected_tags == returned_tags

    # Storing object with same tag - should just update
    feature_set[""metadata""][""extra_metadata""] = 200
    _store_and_assert_feature_set(client, project_name, name, ""tag2"", feature_set)

    _list_and_assert_objects(client, ""feature_sets"", project_name, f""name={name}"", 2)

    response = _list_and_assert_objects(
        client, ""feature_sets"", project_name, f""name={name}&tag=tag2"", 1
    )[""feature_sets""]
    assert response[0][""metadata""][""extra_metadata""] == 200","returned_tags = set()
for feature_set_response in response:
    returned_tags.add(feature_set_response['metadata']['tag'])",returned_tags = {feature_set_response['metadata']['tag'] for feature_set_response in response}
aioprocessing,https://github.com/dano/aioprocessing/tree/master/aioprocessing/executor.py,CoroBuilder,__new__$105,"def __new__(cls, clsname, bases, dct, **kwargs):
        coro_list = dct.get(""coroutines"", [])
        existing_coros = set()

        def find_existing_coros(d):
            for attr in d:
                if attr.startswith(""coro_"") or attr.startswith(""thread_""):
                    existing_coros.add(attr)

        # Determine if any bases include the coroutines attribute, or
        # if either this class or a base class provides an actual
        # implementation for a coroutine method.
        find_existing_coros(dct)
        for b in bases:
            b_dct = b.__dict__
            coro_list.extend(b_dct.get(""coroutines"", []))
            find_existing_coros(b_dct)

        # Add _ExecutorMixin to bases.
        if _ExecutorMixin not in bases:
            bases += (_ExecutorMixin,)

        # Add coro funcs to dct, but only if a definition
        # is not already provided by dct or one of our bases.
        for func in coro_list:
            coro_name = ""coro_{}"".format(func)
            if coro_name not in existing_coros:
                dct[coro_name] = cls.coro_maker(func)

        return super().__new__(cls, clsname, bases, dct)","existing_coros = set()
for attr in d:
    if attr.startswith('coro_') or attr.startswith('thread_'):
        existing_coros.add(attr)",existing_coros = {attr for attr in d if attr.startswith('coro_') or attr.startswith('thread_')}
tapas,https://github.com/google-research/tapas/tree/master/tapas/utils/sqa_utils.py,,_add_tables$53,"def _add_tables(input_dir,
                interaction_dict):
  """"""Adds table protos to all interactions.""""""
  table_files = set()
  for interactions in interaction_dict.values():
    for interaction in interactions:
      table_files.add(interaction.table.table_id)

  table_dict = {}
  for index, table_file in enumerate(sorted(table_files)):
    logging.log_every_n(logging.INFO, 'Read %4d / %4d table files', 100, index,
                        len(table_files))
    table_path = os.path.join(input_dir, table_file)
    with tf.io.gfile.GFile(table_path, 'r') as table_handle:
      table = interaction_pb2.Table()
      rows = list(csv.reader(table_handle))
      headers, rows = rows[0], rows[1:]

      for header in headers:
        table.columns.add().text = header

      for row in rows:
        new_row = table.rows.add()
        for cell in row:
          new_row.cells.add().text = cell

      table.table_id = table_file
      table_dict[table_file] = table

  for interactions in interaction_dict.values():
    for interaction in interactions:
      interaction.table.CopyFrom(table_dict[interaction.table.table_id])","table_files = set()
for interactions in interaction_dict.values():
    for interaction in interactions:
        table_files.add(interaction.table.table_id)",table_files = {interaction.table.table_id for interactions in interaction_dict.values() for interaction in interactions}
salt,https://github.com/saltstack/salt/tree/master/salt/ext/tornado/test/web_test.py,StaticFileTest,get_and_head$1034,"def get_and_head(self, *args, **kwargs):
        """"""Performs a GET and HEAD request and returns the GET response.

        Fails if any ``Content-*`` headers returned by the two requests
        differ.
        """"""
        head_response = self.fetch(*args, method=""HEAD"", **kwargs)
        get_response = self.fetch(*args, method=""GET"", **kwargs)
        content_headers = set()
        for h in itertools.chain(head_response.headers, get_response.headers):
            if h.startswith('Content-'):
                content_headers.add(h)
        for h in content_headers:
            self.assertEqual(head_response.headers.get(h),
                             get_response.headers.get(h),
                             ""%s differs between GET (%s) and HEAD (%s)"" %
                             (h, head_response.headers.get(h),
                              get_response.headers.get(h)))
        return get_response","content_headers = set()
for h in itertools.chain(head_response.headers, get_response.headers):
    if h.startswith('Content-'):
        content_headers.add(h)","content_headers = {h for h in itertools.chain(head_response.headers, get_response.headers) if h.startswith('Content-')}"
moto,https://github.com/spulec/moto/tree/master/tests/test_ec2/test_regions.py,,test_use_boto_regions$19,"def test_use_boto_regions():
    boto_regions = set()
    for region in Session().get_available_regions(""ec2""):
        boto_regions.add(region)
    for region in Session().get_available_regions(""ec2"", partition_name=""aws-us-gov""):
        boto_regions.add(region)
    for region in Session().get_available_regions(""ec2"", partition_name=""aws-cn""):
        boto_regions.add(region)
    moto_regions = set(ec2_backends)

    moto_regions.should.equal(boto_regions)","boto_regions = set()
for region in Session().get_available_regions('ec2'):
    boto_regions.add(region)",boto_regions = {region for region in Session().get_available_regions('ec2')}
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/policy.py,ConfigPollRuleMode,run$784,"def run(self, event, lambda_context):
        cfg_event = json.loads(event['invokingEvent'])
        resource_type = self.policy.resource_manager.resource_type.cfn_type
        resource_id = self.policy.resource_manager.resource_type.id
        client = self._get_client()
        token = event.get('resultToken')

        matched_resources = set()
        for r in PullMode.run(self):
            matched_resources.add(r[resource_id])
        unmatched_resources = set()
        for r in self.policy.resource_manager.get_resource_manager(
                self.policy.resource_type).resources():
            if r[resource_id] not in matched_resources:
                unmatched_resources.add(r[resource_id])

        evaluations = [dict(
            ComplianceResourceType=resource_type,
            ComplianceResourceId=r,
            ComplianceType='NON_COMPLIANT',
            OrderingTimestamp=cfg_event['notificationCreationTime'],
            Annotation='The resource is not compliant with policy:%s.' % (
                self.policy.name))
            for r in matched_resources]
        if evaluations and token:
            self.put_evaluations(client, token, evaluations)

        evaluations = [dict(
            ComplianceResourceType=resource_type,
            ComplianceResourceId=r,
            ComplianceType='COMPLIANT',
            OrderingTimestamp=cfg_event['notificationCreationTime'],
            Annotation='The resource is compliant with policy:%s.' % (
                self.policy.name))
            for r in unmatched_resources]
        if evaluations and token:
            self.put_evaluations(client, token, evaluations)
        return list(matched_resources)","unmatched_resources = set()
for r in self.policy.resource_manager.get_resource_manager(self.policy.resource_type).resources():
    if r[resource_id] not in matched_resources:
        unmatched_resources.add(r[resource_id])",unmatched_resources = {r[resource_id] for r in self.policy.resource_manager.get_resource_manager(self.policy.resource_type).resources() if r[resource_id] not in matched_resources}
nltk,https://github.com/nltk/nltk/tree/master/nltk/sem/evaluate.py,,set2rel$79,"def set2rel(s):
    """"""
    Convert a set containing individuals (strings or numbers) into a set of
    unary tuples. Any tuples of strings already in the set are passed through
    unchanged.

    For example:
      - set(['a', 'b']) => set([('a',), ('b',)])
      - set([3, 27]) => set([('3',), ('27',)])

    :type s: set
    :rtype: set of tuple of str
    """"""
    new = set()
    for elem in s:
        if isinstance(elem, str):
            new.add((elem,))
        elif isinstance(elem, int):
            new.add(str(elem))
        else:
            new.add(elem)
    return new","new = set()
for elem in s:
    if isinstance(elem, str):
        new.add((elem,))
    elif isinstance(elem, int):
        new.add(str(elem))
    else:
        new.add(elem)","new = {(elem,) if isinstance(elem, str) else str(elem) if isinstance(elem, int) else elem for elem in s}"
fitlog,https://github.com/fastnlp/fitlog/tree/master/fitlog/fastserver/server/chart_utils.py,,_refine_path$184,"def _refine_path(paths):
    """"""
    给定list的path，将公共的部分删掉一些. 这里只处理完全一样深度的metric. 主要为了删除相同的metric_name
        [['metric', 'BMESF1MEtric', 'f1'], ['metric', 'BMESF1Metric'], ...]
    :param paths:
    :return:
    """"""
    if len(set(map(len, paths)))!=1:# 如果深度不同直接回去
        path2shortpath = {'-'.join(path):'-'.join(path) for path in paths}
    elif len(paths)==0:
        path2shortpath = {'-'.join(paths[0]): paths[0][-1]}
    else:
        delete_depths = []
        for depth in range(len(paths[0])):
            names = set()
            for path in paths:
                names.add(path[depth])
            if len(names)==1:
                delete_depths.append(depth)
        for i in range(len(paths)):
            for d in reversed(delete_depths):
                paths[i].pop(d)
        path2shortpath = {'-'.join(path): '-'.join(path) for path in paths}
    return path2shortpath","names = set()
for path in paths:
    names.add(path[depth])",names = {path[depth] for path in paths}
graph4nlp,https://github.com/graph4ai/graph4nlp/tree/master/examples/pytorch/semantic_parsing/graph2tree/jobs/src/evaluation.py,,get_split_comma$26,"def get_split_comma(input_str):
    input_str = input_str.replace("","", "" , "")
    input_list = [item.strip() for item in input_str.split()]
    ref_char = ""$""
    for index in range(len(input_list)):
        if input_list[index] == "","":
            if input_list[:index].count(""("") == input_list[:index].count("")""):
                if input_list[index + 1 :].count(""("") == input_list[index + 1 :].count("")""):
                    if input_list[index] == ref_char:
                        raise RuntimeError
                    else:
                        input_list[index] = ref_char
    new_str = "" "".join(input_list).split(""$"")
    result_set = set()
    for str_ in new_str:
        result_set.add(str_.strip())
    return result_set","result_set = set()
for str_ in new_str:
    result_set.add(str_.strip())",result_set = {str_.strip() for str_ in new_str}
edx-platform,https://github.com/edx/edx-platform/tree/master/scripts/xblock/xblock_counts.py,,_get_block_types_from_json_file$150,"def _get_block_types_from_json_file(xblock_json_file):
    """"""
    Retrieves the block types from the provided xBlock configuration JSON file

    Arguments:
        xblock_json_file (str): The name of the xBlock configuration file

    :return:
        set: A set of strings for all the types that are available in the configuration file
    """"""
    if not os.path.isfile(xblock_json_file):
        print('xBlock configuration file does not exist: %s' % xblock_json_file)
        sys.exit(2)
    with open(xblock_json_file) as json_file:
        type_set = set()
        try:
            json_data = json.loads(json_file.read())
        except ValueError as e:
            print('xBlock configuration file does not match the expected layout and is '
                  'missing ""data"" list: %s' % xblock_json_file)
            sys.exit(str(e))
        if 'data' in json_data:
            xblock_type_list = json_data['data']
            for xblock in xblock_type_list:
                type_set.add(xblock['name'])
            return type_set
        else:
            print('xBlock configuration file does not match the expected layout and is '
                  'missing ""data"" list: %s' % xblock_json_file)
            sys.exit(2)","type_set = set()
for xblock in xblock_type_list:
    type_set.add(xblock['name'])",type_set = {xblock['name'] for xblock in xblock_type_list}
textflint,https://github.com/textflint/textflint/tree/master/textflint/generation/transformation/ABSA/absa_transformation.py,ABSATransformation,reverse_opinion$307,"def reverse_opinion(
            self,
            trans_words,
            trans_opinion_words,
            opinion_from,
            opinion_to,
            has_neg):
        r""""""
        Reverse the polarity of original opinion and return the new
        transformed opinion words.

        :param list trans_words: tokenized words of transformed sentence
        :param list trans_opinion_words: transformed opinion words
        :param int opinion_from: start index of opinion
        :param int opinion_to: end index of opinion
        :param bool has_neg: whether exist negation in transformed sentence
        """"""
        opinion_list = trans_words[opinion_from:opinion_to]
        opinion_words = trans_words[opinion_from:opinion_to]
        opi = opinion_list[0]
        trans_opinion_word = None
        from_to = []

        if has_neg and [opinion_from, opinion_to] not in from_to:
            trans_opinion_word = [
                opinion_from,
                opinion_to,
                self.untokenize(opinion_words)]
        elif [opinion_from, opinion_to] not in from_to:
            opi_pos = self.get_postag(trans_words, opinion_from, opinion_to)
            antonyms = self.get_antonyms(opi_pos)[0]
            candidate = set()
            for antonym in antonyms:
                for ant_word in antonym.lemma_names(lang='eng'):
                    if (
                            (ant_word != opi)
                            and (""_"" not in ant_word)
                    ):
                        candidate.add(ant_word)

            refined_candidate = self.refine_candidate(
                trans_words, opinion_from, opinion_to, candidate)
            if len(refined_candidate) == 0:
                trans_opinion_word = [opinion_from, opinion_to,
                                      self.untokenize(['not', opi])]
            else:
                select = random.randint(0, len(refined_candidate) - 1)
                trans_opinion_word = [opinion_from,
                                      opinion_to,
                                      self.untokenize(
                                          [refined_candidate[select]])]
        if trans_opinion_word is not None:
            trans_opinion_words.append(trans_opinion_word)
            from_to.append([opinion_from, opinion_to])
            trans_words[opinion_from: opinion_to] = [trans_opinion_word[2]]

        return trans_words, trans_opinion_words","candidate = set()
for antonym in antonyms:
    for ant_word in antonym.lemma_names(lang='eng'):
        if ant_word != opi and '_' not in ant_word:
            candidate.add(ant_word)",candidate = {ant_word for antonym in antonyms for ant_word in antonym.lemma_names(lang='eng') if ant_word != opi and '_' not in ant_word}
shuup,https://github.com/shuup/shuup/tree/master/shuup_tests/admin/test_modules.py,,test_dashboard_blocks$75,"def test_dashboard_blocks(rf):
    request = rf.get(""/"")
    with replace_modules([ATestModule]):
        block_ids = set()
        for block in chain(*(m.get_dashboard_blocks(request) for m in get_modules())):
            block_ids.add(block.id)
        assert block_ids >= set([""test-0"", ""test-1"", ""test-2"", ""test-3"", ""test-4""])","block_ids = set()
for block in chain(*(m.get_dashboard_blocks(request) for m in get_modules())):
    block_ids.add(block.id)",block_ids = {block.id for block in chain(*(m.get_dashboard_blocks(request) for m in get_modules()))}
docassemble,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/backend.py,,delete_user_data$743,"def delete_user_data(user_id, r, r_user):
    db.session.execute(delete(UserDict).where(UserDict.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UserDictKeys).where(UserDictKeys.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UploadsUserAuth).where(UploadsUserAuth.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.owner_id == user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(GlobalObjectStorage).where(GlobalObjectStorage.user_id == user_id))
    db.session.commit()
    for package_auth in db.session.execute(select(PackageAuth).filter_by(user_id=user_id)).scalars():
        package_auth.user_id = 1
    db.session.commit()
    files_to_delete = list()
    for short_code_item in db.session.execute(select(Shortener).filter_by(user_id=user_id)).scalars():
        for email in db.session.execute(select(Email).filter_by(short=short_code_item.short)).scalars():
            for attachment in db.session.execute(select(EmailAttachment).filter_by(email_id=email.id)).scalars():
                files_to_delete.append(attachment.upload)
    for file_number in files_to_delete:
        the_file = SavedFile(file_number)
        the_file.delete()
    db.session.execute(delete(Shortener).where(Shortener.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UserRoles).where(UserRoles.user_id == user_id))
    db.session.commit()
    for user_auth in db.session.execute(select(UserAuthModel).filter_by(user_id=user_id).with_for_update()).scalars():
        user_auth.password = ''
        user_auth.reset_password_token = ''
    db.session.commit()
    for section in ('playground', 'playgroundmodules', 'playgroundpackages', 'playgroundsources', 'playgroundstatic', 'playgroundtemplate'):
        the_section = SavedFile(user_id, section=section)
        the_section.delete()
    old_email = None
    for user_object in db.session.execute(select(UserModel).filter_by(id=user_id)).scalars():
        old_email = user_object.email
        user_object.active = False
        user_object.first_name = ''
        user_object.last_name = ''
        user_object.nickname = ''
        user_object.email = None
        user_object.country = ''
        user_object.subdivisionfirst = ''
        user_object.subdivisionsecond = ''
        user_object.subdivisionthird = ''
        user_object.organization = ''
        user_object.timezone = None
        user_object.language = None
        user_object.pypi_username = None
        user_object.pypi_password = None
        user_object.otp_secret = None
        user_object.confirmed_at = None
        user_object.last_login = None
        user_object.social_id = 'disabled$' + str(user_id)
    db.session.commit()
    keys_to_delete = set()
    for key in r.keys('*userid:' + str(user_id)):
        keys_to_delete.add(key)
    for key in r.keys('*userid:' + str(user_id) + ':*'):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r.delete(key)
    keys_to_delete = set()
    for key in r_user.keys('*:user:' + str(old_email)):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r_user.delete(key)","keys_to_delete = set()
for key in r.keys('*userid:' + str(user_id)):
    keys_to_delete.add(key)",keys_to_delete = {key for key in r.keys('*userid:' + str(user_id))}
docassemble,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/backend.py,,delete_user_data$743,"def delete_user_data(user_id, r, r_user):
    db.session.execute(delete(UserDict).where(UserDict.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UserDictKeys).where(UserDictKeys.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UploadsUserAuth).where(UploadsUserAuth.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.owner_id == user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(GlobalObjectStorage).where(GlobalObjectStorage.user_id == user_id))
    db.session.commit()
    for package_auth in db.session.execute(select(PackageAuth).filter_by(user_id=user_id)).scalars():
        package_auth.user_id = 1
    db.session.commit()
    files_to_delete = list()
    for short_code_item in db.session.execute(select(Shortener).filter_by(user_id=user_id)).scalars():
        for email in db.session.execute(select(Email).filter_by(short=short_code_item.short)).scalars():
            for attachment in db.session.execute(select(EmailAttachment).filter_by(email_id=email.id)).scalars():
                files_to_delete.append(attachment.upload)
    for file_number in files_to_delete:
        the_file = SavedFile(file_number)
        the_file.delete()
    db.session.execute(delete(Shortener).where(Shortener.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UserRoles).where(UserRoles.user_id == user_id))
    db.session.commit()
    for user_auth in db.session.execute(select(UserAuthModel).filter_by(user_id=user_id).with_for_update()).scalars():
        user_auth.password = ''
        user_auth.reset_password_token = ''
    db.session.commit()
    for section in ('playground', 'playgroundmodules', 'playgroundpackages', 'playgroundsources', 'playgroundstatic', 'playgroundtemplate'):
        the_section = SavedFile(user_id, section=section)
        the_section.delete()
    old_email = None
    for user_object in db.session.execute(select(UserModel).filter_by(id=user_id)).scalars():
        old_email = user_object.email
        user_object.active = False
        user_object.first_name = ''
        user_object.last_name = ''
        user_object.nickname = ''
        user_object.email = None
        user_object.country = ''
        user_object.subdivisionfirst = ''
        user_object.subdivisionsecond = ''
        user_object.subdivisionthird = ''
        user_object.organization = ''
        user_object.timezone = None
        user_object.language = None
        user_object.pypi_username = None
        user_object.pypi_password = None
        user_object.otp_secret = None
        user_object.confirmed_at = None
        user_object.last_login = None
        user_object.social_id = 'disabled$' + str(user_id)
    db.session.commit()
    keys_to_delete = set()
    for key in r.keys('*userid:' + str(user_id)):
        keys_to_delete.add(key)
    for key in r.keys('*userid:' + str(user_id) + ':*'):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r.delete(key)
    keys_to_delete = set()
    for key in r_user.keys('*:user:' + str(old_email)):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r_user.delete(key)","keys_to_delete = set()
for key in r.keys('*userid:' + str(user_id) + ':*'):
    keys_to_delete.add(key)",keys_to_delete = {key for key in r.keys('*userid:' + str(user_id) + ':*')}
docassemble,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/backend.py,,delete_user_data$743,"def delete_user_data(user_id, r, r_user):
    db.session.execute(delete(UserDict).where(UserDict.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UserDictKeys).where(UserDictKeys.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UploadsUserAuth).where(UploadsUserAuth.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.owner_id == user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(GlobalObjectStorage).where(GlobalObjectStorage.user_id == user_id))
    db.session.commit()
    for package_auth in db.session.execute(select(PackageAuth).filter_by(user_id=user_id)).scalars():
        package_auth.user_id = 1
    db.session.commit()
    files_to_delete = list()
    for short_code_item in db.session.execute(select(Shortener).filter_by(user_id=user_id)).scalars():
        for email in db.session.execute(select(Email).filter_by(short=short_code_item.short)).scalars():
            for attachment in db.session.execute(select(EmailAttachment).filter_by(email_id=email.id)).scalars():
                files_to_delete.append(attachment.upload)
    for file_number in files_to_delete:
        the_file = SavedFile(file_number)
        the_file.delete()
    db.session.execute(delete(Shortener).where(Shortener.user_id == user_id))
    db.session.commit()
    db.session.execute(delete(UserRoles).where(UserRoles.user_id == user_id))
    db.session.commit()
    for user_auth in db.session.execute(select(UserAuthModel).filter_by(user_id=user_id).with_for_update()).scalars():
        user_auth.password = ''
        user_auth.reset_password_token = ''
    db.session.commit()
    for section in ('playground', 'playgroundmodules', 'playgroundpackages', 'playgroundsources', 'playgroundstatic', 'playgroundtemplate'):
        the_section = SavedFile(user_id, section=section)
        the_section.delete()
    old_email = None
    for user_object in db.session.execute(select(UserModel).filter_by(id=user_id)).scalars():
        old_email = user_object.email
        user_object.active = False
        user_object.first_name = ''
        user_object.last_name = ''
        user_object.nickname = ''
        user_object.email = None
        user_object.country = ''
        user_object.subdivisionfirst = ''
        user_object.subdivisionsecond = ''
        user_object.subdivisionthird = ''
        user_object.organization = ''
        user_object.timezone = None
        user_object.language = None
        user_object.pypi_username = None
        user_object.pypi_password = None
        user_object.otp_secret = None
        user_object.confirmed_at = None
        user_object.last_login = None
        user_object.social_id = 'disabled$' + str(user_id)
    db.session.commit()
    keys_to_delete = set()
    for key in r.keys('*userid:' + str(user_id)):
        keys_to_delete.add(key)
    for key in r.keys('*userid:' + str(user_id) + ':*'):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r.delete(key)
    keys_to_delete = set()
    for key in r_user.keys('*:user:' + str(old_email)):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r_user.delete(key)","keys_to_delete = set()
for key in r_user.keys('*:user:' + str(old_email)):
    keys_to_delete.add(key)",keys_to_delete = {key for key in r_user.keys('*:user:' + str(old_email))}
neutron,https://github.com/openstack/neutron/tree/master/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_db_sync.py,OvnNbSynchronizer,sync_port_groups$165,"def sync_port_groups(self, ctx):
        """"""Sync Port Groups between neutron and NB.

        @param ctx: neutron_lib.context
        @type  ctx: object of type neutron_lib.context.Context
        """"""

        neutron_sgs = {}
        neutron_pgs = set()
        with db_api.CONTEXT_READER.using(ctx):
            for sg in self.core_plugin.get_security_groups(ctx):
                pg_name = utils.ovn_port_group_name(sg['id'])
                neutron_pgs.add(pg_name)
                neutron_sgs[pg_name] = sg['id']
            neutron_pgs.add(ovn_const.OVN_DROP_PORT_GROUP_NAME)

        ovn_pgs = set()
        port_groups = self.ovn_api.db_list_rows('Port_Group').execute() or []
        for pg in port_groups:
            ovn_pgs.add(pg.name)

        add_pgs = neutron_pgs.difference(ovn_pgs)
        remove_pgs = ovn_pgs.difference(neutron_pgs)

        LOG.debug('Port Groups added %d, removed %d',
                  len(add_pgs), len(remove_pgs))

        if self.mode == SYNC_MODE_REPAIR:
            LOG.debug('Port-Group-SYNC: transaction started @ %s',
                      str(datetime.now()))
            if add_pgs:
                db_ports = self.core_plugin.get_ports(ctx)
                ovn_ports = set(p.name for p in
                                self.ovn_api.lsp_list().execute())
            with self.ovn_api.transaction(check_error=True) as txn:
                pg = ovn_const.OVN_DROP_PORT_GROUP_NAME
                # Process default drop port group first
                if pg in add_pgs:
                    txn.add(self.ovn_api.pg_add(name=pg, acls=[]))
                    add_pgs.remove(pg)
                    # Add ports to the drop port group. Only add those that
                    # already exists in OVN. The rest will be added during the
                    # ports sync operation later.
                    for n_port in db_ports:
                        if ((utils.is_security_groups_enabled(n_port) or
                             utils.is_port_security_enabled(n_port)) and
                                n_port['id'] in ovn_ports):
                            txn.add(self.ovn_api.pg_add_ports(
                                pg, n_port['id']))

                for pg in add_pgs:
                    # If it's a security group PG, add the ext id
                    ext_ids = {ovn_const.OVN_SG_EXT_ID_KEY: neutron_sgs[pg]}
                    txn.add(self.ovn_api.pg_add(name=pg, acls=[],
                                                external_ids=ext_ids))
                    # Add the ports belonging to the SG to this port group
                    for n_port in db_ports:
                        if (neutron_sgs[pg] in n_port['security_groups'] and
                                n_port['id'] in ovn_ports):
                            txn.add(self.ovn_api.pg_add_ports(
                                pg, n_port['id']))
                for pg in remove_pgs:
                    txn.add(self.ovn_api.pg_del(pg))
            LOG.debug('Port-Group-SYNC: transaction finished @ %s',
                      str(datetime.now()))","ovn_pgs = set()
for pg in port_groups:
    ovn_pgs.add(pg.name)",ovn_pgs = {pg.name for pg in port_groups}
dowhy,https://github.com/microsoft/dowhy/tree/master/dowhy/causal_identifiers/backdoor.py,HittingSetAlgorithm,_indices_covered$277,"def _indices_covered(self, el, set_index=None):
        '''
        Obtain indices covered in a particular iteration of the algorithm.
        '''
        covered = set()
        if set_index == None:
            set_index = set([i for i in range(len(self._list_of_sets))])

        for idx in set_index:
            if el in self._list_of_sets[idx]:
                covered.add(idx)
        return covered","covered = set()
for idx in set_index:
    if el in self._list_of_sets[idx]:
        covered.add(idx)",covered = {idx for idx in set_index if el in self._list_of_sets[idx]}
zato,https://github.com/zatosource/zato/tree/master/code/zato-common/src/zato/common/odb/api.py,ODBManager,get_missing_services$601,"def get_missing_services(self, server, locally_deployed):
        """""" Returns services deployed on the server given on input that are not among locally_deployed.
        """"""
        missing = set()

        with closing(self.session()) as session:
            server_services = session.query(
                Service.id, Service.name,
                DeployedService.source_path, DeployedService.source).\
                join(DeployedService, Service.id==DeployedService.service_id).\
                join(Server, DeployedService.server_id==Server.id).\
                filter(Service.is_internal!=true()).\
                all()

            for item in server_services:
                if item.name not in locally_deployed:
                    missing.add(item)

        return missing","missing = set()
for item in server_services:
    if item.name not in locally_deployed:
        missing.add(item)",missing = {item for item in server_services if item.name not in locally_deployed}
yt-dlp,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/YoutubeDL.py,YoutubeDL,__init__$514,"def __init__(self, params=None, auto_init=True):
        """"""Create a FileDownloader object with the given options.
        @param auto_init    Whether to load the default extractors and print header (if verbose).
                            Set to 'no_verbose_header' to not print the header
        """"""
        if params is None:
            params = {}
        self._ies = {}
        self._ies_instances = {}
        self._pps = {'pre_process': [], 'before_dl': [], 'after_move': [], 'post_process': []}
        self._printed_messages = set()
        self._first_webpage_request = True
        self._post_hooks = []
        self._progress_hooks = []
        self._postprocessor_hooks = []
        self._download_retcode = 0
        self._num_downloads = 0
        self._screen_file = [sys.stdout, sys.stderr][params.get('logtostderr', False)]
        self._err_file = sys.stderr
        self.params = params
        self.cache = Cache(self)

        windows_enable_vt_mode()
        self._allow_colors = {
            'screen': not self.params.get('no_color') and supports_terminal_sequences(self._screen_file),
            'err': not self.params.get('no_color') and supports_terminal_sequences(self._err_file),
        }

        if sys.version_info < (3, 6):
            self.report_warning(
                'Python version %d.%d is not supported! Please update to Python 3.6 or above' % sys.version_info[:2])

        if self.params.get('allow_unplayable_formats'):
            self.report_warning(
                f'You have asked for {self._format_err(""UNPLAYABLE"", self.Styles.EMPHASIS)} formats to be listed/downloaded. '
                'This is a developer option intended for debugging. \n'
                '         If you experience any issues while using this option, '
                f'{self._format_err(""DO NOT"", self.Styles.ERROR)} open a bug report')

        def check_deprecated(param, option, suggestion):
            if self.params.get(param) is not None:
                self.report_warning('%s is deprecated. Use %s instead' % (option, suggestion))
                return True
            return False

        if check_deprecated('cn_verification_proxy', '--cn-verification-proxy', '--geo-verification-proxy'):
            if self.params.get('geo_verification_proxy') is None:
                self.params['geo_verification_proxy'] = self.params['cn_verification_proxy']

        check_deprecated('autonumber', '--auto-number', '-o ""%(autonumber)s-%(title)s.%(ext)s""')
        check_deprecated('usetitle', '--title', '-o ""%(title)s-%(id)s.%(ext)s""')
        check_deprecated('useid', '--id', '-o ""%(id)s.%(ext)s""')

        for msg in self.params.get('_warnings', []):
            self.report_warning(msg)
        for msg in self.params.get('_deprecation_warnings', []):
            self.deprecation_warning(msg)

        if 'list-formats' in self.params.get('compat_opts', []):
            self.params['listformats_table'] = False

        if 'overwrites' not in self.params and self.params.get('nooverwrites') is not None:
            # nooverwrites was unnecessarily changed to overwrites
            # in 0c3d0f51778b153f65c21906031c2e091fcfb641
            # This ensures compatibility with both keys
            self.params['overwrites'] = not self.params['nooverwrites']
        elif self.params.get('overwrites') is None:
            self.params.pop('overwrites', None)
        else:
            self.params['nooverwrites'] = not self.params['overwrites']

        if params.get('bidi_workaround', False):
            try:
                import pty
                master, slave = pty.openpty()
                width = compat_get_terminal_size().columns
                if width is None:
                    width_args = []
                else:
                    width_args = ['-w', str(width)]
                sp_kwargs = dict(
                    stdin=subprocess.PIPE,
                    stdout=slave,
                    stderr=self._err_file)
                try:
                    self._output_process = Popen(['bidiv'] + width_args, **sp_kwargs)
                except OSError:
                    self._output_process = Popen(['fribidi', '-c', 'UTF-8'] + width_args, **sp_kwargs)
                self._output_channel = os.fdopen(master, 'rb')
            except OSError as ose:
                if ose.errno == errno.ENOENT:
                    self.report_warning(
                        'Could not find fribidi executable, ignoring --bidi-workaround. '
                        'Make sure that  fribidi  is an executable file in one of the directories in your $PATH.')
                else:
                    raise

        if (sys.platform != 'win32'
                and sys.getfilesystemencoding() in ['ascii', 'ANSI_X3.4-1968']
                and not params.get('restrictfilenames', False)):
            # Unicode filesystem API will throw errors (#1474, #13027)
            self.report_warning(
                'Assuming --restrict-filenames since file system encoding '
                'cannot encode all characters. '
                'Set the LC_ALL environment variable to fix this.')
            self.params['restrictfilenames'] = True

        self.outtmpl_dict = self.parse_outtmpl()

        # Creating format selector here allows us to catch syntax errors before the extraction
        self.format_selector = (
            None if self.params.get('format') is None
            else self.params['format'] if callable(self.params['format'])
            else self.build_format_selector(self.params['format']))

        self._setup_opener()

        if auto_init:
            if auto_init != 'no_verbose_header':
                self.print_debug_header()
            self.add_default_info_extractors()

        for pp_def_raw in self.params.get('postprocessors', []):
            pp_def = dict(pp_def_raw)
            when = pp_def.pop('when', 'post_process')
            pp_class = get_postprocessor(pp_def.pop('key'))
            pp = pp_class(self, **compat_kwargs(pp_def))
            self.add_post_processor(pp, when=when)

        hooks = {
            'post_hooks': self.add_post_hook,
            'progress_hooks': self.add_progress_hook,
            'postprocessor_hooks': self.add_postprocessor_hook,
        }
        for opt, fn in hooks.items():
            for ph in self.params.get(opt, []):
                fn(ph)

        register_socks_protocols()

        def preload_download_archive(fn):
            """"""Preload the archive, if any is specified""""""
            if fn is None:
                return False
            self.write_debug(f'Loading archive file {fn!r}')
            try:
                with locked_file(fn, 'r', encoding='utf-8') as archive_file:
                    for line in archive_file:
                        self.archive.add(line.strip())
            except IOError as ioe:
                if ioe.errno != errno.ENOENT:
                    raise
                return False
            return True

        self.archive = set()
        preload_download_archive(self.params.get('download_archive'))","self.archive = set()
for line in archive_file:
    self.archive.add(line.strip())",self.archive = {line.strip() for line in archive_file}
watchdog,https://github.com/gorakhargosh/watchdog/tree/master/src/watchdog/utils/dirsnapshot.py,DirectorySnapshotDiff,__init__$82,"def __init__(self, ref, snapshot, ignore_device=False):
        created = snapshot.paths - ref.paths
        deleted = ref.paths - snapshot.paths

        if ignore_device:
            def get_inode(directory, full_path):
                return directory.inode(full_path)[0]
        else:
            def get_inode(directory, full_path):
                return directory.inode(full_path)

        # check that all unchanged paths have the same inode
        for path in ref.paths & snapshot.paths:
            if get_inode(ref, path) != get_inode(snapshot, path):
                created.add(path)
                deleted.add(path)

        # find moved paths
        moved = set()
        for path in set(deleted):
            inode = ref.inode(path)
            new_path = snapshot.path(inode)
            if new_path:
                # file is not deleted but moved
                deleted.remove(path)
                moved.add((path, new_path))

        for path in set(created):
            inode = snapshot.inode(path)
            old_path = ref.path(inode)
            if old_path:
                created.remove(path)
                moved.add((old_path, path))

        # find modified paths
        # first check paths that have not moved
        modified = set()
        for path in ref.paths & snapshot.paths:
            if get_inode(ref, path) == get_inode(snapshot, path):
                if ref.mtime(path) != snapshot.mtime(path) or ref.size(path) != snapshot.size(path):
                    modified.add(path)

        for (old_path, new_path) in moved:
            if ref.mtime(old_path) != snapshot.mtime(new_path) or ref.size(old_path) != snapshot.size(new_path):
                modified.add(old_path)

        self._dirs_created = [path for path in created if snapshot.isdir(path)]
        self._dirs_deleted = [path for path in deleted if ref.isdir(path)]
        self._dirs_modified = [path for path in modified if ref.isdir(path)]
        self._dirs_moved = [(frm, to) for (frm, to) in moved if ref.isdir(frm)]

        self._files_created = list(created - set(self._dirs_created))
        self._files_deleted = list(deleted - set(self._dirs_deleted))
        self._files_modified = list(modified - set(self._dirs_modified))
        self._files_moved = list(moved - set(self._dirs_moved))","modified = set()
for path in ref.paths & snapshot.paths:
    if get_inode(ref, path) == get_inode(snapshot, path):
        if ref.mtime(path) != snapshot.mtime(path) or ref.size(path) != snapshot.size(path):
            modified.add(path)","modified = {path for path in ref.paths & snapshot.paths if get_inode(ref, path) == get_inode(snapshot, path) if ref.mtime(path) != snapshot.mtime(path) or ref.size(path) != snapshot.size(path)}"
weblate,https://github.com/WeblateOrg/weblate/tree/master/weblate/checks/icu.py,ICUMessageFormatCheck,check_bad_submessage$393,"def check_bad_submessage(self, result, name, data, src_data, flags):
        """"""Detect any bad sub-message selectors.""""""
        if ""-submessage_selectors"" in flags:
            return

        bad = set()

        # We also want to check individual select choices.
        if (
            src_data
            and ""select"" in data[""types""]
            and ""select"" in src_data[""types""]
            and ""choices"" in data
            and ""choices"" in src_data
        ):
            choices = data[""choices""]
            src_choices = src_data[""choices""]

            for selector in choices:
                if selector not in src_choices:
                    bad.add(selector)

        if bad:
            result[""bad_submessage""].append([name, bad])","bad = set()
for selector in choices:
    if selector not in src_choices:
        bad.add(selector)",bad = {selector for selector in choices if selector not in src_choices}
ShuiZe_0x727,https://github.com/0x727/ShuiZe_0x727/tree/master/Plugins/infoGather/subdomain/Sublist3r/sublist3r.py,,main$866,"def main(domain, threads, savefile, ports, silent, verbose, enable_bruteforce, engines):
    bruteforce_list = set()
    search_list = set()

    # subdomains_queue = list()
    if is_windows:
        subdomains_queue = list()
    else:
        # subdomains_queue = multiprocessing.Manager().list()
        return []

    # Check Bruteforce Status
    if enable_bruteforce or enable_bruteforce is None:
        enable_bruteforce = True

    # Validate domain
    domain_check = re.compile(""^(http|https)?[a-zA-Z0-9]+([\-\.]{1}[a-zA-Z0-9]+)*\.[a-zA-Z]{2,}$"")
    if not domain_check.match(domain):
        if not silent:
            print(R + ""Error: Please enter a valid domain"" + W)
        return []

    if not domain.startswith('http://') or not domain.startswith('https://'):
        domain = 'http://' + domain

    parsed_domain = urlparse.urlparse(domain)

    if not silent:
        pass
        # print(B + ""[-] Enumerating subdomains now for %s"" % parsed_domain.netloc + W)

    if verbose and not silent:
        pass
        # print(Y + ""[-] verbosity is enabled, will show the subdomains results in realtime"" + W)

    supported_engines = {'baidu': BaiduEnum,
                         'yahoo': YahooEnum,
                         # 'google': GoogleEnum,
                         'bing': BingEnum,
                         'ask': AskEnum,
                         'netcraft': NetcraftEnum,
                         'dnsdumpster': DNSdumpster,
                         'virustotal': Virustotal,
                         'threatcrowd': ThreatCrowd,
                         'ssl': CrtSearch,
                         'passivedns': PassiveDNS
                         }

    chosenEnums = []

    if engines is None:
        chosenEnums = [
            BaiduEnum, YahooEnum, GoogleEnum, BingEnum, AskEnum,
            NetcraftEnum, DNSdumpster, Virustotal, ThreatCrowd,
            CrtSearch, PassiveDNS
        ]
    else:
        engines = engines.split(',')
        for engine in engines:
            if engine.lower() in supported_engines:
                chosenEnums.append(supported_engines[engine.lower()])

    # Start the engines enumeration
    enums = [enum(domain, [], q=subdomains_queue, silent=silent, verbose=verbose) for enum in chosenEnums]
    for enum in enums:
        enum.start()
    for enum in enums:
        enum.join()

    subdomains = set(subdomains_queue)
    for subdomain in subdomains:
        search_list.add(subdomain)

    if enable_bruteforce:
        if not silent:
            print(G + ""[-] Starting bruteforce module now using subbrute.."" + W)
        record_type = False
        path_to_file = os.path.dirname(os.path.realpath(__file__))
        subs = os.path.join(path_to_file, 'subbrute', 'names.txt')
        resolvers = os.path.join(path_to_file, 'subbrute', 'resolvers.txt')
        process_count = threads
        output = False
        json_output = False
        bruteforce_list = subbrute.print_target(parsed_domain.netloc, record_type, subs, resolvers, process_count, output, json_output, search_list, verbose)

    subdomains = search_list.union(bruteforce_list)

    if subdomains:
        subdomains = sorted(subdomains, key=subdomain_sorting_key)

        if savefile:
            write_file(savefile, subdomains)

        if not silent:
            print(Y + ""[+] Total Unique Subdomains Found: %s"" % len(subdomains) + W)

        if ports:
            if not silent:
                print(G + ""[-] Start port scan now for the following ports: %s%s"" % (Y, ports) + W)
            ports = ports.split(',')
            pscan = portscan(subdomains, ports)
            pscan.run()

        elif not silent:
            for subdomain in subdomains:
                pass
                # print(G + subdomain + W)
    return subdomains","search_list = set()
for subdomain in subdomains:
    search_list.add(subdomain)",search_list = {subdomain for subdomain in subdomains}
pandas,https://github.com/pandas-dev/pandas/tree/master/pandas/tests/plotting/test_datetimelike.py,TestTSPlot,test_secondary_legend$1122,"def test_secondary_legend(self):
        fig = self.plt.figure()
        ax = fig.add_subplot(211)

        # ts
        df = tm.makeTimeDataFrame()
        df.plot(secondary_y=[""A"", ""B""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert leg.get_texts()[0].get_text() == ""A (right)""
        assert leg.get_texts()[1].get_text() == ""B (right)""
        assert leg.get_texts()[2].get_text() == ""C""
        assert leg.get_texts()[3].get_text() == ""D""
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4
        self.plt.close(fig)

        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        df.plot(secondary_y=[""A"", ""C""], mark_right=False, ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert leg.get_texts()[0].get_text() == ""A""
        assert leg.get_texts()[1].get_text() == ""B""
        assert leg.get_texts()[2].get_text() == ""C""
        assert leg.get_texts()[3].get_text() == ""D""
        self.plt.close(fig)

        fig, ax = self.plt.subplots()
        df.plot(kind=""bar"", secondary_y=[""A""], ax=ax)
        leg = ax.get_legend()
        assert leg.get_texts()[0].get_text() == ""A (right)""
        assert leg.get_texts()[1].get_text() == ""B""
        self.plt.close(fig)

        fig, ax = self.plt.subplots()
        df.plot(kind=""bar"", secondary_y=[""A""], mark_right=False, ax=ax)
        leg = ax.get_legend()
        assert leg.get_texts()[0].get_text() == ""A""
        assert leg.get_texts()[1].get_text() == ""B""
        self.plt.close(fig)

        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        df = tm.makeTimeDataFrame()
        ax = df.plot(secondary_y=[""C"", ""D""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4
        self.plt.close(fig)

        # non-ts
        df = tm.makeDataFrame()
        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        ax = df.plot(secondary_y=[""A"", ""B""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4
        self.plt.close()

        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        ax = df.plot(secondary_y=[""C"", ""D""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4","colors = set()
for line in leg.get_lines():
    colors.add(line.get_color())",colors = {line.get_color() for line in leg.get_lines()}
pandas,https://github.com/pandas-dev/pandas/tree/master/pandas/tests/plotting/test_datetimelike.py,TestTSPlot,test_secondary_legend$1122,"def test_secondary_legend(self):
        fig = self.plt.figure()
        ax = fig.add_subplot(211)

        # ts
        df = tm.makeTimeDataFrame()
        df.plot(secondary_y=[""A"", ""B""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert leg.get_texts()[0].get_text() == ""A (right)""
        assert leg.get_texts()[1].get_text() == ""B (right)""
        assert leg.get_texts()[2].get_text() == ""C""
        assert leg.get_texts()[3].get_text() == ""D""
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4
        self.plt.close(fig)

        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        df.plot(secondary_y=[""A"", ""C""], mark_right=False, ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert leg.get_texts()[0].get_text() == ""A""
        assert leg.get_texts()[1].get_text() == ""B""
        assert leg.get_texts()[2].get_text() == ""C""
        assert leg.get_texts()[3].get_text() == ""D""
        self.plt.close(fig)

        fig, ax = self.plt.subplots()
        df.plot(kind=""bar"", secondary_y=[""A""], ax=ax)
        leg = ax.get_legend()
        assert leg.get_texts()[0].get_text() == ""A (right)""
        assert leg.get_texts()[1].get_text() == ""B""
        self.plt.close(fig)

        fig, ax = self.plt.subplots()
        df.plot(kind=""bar"", secondary_y=[""A""], mark_right=False, ax=ax)
        leg = ax.get_legend()
        assert leg.get_texts()[0].get_text() == ""A""
        assert leg.get_texts()[1].get_text() == ""B""
        self.plt.close(fig)

        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        df = tm.makeTimeDataFrame()
        ax = df.plot(secondary_y=[""C"", ""D""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4
        self.plt.close(fig)

        # non-ts
        df = tm.makeDataFrame()
        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        ax = df.plot(secondary_y=[""A"", ""B""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4
        self.plt.close()

        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        ax = df.plot(secondary_y=[""C"", ""D""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4","colors = set()
for line in leg.get_lines():
    colors.add(line.get_color())",colors = {line.get_color() for line in leg.get_lines()}
pandas,https://github.com/pandas-dev/pandas/tree/master/pandas/tests/plotting/test_datetimelike.py,TestTSPlot,test_secondary_legend$1122,"def test_secondary_legend(self):
        fig = self.plt.figure()
        ax = fig.add_subplot(211)

        # ts
        df = tm.makeTimeDataFrame()
        df.plot(secondary_y=[""A"", ""B""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert leg.get_texts()[0].get_text() == ""A (right)""
        assert leg.get_texts()[1].get_text() == ""B (right)""
        assert leg.get_texts()[2].get_text() == ""C""
        assert leg.get_texts()[3].get_text() == ""D""
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4
        self.plt.close(fig)

        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        df.plot(secondary_y=[""A"", ""C""], mark_right=False, ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert leg.get_texts()[0].get_text() == ""A""
        assert leg.get_texts()[1].get_text() == ""B""
        assert leg.get_texts()[2].get_text() == ""C""
        assert leg.get_texts()[3].get_text() == ""D""
        self.plt.close(fig)

        fig, ax = self.plt.subplots()
        df.plot(kind=""bar"", secondary_y=[""A""], ax=ax)
        leg = ax.get_legend()
        assert leg.get_texts()[0].get_text() == ""A (right)""
        assert leg.get_texts()[1].get_text() == ""B""
        self.plt.close(fig)

        fig, ax = self.plt.subplots()
        df.plot(kind=""bar"", secondary_y=[""A""], mark_right=False, ax=ax)
        leg = ax.get_legend()
        assert leg.get_texts()[0].get_text() == ""A""
        assert leg.get_texts()[1].get_text() == ""B""
        self.plt.close(fig)

        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        df = tm.makeTimeDataFrame()
        ax = df.plot(secondary_y=[""C"", ""D""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4
        self.plt.close(fig)

        # non-ts
        df = tm.makeDataFrame()
        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        ax = df.plot(secondary_y=[""A"", ""B""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4
        self.plt.close()

        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        ax = df.plot(secondary_y=[""C"", ""D""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4","colors = set()
for line in leg.get_lines():
    colors.add(line.get_color())",colors = {line.get_color() for line in leg.get_lines()}
pandas,https://github.com/pandas-dev/pandas/tree/master/pandas/tests/plotting/test_datetimelike.py,TestTSPlot,test_secondary_legend$1122,"def test_secondary_legend(self):
        fig = self.plt.figure()
        ax = fig.add_subplot(211)

        # ts
        df = tm.makeTimeDataFrame()
        df.plot(secondary_y=[""A"", ""B""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert leg.get_texts()[0].get_text() == ""A (right)""
        assert leg.get_texts()[1].get_text() == ""B (right)""
        assert leg.get_texts()[2].get_text() == ""C""
        assert leg.get_texts()[3].get_text() == ""D""
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4
        self.plt.close(fig)

        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        df.plot(secondary_y=[""A"", ""C""], mark_right=False, ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert leg.get_texts()[0].get_text() == ""A""
        assert leg.get_texts()[1].get_text() == ""B""
        assert leg.get_texts()[2].get_text() == ""C""
        assert leg.get_texts()[3].get_text() == ""D""
        self.plt.close(fig)

        fig, ax = self.plt.subplots()
        df.plot(kind=""bar"", secondary_y=[""A""], ax=ax)
        leg = ax.get_legend()
        assert leg.get_texts()[0].get_text() == ""A (right)""
        assert leg.get_texts()[1].get_text() == ""B""
        self.plt.close(fig)

        fig, ax = self.plt.subplots()
        df.plot(kind=""bar"", secondary_y=[""A""], mark_right=False, ax=ax)
        leg = ax.get_legend()
        assert leg.get_texts()[0].get_text() == ""A""
        assert leg.get_texts()[1].get_text() == ""B""
        self.plt.close(fig)

        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        df = tm.makeTimeDataFrame()
        ax = df.plot(secondary_y=[""C"", ""D""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4
        self.plt.close(fig)

        # non-ts
        df = tm.makeDataFrame()
        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        ax = df.plot(secondary_y=[""A"", ""B""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4
        self.plt.close()

        fig = self.plt.figure()
        ax = fig.add_subplot(211)
        ax = df.plot(secondary_y=[""C"", ""D""], ax=ax)
        leg = ax.get_legend()
        assert len(leg.get_lines()) == 4
        assert ax.right_ax.get_legend() is None
        colors = set()
        for line in leg.get_lines():
            colors.add(line.get_color())

        # TODO: color cycle problems
        assert len(colors) == 4","colors = set()
for line in leg.get_lines():
    colors.add(line.get_color())",colors = {line.get_color() for line in leg.get_lines()}
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_gcp/tests/test_resourcemanager.py,ProjectTest,test_project_iam_policy_value_filter$311,"def test_project_iam_policy_value_filter(self):
        factory = self.replay_flight_data('project-iam-policy')
        p = self.load_policy({
            'name': 'resource',
            'resource': 'gcp.project',
            'filters': [{
                'type': 'iam-policy',
                'doc':
                    {'key': 'bindings[*].members[]',
                    'op': 'contains',
                    'value': 'user:abc@gmail.com'}
            }]},
            session_factory=factory)
        resources = p.run()
        self.assertEqual(len(resources), 3)

        for resource in resources:
            self.assertTrue('c7n:iamPolicy' in resource)
            bindings = resource['c7n:iamPolicy']['bindings']
            members = set()
            for binding in bindings:
                for member in binding['members']:
                    members.add(member)
            self.assertTrue('user:abc@gmail.com' in members)","members = set()
for binding in bindings:
    for member in binding['members']:
        members.add(member)",members = {member for binding in bindings for member in binding['members']}
salt,https://github.com/saltstack/salt/tree/master/salt/modules/win_service.py,,get_disabled$142,"def get_disabled():
    """"""
    Return a list of disabled services. Disabled is defined as a service that is
    marked 'Disabled' or 'Manual'.

    Returns:
        list: A list of disabled services.

    CLI Example:

    .. code-block:: bash

        salt '*' service.get_disabled
    """"""
    raw_services = _get_services()
    services = set()
    for service in raw_services:
        if info(service[""ServiceName""])[""StartType""] in [""Manual"", ""Disabled""]:
            services.add(service[""ServiceName""])

    return sorted(services)","services = set()
for service in raw_services:
    if info(service['ServiceName'])['StartType'] in ['Manual', 'Disabled']:
        services.add(service['ServiceName'])","services = {service['ServiceName'] for service in raw_services if info(service['ServiceName'])['StartType'] in ['Manual', 'Disabled']}"
LinOTP,https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/resolver.py,,get_admin_resolvers$285,"def get_admin_resolvers():
    """""" """"""
    admin_realm_name = current_app.config[""ADMIN_REALM_NAME""].lower()

    admin_realm_definition = getRealms(admin_realm_name).get(
        admin_realm_name, {}
    )

    if not admin_realm_definition:
        return []

    admin_resolvers = set()
    for resolver_spec in admin_realm_definition[""useridresolver""]:
        admin_resolvers.add(resolver_spec.rpartition(""."")[2])

    return list(admin_resolvers)","admin_resolvers = set()
for resolver_spec in admin_realm_definition['useridresolver']:
    admin_resolvers.add(resolver_spec.rpartition('.')[2])",admin_resolvers = {resolver_spec.rpartition('.')[2] for resolver_spec in admin_realm_definition['useridresolver']}
taurus,https://github.com/Blazemeter/taurus/tree/master/bzt/modules/blazemeter/blazemeter_reporter.py,BlazeMeterUploader,__get_jtls_and_more$177,"def __get_jtls_and_more(self):
        """"""
        Compress all files in artifacts dir to single zipfile
        :rtype: (io.BytesIO,dict)
        """"""
        mfile = BytesIO()
        listing = {}

        logs = set()
        for handler in self.engine.log.parent.handlers:
            if isinstance(handler, logging.FileHandler):
                logs.add(handler.baseFilename)

        max_file_size = self.settings.get('artifact-upload-size-limit', 10) * 1024 * 1024  # 10MB
        with zipfile.ZipFile(mfile, mode='w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as zfh:
            for root, _, files in os.walk(self.engine.artifacts_dir):
                for filename in files:
                    full_path = os.path.join(root, filename)
                    if full_path in logs:
                        logs.remove(full_path)

                    fsize = os.path.getsize(full_path)
                    if fsize <= max_file_size:
                        zfh.write(full_path, os.path.join(os.path.relpath(root, self.engine.artifacts_dir), filename))
                        listing[full_path] = fsize
                    else:
                        msg = ""File %s exceeds maximum size quota of %s and won't be included into upload""
                        self.log.warning(msg, filename, max_file_size)

            for filename in logs:  # upload logs unconditionally
                zfh.write(filename, os.path.basename(filename))
                listing[filename] = os.path.getsize(filename)
        return mfile, listing","logs = set()
for handler in self.engine.log.parent.handlers:
    if isinstance(handler, logging.FileHandler):
        logs.add(handler.baseFilename)","logs = {handler.baseFilename for handler in self.engine.log.parent.handlers if isinstance(handler, logging.FileHandler)}"
toil,https://github.com/DataBiosphere/toil/tree/master/src/toil/test/batchSystems/batchSystemTest.py,MaxCoresSingleMachineBatchSystemTest,test$867,"def test(self):
        # We'll use fractions to avoid rounding errors. Remember that not every fraction can be
        # represented as a floating point number.
        F = Fraction
        # This test isn't general enough to cover every possible value of minCores in
        # SingleMachineBatchSystem. Instead we hard-code a value and assert it.
        minCores = F(1, 10)
        self.assertEqual(float(minCores), SingleMachineBatchSystem.minCores)
        for maxCores in {F(minCores), minCores * 10, F(1), F(numCores, 2), F(numCores)}:
            for coresPerJob in {F(minCores), F(minCores * 10), F(1), F(maxCores, 2), F(maxCores)}:
                for load in (F(1, 10), F(1), F(10)):
                    jobs = int(maxCores / coresPerJob * load)
                    if jobs >= 1 and minCores <= coresPerJob < maxCores:
                        self.assertEqual(maxCores, float(maxCores))
                        bs = SingleMachineBatchSystem(
                            config=hidden.AbstractBatchSystemTest.createConfig(),
                            maxCores=float(maxCores),
                            # Ensure that memory or disk requirements don't get in the way.
                            maxMemory=jobs * 10,
                            maxDisk=jobs * 10)
                        try:
                            jobIds = set()
                            for i in range(0, int(jobs)):
                                jobIds.add(bs.issueBatchJob(JobDescription(command=self.scriptCommand(),
                                                                           requirements=dict(
                                                                               cores=float(coresPerJob),
                                                                               memory=1, disk=1,
                                                                               accelerators=[],
                                                                               preemptable=preemptable),
                                                                           jobName=str(i), unitName='')))
                            self.assertEqual(len(jobIds), jobs)
                            while jobIds:
                                job = bs.getUpdatedBatchJob(maxWait=10)
                                self.assertIsNotNone(job)
                                jobId, status, wallTime = job.jobID, job.exitStatus, job.wallTime
                                self.assertEqual(status, 0)
                                # would raise KeyError on absence
                                jobIds.remove(jobId)
                        finally:
                            bs.shutdown()
                        concurrentTasks, maxConcurrentTasks = getCounters(self.counterPath)
                        self.assertEqual(concurrentTasks, 0)
                        logger.info('maxCores: {maxCores}, '
                                 'coresPerJob: {coresPerJob}, '
                                 'load: {load}'.format(**locals()))
                        # This is the key assertion:
                        expectedMaxConcurrentTasks = min(maxCores // coresPerJob, jobs)
                        self.assertEqual(maxConcurrentTasks, expectedMaxConcurrentTasks)
                        resetCounters(self.counterPath)","jobIds = set()
for i in range(0, int(jobs)):
    jobIds.add(bs.issueBatchJob(JobDescription(command=self.scriptCommand(), requirements=dict(cores=float(coresPerJob), memory=1, disk=1, accelerators=[], preemptable=preemptable), jobName=str(i), unitName='')))","jobIds = {bs.issueBatchJob(JobDescription(command=self.scriptCommand(), requirements=dict(cores=float(coresPerJob), memory=1, disk=1, accelerators=[], preemptable=preemptable), jobName=str(i), unitName='')) for i in range(0, int(jobs))}"
saleor,https://github.com/saleor/saleor/tree/master/saleor/graphql/attribute/utils.py,AttributeAssignmentMixin,_resolve_attribute_nodes$99,"def _resolve_attribute_nodes(
        cls,
        qs: ""QuerySet"",
        error_class,
        *,
        global_ids: List[str],
        pks: Iterable[int],
    ):
        """"""Retrieve attributes nodes from given global IDs.""""""
        qs = qs.filter(pk__in=pks)
        nodes: List[attribute_models.Attribute] = list(qs)

        if not nodes:
            raise ValidationError(
                (f""Could not resolve to a node: ids={global_ids}.""),
                code=error_class.NOT_FOUND.value,
            )

        nodes_pk_list = set()
        for node in nodes:
            nodes_pk_list.add(node.pk)

        for pk, global_id in zip(pks, global_ids):
            if pk not in nodes_pk_list:
                raise ValidationError(
                    f""Could not resolve {global_id!r} to Attribute"",
                    code=error_class.NOT_FOUND.value,
                )

        return nodes","nodes_pk_list = set()
for node in nodes:
    nodes_pk_list.add(node.pk)",nodes_pk_list = {node.pk for node in nodes}
nmt,https://github.com/tensorflow/nmt/tree/master/nmt/scripts/rouge.py,,_get_ngrams$19,"def _get_ngrams(n, text):
  """"""Calcualtes n-grams.

  Args:
    n: which n-grams to calculate
    text: An array of tokens

  Returns:
    A set of n-grams
  """"""
  ngram_set = set()
  text_length = len(text)
  max_index_ngram_start = text_length - n
  for i in range(max_index_ngram_start + 1):
    ngram_set.add(tuple(text[i:i + n]))
  return ngram_set","ngram_set = set()
for i in range(max_index_ngram_start + 1):
    ngram_set.add(tuple(text[i:i + n]))",ngram_set = {tuple(text[i:i + n]) for i in range(max_index_ngram_start + 1)}
OctoPrint,https://github.com/OctoPrint/OctoPrint/tree/master//versioneer.py,,do_setup$2137,"def do_setup():
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (
        EnvironmentError,
        configparser.NoSectionError,
        configparser.NoOptionError,
    ) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"", file=sys.stderr)
            with io.open(os.path.join(root, ""setup.cfg""), ""at"", encoding=""utf-8"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with io.open(cfg.versionfile_source, ""wt"", encoding=""utf-8"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(
            LONG
            % {
                ""DOLLAR"": ""$"",
                ""STYLE"": cfg.style,
                ""TAG_PREFIX"": cfg.tag_prefix,
                ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
                ""LOOKUP_FILE"": cfg.lookupfile,
            }
        )

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with io.open(ipy, ""rt"", encoding=""utf-8"") as f:
                old = f.read()
        except EnvironmentError:
            old = """"
        if ""from ._version import get_versions"" not in old:
            print("" appending to %s"" % ipy)
            with io.open(ipy, ""at"", encoding=""utf-8"") as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with io.open(manifest_in, ""r"") as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with io.open(manifest_in, ""at"", encoding=""utf-8"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print(
            "" appending versionfile_source ('%s') to MANIFEST.in"" % cfg.versionfile_source
        )
        with io.open(manifest_in, ""at"", encoding=""utf-8"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-time keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","simple_includes = set()
for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}
petastorm,https://github.com/uber/petastorm/tree/master/petastorm/tests/test_predicates.py,,test_and_argegarion$86,"def test_and_argegarion(all_values):
    for values1 in [{'guid_0', 'guid_1'}, {'guid_3', 'guid_6', 'guid_20'}, {'guid_2'}]:
        for values2 in [{'guid_2', 'guid_1'}, {'guid_5', 'guid_9'}, {'guid_2'}]:
            test_predicate = in_reduce(
                [in_set(values1, 'volume_guid'), in_set(values2, 'volume_guid')], all)
            included_values = set()
            for val in all_values:
                if test_predicate.do_include({'volume_guid': val}):
                    included_values.add(val)
            assert included_values == values1.intersection(values2)","included_values = set()
for val in all_values:
    if test_predicate.do_include({'volume_guid': val}):
        included_values.add(val)",included_values = {val for val in all_values if test_predicate.do_include({'volume_guid': val})}
azure-cli,https://github.com/Azure/azure-cli/tree/master/tools/automation/cli_linter/linter.py,Linter,__init__$16,"def __init__(self, command_loader=None, help_file_entries=None, loaded_help=None):
        self._all_yaml_help = help_file_entries
        self._loaded_help = loaded_help
        self._command_loader = command_loader
        self._parameters = {}
        self._help_file_entries = set(help_file_entries.keys())
        self._command_parser = command_loader.cli_ctx.invocation.parser
        for command_name, command in self._command_loader.command_table.items():
            self._parameters[command_name] = set()
            for name, param in command.arguments.items():
                self._parameters[command_name].add(name)","self._parameters[command_name] = set()
for (name, param) in command.arguments.items():
    self._parameters[command_name].add(name)","self._parameters[command_name] = {name for (name, param) in command.arguments.items()}"
petastorm,https://github.com/uber/petastorm/tree/master/petastorm/tests/test_predicates.py,,test_pseudorandom_split_on_integer_field$123,"def test_pseudorandom_split_on_integer_field():
    split_list = [0.3, 0.4, 0.1, 0.0, 0.2]
    int_values = list(range(1000))
    values_num = len(int_values)
    for idx, _ in enumerate(split_list):
        test_predicate = in_pseudorandom_split(split_list, idx, 'int_partitioning_field')
        included_values = set()
        for val in int_values:
            if test_predicate.do_include({'int_partitioning_field': val}):
                included_values.add(val)
        expected_num = values_num * split_list[idx]
        assert pytest.approx(len(included_values), expected_num * 0.1) == expected_num","included_values = set()
for val in int_values:
    if test_predicate.do_include({'int_partitioning_field': val}):
        included_values.add(val)",included_values = {val for val in int_values if test_predicate.do_include({'int_partitioning_field': val})}
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/transpiler/passes/optimization/collect_multiqubit_blocks.py,CollectMultiQBlocks,run$88,"def run(self, dag):
        """"""Run the CollectMultiQBlocks pass on `dag`.

        The blocks contain ""op"" nodes in topological sort order
        such that all gates in a block act on the same set of
        qubits and are adjacent in the circuit.

        The blocks are built by examining predecessors and successors of
        ""cx"" gates in the circuit. u1, u2, u3, cx, id gates will be included.

        After the execution, ``property_set['block_list']`` is set to
        a list of tuples of ``DAGNode`` objects
        """"""

        self.parent = {}  # reset all variables on run
        self.bit_groups = {}
        self.gate_groups = {}

        block_list = []

        def collect_key(x):
            """"""special key function for topological ordering.
            Heuristic for this is to push all gates involving measurement
            or barriers, etc. as far back as possible (because they force
            blocks to end). After that, we process gates in order of lowest
            number of qubits acted on to largest number of qubits acted on
            because these have less chance of increasing the size of blocks
            The key also processes all the non operation notes first so that
            input nodes do not mess with the top sort of op nodes
            """"""
            if isinstance(x, DAGInNode):
                return ""a""
            if not isinstance(x, DAGOpNode):
                return ""d""
            if isinstance(x.op, Gate):
                if x.op.is_parameterized() or getattr(x.op, ""condition"", None) is not None:
                    return ""c""
                return ""b"" + chr(ord(""a"") + len(x.qargs))
            return ""d""

        op_nodes = dag.topological_op_nodes(key=collect_key)
        qubit_indices = {bit: index for index, bit in enumerate(dag.qubits)}

        for nd in op_nodes:
            can_process = True
            makes_too_big = False

            # check if the node is a gate and if it is parameterized
            if (
                getattr(nd.op, ""condition"", None) is not None
                or nd.op.is_parameterized()
                or not isinstance(nd.op, Gate)
            ):
                can_process = False

            cur_qubits = {qubit_indices[bit] for bit in nd.qargs}

            if can_process:
                # if the gate is valid, check if grouping up the bits
                # in the gate would fit within our desired max size
                c_tops = set()
                for bit in cur_qubits:
                    c_tops.add(self.find_set(bit))
                tot_size = 0
                for group in c_tops:
                    tot_size += len(self.bit_groups[group])
                if tot_size > self.max_block_size:
                    makes_too_big = True

            if not can_process:
                # resolve the case where we cannot process this node
                for bit in cur_qubits:
                    # create a gate out of me
                    bit = self.find_set(bit)
                    if len(self.gate_groups[bit]) == 0:
                        continue
                    block_list.append(self.gate_groups[bit][:])
                    cur_set = set(self.bit_groups[bit])
                    for v in cur_set:
                        # reset this bit
                        self.parent[v] = v
                        self.bit_groups[v] = [v]
                        self.gate_groups[v] = []

            if makes_too_big:
                # adding in all of the new qubits would make the group too big
                # we must block off sub portions of the groups until the new
                # group would no longer be too big
                savings = {}
                tot_size = 0
                for bit in cur_qubits:
                    top = self.find_set(bit)
                    if top in savings:
                        savings[top] = savings[top] - 1
                    else:
                        savings[top] = len(self.bit_groups[top]) - 1
                        tot_size += len(self.bit_groups[top])
                slist = []
                for item, value in savings.items():
                    slist.append((value, item))
                slist.sort(reverse=True)
                savings_need = tot_size - self.max_block_size
                for item in slist:
                    # remove groups until the size created would be acceptable
                    # start with blocking out the group that would decrease
                    # the new size the most. This heuristic for which blocks we
                    # create does not necessarily give the optimal blocking. Other
                    # heuristics may be worth considering
                    if savings_need > 0:
                        savings_need = savings_need - item[0]
                        if len(self.gate_groups[item[1]]) >= 1:
                            block_list.append(self.gate_groups[item[1]][:])
                        cur_set = set(self.bit_groups[item[1]])
                        for v in cur_set:
                            self.parent[v] = v
                            self.bit_groups[v] = [v]
                            self.gate_groups[v] = []

            if can_process:
                # if the operation is a gate, either skip it if it is too large
                # or group up all of the qubits involved in the gate
                if len(cur_qubits) > self.max_block_size:
                    # gates acting on more qubits than max_block_size cannot
                    #   be a part of any block and thus we skip them here.
                    # we have already finalized the blocks involving the gate's
                    #   qubits in the above makes_too_big block
                    continue  # unable to be part of a group
                prev = -1
                for bit in cur_qubits:
                    if prev != -1:
                        self.union_set(prev, bit)
                    prev = bit
                self.gate_groups[self.find_set(prev)].append(nd)
        # need to turn all groups that still exist into their own blocks
        for index in self.parent:
            if self.parent[index] == index and len(self.gate_groups[index]) != 0:
                block_list.append(self.gate_groups[index][:])

        self.property_set[""block_list""] = block_list

        return dag","c_tops = set()
for bit in cur_qubits:
    c_tops.add(self.find_set(bit))",c_tops = {self.find_set(bit) for bit in cur_qubits}
supervisor,https://github.com/home-assistant/supervisor/tree/master/supervisor/supervisorctl.py,DefaultControllerPlugin,do_update$1171,"def do_update(self, arg):
        def log(name, message):
            self.ctl.output(""%s: %s"" % (name, message))

        supervisor = self.ctl.get_supervisor()
        try:
            result = supervisor.reloadConfig()
        except xmlrpclib.Fault as e:
            self.ctl.exitstatus = LSBInitExitStatuses.GENERIC
            if e.faultCode == xmlrpc.Faults.SHUTDOWN_STATE:
                self.ctl.output('ERROR: already shutting down')
                return
            else:
                raise

        added, changed, removed = result[0]
        valid_gnames = set(arg.split())

        # If all is specified treat it as if nothing was specified.
        if ""all"" in valid_gnames:
            valid_gnames = set()

        # If any gnames are specified we need to verify that they are
        # valid in order to print a useful error message.
        if valid_gnames:
            groups = set()
            for info in supervisor.getAllProcessInfo():
                groups.add(info['group'])
            # New gnames would not currently exist in this set so
            # add those as well.
            groups.update(added)

            for gname in valid_gnames:
                if gname not in groups:
                    self.ctl.output('ERROR: no such group: %s' % gname)
                    self.ctl.exitstatus = LSBInitExitStatuses.GENERIC

        for gname in removed:
            if valid_gnames and gname not in valid_gnames:
                continue
            results = supervisor.stopProcessGroup(gname)
            log(gname, ""stopped"")

            fails = [res for res in results
                     if res['status'] == xmlrpc.Faults.FAILED]
            if fails:
                self.ctl.output(""%s: %s"" % (gname, ""has problems; not removing""))
                self.ctl.exitstatus = LSBInitExitStatuses.GENERIC
                continue
            supervisor.removeProcessGroup(gname)
            log(gname, ""removed process group"")

        for gname in changed:
            if valid_gnames and gname not in valid_gnames:
                continue
            supervisor.stopProcessGroup(gname)
            log(gname, ""stopped"")

            supervisor.removeProcessGroup(gname)
            supervisor.addProcessGroup(gname)
            log(gname, ""updated process group"")

        for gname in added:
            if valid_gnames and gname not in valid_gnames:
                continue
            supervisor.addProcessGroup(gname)
            log(gname, ""added process group"")","groups = set()
for info in supervisor.getAllProcessInfo():
    groups.add(info['group'])",groups = {info['group'] for info in supervisor.getAllProcessInfo()}
nltk,https://github.com/nltk/nltk/tree/master/nltk/tgrep.py,,treepositions_no_leaves$973,"def treepositions_no_leaves(tree):
    """"""
    Returns all the tree positions in the given tree which are not
    leaf nodes.
    """"""
    treepositions = tree.treepositions()
    # leaves are treeposition tuples that are not prefixes of any
    # other treeposition
    prefixes = set()
    for pos in treepositions:
        for length in range(len(pos)):
            prefixes.add(pos[:length])
    return [pos for pos in treepositions if pos in prefixes]","prefixes = set()
for pos in treepositions:
    for length in range(len(pos)):
        prefixes.add(pos[:length])",prefixes = {pos[:length] for pos in treepositions for length in range(len(pos))}
astropy,https://github.com/astropy/astropy/tree/master/astropy/units/core.py,UnitBase,compose$1317,"def compose(
        self, equivalencies=[], units=None, max_depth=2, include_prefix_units=None
    ):
        """"""
        Return the simplest possible composite unit(s) that represent
        the given unit.  Since there may be multiple equally simple
        compositions of the unit, a list of units is always returned.

        Parameters
        ----------
        equivalencies : list of tuple
            A list of equivalence pairs to also list.  See
            :ref:`astropy:unit_equivalencies`.
            This list is in addition to possible global defaults set by, e.g.,
            `set_enabled_equivalencies`.
            Use `None` to turn off all equivalencies.

        units : set of `~astropy.units.Unit`, optional
            If not provided, any known units may be used to compose
            into.  Otherwise, ``units`` is a dict, module or sequence
            containing the units to compose into.

        max_depth : int, optional
            The maximum recursion depth to use when composing into
            composite units.

        include_prefix_units : bool, optional
            When `True`, include prefixed units in the result.
            Default is `True` if a sequence is passed in to ``units``,
            `False` otherwise.

        Returns
        -------
        units : list of `CompositeUnit`
            A list of candidate compositions.  These will all be
            equally simple, but it may not be possible to
            automatically determine which of the candidates are
            better.
        """"""
        # if units parameter is specified and is a sequence (list|tuple),
        # include_prefix_units is turned on by default.  Ex: units=[u.kpc]
        if include_prefix_units is None:
            include_prefix_units = isinstance(units, (list, tuple))

        # Pre-normalize the equivalencies list
        equivalencies = self._normalize_equivalencies(equivalencies)

        # The namespace of units to compose into should be filtered to
        # only include units with bases in common with self, otherwise
        # they can't possibly provide useful results.  Having too many
        # destination units greatly increases the search space.

        def has_bases_in_common(a, b):
            if len(a.bases) == 0 and len(b.bases) == 0:
                return True
            for ab in a.bases:
                for bb in b.bases:
                    if ab == bb:
                        return True
            return False

        def has_bases_in_common_with_equiv(unit, other):
            if has_bases_in_common(unit, other):
                return True
            for funit, tunit, a, b in equivalencies:
                if tunit is not None:
                    if unit._is_equivalent(funit):
                        if has_bases_in_common(tunit.decompose(), other):
                            return True
                    elif unit._is_equivalent(tunit):
                        if has_bases_in_common(funit.decompose(), other):
                            return True
                else:
                    if unit._is_equivalent(funit):
                        if has_bases_in_common(dimensionless_unscaled, other):
                            return True
            return False

        def filter_units(units):
            filtered_namespace = set()
            for tunit in units:
                if (
                    isinstance(tunit, UnitBase)
                    and (include_prefix_units or not isinstance(tunit, PrefixUnit))
                    and has_bases_in_common_with_equiv(decomposed, tunit.decompose())
                ):
                    filtered_namespace.add(tunit)
            return filtered_namespace

        decomposed = self.decompose()

        if units is None:
            units = filter_units(self._get_units_with_same_physical_type(equivalencies))
            if len(units) == 0:
                units = get_current_unit_registry().non_prefix_units
        elif isinstance(units, dict):
            units = set(filter_units(units.values()))
        elif inspect.ismodule(units):
            units = filter_units(vars(units).values())
        else:
            units = filter_units(_flatten_units_collection(units))

        def sort_results(results):
            if not len(results):
                return []

            # Sort the results so the simplest ones appear first.
            # Simplest is defined as ""the minimum sum of absolute
            # powers"" (i.e. the fewest bases), and preference should
            # be given to results where the sum of powers is positive
            # and the scale is exactly equal to 1.0
            results = list(results)
            results.sort(key=lambda x: np.abs(x.scale))
            results.sort(key=lambda x: np.sum(np.abs(x.powers)))
            results.sort(key=lambda x: np.sum(x.powers) < 0.0)
            results.sort(key=lambda x: not is_effectively_unity(x.scale))

            last_result = results[0]
            filtered = [last_result]
            for result in results[1:]:
                if str(result) != str(last_result):
                    filtered.append(result)
                last_result = result

            return filtered

        return sort_results(
            self._compose(
                equivalencies=equivalencies,
                namespace=units,
                max_depth=max_depth,
                depth=0,
                cached_results={},
            )
        )","filtered_namespace = set()
for tunit in units:
    if isinstance(tunit, UnitBase) and (include_prefix_units or not isinstance(tunit, PrefixUnit)) and has_bases_in_common_with_equiv(decomposed, tunit.decompose()):
        filtered_namespace.add(tunit)","filtered_namespace = {tunit for tunit in units if isinstance(tunit, UnitBase) and (include_prefix_units or not isinstance(tunit, PrefixUnit)) and has_bases_in_common_with_equiv(decomposed, tunit.decompose())}"
pyelftools,https://github.com/eliben/pyelftools/tree/master/test/test_dynamic.py,TestDynamic,extract_sunw$102,"def extract_sunw(filename):
            with open(filename, 'rb') as f:
                elf = ELFFile(f)
                dyn = elf.get_section_by_name('.dynamic')

                seen = set()
                for tag in dyn.iter_tags():
                    if type(tag.entry.d_tag) is str and \
                            tag.entry.d_tag.startswith(""DT_SUNW""):
                        seen.add(tag.entry.d_tag)

            return seen","seen = set()
for tag in dyn.iter_tags():
    if type(tag.entry.d_tag) is str and tag.entry.d_tag.startswith('DT_SUNW'):
        seen.add(tag.entry.d_tag)",seen = {tag.entry.d_tag for tag in dyn.iter_tags() if type(tag.entry.d_tag) is str and tag.entry.d_tag.startswith('DT_SUNW')}
self-attentive-parser,https://github.com/nikitakit/self-attentive-parser/tree/master/src/benepar/decode_chart.py,ChartDecoder,build_vocab$118,"def build_vocab(trees):
        label_set = set()
        for tree in trees:
            for _, _, label in get_labeled_spans(tree):
                if label:
                    label_set.add(label)
        label_set = [""""] + sorted(label_set)
        return {label: i for i, label in enumerate(label_set)}","label_set = set()
for tree in trees:
    for (_, _, label) in get_labeled_spans(tree):
        if label:
            label_set.add(label)","label_set = {label for tree in trees for (_, _, label) in get_labeled_spans(tree) if label}"
python-for-android,https://github.com/kivy/python-for-android/tree/master/testapps/on_device_unit_tests/test_app/tools.py,,get_failed_unittests_from$63,"def get_failed_unittests_from(unittests_output, set_of_tests):
    """"""Parse unittests output trying to find the failed tests""""""
    failed_tests = set()
    for test in set_of_tests:
        if test in unittests_output:
            failed_tests.add(test)
    return failed_tests","failed_tests = set()
for test in set_of_tests:
    if test in unittests_output:
        failed_tests.add(test)",failed_tests = {test for test in set_of_tests if test in unittests_output}
petastorm,https://github.com/uber/petastorm/tree/master/petastorm/tests/test_predicates.py,,test_pseudorandom_split_on_string_field$110,"def test_pseudorandom_split_on_string_field(all_values):
    split_list = [0.3, 0.4, 0.1, 0.0, 0.2]
    values_num = len(all_values)
    for idx in range(len(split_list)):
        test_predicate = in_pseudorandom_split(split_list, idx, 'string_partition_field')
        included_values = set()
        for val in all_values:
            if test_predicate.do_include({'string_partition_field': val}):
                included_values.add(val)
        expected_num = values_num * split_list[idx]
        assert pytest.approx(len(included_values), expected_num * 0.1) == expected_num","included_values = set()
for val in all_values:
    if test_predicate.do_include({'string_partition_field': val}):
        included_values.add(val)",included_values = {val for val in all_values if test_predicate.do_include({'string_partition_field': val})}
Silver,https://github.com/s0md3v/Silver/tree/master/core/resolver.py,,handler$14,"def handler(hostnames):
	ips = set()
	threadpool = concurrent.futures.ThreadPoolExecutor(max_workers=10)
	futures = (threadpool.submit(resolve, hostname) for hostname in hostnames)
	for i, result in enumerate(concurrent.futures.as_completed(futures)):
		if result.result():
			ips.add(result.result())
	return list(ips)","ips = set()
for (i, result) in enumerate(concurrent.futures.as_completed(futures)):
    if result.result():
        ips.add(result.result())","ips = {result.result() for (i, result) in enumerate(concurrent.futures.as_completed(futures)) if result.result()}"
Hypernets,https://github.com/DataCanvasIO/Hypernets/tree/master/hypernets/core/trial.py,TrialHistory,get_space_signatures$140,"def get_space_signatures(self):
        signatures = set()
        for s in [t.space_sample for t in self.trials]:
            signatures.add(s.signature)
        return signatures","signatures = set()
for s in [t.space_sample for t in self.trials]:
    signatures.add(s.signature)",signatures = {s.signature for s in [t.space_sample for t in self.trials]}
learn-to-cluster,https://github.com/yl-1993/learn-to-cluster/tree/master/proposals/stat_cluster.py,,inst2cls$34,"def inst2cls(inst_sets, idx2lb):
    cls_sets = []
    for inst_set in inst_sets:
        cls_set = set()
        for idx in inst_set:
            cls_set.add(idx2lb[idx])
        cls_sets.append(cls_set)
    return cls_sets","cls_set = set()
for idx in inst_set:
    cls_set.add(idx2lb[idx])",cls_set = {idx2lb[idx] for idx in inst_set}
electrum,https://github.com/spesmilo/electrum/tree/master/electrum/coinchooser.py,CoinChooserRandom,bucket_candidates_any$352,"def bucket_candidates_any(self, buckets: List[Bucket], sufficient_funds) -> List[List[Bucket]]:
        '''Returns a list of bucket sets.'''
        if not buckets:
            if sufficient_funds([], bucket_value_sum=0):
                return [[]]
            else:
                raise NotEnoughFunds()

        candidates = set()

        # Add all singletons
        for n, bucket in enumerate(buckets):
            if sufficient_funds([bucket], bucket_value_sum=bucket.value):
                candidates.add((n,))

        # And now some random ones
        attempts = min(100, (len(buckets) - 1) * 10 + 1)
        permutation = list(range(len(buckets)))
        for i in range(attempts):
            # Get a random permutation of the buckets, and
            # incrementally combine buckets until sufficient
            self.p.shuffle(permutation)
            bkts = []
            bucket_value_sum = 0
            for count, index in enumerate(permutation):
                bucket = buckets[index]
                bkts.append(bucket)
                bucket_value_sum += bucket.value
                if sufficient_funds(bkts, bucket_value_sum=bucket_value_sum):
                    candidates.add(tuple(sorted(permutation[:count + 1])))
                    break
            else:
                # note: this assumes that the effective value of any bkt is >= 0
                raise NotEnoughFunds()

        candidates = [[buckets[n] for n in c] for c in candidates]
        return [strip_unneeded(c, sufficient_funds) for c in candidates]","candidates = set()
for (n, bucket) in enumerate(buckets):
    if sufficient_funds([bucket], bucket_value_sum=bucket.value):
        candidates.add((n,))","candidates = {(n,) for (n, bucket) in enumerate(buckets) if sufficient_funds([bucket], bucket_value_sum=bucket.value)}"
intake,https://github.com/intake/intake/tree/master//versioneer.py,,do_setup$1696,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError,
            configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"",
                  file=sys.stderr)
            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with open(cfg.versionfile_source, ""w"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {""DOLLAR"": ""$"",
                        ""STYLE"": cfg.style,
                        ""TAG_PREFIX"": cfg.tag_prefix,
                        ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
                        })

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),
                       ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy, ""r"") as f:
                old = f.read()
        except EnvironmentError:
            old = """"
        if INIT_PY_SNIPPET not in old:
            print("" appending to %s"" % ipy)
            with open(ipy, ""a"") as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with open(manifest_in, ""r"") as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, ""a"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" %
              cfg.versionfile_source)
        with open(manifest_in, ""a"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-subst keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","simple_includes = set()
for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}
sympy,https://github.com/sympy/sympy/tree/master/sympy/solvers/polysys.py,,solve_triangulated$302,"def solve_triangulated(polys, *gens, **args):
    """"""
    Solve a polynomial system using Gianni-Kalkbrenner algorithm.

    The algorithm proceeds by computing one Groebner basis in the ground
    domain and then by iteratively computing polynomial factorizations in
    appropriately constructed algebraic extensions of the ground domain.

    Parameters
    ==========

    polys: a list/tuple/set
        Listing all the equations that are needed to be solved
    gens: generators
        generators of the equations in polys for which we want the
        solutions
    args: Keyword arguments
        Special options for solving the equations

    Returns
    =======

    List[Tuple]
        A List of tuples. Solutions for symbols that satisfy the
        equations listed in polys

    Examples
    ========

    >>> from sympy import solve_triangulated
    >>> from sympy.abc import x, y, z

    >>> F = [x**2 + y + z - 1, x + y**2 + z - 1, x + y + z**2 - 1]

    >>> solve_triangulated(F, x, y, z)
    [(0, 0, 1), (0, 1, 0), (1, 0, 0)]

    References
    ==========

    1. Patrizia Gianni, Teo Mora, Algebraic Solution of System of
    Polynomial Equations using Groebner Bases, AAECC-5 on Applied Algebra,
    Algebraic Algorithms and Error-Correcting Codes, LNCS 356 247--257, 1989

    """"""
    G = groebner(polys, gens, polys=True)
    G = list(reversed(G))

    domain = args.get('domain')

    if domain is not None:
        for i, g in enumerate(G):
            G[i] = g.set_domain(domain)

    f, G = G[0].ltrim(-1), G[1:]
    dom = f.get_domain()

    zeros = f.ground_roots()
    solutions = set()

    for zero in zeros:
        solutions.add(((zero,), dom))

    var_seq = reversed(gens[:-1])
    vars_seq = postfixes(gens[1:])

    for var, vars in zip(var_seq, vars_seq):
        _solutions = set()

        for values, dom in solutions:
            H, mapping = [], list(zip(vars, values))

            for g in G:
                _vars = (var,) + vars

                if g.has_only_gens(*_vars) and g.degree(var) != 0:
                    h = g.ltrim(var).eval(dict(mapping))

                    if g.degree(var) == h.degree():
                        H.append(h)

            p = min(H, key=lambda h: h.degree())
            zeros = p.ground_roots()

            for zero in zeros:
                if not zero.is_Rational:
                    dom_zero = dom.algebraic_field(zero)
                else:
                    dom_zero = dom

                _solutions.add(((zero,) + values, dom_zero))

        solutions = _solutions

    solutions = list(solutions)

    for i, (solution, _) in enumerate(solutions):
        solutions[i] = solution

    return sorted(solutions, key=default_sort_key)","solutions = set()
for zero in zeros:
    solutions.add(((zero,), dom))","solutions = {((zero,), dom) for zero in zeros}"
sympy,https://github.com/sympy/sympy/tree/master/sympy/assumptions/sathandlers.py,ClassFactRegistry,__call__$182,"def __call__(self, expr):
        ret = set()

        handlers1, handlers2 = self[type(expr)]

        for h in handlers1:
            ret.add(h(expr))
        for h in handlers2:
            ret.update(h(expr))
        return ret","ret = set()
for h in handlers1:
    ret.add(h(expr))",ret = {h(expr) for h in handlers1}
simpleai,https://github.com/simpleai-team/simpleai/tree/master/samples/machine_learning/opinion.py,OpinionProblem,__init__$74,"def __init__(self, corpus):
        super(OpinionProblem, self).__init__()

        words = set()
        for opinion in corpus:
            for word in opinion.text.split():
                if word not in STOPWORDS:
                    words.add(word)

        for word in words:
            self.attributes.append(WordIsPresent(word))","words = set()
for opinion in corpus:
    for word in opinion.text.split():
        if word not in STOPWORDS:
            words.add(word)",words = {word for opinion in corpus for word in opinion.text.split() if word not in STOPWORDS}
BertSum,https://github.com/nlpyang/BertSum/tree/master/src/models/trainer.py,Trainer,test$207,"def test(self, test_iter, step, cal_lead=False, cal_oracle=False):
        """""" Validate model.
            valid_iter: validate data iterator
        Returns:
            :obj:`nmt.Statistics`: validation loss statistics
        """"""
        # Set model in validating mode.
        def _get_ngrams(n, text):
            ngram_set = set()
            text_length = len(text)
            max_index_ngram_start = text_length - n
            for i in range(max_index_ngram_start + 1):
                ngram_set.add(tuple(text[i:i + n]))
            return ngram_set

        def _block_tri(c, p):
            tri_c = _get_ngrams(3, c.split())
            for s in p:
                tri_s = _get_ngrams(3, s.split())
                if len(tri_c.intersection(tri_s))>0:
                    return True
            return False

        if (not cal_lead and not cal_oracle):
            self.model.eval()
        stats = Statistics()

        can_path = '%s_step%d.candidate'%(self.args.result_path,step)
        gold_path = '%s_step%d.gold' % (self.args.result_path, step)
        with open(can_path, 'w') as save_pred:
            with open(gold_path, 'w') as save_gold:
                with torch.no_grad():
                    for batch in test_iter:
                        src = batch.src
                        labels = batch.labels
                        segs = batch.segs
                        clss = batch.clss
                        mask = batch.mask
                        mask_cls = batch.mask_cls


                        gold = []
                        pred = []

                        if (cal_lead):
                            selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size
                        elif (cal_oracle):
                            selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in
                                            range(batch.batch_size)]
                        else:
                            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)

                            loss = self.loss(sent_scores, labels.float())
                            loss = (loss * mask.float()).sum()
                            batch_stats = Statistics(float(loss.cpu().data.numpy()), len(labels))
                            stats.update(batch_stats)

                            sent_scores = sent_scores + mask.float()
                            sent_scores = sent_scores.cpu().data.numpy()
                            selected_ids = np.argsort(-sent_scores, 1)
                        # selected_ids = np.sort(selected_ids,1)
                        for i, idx in enumerate(selected_ids):
                            _pred = []
                            if(len(batch.src_str[i])==0):
                                continue
                            for j in selected_ids[i][:len(batch.src_str[i])]:
                                if(j>=len( batch.src_str[i])):
                                    continue
                                candidate = batch.src_str[i][j].strip()
                                if(self.args.block_trigram):
                                    if(not _block_tri(candidate,_pred)):
                                        _pred.append(candidate)
                                else:
                                    _pred.append(candidate)

                                if ((not cal_oracle) and (not self.args.recall_eval) and len(_pred) == 3):
                                    break

                            _pred = '<q>'.join(_pred)
                            if(self.args.recall_eval):
                                _pred = ' '.join(_pred.split()[:len(batch.tgt_str[i].split())])

                            pred.append(_pred)
                            gold.append(batch.tgt_str[i])

                        for i in range(len(gold)):
                            save_gold.write(gold[i].strip()+'\n')
                        for i in range(len(pred)):
                            save_pred.write(pred[i].strip()+'\n')
        if(step!=-1 and self.args.report_rouge):
            rouges = test_rouge(self.args.temp_dir, can_path, gold_path)
            logger.info('Rouges at step %d \n%s' % (step, rouge_results_to_str(rouges)))
        self._report_step(0, step, valid_stats=stats)

        return stats","ngram_set = set()
for i in range(max_index_ngram_start + 1):
    ngram_set.add(tuple(text[i:i + n]))",ngram_set = {tuple(text[i:i + n]) for i in range(max_index_ngram_start + 1)}
legendary,https://github.com/derrod/legendary/tree/master/legendary/cli.py,LegendaryCLI,info$1538,"def info(self, args):
        name_or_path = args.app_name_or_manifest
        app_name = manifest_uri = None
        if os.path.exists(name_or_path) or name_or_path.startswith('http'):
            manifest_uri = name_or_path
        else:
            app_name = self._resolve_aliases(name_or_path)

        if not args.offline and not manifest_uri:
            try:
                if not self.core.login():
                    logger.error('Log in failed!')
                    exit(1)
            except ValueError:
                pass

        # lists that will be printed or turned into JSON data
        info_items = dict(game=list(), manifest=list(), install=list())
        InfoItem = namedtuple('InfoItem', ['name', 'json_name', 'value', 'json_value'])

        if self.core.is_installed(app_name):
            installed_platform = self.core.get_installed_game(app_name).platform
            if installed_platform != args.platform:
                logger.warning(f'Game is installed for platform ""{installed_platform}"", '
                               f'but requested metadata is for ""{args.platform}"", this may '
                               f'lead to unexpected results.')

        game = self.core.get_game(app_name, update_meta=not args.offline, platform=args.platform)
        if game and not self.core.asset_available(game, platform=args.platform):
            logger.warning(f'Asset information for ""{game.app_name}"" is missing, this may be due to the game '
                           f'not being available on the selected platform or currently logged-in account.')
            args.offline = True

        manifest_data = None
        entitlements = None
        # load installed manifest or URI
        if args.offline or manifest_uri:
            if app_name and self.core.is_installed(app_name):
                manifest_data, _ = self.core.get_installed_manifest(app_name)
            elif manifest_uri and manifest_uri.startswith('http'):
                r = self.core.egs.unauth_session.get(manifest_uri)
                r.raise_for_status()
                manifest_data = r.content
            elif manifest_uri and os.path.exists(manifest_uri):
                with open(manifest_uri, 'rb') as f:
                    manifest_data = f.read()
            else:
                logger.info('Game not installed and offline mode enabled, cannot load manifest.')
        elif game:
            entitlements = self.core.egs.get_user_entitlements()
            egl_meta = self.core.egs.get_game_info(game.namespace, game.catalog_item_id)
            game.metadata = egl_meta
            # Get manifest if asset exists for current platform
            if args.platform in game.asset_infos:
                manifest_data, _ = self.core.get_cdn_manifest(game, args.platform)

        if game:
            game_infos = info_items['game']
            game_infos.append(InfoItem('App name', 'app_name', game.app_name, game.app_name))
            game_infos.append(InfoItem('Title', 'title', game.app_title, game.app_title))
            game_infos.append(InfoItem('Latest version', 'version', game.app_version(args.platform),
                                       game.app_version(args.platform)))
            all_versions = {k: v.build_version for k, v in game.asset_infos.items()}
            game_infos.append(InfoItem('All versions', 'platform_versions', all_versions, all_versions))
            # Cloud save support for Mac and Windows
            game_infos.append(InfoItem('Cloud saves supported', 'cloud_saves_supported',
                                       game.supports_cloud_saves or game.supports_mac_cloud_saves,
                                       game.supports_cloud_saves or game.supports_mac_cloud_saves))
            cs_dir = None
            if game.supports_cloud_saves:
                cs_dir = game.metadata['customAttributes']['CloudSaveFolder']['value']
            game_infos.append(InfoItem('Cloud save folder (Windows)', 'cloud_save_folder', cs_dir, cs_dir))

            cs_dir = None
            if game.supports_mac_cloud_saves:
                cs_dir = game.metadata['customAttributes']['CloudSaveFolder_MAC']['value']
            game_infos.append(InfoItem('Cloud save folder (Mac)', 'cloud_save_folder_mac', cs_dir, cs_dir))

            game_infos.append(InfoItem('Is DLC', 'is_dlc', game.is_dlc, game.is_dlc))

            external_activation = game.third_party_store or game.partner_link_type
            game_infos.append(InfoItem('Activates on external platform', 'external_activation',
                                       external_activation or 'No', external_activation))

            # Find custom launch options, if available
            launch_options = []
            i = 1
            while f'extraLaunchOption_{i:03d}_Name' in game.metadata['customAttributes']:
                launch_options.append((
                    game.metadata['customAttributes'][f'extraLaunchOption_{i:03d}_Name']['value'],
                    game.metadata['customAttributes'][f'extraLaunchOption_{i:03d}_Args']['value']
                ))
                i += 1

            if launch_options:
                human_list = []
                json_list = []
                for opt_name, opt_cmd in sorted(launch_options):
                    human_list.append(f'Name: ""{opt_name}"", Parameters: {opt_cmd}')
                    json_list.append(dict(name=opt_name, parameters=opt_cmd))
                game_infos.append(InfoItem('Extra launch options', 'launch_options',
                                           human_list, json_list))
            else:
                game_infos.append(InfoItem('Extra launch options', 'launch_options', None, []))

            # list all owned DLC based on entitlements
            if entitlements and not game.is_dlc:
                owned_entitlements = {i['entitlementName'] for i in entitlements}
                owned_app_names = {g.app_name for g in self.core.get_assets(args.platform)}
                owned_dlc = []
                for dlc in game.metadata.get('dlcItemList', []):
                    installable = dlc.get('releaseInfo', None)
                    if dlc['entitlementName'] in owned_entitlements:
                        owned_dlc.append((installable, None, dlc['title'], dlc['id']))
                    elif installable:
                        app_name = dlc['releaseInfo'][0]['appId']
                        if app_name in owned_app_names:
                            owned_dlc.append((installable, app_name, dlc['title'], dlc['id']))

                if owned_dlc:
                    human_list = []
                    json_list = []
                    for installable, app_name, title, dlc_id in owned_dlc:
                        json_list.append(dict(app_name=app_name, title=title,
                                              installable=installable, id=dlc_id))
                        if installable:
                            human_list.append(f'App name: {app_name}, Title: ""{title}""')
                        else:
                            human_list.append(f'Title: ""{title}"" (no installation required)')
                    game_infos.append(InfoItem('Owned DLC', 'owned_dlc', human_list, json_list))
                else:
                    game_infos.append(InfoItem('Owned DLC', 'owned_dlc', None, []))
            else:
                game_infos.append(InfoItem('Owned DLC', 'owned_dlc', None, []))

            igame = self.core.get_installed_game(app_name)
            if igame:
                installation_info = info_items['install']
                installation_info.append(InfoItem('Platform', 'platform', igame.platform, igame.platform))
                installation_info.append(InfoItem('Version', 'version', igame.version, igame.version))
                disk_size_human = f'{igame.install_size / 1024 / 1024 / 1024:.02f} GiB'
                installation_info.append(InfoItem('Install size', 'disk_size', disk_size_human,
                                                  igame.install_size))
                installation_info.append(InfoItem('Install path', 'install_path', igame.install_path,
                                                  igame.install_path))
                installation_info.append(InfoItem('Save data path', 'save_path', igame.save_path,
                                                  igame.save_path))
                installation_info.append(InfoItem('EGL sync GUID', 'synced_egl_guid', igame.egl_guid,
                                                  igame.egl_guid))
                if igame.install_tags:
                    tags = ', '.join(igame.install_tags)
                else:
                    tags = '(None, all game data selected for install)'
                installation_info.append(InfoItem('Install tags', 'install_tags', tags, igame.install_tags))
                installation_info.append(InfoItem('Requires ownership verification token (DRM)', 'requires_ovt',
                                                  igame.requires_ot, igame.requires_ot))

                installed_dlc_human = []
                installed_dlc_json = []
                for dlc in game.metadata.get('dlcItemList', []):
                    if not dlc.get('releaseInfo', None):
                        continue
                    app_name = dlc['releaseInfo'][0]['appId']
                    if igame := self.core.get_installed_game(app_name):
                        installed_dlc_json.append(dict(app_name=igame.app_name, title=igame.title,
                                                       install_size=igame.install_size))
                        installed_dlc_human.append('App name: {}, Title: ""{}"", Size: {:.02f} GiB'.format(
                            igame.app_name, igame.title, igame.install_size / 1024 / 1024 / 1024
                        ))
                installation_info.append(InfoItem('Installed DLC', 'installed_dlc',
                                                  installed_dlc_human or None,
                                                  installed_dlc_json))

        if manifest_data:
            manifest_info = info_items['manifest']
            manifest = self.core.load_manifest(manifest_data)
            manifest_size = len(manifest_data)
            manifest_size_human = f'{manifest_size / 1024:.01f} KiB'
            manifest_info.append(InfoItem('Manifest size', 'size', manifest_size_human, manifest_size))
            manifest_type = 'JSON' if hasattr(manifest, 'json_data') else 'Binary'
            manifest_info.append(InfoItem('Manifest type', 'type', manifest_type, manifest_type.lower()))
            manifest_info.append(InfoItem('Manifest version', 'version', manifest.version, manifest.version))
            manifest_info.append(InfoItem('Manifest feature level', 'feature_level',
                                          manifest.meta.feature_level, manifest.meta.feature_level))
            manifest_info.append(InfoItem('Manifest app name', 'app_name', manifest.meta.app_name,
                                          manifest.meta.app_name))
            manifest_info.append(InfoItem('Launch EXE', 'launch_exe',
                                          manifest.meta.launch_exe or 'N/A',
                                          manifest.meta.launch_exe))
            manifest_info.append(InfoItem('Launch Command', 'launch_command',
                                          manifest.meta.launch_command or '(None)',
                                          manifest.meta.launch_command))
            manifest_info.append(InfoItem('Build version', 'build_version', manifest.meta.build_version,
                                          manifest.meta.build_version))
            manifest_info.append(InfoItem('Build ID', 'build_id', manifest.meta.build_id,
                                          manifest.meta.build_id))
            if manifest.meta.prereq_ids:
                human_list = [
                    f'Prerequisite IDs: {"", "".join(manifest.meta.prereq_ids)}',
                    f'Prerequisite name: {manifest.meta.prereq_name}',
                    f'Prerequisite path: {manifest.meta.prereq_path}',
                    f'Prerequisite args: {manifest.meta.prereq_args or ""(None)""}',
                ]
                manifest_info.append(InfoItem('Prerequisites', 'prerequisites', human_list,
                                              dict(ids=manifest.meta.prereq_ids,
                                                   name=manifest.meta.prereq_name,
                                                   path=manifest.meta.prereq_path,
                                                   args=manifest.meta.prereq_args)))
            else:
                manifest_info.append(InfoItem('Prerequisites', 'prerequisites', None, None))

            install_tags = {''}
            for fm in manifest.file_manifest_list.elements:
                for tag in fm.install_tags:
                    install_tags.add(tag)

            install_tags = sorted(install_tags)
            install_tags_human = ', '.join(i if i else '(empty)' for i in install_tags)
            manifest_info.append(InfoItem('Install tags', 'install_tags', install_tags_human, install_tags))
            # file and chunk count
            manifest_info.append(InfoItem('Files', 'num_files', manifest.file_manifest_list.count,
                                          manifest.file_manifest_list.count))
            manifest_info.append(InfoItem('Chunks', 'num_chunks', manifest.chunk_data_list.count,
                                          manifest.chunk_data_list.count))
            # total file size
            total_size = sum(fm.file_size for fm in manifest.file_manifest_list.elements)
            file_size = '{:.02f} GiB'.format(total_size / 1024 / 1024 / 1024)
            manifest_info.append(InfoItem('Disk size (uncompressed)', 'disk_size', file_size, total_size))
            # total chunk size
            total_size = sum(c.file_size for c in manifest.chunk_data_list.elements)
            chunk_size = '{:.02f} GiB'.format(total_size / 1024 / 1024 / 1024)
            manifest_info.append(InfoItem('Download size (compressed)', 'download_size',
                                          chunk_size, total_size))

            # if there are install tags break down size by tag
            tag_disk_size = []
            tag_disk_size_human = []
            tag_download_size = []
            tag_download_size_human = []
            if len(install_tags) > 1:
                longest_tag = max(max(len(t) for t in install_tags), len('(empty)'))
                for tag in install_tags:
                    # sum up all file sizes for the tag
                    human_tag = tag or '(empty)'
                    tag_files = [fm for fm in manifest.file_manifest_list.elements if
                                 (tag in fm.install_tags) or (not tag and not fm.install_tags)]
                    tag_file_size = sum(fm.file_size for fm in tag_files)
                    tag_disk_size.append(dict(tag=tag, size=tag_file_size, count=len(tag_files)))
                    tag_file_size_human = '{:.02f} GiB'.format(tag_file_size / 1024 / 1024 / 1024)
                    tag_disk_size_human.append(f'{human_tag.ljust(longest_tag)} - {tag_file_size_human} '
                                               f'(Files: {len(tag_files)})')
                    # tag_disk_size_human.append(f'Size: {tag_file_size_human}, Files: {len(tag_files)}, Tag: ""{tag}""')
                    # accumulate chunk guids used for this tag and count their size too
                    tag_chunk_guids = set()
                    for fm in tag_files:
                        for cp in fm.chunk_parts:
                            tag_chunk_guids.add(cp.guid_num)

                    tag_chunk_size = sum(c.file_size for c in manifest.chunk_data_list.elements
                                         if c.guid_num in tag_chunk_guids)
                    tag_download_size.append(dict(tag=tag, size=tag_chunk_size, count=len(tag_chunk_guids)))
                    tag_chunk_size_human = '{:.02f} GiB'.format(tag_chunk_size / 1024 / 1024 / 1024)
                    tag_download_size_human.append(f'{human_tag.ljust(longest_tag)} - {tag_chunk_size_human} '
                                                   f'(Chunks: {len(tag_chunk_guids)})')

            manifest_info.append(InfoItem('Disk size by install tag', 'tag_disk_size',
                                          tag_disk_size_human or 'N/A', tag_disk_size))
            manifest_info.append(InfoItem('Download size by install tag', 'tag_download_size',
                                          tag_download_size_human or 'N/A', tag_download_size))

        if not args.json:
            def print_info_item(item: InfoItem):
                if item.value is None:
                    print(f'- {item.name}: (None)')
                elif isinstance(item.value, list):
                    print(f'- {item.name}:')
                    for list_item in item.value:
                        print(' + ', list_item)
                elif isinstance(item.value, dict):
                    print(f'- {item.name}:')
                    for k, v in item.value.items():
                        print(' + ', k, ':', v)
                else:
                    print(f'- {item.name}: {item.value}')

            if info_items['game']:
                print('\nGame Information:')
                for info_item in info_items['game']:
                    print_info_item(info_item)
            if info_items['install']:
                print('\nInstallation information:')
                for info_item in info_items['install']:
                    print_info_item(info_item)
            if info_items['manifest']:
                print('\nManifest information:')
                for info_item in info_items['manifest']:
                    print_info_item(info_item)

            if not any(info_items.values()):
                print('No game information available.')
        else:
            json_out = dict(game=dict(), install=dict(), manifest=dict())
            for info_item in info_items['game']:
                json_out['game'][info_item.json_name] = info_item.json_value
            for info_item in info_items['install']:
                json_out['install'][info_item.json_name] = info_item.json_value
            for info_item in info_items['manifest']:
                json_out['manifest'][info_item.json_name] = info_item.json_value
            # set empty items to null
            for key, value in json_out.items():
                if not value:
                    json_out[key] = None
            return self._print_json(json_out, args.pretty_json)","tag_chunk_guids = set()
for fm in tag_files:
    for cp in fm.chunk_parts:
        tag_chunk_guids.add(cp.guid_num)",tag_chunk_guids = {cp.guid_num for fm in tag_files for cp in fm.chunk_parts}
petastorm,https://github.com/uber/petastorm/tree/master/petastorm/tests/test_end_to_end.py,,test_pyarrow_filters_make_reader$864,"def test_pyarrow_filters_make_reader(synthetic_dataset):
    with make_reader(synthetic_dataset.url, workers_count=5, num_epochs=1,
                     filters=[('partition_key', '=', 'p_5'), ]) as reader:
        uv = set()
        for data in reader:
            uv.add(data.partition_key)

        assert uv == {'p_5'}","uv = set()
for data in reader:
    uv.add(data.partition_key)",uv = {data.partition_key for data in reader}
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/networking/ClientNetworkingDomain.py,NetworkDomainManager,STATICLinkURLClassesAndParsers$2351,"def STATICLinkURLClassesAndParsers( url_classes, parsers, existing_url_class_keys_to_parser_keys ):
        
        url_classes = list( url_classes )
        
        NetworkDomainManager.STATICSortURLClassesDescendingComplexity( url_classes )
        
        parsers = list( parsers )
        
        parsers.sort( key = lambda p: p.GetName() )
        
        new_url_class_keys_to_parser_keys = {}
        
        api_pairs = ConvertURLClassesIntoAPIPairs( url_classes )
        
        # anything that goes to an api url will be parsed by that api's parser--it can't have its own
        api_pair_unparsable_url_classes = set()
        
        for ( a, b ) in api_pairs:
            
            api_pair_unparsable_url_classes.add( a )
            
        
        #
        
        # I have to do this backwards, going through parsers and then url_classes, so I can do a proper url match lookup like the real domain manager does it
        # otherwise, if we iterate through url matches looking for parsers to match them, we have gallery url matches thinking they match parser post urls
        # e.g.
        # The page parser might say it supports https://danbooru.donmai.us/posts/3198277
        # But the gallery url class might think it recognises that as https://danbooru.donmai.us/posts
        # 
        # So we have to do the normal lookup in the proper descending complexity order, not searching any further than the first, correct match
        
        for parser in parsers:
            
            example_urls = parser.GetExampleURLs()
            
            for example_url in example_urls:
                
                for url_class in url_classes:
                    
                    if url_class in api_pair_unparsable_url_classes:
                        
                        continue
                        
                    
                    if url_class.Matches( example_url ):
                        
                        # we have a match. this is the 'correct' match for this example url, and we should not search any more, so we break below
                        
                        url_class_key = url_class.GetClassKey()
                        
                        parsable = url_class.IsParsable()
                        linkable = url_class_key not in existing_url_class_keys_to_parser_keys and url_class_key not in new_url_class_keys_to_parser_keys
                        
                        if parsable and linkable:
                            
                            new_url_class_keys_to_parser_keys[ url_class_key ] = parser.GetParserKey()
                            
                        
                        break
                        
                    
                
            
        '''
        #
        
        for url_class in url_classes:
            
            if not url_class.IsParsable() or url_class in api_pair_unparsable_url_classes:
                
                continue
                
            
            url_class_key = url_class.GetClassKey()
            
            if url_class_key in existing_url_class_keys_to_parser_keys:
                
                continue
                
            
            for parser in parsers:
                
                example_urls = parser.GetExampleURLs()
                
                if True in ( url_class.Matches( example_url ) for example_url in example_urls ):
                    
                    new_url_class_keys_to_parser_keys[ url_class_key ] = parser.GetParserKey()
                    
                    break
                    
                
            
        '''
        return new_url_class_keys_to_parser_keys","api_pair_unparsable_url_classes = set()
for (a, b) in api_pairs:
    api_pair_unparsable_url_classes.add(a)","api_pair_unparsable_url_classes = {a for (a, b) in api_pairs}"
salt,https://github.com/saltstack/salt/tree/master/salt/modules/win_service.py,,get_enabled$119,"def get_enabled():
    """"""
    Return a list of enabled services. Enabled is defined as a service that is
    marked to Auto Start.

    Returns:
        list: A list of enabled services

    CLI Example:

    .. code-block:: bash

        salt '*' service.get_enabled
    """"""
    raw_services = _get_services()
    services = set()
    for service in raw_services:
        if info(service[""ServiceName""])[""StartType""] in [""Auto""]:
            services.add(service[""ServiceName""])

    return sorted(services)","services = set()
for service in raw_services:
    if info(service['ServiceName'])['StartType'] in ['Auto']:
        services.add(service['ServiceName'])",services = {service['ServiceName'] for service in raw_services if info(service['ServiceName'])['StartType'] in ['Auto']}
salt,https://github.com/saltstack/salt/tree/master/salt/modules/sysmod.py,,list_runners$620,"def list_runners(*args):
    """"""
    List the runners loaded on the minion

    .. versionadded:: 2014.7.0

    CLI Example:

    .. code-block:: bash

        salt '*' sys.list_runners

    Runner names can be specified as globs.

    .. versionadded:: 2015.5.0

    .. code-block:: bash

        salt '*' sys.list_runners 'm*'

    """"""
    run_ = salt.runner.Runner(__opts__)
    runners = set()
    if not args:
        for func in run_.functions:
            runners.add(func.split(""."")[0])
        return sorted(runners)

    for module in args:
        if ""*"" in module:
            for func in fnmatch.filter(run_.functions, module):
                runners.add(func.split(""."")[0])
        else:
            for func in run_.functions:
                mod_test = func.split(""."")[0]
                if mod_test == module:
                    runners.add(mod_test)
    return sorted(runners)","runners = set()
for func in run_.functions:
    runners.add(func.split('.')[0])",runners = {func.split('.')[0] for func in run_.functions}
django-extensions,https://github.com/django-extensions/django-extensions/tree/master/tests/management/commands/shell_plus_tests/test_utils.py,AutomaticShellPlusImportsTestCase,get_all_names_for_class$18,"def get_all_names_for_class(self, model_to_find_occurrences):  # type: (Type) -> Set[str]
        """"""
        Returns all names under current class is imported.
        :param model_to_find_occurrences: class to find names
        :return: set of names under class is imported.
        """"""
        result = set()
        for name, model_class in self.imported_objects.items():
            if model_class == model_to_find_occurrences:
                result.add(name)
        return result","result = set()
for (name, model_class) in self.imported_objects.items():
    if model_class == model_to_find_occurrences:
        result.add(name)","result = {name for (name, model_class) in self.imported_objects.items() if model_class == model_to_find_occurrences}"
LangSrcCurise,https://github.com/LangziFun/LangSrcCurise/tree/master/core/Subdomain_Api.py,,Sec_Api$191,"def Sec_Api(domain):
    result = set()
    try:
        url = ""https://api.securitytrails.com/v1/domain/{}/subdomains"".format(domain)
        querystring = {""apikey"":seckey}
        response = requests.request(""GET"", url, params=querystring)
        rest = (response.json())
        subdomains = rest['subdomains']
        for s in subdomains:
                result.add(s+'.'+domain)
    except:
        pass
    print('[+ SecurityTrails API] SecTra接口 : {} 捕获子域名总数 : {}'.format(domain, len(result)))
    return list(result)","result = set()
for s in subdomains:
    result.add(s + '.' + domain)",result = {s + '.' + domain for s in subdomains}
forseti-security,https://github.com/forseti-security/forseti-security/tree/master/tests/services/api_tests/model_test.py,,expand_message$130,"def expand_message(messages, type):
    """"""Get the access_details in the form of
       set([member resource permission ])
    """"""
    details = set()
    if type == ""access_by_resource"":
        for access in messages:
            for member in access.members:
                details.add(' '.join([member,
                                      access.resource,
                                      access.role]))
    elif type == ""access_by_member"":
        for access in messages:
            for resource in access.resources:
                details.add(' '.join([access.member,
                                      resource,
                                      access.role]))
    elif type == ""access_by_both"":
        for access in messages:
            details.add(' '.join([access.member,
                                  access.resource,
                                  access.permission]))
    elif type == ""role_permission"":
        for permissionsbyrole in messages:
            for permission in permissionsbyrole.permissions:
                details.add(' '.join([permissionsbyrole.role,
                                      permission]))
    else:
        raise Exception(""type unrecognized"")
    return details","details = set()
for access in messages:
    for member in access.members:
        details.add(' '.join([member, access.resource, access.role]))","details = {' '.join([member, access.resource, access.role]) for access in messages for member in access.members}"
salt,https://github.com/saltstack/salt/tree/master/salt/modules/freebsdkmod.py,,_rm_mods$47,"def _rm_mods(pre_mods, post_mods):
    """"""
    Return a list of the new modules, pass an kldstat dict before running
    modprobe and one after modprobe has run
    """"""
    pre = set()
    post = set()
    for mod in pre_mods:
        pre.add(mod[""module""])
    for mod in post_mods:
        post.add(mod[""module""])
    return pre - post","pre = set()
for mod in pre_mods:
    pre.add(mod['module'])",pre = {mod['module'] for mod in pre_mods}
salt,https://github.com/saltstack/salt/tree/master/salt/modules/freebsdkmod.py,,_rm_mods$47,"def _rm_mods(pre_mods, post_mods):
    """"""
    Return a list of the new modules, pass an kldstat dict before running
    modprobe and one after modprobe has run
    """"""
    pre = set()
    post = set()
    for mod in pre_mods:
        pre.add(mod[""module""])
    for mod in post_mods:
        post.add(mod[""module""])
    return pre - post","post = set()
for mod in post_mods:
    post.add(mod['module'])",post = {mod['module'] for mod in post_mods}
neutron,https://github.com/openstack/neutron/tree/master/neutron/db/migration/alembic_migrations/versions/newton/contract/a8b517cff8ab_add_routerport_bindings_for_ha.py,,upgrade$35,"def upgrade():
    ha_bindings = sa.Table(
        HA_AGENT_BINDINGS,
        sa.MetaData(),
        sa.Column('port_id', sa.String(36)),
        sa.Column('router_id', sa.String(36)),
        sa.Column('l3_agent_id', sa.String(36)),
        sa.Column('state', sa.Enum(constants.HA_ROUTER_STATE_ACTIVE,
                                   constants.HA_ROUTER_STATE_STANDBY,
                                   name='l3_ha_states'))
    )
    router_ports = sa.Table(ROUTER_PORTS,
                            sa.MetaData(),
                            sa.Column('router_id', sa.String(36)),
                            sa.Column('port_id', sa.String(36)),
                            sa.Column('port_type', sa.String(255)))
    session = sa.orm.Session(bind=op.get_bind())
    with session.begin(subtransactions=True):
        router_port_tuples = set()
        for ha_bind in session.query(ha_bindings):
            router_port_tuples.add((ha_bind.router_id, ha_bind.port_id))
        # we have to remove any from the bulk insert that may already exist
        # as a result of Ifd3e007aaf2a2ed8123275aa3a9f540838e3c003 being
        # back-ported
        for router_port in session.query(router_ports).filter(
                router_ports.c.port_type ==
                constants.DEVICE_OWNER_ROUTER_HA_INTF):
            router_port_tuples.discard((router_port.router_id,
                                        router_port.port_id))
        new_records = [dict(router_id=router_id, port_id=port_id,
                            port_type=constants.DEVICE_OWNER_ROUTER_HA_INTF)
                       for router_id, port_id in router_port_tuples]
    op.bulk_insert(router_ports, new_records)
    session.commit()","router_port_tuples = set()
for ha_bind in session.query(ha_bindings):
    router_port_tuples.add((ha_bind.router_id, ha_bind.port_id))","router_port_tuples = {(ha_bind.router_id, ha_bind.port_id) for ha_bind in session.query(ha_bindings)}"
swift,https://github.com/openstack/swift/tree/master/swift/obj/server.py,ObjectController,__init__$132,"def __init__(self, conf, logger=None):
        """"""
        Creates a new WSGI application for the Swift Object Server. An
        example configuration is given at
        <source-dir>/etc/object-server.conf-sample or
        /etc/swift/object-server.conf-sample.
        """"""
        super(ObjectController, self).__init__(conf)
        self.logger = logger or get_logger(conf, log_route='object-server')
        self.node_timeout = float(conf.get('node_timeout', 3))
        self.container_update_timeout = float(
            conf.get('container_update_timeout', 1))
        self.conn_timeout = float(conf.get('conn_timeout', 0.5))
        self.client_timeout = float(conf.get('client_timeout', 60))
        self.disk_chunk_size = int(conf.get('disk_chunk_size', 65536))
        self.network_chunk_size = int(conf.get('network_chunk_size', 65536))
        self.log_requests = config_true_value(conf.get('log_requests', 'true'))
        self.max_upload_time = int(conf.get('max_upload_time', 86400))
        self.slow = int(conf.get('slow', 0))
        self.keep_cache_private = \
            config_true_value(conf.get('keep_cache_private', 'false'))

        default_allowed_headers = '''
            content-disposition,
            content-encoding,
            x-delete-at,
            x-object-manifest,
            x-static-large-object,
            cache-control,
            content-language,
            expires,
            x-robots-tag
        '''
        extra_allowed_headers = [
            header.strip().lower() for header in conf.get(
                'allowed_headers', default_allowed_headers).split(',')
            if header.strip()
        ]
        self.allowed_headers = set()
        for header in extra_allowed_headers:
            if header not in RESERVED_DATAFILE_META:
                self.allowed_headers.add(header)
        if conf.get('auto_create_account_prefix'):
            self.logger.warning('Option auto_create_account_prefix is '
                                'deprecated. Configure '
                                'auto_create_account_prefix under the '
                                'swift-constraints section of '
                                'swift.conf. This option will '
                                'be ignored in a future release.')
            self.auto_create_account_prefix = \
                conf['auto_create_account_prefix']
        else:
            self.auto_create_account_prefix = AUTO_CREATE_ACCOUNT_PREFIX

        self.expiring_objects_account = self.auto_create_account_prefix + \
            (conf.get('expiring_objects_account_name') or 'expiring_objects')
        self.expiring_objects_container_divisor = \
            int(conf.get('expiring_objects_container_divisor') or 86400)
        # Initialization was successful, so now apply the network chunk size
        # parameter as the default read / write buffer size for the network
        # sockets.
        #
        # NOTE WELL: This is a class setting, so until we get set this on a
        # per-connection basis, this affects reading and writing on ALL
        # sockets, those between the proxy servers and external clients, and
        # those between the proxy servers and the other internal servers.
        #
        # ** Because the primary motivation for this is to optimize how data
        # is written back to the proxy server, we could use the value from the
        # disk_chunk_size parameter. However, it affects all created sockets
        # using this class so we have chosen to tie it to the
        # network_chunk_size parameter value instead.
        if six.PY2:
            socket._fileobject.default_bufsize = self.network_chunk_size
        # TODO: find a way to enable similar functionality in py3

        # Provide further setup specific to an object server implementation.
        self.setup(conf)","self.allowed_headers = set()
for header in extra_allowed_headers:
    if header not in RESERVED_DATAFILE_META:
        self.allowed_headers.add(header)",self.allowed_headers = {header for header in extra_allowed_headers if header not in RESERVED_DATAFILE_META}
angr,https://github.com/angr/angr/tree/master/angr/knowledge_plugins/key_definitions/live_definitions.py,LiveDefinitions,_mo_cmp$146,"def _mo_cmp(mo_self: Union['SimMemoryObject', Set['SimMemoryObject']],
                mo_other: Union['SimMemoryObject', Set['SimMemoryObject']], addr: int, size: int):  # pylint:disable=unused-argument
        # comparing bytes from two sets of memory objects
        # we don't need to resort to byte-level comparison. object-level is good enough.

        if type(mo_self) is set and type(mo_other) is set and len(mo_self) == 1 and len(mo_other) == 1:
            a = next(iter(mo_self))
            b = next(iter(mo_other))
            return a.object is b.object and a.endness == b.endness

        values_self = set()
        values_other = set()
        if type(mo_self) is set:
            for mo in mo_self:
                values_self.add(mo.object)
        else:
            values_self.add(mo_self)
        if type(mo_other) is set:
            for mo in mo_other:
                values_other.add(mo.object)
        else:
            values_other.add(mo_other)
        return values_self == values_other","values_self = set()
for mo in mo_self:
    values_self.add(mo.object)",values_self = {mo.object for mo in mo_self}
angr,https://github.com/angr/angr/tree/master/angr/knowledge_plugins/key_definitions/live_definitions.py,LiveDefinitions,_mo_cmp$146,"def _mo_cmp(mo_self: Union['SimMemoryObject', Set['SimMemoryObject']],
                mo_other: Union['SimMemoryObject', Set['SimMemoryObject']], addr: int, size: int):  # pylint:disable=unused-argument
        # comparing bytes from two sets of memory objects
        # we don't need to resort to byte-level comparison. object-level is good enough.

        if type(mo_self) is set and type(mo_other) is set and len(mo_self) == 1 and len(mo_other) == 1:
            a = next(iter(mo_self))
            b = next(iter(mo_other))
            return a.object is b.object and a.endness == b.endness

        values_self = set()
        values_other = set()
        if type(mo_self) is set:
            for mo in mo_self:
                values_self.add(mo.object)
        else:
            values_self.add(mo_self)
        if type(mo_other) is set:
            for mo in mo_other:
                values_other.add(mo.object)
        else:
            values_other.add(mo_other)
        return values_self == values_other","values_other = set()
for mo in mo_other:
    values_other.add(mo.object)",values_other = {mo.object for mo in mo_other}
mlxtend,https://github.com/rasbt/mlxtend/tree/master/mlxtend/preprocessing/transactionencoder.py,TransactionEncoder,fit$33,"def fit(self, X):
        """"""Learn unique column names from transaction DataFrame

        Parameters
        ------------
        X : list of lists
          A python list of lists, where the outer list stores the
          n transactions and the inner list stores the items in each
          transaction.

          For example,
          [['Apple', 'Beer', 'Rice', 'Chicken'],
           ['Apple', 'Beer', 'Rice'],
           ['Apple', 'Beer'],
           ['Apple', 'Bananas'],
           ['Milk', 'Beer', 'Rice', 'Chicken'],
           ['Milk', 'Beer', 'Rice'],
           ['Milk', 'Beer'],
           ['Apple', 'Bananas']]

        """"""
        unique_items = set()
        for transaction in X:
            for item in transaction:
                unique_items.add(item)
        self.columns_ = sorted(unique_items)
        columns_mapping = {}
        for col_idx, item in enumerate(self.columns_):
            columns_mapping[item] = col_idx
        self.columns_mapping_ = columns_mapping
        return self","unique_items = set()
for transaction in X:
    for item in transaction:
        unique_items.add(item)",unique_items = {item for transaction in X for item in transaction}
dnsviz,https://github.com/dnsviz/dnsviz/tree/master/dnsviz/analysis/status.py,DSStatus,__init__$390,"def __init__(self, ds, ds_meta, dnskey, supported_digest_algs):
        self.ds = ds
        self.ds_meta = ds_meta
        self.dnskey = dnskey
        self.warnings = []
        self.errors = []

        if self.dnskey is None:
            self.digest_valid = None
        else:
            self.digest_valid = crypto.validate_ds_digest(ds.digest_type, ds.digest, dnskey.message_for_ds())

        self.validation_status = DS_STATUS_VALID
        if self.digest_valid is None or self.ds.digest_type not in supported_digest_algs:
            # Either we cannot reproduce a digest with this type, or we are
            # explicitly directed to ignore the digest type.
            if self.dnskey is None:
                # In this case, there is no corresponding DNSKEY, so we make
                # the status ""INDETERMINATE"".
                if self.validation_status == DS_STATUS_VALID:
                    self.validation_status = DS_STATUS_INDETERMINATE_NO_DNSKEY
            else:
                # If there is a DNSKEY, then we look at *why* we are ignoring
                # the digest of the DNSKEY.
                if self.ds.digest_type in DS_DIGEST_ALGS_VALIDATION_PROHIBITED:
                    # In this case, specification dictates that the algorithm
                    # MUST NOT be validated, so we mark it as ignored.
                    if self.validation_status == DS_STATUS_VALID:
                        self.validation_status = DS_STATUS_ALGORITHM_IGNORED
                else:
                    # In this case, we can't validate this particular
                    # digest type, either because the code doesn't support it,
                    # or because we have been explicitly directed to ignore it.
                    # In either case, mark it as ""UNKNOWN"", and warn that it is
                    # not supported.
                    if self.validation_status == DS_STATUS_VALID:
                        self.validation_status = DS_STATUS_INDETERMINATE_UNKNOWN_ALGORITHM
                    self.warnings.append(Errors.DigestAlgorithmNotSupported(algorithm=self.ds.digest_type))

        # Independent of whether or not we considered the digest for
        # validation, issue a warning if we are using a digest type for which
        # validation or signing has been prohibited.
        #
        # Signing is prohibited
        if self.ds.digest_type in DS_DIGEST_ALGS_VALIDATION_PROHIBITED:
            self.warnings.append(Errors.DigestAlgorithmValidationProhibited(algorithm=self.ds.digest_type))
        # Validation is prohibited or, at least, not recommended
        if self.ds.digest_type in DS_DIGEST_ALGS_PROHIBITED:
            self.warnings.append(Errors.DigestAlgorithmProhibited(algorithm=self.ds.digest_type))
        elif self.ds.digest_type in DS_DIGEST_ALGS_NOT_RECOMMENDED:
            self.warnings.append(Errors.DigestAlgorithmNotRecommended(algorithm=self.ds.digest_type))

        if self.dnskey is not None and \
                self.dnskey.rdata.flags & fmt.DNSKEY_FLAGS['revoke']:
            if self.dnskey.key_tag != self.ds.key_tag:
                if self.validation_status == DS_STATUS_VALID:
                    self.validation_status = DS_STATUS_INDETERMINATE_MATCH_PRE_REVOKE
            else:
                self.errors.append(Errors.DNSKEYRevokedDS())
                if self.validation_status == DS_STATUS_VALID:
                    self.validation_status = DS_STATUS_INVALID

        if self.digest_valid == False and self.ds.digest_type in supported_digest_algs:
            # only report this if we're not referring to a key revoked post-DS
            if self.dnskey.key_tag == self.ds.key_tag:
                if self.validation_status == DS_STATUS_VALID:
                    self.validation_status = DS_STATUS_INVALID_DIGEST
                self.errors.append(Errors.DigestInvalid())

        # RFC 4509
        if self.ds.digest_type == 1:
            stronger_algs_all_ds = set()
            # Cycle through all other DS records in the DS RRset, and
            # create a list of digest types that are stronger than SHA1
            # and are being used by DS records across the *entire* DS.
            for ds_rdata in self.ds_meta.rrset:
                if ds_rdata.digest_type in DS_DIGEST_ALGS_STRONGER_THAN_SHA1:
                    stronger_algs_all_ds.add(ds_rdata.digest_type)

            # Consider only digest types that we actually support
            stronger_algs_all_ds.intersection_update(supported_digest_algs)

            if stronger_algs_all_ds:
                # If there are DS records in the DS RRset with digest type
                # stronger than SHA1, then this one MUST be ignored by
                # validators (RFC 4509).
                for digest_alg in stronger_algs_all_ds:
                    if digest_alg in DS_DIGEST_ALGS_IGNORING_SHA1:
                        if self.validation_status == DS_STATUS_VALID:
                            self.validation_status = DS_STATUS_ALGORITHM_IGNORED
                        self.warnings.append(Errors.DSDigestAlgorithmIgnored(algorithm=1, new_algorithm=digest_alg))
                    else:
                        self.warnings.append(Errors.DSDigestAlgorithmMaybeIgnored(algorithm=1, new_algorithm=digest_alg))","stronger_algs_all_ds = set()
for ds_rdata in self.ds_meta.rrset:
    if ds_rdata.digest_type in DS_DIGEST_ALGS_STRONGER_THAN_SHA1:
        stronger_algs_all_ds.add(ds_rdata.digest_type)",stronger_algs_all_ds = {ds_rdata.digest_type for ds_rdata in self.ds_meta.rrset if ds_rdata.digest_type in DS_DIGEST_ALGS_STRONGER_THAN_SHA1}
great_expectations,https://github.com/great-expectations/great_expectations/tree/master//versioneer.py,,do_setup$1748,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (OSError, configparser.NoSectionError, configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"", file=sys.stderr)
            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with open(cfg.versionfile_source, ""w"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(
            LONG
            % {
                ""DOLLAR"": ""$"",
                ""STYLE"": cfg.style,
                ""TAG_PREFIX"": cfg.tag_prefix,
                ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
            }
        )

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy) as f:
                old = f.read()
        except OSError:
            old = """"
        if INIT_PY_SNIPPET not in old:
            print("" appending to %s"" % ipy)
            with open(ipy, ""a"") as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with open(manifest_in) as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except OSError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, ""a"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print(
            "" appending versionfile_source ('%s') to MANIFEST.in""
            % cfg.versionfile_source
        )
        with open(manifest_in, ""a"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-subst keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","simple_includes = set()
for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}
client_python,https://github.com/prometheus/client_python/tree/master/prometheus_client/registry.py,RestrictedRegistry,collect$136,"def collect(self):
        collectors = set()
        target_info_metric = None
        with self._registry._lock:
            if 'target_info' in self._name_set and self._registry._target_info:
                target_info_metric = self._registry._target_info_metric()
            for name in self._name_set:
                if name != 'target_info' and name in self._registry._names_to_collectors:
                    collectors.add(self._registry._names_to_collectors[name])
        if target_info_metric:
            yield target_info_metric
        for collector in collectors:
            for metric in collector.collect():
                m = metric._restricted_metric(self._name_set)
                if m:
                    yield m","collectors = set()
for name in self._name_set:
    if name != 'target_info' and name in self._registry._names_to_collectors:
        collectors.add(self._registry._names_to_collectors[name])",collectors = {self._registry._names_to_collectors[name] for name in self._name_set if name != 'target_info' and name in self._registry._names_to_collectors}
viztracer,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_cmdline.py,TestCommandLineBasic,test_log_async$427,"def test_log_async(self):
        def check_func(data):
            tids = set()
            for entry in data[""traceEvents""]:
                tids.add(entry[""tid""])
            self.assertGreaterEqual(len(tids), 4)

        self.template([""viztracer"", ""--log_async"", ""-o"", ""result.json"", ""cmdline_test.py""],
                      script=file_log_async,
                      expected_output_file=""result.json"",
                      check_func=check_func)","tids = set()
for entry in data['traceEvents']:
    tids.add(entry['tid'])",tids = {entry['tid'] for entry in data['traceEvents']}
Amulet-Map-Editor,https://github.com/Amulet-Team/Amulet-Map-Editor/tree/master/amulet_map_editor/api/lang.py,,get_languages$91,"def get_languages() -> List[str]:
    """"""Get a list of all supported language codes.""""""
    langs = set()
    for d in _lang_dirs:
        for l in glob.glob(os.path.join(d, ""*.lang"")):
            langs.add(os.path.basename(l)[:-5])
    return sorted(langs)","langs = set()
for d in _lang_dirs:
    for l in glob.glob(os.path.join(d, '*.lang')):
        langs.add(os.path.basename(l)[:-5])","langs = {os.path.basename(l)[:-5] for d in _lang_dirs for l in glob.glob(os.path.join(d, '*.lang'))}"
SublimeJEDI,https://github.com/srusskih/SublimeJEDI/tree/master/dependencies/jedi/third_party/django-stubs/mypy_django_plugin/django/context.py,DjangoContext,all_registered_model_classes$210,"def all_registered_model_classes(self) -> Set[Type[models.Model]]:
        model_classes = self.apps_registry.get_models()

        all_model_bases = set()
        for model_cls in model_classes:
            for base_cls in model_cls.mro():
                if issubclass(base_cls, models.Model):
                    all_model_bases.add(base_cls)

        return all_model_bases","all_model_bases = set()
for model_cls in model_classes:
    for base_cls in model_cls.mro():
        if issubclass(base_cls, models.Model):
            all_model_bases.add(base_cls)","all_model_bases = {base_cls for model_cls in model_classes for base_cls in model_cls.mro() if issubclass(base_cls, models.Model)}"
viztracer,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_multiprocess.py,TestMultiprocessing,test_os_fork$276,"def test_os_fork(self):
        def check_func(data):
            pids = set()
            for entry in data[""traceEvents""]:
                pids.add(entry[""pid""])
            self.assertGreater(len(pids), 1)

        if sys.platform in [""linux"", ""linux2""]:
            self.template([""viztracer"", ""-o"", ""result.json"", ""cmdline_test.py""],
                          expected_output_file=""result.json"", script=file_fork, check_func=check_func)","pids = set()
for entry in data['traceEvents']:
    pids.add(entry['pid'])",pids = {entry['pid'] for entry in data['traceEvents']}
huey,https://github.com/coleifer/huey/tree/master/huey/api.py,Huey,flush_locks$616,"def flush_locks(self, *names):
        flushed = set()
        locks = self._locks
        if names:
            lock_template = '%s.lock.%%s' % self.name
            named_locks = (lock_template % name.strip() for name in names)
            locks = itertools.chain(locks, named_locks)

        for lock_key in locks:
            if self.delete(lock_key):
                flushed.add(lock_key.split('.lock.', 1)[-1])

        return flushed","flushed = set()
for lock_key in locks:
    if self.delete(lock_key):
        flushed.add(lock_key.split('.lock.', 1)[-1])","flushed = {lock_key.split('.lock.', 1)[-1] for lock_key in locks if self.delete(lock_key)}"
networkx,https://github.com/networkx/networkx/tree/master/networkx/algorithms/approximation/tests/test_treewidth.py,TestTreewidthMinFillIn,test_heuristic_abort$215,"def test_heuristic_abort(self):
        """"""Test if min_fill_in returns None for fully connected graph""""""
        graph = {}
        for u in self.complete:
            graph[u] = set()
            for v in self.complete[u]:
                if u != v:  # ignore self-loop
                    graph[u].add(v)
        next_node = min_fill_in_heuristic(graph)
        if next_node is None:
            pass
        else:
            assert False","graph[u] = set()
for v in self.complete[u]:
    if u != v:
        graph[u].add(v)",graph[u] = {v for v in self.complete[u] if u != v}
GitPython,https://github.com/gitpython-developers/GitPython/tree/master/test/test_refs.py,TestRefs,test_refs$175,"def test_refs(self):
        types_found = set()
        for ref in self.rorepo.refs:
            types_found.add(type(ref))
        assert len(types_found) >= 3","types_found = set()
for ref in self.rorepo.refs:
    types_found.add(type(ref))",types_found = {type(ref) for ref in self.rorepo.refs}
tahoe-lafs,https://github.com/tahoe-lafs/tahoe-lafs/tree/master/src/allmydata/immutable/happiness_upload.py,,share_placement$332,"def share_placement(peers, readonly_peers, shares, peers_to_shares):
    """"""
    Generates the allocations the upload should based on the given
    information. We construct a dictionary of 'share_num' ->
    'server_id' and return it to the caller. Existing allocations
    appear as placements because attempting to place an existing
    allocation will renew the share.

    For more information on the algorithm this class implements, refer to
    docs/specifications/servers-of-happiness.rst
    """"""
    if not peers:
        return dict()

    # First calculate share placement for the readonly servers.
    readonly_shares = set()
    readonly_map = {}
    for peer in sorted(peers_to_shares.keys()):
        if peer in readonly_peers:
            readonly_map.setdefault(peer, peers_to_shares[peer])
            for share in peers_to_shares[peer]:
                readonly_shares.add(share)

    readonly_mappings = _calculate_mappings(readonly_peers, readonly_shares, readonly_map)
    used_peers, used_shares = _extract_ids(readonly_mappings)

    # Calculate share placement for the remaining existing allocations
    new_peers = set(peers) - used_peers
    # Squash a list of sets into one set
    new_shares = shares - used_shares

    servermap = peers_to_shares.copy()
    for peer in sorted(peers_to_shares.keys()):
        if peer in used_peers:
            servermap.pop(peer, None)
        else:
            servermap[peer] = set(servermap[peer]) - used_shares
            if servermap[peer] == set():
                servermap.pop(peer, None)
                # allmydata.test.test_upload.EncodingParameters.test_exception_messages_during_server_selection
                # allmydata.test.test_upload.EncodingParameters.test_problem_layout_comment_52
                # both ^^ trigger a ""keyerror"" here .. just ignoring is right? (fixes the tests, but ...)
                try:
                    new_peers.remove(peer)
                except KeyError:
                    pass

    existing_mappings = _calculate_mappings(new_peers, new_shares, servermap)
    existing_peers, existing_shares = _extract_ids(existing_mappings)

    # Calculate share placement for the remaining peers and shares which
    # won't be preserved by existing allocations.
    new_peers = new_peers - existing_peers - used_peers


    new_shares = new_shares - existing_shares - used_shares
    new_mappings = _calculate_mappings(new_peers, new_shares)
    #print(""new_peers %s"" % new_peers)
    #print(""new_mappings %s"" % new_mappings)
    mappings = dict(list(readonly_mappings.items()) + list(existing_mappings.items()) + list(new_mappings.items()))
    homeless_shares = set()
    for share in mappings:
        if mappings[share] is None:
            homeless_shares.add(share)
    if len(homeless_shares) != 0:
        # 'servermap' should contain only read/write peers
        _distribute_homeless_shares(
            mappings, homeless_shares,
            {
                k: v
                for k, v in list(peers_to_shares.items())
                if k not in readonly_peers
            }
        )

    # now, if any share is *still* mapped to None that means ""don't
    # care which server it goes on"", so we place it on a round-robin
    # of read-write servers

    def round_robin(peers):
        while True:
            for peer in peers:
                yield peer
    peer_iter = round_robin(peers - readonly_peers)

    return {
        k: v.pop() if v else next(peer_iter)
        for k, v in list(mappings.items())
    }","homeless_shares = set()
for share in mappings:
    if mappings[share] is None:
        homeless_shares.add(share)",homeless_shares = {share for share in mappings if mappings[share] is None}
xonsh,https://github.com/xonsh/xonsh/tree/master/xonsh/ply/ply/lex.py,,lex$864,"def lex(module=None, object=None, debug=False, optimize=False, lextab='lextab',
        reflags=int(re.VERBOSE), nowarn=False, outputdir=None, debuglog=None, errorlog=None):

    if lextab is None:
        lextab = 'lextab'

    global lexer

    ldict = None
    stateinfo  = {'INITIAL': 'inclusive'}
    lexobj = Lexer()
    lexobj.lexoptimize = optimize
    global token, input

    if errorlog is None:
        errorlog = PlyLogger(sys.stderr)

    if debug:
        if debuglog is None:
            debuglog = PlyLogger(sys.stderr)

    # Get the module dictionary used for the lexer
    if object:
        module = object

    # Get the module dictionary used for the parser
    if module:
        _items = [(k, getattr(module, k)) for k in dir(module)]
        ldict = dict(_items)
        # If no __file__ attribute is available, try to obtain it from the __module__ instead
        if '__file__' not in ldict:
            ldict['__file__'] = sys.modules[ldict['__module__']].__file__
    else:
        ldict = get_caller_module_dict(2)

    # Determine if the module is package of a package or not.
    # If so, fix the tabmodule setting so that tables load correctly
    pkg = ldict.get('__package__')
    if pkg and isinstance(lextab, str):
        if '.' not in lextab:
            lextab = pkg + '.' + lextab

    # Collect parser information from the dictionary
    linfo = LexerReflect(ldict, log=errorlog, reflags=reflags)
    linfo.get_all()
    if not optimize:
        if linfo.validate_all():
            raise SyntaxError(""Can't build lexer"")

    if optimize and lextab:
        try:
            lexobj.readtab(lextab, ldict)
            token = lexobj.token
            input = lexobj.input
            lexer = lexobj
            return lexobj

        except ImportError:
            pass

    # Dump some basic debugging information
    if debug:
        debuglog.info('lex: tokens   = %r', linfo.tokens)
        debuglog.info('lex: literals = %r', linfo.literals)
        debuglog.info('lex: states   = %r', linfo.stateinfo)

    # Build a dictionary of valid token names
    lexobj.lextokens = set()
    for n in linfo.tokens:
        lexobj.lextokens.add(n)

    # Get literals specification
    if isinstance(linfo.literals, (list, tuple)):
        lexobj.lexliterals = type(linfo.literals[0])().join(linfo.literals)
    else:
        lexobj.lexliterals = linfo.literals

    lexobj.lextokens_all = lexobj.lextokens | set(lexobj.lexliterals)

    # Get the stateinfo dictionary
    stateinfo = linfo.stateinfo

    regexs = {}
    # Build the master regular expressions
    for state in stateinfo:
        regex_list = []

        # Add rules defined by functions first
        for fname, f in linfo.funcsym[state]:
            regex_list.append('(?P<%s>%s)' % (fname, _get_regex(f)))
            if debug:
                debuglog.info(""lex: Adding rule %s -> '%s' (state '%s')"", fname, _get_regex(f), state)

        # Now add all of the simple rules
        for name, r in linfo.strsym[state]:
            regex_list.append('(?P<%s>%s)' % (name, r))
            if debug:
                debuglog.info(""lex: Adding rule %s -> '%s' (state '%s')"", name, r, state)

        regexs[state] = regex_list

    # Build the master regular expressions

    if debug:
        debuglog.info('lex: ==== MASTER REGEXS FOLLOW ====')

    for state in regexs:
        lexre, re_text, re_names = _form_master_re(regexs[state], reflags, ldict, linfo.toknames)
        lexobj.lexstatere[state] = lexre
        lexobj.lexstateretext[state] = re_text
        lexobj.lexstaterenames[state] = re_names
        if debug:
            for i, text in enumerate(re_text):
                debuglog.info(""lex: state '%s' : regex[%d] = '%s'"", state, i, text)

    # For inclusive states, we need to add the regular expressions from the INITIAL state
    for state, stype in stateinfo.items():
        if state != 'INITIAL' and stype == 'inclusive':
            lexobj.lexstatere[state].extend(lexobj.lexstatere['INITIAL'])
            lexobj.lexstateretext[state].extend(lexobj.lexstateretext['INITIAL'])
            lexobj.lexstaterenames[state].extend(lexobj.lexstaterenames['INITIAL'])

    lexobj.lexstateinfo = stateinfo
    lexobj.lexre = lexobj.lexstatere['INITIAL']
    lexobj.lexretext = lexobj.lexstateretext['INITIAL']
    lexobj.lexreflags = reflags

    # Set up ignore variables
    lexobj.lexstateignore = linfo.ignore
    lexobj.lexignore = lexobj.lexstateignore.get('INITIAL', '')

    # Set up error functions
    lexobj.lexstateerrorf = linfo.errorf
    lexobj.lexerrorf = linfo.errorf.get('INITIAL', None)
    if not lexobj.lexerrorf:
        errorlog.warning('No t_error rule is defined')

    # Set up eof functions
    lexobj.lexstateeoff = linfo.eoff
    lexobj.lexeoff = linfo.eoff.get('INITIAL', None)

    # Check state information for ignore and error rules
    for s, stype in stateinfo.items():
        if stype == 'exclusive':
            if s not in linfo.errorf:
                errorlog.warning(""No error rule is defined for exclusive state '%s'"", s)
            if s not in linfo.ignore and lexobj.lexignore:
                errorlog.warning(""No ignore rule is defined for exclusive state '%s'"", s)
        elif stype == 'inclusive':
            if s not in linfo.errorf:
                linfo.errorf[s] = linfo.errorf.get('INITIAL', None)
            if s not in linfo.ignore:
                linfo.ignore[s] = linfo.ignore.get('INITIAL', '')

    # Create global versions of the token() and input() functions
    token = lexobj.token
    input = lexobj.input
    lexer = lexobj

    # If in optimize mode, we write the lextab
    if lextab and optimize:
        if outputdir is None:
            # If no output directory is set, the location of the output files
            # is determined according to the following rules:
            #     - If lextab specifies a package, files go into that package directory
            #     - Otherwise, files go in the same directory as the specifying module
            if isinstance(lextab, types.ModuleType):
                srcfile = lextab.__file__
            else:
                if '.' not in lextab:
                    srcfile = ldict['__file__']
                else:
                    parts = lextab.split('.')
                    pkgname = '.'.join(parts[:-1])
                    exec('import %s' % pkgname)
                    srcfile = getattr(sys.modules[pkgname], '__file__', '')
            outputdir = os.path.dirname(srcfile)
        try:
            lexobj.writetab(lextab, outputdir)
            if lextab in sys.modules:
                del sys.modules[lextab]
        except IOError as e:
            errorlog.warning(""Couldn't write lextab module %r. %s"" % (lextab, e))

    return lexobj","lexobj.lextokens = set()
for n in linfo.tokens:
    lexobj.lextokens.add(n)",lexobj.lextokens = {n for n in linfo.tokens}
docassemble,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/server.py,,copy_playground_modules$1917,"def copy_playground_modules():
    root_dir = os.path.join(FULL_PACKAGE_DIRECTORY, 'docassemble')
    for d in os.listdir(root_dir):
        if re.search(r'^playground[0-9]', d) and os.path.isdir(os.path.join(root_dir, d)):
            try:
                shutil.rmtree(os.path.join(root_dir, d))
            except:
                sys.stderr.write(""copy_playground_modules: error deleting "" + os.path.join(root_dir, d) + ""\n"")
    devs = set()
    for user in db.session.execute(select(UserModel.id).join(UserRoles, UserModel.id == UserRoles.user_id).join(Role, UserRoles.role_id == Role.id).where(and_(UserModel.active == True, or_(Role.name == 'admin', Role.name == 'developer')))):
        devs.add(user.id)
    for user_id in devs:
        mod_dir = SavedFile(user_id, fix=True, section='playgroundmodules')
        local_dirs = [(os.path.join(FULL_PACKAGE_DIRECTORY, 'docassemble', 'playground' + str(user_id)), mod_dir.directory)]
        for dirname in mod_dir.list_of_dirs():
            local_dirs.append((os.path.join(FULL_PACKAGE_DIRECTORY, 'docassemble', 'playground' + str(user_id) + dirname), os.path.join(mod_dir.directory, dirname)))
        for local_dir, mod_directory in local_dirs:
            if os.path.isdir(local_dir):
                try:
                    shutil.rmtree(local_dir)
                except:
                    sys.stderr.write(""copy_playground_modules: error deleting "" + local_dir + "" before replacing it\n"")
            os.makedirs(local_dir, exist_ok=True)
            #sys.stderr.write(""Copying "" + str(mod_directory) + "" to "" + str(local_dir) + ""\n"")
            for f in [f for f in os.listdir(mod_directory) if re.search(r'^[A-Za-z].*\.py$', f)]:
                shutil.copyfile(os.path.join(mod_directory, f), os.path.join(local_dir, f))
            #shutil.copytree(mod_dir.directory, local_dir)
            with open(os.path.join(local_dir, '__init__.py'), 'w', encoding='utf-8') as the_file:
                the_file.write(init_py_file)","devs = set()
for user in db.session.execute(select(UserModel.id).join(UserRoles, UserModel.id == UserRoles.user_id).join(Role, UserRoles.role_id == Role.id).where(and_(UserModel.active == True, or_(Role.name == 'admin', Role.name == 'developer')))):
    devs.add(user.id)","devs = {user.id for user in db.session.execute(select(UserModel.id).join(UserRoles, UserModel.id == UserRoles.user_id).join(Role, UserRoles.role_id == Role.id).where(and_(UserModel.active == True, or_(Role.name == 'admin', Role.name == 'developer'))))}"
PPLM,https://github.com/uber-research/PPLM/tree/master/paper_code/pytorch_pretrained_bert/file_utils.py,,read_set_from_file$255,"def read_set_from_file(filename):
    '''
    Extract a de-duped collection (set) of text from a file.
    Expected file format is one item per line.
    '''
    collection = set()
    with open(filename, 'r', encoding='utf-8') as file_:
        for line in file_:
            collection.add(line.rstrip())
    return collection","collection = set()
for line in file_:
    collection.add(line.rstrip())",collection = {line.rstrip() for line in file_}
dulwich,https://github.com/dulwich/dulwich/tree/master/dulwich/object_store.py,PackBasedObjectStore,pack_loose_objects$483,"def pack_loose_objects(self):
        """"""Pack loose objects.

        Returns: Number of objects packed
        """"""
        objects = set()
        for sha in self._iter_loose_objects():
            objects.add((self._get_loose_object(sha), None))
        self.add_objects(list(objects))
        for obj, path in objects:
            self._remove_loose_object(obj.id)
        return len(objects)","objects = set()
for sha in self._iter_loose_objects():
    objects.add((self._get_loose_object(sha), None))","objects = {(self._get_loose_object(sha), None) for sha in self._iter_loose_objects()}"
watchdog,https://github.com/gorakhargosh/watchdog/tree/master/tests/test_regex_matching_event_handler.py,,test_dispatch$42,"def test_dispatch():
    # Utilities.
    regexes = [r"".*\.py"", r"".*\.txt""]
    ignore_regexes = [r"".*\.pyc""]

    def assert_regexes(handler, event):
        if hasattr(event, 'dest_path'):
            paths = [event.src_path, event.dest_path]
        else:
            paths = [event.src_path]
        filtered_paths = set()
        for p in paths:
            if any(r.match(p) for r in handler.regexes):
                filtered_paths.add(p)
        assert filtered_paths

    dir_del_event_match = DirDeletedEvent('/path/blah.py')
    dir_del_event_not_match = DirDeletedEvent('/path/foobar')
    dir_del_event_ignored = DirDeletedEvent('/path/foobar.pyc')
    file_del_event_match = FileDeletedEvent('/path/blah.txt')
    file_del_event_not_match = FileDeletedEvent('/path/foobar')
    file_del_event_ignored = FileDeletedEvent('/path/blah.pyc')

    dir_cre_event_match = DirCreatedEvent('/path/blah.py')
    dir_cre_event_not_match = DirCreatedEvent('/path/foobar')
    dir_cre_event_ignored = DirCreatedEvent('/path/foobar.pyc')
    file_cre_event_match = FileCreatedEvent('/path/blah.txt')
    file_cre_event_not_match = FileCreatedEvent('/path/foobar')
    file_cre_event_ignored = FileCreatedEvent('/path/blah.pyc')

    dir_mod_event_match = DirModifiedEvent('/path/blah.py')
    dir_mod_event_not_match = DirModifiedEvent('/path/foobar')
    dir_mod_event_ignored = DirModifiedEvent('/path/foobar.pyc')
    file_mod_event_match = FileModifiedEvent('/path/blah.txt')
    file_mod_event_not_match = FileModifiedEvent('/path/foobar')
    file_mod_event_ignored = FileModifiedEvent('/path/blah.pyc')

    dir_mov_event_match = DirMovedEvent('/path/blah.py', '/path/blah')
    dir_mov_event_not_match = DirMovedEvent('/path/foobar', '/path/blah')
    dir_mov_event_ignored = DirMovedEvent('/path/foobar.pyc', '/path/blah')
    file_mov_event_match = FileMovedEvent('/path/blah.txt', '/path/blah')
    file_mov_event_not_match = FileMovedEvent('/path/foobar', '/path/blah')
    file_mov_event_ignored = FileMovedEvent('/path/blah.pyc', '/path/blah')

    all_dir_events = [
        dir_mod_event_match,
        dir_mod_event_not_match,
        dir_mod_event_ignored,
        dir_del_event_match,
        dir_del_event_not_match,
        dir_del_event_ignored,
        dir_cre_event_match,
        dir_cre_event_not_match,
        dir_cre_event_ignored,
        dir_mov_event_match,
        dir_mov_event_not_match,
        dir_mov_event_ignored,
    ]
    all_file_events = [
        file_mod_event_match,
        file_mod_event_not_match,
        file_mod_event_ignored,
        file_del_event_match,
        file_del_event_not_match,
        file_del_event_ignored,
        file_cre_event_match,
        file_cre_event_not_match,
        file_cre_event_ignored,
        file_mov_event_match,
        file_mov_event_not_match,
        file_mov_event_ignored,
    ]
    all_events = all_file_events + all_dir_events

    def assert_check_directory(handler, event):
        assert not (handler.ignore_directories and event.is_directory)

    class TestableEventHandler(RegexMatchingEventHandler):

        def on_any_event(self, event):
            assert_check_directory(self, event)

        def on_modified(self, event):
            assert_check_directory(self, event)
            assert event.event_type == EVENT_TYPE_MODIFIED
            assert_regexes(self, event)

        def on_deleted(self, event):
            assert_check_directory(self, event)
            assert event.event_type == EVENT_TYPE_DELETED
            assert_regexes(self, event)

        def on_moved(self, event):
            assert_check_directory(self, event)
            assert event.event_type == EVENT_TYPE_MOVED
            assert_regexes(self, event)

        def on_created(self, event):
            assert_check_directory(self, event)
            assert event.event_type == EVENT_TYPE_CREATED
            assert_regexes(self, event)

    no_dirs_handler = TestableEventHandler(regexes=regexes,
                                           ignore_regexes=ignore_regexes,
                                           ignore_directories=True)
    handler = TestableEventHandler(regexes=regexes,
                                   ignore_regexes=ignore_regexes,
                                   ignore_directories=False)

    for event in all_events:
        no_dirs_handler.dispatch(event)
    for event in all_events:

        handler.dispatch(event)","filtered_paths = set()
for p in paths:
    if any((r.match(p) for r in handler.regexes)):
        filtered_paths.add(p)",filtered_paths = {p for p in paths if any((r.match(p) for r in handler.regexes))}
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,TestQuantizationFreezePass,freeze_graph$273,"def freeze_graph(
        self,
        use_cuda,
        seed,
        activation_quant_type,
        bias_correction=False,
        weight_quant_type='abs_max',
        for_ci=True,
        quant_skip_pattern='skip_quant',
    ):
        def build_program(main, startup, is_test):
            main.random_seed = seed
            startup.random_seed = seed
            with fluid.unique_name.guard():
                with fluid.program_guard(main, startup):
                    img = fluid.layers.data(
                        name='image', shape=[1, 28, 28], dtype='float32'
                    )
                    label = fluid.layers.data(
                        name='label', shape=[1], dtype='int64'
                    )
                    loss = conv_net(img, label, quant_skip_pattern)
                    if not is_test:
                        opt = fluid.optimizer.Adam(learning_rate=0.001)
                        opt.minimize(loss)
            return [img, label], loss

        random.seed(0)
        np.random.seed(0)

        main = fluid.Program()
        startup = fluid.Program()
        test_program = fluid.Program()
        feeds, loss = build_program(main, startup, False)
        build_program(test_program, startup, True)
        test_program = test_program.clone(for_test=True)
        main_graph = IrGraph(core.Graph(main.desc), for_test=False)
        test_graph = IrGraph(core.Graph(test_program.desc), for_test=True)

        place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
        exe = fluid.Executor(place)
        scope = fluid.Scope()
        with fluid.scope_guard(scope):
            exe.run(startup)
        transform_pass = QuantizationTransformPass(
            scope=scope,
            place=place,
            activation_quantize_type=activation_quant_type,
            weight_quantize_type=weight_quant_type,
            skip_pattern=quant_skip_pattern,
        )
        transform_pass.apply(main_graph)
        transform_pass = QuantizationTransformPass(
            scope=scope,
            place=place,
            activation_quantize_type=activation_quant_type,
            weight_quantize_type=weight_quant_type,
            skip_pattern=quant_skip_pattern,
        )
        transform_pass.apply(test_graph)
        dev_name = '_gpu_' if use_cuda else '_cpu_'
        if not for_ci:
            marked_nodes = set()
            for op in main_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            main_graph.draw(
                '.',
                'main'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        build_strategy = fluid.BuildStrategy()
        build_strategy.memory_optimize = False
        build_strategy.enable_inplace = False
        build_strategy.fuse_all_reduce_ops = False
        binary = fluid.CompiledProgram(main_graph.graph).with_data_parallel(
            loss_name=loss.name, build_strategy=build_strategy
        )
        quantized_test_program = test_graph.to_program()
        iters = 5
        batch_size = 8

        train_reader = paddle.batch(
            paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=500),
            batch_size=batch_size,
        )
        test_reader = paddle.batch(
            paddle.dataset.mnist.test(), batch_size=batch_size
        )
        feeder = fluid.DataFeeder(feed_list=feeds, place=place)
        with fluid.scope_guard(scope):
            for _ in range(iters):
                data = next(train_reader())
                loss_v = exe.run(
                    binary, feed=feeder.feed(data), fetch_list=[loss]
                )
                if not for_ci:
                    print(
                        '{}: {}'.format(
                            'loss'
                            + dev_name
                            + activation_quant_type
                            + '_'
                            + weight_quant_type,
                            loss_v,
                        )
                    )

        test_data = next(test_reader())
        with fluid.program_guard(quantized_test_program):
            w_var = fluid.framework._get_var(
                'conv2d_1.w_0.quantized', quantized_test_program
            )
        # Testing
        with fluid.scope_guard(scope):
            test_loss1, w_quant = exe.run(
                program=quantized_test_program,
                feed=feeder.feed(test_data),
                fetch_list=[loss, w_var],
            )

        # Freeze graph for inference, but the weight of fc/conv is still float type.
        freeze_pass = QuantizationFreezePass(
            scope=scope,
            place=place,
            bias_correction=bias_correction,
            weight_quantize_type=weight_quant_type,
        )
        freeze_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_freeze'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        server_program = test_graph.to_program()
        with fluid.scope_guard(scope):
            (test_loss2,) = exe.run(
                program=server_program,
                feed=feeder.feed(test_data),
                fetch_list=[loss],
            )
        self.assertAlmostEqual(test_loss1, test_loss2, delta=5e-3)
        if not for_ci:
            print(
                '{}: {}'.format(
                    'test_loss1'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    test_loss1,
                )
            )
            print(
                '{}: {}'.format(
                    'test_loss2'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    test_loss2,
                )
            )
        w_freeze = np.array(scope.find_var('conv2d_1.w_0').get_tensor())
        # Maybe failed, this is due to the calculation precision
        # self.assertAlmostEqual(np.sum(w_freeze), np.sum(w_quant))
        if not for_ci:
            print(
                '{}: {}'.format(
                    'w_freeze'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_freeze),
                )
            )
            print(
                '{}: {}'.format(
                    'w_quant'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_quant),
                )
            )

        # Convert parameter to 8-bit.
        convert_int8_pass = ConvertToInt8Pass(scope=scope, place=place)
        convert_int8_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )
        server_program_int8 = test_graph.to_program()
        # Save the 8-bit parameter and model file.
        with fluid.scope_guard(scope):
            fluid.io.save_inference_model(
                'server_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                ['image', 'label'],
                [loss],
                exe,
                server_program_int8,
            )
            # Test whether the 8-bit parameter and model file can be loaded successfully.
            [infer, feed, fetch] = fluid.io.load_inference_model(
                'server_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                exe,
            )
        # Check the loaded 8-bit weight.
        w_8bit = np.array(scope.find_var('conv2d_1.w_0.int8').get_tensor())
        self.assertEqual(w_8bit.dtype, np.int8)
        self.assertEqual(np.sum(w_8bit), np.sum(w_freeze))
        if not for_ci:
            print(
                '{}: {}'.format(
                    'w_8bit'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_8bit),
                )
            )
            print(
                '{}: {}'.format(
                    'w_freeze'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_freeze),
                )
            )

        mobile_pass = TransformForMobilePass()
        mobile_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_mobile'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        mobile_program = test_graph.to_program()
        with fluid.scope_guard(scope):
            fluid.io.save_inference_model(
                'mobile_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                ['image', 'label'],
                [loss],
                exe,
                mobile_program,
            )","marked_nodes = set()
for op in main_graph.all_op_nodes():
    if op.name().find('quantize') > -1:
        marked_nodes.add(op)",marked_nodes = {op for op in main_graph.all_op_nodes() if op.name().find('quantize') > -1}
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,TestQuantizationFreezePass,freeze_graph$273,"def freeze_graph(
        self,
        use_cuda,
        seed,
        activation_quant_type,
        bias_correction=False,
        weight_quant_type='abs_max',
        for_ci=True,
        quant_skip_pattern='skip_quant',
    ):
        def build_program(main, startup, is_test):
            main.random_seed = seed
            startup.random_seed = seed
            with fluid.unique_name.guard():
                with fluid.program_guard(main, startup):
                    img = fluid.layers.data(
                        name='image', shape=[1, 28, 28], dtype='float32'
                    )
                    label = fluid.layers.data(
                        name='label', shape=[1], dtype='int64'
                    )
                    loss = conv_net(img, label, quant_skip_pattern)
                    if not is_test:
                        opt = fluid.optimizer.Adam(learning_rate=0.001)
                        opt.minimize(loss)
            return [img, label], loss

        random.seed(0)
        np.random.seed(0)

        main = fluid.Program()
        startup = fluid.Program()
        test_program = fluid.Program()
        feeds, loss = build_program(main, startup, False)
        build_program(test_program, startup, True)
        test_program = test_program.clone(for_test=True)
        main_graph = IrGraph(core.Graph(main.desc), for_test=False)
        test_graph = IrGraph(core.Graph(test_program.desc), for_test=True)

        place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
        exe = fluid.Executor(place)
        scope = fluid.Scope()
        with fluid.scope_guard(scope):
            exe.run(startup)
        transform_pass = QuantizationTransformPass(
            scope=scope,
            place=place,
            activation_quantize_type=activation_quant_type,
            weight_quantize_type=weight_quant_type,
            skip_pattern=quant_skip_pattern,
        )
        transform_pass.apply(main_graph)
        transform_pass = QuantizationTransformPass(
            scope=scope,
            place=place,
            activation_quantize_type=activation_quant_type,
            weight_quantize_type=weight_quant_type,
            skip_pattern=quant_skip_pattern,
        )
        transform_pass.apply(test_graph)
        dev_name = '_gpu_' if use_cuda else '_cpu_'
        if not for_ci:
            marked_nodes = set()
            for op in main_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            main_graph.draw(
                '.',
                'main'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        build_strategy = fluid.BuildStrategy()
        build_strategy.memory_optimize = False
        build_strategy.enable_inplace = False
        build_strategy.fuse_all_reduce_ops = False
        binary = fluid.CompiledProgram(main_graph.graph).with_data_parallel(
            loss_name=loss.name, build_strategy=build_strategy
        )
        quantized_test_program = test_graph.to_program()
        iters = 5
        batch_size = 8

        train_reader = paddle.batch(
            paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=500),
            batch_size=batch_size,
        )
        test_reader = paddle.batch(
            paddle.dataset.mnist.test(), batch_size=batch_size
        )
        feeder = fluid.DataFeeder(feed_list=feeds, place=place)
        with fluid.scope_guard(scope):
            for _ in range(iters):
                data = next(train_reader())
                loss_v = exe.run(
                    binary, feed=feeder.feed(data), fetch_list=[loss]
                )
                if not for_ci:
                    print(
                        '{}: {}'.format(
                            'loss'
                            + dev_name
                            + activation_quant_type
                            + '_'
                            + weight_quant_type,
                            loss_v,
                        )
                    )

        test_data = next(test_reader())
        with fluid.program_guard(quantized_test_program):
            w_var = fluid.framework._get_var(
                'conv2d_1.w_0.quantized', quantized_test_program
            )
        # Testing
        with fluid.scope_guard(scope):
            test_loss1, w_quant = exe.run(
                program=quantized_test_program,
                feed=feeder.feed(test_data),
                fetch_list=[loss, w_var],
            )

        # Freeze graph for inference, but the weight of fc/conv is still float type.
        freeze_pass = QuantizationFreezePass(
            scope=scope,
            place=place,
            bias_correction=bias_correction,
            weight_quantize_type=weight_quant_type,
        )
        freeze_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_freeze'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        server_program = test_graph.to_program()
        with fluid.scope_guard(scope):
            (test_loss2,) = exe.run(
                program=server_program,
                feed=feeder.feed(test_data),
                fetch_list=[loss],
            )
        self.assertAlmostEqual(test_loss1, test_loss2, delta=5e-3)
        if not for_ci:
            print(
                '{}: {}'.format(
                    'test_loss1'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    test_loss1,
                )
            )
            print(
                '{}: {}'.format(
                    'test_loss2'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    test_loss2,
                )
            )
        w_freeze = np.array(scope.find_var('conv2d_1.w_0').get_tensor())
        # Maybe failed, this is due to the calculation precision
        # self.assertAlmostEqual(np.sum(w_freeze), np.sum(w_quant))
        if not for_ci:
            print(
                '{}: {}'.format(
                    'w_freeze'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_freeze),
                )
            )
            print(
                '{}: {}'.format(
                    'w_quant'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_quant),
                )
            )

        # Convert parameter to 8-bit.
        convert_int8_pass = ConvertToInt8Pass(scope=scope, place=place)
        convert_int8_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )
        server_program_int8 = test_graph.to_program()
        # Save the 8-bit parameter and model file.
        with fluid.scope_guard(scope):
            fluid.io.save_inference_model(
                'server_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                ['image', 'label'],
                [loss],
                exe,
                server_program_int8,
            )
            # Test whether the 8-bit parameter and model file can be loaded successfully.
            [infer, feed, fetch] = fluid.io.load_inference_model(
                'server_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                exe,
            )
        # Check the loaded 8-bit weight.
        w_8bit = np.array(scope.find_var('conv2d_1.w_0.int8').get_tensor())
        self.assertEqual(w_8bit.dtype, np.int8)
        self.assertEqual(np.sum(w_8bit), np.sum(w_freeze))
        if not for_ci:
            print(
                '{}: {}'.format(
                    'w_8bit'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_8bit),
                )
            )
            print(
                '{}: {}'.format(
                    'w_freeze'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_freeze),
                )
            )

        mobile_pass = TransformForMobilePass()
        mobile_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_mobile'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        mobile_program = test_graph.to_program()
        with fluid.scope_guard(scope):
            fluid.io.save_inference_model(
                'mobile_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                ['image', 'label'],
                [loss],
                exe,
                mobile_program,
            )","marked_nodes = set()
for op in test_graph.all_op_nodes():
    if op.name().find('quantize') > -1:
        marked_nodes.add(op)",marked_nodes = {op for op in test_graph.all_op_nodes() if op.name().find('quantize') > -1}
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,TestQuantizationFreezePass,freeze_graph$273,"def freeze_graph(
        self,
        use_cuda,
        seed,
        activation_quant_type,
        bias_correction=False,
        weight_quant_type='abs_max',
        for_ci=True,
        quant_skip_pattern='skip_quant',
    ):
        def build_program(main, startup, is_test):
            main.random_seed = seed
            startup.random_seed = seed
            with fluid.unique_name.guard():
                with fluid.program_guard(main, startup):
                    img = fluid.layers.data(
                        name='image', shape=[1, 28, 28], dtype='float32'
                    )
                    label = fluid.layers.data(
                        name='label', shape=[1], dtype='int64'
                    )
                    loss = conv_net(img, label, quant_skip_pattern)
                    if not is_test:
                        opt = fluid.optimizer.Adam(learning_rate=0.001)
                        opt.minimize(loss)
            return [img, label], loss

        random.seed(0)
        np.random.seed(0)

        main = fluid.Program()
        startup = fluid.Program()
        test_program = fluid.Program()
        feeds, loss = build_program(main, startup, False)
        build_program(test_program, startup, True)
        test_program = test_program.clone(for_test=True)
        main_graph = IrGraph(core.Graph(main.desc), for_test=False)
        test_graph = IrGraph(core.Graph(test_program.desc), for_test=True)

        place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
        exe = fluid.Executor(place)
        scope = fluid.Scope()
        with fluid.scope_guard(scope):
            exe.run(startup)
        transform_pass = QuantizationTransformPass(
            scope=scope,
            place=place,
            activation_quantize_type=activation_quant_type,
            weight_quantize_type=weight_quant_type,
            skip_pattern=quant_skip_pattern,
        )
        transform_pass.apply(main_graph)
        transform_pass = QuantizationTransformPass(
            scope=scope,
            place=place,
            activation_quantize_type=activation_quant_type,
            weight_quantize_type=weight_quant_type,
            skip_pattern=quant_skip_pattern,
        )
        transform_pass.apply(test_graph)
        dev_name = '_gpu_' if use_cuda else '_cpu_'
        if not for_ci:
            marked_nodes = set()
            for op in main_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            main_graph.draw(
                '.',
                'main'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        build_strategy = fluid.BuildStrategy()
        build_strategy.memory_optimize = False
        build_strategy.enable_inplace = False
        build_strategy.fuse_all_reduce_ops = False
        binary = fluid.CompiledProgram(main_graph.graph).with_data_parallel(
            loss_name=loss.name, build_strategy=build_strategy
        )
        quantized_test_program = test_graph.to_program()
        iters = 5
        batch_size = 8

        train_reader = paddle.batch(
            paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=500),
            batch_size=batch_size,
        )
        test_reader = paddle.batch(
            paddle.dataset.mnist.test(), batch_size=batch_size
        )
        feeder = fluid.DataFeeder(feed_list=feeds, place=place)
        with fluid.scope_guard(scope):
            for _ in range(iters):
                data = next(train_reader())
                loss_v = exe.run(
                    binary, feed=feeder.feed(data), fetch_list=[loss]
                )
                if not for_ci:
                    print(
                        '{}: {}'.format(
                            'loss'
                            + dev_name
                            + activation_quant_type
                            + '_'
                            + weight_quant_type,
                            loss_v,
                        )
                    )

        test_data = next(test_reader())
        with fluid.program_guard(quantized_test_program):
            w_var = fluid.framework._get_var(
                'conv2d_1.w_0.quantized', quantized_test_program
            )
        # Testing
        with fluid.scope_guard(scope):
            test_loss1, w_quant = exe.run(
                program=quantized_test_program,
                feed=feeder.feed(test_data),
                fetch_list=[loss, w_var],
            )

        # Freeze graph for inference, but the weight of fc/conv is still float type.
        freeze_pass = QuantizationFreezePass(
            scope=scope,
            place=place,
            bias_correction=bias_correction,
            weight_quantize_type=weight_quant_type,
        )
        freeze_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_freeze'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        server_program = test_graph.to_program()
        with fluid.scope_guard(scope):
            (test_loss2,) = exe.run(
                program=server_program,
                feed=feeder.feed(test_data),
                fetch_list=[loss],
            )
        self.assertAlmostEqual(test_loss1, test_loss2, delta=5e-3)
        if not for_ci:
            print(
                '{}: {}'.format(
                    'test_loss1'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    test_loss1,
                )
            )
            print(
                '{}: {}'.format(
                    'test_loss2'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    test_loss2,
                )
            )
        w_freeze = np.array(scope.find_var('conv2d_1.w_0').get_tensor())
        # Maybe failed, this is due to the calculation precision
        # self.assertAlmostEqual(np.sum(w_freeze), np.sum(w_quant))
        if not for_ci:
            print(
                '{}: {}'.format(
                    'w_freeze'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_freeze),
                )
            )
            print(
                '{}: {}'.format(
                    'w_quant'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_quant),
                )
            )

        # Convert parameter to 8-bit.
        convert_int8_pass = ConvertToInt8Pass(scope=scope, place=place)
        convert_int8_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )
        server_program_int8 = test_graph.to_program()
        # Save the 8-bit parameter and model file.
        with fluid.scope_guard(scope):
            fluid.io.save_inference_model(
                'server_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                ['image', 'label'],
                [loss],
                exe,
                server_program_int8,
            )
            # Test whether the 8-bit parameter and model file can be loaded successfully.
            [infer, feed, fetch] = fluid.io.load_inference_model(
                'server_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                exe,
            )
        # Check the loaded 8-bit weight.
        w_8bit = np.array(scope.find_var('conv2d_1.w_0.int8').get_tensor())
        self.assertEqual(w_8bit.dtype, np.int8)
        self.assertEqual(np.sum(w_8bit), np.sum(w_freeze))
        if not for_ci:
            print(
                '{}: {}'.format(
                    'w_8bit'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_8bit),
                )
            )
            print(
                '{}: {}'.format(
                    'w_freeze'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_freeze),
                )
            )

        mobile_pass = TransformForMobilePass()
        mobile_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_mobile'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        mobile_program = test_graph.to_program()
        with fluid.scope_guard(scope):
            fluid.io.save_inference_model(
                'mobile_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                ['image', 'label'],
                [loss],
                exe,
                mobile_program,
            )","marked_nodes = set()
for op in test_graph.all_op_nodes():
    if op.name().find('quantize') > -1:
        marked_nodes.add(op)",marked_nodes = {op for op in test_graph.all_op_nodes() if op.name().find('quantize') > -1}
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,TestQuantizationFreezePass,freeze_graph$273,"def freeze_graph(
        self,
        use_cuda,
        seed,
        activation_quant_type,
        bias_correction=False,
        weight_quant_type='abs_max',
        for_ci=True,
        quant_skip_pattern='skip_quant',
    ):
        def build_program(main, startup, is_test):
            main.random_seed = seed
            startup.random_seed = seed
            with fluid.unique_name.guard():
                with fluid.program_guard(main, startup):
                    img = fluid.layers.data(
                        name='image', shape=[1, 28, 28], dtype='float32'
                    )
                    label = fluid.layers.data(
                        name='label', shape=[1], dtype='int64'
                    )
                    loss = conv_net(img, label, quant_skip_pattern)
                    if not is_test:
                        opt = fluid.optimizer.Adam(learning_rate=0.001)
                        opt.minimize(loss)
            return [img, label], loss

        random.seed(0)
        np.random.seed(0)

        main = fluid.Program()
        startup = fluid.Program()
        test_program = fluid.Program()
        feeds, loss = build_program(main, startup, False)
        build_program(test_program, startup, True)
        test_program = test_program.clone(for_test=True)
        main_graph = IrGraph(core.Graph(main.desc), for_test=False)
        test_graph = IrGraph(core.Graph(test_program.desc), for_test=True)

        place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
        exe = fluid.Executor(place)
        scope = fluid.Scope()
        with fluid.scope_guard(scope):
            exe.run(startup)
        transform_pass = QuantizationTransformPass(
            scope=scope,
            place=place,
            activation_quantize_type=activation_quant_type,
            weight_quantize_type=weight_quant_type,
            skip_pattern=quant_skip_pattern,
        )
        transform_pass.apply(main_graph)
        transform_pass = QuantizationTransformPass(
            scope=scope,
            place=place,
            activation_quantize_type=activation_quant_type,
            weight_quantize_type=weight_quant_type,
            skip_pattern=quant_skip_pattern,
        )
        transform_pass.apply(test_graph)
        dev_name = '_gpu_' if use_cuda else '_cpu_'
        if not for_ci:
            marked_nodes = set()
            for op in main_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            main_graph.draw(
                '.',
                'main'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        build_strategy = fluid.BuildStrategy()
        build_strategy.memory_optimize = False
        build_strategy.enable_inplace = False
        build_strategy.fuse_all_reduce_ops = False
        binary = fluid.CompiledProgram(main_graph.graph).with_data_parallel(
            loss_name=loss.name, build_strategy=build_strategy
        )
        quantized_test_program = test_graph.to_program()
        iters = 5
        batch_size = 8

        train_reader = paddle.batch(
            paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=500),
            batch_size=batch_size,
        )
        test_reader = paddle.batch(
            paddle.dataset.mnist.test(), batch_size=batch_size
        )
        feeder = fluid.DataFeeder(feed_list=feeds, place=place)
        with fluid.scope_guard(scope):
            for _ in range(iters):
                data = next(train_reader())
                loss_v = exe.run(
                    binary, feed=feeder.feed(data), fetch_list=[loss]
                )
                if not for_ci:
                    print(
                        '{}: {}'.format(
                            'loss'
                            + dev_name
                            + activation_quant_type
                            + '_'
                            + weight_quant_type,
                            loss_v,
                        )
                    )

        test_data = next(test_reader())
        with fluid.program_guard(quantized_test_program):
            w_var = fluid.framework._get_var(
                'conv2d_1.w_0.quantized', quantized_test_program
            )
        # Testing
        with fluid.scope_guard(scope):
            test_loss1, w_quant = exe.run(
                program=quantized_test_program,
                feed=feeder.feed(test_data),
                fetch_list=[loss, w_var],
            )

        # Freeze graph for inference, but the weight of fc/conv is still float type.
        freeze_pass = QuantizationFreezePass(
            scope=scope,
            place=place,
            bias_correction=bias_correction,
            weight_quantize_type=weight_quant_type,
        )
        freeze_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_freeze'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        server_program = test_graph.to_program()
        with fluid.scope_guard(scope):
            (test_loss2,) = exe.run(
                program=server_program,
                feed=feeder.feed(test_data),
                fetch_list=[loss],
            )
        self.assertAlmostEqual(test_loss1, test_loss2, delta=5e-3)
        if not for_ci:
            print(
                '{}: {}'.format(
                    'test_loss1'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    test_loss1,
                )
            )
            print(
                '{}: {}'.format(
                    'test_loss2'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    test_loss2,
                )
            )
        w_freeze = np.array(scope.find_var('conv2d_1.w_0').get_tensor())
        # Maybe failed, this is due to the calculation precision
        # self.assertAlmostEqual(np.sum(w_freeze), np.sum(w_quant))
        if not for_ci:
            print(
                '{}: {}'.format(
                    'w_freeze'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_freeze),
                )
            )
            print(
                '{}: {}'.format(
                    'w_quant'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_quant),
                )
            )

        # Convert parameter to 8-bit.
        convert_int8_pass = ConvertToInt8Pass(scope=scope, place=place)
        convert_int8_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )
        server_program_int8 = test_graph.to_program()
        # Save the 8-bit parameter and model file.
        with fluid.scope_guard(scope):
            fluid.io.save_inference_model(
                'server_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                ['image', 'label'],
                [loss],
                exe,
                server_program_int8,
            )
            # Test whether the 8-bit parameter and model file can be loaded successfully.
            [infer, feed, fetch] = fluid.io.load_inference_model(
                'server_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                exe,
            )
        # Check the loaded 8-bit weight.
        w_8bit = np.array(scope.find_var('conv2d_1.w_0.int8').get_tensor())
        self.assertEqual(w_8bit.dtype, np.int8)
        self.assertEqual(np.sum(w_8bit), np.sum(w_freeze))
        if not for_ci:
            print(
                '{}: {}'.format(
                    'w_8bit'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_8bit),
                )
            )
            print(
                '{}: {}'.format(
                    'w_freeze'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_freeze),
                )
            )

        mobile_pass = TransformForMobilePass()
        mobile_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_mobile'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        mobile_program = test_graph.to_program()
        with fluid.scope_guard(scope):
            fluid.io.save_inference_model(
                'mobile_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                ['image', 'label'],
                [loss],
                exe,
                mobile_program,
            )","marked_nodes = set()
for op in test_graph.all_op_nodes():
    if op.name().find('quantize') > -1:
        marked_nodes.add(op)",marked_nodes = {op for op in test_graph.all_op_nodes() if op.name().find('quantize') > -1}
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/tests/test_quantization_pass.py,TestQuantizationFreezePass,freeze_graph$273,"def freeze_graph(
        self,
        use_cuda,
        seed,
        activation_quant_type,
        bias_correction=False,
        weight_quant_type='abs_max',
        for_ci=True,
        quant_skip_pattern='skip_quant',
    ):
        def build_program(main, startup, is_test):
            main.random_seed = seed
            startup.random_seed = seed
            with fluid.unique_name.guard():
                with fluid.program_guard(main, startup):
                    img = fluid.layers.data(
                        name='image', shape=[1, 28, 28], dtype='float32'
                    )
                    label = fluid.layers.data(
                        name='label', shape=[1], dtype='int64'
                    )
                    loss = conv_net(img, label, quant_skip_pattern)
                    if not is_test:
                        opt = fluid.optimizer.Adam(learning_rate=0.001)
                        opt.minimize(loss)
            return [img, label], loss

        random.seed(0)
        np.random.seed(0)

        main = fluid.Program()
        startup = fluid.Program()
        test_program = fluid.Program()
        feeds, loss = build_program(main, startup, False)
        build_program(test_program, startup, True)
        test_program = test_program.clone(for_test=True)
        main_graph = IrGraph(core.Graph(main.desc), for_test=False)
        test_graph = IrGraph(core.Graph(test_program.desc), for_test=True)

        place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
        exe = fluid.Executor(place)
        scope = fluid.Scope()
        with fluid.scope_guard(scope):
            exe.run(startup)
        transform_pass = QuantizationTransformPass(
            scope=scope,
            place=place,
            activation_quantize_type=activation_quant_type,
            weight_quantize_type=weight_quant_type,
            skip_pattern=quant_skip_pattern,
        )
        transform_pass.apply(main_graph)
        transform_pass = QuantizationTransformPass(
            scope=scope,
            place=place,
            activation_quantize_type=activation_quant_type,
            weight_quantize_type=weight_quant_type,
            skip_pattern=quant_skip_pattern,
        )
        transform_pass.apply(test_graph)
        dev_name = '_gpu_' if use_cuda else '_cpu_'
        if not for_ci:
            marked_nodes = set()
            for op in main_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            main_graph.draw(
                '.',
                'main'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        build_strategy = fluid.BuildStrategy()
        build_strategy.memory_optimize = False
        build_strategy.enable_inplace = False
        build_strategy.fuse_all_reduce_ops = False
        binary = fluid.CompiledProgram(main_graph.graph).with_data_parallel(
            loss_name=loss.name, build_strategy=build_strategy
        )
        quantized_test_program = test_graph.to_program()
        iters = 5
        batch_size = 8

        train_reader = paddle.batch(
            paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=500),
            batch_size=batch_size,
        )
        test_reader = paddle.batch(
            paddle.dataset.mnist.test(), batch_size=batch_size
        )
        feeder = fluid.DataFeeder(feed_list=feeds, place=place)
        with fluid.scope_guard(scope):
            for _ in range(iters):
                data = next(train_reader())
                loss_v = exe.run(
                    binary, feed=feeder.feed(data), fetch_list=[loss]
                )
                if not for_ci:
                    print(
                        '{}: {}'.format(
                            'loss'
                            + dev_name
                            + activation_quant_type
                            + '_'
                            + weight_quant_type,
                            loss_v,
                        )
                    )

        test_data = next(test_reader())
        with fluid.program_guard(quantized_test_program):
            w_var = fluid.framework._get_var(
                'conv2d_1.w_0.quantized', quantized_test_program
            )
        # Testing
        with fluid.scope_guard(scope):
            test_loss1, w_quant = exe.run(
                program=quantized_test_program,
                feed=feeder.feed(test_data),
                fetch_list=[loss, w_var],
            )

        # Freeze graph for inference, but the weight of fc/conv is still float type.
        freeze_pass = QuantizationFreezePass(
            scope=scope,
            place=place,
            bias_correction=bias_correction,
            weight_quantize_type=weight_quant_type,
        )
        freeze_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_freeze'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        server_program = test_graph.to_program()
        with fluid.scope_guard(scope):
            (test_loss2,) = exe.run(
                program=server_program,
                feed=feeder.feed(test_data),
                fetch_list=[loss],
            )
        self.assertAlmostEqual(test_loss1, test_loss2, delta=5e-3)
        if not for_ci:
            print(
                '{}: {}'.format(
                    'test_loss1'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    test_loss1,
                )
            )
            print(
                '{}: {}'.format(
                    'test_loss2'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    test_loss2,
                )
            )
        w_freeze = np.array(scope.find_var('conv2d_1.w_0').get_tensor())
        # Maybe failed, this is due to the calculation precision
        # self.assertAlmostEqual(np.sum(w_freeze), np.sum(w_quant))
        if not for_ci:
            print(
                '{}: {}'.format(
                    'w_freeze'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_freeze),
                )
            )
            print(
                '{}: {}'.format(
                    'w_quant'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_quant),
                )
            )

        # Convert parameter to 8-bit.
        convert_int8_pass = ConvertToInt8Pass(scope=scope, place=place)
        convert_int8_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )
        server_program_int8 = test_graph.to_program()
        # Save the 8-bit parameter and model file.
        with fluid.scope_guard(scope):
            fluid.io.save_inference_model(
                'server_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                ['image', 'label'],
                [loss],
                exe,
                server_program_int8,
            )
            # Test whether the 8-bit parameter and model file can be loaded successfully.
            [infer, feed, fetch] = fluid.io.load_inference_model(
                'server_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                exe,
            )
        # Check the loaded 8-bit weight.
        w_8bit = np.array(scope.find_var('conv2d_1.w_0.int8').get_tensor())
        self.assertEqual(w_8bit.dtype, np.int8)
        self.assertEqual(np.sum(w_8bit), np.sum(w_freeze))
        if not for_ci:
            print(
                '{}: {}'.format(
                    'w_8bit'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_8bit),
                )
            )
            print(
                '{}: {}'.format(
                    'w_freeze'
                    + dev_name
                    + activation_quant_type
                    + '_'
                    + weight_quant_type,
                    np.sum(w_freeze),
                )
            )

        mobile_pass = TransformForMobilePass()
        mobile_pass.apply(test_graph)
        if not for_ci:
            marked_nodes = set()
            for op in test_graph.all_op_nodes():
                if op.name().find('quantize') > -1:
                    marked_nodes.add(op)
            test_graph.draw(
                '.',
                'test_mobile'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                marked_nodes,
            )

        mobile_program = test_graph.to_program()
        with fluid.scope_guard(scope):
            fluid.io.save_inference_model(
                'mobile_int8'
                + dev_name
                + activation_quant_type
                + '_'
                + weight_quant_type,
                ['image', 'label'],
                [loss],
                exe,
                mobile_program,
            )","marked_nodes = set()
for op in test_graph.all_op_nodes():
    if op.name().find('quantize') > -1:
        marked_nodes.add(op)",marked_nodes = {op for op in test_graph.all_op_nodes() if op.name().find('quantize') > -1}
llvmlite,https://github.com/numba/llvmlite/tree/master//versioneer.py,cmd_update_files,run$970,"def run(self):
        print("" creating %s"" % versionfile_source)
        with open(versionfile_source, ""w"") as f:
            assert VCS is not None, ""please set versioneer.VCS""
            LONG = LONG_VERSION_PY[VCS]
            f.write(LONG % {""DOLLAR"": ""$"",
                            ""TAG_PREFIX"": tag_prefix,
                            ""PARENTDIR_PREFIX"": parentdir_prefix,
                            ""VERSIONFILE_SOURCE"": versionfile_source,
                            })

        ipy = os.path.join(os.path.dirname(versionfile_source), ""__init__.py"")
        if os.path.exists(ipy):
            try:
                with open(ipy, ""r"") as f:
                    old = f.read()
            except EnvironmentError:
                old = """"
            if INIT_PY_SNIPPET not in old:
                print("" appending to %s"" % ipy)
                with open(ipy, ""a"") as f:
                    f.write(INIT_PY_SNIPPET)
            else:
                print("" %s unmodified"" % ipy)
        else:
            print("" %s doesn't exist, ok"" % ipy)
            ipy = None

        # Make sure both the top-level ""versioneer.py"" and versionfile_source
        # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
        # they'll be copied into source distributions. Pip won't be able to
        # install the package without this.
        manifest_in = os.path.join(get_root(), ""MANIFEST.in"")
        simple_includes = set()
        try:
            with open(manifest_in, ""r"") as f:
                for line in f:
                    if line.startswith(""include ""):
                        for include in line.split()[1:]:
                            simple_includes.add(include)
        except EnvironmentError:
            pass
        # That doesn't cover everything MANIFEST.in can do
        # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
        # it might give some false negatives. Appending redundant 'include'
        # lines is safe, though.
        if ""versioneer.py"" not in simple_includes:
            print("" appending 'versioneer.py' to MANIFEST.in"")
            with open(manifest_in, ""a"") as f:
                f.write(""include versioneer.py\n"")
        else:
            print("" 'versioneer.py' already in MANIFEST.in"")
        if versionfile_source not in simple_includes:
            print("" appending versionfile_source ('%s') to MANIFEST.in"" %
                  versionfile_source)
            with open(manifest_in, ""a"") as f:
                f.write(""include %s\n"" % versionfile_source)
        else:
            print("" versionfile_source already in MANIFEST.in"")

        # Make VCS-specific changes. For git, this means creating/changing
        # .gitattributes to mark _version.py for export-time keyword
        # substitution.
        do_vcs_install(manifest_in, versionfile_source, ipy)","simple_includes = set()
for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}
MatchZoo-py,https://github.com/NTMC-Community/MatchZoo-py/tree/master/matchzoo/preprocessors/units/frequency_filter.py,FrequencyFilter,fit$52,"def fit(self, list_of_tokens: typing.List[typing.List[str]]):
        """"""Fit `list_of_tokens` by calculating `mode` states.""""""
        valid_terms = set()
        if self._mode == 'tf':
            stats = self._tf(list_of_tokens)
        elif self._mode == 'df':
            stats = self._df(list_of_tokens)
        elif self._mode == 'idf':
            stats = self._idf(list_of_tokens)
        else:
            raise ValueError(f""{self._mode} is not a valid filtering mode.""
                             f""Mode must be one of `tf`, `df`, and `idf`."")

        for k, v in stats.items():
            if self._low <= v < self._high:
                valid_terms.add(k)

        self._context[self._mode] = valid_terms","valid_terms = set()
for (k, v) in stats.items():
    if self._low <= v < self._high:
        valid_terms.add(k)","valid_terms = {k for (k, v) in stats.items() if self._low <= v < self._high}"
viztracer,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_multiprocess.py,TestSubprocess,check_func$256,"def check_func(data):
            pids = set()
            for entry in data[""traceEvents""]:
                pids.add(entry[""pid""])
            self.assertEqual(len(pids), 3)","pids = set()
for entry in data['traceEvents']:
    pids.add(entry['pid'])",pids = {entry['pid'] for entry in data['traceEvents']}
opt_einsum,https://github.com/dgasmith/opt_einsum/tree/master//versioneer.py,,do_setup$1697,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError,
            configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"",
                  file=sys.stderr)
            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with open(cfg.versionfile_source, ""w"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {""DOLLAR"": ""$"",
                        ""STYLE"": cfg.style,
                        ""TAG_PREFIX"": cfg.tag_prefix,
                        ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
                        })

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),
                       ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy, ""r"") as f:
                old = f.read()
        except EnvironmentError:
            old = """"
        if INIT_PY_SNIPPET not in old:
            print("" appending to %s"" % ipy)
            with open(ipy, ""a"") as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with open(manifest_in, ""r"") as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, ""a"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" %
              cfg.versionfile_source)
        with open(manifest_in, ""a"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-subst keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","simple_includes = set()
for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}
tiny_python_projects,https://github.com/kyclark/tiny_python_projects/tree/master/20_password/solution.py,,main$67,"def main():
    args = get_args()
    random.seed(args.seed)  # <1>
    words = set()

    def word_len(word):
        return args.min_word_len <= len(word) <= args.max_word_len

    for fh in args.file:
        for line in fh:
            for word in filter(word_len, map(clean, line.lower().split())):
                words.add(word.title())

    words = sorted(words)
    passwords = [
        ''.join(random.sample(words, args.num_words)) for _ in range(args.num)
    ]

    if args.l33t:
        passwords = map(l33t, passwords)

    print('\n'.join(passwords))","words = set()
for fh in args.file:
    for line in fh:
        for word in filter(word_len, map(clean, line.lower().split())):
            words.add(word.title())","words = {word.title() for fh in args.file for line in fh for word in filter(word_len, map(clean, line.lower().split()))}"
rally,https://github.com/elastic/rally/tree/master/esrally/mechanic/mechanic.py,,extract_all_node_ips$308,"def extract_all_node_ips(ip_port_pairs):
    all_node_ips = set()
    for ip, _ in ip_port_pairs:
        all_node_ips.add(ip)
    return all_node_ips","all_node_ips = set()
for (ip, _) in ip_port_pairs:
    all_node_ips.add(ip)","all_node_ips = {ip for (ip, _) in ip_port_pairs}"
viztracer,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_multiprocess.py,TestSubprocess,test_basic$222,"def test_basic(self):
        def check_func(data):
            pids = set()
            for entry in data[""traceEvents""]:
                pids.add(entry[""pid""])
            self.assertEqual(len(pids), 4)
        self.template([""viztracer"", ""-o"", ""result.json"", ""cmdline_test.py""],
                      expected_output_file=""result.json"", script=file_parent, check_func=check_func)","pids = set()
for entry in data['traceEvents']:
    pids.add(entry['pid'])",pids = {entry['pid'] for entry in data['traceEvents']}
self-critical.pytorch,https://github.com/ruotianluo/self-critical.pytorch/tree/master/scripts/dump_to_lmdb.py,,if_main_my$207,"if __name__ == ""__main__"":
    global args
    args = parse_args()

    args.output_file += args.folder.split('/')[-1]
    if args.folder.find('/') > 0:
        args.output_file = args.folder[:args.folder.rfind('/')+1]+args.output_file
    print(args.output_file)

    img_list = json.load(open(args.input_json, 'r'))['images']
    fn_list = [str(_['cocoid']) for _ in img_list]
    found_ids = set()
    try:
        with open(args.output_file, 'r') as tsvfile:
            reader = csv.DictReader(tsvfile, delimiter='\t', fieldnames=FIELDNAMES)
            for item in reader:
                if item['status'] == 'True':
                    found_ids.add(item['image_id'])
    except:
        pass
    fn_list = [_ for _ in fn_list if _ not in found_ids]
    folder2lmdb(args.folder, fn_list)

    # Test existing.
    found_ids = set()
    with open(args.output_file, 'r') as tsvfile:
        reader = csv.DictReader(tsvfile, delimiter='\t', fieldnames=FIELDNAMES)
        for item in reader:
            if item['status'] == 'True':
                found_ids.add(item['image_id'])

    folder_dataset = FolderLMDB(args.folder+'.lmdb', list(found_ids))
    data_loader = DataLoader(folder_dataset, num_workers=16, collate_fn=lambda x: x)
    for data in tqdm.tqdm(data_loader):
        assert data[0] is not None","found_ids = set()
for item in reader:
    if item['status'] == 'True':
        found_ids.add(item['image_id'])",found_ids = {item['image_id'] for item in reader if item['status'] == 'True'}
self-critical.pytorch,https://github.com/ruotianluo/self-critical.pytorch/tree/master/scripts/dump_to_lmdb.py,,if_main_my$207,"if __name__ == ""__main__"":
    global args
    args = parse_args()

    args.output_file += args.folder.split('/')[-1]
    if args.folder.find('/') > 0:
        args.output_file = args.folder[:args.folder.rfind('/')+1]+args.output_file
    print(args.output_file)

    img_list = json.load(open(args.input_json, 'r'))['images']
    fn_list = [str(_['cocoid']) for _ in img_list]
    found_ids = set()
    try:
        with open(args.output_file, 'r') as tsvfile:
            reader = csv.DictReader(tsvfile, delimiter='\t', fieldnames=FIELDNAMES)
            for item in reader:
                if item['status'] == 'True':
                    found_ids.add(item['image_id'])
    except:
        pass
    fn_list = [_ for _ in fn_list if _ not in found_ids]
    folder2lmdb(args.folder, fn_list)

    # Test existing.
    found_ids = set()
    with open(args.output_file, 'r') as tsvfile:
        reader = csv.DictReader(tsvfile, delimiter='\t', fieldnames=FIELDNAMES)
        for item in reader:
            if item['status'] == 'True':
                found_ids.add(item['image_id'])

    folder_dataset = FolderLMDB(args.folder+'.lmdb', list(found_ids))
    data_loader = DataLoader(folder_dataset, num_workers=16, collate_fn=lambda x: x)
    for data in tqdm.tqdm(data_loader):
        assert data[0] is not None","found_ids = set()
for item in reader:
    if item['status'] == 'True':
        found_ids.add(item['image_id'])",found_ids = {item['image_id'] for item in reader if item['status'] == 'True'}
coveragepy,https://github.com/nedbat/coveragepy/tree/master/coverage/parser.py,PythonParser,lines_matching$87,"def lines_matching(self, *regexes):
        """"""Find the lines matching one of a list of regexes.

        Returns a set of line numbers, the lines that contain a match for one
        of the regexes in `regexes`.  The entire line needn't match, just a
        part of it.

        """"""
        combined = join_regex(regexes)
        regex_c = re.compile(combined)
        matches = set()
        for i, ltext in enumerate(self.lines, start=1):
            if regex_c.search(ltext):
                matches.add(i)
        return matches","matches = set()
for (i, ltext) in enumerate(self.lines, start=1):
    if regex_c.search(ltext):
        matches.add(i)","matches = {i for (i, ltext) in enumerate(self.lines, start=1) if regex_c.search(ltext)}"
shuup,https://github.com/shuup/shuup/tree/master/shuup/campaigns/utils/campaigns.py,,get_lines_suppliers$11,"def get_lines_suppliers(basket):
    """"""
    Returns a list of all suppliers from the basket
    """"""
    suppliers = set()
    for line in basket.get_lines():
        if line.supplier:
            suppliers.add(line.supplier)
    return suppliers","suppliers = set()
for line in basket.get_lines():
    if line.supplier:
        suppliers.add(line.supplier)",suppliers = {line.supplier for line in basket.get_lines() if line.supplier}
aiortc,https://github.com/aiortc/aiortc/tree/master/src/aiortc/rtcdtlstransport.py,RTCDtlsTransport,_register_rtp_receiver$638,"def _register_rtp_receiver(
        self, receiver, parameters: RTCRtpReceiveParameters
    ) -> None:
        ssrcs = set()
        for encoding in parameters.encodings:
            ssrcs.add(encoding.ssrc)

        self._rtp_header_extensions_map.configure(parameters)
        self._rtp_router.register_receiver(
            receiver,
            ssrcs=list(ssrcs),
            payload_types=[codec.payloadType for codec in parameters.codecs],
            mid=parameters.muxId,
        )","ssrcs = set()
for encoding in parameters.encodings:
    ssrcs.add(encoding.ssrc)",ssrcs = {encoding.ssrc for encoding in parameters.encodings}
pyro,https://github.com/pyro-ppl/pyro/tree/master/tests/nn/test_autoregressive.py,AutoRegressiveNNTests,_test_masks$61,"def _test_masks(
        self, input_dim, observed_dim, hidden_dims, permutation, output_dim_multiplier
    ):
        masks, mask_skip = create_mask(
            input_dim, observed_dim, hidden_dims, permutation, output_dim_multiplier
        )

        # First test that hidden layer masks are adequately connected
        # Tracing backwards, works out what inputs each output is connected to
        # It's a dictionary of sets indexed by a tuple (input_dim, param_dim)
        permutation = list(permutation.numpy())

        # Loop over variables
        for idx in range(input_dim):
            # Calculate correct answer
            correct = torch.cat(
                (
                    torch.arange(observed_dim, dtype=torch.long),
                    torch.tensor(
                        sorted(permutation[0 : permutation.index(idx)]),
                        dtype=torch.long,
                    )
                    + observed_dim,
                )
            )

            # Loop over parameters for each variable
            for jdx in range(output_dim_multiplier):
                prev_connections = set()
                # Do output-to-penultimate hidden layer mask
                for kdx in range(masks[-1].size(1)):
                    if masks[-1][idx + jdx * input_dim, kdx]:
                        prev_connections.add(kdx)

                # Do hidden-to-hidden, and hidden-to-input layer masks
                for m in reversed(masks[:-1]):
                    this_connections = set()
                    for kdx in prev_connections:
                        for ldx in range(m.size(1)):
                            if m[kdx, ldx]:
                                this_connections.add(ldx)
                    prev_connections = this_connections

                assert (
                    torch.tensor(list(sorted(prev_connections)), dtype=torch.long)
                    == correct
                ).all()

                # Test the skip-connections mask
                skip_connections = set()
                for kdx in range(mask_skip.size(1)):
                    if mask_skip[idx + jdx * input_dim, kdx]:
                        skip_connections.add(kdx)
                assert (
                    torch.tensor(list(sorted(skip_connections)), dtype=torch.long)
                    == correct
                ).all()","prev_connections = set()
for kdx in range(masks[-1].size(1)):
    if masks[-1][idx + jdx * input_dim, kdx]:
        prev_connections.add(kdx)","prev_connections = {kdx for kdx in range(masks[-1].size(1)) if masks[-1][idx + jdx * input_dim, kdx]}"
pyro,https://github.com/pyro-ppl/pyro/tree/master/tests/nn/test_autoregressive.py,AutoRegressiveNNTests,_test_masks$61,"def _test_masks(
        self, input_dim, observed_dim, hidden_dims, permutation, output_dim_multiplier
    ):
        masks, mask_skip = create_mask(
            input_dim, observed_dim, hidden_dims, permutation, output_dim_multiplier
        )

        # First test that hidden layer masks are adequately connected
        # Tracing backwards, works out what inputs each output is connected to
        # It's a dictionary of sets indexed by a tuple (input_dim, param_dim)
        permutation = list(permutation.numpy())

        # Loop over variables
        for idx in range(input_dim):
            # Calculate correct answer
            correct = torch.cat(
                (
                    torch.arange(observed_dim, dtype=torch.long),
                    torch.tensor(
                        sorted(permutation[0 : permutation.index(idx)]),
                        dtype=torch.long,
                    )
                    + observed_dim,
                )
            )

            # Loop over parameters for each variable
            for jdx in range(output_dim_multiplier):
                prev_connections = set()
                # Do output-to-penultimate hidden layer mask
                for kdx in range(masks[-1].size(1)):
                    if masks[-1][idx + jdx * input_dim, kdx]:
                        prev_connections.add(kdx)

                # Do hidden-to-hidden, and hidden-to-input layer masks
                for m in reversed(masks[:-1]):
                    this_connections = set()
                    for kdx in prev_connections:
                        for ldx in range(m.size(1)):
                            if m[kdx, ldx]:
                                this_connections.add(ldx)
                    prev_connections = this_connections

                assert (
                    torch.tensor(list(sorted(prev_connections)), dtype=torch.long)
                    == correct
                ).all()

                # Test the skip-connections mask
                skip_connections = set()
                for kdx in range(mask_skip.size(1)):
                    if mask_skip[idx + jdx * input_dim, kdx]:
                        skip_connections.add(kdx)
                assert (
                    torch.tensor(list(sorted(skip_connections)), dtype=torch.long)
                    == correct
                ).all()","skip_connections = set()
for kdx in range(mask_skip.size(1)):
    if mask_skip[idx + jdx * input_dim, kdx]:
        skip_connections.add(kdx)","skip_connections = {kdx for kdx in range(mask_skip.size(1)) if mask_skip[idx + jdx * input_dim, kdx]}"
pyro,https://github.com/pyro-ppl/pyro/tree/master/tests/nn/test_autoregressive.py,AutoRegressiveNNTests,_test_masks$61,"def _test_masks(
        self, input_dim, observed_dim, hidden_dims, permutation, output_dim_multiplier
    ):
        masks, mask_skip = create_mask(
            input_dim, observed_dim, hidden_dims, permutation, output_dim_multiplier
        )

        # First test that hidden layer masks are adequately connected
        # Tracing backwards, works out what inputs each output is connected to
        # It's a dictionary of sets indexed by a tuple (input_dim, param_dim)
        permutation = list(permutation.numpy())

        # Loop over variables
        for idx in range(input_dim):
            # Calculate correct answer
            correct = torch.cat(
                (
                    torch.arange(observed_dim, dtype=torch.long),
                    torch.tensor(
                        sorted(permutation[0 : permutation.index(idx)]),
                        dtype=torch.long,
                    )
                    + observed_dim,
                )
            )

            # Loop over parameters for each variable
            for jdx in range(output_dim_multiplier):
                prev_connections = set()
                # Do output-to-penultimate hidden layer mask
                for kdx in range(masks[-1].size(1)):
                    if masks[-1][idx + jdx * input_dim, kdx]:
                        prev_connections.add(kdx)

                # Do hidden-to-hidden, and hidden-to-input layer masks
                for m in reversed(masks[:-1]):
                    this_connections = set()
                    for kdx in prev_connections:
                        for ldx in range(m.size(1)):
                            if m[kdx, ldx]:
                                this_connections.add(ldx)
                    prev_connections = this_connections

                assert (
                    torch.tensor(list(sorted(prev_connections)), dtype=torch.long)
                    == correct
                ).all()

                # Test the skip-connections mask
                skip_connections = set()
                for kdx in range(mask_skip.size(1)):
                    if mask_skip[idx + jdx * input_dim, kdx]:
                        skip_connections.add(kdx)
                assert (
                    torch.tensor(list(sorted(skip_connections)), dtype=torch.long)
                    == correct
                ).all()","this_connections = set()
for kdx in prev_connections:
    for ldx in range(m.size(1)):
        if m[kdx, ldx]:
            this_connections.add(ldx)","this_connections = {ldx for kdx in prev_connections for ldx in range(m.size(1)) if m[kdx, ldx]}"
oppia,https://github.com/oppia/oppia/tree/master/core/domain/suggestion_registry.py,CommunityContributionStats,get_translation_language_codes_that_need_reviewers$1344,"def get_translation_language_codes_that_need_reviewers(self) -> Set[str]:
        """"""Returns the language codes where more reviewers are needed to review
        translations in those language codes. Translation suggestions in a
        given language need more reviewers if the number of translation
        suggestions in that language divided by the number of translation
        reviewers in that language is greater than
        config_domain.MAX_NUMBER_OF_SUGGESTIONS_PER_REVIEWER.

        Returns:
            set. A set of of the language codes where more translation reviewers
            are needed.
        """"""
        language_codes_that_need_reviewers = set()
        for language_code in self.translation_suggestion_counts_by_lang_code:
            if self.are_translation_reviewers_needed_for_lang_code(
                    language_code):
                language_codes_that_need_reviewers.add(language_code)
        return language_codes_that_need_reviewers","language_codes_that_need_reviewers = set()
for language_code in self.translation_suggestion_counts_by_lang_code:
    if self.are_translation_reviewers_needed_for_lang_code(language_code):
        language_codes_that_need_reviewers.add(language_code)",language_codes_that_need_reviewers = {language_code for language_code in self.translation_suggestion_counts_by_lang_code if self.are_translation_reviewers_needed_for_lang_code(language_code)}
DeepCTR,https://github.com/shenweichen/DeepCTR/tree/master/deepctr/layers/interaction.py,OutterProductLayer,build$805,"def build(self, input_shape):

        if not isinstance(input_shape, list) or len(input_shape) < 2:
            raise ValueError('A `OutterProductLayer` layer should be called '
                             'on a list of at least 2 inputs')

        reduced_inputs_shapes = [shape.as_list() for shape in input_shape]
        shape_set = set()

        for i in range(len(input_shape)):
            shape_set.add(tuple(reduced_inputs_shapes[i]))

        if len(shape_set) > 1:
            raise ValueError('A `OutterProductLayer` layer requires '
                             'inputs with same shapes '
                             'Got different shapes: %s' % (shape_set))

        if len(input_shape[0]) != 3 or input_shape[0][1] != 1:
            raise ValueError('A `OutterProductLayer` layer requires '
                             'inputs of a list with same shape tensor like (None,1,embedding_size)'
                             'Got different shapes: %s' % (input_shape[0]))
        num_inputs = len(input_shape)
        num_pairs = int(num_inputs * (num_inputs - 1) / 2)
        input_shape = input_shape[0]
        embed_size = int(input_shape[-1])
        if self.kernel_type == 'mat':

            self.kernel = self.add_weight(shape=(embed_size, num_pairs, embed_size),
                                          initializer=glorot_uniform(
                                              seed=self.seed),
                                          name='kernel')
        elif self.kernel_type == 'vec':
            self.kernel = self.add_weight(shape=(num_pairs, embed_size,), initializer=glorot_uniform(self.seed),
                                          name='kernel'
                                          )
        elif self.kernel_type == 'num':
            self.kernel = self.add_weight(
                shape=(num_pairs, 1), initializer=glorot_uniform(self.seed), name='kernel')

        super(OutterProductLayer, self).build(
            input_shape)","shape_set = set()
for i in range(len(input_shape)):
    shape_set.add(tuple(reduced_inputs_shapes[i]))",shape_set = {tuple(reduced_inputs_shapes[i]) for i in range(len(input_shape))}
pywikibot,https://github.com/wikimedia/pywikibot/tree/master/scripts/archivebot.py,PageArchiver,run$761,"def run(self) -> None:
        """"""Process a single DiscussionPage object.""""""
        if not self.page.botMayEdit():
            return
        whys = self.analyze_page()
        mintoarchive = int(self.get_attr('minthreadstoarchive', 2))
        if self.archived_threads < mintoarchive:
            # We might not want to archive a measly few threads
            # (lowers edit frequency)
            pywikibot.output('Only {} (< {}) threads are old enough. Skipping'
                             .format(self.archived_threads, mintoarchive))
            return
        if whys:
            # Search for the marker template
            rx = re.compile(r'\{\{%s\s*?\n.*?\n\}\}'
                            % (template_title_regex(self.tpl).pattern),
                            re.DOTALL)
            if not rx.search(self.page.header):
                raise MalformedConfigError(
                    ""Couldn't find the template in the header""
                )

            pywikibot.output('Archiving {} thread(s).'
                             .format(self.archived_threads))
            # Save the archives first (so that bugs don't cause a loss of data)
            for title, archive in sorted(self.archives.items()):
                count = archive.archived_threads
                if count == 0:
                    continue
                self.comment_params['count'] = count
                comment = i18n.twtranslate(self.site.code,
                                           'archivebot-archive-summary',
                                           self.comment_params)
                archive.update(comment)

            # Save the page itself
            self.page.header = rx.sub(self.attr2text(), self.page.header)
            self.comment_params['count'] = self.archived_threads
            comma = self.site.mediawiki_message('comma-separator')
            self.comment_params['archives'] = comma.join(
                a.title(as_link=True) for a in self.archives.values()
                if a.archived_threads > 0
            )
            # Find out the reasons and return them localized
            translated_whys = set()
            for why, arg in whys:
                # Archived by timestamp
                if why == 'duration':
                    translated_whys.add(
                        i18n.twtranslate(self.site.code,
                                         'archivebot-older-than',
                                         {'duration': arg,
                                          'count': self.archived_threads}))
                # TODO: handle unsigned or archived by template
            self.comment_params['why'] = comma.join(translated_whys)
            comment = i18n.twtranslate(self.site.code,
                                       'archivebot-page-summary',
                                       self.comment_params)
            self.page.update(comment)","translated_whys = set()
for (why, arg) in whys:
    if why == 'duration':
        translated_whys.add(i18n.twtranslate(self.site.code, 'archivebot-older-than', {'duration': arg, 'count': self.archived_threads}))","translated_whys = {i18n.twtranslate(self.site.code, 'archivebot-older-than', {'duration': arg, 'count': self.archived_threads}) for (why, arg) in whys if why == 'duration'}"
angr,https://github.com/angr/angr/tree/master/angr/analyses/decompiler/ail_simplifier.py,AILSimplifier,_unify_local_variables$148,"def _unify_local_variables(self) -> bool:
        """"""
        Find variables that are definitely equivalent and then eliminate unnecessary copies.
        """"""

        simplified = False

        prop = self._compute_propagation()
        if not prop.equivalence:
            return simplified

        addr_and_idx_to_block: Dict[Tuple[int,int], Block] = { }
        for block in self.func_graph.nodes():
            addr_and_idx_to_block[(block.addr, block.idx)] = block

        equivalences: Dict[Any,Set[Equivalence]] = defaultdict(set)
        for eq in prop.equivalence:
            equivalences[eq.atom1].add(eq)

        for _, eqs in equivalences.items():
            if len(eqs) > 1:
                continue

            eq = next(iter(eqs))

            # Acceptable equivalence classes:
            #
            # stack variable == register
            # register variable == register
            # stack variable == Conv(register, M->N)
            #
            the_def = None
            if isinstance(eq.atom0, SimStackVariable):
                if isinstance(eq.atom1, Register):
                    # stack_var == register
                    reg = eq.atom1
                elif isinstance(eq.atom1, Convert) and isinstance(eq.atom1.operand, Register):
                    # stack_var == Conv(register, M->N)
                    reg = eq.atom1.operand
                else:
                    continue

            elif isinstance(eq.atom0, Register):
                if isinstance(eq.atom1, Register):
                    # register == register
                    reg = eq.atom1
                else:
                    continue

            else:
                continue

            # find the definition of this register
            rd = self._compute_reaching_definitions()
            defs = rd.all_uses.get_uses_by_location(eq.codeloc)
            if len(defs) != 1:
                # there are multiple defs for this register - we do not support replacing all of them
                continue
            for def_ in defs:
                def_: Definition
                if isinstance(def_.atom, atoms.Register) and def_.atom.reg_offset == reg.reg_offset:
                    # found it!
                    the_def = def_
                    break

            if the_def is None:
                continue

            if isinstance(the_def.codeloc, ExternalCodeLocation):
                # this is a function argument. we enter a slightly different logic and try to eliminate copies of this
                # argument if
                # (a) the on-stack copy of it has never been modified in this function
                # (b) the function argument register has never been updated.
                #     TODO: we may loosen requirement (b) once we have real register versioning in AIL.
                defs = [ def_ for def_ in rd.all_definitions if def_.codeloc == eq.codeloc ]
                all_uses_with_def = None
                to_replace, replace_with = None, None
                remove_initial_assignment = None

                if defs and len(defs) == 1:
                    stackvar_def = defs[0]
                    if isinstance(stackvar_def.atom, atoms.MemoryLocation) and isinstance(stackvar_def.atom.addr, SpOffset):
                        # found the stack variable
                        # Make sure there is no other write to this location
                        if any((def_ != stackvar_def and def_.atom == stackvar_def.atom) for def_ in rd.all_definitions if isinstance(def_.atom, atoms.MemoryLocation)):
                            continue

                        # Make sure the register is never updated across this function
                        if any((def_ != the_def and def_.atom == the_def.atom) for def_ in rd.all_definitions if isinstance(def_.atom, atoms.Register)):
                            continue

                        # find all its uses
                        all_stackvar_uses: Set[CodeLocation] = set(rd.all_uses.get_uses(stackvar_def))
                        all_uses_with_def = set()
                        for use in all_stackvar_uses:
                            all_uses_with_def.add((stackvar_def, use))

                        to_replace = Load(None, StackBaseOffset(None, self.project.arch.bits, eq.atom0.offset), eq.atom0.size,
                                          endness=self.project.arch.memory_endness)
                        replace_with = eq.atom1
                        remove_initial_assignment = True

                if all_uses_with_def is None:
                    continue

            else:
                # find all uses of this definition
                # we make a copy of the set since we may touch the set (uses) when replacing expressions
                all_uses: Set[CodeLocation] = set(rd.all_uses.get_uses(the_def))
                all_uses_with_def = set((the_def, use) for use in all_uses)

                remove_initial_assignment = False  # expression folding will take care of it
                if isinstance(eq.atom0, SimStackVariable):
                    # create the memory loading expression
                    to_replace = eq.atom1
                    replace_with = Load(None, StackBaseOffset(None, self.project.arch.bits, eq.atom0.offset), eq.atom0.size,
                               endness=self.project.arch.memory_endness)
                elif isinstance(eq.atom0, Register):
                    to_replace = eq.atom1
                    replace_with = eq.atom0
                else:
                    raise RuntimeError(""Unsupported atom0 type %s."" % type(eq.atom0))

            # TODO: We can only replace all these uses with the stack variable if the stack variable isn't
            # TODO: re-assigned of a new value. Perform this check.

            # replace all uses
            all_uses_replaced = True
            for def_, u in all_uses_with_def:
                if u == eq.codeloc:
                    # skip the very initial assignment location
                    continue
                old_block = addr_and_idx_to_block.get((u.block_addr, u.block_idx), None)
                if old_block is None:
                    continue

                # if there is an updated block, use it
                the_block = self.blocks.get(old_block, old_block)
                stmt: Statement = the_block.statements[u.stmt_idx]

                r, new_block = self._replace_expr_and_update_block(the_block, u.stmt_idx, stmt, def_, u, to_replace,
                                                                   replace_with)
                if r:
                    self.blocks[old_block] = new_block
                else:
                    # failed to replace a use - we need to keep the initial assignment!
                    all_uses_replaced = False
                simplified |= r

            if all_uses_replaced and remove_initial_assignment:
                # the initial statement can be removed
                self._assignments_to_remove.add(eq.codeloc)

        # no need to clear cache at the end of this function
        return simplified","all_uses_with_def = set()
for use in all_stackvar_uses:
    all_uses_with_def.add((stackvar_def, use))","all_uses_with_def = {(stackvar_def, use) for use in all_stackvar_uses}"
Automatic-Udemy-Course-Enroller-GET-PAID-UDEMY-COURSES-for-FREE,https://github.com/aapatre/Automatic-Udemy-Course-Enroller-GET-PAID-UDEMY-COURSES-for-FREE/tree/master/udemy_enroller/scrapers/tutorialbar.py,TutorialBarScraper,_filter_ad_domains$62,"def _filter_ad_domains(self, udemy_links) -> List:
        """"""
        Filter out any known ad domains from the links scraped

        :param list udemy_links: List of urls to filter ad domains from
        :return: A list of filtered urls
        """"""
        ad_links = set()
        for link in udemy_links:
            for ad_domain in self.AD_DOMAINS:
                if link.startswith(ad_domain):
                    ad_links.add(link)
        if ad_links:
            logger.info(f""Removing ad links from courses: {ad_links}"")
        return list(set(udemy_links) - ad_links)","ad_links = set()
for link in udemy_links:
    for ad_domain in self.AD_DOMAINS:
        if link.startswith(ad_domain):
            ad_links.add(link)",ad_links = {link for link in udemy_links for ad_domain in self.AD_DOMAINS if link.startswith(ad_domain)}
neutron,https://github.com/openstack/neutron/tree/master/neutron/agent/l3/extensions/port_forwarding.py,PortForwardingAgentExtension,_sync_and_remove_fip$395,"def _sync_and_remove_fip(self, context, fip_id_cidrs, device, ri):
        if not fip_id_cidrs:
            return
        ha_port = ri.router.get(constants.HA_INTERFACE_KEY)
        fip_ids = [item[0] for item in fip_id_cidrs]
        pfs = self.resource_rpc.bulk_pull(context, resources.PORTFORWARDING,
                                          filter_kwargs={
                                              'floatingip_id': fip_ids})
        exist_fips = set()
        fip_status = {}
        for pf in pfs:
            exist_fips.add(pf.floatingip_id)

        for fip_id_cidr in fip_id_cidrs:
            if fip_id_cidr[0] not in exist_fips:
                if ha_port:
                    ri._remove_vip(fip_id_cidr[1])
                else:
                    device.delete_addr_and_conntrack_state(fip_id_cidr[1])
                fip_status[fip_id_cidr[0]] = 'DOWN'

        if ha_port:
            ri.enable_keepalived()
        self._sending_port_forwarding_fip_status(ri, fip_status)
        for fip_id in fip_status.keys():
            self.mapping.clear_by_fip(fip_id, ri.router_id)","exist_fips = set()
for pf in pfs:
    exist_fips.add(pf.floatingip_id)",exist_fips = {pf.floatingip_id for pf in pfs}
cloudtracker,https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,Athena,__init__$164,"def __init__(self, config, account, start, end, args):
        # Mute boto except errors
        logging.getLogger(""botocore"").setLevel(logging.WARN)
        logging.info(
            ""Source of CloudTrail logs: s3://{bucket}/{path}"".format(
                bucket=config[""s3_bucket""], path=config[""path""]
            )
        )

        # Check start date is not older than a year, as we only create partitions for that far back
        if (
            datetime.datetime.now() - datetime.datetime.strptime(start, ""%Y-%m-%d"")
        ).days > 365:
            raise Exception(
                ""Start date is over a year old. CloudTracker does not create or use partitions over a year old.""
            )

        #
        # Create date filtering
        #
        month_restrictions = set()
        start = start.split(""-"")
        end = end.split(""-"")

        if start[0] == end[0]:
            for month in range(int(start[1]), int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
        else:
            # Add restrictions for months in start year
            for month in range(int(start[1]), 12 + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
            # Add restrictions for months in middle years
            for year in range(int(start[0]), int(end[0])):
                for month in (1, 12 + 1):
                    month_restrictions.add(
                        ""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month)
                    )
            # Add restrictions for months in final year
            for month in range(1, int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month)
                )

        # Combine date filters and add error filter
        self.search_filter = (
            ""(("" + "" or "".join(month_restrictions) + "") and errorcode IS NULL)""
        )

        self.table_name = ""cloudtrail_logs_{}"".format(account[""id""])

        #
        # Display the AWS identity (doubles as a check that boto creds are setup)
        #
        sts = boto3.client(""sts"")
        identity = sts.get_caller_identity()
        logging.info(""Using AWS identity: {}"".format(identity[""Arn""]))
        current_account_id = identity[""Account""]
        region = boto3.session.Session().region_name

        if ""output_s3_bucket"" in config:
            self.output_bucket = config[""output_s3_bucket""]
        else:
            self.output_bucket = ""s3://aws-athena-query-results-{}-{}"".format(
                current_account_id, region
            )
        logging.info(""Using output bucket: {}"".format(self.output_bucket))

        if ""workgroup"" in config:
            self.workgroup = config[""workgroup""]
        logging.info(""Using workgroup: {}"".format(self.workgroup))

        if not config.get('org_id'):
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], account_id=account[""id""]
            )
        else:
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], org_id=config[""org_id""], account_id=account[""id""]
            )

        logging.info(""Account cloudtrail log path: {}"".format(cloudtrail_log_path))

        # Open connections to needed AWS services
        self.athena = boto3.client(""athena"")
        self.s3 = boto3.client(""s3"")

        if args.skip_setup:
            logging.info(""Skipping initial table creation"")
            return

        # Check we can access the S3 bucket
        resp = self.s3.list_objects_v2(
            Bucket=config[""s3_bucket""], Prefix=config[""path""], MaxKeys=1
        )
        if ""Contents"" not in resp or len(resp[""Contents""]) == 0:
            exit(
                ""ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}"".format(
                    bucket=config[""s3_bucket""], path=config[""path""]
                )
            )

        # Ensure our database exists
        self.query_athena(
            ""CREATE DATABASE IF NOT EXISTS {db} {comment}"".format(
                db=self.database, comment=""COMMENT 'Created by CloudTracker'""
            ),
            context=None,
        )

        #
        # Set up table
        #
        query = """"""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (
            `eventversion` string COMMENT 'from deserializer', 
            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', 
            `eventtime` string COMMENT 'from deserializer', 
            `eventsource` string COMMENT 'from deserializer', 
            `eventname` string COMMENT 'from deserializer', 
            `awsregion` string COMMENT 'from deserializer', 
            `sourceipaddress` string COMMENT 'from deserializer', 
            `useragent` string COMMENT 'from deserializer', 
            `errorcode` string COMMENT 'from deserializer', 
            `errormessage` string COMMENT 'from deserializer', 
            `requestparameters` string COMMENT 'from deserializer', 
            `responseelements` string COMMENT 'from deserializer', 
            `additionaleventdata` string COMMENT 'from deserializer', 
            `requestid` string COMMENT 'from deserializer', 
            `eventid` string COMMENT 'from deserializer', 
            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', 
            `eventtype` string COMMENT 'from deserializer', 
            `apiversion` string COMMENT 'from deserializer', 
            `readonly` string COMMENT 'from deserializer', 
            `recipientaccountid` string COMMENT 'from deserializer', 
            `serviceeventdetails` string COMMENT 'from deserializer', 
            `sharedeventid` string COMMENT 'from deserializer', 
            `vpcendpointid` string COMMENT 'from deserializer')
            PARTITIONED BY (region string, year string, month string)
            ROW FORMAT SERDE 
            'com.amazon.emr.hive.serde.CloudTrailSerde' 
            STORED AS INPUTFORMAT 
            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' 
            OUTPUTFORMAT 
            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
            LOCATION '{cloudtrail_log_path}'"""""".format(
            table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path
        )
        self.query_athena(query)

        #
        # Create partitions
        #

        logging.info(
            ""Checking if all partitions for the past {} months exist"".format(
                NUM_MONTHS_FOR_PARTITIONS
            )
        )

        # Get list of current partitions
        query = ""SHOW PARTITIONS {table_name}"".format(table_name=self.table_name)
        partition_list = self.query_athena(query, skip_header=False)

        partition_set = set()
        for partition in partition_list:
            partition_set.add(partition[0])

        # Get region list. Using ec2 here just because it exists in all regions.
        regions = boto3.session.Session().get_available_regions(""ec2"")

        queries_to_make = set()

        # Iterate over every month for the past year and build queries to run to create partitions
        for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
            date_of_interest = datetime.datetime.now() - relativedelta(
                months=num_months_ago
            )
            year = date_of_interest.year
            month = ""{:0>2}"".format(date_of_interest.month)

            query = """"

            for region in regions:
                if (
                    ""region={region}/year={year}/month={month}"".format(
                        region=region, year=year, month=month
                    )
                    in partition_set
                ):
                    continue

                query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(
                    region=region,
                    year=year,
                    month=month,
                    cloudtrail_log_path=cloudtrail_log_path,
                )
            if query != """":
                queries_to_make.add(
                    ""ALTER TABLE {table_name} ADD "".format(table_name=self.table_name)
                    + query
                )

        # Run the queries
        query_count = len(queries_to_make)
        for query in queries_to_make:
            logging.info(""Partition groups remaining to create: {}"".format(query_count))
            self.query_athena(query)
            query_count -= 1","partition_set = set()
for partition in partition_list:
    partition_set.add(partition[0])",partition_set = {partition[0] for partition in partition_list}
cloudtracker,https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,Athena,__init__$164,"def __init__(self, config, account, start, end, args):
        # Mute boto except errors
        logging.getLogger(""botocore"").setLevel(logging.WARN)
        logging.info(
            ""Source of CloudTrail logs: s3://{bucket}/{path}"".format(
                bucket=config[""s3_bucket""], path=config[""path""]
            )
        )

        # Check start date is not older than a year, as we only create partitions for that far back
        if (
            datetime.datetime.now() - datetime.datetime.strptime(start, ""%Y-%m-%d"")
        ).days > 365:
            raise Exception(
                ""Start date is over a year old. CloudTracker does not create or use partitions over a year old.""
            )

        #
        # Create date filtering
        #
        month_restrictions = set()
        start = start.split(""-"")
        end = end.split(""-"")

        if start[0] == end[0]:
            for month in range(int(start[1]), int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
        else:
            # Add restrictions for months in start year
            for month in range(int(start[1]), 12 + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month)
                )
            # Add restrictions for months in middle years
            for year in range(int(start[0]), int(end[0])):
                for month in (1, 12 + 1):
                    month_restrictions.add(
                        ""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month)
                    )
            # Add restrictions for months in final year
            for month in range(1, int(end[1]) + 1):
                month_restrictions.add(
                    ""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month)
                )

        # Combine date filters and add error filter
        self.search_filter = (
            ""(("" + "" or "".join(month_restrictions) + "") and errorcode IS NULL)""
        )

        self.table_name = ""cloudtrail_logs_{}"".format(account[""id""])

        #
        # Display the AWS identity (doubles as a check that boto creds are setup)
        #
        sts = boto3.client(""sts"")
        identity = sts.get_caller_identity()
        logging.info(""Using AWS identity: {}"".format(identity[""Arn""]))
        current_account_id = identity[""Account""]
        region = boto3.session.Session().region_name

        if ""output_s3_bucket"" in config:
            self.output_bucket = config[""output_s3_bucket""]
        else:
            self.output_bucket = ""s3://aws-athena-query-results-{}-{}"".format(
                current_account_id, region
            )
        logging.info(""Using output bucket: {}"".format(self.output_bucket))

        if ""workgroup"" in config:
            self.workgroup = config[""workgroup""]
        logging.info(""Using workgroup: {}"".format(self.workgroup))

        if not config.get('org_id'):
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], account_id=account[""id""]
            )
        else:
            cloudtrail_log_path = ""s3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail"".format(
                bucket=config[""s3_bucket""], path=config[""path""], org_id=config[""org_id""], account_id=account[""id""]
            )

        logging.info(""Account cloudtrail log path: {}"".format(cloudtrail_log_path))

        # Open connections to needed AWS services
        self.athena = boto3.client(""athena"")
        self.s3 = boto3.client(""s3"")

        if args.skip_setup:
            logging.info(""Skipping initial table creation"")
            return

        # Check we can access the S3 bucket
        resp = self.s3.list_objects_v2(
            Bucket=config[""s3_bucket""], Prefix=config[""path""], MaxKeys=1
        )
        if ""Contents"" not in resp or len(resp[""Contents""]) == 0:
            exit(
                ""ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}"".format(
                    bucket=config[""s3_bucket""], path=config[""path""]
                )
            )

        # Ensure our database exists
        self.query_athena(
            ""CREATE DATABASE IF NOT EXISTS {db} {comment}"".format(
                db=self.database, comment=""COMMENT 'Created by CloudTracker'""
            ),
            context=None,
        )

        #
        # Set up table
        #
        query = """"""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (
            `eventversion` string COMMENT 'from deserializer', 
            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', 
            `eventtime` string COMMENT 'from deserializer', 
            `eventsource` string COMMENT 'from deserializer', 
            `eventname` string COMMENT 'from deserializer', 
            `awsregion` string COMMENT 'from deserializer', 
            `sourceipaddress` string COMMENT 'from deserializer', 
            `useragent` string COMMENT 'from deserializer', 
            `errorcode` string COMMENT 'from deserializer', 
            `errormessage` string COMMENT 'from deserializer', 
            `requestparameters` string COMMENT 'from deserializer', 
            `responseelements` string COMMENT 'from deserializer', 
            `additionaleventdata` string COMMENT 'from deserializer', 
            `requestid` string COMMENT 'from deserializer', 
            `eventid` string COMMENT 'from deserializer', 
            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', 
            `eventtype` string COMMENT 'from deserializer', 
            `apiversion` string COMMENT 'from deserializer', 
            `readonly` string COMMENT 'from deserializer', 
            `recipientaccountid` string COMMENT 'from deserializer', 
            `serviceeventdetails` string COMMENT 'from deserializer', 
            `sharedeventid` string COMMENT 'from deserializer', 
            `vpcendpointid` string COMMENT 'from deserializer')
            PARTITIONED BY (region string, year string, month string)
            ROW FORMAT SERDE 
            'com.amazon.emr.hive.serde.CloudTrailSerde' 
            STORED AS INPUTFORMAT 
            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' 
            OUTPUTFORMAT 
            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
            LOCATION '{cloudtrail_log_path}'"""""".format(
            table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path
        )
        self.query_athena(query)

        #
        # Create partitions
        #

        logging.info(
            ""Checking if all partitions for the past {} months exist"".format(
                NUM_MONTHS_FOR_PARTITIONS
            )
        )

        # Get list of current partitions
        query = ""SHOW PARTITIONS {table_name}"".format(table_name=self.table_name)
        partition_list = self.query_athena(query, skip_header=False)

        partition_set = set()
        for partition in partition_list:
            partition_set.add(partition[0])

        # Get region list. Using ec2 here just because it exists in all regions.
        regions = boto3.session.Session().get_available_regions(""ec2"")

        queries_to_make = set()

        # Iterate over every month for the past year and build queries to run to create partitions
        for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
            date_of_interest = datetime.datetime.now() - relativedelta(
                months=num_months_ago
            )
            year = date_of_interest.year
            month = ""{:0>2}"".format(date_of_interest.month)

            query = """"

            for region in regions:
                if (
                    ""region={region}/year={year}/month={month}"".format(
                        region=region, year=year, month=month
                    )
                    in partition_set
                ):
                    continue

                query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(
                    region=region,
                    year=year,
                    month=month,
                    cloudtrail_log_path=cloudtrail_log_path,
                )
            if query != """":
                queries_to_make.add(
                    ""ALTER TABLE {table_name} ADD "".format(table_name=self.table_name)
                    + query
                )

        # Run the queries
        query_count = len(queries_to_make)
        for query in queries_to_make:
            logging.info(""Partition groups remaining to create: {}"".format(query_count))
            self.query_athena(query)
            query_count -= 1","month_restrictions = set()
for month in range(int(start[1]), int(end[1]) + 1):
    month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))","month_restrictions = {""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month) for month in range(int(start[1]), int(end[1]) + 1)}"
text_renderer,https://github.com/Sanster/text_renderer/tree/master/libs/font_utils.py,,check_font_chars$62,"def check_font_chars(ttf, charset):
    """"""
    Get font supported chars and unsupported chars
    :param ttf: TTFont ojbect
    :param charset: chars
    :return: unsupported_chars, supported_chars
    """"""
    #chars = chain.from_iterable([y + (Unicode[y[0]],) for y in x.cmap.items()] for x in ttf[""cmap""].tables)
    chars_int=set()
    for table in ttf['cmap'].tables:
        for k,v in table.cmap.items():
            chars_int.add(k)            

    unsupported_chars = []
    supported_chars = []
    for c in charset:
        if ord(c) not in chars_int:
            unsupported_chars.append(c)
        else:
            supported_chars.append(c)

    ttf.close()
    return unsupported_chars, supported_chars","chars_int = set()
for table in ttf['cmap'].tables:
    for (k, v) in table.cmap.items():
        chars_int.add(k)","chars_int = {k for table in ttf['cmap'].tables for (k, v) in table.cmap.items()}"
MozDef,https://github.com/mozilla/MozDef/tree/master/alerts/proxy_drop_non_standard_port.py,AlertProxyDropNonStandardPort,onAggregation$38,"def onAggregation(self, aggreg):
        category = ""squid""
        tags = [""squid"", ""proxy""]
        severity = ""WARNING""

        destinations = set()
        for event in aggreg[""allevents""]:
            destinations.add(event[""_source""][""details""][""destination""])

        summary = ""Suspicious Proxy DROP event(s) detected from {0} to the following non-std port destination(s): {1}"".format(
            aggreg[""value""], "","".join(sorted(destinations))
        )

        # Create the alert object based on these properties
        return self.createAlertDict(summary, category, tags, aggreg[""events""], severity)","destinations = set()
for event in aggreg['allevents']:
    destinations.add(event['_source']['details']['destination'])",destinations = {event['_source']['details']['destination'] for event in aggreg['allevents']}
redis-py,https://github.com/redis/redis-py/tree/master/redis/utils.py,,merge_result$68,"def merge_result(command, res):
    """"""
    Merge all items in `res` into a list.

    This command is used when sending a command to multiple nodes
    and the result from each node should be merged into a single list.

    res : 'dict'
    """"""
    result = set()

    for v in res.values():
        for value in v:
            result.add(value)

    return list(result)","result = set()
for v in res.values():
    for value in v:
        result.add(value)",result = {value for v in res.values() for value in v}
LinOTP,https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/support.py,,get_public_keys$917,"def get_public_keys():
    """"""
    get a list of all public keys, which could be used to verify
    a linOTP license

    :return: list with unique public keys
    """"""

    pubKeys = {}  # we use a dict to preserve the type of the license
    pubKeys[""linotp""] = PUB_KEY_LINOTP

    key_files = set()
    for key_dir in PUB_KEY_DIRS:
        if os.path.isdir(key_dir):
            for key_file in os.listdir(key_dir):
                for extension in PUB_KEY_EXTS:
                    if key_file.endswith(extension):
                        key_files.add(os.path.join(key_dir, key_file))

    for key_file in key_files:
        try:
            key_text = readPublicKey(key_file)
            if key_text and key_text not in list(pubKeys.values()):
                idx = os.path.split(key_file)[-1]
                if idx[-4:] == "".pem"":
                    idx, _sep, _rest = idx.rpartition("".pem"")
                if idx[-4:] == ""_pub"":
                    idx, _sep, _rest = idx.rpartition(""_pub"")
                pubKeys[idx] = key_text
            else:
                log.error(
                    ""Licence: Public key file is not valid (%r)"", key_file
                )
        except Exception as exx:
            log.error(
                ""Licence: error during reading public key file (%s): %r"",
                key_file,
                exx,
            )

    return pubKeys","key_files = set()
for key_dir in PUB_KEY_DIRS:
    if os.path.isdir(key_dir):
        for key_file in os.listdir(key_dir):
            for extension in PUB_KEY_EXTS:
                if key_file.endswith(extension):
                    key_files.add(os.path.join(key_dir, key_file))","key_files = {os.path.join(key_dir, key_file) for key_dir in PUB_KEY_DIRS if os.path.isdir(key_dir) for key_file in os.listdir(key_dir) for extension in PUB_KEY_EXTS if key_file.endswith(extension)}"
angr,https://github.com/angr/angr/tree/master/angr/utils/graph.py,,compute_dominance_frontier$128,"def compute_dominance_frontier(graph, domtree):
    """"""
    Compute a dominance frontier based on the given post-dominator tree.

    This implementation is based on figure 2 of paper An Efficient Method of Computing Static Single Assignment
    Form by Ron Cytron, etc.

    :param graph:   The graph where we want to compute the dominance frontier.
    :param domtree: The dominator tree
    :returns:       A dict of dominance frontier
    """"""

    df = {}

    # Perform a post-order search on the dominator tree
    for x in networkx.dfs_postorder_nodes(domtree):

        if x not in graph:
            # Skip nodes that are not in the graph
            continue

        df[x] = set()

        # local set
        for y in graph.successors(x):
            if x not in domtree.predecessors(y):
                df[x].add(y)

        # up set
        if x is None:
            continue

        for z in domtree.successors(x):
            if z is x:
                continue
            if z not in df:
                continue
            for y in df[z]:
                if x not in list(domtree.predecessors(y)):
                    df[x].add(y)

    return df","df[x] = set()
for y in graph.successors(x):
    if x not in domtree.predecessors(y):
        df[x].add(y)",df[x] = {y for y in graph.successors(x) if x not in domtree.predecessors(y)}
codechecker,https://github.com/Ericsson/codechecker/tree/master/web/tests/functional/diff_local_remote/test_diff_local_remote.py,LocalRemote,test_local_compare_res_html_output_unresolved$216,"def test_local_compare_res_html_output_unresolved(self):
        """"""Check that html files will be generated by using diff command.""""""
        html_reports = os.path.join(self._local_reports, ""html_reports"")

        get_diff_results([self._run_names[0]], [self._local_reports],
                         '--unresolved', 'html',
                         [""--url"", self._url, '-e', html_reports,
                          ""--verbose"", ""debug""])

        checked_files = set()
        for res in self.get_local_remote_diff(None, 'json'):
            checked_files.add(os.path.basename(res['file']['path']))

        # Check if index.html file was generated.
        html_index = os.path.join(html_reports, ""index.html"")
        self.assertTrue(os.path.exists(html_index))

        html_statistics = os.path.join(html_reports, ""statistics.html"")
        self.assertTrue(os.path.exists(html_statistics))

        # Check that html files were generated for each reports.
        for html_file_names in os.listdir(html_reports):
            suffix = html_file_names.rfind(""_"")
            file_name = html_file_names[:suffix] \
                if suffix != -1 else html_file_names

            if file_name in [""index.html"", ""statistics.html""]:
                continue

            self.assertIn(file_name, checked_files)","checked_files = set()
for res in self.get_local_remote_diff(None, 'json'):
    checked_files.add(os.path.basename(res['file']['path']))","checked_files = {os.path.basename(res['file']['path']) for res in self.get_local_remote_diff(None, 'json')}"
toga,https://github.com/beeware/toga/tree/master/examples/textinput/textinput/app.py,TextInputApp,get_password_content_label$129,"def get_password_content_label(self, content):
        if content.strip() == """":
            return EMPTY_PASSWORD
        contains = set()
        for letter in content:
            if letter in ascii_uppercase:
                contains.add(""uppercase letters"")
            elif letter in ascii_lowercase:
                contains.add(""lowercase letters"")
            elif letter in digits:
                contains.add(""digits"")
            else:
                contains.add(""special characters"")
        return ""Password contains: {}"".format("", "".join(contains))","contains = set()
for letter in content:
    if letter in ascii_uppercase:
        contains.add('uppercase letters')
    elif letter in ascii_lowercase:
        contains.add('lowercase letters')
    elif letter in digits:
        contains.add('digits')
    else:
        contains.add('special characters')",contains = {'uppercase letters' if letter in ascii_uppercase else 'lowercase letters' if letter in ascii_lowercase else 'digits' if letter in digits else 'special characters' for letter in content}
ansible,https://github.com/ansible/ansible/tree/master/test/sanity/code-smell/update-bundled.py,,get_bundled_libs$41,"def get_bundled_libs(paths):
    """"""
    Return the set of known bundled libraries

    :arg paths: The paths which the test has been instructed to check
    :returns: The list of all files which we know to contain bundled libraries.  If a bundled
        library consists of multiple files, this should be the file which has metadata included.
    """"""
    bundled_libs = set()
    for filename in fnmatch.filter(paths, 'lib/ansible/compat/*/__init__.py'):
        bundled_libs.add(filename)

    bundled_libs.add('lib/ansible/module_utils/compat/selectors.py')
    bundled_libs.add('lib/ansible/module_utils/distro/__init__.py')
    bundled_libs.add('lib/ansible/module_utils/six/__init__.py')
    # backports.ssl_match_hostname should be moved to its own file in the future
    bundled_libs.add('lib/ansible/module_utils/urls.py')

    return bundled_libs","bundled_libs = set()
for filename in fnmatch.filter(paths, 'lib/ansible/compat/*/__init__.py'):
    bundled_libs.add(filename)","bundled_libs = {filename for filename in fnmatch.filter(paths, 'lib/ansible/compat/*/__init__.py')}"
forseti-security,https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/scanner/scanners/iap_scanner.py,_RunData,make_iap_resource$320,"def make_iap_resource(self, backend_service, project_full_name):
        """"""Get an IapResource for a service.

        Args:
            backend_service (BackendService): service to create a resource for
            project_full_name (str): The full path to the parent project
                including all ancestors.

        Returns:
            IapResource: the corresponding resource
        """"""
        alternate_services = set()
        direct_access_sources = set()
        for backend in backend_service.backends:
            instance_group = self.find_instance_group_by_url(
                backend.get('group'))
            if not instance_group:
                continue

            network_port = self.instance_group_network_port(
                backend_service, instance_group)

            if not network_port:
                continue

            direct_access_sources.update(
                self.firewall_allowed_sources(network_port, None))
            tags = self.tags_for_instance_group(instance_group)
            for tag in tags:
                direct_access_sources.update(
                    self.firewall_allowed_sources(
                        network_port, tag))

        # Don't count the load balancer as a direct access source.
        # The load balancer egress IPs are documented here:
        #     https://cloud.google.com/compute/docs/load-balancing/http/
        # (In theory they can change, but it's not common (since it's
        # a backwards-incompatible change for HTTP load balancer
        # customers.) 35.191/16 was recently announced; when Google
        # added that one, they sent out a mandatory service
        # announcement a year before the new range was used.)
        direct_access_sources.discard('130.211.0.0/22')
        direct_access_sources.discard('35.191.0.0/16')

        for backend_service2 in self.backend_services:
            if self.is_alternate_service(backend_service, backend_service2):
                alternate_services.add(backend_service2.key)

        return IapResource(
            project_full_name=project_full_name,
            backend_service=backend_service,
            alternate_services=alternate_services,
            direct_access_sources=sorted(direct_access_sources),
            iap_enabled=(backend_service.iap.get('enabled', False)
                         if backend_service.iap else False))","alternate_services = set()
for backend_service2 in self.backend_services:
    if self.is_alternate_service(backend_service, backend_service2):
        alternate_services.add(backend_service2.key)","alternate_services = {backend_service2.key for backend_service2 in self.backend_services if self.is_alternate_service(backend_service, backend_service2)}"
core,https://github.com/home-assistant/core/tree/master/homeassistant/auth/mfa_modules/notify.py,NotifyAuthModule,aync_get_available_notify_services$150,"def aync_get_available_notify_services(self) -> list[str]:
        """"""Return list of notify services.""""""
        unordered_services = set()

        for service in self.hass.services.async_services().get(""notify"", {}):
            if service not in self._exclude:
                unordered_services.add(service)

        if self._include:
            unordered_services &= set(self._include)

        return sorted(unordered_services)","unordered_services = set()
for service in self.hass.services.async_services().get('notify', {}):
    if service not in self._exclude:
        unordered_services.add(service)","unordered_services = {service for service in self.hass.services.async_services().get('notify', {}) if service not in self._exclude}"
pychess,https://github.com/pychess/pychess/tree/master/testing/sittuyin.py,SittuyinTestCase,test_geCaptures$157,"def test_geCaptures(self):
        """"""Testing validate move in Sittuyin variant""""""

        board = SittuyinBoard(setup=FEN4)
        print(board)

        moves = set()
        for move in genCaptures(board.board):
            moves.add(toAN(board.board, move))
        self.assertEqual(moves, set((""d5c6"",)))","moves = set()
for move in genCaptures(board.board):
    moves.add(toAN(board.board, move))","moves = {toAN(board.board, move) for move in genCaptures(board.board)}"
lxmert,https://github.com/airsplay/lxmert/tree/master/data/mscoco_imgfeat/extract_coco_image.py,,generate_tsv$54,"def generate_tsv(prototxt, weights, image_ids, outfile):
    # First check if file exists, and if it is complete
    # never use set, it loses the order!!! F***
    wanted_ids = set([image_id[1] for image_id in image_ids])
    found_ids = set()
    if os.path.exists(outfile):
        with open(outfile, ""r"") as tsvfile:
            reader = csv.DictReader(tsvfile, delimiter='\t', fieldnames=FIELDNAMES)
            for item in reader:
                found_ids.add(item['img_id'])
    missing = wanted_ids - found_ids
    if len(missing) == 0:
        print('already completed {:d}'.format(len(image_ids)))
    else:
        print('missing {:d}/{:d}'.format(len(missing), len(image_ids)))
    if len(missing) > 0:
        caffe.set_mode_gpu()
        caffe.set_device(0)
        net = caffe.Net(prototxt, caffe.TEST, weights=weights)
        with open(outfile, 'ab') as tsvfile:
            writer = csv.DictWriter(tsvfile, delimiter='\t', fieldnames=FIELDNAMES)
            for im_file, image_id in tqdm(image_ids):
                if image_id in missing:
                    try:
                        writer.writerow(get_detections_from_im(net, im_file, image_id))
                    except Exception as e:
                        print(e)","found_ids = set()
for item in reader:
    found_ids.add(item['img_id'])",found_ids = {item['img_id'] for item in reader}
numpy,https://github.com/numpy/numpy/tree/master/numpy/distutils/ccompiler_opt.py,_Feature,feature_names$1261,"def feature_names(self, names=None, force_flags=None, macros=[]):
        """"""
        Returns a set of CPU feature names that supported by platform and the **C** compiler.

        Parameters
        ----------
        names : sequence or None, optional
            Specify certain CPU features to test it against the **C** compiler.
            if None(default), it will test all current supported features.
            **Note**: feature names must be in upper-case.

        force_flags : list or None, optional
            If None(default), default compiler flags for every CPU feature will
            be used during the test.

        macros : list of tuples, optional
            A list of C macro definitions.
        """"""
        assert(
            names is None or (
                not isinstance(names, str) and
                hasattr(names, ""__iter__"")
            )
        )
        assert(force_flags is None or isinstance(force_flags, list))
        if names is None:
            names = self.feature_supported.keys()
        supported_names = set()
        for f in names:
            if self.feature_is_supported(
                f, force_flags=force_flags, macros=macros
            ):
                supported_names.add(f)
        return supported_names","supported_names = set()
for f in names:
    if self.feature_is_supported(f, force_flags=force_flags, macros=macros):
        supported_names.add(f)","supported_names = {f for f in names if self.feature_is_supported(f, force_flags=force_flags, macros=macros)}"
django-autofixture,https://github.com/gregmuellegger/django-autofixture/tree/master/autofixture_tests/tests/test_base.py,TestRelations,test_generate_fk_for_o2o$192,"def test_generate_fk_for_o2o(self):
        # OneToOneField is the same as a ForeignKey with unique=True
        filler = AutoFixture(O2OModel, generate_fk=True)

        all_o2o = set()
        for obj in filler.create(10):
            all_o2o.add(obj.o2o)

        self.assertEqual(set(SimpleModel.objects.all()), all_o2o)","all_o2o = set()
for obj in filler.create(10):
    all_o2o.add(obj.o2o)",all_o2o = {obj.o2o for obj in filler.create(10)}
salt,https://github.com/saltstack/salt/tree/master/salt/modules/win_service.py,,get_all$225,"def get_all():
    """"""
    Return all installed services

    Returns:
        list: Returns a list of all services on the system.

    CLI Example:

    .. code-block:: bash

        salt '*' service.get_all
    """"""
    services = _get_services()

    ret = set()
    for service in services:
        ret.add(service[""ServiceName""])

    return sorted(ret)","ret = set()
for service in services:
    ret.add(service['ServiceName'])",ret = {service['ServiceName'] for service in services}
TexTools-Blender,https://github.com/SavMartin/TexTools-Blender/tree/master//utilities_uv.py,,get_selected_uvs$290,"def get_selected_uvs(bm, uv_layers):
	""""""Returns selected mesh vertices of selected UV's""""""
	uvs = set()
	for face in bm.faces:
		if face.select:
			for loop in face.loops:
				if loop[uv_layers].select:
					uvs.add( loop[uv_layers] )
	return uvs","uvs = set()
for face in bm.faces:
    if face.select:
        for loop in face.loops:
            if loop[uv_layers].select:
                uvs.add(loop[uv_layers])",uvs = {loop[uv_layers] for face in bm.faces if face.select for loop in face.loops if loop[uv_layers].select}
angr,https://github.com/angr/angr/tree/master/angr/analyses/cfg/cfg_fast.py,CFGFast,_get_jumpout_targets$3137,"def _get_jumpout_targets(self, func):
        jumpout_targets = set()
        callgraph_outedges = self.functions.callgraph.out_edges(func.addr, data=True)
        # find the ones whose type is transition
        for _, dst, data in callgraph_outedges:
            if data.get('type', None) == 'transition':
                jumpout_targets.add(dst)
        return jumpout_targets","jumpout_targets = set()
for (_, dst, data) in callgraph_outedges:
    if data.get('type', None) == 'transition':
        jumpout_targets.add(dst)","jumpout_targets = {dst for (_, dst, data) in callgraph_outedges if data.get('type', None) == 'transition'}"
attn2d,https://github.com/elbayadm/attn2d/tree/master/fairseq/file_utils.py,,read_set_from_file$316,"def read_set_from_file(filename):
    '''
    Extract a de-duped collection (set) of text from a file.
    Expected file format is one item per line.
    '''
    collection = set()
    with open(filename, 'r', encoding='utf-8') as file_:
        for line in file_:
            collection.add(line.rstrip())
    return collection","collection = set()
for line in file_:
    collection.add(line.rstrip())",collection = {line.rstrip() for line in file_}
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/base/models/vouchers.py,,generate_codes$77,"def generate_codes(organizer, num=1, prefix=None):
    codes = set()
    batch_size = 500
    if 'postgres' in settings.DATABASES['default']['ENGINE']:
        batch_size = 5_000

    """"""
    We're trying to check if any of the requested codes already exists in the database. Generally, this is a

        SELECT code FROM voucher WHERE code IN (…)

    query. However, it turns out that this query get's rather slow if an organizer has lots of vouchers, even
    with a organizer with just over 50_000 vouchers, we've seen that creating 20_000 new voucher codes took
    just over 30 seconds. There's another way of doing this query on PostgreSQL, which is joining with a
    temporary table

        SELECT code FROM voucher INNER JOIN (VALUES …) vals(v) ON (code = v)

    This is significantly faster, inserting 20_000 vouchers now takes 2-3s instead of 31s on the same dataset.
    It's still slow, and removing the JOIN to the event table doesn't significantly speed it up. We might need
    an entirely different approach at some point.
    """"""

    while len(codes) < num:
        new_codes = set()
        for i in range(min(num - len(codes), batch_size)):  # Work around SQLite's SQLITE_MAX_VARIABLE_NUMBER
            new_codes.add(_generate_random_code(prefix=prefix))

        if 'postgres' in settings.DATABASES['default']['ENGINE']:
            with connection.cursor() as cursor:
                args = list(new_codes) + [organizer.pk]
                tmptable = ""VALUES "" + ("", "".join(['(%s)'] * len(new_codes)))
                cursor.execute(
                    f'SELECT code '
                    f'FROM ""{Voucher._meta.db_table}"" '
                    f'INNER JOIN ({tmptable}) vals(v) ON (""{Voucher._meta.db_table}"".""code"" = ""v"")'
                    f'INNER JOIN ""{Event._meta.db_table}"" ON (""{Voucher._meta.db_table}"".""event_id"" = ""{Event._meta.db_table}"".""id"") '
                    f'WHERE ""{Event._meta.db_table}"".""organizer_id"" = %s',
                    args
                )
                for row in cursor.fetchall():
                    new_codes.remove(row[0])
        else:
            new_codes -= set([v['code'] for v in Voucher.objects.filter(code__in=new_codes).values('code')])

        codes |= new_codes
    return list(codes)","new_codes = set()
for i in range(min(num - len(codes), batch_size)):
    new_codes.add(_generate_random_code(prefix=prefix))","new_codes = {_generate_random_code(prefix=prefix) for i in range(min(num - len(codes), batch_size))}"
pytorch-metric-learning,https://github.com/KevinMusgrave/pytorch-metric-learning/tree/master/tests/utils/test_loss_and_miner_utils.py,TestLossAndMinerUtils,test_convert_to_triplets$90,"def test_convert_to_triplets(self):
        a1 = torch.LongTensor([0, 1, 2, 3])
        p = torch.LongTensor([4, 4, 4, 4])
        a2 = torch.LongTensor([4, 5, 6, 7])
        n = torch.LongTensor([5, 5, 6, 6])
        triplets = lmu.convert_to_triplets((a1, p, a2, n), labels=torch.arange(7))
        self.assertTrue(all(len(x) == 0 for x in triplets))

        a2 = torch.LongTensor([0, 4, 5, 6])
        triplets = lmu.convert_to_triplets((a1, p, a2, n), labels=torch.arange(7))
        self.assertTrue(
            triplets == (torch.tensor([0]), torch.tensor([4]), torch.tensor([5]))
        )

        a1 = torch.LongTensor([0, 1, 0, 2])
        p = torch.LongTensor([5, 6, 7, 8])
        a2 = torch.LongTensor([0, 1, 2, 0])
        n = torch.LongTensor([9, 10, 11, 12])
        triplets = lmu.convert_to_triplets((a1, p, a2, n), labels=torch.arange(13))
        triplets = torch.stack(triplets, dim=1)
        found_set = set()
        for t in triplets:
            found_set.add(tuple(t.cpu().numpy()))
        correct_triplets = {
            (0, 5, 9),
            (0, 5, 12),
            (0, 7, 9),
            (0, 7, 12),
            (1, 6, 10),
            (2, 8, 11),
        }

        self.assertTrue(found_set == correct_triplets)","found_set = set()
for t in triplets:
    found_set.add(tuple(t.cpu().numpy()))",found_set = {tuple(t.cpu().numpy()) for t in triplets}
sqlalchemy-utils,https://github.com/kvesteri/sqlalchemy-utils/tree/master/sqlalchemy_utils/functions/foreign_keys.py,,get_referencing_foreign_keys$53,"def get_referencing_foreign_keys(mixed):
    """"""
    Returns referencing foreign keys for given Table object or declarative
    class.

    :param mixed:
        SA Table object or SA declarative class

    ::

        get_referencing_foreign_keys(User)  # set([ForeignKey('user.id')])

        get_referencing_foreign_keys(User.__table__)


    This function also understands inheritance. This means it returns
    all foreign keys that reference any table in the class inheritance tree.

    Let's say you have three classes which use joined table inheritance,
    namely TextItem, Article and BlogPost with Article and BlogPost inheriting
    TextItem.

    ::

        # This will check all foreign keys that reference either article table
        # or textitem table.
        get_referencing_foreign_keys(Article)

    .. seealso:: :func:`get_tables`
    """"""
    if isinstance(mixed, sa.Table):
        tables = [mixed]
    else:
        tables = get_tables(mixed)

    referencing_foreign_keys = set()

    for table in mixed.metadata.tables.values():
        if table not in tables:
            for constraint in table.constraints:
                if isinstance(constraint, sa.sql.schema.ForeignKeyConstraint):
                    for fk in constraint.elements:
                        if any(fk.references(t) for t in tables):
                            referencing_foreign_keys.add(fk)
    return referencing_foreign_keys","referencing_foreign_keys = set()
for table in mixed.metadata.tables.values():
    if table not in tables:
        for constraint in table.constraints:
            if isinstance(constraint, sa.sql.schema.ForeignKeyConstraint):
                for fk in constraint.elements:
                    if any((fk.references(t) for t in tables)):
                        referencing_foreign_keys.add(fk)","referencing_foreign_keys = {fk for table in mixed.metadata.tables.values() if table not in tables for constraint in table.constraints if isinstance(constraint, sa.sql.schema.ForeignKeyConstraint) for fk in constraint.elements if any((fk.references(t) for t in tables))}"
aswan,https://github.com/momosecurity/aswan/tree/master/www/rule/forms.py,RulesForm,_check_names$78,"def _check_names(self, names, choices, sep=None):
        valid_names = set()
        for english, chinese in choices:
            if english:
                valid_names.add(english)
        if sep:
            all_names = []
            for name in names:
                for e in name.split(sep):
                    all_names.append(e)
            names = all_names
        return all([name in valid_names for name in names])","valid_names = set()
for (english, chinese) in choices:
    if english:
        valid_names.add(english)","valid_names = {english for (english, chinese) in choices if english}"
astropy,https://github.com/astropy/astropy/tree/master/astropy/extern/ply/lex.py,,lex$862,"def lex(module=None, object=None, debug=False, optimize=False, lextab='lextab',
        reflags=int(re.VERBOSE), nowarn=False, outputdir=None, debuglog=None, errorlog=None):

    if lextab is None:
        lextab = 'lextab'

    global lexer

    ldict = None
    stateinfo  = {'INITIAL': 'inclusive'}
    lexobj = Lexer()
    lexobj.lexoptimize = optimize
    global token, input

    if errorlog is None:
        errorlog = PlyLogger(sys.stderr)

    if debug:
        if debuglog is None:
            debuglog = PlyLogger(sys.stderr)

    # Get the module dictionary used for the lexer
    if object:
        module = object

    # Get the module dictionary used for the parser
    if module:
        _items = [(k, getattr(module, k)) for k in dir(module)]
        ldict = dict(_items)
        # If no __file__ attribute is available, try to obtain it from the __module__ instead
        if '__file__' not in ldict:
            ldict['__file__'] = sys.modules[ldict['__module__']].__file__
    else:
        ldict = get_caller_module_dict(2)

    # Determine if the module is package of a package or not.
    # If so, fix the tabmodule setting so that tables load correctly
    pkg = ldict.get('__package__')
    if pkg and isinstance(lextab, str):
        if '.' not in lextab:
            lextab = pkg + '.' + lextab

    # Collect parser information from the dictionary
    linfo = LexerReflect(ldict, log=errorlog, reflags=reflags)
    linfo.get_all()
    if not optimize:
        if linfo.validate_all():
            raise SyntaxError(""Can't build lexer"")

    if optimize and lextab:
        try:
            lexobj.readtab(lextab, ldict)
            token = lexobj.token
            input = lexobj.input
            lexer = lexobj
            return lexobj

        except ImportError:
            pass

    # Dump some basic debugging information
    if debug:
        debuglog.info('lex: tokens   = %r', linfo.tokens)
        debuglog.info('lex: literals = %r', linfo.literals)
        debuglog.info('lex: states   = %r', linfo.stateinfo)

    # Build a dictionary of valid token names
    lexobj.lextokens = set()
    for n in linfo.tokens:
        lexobj.lextokens.add(n)

    # Get literals specification
    if isinstance(linfo.literals, (list, tuple)):
        lexobj.lexliterals = type(linfo.literals[0])().join(linfo.literals)
    else:
        lexobj.lexliterals = linfo.literals

    lexobj.lextokens_all = lexobj.lextokens | set(lexobj.lexliterals)

    # Get the stateinfo dictionary
    stateinfo = linfo.stateinfo

    regexs = {}
    # Build the master regular expressions
    for state in stateinfo:
        regex_list = []

        # Add rules defined by functions first
        for fname, f in linfo.funcsym[state]:
            regex_list.append('(?P<%s>%s)' % (fname, _get_regex(f)))
            if debug:
                debuglog.info(""lex: Adding rule %s -> '%s' (state '%s')"", fname, _get_regex(f), state)

        # Now add all of the simple rules
        for name, r in linfo.strsym[state]:
            regex_list.append('(?P<%s>%s)' % (name, r))
            if debug:
                debuglog.info(""lex: Adding rule %s -> '%s' (state '%s')"", name, r, state)

        regexs[state] = regex_list

    # Build the master regular expressions

    if debug:
        debuglog.info('lex: ==== MASTER REGEXS FOLLOW ====')

    for state in regexs:
        lexre, re_text, re_names = _form_master_re(regexs[state], reflags, ldict, linfo.toknames)
        lexobj.lexstatere[state] = lexre
        lexobj.lexstateretext[state] = re_text
        lexobj.lexstaterenames[state] = re_names
        if debug:
            for i, text in enumerate(re_text):
                debuglog.info(""lex: state '%s' : regex[%d] = '%s'"", state, i, text)

    # For inclusive states, we need to add the regular expressions from the INITIAL state
    for state, stype in stateinfo.items():
        if state != 'INITIAL' and stype == 'inclusive':
            lexobj.lexstatere[state].extend(lexobj.lexstatere['INITIAL'])
            lexobj.lexstateretext[state].extend(lexobj.lexstateretext['INITIAL'])
            lexobj.lexstaterenames[state].extend(lexobj.lexstaterenames['INITIAL'])

    lexobj.lexstateinfo = stateinfo
    lexobj.lexre = lexobj.lexstatere['INITIAL']
    lexobj.lexretext = lexobj.lexstateretext['INITIAL']
    lexobj.lexreflags = reflags

    # Set up ignore variables
    lexobj.lexstateignore = linfo.ignore
    lexobj.lexignore = lexobj.lexstateignore.get('INITIAL', '')

    # Set up error functions
    lexobj.lexstateerrorf = linfo.errorf
    lexobj.lexerrorf = linfo.errorf.get('INITIAL', None)
    if not lexobj.lexerrorf:
        errorlog.warning('No t_error rule is defined')

    # Set up eof functions
    lexobj.lexstateeoff = linfo.eoff
    lexobj.lexeoff = linfo.eoff.get('INITIAL', None)

    # Check state information for ignore and error rules
    for s, stype in stateinfo.items():
        if stype == 'exclusive':
            if s not in linfo.errorf:
                errorlog.warning(""No error rule is defined for exclusive state '%s'"", s)
            if s not in linfo.ignore and lexobj.lexignore:
                errorlog.warning(""No ignore rule is defined for exclusive state '%s'"", s)
        elif stype == 'inclusive':
            if s not in linfo.errorf:
                linfo.errorf[s] = linfo.errorf.get('INITIAL', None)
            if s not in linfo.ignore:
                linfo.ignore[s] = linfo.ignore.get('INITIAL', '')

    # Create global versions of the token() and input() functions
    token = lexobj.token
    input = lexobj.input
    lexer = lexobj

    # If in optimize mode, we write the lextab
    if lextab and optimize:
        if outputdir is None:
            # If no output directory is set, the location of the output files
            # is determined according to the following rules:
            #     - If lextab specifies a package, files go into that package directory
            #     - Otherwise, files go in the same directory as the specifying module
            if isinstance(lextab, types.ModuleType):
                srcfile = lextab.__file__
            else:
                if '.' not in lextab:
                    srcfile = ldict['__file__']
                else:
                    parts = lextab.split('.')
                    pkgname = '.'.join(parts[:-1])
                    exec('import %s' % pkgname)
                    srcfile = getattr(sys.modules[pkgname], '__file__', '')
            outputdir = os.path.dirname(srcfile)
        try:
            lexobj.writetab(lextab, outputdir)
            if lextab in sys.modules:
                del sys.modules[lextab]
        except IOError as e:
            errorlog.warning(""Couldn't write lextab module %r. %s"" % (lextab, e))

    return lexobj","lexobj.lextokens = set()
for n in linfo.tokens:
    lexobj.lextokens.add(n)",lexobj.lextokens = {n for n in linfo.tokens}
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/vpc.py,SGUsage,get_eni_sgs$790,"def get_eni_sgs(self):
        sg_ids = set()
        for nic in self.manager.get_resource_manager('eni').resources():
            for g in nic['Groups']:
                sg_ids.add(g['GroupId'])
        return sg_ids","sg_ids = set()
for nic in self.manager.get_resource_manager('eni').resources():
    for g in nic['Groups']:
        sg_ids.add(g['GroupId'])",sg_ids = {g['GroupId'] for nic in self.manager.get_resource_manager('eni').resources() for g in nic['Groups']}
graphql-compiler,https://github.com/kensho-technologies/graphql-compiler/tree/master/graphql_compiler/schema_generation/graphql_schema.py,,_get_referenced_type_equivalences$38,"def _get_referenced_type_equivalences(graphql_types, type_equivalence_hints):
    """"""Filter union types with no edges from the type equivalence hints dict.""""""
    referenced_types = set()
    for graphql_type in graphql_types.values():
        if isinstance(graphql_type, (GraphQLObjectType, GraphQLInterfaceType)):
            for _, field in graphql_type.fields.items():
                if isinstance(field.type, GraphQLList):
                    referenced_types.add(field.type.of_type.name)
    return {
        original: union
        for original, union in type_equivalence_hints.items()
        if union.name in referenced_types
    }","referenced_types = set()
for graphql_type in graphql_types.values():
    if isinstance(graphql_type, (GraphQLObjectType, GraphQLInterfaceType)):
        for (_, field) in graphql_type.fields.items():
            if isinstance(field.type, GraphQLList):
                referenced_types.add(field.type.of_type.name)","referenced_types = {field.type.of_type.name for graphql_type in graphql_types.values() if isinstance(graphql_type, (GraphQLObjectType, GraphQLInterfaceType)) for (_, field) in graphql_type.fields.items() if isinstance(field.type, GraphQLList)}"
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/pulse/test_transforms.py,,get_pulse_ids$376,"def get_pulse_ids(schedules: List[Schedule]) -> Set[int]:
    """"""Returns ids of pulses used in Schedules.""""""
    ids = set()
    for schedule in schedules:
        for _, inst in schedule.instructions:
            ids.add(inst.pulse.id)
    return ids","ids = set()
for schedule in schedules:
    for (_, inst) in schedule.instructions:
        ids.add(inst.pulse.id)","ids = {inst.pulse.id for schedule in schedules for (_, inst) in schedule.instructions}"
Kiwi,https://github.com/kiwitcms/Kiwi/tree/master/tcms/signals.py,,notify_admins$47,"def notify_admins(sender, **kwargs):
    """"""
    Very simple signal handler which sends emails to site
    admins when a new user has been registered!

    .. warning::

        This handler isn't connected to the ``USER_REGISTERED_SIGNAL`` by default!
    """"""
    from django.conf import settings
    from django.contrib.auth import get_user_model
    from django.urls import reverse

    from tcms.core.utils import request_host_link
    from tcms.core.utils.mailto import mailto

    if kwargs.get(""raw"", False):
        return

    admin_emails = set()
    # super-users can approve others
    for super_user in get_user_model().objects.filter(is_superuser=True):
        admin_emails.add(super_user.email)
    # site admins should be able to do so as well
    for _name, email in settings.ADMINS:
        admin_emails.add(email)

    request = kwargs.get(""request"")
    user = kwargs.get(""user"")
    user_url = request_host_link(request) + reverse(
        ""admin:auth_user_change"", args=[user.pk]
    )

    mailto(
        template_name=""email/user_registered/notify_admins.txt"",
        recipients=list(admin_emails),
        subject=str(_(""New user awaiting approval"")),
        context={
            ""username"": user.username,
            ""user_url"": user_url,
        },
    )","admin_emails = set()
for super_user in get_user_model().objects.filter(is_superuser=True):
    admin_emails.add(super_user.email)",admin_emails = {super_user.email for super_user in get_user_model().objects.filter(is_superuser=True)}
buildbot,https://github.com/buildbot/buildbot/tree/master/master/buildbot/reporters/utils.py,,getResponsibleUsersForSourceStamp$174,"def getResponsibleUsersForSourceStamp(master, sourcestampid):
    changesd = master.data.get((""sourcestamps"", sourcestampid, ""changes""))
    sourcestampd = master.data.get((""sourcestamps"", sourcestampid))
    changes, sourcestamp = yield defer.gatherResults([changesd, sourcestampd])
    blamelist = set()
    # normally, we get only one, but just assume there might be several
    for c in changes:
        blamelist.add(c['author'])
    # Add patch author to blamelist
    if 'patch' in sourcestamp and sourcestamp['patch'] is not None:
        blamelist.add(sourcestamp['patch']['author'])
    blamelist = list(blamelist)
    blamelist.sort()
    return blamelist","blamelist = set()
for c in changes:
    blamelist.add(c['author'])",blamelist = {c['author'] for c in changes}
mopidy,https://github.com/mopidy/mopidy/tree/master/mopidy/http/handlers.py,ClientListHandler,get$275,"def get(self):
        set_mopidy_headers(self)

        names = set()
        for app in self.apps:
            names.add(app[""name""])
        for static in self.statics:
            names.add(static[""name""])
        names.discard(""mopidy"")

        self.render(""data/clients.html"", apps=sorted(list(names)))","names = set()
for app in self.apps:
    names.add(app['name'])",names = {app['name'] for app in self.apps}
luigi,https://github.com/spotify/luigi/tree/master/test/task_forwarded_attributes_test.py,NonYieldingTask,gather_forwarded_attributes$33,"def gather_forwarded_attributes(self):
        """"""
        Returns a set of names of attributes that are forwarded by the TaskProcess and that are not
        *None*. The tests in this file check if and which attributes are present at different times,
        e.g. while running, or before and after a dynamic dependency was yielded.
        """"""
        attrs = set()
        for attr in FORWARDED_ATTRIBUTES:
            if getattr(self, attr, None) is not None:
                attrs.add(attr)
        return attrs","attrs = set()
for attr in FORWARDED_ATTRIBUTES:
    if getattr(self, attr, None) is not None:
        attrs.add(attr)","attrs = {attr for attr in FORWARDED_ATTRIBUTES if getattr(self, attr, None) is not None}"
DALEX,https://github.com/ModelOriented/DALEX/tree/master/python/dalex/dalex/fairness/_group_fairness/plot.py,,plot_density$691,"def plot_density(fobject,
                 other_objects,
                 title):
    data = pd.DataFrame(columns=['y', 'y_hat', 'subgroup', 'model'])
    objects = [fobject]
    if other_objects is not None:
        for other_obj in other_objects:
            objects.append(other_obj)
    for obj in objects:
        for subgroup in np.unique(obj.protected):
            y, y_hat = obj.y[obj.protected == subgroup], obj.y_hat[obj.protected == subgroup]
            data_to_append = pd.DataFrame({'y': y,
                                           'y_hat': y_hat,
                                           'subgroup': np.repeat(subgroup, len(y)),
                                           'model': np.repeat(obj.label, len(y))})
            data = pd.concat([data, data_to_append])

    fig = go.Figure()

    counter = 0
    for model in data.model.unique():
        for i, sub in enumerate(data.subgroup.unique()):
            counter += 1
            fig.add_trace(
                go.Violin(
                    box_visible=True,
                    x=data.loc[(data.subgroup == sub) & (data.model == model)].y_hat,
                    y0=sub + model,
                    name=sub,
                    fillcolor=_theme.get_default_colors(len(data.subgroup.unique()), type='line')[i],
                    opacity=0.9,
                    line_color='black',
                    hoveron='points'

                )
            )

    violins_in_model = int(counter / len(data.model.unique()))
    starter_violins = np.arange(0, counter, violins_in_model)

    fig.update_xaxes(title='prediction')
    fig.update_yaxes(title='model', tickvals=list((starter_violins + (violins_in_model - 1) / 2)),
                     ticktext=list(data.model.unique()))

    # hide doubling entries in legend
    legend_entries = set()
    for trace in fig['data']:
        legend_entries.add(trace['name'])

    for trace in fig['data']:
        if trace['name'] in legend_entries:
            legend_entries.remove(trace['name'])
        else:
            trace['showlegend'] = False

    if title is None:
        title = ""Density plot""
    fig.update_layout(utils._fairness_theme(title))
    fig.update_traces(hovertemplate=""outlier prediction: %{x:.3f}"")

    return fig","legend_entries = set()
for trace in fig['data']:
    legend_entries.add(trace['name'])",legend_entries = {trace['name'] for trace in fig['data']}
ARL,https://github.com/TophantTechnology/ARL/tree/master/app/services/searchEngines.py,BingSearch,match_urls$99,"def match_urls(self, html):
        dom = pq(html)
        result_items = dom(self.pq_query).items()
        urls_result = [item.attr(""href"") for item in result_items]
        urls = set()
        for u in urls_result:
            urls.add(u)
        return list(urls)","urls = set()
for u in urls_result:
    urls.add(u)",urls = {u for u in urls_result}
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/world/resourcehandler.py,ResourceHandler,_load_provided_resources$195,"def _load_provided_resources(self):
		""""""Returns a iterable obj containing all resources this building provides.
		This is outsourced from initialization to a method for the possibility of
		overwriting it.
		Do not alter the returned list; if you need to do so, then copy it.""""""
		produced_resources = set()
		for prod in self.get_component(Producer).get_productions():
			for res in prod.get_produced_resources():
				produced_resources.add(res)

		for res in self.additional_provided_resources:
			produced_resources.add(res)

		return produced_resources","produced_resources = set()
for prod in self.get_component(Producer).get_productions():
    for res in prod.get_produced_resources():
        produced_resources.add(res)",produced_resources = {res for prod in self.get_component(Producer).get_productions() for res in prod.get_produced_resources()}
aws-parallelcluster,https://github.com/aws/aws-parallelcluster/tree/master/tests/integration-tests/framework/tests_configuration/config_utils.py,,get_enabled_tests$21,"def get_enabled_tests(config):
    """"""
    Build a set containing all tests defined in the config file.

    Each entry of the set is in the format {test-suite}/{test-module}::{test-function},
    e.g. 'cfn-init/test_cfn_init.py::test_replace_compute_on_failure'
    """"""
    enabled_test_suites = config.get(""test-suites"")
    enabled_tests = set()
    for suite, tests in enabled_test_suites.items():
        for test in tests.keys():
            enabled_tests.add(""{0}/{1}"".format(suite, test))

    return enabled_tests","enabled_tests = set()
for (suite, tests) in enabled_test_suites.items():
    for test in tests.keys():
        enabled_tests.add('{0}/{1}'.format(suite, test))","enabled_tests = {'{0}/{1}'.format(suite, test) for (suite, tests) in enabled_test_suites.items() for test in tests.keys()}"
mars,https://github.com/mars-project/mars/tree/master/ci/importcheck.py,,_check_absolute_import$42,"def _check_absolute_import(node: ast.AST) -> List[Tuple[int, int]]:
    res = set()
    if isinstance(node, ast.Import):
        for import_name in node.names:
            if import_name.name.startswith('mars.'):
                res.add((node.lineno, node.end_lineno))
    elif isinstance(node, ast.ImportFrom):
        if node.level == 0 and node.module.startswith('mars.'):
            res.add((node.lineno, node.end_lineno))
    elif getattr(node, 'body', []):
        for body_item in node.body:
            res.update(_check_absolute_import(body_item))
    return sorted(res)","res = set()
for import_name in node.names:
    if import_name.name.startswith('mars.'):
        res.add((node.lineno, node.end_lineno))","res = {(node.lineno, node.end_lineno) for import_name in node.names if import_name.name.startswith('mars.')}"
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/ebs.py,SnapshotUnusedFilter,_pull_ami_snapshots$301,"def _pull_ami_snapshots(self):
        amis = self.manager.get_resource_manager('ami').resources()
        ami_snaps = set()
        for i in amis:
            for dev in i.get('BlockDeviceMappings'):
                if 'Ebs' in dev and 'SnapshotId' in dev['Ebs']:
                    ami_snaps.add(dev['Ebs']['SnapshotId'])
        return ami_snaps","ami_snaps = set()
for i in amis:
    for dev in i.get('BlockDeviceMappings'):
        if 'Ebs' in dev and 'SnapshotId' in dev['Ebs']:
            ami_snaps.add(dev['Ebs']['SnapshotId'])",ami_snaps = {dev['Ebs']['SnapshotId'] for i in amis for dev in i.get('BlockDeviceMappings') if 'Ebs' in dev and 'SnapshotId' in dev['Ebs']}
cmakeconverter,https://github.com/pavelliavonau/cmakeconverter/tree/master/cmake_converter/data_converter.py,DataConverter,__verify_configurations_to_parse$96,"def __verify_configurations_to_parse(context):
        absent_settings = set()
        for setting in context.configurations_to_parse:
            if setting not in context.settings:
                absent_settings.add(setting)

        if len(absent_settings) > 0:
            context.configurations_to_parse -= absent_settings
            message(
                context,
                'There are absent settings at {}: {}\n'
                'skipping conversion. Add lost settings or fix mapping of settings at solution'
                .format(context.vcxproj_path, absent_settings),
                'error'
            )
            return False
        return True","absent_settings = set()
for setting in context.configurations_to_parse:
    if setting not in context.settings:
        absent_settings.add(setting)",absent_settings = {setting for setting in context.configurations_to_parse if setting not in context.settings}
pychess,https://github.com/pychess/pychess/tree/master/testing/seirawan.py,SchessTestCase,test_disambig_gating_san$205,"def test_disambig_gating_san(self):
        FEN = ""rnbqkb1r/ppp1pppp/3p1n2/8/2PP4/2N5/PP2PPPP/RHBQKBNR[Eeh] b KQACDEFGHkqabcdefh - 1 3""
        board = LBoard(SCHESS)
        board.applyFen(FEN)

        moves = set()
        for move in genAllMoves(board):
            moves.add(toAN(board, move))
        print(""--------"")
        print(board)

        self.assertIn(""f6d7"", moves)
        self.assertNotIn(""f6d7/H"", moves)
        self.assertNotIn(""f6d7/E"", moves)
        self.assertIn(""b8d7"", moves)
        self.assertIn(""b8d7h"", moves)
        self.assertIn(""b8d7e"", moves)

        self.assertEqual(repr(Move.Move(parseSAN(board, 'Nfd7'))), 'f6d7')
        self.assertEqual(repr(Move.Move(parseSAN(board, 'Nbd7'))), 'b8d7')
        self.assertEqual(repr(Move.Move(parseSAN(board, 'Nbd7/H'))), 'b8d7h')
        self.assertEqual(repr(Move.Move(parseSAN(board, 'Nbd7/E'))), 'b8d7e')","moves = set()
for move in genAllMoves(board):
    moves.add(toAN(board, move))","moves = {toAN(board, move) for move in genAllMoves(board)}"
TextAttack,https://github.com/QData/TextAttack/tree/master/textattack/shared/attacked_text.py,AttackedText,all_words_diff$241,"def all_words_diff(self, other_attacked_text):
        """"""Returns the set of indices for which this and other_attacked_text
        have different words.""""""
        indices = set()
        w1 = self.words
        w2 = other_attacked_text.words
        for i in range(min(len(w1), len(w2))):
            if w1[i] != w2[i]:
                indices.add(i)
        return indices","indices = set()
for i in range(min(len(w1), len(w2))):
    if w1[i] != w2[i]:
        indices.add(i)","indices = {i for i in range(min(len(w1), len(w2))) if w1[i] != w2[i]}"
fairseq,https://github.com/pytorch/fairseq/tree/master/scripts/compare_namespaces.py,,keys$12,"def keys(ns):
        ks = set()
        for k in dir(ns):
            if not k.startswith(""_""):
                ks.add(k)
        return ks","ks = set()
for k in dir(ns):
    if not k.startswith('_'):
        ks.add(k)",ks = {k for k in dir(ns) if not k.startswith('_')}
shuup,https://github.com/shuup/shuup/tree/master/shuup/admin/utils/permissions.py,,get_default_model_permissions$18,"def get_default_model_permissions(model):
    """"""
    Return a set of all default permissions for a given model.

    :param model: Model class
    :type model: django.db.Model
    :return: Set of default model permissions as strings
    :rtype: set[str]
    """"""
    warnings.warn(
        ""Warning! `get_default_model_permissions` is deprecated in Shuup 2.0. ""
        ""Use human readable permission strings instead."",
        DeprecationWarning,
    )
    permissions = set()

    for default in model._meta.default_permissions:
        permissions.add(""%s.%s_%s"" % (model._meta.app_label, default, model._meta.model_name))

    return permissions","permissions = set()
for default in model._meta.default_permissions:
    permissions.add('%s.%s_%s' % (model._meta.app_label, default, model._meta.model_name))","permissions = {'%s.%s_%s' % (model._meta.app_label, default, model._meta.model_name) for default in model._meta.default_permissions}"
salt,https://github.com/saltstack/salt/tree/master/salt/modules/runit.py,,get_svc_broken_path$274,"def get_svc_broken_path(name=""*""):
    """"""
    Return list of broken path(s) in SERVICE_DIR that match ``name``

    A path is broken if it is a broken symlink or can not be a runit service

    name
        a glob for service name. default is '*'

    CLI Example:

    .. code-block:: bash

        salt '*' runit.get_svc_broken_path <service name>
    """"""
    if not SERVICE_DIR:
        raise CommandExecutionError(""Could not find service directory."")

    ret = set()

    for el in glob.glob(os.path.join(SERVICE_DIR, name)):
        if not _is_svc(el):
            ret.add(el)
    return sorted(ret)","ret = set()
for el in glob.glob(os.path.join(SERVICE_DIR, name)):
    if not _is_svc(el):
        ret.add(el)","ret = {el for el in glob.glob(os.path.join(SERVICE_DIR, name)) if not _is_svc(el)}"
oio-sds,https://github.com/open-io/oio-sds/tree/master/oio/cli/admin/service_check.py,DirectoryCheck,_take_action$206,"def _take_action(self, parsed_args):
        import subprocess
        from oio.directory.meta0 import Meta0Client
        from oio.common.json import json

        self.logger.debug(""Checking the directory bootstrap."")

        # Get an official dump from the proxy, check its size
        m0 = Meta0Client({""namespace"": self.app.options.ns})
        prefixes = m0.list()
        if len(prefixes) != CID_PREFIX_COUNT:
            raise ValueError('Found %d entries in meta0, expected %d' % (
                             len(prefixes), CID_PREFIX_COUNT))
        self.logger.info(""The proxy serves a full meta0 dump."")

        # contact each M0 to perform a check: any ""get"" command will
        # fail if the meta0 is not complete. Unfortunately we just have
        # oio-meta0-client to target a specific service.
        for _, host, port, _ in self.filter_services(self.catalog, 'meta0'):
            url = '%s:%d' % (host, port)
            res = subprocess.check_output(
                ['oio-meta0-client', url, 'get', '0000'])
            self.logger.info(res)
        self.logger.info(""All meta0 services are complete."")

        # contact each meta0 to check that all the dumps are identical
        dump0 = None
        first = None
        for _, host, port, _ in self.filter_services(self.catalog, 'meta0'):
            url = '%s:%d' % (host, port)
            dump = subprocess.check_output(['oio-meta0-client', url, 'list'])
            if dump0 is None:
                dump0 = dump
                first = url
            elif dump0 != dump:
                raise ValueError('The dump returned by meta0 %s differs from '
                                 'the dump returned by %s' % (url, first))
        self.logger.info(""All meta0 services serve the same base."")

        # Check all the meta1 are concerned
        reverse_dump = set()
        for _, v in iteritems(json.loads(dump0)):
            for url in v:
                reverse_dump.add(url)
        m1 = {':'.join((descr[1], str(descr[2])))
              for descr in self.filter_services(self.catalog, 'meta1')}
        if m1 != reverse_dump:
            raise ValueError(
                'Meta1 used but not visible: %s, '
                'meta1 visible but not used: %s' % (
                 reverse_dump - m1, m1 - reverse_dump))
        self.logger.info(""All meta1 services have been assigned."")
        yield ('OK', None)","reverse_dump = set()
for (_, v) in iteritems(json.loads(dump0)):
    for url in v:
        reverse_dump.add(url)","reverse_dump = {url for (_, v) in iteritems(json.loads(dump0)) for url in v}"
meta-dataset,https://github.com/google-research/meta-dataset/tree/master/meta_dataset/data/imagenet_specification_test.py,GraphCopyTest,validate_copy$254,"def validate_copy(self, graph, graph_copy):
    """"""Make sure graph_copy is a correct copy of graph.""""""
    # Make sure that for each node in graph, there is exactly one node in
    # graph_copy with the same WordNet id
    for n in graph:
      wn_id = n.wn_id
      found_wn_in_copy = False
      for n_copy in graph_copy:
        if n_copy.wn_id == wn_id:
          found_wn_in_copy = True
          break
      self.assertTrue(found_wn_in_copy)

    # Make sure that for every link in graph there is a corresponding link in
    # graph copy (correspondence is assessed via the WordNet id's of the nodes
    # that are being connected).
    graph_parent_child_links = set()
    for s in graph:
      for c in s.children:
        graph_parent_child_links.add((s.wn_id, c.wn_id))
    for s in graph_copy:
      for p, c in graph_parent_child_links:
        # Find the nodes in graph_copy whose wn_id's are p and c
        for n in graph_copy:
          if n.wn_id == c:
            c_node = n
          if n.wn_id == p:
            p_node = n
        self.assertIn(c_node, p_node.children)
        self.assertIn(p_node, c_node.parents)","graph_parent_child_links = set()
for s in graph:
    for c in s.children:
        graph_parent_child_links.add((s.wn_id, c.wn_id))","graph_parent_child_links = {(s.wn_id, c.wn_id) for s in graph for c in s.children}"
Sublist3r,https://github.com/aboul3la/Sublist3r/tree/master//sublist3r.py,,main$884,"def main(domain, threads, savefile, ports, silent, verbose, enable_bruteforce, engines):
    bruteforce_list = set()
    search_list = set()

    if is_windows:
        subdomains_queue = list()
    else:
        subdomains_queue = multiprocessing.Manager().list()

    # Check Bruteforce Status
    if enable_bruteforce or enable_bruteforce is None:
        enable_bruteforce = True

    # Validate domain
    domain_check = re.compile(""^(http|https)?[a-zA-Z0-9]+([\-\.]{1}[a-zA-Z0-9]+)*\.[a-zA-Z]{2,}$"")
    if not domain_check.match(domain):
        if not silent:
            print(R + ""Error: Please enter a valid domain"" + W)
        return []

    if not domain.startswith('http://') or not domain.startswith('https://'):
        domain = 'http://' + domain

    parsed_domain = urlparse.urlparse(domain)

    if not silent:
        print(B + ""[-] Enumerating subdomains now for %s"" % parsed_domain.netloc + W)

    if verbose and not silent:
        print(Y + ""[-] verbosity is enabled, will show the subdomains results in realtime"" + W)

    supported_engines = {'baidu': BaiduEnum,
                         'yahoo': YahooEnum,
                         'google': GoogleEnum,
                         'bing': BingEnum,
                         'ask': AskEnum,
                         'netcraft': NetcraftEnum,
                         'dnsdumpster': DNSdumpster,
                         'virustotal': Virustotal,
                         'threatcrowd': ThreatCrowd,
                         'ssl': CrtSearch,
                         'passivedns': PassiveDNS
                         }

    chosenEnums = []

    if engines is None:
        chosenEnums = [
            BaiduEnum, YahooEnum, GoogleEnum, BingEnum, AskEnum,
            NetcraftEnum, DNSdumpster, Virustotal, ThreatCrowd,
            CrtSearch, PassiveDNS
        ]
    else:
        engines = engines.split(',')
        for engine in engines:
            if engine.lower() in supported_engines:
                chosenEnums.append(supported_engines[engine.lower()])

    # Start the engines enumeration
    enums = [enum(domain, [], q=subdomains_queue, silent=silent, verbose=verbose) for enum in chosenEnums]
    for enum in enums:
        enum.start()
    for enum in enums:
        enum.join()

    subdomains = set(subdomains_queue)
    for subdomain in subdomains:
        search_list.add(subdomain)

    if enable_bruteforce:
        if not silent:
            print(G + ""[-] Starting bruteforce module now using subbrute.."" + W)
        record_type = False
        path_to_file = os.path.dirname(os.path.realpath(__file__))
        subs = os.path.join(path_to_file, 'subbrute', 'names.txt')
        resolvers = os.path.join(path_to_file, 'subbrute', 'resolvers.txt')
        process_count = threads
        output = False
        json_output = False
        bruteforce_list = subbrute.print_target(parsed_domain.netloc, record_type, subs, resolvers, process_count, output, json_output, search_list, verbose)

    subdomains = search_list.union(bruteforce_list)

    if subdomains:
        subdomains = sorted(subdomains, key=subdomain_sorting_key)

        if savefile:
            write_file(savefile, subdomains)

        if not silent:
            print(Y + ""[-] Total Unique Subdomains Found: %s"" % len(subdomains) + W)

        if ports:
            if not silent:
                print(G + ""[-] Start port scan now for the following ports: %s%s"" % (Y, ports) + W)
            ports = ports.split(',')
            pscan = portscan(subdomains, ports)
            pscan.run()

        elif not silent:
            for subdomain in subdomains:
                print(G + subdomain + W)
    return subdomains","search_list = set()
for subdomain in subdomains:
    search_list.add(subdomain)",search_list = {subdomain for subdomain in subdomains}
hivemind,https://github.com/learning-at-home/hivemind/tree/master/hivemind/averaging/allreduce.py,AllReduceRunner,finalize$281,"def finalize(self, *, cancel: bool = False, exception: Optional[BaseException] = None):
        """"""finish or terminate AllReduceRunner, propagate any errors / cancellations to peers.""""""
        assert not cancel or not exception, ""finalize accepts either exception or cancel, but not both""
        pending_tasks = set()
        if cancel or exception:
            # propagate error to peers
            if cancel or isinstance(exception, asyncio.CancelledError):
                code = averaging_pb2.CANCELLED
            else:
                code = averaging_pb2.INTERNAL_ERROR
            logger.debug(f""{self} - notifying peers about {averaging_pb2.MessageCode.Name(code)}"")
            for peer_id, mode in zip(self.ordered_peer_ids, self.modes):
                if peer_id != self.peer_id and mode != AveragingMode.CLIENT:
                    pending_tasks.add(asyncio.create_task(self._send_error_to_peer(peer_id, code)))

        if not self._future.done():
            if cancel:
                logger.debug(f""{self} - cancelled"")
                self._future.cancel()
            elif exception:
                logger.debug(f""{self} - caught {exception}"")
                self._future.set_exception(exception)
            else:
                logger.debug(f""{self} - finished"")
                self._future.set_result(None)
            self.tensor_part_container.finalize()
            self.tensor_part_reducer.finalize()
            return pending_tasks
        else:
            logger.debug(f""{self} - could not finish: allreduce is already finished: {self._future}"")
            return pending_tasks","pending_tasks = set()
for (peer_id, mode) in zip(self.ordered_peer_ids, self.modes):
    if peer_id != self.peer_id and mode != AveragingMode.CLIENT:
        pending_tasks.add(asyncio.create_task(self._send_error_to_peer(peer_id, code)))","pending_tasks = {asyncio.create_task(self._send_error_to_peer(peer_id, code)) for (peer_id, mode) in zip(self.ordered_peer_ids, self.modes) if peer_id != self.peer_id and mode != AveragingMode.CLIENT}"
docassemble,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/backend.py,,delete_temp_user_data$714,"def delete_temp_user_data(temp_user_id, r):
    db.session.execute(delete(UserDictKeys).where(UserDictKeys.temp_user_id == temp_user_id))
    db.session.commit()
    db.session.execute(delete(UploadsUserAuth).where(UploadsUserAuth.temp_user_id == temp_user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.temp_owner_id == temp_user_id))
    db.session.commit()
    db.session.execute(delete(ChatLog).where(ChatLog.temp_user_id == temp_user_id))
    db.session.commit()
    db.session.execute(delete(GlobalObjectStorage).where(GlobalObjectStorage.temp_user_id == temp_user_id))
    db.session.commit()
    files_to_delete = list()
    for short_code_item in db.session.execute(select(Shortener).filter_by(temp_user_id=temp_user_id)).scalars():
        for email in db.session.execute(select(Email).filter_by(short=short_code_item.short)).scalars():
            for attachment in db.session.execute(select(EmailAttachment).filter_by(email_id=email.id)).scalars():
                files_to_delete.append(attachment.upload)
    for file_number in files_to_delete:
        the_file = SavedFile(file_number)
        the_file.delete()
    db.session.execute(delete(Shortener).where(Shortener.temp_user_id == temp_user_id))
    db.session.commit()
    keys_to_delete = set()
    for key in r.keys('*userid:t' + str(temp_user_id)):
        keys_to_delete.add(key)
    for key in r.keys('*userid:t' + str(temp_user_id) + ':*'):
        keys_to_delete.add(key)
    for key in keys_to_delete:
        r.delete(key)","keys_to_delete = set()
for key in r.keys('*userid:t' + str(temp_user_id)):
    keys_to_delete.add(key)",keys_to_delete = {key for key in r.keys('*userid:t' + str(temp_user_id))}
node-gyp,https://github.com/nodejs/node-gyp/tree/master/gyp/pylib/gyp/generator/cmake.py,,GenerateOutputForConfig$1143,"def GenerateOutputForConfig(target_list, target_dicts, data, params, config_to_use):
    options = params[""options""]
    generator_flags = params[""generator_flags""]
    flavor = gyp.common.GetFlavor(params)

    # generator_dir: relative path from pwd to where make puts build files.
    # Makes migrating from make to cmake easier, cmake doesn't put anything here.
    # Each Gyp configuration creates a different CMakeLists.txt file
    # to avoid incompatibilities between Gyp and CMake configurations.
    generator_dir = os.path.relpath(options.generator_output or ""."")

    # output_dir: relative path from generator_dir to the build directory.
    output_dir = generator_flags.get(""output_dir"", ""out"")

    # build_dir: relative path from source root to our output files.
    # e.g. ""out/Debug""
    build_dir = os.path.normpath(os.path.join(generator_dir, output_dir, config_to_use))

    toplevel_build = os.path.join(options.toplevel_dir, build_dir)

    output_file = os.path.join(toplevel_build, ""CMakeLists.txt"")
    gyp.common.EnsureDirExists(output_file)

    output = open(output_file, ""w"")
    output.write(""cmake_minimum_required(VERSION 2.8.8 FATAL_ERROR)\n"")
    output.write(""cmake_policy(VERSION 2.8.8)\n"")

    gyp_file, project_target, _ = gyp.common.ParseQualifiedTarget(target_list[-1])
    output.write(""project("")
    output.write(project_target)
    output.write("")\n"")

    SetVariable(output, ""configuration"", config_to_use)

    ar = None
    cc = None
    cxx = None

    make_global_settings = data[gyp_file].get(""make_global_settings"", [])
    build_to_top = gyp.common.InvertRelativePath(build_dir, options.toplevel_dir)
    for key, value in make_global_settings:
        if key == ""AR"":
            ar = os.path.join(build_to_top, value)
        if key == ""CC"":
            cc = os.path.join(build_to_top, value)
        if key == ""CXX"":
            cxx = os.path.join(build_to_top, value)

    ar = gyp.common.GetEnvironFallback([""AR_target"", ""AR""], ar)
    cc = gyp.common.GetEnvironFallback([""CC_target"", ""CC""], cc)
    cxx = gyp.common.GetEnvironFallback([""CXX_target"", ""CXX""], cxx)

    if ar:
        SetVariable(output, ""CMAKE_AR"", ar)
    if cc:
        SetVariable(output, ""CMAKE_C_COMPILER"", cc)
    if cxx:
        SetVariable(output, ""CMAKE_CXX_COMPILER"", cxx)

    # The following appears to be as-yet undocumented.
    # http://public.kitware.com/Bug/view.php?id=8392
    output.write(""enable_language(ASM)\n"")
    # ASM-ATT does not support .S files.
    # output.write('enable_language(ASM-ATT)\n')

    if cc:
        SetVariable(output, ""CMAKE_ASM_COMPILER"", cc)

    SetVariable(output, ""builddir"", ""${CMAKE_CURRENT_BINARY_DIR}"")
    SetVariable(output, ""obj"", ""${builddir}/obj"")
    output.write(""\n"")

    # TODO: Undocumented/unsupported (the CMake Java generator depends on it).
    # CMake by default names the object resulting from foo.c to be foo.c.o.
    # Gyp traditionally names the object resulting from foo.c foo.o.
    # This should be irrelevant, but some targets extract .o files from .a
    # and depend on the name of the extracted .o files.
    output.write(""set(CMAKE_C_OUTPUT_EXTENSION_REPLACE 1)\n"")
    output.write(""set(CMAKE_CXX_OUTPUT_EXTENSION_REPLACE 1)\n"")
    output.write(""\n"")

    # Force ninja to use rsp files. Otherwise link and ar lines can get too long,
    # resulting in 'Argument list too long' errors.
    # However, rsp files don't work correctly on Mac.
    if flavor != ""mac"":
        output.write(""set(CMAKE_NINJA_FORCE_RESPONSE_FILE 1)\n"")
    output.write(""\n"")

    namer = CMakeNamer(target_list)

    # The list of targets upon which the 'all' target should depend.
    # CMake has it's own implicit 'all' target, one is not created explicitly.
    all_qualified_targets = set()
    for build_file in params[""build_files""]:
        for qualified_target in gyp.common.AllTargets(
            target_list, target_dicts, os.path.normpath(build_file)
        ):
            all_qualified_targets.add(qualified_target)

    for qualified_target in target_list:
        if flavor == ""mac"":
            gyp_file, _, _ = gyp.common.ParseQualifiedTarget(qualified_target)
            spec = target_dicts[qualified_target]
            gyp.xcode_emulation.MergeGlobalXcodeSettingsToSpec(data[gyp_file], spec)

        WriteTarget(
            namer,
            qualified_target,
            target_dicts,
            build_dir,
            config_to_use,
            options,
            generator_flags,
            all_qualified_targets,
            flavor,
            output,
        )

    output.close()","all_qualified_targets = set()
for build_file in params['build_files']:
    for qualified_target in gyp.common.AllTargets(target_list, target_dicts, os.path.normpath(build_file)):
        all_qualified_targets.add(qualified_target)","all_qualified_targets = {qualified_target for build_file in params['build_files'] for qualified_target in gyp.common.AllTargets(target_list, target_dicts, os.path.normpath(build_file))}"
plop,https://github.com/bdarnell/plop/tree/master/plop/viewer.py,,profile_to_json$46,"def profile_to_json(filename):
    root = os.path.abspath(options.datadir) + os.path.sep
    abspath = os.path.abspath(os.path.join(root, filename))
    assert (abspath + os.path.sep).startswith(root)
    graph = CallGraph.load(abspath)

    total = sum(stack.weights['calls'] for stack in graph.stacks)
    top_stacks = graph.stacks
    #top_stacks = [stack for stack in graph.stacks if stack.weights['calls'] > total*.005]
    filtered_nodes = set()
    for stack in top_stacks:
        filtered_nodes.update(stack.nodes)
    nodes=[dict(attrs=node.attrs, weights=node.weights, id=node.id)
           for node in filtered_nodes]
    nodes = sorted(nodes, key=lambda n: -n['weights']['calls'])
    #index = {node['id']: i for i, node in enumerate(nodes)}
    index = dict([(node['id'], i) for i, node in enumerate(nodes)])


    # High-degree nodes are generally common utility functions, and
    # creating edges from all over the graph tends to obscure more than
    # it helps.
    degrees = Counter()
    dropped = set()
    for edge in six.itervalues(graph.edges):
        degrees[edge.child.id] += 1
        degrees[edge.parent.id] += 1
    for node, degree in six.iteritems(degrees):
        if degree > 6:
            dropped.add(node)

    edges = [dict(source=index[edge.parent.id],
                  target=index[edge.child.id],
                  weights=edge.weights)
             for edge in six.itervalues(graph.edges)
             if (edge.parent.id in index and
                 edge.child.id in index and
                 edge.parent.id not in dropped and
                 edge.child.id not in dropped)]
    stacks = [dict(nodes=[index[n.id] for n in stack.nodes],
                   weights=stack.weights)
              for stack in top_stacks]
    return dict(nodes=nodes, edges=edges, stacks=stacks)","dropped = set()
for (node, degree) in six.iteritems(degrees):
    if degree > 6:
        dropped.add(node)","dropped = {node for (node, degree) in six.iteritems(degrees) if degree > 6}"
aswan,https://github.com/momosecurity/aswan/tree/master/www/rule/views.py,RulesDataView,_get_freq_strategy_args$326,"def _get_freq_strategy_args(self, uuids, client):
        ret = set()
        strategy_bodys = self._get_one_kind_fields_from_uuids(
            uuids, 'freq_strategy', 'strategy_body', client
        )
        for names in strategy_bodys:
            for name in names.split(','):
                ret.add(name)
        return ret","ret = set()
for names in strategy_bodys:
    for name in names.split(','):
        ret.add(name)","ret = {name for names in strategy_bodys for name in names.split(',')}"
ScoutSuite,https://github.com/nccgroup/ScoutSuite/tree/master/ScoutSuite/providers/aws/provider.py,AWSProvider,_check_ec2_zone_distribution$157,"def _check_ec2_zone_distribution(self):
        regions = self.services['ec2']['regions'].values()
        self.services['ec2']['number_of_regions_with_instances'] = sum(r['instances_count'] > 0 for r in regions)

        for regions in self.services['ec2']['regions'].values():
            instances_availability_zones = set()
            for vpcs in regions['vpcs'].values():
                for instance in vpcs['instances'].values():
                    instances_availability_zones.add(instance.get('availability_zone'))
            regions['instances_availability_zones'] = len(instances_availability_zones)","instances_availability_zones = set()
for vpcs in regions['vpcs'].values():
    for instance in vpcs['instances'].values():
        instances_availability_zones.add(instance.get('availability_zone'))",instances_availability_zones = {instance.get('availability_zone') for vpcs in regions['vpcs'].values() for instance in vpcs['instances'].values()}
viztracer,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_multiprocess.py,TestMultiprocessing,check_func$320,"def check_func(data):
            pids = set()
            for entry in data[""traceEvents""]:
                pids.add(entry[""pid""])
            self.assertEqual(len(pids), 3)","pids = set()
for entry in data['traceEvents']:
    pids.add(entry['pid'])",pids = {entry['pid'] for entry in data['traceEvents']}
neat-python,https://github.com/CodeReclaimers/neat-python/tree/master/neat/graphs.py,,feed_forward_layers$59,"def feed_forward_layers(inputs, outputs, connections):
    """"""
    Collect the layers whose members can be evaluated in parallel in a feed-forward network.
    :param inputs: list of the network input nodes
    :param outputs: list of the output node identifiers
    :param connections: list of (input, output) connections in the network.

    Returns a list of layers, with each layer consisting of a set of node identifiers.
    Note that the returned layers do not contain nodes whose output is ultimately
    never used to compute the final network output.
    """"""

    required = required_for_output(inputs, outputs, connections)

    layers = []
    s = set(inputs)
    while 1:
        # Find candidate nodes c for the next layer.  These nodes should connect
        # a node in s to a node not in s.
        c = set(b for (a, b) in connections if a in s and b not in s)
        # Keep only the used nodes whose entire input set is contained in s.
        t = set()
        for n in c:
            if n in required and all(a in s for (a, b) in connections if b == n):
                t.add(n)

        if not t:
            break

        layers.append(t)
        s = s.union(t)

    return layers","t = set()
for n in c:
    if n in required and all((a in s for (a, b) in connections if b == n)):
        t.add(n)","t = {n for n in c if n in required and all((a in s for (a, b) in connections if b == n))}"
lite-transformer,https://github.com/mit-han-lab/lite-transformer/tree/master/fairseq/file_utils.py,,read_set_from_file$303,"def read_set_from_file(filename):
    '''
    Extract a de-duped collection (set) of text from a file.
    Expected file format is one item per line.
    '''
    collection = set()
    with open(filename, 'r', encoding='utf-8') as file_:
        for line in file_:
            collection.add(line.rstrip())
    return collection","collection = set()
for line in file_:
    collection.add(line.rstrip())",collection = {line.rstrip() for line in file_}
pkuseg-python,https://github.com/lancopku/pkuseg-python/tree/master/pkuseg/scorer.py,,getFscore$4,"def getFscore(goldTagList, resTagList, idx_to_chunk_tag):
    scoreList = []
    assert len(resTagList) == len(goldTagList)
    getNewTagList(idx_to_chunk_tag, goldTagList)
    getNewTagList(idx_to_chunk_tag, resTagList)
    goldChunkList = getChunks(goldTagList)
    resChunkList = getChunks(resTagList)
    gold_chunk = 0
    res_chunk = 0
    correct_chunk = 0
    for i in range(len(goldChunkList)):
        res = resChunkList[i]
        gold = goldChunkList[i]
        resChunkAry = res.split(Config.comma)
        tmp = []
        for t in resChunkAry:
            if len(t) > 0:
                tmp.append(t)
        resChunkAry = tmp
        goldChunkAry = gold.split(Config.comma)
        tmp = []
        for t in goldChunkAry:
            if len(t) > 0:
                tmp.append(t)
        goldChunkAry = tmp
        gold_chunk += len(goldChunkAry)
        res_chunk += len(resChunkAry)
        goldChunkSet = set()
        for im in goldChunkAry:
            goldChunkSet.add(im)
        for im in resChunkAry:
            if im in goldChunkSet:
                correct_chunk += 1
    pre = correct_chunk / res_chunk * 100
    rec = correct_chunk / gold_chunk * 100
    f1 = 0 if correct_chunk == 0 else 2 * pre * rec / (pre + rec)
    scoreList.append(f1)
    scoreList.append(pre)
    scoreList.append(rec)
    infoList = []
    infoList.append(gold_chunk)
    infoList.append(res_chunk)
    infoList.append(correct_chunk)
    return scoreList, infoList","goldChunkSet = set()
for im in goldChunkAry:
    goldChunkSet.add(im)",goldChunkSet = {im for im in goldChunkAry}
redis-memory-analyzer,https://github.com/gamenet/redis-memory-analyzer/tree/master/rma/splitter.py,SimpleSplitter,unfold_to_list$83,"def unfold_to_list(tree, separator):
        res = set()

        for sub_tree in tree.values():
            for compound_key in dict_build(sub_tree):
                res.add(separator.join(compound_key))
        return res","res = set()
for sub_tree in tree.values():
    for compound_key in dict_build(sub_tree):
        res.add(separator.join(compound_key))",res = {separator.join(compound_key) for sub_tree in tree.values() for compound_key in dict_build(sub_tree)}
nvim-completion-manager,https://github.com/roxma/nvim-completion-manager/tree/master/pythonx/cm_sources/cm_tmux.py,Source,refresh_keyword$40,"def refresh_keyword(self):
        pat = re.compile(self._split_pattern)
        self._words = set()

        # tmux list-window -F '#{window_index}'
        # tmux capture-pane -p -t ""$window_index.$pane_index""
        proc = subprocess.Popen(args=['tmux', 'list-window', '-F', '#{window_index}'],
                                stdin=subprocess.PIPE,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE)

        outs, errs = proc.communicate(timeout=15)
        window_indices = outs.decode('utf-8')
        logger.info('list-window: %s', window_indices)

        # parse windows
        panes = []
        for win_index in window_indices.strip().split('\n'):
            proc = subprocess.Popen(args=['tmux', 'list-panes', '-t', win_index, '-F', '#{pane_index}'],
                                    stdin=subprocess.PIPE,
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
            outs, errs = proc.communicate(timeout=15)
            pane_ids = outs.decode('utf-8')

            for pane_id in pane_ids.strip().split('\n'):
                proc = subprocess.Popen(args=['tmux', 'capture-pane', '-p', '-t', '{}.{}'.format(win_index,pane_id)],
                                        stdin=subprocess.PIPE,
                                        stdout=subprocess.PIPE,
                                        stderr=subprocess.PIPE)
                outs, errs = proc.communicate(timeout=15)
                try:
                    outs = outs.decode('utf-8')
                    panes.append(outs)
                except Exception as ex:
                    logger.exception('exception, failed to decode output, %s', ex)
                    pass

        for pane in panes:
            for word in re.split(pat,pane):
                self._words.add(word)

        logger.info('keyword refresh complete, count: %s', len(self._words))","self._words = set()
for pane in panes:
    for word in re.split(pat, pane):
        self._words.add(word)","self._words = {word for pane in panes for word in re.split(pat, pane)}"
salt,https://github.com/saltstack/salt/tree/master/salt/modules/sysmod.py,,list_modules$369,"def list_modules(*args):
    """"""
    List the modules loaded on the minion

    .. versionadded:: 2015.5.0

    CLI Example:

    .. code-block:: bash

        salt '*' sys.list_modules

    Module names can be specified as globs.

    .. code-block:: bash

        salt '*' sys.list_modules 's*'

    """"""
    modules = set()
    if not args:
        for func in __salt__:
            modules.add(func.split(""."")[0])
        return sorted(modules)

    for module in args:
        if ""*"" in module:
            for func in fnmatch.filter(__salt__, module):
                modules.add(func.split(""."")[0])
        else:
            for func in __salt__:
                mod_test = func.split(""."")[0]
                if mod_test == module:
                    modules.add(mod_test)
    return sorted(modules)","modules = set()
for func in __salt__:
    modules.add(func.split('.')[0])",modules = {func.split('.')[0] for func in __salt__}
in-toto,https://github.com/in-toto/in-toto/tree/master/in_toto/verifylib.py,,verify_modify_rule$800,"def verify_modify_rule(rule_pattern, artifacts_queue, materials, products):
  """"""
  <Purpose>
    Filters artifacts from artifacts queue using rule pattern and consumes them
    if they are in both the materials dict and in the products doct, but have
    different hashes, i.e. were modified.

  <Arguments>
    rule_pattern:
            A ""MODIFY"" rule pattern (see in_toto.rulelib).

    artifacts_queue:
            Not yet consumed artifacts (paths only).

    materials:
            All materials of an item (including hashes).

    products:
            All products of an item (including hashes).

  <Exceptions>
    None.

  <Side Effects>
    None.

  <Returns>
    The set of consumed artifacts (paths only).

  """"""
  # Filter queued artifacts using the rule pattern
  filtered_artifacts = fnmatch.filter(artifacts_queue, rule_pattern)

  # Filter filtered artifacts that are materials and products
  filtered_artifacts = set(filtered_artifacts) & \
      set(materials.keys()) & set(products.keys())

  # Consume filtered artifacts that have different hashes
  consumed = set()
  for path in filtered_artifacts:
    if materials[path] != products[path]:
      consumed.add(path)

  return consumed","consumed = set()
for path in filtered_artifacts:
    if materials[path] != products[path]:
        consumed.add(path)",consumed = {path for path in filtered_artifacts if materials[path] != products[path]}
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/webapps/galaxy/controllers/history.py,HistoryController,permissions$644,"def permissions(self, trans, payload=None, **kwd):
        """"""
        Sets the permissions on a history.
        """"""
        history_id = kwd.get('id')
        if not history_id:
            return self.message_exception(trans, f'Invalid history id ({str(history_id)}) received')
        history = self.history_manager.get_owned(self.decode_id(history_id), trans.user, current_history=trans.history)
        if trans.request.method == 'GET':
            inputs = []
            all_roles = trans.user.all_roles()
            current_actions = history.default_permissions
            for action_key, action in trans.app.model.Dataset.permitted_actions.items():
                in_roles = set()
                for a in current_actions:
                    if a.action == action.action:
                        in_roles.add(a.role)
                inputs.append({'type': 'select',
                               'multiple': True,
                               'optional': True,
                               'individual': True,
                               'name': action_key,
                               'label': action.action,
                               'help': action.description,
                               'options': [(role.name, trans.security.encode_id(role.id)) for role in set(all_roles)],
                               'value': [trans.security.encode_id(role.id) for role in in_roles]})
            return {'title': 'Change default dataset permissions for history \'%s\'' % history.name, 'inputs': inputs}
        else:
            permissions = {}
            for action_key, action in trans.app.model.Dataset.permitted_actions.items():
                in_roles = payload.get(action_key) or []
                in_roles = [trans.sa_session.query(trans.app.model.Role).get(trans.security.decode_id(x)) for x in in_roles]
                permissions[trans.app.security_agent.get_action(action.action)] = in_roles
            trans.app.security_agent.history_set_default_permissions(history, permissions)
            return {'message': 'Default history \'%s\' dataset permissions have been changed.' % history.name}","in_roles = set()
for a in current_actions:
    if a.action == action.action:
        in_roles.add(a.role)",in_roles = {a.role for a in current_actions if a.action == action.action}
urh,https://github.com/jopohl/urh/tree/master/src/urh/ui/views/TableView.py,TableView,selected_rows$89,"def selected_rows(self):
        rows = set()
        for index in self.selectionModel().selectedIndexes():
            rows.add(index.row())

        return sorted(rows)","rows = set()
for index in self.selectionModel().selectedIndexes():
    rows.add(index.row())",rows = {index.row() for index in self.selectionModel().selectedIndexes()}
neutron,https://github.com/openstack/neutron/tree/master/neutron/agent/l3/dvr_local_router.py,DvrLocalRouter,_delete_arp_cache_for_internal_port$306,"def _delete_arp_cache_for_internal_port(self, subnet_id):
        """"""Function to delete the cached arp entries.""""""
        arp_delete = set()
        for arp_entry in self._pending_arp_set:
            if subnet_id == arp_entry.subnet_id:
                arp_delete.add(arp_entry)
        self._pending_arp_set -= arp_delete","arp_delete = set()
for arp_entry in self._pending_arp_set:
    if subnet_id == arp_entry.subnet_id:
        arp_delete.add(arp_entry)",arp_delete = {arp_entry for arp_entry in self._pending_arp_set if subnet_id == arp_entry.subnet_id}
viztracer,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_multiprocess.py,TestMultiprocessing,test_ignore_multiprosessing$340,"def test_ignore_multiprosessing(self):
        def check_func(data):
            pids = set()
            for entry in data[""traceEvents""]:
                pids.add(entry[""pid""])
            self.assertEqual(len(pids), 1)

        self.template([""viztracer"", ""-o"", ""result.json"", ""--ignore_multiprocess"", ""cmdline_test.py""],
                      expected_output_file=""result.json"",
                      script=file_multiprocessing,
                      check_func=check_func,
                      concurrency=""multiprocessing"")","pids = set()
for entry in data['traceEvents']:
    pids.add(entry['pid'])",pids = {entry['pid'] for entry in data['traceEvents']}
mailinabox,https://github.com/mail-in-a-box/mailinabox/tree/master/management/mailconfig.py,,get_admins$179,"def get_admins(env):
	# Returns a set of users with admin privileges.
	users = set()
	for domain in get_mail_users_ex(env):
		for user in domain[""users""]:
			if ""admin"" in user[""privileges""]:
				users.add(user[""email""])
	return users","users = set()
for domain in get_mail_users_ex(env):
    for user in domain['users']:
        if 'admin' in user['privileges']:
            users.add(user['email'])",users = {user['email'] for domain in get_mail_users_ex(env) for user in domain['users'] if 'admin' in user['privileges']}
shuup,https://github.com/shuup/shuup/tree/master/shuup/admin/modules/orders/json_order_creator.py,JsonOrderCreator,get_removed_product_ids$438,"def get_removed_product_ids(self, state, order_to_update):
        """"""
        Collect product ids for products which were removed from the order.

        :param state: State dictionary.
        :type state: dict
        :param order_to_update: Order object to edit.
        :type order_to_update: shuup.core.models.Order
        :return: set
        """"""

        current_lines = state.get(""lines"", [])
        current_product_ids = set()
        for line in current_lines:
            if line[""type""] == ""product"" and line[""product""] is not None:
                current_product_ids.add(line[""product""][""id""])

        old_prod_ids = set()
        for line in order_to_update.lines.exclude(product_id=None):
            old_prod_ids.add(line.product.id)

        return old_prod_ids - current_product_ids","current_product_ids = set()
for line in current_lines:
    if line['type'] == 'product' and line['product'] is not None:
        current_product_ids.add(line['product']['id'])",current_product_ids = {line['product']['id'] for line in current_lines if line['type'] == 'product' and line['product'] is not None}
shuup,https://github.com/shuup/shuup/tree/master/shuup/admin/modules/orders/json_order_creator.py,JsonOrderCreator,get_removed_product_ids$438,"def get_removed_product_ids(self, state, order_to_update):
        """"""
        Collect product ids for products which were removed from the order.

        :param state: State dictionary.
        :type state: dict
        :param order_to_update: Order object to edit.
        :type order_to_update: shuup.core.models.Order
        :return: set
        """"""

        current_lines = state.get(""lines"", [])
        current_product_ids = set()
        for line in current_lines:
            if line[""type""] == ""product"" and line[""product""] is not None:
                current_product_ids.add(line[""product""][""id""])

        old_prod_ids = set()
        for line in order_to_update.lines.exclude(product_id=None):
            old_prod_ids.add(line.product.id)

        return old_prod_ids - current_product_ids","old_prod_ids = set()
for line in order_to_update.lines.exclude(product_id=None):
    old_prod_ids.add(line.product.id)",old_prod_ids = {line.product.id for line in order_to_update.lines.exclude(product_id=None)}
FARM,https://github.com/deepset-ai/FARM/tree/master/farm/file_utils.py,,read_set_from_file$301,"def read_set_from_file(filename):
    """"""
    Extract a de-duped collection (set) of text from a file.
    Expected file format is one item per line.
    """"""
    collection = set()
    with open(filename, ""r"", encoding=""utf-8"") as file_:
        for line in file_:
            collection.add(line.rstrip())
    return collection","collection = set()
for line in file_:
    collection.add(line.rstrip())",collection = {line.rstrip() for line in file_}
spiderfoot,https://github.com/smicallef/spiderfoot/tree/master//sflib.py,SpiderFoot,parseEmails$1240,"def parseEmails(self, data: str) -> list:
        """"""Extract all email addresses within the supplied content.

        Args:
            data (str): text to search for email addresses

        Returns:
            list: list of email addresses
        """"""
        if not isinstance(data, str):
            return list()

        emails = set()
        matches = re.findall(r'([\%a-zA-Z\.0-9_\-\+]+@[a-zA-Z\.0-9\-]+\.[a-zA-Z\.0-9\-]+)', data)

        for match in matches:
            if self.validEmail(match):
                emails.add(match)

        return list(emails)","emails = set()
for match in matches:
    if self.validEmail(match):
        emails.add(match)",emails = {match for match in matches if self.validEmail(match)}
goatools,https://github.com/tanghaibao/goatools/tree/master//versioneer.py,,do_setup$1697,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError,
            configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"",
                  file=sys.stderr)
            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with open(cfg.versionfile_source, ""w"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {""DOLLAR"": ""$"",
                        ""STYLE"": cfg.style,
                        ""TAG_PREFIX"": cfg.tag_prefix,
                        ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
                        })

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),
                       ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy, ""r"") as f:
                old = f.read()
        except EnvironmentError:
            old = """"
        if INIT_PY_SNIPPET not in old:
            print("" appending to %s"" % ipy)
            with open(ipy, ""a"") as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with open(manifest_in, ""r"") as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, ""a"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" %
              cfg.versionfile_source)
        with open(manifest_in, ""a"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-subst keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","simple_includes = set()
for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}
FreeCAD_assembly3,https://github.com/realthunder/FreeCAD_assembly3/tree/master/freecad/asm3/assembly.py,AsmElementGroup,onChildLabelChange$3305,"def onChildLabelChange(self,obj,label):
        names = set()
        label = label.replace('.','_')
        for o in flattenGroup(self.Object):
            if o != obj:
                names.add(o.Label)
        if label not in names:
            return label
        for i,c in enumerate(reversed(label)):
            if not c.isdigit():
                if i:
                    label = label[:-i]
                break;
        i=0
        while True:
            i=i+1;
            newLabel = '{}{:03d}'.format(label,i);
            if newLabel!=obj.Label and newLabel not in names:
                return newLabel
        return label","names = set()
for o in flattenGroup(self.Object):
    if o != obj:
        names.add(o.Label)",names = {o.Label for o in flattenGroup(self.Object) if o != obj}
freemocap,https://github.com/jonmatthis/freemocap/tree/master/freemocap_blender_addon/freemocap_blender_addon/auto_load.py,,get_classes_in_modules$121,"def get_classes_in_modules(modules):
    classes = set()
    for module in modules:
        for cls in iter_classes_in_module(module):
            classes.add(cls)
    return classes","classes = set()
for module in modules:
    for cls in iter_classes_in_module(module):
        classes.add(cls)",classes = {cls for module in modules for cls in iter_classes_in_module(module)}
detection-rules,https://github.com/elastic/detection-rules/tree/master/detection_rules/rule_formatter.py,,get_preserved_fmt_fields$27,"def get_preserved_fmt_fields():
    from .rule import BaseRuleData
    preserved_keys = set()

    for field in dataclasses.fields(BaseRuleData):  # type: dataclasses.Field
        if field.type in (definitions.Markdown, typing.Optional[definitions.Markdown]):
            preserved_keys.add(field.metadata.get(""data_key"", field.name))
    return preserved_keys","preserved_keys = set()
for field in dataclasses.fields(BaseRuleData):
    if field.type in (definitions.Markdown, typing.Optional[definitions.Markdown]):
        preserved_keys.add(field.metadata.get('data_key', field.name))","preserved_keys = {field.metadata.get('data_key', field.name) for field in dataclasses.fields(BaseRuleData) if field.type in (definitions.Markdown, typing.Optional[definitions.Markdown])}"
networkx,https://github.com/networkx/networkx/tree/master/networkx/algorithms/approximation/tests/test_treewidth.py,TestTreewidthMinDegree,test_heuristic_abort$108,"def test_heuristic_abort(self):
        """"""Test heuristic abort condition for fully connected graph""""""
        graph = {}
        for u in self.complete:
            graph[u] = set()
            for v in self.complete[u]:
                if u != v:  # ignore self-loop
                    graph[u].add(v)

        deg_heuristic = MinDegreeHeuristic(graph)
        node = deg_heuristic.best_node(graph)
        if node is None:
            pass
        else:
            assert False","graph[u] = set()
for v in self.complete[u]:
    if u != v:
        graph[u].add(v)",graph[u] = {v for v in self.complete[u] if u != v}
modin,https://github.com/modin-project/modin/tree/master//versioneer.py,,do_setup$1697,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError,
            configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"",
                  file=sys.stderr)
            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with open(cfg.versionfile_source, ""w"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {""DOLLAR"": ""$"",
                        ""STYLE"": cfg.style,
                        ""TAG_PREFIX"": cfg.tag_prefix,
                        ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
                        })

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),
                       ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy, ""r"") as f:
                old = f.read()
        except EnvironmentError:
            old = """"
        if INIT_PY_SNIPPET not in old:
            print("" appending to %s"" % ipy)
            with open(ipy, ""a"") as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with open(manifest_in, ""r"") as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, ""a"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" %
              cfg.versionfile_source)
        with open(manifest_in, ""a"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-subst keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","simple_includes = set()
for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}
aiokafka,https://github.com/aio-libs/aiokafka/tree/master/aiokafka/consumer/subscription_state.py,SubscriptionState,paused_partitions$283,"def paused_partitions(self) -> Set[TopicPartition]:
        res = set()
        for tp in self.assigned_partitions():
            if self._assigned_state(tp).paused:
                res.add(tp)
        return res","res = set()
for tp in self.assigned_partitions():
    if self._assigned_state(tp).paused:
        res.add(tp)",res = {tp for tp in self.assigned_partitions() if self._assigned_state(tp).paused}
featuretools,https://github.com/alteryx/featuretools/tree/master/featuretools/synthesis/deep_feature_synthesis.py,DeepFeatureSynthesis,__init__$131,"def __init__(
        self,
        target_dataframe_name,
        entityset,
        agg_primitives=None,
        trans_primitives=None,
        where_primitives=None,
        groupby_trans_primitives=None,
        max_depth=2,
        max_features=-1,
        allowed_paths=None,
        ignore_dataframes=None,
        ignore_columns=None,
        primitive_options=None,
        seed_features=None,
        drop_contains=None,
        drop_exact=None,
        where_stacking_limit=1,
    ):

        if target_dataframe_name not in entityset.dataframe_dict:
            es_name = entityset.id or ""entity set""
            msg = ""Provided target dataframe %s does not exist in %s"" % (
                target_dataframe_name,
                es_name,
            )
            raise KeyError(msg)

        # Multiple calls to dfs() should start with a fresh cache
        feature_cache.clear_all()
        feature_cache.enabled = True

        # need to change max_depth to None because DFs terminates when  <0
        if max_depth == -1:
            max_depth = None

        # if just one dataframe, set max depth to 1 (transform stacking rule)
        if len(entityset.dataframe_dict) == 1 and (max_depth is None or max_depth > 1):
            warnings.warn(
                ""Only one dataframe in entityset, changing max_depth to ""
                ""1 since deeper features cannot be created"",
            )
            max_depth = 1

        self.max_depth = max_depth

        self.max_features = max_features

        self.allowed_paths = allowed_paths
        if self.allowed_paths:
            self.allowed_paths = set()
            for path in allowed_paths:
                self.allowed_paths.add(tuple(path))

        if ignore_dataframes is None:
            self.ignore_dataframes = set()
        else:
            if not isinstance(ignore_dataframes, list):
                raise TypeError(""ignore_dataframes must be a list"")
            assert (
                target_dataframe_name not in ignore_dataframes
            ), ""Can't ignore target_dataframe!""
            self.ignore_dataframes = set(ignore_dataframes)

        self.ignore_columns = defaultdict(set)
        if ignore_columns is not None:
            # check if ignore_columns is not {str: list}
            if not all(isinstance(i, str) for i in ignore_columns.keys()) or not all(
                isinstance(i, list) for i in ignore_columns.values()
            ):
                raise TypeError(""ignore_columns should be dict[str -> list]"")
            # check if list values are all of type str
            elif not all(
                all(isinstance(v, str) for v in value)
                for value in ignore_columns.values()
            ):
                raise TypeError(""list values should be of type str"")
            for df_name, cols in ignore_columns.items():
                self.ignore_columns[df_name] = set(cols)
        self.target_dataframe_name = target_dataframe_name
        self.es = entityset

        for library in Library:
            if library.value == self.es.dataframe_type:
                df_library = library
                break

        aggregation_primitive_dict = primitives.get_aggregation_primitives()
        transform_primitive_dict = primitives.get_transform_primitives()
        if agg_primitives is None:
            agg_primitives = [
                p
                for p in primitives.get_default_aggregation_primitives()
                if df_library in p.compatibility
            ]
        self.agg_primitives = sorted(
            [
                check_primitive(
                    p,
                    ""aggregation"",
                    aggregation_primitive_dict,
                    transform_primitive_dict,
                )
                for p in agg_primitives
            ],
        )

        if trans_primitives is None:
            trans_primitives = [
                p
                for p in primitives.get_default_transform_primitives()
                if df_library in p.compatibility
            ]
        self.trans_primitives = sorted(
            [
                check_primitive(
                    p,
                    ""transform"",
                    aggregation_primitive_dict,
                    transform_primitive_dict,
                )
                for p in trans_primitives
            ],
        )

        if where_primitives is None:
            where_primitives = [primitives.Count]
        self.where_primitives = sorted(
            [
                check_primitive(
                    p,
                    ""where"",
                    aggregation_primitive_dict,
                    transform_primitive_dict,
                )
                for p in where_primitives
            ],
        )

        if groupby_trans_primitives is None:
            groupby_trans_primitives = []
        self.groupby_trans_primitives = sorted(
            [
                check_primitive(
                    p,
                    ""groupby transform"",
                    aggregation_primitive_dict,
                    transform_primitive_dict,
                )
                for p in groupby_trans_primitives
            ],
        )

        if primitive_options is None:
            primitive_options = {}
        all_primitives = (
            self.trans_primitives
            + self.agg_primitives
            + self.where_primitives
            + self.groupby_trans_primitives
        )
        bad_primitives = [
            prim.name for prim in all_primitives if df_library not in prim.compatibility
        ]
        if bad_primitives:
            msg = ""Selected primitives are incompatible with {} EntitySets: {}""
            raise ValueError(msg.format(df_library.value, "", "".join(bad_primitives)))

        (
            self.primitive_options,
            self.ignore_dataframes,
            self.ignore_columns,
        ) = generate_all_primitive_options(
            all_primitives,
            primitive_options,
            self.ignore_dataframes,
            self.ignore_columns,
            self.es,
        )
        self.seed_features = sorted(seed_features or [], key=lambda f: f.unique_name())
        self.drop_exact = drop_exact or []
        self.drop_contains = drop_contains or []
        self.where_stacking_limit = where_stacking_limit","self.allowed_paths = set()
for path in allowed_paths:
    self.allowed_paths.add(tuple(path))",self.allowed_paths = {tuple(path) for path in allowed_paths}
aws-parallelcluster,https://github.com/aws/aws-parallelcluster/tree/master/util/rollback_s3_objects.py,,main$88,"def main():
    args = _parse_args()
    logging.info(""Parsed cli args: %s"", vars(args))

    regions = set()
    with open(args.rollback_file_path, encoding=""utf-8"") as rollback_file:
        rollback_data = json.load(rollback_file)
        for bucket in rollback_data.keys():
            regions.add(rollback_data[bucket][""region""])

    sts_credentials = retrieve_sts_credentials(args.credentials, PARTITION_TO_MAIN_REGION[args.partition], regions)
    execute_rollback(args.rollback_file_path, sts_credentials, args.deploy)","regions = set()
for bucket in rollback_data.keys():
    regions.add(rollback_data[bucket]['region'])",regions = {rollback_data[bucket]['region'] for bucket in rollback_data.keys()}
viztracer,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_regression.py,TestFunctionArg,test_functionarg$79,"def test_functionarg(self):
        def f(n):
            tracer.add_func_args(""input"", n)
            if n < 2:
                return 1
            return f(n - 1) + f(n - 2)
        tracer = VizTracer(verbose=0)
        tracer.start()
        f(5)
        tracer.stop()
        tracer.parse()
        inputs = set()
        for d in tracer.data[""traceEvents""]:
            if d[""ph""] == ""X"":
                inputs.add(d[""args""][""input""])
        self.assertEqual(inputs, set([0, 1, 2, 3, 4, 5]))","inputs = set()
for d in tracer.data['traceEvents']:
    if d['ph'] == 'X':
        inputs.add(d['args']['input'])",inputs = {d['args']['input'] for d in tracer.data['traceEvents'] if d['ph'] == 'X'}
python-driver,https://github.com/datastax/python-driver/tree/master/tests/integration/long/test_loadbalancingpolicies.py,LoadBalancingPolicyTests,test_dc_aware_roundrobin_one_remote_host$329,"def test_dc_aware_roundrobin_one_remote_host(self):
        use_multidc([2, 2])
        keyspace = 'test_dc_aware_roundrobin_one_remote_host'
        cluster, session = self._cluster_session_with_lbp(DCAwareRoundRobinPolicy('dc2', used_hosts_per_remote_dc=1))
        self.addCleanup(cluster.shutdown)
        self._wait_for_nodes_up(range(1, 5))

        create_schema(cluster, session, keyspace, replication_strategy=[2, 2])
        self._insert(session, keyspace)
        self._query(session, keyspace)

        self.coordinator_stats.assert_query_count_equals(self, 1, 0)
        self.coordinator_stats.assert_query_count_equals(self, 2, 0)
        self.coordinator_stats.assert_query_count_equals(self, 3, 6)
        self.coordinator_stats.assert_query_count_equals(self, 4, 6)

        self.coordinator_stats.reset_counts()
        bootstrap(5, 'dc1')
        self._wait_for_nodes_up([5])

        self._query(session, keyspace)

        self.coordinator_stats.assert_query_count_equals(self, 1, 0)
        self.coordinator_stats.assert_query_count_equals(self, 2, 0)
        self.coordinator_stats.assert_query_count_equals(self, 3, 6)
        self.coordinator_stats.assert_query_count_equals(self, 4, 6)
        self.coordinator_stats.assert_query_count_equals(self, 5, 0)

        self.coordinator_stats.reset_counts()
        decommission(3)
        decommission(4)
        self._wait_for_nodes_down([3, 4])

        self._query(session, keyspace)

        self.coordinator_stats.assert_query_count_equals(self, 3, 0)
        self.coordinator_stats.assert_query_count_equals(self, 4, 0)
        responses = set()
        for node in [1, 2, 5]:
            responses.add(self.coordinator_stats.get_query_count(node))
        self.assertEqual(set([0, 0, 12]), responses)

        self.coordinator_stats.reset_counts()
        decommission(5)
        self._wait_for_nodes_down([5])

        self._query(session, keyspace)

        self.coordinator_stats.assert_query_count_equals(self, 3, 0)
        self.coordinator_stats.assert_query_count_equals(self, 4, 0)
        self.coordinator_stats.assert_query_count_equals(self, 5, 0)
        responses = set()
        for node in [1, 2]:
            responses.add(self.coordinator_stats.get_query_count(node))
        self.assertEqual(set([0, 12]), responses)

        self.coordinator_stats.reset_counts()
        decommission(1)
        self._wait_for_nodes_down([1])

        self._query(session, keyspace)

        self.coordinator_stats.assert_query_count_equals(self, 1, 0)
        self.coordinator_stats.assert_query_count_equals(self, 2, 12)
        self.coordinator_stats.assert_query_count_equals(self, 3, 0)
        self.coordinator_stats.assert_query_count_equals(self, 4, 0)
        self.coordinator_stats.assert_query_count_equals(self, 5, 0)

        self.coordinator_stats.reset_counts()
        force_stop(2)

        try:
            self._query(session, keyspace)
            self.fail()
        except NoHostAvailable:
            pass","responses = set()
for node in [1, 2, 5]:
    responses.add(self.coordinator_stats.get_query_count(node))","responses = {self.coordinator_stats.get_query_count(node) for node in [1, 2, 5]}"
python-driver,https://github.com/datastax/python-driver/tree/master/tests/integration/long/test_loadbalancingpolicies.py,LoadBalancingPolicyTests,test_dc_aware_roundrobin_one_remote_host$329,"def test_dc_aware_roundrobin_one_remote_host(self):
        use_multidc([2, 2])
        keyspace = 'test_dc_aware_roundrobin_one_remote_host'
        cluster, session = self._cluster_session_with_lbp(DCAwareRoundRobinPolicy('dc2', used_hosts_per_remote_dc=1))
        self.addCleanup(cluster.shutdown)
        self._wait_for_nodes_up(range(1, 5))

        create_schema(cluster, session, keyspace, replication_strategy=[2, 2])
        self._insert(session, keyspace)
        self._query(session, keyspace)

        self.coordinator_stats.assert_query_count_equals(self, 1, 0)
        self.coordinator_stats.assert_query_count_equals(self, 2, 0)
        self.coordinator_stats.assert_query_count_equals(self, 3, 6)
        self.coordinator_stats.assert_query_count_equals(self, 4, 6)

        self.coordinator_stats.reset_counts()
        bootstrap(5, 'dc1')
        self._wait_for_nodes_up([5])

        self._query(session, keyspace)

        self.coordinator_stats.assert_query_count_equals(self, 1, 0)
        self.coordinator_stats.assert_query_count_equals(self, 2, 0)
        self.coordinator_stats.assert_query_count_equals(self, 3, 6)
        self.coordinator_stats.assert_query_count_equals(self, 4, 6)
        self.coordinator_stats.assert_query_count_equals(self, 5, 0)

        self.coordinator_stats.reset_counts()
        decommission(3)
        decommission(4)
        self._wait_for_nodes_down([3, 4])

        self._query(session, keyspace)

        self.coordinator_stats.assert_query_count_equals(self, 3, 0)
        self.coordinator_stats.assert_query_count_equals(self, 4, 0)
        responses = set()
        for node in [1, 2, 5]:
            responses.add(self.coordinator_stats.get_query_count(node))
        self.assertEqual(set([0, 0, 12]), responses)

        self.coordinator_stats.reset_counts()
        decommission(5)
        self._wait_for_nodes_down([5])

        self._query(session, keyspace)

        self.coordinator_stats.assert_query_count_equals(self, 3, 0)
        self.coordinator_stats.assert_query_count_equals(self, 4, 0)
        self.coordinator_stats.assert_query_count_equals(self, 5, 0)
        responses = set()
        for node in [1, 2]:
            responses.add(self.coordinator_stats.get_query_count(node))
        self.assertEqual(set([0, 12]), responses)

        self.coordinator_stats.reset_counts()
        decommission(1)
        self._wait_for_nodes_down([1])

        self._query(session, keyspace)

        self.coordinator_stats.assert_query_count_equals(self, 1, 0)
        self.coordinator_stats.assert_query_count_equals(self, 2, 12)
        self.coordinator_stats.assert_query_count_equals(self, 3, 0)
        self.coordinator_stats.assert_query_count_equals(self, 4, 0)
        self.coordinator_stats.assert_query_count_equals(self, 5, 0)

        self.coordinator_stats.reset_counts()
        force_stop(2)

        try:
            self._query(session, keyspace)
            self.fail()
        except NoHostAvailable:
            pass","responses = set()
for node in [1, 2]:
    responses.add(self.coordinator_stats.get_query_count(node))","responses = {self.coordinator_stats.get_query_count(node) for node in [1, 2]}"
Mobile-Security-Framework-MobSF,https://github.com/MobSF/Mobile-Security-Framework-MobSF/tree/master/mobsf/MobSF/utils.py,,find_process_by$315,"def find_process_by(name):
    """"""Return a set of process path matching name.""""""
    proc = set()
    for p in psutil.process_iter(attrs=['name']):
        if (name == p.info['name']):
            proc.add(p.exe())
    return proc","proc = set()
for p in psutil.process_iter(attrs=['name']):
    if name == p.info['name']:
        proc.add(p.exe())",proc = {p.exe() for p in psutil.process_iter(attrs=['name']) if name == p.info['name']}
pywikibot,https://github.com/wikimedia/pywikibot/tree/master/scripts/maintenance/wikimedia_sites.py,,if_main_my$94,"if __name__ == '__main__':
    fam = set()
    for arg in pywikibot.handle_args():
        if arg in families_list:
            fam.add(arg)
    update_family(fam)","fam = set()
for arg in pywikibot.handle_args():
    if arg in families_list:
        fam.add(arg)",fam = {arg for arg in pywikibot.handle_args() if arg in families_list}
KILT,https://github.com/facebookresearch/KILT/tree/master/kilt/readers/fid/preprocess.py,,convert_kilt$13,"def convert_kilt(inputpath, outputpath):
    data = []
    inputdata = open(inputpath, ""r"")
    for example in tqdm(inputdata):
        d = {}
        ex = json.loads(example)
        d[""question""] = ex[""input""]
        answers = set()
        for a in ex[""output""]:
            if ""answer"" in a:
                answers.add(a[""answer""])
        d[""answers""] = list(answers)
        d[""id""] = ex[""id""]
        passages = []
        for c in ex[""output""][0][""provenance""]:
            p = {""text"": c[""text""], ""title"": """"}
            if ""wikipedia_title"" in c:
                p[""title""] = c[""wikipedia_title""]
            if ""wikipedia_id"" in c:
                p[""wikipedia_id""] = c[""wikipedia_id""]
            passages.append(p)
        d[""ctxs""] = passages
        data.append(d)
    with open(outputpath, ""w"") as fout:
        json.dump(data, fout)","answers = set()
for a in ex['output']:
    if 'answer' in a:
        answers.add(a['answer'])",answers = {a['answer'] for a in ex['output'] if 'answer' in a}
Malt,https://github.com/bnpr/Malt/tree/master/BlenderMalt/MaltNodes.py,,preload_menus$1015,"def preload_menus(structs, functions):
    files = set()
    for name, struct in structs.items():
        files.add(struct['file'])
    for file in files:
        get_structs_menu(file)
    
    files = set()
    for name, function in functions.items():
        files.add(function['file'])
    for file in files:
        get_functions_menu(file)","files = set()
for (name, struct) in structs.items():
    files.add(struct['file'])","files = {struct['file'] for (name, struct) in structs.items()}"
Malt,https://github.com/bnpr/Malt/tree/master/BlenderMalt/MaltNodes.py,,preload_menus$1015,"def preload_menus(structs, functions):
    files = set()
    for name, struct in structs.items():
        files.add(struct['file'])
    for file in files:
        get_structs_menu(file)
    
    files = set()
    for name, function in functions.items():
        files.add(function['file'])
    for file in files:
        get_functions_menu(file)","files = set()
for (name, function) in functions.items():
    files.add(function['file'])","files = {function['file'] for (name, function) in functions.items()}"
ormar,https://github.com/collerek/ormar/tree/master/ormar/models/mixins/relation_mixin.py,RelationMixin,extract_through_names$56,"def extract_through_names(cls) -> Set[str]:
        """"""
        Extracts related fields through names which are shortcuts to through models.

        :return: set of related through fields names
        :rtype: Set
        """"""
        if cls._through_names is not None:
            return cls._through_names

        related_names = set()
        for name, field in cls.Meta.model_fields.items():
            if isinstance(field, BaseField) and field.is_through:
                related_names.add(name)

        cls._through_names = related_names
        return related_names","related_names = set()
for (name, field) in cls.Meta.model_fields.items():
    if isinstance(field, BaseField) and field.is_through:
        related_names.add(name)","related_names = {name for (name, field) in cls.Meta.model_fields.items() if isinstance(field, BaseField) and field.is_through}"
UNITER,https://github.com/ChenRocks/UNITER/tree/master//train_vcr.py,,main$116,"def main(opts):
    hvd.init()
    n_gpu = hvd.size()
    device = torch.device(""cuda"", hvd.local_rank())
    torch.cuda.set_device(hvd.local_rank())
    rank = hvd.rank()
    opts.rank = rank
    LOGGER.info(""device: {} n_gpu: {}, rank: {}, ""
                ""16-bits training: {}"".format(
                    device, n_gpu, hvd.rank(), opts.fp16))

    if opts.gradient_accumulation_steps < 1:
        raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, ""
                         ""should be >= 1"".format(
                            opts.gradient_accumulation_steps))

    set_random_seed(opts.seed)

    # load DBs and image dirs
    all_img_dbs = ImageLmdbGroup(opts.conf_th, opts.max_bb, opts.min_bb,
                                 opts.num_bb, opts.compressed_db)
    # train
    LOGGER.info(f""Loading Train Dataset ""
                f""{opts.train_txt_dbs}, {opts.train_img_dbs}"")
    train_datasets = []
    for txt_path, img_path in zip(opts.train_txt_dbs, opts.train_img_dbs):
        img_db, img_db_gt = load_img_feat(img_path, all_img_dbs, opts)
        qa_txt_db = VcrTxtTokLmdb(txt_path, opts.max_txt_len, task=""qa"")
        qar_txt_db = VcrTxtTokLmdb(txt_path, opts.max_txt_len, task=""qar"")
        train_datasets.append(
            VcrDataset(qa_txt_db, img_db_gt=img_db_gt, img_db=img_db))
        train_datasets.append(
            VcrDataset(qar_txt_db, img_db_gt=img_db_gt, img_db=img_db))
    train_dataset = ConcatDatasetWithLens(train_datasets)
    train_dataloader = build_dataloader(train_dataset, vcr_collate, True, opts)
    # val
    LOGGER.info(f""Loading Val Dataset {opts.val_txt_db}, {opts.val_img_db}"")
    val_img_db, val_img_db_gt = load_img_feat(
        opts.val_img_db, all_img_dbs, opts)
    val_txt_db = VcrTxtTokLmdb(opts.val_txt_db, -1)
    val_dataset = VcrEvalDataset(
        ""val"", val_txt_db, img_db=val_img_db, img_db_gt=val_img_db_gt)
    val_final_dataset = VcrEvalDataset(
        ""test"", val_txt_db, img_db=val_img_db, img_db_gt=val_img_db_gt)
    val_dataloader = build_dataloader(val_dataset, vcr_eval_collate,
                                      False, opts)
    val_final_dataloader = build_dataloader(
        val_final_dataset, vcr_eval_collate,
        False, opts)

    # Prepare model
    if opts.checkpoint and opts.checkpoint_from == ""pretrain"":
        checkpoint = torch.load(opts.checkpoint)
    else:
        checkpoint = {}

    all_dbs = opts.train_txt_dbs + [opts.val_txt_db]
    toker = json.load(open(f'{all_dbs[0]}/meta.json'))['bert']
    assert all(toker == json.load(open(f'{db}/meta.json'))['bert']
               for db in all_dbs)
    model = UniterForVisualCommonsenseReasoning.from_pretrained(
        opts.model_config, checkpoint, img_dim=IMG_DIM)
    model.init_type_embedding()
    model.init_word_embedding(NUM_SPECIAL_TOKENS)
    if opts.checkpoint_from == ""vcr_pretrain"":
        checkpoint = torch.load(opts.checkpoint)
        state_dict = checkpoint.get('model_state', checkpoint)
        matched_state_dict = {}
        unexpected_keys = set()
        missing_keys = set()
        for name, param in model.named_parameters():
            missing_keys.add(name)
        for key, data in state_dict.items():
            if key in missing_keys:
                matched_state_dict[key] = data
                missing_keys.remove(key)
            else:
                unexpected_keys.add(key)
        print(""Unexpected_keys:"", list(unexpected_keys))
        print(""Missing_keys:"", list(missing_keys))
        model.load_state_dict(matched_state_dict, strict=False)
    del checkpoint
    model.to(device)
    # make sure every process has same model parameters in the beginning
    broadcast_tensors([p.data for p in model.parameters()], 0)
    set_dropout(model, opts.dropout)

    # Prepare optimizer
    optimizer = build_optimizer(model, opts)
    model, optimizer = amp.initialize(model, optimizer,
                                      enabled=opts.fp16, opt_level='O2')
    global_step = 0
    if rank == 0:
        save_training_meta(opts)
        TB_LOGGER.create(join(opts.output_dir, 'log'))
        pbar = tqdm(total=opts.num_train_steps)
        model_saver = ModelSaver(join(opts.output_dir, 'ckpt'))
        os.makedirs(join(opts.output_dir, 'results'))  # store VQA predictions
        add_log_to_file(join(opts.output_dir, 'log', 'log.txt'))
    else:
        LOGGER.disabled = True
        pbar = NoOp()
        model_saver = NoOp()

    LOGGER.info(f""***** Running training with {n_gpu} GPUs *****"")
    LOGGER.info(""  Num examples = %d"", len(train_dataset) * hvd.size())
    LOGGER.info(""  Batch size = %d"", opts.train_batch_size)
    LOGGER.info(""  Accumulate steps = %d"", opts.gradient_accumulation_steps)
    LOGGER.info(""  Num steps = %d"", opts.num_train_steps)

    running_loss = RunningMeter('loss')
    model.train()
    n_examples = 0
    n_epoch = 0
    start = time()
    # quick hack for amp delay_unscale bug
    optimizer.zero_grad()
    optimizer.step()
    while True:
        for step, batch in enumerate(train_dataloader):
            n_examples += batch['input_ids'].size(0)

            loss = model(batch, compute_loss=True)
            loss = loss.mean()
            delay_unscale = (step+1) % opts.gradient_accumulation_steps != 0
            with amp.scale_loss(loss, optimizer, delay_unscale=delay_unscale
                                ) as scaled_loss:
                scaled_loss.backward()
                if not delay_unscale:
                    # gather gradients from every processes
                    # do this before unscaling to make sure every process uses
                    # the same gradient scale
                    grads = [p.grad.data for p in model.parameters()
                             if p.requires_grad and p.grad is not None]
                    all_reduce_and_rescale_tensors(grads, float(1))

            running_loss(loss.item())

            if (step + 1) % opts.gradient_accumulation_steps == 0:
                global_step += 1

                # learning rate scheduling
                lr_this_step = get_lr_sched(global_step, opts)
                for i, param_group in enumerate(optimizer.param_groups):
                    if i == 0 or i == 1:
                        param_group['lr'] = lr_this_step * opts.lr_mul
                    elif i == 2 or i == 3:
                        param_group['lr'] = lr_this_step
                    else:
                        raise ValueError()
                TB_LOGGER.add_scalar('lr', lr_this_step, global_step)

                # log loss
                # NOTE: not gathered across GPUs for efficiency
                TB_LOGGER.add_scalar('loss', running_loss.val, global_step)
                TB_LOGGER.step()

                # update model params
                if opts.grad_norm != -1:
                    grad_norm = clip_grad_norm_(amp.master_params(optimizer),
                                                opts.grad_norm)
                    TB_LOGGER.add_scalar('grad_norm', grad_norm, global_step)
                optimizer.step()
                optimizer.zero_grad()
                pbar.update(1)

                if global_step % 100 == 0:
                    # monitor training throughput
                    LOGGER.info(f'============Step {global_step}=============')
                    tot_ex = sum(all_gather_list(n_examples))
                    ex_per_sec = int(tot_ex / (time()-start))
                    LOGGER.info(f'{tot_ex} examples trained at '
                                f'{ex_per_sec} ex/s')
                    TB_LOGGER.add_scalar('perf/ex_per_s',
                                         ex_per_sec, global_step)
                    LOGGER.info('===========================================')

                if global_step % opts.valid_steps == 0:
                    val_log, results = validate(
                        model, val_dataloader)
                    TB_LOGGER.log_scaler_dict(val_log)
                    model_saver.save(model, global_step)
            if global_step >= opts.num_train_steps:
                break
        if global_step >= opts.num_train_steps:
            break
        n_epoch += 1
        LOGGER.info(f""finished {n_epoch} epochs"")
    if global_step % opts.valid_steps != 0:
        val_log, results = validate(
            model, val_dataloader)
        TB_LOGGER.log_scaler_dict(val_log)
    val_log, results = validate(model, val_final_dataloader)
    with open(f'{opts.output_dir}/results/'
              f'results_{global_step}_final_qa_qar_'
              f'rank{rank}.json', 'w') as f:
        json.dump(results, f)
    TB_LOGGER.log_scaler_dict(val_log)
    model_saver.save(model, global_step)","missing_keys = set()
for (name, param) in model.named_parameters():
    missing_keys.add(name)","missing_keys = {name for (name, param) in model.named_parameters()}"
prjxray,https://github.com/SymbiFlow/prjxray/tree/master/fuzzers/060-bram-cascades/top.py,,random_sdp_bram$120,"def random_sdp_bram(luts, name, modules, lines):
    sdp_choices = set()

    for width in (1, 2, 4, 8, 16, 18, 32, 36):
        sdp_choices.add((width, (1, max_address_bits(width))))

    for nbram in range(2, MAX_BRAM + 1):
        sdp_choices.add((nbram * 32, (1, max_address_bits(nbram * 32))))
        sdp_choices.add((nbram * 36, (1, max_address_bits(nbram * 36))))
        sdp_choices.add((nbram * 16, (1, max_address_bits(nbram * 16))))
        sdp_choices.add((nbram * 32, (1, max_address_bits(nbram * 32))))

        # Bias some wide but shallow BRAMs to toggle the lower address bits
        # more.
        for address_bits in range(1, 4):
            sdp_choices.add((nbram * 16, (address_bits, address_bits)))

    sdp_choices = sorted(sdp_choices)

    width, address_bits_range = random.choice(sdp_choices)
    address_bits = random.randint(*address_bits_range)
    return emit_sdp_bram(luts, name, modules, lines, width, address_bits)","sdp_choices = set()
for width in (1, 2, 4, 8, 16, 18, 32, 36):
    sdp_choices.add((width, (1, max_address_bits(width))))","sdp_choices = {(width, (1, max_address_bits(width))) for width in (1, 2, 4, 8, 16, 18, 32, 36)}"
habu,https://github.com/fportantier/habu/tree/master/habu/cli/cmd_gateway_find.py,,cmd_gateway_find$21,"def cmd_gateway_find(network, iface, host, tcp, dport, timeout, verbose):
    """"""
    Try to reach an external IP using any host has a router.

    Useful to find routers in your network.

    First, uses arping to detect alive hosts and obtain MAC addresses.

    Later, create a network packet and put each MAC address as destination.

    Last, print the devices that forwarded correctly the packets.

    Example:

    \b
    # habu.find.gateway 192.168.0.0/24
    192.168.0.1 a4:08:f5:19:17:a4 Sagemcom
    192.168.0.7 b0:98:2b:5d:22:70 Sagemcom
    192.168.0.8 b0:98:2b:5d:1f:e8 Sagemcom
    """"""

    if verbose:
        logging.basicConfig(level=logging.INFO, format='%(message)s')

    conf.verb = False

    if iface:
        iface = search_iface(iface)
        if iface:
            conf.iface = iface['name']
        else:
            logging.error('Interface {} not found. Use habu.interfaces to show valid network interfaces'.format(iface))
            return False

    res, unans = srp(Ether(dst=""ff:ff:ff:ff:ff:ff"")/ARP(pdst=network), timeout=2)

    neighbors = set()

    for _, pkt in res:
        neighbors.add((pkt['Ether'].src, pkt['Ether'].psrc))

    for mac,ip in neighbors:
        if tcp:
            res, unans = srp(Ether(dst=mac)/IP(dst=host)/TCP(dport=dport), timeout=timeout)
        else:
            res, unans = srp(Ether(dst=mac)/IP(dst=host)/ICMP(), timeout=timeout)
        for _,pkt in res:
            if pkt:
                if verbose:
                    print(pkt.show())
                else:
                    print(ip, mac, conf.manufdb._get_manuf(mac))","neighbors = set()
for (_, pkt) in res:
    neighbors.add((pkt['Ether'].src, pkt['Ether'].psrc))","neighbors = {(pkt['Ether'].src, pkt['Ether'].psrc) for (_, pkt) in res}"
ralph,https://github.com/allegro/ralph/tree/master/src/ralph/lib/permissions/models.py,PermByFieldMixin,allowed_fields$227,"def allowed_fields(cls, user, action='change'):
        """"""
        Returns a list with the names of the fields to which the user has
        permission.

        :Example:

            >> user = User.objects.get(username='root')
            >> model.allowed_fields(user, 'change')
            ['parent', 'remarks', 'service_env']

        :param user: User object
        :type user: django User object
        :param action: permission action (change/view)
        :type action: str

        :return: List of field names
        :rtype: list
        """"""
        result = set()
        blacklist = cls._permissions.blacklist

        for field in (cls._meta.fields + cls._meta.many_to_many):
            if (
                field.name not in blacklist and
                cls.has_access_to_field(field.name, user, action)
            ):
                result.add(field.name)
        # If the user does not have rights to view,
        # but has the right to change he can view the field
        if action == 'view':
            result |= cls.allowed_fields(user, 'change')
        return result","result = set()
for field in cls._meta.fields + cls._meta.many_to_many:
    if field.name not in blacklist and cls.has_access_to_field(field.name, user, action):
        result.add(field.name)","result = {field.name for field in cls._meta.fields + cls._meta.many_to_many if field.name not in blacklist and cls.has_access_to_field(field.name, user, action)}"
bridgy,https://github.com/snarfed/bridgy/tree/master//original_post_discovery.py,,_merge_hfeeds$389,"def _merge_hfeeds(feed1, feed2):
  """"""Merge items from two h-feeds into a composite feed. Skips items in
  feed2 that are already represented in feed1, based on the ""url"" property.

  Args:
    feed1: a list of dicts
    feed2: a list of dicts

  Returns:
    a list of dicts
  """"""
  seen = set()
  for item in feed1:
    for url in item.get('properties', {}).get('url', []):
      if isinstance(url, str):
        seen.add(url)

  return feed1 + [item for item in feed2 if all(
    (url not in seen) for url in item.get('properties', {}).get('url', []) if isinstance(url, str))]","seen = set()
for item in feed1:
    for url in item.get('properties', {}).get('url', []):
        if isinstance(url, str):
            seen.add(url)","seen = {url for item in feed1 for url in item.get('properties', {}).get('url', []) if isinstance(url, str)}"
swift,https://github.com/openstack/swift/tree/master/swift/container/backend.py,ContainerBroker,_get_shard_range_rows$1688,"def _get_shard_range_rows(self, connection=None, includes=None,
                              include_deleted=False, states=None,
                              include_own=False, exclude_others=False):
        """"""
        Returns a list of shard range rows.

        To get all shard ranges use ``include_own=True``. To get only the
        broker's own shard range use ``include_own=True`` and
        ``exclude_others=True``.

        :param connection: db connection
        :param includes: restricts the returned list to the shard range that
            includes the given value
        :param include_deleted: include rows marked as deleted
        :param states: include only rows matching the given state(s); can be an
            int or a list of ints.
        :param include_own: boolean that governs whether the row whose name
            matches the broker's path is included in the returned list. If
            True, that row is included, otherwise it is not included. Default
            is False.
        :param exclude_others: boolean that governs whether the rows whose
            names do not match the broker's path are included in the returned
            list. If True, those rows are not included, otherwise they are
            included. Default is False.
        :return: a list of tuples.
        """"""

        if exclude_others and not include_own:
            return []

        included_states = set()
        if isinstance(states, (list, tuple, set)):
            included_states.update(states)
        elif states is not None:
            included_states.add(states)

        # defaults to be used when legacy db's are missing columns
        default_values = {'reported': 0,
                          'tombstones': -1}

        def do_query(conn, defaults=None):
            condition = ''
            conditions = []
            params = []
            if not include_deleted:
                conditions.append('deleted=0')
            if included_states:
                conditions.append('state in (%s)' % ','.join(
                    '?' * len(included_states)))
                params.extend(included_states)
            if not include_own:
                conditions.append('name != ?')
                params.append(self.path)
            if exclude_others:
                conditions.append('name = ?')
                params.append(self.path)
            if includes is not None:
                conditions.extend(('lower < ?', ""(upper = '' OR upper >= ?)""))
                params.extend((includes, includes))
            if conditions:
                condition = ' WHERE ' + ' AND '.join(conditions)
            columns = SHARD_RANGE_KEYS[:-2]
            for column in SHARD_RANGE_KEYS[-2:]:
                if column in defaults:
                    columns += (('%s as %s' %
                                 (default_values[column], column)),)
                else:
                    columns += (column,)
            sql = '''
            SELECT %s
            FROM %s%s;
            ''' % (', '.join(columns), SHARD_RANGE_TABLE, condition)
            data = conn.execute(sql, params)
            data.row_factory = None
            return [row for row in data]

        with self.maybe_get(connection) as conn:
            defaults = set()
            attempts = len(default_values) + 1
            while attempts:
                attempts -= 1
                try:
                    return do_query(conn, defaults)
                except sqlite3.OperationalError as err:
                    if ('no such table: %s' % SHARD_RANGE_TABLE) in str(err):
                        return []
                    if not attempts:
                        raise
                    new_defaults = set()
                    for column in default_values.keys():
                        if 'no such column: %s' % column in str(err):
                            new_defaults.add(column)
                    if not new_defaults:
                        raise
                    if new_defaults.intersection(defaults):
                        raise
                    defaults.update(new_defaults)","new_defaults = set()
for column in default_values.keys():
    if 'no such column: %s' % column in str(err):
        new_defaults.add(column)",new_defaults = {column for column in default_values.keys() if 'no such column: %s' % column in str(err)}
cmakeconverter,https://github.com/pavelliavonau/cmakeconverter/tree/master/cmake_converter/visual_studio/solution.py,VSSolutionConverter,__get_global_configuration_types$412,"def __get_global_configuration_types(solution_data):
        configuration_types_set = set()
        for config in solution_data['sln_configurations']:
            configuration_types_set.add(config.split('|')[0])
        configuration_types_list = list(configuration_types_set)
        configuration_types_list.sort(key=str.lower)
        return configuration_types_list","configuration_types_set = set()
for config in solution_data['sln_configurations']:
    configuration_types_set.add(config.split('|')[0])",configuration_types_set = {config.split('|')[0] for config in solution_data['sln_configurations']}
clevr-dataset-gen,https://github.com/facebookresearch/clevr-dataset-gen/tree/master/question_generation/question_engine.py,,vg_relate_handler$51,"def vg_relate_handler(scene_struct, inputs, side_inputs):
  assert len(inputs) == 1
  assert len(side_inputs) == 1
  output = set()
  for rel in scene_struct['relationships']:
    if rel['predicate'] == side_inputs[0] and rel['subject_idx'] == inputs[0]:
      output.add(rel['object_idx'])
  return sorted(list(output))","output = set()
for rel in scene_struct['relationships']:
    if rel['predicate'] == side_inputs[0] and rel['subject_idx'] == inputs[0]:
        output.add(rel['object_idx'])",output = {rel['object_idx'] for rel in scene_struct['relationships'] if rel['predicate'] == side_inputs[0] and rel['subject_idx'] == inputs[0]}
DeTTECT,https://github.com/rabobank-cdc/DeTTECT/tree/master//eql_yaml.py,,_get_applicable_to_yaml_values$409,"def _get_applicable_to_yaml_values(filename, type):
    """"""
    Get all the applicable to values, in lower case, from the provided YAML file.
    :param filename: file path of the YAML file
    :param type: type of YAML object to get the applicable to values from
    :retturn: set with all applicable to values in lower case
    """"""
    app_to_values = set()

    if type == FILE_TYPE_DATA_SOURCE_ADMINISTRATION:
        _, _, systems, _, _ = load_data_sources(filename)

        for system in systems:
            app_to_values.add(system['applicable_to'].lower())

    return app_to_values","app_to_values = set()
for system in systems:
    app_to_values.add(system['applicable_to'].lower())",app_to_values = {system['applicable_to'].lower() for system in systems}
django-simple-history,https://github.com/jazzband/django-simple-history/tree/master/simple_history/management/commands/clean_old_history.py,Command,handle$37,"def handle(self, *args, **options):
        self.verbosity = options[""verbosity""]

        to_process = set()
        model_strings = options.get(""models"", []) or args

        if model_strings:
            for model_pair in self._handle_model_list(*model_strings):
                to_process.add(model_pair)

        elif options[""auto""]:
            to_process = self._auto_models()

        else:
            self.log(self.COMMAND_HINT)

        self._process(to_process, days_back=options[""days""], dry_run=options[""dry""])","to_process = set()
for model_pair in self._handle_model_list(*model_strings):
    to_process.add(model_pair)",to_process = {model_pair for model_pair in self._handle_model_list(*model_strings)}
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/pulse/macros.py,,measure$22,"def measure(
    qubits: List[int],
    backend=None,
    inst_map: Optional[InstructionScheduleMap] = None,
    meas_map: Optional[Union[List[List[int]], Dict[int, List[int]]]] = None,
    qubit_mem_slots: Optional[Dict[int, int]] = None,
    measure_name: str = ""measure"",
) -> Schedule:
    """"""Return a schedule which measures the requested qubits according to the given
    instruction mapping and measure map, or by using the defaults provided by the backend.

    By default, the measurement results for each qubit are trivially mapped to the qubit
    index. This behavior is overridden by qubit_mem_slots. For instance, to measure
    qubit 0 into MemorySlot(1), qubit_mem_slots can be provided as {0: 1}.

    Args:
        qubits: List of qubits to be measured.
        backend (Union[Backend, BaseBackend]): A backend instance, which contains
            hardware-specific data required for scheduling.
        inst_map: Mapping of circuit operations to pulse schedules. If None, defaults to the
                  ``instruction_schedule_map`` of ``backend``.
        meas_map: List of sets of qubits that must be measured together. If None, defaults to
                  the ``meas_map`` of ``backend``.
        qubit_mem_slots: Mapping of measured qubit index to classical bit index.
        measure_name: Name of the measurement schedule.

    Returns:
        A measurement schedule corresponding to the inputs provided.

    Raises:
        PulseError: If both ``inst_map`` or ``meas_map``, and ``backend`` is None.
    """"""
    schedule = Schedule(name=f""Default measurement schedule for qubits {qubits}"")
    try:
        inst_map = inst_map or backend.defaults().instruction_schedule_map
        meas_map = meas_map or backend.configuration().meas_map
    except AttributeError as ex:
        raise exceptions.PulseError(
            ""inst_map or meas_map, and backend cannot be None simultaneously""
        ) from ex
    if isinstance(meas_map, list):
        meas_map = utils.format_meas_map(meas_map)

    measure_groups = set()
    for qubit in qubits:
        measure_groups.add(tuple(meas_map[qubit]))
    for measure_group_qubits in measure_groups:
        if qubit_mem_slots is not None:
            unused_mem_slots = set(measure_group_qubits) - set(qubit_mem_slots.values())
        try:
            default_sched = inst_map.get(measure_name, measure_group_qubits)
        except exceptions.PulseError as ex:
            raise exceptions.PulseError(
                ""We could not find a default measurement schedule called '{}'. ""
                ""Please provide another name using the 'measure_name' keyword ""
                ""argument. For assistance, the instructions which are defined are: ""
                ""{}"".format(measure_name, inst_map.instructions)
            ) from ex
        for time, inst in default_sched.instructions:
            if inst.channel.index not in qubits:
                continue
            if qubit_mem_slots and isinstance(inst, instructions.Acquire):
                if inst.channel.index in qubit_mem_slots:
                    mem_slot = channels.MemorySlot(qubit_mem_slots[inst.channel.index])
                else:
                    mem_slot = channels.MemorySlot(unused_mem_slots.pop())
                inst = instructions.Acquire(inst.duration, inst.channel, mem_slot=mem_slot)
            # Measurement pulses should only be added if its qubit was measured by the user
            schedule = schedule.insert(time, inst)

    return schedule","measure_groups = set()
for qubit in qubits:
    measure_groups.add(tuple(meas_map[qubit]))",measure_groups = {tuple(meas_map[qubit]) for qubit in qubits}
ccks_baidu_entity_link,https://github.com/panchunguang/ccks_baidu_entity_link/tree/master/code/evaluate.py,,link_eval$121,"def link_eval(y_ture,y_pred):
    true_num = 1e-10
    pred_num = 1e-10
    equal_num = 1e-10

    y_ture_file = open(y_ture,'r')
    y_pred_file = open(y_pred, 'r')
    for line_true,line_pred in zip(y_ture_file,y_pred_file):
        line_true = json.loads(line_true)
        line_pred = json.loads(line_pred)

        mention_data_true = line_true['mention_data']
        mention_data_pred = line_pred['mention_data']
        true_set = set()
        pred_set = set()

        for mention in mention_data_true:
            if mention['kb_id'] != 'NIL':
                true_set.add((mention['kb_id'],mention['mention'],mention['offset']))
        for mention in mention_data_pred:
            if mention['kb_id'] != 'NIL':
                pred_set.add((mention['kb_id'],mention['mention'],mention['offset']))

        true_num += len(true_set)
        pred_num += len(pred_set)
        equal_num += len(true_set&pred_set)
    precision = equal_num / pred_num
    recall = equal_num / true_num
    f1 = (2 * precision * recall) / (precision + recall)

    print('equal_num:',equal_num)
    print('true_num:', true_num)
    print('pred_num:', pred_num)
    print('precision:',precision)
    print('recall:', recall)
    print('f1:', f1)
    return precision, recall, f1","true_set = set()
for mention in mention_data_true:
    if mention['kb_id'] != 'NIL':
        true_set.add((mention['kb_id'], mention['mention'], mention['offset']))","true_set = {(mention['kb_id'], mention['mention'], mention['offset']) for mention in mention_data_true if mention['kb_id'] != 'NIL'}"
ccks_baidu_entity_link,https://github.com/panchunguang/ccks_baidu_entity_link/tree/master/code/evaluate.py,,link_eval$121,"def link_eval(y_ture,y_pred):
    true_num = 1e-10
    pred_num = 1e-10
    equal_num = 1e-10

    y_ture_file = open(y_ture,'r')
    y_pred_file = open(y_pred, 'r')
    for line_true,line_pred in zip(y_ture_file,y_pred_file):
        line_true = json.loads(line_true)
        line_pred = json.loads(line_pred)

        mention_data_true = line_true['mention_data']
        mention_data_pred = line_pred['mention_data']
        true_set = set()
        pred_set = set()

        for mention in mention_data_true:
            if mention['kb_id'] != 'NIL':
                true_set.add((mention['kb_id'],mention['mention'],mention['offset']))
        for mention in mention_data_pred:
            if mention['kb_id'] != 'NIL':
                pred_set.add((mention['kb_id'],mention['mention'],mention['offset']))

        true_num += len(true_set)
        pred_num += len(pred_set)
        equal_num += len(true_set&pred_set)
    precision = equal_num / pred_num
    recall = equal_num / true_num
    f1 = (2 * precision * recall) / (precision + recall)

    print('equal_num:',equal_num)
    print('true_num:', true_num)
    print('pred_num:', pred_num)
    print('precision:',precision)
    print('recall:', recall)
    print('f1:', f1)
    return precision, recall, f1","pred_set = set()
for mention in mention_data_pred:
    if mention['kb_id'] != 'NIL':
        pred_set.add((mention['kb_id'], mention['mention'], mention['offset']))","pred_set = {(mention['kb_id'], mention['mention'], mention['offset']) for mention in mention_data_pred if mention['kb_id'] != 'NIL'}"
GeneticAlgorithmsWithPython,https://github.com/handcraftsman/GeneticAlgorithmsWithPython/tree/master/ch18/ticTacToeTests.py,CenterFilter,get_matches$661,"def get_matches(board, squares):
        result = set()
        for square in squares:
            if square.IsCenter:
                result.add(square.Index)
        return result","result = set()
for square in squares:
    if square.IsCenter:
        result.add(square.Index)",result = {square.Index for square in squares if square.IsCenter}
auto-editor,https://github.com/WyattBlue/auto-editor/tree/master/auto_editor/vanparse.py,ParseOptions,__init__$223,"def __init__(self, sys_args, log, root, *args):
        # Set the default options.
        option_names = []
        for options in args:
            for option in options:
                option_names.append(option['names'][0])
                key = _to_key(option)
                if(option['action'] == 'store_true'):
                    value = False
                elif(option['action'] == 'store_false'):
                    value = True
                elif(option['nargs'] != 1):
                    value = []
                else:
                    value = option['default']
                setattr(self, key, value)

        dirpath = os.path.dirname(os.path.realpath(__file__))
        self.set_config(os.path.join(dirpath, 'config.txt'), root)

        # Figure out command line options changed by user.
        my_list = []
        used_options = []
        _set = []
        setting_inputs = True
        option_list = 'input'
        list_type = str
        i = 0
        group = None
        while i < len(sys_args):
            item = sys_args[i]
            label = 'option' if item.startswith('--') else 'short'

            option = get_option(item, the_args=args)

            def error_message(args, item, label):

                def all_names(args):
                    name_set = set()
                    for options in args:
                        for opt in options:
                            for names in opt['names']:
                                name_set.add(names)
                    return name_set

                opt_list = all_names(args)
                close_matches = difflib.get_close_matches(item, opt_list)
                if(close_matches):
                    return 'Unknown {}: {}\n\n    Did you mean:\n        '.format(
                        label, item) + ', '.join(close_matches)
                return 'Unknown {}: {}'.format(label, item)

            if(option is None):
                # Unknown Option!
                if(setting_inputs and (option_list != 'input' or (option_list == 'input' and not item.startswith('-')))):
                    # Option is actually an input file, like example.mp4

                    if(option_list != 'input'):
                        _op = used_options[-1]
                        if(_op['keywords'] != []):
                            item = self.parse_parameters(item, _op)
                    my_list.append(item)
                else:
                    log.error(error_message(args, item, label))
            else:
                # We found the option.
                if(option_list is not None):
                    setattr(self, option_list, list(map(list_type, my_list)))

                setting_inputs = False
                option_list = None
                my_list = []

                if(option in used_options):
                    log.error('Cannot repeat option {} twice.'.format(option['names'][0]))

                used_options.append(option)

                key = _to_key(option)
                _set.append(key)

                if(option['action'] == 'grouping'):
                    group = key

                nextItem = None if i == len(sys_args) - 1 else sys_args[i+1]
                if(nextItem == '-h' or nextItem == '--help'):
                    print_option_help(args, option)
                    sys.exit()

                if(option['nargs'] != 1):
                    setting_inputs = True
                    option_list = key
                    list_type = option['type']
                elif(option['action'] == 'store_true'):
                    value = True
                elif(option['action'] == 'store_false'):
                    value = False
                else:
                    value = option['type'](nextItem)

                    if(option['choices'] is not None and value not in option['choices']):
                        option_name = option['names'][0]
                        my_choices = ', '.join(option['choices'])
                        log.error('{} is not a choice for {}\nchoices are:\n  {}'.format(
                            value, option_name, my_choices))
                    i += 1
                setattr(self, key, value)

            i += 1
        if(setting_inputs):
            setattr(self, option_list, list(map(list_type, my_list)))
        setattr(self, '_set', _set)
        if(self.help):
            print_program_help(root, args)
            sys.exit()","name_set = set()
for options in args:
    for opt in options:
        for names in opt['names']:
            name_set.add(names)",name_set = {names for options in args for opt in options for names in opt['names']}
salt,https://github.com/saltstack/salt/tree/master/salt/modules/saltutil.py,,list_extmods$852,"def list_extmods():
    """"""
    .. versionadded:: 2017.7.0

    List Salt modules which have been synced externally

    CLI Examples:

    .. code-block:: bash

        salt '*' saltutil.list_extmods
    """"""
    ret = {}
    ext_dir = os.path.join(__opts__[""cachedir""], ""extmods"")
    mod_types = os.listdir(ext_dir)
    for mod_type in mod_types:
        ret[mod_type] = set()
        for _, _, files in salt.utils.path.os_walk(os.path.join(ext_dir, mod_type)):
            for fh_ in files:
                ret[mod_type].add(fh_.split(""."")[0])
        ret[mod_type] = list(ret[mod_type])
    return ret","ret[mod_type] = set()
for (_, _, files) in salt.utils.path.os_walk(os.path.join(ext_dir, mod_type)):
    for fh_ in files:
        ret[mod_type].add(fh_.split('.')[0])","ret[mod_type] = {fh_.split('.')[0] for (_, _, files) in salt.utils.path.os_walk(os.path.join(ext_dir, mod_type)) for fh_ in files}"
nibabel,https://github.com/nipy/nibabel/tree/master//versioneer.py,,do_setup$1758,"def do_setup():
    """"""Do main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError,
            configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"",
                  file=sys.stderr)
            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with open(cfg.versionfile_source, ""w"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {""DOLLAR"": ""$"",
                        ""STYLE"": cfg.style,
                        ""TAG_PREFIX"": cfg.tag_prefix,
                        ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
                        })

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),
                       ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy, ""r"") as f:
                old = f.read()
        except EnvironmentError:
            old = """"
        if INIT_PY_SNIPPET not in old:
            print("" appending to %s"" % ipy)
            with open(ipy, ""a"") as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with open(manifest_in, ""r"") as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, ""a"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" %
              cfg.versionfile_source)
        with open(manifest_in, ""a"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-subst keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","simple_includes = set()
for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}
scipy,https://github.com/scipy/scipy/tree/master/scipy/stats/tests/test_axis_nan_policy.py,,_check_arrays_broadcastable$448,"def _check_arrays_broadcastable(arrays, axis):
    # https://numpy.org/doc/stable/user/basics.broadcasting.html
    # ""When operating on two arrays, NumPy compares their shapes element-wise.
    # It starts with the trailing (i.e. rightmost) dimensions and works its
    # way left.
    # Two dimensions are compatible when
    # 1. they are equal, or
    # 2. one of them is 1
    # ...
    # Arrays do not need to have the same number of dimensions.""
    # (Clarification: if the arrays are compatible according to the criteria
    #  above and an array runs out of dimensions, it is still compatible.)
    # Below, we follow the rules above except ignoring `axis`

    n_dims = max([arr.ndim for arr in arrays])
    if axis is not None:
        # convert to negative axis
        axis = (-n_dims + axis) if axis >= 0 else axis

    for dim in range(1, n_dims+1):  # we'll index from -1 to -n_dims, inclusive
        if -dim == axis:
            continue  # ignore lengths along `axis`

        dim_lengths = set()
        for arr in arrays:
            if dim <= arr.ndim and arr.shape[-dim] != 1:
                dim_lengths.add(arr.shape[-dim])

        if len(dim_lengths) > 1:
            return False
    return True","dim_lengths = set()
for arr in arrays:
    if dim <= arr.ndim and arr.shape[-dim] != 1:
        dim_lengths.add(arr.shape[-dim])",dim_lengths = {arr.shape[-dim] for arr in arrays if dim <= arr.ndim and arr.shape[-dim] != 1}
torchgeo,https://github.com/microsoft/torchgeo/tree/master/torchgeo/datasets/ucmerced.py,UCMerced,__init__$105,"def __init__(
        self,
        root: str = ""data"",
        split: str = ""train"",
        transforms: Optional[Callable[[Dict[str, Tensor]], Dict[str, Tensor]]] = None,
        download: bool = False,
        checksum: bool = False,
    ) -> None:
        """"""Initialize a new UC Merced dataset instance.

        Args:
            root: root directory where dataset can be found
            split: one of ""train"", ""val"", or ""test""
            transforms: a function/transform that takes input sample and its target as
                entry and returns a transformed version
            download: if True, download dataset and store it in the root directory
            checksum: if True, check the MD5 of the downloaded files (may be slow)

        Raises:
            RuntimeError: if ``download=False`` and data is not found, or checksums
                don't match
        """"""
        assert split in self.splits
        self.root = root
        self.transforms = transforms
        self.download = download
        self.checksum = checksum
        self._verify()

        valid_fns = set()
        with open(os.path.join(self.root, f""uc_merced-{split}.txt"")) as f:
            for fn in f:
                valid_fns.add(fn.strip())
        is_in_split: Callable[[str], bool] = lambda x: os.path.basename(x) in valid_fns

        super().__init__(
            root=os.path.join(root, self.base_dir),
            transforms=transforms,
            is_valid_file=is_in_split,
        )","valid_fns = set()
for fn in f:
    valid_fns.add(fn.strip())",valid_fns = {fn.strip() for fn in f}
coding-interview-gym,https://github.com/partho-maple/coding-interview-gym/tree/master/leetcode.com/python/939_Minimum_Area_Rectangle.py,Solution,minAreaRect$31,"def minAreaRect(self, points):
        """"""
        :type points: List[List[int]]
        :rtype: int
        """"""
        minArea = float(""inf"")
        pointsSet = set()
        for x, y in points:
            pointsSet.add((x, y))
        for x1, y1 in points:
            for x2, y2 in points:
                if (x1, y1) != (x2, y2) and (x1 > x2 and y1 > y2): # To check if the points are not same and if the points are diagonal. Because if the points are diagonal then we will be  able to get both height and width from only this 2 points
                    if (x1, y2) in pointsSet and (x2, y1) in pointsSet: # Check if other 2 points also exists, to form a rectangle
                        area = abs(x1 - x2) * abs(y1 - y2)
                        minArea = min(minArea, area)
        return 0 if minArea == float(""inf"") else minArea","pointsSet = set()
for (x, y) in points:
    pointsSet.add((x, y))","pointsSet = {(x, y) for (x, y) in points}"
python-string-similarity,https://github.com/luozhouyang/python-string-similarity/tree/master/strsimpy/qgram.py,QGram,distance_profile$43,"def distance_profile(profile0, profile1):
        union = set()
        for k in profile0.keys():
            union.add(k)
        for k in profile1.keys():
            union.add(k)
        agg = 0
        for k in union:
            v0, v1 = 0, 0
            if profile0.get(k) is not None:
                v0 = int(profile0.get(k))
            if profile1.get(k) is not None:
                v1 = int(profile1.get(k))
            agg += abs(v0 - v1)
        return agg","union = set()
for k in profile0.keys():
    union.add(k)",union = {k for k in profile0.keys()}
RedditDownloader,https://github.com/shadowmoose/RedditDownloader/tree/master/redditdownloader/__main__.py,,run$44,"def run():
	logging.basicConfig(level=logging.WARN, format='%(levelname)-5.5s [%(name)s] %(message)s', datefmt='%H:%M:%S')
	su.print_color('green', ""\r\n"" +
		'====================================\r\n' +
		('   Reddit Media Downloader %s\r\n' % meta.current_version) +
		'====================================\r\n' +
		'    (By ShadowMoose @ Github)\r\n')
	if args.version:
		sys.exit(0)

	if args.run_tests:
		error_count = tests.runner.run_tests(test_subdir=args.run_tests)
		sys.exit(error_count)

	if args.list_settings:
		print('All valid overridable settings:')
		for _s in settings.get_all():
			if _s.public:
				print(""%s.%s"" % (_s.category, _s.name))
				print('\tDescription: %s' % _s.description)
				if not _s.opts:
					print('\tValid value: \n\t\tAny %s' % _s.type)
				else:
					print('\tValid values:')
					for o in _s.opts:
						print('\t\t""%s"": %s' % o)
				print()
		sys.exit()

	settings_file = args.settings or fs.find_file('settings.json')
	_loaded = settings.load(settings_file)
	for ua in unknown_args:
		if '=' not in ua or '/comments/' in ua:
			if '/comments/' in ua:
				direct_sources.append(DirectURLSource(url=ua))
				continue
			elif 'r/' or 'u/' in ua:
				direct_sources.append(DirectInputSource(txt=ua, args={'limit': args.limit}))
				continue
			else:
				su.error(""ERROR: Unkown argument: %s"" % ua)
				sys.exit(1)
		k = ua.split('=')[0].strip('- ')
		v = ua.split('=', 2)[1].strip()
		try:
			settings.put(k, v, save_after=False)
		except KeyError:
			print('Unknown setting: %s' % k)
			sys.exit(50)

	if args.source:
		matched_sources = set()
		for s in args.source:
			for stt in settings.get_sources():
				if re.match(s, stt.get_alias()):
					matched_sources.add(stt)
		direct_sources.extend(matched_sources)

	if args.import_csv:
		direct_sources.append(DirectFileSource(file=args.import_csv, slow_fallback=args.full_csv))

	first_time_auth = False

	if not _loaded and not direct_sources and not args.docker:
		# First-time configuration.
		su.error('Could not find an existing settings file. A new one will be generated!')
		if not console.confirm('Would you like to start the WebUI to help set things up?', True):
			su.print_color('red', ""If you don't open the webUI now, you'll need to edit the settings file yourself."")
			if console.confirm(""Are you sure you'd like to edit settings without the UI (if 'yes', these prompts will not show again)?""):
				settings.put('interface.start_server', False, save_after=True)  # Creates a save.
				print('A settings file has been created for you, at ""%s"". Please customize it.' % settings_file)
				first_time_auth = True
			else:
				print('Please re-run RMD to configure again.')
				sys.exit(1)
		else:
			mode = console.prompt_list('How would you like to open the UI?',
									   settings.get('interface.browser', full_obj=True).opts)
			settings.put('interface.browser', mode, save_after=False)
			settings.put('interface.start_server', True)

	if args.docker:
		print('Running in ""Docker"" mode. Assuming some default settings.')
		settings.put('interface.host', '0.0.0.0', save_after=False)
		settings.put('interface.browser', 'off', save_after=False)
		settings.put('interface.keep_open', True, save_after=False)
		settings.put('interface.start_server', True)

	if args.authorize or first_time_auth:  # In-console oAuth authentication flow
		from static import praw_wrapper
		from urllib.parse import urlparse, parse_qs
		url = praw_wrapper.get_reddit_token_url()
		su.print_color('green', '\nTo manually authorize your account, visit the below URL.')
		su.print_color('yellow', 'Once there, authorize RMD, then copy the URL it redirects you to.')
		su.print_color('yellow', 'NOTE: The redirect page will likely not load, and that is ok.')
		su.print_color('cyan', '\n%s\n' % url)
		token_url = console.col_input('Paste the URL you are redirected to here: ')
		if token_url.strip():
			qs = parse_qs(urlparse(token_url).query)
			if 'state' not in qs or 'code' not in qs:
				su.error('The url provided was not a valid reddit redirect. Please make sure you copied it right!')
			elif qs['state'][0].strip() != settings.get('auth.oauth_key').strip():
				su.error('Invalid reddit redirect state. Please restart and try again.')
			else:
				code = qs['code'][0]
				su.print_color('green', 'Got code. Authorizing account...')
				refresh = praw_wrapper.get_refresh_token(code)
				if refresh:
					settings.put('auth.refresh_token', refresh)
					usr = praw_wrapper.get_current_username()
					su.print_color('cyan', 'Authorized to view account: %s' % usr)
					su.print_color('green', 'Saved authorization token! Please restart RMD to begin downloading!')
				else:
					su.error('Failed to gain an account access token from Reddit with that code. Please try again.')
		sys.exit(0)

	if not ffmpeg_download.install_local():
		print(""RMD was unable to locate (or download) a working FFmpeg binary."")
		print(""For downloading and post-processing, this is a required tool."")
		print(""Please Install FFmpeg manually, or download it from here: https://rmd.page.link/ffmpeg"")
		sys.exit(15)

	# Initialize Database
	sql.init_from_settings()
	print('Using manifest file [%s].' % sql.get_file_location())

	if direct_sources:
		settings.disable_saving()
		settings.put('processing.retry_failed', False)
		for s in settings.get_sources():
			settings.remove_source(s, save_after=False)
		for d in direct_sources:
			settings.add_source(d, prevent_duplicate=False, save_after=False)

	if settings.get('interface.start_server') and not direct_sources:
		print(""Starting WebUI..."")
		ui = WebUI()
	else:
		ui = TerminalUI()
	ui.display()","matched_sources = set()
for s in args.source:
    for stt in settings.get_sources():
        if re.match(s, stt.get_alias()):
            matched_sources.add(stt)","matched_sources = {stt for s in args.source for stt in settings.get_sources() if re.match(s, stt.get_alias())}"
MozDef,https://github.com/mozilla/MozDef/tree/master/alerts/auth0_bruteforce_user.py,AlertAuth0BruteforceUser,onAggregation$29,"def onAggregation(self, aggreg):
        category = 'bruteforce'
        tags = ['auth0']
        severity = self.config.severity
        ip_list = set()

        for event in aggreg['allevents']:
            ip_list.add(event['_source']['details']['sourceipaddress'])

        summary = 'Auth0 Username/Password Bruteforce Attack in Progress against user ({0}) from the following source ip(s): {1}'.format(
            aggreg['value'], "", "".join(sorted(ip_list)[:10]))

        if len(ip_list) >= 10:
            summary += '...'

        return self.createAlertDict(
            summary, category, tags, aggreg['events'], severity
        )","ip_list = set()
for event in aggreg['allevents']:
    ip_list.add(event['_source']['details']['sourceipaddress'])",ip_list = {event['_source']['details']['sourceipaddress'] for event in aggreg['allevents']}
checkmk,https://github.com/tribe29/checkmk/tree/master/cmk/special_agents/agent_elasticsearch.py,,handle_stats$117,"def handle_stats(response):
    shards = response.get(""_shards"")
    if shards is not None:
        sys.stdout.write(""<<<elasticsearch_shards>>>\n"")

        sys.stdout.write(
            ""%s %s %s\n"" % (shards.get(""total""), shards.get(""successful""), shards.get(""failed""))
        )

    docs = response.get(""_all"", {}).get(""total"")
    if docs is not None:
        sys.stdout.write(""<<<elasticsearch_cluster>>>\n"")
        count = docs.get(""docs"", {}).get(""count"")
        size = docs.get(""store"", {}).get(""size_in_bytes"")

        sys.stdout.write(""%s %s\n"" % (count, size))

    indices_data = response.get(""indices"")
    if indices_data is not None:
        indices = set()

        sys.stdout.write(""<<<elasticsearch_indices>>>\n"")
        for index in indices_data:
            indices.add(index.split(""-"")[0])
        for indice in list(indices):
            all_counts = []
            all_sizes = []
            for index in indices_data:
                if index.split(""-"")[0] == indice:
                    all_counts.append(
                        indices_data.get(index, {})
                        .get(""primaries"", {})
                        .get(""docs"", {})
                        .get(""count"")
                    )
                    all_sizes.append(
                        indices_data.get(index, {})
                        .get(""total"", {})
                        .get(""store"", {})
                        .get(""size_in_bytes"")
                    )
            sys.stdout.write(
                ""%s %s %s\n""
                % (indice, sum(all_counts) / len(all_counts), sum(all_sizes) / len(all_sizes))
            )","indices = set()
for index in indices_data:
    indices.add(index.split('-')[0])",indices = {index.split('-')[0] for index in indices_data}
TextAttack,https://github.com/QData/TextAttack/tree/master/textattack/transformations/word_insertions/word_insertion_random_synonym.py,WordInsertionRandomSynonym,_get_synonyms$19,"def _get_synonyms(self, word):
        synonyms = set()
        for syn in wordnet.synsets(word):
            for lemma in syn.lemmas():
                if lemma.name() != word and check_if_one_word(lemma.name()):
                    synonyms.add(lemma.name())
        return list(synonyms)","synonyms = set()
for syn in wordnet.synsets(word):
    for lemma in syn.lemmas():
        if lemma.name() != word and check_if_one_word(lemma.name()):
            synonyms.add(lemma.name())",synonyms = {lemma.name() for syn in wordnet.synsets(word) for lemma in syn.lemmas() if lemma.name() != word and check_if_one_word(lemma.name())}
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_org/c7n_org/cli.py,,filter_accounts$250,"def filter_accounts(accounts_config, tags, accounts, not_accounts=None):
    filtered_accounts = []
    accounts = comma_expand(accounts)
    not_accounts = comma_expand(not_accounts)
    for a in accounts_config.get('accounts', ()):
        if not_accounts and a['name'] in not_accounts:
            continue
        account_id = a.get('account_id') or a.get('project_id') or a.get('subscription_id') or ''
        if accounts and a['name'] not in accounts and account_id not in accounts:
            continue
        if tags:
            found = set()
            for t in tags:
                if t in a.get('tags', ()):
                    found.add(t)
            if not found == set(tags):
                continue
        filtered_accounts.append(a)
    accounts_config['accounts'] = filtered_accounts","found = set()
for t in tags:
    if t in a.get('tags', ()):
        found.add(t)","found = {t for t in tags if t in a.get('tags', ())}"
PythonProgrammingPuzzles,https://github.com/microsoft/PythonProgrammingPuzzles/tree/master/generators/human_eval.py,PrimeSel,sat$5005,"def sat(neighbors: List[int], nums=[14, 7, 11, 13, 7, 4, 19, 2, 55, 13, 31, 14, 2, 9, -7, 0, 88, 13, 13]):
        """"""Find a list of all numbers that are adjacent to a prime number in the list, sorted without duplicates

        [2, 17, 16, 0, 6, 4, 5] => [2, 4, 16, 17]""""""

        def prime(m):
            return all(m % i for i in range(2, m - 1))

        goods = set()
        for i, n in enumerate(nums):
            if (i > 0 and prime(nums[i - 1])) or (i < len(nums) - 1 and prime(nums[i + 1])):
                goods.add(n)

        return set(neighbors) == goods and all(n == min(neighbors[i:]) for i, n in enumerate(neighbors))","goods = set()
for (i, n) in enumerate(nums):
    if i > 0 and prime(nums[i - 1]) or (i < len(nums) - 1 and prime(nums[i + 1])):
        goods.add(n)","goods = {n for (i, n) in enumerate(nums) if i > 0 and prime(nums[i - 1]) or (i < len(nums) - 1 and prime(nums[i + 1]))}"
DeepCTR,https://github.com/shenweichen/DeepCTR/tree/master/deepctr/layers/interaction.py,AFMLayer,build$58,"def build(self, input_shape):

        if not isinstance(input_shape, list) or len(input_shape) < 2:
            # input_shape = input_shape[0]
            # if not isinstance(input_shape, list) or len(input_shape) < 2:
            raise ValueError('A `AttentionalFM` layer should be called '
                             'on a list of at least 2 inputs')

        shape_set = set()
        reduced_input_shape = [shape.as_list() for shape in input_shape]
        for i in range(len(input_shape)):
            shape_set.add(tuple(reduced_input_shape[i]))

        if len(shape_set) > 1:
            raise ValueError('A `AttentionalFM` layer requires '
                             'inputs with same shapes '
                             'Got different shapes: %s' % (shape_set))

        if len(input_shape[0]) != 3 or input_shape[0][1] != 1:
            raise ValueError('A `AttentionalFM` layer requires '
                             'inputs of a list with same shape tensor like\
                             (None, 1, embedding_size)'
                             'Got different shapes: %s' % (input_shape[0]))

        embedding_size = int(input_shape[0][-1])

        self.attention_W = self.add_weight(shape=(embedding_size,
                                                  self.attention_factor), initializer=glorot_normal(seed=self.seed),
                                           regularizer=l2(self.l2_reg_w), name=""attention_W"")
        self.attention_b = self.add_weight(
            shape=(self.attention_factor,), initializer=Zeros(), name=""attention_b"")
        self.projection_h = self.add_weight(shape=(self.attention_factor, 1),
                                            initializer=glorot_normal(seed=self.seed), name=""projection_h"")
        self.projection_p = self.add_weight(shape=(
            embedding_size, 1), initializer=glorot_normal(seed=self.seed), name=""projection_p"")
        self.dropout = tf.keras.layers.Dropout(
            self.dropout_rate, seed=self.seed)

        self.tensordot = tf.keras.layers.Lambda(
            lambda x: tf.tensordot(x[0], x[1], axes=(-1, 0)))

        # Be sure to call this somewhere!
        super(AFMLayer, self).build(input_shape)","shape_set = set()
for i in range(len(input_shape)):
    shape_set.add(tuple(reduced_input_shape[i]))",shape_set = {tuple(reduced_input_shape[i]) for i in range(len(input_shape))}
GeneticAlgorithmsWithPython,https://github.com/handcraftsman/GeneticAlgorithmsWithPython/tree/master/ch18/ticTacToeTests.py,SideFilter,get_matches$687,"def get_matches(board, squares):
        result = set()
        for square in squares:
            if square.IsSide:
                result.add(square.Index)
        return result","result = set()
for square in squares:
    if square.IsSide:
        result.add(square.Index)",result = {square.Index for square in squares if square.IsSide}
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/distributed/fleet/meta_optimizers/sharding_optimizer.py,ShardingOptimizer,_initialization_broadcast$1817,"def _initialization_broadcast(self):
        """"""
        this funtion is to ensure the initialization between dp group to be
        identical when hybrid-dp is used, and the initialization of
        not distributed param between mp group to be identical.
        """"""
        if self.dp_degree <= 1 and self.mp_degree <= 1:
            return

        startup_block = self._startup_program.global_block()

        params = startup_block.all_parameters()
        params_name = []
        not_dist_param_name = set()

        for param in params:
            params_name.append(param.name)
            if not hasattr(param, 'is_distributed') or not param.is_distributed:
                not_dist_param_name.add(param.name)

        # offload and optimize_cast will insert broadcast op
        broadcast_params = set()
        for op in startup_block.ops:
            if op.type == 'c_broadcast':
                broadcast_params.add(op.desc.output_arg_names()[0])

        for param in params_name:
            if param in broadcast_params:
                continue

            rings = []
            # need sync not distributed param in mp group
            if self.mp_degree > 1 and param in not_dist_param_name:
                rings.append(self.mp_ring_id)
            if self.dp_degree > 1:
                rings.append(self.dp_ring_id)

            for ring in rings:
                startup_block.append_op(
                    type='c_broadcast',
                    inputs={'X': param},
                    outputs={'Out': param},
                    attrs={
                        'ring_id': ring,
                        'root': 0,
                        'use_calc_stream': True,
                        OP_ROLE_KEY: OpRole.Forward,
                    },
                )

        startup_block._sync_with_cpp()","broadcast_params = set()
for op in startup_block.ops:
    if op.type == 'c_broadcast':
        broadcast_params.add(op.desc.output_arg_names()[0])",broadcast_params = {op.desc.output_arg_names()[0] for op in startup_block.ops if op.type == 'c_broadcast'}
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/world/traderoute.py,TradeRoute,can_enable$256,"def can_enable(self):
		warehouses = set()
		for waypoint in self.waypoints:
			warehouses.add(waypoint['warehouse'])
		return len(warehouses) > 1","warehouses = set()
for waypoint in self.waypoints:
    warehouses.add(waypoint['warehouse'])",warehouses = {waypoint['warehouse'] for waypoint in self.waypoints}
river,https://github.com/online-ml/river/tree/master/river/tree/hoeffding_tree_classifier.py,HoeffdingTreeClassifier,_attempt_to_split$219,"def _attempt_to_split(
        self, leaf: HTLeaf, parent: DTBranch, parent_branch: int, **kwargs
    ):
        """"""Attempt to split a leaf.

        If the samples seen so far are not from the same class then:

        1. Find split candidates and select the top 2.
        2. Compute the Hoeffding bound.
        3. If the difference between the top 2 split candidates is larger than the Hoeffding bound:
           3.1 Replace the leaf node by a split node (branch node).
           3.2 Add a new leaf node on each branch of the new split node.
           3.3 Update tree's metrics

        Optional: Disable poor attributes. Depends on the tree's configuration.

        Parameters
        ----------
        leaf
            The leaf to evaluate.
        parent
            The leaf's parent.
        parent_branch
            Parent leaf's branch index.
        kwargs
            Other parameters passed to the new branch.
        """"""
        if not leaf.observed_class_distribution_is_pure():  # noqa
            split_criterion = self._new_split_criterion()

            best_split_suggestions = leaf.best_split_suggestions(split_criterion, self)
            best_split_suggestions.sort()
            should_split = False
            if len(best_split_suggestions) < 2:
                should_split = len(best_split_suggestions) > 0
            else:
                hoeffding_bound = self._hoeffding_bound(
                    split_criterion.range_of_merit(leaf.stats),
                    self.split_confidence,
                    leaf.total_weight,
                )
                best_suggestion = best_split_suggestions[-1]
                second_best_suggestion = best_split_suggestions[-2]
                if (
                    best_suggestion.merit - second_best_suggestion.merit
                    > hoeffding_bound
                    or hoeffding_bound < self.tie_threshold
                ):
                    should_split = True
                if self.remove_poor_attrs:
                    poor_atts = set()
                    # Add any poor attribute to set
                    for suggestion in best_split_suggestions:
                        if (
                            suggestion.feature
                            and best_suggestion.merit - suggestion.merit
                            > hoeffding_bound
                        ):
                            poor_atts.add(suggestion.feature)
                    for poor_att in poor_atts:
                        leaf.disable_attribute(poor_att)
            if should_split:
                split_decision = best_split_suggestions[-1]
                if split_decision.feature is None:
                    # Pre-pruning - null wins
                    leaf.deactivate()
                    self._n_inactive_leaves += 1
                    self._n_active_leaves -= 1
                else:
                    branch = self._branch_selector(
                        split_decision.numerical_feature, split_decision.multiway_split
                    )
                    leaves = tuple(
                        self._new_leaf(initial_stats, parent=leaf)
                        for initial_stats in split_decision.children_stats
                    )

                    new_split = split_decision.assemble(
                        branch, leaf.stats, leaf.depth, *leaves, **kwargs
                    )

                    self._n_active_leaves -= 1
                    self._n_active_leaves += len(leaves)
                    if parent is None:
                        self._root = new_split
                    else:
                        parent.children[parent_branch] = new_split

                # Manage memory
                self._enforce_size_limit()","poor_atts = set()
for suggestion in best_split_suggestions:
    if suggestion.feature and best_suggestion.merit - suggestion.merit > hoeffding_bound:
        poor_atts.add(suggestion.feature)",poor_atts = {suggestion.feature for suggestion in best_split_suggestions if suggestion.feature and best_suggestion.merit - suggestion.merit > hoeffding_bound}
tvm,https://github.com/apache/tvm/tree/master/tests/python/contrib/test_ethosu/test_lower_to_te.py,,test_ethosu_conv2d$27,"def test_ethosu_conv2d():
    ifm = relay.var(""ifm"", shape=(1, 10, 20, 30), dtype=""uint8"")
    weight = relay.var(""weight"", shape=(40, 3, 3, 30), dtype=""uint8"")
    scale_bias = relay.var(""scale_bias"", shape=(40, 10), dtype=""uint8"")
    lut = relay.var(""lut"", shape=(), dtype=""uint8"")
    conv = ethosu_ops.ethosu_conv2d(
        ifm,
        weight,
        scale_bias,
        lut,
        ifm_scale=0.5,
        ifm_zero_point=10,
        weight_zero_point=12,
        ofm_scale=0.25,
        ofm_zero_point=14,
        ofm_channels=40,
        padding=(1, 1, 1, 1),
        kernel_shape=(3, 3),
        strides=(1, 1),
        dilation=(1, 1),
    )
    expr = relay.Function(relay.analysis.free_vars(conv), conv)
    mod = tvm.IRModule.from_expr(expr)
    mod = relay.transform.InferType()(mod)
    lowered = lower_to_te(mod[""main""])
    assert len(lowered.outputs) == 1
    assert len(lowered.inputs) == 4
    conv2d_compute = OperatorCompute.from_output(lowered.outputs[0])
    assert conv2d_compute.op.name == ""ethosu_conv2d""
    input_shapes = set()
    for inp in lowered.inputs:
        input_shapes.add(tuple([x.value for x in inp.shape]))
    assert input_shapes == {(40, 10), (1, 10, 20, 30), (40, 3, 3, 30), ()}","input_shapes = set()
for inp in lowered.inputs:
    input_shapes.add(tuple([x.value for x in inp.shape]))",input_shapes = {tuple([x.value for x in inp.shape]) for inp in lowered.inputs}
lemur,https://github.com/Netflix/lemur/tree/master/lemur/sources/service.py,,add_aws_destination_to_sources$422,"def add_aws_destination_to_sources(dst):
    """"""
    Given a destination, check if it can be added as sources, and include it if not already a source
    We identify qualified destinations based on the sync_as_source attributed of the plugin.
    The destination sync_as_source_name reveals the name of the suitable source-plugin.
    We rely on account numbers to avoid duplicates.
    :return: true for success and false for not adding the destination as source
    """"""
    # check that destination can be synced to a source
    destination_plugin = plugins.get(dst.plugin_name)
    if destination_plugin.sync_as_source is None or not destination_plugin.sync_as_source:
        return False
    account_number = get_plugin_option(""accountNumber"", dst.options)
    if account_number is None:
        return False
    path = get_plugin_option(""path"", dst.options)
    if path is None:
        return False

    # a set of all (account number, path) available as sources
    src_account_paths = set()
    sources = get_all()
    for src in sources:
        src_account_paths.add(
            (get_plugin_option(""accountNumber"", src.options), get_plugin_option(""path"", src.options))
        )

    if (account_number, path) not in src_account_paths:
        src_options = copy.deepcopy(
            plugins.get(destination_plugin.sync_as_source_name).options
        )
        set_plugin_option(""accountNumber"", account_number, src_options)
        set_plugin_option(""path"", path, src_options)
        # Set the right endpointType for cloudfront sources.
        if get_plugin_option(""endpointType"", src_options) is not None and path == ""/cloudfront/"":
            set_plugin_option(""endpointType"", ""cloudfront"", src_options)
        create(
            label=dst.label,
            plugin_name=destination_plugin.sync_as_source_name,
            options=src_options,
            description=dst.description,
        )
        return True

    return False","src_account_paths = set()
for src in sources:
    src_account_paths.add((get_plugin_option('accountNumber', src.options), get_plugin_option('path', src.options)))","src_account_paths = {(get_plugin_option('accountNumber', src.options), get_plugin_option('path', src.options)) for src in sources}"
pgmpy,https://github.com/pgmpy/pgmpy/tree/master/pgmpy/inference/mplp.py,Mplp,__init__$50,"def __init__(self, model):
        if not isinstance(model, MarkovNetwork):
            raise TypeError(""Only MarkovNetwork is supported"")

        super(Mplp, self).__init__(model)
        self._initialize_structures()

        # S = \{c \cap c^{'} : c, c^{'} \in C, c \cap c^{'} \neq \emptyset\}
        self.intersection_set_variables = set()
        # We generate the Intersections of all the pairwise edges taken one at a time to form S
        for edge_pair in it.combinations(model.edges(), 2):
            self.intersection_set_variables.add(
                frozenset(edge_pair[0]) & frozenset(edge_pair[1])
            )

        # The corresponding optimization problem = \min_{\delta}{dual_lp(\delta)} where:
        # dual_lp(\delta) = \sum_{i \in V}{max_{x_i}(Objective[nodes])} + \sum_{f /in F}{max_{x_f}(Objective[factors])
        # Objective[nodes] = \theta_i(x_i) + \sum_{f \mid i \in f}{\delta_{fi}(x_i)}
        # Objective[factors] = \theta_f(x_f) - \sum_{i \in f}{\delta_{fi}(x_i)}
        # In a way Objective stores the corresponding optimization problem for all the nodes and the factors.

        # Form Objective and cluster_set in the form of a dictionary.
        self.objective = {}
        self.cluster_set = {}
        for factor in model.get_factors():
            scope = frozenset(factor.scope())
            self.objective[scope] = factor
            # For every factor consisting of more that a single node, we initialize a cluster.
            if len(scope) > 1:
                self.cluster_set[scope] = self.Cluster(
                    self.intersection_set_variables, factor
                )

        # dual_lp(\delta) is the dual linear program
        self.dual_lp = sum(
            [np.amax(self.objective[obj].values) for obj in self.objective]
        )

        # Best integral value of the primal objective is stored here
        self.best_int_objective = 0

        # Assignment of the nodes that results in the ""maximum"" integral value of the primal objective
        self.best_assignment = {}
        # Results of the ""maximum"" integral value of the primal objective.
        self.best_decoded_result = {}
        # This sets the minimum width between the dual objective decrements. Default value = 0.0002. This can be
        # changed in the map_query() method.
        self.dual_threshold = 0.0002
        # This sets the threshold for the integrality gap below which we say that the solution is satisfactory.
        # Default value = 0.0002. This can be changed in the map_query() method.
        self.integrality_gap_threshold = 0.0002","self.intersection_set_variables = set()
for edge_pair in it.combinations(model.edges(), 2):
    self.intersection_set_variables.add(frozenset(edge_pair[0]) & frozenset(edge_pair[1]))","self.intersection_set_variables = {frozenset(edge_pair[0]) & frozenset(edge_pair[1]) for edge_pair in it.combinations(model.edges(), 2)}"
TensorNetwork,https://github.com/google/TensorNetwork/tree/master/tensornetwork/tests/network_test.py,,test_get_parallel_edge$431,"def test_get_parallel_edge(backend):
  a = tn.Node(np.ones((2,) * 5), backend=backend)
  b = tn.Node(np.ones((2,) * 5), backend=backend)
  edges = set()
  for i in {0, 1, 3}:
    edges.add(tn.connect(a[i], b[i]))
  for e in edges:
    assert set(tn.get_parallel_edges(e)) == edges","edges = set()
for i in {0, 1, 3}:
    edges.add(tn.connect(a[i], b[i]))","edges = {tn.connect(a[i], b[i]) for i in {0, 1, 3}}"
PaddleSpeech,https://github.com/PaddlePaddle/PaddleSpeech/tree/master/paddlespeech/t2s/exps/transformer_tts/preprocess.py,,get_input_token$58,"def get_input_token(sentence, output_path):
    '''get phone set from training data and save it
    
    Args:
        sentence (Dict): sentence: {'utt': ([char], str)}
        output_path (str or path): path to save phone_id_map
    '''
    phn_token = set()
    for utt in sentence:
        for phn in sentence[utt][0]:
            if phn != ""<eos>"":
                phn_token.add(phn)
    phn_token = list(phn_token)
    phn_token.sort()
    phn_token = [""<pad>"", ""<unk>""] + phn_token
    phn_token += [""<eos>""]

    with open(output_path, 'w') as f:
        for i, phn in enumerate(phn_token):
            f.write(phn + ' ' + str(i) + '\n')","phn_token = set()
for utt in sentence:
    for phn in sentence[utt][0]:
        if phn != '<eos>':
            phn_token.add(phn)",phn_token = {phn for utt in sentence for phn in sentence[utt][0] if phn != '<eos>'}
django-guardian,https://github.com/django-guardian/django-guardian/tree/master/guardian/shortcuts.py,,get_objects_for_group$655,"def get_objects_for_group(group, perms, klass=None, any_perm=False, accept_global_perms=True):
    """"""
    Returns queryset of objects for which a given ``group`` has *all*
    permissions present at ``perms``.

    :param group: ``Group`` instance for which objects would be returned.
    :param perms: single permission string, or sequence of permission strings
      which should be checked.
      If ``klass`` parameter is not given, those should be full permission
      names rather than only codenames (i.e. ``auth.change_user``). If more than
      one permission is present within sequence, their content type **must** be
      the same or ``MixedContentTypeError`` exception would be raised.
    :param klass: may be a Model, Manager or QuerySet object. If not given
      this parameter would be computed based on given ``params``.
    :param any_perm: if True, any of permission in sequence is accepted
    :param accept_global_perms: if ``True`` takes global permissions into account.
      If any_perm is set to false then the intersection of matching objects based on global and object based permissions
      is returned. Default is ``True``.

    :raises MixedContentTypeError: when computed content type for ``perms``
      and/or ``klass`` clashes.
    :raises WrongAppError: if cannot compute app label for given ``perms``/
      ``klass``.

    Example:

    Let's assume we have a ``Task`` model belonging to the ``tasker`` app with
    the default add_task, change_task and delete_task permissions provided
    by Django::

        >>> from guardian.shortcuts import get_objects_for_group
        >>> from tasker import Task
        >>> group = Group.objects.create('some group')
        >>> task = Task.objects.create('some task')
        >>> get_objects_for_group(group, 'tasker.add_task')
        []
        >>> from guardian.shortcuts import assign_perm
        >>> assign_perm('tasker.add_task', group, task)
        >>> get_objects_for_group(group, 'tasker.add_task')
        [<Task some task>]

    The permission string can also be an iterable. Continuing with the previous example:
        >>> get_objects_for_group(group, ['tasker.add_task', 'tasker.delete_task'])
        []
        >>> assign_perm('tasker.delete_task', group, task)
        >>> get_objects_for_group(group, ['tasker.add_task', 'tasker.delete_task'])
        [<Task some task>]

    Global permissions assigned to the group are also taken into account. Continuing with previous example:
        >>> task_other = Task.objects.create('other task')
        >>> assign_perm('tasker.change_task', group)
        >>> get_objects_for_group(group, ['tasker.change_task'])
        [<Task some task>, <Task other task>]
        >>> get_objects_for_group(group, ['tasker.change_task'], accept_global_perms=False)
        [<Task some task>]

    """"""
    if isinstance(perms, str):
        perms = [perms]
    ctype = None
    app_label = None
    codenames = set()

    # Compute codenames set and ctype if possible
    for perm in perms:
        if '.' in perm:
            new_app_label, codename = perm.split('.', 1)
            if app_label is not None and app_label != new_app_label:
                raise MixedContentTypeError(""Given perms must have same app ""
                                            ""label (%s != %s)"" % (app_label, new_app_label))
            else:
                app_label = new_app_label
        else:
            codename = perm
        codenames.add(codename)
        if app_label is not None:
            new_ctype = ContentType.objects.get(app_label=app_label,
                                                permission__codename=codename)
            if ctype is not None and ctype != new_ctype:
                raise MixedContentTypeError(""ContentType was once computed ""
                                            ""to be %s and another one %s"" % (ctype, new_ctype))
            else:
                ctype = new_ctype

    # Compute queryset and ctype if still missing
    if ctype is None and klass is not None:
        queryset = _get_queryset(klass)
        ctype = get_content_type(queryset.model)
    elif ctype is not None and klass is None:
        queryset = _get_queryset(ctype.model_class())
    elif klass is None:
        raise WrongAppError(""Cannot determine content type"")
    else:
        queryset = _get_queryset(klass)
        if ctype.model_class() != queryset.model:
            raise MixedContentTypeError(""Content type for given perms and ""
                                        ""klass differs"")

    # At this point, we should have both ctype and queryset and they should
    # match which means: ctype.model_class() == queryset.model
    # we should also have ``codenames`` list

    global_perms = set()
    if accept_global_perms:
        global_perm_set = group.permissions.values_list('codename', flat=True)
        for code in codenames:
            if code in global_perm_set:
                global_perms.add(code)
        for code in global_perms:
            codenames.remove(code)
        if len(global_perms) > 0 and (len(codenames) == 0 or any_perm):
            return queryset

    # Now we should extract list of pk values for which we would filter
    # queryset
    group_model = get_group_obj_perms_model(queryset.model)
    groups_obj_perms_queryset = filter_perms_queryset_by_objects(
        group_model.objects
        .filter(group=group)
        .filter(permission__content_type=ctype),
        klass)
    if len(codenames):
        groups_obj_perms_queryset = groups_obj_perms_queryset.filter(
            permission__codename__in=codenames)
    if group_model.objects.is_generic():
        fields = ['object_pk', 'permission__codename']
    else:
        fields = ['content_object__pk', 'permission__codename']
    if not any_perm and len(codenames):
        groups_obj_perms = groups_obj_perms_queryset.values_list(*fields)
        data = list(groups_obj_perms)

        keyfunc = lambda t: t[0]  # sorting/grouping by pk (first in result tuple)
        data = sorted(data, key=keyfunc)
        pk_list = []
        for pk, group in groupby(data, keyfunc):
            obj_codenames = {e[1] for e in group}
            if any_perm or codenames.issubset(obj_codenames):
                pk_list.append(pk)
        objects = queryset.filter(pk__in=pk_list)
        return objects

    field_pk = fields[0]
    values = groups_obj_perms_queryset

    handle_pk_field = _handle_pk_field(queryset)
    if handle_pk_field is not None:
        values = values.annotate(obj_pk=handle_pk_field(expression=field_pk))
        field_pk = 'obj_pk'

    values = values.values_list(field_pk, flat=True)
    return queryset.filter(pk__in=values)","global_perms = set()
for code in codenames:
    if code in global_perm_set:
        global_perms.add(code)",global_perms = {code for code in codenames if code in global_perm_set}
capirca,https://github.com/google/capirca/tree/master/capirca/lib/paloaltofw.py,Rule,TermToOptions$123,"def TermToOptions(from_zone, to_zone, term, service_map):
    """"""Convert term to Palo Alto security rule options.""""""
    options = {}
    options[""from_zone""] = [from_zone]
    options[""to_zone""] = [to_zone]
    options[""description""] = []
    options[""source""] = []
    options[""destination""] = []
    options[""application""] = []
    options[""service""] = []
    options[""logging""] = []

    ACTIONS = {
      ""accept"": ""allow"",
      ""deny"": ""deny"",
      ""reject"": ""reset-client"",
      ""reject-with-tcp-rst"": ""reset-client"",
    }

    new_term = None

    def pan_ports(ports):
      x = []
      for tup in ports:
        if len(tup) > 1 and tup[0] != tup[1]:
          x.append(str(tup[0]) + ""-"" + str(tup[1]))
        else:
          x.append(str(tup[0]))

      return tuple(x)

    # COMMENT
    if term.comment:
      options[""description""] = term.comment

    # LOGGING
    if term.logging:
      for item in term.logging:
        if item.value in [""disable""]:
          options[""logging""] = [""disable""]
          break
        elif item.value in [""log-both""]:
          options[""logging""].append(""log-start"")
          options[""logging""].append(""log-end"")
        elif item.value in [""True"", ""true"", ""syslog"", ""local""]:
          options[""logging""].append(""log-end"")

    # SOURCE-ADDRESS
    if term.source_address:
      saddr_check = set()
      for saddr in term.source_address:
        saddr_check.add(saddr.parent_token)
      saddr_check = sorted(saddr_check)
      for addr in saddr_check:
        options[""source""].append(str(addr))
    # missing source handled during XML document generation

    # DESTINATION-ADDRESS
    if term.destination_address:
      daddr_check = set()
      for daddr in term.destination_address:
        daddr_check.add(daddr.parent_token)
      daddr_check = sorted(daddr_check)
      for addr in daddr_check:
        options[""destination""].append(str(addr))
    # missing destination handled during XML document generation

    # ACTION
    if term.action:
      options[""action""] = ACTIONS[term.action[0]]

    if term.option:
      options[""option""] = term.option

    if term.pan_application:
      for pan_app in term.pan_application:
        options[""application""].append(pan_app)

    if term.source_port or term.destination_port:
      src_ports = pan_ports(term.source_port)
      if term.destination_port:
        ports = pan_ports(term.destination_port)
      else:
        ports = pan_ports([(""0"", ""65535"")])

      # check to see if this service already exists
      for p in term.protocol:
        service_name = service_map.get_service_name(term.name,
                                                    src_ports, ports, p)
        if service_name not in options[""service""]:
          options[""service""].append(service_name)

    elif ""tcp"" in term.protocol or ""udp"" in term.protocol:
      services = {""tcp"", ""udp""} & set(term.protocol)
      others = set(term.protocol) - services
      if others:
        logging.info(
          ""INFO: Term %s in policy %s>%s contains port-less %s ""
          ""with non-port protocol(s). Moving %s to a new term."",
          term.name, from_zone, to_zone,
          ', '.join(list(services)), ', '.join(list(others)))
        new_term = copy.deepcopy(term)
        new_term.protocol = list(others)
        term.protocol = list(services)
        options[""application""] = []
      for p in term.protocol:
        ports = pan_ports([(""0"", ""65535"")])
        # use prefix """" to avoid service name clash with term named ""any""
        service_name = service_map.get_service_name(""any"", (), ports, p, """")
        if service_name not in options[""service""]:
          options[""service""].append(service_name)

    if term.protocol:
      # Add certain protocol names as application in the application list
      # if missing.
      for proto_name in term.protocol:
        if (proto_name in [""igmp"", ""sctp"", ""gre""] and
            proto_name not in options[""application""]):
          options[""application""].append(proto_name)

    return options, new_term","saddr_check = set()
for saddr in term.source_address:
    saddr_check.add(saddr.parent_token)",saddr_check = {saddr.parent_token for saddr in term.source_address}
capirca,https://github.com/google/capirca/tree/master/capirca/lib/paloaltofw.py,Rule,TermToOptions$123,"def TermToOptions(from_zone, to_zone, term, service_map):
    """"""Convert term to Palo Alto security rule options.""""""
    options = {}
    options[""from_zone""] = [from_zone]
    options[""to_zone""] = [to_zone]
    options[""description""] = []
    options[""source""] = []
    options[""destination""] = []
    options[""application""] = []
    options[""service""] = []
    options[""logging""] = []

    ACTIONS = {
      ""accept"": ""allow"",
      ""deny"": ""deny"",
      ""reject"": ""reset-client"",
      ""reject-with-tcp-rst"": ""reset-client"",
    }

    new_term = None

    def pan_ports(ports):
      x = []
      for tup in ports:
        if len(tup) > 1 and tup[0] != tup[1]:
          x.append(str(tup[0]) + ""-"" + str(tup[1]))
        else:
          x.append(str(tup[0]))

      return tuple(x)

    # COMMENT
    if term.comment:
      options[""description""] = term.comment

    # LOGGING
    if term.logging:
      for item in term.logging:
        if item.value in [""disable""]:
          options[""logging""] = [""disable""]
          break
        elif item.value in [""log-both""]:
          options[""logging""].append(""log-start"")
          options[""logging""].append(""log-end"")
        elif item.value in [""True"", ""true"", ""syslog"", ""local""]:
          options[""logging""].append(""log-end"")

    # SOURCE-ADDRESS
    if term.source_address:
      saddr_check = set()
      for saddr in term.source_address:
        saddr_check.add(saddr.parent_token)
      saddr_check = sorted(saddr_check)
      for addr in saddr_check:
        options[""source""].append(str(addr))
    # missing source handled during XML document generation

    # DESTINATION-ADDRESS
    if term.destination_address:
      daddr_check = set()
      for daddr in term.destination_address:
        daddr_check.add(daddr.parent_token)
      daddr_check = sorted(daddr_check)
      for addr in daddr_check:
        options[""destination""].append(str(addr))
    # missing destination handled during XML document generation

    # ACTION
    if term.action:
      options[""action""] = ACTIONS[term.action[0]]

    if term.option:
      options[""option""] = term.option

    if term.pan_application:
      for pan_app in term.pan_application:
        options[""application""].append(pan_app)

    if term.source_port or term.destination_port:
      src_ports = pan_ports(term.source_port)
      if term.destination_port:
        ports = pan_ports(term.destination_port)
      else:
        ports = pan_ports([(""0"", ""65535"")])

      # check to see if this service already exists
      for p in term.protocol:
        service_name = service_map.get_service_name(term.name,
                                                    src_ports, ports, p)
        if service_name not in options[""service""]:
          options[""service""].append(service_name)

    elif ""tcp"" in term.protocol or ""udp"" in term.protocol:
      services = {""tcp"", ""udp""} & set(term.protocol)
      others = set(term.protocol) - services
      if others:
        logging.info(
          ""INFO: Term %s in policy %s>%s contains port-less %s ""
          ""with non-port protocol(s). Moving %s to a new term."",
          term.name, from_zone, to_zone,
          ', '.join(list(services)), ', '.join(list(others)))
        new_term = copy.deepcopy(term)
        new_term.protocol = list(others)
        term.protocol = list(services)
        options[""application""] = []
      for p in term.protocol:
        ports = pan_ports([(""0"", ""65535"")])
        # use prefix """" to avoid service name clash with term named ""any""
        service_name = service_map.get_service_name(""any"", (), ports, p, """")
        if service_name not in options[""service""]:
          options[""service""].append(service_name)

    if term.protocol:
      # Add certain protocol names as application in the application list
      # if missing.
      for proto_name in term.protocol:
        if (proto_name in [""igmp"", ""sctp"", ""gre""] and
            proto_name not in options[""application""]):
          options[""application""].append(proto_name)

    return options, new_term","daddr_check = set()
for daddr in term.destination_address:
    daddr_check.add(daddr.parent_token)",daddr_check = {daddr.parent_token for daddr in term.destination_address}
gsutil,https://github.com/GoogleCloudPlatform/gsutil/tree/master/gslib/wildcard_iterator.py,CloudWildcardIterator,_GetToListFields$445,"def _GetToListFields(self, get_fields=None):
    """"""Prepends 'items/' to the input fields and converts it to a set.

    This way field sets requested for GetBucket can be used in ListBucket calls.
    Note that the input set must contain only bucket or object fields; listing
    fields such as prefixes or nextPageToken should be added after calling
    this function.

    Args:
      get_fields: Iterable fields usable in GetBucket/GetObject calls.

    Returns:
      Set of fields usable in ListBuckets/ListObjects calls.
    """"""
    if get_fields:
      list_fields = set()
      for field in get_fields:
        list_fields.add('items/' + field)
      return list_fields","list_fields = set()
for field in get_fields:
    list_fields.add('items/' + field)",list_fields = {'items/' + field for field in get_fields}
docassemble,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/functions.py,,parse_accept_language$227,"def parse_accept_language(language_header, restrict=True):
    ok_languages = set()
    for lang in word_collection.keys():
        ok_languages.add(lang)
    languages = list()
    for item in language_header.split(','):
        q = 1.0
        lang = item.strip()
        if ';' in lang:
            parts = item.split(';')
            lang = parts[0]
            q = parts[1]
            try:
                q = float(re.sub(r'^q=', '', q, flags=re.IGNORECASE))
            except:
                q = 1.0

        parts = re.split('-|_', lang)

        languages.append([parts[0].strip().lower(), q])
    output = list()
    for item in sorted(languages, key=lambda y: y[1], reverse=True):
        if restrict and item[0] not in ok_languages:
            continue
        if item[0] not in output:
            output.append(item[0])
    return output","ok_languages = set()
for lang in word_collection.keys():
    ok_languages.add(lang)",ok_languages = {lang for lang in word_collection.keys()}
core,https://github.com/home-assistant/core/tree/master/homeassistant/components/unifi/controller.py,UniFiController,update_wireless_clients$287,"def update_wireless_clients(self):
        """"""Update set of known to be wireless clients.""""""
        new_wireless_clients = set()

        for client_id in self.api.clients:
            if (
                client_id not in self.wireless_clients
                and not self.api.clients[client_id].is_wired
            ):
                new_wireless_clients.add(client_id)

        if new_wireless_clients:
            self.wireless_clients |= new_wireless_clients
            unifi_wireless_clients = self.hass.data[UNIFI_WIRELESS_CLIENTS]
            unifi_wireless_clients.update_data(self.wireless_clients, self.config_entry)","new_wireless_clients = set()
for client_id in self.api.clients:
    if client_id not in self.wireless_clients and (not self.api.clients[client_id].is_wired):
        new_wireless_clients.add(client_id)",new_wireless_clients = {client_id for client_id in self.api.clients if client_id not in self.wireless_clients and (not self.api.clients[client_id].is_wired)}
pyproj,https://github.com/pyproj4/pyproj/tree/master/test/test_sync.py,,test_get_transform_grid_list__contains$75,"def test_get_transform_grid_list__contains():
    grids = get_transform_grid_list(
        bbox=BBox(170, -90, -170, 90),
        spatial_test=""contains"",
        include_already_downloaded=True,
    )
    assert len(grids) > 5
    source_ids = set()
    for grid in grids:
        source_ids.add(grid[""properties""][""source_id""])
    assert sorted(source_ids) == [""nz_linz""]","source_ids = set()
for grid in grids:
    source_ids.add(grid['properties']['source_id'])",source_ids = {grid['properties']['source_id'] for grid in grids}
pyproj,https://github.com/pyproj4/pyproj/tree/master/test/test_sync.py,,test_get_transform_grid_list__bbox__out_of_bounds$42,"def test_get_transform_grid_list__bbox__out_of_bounds():
    grids = get_transform_grid_list(
        bbox=BBox(170, -90, 190, 90), include_already_downloaded=True
    )
    assert len(grids) > 10
    source_ids = set()
    for grid in grids:
        source_ids.add(grid[""properties""][""source_id""])
    assert sorted(source_ids) == [
        ""au_ga"",
        ""nc_dittt"",
        ""no_kv"",
        ""nz_linz"",
        ""us_nga"",
        ""us_noaa"",
    ]","source_ids = set()
for grid in grids:
    source_ids.add(grid['properties']['source_id'])",source_ids = {grid['properties']['source_id'] for grid in grids}
salt,https://github.com/saltstack/salt/tree/master/salt/beacons/inotify.py,,beacon$174,"def beacon(config):
    """"""
    Watch the configured files

    Example Config

    .. code-block:: yaml

        beacons:
          inotify:
            - files:
                /path/to/file/or/dir:
                  mask:
                    - open
                    - create
                    - close_write
                  recurse: True
                  auto_add: True
                  exclude:
                    - /path/to/file/or/dir/exclude1
                    - /path/to/file/or/dir/exclude2
                    - /path/to/file/or/dir/regex[a-m]*$:
                        regex: True
            - coalesce: True

    The mask list can contain the following events (the default mask is create,
    delete, and modify):

    * access            - File accessed
    * attrib            - File metadata changed
    * close_nowrite     - Unwritable file closed
    * close_write       - Writable file closed
    * create            - File created in watched directory
    * delete            - File deleted from watched directory
    * delete_self       - Watched file or directory deleted
    * modify            - File modified
    * moved_from        - File moved out of watched directory
    * moved_to          - File moved into watched directory
    * move_self         - Watched file moved
    * open              - File opened

    The mask can also contain the following options:

    * dont_follow       - Don't dereference symbolic links
    * excl_unlink       - Omit events for children after they have been unlinked
    * oneshot           - Remove watch after one event
    * onlydir           - Operate only if name is directory

    recurse:
      Recursively watch files in the directory
    auto_add:
      Automatically start watching files that are created in the watched directory
    exclude:
      Exclude directories or files from triggering events in the watched directory.
      Can use regex if regex is set to True
    coalesce:
      If this coalescing option is enabled, events are filtered based on
      their unicity, only unique events are enqueued, doublons are discarded.
      An event is unique when the combination of its fields (wd, mask,
      cookie, name) is unique among events of a same batch. After a batch of
      events is processed any events are accepted again.
      This option is top-level (at the same level as the path) and therefore
      affects all paths that are being watched. This is due to this option
      being at the Notifier level in pyinotify.
    """"""

    whitelist = [""_beacon_name""]
    config = salt.utils.beacons.remove_hidden_options(config, whitelist)

    config = salt.utils.beacons.list_to_dict(config)

    ret = []
    notifier = _get_notifier(config)
    wm = notifier._watch_manager

    # Read in existing events
    if notifier.check_events(1):
        notifier.read_events()
        notifier.process_events()
        queue = __context__[""inotify.queue""]
        while queue:
            event = queue.popleft()

            _append = True
            # Find the matching path in config
            path = event.path
            while path != ""/"":
                if path in config.get(""files"", {}):
                    break
                path = os.path.dirname(path)

            excludes = config[""files""].get(path, {}).get(""exclude"", """")

            if excludes and isinstance(excludes, list):
                for exclude in excludes:
                    if isinstance(exclude, dict):
                        _exclude = next(iter(exclude))
                        if exclude[_exclude].get(""regex"", False):
                            try:
                                if re.search(_exclude, event.pathname):
                                    _append = False
                            except Exception:  # pylint: disable=broad-except
                                log.warning(""Failed to compile regex: %s"", _exclude)
                        else:
                            exclude = _exclude
                    elif ""*"" in exclude:
                        if fnmatch.fnmatch(event.pathname, exclude):
                            _append = False
                    else:
                        if event.pathname.startswith(exclude):
                            _append = False

            if _append:
                sub = {
                    ""tag"": event.path,
                    ""path"": event.pathname,
                    ""change"": event.maskname,
                }
                ret.append(sub)
            else:
                log.info(""Excluding %s from event for %s"", event.pathname, path)

    # Get paths currently being watched
    current = set()
    for wd in wm.watches:
        current.add(wm.watches[wd].path)

    # Update existing watches and add new ones
    # TODO: make the config handle more options
    for path in config.get(""files"", ()):

        if isinstance(config[""files""][path], dict):
            mask = config[""files""][path].get(""mask"", DEFAULT_MASK)
            if isinstance(mask, list):
                r_mask = 0
                for sub in mask:
                    r_mask |= _get_mask(sub)
            elif isinstance(mask, bytes):
                r_mask = _get_mask(mask)
            else:
                r_mask = mask
            mask = r_mask
            rec = config[""files""][path].get(""recurse"", False)
            auto_add = config[""files""][path].get(""auto_add"", False)
        else:
            mask = DEFAULT_MASK
            rec = False
            auto_add = False

        if path in current:
            for wd in wm.watches:
                if path == wm.watches[wd].path:
                    update = False
                    if wm.watches[wd].mask != mask:
                        update = True
                    if wm.watches[wd].auto_add != auto_add:
                        update = True
                    if update:
                        wm.update_watch(wd, mask=mask, rec=rec, auto_add=auto_add)
        elif os.path.exists(path):
            excludes = config[""files""][path].get(""exclude"", """")
            excl = None
            if isinstance(excludes, list):
                excl = []
                for exclude in excludes:
                    if isinstance(exclude, dict):
                        excl.append(list(exclude)[0])
                    else:
                        excl.append(exclude)
                excl = pyinotify.ExcludeFilter(excl)

            wm.add_watch(path, mask, rec=rec, auto_add=auto_add, exclude_filter=excl)

    # Return event data
    return ret","current = set()
for wd in wm.watches:
    current.add(wm.watches[wd].path)",current = {wm.watches[wd].path for wd in wm.watches}
mackup,https://github.com/lra/mackup/tree/master/mackup/appsdb.py,ApplicationsDatabase,get_app_names$143,"def get_app_names(self):
        """"""
        Return application names.

        Return the list of application names that are available in the
        database.

        Returns:
            set of str.
        """"""
        app_names = set()
        for name in self.apps:
            app_names.add(name)

        return app_names","app_names = set()
for name in self.apps:
    app_names.add(name)",app_names = {name for name in self.apps}
WhatBreach,https://github.com/Ekultek/WhatBreach/tree/master/whatbreach/main.py,,main$29,"def main():
    try:
        opt = Parser().optparse()
        print(BANNER)
        res = Parser().check_opts(opt)
        if res is not None:
            to_search = res
        else:
            to_search = []
        do_not_search = []

        if len(to_search) == 0:
            if opt.singleEmail is None and opt.emailFile is None:
                warn(""you have not provided an email to scan, redirecting to the help menu"")
                subprocess.call([""python"", ""whatbreach.py"", ""--help""])
                exit(1)
            api_tokens = grab_api_tokens()
            if opt.searchHunterIo and opt.singleEmail is not None:
                info(""starting search on hunter.io using {}"".format(opt.singleEmail))
                file_results = HunterIoHook(
                    opt.singleEmail, api_tokens[""hunter.io""], verify_emails=opt.verifyEmailsThroughHunterIo
                ).hooker()
                if file_results is not None:
                    with open(file_results) as data:
                        emails = json.loads(data.read())[""discovered_emails""]
                    for email in emails:
                        to_search.append(email)
                else:
                    to_search.append(opt.singleEmail)
            elif opt.searchHunterIo and opt.emailFile is not None:
                if not test_file(opt.emailFile):
                    error(""unable to open filename, does it exist?"")
                    exit(1)
                api_tokens = grab_api_tokens()
                with open(opt.emailFile) as data:
                    for email in data.readlines():
                        email = email.strip()
                        file_results = HunterIoHook(
                            email, api_tokens[""hunter.io""], verify_emails=opt.verifyEmailsThroughHunterIo
                        ).hooker()
                        with open(file_results) as results:
                            discovered_emails = json.loads(results.read())[""discovered_emails""]
                        for discovered in discovered_emails:
                            to_search.append(discovered)
            elif opt.singleEmail is not None:
                info(""starting search on single email address: {}"".format(opt.singleEmail))
                to_search = [opt.singleEmail]
            elif opt.emailFile is not None:
                if not test_file(opt.emailFile):
                    error(""unable to open filename, does it exist?"")
                    exit(1)
                with open(opt.emailFile) as emails:
                    info(""parsing email file: {}"".format(opt.emailFile))
                    to_search = emails.readlines()
                info(""starting search on a total of {} email(s)"".format(len(to_search)))

        for email in to_search:
            email = email.strip()

            if opt.checkTenMinuteEmail:
                if check_ten_minute_email(email, TEN_MINUTE_EMAIL_EXTENSION_LIST):
                    warn(""email: {} appears to be a ten minute email"".format(email))
                    answer = prompt(""would you like to process the email[y/N]"")
                    if answer.startswith(""n""):
                        do_not_search.append(email)

            if opt.checkEmailAccounts:
                info(""searching for possible profiles related to {}"".format(email))
                searcher = EmailRepHook(email)
                results = searcher.hooker()
                if results is not None and len(results) != 0:
                    info(
                        ""found a total of {} possible profiles associated with {} on the following domains:"".format(
                            len(results), email
                        )
                    )
                    for domain in results:
                        print(""\t-> {}"".format(domain.title()))
                else:
                    warn(""no possible profiles discovered for email: {}"".format(email))

            if email not in do_not_search:
                if opt.throttleRequests != 0:
                    time.sleep(opt.throttleRequests)
                info(""searching breached accounts on HIBP related to: {}"".format(email))
                account_dumps = BeenPwnedHook(
                    email, api_tokens[""haveibeenpwned.com""], opt, retry=opt.retryOnFail
                ).account_hooker()
                info(""searching for paste dumps on HIBP related to: {}"".format(email))

                if opt.searchPastebin:
                    paste_dumps = BeenPwnedHook(
                        email, api_tokens[""haveibeenpwned.com""], opt, retry=opt.retryOnFail
                    ).paste_hooker()
                else:
                    warn(""suppressing discovered pastes"")
                    paste_dumps = []

                if opt.searchWeLeakInfo:
                    info(""searching weleakinfo.com for breaches related to: {}"".format(email))
                    searcher = WeLeakInfoHook(email, api_tokens[""weleakinfo.com""])
                    tmp = set()
                    results = searcher.hooker()
                    if results is not None:
                        if account_dumps is not None:
                            original_length = len(account_dumps)
                        else:
                            original_length = 0
                        if account_dumps is not None:
                            for item in account_dumps:
                                tmp.add(item)
                        if results is not None:
                            for item in results:
                                tmp.add(item)
                        if len(tmp) != 0:
                            account_dumps = list(tmp)
                            new_length = len(account_dumps)
                            amount_discovered = new_length - original_length
                            if amount_discovered != 0:
                                info(
                                    ""discovered a total of {} more breaches from weleakinfo.com"".format(
                                        new_length - original_length
                                    )
                                )
                            else:
                                warn(""did not discover any breaches"")
                        else:
                            warn(""did not discover any new databases from weleakinfo.com"")
                    else:
                        warn(""no databases discovered on weleakinfo"")

                if opt.searchSnusBase:
                    info(""searching snusbase.com for breaches related to '{}'"".format(email))
                    snusbase_leaks = SnusbaseHooker(
                        email, api_tokens[""snusbase.com""][""username""],
                        api_tokens[""snusbase.com""][""password""]
                    ).main()
                    if snusbase_leaks is not None and len(snusbase_leaks) != 0:
                        info(""found a total of {} more leaks using snusbase"".format(len(snusbase_leaks)))
                        for item in snusbase_leaks:
                            account_dumps.append(item)
                            set(account_dumps)
                            account_dumps = list(account_dumps)
                    else:
                        warn(""did not find anymore leaks using snusbase"")

                if account_dumps is not None and paste_dumps is not None and len(account_dumps) != 0:
                    info(
                        ""found a total of {} database breach(es) and a total of {} paste(s) pertaining to: {}"".format(
                            len(account_dumps), len(paste_dumps), email
                        )
                    )
                    if opt.searchDehashed:
                        if len(account_dumps) > 20:
                            warn(
                                ""large amount of database breaches, obtaining links from ""
                                ""dehashed (this may take a minute)""
                            )
                        found_databases = DehashedHook(account_dumps).hooker()
                    else:
                        warn(""suppressing discovered databases"")
                        found_databases = {}
                    for i, dump in enumerate(paste_dumps, start=1):
                        found_databases[""Paste#{}"".format(i)] = str(dump)
                    display_found_databases(found_databases, download_pastes=opt.downloadPastes)
                    if opt.downloadDatabase:
                        for item in found_databases.keys():
                            if ""Paste"" not in item:
                                info(""searching for downloadable databases using query: {}"".format(item.lower()))
                                downloaded = DatabasesTodayHook(
                                    str(item), downloads_directory=opt.saveDirectory
                                ).hooker()
                                if len(downloaded) != 0:
                                    info(
                                        ""downloaded a total of {} database(s) pertaining to query: {}"".format(
                                            len(downloaded), item
                                        )
                                    )
                                    display_found_databases(
                                        downloaded, is_downloaded=True, download_pastes=opt.downloadPastes
                                    )
                                else:
                                    warn(
                                        ""no databases appeared to be present and downloadable related to query: {}"".format(
                                            str(item)
                                        )
                                    )

                elif account_dumps is not None and paste_dumps is None and len(account_dumps) != 0:
                    info(""found a total of {} database breach(es) pertaining to: {}"".format(len(account_dumps), email))
                    if opt.searchDehashed:
                        if len(account_dumps) > 20:
                            warn(
                                ""large amount of database breaches, obtaining links from ""
                                ""dehashed (this may take a minute)""
                            )
                        found_databases = DehashedHook(account_dumps).hooker()
                    else:
                        warn(""suppressing discovered databases"")
                        found_databases = {}
                    if len(found_databases) != 0:
                        display_found_databases(found_databases, download_pastes=opt.downloadPastes)
                        if opt.downloadDatabase:
                            for item in found_databases.keys():
                                if ""Paste"" not in item:
                                    info(""searching for downloadable databases using query: {}"".format(item.lower()))
                                    downloaded = DatabasesTodayHook(
                                        str(item), downloads_directory=opt.saveDirectory
                                    ).hooker()
                                    if len(downloaded) != 0:
                                        info(
                                            ""downloaded a total of {} database(s) pertaining to query: {}"".format(
                                                len(downloaded), item
                                            )
                                        )
                                        display_found_databases(
                                            downloaded, is_downloaded=True, download_pastes=opt.downloadPastes
                                        )
                                    else:
                                        warn(
                                            ""no databases appeared to be present and downloadable related to query: {}"".format(
                                                str(item)
                                            )
                                        )
                    else:
                        warn(""no output to show, most likely due to output suppression or dehashed"")
                elif account_dumps is None and paste_dumps is not None:
                    # this should never happen
                    error(""no database dumps found nor any pastes found for: {}"".format(email))
                else:
                    error(""email {} was not found in any breach"".format(email))

        if opt.staySalty:
            # i know that you think that you know shit
            # all the shade that's coming at me I wonder who throws it
            # you can't see the vision boy, you must be outta focus
            # that's a real hot program homie, I wonder who wrote it? oh shit
            # (lyrics ripped from iSpy by Kyle, all I do is steal bruh)
            warn(""all this code was stolen with <3 by Eku"")
    except KeyboardInterrupt:
        error(""user quit the session"")","tmp = set()
for item in account_dumps:
    tmp.add(item)",tmp = {item for item in account_dumps}
mindmeld,https://github.com/cisco/mindmeld/tree/master/mindmeld/components/role_classifier.py,RoleClassifier,fit$79,"def fit(self,
            queries=None,
            label_set=None,
            incremental_timestamp=None,
            load_cached=True, **kwargs):
        """"""Trains a statistical model for role classification using the provided training examples.

        Args:
            queries (list of ProcessedQuery): The labeled queries to use as training data
            label_set (list, optional): A label set to load. If not specified, the default
                training set will be loaded.
            incremental_timestamp (str, optional): The timestamp folder to cache models in
        """"""
        logger.info(
            ""Fitting role classifier: domain=%r, intent=%r, entity_type=%r"",
            self.domain,
            self.intent,
            self.entity_type,
        )

        # create model with given params
        model_config = self._get_model_config(**kwargs)

        label_set = label_set or model_config.train_label_set or DEFAULT_TRAIN_SET_REGEX
        queries = self._resolve_queries(queries, label_set)

        new_hash = self._get_model_hash(model_config, queries)
        cached_model_path = self._resource_loader.hash_to_model_path.get(new_hash)

        if incremental_timestamp and cached_model_path:
            logger.info(""No need to fit. Previous model is cached."")
            if load_cached:
                # load() sets self.ready = True
                self.load(cached_model_path)
                return True
            return False

        # These examples and labels are flat lists, not
        # a ProcessedQueryList.Iterator
        examples, labels = self._get_examples_and_labels(queries)

        if examples:
            # Build roles set
            self.roles = set()
            for label in labels:
                self.roles.add(label)

            model = create_model(model_config)
            model.initialize_resources(self._resource_loader, examples, labels)
            model.fit(examples, labels)
            self._model = model
            self.config = ClassifierConfig.from_model_config(self._model.config)

        self.hash = new_hash

        self.ready = True
        self.dirty = True
        return True","self.roles = set()
for label in labels:
    self.roles.add(label)",self.roles = {label for label in labels}
yandex_smart_home,https://github.com/dmitry-k/yandex_smart_home/tree/master/custom_components/yandex_smart_home/capability_color.py,ColorSettingCapability,get_supported_scenes$150,"def get_supported_scenes(scenes_map: dict[str, list[str]],
                             entity_effect_list: list[str]) -> list[str]:
        yandex_scenes = set()
        for effect in entity_effect_list:
            for yandex_scene, ha_effects in scenes_map.items():
                if effect in ha_effects:
                    yandex_scenes.add(yandex_scene)

        return sorted(list(yandex_scenes))","yandex_scenes = set()
for effect in entity_effect_list:
    for (yandex_scene, ha_effects) in scenes_map.items():
        if effect in ha_effects:
            yandex_scenes.add(yandex_scene)","yandex_scenes = {yandex_scene for effect in entity_effect_list for (yandex_scene, ha_effects) in scenes_map.items() if effect in ha_effects}"
GeneticAlgorithmsWithPython,https://github.com/handcraftsman/GeneticAlgorithmsWithPython/tree/master/ch18/ticTacToeTests.py,LocationFilter,get_matches$524,"def get_matches(self, board, squares):
        result = set()
        for square in squares:
            if self.func(square):
                result.add(square.Index)
        return result","result = set()
for square in squares:
    if self.func(square):
        result.add(square.Index)",result = {square.Index for square in squares if self.func(square)}
onnxmltools,https://github.com/onnx/onnxmltools/tree/master/onnxmltools/convert/coreml/_parse.py,,parse_coreml$427,"def parse_coreml(model, initial_types=None, target_opset=None, custom_conversion_functions=None, custom_shape_calculators=None):
    '''
    This is the root function of the whole parsing procedure.
    :param model: CoreML model
    :param initial_types: A list providing some types for some root variables. Each element is a tuple of a variable
    name and a type defined in data_types.py.
    :param target_opset: number, for example, 7 for ONNX 1.2, and 8 for ONNX 1.3.
    :param custom_conversion_functions: a dictionary for specifying the user customized conversion function
    :param custom_shape_calculators: a dictionary for specifying the user customized shape calculator
    :return: a Topology object. It's a intermediate representation of the input CoreML model
    '''

    # Add model-level input and output names into a set. The set will be fed into our Topology so that all its elements
    # will not be used to declare variables
    reserved_variable_names = set()
    for var in list(model.description.input) + list(model.description.output):
        reserved_variable_names.add(var.name)

    # Determine the batch size for parsing CoreML model's input and output features. Note that batch size is always
    # missing in all CoreML models.
    default_batch_size = 'None'

    # Topology is shared by both of CoreML and scikit-learn conversion frameworks, so we have a wrapper class,
    # CoremlModelContainer, to make sure our topology-related functions can seamlessly handle both of CoreML and
    # scikit-learn.
    topology = Topology(CoremlModelContainer(model),
                        default_batch_size,
                        initial_types,
                        reserved_variable_names,
                        target_opset=target_opset,
                        custom_conversion_functions=custom_conversion_functions,
                        custom_shape_calculators=custom_shape_calculators)
    scope = topology.declare_scope('__root__')

    # Instead of using CoremlModelContainer, we directly pass the model in because _parse_model is CoreML-specific.
    _parse_model(topology, scope, model)
    topology.compile()

    for variable in topology.find_root_and_sink_variables():
        color_space = getattr(variable.type, 'color_space', None)
        if color_space:
            if topology.metadata_props.setdefault('Image.BitmapPixelFormat', color_space) != color_space:
                warnings.warn('Conflicting pixel formats found. In ONNX, all input/output images must use the same pixel format.')
        # Use original CoreML names for model-level input(s)/output(s)
        if variable.raw_name not in reserved_variable_names:
            continue
        topology.rename_variable(variable.onnx_name, variable.raw_name)
    return topology","reserved_variable_names = set()
for var in list(model.description.input) + list(model.description.output):
    reserved_variable_names.add(var.name)",reserved_variable_names = {var.name for var in list(model.description.input) + list(model.description.output)}
alot,https://github.com/pazz/alot/tree/master/alot/commands/envelope.py,SendCommand,_get_keys_addresses$176,"def _get_keys_addresses(self):
        addresses = set()
        for key in self.envelope.encrypt_keys.values():
            for uid in key.uids:
                addresses.add(uid.email)
        return addresses","addresses = set()
for key in self.envelope.encrypt_keys.values():
    for uid in key.uids:
        addresses.add(uid.email)",addresses = {uid.email for key in self.envelope.encrypt_keys.values() for uid in key.uids}
mongoaudit,https://github.com/stampery/mongoaudit/tree/master/mongoaudit/testers/testers.py,,try_dedicated_user$222,"def try_dedicated_user(test):
    """"""
    Verify that the role only applies to one database
    """"""
    roles = test.tester.get_roles()
    user_role_dbs = set()
    for role in roles['roles']:
        user_role_dbs.add(role['db'])

    return TestResult(success=bool(len(user_role_dbs)), message=decode_to_string(user_role_dbs))","user_role_dbs = set()
for role in roles['roles']:
    user_role_dbs.add(role['db'])",user_role_dbs = {role['db'] for role in roles['roles']}
neural-backed-decision-trees,https://github.com/alvinwan/neural-backed-decision-trees/tree/master/nbdt/hierarchy.py,,match_wnid_leaves$146,"def match_wnid_leaves(wnids, G, tree_name):
    wnid_set = set()
    for wnid in wnids:
        wnid_set.add(wnid.strip())

    leaves_seen = get_seen_wnids(wnid_set, get_leaves(G))
    return leaves_seen, wnid_set","wnid_set = set()
for wnid in wnids:
    wnid_set.add(wnid.strip())",wnid_set = {wnid.strip() for wnid in wnids}
horovod,https://github.com/horovod/horovod/tree/master/horovod/runner/util/network.py,,get_local_host_addresses$28,"def get_local_host_addresses():
    local_addresses = set()
    for intf_info_list in psutil.net_if_addrs().values():
        for intf_info in intf_info_list:
            if intf_info.family == socket.AF_INET:
                local_addresses.add(intf_info.address)
    return local_addresses","local_addresses = set()
for intf_info_list in psutil.net_if_addrs().values():
    for intf_info in intf_info_list:
        if intf_info.family == socket.AF_INET:
            local_addresses.add(intf_info.address)",local_addresses = {intf_info.address for intf_info_list in psutil.net_if_addrs().values() for intf_info in intf_info_list if intf_info.family == socket.AF_INET}
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/hsm.py,HSMClusterSubnet,get_related_ids$33,"def get_related_ids(self, clusters):
        subnet_ids = set()
        for cluster in clusters:
            for subnet in cluster.get('SubnetMapping').values():
                subnet_ids.add(subnet)
        return list(subnet_ids)","subnet_ids = set()
for cluster in clusters:
    for subnet in cluster.get('SubnetMapping').values():
        subnet_ids.add(subnet)",subnet_ids = {subnet for cluster in clusters for subnet in cluster.get('SubnetMapping').values()}
rope,https://github.com/python-rope/rope/tree/master/rope/refactor/importutils/module_imports.py,_GlobalUnboundNameFinder,__init__$428,"def __init__(self, pymodule, wanted_pyobject):
        super(_GlobalUnboundNameFinder, self).__init__(pymodule)
        self.unbound = set()
        self.names = set()
        for name, pyname in pymodule._get_structural_attributes().items():
            if not isinstance(pyname, (pynames.ImportedName, pynames.ImportedModule)):
                self.names.add(name)
        wanted_scope = wanted_pyobject.get_scope()
        self.start = wanted_scope.get_start()
        self.end = wanted_scope.get_end() + 1","self.names = set()
for (name, pyname) in pymodule._get_structural_attributes().items():
    if not isinstance(pyname, (pynames.ImportedName, pynames.ImportedModule)):
        self.names.add(name)","self.names = {name for (name, pyname) in pymodule._get_structural_attributes().items() if not isinstance(pyname, (pynames.ImportedName, pynames.ImportedModule))}"
skywater-pdk,https://github.com/google/skywater-pdk/tree/master/scripts/python-skywater-pdk/skywater_pdk/liberty.py,,remove_ccsnoise_from_dict$289,"def remove_ccsnoise_from_dict(data, dataname):
    if ""timing"" in data:
        remove_ccsnoise_from_timing(data, dataname)

    ccsn_keys = set()
    for k in data:
        if ""ccsn_"" in k:
            ccsn_keys.add(k)

    for k in ccsn_keys:
        if debug:
            print(""{:s}: Removing {}"".format(dataname, k))
        del data[k]","ccsn_keys = set()
for k in data:
    if 'ccsn_' in k:
        ccsn_keys.add(k)",ccsn_keys = {k for k in data if 'ccsn_' in k}
botflow,https://github.com/kkyon/botflow/tree/master/botflow/botbase.py,BotManager,get_all_q$143,"def get_all_q(self):
        qs = set()
        for b in self._bots:
            for q in b.iq + b.oq:
                if not isinstance(q, SinkQueue):
                    qs.add(q)

        return qs","qs = set()
for b in self._bots:
    for q in b.iq + b.oq:
        if not isinstance(q, SinkQueue):
            qs.add(q)","qs = {q for b in self._bots for q in b.iq + b.oq if not isinstance(q, SinkQueue)}"
news-please,https://github.com/fhamborg/news-please/tree/master/newsplease/crawler/spiders/gdelt_crawler.py,GdeltCrawler,rss_parse$52,"def rss_parse(self, response):
        """"""
        Extracts all article links and initiates crawling them.

        :param obj response: The scrapy response
        """"""
        # get last_update zip url
        match = re.match(re_export, response.text)
        if match:
            last_update_zip_url = match.group(1)
            # fetch zip file
            r = requests.get(last_update_zip_url)
            # unzip
            z = zipfile.ZipFile(io.BytesIO(r.content))
            extracted = z.namelist()
            z.extractall('/tmp')
            csv_file_path = '/tmp/%s' % extracted[0]
            # read csv to get all urls
            urls = set()  # set to remove duplicates
            with open(csv_file_path) as csv_file:
                csv_reader = csv.reader(csv_file, delimiter='\t')
                for row in csv_reader:
                    urls.add(row[-1])
            # rm the file
            os.remove(csv_file_path)
            for url in urls:
                yield scrapy.Request(url, lambda resp: self.article_parse(
                    resp, 'gdelt'))","urls = set()
for row in csv_reader:
    urls.add(row[-1])",urls = {row[-1] for row in csv_reader}
torch-light,https://github.com/ne7ermore/torch-light/tree/master/information-extraction/train.py,,test$140,"def test(i, predict):
    model.eval()
    t = pre = groud = 0
    inf = open(""data/dev_data.json"", encoding=""utf8"")
    for line in inf:
        line = json.loads(line)
        text = line[""text""]
        g_triples = set()
        for trip in line[""spo_list""]:
            g_triples.add((trip[""subject""], trip[""predicate""], trip[""object""]))

        p_triples = predict.predict(text)
        pre += len(p_triples)
        groud += len(g_triples)
        t += len(p_triples.intersection(g_triples))

    print(
        f""test epoch {i+1}/{args.epochs} precision: {t/(pre+0.001):.4f} recall: {t/groud:.4f} f1: {2*t/(pre+groud):.4f}"")
    return 2*t/(pre+groud)","g_triples = set()
for trip in line['spo_list']:
    g_triples.add((trip['subject'], trip['predicate'], trip['object']))","g_triples = {(trip['subject'], trip['predicate'], trip['object']) for trip in line['spo_list']}"
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/building/farm.py,FarmOptionCache,_get_raw_options$44,"def _get_raw_options(self, farm_spots_set, field_spots_set, road_spots_set):
		field_row3 = {}
		field_col3 = {}

		for coords in farm_spots_set:
			x, y = coords
			row_score = 1
			if (x - 3, y) in field_spots_set:
				row_score += 1
			if (x + 3, y) in field_spots_set:
				row_score += 1
			field_row3[coords] = row_score

			col_score = 1
			if (x, y - 3) in field_spots_set:
				col_score += 1
			if (x, y + 3) in field_spots_set:
				col_score += 1
			field_col3[coords] = col_score

		road_row3 = set()
		road_col3 = set()
		for (x, y) in road_spots_set:
			if (x + 2, y) in road_spots_set and (x + 1, y) in road_spots_set:
				road_row3.add((x, y))
			if (x, y + 2) in road_spots_set and (x, y + 1) in road_spots_set:
				road_col3.add((x, y))

		road_row9 = set()
		for (x, y) in road_row3:
			if (x - 3, y) in road_row3 and (x + 3, y) in road_row3:
				road_row9.add((x, y))

		road_col9 = set()
		for (x, y) in road_col3:
			if (x, y - 3) in road_col3 and (x, y + 3) in road_col3:
				road_col9.add((x, y))

		raw_options = []
		for coords in sorted(farm_spots_set):
			x, y = coords

			row_score = field_row3[coords] - 1
			if (x, y - 1) in road_row9:
				score = row_score
				if (x, y - 4) in field_row3:
					score += field_row3[(x, y - 4)]
				if (x, y + 3) in field_row3:
					score += field_row3[(x, y + 3)]
				if score > 0:
					raw_options.append((score, coords, 0))
			if (x, y + 3) in road_row9:
				score = row_score
				if (x, y - 3) in field_row3:
					score += field_row3[(x, y - 3)]
				if (x, y + 4) in field_row3:
					score += field_row3[(x, y + 4)]
				if score > 0:
					raw_options.append((score, coords, 1))

			col_score = field_col3[coords] - 1
			if (x - 1, y) in road_col9:
				score = col_score
				if (x - 4, y) in field_col3:
					score += field_col3[(x - 4, y)]
				if (x + 3, y) in field_col3:
					score += field_col3[(x + 3, y)]
				if score > 0:
					raw_options.append((score, coords, 2))
			if (x + 3, y) in road_col9:
				score = col_score
				if (x - 3, y) in field_col3:
					score += field_col3[(x - 3, y)]
				if (x + 4, y) in field_col3:
					score += field_col3[(x + 4, y)]
				if score > 0:
					raw_options.append((score, coords, 3))

		return raw_options","road_row9 = set()
for (x, y) in road_row3:
    if (x - 3, y) in road_row3 and (x + 3, y) in road_row3:
        road_row9.add((x, y))","road_row9 = {(x, y) for (x, y) in road_row3 if (x - 3, y) in road_row3 and (x + 3, y) in road_row3}"
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/building/farm.py,FarmOptionCache,_get_raw_options$44,"def _get_raw_options(self, farm_spots_set, field_spots_set, road_spots_set):
		field_row3 = {}
		field_col3 = {}

		for coords in farm_spots_set:
			x, y = coords
			row_score = 1
			if (x - 3, y) in field_spots_set:
				row_score += 1
			if (x + 3, y) in field_spots_set:
				row_score += 1
			field_row3[coords] = row_score

			col_score = 1
			if (x, y - 3) in field_spots_set:
				col_score += 1
			if (x, y + 3) in field_spots_set:
				col_score += 1
			field_col3[coords] = col_score

		road_row3 = set()
		road_col3 = set()
		for (x, y) in road_spots_set:
			if (x + 2, y) in road_spots_set and (x + 1, y) in road_spots_set:
				road_row3.add((x, y))
			if (x, y + 2) in road_spots_set and (x, y + 1) in road_spots_set:
				road_col3.add((x, y))

		road_row9 = set()
		for (x, y) in road_row3:
			if (x - 3, y) in road_row3 and (x + 3, y) in road_row3:
				road_row9.add((x, y))

		road_col9 = set()
		for (x, y) in road_col3:
			if (x, y - 3) in road_col3 and (x, y + 3) in road_col3:
				road_col9.add((x, y))

		raw_options = []
		for coords in sorted(farm_spots_set):
			x, y = coords

			row_score = field_row3[coords] - 1
			if (x, y - 1) in road_row9:
				score = row_score
				if (x, y - 4) in field_row3:
					score += field_row3[(x, y - 4)]
				if (x, y + 3) in field_row3:
					score += field_row3[(x, y + 3)]
				if score > 0:
					raw_options.append((score, coords, 0))
			if (x, y + 3) in road_row9:
				score = row_score
				if (x, y - 3) in field_row3:
					score += field_row3[(x, y - 3)]
				if (x, y + 4) in field_row3:
					score += field_row3[(x, y + 4)]
				if score > 0:
					raw_options.append((score, coords, 1))

			col_score = field_col3[coords] - 1
			if (x - 1, y) in road_col9:
				score = col_score
				if (x - 4, y) in field_col3:
					score += field_col3[(x - 4, y)]
				if (x + 3, y) in field_col3:
					score += field_col3[(x + 3, y)]
				if score > 0:
					raw_options.append((score, coords, 2))
			if (x + 3, y) in road_col9:
				score = col_score
				if (x - 3, y) in field_col3:
					score += field_col3[(x - 3, y)]
				if (x + 4, y) in field_col3:
					score += field_col3[(x + 4, y)]
				if score > 0:
					raw_options.append((score, coords, 3))

		return raw_options","road_col9 = set()
for (x, y) in road_col3:
    if (x, y - 3) in road_col3 and (x, y + 3) in road_col3:
        road_col9.add((x, y))","road_col9 = {(x, y) for (x, y) in road_col3 if (x, y - 3) in road_col3 and (x, y + 3) in road_col3}"
DALEX,https://github.com/ModelOriented/DALEX/tree/master/python/dalex/dalex/aspect/_predict_triplot/utils.py,,text_abbreviate$130,"def text_abbreviate(text, max_length, skip_chars=""[!@#$=., _^*]"", split_char=""=""):
    if max_length < 1:
        return text
    max_length = int(max_length)
    ## split text to two parts (1st before last split char, second after)
    txt = text.rsplit(split_char, 1)
    # get var_name as 1st part
    var_name = txt[0]

    if len(var_name) <= max_length:
        return text
    # skip skip_chars from var_name
    var_name = re.sub(skip_chars, """", var_name)
    if len(var_name) <= max_length:
        return var_name + ""="" + txt[1]
    abbreviate_index = set()
    # get all upper case chars and numbers
    for i, char in enumerate(var_name):
        if char == char.upper():
            abbreviate_index.add(i)
    if len(abbreviate_index) == 0:
        abbreviate_index.add(0)
    uppers_set = deepcopy(abbreviate_index)
    curr_len = len(abbreviate_index)

    if curr_len < max_length:
        i = 1
        while curr_len < max_length:
            for ind in uppers_set:
                if curr_len < max_length:
                    if ind + i not in abbreviate_index:
                        abbreviate_index.add(ind + i)
                        curr_len += 1

            i += 1
    abbreviate = """"
    for ind in sorted(abbreviate_index):
        abbreviate += var_name[ind]
    return abbreviate[:max_length] + "" ="" + txt[1]","abbreviate_index = set()
for (i, char) in enumerate(var_name):
    if char == char.upper():
        abbreviate_index.add(i)","abbreviate_index = {i for (i, char) in enumerate(var_name) if char == char.upper()}"
textflint,https://github.com/textflint/textflint/tree/master/textflint/generation/transformation/UT/swap_syn_wordnet.py,SwapSynWordNet,_get_candidates$38,"def _get_candidates(self, word, pos=None, n=5):
        r""""""
        Returns a list containing all possible words with 1 character replaced
        by a homoglyph.

        """"""
        synonym = set()
        # filter different pos in get_wsd function
        synsets = self.processor.get_synsets([(word, pos)])[0]

        for syn in synsets:
            for syn_word in syn.lemma_names(lang=self.language):
                if (
                    (syn_word != word)
                    and (""_"" not in syn_word)
                ):
                    # WordNet can suggest phrases that are joined by '_' but we
                    # ignore phrases.
                    synonym.add(syn_word)
        if not synonym:
            return []

        return list(synonym)[:n]","synonym = set()
for syn in synsets:
    for syn_word in syn.lemma_names(lang=self.language):
        if syn_word != word and '_' not in syn_word:
            synonym.add(syn_word)",synonym = {syn_word for syn in synsets for syn_word in syn.lemma_names(lang=self.language) if syn_word != word and '_' not in syn_word}
ludwig,https://github.com/ludwig-ai/ludwig/tree/master/ludwig/visualize.py,,_validate_output_feature_name_from_test_stats$137,"def _validate_output_feature_name_from_test_stats(
        output_feature_name,
        test_stats_per_model
):
    """"""Validate prediction output_feature_name from model test stats and return it as list.

    :param output_feature_name: output_feature_name containing ground truth
    :param test_stats_per_model: list of per model test stats
    :return output_feature_names: list of output_feature_name(s) containing ground truth
    """"""
    output_feature_names_set = set()
    for ls in test_stats_per_model:
        for key in ls:
            output_feature_names_set.add(key)
    try:
        if output_feature_name in output_feature_names_set:
            return [output_feature_name]
        else:
            return output_feature_names_set
    # raised if output_feature_name is emtpy iterable (e.g. [] in set())
    except TypeError:
        return output_feature_names_set","output_feature_names_set = set()
for ls in test_stats_per_model:
    for key in ls:
        output_feature_names_set.add(key)",output_feature_names_set = {key for ls in test_stats_per_model for key in ls}
angr,https://github.com/angr/angr/tree/master/angr/analyses/identifier/functions/printf.py,printf,pre_test$44,"def pre_test(self, func, runner):
        # make sure it prints alphanumeric stuff
        length = 10
        test_str = self.rand_str(length, string.ascii_letters + string.digits)
        test_input = [test_str]
        test_output = [test_str]
        max_steps = len(test_str) * 3 + 20
        stdout = test_str
        test = TestData(test_input, test_output, None, max_steps, expected_stdout=stdout)
        if not runner.test(func, test):
            return False

        # find interesting characters
        test_input = [claripy.BVS(""input"", 10*8)]
        test_output = [None]
        test = TestData(test_input, test_output, None, max_steps)
        s = runner.get_base_call_state(func, test)
        pg = runner.project.factory.simulation_manager(s)
        pg.run(n=18)
        interesting_chars = set()
        for p in pg.active:
            for g in p.history.jump_guards:
                if g.op == ""__ne__"" or g.op == ""__eq__"":
                    for a in g.args:
                        if not a.symbolic:
                            interesting_chars.add(s.solver.eval(a))

        interesting_chars = set(chr(a) for a in interesting_chars if 0 < a < 0x80)
        alphanum = set(string.ascii_letters + string.digits)
        possible_format_specifiers = [c for c in interesting_chars if c not in alphanum and c in string.printable and c not in string.whitespace]
        possible_formats = [c for c in interesting_chars if c in alphanum]

        if len(possible_format_specifiers) > 10:
            # too many to test :(
            return False

        # find the format specifier
        second_str = ""findme""
        for char in possible_format_specifiers:
            if self.format_spec_char is not None:
                break
            for cc in possible_formats:
                test_str = char + cc + ""\n\x00""
                test_input = [test_str, second_str]
                test_output = [test_str, second_str]
                stdout = second_str + ""\n""
                max_steps = 20
                test = TestData(test_input, test_output, None, max_steps, expected_stdout=stdout)
                if runner.test(func, test):
                    self.format_spec_char = char
                    self.string_spec_char = cc
                    break

        # brute force...
        if self.format_spec_char is None:
            second_str = ""findme""
            for char in possible_format_specifiers:
                if self.format_spec_char is not None:
                    break
                for cc in string.ascii_lowercase:
                    if cc in possible_formats:
                        continue
                    test_str = char + cc + ""\n\x00""
                    test_input = [test_str, second_str]
                    test_output = [test_str, second_str]
                    stdout = second_str + ""\n""
                    max_steps = 10
                    test = TestData(test_input, test_output, None, max_steps, expected_stdout=stdout)
                    if runner.test(func, test):
                        self.format_spec_char = char
                        self.string_spec_char = cc
                        break

        if self.format_spec_char is None:
            l.warning(""format spec is none :("")
            return False

        return True","interesting_chars = set()
for p in pg.active:
    for g in p.history.jump_guards:
        if g.op == '__ne__' or g.op == '__eq__':
            for a in g.args:
                if not a.symbolic:
                    interesting_chars.add(s.solver.eval(a))",interesting_chars = {s.solver.eval(a) for p in pg.active for g in p.history.jump_guards if g.op == '__ne__' or g.op == '__eq__' for a in g.args if not a.symbolic}
LinOTP,https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/monitoring.py,MonitorHandler,token_count$48,"def token_count(self, realm_list, status=None):
        """"""
        Give the number of tokens (with status) per realm
        if multiple tokens are given, give summary for all tokens
        tokens which are in multiple realms are only counted once!

        :param realm_list: list of realms which must be queried
        :param status: string which contains requested token status
        :return: dict with the keys: active, inactive,
            assigned, unassigned, total
        """"""

        if not isinstance(realm_list, (list, tuple)):
            realms = [realm_list]
        else:
            # copy realms so that we can delete items safely
            realms = realm_list[:]

        if len(realms) < 1:
            realms = [""/:no realm:/""]

        result = {}
        cond = tuple()

        for realm in realms:
            realm = realm.strip().lower()
            if ""/:no realm:/"" in realm or realm == """":
                #  get all tokenrealm ids
                token_id_tuples = db.session.query(TokenRealm.token_id).all()
                token_ids = set()
                for token_tuple in token_id_tuples:
                    token_ids.add(token_tuple[0])
                # all tokens, which are not references in TokenRealm
                cond += (and_(not_(Token.LinOtpTokenId.in_(token_ids))),)
                if ""/:no realm:/"" in realm:
                    realms.remove(""/:no realm:/"")

            else:
                cond += (
                    and_(
                        TokenRealm.realm_id == Realm.id,
                        Realm.name == realm,
                        TokenRealm.token_id == Token.LinOtpTokenId,
                    ),
                )

        # realm condition:
        r_condition = or_(*cond)

        if ""total"" in status:

            # count all tokens in the given realms
            result[""total""] = (
                db.session.query(Token.LinOtpTokenId)
                .filter(r_condition)
                .distinct()
                .count()
            )

        if ""total users"" in status:

            # according to the token users license spec, we count only the
            # distinct users of all assigned and active tokens in the given
            # realms

            result[""total users""] = (
                db.session.query(Token.LinOtpUserid, Token.LinOtpIdResClass)
                .filter(r_condition)
                .filter(Token.LinOtpUserid != """")
                .filter(Token.LinOtpIsactive)
                .distinct()
                .count()
            )

        for stat in status:

            if stat in [""total users"", ""total""]:
                continue

            conditions = (and_(r_condition),)
            # handle combinations like:
            # status=unassigned & active, unassigned & inactive

            if ""&"" in stat:
                stati = stat.split(""&"")
                if ""assigned"" in stati:
                    conditions += (and_(Token.LinOtpUserid != """"),)
                else:
                    conditions += (and_(Token.LinOtpUserid == """"),)
                if ""active"" in stati:
                    conditions += (and_(Token.LinOtpIsactive),)
                else:
                    conditions += (and_(Token.LinOtpIsactive == False),)
            else:
                # handle single expressions like
                # status=unassigned,active
                if ""assigned"" == stat:
                    conditions += (and_(Token.LinOtpUserid != """"),)
                elif ""unassigned"" == stat:
                    conditions += (and_(Token.LinOtpUserid == """"),)
                elif ""active"" == stat:
                    conditions += (and_(Token.LinOtpIsactive),)
                elif ""inactive"" == stat:
                    conditions += (and_(Token.LinOtpIsactive == False),)

            #  create the final condition as AND of all conditions
            condition = and_(*conditions)

            result[stat] = (
                db.session.query(Token.LinOtpTokenId)
                .filter(condition)
                .distinct()
                .count()
            )

        return result","token_ids = set()
for token_tuple in token_id_tuples:
    token_ids.add(token_tuple[0])",token_ids = {token_tuple[0] for token_tuple in token_id_tuples}
SerpentAI,https://github.com/SerpentAI/SerpentAI/tree/master/serpent/machine_learning/reinforcement_learning/agents/rainbow_dqn_agent.py,RainbowDQNAgent,add_human_observations_to_replay_memory$271,"def add_human_observations_to_replay_memory(self):
        keyboard_key_value_label_mapping = self._generate_keyboard_key_value_mapping()
        input_label_action_space_mapping = dict()

        for label_action_space_value in list(enumerate(self.game_inputs[0][""inputs""])):
            input_label_action_space_mapping[label_action_space_value[1]] = label_action_space_value[0]

        with h5py.File(f""datasets/{self.name}_input_recording.h5"", ""r"") as f:
            timestamps = set()

            for key in f.keys():
                timestamps.add(float(key.split(""-"")[0]))

            for timestamp in sorted(list(timestamps)):
                # Frames
                png_frames = f[f""{timestamp}-frames""].value
                numpy_frames = [skimage.util.img_as_float(skimage.io.imread(io.BytesIO(b))) for b in png_frames]
                pytorch_frames = [torch.tensor(torch.from_numpy(frame), dtype=torch.float32) for frame in numpy_frames]
                frames = torch.stack(pytorch_frames, 0)

                # Action
                action_key = tuple(sorted([b.decode(""utf-8"") for b in f[f""{timestamp}-keyboard-inputs-active""]]))

                if action_key not in keyboard_key_value_label_mapping:
                    continue

                label = keyboard_key_value_label_mapping[action_key]
                action = input_label_action_space_mapping[label]

                # Reward
                reward = f[f""{timestamp}-reward""].value

                # Terminal Flag
                terminal = f[f""{timestamp}-terminal""].value

                self.replay_memory.append(frames, action, reward, terminal)
                self.remaining_observe_steps -= 1

                if self.remaining_observe_steps == 0:
                    self.set_mode(RainbowDQNAgentModes.TRAIN)
                    break","timestamps = set()
for key in f.keys():
    timestamps.add(float(key.split('-')[0]))",timestamps = {float(key.split('-')[0]) for key in f.keys()}
tuna,https://github.com/nschloe/tuna/tree/master/tuna/_runtime_profile.py,,read_runtime_profile$4,"def read_runtime_profile(prof_filename):
    stats = pstats.Stats(prof_filename)

    # # stats.strip_dirs()
    # stats.sort_stats(""cumulative"")
    # # # stats.sort_stats(""time"")
    # stats.print_stats(10)
    # exit(1)
    # s = stats.get_stats_profile()
    # print(s)
    # exit(1)

    # One way of picking the root nodes would be to search through stats.stats.items()
    # and check which don't have parents. This, however, doesn't work if there are loops
    # in the graph which happens, for example, if exec() is called somewhere in the
    # program. For this reason, find all nodes without parents _and_ simply hardcode
    # `<built-in method builtins.exec>`.
    roots = set()
    for key, value in stats.stats.items():
        # The object {('~', 0, ""<method 'disable' of '_lsprof.Profiler' objects>"")}
        # is part of the profile and a root node. Its runtime is usually miniscule and
        # doesn't appear the the final output. Hence, only consider root nodes with a
        # cumtime larger than a threshold.
        # If there is only one remaining root (which is most often the case), one can
        # cut of the artificial ""root"" node. See below.
        if not value[4] and value[3] > 1.0e-5:
            roots.add(key)

    default_roots = [
        (""~"", 0, ""<built-in method builtins.exec>""),
        (""~"", 0, ""<built-in method exec>""),
    ]
    for default_root in default_roots:
        if default_root in stats.stats:
            roots.add(default_root)
    roots = list(roots)

    # Collect children
    children = {key: [] for key in stats.stats.keys()}
    for key, value in stats.stats.items():
        _, _, _, _, parents = value
        for parent in parents:
            children[parent].append(key)

    def populate(key, parent, all_ancestors):
        # stats.stats[key] returns a tuple of length 5 with the following data:
        # [0]: total calls
        # [1]: prim calls
        # [2]: selftime
        # [3]: cumtime
        # [4]: a dictionary of callers
        if parent is None:
            parent_times = {}
            _, _, selftime, cumtime, _ = stats.stats[key]
        else:
            _, _, _, _, parent_times = stats.stats[key]
            _, _, selftime, cumtime = parent_times[parent]

        # Convert the tuple key into a string
        name = ""{}::{}::{}"".format(*key)

        if key in all_ancestors:
            # avoid loops
            return {}

        if len(parent_times) <= 1:
            # Handle children
            c = [populate(child, key, all_ancestors + [key]) for child in children[key]]
            c.append(
                {
                    ""text"": [name + ""::self"", f""{selftime:.3} s""],
                    ""color"": 0,
                    ""value"": selftime,
                }
            )
            return {""text"": [name], ""color"": 0, ""children"": c}

        # More than one parent; we cannot further determine the call times.
        # Terminate the tree here.
        if children[key]:
            c = [
                {
                    ""text"": [
                        ""Possible calls of"",
                        "", "".join(
                            ""{}::{}::{}"".format(*child) for child in children[key]
                        ),
                    ],
                    ""color"": 3,
                    ""value"": cumtime,
                }
            ]
            return {""text"": [name], ""color"": 0, ""children"": c}

        return {""text"": [name, f""{selftime:.3f}""], ""color"": 0, ""value"": selftime}

    if len(roots) == 1:
        data = populate(roots[0], None, [])
    else:
        # If there is more than one root, add an artificial ""main root"" item that is
        # parent to all roots.
        assert len(roots) > 1
        data = {
            ""text"": [""root""],
            ""color"": 0,
            ""children"": [populate(root, None, []) for root in roots],
        }
    return data","roots = set()
for (key, value) in stats.stats.items():
    if not value[4] and value[3] > 1e-05:
        roots.add(key)","roots = {key for (key, value) in stats.stats.items() if not value[4] and value[3] > 1e-05}"
metaworld,https://github.com/rlworkgroup/metaworld/tree/master/tests/integration/test_new_api.py,,test_all_ml45$111,"def test_all_ml45():
    ml45 = ML45()
    train_env_instances = {env_name: env_cls()
                           for (env_name, env_cls) in ml45.train_classes.items()}
    train_env_rand_vecs = check_tasks_unique(ml45.train_tasks,
                                       ml45._train_classes.keys())
    for task in ml45.train_tasks:
        env = train_env_instances[task.env_name]
        env.set_task(task)
        obs = env.reset()
        assert env.random_init == True
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
    for env in train_env_instances.values():
        env.close()

    del train_env_instances

    test_env_instances = {env_name: env_cls()
                          for (env_name, env_cls) in ml45.test_classes.items()}
    test_env_rand_vecs = check_tasks_unique(ml45.test_tasks,
                                       ml45._test_classes.keys())
    for task in ml45.test_tasks:
        env = test_env_instances[task.env_name]
        env.set_task(task)
        obs = env.reset()
        assert np.all(obs[-3:] == np.array([0,0,0]))
        assert env.observation_space.shape == (39,)
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
    for env in test_env_instances.values():
        env.close()
    train_test_rand_vecs = set()
    for rand_vecs in train_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    for rand_vecs in test_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    assert len(train_test_rand_vecs) == (len(ml45.test_classes.keys()) + len(ml45.train_classes.keys())) * metaworld._N_GOALS
    del test_env_instances","train_test_rand_vecs = set()
for rand_vecs in train_env_rand_vecs.values():
    for rand_vec in rand_vecs:
        train_test_rand_vecs.add(tuple(rand_vec))",train_test_rand_vecs = {tuple(rand_vec) for rand_vecs in train_env_rand_vecs.values() for rand_vec in rand_vecs}
sentence-transformers,https://github.com/UKPLab/sentence-transformers/tree/master/sentence_transformers/evaluation/ParaphraseMiningEvaluator.py,ParaphraseMiningEvaluator,__init__$21,"def __init__(self, sentences_map: Dict[str, str], duplicates_list: List[Tuple[str, str]] = None, duplicates_dict: Dict[str, Dict[str, bool]] = None, add_transitive_closure: bool = False, query_chunk_size:int = 5000, corpus_chunk_size:int = 100000, max_pairs: int = 500000, top_k: int = 100, show_progress_bar: bool = False, batch_size: int = 16, name: str = '', write_csv: bool = True):
        """"""

        :param sentences_map: A dictionary that maps sentence-ids to sentences, i.e. sentences_map[id] => sentence.
        :param duplicates_list: Duplicates_list is a list with id pairs [(id1, id2), (id1, id5)] that identifies the duplicates / paraphrases in the sentences_map
        :param duplicates_dict: A default dictionary mapping [id1][id2] to true if id1 and id2 are duplicates. Must be symmetric, i.e., if [id1][id2] => True, then [id2][id1] => True.
        :param add_transitive_closure: If true, it adds a transitive closure, i.e. if dup[a][b] and dup[b][c], then dup[a][c]
        :param query_chunk_size: To identify the paraphrases, the cosine-similarity between all sentence-pairs will be computed. As this might require a lot of memory, we perform a batched computation.  #query_batch_size sentences will be compared against up to #corpus_batch_size sentences. In the default setting, 5000 sentences will be grouped together and compared up-to against 100k other sentences.
        :param corpus_chunk_size: The corpus will be batched, to reduce the memory requirement
        :param max_pairs: We will only extract up to #max_pairs potential paraphrase candidates.
        :param top_k: For each query, we extract the top_k most similar pairs and add it to a sorted list. I.e., for one sentence we cannot find more than top_k paraphrases
        :param show_progress_bar: Output a progress bar
        :param batch_size: Batch size for computing sentence embeddings
        :param name: Name of the experiment
        :param write_csv: Write results to CSV file
        """"""
        self.sentences = []
        self.ids = []

        for id, sentence in sentences_map.items():
            self.sentences.append(sentence)
            self.ids.append(id)

        self.name = name
        self.show_progress_bar = show_progress_bar
        self.batch_size = batch_size
        self.query_chunk_size = query_chunk_size
        self.corpus_chunk_size = corpus_chunk_size
        self.max_pairs = max_pairs
        self.top_k = top_k

        self.duplicates = duplicates_dict if duplicates_dict is not None else defaultdict(lambda: defaultdict(bool))
        if duplicates_list is not None:
            for id1, id2 in duplicates_list:
                if id1 in sentences_map and id2 in sentences_map:
                    self.duplicates[id1][id2] = True
                    self.duplicates[id2][id1] = True


        #Add transitive closure
        if add_transitive_closure:
            self.duplicates = self.add_transitive_closure(self.duplicates)


        positive_key_pairs = set()
        for key1 in self.duplicates:
            for key2 in self.duplicates[key1]:
                if key1 in sentences_map and key2 in sentences_map and (self.duplicates[key1][key2] or self.duplicates[key2][key1]):
                    positive_key_pairs.add(tuple(sorted([key1, key2])))

        self.total_num_duplicates = len(positive_key_pairs)

        if name:
            name = ""_"" + name

        self.csv_file: str = ""paraphrase_mining_evaluation"" + name + ""_results.csv""
        self.csv_headers = [""epoch"", ""steps"", ""precision"", ""recall"", ""f1"", ""threshold"", ""average_precision""]
        self.write_csv = write_csv","positive_key_pairs = set()
for key1 in self.duplicates:
    for key2 in self.duplicates[key1]:
        if key1 in sentences_map and key2 in sentences_map and (self.duplicates[key1][key2] or self.duplicates[key2][key1]):
            positive_key_pairs.add(tuple(sorted([key1, key2])))","positive_key_pairs = {tuple(sorted([key1, key2])) for key1 in self.duplicates for key2 in self.duplicates[key1] if key1 in sentences_map and key2 in sentences_map and (self.duplicates[key1][key2] or self.duplicates[key2][key1])}"
hamster,https://github.com/projecthamster/hamster/tree/master/waflib/Tools/errcheck.py,,check_invalid_constraints$79,"def check_invalid_constraints(self):
	feat = set()
	for x in list(TaskGen.feats.values()):
		feat.union(set(x))
	for (x, y) in TaskGen.task_gen.prec.items():
		feat.add(x)
		feat.union(set(y))
	ext = set()
	for x in TaskGen.task_gen.mappings.values():
		ext.add(x.__name__)
	invalid = ext & feat
	if invalid:
		Logs.error('The methods %r have invalid annotations:  @extension <-> @feature/@before_method/@after_method', list(invalid))

	# the build scripts have been read, so we can check for invalid after/before attributes on task classes
	for cls in list(Task.classes.values()):
		if sys.hexversion > 0x3000000 and issubclass(cls, Task.Task) and isinstance(cls.hcode, str):
			raise Errors.WafError('Class %r has hcode value %r of type <str>, expecting <bytes> (use Utils.h_cmd() ?)' % (cls, cls.hcode))

		for x in ('before', 'after'):
			for y in Utils.to_list(getattr(cls, x, [])):
				if not Task.classes.get(y):
					Logs.error('Erroneous order constraint %r=%r on task class %r', x, y, cls.__name__)
		if getattr(cls, 'rule', None):
			Logs.error('Erroneous attribute ""rule"" on task class %r (rename to ""run_str"")', cls.__name__)","ext = set()
for x in TaskGen.task_gen.mappings.values():
    ext.add(x.__name__)",ext = {x.__name__ for x in TaskGen.task_gen.mappings.values()}
pyproj,https://github.com/pyproj4/pyproj/tree/master/test/test_sync.py,,test_get_transform_grid_list__source_id$61,"def test_get_transform_grid_list__source_id():
    grids = get_transform_grid_list(
        bbox=BBox(170, -90, -170, 90),
        source_id=""us_noaa"",
        include_already_downloaded=True,
    )
    assert len(grids) > 5
    source_ids = set()
    for grid in grids:
        source_ids.add(grid[""properties""][""source_id""])
    assert sorted(source_ids) == [""us_noaa""]","source_ids = set()
for grid in grids:
    source_ids.add(grid['properties']['source_id'])",source_ids = {grid['properties']['source_id'] for grid in grids}
mypy,https://github.com/python/mypy/tree/master/mypyc/analysis/dataflow.py,,non_trivial_sources$397,"def non_trivial_sources(op: Op) -> Set[Value]:
    result = set()
    for source in op.sources():
        if not isinstance(source, Integer):
            result.add(source)
    return result","result = set()
for source in op.sources():
    if not isinstance(source, Integer):
        result.add(source)","result = {source for source in op.sources() if not isinstance(source, Integer)}"
mutagen,https://github.com/quodlibet/mutagen/tree/master/tests/test_tools_mid3iconv.py,TMid3Iconv,test_test_data$45,"def test_test_data(self):
        results = set()
        for codec in CODECS:
            results.add(AMBIGUOUS.decode(codec))
        self.failUnlessEqual(len(results), len(CODECS))","results = set()
for codec in CODECS:
    results.add(AMBIGUOUS.decode(codec))",results = {AMBIGUOUS.decode(codec) for codec in CODECS}
petastorm,https://github.com/uber/petastorm/tree/master/petastorm/tests/test_predicates.py,,test_negation$76,"def test_negation(all_values):
    for values in [{'guid_2', 'guid_1'}, {'guid_5', 'guid_9'}, {'guid_2'}]:
        test_predicate = in_negate(in_set(values, 'volume_guid'))
        included_values = set()
        for val in all_values:
            if test_predicate.do_include({'volume_guid': val}):
                included_values.add(val)
        assert included_values == all_values.difference(values)","included_values = set()
for val in all_values:
    if test_predicate.do_include({'volume_guid': val}):
        included_values.add(val)",included_values = {val for val in all_values if test_predicate.do_include({'volume_guid': val})}
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/mu.py,LambdaManager,diff_tags$444,"def diff_tags(old_tags, new_tags):
        add = {}
        remove = set()
        for k, v in new_tags.items():
            if k not in old_tags or old_tags[k] != v:
                add[k] = v
        for k in old_tags:
            if k not in new_tags:
                remove.add(k)
        return add, list(remove)","remove = set()
for k in old_tags:
    if k not in new_tags:
        remove.add(k)",remove = {k for k in old_tags if k not in new_tags}
nltk,https://github.com/nltk/nltk/tree/master/nltk/classify/decisiontree.py,DecisionTreeClassifier,train$137,"def train(
        labeled_featuresets,
        entropy_cutoff=0.05,
        depth_cutoff=100,
        support_cutoff=10,
        binary=False,
        feature_values=None,
        verbose=False,
    ):
        """"""
        :param binary: If true, then treat all feature/value pairs as
            individual binary features, rather than using a single n-way
            branch for each feature.
        """"""
        # Collect a list of all feature names.
        feature_names = set()
        for featureset, label in labeled_featuresets:
            for fname in featureset:
                feature_names.add(fname)

        # Collect a list of the values each feature can take.
        if feature_values is None and binary:
            feature_values = defaultdict(set)
            for featureset, label in labeled_featuresets:
                for fname, fval in featureset.items():
                    feature_values[fname].add(fval)

        # Start with a stump.
        if not binary:
            tree = DecisionTreeClassifier.best_stump(
                feature_names, labeled_featuresets, verbose
            )
        else:
            tree = DecisionTreeClassifier.best_binary_stump(
                feature_names, labeled_featuresets, feature_values, verbose
            )

        # Refine the stump.
        tree.refine(
            labeled_featuresets,
            entropy_cutoff,
            depth_cutoff - 1,
            support_cutoff,
            binary,
            feature_values,
            verbose,
        )

        # Return it
        return tree","feature_names = set()
for (featureset, label) in labeled_featuresets:
    for fname in featureset:
        feature_names.add(fname)","feature_names = {fname for (featureset, label) in labeled_featuresets for fname in featureset}"
metaworld,https://github.com/rlworkgroup/metaworld/tree/master/tests/integration/test_new_api.py,,test_all_ml10$62,"def test_all_ml10():
    ml10 = ML10()
    train_env_instances = {env_name: env_cls()
                           for (env_name, env_cls) in ml10.train_classes.items()}
    train_env_rand_vecs = check_tasks_unique(ml10.train_tasks,
                                       ml10._train_classes.keys())
    for task in ml10.train_tasks:
        env = train_env_instances[task.env_name]
        env.set_task(task)
        env.reset()
        assert env.random_init == True
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
        step_env(env, max_path_length=STEPS, render=False)
    for env in train_env_instances.values():
        env.close()
    del train_env_instances

    test_env_instances = {env_name: env_cls()
                          for (env_name, env_cls) in ml10.test_classes.items()}
    test_env_rand_vecs = check_tasks_unique(ml10.test_tasks,
                                       ml10._test_classes.keys())
    for task in ml10.test_tasks:
        env = test_env_instances[task.env_name]
        env.set_task(task)
        env.reset()
        assert env.random_init == True
        old_obj_init = env.obj_init_pos
        old_target_pos = env._target_pos
        step_env(env, max_path_length=STEPS, render=False)
        assert np.all(np.allclose(old_obj_init, env.obj_init_pos))
        assert np.all(np.allclose(old_target_pos, env._target_pos))
        step_env(env, max_path_length=STEPS, render=False)
    for env in test_env_instances.values():
        env.close()
    train_test_rand_vecs = set()
    for rand_vecs in train_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    for rand_vecs in test_env_rand_vecs.values():
        for rand_vec in rand_vecs:
            train_test_rand_vecs.add(tuple(rand_vec))
    assert len(train_test_rand_vecs) == (len(ml10.test_classes.keys()) + len(ml10.train_classes.keys())) * metaworld._N_GOALS
    del test_env_instances","train_test_rand_vecs = set()
for rand_vecs in train_env_rand_vecs.values():
    for rand_vec in rand_vecs:
        train_test_rand_vecs.add(tuple(rand_vec))",train_test_rand_vecs = {tuple(rand_vec) for rand_vecs in train_env_rand_vecs.values() for rand_vec in rand_vecs}
fuck-coding-interviews,https://github.com/vinta/fuck-coding-interviews/tree/master/problems/pangrams.py,,pangrams$12,"def pangrams(s):
    alphabet = set()
    for char in s.lower():
        if char != ' ':
            alphabet.add(char)

    if len(alphabet) == 26:
        return 'pangram'
    else:
        return 'not pangram'","alphabet = set()
for char in s.lower():
    if char != ' ':
        alphabet.add(char)",alphabet = {char for char in s.lower() if char != ' '}
Diamond,https://github.com/python-diamond/Diamond/tree/master/src/collectors/filestat/filestat.py,FilestatCollector,get_userlist$181,"def get_userlist(self):
        """"""
        This collects all the users with open files on the system, and filters
        based on the variables user_include and user_exclude
        """"""
    # convert user/group  lists to arrays if strings
        if isinstance(self.config['user_include'], basestring):
            self.config['user_include'] = self.config['user_include'].split()
        if isinstance(self.config['user_exclude'], basestring):
            self.config['user_exclude'] = self.config['user_exclude'].split()
        if isinstance(self.config['group_include'], basestring):
            self.config['group_include'] = self.config['group_include'].split()
        if isinstance(self.config['group_exclude'], basestring):
            self.config['group_exclude'] = self.config['group_exclude'].split()

        rawusers = set()
        for line in self.get_output('L'):
            if line.startswith('L'):
                rawusers.add(line[1:].rstrip('\n'))
        rawusers = list(rawusers)

        userlist = []

        # remove any not on the user include list
        if ((self.config['user_include'] is None or
             len(self.config['user_include']) == 0)):
            userlist = rawusers
        else:
            # only work with specified include list, which is added at the end
            userlist = []

        # add any user in the group include list
        addedByGroup = []
        if ((self.config['group_include'] is not None and
             len(self.config['group_include']) > 0)):
            for u in rawusers:
                self.log.info(u)
                # get list of groups of user
                user_groups = os.popen(""id -Gn %s"" % (u)).read().split()
                for gi in self.config['group_include']:
                    if gi in user_groups and u not in userlist:
                        userlist.append(u)
                        addedByGroup.append(u)
                        break

        # remove any user in the exclude group list
        if ((self.config['group_exclude'] is not None and
             len(self.config['group_exclude']) > 0)):
            # create tmp list to iterate over while editing userlist
            tmplist = userlist[:]
            for u in tmplist:
                # get list of groups of user
                groups = os.popen(""id -Gn %s"" % (u)).read().split()
                for gi in self.config['group_exclude']:
                    if gi in groups:
                        userlist.remove(u)
                        break

        # remove any that aren't within the uid limits
        # make sure uid_min/max are ints
        self.config['uid_min'] = int(self.config['uid_min'])
        self.config['uid_max'] = int(self.config['uid_max'])
        tmplist = userlist[:]
        for u in tmplist:
            if ((self.config['user_include'] is None or
                 u not in self.config['user_include'])):
                if u not in addedByGroup:
                    uid = int(os.popen(""id -u %s"" % (u)).read())
                    if ((uid < self.config['uid_min'] and
                         self.config['uid_min'] is not None and
                         u in userlist)):
                        userlist.remove(u)
                    if ((uid > self.config['uid_max'] and
                         self.config['uid_max'] is not None and
                         u in userlist)):
                        userlist.remove(u)

        # add users that are in the users include list
        if ((self.config['user_include'] is not None and
             len(self.config['user_include']) > 0)):
            for u in self.config['user_include']:
                if u in rawusers and u not in userlist:
                    userlist.append(u)

        # remove any that is on the user exclude list
        if ((self.config['user_exclude'] is not None and
             len(self.config['user_exclude']) > 0)):
            for u in self.config['user_exclude']:
                if u in userlist:
                    userlist.remove(u)

        return userlist","rawusers = set()
for line in self.get_output('L'):
    if line.startswith('L'):
        rawusers.add(line[1:].rstrip('\n'))",rawusers = {line[1:].rstrip('\n') for line in self.get_output('L') if line.startswith('L')}
nltk,https://github.com/nltk/nltk/tree/master/nltk/test/unit/translate/test_ibm_model.py,TestIBMModel,test_neighboring_finds_neighbor_alignments$137,"def test_neighboring_finds_neighbor_alignments(self):
        # arrange
        a_info = AlignmentInfo(
            (0, 3, 2),
            (None, ""des"", ""œufs"", ""verts""),
            (""UNUSED"", ""green"", ""eggs""),
            [[], [], [2], [1]],
        )
        ibm_model = IBMModel([])

        # act
        neighbors = ibm_model.neighboring(a_info)

        # assert
        neighbor_alignments = set()
        for neighbor in neighbors:
            neighbor_alignments.add(neighbor.alignment)
        expected_alignments = {
            # moves
            (0, 0, 2),
            (0, 1, 2),
            (0, 2, 2),
            (0, 3, 0),
            (0, 3, 1),
            (0, 3, 3),
            # swaps
            (0, 2, 3),
            # original alignment
            (0, 3, 2),
        }
        self.assertEqual(neighbor_alignments, expected_alignments)","neighbor_alignments = set()
for neighbor in neighbors:
    neighbor_alignments.add(neighbor.alignment)",neighbor_alignments = {neighbor.alignment for neighbor in neighbors}
lbry-sdk,https://github.com/lbryio/lbry-sdk/tree/master/lbry/testcase.py,CommandTestCase,get_all_addresses$459,"def get_all_addresses(tx):
        addresses = set()
        for txi in tx['inputs']:
            addresses.add(txi['address'])
        for txo in tx['outputs']:
            addresses.add(txo['address'])
        return list(addresses)","addresses = set()
for txi in tx['inputs']:
    addresses.add(txi['address'])",addresses = {txi['address'] for txi in tx['inputs']}
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/framework.py,IrGraph,draw$5141,"def draw(self, save_path, name, marked_nodes=None, remove_ctr_var=True):
        """"""
        Draw the graph. If `dot` command is installed, the drawn graph
        will be saved as pdf file type, otherwise dot file type is used.

        Args:
            save_path(str): the save path of drawn graph.
            name(str): the name of drawn graph.
            marked_nodes(set(IrNode)): nodes that are needed to be marked.
            Default value is None.
            remove_ctr_var(bool): If it is set True, all control variable nodes
            in the graph will be removed. Default value is True.
        """"""

        def _convert_to_pdf(dot_file_path):
            pdf_save_path = os.path.splitext(dot_file_path)[0] + '.pdf'
            exited_code = subprocess.call(
                'dot -Tpdf ' + dot_file_path + ' -o ' + pdf_save_path,
                shell=True,
            )
            if exited_code != 0:
                print('The dot command is needed for creating pdf files.')
                print(
                    'The {} is saved as the dot filetype.'.format(dot_file_path)
                )

        remove_ctr_vars = set()
        if remove_ctr_var:
            for node in self.all_var_nodes():
                if node.is_ctrl_var():
                    remove_ctr_vars.add(node)
            self.safe_remove_nodes(remove_ctr_vars)
        print('Total ops num = {}.'.format(len(self.all_op_nodes())))

        if marked_nodes is not None:
            if not isinstance(marked_nodes, set):
                if isinstance(marked_nodes, Iterable):
                    marked_nodes = set(marked_nodes)
                else:
                    marked_nodes = {marked_nodes}
            marked_nodes = {n.node for n in marked_nodes}
            remove_ctr_vars = {n.node for n in remove_ctr_vars}
            marked_nodes = marked_nodes - remove_ctr_vars
            if self.graph.has('__graphviz__marked_node__'):
                self.graph.erase('__graphviz__marked_node__')
            self.graph.set('__graphviz__marked_node__', marked_nodes)
        if not os.path.exists(save_path):
            os.makedirs(save_path)
        viz_dot_path = os.path.join(save_path, name) + '.dot'
        viz_pass = core.get_pass('graph_viz_pass')
        viz_pass.set('graph_viz_path', viz_dot_path)
        viz_pass.apply(self.graph)
        _convert_to_pdf(viz_dot_path)","remove_ctr_vars = set()
for node in self.all_var_nodes():
    if node.is_ctrl_var():
        remove_ctr_vars.add(node)",remove_ctr_vars = {node for node in self.all_var_nodes() if node.is_ctrl_var()}
unknown-horizons,https://github.com/unknown-horizons/unknown-horizons/tree/master/horizons/ai/aiplayer/productionbuilder.py,ProductionBuilder,_init_road_connectivity_cache$95,"def _init_road_connectivity_cache(self):
		self.road_connectivity_cache = PotentialRoadConnectivityCache(self)
		coords_set = set()
		for coords in self.plan:
			coords_set.add(coords)
		for coords in self.land_manager.roads:
			coords_set.add(coords)
		self.road_connectivity_cache.modify_area(list(sorted(coords_set)))","coords_set = set()
for coords in self.plan:
    coords_set.add(coords)",coords_set = {coords for coords in self.plan}
Mobile-Security-Framework-MobSF,https://github.com/MobSF/Mobile-Security-Framework-MobSF/tree/master/mobsf/StaticAnalyzer/views/ios/macho_analysis.py,Checksec,has_canary$200,"def has_canary(self):
        stk_check = '___stack_chk_fail'
        stk_guard = '___stack_chk_guard'
        ipt_list = set()
        for ipt in self.macho.imported_functions:
            ipt_list.add(str(ipt))
        return stk_check in ipt_list and stk_guard in ipt_list","ipt_list = set()
for ipt in self.macho.imported_functions:
    ipt_list.add(str(ipt))",ipt_list = {str(ipt) for ipt in self.macho.imported_functions}
scikit-image,https://github.com/scikit-image/scikit-image/tree/master/skimage/io/manage_plugins.py,,find_available_plugins$137,"def find_available_plugins(loaded=False):
    """"""List available plugins.

    Parameters
    ----------
    loaded : bool
        If True, show only those plugins currently loaded.  By default,
        all plugins are shown.

    Returns
    -------
    p : dict
        Dictionary with plugin names as keys and exposed functions as
        values.

    """"""
    active_plugins = set()
    for plugin_func in plugin_store.values():
        for plugin, func in plugin_func:
            active_plugins.add(plugin)

    d = {}
    for plugin in plugin_provides:
        if not loaded or plugin in active_plugins:
            d[plugin] = [f for f in plugin_provides[plugin]
                         if not f.startswith('_')]

    return d","active_plugins = set()
for plugin_func in plugin_store.values():
    for (plugin, func) in plugin_func:
        active_plugins.add(plugin)","active_plugins = {plugin for plugin_func in plugin_store.values() for (plugin, func) in plugin_func}"
viztracer,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_regression.py,TestIssue160,check_func$271,"def check_func(data):
            pids = set()
            for entry in data[""traceEvents""]:
                pids.add(entry[""pid""])
            self.assertEqual(len(pids), 2)","pids = set()
for entry in data['traceEvents']:
    pids.add(entry['pid'])",pids = {entry['pid'] for entry in data['traceEvents']}
clusterfuzz,https://github.com/google/clusterfuzz/tree/master/src/clusterfuzz/_internal/bot/fuzzers/dictionary_manager.py,,merge_dictionary_files$112,"def merge_dictionary_files(original_dictionary_path,
                           recommended_dictionary_path, merged_dictionary_path):
  """"""Merge a list of dictionaries with given paths into a singe dictionary.""""""
  if original_dictionary_path and os.path.exists(original_dictionary_path):
    merged_dictionary_data = utils.read_data_from_file(
        original_dictionary_path, eval_data=False).decode('utf-8')
  else:
    merged_dictionary_data = ''

  recommended_dictionary_lines = utils.read_data_from_file(
      recommended_dictionary_path,
      eval_data=False).decode('utf-8').splitlines()

  dictionary_lines_to_add = set()
  for line in recommended_dictionary_lines:
    if line not in merged_dictionary_data:
      dictionary_lines_to_add.add(line)

  merged_dictionary_data += '\n%s\n' % RECOMMENDED_DICTIONARY_HEADER

  merged_dictionary_data += '\n'.join(dictionary_lines_to_add)
  utils.write_data_to_file(merged_dictionary_data, merged_dictionary_path)","dictionary_lines_to_add = set()
for line in recommended_dictionary_lines:
    if line not in merged_dictionary_data:
        dictionary_lines_to_add.add(line)",dictionary_lines_to_add = {line for line in recommended_dictionary_lines if line not in merged_dictionary_data}
pycorrector,https://github.com/shibing624/pycorrector/tree/master/pycorrector/corrector.py,Corrector,_confusion_word_set$162,"def _confusion_word_set(self, word):
        confusion_word_set = set()
        candidate_words = list(self.known(edit_distance_word(word, self.cn_char_set)))
        for candidate_word in candidate_words:
            if pypinyin.lazy_pinyin(candidate_word) == pypinyin.lazy_pinyin(word):
                # same pinyin
                confusion_word_set.add(candidate_word)
        return confusion_word_set","confusion_word_set = set()
for candidate_word in candidate_words:
    if pypinyin.lazy_pinyin(candidate_word) == pypinyin.lazy_pinyin(word):
        confusion_word_set.add(candidate_word)",confusion_word_set = {candidate_word for candidate_word in candidate_words if pypinyin.lazy_pinyin(candidate_word) == pypinyin.lazy_pinyin(word)}
word_cloud,https://github.com/amueller/word_cloud/tree/master//versioneer.py,,do_setup$1697,"def do_setup():
    """"""Main VCS-independent setup function for installing Versioneer.""""""
    root = get_root()
    try:
        cfg = get_config_from_root(root)
    except (EnvironmentError, configparser.NoSectionError,
            configparser.NoOptionError) as e:
        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):
            print(""Adding sample versioneer config to setup.cfg"",
                  file=sys.stderr)
            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:
                f.write(SAMPLE_CONFIG)
        print(CONFIG_ERROR, file=sys.stderr)
        return 1

    print("" creating %s"" % cfg.versionfile_source)
    with open(cfg.versionfile_source, ""w"") as f:
        LONG = LONG_VERSION_PY[cfg.VCS]
        f.write(LONG % {""DOLLAR"": ""$"",
                        ""STYLE"": cfg.style,
                        ""TAG_PREFIX"": cfg.tag_prefix,
                        ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,
                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
                        })

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),
                       ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy, ""r"") as f:
                old = f.read()
        except EnvironmentError:
            old = """"
        if INIT_PY_SNIPPET not in old:
            print("" appending to %s"" % ipy)
            with open(ipy, ""a"") as f:
                f.write(INIT_PY_SNIPPET)
        else:
            print("" %s unmodified"" % ipy)
    else:
        print("" %s doesn't exist, ok"" % ipy)
        ipy = None

    # Make sure both the top-level ""versioneer.py"" and versionfile_source
    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so
    # they'll be copied into source distributions. Pip won't be able to
    # install the package without this.
    manifest_in = os.path.join(root, ""MANIFEST.in"")
    simple_includes = set()
    try:
        with open(manifest_in, ""r"") as f:
            for line in f:
                if line.startswith(""include ""):
                    for include in line.split()[1:]:
                        simple_includes.add(include)
    except EnvironmentError:
        pass
    # That doesn't cover everything MANIFEST.in can do
    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so
    # it might give some false negatives. Appending redundant 'include'
    # lines is safe, though.
    if ""versioneer.py"" not in simple_includes:
        print("" appending 'versioneer.py' to MANIFEST.in"")
        with open(manifest_in, ""a"") as f:
            f.write(""include versioneer.py\n"")
    else:
        print("" 'versioneer.py' already in MANIFEST.in"")
    if cfg.versionfile_source not in simple_includes:
        print("" appending versionfile_source ('%s') to MANIFEST.in"" %
              cfg.versionfile_source)
        with open(manifest_in, ""a"") as f:
            f.write(""include %s\n"" % cfg.versionfile_source)
    else:
        print("" versionfile_source already in MANIFEST.in"")

    # Make VCS-specific changes. For git, this means creating/changing
    # .gitattributes to mark _version.py for export-subst keyword
    # substitution.
    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)
    return 0","simple_includes = set()
for line in f:
    if line.startswith('include '):
        for include in line.split()[1:]:
            simple_includes.add(include)",simple_includes = {include for line in f if line.startswith('include ') for include in line.split()[1:]}
awx,https://github.com/ansible/awx/tree/master/awx/sso/fields.py,SAMLOrgInfoField,to_internal_value$655,"def to_internal_value(self, data):
        data = super(SAMLOrgInfoField, self).to_internal_value(data)
        invalid_keys = set()
        for key in data.keys():
            if not re.match(r'^[a-z]{2}(?:-[a-z]{2})??$', key, re.I):
                invalid_keys.add(key)
        if invalid_keys:
            invalid_keys = sorted(list(invalid_keys))
            keys_display = json.dumps(invalid_keys).lstrip('[').rstrip(']')
            self.fail('invalid_lang_code', invalid_lang_codes=keys_display)
        return data","invalid_keys = set()
for key in data.keys():
    if not re.match('^[a-z]{2}(?:-[a-z]{2})??$', key, re.I):
        invalid_keys.add(key)","invalid_keys = {key for key in data.keys() if not re.match('^[a-z]{2}(?:-[a-z]{2})??$', key, re.I)}"
forseti-security,https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/notifier/notifiers/inventory_summary.py,InventorySummary,_get_gsuite_dwd_status$211,"def _get_gsuite_dwd_status(summary_data):
        """"""Get the status of whether G Suite DwD is enabled or not.

        Args:
            summary_data (list): Summary of inventory data as a list of dicts.
                Example: [{resource_type, count}, {}, {}, ...]

        Returns:
            str: disabled or enabled.
        """"""
        gsuite_types = set(['gsuite_user'])
        summary_data_keys = set()
        if summary_data is None:
            return 'disabled'

        for resource in summary_data:
            summary_data_keys.add(resource['resource_type'])

        if gsuite_types.issubset(summary_data_keys):
            return 'enabled'
        return 'disabled'","summary_data_keys = set()
for resource in summary_data:
    summary_data_keys.add(resource['resource_type'])",summary_data_keys = {resource['resource_type'] for resource in summary_data}
pifuhd,https://github.com/facebookresearch/pifuhd/tree/master/lib/model/HGPIFuNetwNML.py,HGPIFuNetwNML,loadFromHGHPIFu$73,"def loadFromHGHPIFu(self, net):
        hgnet = net.image_filter
        pretrained_dict = hgnet.state_dict()            
        model_dict = self.image_filter.state_dict()

        pretrained_dict = {k: v for k, v in hgnet.state_dict().items() if k in model_dict}                    

        for k, v in pretrained_dict.items():                      
            if v.size() == model_dict[k].size():
                model_dict[k] = v

        not_initialized = set()
               
        for k, v in model_dict.items():
            if k not in pretrained_dict or v.size() != pretrained_dict[k].size():
                not_initialized.add(k.split('.')[0])
        
        print('not initialized', sorted(not_initialized))
        self.image_filter.load_state_dict(model_dict) 

        pretrained_dict = net.mlp.state_dict()            
        model_dict = self.mlp.state_dict()

        pretrained_dict = {k: v for k, v in net.mlp.state_dict().items() if k in model_dict}                    

        for k, v in pretrained_dict.items():                      
            if v.size() == model_dict[k].size():
                model_dict[k] = v

        not_initialized = set()
               
        for k, v in model_dict.items():
            if k not in pretrained_dict or v.size() != pretrained_dict[k].size():
                not_initialized.add(k.split('.')[0])
        
        print('not initialized', sorted(not_initialized))
        self.mlp.load_state_dict(model_dict)","not_initialized = set()
for (k, v) in model_dict.items():
    if k not in pretrained_dict or v.size() != pretrained_dict[k].size():
        not_initialized.add(k.split('.')[0])","not_initialized = {k.split('.')[0] for (k, v) in model_dict.items() if k not in pretrained_dict or v.size() != pretrained_dict[k].size()}"
pifuhd,https://github.com/facebookresearch/pifuhd/tree/master/lib/model/HGPIFuNetwNML.py,HGPIFuNetwNML,loadFromHGHPIFu$73,"def loadFromHGHPIFu(self, net):
        hgnet = net.image_filter
        pretrained_dict = hgnet.state_dict()            
        model_dict = self.image_filter.state_dict()

        pretrained_dict = {k: v for k, v in hgnet.state_dict().items() if k in model_dict}                    

        for k, v in pretrained_dict.items():                      
            if v.size() == model_dict[k].size():
                model_dict[k] = v

        not_initialized = set()
               
        for k, v in model_dict.items():
            if k not in pretrained_dict or v.size() != pretrained_dict[k].size():
                not_initialized.add(k.split('.')[0])
        
        print('not initialized', sorted(not_initialized))
        self.image_filter.load_state_dict(model_dict) 

        pretrained_dict = net.mlp.state_dict()            
        model_dict = self.mlp.state_dict()

        pretrained_dict = {k: v for k, v in net.mlp.state_dict().items() if k in model_dict}                    

        for k, v in pretrained_dict.items():                      
            if v.size() == model_dict[k].size():
                model_dict[k] = v

        not_initialized = set()
               
        for k, v in model_dict.items():
            if k not in pretrained_dict or v.size() != pretrained_dict[k].size():
                not_initialized.add(k.split('.')[0])
        
        print('not initialized', sorted(not_initialized))
        self.mlp.load_state_dict(model_dict)","not_initialized = set()
for (k, v) in model_dict.items():
    if k not in pretrained_dict or v.size() != pretrained_dict[k].size():
        not_initialized.add(k.split('.')[0])","not_initialized = {k.split('.')[0] for (k, v) in model_dict.items() if k not in pretrained_dict or v.size() != pretrained_dict[k].size()}"
qiskit-terra,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/pulse/test_transforms.py,TestAddImplicitAcquires,test_dont_add_all$234,"def test_dont_add_all(self):
        """"""Test that acquires aren't added if no qubits in the sublist aren't being acquired.""""""
        sched = transforms.add_implicit_acquires(self.sched, [[4, 5], [0, 2], [1, 3]])
        acquired_qubits = set()
        for _, inst in sched.instructions:
            if isinstance(inst, Acquire):
                acquired_qubits.add(inst.acquire.index)
        self.assertEqual(acquired_qubits, {0, 1, 2, 3})","acquired_qubits = set()
for (_, inst) in sched.instructions:
    if isinstance(inst, Acquire):
        acquired_qubits.add(inst.acquire.index)","acquired_qubits = {inst.acquire.index for (_, inst) in sched.instructions if isinstance(inst, Acquire)}"
unilm,https://github.com/microsoft/unilm/tree/master/infoxlm/fairseq/scripts/compare_namespaces.py,,main$7,"def main():

    ns1 = eval(input('Namespace 1: '))
    ns2 = eval(input('Namespace 2: '))

    def keys(ns):
        ks = set()
        for k in dir(ns):
            if not k.startswith('_'):
                ks.add(k)
        return ks

    k1 = keys(ns1)
    k2 = keys(ns2)

    def print_keys(ks, ns1, ns2=None):
        for k in ks:
            if ns2 is None:
                print('{}\t{}'.format(k, getattr(ns1, k, None)))
            else:
                print('{}\t{}\t{}'.format(k, getattr(ns1, k, None), getattr(ns2, k, None)))

    print('Keys unique to namespace 1:')
    print_keys(k1 - k2, ns1)
    print()

    print('Keys unique to namespace 2:')
    print_keys(k2 - k1, ns2)
    print()

    print('Overlapping keys with different values:')
    ks = [k for k in k1 & k2 if getattr(ns1, k, 'None') != getattr(ns2, k, 'None')]
    print_keys(ks, ns1, ns2)
    print()","ks = set()
for k in dir(ns):
    if not k.startswith('_'):
        ks.add(k)",ks = {k for k in dir(ns) if not k.startswith('_')}
opencensus-python,https://github.com/census-instrumentation/opencensus-python/tree/master/opencensus/metrics/export/gauge.py,Registry,get_metrics$501,"def get_metrics(self):
        """"""Get a metric for each gauge in the registry at the current time.

        :rtype: set(:class:`opencensus.metrics.export.metric.Metric`)
        :return: A set of `Metric`s, one for each registered gauge.
        """"""
        now = datetime.utcnow()
        metrics = set()
        for gauge in self.gauges.values():
            metrics.add(gauge.get_metric(now))
        return metrics","metrics = set()
for gauge in self.gauges.values():
    metrics.add(gauge.get_metric(now))",metrics = {gauge.get_metric(now) for gauge in self.gauges.values()}
devpi,https://github.com/devpi/devpi/tree/master/.ci/cleanup_devpi_indices.py,,get_release_dates$26,"def get_release_dates(baseurl, username, indexname, projectname):
    response = session.get(baseurl + username + ""/"" + indexname + ""/"" + projectname)
    assert response.status_code == 200
    result = response.json()[""result""]
    dates = set()
    for value in result.values():
        for link in value.get(""+links"", []):
            for log in link.get(""log"", []):
                dates.add(tuple(log[""when""]))
    return dates","dates = set()
for value in result.values():
    for link in value.get('+links', []):
        for log in link.get('log', []):
            dates.add(tuple(log['when']))","dates = {tuple(log['when']) for value in result.values() for link in value.get('+links', []) for log in link.get('log', [])}"
tvm,https://github.com/apache/tvm/tree/master/python/tvm/relay/frontend/caffe2.py,Caffe2NetDef,from_caffe2$422,"def from_caffe2(self, init_net, predict_net):
        """"""Construct Relay expression from caffe2 graph.

        Parameters
        ----------
        init_net : protobuf object
        predict_net : protobuf object

        Returns
        -------
        mod : tvm.IRModule
            The module that optimizations will be performed on.

        params : dict
            A dict of name: tvm.nd.array pairs, used as pretrained weights
        """"""
        # pylint: disable=import-outside-toplevel
        from caffe2.python import workspace

        workspace.RunNetOnce(init_net)

        # Input
        input_name = predict_net.op[0].input[0]

        # Params
        self._params = {}
        used_blobs = set()
        for c2_op in predict_net.op:
            for i in c2_op.input:
                used_blobs.add(i)
        for blob in workspace.Blobs():
            if blob in used_blobs and blob != input_name:
                self._params[blob] = _nd.array(workspace.FetchBlob(blob))

        # Variables
        self._nodes = {}
        for blob in predict_net.external_input:
            if blob in self._params:
                self._nodes[blob] = new_var(
                    blob, shape=self._params[blob].shape, dtype=self._params[blob].dtype
                )
            else:
                shape = self._shape[blob] if blob in self._shape else ()
                if isinstance(self._dtype, dict) and blob in self._dtype:
                    dtype = str(self._dtype[blob])
                elif isinstance(self._dtype, str):
                    dtype = self._dtype
                else:
                    dtype = ""float32""
                self._nodes[blob] = new_var(blob, shape=shape, dtype=dtype)

        # Ops
        for c2_op in predict_net.op:
            for blob in c2_op.output:
                self._ops[blob] = c2_op

        for c2_op in predict_net.op:
            self._process_op(c2_op)

        # Outputs
        out = []
        for blob in predict_net.external_output:
            out.append(self._nodes[blob])

        if len(out) > 1:
            outputs = _expr.Tuple(out)
        else:
            outputs = out[0]

        func = _function.Function(analysis.free_vars(outputs), outputs)
        self._mod[""main""] = func

        return self._mod, self._params","used_blobs = set()
for c2_op in predict_net.op:
    for i in c2_op.input:
        used_blobs.add(i)",used_blobs = {i for c2_op in predict_net.op for i in c2_op.input}
ARL,https://github.com/TophantTechnology/ARL/tree/master/app/tasks/scheduler.py,DomainExecutor,run$100,"def run(self):
        self.update_task_field(""start_time"", utils.curr_date())
        self.domain_fetch()
        for domain_info in self.domain_info_list:
            self.domain_set.add(domain_info.domain)

        self.set_scope_domain()

        new_domain_set = self.domain_set - self.scope_domain_set
        self.new_domain_set = new_domain_set

        self.set_wildcard_ip_set()

        self.set_domain_info_list()

        #仅仅对新增域名保留
        self.start_ip_fetch()
        self.start_site_fetch()

        # cidr ip 结果统计，插入cip 集合中
        self.insert_cip_stat()

        # 任务指纹信息统计
        self.insert_finger_stat()
        # 任务结果统计
        self.insert_task_stat()

        self.update_task_field(""status"", TaskStatus.DONE)
        self.update_task_field(""end_time"", utils.curr_date())

        ret_new_domain_set = set()
        for domain_info in self.domain_info_list:
            ret_new_domain_set.add(domain_info.domain)

        return ret_new_domain_set","ret_new_domain_set = set()
for domain_info in self.domain_info_list:
    ret_new_domain_set.add(domain_info.domain)",ret_new_domain_set = {domain_info.domain for domain_info in self.domain_info_list}
docassemble,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,Question,sub_fields_used$6151,"def sub_fields_used(self):
        all_fields_used = set()
        for var_name in self.fields_used:
            all_fields_used.add(var_name)
        if len(self.fields) > 0 and hasattr(self.fields[0], 'choices'):
            for choice in self.fields[0].choices:
                if isinstance(choice['key'], Question):
                    all_fields_used.update(choice['key'].sub_fields_used())
        return all_fields_used","all_fields_used = set()
for var_name in self.fields_used:
    all_fields_used.add(var_name)",all_fields_used = {var_name for var_name in self.fields_used}
aws-cli,https://github.com/aws/aws-cli/tree/master/awscli/text.py,,_all_scalar_keys$85,"def _all_scalar_keys(list_of_dicts):
    keys_seen = set()
    for item_dict in list_of_dicts:
        for key, value in item_dict.items():
            if not isinstance(value, (dict, list)):
                keys_seen.add(key)
    return list(sorted(keys_seen))","keys_seen = set()
for item_dict in list_of_dicts:
    for (key, value) in item_dict.items():
        if not isinstance(value, (dict, list)):
            keys_seen.add(key)","keys_seen = {key for item_dict in list_of_dicts for (key, value) in item_dict.items() if not isinstance(value, (dict, list))}"
word2vec-tutorial,https://github.com/zake7749/word2vec-tutorial/tree/master//segment.py,,main$6,"def main():

    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

    # jieba custom setting.
    jieba.set_dictionary('jieba_dict/dict.txt.big')

    # load stopwords set
    stopword_set = set()
    with open('jieba_dict/stopwords.txt','r', encoding='utf-8') as stopwords:
        for stopword in stopwords:
            stopword_set.add(stopword.strip('\n'))

    output = open('wiki_seg.txt', 'w', encoding='utf-8')
    with open('wiki_zh_tw.txt', 'r', encoding='utf-8') as content :
        for texts_num, line in enumerate(content):
            line = line.strip('\n')
            words = jieba.cut(line, cut_all=False)
            for word in words:
                if word not in stopword_set:
                    output.write(word + ' ')
            output.write('\n')

            if (texts_num + 1) % 10000 == 0:
                logging.info(""已完成前 %d 行的斷詞"" % (texts_num + 1))
    output.close()","stopword_set = set()
for stopword in stopwords:
    stopword_set.add(stopword.strip('\n'))",stopword_set = {stopword.strip('\n') for stopword in stopwords}
imgaug,https://github.com/aleju/imgaug/tree/master/test/augmenters/test_size.py,TestResize,test_decrease_size_by_tuples_of_floats__one_per_side$1551,"def test_decrease_size_by_tuples_of_floats__one_per_side(self):
        image2d = self.image2d[0:4, 0:4]
        image3d = self.image3d[0:4, 0:4, :]
        aug = iaa.Resize({""height"": (0.76, 1.0), ""width"": (0.76, 1.0)})
        not_seen2d = set()
        not_seen3d = set()
        for hsize in sm.xrange(3, 4+1):
            for wsize in sm.xrange(3, 4+1):
                not_seen2d.add((hsize, wsize))
        for hsize in sm.xrange(3, 4+1):
            for wsize in sm.xrange(3, 4+1):
                not_seen3d.add((hsize, wsize, 3))
        possible2d = set(list(not_seen2d))
        possible3d = set(list(not_seen3d))
        for _ in sm.xrange(100):
            observed2d = aug.augment_image(image2d)
            observed3d = aug.augment_image(image3d)
            assert observed2d.shape in possible2d
            assert observed3d.shape in possible3d
            if observed2d.shape in not_seen2d:
                not_seen2d.remove(observed2d.shape)
            if observed3d.shape in not_seen3d:
                not_seen3d.remove(observed3d.shape)
            if not not_seen2d and not not_seen3d:
                break
        assert not not_seen2d
        assert not not_seen3d","not_seen2d = set()
for hsize in sm.xrange(3, 4 + 1):
    for wsize in sm.xrange(3, 4 + 1):
        not_seen2d.add((hsize, wsize))","not_seen2d = {(hsize, wsize) for hsize in sm.xrange(3, 4 + 1) for wsize in sm.xrange(3, 4 + 1)}"
imgaug,https://github.com/aleju/imgaug/tree/master/test/augmenters/test_size.py,TestResize,test_decrease_size_by_tuples_of_floats__one_per_side$1551,"def test_decrease_size_by_tuples_of_floats__one_per_side(self):
        image2d = self.image2d[0:4, 0:4]
        image3d = self.image3d[0:4, 0:4, :]
        aug = iaa.Resize({""height"": (0.76, 1.0), ""width"": (0.76, 1.0)})
        not_seen2d = set()
        not_seen3d = set()
        for hsize in sm.xrange(3, 4+1):
            for wsize in sm.xrange(3, 4+1):
                not_seen2d.add((hsize, wsize))
        for hsize in sm.xrange(3, 4+1):
            for wsize in sm.xrange(3, 4+1):
                not_seen3d.add((hsize, wsize, 3))
        possible2d = set(list(not_seen2d))
        possible3d = set(list(not_seen3d))
        for _ in sm.xrange(100):
            observed2d = aug.augment_image(image2d)
            observed3d = aug.augment_image(image3d)
            assert observed2d.shape in possible2d
            assert observed3d.shape in possible3d
            if observed2d.shape in not_seen2d:
                not_seen2d.remove(observed2d.shape)
            if observed3d.shape in not_seen3d:
                not_seen3d.remove(observed3d.shape)
            if not not_seen2d and not not_seen3d:
                break
        assert not not_seen2d
        assert not not_seen3d","not_seen3d = set()
for hsize in sm.xrange(3, 4 + 1):
    for wsize in sm.xrange(3, 4 + 1):
        not_seen3d.add((hsize, wsize, 3))","not_seen3d = {(hsize, wsize, 3) for hsize in sm.xrange(3, 4 + 1) for wsize in sm.xrange(3, 4 + 1)}"
opentelemetry-python,https://github.com/open-telemetry/opentelemetry-python/tree/master/opentelemetry-api/tests/propagators/test_composite.py,TestCompositePropagator,test_fields$122,"def test_fields(self):
        propagator = CompositePropagator(
            [
                self.mock_propagator_0,
                self.mock_propagator_1,
                self.mock_propagator_2,
            ]
        )

        mock_setter = Mock()

        propagator.inject({}, setter=mock_setter)

        inject_fields = set()

        for mock_call in mock_setter.mock_calls:
            inject_fields.add(mock_call[1][1])

        self.assertEqual(inject_fields, propagator.fields)","inject_fields = set()
for mock_call in mock_setter.mock_calls:
    inject_fields.add(mock_call[1][1])",inject_fields = {mock_call[1][1] for mock_call in mock_setter.mock_calls}
anago,https://github.com/Hironsan/anago/tree/master/tests/test_wrapper.py,TestWrapper,test_train_vocab_init$84,"def test_train_vocab_init(self):
        vocab = set()
        for words in np.r_[self.x_train, self.x_test, self.x_test]:
            for word in words:
                vocab.add(word)
        model = anago.Sequence(initial_vocab=vocab, embeddings=self.embeddings)
        model.fit(self.x_train, self.y_train, self.x_test, self.y_test)","vocab = set()
for words in np.r_[self.x_train, self.x_test, self.x_test]:
    for word in words:
        vocab.add(word)","vocab = {word for words in np.r_[self.x_train, self.x_test, self.x_test] for word in words}"
Cura,https://github.com/Ultimaker/Cura/tree/master/cura/Settings/IntentManager.py,IntentManager,intentCategories$61,"def intentCategories(self, definition_id: str, nozzle_id: str, material_id: str) -> List[str]:
        """"""Collects and returns all intent categories available for the given

        parameters. Note that the 'default' category is always available.

        :param definition_id: ID of the printer.
        :param nozzle_name: Name of the nozzle.
        :param material_id: ID of the material.
        :return: A set of intent category names.
        """"""
        categories = set()
        for intent in self.intentMetadatas(definition_id, nozzle_id, material_id):
            categories.add(intent[""intent_category""])
        categories.add(""default"") #The ""empty"" intent is not an actual profile specific to the configuration but we do want it to appear in the categories list.
        return list(categories)","categories = set()
for intent in self.intentMetadatas(definition_id, nozzle_id, material_id):
    categories.add(intent['intent_category'])","categories = {intent['intent_category'] for intent in self.intentMetadatas(definition_id, nozzle_id, material_id)}"
brian2,https://github.com/brian-team/brian2/tree/master/brian2/synapses/synapses.py,Synapses,_expression_index_dependence$1913,"def _expression_index_dependence(self, expr, namespace, additional_indices=None):
        """"""
        Returns the set of synaptic indices that expr depends on
        """"""
        nr = NodeRenderer()
        expr = nr.render_expr(expr)
        deps = set()
        if additional_indices is None:
            additional_indices = {}
        identifiers = get_identifiers_recursively([expr], self.variables)
        variables = self.resolve_all(
            {name for name in identifiers if not name in additional_indices}, namespace
        )
        if any(getattr(var, ""auto_vectorise"", False) for var in variables.values()):
            identifiers.add(""_vectorisation_idx"")

        for varname in identifiers:
            # Special handling of i and j -- they do not actually use pre-/
            # postsynaptic indices (except for subgroups), they *are* the
            # pre-/postsynaptic indices
            if varname == ""i"":
                deps.add(""_presynaptic_idx"")
            elif varname == ""j"":
                deps.add(""_iterator_idx"")
            elif varname in additional_indices:
                deps.add(additional_indices[varname])
            else:
                deps.add(self.variables.indices[varname])
        if ""0"" in deps:
            deps.remove(""0"")
        return deps","deps = set()
for varname in identifiers:
    if varname == 'i':
        deps.add('_presynaptic_idx')
    elif varname == 'j':
        deps.add('_iterator_idx')
    elif varname in additional_indices:
        deps.add(additional_indices[varname])
    else:
        deps.add(self.variables.indices[varname])",deps = {'_presynaptic_idx' if varname == 'i' else '_iterator_idx' if varname == 'j' else additional_indices[varname] if varname in additional_indices else self.variables.indices[varname] for varname in identifiers}
moto,https://github.com/spulec/moto/tree/master/tests/test_ds/test_ds.py,,test_ds_describe_directories$98,"def test_ds_describe_directories():
    """"""Test good and bad invocations of describe_directories().""""""
    client = boto3.client(""ds"", region_name=TEST_REGION)
    ec2_client = boto3.client(""ec2"", region_name=TEST_REGION)

    expected_ids = set()
    limit = 10
    for _ in range(limit):
        expected_ids.add(create_test_directory(client, ec2_client))

    # Test that if no directory IDs are specified, all are returned.
    result = client.describe_directories()
    directories = result[""DirectoryDescriptions""]
    directory_ids = [x[""DirectoryId""] for x in directories]

    assert len(directories) == limit
    assert set(directory_ids) == expected_ids
    for idx, dir_info in enumerate(directories):
        assert dir_info[""DesiredNumberOfDomainControllers""] == 0
        assert not dir_info[""SsoEnabled""]
        assert dir_info[""DirectoryId""] == directory_ids[idx]
        assert dir_info[""Name""].startswith(""test-"")
        assert dir_info[""Size""] == ""Large""
        assert dir_info[""Alias""] == directory_ids[idx]
        assert dir_info[""AccessUrl""] == f""{directory_ids[idx]}.awsapps.com""
        assert dir_info[""Stage""] == ""Active""
        assert dir_info[""LaunchTime""] <= datetime.now(timezone.utc)
        assert dir_info[""StageLastUpdatedDateTime""] <= datetime.now(timezone.utc)
        assert dir_info[""Type""] == ""SimpleAD""
        assert dir_info[""VpcSettings""][""VpcId""].startswith(""vpc-"")
        assert len(dir_info[""VpcSettings""][""SubnetIds""]) == 2
        assert dir_info[""VpcSettings""][""SecurityGroupId""].startswith(""sg-"")
        assert len(dir_info[""DnsIpAddrs""]) == 2
    assert ""NextToken"" not in result

    # Test with a specific directory ID.
    result = client.describe_directories(DirectoryIds=[directory_ids[5]])
    assert len(result[""DirectoryDescriptions""]) == 1
    assert result[""DirectoryDescriptions""][0][""DirectoryId""] == directory_ids[5]

    # Test with a bad directory ID.
    bad_id = get_random_hex(3)
    with pytest.raises(ClientError) as exc:
        client.describe_directories(DirectoryIds=[bad_id])
    err = exc.value.response[""Error""]
    assert err[""Code""] == ""ValidationException""
    assert (
        f""Value '{bad_id}' at 'directoryId' failed to satisfy constraint: ""
        f""Member must satisfy regular expression pattern: ^d-[0-9a-f]{{10}}$""
    ) in err[""Message""]

    # Test with an invalid next token.
    with pytest.raises(ClientError) as exc:
        client.describe_directories(NextToken=""bogus"")
    err = exc.value.response[""Error""]
    assert err[""Code""] == ""InvalidNextTokenException""
    assert ""Invalid value passed for the NextToken parameter"" in err[""Message""]

    # Test with a limit.
    result = client.describe_directories(Limit=5)
    assert len(result[""DirectoryDescriptions""]) == 5
    directories = result[""DirectoryDescriptions""]
    for idx in range(5):
        assert directories[idx][""DirectoryId""] == directory_ids[idx]
    assert result[""NextToken""]

    result = client.describe_directories(Limit=1, NextToken=result[""NextToken""])
    assert len(result[""DirectoryDescriptions""]) == 1
    assert result[""DirectoryDescriptions""][0][""DirectoryId""] == directory_ids[5]","expected_ids = set()
for _ in range(limit):
    expected_ids.add(create_test_directory(client, ec2_client))","expected_ids = {create_test_directory(client, ec2_client) for _ in range(limit)}"
polyglot,https://github.com/aboSamoor/polyglot/tree/master/polyglot/downloader.py,,_unzip_iter$1221,"def _unzip_iter(filename, root, verbose=True):
  if verbose:
    sys.stdout.write('Unzipping %s' % os.path.split(filename)[1])
    sys.stdout.flush()

  try: zf = zipfile.ZipFile(filename)
  except zipfile.error as e:
    yield ErrorMessage(filename, 'Error with downloaded zip file')
    return
  except Exception as e:
    yield ErrorMessage(filename, e)
    return

  # Get lists of directories & files
  namelist = zf.namelist()
  dirlist = set()
  for x in namelist:
    if x.endswith('/'):
      dirlist.add(x)
    else:
      dirlist.add(x.rsplit('/',1)[0] + '/')
  filelist = [x for x in namelist if not x.endswith('/')]

  # Create the target directory if it doesn't exist
  if not os.path.exists(root):
    os.mkdir(root)

  # Create the directory structure
  for dirname in sorted(dirlist):
    pieces = dirname[:-1].split('/')
    for i in range(len(pieces)):
      dirpath = os.path.join(root, *pieces[:i+1])
      if not os.path.exists(dirpath):
        os.mkdir(dirpath)

  # Extract files.
  for i, filename in enumerate(filelist):
    filepath = os.path.join(root, *filename.split('/'))

    with open(filepath, 'wb') as outfile:
      try:
        contents = zf.read(filename)
      except Exception as e:
        yield ErrorMessage(filename, e)
        return
      outfile.write(contents)

    if verbose and (i*10/len(filelist) > (i-1)*10/len(filelist)):
      sys.stdout.write('.')
      sys.stdout.flush()
  if verbose:
    print()","dirlist = set()
for x in namelist:
    if x.endswith('/'):
        dirlist.add(x)
    else:
        dirlist.add(x.rsplit('/', 1)[0] + '/')","dirlist = {x if x.endswith('/') else x.rsplit('/', 1)[0] + '/' for x in namelist}"
FastMOT,https://github.com/GeekAlexis/FastMOT/tree/master/fastmot/plugins/get_compute.py,,main$19,"def main():
    libnames = ('libcuda.so', 'libcuda.dylib', 'cuda.dll')
    for libname in libnames:
        try:
            cuda = ctypes.CDLL(libname)
        except OSError:
            continue
        else:
            break
    else:
        return

    gpu_archs = set()

    n_gpus = ctypes.c_int()
    cc_major = ctypes.c_int()
    cc_minor = ctypes.c_int()

    result = ctypes.c_int()
    device = ctypes.c_int()
    error_str = ctypes.c_char_p()

    result = cuda.cuInit(0)
    if result != CUDA_SUCCESS:
        cuda.cuGetErrorString(result, ctypes.byref(error_str))
        print('cuInit failed with error code %d: %s' % (result, error_str.value.decode()))
        return 1

    result = cuda.cuDeviceGetCount(ctypes.byref(n_gpus))
    if result != CUDA_SUCCESS:
        cuda.cuGetErrorString(result, ctypes.byref(error_str))
        print('cuDeviceGetCount failed with error code %d: %s' % (result, error_str.value.decode()))
        return 1

    for i in range(n_gpus.value):
        if cuda.cuDeviceComputeCapability(ctypes.byref(cc_major), ctypes.byref(cc_minor), device) == CUDA_SUCCESS:
            gpu_archs.add(str(cc_major.value) + str(cc_minor.value))
    print(' '.join(gpu_archs))

    return 0","gpu_archs = set()
for i in range(n_gpus.value):
    if cuda.cuDeviceComputeCapability(ctypes.byref(cc_major), ctypes.byref(cc_minor), device) == CUDA_SUCCESS:
        gpu_archs.add(str(cc_major.value) + str(cc_minor.value))","gpu_archs = {str(cc_major.value) + str(cc_minor.value) for i in range(n_gpus.value) if cuda.cuDeviceComputeCapability(ctypes.byref(cc_major), ctypes.byref(cc_minor), device) == CUDA_SUCCESS}"
Cura,https://github.com/Ultimaker/Cura/tree/master/plugins/UM3NetworkPrinting/src/Cloud/CloudOutputDeviceManager.py,CloudOutputDeviceManager,_devicesRemovedFromAccount$305,"def _devicesRemovedFromAccount(self, removed_device_ids: Set[str]) -> None:
        """"""
        Removes the CloudOutputDevice from the received device ids and marks the specific printers as ""removed from
        account"". In addition, it generates a message to inform the user about the printers that are no longer linked to
        his/her account. The message is not generated if all the printers have been previously reported as not linked
        to the account.

        :param removed_device_ids: Set of device ids, whose CloudOutputDevice needs to be removed
        :return: None
        """"""

        if not CuraApplication.getInstance().getCuraAPI().account.isLoggedIn:
            return

        # Do not report device ids which have been previously marked as non-linked to the account
        ignored_device_ids = set()
        for device_id in removed_device_ids:
            if not parseBool(self._um_cloud_printers[device_id].getMetaDataEntry(META_UM_LINKED_TO_ACCOUNT, ""true"")):
                ignored_device_ids.add(device_id)

        # Keep the reported_device_ids list in a class variable, so that the message button actions can access it and
        # take the necessary steps to fulfill their purpose.
        self.reported_device_ids = removed_device_ids - ignored_device_ids
        if not self.reported_device_ids:
            return

        output_device_manager = CuraApplication.getInstance().getOutputDeviceManager()

        # Remove the output device from the printers
        for device_id in removed_device_ids:
            global_stack: Optional[GlobalStack] = self._um_cloud_printers.get(device_id, None)
            if not global_stack:
                continue
            if device_id in output_device_manager.getOutputDeviceIds():
                output_device_manager.removeOutputDevice(device_id)
            if device_id in self._remote_clusters:
                del self._remote_clusters[device_id]

            # Update the printer's metadata to mark it as not linked to the account
            global_stack.setMetaDataEntry(META_UM_LINKED_TO_ACCOUNT, False)

        # Generate message to show
        device_names = """".join([""<li>{} ({})</li>"".format(self._um_cloud_printers[device].name,
                                                          self._um_cloud_printers[device].definition.name) for device in
                                self.reported_device_ids])
        self._removed_printers_message = RemovedPrintersMessage(self.reported_device_ids, device_names)
        self._removed_printers_message.actionTriggered.connect(self._onRemovedPrintersMessageActionTriggered)
        self._removed_printers_message.show()","ignored_device_ids = set()
for device_id in removed_device_ids:
    if not parseBool(self._um_cloud_printers[device_id].getMetaDataEntry(META_UM_LINKED_TO_ACCOUNT, 'true')):
        ignored_device_ids.add(device_id)","ignored_device_ids = {device_id for device_id in removed_device_ids if not parseBool(self._um_cloud_printers[device_id].getMetaDataEntry(META_UM_LINKED_TO_ACCOUNT, 'true'))}"
tenet,https://github.com/gaasedelen/tenet/tree/master/plugins/tenet/trace/file.py,TraceFile,get_reg_mask_ids_containing$395,"def get_reg_mask_ids_containing(self, reg_name):
        """"""
        Return a set of reg mask ids containing the given register name.
        """"""
        reg_id = self.arch.REGISTERS.index(reg_name.upper())
        reg_mask = 1 << reg_id
        
        found = set()
        for i, current_mask in enumerate(self.masks):
            if current_mask & reg_mask:
                found.add(i)
        
        return found","found = set()
for (i, current_mask) in enumerate(self.masks):
    if current_mask & reg_mask:
        found.add(i)","found = {i for (i, current_mask) in enumerate(self.masks) if current_mask & reg_mask}"
taurus,https://github.com/Blazemeter/taurus/tree/master/bzt/modules/reporting.py,FinalStatus,__get_sample_element$188,"def __get_sample_element(self, sample, label_name):
        failed_samples_count = sample['fail']
        success_samples_count = sample['succ']
        total_samples_count = failed_samples_count + success_samples_count
        assert total_samples_count > 0, ""Total samples is zero for %s"" % label_name
        success_samples_perc = (success_samples_count * 100) / total_samples_count

        errors = set()
        for err_desc in sample['errors']:
            errors.add(err_desc[""msg""])

        return (
            label_name,
            ""FAIL"" if failed_samples_count > 0 else ""OK"",
            ""{0:.2f}%"".format(round(success_samples_perc, 2)),
            ""{0:.3f}"".format(round(sample['avg_rt'], 3)),
            ""\n"".join(errors)
        )","errors = set()
for err_desc in sample['errors']:
    errors.add(err_desc['msg'])",errors = {err_desc['msg'] for err_desc in sample['errors']}
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/contrib/slim/quantization/post_training_quantization.py,,_remove_ctrl_vars$86,"def _remove_ctrl_vars(graph):
    remove_ctr_vars = set()
    for node in graph.all_var_nodes():
        if node.is_ctrl_var():
            remove_ctr_vars.add(node)
    graph.safe_remove_nodes(remove_ctr_vars)
    return graph","remove_ctr_vars = set()
for node in graph.all_var_nodes():
    if node.is_ctrl_var():
        remove_ctr_vars.add(node)",remove_ctr_vars = {node for node in graph.all_var_nodes() if node.is_ctrl_var()}
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/gui/ClientGUIScrolledPanelsManagement.py,_MediaPanel,_GetUnsetMediaViewFiletypes$2056,"def _GetUnsetMediaViewFiletypes( self ):
            
            editable_mimes = set( HC.SEARCHABLE_MIMES )
            
            set_mimes = set()
            
            for ( mime, media_show_action, media_start_paused, media_start_with_embed, preview_show_action, preview_start_paused, preview_start_with_embed, zoom_info ) in self._media_viewer_options.GetData():
                
                set_mimes.add( mime )
                
            
            unset_mimes = editable_mimes.difference( set_mimes )
            
            return unset_mimes","set_mimes = set()
for (mime, media_show_action, media_start_paused, media_start_with_embed, preview_show_action, preview_start_paused, preview_start_with_embed, zoom_info) in self._media_viewer_options.GetData():
    set_mimes.add(mime)","set_mimes = {mime for (mime, media_show_action, media_start_paused, media_start_with_embed, preview_show_action, preview_start_paused, preview_start_with_embed, zoom_info) in self._media_viewer_options.GetData()}"
