repo_name,file_path,file_html,class_name,me_code,old_code,new_code,bool_code,chatGPT_code,if_correct,reversed_code,non_replace_var_refactored_code,refactored_code,acc,instruction,sys_msg,exam_msg,user_msg
Efficient-Segmentation-Networks,https://github.com/xiaoyufenfei/Efficient-Segmentation-Networks/tree/master/dataset/camvid.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Efficient-Segmentation-Networks/dataset/camvid.py,CamVidDataSet,"def __init__(self, root='', list_path='', max_iters=None, crop_size=(360, 360), mean=(128, 128, 128), scale=True, mirror=True, ignore_label=11):
    self.root = root
    self.list_path = list_path
    (self.crop_h, self.crop_w) = crop_size
    self.scale = scale
    self.ignore_label = ignore_label
    self.mean = mean
    self.is_mirror = mirror
    self.img_ids = [i_id.strip() for i_id in open(list_path)]
    if not max_iters == None:
        self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))
    self.files = []
    for name in self.img_ids:
        img_file = osp.join(self.root, name.split()[0])
        label_file = osp.join(self.root, name.split()[1])
        self.files.append({'img': img_file, 'label': label_file, 'name': name})
    print('length of train set: ', len(self.files))","def __init__(self, root='', list_path='', max_iters=None, crop_size=(360, 360), mean=(128, 128, 128), scale=True, mirror=True, ignore_label=11):
    self.root = root
    self.list_path = list_path
    (self.crop_h, self.crop_w) = crop_size
    self.scale = scale
    self.ignore_label = ignore_label
    self.mean = mean
    self.is_mirror = mirror
    self.img_ids = [i_id.strip() for i_id in open(list_path)]
    if not max_iters == None:
        self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))
    self.files = []
    for name in self.img_ids:
        img_file = osp.join(self.root, name.split()[0])
        label_file = osp.join(self.root, name.split()[1])
        self.files.append({'img': img_file, 'label': label_file, 'name': name})
    print('length of train set: ', len(self.files))","with open(list_path, 'r') as f:
    self.img_ids = [i_id.strip() for i_id in f]",1,,,,,,,,,,
illustration2vec,https://github.com/rezoo/illustration2vec/tree/master/i2v/caffe_i2v.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/illustration2vec/i2v/caffe_i2v.py,,"def make_i2v_with_caffe(net_path, param_path, tag_path=None, threshold_path=None):
    mean = np.array([164.76139251, 167.47864617, 181.13838569])
    net = Classifier(net_path, param_path, mean=mean, channel_swap=(2, 1, 0))
    kwargs = {}
    if tag_path is not None:
        tags = json.loads(open(tag_path, 'r').read())
        assert len(tags) == 1539
        kwargs['tags'] = tags
    if threshold_path is not None:
        fscore_threshold = np.load(threshold_path)['threshold']
        kwargs['threshold'] = fscore_threshold
    return CaffeI2V(net, **kwargs)","if tag_path is not None:
    tags = json.loads(open(tag_path, 'r').read())
    assert len(tags) == 1539
    kwargs['tags'] = tags","if tag_path is not None:
    with open(tag_path, 'r') as f:
        tags = json.loads(f.read())
        assert len(tags) == 1539
        kwargs['tags'] = tags",1,,,,,,,,,,
gpodder,https://github.com/gpodder/gpodder/tree/master/src/gpodder/download.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gpodder/src/gpodder/download.py,DownloadTask,"def __init__(self, episode, config, downloader=None):
    assert episode.download_task is None
    self.__lock = threading.RLock()
    self.__status = DownloadTask.NEW
    self.__activity = DownloadTask.ACTIVITY_DOWNLOAD
    self.__status_changed = True
    self.__episode = episode
    self._config = config
    self.__downloader = downloader
    self.filename = self.__episode.local_filename(create=True)
    self.tempname = self.filename + '.partial'
    self.total_size = self.__episode.file_size
    self.speed = 0.0
    self.progress = 0.0
    self.error_message = None
    self._notification_shown = False
    self.__start_time = 0
    self.__start_blocks = 0
    self.__limit_rate_value = self._config.limit_rate_value
    self.__limit_rate = self._config.limit_rate
    self._progress_updated = None
    self._last_progress_updated = 0.0
    if os.path.exists(self.tempname):
        try:
            already_downloaded = os.path.getsize(self.tempname)
            if self.total_size > 0:
                self.progress = max(0.0, min(1.0, already_downloaded / self.total_size))
        except OSError as os_error:
            logger.error('Cannot get size for %s', os_error)
    else:
        open(self.tempname, 'w').close()
    episode.download_task = self","if os.path.exists(self.tempname):
    try:
        already_downloaded = os.path.getsize(self.tempname)
        if self.total_size > 0:
            self.progress = max(0.0, min(1.0, already_downloaded / self.total_size))
    except OSError as os_error:
        logger.error('Cannot get size for %s', os_error)
else:
    open(self.tempname, 'w').close()","if os.path.exists(self.tempname):
    try:
        already_downloaded = os.path.getsize(self.tempname)
        if self.total_size > 0:
            self.progress = max(0.0, min(1.0, already_downloaded / self.total_size))
    except OSError as os_error:
        logger.error('Cannot get size for %s', os_error)
else:
    with open(self.tempname, 'w'):
        pass",1,,,,,,,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/options.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/options.py,,"def parseOpts(overrideArguments=None):

    def _readOptions(filename_bytes, default=[]):
        try:
            optionf = open(filename_bytes)
        except IOError:
            return default
        try:
            contents = optionf.read()
            if sys.version_info < (3,):
                contents = contents.decode(preferredencoding())
            res = compat_shlex_split(contents, comments=True)
        finally:
            optionf.close()
        return res

    def _readUserConf():
        xdg_config_home = compat_getenv('XDG_CONFIG_HOME')
        if xdg_config_home:
            userConfFile = os.path.join(xdg_config_home, 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(xdg_config_home, 'youtube-dl.conf')
        else:
            userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl.conf')
        userConf = _readOptions(userConfFile, None)
        if userConf is None:
            appdata_dir = compat_getenv('appdata')
            if appdata_dir:
                userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config'), default=None)
                if userConf is None:
                    userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config.txt'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf.txt'), default=None)
        if userConf is None:
            userConf = []
        return userConf

    def _format_option_string(option):
        """""" ('-o', '--option') -> -o, --format METAVAR""""""
        opts = []
        if option._short_opts:
            opts.append(option._short_opts[0])
        if option._long_opts:
            opts.append(option._long_opts[0])
        if len(opts) > 1:
            opts.insert(1, ', ')
        if option.takes_value():
            opts.append(' %s' % option.metavar)
        return ''.join(opts)

    def _comma_separated_values_options_callback(option, opt_str, value, parser):
        setattr(parser.values, option.dest, value.split(','))
    columns = compat_get_terminal_size().columns
    max_width = columns if columns else 80
    max_help_position = 80
    fmt = optparse.IndentedHelpFormatter(width=max_width, max_help_position=max_help_position)
    fmt.format_option_strings = _format_option_string
    kw = {'version': __version__, 'formatter': fmt, 'usage': '%prog [OPTIONS] URL [URL...]', 'conflict_handler': 'resolve'}
    parser = optparse.OptionParser(**compat_kwargs(kw))
    general = optparse.OptionGroup(parser, 'General Options')
    general.add_option('-h', '--help', action='help', help='Print this help text and exit')
    general.add_option('--version', action='version', help='Print program version and exit')
    general.add_option('-U', '--update', action='store_true', dest='update_self', help='Update this program to latest version. Make sure that you have sufficient permissions (run with sudo if needed)')
    general.add_option('-i', '--ignore-errors', action='store_true', dest='ignoreerrors', default=False, help='Continue on download errors, for example to skip unavailable videos in a playlist')
    general.add_option('--abort-on-error', action='store_false', dest='ignoreerrors', help='Abort downloading of further videos (in the playlist or the command line) if an error occurs')
    general.add_option('--dump-user-agent', action='store_true', dest='dump_user_agent', default=False, help='Display the current browser identification')
    general.add_option('--list-extractors', action='store_true', dest='list_extractors', default=False, help='List all supported extractors')
    general.add_option('--extractor-descriptions', action='store_true', dest='list_extractor_descriptions', default=False, help='Output descriptions of all supported extractors')
    general.add_option('--force-generic-extractor', action='store_true', dest='force_generic_extractor', default=False, help='Force extraction to use the generic extractor')
    general.add_option('--default-search', dest='default_search', metavar='PREFIX', help='Use this prefix for unqualified URLs. For example ""gvsearch2:"" downloads two videos from google videos for youtube-dl ""large apple"". Use the value ""auto"" to let youtube-dl guess (""auto_warning"" to emit a warning when guessing). ""error"" just throws an error. The default value ""fixup_error"" repairs broken URLs, but emits an error if this is not possible instead of searching.')
    general.add_option('--ignore-config', action='store_true', help='Do not read configuration files. When given in the global configuration file /etc/youtube-dl.conf: Do not read the user configuration in ~/.config/youtube-dl/config (%APPDATA%/youtube-dl/config.txt on Windows)')
    general.add_option('--config-location', dest='config_location', metavar='PATH', help='Location of the configuration file; either the path to the config or its containing directory.')
    general.add_option('--flat-playlist', action='store_const', dest='extract_flat', const='in_playlist', default=False, help='Do not extract the videos of a playlist, only list them.')
    general.add_option('--mark-watched', action='store_true', dest='mark_watched', default=False, help='Mark videos watched (YouTube only)')
    general.add_option('--no-mark-watched', action='store_false', dest='mark_watched', default=False, help='Do not mark videos watched (YouTube only)')
    general.add_option('--no-color', '--no-colors', action='store_true', dest='no_color', default=False, help='Do not emit color codes in output')
    network = optparse.OptionGroup(parser, 'Network Options')
    network.add_option('--proxy', dest='proxy', default=None, metavar='URL', help='Use the specified HTTP/HTTPS/SOCKS proxy. To enable SOCKS proxy, specify a proper scheme. For example socks5://127.0.0.1:1080/. Pass in an empty string (--proxy """") for direct connection')
    network.add_option('--socket-timeout', dest='socket_timeout', type=float, default=None, metavar='SECONDS', help='Time to wait before giving up, in seconds')
    network.add_option('--source-address', metavar='IP', dest='source_address', default=None, help='Client-side IP address to bind to')
    network.add_option('-4', '--force-ipv4', action='store_const', const='0.0.0.0', dest='source_address', help='Make all connections via IPv4')
    network.add_option('-6', '--force-ipv6', action='store_const', const='::', dest='source_address', help='Make all connections via IPv6')
    geo = optparse.OptionGroup(parser, 'Geo Restriction')
    geo.add_option('--geo-verification-proxy', dest='geo_verification_proxy', default=None, metavar='URL', help='Use this proxy to verify the IP address for some geo-restricted sites. The default proxy specified by --proxy (or none, if the option is not present) is used for the actual downloading.')
    geo.add_option('--cn-verification-proxy', dest='cn_verification_proxy', default=None, metavar='URL', help=optparse.SUPPRESS_HELP)
    geo.add_option('--geo-bypass', action='store_true', dest='geo_bypass', default=True, help='Bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--no-geo-bypass', action='store_false', dest='geo_bypass', default=True, help='Do not bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--geo-bypass-country', metavar='CODE', dest='geo_bypass_country', default=None, help='Force bypass geographic restriction with explicitly provided two-letter ISO 3166-2 country code')
    geo.add_option('--geo-bypass-ip-block', metavar='IP_BLOCK', dest='geo_bypass_ip_block', default=None, help='Force bypass geographic restriction with explicitly provided IP block in CIDR notation')
    selection = optparse.OptionGroup(parser, 'Video Selection')
    selection.add_option('--playlist-start', dest='playliststart', metavar='NUMBER', default=1, type=int, help='Playlist video to start at (default is %default)')
    selection.add_option('--playlist-end', dest='playlistend', metavar='NUMBER', default=None, type=int, help='Playlist video to end at (default is last)')
    selection.add_option('--playlist-items', dest='playlist_items', metavar='ITEM_SPEC', default=None, help='Playlist video items to download. Specify indices of the videos in the playlist separated by commas like: ""--playlist-items 1,2,5,8"" if you want to download videos indexed 1, 2, 5, 8 in the playlist. You can specify range: ""--playlist-items 1-3,7,10-13"", it will download the videos at index 1, 2, 3, 7, 10, 11, 12 and 13.')
    selection.add_option('--match-title', dest='matchtitle', metavar='REGEX', help='Download only matching titles (regex or caseless sub-string)')
    selection.add_option('--reject-title', dest='rejecttitle', metavar='REGEX', help='Skip download for matching titles (regex or caseless sub-string)')
    selection.add_option('--max-downloads', dest='max_downloads', metavar='NUMBER', type=int, default=None, help='Abort after downloading NUMBER files')
    selection.add_option('--min-filesize', metavar='SIZE', dest='min_filesize', default=None, help='Do not download any videos smaller than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--max-filesize', metavar='SIZE', dest='max_filesize', default=None, help='Do not download any videos larger than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--date', metavar='DATE', dest='date', default=None, help='Download only videos uploaded in this date')
    selection.add_option('--datebefore', metavar='DATE', dest='datebefore', default=None, help='Download only videos uploaded on or before this date (i.e. inclusive)')
    selection.add_option('--dateafter', metavar='DATE', dest='dateafter', default=None, help='Download only videos uploaded on or after this date (i.e. inclusive)')
    selection.add_option('--min-views', metavar='COUNT', dest='min_views', default=None, type=int, help='Do not download any videos with less than COUNT views')
    selection.add_option('--max-views', metavar='COUNT', dest='max_views', default=None, type=int, help='Do not download any videos with more than COUNT views')
    selection.add_option('--match-filter', metavar='FILTER', dest='match_filter', default=None, help='Generic video filter. Specify any key (see the ""OUTPUT TEMPLATE"" for a list of available keys) to match if the key is present, !key to check if the key is not present, key > NUMBER (like ""comment_count > 12"", also works with >=, <, <=, !=, =) to compare against a number, key = \'LITERAL\' (like ""uploader = \'Mike Smith\'"", also works with !=) to match against a string literal and & to require multiple matches. Values which are not known are excluded unless you put a question mark (?) after the operator. For example, to only match videos that have been liked more than 100 times and disliked less than 50 times (or the dislike functionality is not available at the given service), but who also have a description, use --match-filter ""like_count > 100 & dislike_count <? 50 & description"" .')
    selection.add_option('--no-playlist', action='store_true', dest='noplaylist', default=False, help='Download only the video, if the URL refers to a video and a playlist.')
    selection.add_option('--yes-playlist', action='store_false', dest='noplaylist', default=False, help='Download the playlist, if the URL refers to a video and a playlist.')
    selection.add_option('--age-limit', metavar='YEARS', dest='age_limit', default=None, type=int, help='Download only videos suitable for the given age')
    selection.add_option('--download-archive', metavar='FILE', dest='download_archive', help='Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it.')
    selection.add_option('--include-ads', dest='include_ads', action='store_true', help='Download advertisements as well (experimental)')
    authentication = optparse.OptionGroup(parser, 'Authentication Options')
    authentication.add_option('-u', '--username', dest='username', metavar='USERNAME', help='Login with this account ID')
    authentication.add_option('-p', '--password', dest='password', metavar='PASSWORD', help='Account password. If this option is left out, youtube-dl will ask interactively.')
    authentication.add_option('-2', '--twofactor', dest='twofactor', metavar='TWOFACTOR', help='Two-factor authentication code')
    authentication.add_option('-n', '--netrc', action='store_true', dest='usenetrc', default=False, help='Use .netrc authentication data')
    authentication.add_option('--video-password', dest='videopassword', metavar='PASSWORD', help='Video password (vimeo, smotri, youku)')
    adobe_pass = optparse.OptionGroup(parser, 'Adobe Pass Options')
    adobe_pass.add_option('--ap-mso', dest='ap_mso', metavar='MSO', help='Adobe Pass multiple-system operator (TV provider) identifier, use --ap-list-mso for a list of available MSOs')
    adobe_pass.add_option('--ap-username', dest='ap_username', metavar='USERNAME', help='Multiple-system operator account login')
    adobe_pass.add_option('--ap-password', dest='ap_password', metavar='PASSWORD', help='Multiple-system operator account password. If this option is left out, youtube-dl will ask interactively.')
    adobe_pass.add_option('--ap-list-mso', action='store_true', dest='ap_list_mso', default=False, help='List all supported multiple-system operators')
    video_format = optparse.OptionGroup(parser, 'Video Format Options')
    video_format.add_option('-f', '--format', action='store', dest='format', metavar='FORMAT', default=None, help='Video format code, see the ""FORMAT SELECTION"" for all the info')
    video_format.add_option('--all-formats', action='store_const', dest='format', const='all', help='Download all available video formats')
    video_format.add_option('--prefer-free-formats', action='store_true', dest='prefer_free_formats', default=False, help='Prefer free video formats unless a specific one is requested')
    video_format.add_option('-F', '--list-formats', action='store_true', dest='listformats', help='List all available formats of requested videos')
    video_format.add_option('--youtube-include-dash-manifest', action='store_true', dest='youtube_include_dash_manifest', default=True, help=optparse.SUPPRESS_HELP)
    video_format.add_option('--youtube-skip-dash-manifest', action='store_false', dest='youtube_include_dash_manifest', help='Do not download the DASH manifests and related data on YouTube videos')
    video_format.add_option('--merge-output-format', action='store', dest='merge_output_format', metavar='FORMAT', default=None, help='If a merge is required (e.g. bestvideo+bestaudio), output to given container format. One of mkv, mp4, ogg, webm, flv. Ignored if no merge is required')
    subtitles = optparse.OptionGroup(parser, 'Subtitle Options')
    subtitles.add_option('--write-sub', '--write-srt', action='store_true', dest='writesubtitles', default=False, help='Write subtitle file')
    subtitles.add_option('--write-auto-sub', '--write-automatic-sub', action='store_true', dest='writeautomaticsub', default=False, help='Write automatically generated subtitle file (YouTube only)')
    subtitles.add_option('--all-subs', action='store_true', dest='allsubtitles', default=False, help='Download all the available subtitles of the video')
    subtitles.add_option('--list-subs', action='store_true', dest='listsubtitles', default=False, help='List all available subtitles for the video')
    subtitles.add_option('--sub-format', action='store', dest='subtitlesformat', metavar='FORMAT', default='best', help='Subtitle format, accepts formats preference, for example: ""srt"" or ""ass/srt/best""')
    subtitles.add_option('--sub-lang', '--sub-langs', '--srt-lang', action='callback', dest='subtitleslangs', metavar='LANGS', type='str', default=[], callback=_comma_separated_values_options_callback, help='Languages of the subtitles to download (optional) separated by commas, use --list-subs for available language tags')
    downloader = optparse.OptionGroup(parser, 'Download Options')
    downloader.add_option('-r', '--limit-rate', '--rate-limit', dest='ratelimit', metavar='RATE', help='Maximum download rate in bytes per second (e.g. 50K or 4.2M)')
    downloader.add_option('-R', '--retries', dest='retries', metavar='RETRIES', default=10, help='Number of retries (default is %default), or ""infinite"".')
    downloader.add_option('--fragment-retries', dest='fragment_retries', metavar='RETRIES', default=10, help='Number of retries for a fragment (default is %default), or ""infinite"" (DASH, hlsnative and ISM)')
    downloader.add_option('--skip-unavailable-fragments', action='store_true', dest='skip_unavailable_fragments', default=True, help='Skip unavailable fragments (DASH, hlsnative and ISM)')
    downloader.add_option('--abort-on-unavailable-fragment', action='store_false', dest='skip_unavailable_fragments', help='Abort downloading when some fragment is not available')
    downloader.add_option('--keep-fragments', action='store_true', dest='keep_fragments', default=False, help='Keep downloaded fragments on disk after downloading is finished; fragments are erased by default')
    downloader.add_option('--buffer-size', dest='buffersize', metavar='SIZE', default='1024', help='Size of download buffer (e.g. 1024 or 16K) (default is %default)')
    downloader.add_option('--no-resize-buffer', action='store_true', dest='noresizebuffer', default=False, help='Do not automatically adjust the buffer size. By default, the buffer size is automatically resized from an initial value of SIZE.')
    downloader.add_option('--http-chunk-size', dest='http_chunk_size', metavar='SIZE', default=None, help='Size of a chunk for chunk-based HTTP downloading (e.g. 10485760 or 10M) (default is disabled). May be useful for bypassing bandwidth throttling imposed by a webserver (experimental)')
    downloader.add_option('--test', action='store_true', dest='test', default=False, help=optparse.SUPPRESS_HELP)
    downloader.add_option('--playlist-reverse', action='store_true', help='Download playlist videos in reverse order')
    downloader.add_option('--playlist-random', action='store_true', help='Download playlist videos in random order')
    downloader.add_option('--xattr-set-filesize', dest='xattr_set_filesize', action='store_true', help='Set file xattribute ytdl.filesize with expected file size')
    downloader.add_option('--hls-prefer-native', dest='hls_prefer_native', action='store_true', default=None, help='Use the native HLS downloader instead of ffmpeg')
    downloader.add_option('--hls-prefer-ffmpeg', dest='hls_prefer_native', action='store_false', default=None, help='Use ffmpeg instead of the native HLS downloader')
    downloader.add_option('--hls-use-mpegts', dest='hls_use_mpegts', action='store_true', help='Use the mpegts container for HLS videos, allowing to play the video while downloading (some players may not be able to play it)')
    downloader.add_option('--external-downloader', dest='external_downloader', metavar='COMMAND', help='Use the specified external downloader. Currently supports %s' % ','.join(list_external_downloaders()))
    downloader.add_option('--external-downloader-args', dest='external_downloader_args', metavar='ARGS', help='Give these arguments to the external downloader')
    workarounds = optparse.OptionGroup(parser, 'Workarounds')
    workarounds.add_option('--encoding', dest='encoding', metavar='ENCODING', help='Force the specified encoding (experimental)')
    workarounds.add_option('--no-check-certificate', action='store_true', dest='no_check_certificate', default=False, help='Suppress HTTPS certificate validation')
    workarounds.add_option('--prefer-insecure', '--prefer-unsecure', action='store_true', dest='prefer_insecure', help='Use an unencrypted connection to retrieve information about the video. (Currently supported only for YouTube)')
    workarounds.add_option('--user-agent', metavar='UA', dest='user_agent', help='Specify a custom user agent')
    workarounds.add_option('--referer', metavar='URL', dest='referer', default=None, help='Specify a custom referer, use if the video access is restricted to one domain')
    workarounds.add_option('--add-header', metavar='FIELD:VALUE', dest='headers', action='append', help=""Specify a custom HTTP header and its value, separated by a colon ':'. You can use this option multiple times"")
    workarounds.add_option('--bidi-workaround', dest='bidi_workaround', action='store_true', help='Work around terminals that lack bidirectional text support. Requires bidiv or fribidi executable in PATH')
    workarounds.add_option('--sleep-interval', '--min-sleep-interval', metavar='SECONDS', dest='sleep_interval', type=float, help='Number of seconds to sleep before each download when used alone or a lower bound of a range for randomized sleep before each download (minimum possible number of seconds to sleep) when used along with --max-sleep-interval.')
    workarounds.add_option('--max-sleep-interval', metavar='SECONDS', dest='max_sleep_interval', type=float, help='Upper bound of a range for randomized sleep before each download (maximum possible number of seconds to sleep). Must only be used along with --min-sleep-interval.')
    verbosity = optparse.OptionGroup(parser, 'Verbosity / Simulation Options')
    verbosity.add_option('-q', '--quiet', action='store_true', dest='quiet', default=False, help='Activate quiet mode')
    verbosity.add_option('--no-warnings', dest='no_warnings', action='store_true', default=False, help='Ignore warnings')
    verbosity.add_option('-s', '--simulate', action='store_true', dest='simulate', default=False, help='Do not download the video and do not write anything to disk')
    verbosity.add_option('--skip-download', action='store_true', dest='skip_download', default=False, help='Do not download the video')
    verbosity.add_option('-g', '--get-url', action='store_true', dest='geturl', default=False, help='Simulate, quiet but print URL')
    verbosity.add_option('-e', '--get-title', action='store_true', dest='gettitle', default=False, help='Simulate, quiet but print title')
    verbosity.add_option('--get-id', action='store_true', dest='getid', default=False, help='Simulate, quiet but print id')
    verbosity.add_option('--get-thumbnail', action='store_true', dest='getthumbnail', default=False, help='Simulate, quiet but print thumbnail URL')
    verbosity.add_option('--get-description', action='store_true', dest='getdescription', default=False, help='Simulate, quiet but print video description')
    verbosity.add_option('--get-duration', action='store_true', dest='getduration', default=False, help='Simulate, quiet but print video length')
    verbosity.add_option('--get-filename', action='store_true', dest='getfilename', default=False, help='Simulate, quiet but print output filename')
    verbosity.add_option('--get-format', action='store_true', dest='getformat', default=False, help='Simulate, quiet but print output format')
    verbosity.add_option('-j', '--dump-json', action='store_true', dest='dumpjson', default=False, help='Simulate, quiet but print JSON information. See the ""OUTPUT TEMPLATE"" for a description of available keys.')
    verbosity.add_option('-J', '--dump-single-json', action='store_true', dest='dump_single_json', default=False, help='Simulate, quiet but print JSON information for each command-line argument. If the URL refers to a playlist, dump the whole playlist information in a single line.')
    verbosity.add_option('--print-json', action='store_true', dest='print_json', default=False, help='Be quiet and print the video information as JSON (video is still being downloaded).')
    verbosity.add_option('--newline', action='store_true', dest='progress_with_newline', default=False, help='Output progress bar as new lines')
    verbosity.add_option('--no-progress', action='store_true', dest='noprogress', default=False, help='Do not print progress bar')
    verbosity.add_option('--console-title', action='store_true', dest='consoletitle', default=False, help='Display progress in console titlebar')
    verbosity.add_option('-v', '--verbose', action='store_true', dest='verbose', default=False, help='Print various debugging information')
    verbosity.add_option('--dump-pages', '--dump-intermediate-pages', action='store_true', dest='dump_intermediate_pages', default=False, help='Print downloaded pages encoded using base64 to debug problems (very verbose)')
    verbosity.add_option('--write-pages', action='store_true', dest='write_pages', default=False, help='Write downloaded intermediary pages to files in the current directory to debug problems')
    verbosity.add_option('--youtube-print-sig-code', action='store_true', dest='youtube_print_sig_code', default=False, help=optparse.SUPPRESS_HELP)
    verbosity.add_option('--print-traffic', '--dump-headers', dest='debug_printtraffic', action='store_true', default=False, help='Display sent and read HTTP traffic')
    verbosity.add_option('-C', '--call-home', dest='call_home', action='store_true', default=False, help='Contact the youtube-dl server for debugging')
    verbosity.add_option('--no-call-home', dest='call_home', action='store_false', default=False, help='Do NOT contact the youtube-dl server for debugging')
    filesystem = optparse.OptionGroup(parser, 'Filesystem Options')
    filesystem.add_option('-a', '--batch-file', dest='batchfile', metavar='FILE', help=""File containing URLs to download ('-' for stdin), one URL per line. Lines starting with '#', ';' or ']' are considered as comments and ignored."")
    filesystem.add_option('--id', default=False, action='store_true', dest='useid', help='Use only video ID in file name')
    filesystem.add_option('-o', '--output', dest='outtmpl', metavar='TEMPLATE', help='Output filename template, see the ""OUTPUT TEMPLATE"" for all the info')
    filesystem.add_option('--autonumber-size', dest='autonumber_size', metavar='NUMBER', type=int, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('--autonumber-start', dest='autonumber_start', metavar='NUMBER', default=1, type=int, help='Specify the start value for %(autonumber)s (default is %default)')
    filesystem.add_option('--restrict-filenames', action='store_true', dest='restrictfilenames', default=False, help='Restrict filenames to only ASCII characters, and avoid ""&"" and spaces in filenames')
    filesystem.add_option('-A', '--auto-number', action='store_true', dest='autonumber', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-t', '--title', action='store_true', dest='usetitle', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-l', '--literal', default=False, action='store_true', dest='usetitle', help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-w', '--no-overwrites', action='store_true', dest='nooverwrites', default=False, help='Do not overwrite files')
    filesystem.add_option('-c', '--continue', action='store_true', dest='continue_dl', default=True, help='Force resume of partially downloaded files. By default, youtube-dl will resume downloads if possible.')
    filesystem.add_option('--no-continue', action='store_false', dest='continue_dl', help='Do not resume partially downloaded files (restart from beginning)')
    filesystem.add_option('--no-part', action='store_true', dest='nopart', default=False, help='Do not use .part files - write directly into output file')
    filesystem.add_option('--no-mtime', action='store_false', dest='updatetime', default=True, help='Do not use the Last-modified header to set the file modification time')
    filesystem.add_option('--write-description', action='store_true', dest='writedescription', default=False, help='Write video description to a .description file')
    filesystem.add_option('--write-info-json', action='store_true', dest='writeinfojson', default=False, help='Write video metadata to a .info.json file')
    filesystem.add_option('--write-annotations', action='store_true', dest='writeannotations', default=False, help='Write video annotations to a .annotations.xml file')
    filesystem.add_option('--load-info-json', '--load-info', dest='load_info_filename', metavar='FILE', help='JSON file containing the video information (created with the ""--write-info-json"" option)')
    filesystem.add_option('--cookies', dest='cookiefile', metavar='FILE', help='File to read cookies from and dump cookie jar in')
    filesystem.add_option('--cache-dir', dest='cachedir', default=None, metavar='DIR', help='Location in the filesystem where youtube-dl can store some downloaded information permanently. By default $XDG_CACHE_HOME/youtube-dl or ~/.cache/youtube-dl . At the moment, only YouTube player files (for videos with obfuscated signatures) are cached, but that may change.')
    filesystem.add_option('--no-cache-dir', action='store_const', const=False, dest='cachedir', help='Disable filesystem caching')
    filesystem.add_option('--rm-cache-dir', action='store_true', dest='rm_cachedir', help='Delete all filesystem cache files')
    thumbnail = optparse.OptionGroup(parser, 'Thumbnail images')
    thumbnail.add_option('--write-thumbnail', action='store_true', dest='writethumbnail', default=False, help='Write thumbnail image to disk')
    thumbnail.add_option('--write-all-thumbnails', action='store_true', dest='write_all_thumbnails', default=False, help='Write all thumbnail image formats to disk')
    thumbnail.add_option('--list-thumbnails', action='store_true', dest='list_thumbnails', default=False, help='Simulate and list all available thumbnail formats')
    postproc = optparse.OptionGroup(parser, 'Post-processing Options')
    postproc.add_option('-x', '--extract-audio', action='store_true', dest='extractaudio', default=False, help='Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe)')
    postproc.add_option('--audio-format', metavar='FORMAT', dest='audioformat', default='best', help='Specify audio format: ""best"", ""aac"", ""flac"", ""mp3"", ""m4a"", ""opus"", ""vorbis"", or ""wav""; ""%default"" by default; No effect without -x')
    postproc.add_option('--audio-quality', metavar='QUALITY', dest='audioquality', default='5', help='Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default %default)')
    postproc.add_option('--recode-video', metavar='FORMAT', dest='recodevideo', default=None, help='Encode the video to another format if necessary (currently supported: mp4|flv|ogg|webm|mkv|avi)')
    postproc.add_option('--postprocessor-args', dest='postprocessor_args', metavar='ARGS', help='Give these arguments to the postprocessor')
    postproc.add_option('-k', '--keep-video', action='store_true', dest='keepvideo', default=False, help='Keep the video file on disk after the post-processing; the video is erased by default')
    postproc.add_option('--no-post-overwrites', action='store_true', dest='nopostoverwrites', default=False, help='Do not overwrite post-processed files; the post-processed files are overwritten by default')
    postproc.add_option('--embed-subs', action='store_true', dest='embedsubtitles', default=False, help='Embed subtitles in the video (only for mp4, webm and mkv videos)')
    postproc.add_option('--embed-thumbnail', action='store_true', dest='embedthumbnail', default=False, help='Embed thumbnail in the audio as cover art')
    postproc.add_option('--add-metadata', action='store_true', dest='addmetadata', default=False, help='Write metadata to the video file')
    postproc.add_option('--metadata-from-title', metavar='FORMAT', dest='metafromtitle', help='Parse additional metadata like song title / artist from the video title. The format syntax is the same as --output. Regular expression with named capture groups may also be used. The parsed parameters replace existing values. Example: --metadata-from-title ""%(artist)s - %(title)s"" matches a title like ""Coldplay - Paradise"". Example (regex): --metadata-from-title ""(?P<artist>.+?) - (?P<title>.+)""')
    postproc.add_option('--xattrs', action='store_true', dest='xattrs', default=False, help=""Write metadata to the video file's xattrs (using dublin core and xdg standards)"")
    postproc.add_option('--fixup', metavar='POLICY', dest='fixup', default='detect_or_warn', help='Automatically correct known faults of the file. One of never (do nothing), warn (only emit a warning), detect_o","try:
    optionf = open(filename_bytes)
except IOError:
    return default","with open(filename_bytes) as optionf:
    try:
        # use optionf here
    except IOError:
        return default",1,,,,,,,,,,
blueprint,https://github.com/devstructure/blueprint/tree/master/blueprint/frontend/cfengine3.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/blueprint/blueprint/frontend/cfengine3.py,Sketch,"def dumpf(self, gzip=False):
    """"""
        Generate files containing CFEngine 3 code and templates.  The directory
        structure generated is a sketch (sketch.json plus all the rest).
        """"""
    os.mkdir(self.name)
    filename = os.path.join(self.name, 'sketch.json')
    f = codecs.open(filename, 'w', encoding='utf-8')
    self._dump(f.write, inline=False)
    f.close()
    self.policy.make_content()
    for (pathname, dirname, content, meta) in self.allfiles():
        pathname = os.path.join(self.name, dirname, pathname[1:])
        try:
            os.makedirs(os.path.dirname(pathname))
        except OSError as e:
            if errno.EEXIST != e.errno:
                raise e
        if isinstance(content, unicode):
            f = codecs.open(pathname, 'w', encoding='utf-8')
        else:
            f = open(pathname, 'w')
        f.write(content)
        f.close()
    if gzip:
        filename = 'cfengine3-{0}.tar.gz'.format(self.name)
        tarball = tarfile.open(filename, 'w:gz')
        tarball.add(self.name)
        tarball.close()
        return filename
    return filename","if isinstance(content, unicode):
    f = codecs.open(pathname, 'w', encoding='utf-8')
else:
    f = open(pathname, 'w')","if isinstance(content, unicode):
    with codecs.open(pathname, 'w', encoding='utf-8') as f:
        # do something with f
else:
    with open(pathname, 'w') as f:
        # do something with f",1,,,,,,,,,,
flair,https://github.com/flairNLP/flair/tree/master/flair/datasets/sequence_labeling.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flair/flair/datasets/sequence_labeling.py,NER_ENGLISH_STACKOVERFLOW,"def __init__(self, base_path: Union[str, Path]=None, in_memory: bool=True, **corpusargs):
    """"""
        Initialize the STACKOVERFLOW_NER corpus. The first time you call this constructor it will automatically
        download the dataset.
        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this
        to point to a different folder but typically this should not be necessary.
        POS tags instead
        :param in_memory: If True, keeps dataset in memory giving speedups in training.
        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object
        """"""
    if not base_path:
        base_path = flair.cache_root / 'datasets'
    else:
        base_path = Path(base_path)
    '\n        The Datasets are represented in the Conll format.\n           In this format each line of the Dataset is in the following format:\n           <word>+""\t""+<NE>""\t""+<word>+""\t""<markdown>\n           The end of sentence is marked with an empty line.\n           In each line NE represented the human annotated named entity\n           and <markdown> represented the code tags provided by the users who wrote the posts.\n           '
    columns = {0: 'word', 1: 'ner', 3: 'markdown'}
    entity_mapping = {'Library_Function': 'Function', 'Function_Name': 'Function', 'Class_Name': 'Class', 'Library_Class': 'Class', 'Organization': 'Website', 'Library_Variable': 'Variable', 'Variable_Name': 'Variable', 'Error_Name': 'O', 'Keyboard_IP': 'O', 'Value': 'O', 'Output_Block': 'O'}
    dataset_name = self.__class__.__name__.lower()
    data_folder = base_path / dataset_name
    STACKOVERFLOW_NER_path = 'https://raw.githubusercontent.com/jeniyat/StackOverflowNER/master/resources/annotated_ner_data/StackOverflow/'
    banned_sentences = ['code omitted for annotation', 'omitted for annotation', 'CODE_BLOCK :', 'OP_BLOCK :', 'Question_URL :', 'Question_ID :']
    files = ['train', 'test', 'dev']
    for file in files:
        questions = 0
        answers = 0
        cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
        for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
            if line.startswith('Question_ID'):
                questions += 1
            if line.startswith('Answer_to_Question_ID'):
                answers += 1
        log.info(f'File {file} has {questions} questions and {answers} answers.')
    super(NER_ENGLISH_STACKOVERFLOW, self).__init__(data_folder, columns, train_file='train.txt', test_file='test.txt', dev_file='dev.txt', encoding='utf-8', banned_sentences=banned_sentences, in_memory=in_memory, label_name_map=entity_mapping, **corpusargs)","for file in files:
    questions = 0
    answers = 0
    cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
    for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
        if line.startswith('Question_ID'):
            questions += 1
        if line.startswith('Answer_to_Question_ID'):
            answers += 1
    log.info(f'File {file} has {questions} questions and {answers} answers.')","for file in files:
    questions = 0
    answers = 0
    cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
    with open(data_folder / (file + '.txt'), mode='r', encoding='utf-8') as f:
        for line in f:
            if line.startswith('Question_ID'):
                questions += 1
            if line.startswith('Answer_to_Question_ID'):
                answers += 1
    log.info(f'File {file} has {questions} questions and {answers} answers.')",1,,,,,,,,,,
vega,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/nas/sp_nas/src/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/nas/sp_nas/src/util.py,,"def coco_eval(result_files, result_types, coco, max_dets=(100, 300, 1000), single_result=False):
    """"""Construct the trainer of SpNas.""""""
    anns = json.load(open(result_files['bbox']))
    if not anns:
        return summary_init
    if mmcv.is_str(coco):
        coco = COCO(coco)
    if isinstance(coco, COCO):
        for res_type in result_types:
            result_file = result_files[res_type]
            if result_file.endswith('.json'):
                coco_dets = coco.loadRes(result_file)
                gt_img_ids = coco.getImgIds()
                det_img_ids = coco_dets.getImgIds()
                iou_type = 'bbox' if res_type == 'proposal' else res_type
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                tgt_ids = gt_img_ids if not single_result else det_img_ids
                if single_result:
                    res_dict = dict()
                    for id_i in tgt_ids:
                        cocoEval = COCOeval(coco, coco_dets, iou_type)
                        if res_type == 'proposal':
                            cocoEval.params.useCats = 0
                            cocoEval.params.maxDets = list(max_dets)
                        cocoEval.params.imgIds = [id_i]
                        cocoEval.evaluate()
                        cocoEval.accumulate()
                        cocoEval.summarize()
                        res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = tgt_ids
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}
    else:
        raise ValueError('Type of coco is wrong.')
    return summary_metrics","def coco_eval(result_files, result_types, coco, max_dets=(100, 300, 1000), single_result=False):
    """"""Construct the trainer of SpNas.""""""
    anns = json.load(open(result_files['bbox']))
    if not anns:
        return summary_init
    if mmcv.is_str(coco):
        coco = COCO(coco)
    if isinstance(coco, COCO):
        for res_type in result_types:
            result_file = result_files[res_type]
            if result_file.endswith('.json'):
                coco_dets = coco.loadRes(result_file)
                gt_img_ids = coco.getImgIds()
                det_img_ids = coco_dets.getImgIds()
                iou_type = 'bbox' if res_type == 'proposal' else res_type
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                tgt_ids = gt_img_ids if not single_result else det_img_ids
                if single_result:
                    res_dict = dict()
                    for id_i in tgt_ids:
                        cocoEval = COCOeval(coco, coco_dets, iou_type)
                        if res_type == 'proposal':
                            cocoEval.params.useCats = 0
                            cocoEval.params.maxDets = list(max_dets)
                        cocoEval.params.imgIds = [id_i]
                        cocoEval.evaluate()
                        cocoEval.accumulate()
                        cocoEval.summarize()
                        res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = tgt_ids
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}
    else:
        raise ValueError('Type of coco is wrong.')
    return summary_metrics","def coco_eval(result_files, result_types, coco, max_dets=(100, 300, 1000), single_result=False):
    """"""Construct the trainer of SpNas.""""""
    with open(result_files['bbox']) as f:
        anns = json.load(f)
    if not anns:
        return summary_init
    if mmcv.is_str(coco):
        coco = COCO(coco)
    if isinstance(coco, COCO):
        for res_type in result_types:
            result_file = result_files[res_type]
            if result_file.endswith('.json'):
                with open(result_file) as f:
                    coco_dets = coco.loadRes(f)
                gt_img_ids = coco.getImgIds()
                det_img_ids = coco_dets.getImgIds()
                iou_type = 'bbox' if res_type == 'proposal' else res_type
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                tgt_ids = gt_img_ids if not single_result else det_img_ids
                if single_result:
                    res_dict = dict()
                    for id_i in tgt_ids:
                        cocoEval = COCOeval(coco, coco_dets, iou_type)
                        if res_type == 'proposal':
                            cocoEval.params.useCats = 0
                            cocoEval.params.maxDets = list(max_dets)
                        cocoEval.params.imgIds = [id_i]
                        cocoEval.evaluate()
                        cocoEval.accumulate()
                        cocoEval.summarize()
                        res_dict.update({coco.imgs[id_i]['file_name']: cocoEval.stats[1]})
                cocoEval = COCOeval(coco, coco_dets, iou_type)
                if res_type == 'proposal':
                    cocoEval.params.useCats = 0
                    cocoEval.params.maxDets = list(max_dets)
                cocoEval.params.imgIds = tgt_ids
                cocoEval.evaluate()
                cocoEval.accumulate()
                cocoEval.summarize()
                summary_metrics = {'Precision/mAP': cocoEval.stats[0], 'Precision/mAP@.50IOU': cocoEval.stats[1], 'Precision/mAP@.75IOU': cocoEval.stats[2], 'Precision/mAP (small)': cocoEval.stats[3], 'Precision/mAP (medium)': cocoEval.stats[4], 'Precision/mAP (large)': cocoEval.stats[5], 'Recall/AR@1': cocoEval.stats[6], 'Recall/AR@10': cocoEval.stats[7], 'Recall/AR@100': cocoEval.stats[8], 'Recall/AR@100 (small)': cocoEval.stats[9], 'Recall/AR@100 (medium)': cocoEval.stats[10], 'Recall/AR@100 (large)': cocoEval.stats[11]}
    else:
        raise ValueError('Type of coco is wrong.')
    return summary_metrics",1,,,,,,,,,,
ansible-modules-extras,https://github.com/ansible/ansible-modules-extras/tree/master/database/mssql/mssql_db.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-extras/database/mssql/mssql_db.py,,"def db_import(conn, cursor, module, db, target):
    if os.path.isfile(target):
        backup = open(target, 'r')
        try:
            sqlQuery = 'USE [%s]\n' % db
            for line in backup:
                if line is None:
                    break
                elif line.startswith('GO'):
                    cursor.execute(sqlQuery)
                    sqlQuery = 'USE [%s]\n' % db
                else:
                    sqlQuery += line
            cursor.execute(sqlQuery)
            conn.commit()
        finally:
            backup.close()
        return (0, 'import successful', '')
    else:
        return (1, 'cannot find target file', 'cannot find target file')","if os.path.isfile(target):
    backup = open(target, 'r')
    try:
        sqlQuery = 'USE [%s]\n' % db
        for line in backup:
            if line is None:
                break
            elif line.startswith('GO'):
                cursor.execute(sqlQuery)
                sqlQuery = 'USE [%s]\n' % db
            else:
                sqlQuery += line
        cursor.execute(sqlQuery)
        conn.commit()
    finally:
        backup.close()
    return (0, 'import successful', '')
else:
    return (1, 'cannot find target file', 'cannot find target file')","if os.path.isfile(target):
    with open(target, 'r') as backup:
        try:
            sqlQuery = 'USE [%s]\n' % db
            for line in backup:
                if line is None:
                    break
                elif line.startswith('GO'):
                    cursor.execute(sqlQuery)
                    sqlQuery = 'USE [%s]\n' % db
                else:
                    sqlQuery += line
            cursor.execute(sqlQuery)
            conn.commit()
        finally:
            backup.close()
    return (0, 'import successful', '')
else:
    return (1, 'cannot find target file', 'cannot find target file')",1,,,,,,,,,,
emmett,https://github.com/emmett-framework/emmett/tree/master/emmett/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/emmett/emmett/utils.py,,"def write_file(filename, value, mode='w'):
    f = open(filename, mode)
    try:
        return f.write(value)
    finally:
        f.close()","def write_file(filename, value, mode='w'):
    f = open(filename, mode)
    try:
        return f.write(value)
    finally:
        f.close()","def write_file(filename, value, mode='w'):
    with open(filename, mode) as f:
        return f.write(value)",1,,,,,,,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    with open(obj_filename, 'w') as fout_obj, open(mtl_filename, 'w') as fout_mtl:
        fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
        v_cnt = 0
        ins_cnt = 0
        for i in range(bbox.shape[0]):
            if easy_view and label[i] not in g_easy_view_labels:
                continue
            if exclude_table and label[i] == g_classes.index('table'):
                continue
            length = bbox[i, 3:6] - bbox[i, 0:3]
            a = length[0]
            b = length[1]
            c = length[2]
            x = bbox[i, 0]
            y = bbox[i, 1]
            z = bbox[i, 2]
            color = np.array(g_label2color[label[i]], dtype=float) / 255.0
            material = 'material%d' % ins_cnt
            fout_obj.write('usemtl %s\n' % material)
            fout_obj.write('v %f %f %f\n' % (x, y, z + c))
            fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
            fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
            fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
            fout_obj.write('v %f %f %f\n' % (x, y, z))
            fout_obj.write('v %f %f %f\n' % (x, y + b, z))
            fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
            fout_obj.write('v %f %f %f\n' % (x + a, y, z))
            fout_obj.write('g default\n')
            fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
            fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
            fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
            fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
            fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
            fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
            fout_obj.write('\n')
            fout_mtl.write('newmtl %s\n' % material)
            fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
            fout_mtl.write('\n')
            v_cnt += 8
            ins_cnt += 1",1,,,,,,,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    with open(obj_filename, 'w') as fout_obj, open(mtl_filename, 'w') as fout_mtl:
        fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
        v_cnt = 0
        ins_cnt = 0
        for i in range(bbox.shape[0]):
            if easy_view and label[i] not in g_easy_view_labels:
                continue
            if exclude_table and label[i] == g_classes.index('table'):
                continue
            length = bbox[i, 3:6] - bbox[i, 0:3]
            a = length[0]
            b = length[1]
            c = length[2]
            x = bbox[i, 0]
            y = bbox[i, 1]
            z = bbox[i, 2]
            color = np.array(g_label2color[label[i]], dtype=float) / 255.0
            material = 'material%d' % ins_cnt
            fout_obj.write('usemtl %s\n' % material)
            fout_obj.write('v %f %f %f\n' % (x, y, z + c))
            fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
            fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
            fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
            fout_obj.write('v %f %f %f\n' % (x, y, z))
            fout_obj.write('v %f %f %f\n' % (x, y + b, z))
            fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
            fout_obj.write('v %f %f %f\n' % (x + a, y, z))
            fout_obj.write('g default\n')
            fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
            fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
            fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
            fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
            fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
            fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
            fout_obj.write('\n')
            fout_mtl.write('newmtl %s\n' % material)
            fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
            fout_mtl.write('\n')
            v_cnt += 8
            ins_cnt += 1",1,,,,,,,,,,
tahoe-lafs,https://github.com/tahoe-lafs/tahoe-lafs/tree/master/src/allmydata/test/test_multi_introducers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tahoe-lafs/src/allmydata/test/test_multi_introducers.py,NoDefault,"def setUp(self):
    self.basedir = os.path.dirname(self.mktemp())
    c = open(os.path.join(self.basedir, 'tahoe.cfg'), 'w')
    config = {'hide-ip': False, 'listen': 'tcp', 'port': None, 'location': None, 'hostname': 'example.net'}
    write_node_config(c, config)
    c.write('[storage]\n')
    c.write('enabled = false\n')
    c.close()
    os.mkdir(os.path.join(self.basedir, 'private'))
    self.yaml_path = FilePath(os.path.join(self.basedir, 'private', 'introducers.yaml'))","def setUp(self):
    self.basedir = os.path.dirname(self.mktemp())
    c = open(os.path.join(self.basedir, 'tahoe.cfg'), 'w')
    config = {'hide-ip': False, 'listen': 'tcp', 'port': None, 'location': None, 'hostname': 'example.net'}
    write_node_config(c, config)
    c.write('[storage]\n')
    c.write('enabled = false\n')
    c.close()
    os.mkdir(os.path.join(self.basedir, 'private'))
    self.yaml_path = FilePath(os.path.join(self.basedir, 'private', 'introducers.yaml'))","def setUp(self):
    self.basedir = os.path.dirname(self.mktemp())
    with open(os.path.join(self.basedir, 'tahoe.cfg'), 'w') as c:
        config = {'hide-ip': False, 'listen': 'tcp', 'port': None, 'location': None, 'hostname': 'example.net'}
        write_node_config(c, config)
        c.write('[storage]\n')
        c.write('enabled = false\n')
    os.mkdir(os.path.join(self.basedir, 'private'))
    self.yaml_path = FilePath(os.path.join(self.basedir, 'private', 'introducers.yaml'))",1,,,,,,,,,,
evalml,https://github.com/alteryx/evalml/tree/master/evalml/utils/gen_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/evalml/evalml/utils/gen_utils.py,,"def _file_path_check(filepath=None, format='png', interactive=False, is_plotly=False):
    """"""Helper function to check the filepath being passed.

    Args:
        filepath (str or Path, optional): Location to save file.
        format (str): Extension for figure to be saved as. Defaults to 'png'.
        interactive (bool, optional): If True and fig is of type plotly.Figure, sets the format to 'html'.
        is_plotly (bool, optional): Check to see if the fig being passed is of type plotly.Figure.

    Returns:
        String representing the final filepath the image will be saved to.
    """"""
    if filepath:
        filepath = str(filepath)
        (path_and_name, extension) = os.path.splitext(filepath)
        extension = extension[1:].lower() if extension else None
        if is_plotly and interactive:
            format_ = 'html'
        elif not extension and (not interactive):
            format_ = format
        else:
            format_ = extension
        filepath = f'{path_and_name}.{format_}'
        try:
            f = open(filepath, 'w')
            f.close()
        except (IOError, FileNotFoundError):
            raise ValueError('Specified filepath is not writeable: {}'.format(filepath))
    return filepath","try:
    f = open(filepath, 'w')
    f.close()
except (IOError, FileNotFoundError):
    raise ValueError('Specified filepath is not writeable: {}'.format(filepath))","with open(filepath, 'w') as f:
    try:
        f.close()
    except (IOError, FileNotFoundError):
        raise ValueError('Specified filepath is not writeable: {}'.format(filepath))",1,,,,,,,,,,
Fabrik,https://github.com/Cloud-CV/Fabrik/tree/master/tests/unit/keras_app/test_views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Fabrik/tests/unit/keras_app/test_views.py,DenseExportTest,"def test_keras_export(self):
    tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app', 'keras_export_test.json'), 'r')
    response = json.load(tests)
    tests.close()
    net = yaml.safe_load(json.dumps(response['net']))
    net = {'l0': net['Input2'], 'l1': net['InnerProduct']}
    net['l0']['connection']['output'].append('l1')
    inp = data(net['l0'], '', 'l0')['l0']
    temp = dense(net['l1'], [inp], 'l1')
    model = Model(inp, temp['l1'])
    self.assertEqual(model.layers[2].__class__.__name__, 'Dense')
    net['l1']['params']['weight_filler'] = 'glorot_normal'
    net['l1']['params']['bias_filler'] = 'glorot_normal'
    inp = data(net['l0'], '', 'l0')['l0']
    temp = dense(net['l1'], [inp], 'l1')
    model = Model(inp, temp['l1'])
    self.assertEqual(model.layers[2].__class__.__name__, 'Dense')","def test_keras_export(self):
    tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app', 'keras_export_test.json'), 'r')
    response = json.load(tests)
    tests.close()
    net = yaml.safe_load(json.dumps(response['net']))
    net = {'l0': net['Input2'], 'l1': net['InnerProduct']}
    net['l0']['connection']['output'].append('l1')
    inp = data(net['l0'], '', 'l0')['l0']
    temp = dense(net['l1'], [inp], 'l1')
    model = Model(inp, temp['l1'])
    self.assertEqual(model.layers[2].__class__.__name__, 'Dense')
    net['l1']['params']['weight_filler'] = 'glorot_normal'
    net['l1']['params']['bias_filler'] = 'glorot_normal'
    inp = data(net['l0'], '', 'l0')['l0']
    temp = dense(net['l1'], [inp], 'l1')
    model = Model(inp, temp['l1'])
    self.assertEqual(model.layers[2].__class__.__name__, 'Dense')","def test_keras_export(self):
    with open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app', 'keras_export_test.json'), 'r') as tests:
        response = json.load(tests)
    net = yaml.safe_load(json.dumps(response['net']))
    net = {'l0': net['Input2'], 'l1': net['InnerProduct']}
    net['l0']['connection']['output'].append('l1')
    inp = data(net['l0'], '', 'l0')['l0']
    temp = dense(net['l1'], [inp], 'l1')
    model = Model(inp, temp['l1'])
    self.assertEqual(model.layers[2].__class__.__name__, 'Dense')
    net['l1']['params']['weight_filler'] = 'glorot_normal'
    net['l1']['params']['bias_filler'] = 'glorot_normal'
    inp = data(net['l0'], '', 'l0')['l0']
    temp = dense(net['l1'], [inp], 'l1')
    model = Model(inp, temp['l1'])
    self.assertEqual(model.layers[2].__class__.__name__, 'Dense')",1,,,,,,,,,,
Pinout.xyz,https://github.com/Gadgetoid/Pinout.xyz/tree/master//generate-json.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pinout.xyz//generate-json.py,,"def load_md(filename):
    filename = 'src/{}/{}'.format(lang, filename)
    try:
        html = markdown.markdown(open(filename).read(), extensions=['fenced_code'])
        return html
    except IOError:
        print('Unable to load markdown from {}'.format(filename))
        return ''","try:
    html = markdown.markdown(open(filename).read(), extensions=['fenced_code'])
    return html
except IOError:
    print('Unable to load markdown from {}'.format(filename))
    return ''","try:
    with open(filename, 'r') as f:
        html = markdown.markdown(f.read(), extensions=['fenced_code'])
        return html
except IOError:
    print('Unable to load markdown from {}'.format(filename))
    return ''",1,,,,,,,,,,
LASER,https://github.com/facebookresearch/LASER/tree/master/tasks/bucc/bucc.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LASER/tasks/bucc/bucc.py,,"def BuccExtract(cand2score, th, fname):
    if fname:
        of = open(fname, 'w', encoding=args.encoding)
    bitexts = []
    for ((src, trg), score) in cand2score.items():
        if score >= th:
            bitexts.append(src + '\t' + trg)
            if fname:
                of.write(src + '\t' + trg + '\n')
    if fname:
        of.close()
    return bitexts","if fname:
    of = open(fname, 'w', encoding=args.encoding)","if fname:
    with open(fname, 'w', encoding=args.encoding) as of:",1,,,,,,,,,,
keras-retinanet,https://github.com/fizyr/keras-retinanet/tree/master/keras_retinanet/preprocessing/open_images.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras-retinanet/keras_retinanet/preprocessing/open_images.py,OpenImagesGenerator,"def __init__(self, main_dir, subset, version='v4', labels_filter=None, annotation_cache_dir='.', parent_label=None, **kwargs):
    if version == 'challenge2018':
        metadata = 'challenge2018'
    elif version == 'v4':
        metadata = '2018_04'
    elif version == 'v3':
        metadata = '2017_11'
    else:
        raise NotImplementedError('There is currently no implementation for versions older than v3')
    if version == 'challenge2018':
        self.base_dir = os.path.join(main_dir, 'images', 'train')
    else:
        self.base_dir = os.path.join(main_dir, 'images', subset)
    metadata_dir = os.path.join(main_dir, metadata)
    annotation_cache_json = os.path.join(annotation_cache_dir, subset + '.json')
    self.hierarchy = load_hierarchy(metadata_dir, version=version)
    (id_to_labels, cls_index) = get_labels(metadata_dir, version=version)
    if os.path.exists(annotation_cache_json):
        with open(annotation_cache_json, 'r') as f:
            self.annotations = json.loads(f.read())
    else:
        self.annotations = generate_images_annotations_json(main_dir, metadata_dir, subset, cls_index, version=version)
        json.dump(self.annotations, open(annotation_cache_json, 'w'))
    if labels_filter is not None or parent_label is not None:
        (self.id_to_labels, self.annotations) = self.__filter_data(id_to_labels, cls_index, labels_filter, parent_label)
    else:
        self.id_to_labels = id_to_labels
    self.id_to_image_id = dict([(i, k) for (i, k) in enumerate(self.annotations)])
    super(OpenImagesGenerator, self).__init__(**kwargs)","if os.path.exists(annotation_cache_json):
    with open(annotation_cache_json, 'r') as f:
        self.annotations = json.loads(f.read())
else:
    self.annotations = generate_images_annotations_json(main_dir, metadata_dir, subset, cls_index, version=version)
    json.dump(self.annotations, open(annotation_cache_json, 'w'))","with open(annotation_cache_json, 'r') as f:
    if os.path.exists(annotation_cache_json):
        self.annotations = json.loads(f.read())
with open(annotation_cache_json, 'w') as f:
    if not os.path.exists(annotation_cache_json):
        self.annotations = generate_images_annotations_json(main_dir, metadata_dir, subset, cls_index, version=version)
        json.dump(self.annotations, f)",1,,,,,,,,,,
IPGeoLocation,https://github.com/maldevel/IPGeoLocation/tree/master/core/IpGeoLocationLib.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/IPGeoLocation/core/IpGeoLocationLib.py,IpGeoLocationLib,"def __loadUserAgents(self):
    """"""Load user-agent strings from file""""""
    if not self.UserAgentFile:
        raise UserAgentFileNotSpecifiedError()
    self.UserAgents = [line.strip() for line in open(self.UserAgentFile, 'r') if line.strip()]
    self.Logger.Print('{} User-Agent strings loaded.'.format(len(self.UserAgents)))
    if len(self.UserAgents) == 0:
        raise UserAgentFileEmptyError()","def __loadUserAgents(self):
    """"""Load user-agent strings from file""""""
    if not self.UserAgentFile:
        raise UserAgentFileNotSpecifiedError()
    self.UserAgents = [line.strip() for line in open(self.UserAgentFile, 'r') if line.strip()]
    self.Logger.Print('{} User-Agent strings loaded.'.format(len(self.UserAgents)))
    if len(self.UserAgents) == 0:
        raise UserAgentFileEmptyError()","def __loadUserAgents(self):
    """"""Load user-agent strings from file""""""
    if not self.UserAgentFile:
        raise UserAgentFileNotSpecifiedError()
    with open(self.UserAgentFile, 'r') as f:
        self.UserAgents = [line.strip() for line in f if line.strip()]
    self.Logger.Print('{} User-Agent strings loaded.'.format(len(self.UserAgents)))
    if len(self.UserAgents) == 0:
        raise UserAgentFileEmptyError()",1,,,,,,,,,,
coa_tools,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/addon_updater.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/addon_updater.py,Singleton_updater,"def save_updater_json(self):
    if self._update_ready == True:
        if type(self._update_version) == type((0, 0, 0)):
            self._json['update_ready'] = True
            self._json['version_text']['link'] = self._update_link
            self._json['version_text']['version'] = self._update_version
        else:
            self._json['update_ready'] = False
            self._json['version_text'] = {}
    else:
        self._json['update_ready'] = False
        self._json['version_text'] = {}
    jpath = os.path.join(self._updater_path, 'updater_status.json')
    outf = open(jpath, 'w')
    data_out = json.dumps(self._json, indent=4)
    outf.write(data_out)
    outf.close()
    if self._verbose:
        print(self._addon + ': Wrote out updater json settings to file, with the contents:')
        print(self._json)","def save_updater_json(self):
    if self._update_ready == True:
        if type(self._update_version) == type((0, 0, 0)):
            self._json['update_ready'] = True
            self._json['version_text']['link'] = self._update_link
            self._json['version_text']['version'] = self._update_version
        else:
            self._json['update_ready'] = False
            self._json['version_text'] = {}
    else:
        self._json['update_ready'] = False
        self._json['version_text'] = {}
    jpath = os.path.join(self._updater_path, 'updater_status.json')
    outf = open(jpath, 'w')
    data_out = json.dumps(self._json, indent=4)
    outf.write(data_out)
    outf.close()
    if self._verbose:
        print(self._addon + ': Wrote out updater json settings to file, with the contents:')
        print(self._json)","def save_updater_json(self):
    if self._update_ready == True:
        if type(self._update_version) == type((0, 0, 0)):
            self._json['update_ready'] = True
            self._json['version_text']['link'] = self._update_link
            self._json['version_text']['version'] = self._update_version
        else:
            self._json['update_ready'] = False
            self._json['version_text'] = {}
    else:
        self._json['update_ready'] = False
        self._json['version_text'] = {}
    jpath = os.path.join(self._updater_path, 'updater_status.json')
    with open(jpath, 'w') as outf:
        data_out = json.dumps(self._json, indent=4)
        outf.write(data_out)
    if self._verbose:
        print(self._addon + ': Wrote out updater json settings to file, with the contents:')
        print(self._json)",1,,,,,,,,,,
bertviz,https://github.com/jessevig/bertviz/tree/master/bertviz/transformers_neuron_view/modeling_openai.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/transformers_neuron_view/modeling_openai.py,,"def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):
    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)
    """"""
    import re
    import numpy as np
    if '.ckpt' in openai_checkpoint_folder_path:
        openai_checkpoint_folder_path = os.path.dirname(openai_checkpoint_folder_path)
    logger.info('Loading weights from {}'.format(openai_checkpoint_folder_path))
    names = json.load(open(openai_checkpoint_folder_path + '/parameters_names.json', 'r', encoding='utf-8'))
    shapes = json.load(open(openai_checkpoint_folder_path + '/params_shapes.json', 'r', encoding='utf-8'))
    offsets = np.cumsum([np.prod(shape) for shape in shapes])
    init_params = [np.load(openai_checkpoint_folder_path + '/params_{}.npy'.format(n)) for n in range(10)]
    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]
    init_params = [param.reshape(shape) for (param, shape) in zip(init_params, shapes)]
    init_params = [arr.squeeze() for arr in init_params]
    try:
        assert model.tokens_embed.weight.shape == init_params[1].shape
        assert model.positions_embed.weight.shape == init_params[0].shape
    except AssertionError as e:
        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)
        e.args += (model.positions_embed.weight.shape, init_params[0].shape)
        raise
    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])
    model.positions_embed.weight.data = torch.from_numpy(init_params[0])
    names.pop(0)
    init_params.pop(0)
    init_params.pop(0)
    for (name, array) in zip(names, init_params):
        name = name[6:]
        assert name[-2:] == ':0'
        name = name[:-2]
        name = name.split('/')
        pointer = model
        for m_name in name:
            if re.fullmatch('[A-Za-z]+\\d+', m_name):
                l = re.split('(\\d+)', m_name)
            else:
                l = [m_name]
            if l[0] == 'g':
                pointer = getattr(pointer, 'weight')
            elif l[0] == 'b':
                pointer = getattr(pointer, 'bias')
            elif l[0] == 'w':
                pointer = getattr(pointer, 'weight')
            else:
                pointer = getattr(pointer, l[0])
            if len(l) >= 2:
                num = int(l[1])
                pointer = pointer[num]
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        logger.info('Initialize PyTorch weight {}'.format(name))
        pointer.data = torch.from_numpy(array)
    return model","def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):
    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)
    """"""
    import re
    import numpy as np
    if '.ckpt' in openai_checkpoint_folder_path:
        openai_checkpoint_folder_path = os.path.dirname(openai_checkpoint_folder_path)
    logger.info('Loading weights from {}'.format(openai_checkpoint_folder_path))
    names = json.load(open(openai_checkpoint_folder_path + '/parameters_names.json', 'r', encoding='utf-8'))
    shapes = json.load(open(openai_checkpoint_folder_path + '/params_shapes.json', 'r', encoding='utf-8'))
    offsets = np.cumsum([np.prod(shape) for shape in shapes])
    init_params = [np.load(openai_checkpoint_folder_path + '/params_{}.npy'.format(n)) for n in range(10)]
    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]
    init_params = [param.reshape(shape) for (param, shape) in zip(init_params, shapes)]
    init_params = [arr.squeeze() for arr in init_params]
    try:
        assert model.tokens_embed.weight.shape == init_params[1].shape
        assert model.positions_embed.weight.shape == init_params[0].shape
    except AssertionError as e:
        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)
        e.args += (model.positions_embed.weight.shape, init_params[0].shape)
        raise
    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])
    model.positions_embed.weight.data = torch.from_numpy(init_params[0])
    names.pop(0)
    init_params.pop(0)
    init_params.pop(0)
    for (name, array) in zip(names, init_params):
        name = name[6:]
        assert name[-2:] == ':0'
        name = name[:-2]
        name = name.split('/')
        pointer = model
        for m_name in name:
            if re.fullmatch('[A-Za-z]+\\d+', m_name):
                l = re.split('(\\d+)', m_name)
            else:
                l = [m_name]
            if l[0] == 'g':
                pointer = getattr(pointer, 'weight')
            elif l[0] == 'b':
                pointer = getattr(pointer, 'bias')
            elif l[0] == 'w':
                pointer = getattr(pointer, 'weight')
            else:
                pointer = getattr(pointer, l[0])
            if len(l) >= 2:
                num = int(l[1])
                pointer = pointer[num]
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        logger.info('Initialize PyTorch weight {}'.format(name))
        pointer.data = torch.from_numpy(array)
    return model","def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):
    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)
    """"""
    import re
    import numpy as np
    if '.ckpt' in openai_checkpoint_folder_path:
        openai_checkpoint_folder_path = os.path.dirname(openai_checkpoint_folder_path)
    logger.info('Loading weights from {}'.format(openai_checkpoint_folder_path))
    with open(openai_checkpoint_folder_path + '/parameters_names.json', 'r', encoding='utf-8') as f:
        names = json.load(f)
    shapes = json.load(open(openai_checkpoint_folder_path + '/params_shapes.json', 'r', encoding='utf-8'))
    offsets = np.cumsum([np.prod(shape) for shape in shapes])
    init_params = [np.load(openai_checkpoint_folder_path + '/params_{}.npy'.format(n)) for n in range(10)]
    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]
    init_params = [param.reshape(shape) for (param, shape) in zip(init_params, shapes)]
    init_params = [arr.squeeze() for arr in init_params]
    try:
        assert model.tokens_embed.weight.shape == init_params[1].shape
        assert model.positions_embed.weight.shape == init_params[0].shape
    except AssertionError as e:
        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)
        e.args += (model.positions_embed.weight.shape, init_params[0].shape)
        raise
    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])
    model.positions_embed.weight.data = torch.from_numpy(init_params[0])
    names.pop(0)
    init_params.pop(0)
    init_params.pop(0)
    for (name, array) in zip(names, init_params):
        name = name[6:]
        assert name[-2:] == ':0'
        name = name[:-2]
        name = name.split('/')
        pointer = model
        for m_name in name:
            if re.fullmatch('[A-Za-z]+\\d+', m_name):
                l = re.split('(\\d+)', m_name)
            else:
                l = [m_name]
            if l[0] == 'g':
                pointer = getattr(pointer, 'weight')
            elif l[0] == 'b':
                pointer = getattr(pointer, 'bias')
            elif l[0] == 'w':
                pointer = getattr(pointer, 'weight')
            else:
                pointer = getattr(pointer, l[0])
            if len(l) >= 2:
                num = int(l[1])
                pointer = pointer[num]
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        logger.info('Initialize PyTorch weight {}'.format(name))
        pointer.data = torch.from_numpy(array)
    return model",1,,,,,,,,,,
bertviz,https://github.com/jessevig/bertviz/tree/master/bertviz/transformers_neuron_view/modeling_openai.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/transformers_neuron_view/modeling_openai.py,,"def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):
    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)
    """"""
    import re
    import numpy as np
    if '.ckpt' in openai_checkpoint_folder_path:
        openai_checkpoint_folder_path = os.path.dirname(openai_checkpoint_folder_path)
    logger.info('Loading weights from {}'.format(openai_checkpoint_folder_path))
    names = json.load(open(openai_checkpoint_folder_path + '/parameters_names.json', 'r', encoding='utf-8'))
    shapes = json.load(open(openai_checkpoint_folder_path + '/params_shapes.json', 'r', encoding='utf-8'))
    offsets = np.cumsum([np.prod(shape) for shape in shapes])
    init_params = [np.load(openai_checkpoint_folder_path + '/params_{}.npy'.format(n)) for n in range(10)]
    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]
    init_params = [param.reshape(shape) for (param, shape) in zip(init_params, shapes)]
    init_params = [arr.squeeze() for arr in init_params]
    try:
        assert model.tokens_embed.weight.shape == init_params[1].shape
        assert model.positions_embed.weight.shape == init_params[0].shape
    except AssertionError as e:
        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)
        e.args += (model.positions_embed.weight.shape, init_params[0].shape)
        raise
    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])
    model.positions_embed.weight.data = torch.from_numpy(init_params[0])
    names.pop(0)
    init_params.pop(0)
    init_params.pop(0)
    for (name, array) in zip(names, init_params):
        name = name[6:]
        assert name[-2:] == ':0'
        name = name[:-2]
        name = name.split('/')
        pointer = model
        for m_name in name:
            if re.fullmatch('[A-Za-z]+\\d+', m_name):
                l = re.split('(\\d+)', m_name)
            else:
                l = [m_name]
            if l[0] == 'g':
                pointer = getattr(pointer, 'weight')
            elif l[0] == 'b':
                pointer = getattr(pointer, 'bias')
            elif l[0] == 'w':
                pointer = getattr(pointer, 'weight')
            else:
                pointer = getattr(pointer, l[0])
            if len(l) >= 2:
                num = int(l[1])
                pointer = pointer[num]
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        logger.info('Initialize PyTorch weight {}'.format(name))
        pointer.data = torch.from_numpy(array)
    return model","def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):
    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)
    """"""
    import re
    import numpy as np
    if '.ckpt' in openai_checkpoint_folder_path:
        openai_checkpoint_folder_path = os.path.dirname(openai_checkpoint_folder_path)
    logger.info('Loading weights from {}'.format(openai_checkpoint_folder_path))
    names = json.load(open(openai_checkpoint_folder_path + '/parameters_names.json', 'r', encoding='utf-8'))
    shapes = json.load(open(openai_checkpoint_folder_path + '/params_shapes.json', 'r', encoding='utf-8'))
    offsets = np.cumsum([np.prod(shape) for shape in shapes])
    init_params = [np.load(openai_checkpoint_folder_path + '/params_{}.npy'.format(n)) for n in range(10)]
    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]
    init_params = [param.reshape(shape) for (param, shape) in zip(init_params, shapes)]
    init_params = [arr.squeeze() for arr in init_params]
    try:
        assert model.tokens_embed.weight.shape == init_params[1].shape
        assert model.positions_embed.weight.shape == init_params[0].shape
    except AssertionError as e:
        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)
        e.args += (model.positions_embed.weight.shape, init_params[0].shape)
        raise
    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])
    model.positions_embed.weight.data = torch.from_numpy(init_params[0])
    names.pop(0)
    init_params.pop(0)
    init_params.pop(0)
    for (name, array) in zip(names, init_params):
        name = name[6:]
        assert name[-2:] == ':0'
        name = name[:-2]
        name = name.split('/')
        pointer = model
        for m_name in name:
            if re.fullmatch('[A-Za-z]+\\d+', m_name):
                l = re.split('(\\d+)', m_name)
            else:
                l = [m_name]
            if l[0] == 'g':
                pointer = getattr(pointer, 'weight')
            elif l[0] == 'b':
                pointer = getattr(pointer, 'bias')
            elif l[0] == 'w':
                pointer = getattr(pointer, 'weight')
            else:
                pointer = getattr(pointer, l[0])
            if len(l) >= 2:
                num = int(l[1])
                pointer = pointer[num]
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        logger.info('Initialize PyTorch weight {}'.format(name))
        pointer.data = torch.from_numpy(array)
    return model","def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):
    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)
    """"""
    import re
    import numpy as np
    if '.ckpt' in openai_checkpoint_folder_path:
        openai_checkpoint_folder_path = os.path.dirname(openai_checkpoint_folder_path)
    logger.info('Loading weights from {}'.format(openai_checkpoint_folder_path))
    names = json.load(open(openai_checkpoint_folder_path + '/parameters_names.json', 'r', encoding='utf-8'))
    with open(openai_checkpoint_folder_path + '/params_shapes.json', 'r', encoding='utf-8') as f:
        shapes = json.load(f)
    offsets = np.cumsum([np.prod(shape) for shape in shapes])
    init_params = [np.load(openai_checkpoint_folder_path + '/params_{}.npy'.format(n)) for n in range(10)]
    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]
    init_params = [param.reshape(shape) for (param, shape) in zip(init_params, shapes)]
    init_params = [arr.squeeze() for arr in init_params]
    try:
        assert model.tokens_embed.weight.shape == init_params[1].shape
        assert model.positions_embed.weight.shape == init_params[0].shape
    except AssertionError as e:
        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)
        e.args += (model.positions_embed.weight.shape, init_params[0].shape)
        raise
    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])
    model.positions_embed.weight.data = torch.from_numpy(init_params[0])
    names.pop(0)
    init_params.pop(0)
    init_params.pop(0)
    for (name, array) in zip(names, init_params):
        name = name[6:]
        assert name[-2:] == ':0'
        name = name[:-2]
        name = name.split('/')
        pointer = model
        for m_name in name:
            if re.fullmatch('[A-Za-z]+\\d+', m_name):
                l = re.split('(\\d+)', m_name)
            else:
                l = [m_name]
            if l[0] == 'g':
                pointer = getattr(pointer, 'weight')
            elif l[0] == 'b':
                pointer = getattr(pointer, 'bias')
            elif l[0] == 'w':
                pointer = getattr(pointer, 'weight')
            else:
                pointer = getattr(pointer, l[0])
            if len(l) >= 2:
                num = int(l[1])
                pointer = pointer[num]
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        logger.info('Initialize PyTorch weight {}'.format(name))
        pointer.data = torch.from_numpy(array)
    return model",1,,,,,,,,,,
pyNES,https://github.com/gutomaia/pyNES/tree/master/pynes/tests/image_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyNES/pynes/tests/image_test.py,ImageTest,"def test_export_namespace(self):
    try:
        os.remove('/tmp/level.png')
    except:
        pass
    self.assertFileNotExists('/tmp/level.png')
    image.export_nametable('fixtures/nerdynights/scrolling/SMBlevel.bin', 'fixtures/nerdynights/scrolling/mario.chr', '/tmp/level.png')
    self.assertFileExists('/tmp/level.png')
    img = Image.open('/tmp/level.png')
    (sprs, indexes) = image.acquire_chr(img, optimize_repeated=False)
    sprite.length(sprs)
    self.assertEquals(1024, sprite.length(sprs))
    return
    nt_file = open('fixtures/nerdynights/scrolling/SMBlevel.bin')
    nt = nt_file.read()
    nt_file.close()
    nts = [ord(n) for n in nt]
    mario = sprite.load_sprites('fixtures/nerdynights/scrolling/mario.chr')
    for i in range(32):
        for j in range(32):
            self.assertSpriteEquals(sprite.get_sprite(nts[i * j] + 256, mario), sprite.get_sprite(i * j, sprs))
    os.remove('/tmp/level.png')","def test_export_namespace(self):
    try:
        os.remove('/tmp/level.png')
    except:
        pass
    self.assertFileNotExists('/tmp/level.png')
    image.export_nametable('fixtures/nerdynights/scrolling/SMBlevel.bin', 'fixtures/nerdynights/scrolling/mario.chr', '/tmp/level.png')
    self.assertFileExists('/tmp/level.png')
    img = Image.open('/tmp/level.png')
    (sprs, indexes) = image.acquire_chr(img, optimize_repeated=False)
    sprite.length(sprs)
    self.assertEquals(1024, sprite.length(sprs))
    return
    nt_file = open('fixtures/nerdynights/scrolling/SMBlevel.bin')
    nt = nt_file.read()
    nt_file.close()
    nts = [ord(n) for n in nt]
    mario = sprite.load_sprites('fixtures/nerdynights/scrolling/mario.chr')
    for i in range(32):
        for j in range(32):
            self.assertSpriteEquals(sprite.get_sprite(nts[i * j] + 256, mario), sprite.get_sprite(i * j, sprs))
    os.remove('/tmp/level.png')","def test_export_namespace(self):
    try:
        os.remove('/tmp/level.png')
    except:
        pass
    self.assertFileNotExists('/tmp/level.png')
    image.export_nametable('fixtures/nerdynights/scrolling/SMBlevel.bin', 'fixtures/nerdynights/scrolling/mario.chr', '/tmp/level.png')
    self.assertFileExists('/tmp/level.png')
    img = Image.open('/tmp/level.png')
    (sprs, indexes) = image.acquire_chr(img, optimize_repeated=False)
    sprite.length(sprs)
    self.assertEquals(1024, sprite.length(sprs))
    with open('fixtures/nerdynights/scrolling/SMBlevel.bin') as nt_file:
        nt = nt_file.read()
    nts = [ord(n) for n in nt]
    mario = sprite.load_sprites('fixtures/nerdynights/scrolling/mario.chr')
    for i in range(32):
        for j in range(32):
            self.assertSpriteEquals(sprite.get_sprite(nts[i * j] + 256, mario), sprite.get_sprite(i * j, sprs))
    os.remove('/tmp/level.png')",1,,,,,,,,,,
eliot,https://github.com/itamarst/eliot/tree/master/eliot/tests/test_output.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/eliot/eliot/tests/test_output.py,ToFileTests,"def test_filedestination_flushes(self):
    """"""
        L{FileDestination} flushes after every write, to ensure logs get
        written out even if the local buffer hasn't filled up.
        """"""
    path = mktemp()
    f = open(path, 'wb', 1024 * 1024 * 10)
    message1 = {'x': 123}
    destination = FileDestination(file=f)
    destination(message1)
    self.assertEqual([json.loads(line) for line in open(path, 'rb').read().splitlines()], [message1])","def test_filedestination_flushes(self):
    """"""
        L{FileDestination} flushes after every write, to ensure logs get
        written out even if the local buffer hasn't filled up.
        """"""
    path = mktemp()
    f = open(path, 'wb', 1024 * 1024 * 10)
    message1 = {'x': 123}
    destination = FileDestination(file=f)
    destination(message1)
    self.assertEqual([json.loads(line) for line in open(path, 'rb').read().splitlines()], [message1])","def test_filedestination_flushes(self):
    """"""
        L{FileDestination} flushes after every write, to ensure logs get
        written out even if the local buffer hasn't filled up.
        """"""
    path = mktemp()
    with open(path, 'wb', 1024 * 1024 * 10) as f:
        message1 = {'x': 123}
        destination = FileDestination(file=f)
        destination(message1)
        self.assertEqual([json.loads(line) for line in open(path, 'rb').read().splitlines()], [message1])",1,,,,,,,,,,
eliot,https://github.com/itamarst/eliot/tree/master/eliot/tests/test_output.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/eliot/eliot/tests/test_output.py,ToFileTests,"def test_filedestination_flushes(self):
    """"""
        L{FileDestination} flushes after every write, to ensure logs get
        written out even if the local buffer hasn't filled up.
        """"""
    path = mktemp()
    f = open(path, 'wb', 1024 * 1024 * 10)
    message1 = {'x': 123}
    destination = FileDestination(file=f)
    destination(message1)
    self.assertEqual([json.loads(line) for line in open(path, 'rb').read().splitlines()], [message1])","def test_filedestination_flushes(self):
    """"""
        L{FileDestination} flushes after every write, to ensure logs get
        written out even if the local buffer hasn't filled up.
        """"""
    path = mktemp()
    f = open(path, 'wb', 1024 * 1024 * 10)
    message1 = {'x': 123}
    destination = FileDestination(file=f)
    destination(message1)
    self.assertEqual([json.loads(line) for line in open(path, 'rb').read().splitlines()], [message1])","def test_filedestination_flushes(self):
    """"""
        L{FileDestination} flushes after every write, to ensure logs get
        written out even if the local buffer hasn't filled up.
        """"""
    path = mktemp()
    f = open(path, 'wb', 1024 * 1024 * 10)
    message1 = {'x': 123}
    destination = FileDestination(file=f)
    destination(message1)
    with open(path, 'rb') as f:
        self.assertEqual([json.loads(line) for line in f.read().splitlines()], [message1])",1,,,,,,,,,,
luigi,https://github.com/spotify/luigi/tree/master/luigi/contrib/scalding.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/luigi/luigi/contrib/scalding.py,ScaldingJobRunner,"def get_job_class(self, source):
    job_name = os.path.splitext(os.path.basename(source))[0]
    package = None
    job_class = None
    for line in open(source).readlines():
        p = re.search('package\\s+([^\\s\\(]+)', line)
        if p:
            package = p.groups()[0]
        p = re.search('class\\s+([^\\s\\(]+).*extends\\s+.*Job', line)
        if p:
            job_class = p.groups()[0]
            if job_class == job_name:
                break
    if job_class:
        if package:
            job_class = package + '.' + job_class
        logger.debug('Found scalding job class: %s', job_class)
        return job_class
    else:
        raise luigi.contrib.hadoop.HadoopJobError('Coudl not find scalding job class.')","def get_job_class(self, source):
    job_name = os.path.splitext(os.path.basename(source))[0]
    package = None
    job_class = None
    for line in open(source).readlines():
        p = re.search('package\\s+([^\\s\\(]+)', line)
        if p:
            package = p.groups()[0]
        p = re.search('class\\s+([^\\s\\(]+).*extends\\s+.*Job', line)
        if p:
            job_class = p.groups()[0]
            if job_class == job_name:
                break
    if job_class:
        if package:
            job_class = package + '.' + job_class
        logger.debug('Found scalding job class: %s', job_class)
        return job_class
    else:
        raise luigi.contrib.hadoop.HadoopJobError('Coudl not find scalding job class.')","def get_job_class(self, source):
    job_name = os.path.splitext(os.path.basename(source))[0]
    package = None
    job_class = None
    with open(source, 'r') as f:
        for line in f.readlines():
            p = re.search('package\\s+([^\\s\\(]+)', line)
            if p:
                package = p.groups()[0]
            p = re.search('class\\s+([^\\s\\(]+).*extends\\s+.*Job', line)
            if p:
                job_class = p.groups()[0]
                if job_class == job_name:
                    break
    if job_class:
        if package:
            job_class = package + '.' + job_class
        logger.debug('Found scalding job class: %s', job_class)
        return job_class
    else:
        raise luigi.contrib.hadoop.HadoopJobError('Coudl not find scalding job class.')",1,,,,,,,,,,
Python-Backdoor,https://github.com/xp4xbox/Python-Backdoor/tree/master/src/archive/old_keylogger/source.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Python-Backdoor/src/archive/old_keylogger/source.py,,"def CheckForOption():
    global strLogs
    while True:
        time.sleep(0.5)
        if os.path.isfile(strSettings) and os.path.getsize(strSettings) > 0 and (os.path.getsize(strSettings) < 20):
            txtData = open(strSettings, 'r')
            txt = txtData.read()
            if txt == 'stop':
                txtData.close()
                open(strSettings, 'w').close()
                break
            elif txt == 'dump':
                txtData = open(TMP + '/spblog.txt', 'w')
                txtData.write(strLogs)
                txtData.close()
                strLogs = ''
                open(strSettings, 'w').close()
    os._exit(0)","if os.path.isfile(strSettings) and os.path.getsize(strSettings) > 0 and (os.path.getsize(strSettings) < 20):
    txtData = open(strSettings, 'r')
    txt = txtData.read()
    if txt == 'stop':
        txtData.close()
        open(strSettings, 'w').close()
        break
    elif txt == 'dump':
        txtData = open(TMP + '/spblog.txt', 'w')
        txtData.write(strLogs)
        txtData.close()
        strLogs = ''
        open(strSettings, 'w').close()","if os.path.isfile(strSettings) and os.path.getsize(strSettings) > 0 and (os.path.getsize(strSettings) < 20):
    with open(strSettings, 'r') as txtData:
        txt = txtData.read()
        if txt == 'stop':
            txtData.close()
            open(strSettings, 'w').close()
            break
        elif txt == 'dump':
            with open(TMP + '/spblog.txt', 'w') as txtData:
                txtData.write(strLogs)
            strLogs = ''
            open(strSettings, 'w').close()",1,,,,,,,,,,
Python-Backdoor,https://github.com/xp4xbox/Python-Backdoor/tree/master/src/archive/old_keylogger/source.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Python-Backdoor/src/archive/old_keylogger/source.py,,"def CheckForOption():
    global strLogs
    while True:
        time.sleep(0.5)
        if os.path.isfile(strSettings) and os.path.getsize(strSettings) > 0 and (os.path.getsize(strSettings) < 20):
            txtData = open(strSettings, 'r')
            txt = txtData.read()
            if txt == 'stop':
                txtData.close()
                open(strSettings, 'w').close()
                break
            elif txt == 'dump':
                txtData = open(TMP + '/spblog.txt', 'w')
                txtData.write(strLogs)
                txtData.close()
                strLogs = ''
                open(strSettings, 'w').close()
    os._exit(0)","if txt == 'dump':
    txtData = open(TMP + '/spblog.txt', 'w')
    txtData.write(strLogs)
    txtData.close()
    strLogs = ''
    open(strSettings, 'w').close()","if txt == 'dump':
    with open(TMP + '/spblog.txt', 'w') as txtData:
        txtData.write(strLogs)
    strLogs = ''
    open(strSettings, 'w').close()",1,,,,,,,,,,
Python-Backdoor,https://github.com/xp4xbox/Python-Backdoor/tree/master/src/archive/old_keylogger/source.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Python-Backdoor/src/archive/old_keylogger/source.py,,"def CheckForOption():
    global strLogs
    while True:
        time.sleep(0.5)
        if os.path.isfile(strSettings) and os.path.getsize(strSettings) > 0 and (os.path.getsize(strSettings) < 20):
            txtData = open(strSettings, 'r')
            txt = txtData.read()
            if txt == 'stop':
                txtData.close()
                open(strSettings, 'w').close()
                break
            elif txt == 'dump':
                txtData = open(TMP + '/spblog.txt', 'w')
                txtData.write(strLogs)
                txtData.close()
                strLogs = ''
                open(strSettings, 'w').close()
    os._exit(0)","if txt == 'stop':
    txtData.close()
    open(strSettings, 'w').close()
    break
elif txt == 'dump':
    txtData = open(TMP + '/spblog.txt', 'w')
    txtData.write(strLogs)
    txtData.close()
    strLogs = ''
    open(strSettings, 'w').close()","if txt == 'stop':
    txtData.close()
    with open(strSettings, 'w'):
        pass
    break
elif txt == 'dump':
    txtData = open(TMP + '/spblog.txt', 'w')
    txtData.write(strLogs)
    txtData.close()
    strLogs = ''
    with open(strSettings, 'w'):
        pass",1,,,,,,,,,,
Python-Backdoor,https://github.com/xp4xbox/Python-Backdoor/tree/master/src/archive/old_keylogger/source.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Python-Backdoor/src/archive/old_keylogger/source.py,,"def CheckForOption():
    global strLogs
    while True:
        time.sleep(0.5)
        if os.path.isfile(strSettings) and os.path.getsize(strSettings) > 0 and (os.path.getsize(strSettings) < 20):
            txtData = open(strSettings, 'r')
            txt = txtData.read()
            if txt == 'stop':
                txtData.close()
                open(strSettings, 'w').close()
                break
            elif txt == 'dump':
                txtData = open(TMP + '/spblog.txt', 'w')
                txtData.write(strLogs)
                txtData.close()
                strLogs = ''
                open(strSettings, 'w').close()
    os._exit(0)","if txt == 'dump':
    txtData = open(TMP + '/spblog.txt', 'w')
    txtData.write(strLogs)
    txtData.close()
    strLogs = ''
    open(strSettings, 'w').close()","if txt == 'dump':
    txtData = open(TMP + '/spblog.txt', 'w')
    txtData.write(strLogs)
    txtData.close()
    strLogs = ''
    with open(strSettings, 'w'):
        pass",1,,,,,,,,,,
Fabrik,https://github.com/Cloud-CV/Fabrik/tree/master/tests/unit/keras_app/test_views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Fabrik/tests/unit/keras_app/test_views.py,BatchNormImportTest,"def test_keras_import(self):
    model = Sequential()
    model.add(BatchNormalization(center=True, scale=True, beta_regularizer=regularizers.l2(0.01), gamma_regularizer=regularizers.l2(0.01), beta_constraint='max_norm', gamma_constraint='max_norm', input_shape=(16, 10)))
    model.build()
    json_string = Model.to_json(model)
    with open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'w') as out:
        json.dump(json.loads(json_string), out, indent=4)
    sample_file = open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'r')
    response = self.client.post(reverse('keras-import'), {'file': sample_file})
    response = json.loads(response.content)
    layerId = sorted(response['net'].keys())
    self.assertEqual(response['result'], 'success')
    self.assertEqual(response['net'][layerId[0]]['info']['type'], 'Scale')
    self.assertEqual(response['net'][layerId[1]]['info']['type'], 'BatchNorm')","def test_keras_import(self):
    model = Sequential()
    model.add(BatchNormalization(center=True, scale=True, beta_regularizer=regularizers.l2(0.01), gamma_regularizer=regularizers.l2(0.01), beta_constraint='max_norm', gamma_constraint='max_norm', input_shape=(16, 10)))
    model.build()
    json_string = Model.to_json(model)
    with open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'w') as out:
        json.dump(json.loads(json_string), out, indent=4)
    sample_file = open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'r')
    response = self.client.post(reverse('keras-import'), {'file': sample_file})
    response = json.loads(response.content)
    layerId = sorted(response['net'].keys())
    self.assertEqual(response['result'], 'success')
    self.assertEqual(response['net'][layerId[0]]['info']['type'], 'Scale')
    self.assertEqual(response['net'][layerId[1]]['info']['type'], 'BatchNorm')","def test_keras_import(self):
    model = Sequential()
    model.add(BatchNormalization(center=True, scale=True, beta_regularizer=regularizers.l2(0.01), gamma_regularizer=regularizers.l2(0.01), beta_constraint='max_norm', gamma_constraint='max_norm', input_shape=(16, 10)))
    model.build()
    json_string = Model.to_json(model)
    with open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'w') as out:
        json.dump(json.loads(json_string), out, indent=4)
    with open(os.path.join(settings.BASE_DIR, 'media', 'test.json'), 'r') as sample_file:
        response = self.client.post(reverse('keras-import'), {'file': sample_file})
    response = json.loads(response.content)
    layerId = sorted(response['net'].keys())
    self.assertEqual(response['result'], 'success')
    self.assertEqual(response['net'][layerId[0]]['info']['type'], 'Scale')
    self.assertEqual(response['net'][layerId[1]]['info']['type'], 'BatchNorm')",1,,,,,,,,,,
videos,https://github.com/3b1b/videos/tree/master/_2017/nn/network.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2017/nn/network.py,,"def save_organized_images(n_images_per_number=10):
    (training_data, validation_data, test_data) = load_data_wrapper()
    image_map = dict([(k, []) for k in range(10)])
    for (im, output_arr) in training_data:
        if min(list(map(len, list(image_map.values())))) >= n_images_per_number:
            break
        value = int(np.argmax(output_arr))
        if len(image_map[value]) >= n_images_per_number:
            continue
        image_map[value].append(im)
    data_file = open(IMAGE_MAP_DATA_FILE, mode='wb')
    pickle.dump(image_map, data_file)
    data_file.close()","def save_organized_images(n_images_per_number=10):
    (training_data, validation_data, test_data) = load_data_wrapper()
    image_map = dict([(k, []) for k in range(10)])
    for (im, output_arr) in training_data:
        if min(list(map(len, list(image_map.values())))) >= n_images_per_number:
            break
        value = int(np.argmax(output_arr))
        if len(image_map[value]) >= n_images_per_number:
            continue
        image_map[value].append(im)
    data_file = open(IMAGE_MAP_DATA_FILE, mode='wb')
    pickle.dump(image_map, data_file)
    data_file.close()","def save_organized_images(n_images_per_number=10):
    (training_data, validation_data, test_data) = load_data_wrapper()
    image_map = dict([(k, []) for k in range(10)])
    for (im, output_arr) in training_data:
        if min(list(map(len, list(image_map.values())))) >= n_images_per_number:
            break
        value = int(np.argmax(output_arr))
        if len(image_map[value]) >= n_images_per_number:
            continue
        image_map[value].append(im)
    with open(IMAGE_MAP_DATA_FILE, mode='wb') as data_file:
        pickle.dump(image_map, data_file)",1,,,,,,,,,,
RigNet,https://github.com/zhan-xu/RigNet/tree/master/utils/rig_parser.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RigNet/utils/rig_parser.py,Skel,"def save(self, filename):
    fout = open(filename, 'w')
    this_level = [self.root]
    hier_level = 1
    while this_level:
        next_level = []
        for p_node in this_level:
            pos = p_node.pos
            parent = p_node.parent.name if p_node.parent is not None else 'None'
            line = '{0} {1} {2:8f} {3:8f} {4:8f} {5}\n'.format(hier_level, p_node.name, pos[0], pos[1], pos[2], parent)
            fout.write(line)
            for c_node in p_node.children:
                next_level.append(c_node)
        this_level = next_level
        hier_level += 1
    fout.close()","def save(self, filename):
    fout = open(filename, 'w')
    this_level = [self.root]
    hier_level = 1
    while this_level:
        next_level = []
        for p_node in this_level:
            pos = p_node.pos
            parent = p_node.parent.name if p_node.parent is not None else 'None'
            line = '{0} {1} {2:8f} {3:8f} {4:8f} {5}\n'.format(hier_level, p_node.name, pos[0], pos[1], pos[2], parent)
            fout.write(line)
            for c_node in p_node.children:
                next_level.append(c_node)
        this_level = next_level
        hier_level += 1
    fout.close()","def save(self, filename):
    with open(filename, 'w') as fout:
        this_level = [self.root]
        hier_level = 1
        while this_level:
            next_level = []
            for p_node in this_level:
                pos = p_node.pos
                parent = p_node.parent.name if p_node.parent is not None else 'None'
                line = '{0} {1} {2:8f} {3:8f} {4:8f} {5}\n'.format(hier_level, p_node.name, pos[0], pos[1], pos[2], parent)
                fout.write(line)
                for c_node in p_node.children:
                    next_level.append(c_node)
            this_level = next_level
            hier_level += 1",1,,,,,,,,,,
EGVSR,https://github.com/Thmen/EGVSR/tree/master/codes/data/unpaired_lmdb_dataset.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EGVSR/codes/data/unpaired_lmdb_dataset.py,UnpairedLMDBDataset,"def __init__(self, data_opt, **kwargs):
    """""" LMDB dataset with unpaired data, for BD degradation
        """"""
    super(UnpairedLMDBDataset, self).__init__(data_opt, **kwargs)
    meta = pickle.load(open(osp.join(self.seq_dir, 'meta_info.pkl'), 'rb'))
    self.keys = sorted(meta['keys'])
    if self.filter_file is not None:
        with open(self.filter_file, 'r') as f:
            sel_seqs = {line.strip() for line in f}
        self.keys = list(filter(lambda x: self.parse_lmdb_key(x)[0] in sel_seqs, self.keys))
    self.env = None","def __init__(self, data_opt, **kwargs):
    """""" LMDB dataset with unpaired data, for BD degradation
        """"""
    super(UnpairedLMDBDataset, self).__init__(data_opt, **kwargs)
    meta = pickle.load(open(osp.join(self.seq_dir, 'meta_info.pkl'), 'rb'))
    self.keys = sorted(meta['keys'])
    if self.filter_file is not None:
        with open(self.filter_file, 'r') as f:
            sel_seqs = {line.strip() for line in f}
        self.keys = list(filter(lambda x: self.parse_lmdb_key(x)[0] in sel_seqs, self.keys))
    self.env = None","def __init__(self, data_opt, **kwargs):
    """""" LMDB dataset with unpaired data, for BD degradation
        """"""
    super(UnpairedLMDBDataset, self).__init__(data_opt, **kwargs)
    with open(osp.join(self.seq_dir, 'meta_info.pkl'), 'rb') as f:
        meta = pickle.load(f)
    self.keys = sorted(meta['keys'])
    if self.filter_file is not None:
        with open(self.filter_file, 'r') as f:
            sel_seqs = {line.strip() for line in f}
        self.keys = list(filter(lambda x: self.parse_lmdb_key(x)[0] in sel_seqs, self.keys))
    self.env = None",1,,,,,,,,,,
NLP-BERT--ChineseVersion,https://github.com/AlanTur1ng/NLP-BERT--ChineseVersion/tree/master/bert_pytorch/dataset/dataset.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NLP-BERT--ChineseVersion/bert_pytorch/dataset/dataset.py,BERTDataset,"def get_random_line(self):
    if self.on_memory:
        return self.lines[random.randrange(len(self.lines))][1]
    line = self.file.__next__()
    if line is None:
        self.file.close()
        self.file = open(self.corpus_path, 'r', encoding=self.encoding)
        for _ in range(random.randint(self.corpus_lines if self.corpus_lines < 1000 else 1000)):
            self.random_file.__next__()
        line = self.random_file.__next__()
    return line[:-1].split('\t')[1]","if line is None:
    self.file.close()
    self.file = open(self.corpus_path, 'r', encoding=self.encoding)
    for _ in range(random.randint(self.corpus_lines if self.corpus_lines < 1000 else 1000)):
        self.random_file.__next__()
    line = self.random_file.__next__()","with open(self.corpus_path, 'r', encoding=self.encoding) as file:
    if line is None:
        for _ in range(random.randint(self.corpus_lines if self.corpus_lines < 1000 else 1000)):
            self.random_file.__next__()
        line = self.random_file.__next__()",1,,,,,,,,,,
jcvi,https://github.com/tanghaibao/jcvi/tree/master/jcvi/variation/delly.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jcvi/jcvi/variation/delly.py,,"def mito(args):
    """"""
    %prog mito chrM.fa input.bam

    Identify mitochondrial deletions.
    """"""
    p = OptionParser(mito.__doc__)
    p.set_aws_opts(store='hli-mv-data-science/htang/mito-deletions')
    p.add_option('--realignonly', default=False, action='store_true', help='Realign only')
    p.add_option('--svonly', default=False, action='store_true', help='Run Realign => SV calls only')
    p.add_option('--support', default=1, type='int', help='Minimum number of supporting reads')
    p.set_home('speedseq', default='/mnt/software/speedseq/bin')
    p.set_cpus()
    (opts, args) = p.parse_args(args)
    if len(args) != 2:
        sys.exit(not p.print_help())
    (chrMfa, bamfile) = args
    store = opts.output_path
    cleanup = not opts.nocleanup
    if not op.exists(chrMfa):
        logging.debug('File `{}` missing. Exiting.'.format(chrMfa))
        return
    chrMfai = chrMfa + '.fai'
    if not op.exists(chrMfai):
        cmd = 'samtools index {}'.format(chrMfa)
        sh(cmd)
    if not bamfile.endswith('.bam'):
        bamfiles = [x.strip() for x in open(bamfile)]
    else:
        bamfiles = [bamfile]
    if store:
        computed = ls_s3(store)
        computed = [op.basename(x).split('.')[0] for x in computed if x.endswith('.depth')]
        remaining_samples = [x for x in bamfiles if op.basename(x).split('.')[0] not in computed]
        logging.debug('Already computed on `{}`: {}'.format(store, len(bamfiles) - len(remaining_samples)))
        bamfiles = remaining_samples
    logging.debug('Total samples: {}'.format(len(bamfiles)))
    for bamfile in bamfiles:
        run_mito(chrMfa, bamfile, opts, realignonly=opts.realignonly, svonly=opts.svonly, store=store, cleanup=cleanup)","if not bamfile.endswith('.bam'):
    bamfiles = [x.strip() for x in open(bamfile)]
else:
    bamfiles = [bamfile]","if not bamfile.endswith('.bam'):
    with open(bamfile) as f:
        bamfiles = [x.strip() for x in f]
else:
    bamfiles = [bamfile]",1,,,,,,,,,,
solo-learn,https://github.com/vturrisi/solo-learn/tree/master/tests/utils/test_auto_resumer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/solo-learn/tests/utils/test_auto_resumer.py,,"def test_checkpointer():
    method_kwargs = {'proj_hidden_dim': 2048, 'proj_output_dim': 2048, 'lamb': 0.005, 'scale_loss': 0.025}
    cfg = gen_base_cfg('barlow_twins', batch_size=2, num_classes=100)
    cfg.method_kwargs = method_kwargs
    cfg = Checkpointer.add_and_assert_specific_cfg(cfg)
    model = BarlowTwins(cfg)
    ckpt_callback = Checkpointer(cfg)
    trainer = gen_trainer(cfg, ckpt_callback)
    (train_dl, val_dl) = prepare_dummy_dataloaders('imagenet100', num_large_crops=cfg.data.num_large_crops, num_small_crops=cfg.data.num_small_crops, num_classes=cfg.data.num_classes, batch_size=cfg.optimizer.batch_size)
    trainer.fit(model, train_dl, val_dl)
    args_path = ckpt_callback.path / 'args.json'
    assert args_path.exists()
    loaded_cfg = json.load(open(args_path))
    cfg_dict = OmegaConf.to_container(cfg)
    for k in cfg_dict:
        assert cfg_dict[k] == loaded_cfg[k]
    auto_resumer = AutoResumer(ckpt_callback.logdir, max_hours=1)
    assert auto_resumer.find_checkpoint(cfg) is not None
    cfg = auto_resumer.add_and_assert_specific_cfg(cfg)
    assert not OmegaConf.is_missing(cfg, 'auto_resume')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.enabled')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.max_hours')
    shutil.rmtree(ckpt_callback.logdir)","def test_checkpointer():
    method_kwargs = {'proj_hidden_dim': 2048, 'proj_output_dim': 2048, 'lamb': 0.005, 'scale_loss': 0.025}
    cfg = gen_base_cfg('barlow_twins', batch_size=2, num_classes=100)
    cfg.method_kwargs = method_kwargs
    cfg = Checkpointer.add_and_assert_specific_cfg(cfg)
    model = BarlowTwins(cfg)
    ckpt_callback = Checkpointer(cfg)
    trainer = gen_trainer(cfg, ckpt_callback)
    (train_dl, val_dl) = prepare_dummy_dataloaders('imagenet100', num_large_crops=cfg.data.num_large_crops, num_small_crops=cfg.data.num_small_crops, num_classes=cfg.data.num_classes, batch_size=cfg.optimizer.batch_size)
    trainer.fit(model, train_dl, val_dl)
    args_path = ckpt_callback.path / 'args.json'
    assert args_path.exists()
    loaded_cfg = json.load(open(args_path))
    cfg_dict = OmegaConf.to_container(cfg)
    for k in cfg_dict:
        assert cfg_dict[k] == loaded_cfg[k]
    auto_resumer = AutoResumer(ckpt_callback.logdir, max_hours=1)
    assert auto_resumer.find_checkpoint(cfg) is not None
    cfg = auto_resumer.add_and_assert_specific_cfg(cfg)
    assert not OmegaConf.is_missing(cfg, 'auto_resume')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.enabled')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.max_hours')
    shutil.rmtree(ckpt_callback.logdir)","def test_checkpointer():
    method_kwargs = {'proj_hidden_dim': 2048, 'proj_output_dim': 2048, 'lamb': 0.005, 'scale_loss': 0.025}
    cfg = gen_base_cfg('barlow_twins', batch_size=2, num_classes=100)
    cfg.method_kwargs = method_kwargs
    cfg = Checkpointer.add_and_assert_specific_cfg(cfg)
    model = BarlowTwins(cfg)
    ckpt_callback = Checkpointer(cfg)
    trainer = gen_trainer(cfg, ckpt_callback)
    (train_dl, val_dl) = prepare_dummy_dataloaders('imagenet100', num_large_crops=cfg.data.num_large_crops, num_small_crops=cfg.data.num_small_crops, num_classes=cfg.data.num_classes, batch_size=cfg.optimizer.batch_size)
    trainer.fit(model, train_dl, val_dl)
    args_path = ckpt_callback.path / 'args.json'
    assert args_path.exists()
    with open(args_path) as f:
        loaded_cfg = json.load(f)
    cfg_dict = OmegaConf.to_container(cfg)
    for k in cfg_dict:
        assert cfg_dict[k] == loaded_cfg[k]
    auto_resumer = AutoResumer(ckpt_callback.logdir, max_hours=1)
    assert auto_resumer.find_checkpoint(cfg) is not None
    cfg = auto_resumer.add_and_assert_specific_cfg(cfg)
    assert not OmegaConf.is_missing(cfg, 'auto_resume')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.enabled')
    assert not OmegaConf.is_missing(cfg, 'auto_resume.max_hours')
    shutil.rmtree(ckpt_callback.logdir)",1,,,,,,,,,,
TextAttack,https://github.com/QData/TextAttack/tree/master/tests/test_command_line/test_eval.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TextAttack/tests/test_command_line/test_eval.py,,"def test_command_line_eval(name, command, sample_output_file):
    """"""Tests the command-line function, `textattack eval`.

    Different from other tests, this one compares the sample output file
    to *stderr* output of the evaluation.
    """"""
    desired_text = open(sample_output_file).read().strip()
    desired_text_lines = desired_text.split('\n')
    result = run_command_and_get_result(command)
    assert result.stdout is not None
    assert result.stderr is not None
    stdout = result.stdout.decode().strip()
    print('stdout =>', stdout)
    stderr = result.stderr.decode().strip()
    print('stderr =>', stderr)
    print('desired_text =>', desired_text)
    stderr_lines = stderr.split('\n')
    assert desired_text_lines <= stderr_lines
    assert result.returncode == 0","def test_command_line_eval(name, command, sample_output_file):
    """"""Tests the command-line function, `textattack eval`.

    Different from other tests, this one compares the sample output file
    to *stderr* output of the evaluation.
    """"""
    desired_text = open(sample_output_file).read().strip()
    desired_text_lines = desired_text.split('\n')
    result = run_command_and_get_result(command)
    assert result.stdout is not None
    assert result.stderr is not None
    stdout = result.stdout.decode().strip()
    print('stdout =>', stdout)
    stderr = result.stderr.decode().strip()
    print('stderr =>', stderr)
    print('desired_text =>', desired_text)
    stderr_lines = stderr.split('\n')
    assert desired_text_lines <= stderr_lines
    assert result.returncode == 0","def test_command_line_eval(name, command, sample_output_file):
    """"""Tests the command-line function, `textattack eval`.

    Different from other tests, this one compares the sample output file
    to *stderr* output of the evaluation.
    """"""
    with open(sample_output_file) as f:
        desired_text = f.read().strip()
    desired_text_lines = desired_text.split('\n')
    result = run_command_and_get_result(command)
    assert result.stdout is not None
    assert result.stderr is not None
    stdout = result.stdout.decode().strip()
    print('stdout =>', stdout)
    stderr = result.stderr.decode().strip()
    print('stderr =>', stderr)
    print('desired_text =>', desired_text)
    stderr_lines = stderr.split('\n')
    assert desired_text_lines <= stderr_lines
    assert result.returncode == 0",1,,,,,,,,,,
keras-YOLOv3-mobilenet,https://github.com/Adamdad/keras-YOLOv3-mobilenet/tree/master//kmeans.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras-YOLOv3-mobilenet//kmeans.py,YOLO_Kmeans,"def txt2boxes(self):
    f = open(self.filename, 'r')
    dataSet = []
    for line in f:
        infos = line.split(' ')
        length = len(infos)
        for i in range(1, length):
            width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
            height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
            dataSet.append([width, height])
    result = np.array(dataSet)
    f.close()
    return result","def txt2boxes(self):
    f = open(self.filename, 'r')
    dataSet = []
    for line in f:
        infos = line.split(' ')
        length = len(infos)
        for i in range(1, length):
            width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
            height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
            dataSet.append([width, height])
    result = np.array(dataSet)
    f.close()
    return result","def txt2boxes(self):
    with open(self.filename, 'r') as f:
        dataSet = []
        for line in f:
            infos = line.split(' ')
            length = len(infos)
            for i in range(1, length):
                width = int(infos[i].split(',')[2]) - int(infos[i].split(',')[0])
                height = int(infos[i].split(',')[3]) - int(infos[i].split(',')[1])
                dataSet.append([width, height])
    result = np.array(dataSet)
    return result",1,,,,,,,,,,
django-rest-framework-mongoengine,https://github.com/umutbozkurt/django-rest-framework-mongoengine/tree/master/tests/test_files.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-rest-framework-mongoengine/tests/test_files.py,TestFilesIntegration,"def setUp(self):
    self.files = [open(pwd + 'cat1.jpg', 'rb'), open(pwd + 'cat2.jpg', 'rb')]
    self.uploads = [UploadedFile(f, f.name, 'image/jpeg', os.path.getsize(f.name)) for f in self.files]","def setUp(self):
    self.files = [open(pwd + 'cat1.jpg', 'rb'), open(pwd + 'cat2.jpg', 'rb')]
    self.uploads = [UploadedFile(f, f.name, 'image/jpeg', os.path.getsize(f.name)) for f in self.files]","def setUp(self):
    with open(pwd + 'cat1.jpg', 'rb') as f1, open(pwd + 'cat2.jpg', 'rb') as f2:
        self.files = [f1, f2]
        self.uploads = [UploadedFile(f, f.name, 'image/jpeg', os.path.getsize(f.name)) for f in self.files]",1,,,,,,,,,,
django-rest-framework-mongoengine,https://github.com/umutbozkurt/django-rest-framework-mongoengine/tree/master/tests/test_files.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-rest-framework-mongoengine/tests/test_files.py,TestFilesIntegration,"def setUp(self):
    self.files = [open(pwd + 'cat1.jpg', 'rb'), open(pwd + 'cat2.jpg', 'rb')]
    self.uploads = [UploadedFile(f, f.name, 'image/jpeg', os.path.getsize(f.name)) for f in self.files]","def setUp(self):
    self.files = [open(pwd + 'cat1.jpg', 'rb'), open(pwd + 'cat2.jpg', 'rb')]
    self.uploads = [UploadedFile(f, f.name, 'image/jpeg', os.path.getsize(f.name)) for f in self.files]","def setUp(self):
    with open(pwd + 'cat1.jpg', 'rb') as f1, open(pwd + 'cat2.jpg', 'rb') as f2:
        self.files = [f1, f2]
        self.uploads = [UploadedFile(f, f.name, 'image/jpeg', os.path.getsize(f.name)) for f in self.files]",1,,,,,,,,,,
assistant-sdk-python,https://github.com/googlesamples/assistant-sdk-python/tree/master/google-assistant-sdk/googlesamples/assistant/grpc/pushtotalk.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/assistant-sdk-python/google-assistant-sdk/googlesamples/assistant/grpc/pushtotalk.py,,"def main(api_endpoint, credentials, project_id, device_model_id, device_id, device_config, lang, display, verbose, input_audio_file, output_audio_file, audio_sample_rate, audio_sample_width, audio_iter_size, audio_block_size, audio_flush_size, grpc_deadline, once, *args, **kwargs):
    """"""Samples for the Google Assistant API.

    Examples:
      Run the sample with microphone input and speaker output:

        $ python -m googlesamples.assistant

      Run the sample with file input and speaker output:

        $ python -m googlesamples.assistant -i <input file>

      Run the sample with file input and output:

        $ python -m googlesamples.assistant -i <input file> -o <output file>
    """"""
    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)
    try:
        with open(credentials, 'r') as f:
            credentials = google.oauth2.credentials.Credentials(token=None, **json.load(f))
            http_request = google.auth.transport.requests.Request()
            credentials.refresh(http_request)
    except Exception as e:
        logging.error('Error loading credentials: %s', e)
        logging.error('Run google-oauthlib-tool to initialize new OAuth 2.0 credentials.')
        sys.exit(-1)
    grpc_channel = google.auth.transport.grpc.secure_authorized_channel(credentials, http_request, api_endpoint)
    logging.info('Connecting to %s', api_endpoint)
    audio_device = None
    if input_audio_file:
        audio_source = audio_helpers.WaveSource(open(input_audio_file, 'rb'), sample_rate=audio_sample_rate, sample_width=audio_sample_width)
    else:
        audio_source = audio_device = audio_device or audio_helpers.SoundDeviceStream(sample_rate=audio_sample_rate, sample_width=audio_sample_width, block_size=audio_block_size, flush_size=audio_flush_size)
    if output_audio_file:
        audio_sink = audio_helpers.WaveSink(open(output_audio_file, 'wb'), sample_rate=audio_sample_rate, sample_width=audio_sample_width)
    else:
        audio_sink = audio_device = audio_device or audio_helpers.SoundDeviceStream(sample_rate=audio_sample_rate, sample_width=audio_sample_width, block_size=audio_block_size, flush_size=audio_flush_size)
    conversation_stream = audio_helpers.ConversationStream(source=audio_source, sink=audio_sink, iter_size=audio_iter_size, sample_width=audio_sample_width)
    if not device_id or not device_model_id:
        try:
            with open(device_config) as f:
                device = json.load(f)
                device_id = device['id']
                device_model_id = device['model_id']
                logging.info('Using device model %s and device id %s', device_model_id, device_id)
        except Exception as e:
            logging.warning('Device config not found: %s' % e)
            logging.info('Registering device')
            if not device_model_id:
                logging.error('Option --device-model-id required when registering a device instance.')
                sys.exit(-1)
            if not project_id:
                logging.error('Option --project-id required when registering a device instance.')
                sys.exit(-1)
            device_base_url = 'https://%s/v1alpha2/projects/%s/devices' % (api_endpoint, project_id)
            device_id = str(uuid.uuid1())
            payload = {'id': device_id, 'model_id': device_model_id, 'client_type': 'SDK_SERVICE'}
            session = google.auth.transport.requests.AuthorizedSession(credentials)
            r = session.post(device_base_url, data=json.dumps(payload))
            if r.status_code != 200:
                logging.error('Failed to register device: %s', r.text)
                sys.exit(-1)
            logging.info('Device registered: %s', device_id)
            pathlib.Path(os.path.dirname(device_config)).mkdir(exist_ok=True)
            with open(device_config, 'w') as f:
                json.dump(payload, f)
    device_handler = device_helpers.DeviceRequestHandler(device_id)

    @device_handler.command('action.devices.commands.OnOff')
    def onoff(on):
        if on:
            logging.info('Turning device on')
        else:
            logging.info('Turning device off')

    @device_handler.command('com.example.commands.BlinkLight')
    def blink(speed, number):
        logging.info('Blinking device %s times.' % number)
        delay = 1
        if speed == 'SLOWLY':
            delay = 2
        elif speed == 'QUICKLY':
            delay = 0.5
        for i in range(int(number)):
            logging.info('Device is blinking.')
            time.sleep(delay)
    with SampleAssistant(lang, device_model_id, device_id, conversation_stream, display, grpc_channel, grpc_deadline, device_handler) as assistant:
        if input_audio_file or output_audio_file:
            assistant.assist()
            return
        wait_for_user_trigger = not once
        while True:
            if wait_for_user_trigger:
                click.pause(info='Press Enter to send a new request...')
            continue_conversation = assistant.assist()
            wait_for_user_trigger = not continue_conversation
            if once and (not continue_conversation):
                break","if input_audio_file:
    audio_source = audio_helpers.WaveSource(open(input_audio_file, 'rb'), sample_rate=audio_sample_rate, sample_width=audio_sample_width)
else:
    audio_source = audio_device = audio_device or audio_helpers.SoundDeviceStream(sample_rate=audio_sample_rate, sample_width=audio_sample_width, block_size=audio_block_size, flush_size=audio_flush_size)","with open(input_audio_file, 'rb') as f:
    audio_source = audio_helpers.WaveSource(f, sample_rate=audio_sample_rate, sample_width=audio_sample_width) if input_audio_file else audio_device = audio_device or audio_helpers.SoundDeviceStream(sample_rate=audio_sample_rate, sample_width=audio_sample_width, block_size=audio_block_size, flush_size=audio_flush_size)",1,,,,,,,,,,
assistant-sdk-python,https://github.com/googlesamples/assistant-sdk-python/tree/master/google-assistant-sdk/googlesamples/assistant/grpc/pushtotalk.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/assistant-sdk-python/google-assistant-sdk/googlesamples/assistant/grpc/pushtotalk.py,,"def main(api_endpoint, credentials, project_id, device_model_id, device_id, device_config, lang, display, verbose, input_audio_file, output_audio_file, audio_sample_rate, audio_sample_width, audio_iter_size, audio_block_size, audio_flush_size, grpc_deadline, once, *args, **kwargs):
    """"""Samples for the Google Assistant API.

    Examples:
      Run the sample with microphone input and speaker output:

        $ python -m googlesamples.assistant

      Run the sample with file input and speaker output:

        $ python -m googlesamples.assistant -i <input file>

      Run the sample with file input and output:

        $ python -m googlesamples.assistant -i <input file> -o <output file>
    """"""
    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)
    try:
        with open(credentials, 'r') as f:
            credentials = google.oauth2.credentials.Credentials(token=None, **json.load(f))
            http_request = google.auth.transport.requests.Request()
            credentials.refresh(http_request)
    except Exception as e:
        logging.error('Error loading credentials: %s', e)
        logging.error('Run google-oauthlib-tool to initialize new OAuth 2.0 credentials.')
        sys.exit(-1)
    grpc_channel = google.auth.transport.grpc.secure_authorized_channel(credentials, http_request, api_endpoint)
    logging.info('Connecting to %s', api_endpoint)
    audio_device = None
    if input_audio_file:
        audio_source = audio_helpers.WaveSource(open(input_audio_file, 'rb'), sample_rate=audio_sample_rate, sample_width=audio_sample_width)
    else:
        audio_source = audio_device = audio_device or audio_helpers.SoundDeviceStream(sample_rate=audio_sample_rate, sample_width=audio_sample_width, block_size=audio_block_size, flush_size=audio_flush_size)
    if output_audio_file:
        audio_sink = audio_helpers.WaveSink(open(output_audio_file, 'wb'), sample_rate=audio_sample_rate, sample_width=audio_sample_width)
    else:
        audio_sink = audio_device = audio_device or audio_helpers.SoundDeviceStream(sample_rate=audio_sample_rate, sample_width=audio_sample_width, block_size=audio_block_size, flush_size=audio_flush_size)
    conversation_stream = audio_helpers.ConversationStream(source=audio_source, sink=audio_sink, iter_size=audio_iter_size, sample_width=audio_sample_width)
    if not device_id or not device_model_id:
        try:
            with open(device_config) as f:
                device = json.load(f)
                device_id = device['id']
                device_model_id = device['model_id']
                logging.info('Using device model %s and device id %s', device_model_id, device_id)
        except Exception as e:
            logging.warning('Device config not found: %s' % e)
            logging.info('Registering device')
            if not device_model_id:
                logging.error('Option --device-model-id required when registering a device instance.')
                sys.exit(-1)
            if not project_id:
                logging.error('Option --project-id required when registering a device instance.')
                sys.exit(-1)
            device_base_url = 'https://%s/v1alpha2/projects/%s/devices' % (api_endpoint, project_id)
            device_id = str(uuid.uuid1())
            payload = {'id': device_id, 'model_id': device_model_id, 'client_type': 'SDK_SERVICE'}
            session = google.auth.transport.requests.AuthorizedSession(credentials)
            r = session.post(device_base_url, data=json.dumps(payload))
            if r.status_code != 200:
                logging.error('Failed to register device: %s', r.text)
                sys.exit(-1)
            logging.info('Device registered: %s', device_id)
            pathlib.Path(os.path.dirname(device_config)).mkdir(exist_ok=True)
            with open(device_config, 'w') as f:
                json.dump(payload, f)
    device_handler = device_helpers.DeviceRequestHandler(device_id)

    @device_handler.command('action.devices.commands.OnOff')
    def onoff(on):
        if on:
            logging.info('Turning device on')
        else:
            logging.info('Turning device off')

    @device_handler.command('com.example.commands.BlinkLight')
    def blink(speed, number):
        logging.info('Blinking device %s times.' % number)
        delay = 1
        if speed == 'SLOWLY':
            delay = 2
        elif speed == 'QUICKLY':
            delay = 0.5
        for i in range(int(number)):
            logging.info('Device is blinking.')
            time.sleep(delay)
    with SampleAssistant(lang, device_model_id, device_id, conversation_stream, display, grpc_channel, grpc_deadline, device_handler) as assistant:
        if input_audio_file or output_audio_file:
            assistant.assist()
            return
        wait_for_user_trigger = not once
        while True:
            if wait_for_user_trigger:
                click.pause(info='Press Enter to send a new request...')
            continue_conversation = assistant.assist()
            wait_for_user_trigger = not continue_conversation
            if once and (not continue_conversation):
                break","if output_audio_file:
    audio_sink = audio_helpers.WaveSink(open(output_audio_file, 'wb'), sample_rate=audio_sample_rate, sample_width=audio_sample_width)
else:
    audio_sink = audio_device = audio_device or audio_helpers.SoundDeviceStream(sample_rate=audio_sample_rate, sample_width=audio_sample_width, block_size=audio_block_size, flush_size=audio_flush_size)","with open(output_audio_file, 'wb') as f:
    audio_sink = audio_helpers.WaveSink(f, sample_rate=audio_sample_rate, sample_width=audio_sample_width) if output_audio_file else audio_device = audio_device or audio_helpers.SoundDeviceStream(sample_rate=audio_sample_rate, sample_width=audio_sample_width, block_size=audio_block_size, flush_size=audio_flush_size)",1,,,,,,,,,,
pylearn2,https://github.com/lisa-lab/pylearn2/tree/master/pylearn2/scripts/datasets/step_through_small_norb.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pylearn2/pylearn2/scripts/datasets/step_through_small_norb.py,,"def load_norb(args):
    if args.which_set in ('test', 'train'):
        return SmallNORB(args.which_set, True)
    else:
        norb_file = open(args.which_set)
        return pickle.load(norb_file)","if args.which_set in ('test', 'train'):
    return SmallNORB(args.which_set, True)
else:
    norb_file = open(args.which_set)
    return pickle.load(norb_file)","if args.which_set in ('test', 'train'):
    return SmallNORB(args.which_set, True)
else:
    with open(args.which_set, 'rb') as norb_file:
        return pickle.load(norb_file)",1,,,,,,,,,,
borgmatic,https://github.com/borgmatic-collective/borgmatic/tree/master/borgmatic/config/load.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/borgmatic/borgmatic/config/load.py,,"def load_configuration(filename):
    """"""
    Load the given configuration file and return its contents as a data structure of nested dicts
    and lists.

    Raise ruamel.yaml.error.YAMLError if something goes wrong parsing the YAML, or RecursionError
    if there are too many recursive includes.
    """"""
    yaml = ruamel.yaml.YAML(typ='safe')
    yaml.Constructor = Include_constructor
    return yaml.load(open(filename))","def load_configuration(filename):
    """"""
    Load the given configuration file and return its contents as a data structure of nested dicts
    and lists.

    Raise ruamel.yaml.error.YAMLError if something goes wrong parsing the YAML, or RecursionError
    if there are too many recursive includes.
    """"""
    yaml = ruamel.yaml.YAML(typ='safe')
    yaml.Constructor = Include_constructor
    return yaml.load(open(filename))","def load_configuration(filename):
    """"""
    Load the given configuration file and return its contents as a data structure of nested dicts
    and lists.

    Raise ruamel.yaml.error.YAMLError if something goes wrong parsing the YAML, or RecursionError
    if there are too many recursive includes.
    """"""
    yaml = ruamel.yaml.YAML(typ='safe')
    yaml.Constructor = Include_constructor
    with open(filename) as f:
        return yaml.load(f)",1,,,,,,,,,,
openpilot,https://github.com/commaai/openpilot/tree/master/tools/sim/lib/manual_ctrl.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openpilot/tools/sim/lib/manual_ctrl.py,,"def wheel_poll_thread(q: 'Queue[str]') -> NoReturn:
    fn = '/dev/input/js0'
    print('Opening %s...' % fn)
    jsdev = open(fn, 'rb')
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2147510803 + 65536 * len(buf), buf)
    js_name = buf.tobytes().rstrip(b'\x00').decode('utf-8')
    print('Device name: %s' % js_name)
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576337, buf)
    num_axes = buf[0]
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576338, buf)
    num_buttons = buf[0]
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2151705138, buf)
    for _axis in buf[:num_axes]:
        axis_name = axis_names.get(_axis, 'unknown(0x%02x)' % _axis)
        axis_map.append(axis_name)
        axis_states[axis_name] = 0.0
    buf = array.array('H', [0] * 200)
    ioctl(jsdev, 2151705140, buf)
    for btn in buf[:num_buttons]:
        btn_name = button_names.get(btn, 'unknown(0x%03x)' % btn)
        button_map.append(btn_name)
        button_states[btn_name] = 0
    print('%d axes found: %s' % (num_axes, ', '.join(axis_map)))
    print('%d buttons found: %s' % (num_buttons, ', '.join(button_map)))
    import evdev
    from evdev import ecodes, InputDevice
    device = evdev.list_devices()[0]
    evtdev = InputDevice(device)
    val = 24000
    evtdev.write(ecodes.EV_FF, ecodes.FF_AUTOCENTER, val)
    while True:
        evbuf = jsdev.read(8)
        (value, mtype, number) = struct.unpack('4xhBB', evbuf)
        if mtype & 2:
            axis = axis_map[number]
            if axis == 'z':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('throttle_%f' % normalized)
            elif axis == 'rz':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('brake_%f' % normalized)
            elif axis == 'x':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = fvalue
                q.put('steer_%f' % normalized)
        elif mtype & 1:
            if value == 1:
                if number in [0, 19]:
                    q.put('cruise_down')
                elif number in [3, 18]:
                    q.put('cruise_up')
                elif number in [1, 6]:
                    q.put('cruise_cancel')
                elif number in [10, 21]:
                    q.put('reverse_switch')","def wheel_poll_thread(q: 'Queue[str]') -> NoReturn:
    fn = '/dev/input/js0'
    print('Opening %s...' % fn)
    jsdev = open(fn, 'rb')
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2147510803 + 65536 * len(buf), buf)
    js_name = buf.tobytes().rstrip(b'\x00').decode('utf-8')
    print('Device name: %s' % js_name)
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576337, buf)
    num_axes = buf[0]
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576338, buf)
    num_buttons = buf[0]
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2151705138, buf)
    for _axis in buf[:num_axes]:
        axis_name = axis_names.get(_axis, 'unknown(0x%02x)' % _axis)
        axis_map.append(axis_name)
        axis_states[axis_name] = 0.0
    buf = array.array('H', [0] * 200)
    ioctl(jsdev, 2151705140, buf)
    for btn in buf[:num_buttons]:
        btn_name = button_names.get(btn, 'unknown(0x%03x)' % btn)
        button_map.append(btn_name)
        button_states[btn_name] = 0
    print('%d axes found: %s' % (num_axes, ', '.join(axis_map)))
    print('%d buttons found: %s' % (num_buttons, ', '.join(button_map)))
    import evdev
    from evdev import ecodes, InputDevice
    device = evdev.list_devices()[0]
    evtdev = InputDevice(device)
    val = 24000
    evtdev.write(ecodes.EV_FF, ecodes.FF_AUTOCENTER, val)
    while True:
        evbuf = jsdev.read(8)
        (value, mtype, number) = struct.unpack('4xhBB', evbuf)
        if mtype & 2:
            axis = axis_map[number]
            if axis == 'z':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('throttle_%f' % normalized)
            elif axis == 'rz':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('brake_%f' % normalized)
            elif axis == 'x':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = fvalue
                q.put('steer_%f' % normalized)
        elif mtype & 1:
            if value == 1:
                if number in [0, 19]:
                    q.put('cruise_down')
                elif number in [3, 18]:
                    q.put('cruise_up')
                elif number in [1, 6]:
                    q.put('cruise_cancel')
                elif number in [10, 21]:
                    q.put('reverse_switch')","def wheel_poll_thread(q: 'Queue[str]') -> NoReturn:
    fn = '/dev/input/js0'
    print('Opening %s...' % fn)
    with open(fn, 'rb') as jsdev:
        buf = array.array('B', [0] * 64)
        ioctl(jsdev, 2147510803 + 65536 * len(buf), buf)
        js_name = buf.tobytes().rstrip(b'\x00').decode('utf-8')
        print('Device name: %s' % js_name)
        buf = array.array('B', [0])
        ioctl(jsdev, 2147576337, buf)
        num_axes = buf[0]
        buf = array.array('B', [0])
        ioctl(jsdev, 2147576338, buf)
        num_buttons = buf[0]
        buf = array.array('B', [0] * 64)
        ioctl(jsdev, 2151705138, buf)
        for _axis in buf[:num_axes]:
            axis_name = axis_names.get(_axis, 'unknown(0x%02x)' % _axis)
            axis_map.append(axis_name)
            axis_states[axis_name] = 0.0
        buf = array.array('H', [0] * 200)
        ioctl(jsdev, 2151705140, buf)
        for btn in buf[:num_buttons]:
            btn_name = button_names.get(btn, 'unknown(0x%03x)' % btn)
            button_map.append(btn_name)
            button_states[btn_name] = 0
        print('%d axes found: %s' % (num_axes, ', '.join(axis_map)))
        print('%d buttons found: %s' % (num_buttons, ', '.join(button_map)))
        import evdev
        from evdev import ecodes, InputDevice
        device = evdev.list_devices()[0]
        evtdev = InputDevice(device)
        val = 24000
        evtdev.write(ecodes.EV_FF, ecodes.FF_AUTOCENTER, val)
        while True:
            evbuf = jsdev.read(8)
            (value, mtype, number) = struct.unpack('4xhBB', evbuf)
            if mtype & 2:
                axis = axis_map[number]
                if axis == 'z':
                    fvalue = value / 32767.0
                    axis_states[axis] = fvalue
                    normalized = (1 - fvalue) * 50
                    q.put('throttle_%f' % normalized)
                elif axis == 'rz':
                    fvalue = value / 32767.0
                    axis_states[axis] = fvalue
                    normalized = (1 - fvalue) * 50
                    q.put('brake_%f' % normalized)
                elif axis == 'x':
                    fvalue = value / 32767.0
                    axis_states[axis] = fvalue
                    normalized = fvalue
                    q.put('steer_%f' % normalized)
            elif mtype & 1:
                if value == 1:
                    if number in [0, 19]:
                        q.put('cruise_down')
                    elif number in [3, 18]:
                        q.put('cruise_up')
                    elif number in [1, 6]:
                        q.put('cruise_cancel')
                    elif number in [10, 21]:
                        q.put('reverse_switch')",1,,,,,,,,,,
enumerate-iam,https://github.com/andresriancho/enumerate-iam/tree/master/enumerate_iam/generate_bruteforce_tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/enumerate-iam/enumerate_iam/generate_bruteforce_tests.py,,"def main():
    bruteforce_tests = dict()
    for filename in os.listdir(API_DEFINITIONS):
        if not filename.endswith('.min.json'):
            continue
        api_json_data = open(os.path.join(API_DEFINITIONS, filename)).read()
        api_json = json.loads(api_json_data)
        service_name = extract_service_name(filename, api_json)
        if service_name is None:
            print('%s does not define a service name' % filename)
            continue
        operations = extract_operations(api_json)
        if not operations:
            continue
        if service_name in bruteforce_tests:
            bruteforce_tests[service_name].extend(operations)
        else:
            bruteforce_tests[service_name] = operations
    output = OUTPUT_FMT % json.dumps(bruteforce_tests, indent=4, sort_keys=True)
    open(OUTPUT_FILE, 'w').write(output)","def main():
    bruteforce_tests = dict()
    for filename in os.listdir(API_DEFINITIONS):
        if not filename.endswith('.min.json'):
            continue
        api_json_data = open(os.path.join(API_DEFINITIONS, filename)).read()
        api_json = json.loads(api_json_data)
        service_name = extract_service_name(filename, api_json)
        if service_name is None:
            print('%s does not define a service name' % filename)
            continue
        operations = extract_operations(api_json)
        if not operations:
            continue
        if service_name in bruteforce_tests:
            bruteforce_tests[service_name].extend(operations)
        else:
            bruteforce_tests[service_name] = operations
    output = OUTPUT_FMT % json.dumps(bruteforce_tests, indent=4, sort_keys=True)
    open(OUTPUT_FILE, 'w').write(output)","def main():
    bruteforce_tests = dict()
    for filename in os.listdir(API_DEFINITIONS):
        if not filename.endswith('.min.json'):
            continue
        with open(os.path.join(API_DEFINITIONS, filename)) as f:
            api_json_data = f.read()
        api_json = json.loads(api_json_data)
        service_name = extract_service_name(filename, api_json)
        if service_name is None:
            print('%s does not define a service name' % filename)
            continue
        operations = extract_operations(api_json)
        if not operations:
            continue
        if service_name in bruteforce_tests:
            bruteforce_tests[service_name].extend(operations)
        else:
            bruteforce_tests[service_name] = operations
    output = OUTPUT_FMT % json.dumps(bruteforce_tests, indent=4, sort_keys=True)
    with open(OUTPUT_FILE, 'w') as f:
        f.write(output)",1,,,,,,,,,,
enumerate-iam,https://github.com/andresriancho/enumerate-iam/tree/master/enumerate_iam/generate_bruteforce_tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/enumerate-iam/enumerate_iam/generate_bruteforce_tests.py,,"def main():
    bruteforce_tests = dict()
    for filename in os.listdir(API_DEFINITIONS):
        if not filename.endswith('.min.json'):
            continue
        api_json_data = open(os.path.join(API_DEFINITIONS, filename)).read()
        api_json = json.loads(api_json_data)
        service_name = extract_service_name(filename, api_json)
        if service_name is None:
            print('%s does not define a service name' % filename)
            continue
        operations = extract_operations(api_json)
        if not operations:
            continue
        if service_name in bruteforce_tests:
            bruteforce_tests[service_name].extend(operations)
        else:
            bruteforce_tests[service_name] = operations
    output = OUTPUT_FMT % json.dumps(bruteforce_tests, indent=4, sort_keys=True)
    open(OUTPUT_FILE, 'w').write(output)","for filename in os.listdir(API_DEFINITIONS):
    if not filename.endswith('.min.json'):
        continue
    api_json_data = open(os.path.join(API_DEFINITIONS, filename)).read()
    api_json = json.loads(api_json_data)
    service_name = extract_service_name(filename, api_json)
    if service_name is None:
        print('%s does not define a service name' % filename)
        continue
    operations = extract_operations(api_json)
    if not operations:
        continue
    if service_name in bruteforce_tests:
        bruteforce_tests[service_name].extend(operations)
    else:
        bruteforce_tests[service_name] = operations","for filename in os.listdir(API_DEFINITIONS):
    if not filename.endswith('.min.json'):
        continue
    with open(os.path.join(API_DEFINITIONS, filename)) as f:
        api_json_data = f.read()
        api_json = json.loads(api_json_data)
        service_name = extract_service_name(filename, api_json)
        if service_name is None:
            print('%s does not define a service name' % filename)
            continue
        operations = extract_operations(api_json)
        if not operations:
            continue
        if service_name in bruteforce_tests:
            bruteforce_tests[service_name].extend(operations)
        else:
            bruteforce_tests[service_name] = operations",1,,,,,,,,,,
micropython-lib,https://github.com/micropython/micropython-lib/tree/master/python-stdlib/quopri/quopri.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/micropython-lib/python-stdlib/quopri/quopri.py,,"def main():
    import sys
    import getopt
    try:
        (opts, args) = getopt.getopt(sys.argv[1:], 'td')
    except getopt.error as msg:
        sys.stdout = sys.stderr
        print(msg)
        print('usage: quopri [-t | -d] [file] ...')
        print('-t: quote tabs')
        print('-d: decode; default encode')
        sys.exit(2)
    deco = 0
    tabs = 0
    for (o, a) in opts:
        if o == '-t':
            tabs = 1
        if o == '-d':
            deco = 1
    if tabs and deco:
        sys.stdout = sys.stderr
        print('-t and -d are mutually exclusive')
        sys.exit(2)
    if not args:
        args = ['-']
    sts = 0
    for file in args:
        if file == '-':
            fp = sys.stdin.buffer
        else:
            try:
                fp = open(file, 'rb')
            except IOError as msg:
                sys.stderr.write(""%s: can't open (%s)\n"" % (file, msg))
                sts = 1
                continue
        try:
            if deco:
                decode(fp, sys.stdout.buffer)
            else:
                encode(fp, sys.stdout.buffer, tabs)
        finally:
            if file != '-':
                fp.close()
    if sts:
        sys.exit(sts)","try:
    fp = open(file, 'rb')
except IOError as msg:
    sys.stderr.write(""%s: can't open (%s)\n"" % (file, msg))
    sts = 1
    continue","with open(file, 'rb') as fp:
    try:
        # code that uses fp
    except IOError as msg:
        sys.stderr.write(""%s: can't open (%s)\n"" % (file, msg))
        sts = 1
        continue",1,,,,,,,,,,
azure-devops-cli-extension,https://github.com/Azure/azure-devops-cli-extension/tree/master/azure-devops/azext_devops/dev/common/external_tool.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-devops-cli-extension/azure-devops/azext_devops/dev/common/external_tool.py,ExternalToolInvoker,"def start(self, command_args, env):
    if self._proc is not None:
        raise RuntimeError('Attempted to invoke already-running external tool')
    logger.debug('Running external command: %s', ' '.join(command_args))
    DEVNULL = open(os.devnull, 'w')
    self._args = command_args
    self._proc = subprocess.Popen(command_args, shell=False, stdin=DEVNULL, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)","def start(self, command_args, env):
    if self._proc is not None:
        raise RuntimeError('Attempted to invoke already-running external tool')
    logger.debug('Running external command: %s', ' '.join(command_args))
    DEVNULL = open(os.devnull, 'w')
    self._args = command_args
    self._proc = subprocess.Popen(command_args, shell=False, stdin=DEVNULL, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)","def start(self, command_args, env):
    if self._proc is not None:
        raise RuntimeError('Attempted to invoke already-running external tool')
    logger.debug('Running external command: %s', ' '.join(command_args))
    with open(os.devnull, 'w') as DEVNULL:
        self._args = command_args
        self._proc = subprocess.Popen(command_args, shell=False, stdin=DEVNULL, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)",1,,,,,,,,,,
2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement,https://github.com/Mingtzge/2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement/tree/master/recognize_process/tools/test_crnn_jmz.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/2019-CCF-BDCI-OCR-MCZJ-OCR-IdentificationIDElement/recognize_process/tools/test_crnn_jmz.py,,"def _sparse_matrix_to_list(sparse_matrix, char_map_dict_path=None):
    """"""
    灏嗙煩闃垫媶鍒嗕负list锛屽弬鑰冿細https://github.com/bai-shang/crnn_ctc_ocr.Tensorflow
    :param sparse_matrix:
    :param char_map_dict_path:
    :return:
    """"""
    indices = sparse_matrix.indices
    values = sparse_matrix.values
    dense_shape = sparse_matrix.dense_shape
    char_map_dict = json.load(open(char_map_dict_path, 'r', encoding='utf-8'))
    if char_map_dict is None:
        print('error')
    assert isinstance(char_map_dict, dict) and 'char_map_dict is not a dict'
    dense_matrix = len(char_map_dict.keys()) * np.ones(dense_shape, dtype=np.int32)
    for (i, indice) in enumerate(indices):
        dense_matrix[indice[0], indice[1]] = values[i]
    string_list = []
    for row in dense_matrix:
        string = []
        for val in row:
            string.append(_int_to_string(val, char_map_dict))
        string_list.append(''.join((s for s in string if s != '*')))
    return string_list","def _sparse_matrix_to_list(sparse_matrix, char_map_dict_path=None):
    """"""
    灏嗙煩闃垫媶鍒嗕负list锛屽弬鑰冿細https://github.com/bai-shang/crnn_ctc_ocr.Tensorflow
    :param sparse_matrix:
    :param char_map_dict_path:
    :return:
    """"""
    indices = sparse_matrix.indices
    values = sparse_matrix.values
    dense_shape = sparse_matrix.dense_shape
    char_map_dict = json.load(open(char_map_dict_path, 'r', encoding='utf-8'))
    if char_map_dict is None:
        print('error')
    assert isinstance(char_map_dict, dict) and 'char_map_dict is not a dict'
    dense_matrix = len(char_map_dict.keys()) * np.ones(dense_shape, dtype=np.int32)
    for (i, indice) in enumerate(indices):
        dense_matrix[indice[0], indice[1]] = values[i]
    string_list = []
    for row in dense_matrix:
        string = []
        for val in row:
            string.append(_int_to_string(val, char_map_dict))
        string_list.append(''.join((s for s in string if s != '*')))
    return string_list","def _sparse_matrix_to_list(sparse_matrix, char_map_dict_path=None):
    """"""
    灏嗙煩闃垫媶鍒嗕负list锛屽弬鑰冿細https://github.com/bai-shang/crnn_ctc_ocr.Tensorflow
    :param sparse_matrix:
    :param char_map_dict_path:
    :return:
    """"""
    indices = sparse_matrix.indices
    values = sparse_matrix.values
    dense_shape = sparse_matrix.dense_shape
    with open(char_map_dict_path, 'r', encoding='utf-8') as f:
        char_map_dict = json.load(f)
    if char_map_dict is None:
        print('error')
    assert isinstance(char_map_dict, dict) and 'char_map_dict is not a dict'
    dense_matrix = len(char_map_dict.keys()) * np.ones(dense_shape, dtype=np.int32)
    for (i, indice) in enumerate(indices):
        dense_matrix[indice[0], indice[1]] = values[i]
    string_list = []
    for row in dense_matrix:
        string = []
        for val in row:
            string.append(_int_to_string(val, char_map_dict))
        string_list.append(''.join((s for s in string if s != '*')))
    return string_list",1,,,,,,,,,,
portia,https://github.com/scrapinghub/portia/tree/master/slybot/slybot/tests/test_spider.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/portia/slybot/slybot/tests/test_spider.py,SpiderTest,"def test_links_from_atom(self):
    body = open(join(_PATH, 'data', 'atom_sample.xml')).read()
    response = UTF8XmlResponse(url='http://example.com/sample.xml', body=body, headers={'Content-Type': 'application/atom+xml; charset=UTF-8'})
    name = 'sitemaps'
    spider = self.smanager.create(name)
    urls = [r.url for r in spider.parse(response)]
    self.assertEqual(len(urls), 3)
    self.assertEqual(set(urls), set(['http://www.webupd8.org/sitemap.xml?page=1', 'http://www.webupd8.org/sitemap.xml?page=2', 'http://www.webupd8.org/sitemap.xml?page=3']))","def test_links_from_atom(self):
    body = open(join(_PATH, 'data', 'atom_sample.xml')).read()
    response = UTF8XmlResponse(url='http://example.com/sample.xml', body=body, headers={'Content-Type': 'application/atom+xml; charset=UTF-8'})
    name = 'sitemaps'
    spider = self.smanager.create(name)
    urls = [r.url for r in spider.parse(response)]
    self.assertEqual(len(urls), 3)
    self.assertEqual(set(urls), set(['http://www.webupd8.org/sitemap.xml?page=1', 'http://www.webupd8.org/sitemap.xml?page=2', 'http://www.webupd8.org/sitemap.xml?page=3']))","def test_links_from_atom(self):
    with open(join(_PATH, 'data', 'atom_sample.xml')) as f:
        body = f.read()
    response = UTF8XmlResponse(url='http://example.com/sample.xml', body=body, headers={'Content-Type': 'application/atom+xml; charset=UTF-8'})
    name = 'sitemaps'
    spider = self.smanager.create(name)
    urls = [r.url for r in spider.parse(response)]
    self.assertEqual(len(urls), 3)
    self.assertEqual(set(urls), set(['http://www.webupd8.org/sitemap.xml?page=1', 'http://www.webupd8.org/sitemap.xml?page=2', 'http://www.webupd8.org/sitemap.xml?page=3']))",1,,,,,,,,,,
PaddleX,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex/cv/models/base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex/cv/models/base.py,BaseAPI,"def save_model(self, save_dir):
    if not osp.isdir(save_dir):
        if osp.exists(save_dir):
            os.remove(save_dir)
        os.makedirs(save_dir)
    if self.train_prog is not None:
        fluid.save(self.train_prog, osp.join(save_dir, 'model'))
    else:
        fluid.save(self.test_prog, osp.join(save_dir, 'model'))
    model_info = self.get_model_info()
    model_info['status'] = self.status
    with open(osp.join(save_dir, 'model.yml'), encoding='utf-8', mode='w') as f:
        yaml.dump(model_info, f)
    if hasattr(self, 'eval_details'):
        with open(osp.join(save_dir, 'eval_details.json'), 'w') as f:
            json.dump(self.eval_details, f)
    if self.status == 'Prune':
        shapes = {}
        for block in self.train_prog.blocks:
            for param in block.all_parameters():
                pd_var = fluid.global_scope().find_var(param.name)
                pd_param = pd_var.get_tensor()
                shapes[param.name] = np.array(pd_param).shape
        with open(osp.join(save_dir, 'prune.yml'), encoding='utf-8', mode='w') as f:
            yaml.dump(shapes, f)
    open(osp.join(save_dir, '.success'), 'w').close()
    logging.info('Model saved in {}.'.format(save_dir))","def save_model(self, save_dir):
    if not osp.isdir(save_dir):
        if osp.exists(save_dir):
            os.remove(save_dir)
        os.makedirs(save_dir)
    if self.train_prog is not None:
        fluid.save(self.train_prog, osp.join(save_dir, 'model'))
    else:
        fluid.save(self.test_prog, osp.join(save_dir, 'model'))
    model_info = self.get_model_info()
    model_info['status'] = self.status
    with open(osp.join(save_dir, 'model.yml'), encoding='utf-8', mode='w') as f:
        yaml.dump(model_info, f)
    if hasattr(self, 'eval_details'):
        with open(osp.join(save_dir, 'eval_details.json'), 'w') as f:
            json.dump(self.eval_details, f)
    if self.status == 'Prune':
        shapes = {}
        for block in self.train_prog.blocks:
            for param in block.all_parameters():
                pd_var = fluid.global_scope().find_var(param.name)
                pd_param = pd_var.get_tensor()
                shapes[param.name] = np.array(pd_param).shape
        with open(osp.join(save_dir, 'prune.yml'), encoding='utf-8', mode='w') as f:
            yaml.dump(shapes, f)
    open(osp.join(save_dir, '.success'), 'w').close()
    logging.info('Model saved in {}.'.format(save_dir))","def save_model(self, save_dir):
    if not osp.isdir(save_dir):
        if osp.exists(save_dir):
            os.remove(save_dir)
        os.makedirs(save_dir)
    if self.train_prog is not None:
        fluid.save(self.train_prog, osp.join(save_dir, 'model'))
    else:
        fluid.save(self.test_prog, osp.join(save_dir, 'model'))
    model_info = self.get_model_info()
    model_info['status'] = self.status
    with open(osp.join(save_dir, 'model.yml'), encoding='utf-8', mode='w') as f:
        yaml.dump(model_info, f)
    if hasattr(self, 'eval_details'):
        with open(osp.join(save_dir, 'eval_details.json'), 'w') as f:
            json.dump(self.eval_details, f)
    if self.status == 'Prune':
        shapes = {}
        for block in self.train_prog.blocks:
            for param in block.all_parameters():
                pd_var = fluid.global_scope().find_var(param.name)
                pd_param = pd_var.get_tensor()
                shapes[param.name] = np.array(pd_param).shape
        with open(osp.join(save_dir, 'prune.yml'), encoding='utf-8', mode='w') as f:
            yaml.dump(shapes, f)
    with open(osp.join(save_dir, '.success'), 'w'):
        pass
    logging.info('Model saved in {}.'.format(save_dir))",1,,,,,,,,,,
py-apple-quadruped-robot,https://github.com/ToanTech/py-apple-quadruped-robot/tree/master/Py Apple Dynamics V6.8/Py Apple Dynamics V6.8 鍥轰欢鍙婄▼搴/V6.8 婧愪唬鐮/main.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/py-apple-quadruped-robot/Py Apple Dynamics V6.8/Py Apple Dynamics V6.8 鍥轰欢鍙婄▼搴/V6.8 婧愪唬鐮/main.py,,"def app_2():
    try:
        exec(open('my_code.py').read())
    except:
        print('绉鏈ㄧ紪绋嬩唬鐮佹墽琛屽嚭閿欙紝璺宠繃...')","try:
    exec(open('my_code.py').read())
except:
    print('绉鏈ㄧ紪绋嬩唬鐮佹墽琛屽嚭閿欙紝璺宠繃...')","with open('my_code.py', 'r') as f:
    try:
        exec(f.read())
    except:
        print('绉鏈ㄧ紪绋嬩唬鐮佹墽琛屽嚭閿欙紝璺宠繃...')",1,,,,,,,,,,
brozzler,https://github.com/internetarchive/brozzler/tree/master/tests/test_cluster.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brozzler/tests/test_cluster.py,,"def test_brozzle_site(httpd):
    test_id = 'test_brozzle_site-%s' % datetime.datetime.utcnow().isoformat()
    rr = doublethink.Rethinker('localhost', db='brozzler')
    site = brozzler.Site(rr, {'seed': make_url(httpd, '/site1/'), 'warcprox_meta': {'captures-table-extra-fields': {'test_id': test_id}}})
    page1 = make_url(httpd, '/site1/')
    page2 = make_url(httpd, '/site1/file1.txt')
    robots = make_url(httpd, '/robots.txt')
    try:
        stop_service('brozzler-worker')
        assert site.id is None
        frontier = brozzler.RethinkDbFrontier(rr)
        brozzler.new_site(frontier, site)
        assert site.id is not None
        assert len(list(frontier.site_pages(site.id))) == 1
    finally:
        start_service('brozzler-worker')
    start = time.time()
    while site.status != 'FINISHED' and time.time() - start < 300:
        time.sleep(0.5)
        site.refresh()
    assert site.status == 'FINISHED'
    pages = list(frontier.site_pages(site.id))
    assert len(pages) == 2
    assert {page.url for page in pages} == {make_url(httpd, '/site1/'), make_url(httpd, '/site1/file1.txt')}
    time.sleep(2)
    captures = rr.table('captures').filter({'test_id': test_id}).run()
    captures_by_url = {c['url']: c for c in captures if c['http_method'] != 'HEAD'}
    assert robots in captures_by_url
    assert page1 in captures_by_url
    assert page2 in captures_by_url
    assert 'screenshot:%s' % page1 in captures_by_url
    assert 'thumbnail:%s' % page1 in captures_by_url
    t14 = captures_by_url[page2]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, page2)
    expected_payload = open(os.path.join(os.path.dirname(__file__), 'htdocs', 'site1', 'file1.txt'), 'rb').read()
    assert requests.get(wb_url).content == expected_payload
    url = 'screenshot:%s' % page1
    t14 = captures_by_url[url]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, url)
    response = requests.get(wb_url)
    assert response.status_code == 200
    assert response.headers['content-type'] == 'image/jpeg'
    url = 'thumbnail:%s' % page1
    t14 = captures_by_url[url]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, url)
    response = requests.get(wb_url)
    assert response.status_code == 200
    assert response.headers['content-type'] == 'image/jpeg'","def test_brozzle_site(httpd):
    test_id = 'test_brozzle_site-%s' % datetime.datetime.utcnow().isoformat()
    rr = doublethink.Rethinker('localhost', db='brozzler')
    site = brozzler.Site(rr, {'seed': make_url(httpd, '/site1/'), 'warcprox_meta': {'captures-table-extra-fields': {'test_id': test_id}}})
    page1 = make_url(httpd, '/site1/')
    page2 = make_url(httpd, '/site1/file1.txt')
    robots = make_url(httpd, '/robots.txt')
    try:
        stop_service('brozzler-worker')
        assert site.id is None
        frontier = brozzler.RethinkDbFrontier(rr)
        brozzler.new_site(frontier, site)
        assert site.id is not None
        assert len(list(frontier.site_pages(site.id))) == 1
    finally:
        start_service('brozzler-worker')
    start = time.time()
    while site.status != 'FINISHED' and time.time() - start < 300:
        time.sleep(0.5)
        site.refresh()
    assert site.status == 'FINISHED'
    pages = list(frontier.site_pages(site.id))
    assert len(pages) == 2
    assert {page.url for page in pages} == {make_url(httpd, '/site1/'), make_url(httpd, '/site1/file1.txt')}
    time.sleep(2)
    captures = rr.table('captures').filter({'test_id': test_id}).run()
    captures_by_url = {c['url']: c for c in captures if c['http_method'] != 'HEAD'}
    assert robots in captures_by_url
    assert page1 in captures_by_url
    assert page2 in captures_by_url
    assert 'screenshot:%s' % page1 in captures_by_url
    assert 'thumbnail:%s' % page1 in captures_by_url
    t14 = captures_by_url[page2]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, page2)
    expected_payload = open(os.path.join(os.path.dirname(__file__), 'htdocs', 'site1', 'file1.txt'), 'rb').read()
    assert requests.get(wb_url).content == expected_payload
    url = 'screenshot:%s' % page1
    t14 = captures_by_url[url]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, url)
    response = requests.get(wb_url)
    assert response.status_code == 200
    assert response.headers['content-type'] == 'image/jpeg'
    url = 'thumbnail:%s' % page1
    t14 = captures_by_url[url]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, url)
    response = requests.get(wb_url)
    assert response.status_code == 200
    assert response.headers['content-type'] == 'image/jpeg'","expected_payload = open(os.path.join(os.path.dirname(__file__), 'htdocs', 'site1', 'file1.txt'), 'rb').read()

can be refactored as:

with open(os.path.join(os.path.dirname(__file__), 'htdocs', 'site1', 'file1.txt'), 'rb') as f:
    expected_payload = f.read()",1,,,,,,,,,,
ansible,https://github.com/ansible/ansible/tree/master/lib/ansible/module_utils/basic.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/lib/ansible/module_utils/basic.py,AnsibleModule,"def is_special_selinux_path(self, path):
    """"""
        Returns a tuple containing (True, selinux_context) if the given path is on a
        NFS or other 'special' fs  mount point, otherwise the return will be (False, None).
        """"""
    try:
        f = open('/proc/mounts', 'r')
        mount_data = f.readlines()
        f.close()
    except Exception:
        return (False, None)
    path_mount_point = self.find_mount_point(path)
    for line in mount_data:
        (device, mount_point, fstype, options, rest) = line.split(' ', 4)
        if to_bytes(path_mount_point) == to_bytes(mount_point):
            for fs in self._selinux_special_fs:
                if fs in fstype:
                    special_context = self.selinux_context(path_mount_point)
                    return (True, special_context)
    return (False, None)","try:
    f = open('/proc/mounts', 'r')
    mount_data = f.readlines()
    f.close()
except Exception:
    return (False, None)","try:
    with open('/proc/mounts', 'r') as f:
        mount_data = f.readlines()
except Exception:
    return (False, None)",1,,,,,,,,,,
attic,https://github.com/jborg/attic/tree/master/attic/archive.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/attic/attic/archive.py,Archive,"def open_simple(p, s):
    return open(p, 'rb')","def open_simple(p, s):
    return open(p, 'rb')","def open_simple(p, s):
    with open(p, 'rb') as f:
        return f",1,,,,,,,,,,
awx,https://github.com/ansible/awx/tree/master/awx/main/tests/unit/test_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/awx/awx/main/tests/unit/test_tasks.py,TestInventoryUpdateCredentials,"def test_gce_source(self, cred_env_var, inventory_update, private_data_dir, mocker, mock_me):
    task = jobs.RunInventoryUpdate()
    task.instance = inventory_update
    gce = CredentialType.defaults['gce']()
    inventory_update.source = 'gce'

    def get_cred():
        cred = Credential(pk=1, credential_type=gce, inputs={'username': 'bob', 'project': 'some-project', 'ssh_key_data': self.EXAMPLE_PRIVATE_KEY})
        cred.inputs['ssh_key_data'] = encrypt_field(cred, 'ssh_key_data')
        return cred
    inventory_update.get_cloud_credential = get_cred
    inventory_update.get_extra_credentials = mocker.Mock(return_value=[])

    def run(expected_gce_zone):
        (private_data_files, ssh_key_data) = task.build_private_data_files(inventory_update, private_data_dir)
        env = task.build_env(inventory_update, private_data_dir, private_data_files)
        safe_env = {}
        credentials = task.build_credentials_list(inventory_update)
        for credential in credentials:
            if credential:
                credential.credential_type.inject_credential(credential, env, safe_env, [], private_data_dir)
        assert env['GCE_ZONE'] == expected_gce_zone
        json_data = json.load(open(env[cred_env_var], 'rb'))
        assert json_data['type'] == 'service_account'
        assert json_data['private_key'] == self.EXAMPLE_PRIVATE_KEY
        assert json_data['client_email'] == 'bob'
        assert json_data['project_id'] == 'some-project'","def run(expected_gce_zone):
    (private_data_files, ssh_key_data) = task.build_private_data_files(inventory_update, private_data_dir)
    env = task.build_env(inventory_update, private_data_dir, private_data_files)
    safe_env = {}
    credentials = task.build_credentials_list(inventory_update)
    for credential in credentials:
        if credential:
            credential.credential_type.inject_credential(credential, env, safe_env, [], private_data_dir)
    assert env['GCE_ZONE'] == expected_gce_zone
    json_data = json.load(open(env[cred_env_var], 'rb'))
    assert json_data['type'] == 'service_account'
    assert json_data['private_key'] == self.EXAMPLE_PRIVATE_KEY
    assert json_data['client_email'] == 'bob'
    assert json_data['project_id'] == 'some-project'","def run(expected_gce_zone):
    (private_data_files, ssh_key_data) = task.build_private_data_files(inventory_update, private_data_dir)
    env = task.build_env(inventory_update, private_data_dir, private_data_files)
    safe_env = {}
    credentials = task.build_credentials_list(inventory_update)
    for credential in credentials:
        if credential:
            credential.credential_type.inject_credential(credential, env, safe_env, [], private_data_dir)
    assert env['GCE_ZONE'] == expected_gce_zone
    with open(env[cred_env_var], 'rb') as f:
        json_data = json.load(f)
    assert json_data['type'] == 'service_account'
    assert json_data['private_key'] == self.EXAMPLE_PRIVATE_KEY
    assert json_data['client_email'] == 'bob'
    assert json_data['project_id'] == 'some-project'",1,,,,,,,,,,
dupeguru,https://github.com/arsenetar/dupeguru/tree/master/hscommon/pygettext.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dupeguru/hscommon/pygettext.py,,"def main(source_files, outpath, keywords=None):
    global default_keywords

    class Options:
        GNU = 1
        SOLARIS = 2
        extractall = 0
        escape = 0
        keywords = []
        outfile = 'messages.pot'
        writelocations = 1
        locationstyle = GNU
        verbose = 0
        width = 78
        excludefilename = ''
        docstrings = 0
        nodocstrings = {}
    options = Options()
    options.outfile = outpath
    if keywords:
        options.keywords = keywords
    make_escapes(options.escape)
    options.keywords.extend(default_keywords)
    if options.excludefilename:
        try:
            fp = open(options.excludefilename, encoding='utf-8')
            options.toexclude = fp.readlines()
            fp.close()
        except IOError:
            print(""Can't read --exclude-file: %s"" % options.excludefilename, file=sys.stderr)
            sys.exit(1)
    else:
        options.toexclude = []
    eater = TokenEater(options)
    for filename in source_files:
        if options.verbose:
            print('Working on %s' % filename)
        fp = open(filename, encoding='utf-8')
        closep = 1
        try:
            eater.set_filename(filename)
            try:
                tokens = tokenize.generate_tokens(fp.readline)
                for _token in tokens:
                    eater(*_token)
            except tokenize.TokenError as e:
                print('%s: %s, line %d, column %d' % (e.args[0], filename, e.args[1][0], e.args[1][1]), file=sys.stderr)
        finally:
            if closep:
                fp.close()
    fp = open(options.outfile, 'w', encoding='utf-8')
    closep = 1
    try:
        eater.write(fp)
    finally:
        if closep:
            fp.close()","def main(source_files, outpath, keywords=None):
    global default_keywords

    class Options:
        GNU = 1
        SOLARIS = 2
        extractall = 0
        escape = 0
        keywords = []
        outfile = 'messages.pot'
        writelocations = 1
        locationstyle = GNU
        verbose = 0
        width = 78
        excludefilename = ''
        docstrings = 0
        nodocstrings = {}
    options = Options()
    options.outfile = outpath
    if keywords:
        options.keywords = keywords
    make_escapes(options.escape)
    options.keywords.extend(default_keywords)
    if options.excludefilename:
        try:
            fp = open(options.excludefilename, encoding='utf-8')
            options.toexclude = fp.readlines()
            fp.close()
        except IOError:
            print(""Can't read --exclude-file: %s"" % options.excludefilename, file=sys.stderr)
            sys.exit(1)
    else:
        options.toexclude = []
    eater = TokenEater(options)
    for filename in source_files:
        if options.verbose:
            print('Working on %s' % filename)
        fp = open(filename, encoding='utf-8')
        closep = 1
        try:
            eater.set_filename(filename)
            try:
                tokens = tokenize.generate_tokens(fp.readline)
                for _token in tokens:
                    eater(*_token)
            except tokenize.TokenError as e:
                print('%s: %s, line %d, column %d' % (e.args[0], filename, e.args[1][0], e.args[1][1]), file=sys.stderr)
        finally:
            if closep:
                fp.close()
    fp = open(options.outfile, 'w', encoding='utf-8')
    closep = 1
    try:
        eater.write(fp)
    finally:
        if closep:
            fp.close()","def main(source_files, outpath, keywords=None):
    global default_keywords

    class Options:
        GNU = 1
        SOLARIS = 2
        extractall = 0
        escape = 0
        keywords = []
        outfile = 'messages.pot'
        writelocations = 1
        locationstyle = GNU
        verbose = 0
        width = 78
        excludefilename = ''
        docstrings = 0
        nodocstrings = {}
    options = Options()
    options.outfile = outpath
    if keywords:
        options.keywords = keywords
    make_escapes(options.escape)
    options.keywords.extend(default_keywords)
    if options.excludefilename:
        try:
            with open(options.excludefilename, encoding='utf-8') as fp:
                options.toexclude = fp.readlines()
        except IOError:
            print(""Can't read --exclude-file: %s"" % options.excludefilename, file=sys.stderr)
            sys.exit(1)
    else:
        options.toexclude = []
    eater = TokenEater(options)
    for filename in source_files:
        if options.verbose:
            print('Working on %s' % filename)
        with open(filename, encoding='utf-8') as fp:
            closep = 1
            try:
                eater.set_filename(filename)
                try:
                    tokens = tokenize.generate_tokens(fp.readline)
                    for _token in tokens:
                        eater(*_token)
                except tokenize.TokenError as e:
                    print('%s: %s, line %d, column %d' % (e.args[0], filename, e.args[1][0], e.args[1][1]), file=sys.stderr)
            finally:
                if closep:
                    fp.close()
    with open(options.outfile, 'w', encoding='utf-8') as fp:
        closep = 1
        try:
            eater.write(fp)
        finally:
            if closep:
                fp.close()",1,,,,,,,,,,
dupeguru,https://github.com/arsenetar/dupeguru/tree/master/hscommon/pygettext.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dupeguru/hscommon/pygettext.py,,"def main(source_files, outpath, keywords=None):
    global default_keywords

    class Options:
        GNU = 1
        SOLARIS = 2
        extractall = 0
        escape = 0
        keywords = []
        outfile = 'messages.pot'
        writelocations = 1
        locationstyle = GNU
        verbose = 0
        width = 78
        excludefilename = ''
        docstrings = 0
        nodocstrings = {}
    options = Options()
    options.outfile = outpath
    if keywords:
        options.keywords = keywords
    make_escapes(options.escape)
    options.keywords.extend(default_keywords)
    if options.excludefilename:
        try:
            fp = open(options.excludefilename, encoding='utf-8')
            options.toexclude = fp.readlines()
            fp.close()
        except IOError:
            print(""Can't read --exclude-file: %s"" % options.excludefilename, file=sys.stderr)
            sys.exit(1)
    else:
        options.toexclude = []
    eater = TokenEater(options)
    for filename in source_files:
        if options.verbose:
            print('Working on %s' % filename)
        fp = open(filename, encoding='utf-8')
        closep = 1
        try:
            eater.set_filename(filename)
            try:
                tokens = tokenize.generate_tokens(fp.readline)
                for _token in tokens:
                    eater(*_token)
            except tokenize.TokenError as e:
                print('%s: %s, line %d, column %d' % (e.args[0], filename, e.args[1][0], e.args[1][1]), file=sys.stderr)
        finally:
            if closep:
                fp.close()
    fp = open(options.outfile, 'w', encoding='utf-8')
    closep = 1
    try:
        eater.write(fp)
    finally:
        if closep:
            fp.close()","for filename in source_files:
    if options.verbose:
        print('Working on %s' % filename)
    fp = open(filename, encoding='utf-8')
    closep = 1
    try:
        eater.set_filename(filename)
        try:
            tokens = tokenize.generate_tokens(fp.readline)
            for _token in tokens:
                eater(*_token)
        except tokenize.TokenError as e:
            print('%s: %s, line %d, column %d' % (e.args[0], filename, e.args[1][0], e.args[1][1]), file=sys.stderr)
    finally:
        if closep:
            fp.close()","for filename in source_files:
    if options.verbose:
        print('Working on %s' % filename)
    with open(filename, encoding='utf-8') as fp:
        closep = 1
        try:
            eater.set_filename(filename)
            try:
                tokens = tokenize.generate_tokens(fp.readline)
                for _token in tokens:
                    eater(*_token)
            except tokenize.TokenError as e:
                print('%s: %s, line %d, column %d' % (e.args[0], filename, e.args[1][0], e.args[1][1]), file=sys.stderr)
        finally:
            if closep:
                fp.close()",1,,,,,,,,,,
dupeguru,https://github.com/arsenetar/dupeguru/tree/master/hscommon/pygettext.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dupeguru/hscommon/pygettext.py,,"def main(source_files, outpath, keywords=None):
    global default_keywords

    class Options:
        GNU = 1
        SOLARIS = 2
        extractall = 0
        escape = 0
        keywords = []
        outfile = 'messages.pot'
        writelocations = 1
        locationstyle = GNU
        verbose = 0
        width = 78
        excludefilename = ''
        docstrings = 0
        nodocstrings = {}
    options = Options()
    options.outfile = outpath
    if keywords:
        options.keywords = keywords
    make_escapes(options.escape)
    options.keywords.extend(default_keywords)
    if options.excludefilename:
        try:
            fp = open(options.excludefilename, encoding='utf-8')
            options.toexclude = fp.readlines()
            fp.close()
        except IOError:
            print(""Can't read --exclude-file: %s"" % options.excludefilename, file=sys.stderr)
            sys.exit(1)
    else:
        options.toexclude = []
    eater = TokenEater(options)
    for filename in source_files:
        if options.verbose:
            print('Working on %s' % filename)
        fp = open(filename, encoding='utf-8')
        closep = 1
        try:
            eater.set_filename(filename)
            try:
                tokens = tokenize.generate_tokens(fp.readline)
                for _token in tokens:
                    eater(*_token)
            except tokenize.TokenError as e:
                print('%s: %s, line %d, column %d' % (e.args[0], filename, e.args[1][0], e.args[1][1]), file=sys.stderr)
        finally:
            if closep:
                fp.close()
    fp = open(options.outfile, 'w', encoding='utf-8')
    closep = 1
    try:
        eater.write(fp)
    finally:
        if closep:
            fp.close()","try:
    fp = open(options.excludefilename, encoding='utf-8')
    options.toexclude = fp.readlines()
    fp.close()
except IOError:
    print(""Can't read --exclude-file: %s"" % options.excludefilename, file=sys.stderr)
    sys.exit(1)","try:
    with open(options.excludefilename, encoding='utf-8') as fp:
        options.toexclude = fp.readlines()
except IOError:
    print(""Can't read --exclude-file: %s"" % options.excludefilename, file=sys.stderr)
    sys.exit(1)",1,,,,,,,,,,
whoogle-search,https://github.com/benbusby/whoogle-search/tree/master/app/utils/bangs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/whoogle-search/app/utils/bangs.py,,"def gen_bangs_json(bangs_file: str) -> None:
    """"""Generates a json file from the DDG bangs list

    Args:
        bangs_file: The str path to the new DDG bangs json file

    Returns:
        None

    """"""
    try:
        r = requests.get(DDG_BANGS)
        r.raise_for_status()
    except requests.exceptions.HTTPError as err:
        raise SystemExit(err)
    data = json.loads(r.text)
    bangs_data = {}
    for row in data:
        bang_command = '!' + row['t']
        bangs_data[bang_command] = {'url': row['u'].replace('{{{s}}}', '{}'), 'suggestion': bang_command + ' (' + row['s'] + ')'}
    json.dump(bangs_data, open(bangs_file, 'w'))","def gen_bangs_json(bangs_file: str) -> None:
    """"""Generates a json file from the DDG bangs list

    Args:
        bangs_file: The str path to the new DDG bangs json file

    Returns:
        None

    """"""
    try:
        r = requests.get(DDG_BANGS)
        r.raise_for_status()
    except requests.exceptions.HTTPError as err:
        raise SystemExit(err)
    data = json.loads(r.text)
    bangs_data = {}
    for row in data:
        bang_command = '!' + row['t']
        bangs_data[bang_command] = {'url': row['u'].replace('{{{s}}}', '{}'), 'suggestion': bang_command + ' (' + row['s'] + ')'}
    json.dump(bangs_data, open(bangs_file, 'w'))","def gen_bangs_json(bangs_file: str) -> None:
    """"""Generates a json file from the DDG bangs list

    Args:
        bangs_file: The str path to the new DDG bangs json file

    Returns:
        None

    """"""
    try:
        r = requests.get(DDG_BANGS)
        r.raise_for_status()
    except requests.exceptions.HTTPError as err:
        raise SystemExit(err)
    data = json.loads(r.text)
    bangs_data = {}
    for row in data:
        bang_command = '!' + row['t']
        bangs_data[bang_command] = {'url': row['u'].replace('{{{s}}}', '{}'), 'suggestion': bang_command + ' (' + row['s'] + ')'}
    with open(bangs_file, 'w') as f:
        json.dump(bangs_data, f)",1,,,,,,,,,,
DeepKE,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/relation_extraction/document/evaluation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/relation_extraction/document/evaluation.py,,"def gen_train_facts(data_file_name, truth_dir):
    fact_file_name = data_file_name[data_file_name.find('train_'):]
    fact_file_name = os.path.join(truth_dir, fact_file_name.replace('.json', '.fact'))
    if os.path.exists(fact_file_name):
        fact_in_train = set([])
        triples = json.load(open(fact_file_name))
        for x in triples:
            fact_in_train.add(tuple(x))
        return fact_in_train
    fact_in_train = set([])
    ori_data = json.load(open(data_file_name))
    for data in ori_data:
        vertexSet = data['vertexSet']
        for label in data['labels']:
            rel = label['r']
            for n1 in vertexSet[label['h']]:
                for n2 in vertexSet[label['t']]:
                    fact_in_train.add((n1['name'], n2['name'], rel))
    json.dump(list(fact_in_train), open(fact_file_name, 'w'))
    return fact_in_train","def gen_train_facts(data_file_name, truth_dir):
    fact_file_name = data_file_name[data_file_name.find('train_'):]
    fact_file_name = os.path.join(truth_dir, fact_file_name.replace('.json', '.fact'))
    if os.path.exists(fact_file_name):
        fact_in_train = set([])
        triples = json.load(open(fact_file_name))
        for x in triples:
            fact_in_train.add(tuple(x))
        return fact_in_train
    fact_in_train = set([])
    ori_data = json.load(open(data_file_name))
    for data in ori_data:
        vertexSet = data['vertexSet']
        for label in data['labels']:
            rel = label['r']
            for n1 in vertexSet[label['h']]:
                for n2 in vertexSet[label['t']]:
                    fact_in_train.add((n1['name'], n2['name'], rel))
    json.dump(list(fact_in_train), open(fact_file_name, 'w'))
    return fact_in_train","def gen_train_facts(data_file_name, truth_dir):
    fact_file_name = data_file_name[data_file_name.find('train_'):]
    fact_file_name = os.path.join(truth_dir, fact_file_name.replace('.json', '.fact'))
    if os.path.exists(fact_file_name):
        fact_in_train = set([])
        triples = json.load(open(fact_file_name))
        for x in triples:
            fact_in_train.add(tuple(x))
        return fact_in_train
    fact_in_train = set([])
    with open(data_file_name) as f:
        ori_data = json.load(f)
    for data in ori_data:
        vertexSet = data['vertexSet']
        for label in data['labels']:
            rel = label['r']
            for n1 in vertexSet[label['h']]:
                for n2 in vertexSet[label['t']]:
                    fact_in_train.add((n1['name'], n2['name'], rel))
    json.dump(list(fact_in_train), open(fact_file_name, 'w'))
    return fact_in_train",1,,,,,,,,,,
DeepKE,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/relation_extraction/document/evaluation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/relation_extraction/document/evaluation.py,,"def gen_train_facts(data_file_name, truth_dir):
    fact_file_name = data_file_name[data_file_name.find('train_'):]
    fact_file_name = os.path.join(truth_dir, fact_file_name.replace('.json', '.fact'))
    if os.path.exists(fact_file_name):
        fact_in_train = set([])
        triples = json.load(open(fact_file_name))
        for x in triples:
            fact_in_train.add(tuple(x))
        return fact_in_train
    fact_in_train = set([])
    ori_data = json.load(open(data_file_name))
    for data in ori_data:
        vertexSet = data['vertexSet']
        for label in data['labels']:
            rel = label['r']
            for n1 in vertexSet[label['h']]:
                for n2 in vertexSet[label['t']]:
                    fact_in_train.add((n1['name'], n2['name'], rel))
    json.dump(list(fact_in_train), open(fact_file_name, 'w'))
    return fact_in_train","def gen_train_facts(data_file_name, truth_dir):
    fact_file_name = data_file_name[data_file_name.find('train_'):]
    fact_file_name = os.path.join(truth_dir, fact_file_name.replace('.json', '.fact'))
    if os.path.exists(fact_file_name):
        fact_in_train = set([])
        triples = json.load(open(fact_file_name))
        for x in triples:
            fact_in_train.add(tuple(x))
        return fact_in_train
    fact_in_train = set([])
    ori_data = json.load(open(data_file_name))
    for data in ori_data:
        vertexSet = data['vertexSet']
        for label in data['labels']:
            rel = label['r']
            for n1 in vertexSet[label['h']]:
                for n2 in vertexSet[label['t']]:
                    fact_in_train.add((n1['name'], n2['name'], rel))
    json.dump(list(fact_in_train), open(fact_file_name, 'w'))
    return fact_in_train","def gen_train_facts(data_file_name, truth_dir):
    fact_file_name = data_file_name[data_file_name.find('train_'):]
    fact_file_name = os.path.join(truth_dir, fact_file_name.replace('.json', '.fact'))
    if os.path.exists(fact_file_name):
        fact_in_train = set([])
        triples = json.load(open(fact_file_name))
        for x in triples:
            fact_in_train.add(tuple(x))
        return fact_in_train
    fact_in_train = set([])
    ori_data = json.load(open(data_file_name))
    for data in ori_data:
        vertexSet = data['vertexSet']
        for label in data['labels']:
            rel = label['r']
            for n1 in vertexSet[label['h']]:
                for n2 in vertexSet[label['t']]:
                    fact_in_train.add((n1['name'], n2['name'], rel))
    with open(fact_file_name, 'w') as f:
        json.dump(list(fact_in_train), f)
    return fact_in_train",1,,,,,,,,,,
DeepKE,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/relation_extraction/document/evaluation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/relation_extraction/document/evaluation.py,,"def gen_train_facts(data_file_name, truth_dir):
    fact_file_name = data_file_name[data_file_name.find('train_'):]
    fact_file_name = os.path.join(truth_dir, fact_file_name.replace('.json', '.fact'))
    if os.path.exists(fact_file_name):
        fact_in_train = set([])
        triples = json.load(open(fact_file_name))
        for x in triples:
            fact_in_train.add(tuple(x))
        return fact_in_train
    fact_in_train = set([])
    ori_data = json.load(open(data_file_name))
    for data in ori_data:
        vertexSet = data['vertexSet']
        for label in data['labels']:
            rel = label['r']
            for n1 in vertexSet[label['h']]:
                for n2 in vertexSet[label['t']]:
                    fact_in_train.add((n1['name'], n2['name'], rel))
    json.dump(list(fact_in_train), open(fact_file_name, 'w'))
    return fact_in_train","if os.path.exists(fact_file_name):
    fact_in_train = set([])
    triples = json.load(open(fact_file_name))
    for x in triples:
        fact_in_train.add(tuple(x))
    return fact_in_train","if os.path.exists(fact_file_name):
    fact_in_train = set([])
    with open(fact_file_name, 'r') as f:
        triples = json.load(f)
        for x in triples:
            fact_in_train.add(tuple(x))
    return fact_in_train",1,,,,,,,,,,
