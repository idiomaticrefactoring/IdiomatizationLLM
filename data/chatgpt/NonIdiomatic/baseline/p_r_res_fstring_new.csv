repo_name,file_path,file_html,class_name,me_code,old_code,new_code,gd_truth_code,acc,baseline_acc,our_method,,
azure-cli,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/netappfiles/tests/latest/test_volume_commands.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/netappfiles/tests/latest/test_volume_commands.py,AzureNetAppFilesVolumeServiceScenarioTest,"def test_export_policy(self):
    account_name = self.create_random_name(prefix='cli-acc-', length=24)
    pool_name = self.create_random_name(prefix='cli-pool-', length=24)
    volume_name = self.create_random_name(prefix='cli-vol-', length=24)
    volume = self.create_volume(account_name, pool_name, volume_name, '{rg}')
    assert volume['name'] == account_name + '/' + pool_name + '/' + volume_name
    vol_with_export_policy = self.cmd(""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.3.0/24' --rule-index 3 --unix-read-only true --unix-read-write false --cifs false --nfsv3 true --nfsv41 false --has-root-access false"" % (account_name, pool_name, volume_name)).get_output_in_json()
    assert vol_with_export_policy['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert vol_with_export_policy['exportPolicy']['rules'][0]['allowedClients'] == '1.2.3.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['ruleIndex'] == 3
    assert vol_with_export_policy['exportPolicy']['rules'][0]['cifs'] is False
    assert vol_with_export_policy['exportPolicy']['rules'][0]['hasRootAccess'] is False
    vol_with_export_policy = self.cmd(""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.4.0/24' --rule-index 2 --unix-read-only true --unix-read-write false --cifs true --nfsv3 true --nfsv41 false"" % (account_name, pool_name, volume_name)).get_output_in_json()
    assert vol_with_export_policy['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert vol_with_export_policy['exportPolicy']['rules'][1]['allowedClients'] == '1.2.3.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['allowedClients'] == '1.2.4.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['cifs'] is True
    assert len(vol_with_export_policy['exportPolicy']['rules']) == 3
    export_policy = self.cmd('netappfiles volume export-policy list -g {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)).get_output_in_json()
    assert len(export_policy['rules']) == 3
    self.cmd('netappfiles volume export-policy remove -g {rg} -a %s -p %s -v %s --rule-index 3' % (account_name, pool_name, volume_name)).get_output_in_json()
    if self.is_live or self.in_recording:
        time.sleep(240)
    volume = self.cmd('az netappfiles volume show --resource-group {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)).get_output_in_json()
    assert volume['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert len(volume['exportPolicy']['rules']) == 2","'az netappfiles volume show --resource-group {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)",f'az netappfiles volume show --resource-group {{rg}} -a {account_name} -p {pool_name} -v {volume_name}',["f'az netappfiles volume show --resource-group {rg} -a {account_name} -p {pool_name} -v {volume_name}'"],-1,1,-1,1,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration)",f'{out_dir}/debug_batch_images_{iteration}.png',['f"{out_dir}/{\'debug_batch_images\'}_{iteration}.png"'],-1,1,-1,1,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration)",f'{out_dir}/debug_batch_targets_{iteration}.png',['f"{out_dir}/{\'debug_batch_targets\'}_{iteration}.png"'],-1,1,-1,1,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration)",f'{out_dir}/debug_batch_outputs_{iteration}.png',['f"{out_dir}/{\'debug_batch_outputs\'}_{iteration}.png"'],-1,1,-1,1,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration)",f'{out_dir}/debug_test_images_{iteration}.png',['f"{out_dir}/{\'debug_test_images\'}_{iteration}.png"'],-1,1,-1,1,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration)",f'{out_dir}/debug_test_targets_{iteration}.png',['f"{out_dir}/{\'debug_test_targets\'}_{iteration}.png"'],-1,1,-1,1,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration)",f'{out_dir}/debug_test_outputs_{iteration}.png',['f"{out_dir}/{\'debug_test_outputs\'}_{iteration}.png"'],-1,1,-1,1,
hangoutsbot,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/xkcd.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/xkcd.py,,"def _search_comic(bot, event, terms):
    request = (yield from aiohttp.request('get', 'https://relevantxkcd.appspot.com/process?%s' % urllib.parse.urlencode({'action': 'xkcd', 'query': ' '.join(terms)})))
    raw = (yield from request.read())
    values = [row.strip().split(' ')[0] for row in raw.decode().strip().split('\n')]
    weight = float(values.pop(0))
    values.pop(0)
    comics = [int(i) for i in values]
    num = comics.pop(0)
    msg = 'Most relevant xkcd: #%d (relevance: %.2f%%)\nOther relevant comics: %s' % (num, weight * 100, ', '.join(('#%d' % i for i in comics)))
    yield from _get_comic(bot, num)
    yield from bot.coro_send_message(event.conv.id_, msg)
    yield from _print_comic(bot, event, num)","'Most relevant xkcd: #%d (relevance: %.2f%%)\nOther relevant comics: %s' % (num, weight * 100, ', '.join(('#%d' % i for i in comics)))","f""Most relevant xkcd: #{num} (relevance: {weight * 100:.2f}%)\nOther relevant comics: {', '.join((f'#{i}' for i in comics))}""","['f""Most relevant xkcd: #{num} (relevance: {weight * 100:.2f}%)\\nOther relevant comics: {\', \'.join((\'#%d\' % i for i in comics))}""']",-1,1,-1,1,
recordlinkage,https://github.com/J535D165/recordlinkage/tree/master//versioneer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/recordlinkage//versioneer.py,cmd_version,"def run(self):
    vers = get_versions(verbose=True)
    print('Version: %s' % vers['version'])
    print(' full-revisionid: %s' % vers.get('full-revisionid'))
    print(' dirty: %s' % vers.get('dirty'))
    if vers['error']:
        print(' error: %s' % vers['error'])",' full-revisionid: %s' % vers.get('full-revisionid'),f" full-revisionid: {vers.get('full-revisionid')}",['f"full-revisionid: {vers.get(\'full-revisionid\')}"'],-1,1,-1,1,
recordlinkage,https://github.com/J535D165/recordlinkage/tree/master//versioneer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/recordlinkage//versioneer.py,cmd_version,"def run(self):
    vers = get_versions(verbose=True)
    print('Version: %s' % vers['version'])
    print(' full-revisionid: %s' % vers.get('full-revisionid'))
    print(' dirty: %s' % vers.get('dirty'))
    if vers['error']:
        print(' error: %s' % vers['error'])",' error: %s' % vers['error'],f" error: {vers['error']}",['f"error: {vers[\'error\']}"'],-1,1,-1,1,
galaxy,https://github.com/ansible/galaxy/tree/master/lib/tool_shed/dependencies/repository/relation_builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/tool_shed/dependencies/repository/relation_builder.py,RelationBuilder,"def get_updated_changeset_revisions_for_repository_dependencies(self, key_rd_dicts):
    updated_key_rd_dicts = []
    for key_rd_dict in key_rd_dicts:
        key = next(iter(key_rd_dict))
        repository_dependency = key_rd_dict[key]
        (rd_toolshed, rd_name, rd_owner, rd_changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td) = common_util.parse_repository_dependency_tuple(repository_dependency)
        if suc.tool_shed_is_this_tool_shed(rd_toolshed):
            repository = tool_shed.util.repository_util.get_repository_by_name_and_owner(self.app, rd_name, rd_owner)
            if repository:
                repository_id = self.app.security.encode_id(repository.id)
                repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, rd_changeset_revision)
                if repository_metadata:
                    new_key_rd_dict = {}
                    new_key_rd_dict[key] = repository_dependency
                    updated_key_rd_dicts.append(key_rd_dict)
                else:
                    changeset_revision = metadata_util.get_next_downloadable_changeset_revision(self.app, repository, rd_changeset_revision)
                    if changeset_revision != rd_changeset_revision:
                        repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, changeset_revision)
                    if repository_metadata:
                        new_key_rd_dict = {}
                        new_key_rd_dict[key] = [rd_toolshed, rd_name, rd_owner, repository_metadata.changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td]
                        updated_key_rd_dicts.append(new_key_rd_dict)
                    else:
                        repository_components_tuple = container_util.get_components_from_key(key)
                        components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                        (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                        if len(components_list) in (4, 5):
                            rd_only_if_compiling_contained_td = 'False'
                        message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                        message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                        log.debug(message)
            else:
                repository_components_tuple = container_util.get_components_from_key(key)
                components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                log.debug(message)
    return updated_key_rd_dicts","The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))","[""f'The revision {str(rd_changeset_revision)} defined for repository {str(rd_name)} owned by {str(rd_owner)} is invalid, so repository'""]","[""f'The revision {rd_changeset_revision} defined for repository {rd_name} owned by {rd_owner} is invalid, so repository dependencies defined for repository {repository_name} will be ignored.'",-1,1,-1,1,
galaxy,https://github.com/ansible/galaxy/tree/master/lib/tool_shed/dependencies/repository/relation_builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/tool_shed/dependencies/repository/relation_builder.py,RelationBuilder,"def get_updated_changeset_revisions_for_repository_dependencies(self, key_rd_dicts):
    updated_key_rd_dicts = []
    for key_rd_dict in key_rd_dicts:
        key = next(iter(key_rd_dict))
        repository_dependency = key_rd_dict[key]
        (rd_toolshed, rd_name, rd_owner, rd_changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td) = common_util.parse_repository_dependency_tuple(repository_dependency)
        if suc.tool_shed_is_this_tool_shed(rd_toolshed):
            repository = tool_shed.util.repository_util.get_repository_by_name_and_owner(self.app, rd_name, rd_owner)
            if repository:
                repository_id = self.app.security.encode_id(repository.id)
                repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, rd_changeset_revision)
                if repository_metadata:
                    new_key_rd_dict = {}
                    new_key_rd_dict[key] = repository_dependency
                    updated_key_rd_dicts.append(key_rd_dict)
                else:
                    changeset_revision = metadata_util.get_next_downloadable_changeset_revision(self.app, repository, rd_changeset_revision)
                    if changeset_revision != rd_changeset_revision:
                        repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, changeset_revision)
                    if repository_metadata:
                        new_key_rd_dict = {}
                        new_key_rd_dict[key] = [rd_toolshed, rd_name, rd_owner, repository_metadata.changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td]
                        updated_key_rd_dicts.append(new_key_rd_dict)
                    else:
                        repository_components_tuple = container_util.get_components_from_key(key)
                        components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                        (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                        if len(components_list) in (4, 5):
                            rd_only_if_compiling_contained_td = 'False'
                        message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                        message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                        log.debug(message)
            else:
                repository_components_tuple = container_util.get_components_from_key(key)
                components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                log.debug(message)
    return updated_key_rd_dicts","'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))","[""f'The revision {str(rd_changeset_revision)} defined for repository {str(rd_name)} owned by {str(rd_owner)} is invalid, so repository'""]",no_found,0,,,,
bridgy,https://github.com/snarfed/bridgy/tree/master/tests/test_cron.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bridgy/tests/test_cron.py,CronTest,"def test_update_mastodon_pictures_get_actor_404(self):
    self.expect_requests_get('https://foo.com' + test_mastodon.API_ACCOUNT % 123, headers={'Authorization': 'Bearer towkin'}).AndRaise(requests.exceptions.HTTPError(response=util.Struct(status_code='404', text='foo')))
    self.mox.ReplayAll()
    mastodon = self._setup_mastodon()
    resp = self.client.get('/cron/update_mastodon_pictures')
    self.assertEqual(200, resp.status_code)
    self.assertEqual('http://before', mastodon.key.get().picture)",test_mastodon.API_ACCOUNT % 123,["f'{test_mastodon.API_ACCOUNT}{123}'"],no_found,,0,-1,0,
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/blocks/plan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/blocks/plan.py,TrainPlan,"def _run(self, supervisor, session):
    train_feed_dict = self.train_feeds.copy()
    train_fetches = {'train_op': self.train_op, 'loss': self.loss_total, 'step': self.global_step}
    if self.compute_summaries:
        train_fetches['summaries'] = self.summaries
    if self.examples:
        (epochs, train_size) = self._by_feed_dict(train_feed_dict)
    else:
        (epochs, train_size) = self._by_input_tensor(train_feed_dict)
    if self.dev_examples:
        gen_dev_batches = util.epochs(((len(batch), self.compiler.build_feed_dict(batch)) for batch in util.group_by_batches(self.dev_examples, self.batch_size)), shuffle=False)
        ckpt = tf.train.get_checkpoint_state(self.logdir)
        if ckpt and ckpt.model_checkpoint_path:
            (_, self._best_loss, _) = self._eval_batches(supervisor, session, next(gen_dev_batches), None, is_dev=True)
            if self._best_loss is None:
                return
    for (epoch, batches) in enumerate(epochs, 1):
        train_loss = 0.0
        for _ in batches:
            if self._should_stop(supervisor):
                return
            results = session.run(train_fetches, train_feed_dict)
            train_loss += results['loss']
            if self.compute_summaries:
                supervisor.summary_computed(session, results['summaries'], results['step'])
        if train_size == 0:
            raise ValueError('examples must be non-empty')
        if self.exact_batch_sizes and epoch == 1:
            if train_size < self.batch_size:
                raise ValueError('when exact_batch_sizes is true, examples must have at least batch_size items; %s vs. %s' % (train_size, self.batch_size))
            train_size -= train_size % self.batch_size
        train_loss /= train_size
        self.report_loss(results['step'], train_loss)
        log_str = 'epoch:%5d train[loss: %.3e]' % (epoch, train_loss)
        if self.dev_examples:
            (dev_size, dev_loss, dev_metrics) = self._eval_batches(supervisor, session, next(gen_dev_batches), results['step'], is_dev=True)
            if dev_size is None:
                return
            if epoch == 1:
                self.log_and_print('train_size: %d dev_size: %d' % (train_size, dev_size))
            log_str += ' dev[%s]' % _eval_str(dev_size, dev_loss, dev_metrics)
            self.log_and_print(log_str)
            self._save_best(session, supervisor.saver, dev_loss, results['step'])
        else:
            if epoch == 1:
                self.log_and_print('train_size: %d' % train_size)
            self.log_and_print(log_str)
    if not self.dev_examples and self.is_chief_trainer:
        save_path = os.path.join(self.logdir, 'model.ckpt')
        save_fname = supervisor.saver.save(session, save_path, global_step=results['step'])
        self.log_and_print('final model saved in file: %s' % save_fname)"," dev[%s]' % _eval_str(dev_size, dev_loss, dev_metrics)","[""f' dev[{_eval_str(dev_size, dev_loss, dev_metrics)}]'""]",no_found,0,,,,
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/blocks/plan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/blocks/plan.py,TrainPlan,"def _run(self, supervisor, session):
    train_feed_dict = self.train_feeds.copy()
    train_fetches = {'train_op': self.train_op, 'loss': self.loss_total, 'step': self.global_step}
    if self.compute_summaries:
        train_fetches['summaries'] = self.summaries
    if self.examples:
        (epochs, train_size) = self._by_feed_dict(train_feed_dict)
    else:
        (epochs, train_size) = self._by_input_tensor(train_feed_dict)
    if self.dev_examples:
        gen_dev_batches = util.epochs(((len(batch), self.compiler.build_feed_dict(batch)) for batch in util.group_by_batches(self.dev_examples, self.batch_size)), shuffle=False)
        ckpt = tf.train.get_checkpoint_state(self.logdir)
        if ckpt and ckpt.model_checkpoint_path:
            (_, self._best_loss, _) = self._eval_batches(supervisor, session, next(gen_dev_batches), None, is_dev=True)
            if self._best_loss is None:
                return
    for (epoch, batches) in enumerate(epochs, 1):
        train_loss = 0.0
        for _ in batches:
            if self._should_stop(supervisor):
                return
            results = session.run(train_fetches, train_feed_dict)
            train_loss += results['loss']
            if self.compute_summaries:
                supervisor.summary_computed(session, results['summaries'], results['step'])
        if train_size == 0:
            raise ValueError('examples must be non-empty')
        if self.exact_batch_sizes and epoch == 1:
            if train_size < self.batch_size:
                raise ValueError('when exact_batch_sizes is true, examples must have at least batch_size items; %s vs. %s' % (train_size, self.batch_size))
            train_size -= train_size % self.batch_size
        train_loss /= train_size
        self.report_loss(results['step'], train_loss)
        log_str = 'epoch:%5d train[loss: %.3e]' % (epoch, train_loss)
        if self.dev_examples:
            (dev_size, dev_loss, dev_metrics) = self._eval_batches(supervisor, session, next(gen_dev_batches), results['step'], is_dev=True)
            if dev_size is None:
                return
            if epoch == 1:
                self.log_and_print('train_size: %d dev_size: %d' % (train_size, dev_size))
            log_str += ' dev[%s]' % _eval_str(dev_size, dev_loss, dev_metrics)
            self.log_and_print(log_str)
            self._save_best(session, supervisor.saver, dev_loss, results['step'])
        else:
            if epoch == 1:
                self.log_and_print('train_size: %d' % train_size)
            self.log_and_print(log_str)
    if not self.dev_examples and self.is_chief_trainer:
        save_path = os.path.join(self.logdir, 'model.ckpt')
        save_fname = supervisor.saver.save(session, save_path, global_step=results['step'])
        self.log_and_print('final model saved in file: %s' % save_fname)",'final model saved in file: %s' % save_fname,["f'final model saved in file: {save_fname}'"],no_found,0,,,,
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/blocks/plan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/blocks/plan.py,TrainPlan,"def _run(self, supervisor, session):
    train_feed_dict = self.train_feeds.copy()
    train_fetches = {'train_op': self.train_op, 'loss': self.loss_total, 'step': self.global_step}
    if self.compute_summaries:
        train_fetches['summaries'] = self.summaries
    if self.examples:
        (epochs, train_size) = self._by_feed_dict(train_feed_dict)
    else:
        (epochs, train_size) = self._by_input_tensor(train_feed_dict)
    if self.dev_examples:
        gen_dev_batches = util.epochs(((len(batch), self.compiler.build_feed_dict(batch)) for batch in util.group_by_batches(self.dev_examples, self.batch_size)), shuffle=False)
        ckpt = tf.train.get_checkpoint_state(self.logdir)
        if ckpt and ckpt.model_checkpoint_path:
            (_, self._best_loss, _) = self._eval_batches(supervisor, session, next(gen_dev_batches), None, is_dev=True)
            if self._best_loss is None:
                return
    for (epoch, batches) in enumerate(epochs, 1):
        train_loss = 0.0
        for _ in batches:
            if self._should_stop(supervisor):
                return
            results = session.run(train_fetches, train_feed_dict)
            train_loss += results['loss']
            if self.compute_summaries:
                supervisor.summary_computed(session, results['summaries'], results['step'])
        if train_size == 0:
            raise ValueError('examples must be non-empty')
        if self.exact_batch_sizes and epoch == 1:
            if train_size < self.batch_size:
                raise ValueError('when exact_batch_sizes is true, examples must have at least batch_size items; %s vs. %s' % (train_size, self.batch_size))
            train_size -= train_size % self.batch_size
        train_loss /= train_size
        self.report_loss(results['step'], train_loss)
        log_str = 'epoch:%5d train[loss: %.3e]' % (epoch, train_loss)
        if self.dev_examples:
            (dev_size, dev_loss, dev_metrics) = self._eval_batches(supervisor, session, next(gen_dev_batches), results['step'], is_dev=True)
            if dev_size is None:
                return
            if epoch == 1:
                self.log_and_print('train_size: %d dev_size: %d' % (train_size, dev_size))
            log_str += ' dev[%s]' % _eval_str(dev_size, dev_loss, dev_metrics)
            self.log_and_print(log_str)
            self._save_best(session, supervisor.saver, dev_loss, results['step'])
        else:
            if epoch == 1:
                self.log_and_print('train_size: %d' % train_size)
            self.log_and_print(log_str)
    if not self.dev_examples and self.is_chief_trainer:
        save_path = os.path.join(self.logdir, 'model.ckpt')
        save_fname = supervisor.saver.save(session, save_path, global_step=results['step'])
        self.log_and_print('final model saved in file: %s' % save_fname)","'when exact_batch_sizes is true, examples must have at least batch_size items; %s vs. %s' % (train_size, self.batch_size)","[""f'when exact_batch_sizes is true, examples must have at least batch_size items; {train_size} vs. {self.batch_size}'""]",no_found,0,,,,
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/blocks/plan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/blocks/plan.py,TrainPlan,"def _run(self, supervisor, session):
    train_feed_dict = self.train_feeds.copy()
    train_fetches = {'train_op': self.train_op, 'loss': self.loss_total, 'step': self.global_step}
    if self.compute_summaries:
        train_fetches['summaries'] = self.summaries
    if self.examples:
        (epochs, train_size) = self._by_feed_dict(train_feed_dict)
    else:
        (epochs, train_size) = self._by_input_tensor(train_feed_dict)
    if self.dev_examples:
        gen_dev_batches = util.epochs(((len(batch), self.compiler.build_feed_dict(batch)) for batch in util.group_by_batches(self.dev_examples, self.batch_size)), shuffle=False)
        ckpt = tf.train.get_checkpoint_state(self.logdir)
        if ckpt and ckpt.model_checkpoint_path:
            (_, self._best_loss, _) = self._eval_batches(supervisor, session, next(gen_dev_batches), None, is_dev=True)
            if self._best_loss is None:
                return
    for (epoch, batches) in enumerate(epochs, 1):
        train_loss = 0.0
        for _ in batches:
            if self._should_stop(supervisor):
                return
            results = session.run(train_fetches, train_feed_dict)
            train_loss += results['loss']
            if self.compute_summaries:
                supervisor.summary_computed(session, results['summaries'], results['step'])
        if train_size == 0:
            raise ValueError('examples must be non-empty')
        if self.exact_batch_sizes and epoch == 1:
            if train_size < self.batch_size:
                raise ValueError('when exact_batch_sizes is true, examples must have at least batch_size items; %s vs. %s' % (train_size, self.batch_size))
            train_size -= train_size % self.batch_size
        train_loss /= train_size
        self.report_loss(results['step'], train_loss)
        log_str = 'epoch:%5d train[loss: %.3e]' % (epoch, train_loss)
        if self.dev_examples:
            (dev_size, dev_loss, dev_metrics) = self._eval_batches(supervisor, session, next(gen_dev_batches), results['step'], is_dev=True)
            if dev_size is None:
                return
            if epoch == 1:
                self.log_and_print('train_size: %d dev_size: %d' % (train_size, dev_size))
            log_str += ' dev[%s]' % _eval_str(dev_size, dev_loss, dev_metrics)
            self.log_and_print(log_str)
            self._save_best(session, supervisor.saver, dev_loss, results['step'])
        else:
            if epoch == 1:
                self.log_and_print('train_size: %d' % train_size)
            self.log_and_print(log_str)
    if not self.dev_examples and self.is_chief_trainer:
        save_path = os.path.join(self.logdir, 'model.ckpt')
        save_fname = supervisor.saver.save(session, save_path, global_step=results['step'])
        self.log_and_print('final model saved in file: %s' % save_fname)",'train_size: %d' % train_size,["f'train_size: {train_size}'"],no_found,0,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/options.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/options.py,,"def parseOpts(overrideArguments=None):

    def _readOptions(filename_bytes, default=[]):
        try:
            optionf = open(filename_bytes)
        except IOError:
            return default
        try:
            contents = optionf.read()
            if sys.version_info < (3,):
                contents = contents.decode(preferredencoding())
            res = compat_shlex_split(contents, comments=True)
        finally:
            optionf.close()
        return res

    def _readUserConf():
        xdg_config_home = compat_getenv('XDG_CONFIG_HOME')
        if xdg_config_home:
            userConfFile = os.path.join(xdg_config_home, 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(xdg_config_home, 'youtube-dl.conf')
        else:
            userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl.conf')
        userConf = _readOptions(userConfFile, None)
        if userConf is None:
            appdata_dir = compat_getenv('appdata')
            if appdata_dir:
                userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config'), default=None)
                if userConf is None:
                    userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config.txt'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf.txt'), default=None)
        if userConf is None:
            userConf = []
        return userConf

    def _format_option_string(option):
        """""" ('-o', '--option') -> -o, --format METAVAR""""""
        opts = []
        if option._short_opts:
            opts.append(option._short_opts[0])
        if option._long_opts:
            opts.append(option._long_opts[0])
        if len(opts) > 1:
            opts.insert(1, ', ')
        if option.takes_value():
            opts.append(' %s' % option.metavar)
        return ''.join(opts)

    def _comma_separated_values_options_callback(option, opt_str, value, parser):
        setattr(parser.values, option.dest, value.split(','))
    columns = compat_get_terminal_size().columns
    max_width = columns if columns else 80
    max_help_position = 80
    fmt = optparse.IndentedHelpFormatter(width=max_width, max_help_position=max_help_position)
    fmt.format_option_strings = _format_option_string
    kw = {'version': __version__, 'formatter': fmt, 'usage': '%prog [OPTIONS] URL [URL...]', 'conflict_handler': 'resolve'}
    parser = optparse.OptionParser(**compat_kwargs(kw))
    general = optparse.OptionGroup(parser, 'General Options')
    general.add_option('-h', '--help', action='help', help='Print this help text and exit')
    general.add_option('--version', action='version', help='Print program version and exit')
    general.add_option('-U', '--update', action='store_true', dest='update_self', help='Update this program to latest version. Make sure that you have sufficient permissions (run with sudo if needed)')
    general.add_option('-i', '--ignore-errors', action='store_true', dest='ignoreerrors', default=False, help='Continue on download errors, for example to skip unavailable videos in a playlist')
    general.add_option('--abort-on-error', action='store_false', dest='ignoreerrors', help='Abort downloading of further videos (in the playlist or the command line) if an error occurs')
    general.add_option('--dump-user-agent', action='store_true', dest='dump_user_agent', default=False, help='Display the current browser identification')
    general.add_option('--list-extractors', action='store_true', dest='list_extractors', default=False, help='List all supported extractors')
    general.add_option('--extractor-descriptions', action='store_true', dest='list_extractor_descriptions', default=False, help='Output descriptions of all supported extractors')
    general.add_option('--force-generic-extractor', action='store_true', dest='force_generic_extractor', default=False, help='Force extraction to use the generic extractor')
    general.add_option('--default-search', dest='default_search', metavar='PREFIX', help='Use this prefix for unqualified URLs. For example ""gvsearch2:"" downloads two videos from google videos for youtube-dl ""large apple"". Use the value ""auto"" to let youtube-dl guess (""auto_warning"" to emit a warning when guessing). ""error"" just throws an error. The default value ""fixup_error"" repairs broken URLs, but emits an error if this is not possible instead of searching.')
    general.add_option('--ignore-config', action='store_true', help='Do not read configuration files. When given in the global configuration file /etc/youtube-dl.conf: Do not read the user configuration in ~/.config/youtube-dl/config (%APPDATA%/youtube-dl/config.txt on Windows)')
    general.add_option('--config-location', dest='config_location', metavar='PATH', help='Location of the configuration file; either the path to the config or its containing directory.')
    general.add_option('--flat-playlist', action='store_const', dest='extract_flat', const='in_playlist', default=False, help='Do not extract the videos of a playlist, only list them.')
    general.add_option('--mark-watched', action='store_true', dest='mark_watched', default=False, help='Mark videos watched (YouTube only)')
    general.add_option('--no-mark-watched', action='store_false', dest='mark_watched', default=False, help='Do not mark videos watched (YouTube only)')
    general.add_option('--no-color', '--no-colors', action='store_true', dest='no_color', default=False, help='Do not emit color codes in output')
    network = optparse.OptionGroup(parser, 'Network Options')
    network.add_option('--proxy', dest='proxy', default=None, metavar='URL', help='Use the specified HTTP/HTTPS/SOCKS proxy. To enable SOCKS proxy, specify a proper scheme. For example socks5://127.0.0.1:1080/. Pass in an empty string (--proxy """") for direct connection')
    network.add_option('--socket-timeout', dest='socket_timeout', type=float, default=None, metavar='SECONDS', help='Time to wait before giving up, in seconds')
    network.add_option('--source-address', metavar='IP', dest='source_address', default=None, help='Client-side IP address to bind to')
    network.add_option('-4', '--force-ipv4', action='store_const', const='0.0.0.0', dest='source_address', help='Make all connections via IPv4')
    network.add_option('-6', '--force-ipv6', action='store_const', const='::', dest='source_address', help='Make all connections via IPv6')
    geo = optparse.OptionGroup(parser, 'Geo Restriction')
    geo.add_option('--geo-verification-proxy', dest='geo_verification_proxy', default=None, metavar='URL', help='Use this proxy to verify the IP address for some geo-restricted sites. The default proxy specified by --proxy (or none, if the option is not present) is used for the actual downloading.')
    geo.add_option('--cn-verification-proxy', dest='cn_verification_proxy', default=None, metavar='URL', help=optparse.SUPPRESS_HELP)
    geo.add_option('--geo-bypass', action='store_true', dest='geo_bypass', default=True, help='Bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--no-geo-bypass', action='store_false', dest='geo_bypass', default=True, help='Do not bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--geo-bypass-country', metavar='CODE', dest='geo_bypass_country', default=None, help='Force bypass geographic restriction with explicitly provided two-letter ISO 3166-2 country code')
    geo.add_option('--geo-bypass-ip-block', metavar='IP_BLOCK', dest='geo_bypass_ip_block', default=None, help='Force bypass geographic restriction with explicitly provided IP block in CIDR notation')
    selection = optparse.OptionGroup(parser, 'Video Selection')
    selection.add_option('--playlist-start', dest='playliststart', metavar='NUMBER', default=1, type=int, help='Playlist video to start at (default is %default)')
    selection.add_option('--playlist-end', dest='playlistend', metavar='NUMBER', default=None, type=int, help='Playlist video to end at (default is last)')
    selection.add_option('--playlist-items', dest='playlist_items', metavar='ITEM_SPEC', default=None, help='Playlist video items to download. Specify indices of the videos in the playlist separated by commas like: ""--playlist-items 1,2,5,8"" if you want to download videos indexed 1, 2, 5, 8 in the playlist. You can specify range: ""--playlist-items 1-3,7,10-13"", it will download the videos at index 1, 2, 3, 7, 10, 11, 12 and 13.')
    selection.add_option('--match-title', dest='matchtitle', metavar='REGEX', help='Download only matching titles (regex or caseless sub-string)')
    selection.add_option('--reject-title', dest='rejecttitle', metavar='REGEX', help='Skip download for matching titles (regex or caseless sub-string)')
    selection.add_option('--max-downloads', dest='max_downloads', metavar='NUMBER', type=int, default=None, help='Abort after downloading NUMBER files')
    selection.add_option('--min-filesize', metavar='SIZE', dest='min_filesize', default=None, help='Do not download any videos smaller than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--max-filesize', metavar='SIZE', dest='max_filesize', default=None, help='Do not download any videos larger than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--date', metavar='DATE', dest='date', default=None, help='Download only videos uploaded in this date')
    selection.add_option('--datebefore', metavar='DATE', dest='datebefore', default=None, help='Download only videos uploaded on or before this date (i.e. inclusive)')
    selection.add_option('--dateafter', metavar='DATE', dest='dateafter', default=None, help='Download only videos uploaded on or after this date (i.e. inclusive)')
    selection.add_option('--min-views', metavar='COUNT', dest='min_views', default=None, type=int, help='Do not download any videos with less than COUNT views')
    selection.add_option('--max-views', metavar='COUNT', dest='max_views', default=None, type=int, help='Do not download any videos with more than COUNT views')
    selection.add_option('--match-filter', metavar='FILTER', dest='match_filter', default=None, help='Generic video filter. Specify any key (see the ""OUTPUT TEMPLATE"" for a list of available keys) to match if the key is present, !key to check if the key is not present, key > NUMBER (like ""comment_count > 12"", also works with >=, <, <=, !=, =) to compare against a number, key = \'LITERAL\' (like ""uploader = \'Mike Smith\'"", also works with !=) to match against a string literal and & to require multiple matches. Values which are not known are excluded unless you put a question mark (?) after the operator. For example, to only match videos that have been liked more than 100 times and disliked less than 50 times (or the dislike functionality is not available at the given service), but who also have a description, use --match-filter ""like_count > 100 & dislike_count <? 50 & description"" .')
    selection.add_option('--no-playlist', action='store_true', dest='noplaylist', default=False, help='Download only the video, if the URL refers to a video and a playlist.')
    selection.add_option('--yes-playlist', action='store_false', dest='noplaylist', default=False, help='Download the playlist, if the URL refers to a video and a playlist.')
    selection.add_option('--age-limit', metavar='YEARS', dest='age_limit', default=None, type=int, help='Download only videos suitable for the given age')
    selection.add_option('--download-archive', metavar='FILE', dest='download_archive', help='Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it.')
    selection.add_option('--include-ads', dest='include_ads', action='store_true', help='Download advertisements as well (experimental)')
    authentication = optparse.OptionGroup(parser, 'Authentication Options')
    authentication.add_option('-u', '--username', dest='username', metavar='USERNAME', help='Login with this account ID')
    authentication.add_option('-p', '--password', dest='password', metavar='PASSWORD', help='Account password. If this option is left out, youtube-dl will ask interactively.')
    authentication.add_option('-2', '--twofactor', dest='twofactor', metavar='TWOFACTOR', help='Two-factor authentication code')
    authentication.add_option('-n', '--netrc', action='store_true', dest='usenetrc', default=False, help='Use .netrc authentication data')
    authentication.add_option('--video-password', dest='videopassword', metavar='PASSWORD', help='Video password (vimeo, smotri, youku)')
    adobe_pass = optparse.OptionGroup(parser, 'Adobe Pass Options')
    adobe_pass.add_option('--ap-mso', dest='ap_mso', metavar='MSO', help='Adobe Pass multiple-system operator (TV provider) identifier, use --ap-list-mso for a list of available MSOs')
    adobe_pass.add_option('--ap-username', dest='ap_username', metavar='USERNAME', help='Multiple-system operator account login')
    adobe_pass.add_option('--ap-password', dest='ap_password', metavar='PASSWORD', help='Multiple-system operator account password. If this option is left out, youtube-dl will ask interactively.')
    adobe_pass.add_option('--ap-list-mso', action='store_true', dest='ap_list_mso', default=False, help='List all supported multiple-system operators')
    video_format = optparse.OptionGroup(parser, 'Video Format Options')
    video_format.add_option('-f', '--format', action='store', dest='format', metavar='FORMAT', default=None, help='Video format code, see the ""FORMAT SELECTION"" for all the info')
    video_format.add_option('--all-formats', action='store_const', dest='format', const='all', help='Download all available video formats')
    video_format.add_option('--prefer-free-formats', action='store_true', dest='prefer_free_formats', default=False, help='Prefer free video formats unless a specific one is requested')
    video_format.add_option('-F', '--list-formats', action='store_true', dest='listformats', help='List all available formats of requested videos')
    video_format.add_option('--youtube-include-dash-manifest', action='store_true', dest='youtube_include_dash_manifest', default=True, help=optparse.SUPPRESS_HELP)
    video_format.add_option('--youtube-skip-dash-manifest', action='store_false', dest='youtube_include_dash_manifest', help='Do not download the DASH manifests and related data on YouTube videos')
    video_format.add_option('--merge-output-format', action='store', dest='merge_output_format', metavar='FORMAT', default=None, help='If a merge is required (e.g. bestvideo+bestaudio), output to given container format. One of mkv, mp4, ogg, webm, flv. Ignored if no merge is required')
    subtitles = optparse.OptionGroup(parser, 'Subtitle Options')
    subtitles.add_option('--write-sub', '--write-srt', action='store_true', dest='writesubtitles', default=False, help='Write subtitle file')
    subtitles.add_option('--write-auto-sub', '--write-automatic-sub', action='store_true', dest='writeautomaticsub', default=False, help='Write automatically generated subtitle file (YouTube only)')
    subtitles.add_option('--all-subs', action='store_true', dest='allsubtitles', default=False, help='Download all the available subtitles of the video')
    subtitles.add_option('--list-subs', action='store_true', dest='listsubtitles', default=False, help='List all available subtitles for the video')
    subtitles.add_option('--sub-format', action='store', dest='subtitlesformat', metavar='FORMAT', default='best', help='Subtitle format, accepts formats preference, for example: ""srt"" or ""ass/srt/best""')
    subtitles.add_option('--sub-lang', '--sub-langs', '--srt-lang', action='callback', dest='subtitleslangs', metavar='LANGS', type='str', default=[], callback=_comma_separated_values_options_callback, help='Languages of the subtitles to download (optional) separated by commas, use --list-subs for available language tags')
    downloader = optparse.OptionGroup(parser, 'Download Options')
    downloader.add_option('-r', '--limit-rate', '--rate-limit', dest='ratelimit', metavar='RATE', help='Maximum download rate in bytes per second (e.g. 50K or 4.2M)')
    downloader.add_option('-R', '--retries', dest='retries', metavar='RETRIES', default=10, help='Number of retries (default is %default), or ""infinite"".')
    downloader.add_option('--fragment-retries', dest='fragment_retries', metavar='RETRIES', default=10, help='Number of retries for a fragment (default is %default), or ""infinite"" (DASH, hlsnative and ISM)')
    downloader.add_option('--skip-unavailable-fragments', action='store_true', dest='skip_unavailable_fragments', default=True, help='Skip unavailable fragments (DASH, hlsnative and ISM)')
    downloader.add_option('--abort-on-unavailable-fragment', action='store_false', dest='skip_unavailable_fragments', help='Abort downloading when some fragment is not available')
    downloader.add_option('--keep-fragments', action='store_true', dest='keep_fragments', default=False, help='Keep downloaded fragments on disk after downloading is finished; fragments are erased by default')
    downloader.add_option('--buffer-size', dest='buffersize', metavar='SIZE', default='1024', help='Size of download buffer (e.g. 1024 or 16K) (default is %default)')
    downloader.add_option('--no-resize-buffer', action='store_true', dest='noresizebuffer', default=False, help='Do not automatically adjust the buffer size. By default, the buffer size is automatically resized from an initial value of SIZE.')
    downloader.add_option('--http-chunk-size', dest='http_chunk_size', metavar='SIZE', default=None, help='Size of a chunk for chunk-based HTTP downloading (e.g. 10485760 or 10M) (default is disabled). May be useful for bypassing bandwidth throttling imposed by a webserver (experimental)')
    downloader.add_option('--test', action='store_true', dest='test', default=False, help=optparse.SUPPRESS_HELP)
    downloader.add_option('--playlist-reverse', action='store_true', help='Download playlist videos in reverse order')
    downloader.add_option('--playlist-random', action='store_true', help='Download playlist videos in random order')
    downloader.add_option('--xattr-set-filesize', dest='xattr_set_filesize', action='store_true', help='Set file xattribute ytdl.filesize with expected file size')
    downloader.add_option('--hls-prefer-native', dest='hls_prefer_native', action='store_true', default=None, help='Use the native HLS downloader instead of ffmpeg')
    downloader.add_option('--hls-prefer-ffmpeg', dest='hls_prefer_native', action='store_false', default=None, help='Use ffmpeg instead of the native HLS downloader')
    downloader.add_option('--hls-use-mpegts', dest='hls_use_mpegts', action='store_true', help='Use the mpegts container for HLS videos, allowing to play the video while downloading (some players may not be able to play it)')
    downloader.add_option('--external-downloader', dest='external_downloader', metavar='COMMAND', help='Use the specified external downloader. Currently supports %s' % ','.join(list_external_downloaders()))
    downloader.add_option('--external-downloader-args', dest='external_downloader_args', metavar='ARGS', help='Give these arguments to the external downloader')
    workarounds = optparse.OptionGroup(parser, 'Workarounds')
    workarounds.add_option('--encoding', dest='encoding', metavar='ENCODING', help='Force the specified encoding (experimental)')
    workarounds.add_option('--no-check-certificate', action='store_true', dest='no_check_certificate', default=False, help='Suppress HTTPS certificate validation')
    workarounds.add_option('--prefer-insecure', '--prefer-unsecure', action='store_true', dest='prefer_insecure', help='Use an unencrypted connection to retrieve information about the video. (Currently supported only for YouTube)')
    workarounds.add_option('--user-agent', metavar='UA', dest='user_agent', help='Specify a custom user agent')
    workarounds.add_option('--referer', metavar='URL', dest='referer', default=None, help='Specify a custom referer, use if the video access is restricted to one domain')
    workarounds.add_option('--add-header', metavar='FIELD:VALUE', dest='headers', action='append', help=""Specify a custom HTTP header and its value, separated by a colon ':'. You can use this option multiple times"")
    workarounds.add_option('--bidi-workaround', dest='bidi_workaround', action='store_true', help='Work around terminals that lack bidirectional text support. Requires bidiv or fribidi executable in PATH')
    workarounds.add_option('--sleep-interval', '--min-sleep-interval', metavar='SECONDS', dest='sleep_interval', type=float, help='Number of seconds to sleep before each download when used alone or a lower bound of a range for randomized sleep before each download (minimum possible number of seconds to sleep) when used along with --max-sleep-interval.')
    workarounds.add_option('--max-sleep-interval', metavar='SECONDS', dest='max_sleep_interval', type=float, help='Upper bound of a range for randomized sleep before each download (maximum possible number of seconds to sleep). Must only be used along with --min-sleep-interval.')
    verbosity = optparse.OptionGroup(parser, 'Verbosity / Simulation Options')
    verbosity.add_option('-q', '--quiet', action='store_true', dest='quiet', default=False, help='Activate quiet mode')
    verbosity.add_option('--no-warnings', dest='no_warnings', action='store_true', default=False, help='Ignore warnings')
    verbosity.add_option('-s', '--simulate', action='store_true', dest='simulate', default=False, help='Do not download the video and do not write anything to disk')
    verbosity.add_option('--skip-download', action='store_true', dest='skip_download', default=False, help='Do not download the video')
    verbosity.add_option('-g', '--get-url', action='store_true', dest='geturl', default=False, help='Simulate, quiet but print URL')
    verbosity.add_option('-e', '--get-title', action='store_true', dest='gettitle', default=False, help='Simulate, quiet but print title')
    verbosity.add_option('--get-id', action='store_true', dest='getid', default=False, help='Simulate, quiet but print id')
    verbosity.add_option('--get-thumbnail', action='store_true', dest='getthumbnail', default=False, help='Simulate, quiet but print thumbnail URL')
    verbosity.add_option('--get-description', action='store_true', dest='getdescription', default=False, help='Simulate, quiet but print video description')
    verbosity.add_option('--get-duration', action='store_true', dest='getduration', default=False, help='Simulate, quiet but print video length')
    verbosity.add_option('--get-filename', action='store_true', dest='getfilename', default=False, help='Simulate, quiet but print output filename')
    verbosity.add_option('--get-format', action='store_true', dest='getformat', default=False, help='Simulate, quiet but print output format')
    verbosity.add_option('-j', '--dump-json', action='store_true', dest='dumpjson', default=False, help='Simulate, quiet but print JSON information. See the ""OUTPUT TEMPLATE"" for a description of available keys.')
    verbosity.add_option('-J', '--dump-single-json', action='store_true', dest='dump_single_json', default=False, help='Simulate, quiet but print JSON information for each command-line argument. If the URL refers to a playlist, dump the whole playlist information in a single line.')
    verbosity.add_option('--print-json', action='store_true', dest='print_json', default=False, help='Be quiet and print the video information as JSON (video is still being downloaded).')
    verbosity.add_option('--newline', action='store_true', dest='progress_with_newline', default=False, help='Output progress bar as new lines')
    verbosity.add_option('--no-progress', action='store_true', dest='noprogress', default=False, help='Do not print progress bar')
    verbosity.add_option('--console-title', action='store_true', dest='consoletitle', default=False, help='Display progress in console titlebar')
    verbosity.add_option('-v', '--verbose', action='store_true', dest='verbose', default=False, help='Print various debugging information')
    verbosity.add_option('--dump-pages', '--dump-intermediate-pages', action='store_true', dest='dump_intermediate_pages', default=False, help='Print downloaded pages encoded using base64 to debug problems (very verbose)')
    verbosity.add_option('--write-pages', action='store_true', dest='write_pages', default=False, help='Write downloaded intermediary pages to files in the current directory to debug problems')
    verbosity.add_option('--youtube-print-sig-code', action='store_true', dest='youtube_print_sig_code', default=False, help=optparse.SUPPRESS_HELP)
    verbosity.add_option('--print-traffic', '--dump-headers', dest='debug_printtraffic', action='store_true', default=False, help='Display sent and read HTTP traffic')
    verbosity.add_option('-C', '--call-home', dest='call_home', action='store_true', default=False, help='Contact the youtube-dl server for debugging')
    verbosity.add_option('--no-call-home', dest='call_home', action='store_false', default=False, help='Do NOT contact the youtube-dl server for debugging')
    filesystem = optparse.OptionGroup(parser, 'Filesystem Options')
    filesystem.add_option('-a', '--batch-file', dest='batchfile', metavar='FILE', help=""File containing URLs to download ('-' for stdin), one URL per line. Lines starting with '#', ';' or ']' are considered as comments and ignored."")
    filesystem.add_option('--id', default=False, action='store_true', dest='useid', help='Use only video ID in file name')
    filesystem.add_option('-o', '--output', dest='outtmpl', metavar='TEMPLATE', help='Output filename template, see the ""OUTPUT TEMPLATE"" for all the info')
    filesystem.add_option('--autonumber-size', dest='autonumber_size', metavar='NUMBER', type=int, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('--autonumber-start', dest='autonumber_start', metavar='NUMBER', default=1, type=int, help='Specify the start value for %(autonumber)s (default is %default)')
    filesystem.add_option('--restrict-filenames', action='store_true', dest='restrictfilenames', default=False, help='Restrict filenames to only ASCII characters, and avoid ""&"" and spaces in filenames')
    filesystem.add_option('-A', '--auto-number', action='store_true', dest='autonumber', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-t', '--title', action='store_true', dest='usetitle', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-l', '--literal', default=False, action='store_true', dest='usetitle', help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-w', '--no-overwrites', action='store_true', dest='nooverwrites', default=False, help='Do not overwrite files')
    filesystem.add_option('-c', '--continue', action='store_true', dest='continue_dl', default=True, help='Force resume of partially downloaded files. By default, youtube-dl will resume downloads if possible.')
    filesystem.add_option('--no-continue', action='store_false', dest='continue_dl', help='Do not resume partially downloaded files (restart from beginning)')
    filesystem.add_option('--no-part', action='store_true', dest='nopart', default=False, help='Do not use .part files - write directly into output file')
    filesystem.add_option('--no-mtime', action='store_false', dest='updatetime', default=True, help='Do not use the Last-modified header to set the file modification time')
    filesystem.add_option('--write-description', action='store_true', dest='writedescription', default=False, help='Write video description to a .description file')
    filesystem.add_option('--write-info-json', action='store_true', dest='writeinfojson', default=False, help='Write video metadata to a .info.json file')
    filesystem.add_option('--write-annotations', action='store_true', dest='writeannotations', default=False, help='Write video annotations to a .annotations.xml file')
    filesystem.add_option('--load-info-json', '--load-info', dest='load_info_filename', metavar='FILE', help='JSON file containing the video information (created with the ""--write-info-json"" option)')
    filesystem.add_option('--cookies', dest='cookiefile', metavar='FILE', help='File to read cookies from and dump cookie jar in')
    filesystem.add_option('--cache-dir', dest='cachedir', default=None, metavar='DIR', help='Location in the filesystem where youtube-dl can store some downloaded information permanently. By default $XDG_CACHE_HOME/youtube-dl or ~/.cache/youtube-dl . At the moment, only YouTube player files (for videos with obfuscated signatures) are cached, but that may change.')
    filesystem.add_option('--no-cache-dir', action='store_const', const=False, dest='cachedir', help='Disable filesystem caching')
    filesystem.add_option('--rm-cache-dir', action='store_true', dest='rm_cachedir', help='Delete all filesystem cache files')
    thumbnail = optparse.OptionGroup(parser, 'Thumbnail images')
    thumbnail.add_option('--write-thumbnail', action='store_true', dest='writethumbnail', default=False, help='Write thumbnail image to disk')
    thumbnail.add_option('--write-all-thumbnails', action='store_true', dest='write_all_thumbnails', default=False, help='Write all thumbnail image formats to disk')
    thumbnail.add_option('--list-thumbnails', action='store_true', dest='list_thumbnails', default=False, help='Simulate and list all available thumbnail formats')
    postproc = optparse.OptionGroup(parser, 'Post-processing Options')
    postproc.add_option('-x', '--extract-audio', action='store_true', dest='extractaudio', default=False, help='Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe)')
    postproc.add_option('--audio-format', metavar='FORMAT', dest='audioformat', default='best', help='Specify audio format: ""best"", ""aac"", ""flac"", ""mp3"", ""m4a"", ""opus"", ""vorbis"", or ""wav""; ""%default"" by default; No effect without -x')
    postproc.add_option('--audio-quality', metavar='QUALITY', dest='audioquality', default='5', help='Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default %default)')
    postproc.add_option('--recode-video', metavar='FORMAT', dest='recodevideo', default=None, help='Encode the video to another format if necessary (currently supported: mp4|flv|ogg|webm|mkv|avi)')
    postproc.add_option('--postprocessor-args', dest='postprocessor_args', metavar='ARGS', help='Give these arguments to the postprocessor')
    postproc.add_option('-k', '--keep-video', action='store_true', dest='keepvideo', default=False, help='Keep the video file on disk after the post-processing; the video is erased by default')
    postproc.add_option('--no-post-overwrites', action='store_true', dest='nopostoverwrites', default=False, help='Do not overwrite post-processed files; the post-processed files are overwritten by default')
    postproc.add_option('--embed-subs', action='store_true', dest='embedsubtitles', default=False, help='Embed subtitles in the video (only for mp4, webm and mkv videos)')
    postproc.add_option('--embed-thumbnail', action='store_true', dest='embedthumbnail', default=False, help='Embed thumbnail in the audio as cover art')
    postproc.add_option('--add-metadata', action='store_true', dest='addmetadata', default=False, help='Write metadata to the video file')
    postproc.add_option('--metadata-from-title', metavar='FORMAT', dest='metafromtitle', help='Parse additional metadata like song title / artist from the video title. The format syntax is the same as --output. Regular expression with named capture groups may also be used. The parsed parameters replace existing values. Example: --metadata-from-title ""%(artist)s - %(title)s"" matches a title like ""Coldplay - Paradise"". Example (regex): --metadata-from-title ""(?P<artist>.+?) - (?P<title>.+)""')
    postproc.add_option('--xattrs', action='store_true', dest='xattrs', default=False, help=""Write metadata to the video file's xattrs (using dublin core and xdg standards)"")
    postproc.add_option('--fixup', metavar='POLICY', dest='fixup', default='detect_or_warn', help='Automatically correct known faults of the file. One of never (do nothing), warn (only emit a warning), detect_o","'Use the specified external downloader. Currently supports %s' % ','.join(list_external_downloaders())","['f""Use the specified external downloader. Currently supports {\',\'.join(list_external_downloaders())}""']",no_found,0,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/options.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/options.py,,"def parseOpts(overrideArguments=None):

    def _readOptions(filename_bytes, default=[]):
        try:
            optionf = open(filename_bytes)
        except IOError:
            return default
        try:
            contents = optionf.read()
            if sys.version_info < (3,):
                contents = contents.decode(preferredencoding())
            res = compat_shlex_split(contents, comments=True)
        finally:
            optionf.close()
        return res

    def _readUserConf():
        xdg_config_home = compat_getenv('XDG_CONFIG_HOME')
        if xdg_config_home:
            userConfFile = os.path.join(xdg_config_home, 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(xdg_config_home, 'youtube-dl.conf')
        else:
            userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl.conf')
        userConf = _readOptions(userConfFile, None)
        if userConf is None:
            appdata_dir = compat_getenv('appdata')
            if appdata_dir:
                userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config'), default=None)
                if userConf is None:
                    userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config.txt'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf.txt'), default=None)
        if userConf is None:
            userConf = []
        return userConf

    def _format_option_string(option):
        """""" ('-o', '--option') -> -o, --format METAVAR""""""
        opts = []
        if option._short_opts:
            opts.append(option._short_opts[0])
        if option._long_opts:
            opts.append(option._long_opts[0])
        if len(opts) > 1:
            opts.insert(1, ', ')
        if option.takes_value():
            opts.append(' %s' % option.metavar)
        return ''.join(opts)

    def _comma_separated_values_options_callback(option, opt_str, value, parser):
        setattr(parser.values, option.dest, value.split(','))
    columns = compat_get_terminal_size().columns
    max_width = columns if columns else 80
    max_help_position = 80
    fmt = optparse.IndentedHelpFormatter(width=max_width, max_help_position=max_help_position)
    fmt.format_option_strings = _format_option_string
    kw = {'version': __version__, 'formatter': fmt, 'usage': '%prog [OPTIONS] URL [URL...]', 'conflict_handler': 'resolve'}
    parser = optparse.OptionParser(**compat_kwargs(kw))
    general = optparse.OptionGroup(parser, 'General Options')
    general.add_option('-h', '--help', action='help', help='Print this help text and exit')
    general.add_option('--version', action='version', help='Print program version and exit')
    general.add_option('-U', '--update', action='store_true', dest='update_self', help='Update this program to latest version. Make sure that you have sufficient permissions (run with sudo if needed)')
    general.add_option('-i', '--ignore-errors', action='store_true', dest='ignoreerrors', default=False, help='Continue on download errors, for example to skip unavailable videos in a playlist')
    general.add_option('--abort-on-error', action='store_false', dest='ignoreerrors', help='Abort downloading of further videos (in the playlist or the command line) if an error occurs')
    general.add_option('--dump-user-agent', action='store_true', dest='dump_user_agent', default=False, help='Display the current browser identification')
    general.add_option('--list-extractors', action='store_true', dest='list_extractors', default=False, help='List all supported extractors')
    general.add_option('--extractor-descriptions', action='store_true', dest='list_extractor_descriptions', default=False, help='Output descriptions of all supported extractors')
    general.add_option('--force-generic-extractor', action='store_true', dest='force_generic_extractor', default=False, help='Force extraction to use the generic extractor')
    general.add_option('--default-search', dest='default_search', metavar='PREFIX', help='Use this prefix for unqualified URLs. For example ""gvsearch2:"" downloads two videos from google videos for youtube-dl ""large apple"". Use the value ""auto"" to let youtube-dl guess (""auto_warning"" to emit a warning when guessing). ""error"" just throws an error. The default value ""fixup_error"" repairs broken URLs, but emits an error if this is not possible instead of searching.')
    general.add_option('--ignore-config', action='store_true', help='Do not read configuration files. When given in the global configuration file /etc/youtube-dl.conf: Do not read the user configuration in ~/.config/youtube-dl/config (%APPDATA%/youtube-dl/config.txt on Windows)')
    general.add_option('--config-location', dest='config_location', metavar='PATH', help='Location of the configuration file; either the path to the config or its containing directory.')
    general.add_option('--flat-playlist', action='store_const', dest='extract_flat', const='in_playlist', default=False, help='Do not extract the videos of a playlist, only list them.')
    general.add_option('--mark-watched', action='store_true', dest='mark_watched', default=False, help='Mark videos watched (YouTube only)')
    general.add_option('--no-mark-watched', action='store_false', dest='mark_watched', default=False, help='Do not mark videos watched (YouTube only)')
    general.add_option('--no-color', '--no-colors', action='store_true', dest='no_color', default=False, help='Do not emit color codes in output')
    network = optparse.OptionGroup(parser, 'Network Options')
    network.add_option('--proxy', dest='proxy', default=None, metavar='URL', help='Use the specified HTTP/HTTPS/SOCKS proxy. To enable SOCKS proxy, specify a proper scheme. For example socks5://127.0.0.1:1080/. Pass in an empty string (--proxy """") for direct connection')
    network.add_option('--socket-timeout', dest='socket_timeout', type=float, default=None, metavar='SECONDS', help='Time to wait before giving up, in seconds')
    network.add_option('--source-address', metavar='IP', dest='source_address', default=None, help='Client-side IP address to bind to')
    network.add_option('-4', '--force-ipv4', action='store_const', const='0.0.0.0', dest='source_address', help='Make all connections via IPv4')
    network.add_option('-6', '--force-ipv6', action='store_const', const='::', dest='source_address', help='Make all connections via IPv6')
    geo = optparse.OptionGroup(parser, 'Geo Restriction')
    geo.add_option('--geo-verification-proxy', dest='geo_verification_proxy', default=None, metavar='URL', help='Use this proxy to verify the IP address for some geo-restricted sites. The default proxy specified by --proxy (or none, if the option is not present) is used for the actual downloading.')
    geo.add_option('--cn-verification-proxy', dest='cn_verification_proxy', default=None, metavar='URL', help=optparse.SUPPRESS_HELP)
    geo.add_option('--geo-bypass', action='store_true', dest='geo_bypass', default=True, help='Bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--no-geo-bypass', action='store_false', dest='geo_bypass', default=True, help='Do not bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--geo-bypass-country', metavar='CODE', dest='geo_bypass_country', default=None, help='Force bypass geographic restriction with explicitly provided two-letter ISO 3166-2 country code')
    geo.add_option('--geo-bypass-ip-block', metavar='IP_BLOCK', dest='geo_bypass_ip_block', default=None, help='Force bypass geographic restriction with explicitly provided IP block in CIDR notation')
    selection = optparse.OptionGroup(parser, 'Video Selection')
    selection.add_option('--playlist-start', dest='playliststart', metavar='NUMBER', default=1, type=int, help='Playlist video to start at (default is %default)')
    selection.add_option('--playlist-end', dest='playlistend', metavar='NUMBER', default=None, type=int, help='Playlist video to end at (default is last)')
    selection.add_option('--playlist-items', dest='playlist_items', metavar='ITEM_SPEC', default=None, help='Playlist video items to download. Specify indices of the videos in the playlist separated by commas like: ""--playlist-items 1,2,5,8"" if you want to download videos indexed 1, 2, 5, 8 in the playlist. You can specify range: ""--playlist-items 1-3,7,10-13"", it will download the videos at index 1, 2, 3, 7, 10, 11, 12 and 13.')
    selection.add_option('--match-title', dest='matchtitle', metavar='REGEX', help='Download only matching titles (regex or caseless sub-string)')
    selection.add_option('--reject-title', dest='rejecttitle', metavar='REGEX', help='Skip download for matching titles (regex or caseless sub-string)')
    selection.add_option('--max-downloads', dest='max_downloads', metavar='NUMBER', type=int, default=None, help='Abort after downloading NUMBER files')
    selection.add_option('--min-filesize', metavar='SIZE', dest='min_filesize', default=None, help='Do not download any videos smaller than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--max-filesize', metavar='SIZE', dest='max_filesize', default=None, help='Do not download any videos larger than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--date', metavar='DATE', dest='date', default=None, help='Download only videos uploaded in this date')
    selection.add_option('--datebefore', metavar='DATE', dest='datebefore', default=None, help='Download only videos uploaded on or before this date (i.e. inclusive)')
    selection.add_option('--dateafter', metavar='DATE', dest='dateafter', default=None, help='Download only videos uploaded on or after this date (i.e. inclusive)')
    selection.add_option('--min-views', metavar='COUNT', dest='min_views', default=None, type=int, help='Do not download any videos with less than COUNT views')
    selection.add_option('--max-views', metavar='COUNT', dest='max_views', default=None, type=int, help='Do not download any videos with more than COUNT views')
    selection.add_option('--match-filter', metavar='FILTER', dest='match_filter', default=None, help='Generic video filter. Specify any key (see the ""OUTPUT TEMPLATE"" for a list of available keys) to match if the key is present, !key to check if the key is not present, key > NUMBER (like ""comment_count > 12"", also works with >=, <, <=, !=, =) to compare against a number, key = \'LITERAL\' (like ""uploader = \'Mike Smith\'"", also works with !=) to match against a string literal and & to require multiple matches. Values which are not known are excluded unless you put a question mark (?) after the operator. For example, to only match videos that have been liked more than 100 times and disliked less than 50 times (or the dislike functionality is not available at the given service), but who also have a description, use --match-filter ""like_count > 100 & dislike_count <? 50 & description"" .')
    selection.add_option('--no-playlist', action='store_true', dest='noplaylist', default=False, help='Download only the video, if the URL refers to a video and a playlist.')
    selection.add_option('--yes-playlist', action='store_false', dest='noplaylist', default=False, help='Download the playlist, if the URL refers to a video and a playlist.')
    selection.add_option('--age-limit', metavar='YEARS', dest='age_limit', default=None, type=int, help='Download only videos suitable for the given age')
    selection.add_option('--download-archive', metavar='FILE', dest='download_archive', help='Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it.')
    selection.add_option('--include-ads', dest='include_ads', action='store_true', help='Download advertisements as well (experimental)')
    authentication = optparse.OptionGroup(parser, 'Authentication Options')
    authentication.add_option('-u', '--username', dest='username', metavar='USERNAME', help='Login with this account ID')
    authentication.add_option('-p', '--password', dest='password', metavar='PASSWORD', help='Account password. If this option is left out, youtube-dl will ask interactively.')
    authentication.add_option('-2', '--twofactor', dest='twofactor', metavar='TWOFACTOR', help='Two-factor authentication code')
    authentication.add_option('-n', '--netrc', action='store_true', dest='usenetrc', default=False, help='Use .netrc authentication data')
    authentication.add_option('--video-password', dest='videopassword', metavar='PASSWORD', help='Video password (vimeo, smotri, youku)')
    adobe_pass = optparse.OptionGroup(parser, 'Adobe Pass Options')
    adobe_pass.add_option('--ap-mso', dest='ap_mso', metavar='MSO', help='Adobe Pass multiple-system operator (TV provider) identifier, use --ap-list-mso for a list of available MSOs')
    adobe_pass.add_option('--ap-username', dest='ap_username', metavar='USERNAME', help='Multiple-system operator account login')
    adobe_pass.add_option('--ap-password', dest='ap_password', metavar='PASSWORD', help='Multiple-system operator account password. If this option is left out, youtube-dl will ask interactively.')
    adobe_pass.add_option('--ap-list-mso', action='store_true', dest='ap_list_mso', default=False, help='List all supported multiple-system operators')
    video_format = optparse.OptionGroup(parser, 'Video Format Options')
    video_format.add_option('-f', '--format', action='store', dest='format', metavar='FORMAT', default=None, help='Video format code, see the ""FORMAT SELECTION"" for all the info')
    video_format.add_option('--all-formats', action='store_const', dest='format', const='all', help='Download all available video formats')
    video_format.add_option('--prefer-free-formats', action='store_true', dest='prefer_free_formats', default=False, help='Prefer free video formats unless a specific one is requested')
    video_format.add_option('-F', '--list-formats', action='store_true', dest='listformats', help='List all available formats of requested videos')
    video_format.add_option('--youtube-include-dash-manifest', action='store_true', dest='youtube_include_dash_manifest', default=True, help=optparse.SUPPRESS_HELP)
    video_format.add_option('--youtube-skip-dash-manifest', action='store_false', dest='youtube_include_dash_manifest', help='Do not download the DASH manifests and related data on YouTube videos')
    video_format.add_option('--merge-output-format', action='store', dest='merge_output_format', metavar='FORMAT', default=None, help='If a merge is required (e.g. bestvideo+bestaudio), output to given container format. One of mkv, mp4, ogg, webm, flv. Ignored if no merge is required')
    subtitles = optparse.OptionGroup(parser, 'Subtitle Options')
    subtitles.add_option('--write-sub', '--write-srt', action='store_true', dest='writesubtitles', default=False, help='Write subtitle file')
    subtitles.add_option('--write-auto-sub', '--write-automatic-sub', action='store_true', dest='writeautomaticsub', default=False, help='Write automatically generated subtitle file (YouTube only)')
    subtitles.add_option('--all-subs', action='store_true', dest='allsubtitles', default=False, help='Download all the available subtitles of the video')
    subtitles.add_option('--list-subs', action='store_true', dest='listsubtitles', default=False, help='List all available subtitles for the video')
    subtitles.add_option('--sub-format', action='store', dest='subtitlesformat', metavar='FORMAT', default='best', help='Subtitle format, accepts formats preference, for example: ""srt"" or ""ass/srt/best""')
    subtitles.add_option('--sub-lang', '--sub-langs', '--srt-lang', action='callback', dest='subtitleslangs', metavar='LANGS', type='str', default=[], callback=_comma_separated_values_options_callback, help='Languages of the subtitles to download (optional) separated by commas, use --list-subs for available language tags')
    downloader = optparse.OptionGroup(parser, 'Download Options')
    downloader.add_option('-r', '--limit-rate', '--rate-limit', dest='ratelimit', metavar='RATE', help='Maximum download rate in bytes per second (e.g. 50K or 4.2M)')
    downloader.add_option('-R', '--retries', dest='retries', metavar='RETRIES', default=10, help='Number of retries (default is %default), or ""infinite"".')
    downloader.add_option('--fragment-retries', dest='fragment_retries', metavar='RETRIES', default=10, help='Number of retries for a fragment (default is %default), or ""infinite"" (DASH, hlsnative and ISM)')
    downloader.add_option('--skip-unavailable-fragments', action='store_true', dest='skip_unavailable_fragments', default=True, help='Skip unavailable fragments (DASH, hlsnative and ISM)')
    downloader.add_option('--abort-on-unavailable-fragment', action='store_false', dest='skip_unavailable_fragments', help='Abort downloading when some fragment is not available')
    downloader.add_option('--keep-fragments', action='store_true', dest='keep_fragments', default=False, help='Keep downloaded fragments on disk after downloading is finished; fragments are erased by default')
    downloader.add_option('--buffer-size', dest='buffersize', metavar='SIZE', default='1024', help='Size of download buffer (e.g. 1024 or 16K) (default is %default)')
    downloader.add_option('--no-resize-buffer', action='store_true', dest='noresizebuffer', default=False, help='Do not automatically adjust the buffer size. By default, the buffer size is automatically resized from an initial value of SIZE.')
    downloader.add_option('--http-chunk-size', dest='http_chunk_size', metavar='SIZE', default=None, help='Size of a chunk for chunk-based HTTP downloading (e.g. 10485760 or 10M) (default is disabled). May be useful for bypassing bandwidth throttling imposed by a webserver (experimental)')
    downloader.add_option('--test', action='store_true', dest='test', default=False, help=optparse.SUPPRESS_HELP)
    downloader.add_option('--playlist-reverse', action='store_true', help='Download playlist videos in reverse order')
    downloader.add_option('--playlist-random', action='store_true', help='Download playlist videos in random order')
    downloader.add_option('--xattr-set-filesize', dest='xattr_set_filesize', action='store_true', help='Set file xattribute ytdl.filesize with expected file size')
    downloader.add_option('--hls-prefer-native', dest='hls_prefer_native', action='store_true', default=None, help='Use the native HLS downloader instead of ffmpeg')
    downloader.add_option('--hls-prefer-ffmpeg', dest='hls_prefer_native', action='store_false', default=None, help='Use ffmpeg instead of the native HLS downloader')
    downloader.add_option('--hls-use-mpegts', dest='hls_use_mpegts', action='store_true', help='Use the mpegts container for HLS videos, allowing to play the video while downloading (some players may not be able to play it)')
    downloader.add_option('--external-downloader', dest='external_downloader', metavar='COMMAND', help='Use the specified external downloader. Currently supports %s' % ','.join(list_external_downloaders()))
    downloader.add_option('--external-downloader-args', dest='external_downloader_args', metavar='ARGS', help='Give these arguments to the external downloader')
    workarounds = optparse.OptionGroup(parser, 'Workarounds')
    workarounds.add_option('--encoding', dest='encoding', metavar='ENCODING', help='Force the specified encoding (experimental)')
    workarounds.add_option('--no-check-certificate', action='store_true', dest='no_check_certificate', default=False, help='Suppress HTTPS certificate validation')
    workarounds.add_option('--prefer-insecure', '--prefer-unsecure', action='store_true', dest='prefer_insecure', help='Use an unencrypted connection to retrieve information about the video. (Currently supported only for YouTube)')
    workarounds.add_option('--user-agent', metavar='UA', dest='user_agent', help='Specify a custom user agent')
    workarounds.add_option('--referer', metavar='URL', dest='referer', default=None, help='Specify a custom referer, use if the video access is restricted to one domain')
    workarounds.add_option('--add-header', metavar='FIELD:VALUE', dest='headers', action='append', help=""Specify a custom HTTP header and its value, separated by a colon ':'. You can use this option multiple times"")
    workarounds.add_option('--bidi-workaround', dest='bidi_workaround', action='store_true', help='Work around terminals that lack bidirectional text support. Requires bidiv or fribidi executable in PATH')
    workarounds.add_option('--sleep-interval', '--min-sleep-interval', metavar='SECONDS', dest='sleep_interval', type=float, help='Number of seconds to sleep before each download when used alone or a lower bound of a range for randomized sleep before each download (minimum possible number of seconds to sleep) when used along with --max-sleep-interval.')
    workarounds.add_option('--max-sleep-interval', metavar='SECONDS', dest='max_sleep_interval', type=float, help='Upper bound of a range for randomized sleep before each download (maximum possible number of seconds to sleep). Must only be used along with --min-sleep-interval.')
    verbosity = optparse.OptionGroup(parser, 'Verbosity / Simulation Options')
    verbosity.add_option('-q', '--quiet', action='store_true', dest='quiet', default=False, help='Activate quiet mode')
    verbosity.add_option('--no-warnings', dest='no_warnings', action='store_true', default=False, help='Ignore warnings')
    verbosity.add_option('-s', '--simulate', action='store_true', dest='simulate', default=False, help='Do not download the video and do not write anything to disk')
    verbosity.add_option('--skip-download', action='store_true', dest='skip_download', default=False, help='Do not download the video')
    verbosity.add_option('-g', '--get-url', action='store_true', dest='geturl', default=False, help='Simulate, quiet but print URL')
    verbosity.add_option('-e', '--get-title', action='store_true', dest='gettitle', default=False, help='Simulate, quiet but print title')
    verbosity.add_option('--get-id', action='store_true', dest='getid', default=False, help='Simulate, quiet but print id')
    verbosity.add_option('--get-thumbnail', action='store_true', dest='getthumbnail', default=False, help='Simulate, quiet but print thumbnail URL')
    verbosity.add_option('--get-description', action='store_true', dest='getdescription', default=False, help='Simulate, quiet but print video description')
    verbosity.add_option('--get-duration', action='store_true', dest='getduration', default=False, help='Simulate, quiet but print video length')
    verbosity.add_option('--get-filename', action='store_true', dest='getfilename', default=False, help='Simulate, quiet but print output filename')
    verbosity.add_option('--get-format', action='store_true', dest='getformat', default=False, help='Simulate, quiet but print output format')
    verbosity.add_option('-j', '--dump-json', action='store_true', dest='dumpjson', default=False, help='Simulate, quiet but print JSON information. See the ""OUTPUT TEMPLATE"" for a description of available keys.')
    verbosity.add_option('-J', '--dump-single-json', action='store_true', dest='dump_single_json', default=False, help='Simulate, quiet but print JSON information for each command-line argument. If the URL refers to a playlist, dump the whole playlist information in a single line.')
    verbosity.add_option('--print-json', action='store_true', dest='print_json', default=False, help='Be quiet and print the video information as JSON (video is still being downloaded).')
    verbosity.add_option('--newline', action='store_true', dest='progress_with_newline', default=False, help='Output progress bar as new lines')
    verbosity.add_option('--no-progress', action='store_true', dest='noprogress', default=False, help='Do not print progress bar')
    verbosity.add_option('--console-title', action='store_true', dest='consoletitle', default=False, help='Display progress in console titlebar')
    verbosity.add_option('-v', '--verbose', action='store_true', dest='verbose', default=False, help='Print various debugging information')
    verbosity.add_option('--dump-pages', '--dump-intermediate-pages', action='store_true', dest='dump_intermediate_pages', default=False, help='Print downloaded pages encoded using base64 to debug problems (very verbose)')
    verbosity.add_option('--write-pages', action='store_true', dest='write_pages', default=False, help='Write downloaded intermediary pages to files in the current directory to debug problems')
    verbosity.add_option('--youtube-print-sig-code', action='store_true', dest='youtube_print_sig_code', default=False, help=optparse.SUPPRESS_HELP)
    verbosity.add_option('--print-traffic', '--dump-headers', dest='debug_printtraffic', action='store_true', default=False, help='Display sent and read HTTP traffic')
    verbosity.add_option('-C', '--call-home', dest='call_home', action='store_true', default=False, help='Contact the youtube-dl server for debugging')
    verbosity.add_option('--no-call-home', dest='call_home', action='store_false', default=False, help='Do NOT contact the youtube-dl server for debugging')
    filesystem = optparse.OptionGroup(parser, 'Filesystem Options')
    filesystem.add_option('-a', '--batch-file', dest='batchfile', metavar='FILE', help=""File containing URLs to download ('-' for stdin), one URL per line. Lines starting with '#', ';' or ']' are considered as comments and ignored."")
    filesystem.add_option('--id', default=False, action='store_true', dest='useid', help='Use only video ID in file name')
    filesystem.add_option('-o', '--output', dest='outtmpl', metavar='TEMPLATE', help='Output filename template, see the ""OUTPUT TEMPLATE"" for all the info')
    filesystem.add_option('--autonumber-size', dest='autonumber_size', metavar='NUMBER', type=int, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('--autonumber-start', dest='autonumber_start', metavar='NUMBER', default=1, type=int, help='Specify the start value for %(autonumber)s (default is %default)')
    filesystem.add_option('--restrict-filenames', action='store_true', dest='restrictfilenames', default=False, help='Restrict filenames to only ASCII characters, and avoid ""&"" and spaces in filenames')
    filesystem.add_option('-A', '--auto-number', action='store_true', dest='autonumber', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-t', '--title', action='store_true', dest='usetitle', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-l', '--literal', default=False, action='store_true', dest='usetitle', help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-w', '--no-overwrites', action='store_true', dest='nooverwrites', default=False, help='Do not overwrite files')
    filesystem.add_option('-c', '--continue', action='store_true', dest='continue_dl', default=True, help='Force resume of partially downloaded files. By default, youtube-dl will resume downloads if possible.')
    filesystem.add_option('--no-continue', action='store_false', dest='continue_dl', help='Do not resume partially downloaded files (restart from beginning)')
    filesystem.add_option('--no-part', action='store_true', dest='nopart', default=False, help='Do not use .part files - write directly into output file')
    filesystem.add_option('--no-mtime', action='store_false', dest='updatetime', default=True, help='Do not use the Last-modified header to set the file modification time')
    filesystem.add_option('--write-description', action='store_true', dest='writedescription', default=False, help='Write video description to a .description file')
    filesystem.add_option('--write-info-json', action='store_true', dest='writeinfojson', default=False, help='Write video metadata to a .info.json file')
    filesystem.add_option('--write-annotations', action='store_true', dest='writeannotations', default=False, help='Write video annotations to a .annotations.xml file')
    filesystem.add_option('--load-info-json', '--load-info', dest='load_info_filename', metavar='FILE', help='JSON file containing the video information (created with the ""--write-info-json"" option)')
    filesystem.add_option('--cookies', dest='cookiefile', metavar='FILE', help='File to read cookies from and dump cookie jar in')
    filesystem.add_option('--cache-dir', dest='cachedir', default=None, metavar='DIR', help='Location in the filesystem where youtube-dl can store some downloaded information permanently. By default $XDG_CACHE_HOME/youtube-dl or ~/.cache/youtube-dl . At the moment, only YouTube player files (for videos with obfuscated signatures) are cached, but that may change.')
    filesystem.add_option('--no-cache-dir', action='store_const', const=False, dest='cachedir', help='Disable filesystem caching')
    filesystem.add_option('--rm-cache-dir', action='store_true', dest='rm_cachedir', help='Delete all filesystem cache files')
    thumbnail = optparse.OptionGroup(parser, 'Thumbnail images')
    thumbnail.add_option('--write-thumbnail', action='store_true', dest='writethumbnail', default=False, help='Write thumbnail image to disk')
    thumbnail.add_option('--write-all-thumbnails', action='store_true', dest='write_all_thumbnails', default=False, help='Write all thumbnail image formats to disk')
    thumbnail.add_option('--list-thumbnails', action='store_true', dest='list_thumbnails', default=False, help='Simulate and list all available thumbnail formats')
    postproc = optparse.OptionGroup(parser, 'Post-processing Options')
    postproc.add_option('-x', '--extract-audio', action='store_true', dest='extractaudio', default=False, help='Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe)')
    postproc.add_option('--audio-format', metavar='FORMAT', dest='audioformat', default='best', help='Specify audio format: ""best"", ""aac"", ""flac"", ""mp3"", ""m4a"", ""opus"", ""vorbis"", or ""wav""; ""%default"" by default; No effect without -x')
    postproc.add_option('--audio-quality', metavar='QUALITY', dest='audioquality', default='5', help='Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default %default)')
    postproc.add_option('--recode-video', metavar='FORMAT', dest='recodevideo', default=None, help='Encode the video to another format if necessary (currently supported: mp4|flv|ogg|webm|mkv|avi)')
    postproc.add_option('--postprocessor-args', dest='postprocessor_args', metavar='ARGS', help='Give these arguments to the postprocessor')
    postproc.add_option('-k', '--keep-video', action='store_true', dest='keepvideo', default=False, help='Keep the video file on disk after the post-processing; the video is erased by default')
    postproc.add_option('--no-post-overwrites', action='store_true', dest='nopostoverwrites', default=False, help='Do not overwrite post-processed files; the post-processed files are overwritten by default')
    postproc.add_option('--embed-subs', action='store_true', dest='embedsubtitles', default=False, help='Embed subtitles in the video (only for mp4, webm and mkv videos)')
    postproc.add_option('--embed-thumbnail', action='store_true', dest='embedthumbnail', default=False, help='Embed thumbnail in the audio as cover art')
    postproc.add_option('--add-metadata', action='store_true', dest='addmetadata', default=False, help='Write metadata to the video file')
    postproc.add_option('--metadata-from-title', metavar='FORMAT', dest='metafromtitle', help='Parse additional metadata like song title / artist from the video title. The format syntax is the same as --output. Regular expression with named capture groups may also be used. The parsed parameters replace existing values. Example: --metadata-from-title ""%(artist)s - %(title)s"" matches a title like ""Coldplay - Paradise"". Example (regex): --metadata-from-title ""(?P<artist>.+?) - (?P<title>.+)""')
    postproc.add_option('--xattrs', action='store_true', dest='xattrs', default=False, help=""Write metadata to the video file's xattrs (using dublin core and xdg standards)"")
    postproc.add_option('--fixup', metavar='POLICY', dest='fixup', default='detect_or_warn', help='Automatically correct known faults of the file. One of never (do nothing), warn (only emit a warning), detect_o",' %s' % option.metavar,["f'{option.metavar}'"],no_found,0,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/options.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/options.py,,"def parseOpts(overrideArguments=None):

    def _readOptions(filename_bytes, default=[]):
        try:
            optionf = open(filename_bytes)
        except IOError:
            return default
        try:
            contents = optionf.read()
            if sys.version_info < (3,):
                contents = contents.decode(preferredencoding())
            res = compat_shlex_split(contents, comments=True)
        finally:
            optionf.close()
        return res

    def _readUserConf():
        xdg_config_home = compat_getenv('XDG_CONFIG_HOME')
        if xdg_config_home:
            userConfFile = os.path.join(xdg_config_home, 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(xdg_config_home, 'youtube-dl.conf')
        else:
            userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl.conf')
        userConf = _readOptions(userConfFile, None)
        if userConf is None:
            appdata_dir = compat_getenv('appdata')
            if appdata_dir:
                userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config'), default=None)
                if userConf is None:
                    userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config.txt'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf.txt'), default=None)
        if userConf is None:
            userConf = []
        return userConf

    def _format_option_string(option):
        """""" ('-o', '--option') -> -o, --format METAVAR""""""
        opts = []
        if option._short_opts:
            opts.append(option._short_opts[0])
        if option._long_opts:
            opts.append(option._long_opts[0])
        if len(opts) > 1:
            opts.insert(1, ', ')
        if option.takes_value():
            opts.append(' %s' % option.metavar)
        return ''.join(opts)

    def _comma_separated_values_options_callback(option, opt_str, value, parser):
        setattr(parser.values, option.dest, value.split(','))
    columns = compat_get_terminal_size().columns
    max_width = columns if columns else 80
    max_help_position = 80
    fmt = optparse.IndentedHelpFormatter(width=max_width, max_help_position=max_help_position)
    fmt.format_option_strings = _format_option_string
    kw = {'version': __version__, 'formatter': fmt, 'usage': '%prog [OPTIONS] URL [URL...]', 'conflict_handler': 'resolve'}
    parser = optparse.OptionParser(**compat_kwargs(kw))
    general = optparse.OptionGroup(parser, 'General Options')
    general.add_option('-h', '--help', action='help', help='Print this help text and exit')
    general.add_option('--version', action='version', help='Print program version and exit')
    general.add_option('-U', '--update', action='store_true', dest='update_self', help='Update this program to latest version. Make sure that you have sufficient permissions (run with sudo if needed)')
    general.add_option('-i', '--ignore-errors', action='store_true', dest='ignoreerrors', default=False, help='Continue on download errors, for example to skip unavailable videos in a playlist')
    general.add_option('--abort-on-error', action='store_false', dest='ignoreerrors', help='Abort downloading of further videos (in the playlist or the command line) if an error occurs')
    general.add_option('--dump-user-agent', action='store_true', dest='dump_user_agent', default=False, help='Display the current browser identification')
    general.add_option('--list-extractors', action='store_true', dest='list_extractors', default=False, help='List all supported extractors')
    general.add_option('--extractor-descriptions', action='store_true', dest='list_extractor_descriptions', default=False, help='Output descriptions of all supported extractors')
    general.add_option('--force-generic-extractor', action='store_true', dest='force_generic_extractor', default=False, help='Force extraction to use the generic extractor')
    general.add_option('--default-search', dest='default_search', metavar='PREFIX', help='Use this prefix for unqualified URLs. For example ""gvsearch2:"" downloads two videos from google videos for youtube-dl ""large apple"". Use the value ""auto"" to let youtube-dl guess (""auto_warning"" to emit a warning when guessing). ""error"" just throws an error. The default value ""fixup_error"" repairs broken URLs, but emits an error if this is not possible instead of searching.')
    general.add_option('--ignore-config', action='store_true', help='Do not read configuration files. When given in the global configuration file /etc/youtube-dl.conf: Do not read the user configuration in ~/.config/youtube-dl/config (%APPDATA%/youtube-dl/config.txt on Windows)')
    general.add_option('--config-location', dest='config_location', metavar='PATH', help='Location of the configuration file; either the path to the config or its containing directory.')
    general.add_option('--flat-playlist', action='store_const', dest='extract_flat', const='in_playlist', default=False, help='Do not extract the videos of a playlist, only list them.')
    general.add_option('--mark-watched', action='store_true', dest='mark_watched', default=False, help='Mark videos watched (YouTube only)')
    general.add_option('--no-mark-watched', action='store_false', dest='mark_watched', default=False, help='Do not mark videos watched (YouTube only)')
    general.add_option('--no-color', '--no-colors', action='store_true', dest='no_color', default=False, help='Do not emit color codes in output')
    network = optparse.OptionGroup(parser, 'Network Options')
    network.add_option('--proxy', dest='proxy', default=None, metavar='URL', help='Use the specified HTTP/HTTPS/SOCKS proxy. To enable SOCKS proxy, specify a proper scheme. For example socks5://127.0.0.1:1080/. Pass in an empty string (--proxy """") for direct connection')
    network.add_option('--socket-timeout', dest='socket_timeout', type=float, default=None, metavar='SECONDS', help='Time to wait before giving up, in seconds')
    network.add_option('--source-address', metavar='IP', dest='source_address', default=None, help='Client-side IP address to bind to')
    network.add_option('-4', '--force-ipv4', action='store_const', const='0.0.0.0', dest='source_address', help='Make all connections via IPv4')
    network.add_option('-6', '--force-ipv6', action='store_const', const='::', dest='source_address', help='Make all connections via IPv6')
    geo = optparse.OptionGroup(parser, 'Geo Restriction')
    geo.add_option('--geo-verification-proxy', dest='geo_verification_proxy', default=None, metavar='URL', help='Use this proxy to verify the IP address for some geo-restricted sites. The default proxy specified by --proxy (or none, if the option is not present) is used for the actual downloading.')
    geo.add_option('--cn-verification-proxy', dest='cn_verification_proxy', default=None, metavar='URL', help=optparse.SUPPRESS_HELP)
    geo.add_option('--geo-bypass', action='store_true', dest='geo_bypass', default=True, help='Bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--no-geo-bypass', action='store_false', dest='geo_bypass', default=True, help='Do not bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--geo-bypass-country', metavar='CODE', dest='geo_bypass_country', default=None, help='Force bypass geographic restriction with explicitly provided two-letter ISO 3166-2 country code')
    geo.add_option('--geo-bypass-ip-block', metavar='IP_BLOCK', dest='geo_bypass_ip_block', default=None, help='Force bypass geographic restriction with explicitly provided IP block in CIDR notation')
    selection = optparse.OptionGroup(parser, 'Video Selection')
    selection.add_option('--playlist-start', dest='playliststart', metavar='NUMBER', default=1, type=int, help='Playlist video to start at (default is %default)')
    selection.add_option('--playlist-end', dest='playlistend', metavar='NUMBER', default=None, type=int, help='Playlist video to end at (default is last)')
    selection.add_option('--playlist-items', dest='playlist_items', metavar='ITEM_SPEC', default=None, help='Playlist video items to download. Specify indices of the videos in the playlist separated by commas like: ""--playlist-items 1,2,5,8"" if you want to download videos indexed 1, 2, 5, 8 in the playlist. You can specify range: ""--playlist-items 1-3,7,10-13"", it will download the videos at index 1, 2, 3, 7, 10, 11, 12 and 13.')
    selection.add_option('--match-title', dest='matchtitle', metavar='REGEX', help='Download only matching titles (regex or caseless sub-string)')
    selection.add_option('--reject-title', dest='rejecttitle', metavar='REGEX', help='Skip download for matching titles (regex or caseless sub-string)')
    selection.add_option('--max-downloads', dest='max_downloads', metavar='NUMBER', type=int, default=None, help='Abort after downloading NUMBER files')
    selection.add_option('--min-filesize', metavar='SIZE', dest='min_filesize', default=None, help='Do not download any videos smaller than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--max-filesize', metavar='SIZE', dest='max_filesize', default=None, help='Do not download any videos larger than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--date', metavar='DATE', dest='date', default=None, help='Download only videos uploaded in this date')
    selection.add_option('--datebefore', metavar='DATE', dest='datebefore', default=None, help='Download only videos uploaded on or before this date (i.e. inclusive)')
    selection.add_option('--dateafter', metavar='DATE', dest='dateafter', default=None, help='Download only videos uploaded on or after this date (i.e. inclusive)')
    selection.add_option('--min-views', metavar='COUNT', dest='min_views', default=None, type=int, help='Do not download any videos with less than COUNT views')
    selection.add_option('--max-views', metavar='COUNT', dest='max_views', default=None, type=int, help='Do not download any videos with more than COUNT views')
    selection.add_option('--match-filter', metavar='FILTER', dest='match_filter', default=None, help='Generic video filter. Specify any key (see the ""OUTPUT TEMPLATE"" for a list of available keys) to match if the key is present, !key to check if the key is not present, key > NUMBER (like ""comment_count > 12"", also works with >=, <, <=, !=, =) to compare against a number, key = \'LITERAL\' (like ""uploader = \'Mike Smith\'"", also works with !=) to match against a string literal and & to require multiple matches. Values which are not known are excluded unless you put a question mark (?) after the operator. For example, to only match videos that have been liked more than 100 times and disliked less than 50 times (or the dislike functionality is not available at the given service), but who also have a description, use --match-filter ""like_count > 100 & dislike_count <? 50 & description"" .')
    selection.add_option('--no-playlist', action='store_true', dest='noplaylist', default=False, help='Download only the video, if the URL refers to a video and a playlist.')
    selection.add_option('--yes-playlist', action='store_false', dest='noplaylist', default=False, help='Download the playlist, if the URL refers to a video and a playlist.')
    selection.add_option('--age-limit', metavar='YEARS', dest='age_limit', default=None, type=int, help='Download only videos suitable for the given age')
    selection.add_option('--download-archive', metavar='FILE', dest='download_archive', help='Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it.')
    selection.add_option('--include-ads', dest='include_ads', action='store_true', help='Download advertisements as well (experimental)')
    authentication = optparse.OptionGroup(parser, 'Authentication Options')
    authentication.add_option('-u', '--username', dest='username', metavar='USERNAME', help='Login with this account ID')
    authentication.add_option('-p', '--password', dest='password', metavar='PASSWORD', help='Account password. If this option is left out, youtube-dl will ask interactively.')
    authentication.add_option('-2', '--twofactor', dest='twofactor', metavar='TWOFACTOR', help='Two-factor authentication code')
    authentication.add_option('-n', '--netrc', action='store_true', dest='usenetrc', default=False, help='Use .netrc authentication data')
    authentication.add_option('--video-password', dest='videopassword', metavar='PASSWORD', help='Video password (vimeo, smotri, youku)')
    adobe_pass = optparse.OptionGroup(parser, 'Adobe Pass Options')
    adobe_pass.add_option('--ap-mso', dest='ap_mso', metavar='MSO', help='Adobe Pass multiple-system operator (TV provider) identifier, use --ap-list-mso for a list of available MSOs')
    adobe_pass.add_option('--ap-username', dest='ap_username', metavar='USERNAME', help='Multiple-system operator account login')
    adobe_pass.add_option('--ap-password', dest='ap_password', metavar='PASSWORD', help='Multiple-system operator account password. If this option is left out, youtube-dl will ask interactively.')
    adobe_pass.add_option('--ap-list-mso', action='store_true', dest='ap_list_mso', default=False, help='List all supported multiple-system operators')
    video_format = optparse.OptionGroup(parser, 'Video Format Options')
    video_format.add_option('-f', '--format', action='store', dest='format', metavar='FORMAT', default=None, help='Video format code, see the ""FORMAT SELECTION"" for all the info')
    video_format.add_option('--all-formats', action='store_const', dest='format', const='all', help='Download all available video formats')
    video_format.add_option('--prefer-free-formats', action='store_true', dest='prefer_free_formats', default=False, help='Prefer free video formats unless a specific one is requested')
    video_format.add_option('-F', '--list-formats', action='store_true', dest='listformats', help='List all available formats of requested videos')
    video_format.add_option('--youtube-include-dash-manifest', action='store_true', dest='youtube_include_dash_manifest', default=True, help=optparse.SUPPRESS_HELP)
    video_format.add_option('--youtube-skip-dash-manifest', action='store_false', dest='youtube_include_dash_manifest', help='Do not download the DASH manifests and related data on YouTube videos')
    video_format.add_option('--merge-output-format', action='store', dest='merge_output_format', metavar='FORMAT', default=None, help='If a merge is required (e.g. bestvideo+bestaudio), output to given container format. One of mkv, mp4, ogg, webm, flv. Ignored if no merge is required')
    subtitles = optparse.OptionGroup(parser, 'Subtitle Options')
    subtitles.add_option('--write-sub', '--write-srt', action='store_true', dest='writesubtitles', default=False, help='Write subtitle file')
    subtitles.add_option('--write-auto-sub', '--write-automatic-sub', action='store_true', dest='writeautomaticsub', default=False, help='Write automatically generated subtitle file (YouTube only)')
    subtitles.add_option('--all-subs', action='store_true', dest='allsubtitles', default=False, help='Download all the available subtitles of the video')
    subtitles.add_option('--list-subs', action='store_true', dest='listsubtitles', default=False, help='List all available subtitles for the video')
    subtitles.add_option('--sub-format', action='store', dest='subtitlesformat', metavar='FORMAT', default='best', help='Subtitle format, accepts formats preference, for example: ""srt"" or ""ass/srt/best""')
    subtitles.add_option('--sub-lang', '--sub-langs', '--srt-lang', action='callback', dest='subtitleslangs', metavar='LANGS', type='str', default=[], callback=_comma_separated_values_options_callback, help='Languages of the subtitles to download (optional) separated by commas, use --list-subs for available language tags')
    downloader = optparse.OptionGroup(parser, 'Download Options')
    downloader.add_option('-r', '--limit-rate', '--rate-limit', dest='ratelimit', metavar='RATE', help='Maximum download rate in bytes per second (e.g. 50K or 4.2M)')
    downloader.add_option('-R', '--retries', dest='retries', metavar='RETRIES', default=10, help='Number of retries (default is %default), or ""infinite"".')
    downloader.add_option('--fragment-retries', dest='fragment_retries', metavar='RETRIES', default=10, help='Number of retries for a fragment (default is %default), or ""infinite"" (DASH, hlsnative and ISM)')
    downloader.add_option('--skip-unavailable-fragments', action='store_true', dest='skip_unavailable_fragments', default=True, help='Skip unavailable fragments (DASH, hlsnative and ISM)')
    downloader.add_option('--abort-on-unavailable-fragment', action='store_false', dest='skip_unavailable_fragments', help='Abort downloading when some fragment is not available')
    downloader.add_option('--keep-fragments', action='store_true', dest='keep_fragments', default=False, help='Keep downloaded fragments on disk after downloading is finished; fragments are erased by default')
    downloader.add_option('--buffer-size', dest='buffersize', metavar='SIZE', default='1024', help='Size of download buffer (e.g. 1024 or 16K) (default is %default)')
    downloader.add_option('--no-resize-buffer', action='store_true', dest='noresizebuffer', default=False, help='Do not automatically adjust the buffer size. By default, the buffer size is automatically resized from an initial value of SIZE.')
    downloader.add_option('--http-chunk-size', dest='http_chunk_size', metavar='SIZE', default=None, help='Size of a chunk for chunk-based HTTP downloading (e.g. 10485760 or 10M) (default is disabled). May be useful for bypassing bandwidth throttling imposed by a webserver (experimental)')
    downloader.add_option('--test', action='store_true', dest='test', default=False, help=optparse.SUPPRESS_HELP)
    downloader.add_option('--playlist-reverse', action='store_true', help='Download playlist videos in reverse order')
    downloader.add_option('--playlist-random', action='store_true', help='Download playlist videos in random order')
    downloader.add_option('--xattr-set-filesize', dest='xattr_set_filesize', action='store_true', help='Set file xattribute ytdl.filesize with expected file size')
    downloader.add_option('--hls-prefer-native', dest='hls_prefer_native', action='store_true', default=None, help='Use the native HLS downloader instead of ffmpeg')
    downloader.add_option('--hls-prefer-ffmpeg', dest='hls_prefer_native', action='store_false', default=None, help='Use ffmpeg instead of the native HLS downloader')
    downloader.add_option('--hls-use-mpegts', dest='hls_use_mpegts', action='store_true', help='Use the mpegts container for HLS videos, allowing to play the video while downloading (some players may not be able to play it)')
    downloader.add_option('--external-downloader', dest='external_downloader', metavar='COMMAND', help='Use the specified external downloader. Currently supports %s' % ','.join(list_external_downloaders()))
    downloader.add_option('--external-downloader-args', dest='external_downloader_args', metavar='ARGS', help='Give these arguments to the external downloader')
    workarounds = optparse.OptionGroup(parser, 'Workarounds')
    workarounds.add_option('--encoding', dest='encoding', metavar='ENCODING', help='Force the specified encoding (experimental)')
    workarounds.add_option('--no-check-certificate', action='store_true', dest='no_check_certificate', default=False, help='Suppress HTTPS certificate validation')
    workarounds.add_option('--prefer-insecure', '--prefer-unsecure', action='store_true', dest='prefer_insecure', help='Use an unencrypted connection to retrieve information about the video. (Currently supported only for YouTube)')
    workarounds.add_option('--user-agent', metavar='UA', dest='user_agent', help='Specify a custom user agent')
    workarounds.add_option('--referer', metavar='URL', dest='referer', default=None, help='Specify a custom referer, use if the video access is restricted to one domain')
    workarounds.add_option('--add-header', metavar='FIELD:VALUE', dest='headers', action='append', help=""Specify a custom HTTP header and its value, separated by a colon ':'. You can use this option multiple times"")
    workarounds.add_option('--bidi-workaround', dest='bidi_workaround', action='store_true', help='Work around terminals that lack bidirectional text support. Requires bidiv or fribidi executable in PATH')
    workarounds.add_option('--sleep-interval', '--min-sleep-interval', metavar='SECONDS', dest='sleep_interval', type=float, help='Number of seconds to sleep before each download when used alone or a lower bound of a range for randomized sleep before each download (minimum possible number of seconds to sleep) when used along with --max-sleep-interval.')
    workarounds.add_option('--max-sleep-interval', metavar='SECONDS', dest='max_sleep_interval', type=float, help='Upper bound of a range for randomized sleep before each download (maximum possible number of seconds to sleep). Must only be used along with --min-sleep-interval.')
    verbosity = optparse.OptionGroup(parser, 'Verbosity / Simulation Options')
    verbosity.add_option('-q', '--quiet', action='store_true', dest='quiet', default=False, help='Activate quiet mode')
    verbosity.add_option('--no-warnings', dest='no_warnings', action='store_true', default=False, help='Ignore warnings')
    verbosity.add_option('-s', '--simulate', action='store_true', dest='simulate', default=False, help='Do not download the video and do not write anything to disk')
    verbosity.add_option('--skip-download', action='store_true', dest='skip_download', default=False, help='Do not download the video')
    verbosity.add_option('-g', '--get-url', action='store_true', dest='geturl', default=False, help='Simulate, quiet but print URL')
    verbosity.add_option('-e', '--get-title', action='store_true', dest='gettitle', default=False, help='Simulate, quiet but print title')
    verbosity.add_option('--get-id', action='store_true', dest='getid', default=False, help='Simulate, quiet but print id')
    verbosity.add_option('--get-thumbnail', action='store_true', dest='getthumbnail', default=False, help='Simulate, quiet but print thumbnail URL')
    verbosity.add_option('--get-description', action='store_true', dest='getdescription', default=False, help='Simulate, quiet but print video description')
    verbosity.add_option('--get-duration', action='store_true', dest='getduration', default=False, help='Simulate, quiet but print video length')
    verbosity.add_option('--get-filename', action='store_true', dest='getfilename', default=False, help='Simulate, quiet but print output filename')
    verbosity.add_option('--get-format', action='store_true', dest='getformat', default=False, help='Simulate, quiet but print output format')
    verbosity.add_option('-j', '--dump-json', action='store_true', dest='dumpjson', default=False, help='Simulate, quiet but print JSON information. See the ""OUTPUT TEMPLATE"" for a description of available keys.')
    verbosity.add_option('-J', '--dump-single-json', action='store_true', dest='dump_single_json', default=False, help='Simulate, quiet but print JSON information for each command-line argument. If the URL refers to a playlist, dump the whole playlist information in a single line.')
    verbosity.add_option('--print-json', action='store_true', dest='print_json', default=False, help='Be quiet and print the video information as JSON (video is still being downloaded).')
    verbosity.add_option('--newline', action='store_true', dest='progress_with_newline', default=False, help='Output progress bar as new lines')
    verbosity.add_option('--no-progress', action='store_true', dest='noprogress', default=False, help='Do not print progress bar')
    verbosity.add_option('--console-title', action='store_true', dest='consoletitle', default=False, help='Display progress in console titlebar')
    verbosity.add_option('-v', '--verbose', action='store_true', dest='verbose', default=False, help='Print various debugging information')
    verbosity.add_option('--dump-pages', '--dump-intermediate-pages', action='store_true', dest='dump_intermediate_pages', default=False, help='Print downloaded pages encoded using base64 to debug problems (very verbose)')
    verbosity.add_option('--write-pages', action='store_true', dest='write_pages', default=False, help='Write downloaded intermediary pages to files in the current directory to debug problems')
    verbosity.add_option('--youtube-print-sig-code', action='store_true', dest='youtube_print_sig_code', default=False, help=optparse.SUPPRESS_HELP)
    verbosity.add_option('--print-traffic', '--dump-headers', dest='debug_printtraffic', action='store_true', default=False, help='Display sent and read HTTP traffic')
    verbosity.add_option('-C', '--call-home', dest='call_home', action='store_true', default=False, help='Contact the youtube-dl server for debugging')
    verbosity.add_option('--no-call-home', dest='call_home', action='store_false', default=False, help='Do NOT contact the youtube-dl server for debugging')
    filesystem = optparse.OptionGroup(parser, 'Filesystem Options')
    filesystem.add_option('-a', '--batch-file', dest='batchfile', metavar='FILE', help=""File containing URLs to download ('-' for stdin), one URL per line. Lines starting with '#', ';' or ']' are considered as comments and ignored."")
    filesystem.add_option('--id', default=False, action='store_true', dest='useid', help='Use only video ID in file name')
    filesystem.add_option('-o', '--output', dest='outtmpl', metavar='TEMPLATE', help='Output filename template, see the ""OUTPUT TEMPLATE"" for all the info')
    filesystem.add_option('--autonumber-size', dest='autonumber_size', metavar='NUMBER', type=int, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('--autonumber-start', dest='autonumber_start', metavar='NUMBER', default=1, type=int, help='Specify the start value for %(autonumber)s (default is %default)')
    filesystem.add_option('--restrict-filenames', action='store_true', dest='restrictfilenames', default=False, help='Restrict filenames to only ASCII characters, and avoid ""&"" and spaces in filenames')
    filesystem.add_option('-A', '--auto-number', action='store_true', dest='autonumber', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-t', '--title', action='store_true', dest='usetitle', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-l', '--literal', default=False, action='store_true', dest='usetitle', help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-w', '--no-overwrites', action='store_true', dest='nooverwrites', default=False, help='Do not overwrite files')
    filesystem.add_option('-c', '--continue', action='store_true', dest='continue_dl', default=True, help='Force resume of partially downloaded files. By default, youtube-dl will resume downloads if possible.')
    filesystem.add_option('--no-continue', action='store_false', dest='continue_dl', help='Do not resume partially downloaded files (restart from beginning)')
    filesystem.add_option('--no-part', action='store_true', dest='nopart', default=False, help='Do not use .part files - write directly into output file')
    filesystem.add_option('--no-mtime', action='store_false', dest='updatetime', default=True, help='Do not use the Last-modified header to set the file modification time')
    filesystem.add_option('--write-description', action='store_true', dest='writedescription', default=False, help='Write video description to a .description file')
    filesystem.add_option('--write-info-json', action='store_true', dest='writeinfojson', default=False, help='Write video metadata to a .info.json file')
    filesystem.add_option('--write-annotations', action='store_true', dest='writeannotations', default=False, help='Write video annotations to a .annotations.xml file')
    filesystem.add_option('--load-info-json', '--load-info', dest='load_info_filename', metavar='FILE', help='JSON file containing the video information (created with the ""--write-info-json"" option)')
    filesystem.add_option('--cookies', dest='cookiefile', metavar='FILE', help='File to read cookies from and dump cookie jar in')
    filesystem.add_option('--cache-dir', dest='cachedir', default=None, metavar='DIR', help='Location in the filesystem where youtube-dl can store some downloaded information permanently. By default $XDG_CACHE_HOME/youtube-dl or ~/.cache/youtube-dl . At the moment, only YouTube player files (for videos with obfuscated signatures) are cached, but that may change.')
    filesystem.add_option('--no-cache-dir', action='store_const', const=False, dest='cachedir', help='Disable filesystem caching')
    filesystem.add_option('--rm-cache-dir', action='store_true', dest='rm_cachedir', help='Delete all filesystem cache files')
    thumbnail = optparse.OptionGroup(parser, 'Thumbnail images')
    thumbnail.add_option('--write-thumbnail', action='store_true', dest='writethumbnail', default=False, help='Write thumbnail image to disk')
    thumbnail.add_option('--write-all-thumbnails', action='store_true', dest='write_all_thumbnails', default=False, help='Write all thumbnail image formats to disk')
    thumbnail.add_option('--list-thumbnails', action='store_true', dest='list_thumbnails', default=False, help='Simulate and list all available thumbnail formats')
    postproc = optparse.OptionGroup(parser, 'Post-processing Options')
    postproc.add_option('-x', '--extract-audio', action='store_true', dest='extractaudio', default=False, help='Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe)')
    postproc.add_option('--audio-format', metavar='FORMAT', dest='audioformat', default='best', help='Specify audio format: ""best"", ""aac"", ""flac"", ""mp3"", ""m4a"", ""opus"", ""vorbis"", or ""wav""; ""%default"" by default; No effect without -x')
    postproc.add_option('--audio-quality', metavar='QUALITY', dest='audioquality', default='5', help='Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default %default)')
    postproc.add_option('--recode-video', metavar='FORMAT', dest='recodevideo', default=None, help='Encode the video to another format if necessary (currently supported: mp4|flv|ogg|webm|mkv|avi)')
    postproc.add_option('--postprocessor-args', dest='postprocessor_args', metavar='ARGS', help='Give these arguments to the postprocessor')
    postproc.add_option('-k', '--keep-video', action='store_true', dest='keepvideo', default=False, help='Keep the video file on disk after the post-processing; the video is erased by default')
    postproc.add_option('--no-post-overwrites', action='store_true', dest='nopostoverwrites', default=False, help='Do not overwrite post-processed files; the post-processed files are overwritten by default')
    postproc.add_option('--embed-subs', action='store_true', dest='embedsubtitles', default=False, help='Embed subtitles in the video (only for mp4, webm and mkv videos)')
    postproc.add_option('--embed-thumbnail', action='store_true', dest='embedthumbnail', default=False, help='Embed thumbnail in the audio as cover art')
    postproc.add_option('--add-metadata', action='store_true', dest='addmetadata', default=False, help='Write metadata to the video file')
    postproc.add_option('--metadata-from-title', metavar='FORMAT', dest='metafromtitle', help='Parse additional metadata like song title / artist from the video title. The format syntax is the same as --output. Regular expression with named capture groups may also be used. The parsed parameters replace existing values. Example: --metadata-from-title ""%(artist)s - %(title)s"" matches a title like ""Coldplay - Paradise"". Example (regex): --metadata-from-title ""(?P<artist>.+?) - (?P<title>.+)""')
    postproc.add_option('--xattrs', action='store_true', dest='xattrs', default=False, help=""Write metadata to the video file's xattrs (using dublin core and xdg standards)"")
    postproc.add_option('--fixup', metavar='POLICY', dest='fixup', default='detect_or_warn', help='Automatically correct known faults of the file. One of never (do nothing), warn (only emit a warning), detect_o",'config-location %s does not exist.' % location,["f'config-location {location} does not exist.'"],no_found,0,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/options.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/options.py,,"def parseOpts(overrideArguments=None):

    def _readOptions(filename_bytes, default=[]):
        try:
            optionf = open(filename_bytes)
        except IOError:
            return default
        try:
            contents = optionf.read()
            if sys.version_info < (3,):
                contents = contents.decode(preferredencoding())
            res = compat_shlex_split(contents, comments=True)
        finally:
            optionf.close()
        return res

    def _readUserConf():
        xdg_config_home = compat_getenv('XDG_CONFIG_HOME')
        if xdg_config_home:
            userConfFile = os.path.join(xdg_config_home, 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(xdg_config_home, 'youtube-dl.conf')
        else:
            userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl', 'config')
            if not os.path.isfile(userConfFile):
                userConfFile = os.path.join(compat_expanduser('~'), '.config', 'youtube-dl.conf')
        userConf = _readOptions(userConfFile, None)
        if userConf is None:
            appdata_dir = compat_getenv('appdata')
            if appdata_dir:
                userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config'), default=None)
                if userConf is None:
                    userConf = _readOptions(os.path.join(appdata_dir, 'youtube-dl', 'config.txt'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf'), default=None)
        if userConf is None:
            userConf = _readOptions(os.path.join(compat_expanduser('~'), 'youtube-dl.conf.txt'), default=None)
        if userConf is None:
            userConf = []
        return userConf

    def _format_option_string(option):
        """""" ('-o', '--option') -> -o, --format METAVAR""""""
        opts = []
        if option._short_opts:
            opts.append(option._short_opts[0])
        if option._long_opts:
            opts.append(option._long_opts[0])
        if len(opts) > 1:
            opts.insert(1, ', ')
        if option.takes_value():
            opts.append(' %s' % option.metavar)
        return ''.join(opts)

    def _comma_separated_values_options_callback(option, opt_str, value, parser):
        setattr(parser.values, option.dest, value.split(','))
    columns = compat_get_terminal_size().columns
    max_width = columns if columns else 80
    max_help_position = 80
    fmt = optparse.IndentedHelpFormatter(width=max_width, max_help_position=max_help_position)
    fmt.format_option_strings = _format_option_string
    kw = {'version': __version__, 'formatter': fmt, 'usage': '%prog [OPTIONS] URL [URL...]', 'conflict_handler': 'resolve'}
    parser = optparse.OptionParser(**compat_kwargs(kw))
    general = optparse.OptionGroup(parser, 'General Options')
    general.add_option('-h', '--help', action='help', help='Print this help text and exit')
    general.add_option('--version', action='version', help='Print program version and exit')
    general.add_option('-U', '--update', action='store_true', dest='update_self', help='Update this program to latest version. Make sure that you have sufficient permissions (run with sudo if needed)')
    general.add_option('-i', '--ignore-errors', action='store_true', dest='ignoreerrors', default=False, help='Continue on download errors, for example to skip unavailable videos in a playlist')
    general.add_option('--abort-on-error', action='store_false', dest='ignoreerrors', help='Abort downloading of further videos (in the playlist or the command line) if an error occurs')
    general.add_option('--dump-user-agent', action='store_true', dest='dump_user_agent', default=False, help='Display the current browser identification')
    general.add_option('--list-extractors', action='store_true', dest='list_extractors', default=False, help='List all supported extractors')
    general.add_option('--extractor-descriptions', action='store_true', dest='list_extractor_descriptions', default=False, help='Output descriptions of all supported extractors')
    general.add_option('--force-generic-extractor', action='store_true', dest='force_generic_extractor', default=False, help='Force extraction to use the generic extractor')
    general.add_option('--default-search', dest='default_search', metavar='PREFIX', help='Use this prefix for unqualified URLs. For example ""gvsearch2:"" downloads two videos from google videos for youtube-dl ""large apple"". Use the value ""auto"" to let youtube-dl guess (""auto_warning"" to emit a warning when guessing). ""error"" just throws an error. The default value ""fixup_error"" repairs broken URLs, but emits an error if this is not possible instead of searching.')
    general.add_option('--ignore-config', action='store_true', help='Do not read configuration files. When given in the global configuration file /etc/youtube-dl.conf: Do not read the user configuration in ~/.config/youtube-dl/config (%APPDATA%/youtube-dl/config.txt on Windows)')
    general.add_option('--config-location', dest='config_location', metavar='PATH', help='Location of the configuration file; either the path to the config or its containing directory.')
    general.add_option('--flat-playlist', action='store_const', dest='extract_flat', const='in_playlist', default=False, help='Do not extract the videos of a playlist, only list them.')
    general.add_option('--mark-watched', action='store_true', dest='mark_watched', default=False, help='Mark videos watched (YouTube only)')
    general.add_option('--no-mark-watched', action='store_false', dest='mark_watched', default=False, help='Do not mark videos watched (YouTube only)')
    general.add_option('--no-color', '--no-colors', action='store_true', dest='no_color', default=False, help='Do not emit color codes in output')
    network = optparse.OptionGroup(parser, 'Network Options')
    network.add_option('--proxy', dest='proxy', default=None, metavar='URL', help='Use the specified HTTP/HTTPS/SOCKS proxy. To enable SOCKS proxy, specify a proper scheme. For example socks5://127.0.0.1:1080/. Pass in an empty string (--proxy """") for direct connection')
    network.add_option('--socket-timeout', dest='socket_timeout', type=float, default=None, metavar='SECONDS', help='Time to wait before giving up, in seconds')
    network.add_option('--source-address', metavar='IP', dest='source_address', default=None, help='Client-side IP address to bind to')
    network.add_option('-4', '--force-ipv4', action='store_const', const='0.0.0.0', dest='source_address', help='Make all connections via IPv4')
    network.add_option('-6', '--force-ipv6', action='store_const', const='::', dest='source_address', help='Make all connections via IPv6')
    geo = optparse.OptionGroup(parser, 'Geo Restriction')
    geo.add_option('--geo-verification-proxy', dest='geo_verification_proxy', default=None, metavar='URL', help='Use this proxy to verify the IP address for some geo-restricted sites. The default proxy specified by --proxy (or none, if the option is not present) is used for the actual downloading.')
    geo.add_option('--cn-verification-proxy', dest='cn_verification_proxy', default=None, metavar='URL', help=optparse.SUPPRESS_HELP)
    geo.add_option('--geo-bypass', action='store_true', dest='geo_bypass', default=True, help='Bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--no-geo-bypass', action='store_false', dest='geo_bypass', default=True, help='Do not bypass geographic restriction via faking X-Forwarded-For HTTP header')
    geo.add_option('--geo-bypass-country', metavar='CODE', dest='geo_bypass_country', default=None, help='Force bypass geographic restriction with explicitly provided two-letter ISO 3166-2 country code')
    geo.add_option('--geo-bypass-ip-block', metavar='IP_BLOCK', dest='geo_bypass_ip_block', default=None, help='Force bypass geographic restriction with explicitly provided IP block in CIDR notation')
    selection = optparse.OptionGroup(parser, 'Video Selection')
    selection.add_option('--playlist-start', dest='playliststart', metavar='NUMBER', default=1, type=int, help='Playlist video to start at (default is %default)')
    selection.add_option('--playlist-end', dest='playlistend', metavar='NUMBER', default=None, type=int, help='Playlist video to end at (default is last)')
    selection.add_option('--playlist-items', dest='playlist_items', metavar='ITEM_SPEC', default=None, help='Playlist video items to download. Specify indices of the videos in the playlist separated by commas like: ""--playlist-items 1,2,5,8"" if you want to download videos indexed 1, 2, 5, 8 in the playlist. You can specify range: ""--playlist-items 1-3,7,10-13"", it will download the videos at index 1, 2, 3, 7, 10, 11, 12 and 13.')
    selection.add_option('--match-title', dest='matchtitle', metavar='REGEX', help='Download only matching titles (regex or caseless sub-string)')
    selection.add_option('--reject-title', dest='rejecttitle', metavar='REGEX', help='Skip download for matching titles (regex or caseless sub-string)')
    selection.add_option('--max-downloads', dest='max_downloads', metavar='NUMBER', type=int, default=None, help='Abort after downloading NUMBER files')
    selection.add_option('--min-filesize', metavar='SIZE', dest='min_filesize', default=None, help='Do not download any videos smaller than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--max-filesize', metavar='SIZE', dest='max_filesize', default=None, help='Do not download any videos larger than SIZE (e.g. 50k or 44.6m)')
    selection.add_option('--date', metavar='DATE', dest='date', default=None, help='Download only videos uploaded in this date')
    selection.add_option('--datebefore', metavar='DATE', dest='datebefore', default=None, help='Download only videos uploaded on or before this date (i.e. inclusive)')
    selection.add_option('--dateafter', metavar='DATE', dest='dateafter', default=None, help='Download only videos uploaded on or after this date (i.e. inclusive)')
    selection.add_option('--min-views', metavar='COUNT', dest='min_views', default=None, type=int, help='Do not download any videos with less than COUNT views')
    selection.add_option('--max-views', metavar='COUNT', dest='max_views', default=None, type=int, help='Do not download any videos with more than COUNT views')
    selection.add_option('--match-filter', metavar='FILTER', dest='match_filter', default=None, help='Generic video filter. Specify any key (see the ""OUTPUT TEMPLATE"" for a list of available keys) to match if the key is present, !key to check if the key is not present, key > NUMBER (like ""comment_count > 12"", also works with >=, <, <=, !=, =) to compare against a number, key = \'LITERAL\' (like ""uploader = \'Mike Smith\'"", also works with !=) to match against a string literal and & to require multiple matches. Values which are not known are excluded unless you put a question mark (?) after the operator. For example, to only match videos that have been liked more than 100 times and disliked less than 50 times (or the dislike functionality is not available at the given service), but who also have a description, use --match-filter ""like_count > 100 & dislike_count <? 50 & description"" .')
    selection.add_option('--no-playlist', action='store_true', dest='noplaylist', default=False, help='Download only the video, if the URL refers to a video and a playlist.')
    selection.add_option('--yes-playlist', action='store_false', dest='noplaylist', default=False, help='Download the playlist, if the URL refers to a video and a playlist.')
    selection.add_option('--age-limit', metavar='YEARS', dest='age_limit', default=None, type=int, help='Download only videos suitable for the given age')
    selection.add_option('--download-archive', metavar='FILE', dest='download_archive', help='Download only videos not listed in the archive file. Record the IDs of all downloaded videos in it.')
    selection.add_option('--include-ads', dest='include_ads', action='store_true', help='Download advertisements as well (experimental)')
    authentication = optparse.OptionGroup(parser, 'Authentication Options')
    authentication.add_option('-u', '--username', dest='username', metavar='USERNAME', help='Login with this account ID')
    authentication.add_option('-p', '--password', dest='password', metavar='PASSWORD', help='Account password. If this option is left out, youtube-dl will ask interactively.')
    authentication.add_option('-2', '--twofactor', dest='twofactor', metavar='TWOFACTOR', help='Two-factor authentication code')
    authentication.add_option('-n', '--netrc', action='store_true', dest='usenetrc', default=False, help='Use .netrc authentication data')
    authentication.add_option('--video-password', dest='videopassword', metavar='PASSWORD', help='Video password (vimeo, smotri, youku)')
    adobe_pass = optparse.OptionGroup(parser, 'Adobe Pass Options')
    adobe_pass.add_option('--ap-mso', dest='ap_mso', metavar='MSO', help='Adobe Pass multiple-system operator (TV provider) identifier, use --ap-list-mso for a list of available MSOs')
    adobe_pass.add_option('--ap-username', dest='ap_username', metavar='USERNAME', help='Multiple-system operator account login')
    adobe_pass.add_option('--ap-password', dest='ap_password', metavar='PASSWORD', help='Multiple-system operator account password. If this option is left out, youtube-dl will ask interactively.')
    adobe_pass.add_option('--ap-list-mso', action='store_true', dest='ap_list_mso', default=False, help='List all supported multiple-system operators')
    video_format = optparse.OptionGroup(parser, 'Video Format Options')
    video_format.add_option('-f', '--format', action='store', dest='format', metavar='FORMAT', default=None, help='Video format code, see the ""FORMAT SELECTION"" for all the info')
    video_format.add_option('--all-formats', action='store_const', dest='format', const='all', help='Download all available video formats')
    video_format.add_option('--prefer-free-formats', action='store_true', dest='prefer_free_formats', default=False, help='Prefer free video formats unless a specific one is requested')
    video_format.add_option('-F', '--list-formats', action='store_true', dest='listformats', help='List all available formats of requested videos')
    video_format.add_option('--youtube-include-dash-manifest', action='store_true', dest='youtube_include_dash_manifest', default=True, help=optparse.SUPPRESS_HELP)
    video_format.add_option('--youtube-skip-dash-manifest', action='store_false', dest='youtube_include_dash_manifest', help='Do not download the DASH manifests and related data on YouTube videos')
    video_format.add_option('--merge-output-format', action='store', dest='merge_output_format', metavar='FORMAT', default=None, help='If a merge is required (e.g. bestvideo+bestaudio), output to given container format. One of mkv, mp4, ogg, webm, flv. Ignored if no merge is required')
    subtitles = optparse.OptionGroup(parser, 'Subtitle Options')
    subtitles.add_option('--write-sub', '--write-srt', action='store_true', dest='writesubtitles', default=False, help='Write subtitle file')
    subtitles.add_option('--write-auto-sub', '--write-automatic-sub', action='store_true', dest='writeautomaticsub', default=False, help='Write automatically generated subtitle file (YouTube only)')
    subtitles.add_option('--all-subs', action='store_true', dest='allsubtitles', default=False, help='Download all the available subtitles of the video')
    subtitles.add_option('--list-subs', action='store_true', dest='listsubtitles', default=False, help='List all available subtitles for the video')
    subtitles.add_option('--sub-format', action='store', dest='subtitlesformat', metavar='FORMAT', default='best', help='Subtitle format, accepts formats preference, for example: ""srt"" or ""ass/srt/best""')
    subtitles.add_option('--sub-lang', '--sub-langs', '--srt-lang', action='callback', dest='subtitleslangs', metavar='LANGS', type='str', default=[], callback=_comma_separated_values_options_callback, help='Languages of the subtitles to download (optional) separated by commas, use --list-subs for available language tags')
    downloader = optparse.OptionGroup(parser, 'Download Options')
    downloader.add_option('-r', '--limit-rate', '--rate-limit', dest='ratelimit', metavar='RATE', help='Maximum download rate in bytes per second (e.g. 50K or 4.2M)')
    downloader.add_option('-R', '--retries', dest='retries', metavar='RETRIES', default=10, help='Number of retries (default is %default), or ""infinite"".')
    downloader.add_option('--fragment-retries', dest='fragment_retries', metavar='RETRIES', default=10, help='Number of retries for a fragment (default is %default), or ""infinite"" (DASH, hlsnative and ISM)')
    downloader.add_option('--skip-unavailable-fragments', action='store_true', dest='skip_unavailable_fragments', default=True, help='Skip unavailable fragments (DASH, hlsnative and ISM)')
    downloader.add_option('--abort-on-unavailable-fragment', action='store_false', dest='skip_unavailable_fragments', help='Abort downloading when some fragment is not available')
    downloader.add_option('--keep-fragments', action='store_true', dest='keep_fragments', default=False, help='Keep downloaded fragments on disk after downloading is finished; fragments are erased by default')
    downloader.add_option('--buffer-size', dest='buffersize', metavar='SIZE', default='1024', help='Size of download buffer (e.g. 1024 or 16K) (default is %default)')
    downloader.add_option('--no-resize-buffer', action='store_true', dest='noresizebuffer', default=False, help='Do not automatically adjust the buffer size. By default, the buffer size is automatically resized from an initial value of SIZE.')
    downloader.add_option('--http-chunk-size', dest='http_chunk_size', metavar='SIZE', default=None, help='Size of a chunk for chunk-based HTTP downloading (e.g. 10485760 or 10M) (default is disabled). May be useful for bypassing bandwidth throttling imposed by a webserver (experimental)')
    downloader.add_option('--test', action='store_true', dest='test', default=False, help=optparse.SUPPRESS_HELP)
    downloader.add_option('--playlist-reverse', action='store_true', help='Download playlist videos in reverse order')
    downloader.add_option('--playlist-random', action='store_true', help='Download playlist videos in random order')
    downloader.add_option('--xattr-set-filesize', dest='xattr_set_filesize', action='store_true', help='Set file xattribute ytdl.filesize with expected file size')
    downloader.add_option('--hls-prefer-native', dest='hls_prefer_native', action='store_true', default=None, help='Use the native HLS downloader instead of ffmpeg')
    downloader.add_option('--hls-prefer-ffmpeg', dest='hls_prefer_native', action='store_false', default=None, help='Use ffmpeg instead of the native HLS downloader')
    downloader.add_option('--hls-use-mpegts', dest='hls_use_mpegts', action='store_true', help='Use the mpegts container for HLS videos, allowing to play the video while downloading (some players may not be able to play it)')
    downloader.add_option('--external-downloader', dest='external_downloader', metavar='COMMAND', help='Use the specified external downloader. Currently supports %s' % ','.join(list_external_downloaders()))
    downloader.add_option('--external-downloader-args', dest='external_downloader_args', metavar='ARGS', help='Give these arguments to the external downloader')
    workarounds = optparse.OptionGroup(parser, 'Workarounds')
    workarounds.add_option('--encoding', dest='encoding', metavar='ENCODING', help='Force the specified encoding (experimental)')
    workarounds.add_option('--no-check-certificate', action='store_true', dest='no_check_certificate', default=False, help='Suppress HTTPS certificate validation')
    workarounds.add_option('--prefer-insecure', '--prefer-unsecure', action='store_true', dest='prefer_insecure', help='Use an unencrypted connection to retrieve information about the video. (Currently supported only for YouTube)')
    workarounds.add_option('--user-agent', metavar='UA', dest='user_agent', help='Specify a custom user agent')
    workarounds.add_option('--referer', metavar='URL', dest='referer', default=None, help='Specify a custom referer, use if the video access is restricted to one domain')
    workarounds.add_option('--add-header', metavar='FIELD:VALUE', dest='headers', action='append', help=""Specify a custom HTTP header and its value, separated by a colon ':'. You can use this option multiple times"")
    workarounds.add_option('--bidi-workaround', dest='bidi_workaround', action='store_true', help='Work around terminals that lack bidirectional text support. Requires bidiv or fribidi executable in PATH')
    workarounds.add_option('--sleep-interval', '--min-sleep-interval', metavar='SECONDS', dest='sleep_interval', type=float, help='Number of seconds to sleep before each download when used alone or a lower bound of a range for randomized sleep before each download (minimum possible number of seconds to sleep) when used along with --max-sleep-interval.')
    workarounds.add_option('--max-sleep-interval', metavar='SECONDS', dest='max_sleep_interval', type=float, help='Upper bound of a range for randomized sleep before each download (maximum possible number of seconds to sleep). Must only be used along with --min-sleep-interval.')
    verbosity = optparse.OptionGroup(parser, 'Verbosity / Simulation Options')
    verbosity.add_option('-q', '--quiet', action='store_true', dest='quiet', default=False, help='Activate quiet mode')
    verbosity.add_option('--no-warnings', dest='no_warnings', action='store_true', default=False, help='Ignore warnings')
    verbosity.add_option('-s', '--simulate', action='store_true', dest='simulate', default=False, help='Do not download the video and do not write anything to disk')
    verbosity.add_option('--skip-download', action='store_true', dest='skip_download', default=False, help='Do not download the video')
    verbosity.add_option('-g', '--get-url', action='store_true', dest='geturl', default=False, help='Simulate, quiet but print URL')
    verbosity.add_option('-e', '--get-title', action='store_true', dest='gettitle', default=False, help='Simulate, quiet but print title')
    verbosity.add_option('--get-id', action='store_true', dest='getid', default=False, help='Simulate, quiet but print id')
    verbosity.add_option('--get-thumbnail', action='store_true', dest='getthumbnail', default=False, help='Simulate, quiet but print thumbnail URL')
    verbosity.add_option('--get-description', action='store_true', dest='getdescription', default=False, help='Simulate, quiet but print video description')
    verbosity.add_option('--get-duration', action='store_true', dest='getduration', default=False, help='Simulate, quiet but print video length')
    verbosity.add_option('--get-filename', action='store_true', dest='getfilename', default=False, help='Simulate, quiet but print output filename')
    verbosity.add_option('--get-format', action='store_true', dest='getformat', default=False, help='Simulate, quiet but print output format')
    verbosity.add_option('-j', '--dump-json', action='store_true', dest='dumpjson', default=False, help='Simulate, quiet but print JSON information. See the ""OUTPUT TEMPLATE"" for a description of available keys.')
    verbosity.add_option('-J', '--dump-single-json', action='store_true', dest='dump_single_json', default=False, help='Simulate, quiet but print JSON information for each command-line argument. If the URL refers to a playlist, dump the whole playlist information in a single line.')
    verbosity.add_option('--print-json', action='store_true', dest='print_json', default=False, help='Be quiet and print the video information as JSON (video is still being downloaded).')
    verbosity.add_option('--newline', action='store_true', dest='progress_with_newline', default=False, help='Output progress bar as new lines')
    verbosity.add_option('--no-progress', action='store_true', dest='noprogress', default=False, help='Do not print progress bar')
    verbosity.add_option('--console-title', action='store_true', dest='consoletitle', default=False, help='Display progress in console titlebar')
    verbosity.add_option('-v', '--verbose', action='store_true', dest='verbose', default=False, help='Print various debugging information')
    verbosity.add_option('--dump-pages', '--dump-intermediate-pages', action='store_true', dest='dump_intermediate_pages', default=False, help='Print downloaded pages encoded using base64 to debug problems (very verbose)')
    verbosity.add_option('--write-pages', action='store_true', dest='write_pages', default=False, help='Write downloaded intermediary pages to files in the current directory to debug problems')
    verbosity.add_option('--youtube-print-sig-code', action='store_true', dest='youtube_print_sig_code', default=False, help=optparse.SUPPRESS_HELP)
    verbosity.add_option('--print-traffic', '--dump-headers', dest='debug_printtraffic', action='store_true', default=False, help='Display sent and read HTTP traffic')
    verbosity.add_option('-C', '--call-home', dest='call_home', action='store_true', default=False, help='Contact the youtube-dl server for debugging')
    verbosity.add_option('--no-call-home', dest='call_home', action='store_false', default=False, help='Do NOT contact the youtube-dl server for debugging')
    filesystem = optparse.OptionGroup(parser, 'Filesystem Options')
    filesystem.add_option('-a', '--batch-file', dest='batchfile', metavar='FILE', help=""File containing URLs to download ('-' for stdin), one URL per line. Lines starting with '#', ';' or ']' are considered as comments and ignored."")
    filesystem.add_option('--id', default=False, action='store_true', dest='useid', help='Use only video ID in file name')
    filesystem.add_option('-o', '--output', dest='outtmpl', metavar='TEMPLATE', help='Output filename template, see the ""OUTPUT TEMPLATE"" for all the info')
    filesystem.add_option('--autonumber-size', dest='autonumber_size', metavar='NUMBER', type=int, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('--autonumber-start', dest='autonumber_start', metavar='NUMBER', default=1, type=int, help='Specify the start value for %(autonumber)s (default is %default)')
    filesystem.add_option('--restrict-filenames', action='store_true', dest='restrictfilenames', default=False, help='Restrict filenames to only ASCII characters, and avoid ""&"" and spaces in filenames')
    filesystem.add_option('-A', '--auto-number', action='store_true', dest='autonumber', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-t', '--title', action='store_true', dest='usetitle', default=False, help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-l', '--literal', default=False, action='store_true', dest='usetitle', help=optparse.SUPPRESS_HELP)
    filesystem.add_option('-w', '--no-overwrites', action='store_true', dest='nooverwrites', default=False, help='Do not overwrite files')
    filesystem.add_option('-c', '--continue', action='store_true', dest='continue_dl', default=True, help='Force resume of partially downloaded files. By default, youtube-dl will resume downloads if possible.')
    filesystem.add_option('--no-continue', action='store_false', dest='continue_dl', help='Do not resume partially downloaded files (restart from beginning)')
    filesystem.add_option('--no-part', action='store_true', dest='nopart', default=False, help='Do not use .part files - write directly into output file')
    filesystem.add_option('--no-mtime', action='store_false', dest='updatetime', default=True, help='Do not use the Last-modified header to set the file modification time')
    filesystem.add_option('--write-description', action='store_true', dest='writedescription', default=False, help='Write video description to a .description file')
    filesystem.add_option('--write-info-json', action='store_true', dest='writeinfojson', default=False, help='Write video metadata to a .info.json file')
    filesystem.add_option('--write-annotations', action='store_true', dest='writeannotations', default=False, help='Write video annotations to a .annotations.xml file')
    filesystem.add_option('--load-info-json', '--load-info', dest='load_info_filename', metavar='FILE', help='JSON file containing the video information (created with the ""--write-info-json"" option)')
    filesystem.add_option('--cookies', dest='cookiefile', metavar='FILE', help='File to read cookies from and dump cookie jar in')
    filesystem.add_option('--cache-dir', dest='cachedir', default=None, metavar='DIR', help='Location in the filesystem where youtube-dl can store some downloaded information permanently. By default $XDG_CACHE_HOME/youtube-dl or ~/.cache/youtube-dl . At the moment, only YouTube player files (for videos with obfuscated signatures) are cached, but that may change.')
    filesystem.add_option('--no-cache-dir', action='store_const', const=False, dest='cachedir', help='Disable filesystem caching')
    filesystem.add_option('--rm-cache-dir', action='store_true', dest='rm_cachedir', help='Delete all filesystem cache files')
    thumbnail = optparse.OptionGroup(parser, 'Thumbnail images')
    thumbnail.add_option('--write-thumbnail', action='store_true', dest='writethumbnail', default=False, help='Write thumbnail image to disk')
    thumbnail.add_option('--write-all-thumbnails', action='store_true', dest='write_all_thumbnails', default=False, help='Write all thumbnail image formats to disk')
    thumbnail.add_option('--list-thumbnails', action='store_true', dest='list_thumbnails', default=False, help='Simulate and list all available thumbnail formats')
    postproc = optparse.OptionGroup(parser, 'Post-processing Options')
    postproc.add_option('-x', '--extract-audio', action='store_true', dest='extractaudio', default=False, help='Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe)')
    postproc.add_option('--audio-format', metavar='FORMAT', dest='audioformat', default='best', help='Specify audio format: ""best"", ""aac"", ""flac"", ""mp3"", ""m4a"", ""opus"", ""vorbis"", or ""wav""; ""%default"" by default; No effect without -x')
    postproc.add_option('--audio-quality', metavar='QUALITY', dest='audioquality', default='5', help='Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default %default)')
    postproc.add_option('--recode-video', metavar='FORMAT', dest='recodevideo', default=None, help='Encode the video to another format if necessary (currently supported: mp4|flv|ogg|webm|mkv|avi)')
    postproc.add_option('--postprocessor-args', dest='postprocessor_args', metavar='ARGS', help='Give these arguments to the postprocessor')
    postproc.add_option('-k', '--keep-video', action='store_true', dest='keepvideo', default=False, help='Keep the video file on disk after the post-processing; the video is erased by default')
    postproc.add_option('--no-post-overwrites', action='store_true', dest='nopostoverwrites', default=False, help='Do not overwrite post-processed files; the post-processed files are overwritten by default')
    postproc.add_option('--embed-subs', action='store_true', dest='embedsubtitles', default=False, help='Embed subtitles in the video (only for mp4, webm and mkv videos)')
    postproc.add_option('--embed-thumbnail', action='store_true', dest='embedthumbnail', default=False, help='Embed thumbnail in the audio as cover art')
    postproc.add_option('--add-metadata', action='store_true', dest='addmetadata', default=False, help='Write metadata to the video file')
    postproc.add_option('--metadata-from-title', metavar='FORMAT', dest='metafromtitle', help='Parse additional metadata like song title / artist from the video title. The format syntax is the same as --output. Regular expression with named capture groups may also be used. The parsed parameters replace existing values. Example: --metadata-from-title ""%(artist)s - %(title)s"" matches a title like ""Coldplay - Paradise"". Example (regex): --metadata-from-title ""(?P<artist>.+?) - (?P<title>.+)""')
    postproc.add_option('--xattrs', action='store_true', dest='xattrs', default=False, help=""Write metadata to the video file's xattrs (using dublin core and xdg standards)"")
    postproc.add_option('--fixup', metavar='POLICY', dest='fixup', default='detect_or_warn', help='Automatically correct known faults of the file. One of never (do nothing), warn (only emit a warning), detect_o","'[debug] %s: %s\n' % (conf_label, repr(_hide_login_info(conf)))",["f'[debug] {conf_label}: {repr(_hide_login_info(conf))}\\n'"],no_found,0,,,,
wig,https://github.com/jekyc/wig/tree/master/wig/classes/request2.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wig/wig/classes/request2.py,Response,"def __repr__(self):

    def get_string(r):
        string = r.url + '\n'
        string += '%s %s\n' % (r.status['code'], r.status['text'])
        string += '\n'.join([header + ': ' + r.headers[header] for header in r.headers])
        string += '\n\n'
        string += 'MD5:            ' + self.md5 + '\n'
        string += 'MD5 Error page: ' + self.md5_404 + '\n'
        return string
    return get_string(self)","'%s %s\n' % (r.status['code'], r.status['text'])",['f"{r.status[\'code\']} {r.status[\'text\']}\\n"'],no_found,1,,,,
capa,https://github.com/mandiant/capa/tree/master/scripts/profile-time.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capa/scripts/profile-time.py,,"def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    label = subprocess.run('git show --pretty=oneline --abbrev-commit | head -n 1', shell=True, capture_output=True, text=True).stdout.strip()
    is_dirty = subprocess.run(""git status | grep 'modified: ' | grep -v 'rules' | grep -v 'tests/data'"", shell=True, capture_output=True, text=True).stdout != ''
    if is_dirty:
        label += ' (dirty)'
    parser = argparse.ArgumentParser(description='Profile capa performance')
    capa.main.install_common_args(parser, wanted={'format', 'sample', 'signatures', 'rules'})
    parser.add_argument('--number', type=int, default=3, help='batch size of profile collection')
    parser.add_argument('--repeat', type=int, default=30, help='batch count of profile collection')
    parser.add_argument('--label', type=str, default=label, help='description of the profile collection')
    args = parser.parse_args(args=argv)
    capa.main.handle_common_args(args)
    try:
        taste = capa.helpers.get_file_taste(args.sample)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        with capa.main.timing('load rules'):
            rules = capa.rules.RuleSet(capa.main.get_rules(args.rules, disable_progress=True))
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        sig_paths = capa.main.get_signatures(args.signatures)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    if args.format == 'freeze' or (args.format == 'auto' and capa.features.freeze.is_freeze(taste)):
        with open(args.sample, 'rb') as f:
            extractor = capa.features.freeze.load(f.read())
    else:
        extractor = capa.main.get_extractor(args.sample, args.format, capa.main.BACKEND_VIV, sig_paths, should_save_workspace=False)
    with tqdm.tqdm(total=args.number * args.repeat) as pbar:

        def do_iteration():
            capa.perf.reset()
            capa.main.find_capabilities(rules, extractor, disable_progress=True)
            pbar.update(1)
        samples = timeit.repeat(do_iteration, number=args.number, repeat=args.repeat)
    logger.debug('perf: find capabilities: min: %0.2fs' % (min(samples) / float(args.number)))
    logger.debug('perf: find capabilities: avg: %0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)))
    logger.debug('perf: find capabilities: max: %0.2fs' % (max(samples) / float(args.number)))
    for (counter, count) in capa.perf.counters.most_common():
        logger.debug('perf: counter: {:}: {:,}'.format(counter, count))
    print(tabulate.tabulate([(args.label, '{:,}'.format(capa.perf.counters['evaluate.feature']), '%0.2fs' % (min(samples) / float(args.number)), '%0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)), '%0.2fs' % (max(samples) / float(args.number)))], headers=['label', 'count(evaluations)', 'min(time)', 'avg(time)', 'max(time)'], tablefmt='github'))
    return 0",'perf: find capabilities: min: %0.2fs' % (min(samples) / float(args.number)),["f'perf: find capabilities: min: {min(samples) / float(args.number):.2f}s'"],no_found,0,,,,
capa,https://github.com/mandiant/capa/tree/master/scripts/profile-time.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capa/scripts/profile-time.py,,"def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    label = subprocess.run('git show --pretty=oneline --abbrev-commit | head -n 1', shell=True, capture_output=True, text=True).stdout.strip()
    is_dirty = subprocess.run(""git status | grep 'modified: ' | grep -v 'rules' | grep -v 'tests/data'"", shell=True, capture_output=True, text=True).stdout != ''
    if is_dirty:
        label += ' (dirty)'
    parser = argparse.ArgumentParser(description='Profile capa performance')
    capa.main.install_common_args(parser, wanted={'format', 'sample', 'signatures', 'rules'})
    parser.add_argument('--number', type=int, default=3, help='batch size of profile collection')
    parser.add_argument('--repeat', type=int, default=30, help='batch count of profile collection')
    parser.add_argument('--label', type=str, default=label, help='description of the profile collection')
    args = parser.parse_args(args=argv)
    capa.main.handle_common_args(args)
    try:
        taste = capa.helpers.get_file_taste(args.sample)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        with capa.main.timing('load rules'):
            rules = capa.rules.RuleSet(capa.main.get_rules(args.rules, disable_progress=True))
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        sig_paths = capa.main.get_signatures(args.signatures)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    if args.format == 'freeze' or (args.format == 'auto' and capa.features.freeze.is_freeze(taste)):
        with open(args.sample, 'rb') as f:
            extractor = capa.features.freeze.load(f.read())
    else:
        extractor = capa.main.get_extractor(args.sample, args.format, capa.main.BACKEND_VIV, sig_paths, should_save_workspace=False)
    with tqdm.tqdm(total=args.number * args.repeat) as pbar:

        def do_iteration():
            capa.perf.reset()
            capa.main.find_capabilities(rules, extractor, disable_progress=True)
            pbar.update(1)
        samples = timeit.repeat(do_iteration, number=args.number, repeat=args.repeat)
    logger.debug('perf: find capabilities: min: %0.2fs' % (min(samples) / float(args.number)))
    logger.debug('perf: find capabilities: avg: %0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)))
    logger.debug('perf: find capabilities: max: %0.2fs' % (max(samples) / float(args.number)))
    for (counter, count) in capa.perf.counters.most_common():
        logger.debug('perf: counter: {:}: {:,}'.format(counter, count))
    print(tabulate.tabulate([(args.label, '{:,}'.format(capa.perf.counters['evaluate.feature']), '%0.2fs' % (min(samples) / float(args.number)), '%0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)), '%0.2fs' % (max(samples) / float(args.number)))], headers=['label', 'count(evaluations)', 'min(time)', 'avg(time)', 'max(time)'], tablefmt='github'))
    return 0",'perf: find capabilities: avg: %0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)),["f'perf: find capabilities: avg: {sum(samples) / float(args.repeat) / float(args.number):.2f}s'"],no_found,0,,,,
capa,https://github.com/mandiant/capa/tree/master/scripts/profile-time.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capa/scripts/profile-time.py,,"def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    label = subprocess.run('git show --pretty=oneline --abbrev-commit | head -n 1', shell=True, capture_output=True, text=True).stdout.strip()
    is_dirty = subprocess.run(""git status | grep 'modified: ' | grep -v 'rules' | grep -v 'tests/data'"", shell=True, capture_output=True, text=True).stdout != ''
    if is_dirty:
        label += ' (dirty)'
    parser = argparse.ArgumentParser(description='Profile capa performance')
    capa.main.install_common_args(parser, wanted={'format', 'sample', 'signatures', 'rules'})
    parser.add_argument('--number', type=int, default=3, help='batch size of profile collection')
    parser.add_argument('--repeat', type=int, default=30, help='batch count of profile collection')
    parser.add_argument('--label', type=str, default=label, help='description of the profile collection')
    args = parser.parse_args(args=argv)
    capa.main.handle_common_args(args)
    try:
        taste = capa.helpers.get_file_taste(args.sample)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        with capa.main.timing('load rules'):
            rules = capa.rules.RuleSet(capa.main.get_rules(args.rules, disable_progress=True))
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        sig_paths = capa.main.get_signatures(args.signatures)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    if args.format == 'freeze' or (args.format == 'auto' and capa.features.freeze.is_freeze(taste)):
        with open(args.sample, 'rb') as f:
            extractor = capa.features.freeze.load(f.read())
    else:
        extractor = capa.main.get_extractor(args.sample, args.format, capa.main.BACKEND_VIV, sig_paths, should_save_workspace=False)
    with tqdm.tqdm(total=args.number * args.repeat) as pbar:

        def do_iteration():
            capa.perf.reset()
            capa.main.find_capabilities(rules, extractor, disable_progress=True)
            pbar.update(1)
        samples = timeit.repeat(do_iteration, number=args.number, repeat=args.repeat)
    logger.debug('perf: find capabilities: min: %0.2fs' % (min(samples) / float(args.number)))
    logger.debug('perf: find capabilities: avg: %0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)))
    logger.debug('perf: find capabilities: max: %0.2fs' % (max(samples) / float(args.number)))
    for (counter, count) in capa.perf.counters.most_common():
        logger.debug('perf: counter: {:}: {:,}'.format(counter, count))
    print(tabulate.tabulate([(args.label, '{:,}'.format(capa.perf.counters['evaluate.feature']), '%0.2fs' % (min(samples) / float(args.number)), '%0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)), '%0.2fs' % (max(samples) / float(args.number)))], headers=['label', 'count(evaluations)', 'min(time)', 'avg(time)', 'max(time)'], tablefmt='github'))
    return 0",'perf: find capabilities: max: %0.2fs' % (max(samples) / float(args.number)),["f'perf: find capabilities: max: {max(samples) / float(args.number):.2f}s'"],no_found,0,,,,
capa,https://github.com/mandiant/capa/tree/master/scripts/profile-time.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capa/scripts/profile-time.py,,"def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    label = subprocess.run('git show --pretty=oneline --abbrev-commit | head -n 1', shell=True, capture_output=True, text=True).stdout.strip()
    is_dirty = subprocess.run(""git status | grep 'modified: ' | grep -v 'rules' | grep -v 'tests/data'"", shell=True, capture_output=True, text=True).stdout != ''
    if is_dirty:
        label += ' (dirty)'
    parser = argparse.ArgumentParser(description='Profile capa performance')
    capa.main.install_common_args(parser, wanted={'format', 'sample', 'signatures', 'rules'})
    parser.add_argument('--number', type=int, default=3, help='batch size of profile collection')
    parser.add_argument('--repeat', type=int, default=30, help='batch count of profile collection')
    parser.add_argument('--label', type=str, default=label, help='description of the profile collection')
    args = parser.parse_args(args=argv)
    capa.main.handle_common_args(args)
    try:
        taste = capa.helpers.get_file_taste(args.sample)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        with capa.main.timing('load rules'):
            rules = capa.rules.RuleSet(capa.main.get_rules(args.rules, disable_progress=True))
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        sig_paths = capa.main.get_signatures(args.signatures)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    if args.format == 'freeze' or (args.format == 'auto' and capa.features.freeze.is_freeze(taste)):
        with open(args.sample, 'rb') as f:
            extractor = capa.features.freeze.load(f.read())
    else:
        extractor = capa.main.get_extractor(args.sample, args.format, capa.main.BACKEND_VIV, sig_paths, should_save_workspace=False)
    with tqdm.tqdm(total=args.number * args.repeat) as pbar:

        def do_iteration():
            capa.perf.reset()
            capa.main.find_capabilities(rules, extractor, disable_progress=True)
            pbar.update(1)
        samples = timeit.repeat(do_iteration, number=args.number, repeat=args.repeat)
    logger.debug('perf: find capabilities: min: %0.2fs' % (min(samples) / float(args.number)))
    logger.debug('perf: find capabilities: avg: %0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)))
    logger.debug('perf: find capabilities: max: %0.2fs' % (max(samples) / float(args.number)))
    for (counter, count) in capa.perf.counters.most_common():
        logger.debug('perf: counter: {:}: {:,}'.format(counter, count))
    print(tabulate.tabulate([(args.label, '{:,}'.format(capa.perf.counters['evaluate.feature']), '%0.2fs' % (min(samples) / float(args.number)), '%0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)), '%0.2fs' % (max(samples) / float(args.number)))], headers=['label', 'count(evaluations)', 'min(time)', 'avg(time)', 'max(time)'], tablefmt='github'))
    return 0",'%0.2fs' % (min(samples) / float(args.number)),["f'{min(samples) / float(args.number):.2f}s'"],no_found,0,,,,
capa,https://github.com/mandiant/capa/tree/master/scripts/profile-time.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capa/scripts/profile-time.py,,"def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    label = subprocess.run('git show --pretty=oneline --abbrev-commit | head -n 1', shell=True, capture_output=True, text=True).stdout.strip()
    is_dirty = subprocess.run(""git status | grep 'modified: ' | grep -v 'rules' | grep -v 'tests/data'"", shell=True, capture_output=True, text=True).stdout != ''
    if is_dirty:
        label += ' (dirty)'
    parser = argparse.ArgumentParser(description='Profile capa performance')
    capa.main.install_common_args(parser, wanted={'format', 'sample', 'signatures', 'rules'})
    parser.add_argument('--number', type=int, default=3, help='batch size of profile collection')
    parser.add_argument('--repeat', type=int, default=30, help='batch count of profile collection')
    parser.add_argument('--label', type=str, default=label, help='description of the profile collection')
    args = parser.parse_args(args=argv)
    capa.main.handle_common_args(args)
    try:
        taste = capa.helpers.get_file_taste(args.sample)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        with capa.main.timing('load rules'):
            rules = capa.rules.RuleSet(capa.main.get_rules(args.rules, disable_progress=True))
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        sig_paths = capa.main.get_signatures(args.signatures)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    if args.format == 'freeze' or (args.format == 'auto' and capa.features.freeze.is_freeze(taste)):
        with open(args.sample, 'rb') as f:
            extractor = capa.features.freeze.load(f.read())
    else:
        extractor = capa.main.get_extractor(args.sample, args.format, capa.main.BACKEND_VIV, sig_paths, should_save_workspace=False)
    with tqdm.tqdm(total=args.number * args.repeat) as pbar:

        def do_iteration():
            capa.perf.reset()
            capa.main.find_capabilities(rules, extractor, disable_progress=True)
            pbar.update(1)
        samples = timeit.repeat(do_iteration, number=args.number, repeat=args.repeat)
    logger.debug('perf: find capabilities: min: %0.2fs' % (min(samples) / float(args.number)))
    logger.debug('perf: find capabilities: avg: %0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)))
    logger.debug('perf: find capabilities: max: %0.2fs' % (max(samples) / float(args.number)))
    for (counter, count) in capa.perf.counters.most_common():
        logger.debug('perf: counter: {:}: {:,}'.format(counter, count))
    print(tabulate.tabulate([(args.label, '{:,}'.format(capa.perf.counters['evaluate.feature']), '%0.2fs' % (min(samples) / float(args.number)), '%0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)), '%0.2fs' % (max(samples) / float(args.number)))], headers=['label', 'count(evaluations)', 'min(time)', 'avg(time)', 'max(time)'], tablefmt='github'))
    return 0",'%0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)),["f'{sum(samples) / float(args.repeat) / float(args.number):.2f}s'"],no_found,0,,,,
capa,https://github.com/mandiant/capa/tree/master/scripts/profile-time.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capa/scripts/profile-time.py,,"def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    label = subprocess.run('git show --pretty=oneline --abbrev-commit | head -n 1', shell=True, capture_output=True, text=True).stdout.strip()
    is_dirty = subprocess.run(""git status | grep 'modified: ' | grep -v 'rules' | grep -v 'tests/data'"", shell=True, capture_output=True, text=True).stdout != ''
    if is_dirty:
        label += ' (dirty)'
    parser = argparse.ArgumentParser(description='Profile capa performance')
    capa.main.install_common_args(parser, wanted={'format', 'sample', 'signatures', 'rules'})
    parser.add_argument('--number', type=int, default=3, help='batch size of profile collection')
    parser.add_argument('--repeat', type=int, default=30, help='batch count of profile collection')
    parser.add_argument('--label', type=str, default=label, help='description of the profile collection')
    args = parser.parse_args(args=argv)
    capa.main.handle_common_args(args)
    try:
        taste = capa.helpers.get_file_taste(args.sample)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        with capa.main.timing('load rules'):
            rules = capa.rules.RuleSet(capa.main.get_rules(args.rules, disable_progress=True))
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        sig_paths = capa.main.get_signatures(args.signatures)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    if args.format == 'freeze' or (args.format == 'auto' and capa.features.freeze.is_freeze(taste)):
        with open(args.sample, 'rb') as f:
            extractor = capa.features.freeze.load(f.read())
    else:
        extractor = capa.main.get_extractor(args.sample, args.format, capa.main.BACKEND_VIV, sig_paths, should_save_workspace=False)
    with tqdm.tqdm(total=args.number * args.repeat) as pbar:

        def do_iteration():
            capa.perf.reset()
            capa.main.find_capabilities(rules, extractor, disable_progress=True)
            pbar.update(1)
        samples = timeit.repeat(do_iteration, number=args.number, repeat=args.repeat)
    logger.debug('perf: find capabilities: min: %0.2fs' % (min(samples) / float(args.number)))
    logger.debug('perf: find capabilities: avg: %0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)))
    logger.debug('perf: find capabilities: max: %0.2fs' % (max(samples) / float(args.number)))
    for (counter, count) in capa.perf.counters.most_common():
        logger.debug('perf: counter: {:}: {:,}'.format(counter, count))
    print(tabulate.tabulate([(args.label, '{:,}'.format(capa.perf.counters['evaluate.feature']), '%0.2fs' % (min(samples) / float(args.number)), '%0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)), '%0.2fs' % (max(samples) / float(args.number)))], headers=['label', 'count(evaluations)', 'min(time)', 'avg(time)', 'max(time)'], tablefmt='github'))
    return 0",'%0.2fs' % (max(samples) / float(args.number)),["f'{max(samples) / float(args.number):.2f}s'"],no_found,0,,,,
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/test/api/test_create.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/test/api/test_create.py,,"def test_create_switches_raise_except():
    net = pp.create_empty_network()
    b1 = pp.create_bus(net, 110)
    b2 = pp.create_bus(net, 110)
    b3 = pp.create_bus(net, 15)
    b4 = pp.create_bus(net, 15)
    b5 = pp.create_bus(net, 0.9)
    b6 = pp.create_bus(net, 0.4)
    l1 = pp.create_line(net, b1, b2, length_km=1, std_type='48-AL1/8-ST1A 10.0')
    t1 = pp.create_transformer(net, b2, b3, std_type='160 MVA 380/110 kV')
    t3w1 = pp.create_transformer3w_from_parameters(net, hv_bus=b4, mv_bus=b5, lv_bus=b6, vn_hv_kv=15.0, vn_mv_kv=0.9, vn_lv_kv=0.45, sn_hv_mva=0.6, sn_mv_mva=0.5, sn_lv_mva=0.4, vk_hv_percent=1.0, vk_mv_percent=1.0, vk_lv_percent=1.0, vkr_hv_percent=0.3, vkr_mv_percent=0.3, vkr_lv_percent=0.3, pfe_kw=0.2, i0_percent=0.3, tap_neutral=0.0)
    sw = pp.create_switch(net, bus=b1, element=l1, et='l', z_ohm=0.0)
    with pytest.raises(UserWarning, match='Switches with indexes \\[0\\] already exist.'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0, index=[sw, 1, 2])
    with pytest.raises(UserWarning, match='Cannot attach to buses \\{6\\}, they do not exist'):
        pp.create_switches(net, buses=[6, b2, b3], elements=[l1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Line 1 does not exist'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Line %s not connected to bus %s' % (l1, b3)):
        pp.create_switches(net, buses=[b3, b2, b3], elements=[l1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Trafo 1 does not exist'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, 1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Trafo %s not connected to bus %s' % (t1, b1)):
        pp.create_switches(net, buses=[b1, b1, b3], elements=[l1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Cannot attach to bus 6, bus does not exist'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, t1, 6], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Trafo3w 1 does not exist'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, t1, 1], et=['l', 't', 't3'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Trafo3w %s not connected to bus %s' % (t3w1, b3)):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, t1, t3w1], et=['l', 't', 't3'], z_ohm=0.0)","'Line %s not connected to bus %s' % (l1, b3)",["f'Line {l1} not connected to bus {b3}'"],no_found,0,,,,
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/test/api/test_create.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/test/api/test_create.py,,"def test_create_switches_raise_except():
    net = pp.create_empty_network()
    b1 = pp.create_bus(net, 110)
    b2 = pp.create_bus(net, 110)
    b3 = pp.create_bus(net, 15)
    b4 = pp.create_bus(net, 15)
    b5 = pp.create_bus(net, 0.9)
    b6 = pp.create_bus(net, 0.4)
    l1 = pp.create_line(net, b1, b2, length_km=1, std_type='48-AL1/8-ST1A 10.0')
    t1 = pp.create_transformer(net, b2, b3, std_type='160 MVA 380/110 kV')
    t3w1 = pp.create_transformer3w_from_parameters(net, hv_bus=b4, mv_bus=b5, lv_bus=b6, vn_hv_kv=15.0, vn_mv_kv=0.9, vn_lv_kv=0.45, sn_hv_mva=0.6, sn_mv_mva=0.5, sn_lv_mva=0.4, vk_hv_percent=1.0, vk_mv_percent=1.0, vk_lv_percent=1.0, vkr_hv_percent=0.3, vkr_mv_percent=0.3, vkr_lv_percent=0.3, pfe_kw=0.2, i0_percent=0.3, tap_neutral=0.0)
    sw = pp.create_switch(net, bus=b1, element=l1, et='l', z_ohm=0.0)
    with pytest.raises(UserWarning, match='Switches with indexes \\[0\\] already exist.'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0, index=[sw, 1, 2])
    with pytest.raises(UserWarning, match='Cannot attach to buses \\{6\\}, they do not exist'):
        pp.create_switches(net, buses=[6, b2, b3], elements=[l1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Line 1 does not exist'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Line %s not connected to bus %s' % (l1, b3)):
        pp.create_switches(net, buses=[b3, b2, b3], elements=[l1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Trafo 1 does not exist'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, 1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Trafo %s not connected to bus %s' % (t1, b1)):
        pp.create_switches(net, buses=[b1, b1, b3], elements=[l1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Cannot attach to bus 6, bus does not exist'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, t1, 6], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Trafo3w 1 does not exist'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, t1, 1], et=['l', 't', 't3'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Trafo3w %s not connected to bus %s' % (t3w1, b3)):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, t1, t3w1], et=['l', 't', 't3'], z_ohm=0.0)","'Trafo %s not connected to bus %s' % (t1, b1)",["f'Trafo {t1} not connected to bus {b1}'"],no_found,0,,,,
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/test/api/test_create.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/test/api/test_create.py,,"def test_create_switches_raise_except():
    net = pp.create_empty_network()
    b1 = pp.create_bus(net, 110)
    b2 = pp.create_bus(net, 110)
    b3 = pp.create_bus(net, 15)
    b4 = pp.create_bus(net, 15)
    b5 = pp.create_bus(net, 0.9)
    b6 = pp.create_bus(net, 0.4)
    l1 = pp.create_line(net, b1, b2, length_km=1, std_type='48-AL1/8-ST1A 10.0')
    t1 = pp.create_transformer(net, b2, b3, std_type='160 MVA 380/110 kV')
    t3w1 = pp.create_transformer3w_from_parameters(net, hv_bus=b4, mv_bus=b5, lv_bus=b6, vn_hv_kv=15.0, vn_mv_kv=0.9, vn_lv_kv=0.45, sn_hv_mva=0.6, sn_mv_mva=0.5, sn_lv_mva=0.4, vk_hv_percent=1.0, vk_mv_percent=1.0, vk_lv_percent=1.0, vkr_hv_percent=0.3, vkr_mv_percent=0.3, vkr_lv_percent=0.3, pfe_kw=0.2, i0_percent=0.3, tap_neutral=0.0)
    sw = pp.create_switch(net, bus=b1, element=l1, et='l', z_ohm=0.0)
    with pytest.raises(UserWarning, match='Switches with indexes \\[0\\] already exist.'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0, index=[sw, 1, 2])
    with pytest.raises(UserWarning, match='Cannot attach to buses \\{6\\}, they do not exist'):
        pp.create_switches(net, buses=[6, b2, b3], elements=[l1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Line 1 does not exist'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Line %s not connected to bus %s' % (l1, b3)):
        pp.create_switches(net, buses=[b3, b2, b3], elements=[l1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Trafo 1 does not exist'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, 1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Trafo %s not connected to bus %s' % (t1, b1)):
        pp.create_switches(net, buses=[b1, b1, b3], elements=[l1, t1, b4], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Cannot attach to bus 6, bus does not exist'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, t1, 6], et=['l', 't', 'b'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Trafo3w 1 does not exist'):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, t1, 1], et=['l', 't', 't3'], z_ohm=0.0)
    with pytest.raises(UserWarning, match='Trafo3w %s not connected to bus %s' % (t3w1, b3)):
        pp.create_switches(net, buses=[b1, b2, b3], elements=[l1, t1, t3w1], et=['l', 't', 't3'], z_ohm=0.0)","'Trafo3w %s not connected to bus %s' % (t3w1, b3)",["f'Trafo3w {t3w1} not connected to bus {b3}'"],no_found,0,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_dist_base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_dist_base.py,TestDistRunnerBase,"def run_use_fleet_api_trainer(self, args):
    assert args.update_method == 'nccl2' or 'bkcl'
    self.lr = args.lr
    exec_strategy = fluid.ExecutionStrategy()
    exec_strategy.num_threads = 1
    dist_strategy = DistributedStrategy()
    dist_strategy.exec_strategy = exec_strategy
    dist_strategy.fuse_memory_size = 1
    dist_strategy.fuse_laryer_size = 1
    if args.use_local_sgd:
        dist_strategy.use_local_sgd = True
    if args.ut4grad_allreduce:
        dist_strategy._ut4grad_allreduce = True
    if args.sync_batch_norm:
        dist_strategy.sync_batch_norm = True
    role = role_maker.PaddleCloudRoleMaker(is_collective=True)
    fleet.init(role)
    print_to_err('use_fleet', 'fleet.node_num:')
    (test_program, avg_cost, train_reader, test_reader, batch_acc, predict) = self.get_model(batch_size=args.batch_size, dist_strategy=dist_strategy)
    trainer_prog = fleet._origin_program
    dist_prog = fleet.main_program
    if fluid.core.is_compiled_with_cuda():
        device_id = int(os.getenv('FLAGS_selected_gpus', '0'))
        place = fluid.CUDAPlace(device_id)
    elif fluid.core.is_compiled_with_xpu():
        device_id = int(os.getenv('FLAGS_selected_xpus', '0'))
        place = fluid.XPUPlace(device_id)
    else:
        raise ValueError('fleet dygraph api must in paddlepaddle-xpu or paddlepaddle-gpu.')
    exe = fluid.Executor(place)
    exe.run(fluid.default_startup_program())
    eprint(type(self).__name__, 'run worker startup program done.')
    feed_var_list = [var for var in trainer_prog.global_block().vars.values() if var.is_data]
    eprint('feed_var_list:', feed_var_list)
    if feed_var_list[0].name == 'label':
        feed_var_list = feed_var_list[::-1]
    feeder = fluid.DataFeeder(feed_var_list, place)
    reader_generator = train_reader()

    def get_data():
        origin_batch = next(reader_generator)
        if args.update_method != 'local' and args.use_reader_alloc:
            new_batch = []
            for (offset, item) in enumerate(origin_batch):
                if offset % 2 == args.trainer_id:
                    new_batch.append(item)
            return new_batch
        else:
            return origin_batch
    print_to_err(type(self).__name__, 'begin to train on trainer')
    out_losses = []
    for i in range(RUN_STEP):
        (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
        out_losses.append(loss[0])
        print_to_err(type(self).__name__, 'run step %d finished' % i)
    print_to_err(type(self).__name__, 'trainer run finished')
    sys.stdout.buffer.write(pickle.dumps(out_losses))
    if args.save_model:
        model_save_dir = '/tmp'
        if fleet.worker_index() == 0:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer')
        else:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables_2')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables_2')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer_2')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer_2')
        paddle.distributed.io.save_persistables(exe, model_save_dir_fluid, fleet._origin_program)
        fleet.save_persistables(executor=exe, dirname=model_save_dir_fleet)
        feeded_var_names = [var.name for var in feed_var_list]
        fluid.io.save_inference_model(infer_save_dir_fluid, feeded_var_names, [avg_cost], exe, fleet._origin_program)
        fleet.save_inference_model(exe, infer_save_dir_fleet, feeded_var_names, [avg_cost])",'run step %d finished' % i,["f'run step {i} finished'"],no_found,0,,,,
pgadmin4,https://github.com/postgres/pgadmin4/tree/master/web/pgadmin/browser/server_groups/servers/databases/schemas/tables/indexes/tests/test_indexes_get_nodes.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgadmin4/web/pgadmin/browser/server_groups/servers/databases/schemas/tables/indexes/tests/test_indexes_get_nodes.py,IndexesGetTestCase,"def setUp(self):
    """""" Creating index required in further steps""""""
    self.db_name = parent_node_dict['database'][-1]['db_name']
    schema_info = parent_node_dict['schema'][-1]
    self.server_id = schema_info['server_id']
    self.db_id = schema_info['db_id']
    db_con = database_utils.connect_database(self, utils.SERVER_GROUP, self.server_id, self.db_id)
    if not db_con['data']['connected']:
        raise Exception('Could not connect to database to add a table.')
    self.schema_id = schema_info['schema_id']
    self.schema_name = schema_info['schema_name']
    schema_response = schema_utils.verify_schemas(self.server, self.db_name, self.schema_name)
    if not schema_response:
        raise Exception('Could not find the schema to add a table.')
    self.table_name = 'table_column_%s' % str(uuid.uuid4())[1:8]
    self.table_id = tables_utils.create_table(self.server, self.db_name, self.schema_name, self.table_name)
    self.column_name = 'test_column_delete_%s' % str(uuid.uuid4())[1:8]
    self.column_id = columns_utils.create_column(self.server, self.db_name, self.schema_name, self.table_name, self.column_name)
    self.index_name = 'test_index_delete_%s' % str(uuid.uuid4())[1:8]
    self.index_id = indexes_utils.create_index(self.server, self.db_name, self.schema_name, self.table_name, self.index_name, self.column_name)
    if self.is_list:
        self.index_name_1 = 'test_index_delete_%s' % str(uuid.uuid4())[1:8]
        self.index_ids = [self.index_id, indexes_utils.create_index(self.server, self.db_name, self.schema_name, self.table_name, self.index_name_1, self.column_name)]",'table_column_%s' % str(uuid.uuid4())[1:8],["f'table_column_{str(uuid.uuid4())[1:8]}'"],no_found,1,,,,
pgadmin4,https://github.com/postgres/pgadmin4/tree/master/web/pgadmin/browser/server_groups/servers/databases/schemas/tables/indexes/tests/test_indexes_get_nodes.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgadmin4/web/pgadmin/browser/server_groups/servers/databases/schemas/tables/indexes/tests/test_indexes_get_nodes.py,IndexesGetTestCase,"def setUp(self):
    """""" Creating index required in further steps""""""
    self.db_name = parent_node_dict['database'][-1]['db_name']
    schema_info = parent_node_dict['schema'][-1]
    self.server_id = schema_info['server_id']
    self.db_id = schema_info['db_id']
    db_con = database_utils.connect_database(self, utils.SERVER_GROUP, self.server_id, self.db_id)
    if not db_con['data']['connected']:
        raise Exception('Could not connect to database to add a table.')
    self.schema_id = schema_info['schema_id']
    self.schema_name = schema_info['schema_name']
    schema_response = schema_utils.verify_schemas(self.server, self.db_name, self.schema_name)
    if not schema_response:
        raise Exception('Could not find the schema to add a table.')
    self.table_name = 'table_column_%s' % str(uuid.uuid4())[1:8]
    self.table_id = tables_utils.create_table(self.server, self.db_name, self.schema_name, self.table_name)
    self.column_name = 'test_column_delete_%s' % str(uuid.uuid4())[1:8]
    self.column_id = columns_utils.create_column(self.server, self.db_name, self.schema_name, self.table_name, self.column_name)
    self.index_name = 'test_index_delete_%s' % str(uuid.uuid4())[1:8]
    self.index_id = indexes_utils.create_index(self.server, self.db_name, self.schema_name, self.table_name, self.index_name, self.column_name)
    if self.is_list:
        self.index_name_1 = 'test_index_delete_%s' % str(uuid.uuid4())[1:8]
        self.index_ids = [self.index_id, indexes_utils.create_index(self.server, self.db_name, self.schema_name, self.table_name, self.index_name_1, self.column_name)]",'test_column_delete_%s' % str(uuid.uuid4())[1:8],["f'test_column_delete_{str(uuid.uuid4())[1:8]}'"],no_found,0,,,,
pgadmin4,https://github.com/postgres/pgadmin4/tree/master/web/pgadmin/browser/server_groups/servers/databases/schemas/tables/indexes/tests/test_indexes_get_nodes.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgadmin4/web/pgadmin/browser/server_groups/servers/databases/schemas/tables/indexes/tests/test_indexes_get_nodes.py,IndexesGetTestCase,"def setUp(self):
    """""" Creating index required in further steps""""""
    self.db_name = parent_node_dict['database'][-1]['db_name']
    schema_info = parent_node_dict['schema'][-1]
    self.server_id = schema_info['server_id']
    self.db_id = schema_info['db_id']
    db_con = database_utils.connect_database(self, utils.SERVER_GROUP, self.server_id, self.db_id)
    if not db_con['data']['connected']:
        raise Exception('Could not connect to database to add a table.')
    self.schema_id = schema_info['schema_id']
    self.schema_name = schema_info['schema_name']
    schema_response = schema_utils.verify_schemas(self.server, self.db_name, self.schema_name)
    if not schema_response:
        raise Exception('Could not find the schema to add a table.')
    self.table_name = 'table_column_%s' % str(uuid.uuid4())[1:8]
    self.table_id = tables_utils.create_table(self.server, self.db_name, self.schema_name, self.table_name)
    self.column_name = 'test_column_delete_%s' % str(uuid.uuid4())[1:8]
    self.column_id = columns_utils.create_column(self.server, self.db_name, self.schema_name, self.table_name, self.column_name)
    self.index_name = 'test_index_delete_%s' % str(uuid.uuid4())[1:8]
    self.index_id = indexes_utils.create_index(self.server, self.db_name, self.schema_name, self.table_name, self.index_name, self.column_name)
    if self.is_list:
        self.index_name_1 = 'test_index_delete_%s' % str(uuid.uuid4())[1:8]
        self.index_ids = [self.index_id, indexes_utils.create_index(self.server, self.db_name, self.schema_name, self.table_name, self.index_name_1, self.column_name)]",'test_index_delete_%s' % str(uuid.uuid4())[1:8],["f'test_index_delete_{str(uuid.uuid4())[1:8]}'"],no_found,0,,,,
pgadmin4,https://github.com/postgres/pgadmin4/tree/master/web/pgadmin/browser/server_groups/servers/databases/schemas/tables/indexes/tests/test_indexes_get_nodes.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgadmin4/web/pgadmin/browser/server_groups/servers/databases/schemas/tables/indexes/tests/test_indexes_get_nodes.py,IndexesGetTestCase,"def setUp(self):
    """""" Creating index required in further steps""""""
    self.db_name = parent_node_dict['database'][-1]['db_name']
    schema_info = parent_node_dict['schema'][-1]
    self.server_id = schema_info['server_id']
    self.db_id = schema_info['db_id']
    db_con = database_utils.connect_database(self, utils.SERVER_GROUP, self.server_id, self.db_id)
    if not db_con['data']['connected']:
        raise Exception('Could not connect to database to add a table.')
    self.schema_id = schema_info['schema_id']
    self.schema_name = schema_info['schema_name']
    schema_response = schema_utils.verify_schemas(self.server, self.db_name, self.schema_name)
    if not schema_response:
        raise Exception('Could not find the schema to add a table.')
    self.table_name = 'table_column_%s' % str(uuid.uuid4())[1:8]
    self.table_id = tables_utils.create_table(self.server, self.db_name, self.schema_name, self.table_name)
    self.column_name = 'test_column_delete_%s' % str(uuid.uuid4())[1:8]
    self.column_id = columns_utils.create_column(self.server, self.db_name, self.schema_name, self.table_name, self.column_name)
    self.index_name = 'test_index_delete_%s' % str(uuid.uuid4())[1:8]
    self.index_id = indexes_utils.create_index(self.server, self.db_name, self.schema_name, self.table_name, self.index_name, self.column_name)
    if self.is_list:
        self.index_name_1 = 'test_index_delete_%s' % str(uuid.uuid4())[1:8]
        self.index_ids = [self.index_id, indexes_utils.create_index(self.server, self.db_name, self.schema_name, self.table_name, self.index_name_1, self.column_name)]",'test_index_delete_%s' % str(uuid.uuid4())[1:8],["f'test_index_delete_{str(uuid.uuid4())[1:8]}'"],no_found,0,,,,
FedML,https://github.com/FedML-AI/FedML/tree/master/fedml_api/model/cv/efficientnet_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FedML/fedml_api/model/cv/efficientnet_utils.py,BlockDecoder,"def _encode_block_string(block):
    """"""Encode a block to a string.
        Args:
            block (namedtuple): A BlockArgs type argument.
        Returns:
            block_string: A String form of BlockArgs.
        """"""
    args = ['r%d' % block.num_repeat, 'k%d' % block.kernel_size, 's%d%d' % (block.strides[0], block.strides[1]), 'e%s' % block.expand_ratio, 'i%d' % block.input_filters, 'o%d' % block.output_filters]
    if 0 < block.se_ratio <= 1:
        args.append('se%s' % block.se_ratio)
    if block.id_skip is False:
        args.append('noskip')
    return '_'.join(args)",'k%d' % block.kernel_size,["f'k{block.kernel_size}'"],no_found,0,,,,
geopy,https://github.com/geopy/geopy/tree/master/geopy/geocoders/smartystreets.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/geopy/geopy/geocoders/smartystreets.py,LiveAddress,"def __init__(self, auth_id, auth_token, *, timeout=DEFAULT_SENTINEL, proxies=DEFAULT_SENTINEL, user_agent=None, ssl_context=DEFAULT_SENTINEL, adapter_factory=None):
    """"""

        :param str auth_id: Valid `Auth ID` from SmartyStreets.

        :param str auth_token: Valid `Auth Token` from SmartyStreets.

        :param int timeout:
            See :attr:`geopy.geocoders.options.default_timeout`.

        :param dict proxies:
            See :attr:`geopy.geocoders.options.default_proxies`.

        :param str user_agent:
            See :attr:`geopy.geocoders.options.default_user_agent`.

        :type ssl_context: :class:`ssl.SSLContext`
        :param ssl_context:
            See :attr:`geopy.geocoders.options.default_ssl_context`.

        :param callable adapter_factory:
            See :attr:`geopy.geocoders.options.default_adapter_factory`.

            .. versionadded:: 2.0
        """"""
    super().__init__(scheme='https', timeout=timeout, proxies=proxies, user_agent=user_agent, ssl_context=ssl_context, adapter_factory=adapter_factory)
    self.auth_id = auth_id
    self.auth_token = auth_token
    domain = 'api.smartystreets.com'
    self.api = '%s://%s%s' % (self.scheme, domain, self.geocode_path)","'%s://%s%s' % (self.scheme, domain, self.geocode_path)",["f'{self.scheme}://{domain}{self.geocode_path}'"],no_found,1,,,,
shuup,https://github.com/shuup/shuup/tree/master/shuup/admin/modules/products/views/edit_media.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/admin/modules/products/views/edit_media.py,ProductMediaBulkAdderView,"def post(self, *args, **kwargs):
    ids = self.request.POST.getlist('file_ids')
    shop_product_id = kwargs.pop('pk')
    kind = self.request.POST.get('kind')
    shop = self.request.shop
    shop_id = self.request.POST.get('shop_id', shop.pk)
    if not ids or not shop_product_id:
        return JsonResponse({'response': 'error', 'message': 'Error! Bad request.'}, status=400)
    if not Shop.objects.filter(pk=shop_id).exists():
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop id `%s`.' % shop_id}, status=400)
    shop_product = ShopProduct.objects.filter(pk=shop_product_id, shop_id=shop_id).first()
    if not shop_product:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop product id `%s`.' % shop_product_id}, status=400)
    if kind == 'images':
        kind = ProductMediaKind.IMAGE
    elif kind == 'media':
        kind = ProductMediaKind.GENERIC_FILE
    else:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid file kind `%s`.' % kind}, status=400)
    for file_id in ids:
        if not File.objects.filter(id=file_id).exists():
            return JsonResponse({'response': 'error', 'message': 'Error! Invalid file id `%s`.' % file_id}, status=400)
    added = []
    for file_id in ids:
        if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
            image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
            image.shops.add(shop_id)
            added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})
    return JsonResponse({'response': 'success', 'added': added, 'message': force_text(_('Files added to the product.'))})",'Error! Invalid file id `%s`.' % file_id,["f'Error! Invalid file id `{file_id}`.'"],no_found,0,,,,
Transformer-TTS,https://github.com/soobinseo/Transformer-TTS/tree/master//train_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer-TTS//train_transformer.py,,"def main():
    dataset = get_dataset()
    global_step = 0
    m = nn.DataParallel(Model().cuda())
    m.train()
    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)
    pos_weight = t.FloatTensor([5.0]).cuda()
    writer = SummaryWriter()
    for epoch in range(hp.epochs):
        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
        pbar = tqdm(dataloader)
        for (i, data) in enumerate(pbar):
            pbar.set_description('Processing at epoch %d' % epoch)
            global_step += 1
            if global_step < 400000:
                adjust_learning_rate(optimizer, global_step)
            (character, mel, mel_input, pos_text, pos_mel, _) = data
            stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
            character = character.cuda()
            mel = mel.cuda()
            mel_input = mel_input.cuda()
            pos_text = pos_text.cuda()
            pos_mel = pos_mel.cuda()
            (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
            mel_loss = nn.L1Loss()(mel_pred, mel)
            post_mel_loss = nn.L1Loss()(postnet_pred, mel)
            loss = mel_loss + post_mel_loss
            writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
            writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
            if global_step % hp.image_step == 1:
                for (i, prob) in enumerate(attn_probs):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_enc):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_dec):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(m.parameters(), 1.0)
            optimizer.step()
            if global_step % hp.save_step == 0:
                t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))",'checkpoint_transformer_%d.pth.tar' % global_step,["f'checkpoint_transformer_{global_step}.pth.tar'"],no_found,0,,,,
Transformer-TTS,https://github.com/soobinseo/Transformer-TTS/tree/master//train_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer-TTS//train_transformer.py,,"def main():
    dataset = get_dataset()
    global_step = 0
    m = nn.DataParallel(Model().cuda())
    m.train()
    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)
    pos_weight = t.FloatTensor([5.0]).cuda()
    writer = SummaryWriter()
    for epoch in range(hp.epochs):
        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
        pbar = tqdm(dataloader)
        for (i, data) in enumerate(pbar):
            pbar.set_description('Processing at epoch %d' % epoch)
            global_step += 1
            if global_step < 400000:
                adjust_learning_rate(optimizer, global_step)
            (character, mel, mel_input, pos_text, pos_mel, _) = data
            stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
            character = character.cuda()
            mel = mel.cuda()
            mel_input = mel_input.cuda()
            pos_text = pos_text.cuda()
            pos_mel = pos_mel.cuda()
            (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
            mel_loss = nn.L1Loss()(mel_pred, mel)
            post_mel_loss = nn.L1Loss()(postnet_pred, mel)
            loss = mel_loss + post_mel_loss
            writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
            writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
            if global_step % hp.image_step == 1:
                for (i, prob) in enumerate(attn_probs):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_enc):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_dec):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(m.parameters(), 1.0)
            optimizer.step()
            if global_step % hp.save_step == 0:
                t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))",'Attention_%d_0' % global_step,["f'Attention_{global_step}_0'"],no_found,0,,,,
Transformer-TTS,https://github.com/soobinseo/Transformer-TTS/tree/master//train_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer-TTS//train_transformer.py,,"def main():
    dataset = get_dataset()
    global_step = 0
    m = nn.DataParallel(Model().cuda())
    m.train()
    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)
    pos_weight = t.FloatTensor([5.0]).cuda()
    writer = SummaryWriter()
    for epoch in range(hp.epochs):
        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
        pbar = tqdm(dataloader)
        for (i, data) in enumerate(pbar):
            pbar.set_description('Processing at epoch %d' % epoch)
            global_step += 1
            if global_step < 400000:
                adjust_learning_rate(optimizer, global_step)
            (character, mel, mel_input, pos_text, pos_mel, _) = data
            stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
            character = character.cuda()
            mel = mel.cuda()
            mel_input = mel_input.cuda()
            pos_text = pos_text.cuda()
            pos_mel = pos_mel.cuda()
            (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
            mel_loss = nn.L1Loss()(mel_pred, mel)
            post_mel_loss = nn.L1Loss()(postnet_pred, mel)
            loss = mel_loss + post_mel_loss
            writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
            writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
            if global_step % hp.image_step == 1:
                for (i, prob) in enumerate(attn_probs):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_enc):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_dec):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(m.parameters(), 1.0)
            optimizer.step()
            if global_step % hp.save_step == 0:
                t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))",'Attention_enc_%d_0' % global_step,["f'Attention_enc_{global_step}_0'"],no_found,0,,,,
Transformer-TTS,https://github.com/soobinseo/Transformer-TTS/tree/master//train_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer-TTS//train_transformer.py,,"def main():
    dataset = get_dataset()
    global_step = 0
    m = nn.DataParallel(Model().cuda())
    m.train()
    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)
    pos_weight = t.FloatTensor([5.0]).cuda()
    writer = SummaryWriter()
    for epoch in range(hp.epochs):
        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
        pbar = tqdm(dataloader)
        for (i, data) in enumerate(pbar):
            pbar.set_description('Processing at epoch %d' % epoch)
            global_step += 1
            if global_step < 400000:
                adjust_learning_rate(optimizer, global_step)
            (character, mel, mel_input, pos_text, pos_mel, _) = data
            stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
            character = character.cuda()
            mel = mel.cuda()
            mel_input = mel_input.cuda()
            pos_text = pos_text.cuda()
            pos_mel = pos_mel.cuda()
            (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
            mel_loss = nn.L1Loss()(mel_pred, mel)
            post_mel_loss = nn.L1Loss()(postnet_pred, mel)
            loss = mel_loss + post_mel_loss
            writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
            writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
            if global_step % hp.image_step == 1:
                for (i, prob) in enumerate(attn_probs):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_enc):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_dec):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(m.parameters(), 1.0)
            optimizer.step()
            if global_step % hp.save_step == 0:
                t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))",'Attention_dec_%d_0' % global_step,["f'Attention_dec_{global_step}_0'"],no_found,0,,,,
trackma,https://github.com/z411/trackma/tree/master/trackma/ui/qt/accounts.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/trackma/trackma/ui/qt/accounts.py,AccountAddDialog,"def s_request_pin(self):
    auth_url = self.adding_api[3]
    if self.adding_api[2] == utils.LOGIN_OAUTH_PKCE:
        self.adding_extra = {'code_verifier': utils.oauth_generate_pkce()}
        auth_url = auth_url % self.adding_extra['code_verifier']
    self.adding_allow = True
    QtGui.QDesktopServices.openUrl(QtCore.QUrl(auth_url))",auth_url % self.adding_extra['code_verifier'],['f"{auth_url}{self.adding_extra[\'code_verifier\']}"'],no_found,0,0,-1,0,
keystone,https://github.com/openstack/keystone/tree/master/keystone/tests/unit/test_v3_resource.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keystone/keystone/tests/unit/test_v3_resource.py,ResourceTestCase,"def test_get_project_with_subtree_as_list_and_subtree_as_ids(self):
    """"""Attempt to get a project subtree as both a list and as IDs.

        This uses ``GET /projects/{project_id}?subtree_as_list&subtree_as_ids``
        which should fail with a bad request due to the conflicting query
        strings.

        """"""
    projects = self._create_projects_hierarchy(hierarchy_size=2)
    self.get('/projects/%(project_id)s?subtree_as_list&subtree_as_ids' % {'project_id': projects[1]['project']['id']}, expected_status=http.client.BAD_REQUEST)",'/projects/%(project_id)s?subtree_as_list&subtree_as_ids' % {'project_id': projects[1]['project']['id']},['f"/projects/{projects[1][\'project\'][\'id\']}?subtree_as_list&subtree_as_ids"'],no_found,1,,,,
aws-cli,https://github.com/aws/aws-cli/tree/master/awscli/customizations/cloudtrail/validation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-cli/awscli/customizations/cloudtrail/validation.py,CloudTrailValidateLogs,"def _write_summary_text(self):
    if not self._is_last_status_double_space:
        sys.stdout.write('\n')
    sys.stdout.write('Results requested for %s to %s\n' % (format_display_date(self.start_time), format_display_date(self.end_time)))
    if not self._valid_digests and (not self._invalid_digests):
        sys.stdout.write('No digests found\n')
        return
    if not self._found_start_time or not self._found_end_time:
        sys.stdout.write('No valid digests found in range\n')
    else:
        sys.stdout.write('Results found for %s to %s:\n' % (format_display_date(self._found_start_time), format_display_date(self._found_end_time)))
    self._write_ratio(self._valid_digests, self._invalid_digests, 'digest')
    self._write_ratio(self._valid_logs, self._invalid_logs, 'log')
    sys.stdout.write('\n')","'Results requested for %s to %s\n' % (format_display_date(self.start_time), format_display_date(self.end_time))",["f'Results requested for {format_display_date(self.start_time)} to {format_display_date(self.end_time)}\\n'"],no_found,-1,,,,
aws-cli,https://github.com/aws/aws-cli/tree/master/awscli/customizations/cloudtrail/validation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-cli/awscli/customizations/cloudtrail/validation.py,CloudTrailValidateLogs,"def _write_summary_text(self):
    if not self._is_last_status_double_space:
        sys.stdout.write('\n')
    sys.stdout.write('Results requested for %s to %s\n' % (format_display_date(self.start_time), format_display_date(self.end_time)))
    if not self._valid_digests and (not self._invalid_digests):
        sys.stdout.write('No digests found\n')
        return
    if not self._found_start_time or not self._found_end_time:
        sys.stdout.write('No valid digests found in range\n')
    else:
        sys.stdout.write('Results found for %s to %s:\n' % (format_display_date(self._found_start_time), format_display_date(self._found_end_time)))
    self._write_ratio(self._valid_digests, self._invalid_digests, 'digest')
    self._write_ratio(self._valid_logs, self._invalid_logs, 'log')
    sys.stdout.write('\n')","'Results found for %s to %s:\n' % (format_display_date(self._found_start_time), format_display_date(self._found_end_time))",["f'Results found for {format_display_date(self._found_start_time)} to {format_display_date(self._found_end_time)}:\\n'"],no_found,-1,,,,
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))",'Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)),['f"Deprecation validation issues with \\n\\t{\'\\n\\t\'.join(sorted(issues))}"'],no_found,0,,,,
numpy,https://github.com/numpy/numpy/tree/master/numpy/distutils/command/install.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/distutils/command/install.py,install,"def run(self):
    if not have_setuptools:
        r = old_install.run(self)
    else:
        r = self.setuptools_run()
    if self.record:
        with open(self.record, 'r') as f:
            lines = []
            need_rewrite = False
            for l in f:
                l = l.rstrip()
                if ' ' in l:
                    need_rewrite = True
                    l = '""%s""' % l
                lines.append(l)
        if need_rewrite:
            self.execute(write_file, (self.record, lines), ""re-writing list of installed files to '%s'"" % self.record)
    return r","""re-writing list of installed files to '%s'"" % self.record",['f"re-writing list of installed files to \'{self.record}\'"'],no_found,0,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/once.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/once.py,OnceIE,"def _extract_once_formats(self, url, http_formats_preference=None):
    (domain_id, application_id, media_item_id) = re.match(OnceIE._VALID_URL, url).groups()
    formats = self._extract_m3u8_formats(self.ADAPTIVE_URL_TEMPLATE % (domain_id, application_id, media_item_id), media_item_id, 'mp4', m3u8_id='hls', fatal=False)
    progressive_formats = []
    for adaptive_format in formats:
        adaptive_format['url'] = re.sub('\\badsegmentlength=\\d+', 'adsegmentlength=0', adaptive_format['url'])
        rendition_id = self._search_regex('/now/media/playlist/[^/]+/[^/]+/([^/]+)', adaptive_format['url'], 'redition id', default=None)
        if rendition_id:
            progressive_format = adaptive_format.copy()
            progressive_format.update({'url': self.PROGRESSIVE_URL_TEMPLATE % (domain_id, application_id, rendition_id, media_item_id), 'format_id': adaptive_format['format_id'].replace('hls', 'http'), 'protocol': 'http', 'preference': http_formats_preference})
            progressive_formats.append(progressive_format)
    self._check_formats(progressive_formats, media_item_id)
    formats.extend(progressive_formats)
    return formats","self.ADAPTIVE_URL_TEMPLATE % (domain_id, application_id, media_item_id)",["f'{self.ADAPTIVE_URL_TEMPLATE}{domain_id}/{application_id}/{media_item_id}'"],no_found,0,0,-1,0,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/once.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/once.py,OnceIE,"def _extract_once_formats(self, url, http_formats_preference=None):
    (domain_id, application_id, media_item_id) = re.match(OnceIE._VALID_URL, url).groups()
    formats = self._extract_m3u8_formats(self.ADAPTIVE_URL_TEMPLATE % (domain_id, application_id, media_item_id), media_item_id, 'mp4', m3u8_id='hls', fatal=False)
    progressive_formats = []
    for adaptive_format in formats:
        adaptive_format['url'] = re.sub('\\badsegmentlength=\\d+', 'adsegmentlength=0', adaptive_format['url'])
        rendition_id = self._search_regex('/now/media/playlist/[^/]+/[^/]+/([^/]+)', adaptive_format['url'], 'redition id', default=None)
        if rendition_id:
            progressive_format = adaptive_format.copy()
            progressive_format.update({'url': self.PROGRESSIVE_URL_TEMPLATE % (domain_id, application_id, rendition_id, media_item_id), 'format_id': adaptive_format['format_id'].replace('hls', 'http'), 'protocol': 'http', 'preference': http_formats_preference})
            progressive_formats.append(progressive_format)
    self._check_formats(progressive_formats, media_item_id)
    formats.extend(progressive_formats)
    return formats","self.PROGRESSIVE_URL_TEMPLATE % (domain_id, application_id, rendition_id, media_item_id)",["f'{self.PROGRESSIVE_URL_TEMPLATE}{domain_id}/{application_id}/{rendition_id}/{media_item_id}'"],no_found,0,0,-1,0,
pyannote-audio,https://github.com/pyannote/pyannote-audio/tree/master/pyannote/audio/_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyannote-audio/pyannote/audio/_version.py,,"def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):
    if not os.path.exists(os.path.join(root, '.git')):
        if verbose:
            print('no .git in %s' % root)
        raise NotThisMethod('no .git directory')
    GITS = ['git']
    if sys.platform == 'win32':
        GITS = ['git.cmd', 'git.exe']
    describe_out = run_command(GITS, ['describe', '--tags', '--dirty', '--always', '--long'], cwd=root)
    if describe_out is None:
        raise NotThisMethod(""'git describe' failed"")
    describe_out = describe_out.strip()
    full_out = run_command(GITS, ['rev-parse', 'HEAD'], cwd=root)
    if full_out is None:
        raise NotThisMethod(""'git rev-parse' failed"")
    full_out = full_out.strip()
    pieces = {}
    pieces['long'] = full_out
    pieces['short'] = full_out[:7]
    pieces['error'] = None
    git_describe = describe_out
    dirty = git_describe.endswith('-dirty')
    pieces['dirty'] = dirty
    if dirty:
        git_describe = git_describe[:git_describe.rindex('-dirty')]
    if '-' in git_describe:
        mo = re.search('^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)
        if not mo:
            pieces['error'] = ""unable to parse git-describe output: '%s'"" % describe_out
            return pieces
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = ""tag '%s' doesn't start with prefix '%s'""
                print(fmt % (full_tag, tag_prefix))
            pieces['error'] = ""tag '%s' doesn't start with prefix '%s'"" % (full_tag, tag_prefix)
            return pieces
        pieces['closest-tag'] = full_tag[len(tag_prefix):]
        pieces['distance'] = int(mo.group(2))
        pieces['short'] = mo.group(3)
    else:
        pieces['closest-tag'] = None
        count_out = run_command(GITS, ['rev-list', 'HEAD', '--count'], cwd=root)
        pieces['distance'] = int(count_out)
    return pieces","""tag '%s' doesn't start with prefix '%s'"" % (full_tag, tag_prefix)",['f"tag \'{full_tag}\' doesn\'t start with prefix \'{tag_prefix}\'"'],no_found,0,,,,
sprutio,https://github.com/LTD-Beget/sprutio/tree/master/app/classes/core/FMAuth.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sprutio/app/classes/core/FMAuth.py,FMAuth,"def authenticate_by_token(request, token):
    redis = request.redis.get(threading.currentThread())
    ':type : connectors.RedisConnector.RedisConnector'
    if redis.exists(str(token)):
        try:
            params = redis.get(token)
            params = json.loads(str(params))
            redis.set(token, json.dumps(params))
            request.set_secure_cookie(DEFAULT_COOKIE_TOKEN_NAME, token, COOKIE_EXPIRE)
            return token
        except Exception as e:
            request.application.logger.error('Error in FMAuth: %s, traceback = %s' % (str(e), traceback.format_exc()))
            return False
    return False","'Error in FMAuth: %s, traceback = %s' % (str(e), traceback.format_exc())","[""f'Error in FMAuth: {str(e)}, traceback = {traceback.format_exc()}'""]",no_found,0,,,,
lingvo,https://github.com/tensorflow/lingvo/tree/master/lingvo/core/batch_major_attention.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lingvo/lingvo/core/batch_major_attention.py,FunnelUpsampleLayer,"def FProp(self, theta, x, all_hiddens=None):
    """"""Upsample to the inputs.

    Args:
      theta: weights defined in this layer.
      x: input tensor, [batch, time, dim] upsampling is applied to the time dim.
      all_hiddens: None or the list of hiddens states from all encoder layers,
        where each hidden state is a NestedMap with 'vec' and 'padding' keys.
        See the Builder class below for more details.

    Returns:
      Upsampled tensor, with the upsampling applied to the second dim in x.
    """"""
    p = self.params
    if x.shape.ndims is not None and x.shape.ndims != 3:
        raise ValueError('FunnelUpsampleLayer expects input to be rank 3, but got %d' % x.shape.ndims)
    if p.upsample_type not in ['REPEAT', 'DECONV']:
        raise ValueError('Only supports upsample_type REPEAT and DECONV, but got %s' % p.upsample_type)
    assert isinstance(p.upsample_rate, int)
    if p.upsample_rate == 1:
        return x
    if p.begin_intact > 0:
        intact = x[:, :p.begin_intact]
        hid = x[:, p.begin_intact:]
    else:
        hid = x
    if p.upsample_type == 'REPEAT':
        upsampled = tf.repeat(hid, repeats=p.upsample_rate, axis=1)
    elif p.upsample_type == 'DECONV':
        upsampled = tf.einsum('BLD,DNH->BLNH', hid, theta.weight)
        (bsz, seq_len) = py_utils.GetShape(hid, 3)[:2]
        upsampled = tf.reshape(upsampled, [bsz, p.upsample_rate * seq_len, p.hidden_dim])
    if p.begin_intact > 0:
        sep_len = 1
        if p.trunc_seq:
            num_pad = p.begin_intact * p.upsample_rate - p.begin_intact
            upsampled = tf.pad(upsampled, [[0, 0], [0, num_pad], [0, 0]])
        else:
            upsampled = upsampled[:, :-sep_len]
        upsampled = tf.concat([intact, upsampled], axis=1, name='concat_upsampled')
    if p.shortcut_index is not None:
        assert all_hiddens, 'all_hiddens must be provided for shortcut.'
        upsampled_shape = tf.shape(upsampled)
        shortcut_shape = tf.shape(all_hiddens[p.shortcut_index].vec)
        upsampled = py_utils.with_dependencies([py_utils.assert_shape_match(upsampled_shape, shortcut_shape)], upsampled + all_hiddens[p.shortcut_index].vec)
        if p.decoder_stack is not None:
            decoder_input = py_utils.NestedMap(vec=upsampled, paddings=all_hiddens[p.shortcut_index].paddings)
            upsampled = self.decoder_stack(decoder_input).vec
    return upsampled","'FunnelUpsampleLayer expects input to be rank 3, but got %d' % x.shape.ndims","[""f'FunnelUpsampleLayer expects input to be rank 3, but got {x.shape.ndims}'""]",no_found,0,,,,
lingvo,https://github.com/tensorflow/lingvo/tree/master/lingvo/core/batch_major_attention.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lingvo/lingvo/core/batch_major_attention.py,FunnelUpsampleLayer,"def FProp(self, theta, x, all_hiddens=None):
    """"""Upsample to the inputs.

    Args:
      theta: weights defined in this layer.
      x: input tensor, [batch, time, dim] upsampling is applied to the time dim.
      all_hiddens: None or the list of hiddens states from all encoder layers,
        where each hidden state is a NestedMap with 'vec' and 'padding' keys.
        See the Builder class below for more details.

    Returns:
      Upsampled tensor, with the upsampling applied to the second dim in x.
    """"""
    p = self.params
    if x.shape.ndims is not None and x.shape.ndims != 3:
        raise ValueError('FunnelUpsampleLayer expects input to be rank 3, but got %d' % x.shape.ndims)
    if p.upsample_type not in ['REPEAT', 'DECONV']:
        raise ValueError('Only supports upsample_type REPEAT and DECONV, but got %s' % p.upsample_type)
    assert isinstance(p.upsample_rate, int)
    if p.upsample_rate == 1:
        return x
    if p.begin_intact > 0:
        intact = x[:, :p.begin_intact]
        hid = x[:, p.begin_intact:]
    else:
        hid = x
    if p.upsample_type == 'REPEAT':
        upsampled = tf.repeat(hid, repeats=p.upsample_rate, axis=1)
    elif p.upsample_type == 'DECONV':
        upsampled = tf.einsum('BLD,DNH->BLNH', hid, theta.weight)
        (bsz, seq_len) = py_utils.GetShape(hid, 3)[:2]
        upsampled = tf.reshape(upsampled, [bsz, p.upsample_rate * seq_len, p.hidden_dim])
    if p.begin_intact > 0:
        sep_len = 1
        if p.trunc_seq:
            num_pad = p.begin_intact * p.upsample_rate - p.begin_intact
            upsampled = tf.pad(upsampled, [[0, 0], [0, num_pad], [0, 0]])
        else:
            upsampled = upsampled[:, :-sep_len]
        upsampled = tf.concat([intact, upsampled], axis=1, name='concat_upsampled')
    if p.shortcut_index is not None:
        assert all_hiddens, 'all_hiddens must be provided for shortcut.'
        upsampled_shape = tf.shape(upsampled)
        shortcut_shape = tf.shape(all_hiddens[p.shortcut_index].vec)
        upsampled = py_utils.with_dependencies([py_utils.assert_shape_match(upsampled_shape, shortcut_shape)], upsampled + all_hiddens[p.shortcut_index].vec)
        if p.decoder_stack is not None:
            decoder_input = py_utils.NestedMap(vec=upsampled, paddings=all_hiddens[p.shortcut_index].paddings)
            upsampled = self.decoder_stack(decoder_input).vec
    return upsampled","'Only supports upsample_type REPEAT and DECONV, but got %s' % p.upsample_type","[""f'Only supports upsample_type REPEAT and DECONV, but got {p.upsample_type}'""]",no_found,0,,,,
django,https://github.com/django/django/tree/master/tests/gis_tests/test_geoip2.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django/tests/gis_tests/test_geoip2.py,GeoIPTest,"def test_repr(self):
    path = settings.GEOIP_PATH
    g = GeoIP2(path=path)
    meta = g._reader.metadata()
    version = '%s.%s' % (meta.binary_format_major_version, meta.binary_format_minor_version)
    country_path = g._country_file
    city_path = g._city_file
    expected = '<GeoIP2 [v%(version)s] _country_file=""%(country)s"", _city_file=""%(city)s"">' % {'version': version, 'country': country_path, 'city': city_path}
    self.assertEqual(repr(g), expected)","'%s.%s' % (meta.binary_format_major_version, meta.binary_format_minor_version)",["f'{meta.binary_format_major_version}.{meta.binary_format_minor_version}'"],no_found,1,,,,
django,https://github.com/django/django/tree/master/tests/gis_tests/test_geoip2.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django/tests/gis_tests/test_geoip2.py,GeoIPTest,"def test_repr(self):
    path = settings.GEOIP_PATH
    g = GeoIP2(path=path)
    meta = g._reader.metadata()
    version = '%s.%s' % (meta.binary_format_major_version, meta.binary_format_minor_version)
    country_path = g._country_file
    city_path = g._city_file
    expected = '<GeoIP2 [v%(version)s] _country_file=""%(country)s"", _city_file=""%(city)s"">' % {'version': version, 'country': country_path, 'city': city_path}
    self.assertEqual(repr(g), expected)","'<GeoIP2 [v%(version)s] _country_file=""%(country)s"", _city_file=""%(city)s"">' % {'version': version, 'country': country_path, 'city': city_path}","['f\'<GeoIP2 [v{version}] _country_file=""{country_path}"", _city_file=""{city_path}"">\'']",no_found,0,,,,
jetson_stats,https://github.com/rbonghi/jetson_stats/tree/master/jtop/tests/test_fan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jetson_stats/jtop/tests/test_fan.py,,"def copyDirectory(src, dest):
    try:
        shutil.copytree(src, dest)
    except shutil.Error as e:
        print('Directory not copied. Error: %s' % e)
    except OSError as e:
        print('Directory not copied. Error: %s' % e)",'Directory not copied. Error: %s' % e,["f'Directory not copied. Error: {e}'"],no_found,1,,,,
jetson_stats,https://github.com/rbonghi/jetson_stats/tree/master/jtop/tests/test_fan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jetson_stats/jtop/tests/test_fan.py,,"def copyDirectory(src, dest):
    try:
        shutil.copytree(src, dest)
    except shutil.Error as e:
        print('Directory not copied. Error: %s' % e)
    except OSError as e:
        print('Directory not copied. Error: %s' % e)",'Directory not copied. Error: %s' % e,["f'Directory not copied. Error: {e}'"],no_found,0,,,,
mitmproxy,https://github.com/mitmproxy/mitmproxy/tree/master/mitmproxy/addons/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mitmproxy/mitmproxy/addons/core.py,Core,"def revert(self, flows: typing.Sequence[flow.Flow]) -> None:
    """"""
            Revert flow changes.
        """"""
    updated = []
    for f in flows:
        if f.modified():
            f.revert()
            updated.append(f)
    ctx.log.alert('Reverted %s flows.' % len(updated))
    ctx.master.addons.trigger(hooks.UpdateHook(updated))",'Reverted %s flows.' % len(updated),["f'Reverted {len(updated)} flows.'"],no_found,1,,,,
TensorFlow-and-DeepLearning-Tutorial,https://github.com/CreatCodeBuild/TensorFlow-and-DeepLearning-Tutorial/tree/master/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorFlow-and-DeepLearning-Tutorial/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,Network,"def run(self, data_iterator, train_samples, train_labels, test_samples, test_labels):
    """"""
        Session
        :data_iterator: a function that yields chuck of data
        """"""

    def print_confusion_matrix(confusionMatrix):
        print('Confusion    Matrix:')
        for (i, line) in enumerate(confusionMatrix):
            print(line, line[i] / np.sum(line))
        a = 0
        for (i, column) in enumerate(np.transpose(confusionMatrix, (1, 0))):
            a += column[i] / np.sum(column) * (np.sum(column) / 26000)
            print(column[i] / np.sum(column))
        print('\n', np.sum(confusionMatrix), a)
    self.writer = tf.summary.FileWriter('./board', tf.get_default_graph())
    with tf.Session(graph=tf.get_default_graph()) as session:
        tf.initialize_all_variables().run()
        print('Start Training')
        for (i, samples, labels) in data_iterator(train_samples, train_labels, self.train_batch_size):
            (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
            self.writer.add_summary(summary, i)
            (accuracy, _) = self.accuracy(predictions, labels)
            if i % 50 == 0:
                print('Minibatch loss at step %d: %f' % (i, l))
                print('Minibatch accuracy: %.1f%%' % accuracy)
        accuracies = []
        confusionMatrices = []
        for (i, samples, labels) in data_iterator(test_samples, test_labels, self.test_batch_size):
            print('samples shape', samples.shape)
            (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
            self.writer.add_summary(summary, i)
            (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
            accuracies.append(accuracy)
            confusionMatrices.append(cm)
            print('Test Accuracy: %.1f%%' % accuracy)
        print(' Average  Accuracy:', np.average(accuracies))
        print('Standard Deviation:', np.std(accuracies))
        print_confusion_matrix(np.add.reduce(confusionMatrices))",'Test Accuracy: %.1f%%' % accuracy,["f'Test Accuracy: {accuracy:.1f}%'"],no_found,0,,,,
ansible-modules-extras,https://github.com/ansible/ansible-modules-extras/tree/master/database/mssql/mssql_db.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-extras/database/mssql/mssql_db.py,,"def db_import(conn, cursor, module, db, target):
    if os.path.isfile(target):
        backup = open(target, 'r')
        try:
            sqlQuery = 'USE [%s]\n' % db
            for line in backup:
                if line is None:
                    break
                elif line.startswith('GO'):
                    cursor.execute(sqlQuery)
                    sqlQuery = 'USE [%s]\n' % db
                else:
                    sqlQuery += line
            cursor.execute(sqlQuery)
            conn.commit()
        finally:
            backup.close()
        return (0, 'import successful', '')
    else:
        return (1, 'cannot find target file', 'cannot find target file')",'USE [%s]\n' % db,["f'USE [{db}]\\n'"],no_found,1,,,,
ansible-modules-extras,https://github.com/ansible/ansible-modules-extras/tree/master/database/mssql/mssql_db.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-extras/database/mssql/mssql_db.py,,"def db_import(conn, cursor, module, db, target):
    if os.path.isfile(target):
        backup = open(target, 'r')
        try:
            sqlQuery = 'USE [%s]\n' % db
            for line in backup:
                if line is None:
                    break
                elif line.startswith('GO'):
                    cursor.execute(sqlQuery)
                    sqlQuery = 'USE [%s]\n' % db
                else:
                    sqlQuery += line
            cursor.execute(sqlQuery)
            conn.commit()
        finally:
            backup.close()
        return (0, 'import successful', '')
    else:
        return (1, 'cannot find target file', 'cannot find target file')",'USE [%s]\n' % db,["f'USE [{db}]\\n'"],no_found,0,,,,
pylearn2,https://github.com/lisa-lab/pylearn2/tree/master/pylearn2/models/dbm/ising.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pylearn2/pylearn2/models/dbm/ising.py,IsingHidden,"def mf_update(self, state_below, state_above, layer_above=None, double_weights=False, iter_name=None):
    """"""
        .. todo::

            WRITEME
        """"""
    self.input_space.validate(state_below)
    if self.requires_reformat:
        if not isinstance(state_below, tuple):
            for sb in get_debug_values(state_below):
                if sb.shape[0] != self.dbm.batch_size:
                    raise ValueError('self.dbm.batch_size is %d but got ' + 'shape of %d' % (self.dbm.batch_size, sb.shape[0]))
                assert reduce(operator.mul, sb.shape[1:]) == self.input_dim
        state_below = self.input_space.format_as(state_below, self.desired_space)
    if iter_name is None:
        iter_name = 'anon'
    if state_above is not None:
        assert layer_above is not None
        msg = layer_above.downward_message(state_above)
        msg.name = 'msg_from_' + layer_above.layer_name + '_to_' + self.layer_name + '[' + iter_name + ']'
    else:
        msg = None
    if double_weights:
        state_below = 2.0 * state_below
        state_below.name = self.layer_name + '_' + iter_name + '_2state'
    z = self.transformer.lmul(state_below) + self.b
    if self.layer_name is not None and iter_name is not None:
        z.name = self.layer_name + '_' + iter_name + '_z'
    if msg is not None:
        z = z + msg
    h = T.tanh(self.beta * z)
    return h","'shape of %d' % (self.dbm.batch_size, sb.shape[0])",["f'shape of {self.dbm.batch_size}'"],no_found,0,,,,
LightNet,https://github.com/linksense/LightNet/tree/master/scripts/model_measure.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LightNet/scripts/model_measure.py,,"def measure_layer(layer, x):
    global count_ops, count_params
    delta_ops = 0
    delta_params = 0
    multi_add = 1
    type_name = get_layer_info(layer)
    if type_name in ['Conv2d']:
        out_h = int((x.size()[2] + 2 * layer.padding[0] - layer.kernel_size[0]) / layer.stride[0] + 1)
        out_w = int((x.size()[3] + 2 * layer.padding[1] - layer.kernel_size[1]) / layer.stride[1] + 1)
        delta_ops = layer.in_channels * layer.out_channels * layer.kernel_size[0] * layer.kernel_size[1] * out_h * out_w / layer.groups * multi_add
        delta_params = get_layer_param(layer)
    elif type_name in ['ReLU', 'ReLU6', 'LeakyReLU', 'Sigmoid']:
        delta_ops = x.numel()
        delta_params = get_layer_param(layer)
    elif type_name in ['AvgPool2d']:
        in_w = x.size()[2]
        kernel_ops = layer.kernel_size * layer.kernel_size
        out_w = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)
        out_h = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)
        delta_ops = x.size()[0] * x.size()[1] * out_w * out_h * kernel_ops
        delta_params = get_layer_param(layer)
    elif type_name in ['AdaptiveAvgPool2d']:
        delta_ops = x.size()[0] * x.size()[1] * x.size()[2] * x.size()[3]
        delta_params = get_layer_param(layer)
    elif type_name in ['Linear']:
        weight_ops = layer.weight.numel() * multi_add
        bias_ops = layer.bias.numel()
        delta_ops = x.size()[0] * (weight_ops + bias_ops)
        delta_params = get_layer_param(layer)
    elif type_name in ['BatchNorm2d', 'Dropout2d', 'DropChannel', 'Dropout', 'InPlaceABN', 'InPlaceABNSync', 'Upsample', 'MaxPool2d']:
        delta_params = get_layer_param(layer)
    else:
        raise TypeError('unknown layer type: %s' % type_name)
    count_ops += delta_ops
    count_params += delta_params
    return",'unknown layer type: %s' % type_name,["f'unknown layer type: {type_name}'"],no_found,0,,,,
kale,https://github.com/kubeflow-kale/kale/tree/master/backend/kale/kfserving/transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kale/backend/kale/kfserving/transformer.py,KaleTransformer,"def _load_transformer_assets(self):
    marshal.set_data_dir(serveutils.TRANSFORMER_ASSETS_DIR)
    log.info('Loading transformer function...')
    _fn = marshal.load(serveutils.TRANSFORMER_FN_ASSET_NAME)
    self.fn = types.FunctionType(_fn.__code__, globals(), _fn.__name__, _fn.__defaults__, _fn.__closure__)
    log.info('Processing source notebook for imports and functions...')
    processor = NotebookProcessor(nb_path=os.path.join(serveutils.TRANSFORMER_ASSETS_DIR, serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME), skip_validation=True)
    self.init_code = processor.get_imports_and_functions()
    log.info('Initialization code:\n%s' % self.init_code)
    log.info('Running initialization code...')
    exec(self.init_code, globals())
    log.info(""Loading transformer's assets..."")
    for file in os.listdir(serveutils.TRANSFORMER_ASSETS_DIR):
        if file in [serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME, serveutils.TRANSFORMER_FN_ASSET_NAME]:
            continue
        basename = os.path.splitext(file)[0]
        self.assets[basename] = marshal.load(basename)
    log.info('Assets successfully loaded: %s' % self.assets.keys())
    log.info('Initializing assets...')
    for (asset_name, asset_value) in self.assets.items():
        globals()[asset_name] = asset_value",'Initialization code:\n%s' % self.init_code,["f'Initialization code:\\n{self.init_code}'"],no_found,0,,,,
kale,https://github.com/kubeflow-kale/kale/tree/master/backend/kale/kfserving/transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kale/backend/kale/kfserving/transformer.py,KaleTransformer,"def _load_transformer_assets(self):
    marshal.set_data_dir(serveutils.TRANSFORMER_ASSETS_DIR)
    log.info('Loading transformer function...')
    _fn = marshal.load(serveutils.TRANSFORMER_FN_ASSET_NAME)
    self.fn = types.FunctionType(_fn.__code__, globals(), _fn.__name__, _fn.__defaults__, _fn.__closure__)
    log.info('Processing source notebook for imports and functions...')
    processor = NotebookProcessor(nb_path=os.path.join(serveutils.TRANSFORMER_ASSETS_DIR, serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME), skip_validation=True)
    self.init_code = processor.get_imports_and_functions()
    log.info('Initialization code:\n%s' % self.init_code)
    log.info('Running initialization code...')
    exec(self.init_code, globals())
    log.info(""Loading transformer's assets..."")
    for file in os.listdir(serveutils.TRANSFORMER_ASSETS_DIR):
        if file in [serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME, serveutils.TRANSFORMER_FN_ASSET_NAME]:
            continue
        basename = os.path.splitext(file)[0]
        self.assets[basename] = marshal.load(basename)
    log.info('Assets successfully loaded: %s' % self.assets.keys())
    log.info('Initializing assets...')
    for (asset_name, asset_value) in self.assets.items():
        globals()[asset_name] = asset_value",'Assets successfully loaded: %s' % self.assets.keys(),["f'Assets successfully loaded: {self.assets.keys()}'"],no_found,0,,,,
magenta,https://github.com/magenta/magenta/tree/master/magenta/models/image_stylization/image_stylization_convert_tflite.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/magenta/magenta/models/image_stylization/image_stylization_convert_tflite.py,,"def _convert_to_tflite(saved_model_dir, num_styles, image_size, quantize, output_model):
    """"""Convert a image stylization saved model to TensorFlow Lite format.""""""
    if tf.io.gfile.isdir(output_model):
        if quantize:
            filename = 'stylize_quantized.tflite'
        else:
            filename = 'stylize.tflite'
        output_model = os.path.join(output_model, filename)
    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=saved_model_dir, input_shapes={'input_image': [None, image_size, image_size, 3], 'style_weights': num_styles})
    if quantize:
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()
    with tf.io.gfile.GFile(output_model, 'wb') as f:
        f.write(tflite_model)
    tf.logging.info('Converted to TF Lite model: %s; Size: %d KB.' % (output_model, len(tflite_model) / 1024))","'Converted to TF Lite model: %s; Size: %d KB.' % (output_model, len(tflite_model) / 1024)",["f'Converted to TF Lite model: {output_model}; Size: {len(tflite_model) / 1024} KB.'"],no_found,1,,,,
pootle,https://github.com/translate/pootle/tree/master/tests/views/admin.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pootle/tests/views/admin.py,,"def test_admin_view_project_delete_tp(english, client, admin):
    user = admin
    project = Project.objects.get(code='project0')
    tp = TranslationProject.objects.create(language=english, project=project)
    project.config['pootle.core.lang_mapping'] = {tp.language.code: 'foo'}
    client.login(username=user.username, password=TEST_USERS['admin']['password'])
    get_response = _admin_view_get(client, project)
    post_data = {}
    formset = get_response.context['formset']
    forms = formset.forms + formset.extra_forms + [formset.management_form]
    for form in forms:
        for field in form.fields:
            post_data['%s-%s' % (form.prefix, field)] = form.fields[field].initial or form.initial.get(field, '')
    tp_pk = post_data['form-0-id']
    post_data['form-0-DELETE'] = 'true'
    response = _admin_view_post(client, project, **post_data)
    assert tp_pk not in project.translationproject_set.values_list('pk', flat=True)
    _test_admin_view(response, project)
    assert project.config['pootle.core.lang_mapping'] == {}","'%s-%s' % (form.prefix, field)",["f'{form.prefix}-{field}'"],no_found,0,,,,
q,https://github.com/zestyping/q/tree/master/test/test_suite.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/q/test/test_suite.py,ParsingModeTests,"def test_strict_mode_too_small_specific_column_count(self):
    tmpfile = self.create_file_with_data(sample_data_no_header)
    cmd = Q_EXECUTABLE + ' -d , -m strict -c 2 ""select count(*) from %s""' % tmpfile.name
    (retcode, o, e) = run_command(cmd)
    self.assertNotEqual(retcode, 0)
    self.assertEqual(len(o), 0)
    self.assertEqual(len(e), 1)
    self.assertEqual(e[0], six.b('Strict mode. Column count is expected to be 2 but is 3'))
    self.cleanup(tmpfile)","' -d , -m strict -c 2 ""select count(*) from %s""' % tmpfile.name","['f\' -d , -m strict -c 2 ""select count(*) from {tmpfile.name}""\'']",no_found,1,,,,
ansible-modules-extras,https://github.com/ansible/ansible-modules-extras/tree/master/cloud/cloudstack/cs_firewall.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-extras/cloud/cloudstack/cs_firewall.py,AnsibleCloudStackFirewall,"def create_firewall_rule(self):
    firewall_rule = self.get_firewall_rule()
    if not firewall_rule:
        self.result['changed'] = True
        args = {}
        args['cidrlist'] = self.module.params.get('cidr')
        args['protocol'] = self.module.params.get('protocol')
        args['startport'] = self.module.params.get('start_port')
        args['endport'] = self.get_or_fallback('end_port', 'start_port')
        args['icmptype'] = self.module.params.get('icmp_type')
        args['icmpcode'] = self.module.params.get('icmp_code')
        fw_type = self.module.params.get('type')
        if not self.module.check_mode:
            if fw_type == 'egress':
                args['networkid'] = self.get_network(key='id')
                res = self.cs.createEgressFirewallRule(**args)
            else:
                args['ipaddressid'] = self.get_ip_address('id')
                res = self.cs.createFirewallRule(**args)
            if 'errortext' in res:
                self.module.fail_json(msg=""Failed: '%s'"" % res['errortext'])
            poll_async = self.module.params.get('poll_async')
            if poll_async:
                firewall_rule = self.poll_job(res, 'firewallrule')
    return firewall_rule","""Failed: '%s'"" % res['errortext']",['f"Failed: \'{res[\'errortext\']}\'"'],no_found,0,,,,
PGL,https://github.com/PaddlePaddle/PGL/tree/master/apps/Graph4Rec/env_run/src/datasets/ego_graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/apps/Graph4Rec/env_run/src/datasets/ego_graph.py,EgoGraphGenerator,"def __init__(self, config, graph, **kwargs):
    self.config = config
    self.graph = graph
    self.rank = kwargs.get('rank', 0)
    self.nrank = kwargs.get('nrank', 1)
    self.kwargs = kwargs
    self.edge_types = self.graph.get_edge_types()
    self.sample_num_list = kwargs.get('sample_list', self.config.sample_num_list)
    log.info('sample_num_list is %s' % repr(self.sample_num_list))",'sample_num_list is %s' % repr(self.sample_num_list),["f'sample_num_list is {repr(self.sample_num_list)}'"],no_found,1,,,,
pi-timolo,https://github.com/pageauc/pi-timolo/tree/master/source/pi-timolo-67/pi-timolo.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pi-timolo/source/pi-timolo-67/pi-timolo.py,,"def subDirCreate(directory, prefix):
    now = datetime.datetime.now()
    subDirName = '%s%d-%02d-%02d-%02d:%02d' % (prefix, now.year, now.month, now.day, now.hour, now.minute)
    subDirPath = os.path.join(directory, subDirName)
    if not os.path.exists(subDirPath):
        try:
            os.makedirs(subDirPath)
        except OSError as err:
            logging.error('Cannot Create Directory %s - %s, using default location.', subDirPath, err)
            subDirPath = directory
        else:
            logging.info('Created %s', subDirPath)
    else:
        subDirPath = directory
    return subDirPath","'%s%d-%02d-%02d-%02d:%02d' % (prefix, now.year, now.month, now.day, now.hour, now.minute)",["f'{prefix}{now.year}-{now.month:02d}-{now.day:02d}-{now.hour:02d}:{now.minute:02d}'"],subDirName = f'{prefix}{now.year}-{now.month:02}-{now.day:02}-{now.hour:02}:{now.minute:02}',1,,,,
pyquil,https://github.com/rigetti/pyquil/tree/master/pyquil/paulis.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/paulis.py,PauliTerm,"def __repr__(self) -> str:
    term_strs = []
    for index in self._ops.keys():
        term_strs.append('%s%s' % (self[index], index))
    if len(term_strs) == 0:
        term_strs.append('I')
    out = '%s*%s' % (self.coefficient, '*'.join(term_strs))
    return out","'%s*%s' % (self.coefficient, '*'.join(term_strs))",['f"{self.coefficient}*{\'*\'.join(term_strs)}"'],no_found,1,,,,
pyquil,https://github.com/rigetti/pyquil/tree/master/pyquil/paulis.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/paulis.py,PauliTerm,"def __repr__(self) -> str:
    term_strs = []
    for index in self._ops.keys():
        term_strs.append('%s%s' % (self[index], index))
    if len(term_strs) == 0:
        term_strs.append('I')
    out = '%s*%s' % (self.coefficient, '*'.join(term_strs))
    return out","'%s%s' % (self[index], index)",["f'{self[index]}{index}'"],no_found,1,,,,
recordlinkage,https://github.com/J535D165/recordlinkage/tree/master//versioneer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/recordlinkage//versioneer.py,cmd_version,"def run(self):
    vers = get_versions(verbose=True)
    print('Version: %s' % vers['version'])
    print(' full-revisionid: %s' % vers.get('full-revisionid'))
    print(' dirty: %s' % vers.get('dirty'))
    if vers['error']:
        print(' error: %s' % vers['error'])",' full-revisionid: %s' % vers.get('full-revisionid'),['f"full-revisionid: {vers.get(\'full-revisionid\')}"'],no_found,0,,,,
recordlinkage,https://github.com/J535D165/recordlinkage/tree/master//versioneer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/recordlinkage//versioneer.py,cmd_version,"def run(self):
    vers = get_versions(verbose=True)
    print('Version: %s' % vers['version'])
    print(' full-revisionid: %s' % vers.get('full-revisionid'))
    print(' dirty: %s' % vers.get('dirty'))
    if vers['error']:
        print(' error: %s' % vers['error'])",' error: %s' % vers['error'],['f"error: {vers[\'error\']}"'],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()",'mtllib %s\n' % os.path.basename(mtl_filename),["f'mtllib {os.path.basename(mtl_filename)}\\n'"],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()",'material%d' % ins_cnt,["f'material{ins_cnt}'"],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()",'usemtl %s\n' % material,["f'usemtl {material}\\n'"],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'v %f %f %f\n' % (x, y, z + c)",['"""v {x} {y} {z + c}\n"""'],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'v %f %f %f\n' % (x, y + b, z + c)",['"""v {x} {y + b} {z + c}\n"""'],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'v %f %f %f\n' % (x + a, y + b, z + c)",['"""v {x + a} {y + b} {z + c}\n"""'],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'v %f %f %f\n' % (x + a, y, z + c)",['"""v {x + a} {y} {z + c}\n"""'],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'v %f %f %f\n' % (x, y, z)",['"""v {x} {y} {z}\n"""'],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'v %f %f %f\n' % (x, y + b, z)",['"""v {x} {y + b} {z}\n"""'],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'v %f %f %f\n' % (x + a, y + b, z)",['"""v {x + a} {y + b} {z}\n"""'],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'v %f %f %f\n' % (x + a, y, z)",['"""v {x + a} {y} {z}\n"""'],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt)",["f'f {4 + v_cnt} {3 + v_cnt} {2 + v_cnt} {1 + v_cnt}\\n'"],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt)",["f'f {1 + v_cnt} {2 + v_cnt} {6 + v_cnt} {5 + v_cnt}\\n'"],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt)",["f'f {7 + v_cnt} {6 + v_cnt} {2 + v_cnt} {3 + v_cnt}\\n'"],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt)",["f'f {4 + v_cnt} {8 + v_cnt} {7 + v_cnt} {3 + v_cnt}\\n'"],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt)",["f'f {5 + v_cnt} {8 + v_cnt} {4 + v_cnt} {1 + v_cnt}\\n'"],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt)",["f'f {5 + v_cnt} {6 + v_cnt} {7 + v_cnt} {8 + v_cnt}\\n'"],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()",'newmtl %s\n' % material,["f'newmtl {material}\\n'"],no_found,0,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","'Kd %f %f %f\n' % (color[0], color[1], color[2])",["f'Kd {color[0]} {color[1]} {color[2]}\\n'"],no_found,0,,,,
muffin,https://github.com/klen/muffin/tree/master/muffin/manage.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muffin/muffin/manage.py,Manager,"def process_arg(name, *, value=..., **opts):
    argname = name.lower()
    arghelp = docs.get(name, '')
    if value is ...:
        return parser.add_argument(argname, help=arghelp, **opts)
    argname = argname.replace('_', '-')
    if isinstance(value, bool):
        if value:
            return parser.add_argument('--no-' + argname, dest=name, action='store_false', help=arghelp or f'Disable {name}')
        return parser.add_argument('--' + argname, dest=name, action='store_true', help=arghelp or f'Enable {name}')
    if isinstance(value, list):
        return parser.add_argument('--' + argname, action='append', default=value, help=arghelp)
    return parser.add_argument('--' + argname, type=anns.get(name, type(value)), default=value, help=arghelp + ' [%s]' % repr(value))",' [%s]' % repr(value),["f'[{repr(value)}]'"],no_found,1,,,,
napalm,https://github.com/napalm-automation/napalm/tree/master/napalm/base/helpers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/napalm/napalm/base/helpers.py,,"def find_txt(xml_tree, path, default='', namespaces=None):
    """"""
    Extracts the text value from an XML tree, using XPath.
    In case of error or text element unavailability, will return a default value.

    :param xml_tree:   the XML Tree object. Assumed is <type 'lxml.etree._Element'>.
    :param path:       XPath to be applied, in order to extract the desired data.
    :param default:    Value to be returned in case of error.
    :param namespaces: prefix-namespace mappings to process XPath
    :return: a str value.
    """"""
    value = ''
    try:
        xpath_applied = xml_tree.xpath(path, namespaces=namespaces)
        xpath_length = len(xpath_applied)
        if xpath_length and xpath_applied[0] is not None:
            xpath_result = xpath_applied[0]
            if isinstance(xpath_result, type(xml_tree)):
                if xpath_result.text:
                    value = xpath_result.text.strip()
                else:
                    value = default
            else:
                value = xpath_result
        elif xpath_applied == '':
            logger.debug('Unable to find the specified-text-element/XML path: %s in                          the XML tree provided. Total Items in XML tree: %d ' % (path, xpath_length))
    except Exception as findTxtErr01:
        logger.error(findTxtErr01)
        value = default
    return str(value)","'Unable to find the specified-text-element/XML path: %s in                          the XML tree provided. Total Items in XML tree: %d ' % (path, xpath_length)",["f'Unable to find the specified-text-element/XML path: {path} in the XML tree provided. Total Items in XML tree: {xpath_length}'"],no_found,1,,,,
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)",'tokenizing and selecting specific posts %2f' % (time() - st_time),["f'tokenizing and selecting specific posts {time() - st_time}'"],no_found,0,,,,
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)",'Total found %d' % len(processed_items),["f'Total found {len(processed_items)}'"],no_found,0,,,,
madmom,https://github.com/CPJKU/madmom/tree/master/madmom/io/audio.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/madmom/madmom/io/audio.py,,"def decode_to_memory(infile, fmt='f32le', sample_rate=None, num_channels=1, channel=None, skip=None, max_len=None, cmd_decode='ffmpeg', cmd_probe='ffprobe', replaygain_mode=None, replaygain_preamp=0.0):
    """"""
    Decode the given audio and return it as a binary string representation.

    Parameters
    ----------
    infile : str
        Name of the audio sound file to decode.
    fmt : {'f32le', 's16le'}, optional
        Format of the samples:
        - 'f32le' for float32, little-endian,
        - 's16le' for signed 16-bit int, little-endian.
    sample_rate : int, optional
        Sample rate to re-sample the signal to (if set) [Hz].
    num_channels : int, optional
        Number of channels to reduce the signal to.
        If 'None', return the signal with its original channels,
        or whatever is selected by `channel`.
    channel : int, optional
        When reducing a signal to `num_channels` of 1, use this channel,
        or 'None' to return the average across all channels.
    skip : float, optional
        Number of seconds to skip at beginning of file.
    max_len : float, optional
        Maximum length in seconds to decode.
    cmd_decode : {'ffmpeg', 'avconv'}, optional
        Decoding command (defaults to ffmpeg, alternatively supports avconv).
    cmd_probe : {'ffprobe', 'avprobe'}, optional
        Probing command (defaults to ffprobe, alternatively supports avprobe).
    replaygain_mode : {None, 'track','album'}, optional
        Specify the ReplayGain volume-levelling mode (None to disable).
    replaygain_preamp : float, optional
        ReplayGain preamp volume change level (in dB).

    Returns
    -------
    samples : str
        Binary string representation of the audio samples.

    """"""
    if not isinstance(infile, (string_types, file_types, Signal)):
        raise ValueError('only file names, file objects or Signal instances are supported as `infile`, not %s.' % infile)
    (_, proc) = decode_to_pipe(infile, fmt=fmt, sample_rate=sample_rate, num_channels=num_channels, channel=channel, skip=skip, max_len=max_len, cmd=cmd_decode, replaygain_mode=replaygain_mode, replaygain_preamp=replaygain_preamp)
    if isinstance(infile, Signal):
        try:
            (signal, _) = proc.communicate(np.getbuffer(infile))
        except AttributeError:
            mv = memoryview(infile)
            (signal, _) = proc.communicate(mv.cast('b'))
    elif isinstance(infile, file_types):
        (signal, _) = proc.communicate(infile.read())
        infile.seek(0)
        if not signal and nonstreamable_mp4_file_object(infile, cmd_probe):
            try:
                delete_file = False
                try:
                    path = infile.name
                except AttributeError:
                    with tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix='.mp4') as f:
                        f.write(infile.read())
                    infile.seek(0)
                    path = f.name
                    delete_file = True
                signal = decode_to_memory(path, fmt, sample_rate, num_channels, channel, skip, max_len, cmd_decode, cmd_probe, replaygain_mode, replaygain_preamp)
            finally:
                if delete_file:
                    os.remove(path)
    else:
        (signal, _) = proc.communicate()
    if proc.returncode != 0:
        raise subprocess.CalledProcessError(proc.returncode, cmd_decode)
    return signal","'only file names, file objects or Signal instances are supported as `infile`, not %s.' % infile","[""f'only file names, file objects or Signal instances are supported as `infile`, not {infile}.'""]",no_found,0,,,,
pyhanlp,https://github.com/hankcs/pyhanlp/tree/master/tests/book/ch10/demo_clustering_f.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyhanlp/tests/book/ch10/demo_clustering_f.py,,"if __name__ == '__main__':
    for algorithm in ('kmeans', 'repeated bisection'):
        print('%s F1=%.2f\n' % (algorithm, ClusterAnalyzer.evaluate(sogou_corpus_path, algorithm) * 100))","'%s F1=%.2f\n' % (algorithm, ClusterAnalyzer.evaluate(sogou_corpus_path, algorithm) * 100)","[""f'{algorithm} F1={ClusterAnalyzer.evaluate(sogou_corpus_path, algorithm) * 100:.2f}\\n'""]",no_found,1,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)",['f"{out_dir}/{\'debug_batch_images\'}_{iteration_to_remove}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)",['f"{out_dir}/{\'debug_batch_targets\'}_{iteration_to_remove}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)",['f"{out_dir}/{\'debug_batch_outputs\'}_{iteration_to_remove}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)",['f"{out_dir}/{\'debug_batch_images\'}_{0}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)",['f"{out_dir}/{\'debug_batch_targets\'}_{0}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)",['f"{out_dir}/{\'debug_batch_outputs\'}_{0}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration)",['f"{out_dir}/{\'debug_batch_images\'}_{iteration}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration)",['f"{out_dir}/{\'debug_batch_targets\'}_{iteration}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration)",['f"{out_dir}/{\'debug_batch_outputs\'}_{iteration}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration)",['f"{out_dir}/{\'debug_test_images\'}_{iteration}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration)",['f"{out_dir}/{\'debug_test_outputs\'}_{iteration}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)",['f"{out_dir}/{\'debug_batch_images\'}_{iteration_to_remove}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)",['f"{out_dir}/{\'debug_batch_targets\'}_{iteration_to_remove}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)",['f"{out_dir}/{\'debug_batch_outputs\'}_{iteration_to_remove}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)",['f"{out_dir}/{\'debug_batch_images\'}_{0}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)",['f"{out_dir}/{\'debug_batch_targets\'}_{0}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)",['f"{out_dir}/{\'debug_batch_outputs\'}_{0}.png"'],no_found,0,,,,
panoptic-deeplab,https://github.com/bowenc0221/panoptic-deeplab/tree/master/segmentation/utils/debug.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/panoptic-deeplab/segmentation/utils/debug.py,,"def save_debug_images(dataset, batch_images, batch_targets, batch_outputs, out_dir=None, iteration=0, target_keys=('semantic', 'center', 'offset', 'center_weights', 'offset_weights'), output_keys=('semantic', 'center', 'offset'), iteration_to_remove=-1, is_train=True):
    """"""Saves a mini-batch of images for debugging purpose.
        - image: the augmented input image
        - label: the augmented labels including
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
            - instance_ignore_mask: ignore mask
        - prediction: the raw output of the model (without post-processing)
            - semantic: semantic segmentation label
            - center: center heatmap
            - offset: offset field
    Args:
        dataset: The Dataset.
        batch_images: Tensor of shape [N, 3, H, W], a batch of input images.
        batch_targets: Dict, a dict containing batch of targets.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
            - semantic_weights: a Tensor of shape [N, H, W]
            - center_weights: a Tensor of shape [N, H, W]
            - offset_weights: a Tensor of shape [N, H, W]
        batch_outputs: Dict, a dict containing batch of outputs.
            - semantic: a Tensor of shape [N, H, W]
            - center: a Tensor of shape [N, 1, H, W]
            - offset: a Tensor of shape [N, 2, H, W]
        out_dir: String, the directory to which the results will be saved.
        iteration: Integer, iteration number.
        target_keys: List, target keys to save.
        output_keys: List, output keys to save.
        iteration_to_remove: Integer, iteration number to remove.
        is_train: Boolean, save train or test debugging image.
    """"""
    batch_size = batch_images.size(0)
    map_height = batch_images.size(2)
    map_width = batch_images.size(3)
    grid_image = np.zeros((map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_targets = len(target_keys)
    grid_target = np.zeros((num_targets * map_height, batch_size * map_width, 3), dtype=np.uint8)
    num_outputs = len(output_keys)
    grid_output = np.zeros((num_outputs * map_height, batch_size * map_width, 3), dtype=np.uint8)
    semantic_pred = torch.argmax(batch_outputs['semantic'].detach(), dim=1)
    if 'foreground' in batch_outputs:
        foreground_pred = torch.argmax(batch_outputs['foreground'].detach(), dim=1)
    else:
        foreground_pred = None
    for i in range(batch_size):
        width_begin = map_width * i
        width_end = map_width * (i + 1)
        image = dataset.reverse_transform(batch_images[i])
        grid_image[:, width_begin:width_end, :] = image
        if 'semantic' in target_keys:
            gt_sem = batch_targets['semantic'][i].cpu().numpy()
            gt_sem = label_to_color_image(gt_sem, dataset.create_label_colormap())
            grid_target[:map_height, width_begin:width_end, :] = gt_sem
        if 'center' in target_keys:
            gt_ctr = batch_targets['center'][i].squeeze().cpu().numpy()
            gt_ctr = gt_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            gt_ctr = gt_ctr.clip(0, 255)
            grid_target[map_height:2 * map_height, width_begin:width_end, :] = gt_ctr
        if 'offset' in target_keys:
            gt_off = batch_targets['offset'][i].permute(1, 2, 0).cpu().numpy()
            gt_off = flow_compute_color(gt_off[:, :, 1], gt_off[:, :, 0])
            grid_target[2 * map_height:3 * map_height, width_begin:width_end, :] = gt_off
        if 'semantic_weights' in target_keys:
            gt_ign = batch_targets['semantic_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] / np.max(gt_ign) * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[3 * map_height:4 * map_height, width_begin:width_end, :] = gt_ign
        if 'center_weights' in target_keys:
            gt_ign = batch_targets['center_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[4 * map_height:5 * map_height, width_begin:width_end, :] = gt_ign
        if 'offset_weights' in target_keys:
            gt_ign = batch_targets['offset_weights'][i].cpu().numpy()
            gt_ign = gt_ign[:, :, None] * 255
            gt_ign = np.tile(gt_ign, (1, 1, 3))
            grid_target[5 * map_height:6 * map_height, width_begin:width_end, :] = gt_ign
        if 'foreground' in target_keys:
            gt_fg = batch_targets['foreground'][i].cpu().numpy()
            gt_fg = gt_fg[:, :, None] * 255
            grid_target[6 * map_height:7 * map_height, width_begin:width_end, :] = gt_fg
        if 'semantic' in output_keys:
            pred_sem = semantic_pred[i].cpu().numpy()
            pred_sem = label_to_color_image(pred_sem, dataset.create_label_colormap())
            grid_output[:map_height, width_begin:width_end, :] = pred_sem
        if 'center' in output_keys:
            pred_ctr = batch_outputs['center'][i].detach().squeeze().cpu().numpy()
            pred_ctr = pred_ctr[:, :, None] * np.array([255, 0, 0]).reshape((1, 1, 3))
            pred_ctr = pred_ctr.clip(0, 255)
            grid_output[map_height:2 * map_height, width_begin:width_end, :] = pred_ctr
        if 'offset' in output_keys:
            pred_ctr = batch_outputs['offset'][i].detach().permute(1, 2, 0).cpu().numpy()
            pred_ctr = flow_compute_color(pred_ctr[:, :, 1], pred_ctr[:, :, 0])
            grid_output[2 * map_height:3 * map_height, width_begin:width_end, :] = pred_ctr
        if 'foreground' in output_keys:
            if foreground_pred is not None:
                pred_fg = foreground_pred[i].cpu().numpy()
                pred_fg = pred_fg[:, :, None] * 255
                grid_output[3 * map_height:4 * map_height, width_begin:width_end, :] = pred_fg
    if out_dir is not None:
        if is_train:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
        else:
            pil_image = img.fromarray(grid_image.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_images', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
            if grid_target.size:
                pil_image = img.fromarray(grid_target.astype(dtype=np.uint8))
                with open('%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration), mode='wb') as f:
                    pil_image.save(f, 'PNG')
            pil_image = img.fromarray(grid_output.astype(dtype=np.uint8))
            with open('%s/%s_%d.png' % (out_dir, 'debug_test_outputs', iteration), mode='wb') as f:
                pil_image.save(f, 'PNG')
    if is_train:
        if iteration_to_remove >= 0:
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', iteration_to_remove))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_images', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_targets', 0))
            if os.path.exists('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0)):
                os.remove('%s/%s_%d.png' % (out_dir, 'debug_batch_outputs', 0))","'%s/%s_%d.png' % (out_dir, 'debug_test_targets', iteration)",['f"{out_dir}/{\'debug_test_targets\'}_{iteration}.png"'],no_found,0,,,,
tvm,https://github.com/apache/tvm/tree/master/python/tvm/autotvm/tuner/sa_model_optimizer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/autotvm/tuner/sa_model_optimizer.py,SimulatedAnnealingOptimizer,"def find_maximums(self, model, num, exclusive):
    tic = time.time()
    (temp, n_iter, early_stop, log_interval) = (self.temp, self.n_iter, self.early_stop, self.log_interval)
    if self.persistent and self.points is not None:
        points = self.points
    else:
        points = self.task.config_space.sample_ints(self.parallel_size)
    scores = model.predict(points)
    heap_items = [(float('-inf'), -1 - i) for i in range(num)]
    heapq.heapify(heap_items)
    in_heap = set(exclusive)
    in_heap.update([x[1] for x in heap_items])
    for (s, p) in zip(scores, points):
        if s > heap_items[0][0] and p not in in_heap:
            pop = heapq.heapreplace(heap_items, (s, p))
            in_heap.remove(pop[1])
            in_heap.add(p)
    k = 0
    k_last_modify = 0
    if isinstance(temp, (tuple, list, np.ndarray)):
        t = temp[0]
        cool = 1.0 * (temp[0] - temp[1]) / (n_iter + 1)
    else:
        t = temp
        cool = 0
    while k < n_iter and k < k_last_modify + early_stop:
        new_points = np.empty_like(points)
        for (i, p) in enumerate(points):
            new_points[i] = self.task.config_space.random_walk(p)
        new_scores = model.predict(new_points)
        ac_prob = np.exp(np.minimum((new_scores - scores) / (t + 1e-05), 1))
        ac_index = np.random.random(len(ac_prob)) < ac_prob
        points[ac_index] = new_points[ac_index]
        scores[ac_index] = new_scores[ac_index]
        for (s, p) in zip(new_scores, new_points):
            if s > heap_items[0][0] and p not in in_heap:
                pop = heapq.heapreplace(heap_items, (s, p))
                in_heap.remove(pop[1])
                in_heap.add(p)
                k_last_modify = k
        k += 1
        t -= cool
        if log_interval and k % log_interval == 0:
            t_str = '%.2f' % t
            logger.debug('SA iter: %d\tlast_update: %d\tmax-0: %.2f\tmax-1: %.2f\ttemp: %s\telapsed: %.2f', k, k_last_modify, heap_items[0][0], np.max([v for (v, _) in heap_items]), t_str, time.time() - tic)
    heap_items.sort(key=lambda item: -item[0])
    heap_items = [x for x in heap_items if x[0] >= 0]
    logger.debug('SA iter: %d\tlast_update: %d\telapsed: %.2f', k, k_last_modify, time.time() - tic)
    logger.debug('SA Maximums: %s', heap_items)
    if self.persistent:
        self.points = points
    return [x[1] for x in heap_items]",'%.2f' % t,["f'{t:.2f}'"],no_found,1,,,,
freeipa,https://github.com/freeipa/freeipa/tree/master/ipaserver/plugins/host.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/plugins/host.py,host,"def get_managed_hosts(self, dn):
    host_filter = 'managedBy=%s' % dn
    host_attrs = ['fqdn']
    ldap = self.api.Backend.ldap2
    managed_hosts = []
    try:
        (hosts, _truncated) = ldap.find_entries(base_dn=DN(self.container_dn, api.env.basedn), filter=host_filter, attrs_list=host_attrs)
        for host in hosts:
            managed_hosts.append(host.dn)
    except errors.NotFound:
        return []
    return managed_hosts",'managedBy=%s' % dn,["f'managedBy={dn}'"],no_found,1,,,,
azure-cli,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/netappfiles/tests/latest/test_volume_commands.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/netappfiles/tests/latest/test_volume_commands.py,AzureNetAppFilesVolumeServiceScenarioTest,"def test_export_policy(self):
    account_name = self.create_random_name(prefix='cli-acc-', length=24)
    pool_name = self.create_random_name(prefix='cli-pool-', length=24)
    volume_name = self.create_random_name(prefix='cli-vol-', length=24)
    volume = self.create_volume(account_name, pool_name, volume_name, '{rg}')
    assert volume['name'] == account_name + '/' + pool_name + '/' + volume_name
    vol_with_export_policy = self.cmd(""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.3.0/24' --rule-index 3 --unix-read-only true --unix-read-write false --cifs false --nfsv3 true --nfsv41 false --has-root-access false"" % (account_name, pool_name, volume_name)).get_output_in_json()
    assert vol_with_export_policy['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert vol_with_export_policy['exportPolicy']['rules'][0]['allowedClients'] == '1.2.3.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['ruleIndex'] == 3
    assert vol_with_export_policy['exportPolicy']['rules'][0]['cifs'] is False
    assert vol_with_export_policy['exportPolicy']['rules'][0]['hasRootAccess'] is False
    vol_with_export_policy = self.cmd(""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.4.0/24' --rule-index 2 --unix-read-only true --unix-read-write false --cifs true --nfsv3 true --nfsv41 false"" % (account_name, pool_name, volume_name)).get_output_in_json()
    assert vol_with_export_policy['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert vol_with_export_policy['exportPolicy']['rules'][1]['allowedClients'] == '1.2.3.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['allowedClients'] == '1.2.4.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['cifs'] is True
    assert len(vol_with_export_policy['exportPolicy']['rules']) == 3
    export_policy = self.cmd('netappfiles volume export-policy list -g {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)).get_output_in_json()
    assert len(export_policy['rules']) == 3
    self.cmd('netappfiles volume export-policy remove -g {rg} -a %s -p %s -v %s --rule-index 3' % (account_name, pool_name, volume_name)).get_output_in_json()
    if self.is_live or self.in_recording:
        time.sleep(240)
    volume = self.cmd('az netappfiles volume show --resource-group {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)).get_output_in_json()
    assert volume['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert len(volume['exportPolicy']['rules']) == 2","""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.3.0/24' --rule-index 3 --unix-read-only true --unix-read-write false --cifs false --nfsv3 true --nfsv41 false --has-root-access false"" % (account_name, pool_name, volume_name)",['f"netappfiles volume export-policy add -g {rg} -a {account_name} -p {pool_name} -v {volume_name} --allowed-clients \'1.2.3.0/24\' --rule-index 3 --unix-read-only true --unix-read-write false --cifs false --nfsv3 true --nfsv41 false --has-root-access false"'],no_found,0,,,,
azure-cli,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/netappfiles/tests/latest/test_volume_commands.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/netappfiles/tests/latest/test_volume_commands.py,AzureNetAppFilesVolumeServiceScenarioTest,"def test_export_policy(self):
    account_name = self.create_random_name(prefix='cli-acc-', length=24)
    pool_name = self.create_random_name(prefix='cli-pool-', length=24)
    volume_name = self.create_random_name(prefix='cli-vol-', length=24)
    volume = self.create_volume(account_name, pool_name, volume_name, '{rg}')
    assert volume['name'] == account_name + '/' + pool_name + '/' + volume_name
    vol_with_export_policy = self.cmd(""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.3.0/24' --rule-index 3 --unix-read-only true --unix-read-write false --cifs false --nfsv3 true --nfsv41 false --has-root-access false"" % (account_name, pool_name, volume_name)).get_output_in_json()
    assert vol_with_export_policy['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert vol_with_export_policy['exportPolicy']['rules'][0]['allowedClients'] == '1.2.3.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['ruleIndex'] == 3
    assert vol_with_export_policy['exportPolicy']['rules'][0]['cifs'] is False
    assert vol_with_export_policy['exportPolicy']['rules'][0]['hasRootAccess'] is False
    vol_with_export_policy = self.cmd(""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.4.0/24' --rule-index 2 --unix-read-only true --unix-read-write false --cifs true --nfsv3 true --nfsv41 false"" % (account_name, pool_name, volume_name)).get_output_in_json()
    assert vol_with_export_policy['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert vol_with_export_policy['exportPolicy']['rules'][1]['allowedClients'] == '1.2.3.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['allowedClients'] == '1.2.4.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['cifs'] is True
    assert len(vol_with_export_policy['exportPolicy']['rules']) == 3
    export_policy = self.cmd('netappfiles volume export-policy list -g {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)).get_output_in_json()
    assert len(export_policy['rules']) == 3
    self.cmd('netappfiles volume export-policy remove -g {rg} -a %s -p %s -v %s --rule-index 3' % (account_name, pool_name, volume_name)).get_output_in_json()
    if self.is_live or self.in_recording:
        time.sleep(240)
    volume = self.cmd('az netappfiles volume show --resource-group {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)).get_output_in_json()
    assert volume['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert len(volume['exportPolicy']['rules']) == 2","""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.4.0/24' --rule-index 2 --unix-read-only true --unix-read-write false --cifs true --nfsv3 true --nfsv41 false"" % (account_name, pool_name, volume_name)",['print(f"netappfiles volume export-policy add -g {rg} -a {account_name} -p {pool_name} -v {volume_name} --allowed-clients \'1.2.4.0/24\' --rule-index 2 --unix-read-only true --unix-read-write false --cifs true --nfsv3 true --nfsv41 false")'],no_found,0,,,,
azure-cli,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/netappfiles/tests/latest/test_volume_commands.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/netappfiles/tests/latest/test_volume_commands.py,AzureNetAppFilesVolumeServiceScenarioTest,"def test_export_policy(self):
    account_name = self.create_random_name(prefix='cli-acc-', length=24)
    pool_name = self.create_random_name(prefix='cli-pool-', length=24)
    volume_name = self.create_random_name(prefix='cli-vol-', length=24)
    volume = self.create_volume(account_name, pool_name, volume_name, '{rg}')
    assert volume['name'] == account_name + '/' + pool_name + '/' + volume_name
    vol_with_export_policy = self.cmd(""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.3.0/24' --rule-index 3 --unix-read-only true --unix-read-write false --cifs false --nfsv3 true --nfsv41 false --has-root-access false"" % (account_name, pool_name, volume_name)).get_output_in_json()
    assert vol_with_export_policy['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert vol_with_export_policy['exportPolicy']['rules'][0]['allowedClients'] == '1.2.3.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['ruleIndex'] == 3
    assert vol_with_export_policy['exportPolicy']['rules'][0]['cifs'] is False
    assert vol_with_export_policy['exportPolicy']['rules'][0]['hasRootAccess'] is False
    vol_with_export_policy = self.cmd(""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.4.0/24' --rule-index 2 --unix-read-only true --unix-read-write false --cifs true --nfsv3 true --nfsv41 false"" % (account_name, pool_name, volume_name)).get_output_in_json()
    assert vol_with_export_policy['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert vol_with_export_policy['exportPolicy']['rules'][1]['allowedClients'] == '1.2.3.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['allowedClients'] == '1.2.4.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['cifs'] is True
    assert len(vol_with_export_policy['exportPolicy']['rules']) == 3
    export_policy = self.cmd('netappfiles volume export-policy list -g {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)).get_output_in_json()
    assert len(export_policy['rules']) == 3
    self.cmd('netappfiles volume export-policy remove -g {rg} -a %s -p %s -v %s --rule-index 3' % (account_name, pool_name, volume_name)).get_output_in_json()
    if self.is_live or self.in_recording:
        time.sleep(240)
    volume = self.cmd('az netappfiles volume show --resource-group {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)).get_output_in_json()
    assert volume['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert len(volume['exportPolicy']['rules']) == 2","'netappfiles volume export-policy list -g {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)",["f'netappfiles volume export-policy list -g {rg} -a {account_name} -p {pool_name} -v {volume_name}'"],no_found,0,,,,
azure-cli,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/netappfiles/tests/latest/test_volume_commands.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/netappfiles/tests/latest/test_volume_commands.py,AzureNetAppFilesVolumeServiceScenarioTest,"def test_export_policy(self):
    account_name = self.create_random_name(prefix='cli-acc-', length=24)
    pool_name = self.create_random_name(prefix='cli-pool-', length=24)
    volume_name = self.create_random_name(prefix='cli-vol-', length=24)
    volume = self.create_volume(account_name, pool_name, volume_name, '{rg}')
    assert volume['name'] == account_name + '/' + pool_name + '/' + volume_name
    vol_with_export_policy = self.cmd(""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.3.0/24' --rule-index 3 --unix-read-only true --unix-read-write false --cifs false --nfsv3 true --nfsv41 false --has-root-access false"" % (account_name, pool_name, volume_name)).get_output_in_json()
    assert vol_with_export_policy['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert vol_with_export_policy['exportPolicy']['rules'][0]['allowedClients'] == '1.2.3.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['ruleIndex'] == 3
    assert vol_with_export_policy['exportPolicy']['rules'][0]['cifs'] is False
    assert vol_with_export_policy['exportPolicy']['rules'][0]['hasRootAccess'] is False
    vol_with_export_policy = self.cmd(""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.4.0/24' --rule-index 2 --unix-read-only true --unix-read-write false --cifs true --nfsv3 true --nfsv41 false"" % (account_name, pool_name, volume_name)).get_output_in_json()
    assert vol_with_export_policy['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert vol_with_export_policy['exportPolicy']['rules'][1]['allowedClients'] == '1.2.3.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['allowedClients'] == '1.2.4.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['cifs'] is True
    assert len(vol_with_export_policy['exportPolicy']['rules']) == 3
    export_policy = self.cmd('netappfiles volume export-policy list -g {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)).get_output_in_json()
    assert len(export_policy['rules']) == 3
    self.cmd('netappfiles volume export-policy remove -g {rg} -a %s -p %s -v %s --rule-index 3' % (account_name, pool_name, volume_name)).get_output_in_json()
    if self.is_live or self.in_recording:
        time.sleep(240)
    volume = self.cmd('az netappfiles volume show --resource-group {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)).get_output_in_json()
    assert volume['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert len(volume['exportPolicy']['rules']) == 2","'netappfiles volume export-policy remove -g {rg} -a %s -p %s -v %s --rule-index 3' % (account_name, pool_name, volume_name)",["f'netappfiles volume export-policy remove -g {rg} -a {account_name} -p {pool_name} -v {volume_name} --rule-index 3'"],no_found,0,,,,
azure-cli,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/netappfiles/tests/latest/test_volume_commands.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/netappfiles/tests/latest/test_volume_commands.py,AzureNetAppFilesVolumeServiceScenarioTest,"def test_export_policy(self):
    account_name = self.create_random_name(prefix='cli-acc-', length=24)
    pool_name = self.create_random_name(prefix='cli-pool-', length=24)
    volume_name = self.create_random_name(prefix='cli-vol-', length=24)
    volume = self.create_volume(account_name, pool_name, volume_name, '{rg}')
    assert volume['name'] == account_name + '/' + pool_name + '/' + volume_name
    vol_with_export_policy = self.cmd(""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.3.0/24' --rule-index 3 --unix-read-only true --unix-read-write false --cifs false --nfsv3 true --nfsv41 false --has-root-access false"" % (account_name, pool_name, volume_name)).get_output_in_json()
    assert vol_with_export_policy['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert vol_with_export_policy['exportPolicy']['rules'][0]['allowedClients'] == '1.2.3.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['ruleIndex'] == 3
    assert vol_with_export_policy['exportPolicy']['rules'][0]['cifs'] is False
    assert vol_with_export_policy['exportPolicy']['rules'][0]['hasRootAccess'] is False
    vol_with_export_policy = self.cmd(""netappfiles volume export-policy add -g {rg} -a %s -p %s -v %s --allowed-clients '1.2.4.0/24' --rule-index 2 --unix-read-only true --unix-read-write false --cifs true --nfsv3 true --nfsv41 false"" % (account_name, pool_name, volume_name)).get_output_in_json()
    assert vol_with_export_policy['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert vol_with_export_policy['exportPolicy']['rules'][1]['allowedClients'] == '1.2.3.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['allowedClients'] == '1.2.4.0/24'
    assert vol_with_export_policy['exportPolicy']['rules'][0]['cifs'] is True
    assert len(vol_with_export_policy['exportPolicy']['rules']) == 3
    export_policy = self.cmd('netappfiles volume export-policy list -g {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)).get_output_in_json()
    assert len(export_policy['rules']) == 3
    self.cmd('netappfiles volume export-policy remove -g {rg} -a %s -p %s -v %s --rule-index 3' % (account_name, pool_name, volume_name)).get_output_in_json()
    if self.is_live or self.in_recording:
        time.sleep(240)
    volume = self.cmd('az netappfiles volume show --resource-group {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)).get_output_in_json()
    assert volume['name'] == account_name + '/' + pool_name + '/' + volume_name
    assert len(volume['exportPolicy']['rules']) == 2","'az netappfiles volume show --resource-group {rg} -a %s -p %s -v %s' % (account_name, pool_name, volume_name)",["f'az netappfiles volume show --resource-group {rg} -a {account_name} -p {pool_name} -v {volume_name}'"],no_found,0,,,,
numpy,https://github.com/numpy/numpy/tree/master/numpy/_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/_version.py,,"def render_pep440_post_branch(pieces):
    """"""TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .

    The "".dev0"" means not master branch.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]
    """"""
    if pieces['closest-tag']:
        rendered = pieces['closest-tag']
        if pieces['distance'] or pieces['dirty']:
            rendered += '.post%d' % pieces['distance']
            if pieces['branch'] != 'master':
                rendered += '.dev0'
            rendered += plus_or_dot(pieces)
            rendered += 'g%s' % pieces['short']
            if pieces['dirty']:
                rendered += '.dirty'
    else:
        rendered = '0.post%d' % pieces['distance']
        if pieces['branch'] != 'master':
            rendered += '.dev0'
        rendered += '+g%s' % pieces['short']
        if pieces['dirty']:
            rendered += '.dirty'
    return rendered",'0.post%d' % pieces['distance'],["f'0.post{pieces['distance']}'"],no_found,1,,,,
numpy,https://github.com/numpy/numpy/tree/master/numpy/_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/_version.py,,"def render_pep440_post_branch(pieces):
    """"""TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .

    The "".dev0"" means not master branch.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]
    """"""
    if pieces['closest-tag']:
        rendered = pieces['closest-tag']
        if pieces['distance'] or pieces['dirty']:
            rendered += '.post%d' % pieces['distance']
            if pieces['branch'] != 'master':
                rendered += '.dev0'
            rendered += plus_or_dot(pieces)
            rendered += 'g%s' % pieces['short']
            if pieces['dirty']:
                rendered += '.dirty'
    else:
        rendered = '0.post%d' % pieces['distance']
        if pieces['branch'] != 'master':
            rendered += '.dev0'
        rendered += '+g%s' % pieces['short']
        if pieces['dirty']:
            rendered += '.dirty'
    return rendered",'+g%s' % pieces['short'],['f"+g{pieces[\'short\']}"'],no_found,1,,,,
numpy,https://github.com/numpy/numpy/tree/master/numpy/_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/_version.py,,"def render_pep440_post_branch(pieces):
    """"""TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .

    The "".dev0"" means not master branch.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]
    """"""
    if pieces['closest-tag']:
        rendered = pieces['closest-tag']
        if pieces['distance'] or pieces['dirty']:
            rendered += '.post%d' % pieces['distance']
            if pieces['branch'] != 'master':
                rendered += '.dev0'
            rendered += plus_or_dot(pieces)
            rendered += 'g%s' % pieces['short']
            if pieces['dirty']:
                rendered += '.dirty'
    else:
        rendered = '0.post%d' % pieces['distance']
        if pieces['branch'] != 'master':
            rendered += '.dev0'
        rendered += '+g%s' % pieces['short']
        if pieces['dirty']:
            rendered += '.dirty'
    return rendered",'.post%d' % pieces['distance'],['f".post{pieces[\'distance\']}"'],no_found,0,,,,
numpy,https://github.com/numpy/numpy/tree/master/numpy/_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/_version.py,,"def render_pep440_post_branch(pieces):
    """"""TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .

    The "".dev0"" means not master branch.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]
    """"""
    if pieces['closest-tag']:
        rendered = pieces['closest-tag']
        if pieces['distance'] or pieces['dirty']:
            rendered += '.post%d' % pieces['distance']
            if pieces['branch'] != 'master':
                rendered += '.dev0'
            rendered += plus_or_dot(pieces)
            rendered += 'g%s' % pieces['short']
            if pieces['dirty']:
                rendered += '.dirty'
    else:
        rendered = '0.post%d' % pieces['distance']
        if pieces['branch'] != 'master':
            rendered += '.dev0'
        rendered += '+g%s' % pieces['short']
        if pieces['dirty']:
            rendered += '.dirty'
    return rendered",'g%s' % pieces['short'],["f'g{pieces['short']}'"],no_found,0,,,,
django-rest-framework,https://github.com/encode/django-rest-framework/tree/master/rest_framework/viewsets.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-rest-framework/rest_framework/viewsets.py,ViewSetMixin,"def get_extra_action_url_map(self):
    """"""
        Build a map of {names: urls} for the extra actions.

        This method will noop if `detail` was not provided as a view initkwarg.
        """"""
    action_urls = OrderedDict()
    if self.detail is None:
        return action_urls
    actions = [action for action in self.get_extra_actions() if action.detail == self.detail]
    for action in actions:
        try:
            url_name = '%s-%s' % (self.basename, action.url_name)
            url = reverse(url_name, self.args, self.kwargs, request=self.request)
            view = self.__class__(**action.kwargs)
            action_urls[view.get_view_name()] = url
        except NoReverseMatch:
            pass
    return action_urls","'%s-%s' % (self.basename, action.url_name)",["f'{self.basename}-{action.url_name}'"],no_found,1,,,,
mock,https://github.com/testing-cabal/mock/tree/master/mock/tests/testpatch.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mock/mock/tests/testpatch.py,PatchTest,"def test_nested_patch_with_spec_as_list(self):

    @patch('%s.open' % builtin_string)
    @patch('%s.SomeClass' % __name__, spec=['wibble'])
    def test(MockSomeClass, MockOpen):
        self.assertEqual(SomeClass, MockSomeClass)
        self.assertTrue(is_instance(SomeClass.wibble, MagicMock))
        self.assertRaises(AttributeError, lambda : SomeClass.not_wibble)
    test()",'%s.open' % builtin_string,["f'{builtin_string}.open'"],no_found,0,,,,
mock,https://github.com/testing-cabal/mock/tree/master/mock/tests/testpatch.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mock/mock/tests/testpatch.py,PatchTest,"def test_nested_patch_with_spec_as_list(self):

    @patch('%s.open' % builtin_string)
    @patch('%s.SomeClass' % __name__, spec=['wibble'])
    def test(MockSomeClass, MockOpen):
        self.assertEqual(SomeClass, MockSomeClass)
        self.assertTrue(is_instance(SomeClass.wibble, MagicMock))
        self.assertRaises(AttributeError, lambda : SomeClass.not_wibble)
    test()",'%s.SomeClass' % __name__,["f'{__name__}.SomeClass'"],no_found,0,,,,
s3fs,https://github.com/fsspec/s3fs/tree/master/s3fs/tests/test_s3fs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/s3fs/s3fs/tests/test_s3fs.py,,"def test_s3_big_ls(s3):
    for x in range(1200):
        s3.touch(test_bucket_name + '/thousand/%i.part' % x)
    assert len(s3.find(test_bucket_name)) > 1200
    s3.rm(test_bucket_name + '/thousand/', recursive=True)
    assert len(s3.find(test_bucket_name + '/thousand/')) == 0",'/thousand/%i.part' % x,["f'/thousand/{x}.part'"],no_found,1,,,,
fiftyone,https://github.com/voxel51/fiftyone/tree/master/fiftyone/utils/data/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fiftyone/fiftyone/utils/data/exporters.py,ImageClassificationDirectoryTreeExporter,"def export_sample(self, image_or_path, classification, metadata=None):
    _label = _parse_classifications(classification, include_confidence=False, include_attributes=False)
    if _label is None:
        _label = '_unlabeled'
    self._class_counts[_label] += 1
    if etau.is_str(image_or_path):
        image_path = fou.normalize_path(image_or_path)
    else:
        img = image_or_path
        image_path = self._default_filename_patt % self._class_counts[_label]
    if self.rel_dir is not None:
        filename = fou.safe_relpath(image_path, self.rel_dir)
    else:
        filename = os.path.basename(image_path)
    (name, ext) = os.path.splitext(filename)
    key = (_label, filename)
    self._filename_counts[key] += 1
    count = self._filename_counts[key]
    if count > 1:
        filename = name + '-%d' % count + ext
    outpath = os.path.join(self.export_dir, _label, filename)
    self._media_exporter.export(image_or_path, outpath=outpath)",self._default_filename_patt % self._class_counts[_label],["f'{self._default_filename_patt}{self._class_counts[_label]}'"],no_found,0,0,-1,0,
fiftyone,https://github.com/voxel51/fiftyone/tree/master/fiftyone/utils/data/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fiftyone/fiftyone/utils/data/exporters.py,ImageClassificationDirectoryTreeExporter,"def export_sample(self, image_or_path, classification, metadata=None):
    _label = _parse_classifications(classification, include_confidence=False, include_attributes=False)
    if _label is None:
        _label = '_unlabeled'
    self._class_counts[_label] += 1
    if etau.is_str(image_or_path):
        image_path = fou.normalize_path(image_or_path)
    else:
        img = image_or_path
        image_path = self._default_filename_patt % self._class_counts[_label]
    if self.rel_dir is not None:
        filename = fou.safe_relpath(image_path, self.rel_dir)
    else:
        filename = os.path.basename(image_path)
    (name, ext) = os.path.splitext(filename)
    key = (_label, filename)
    self._filename_counts[key] += 1
    count = self._filename_counts[key]
    if count > 1:
        filename = name + '-%d' % count + ext
    outpath = os.path.join(self.export_dir, _label, filename)
    self._media_exporter.export(image_or_path, outpath=outpath)",'-%d' % count,["f'-{count}'"],no_found,0,,,,
hangoutsbot,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/xkcd.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/xkcd.py,,"def _search_comic(bot, event, terms):
    request = (yield from aiohttp.request('get', 'https://relevantxkcd.appspot.com/process?%s' % urllib.parse.urlencode({'action': 'xkcd', 'query': ' '.join(terms)})))
    raw = (yield from request.read())
    values = [row.strip().split(' ')[0] for row in raw.decode().strip().split('\n')]
    weight = float(values.pop(0))
    values.pop(0)
    comics = [int(i) for i in values]
    num = comics.pop(0)
    msg = 'Most relevant xkcd: #%d (relevance: %.2f%%)\nOther relevant comics: %s' % (num, weight * 100, ', '.join(('#%d' % i for i in comics)))
    yield from _get_comic(bot, num)
    yield from bot.coro_send_message(event.conv.id_, msg)
    yield from _print_comic(bot, event, num)","'Most relevant xkcd: #%d (relevance: %.2f%%)\nOther relevant comics: %s' % (num, weight * 100, ', '.join(('#%d' % i for i in comics)))","['f""Most relevant xkcd: #{num} (relevance: {weight * 100:.2f}%)\\nOther relevant comics: {\', \'.join((\'#%d\' % i for i in comics))}""']",no_found,0,,,,
hangoutsbot,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/xkcd.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/xkcd.py,,"def _search_comic(bot, event, terms):
    request = (yield from aiohttp.request('get', 'https://relevantxkcd.appspot.com/process?%s' % urllib.parse.urlencode({'action': 'xkcd', 'query': ' '.join(terms)})))
    raw = (yield from request.read())
    values = [row.strip().split(' ')[0] for row in raw.decode().strip().split('\n')]
    weight = float(values.pop(0))
    values.pop(0)
    comics = [int(i) for i in values]
    num = comics.pop(0)
    msg = 'Most relevant xkcd: #%d (relevance: %.2f%%)\nOther relevant comics: %s' % (num, weight * 100, ', '.join(('#%d' % i for i in comics)))
    yield from _get_comic(bot, num)
    yield from bot.coro_send_message(event.conv.id_, msg)
    yield from _print_comic(bot, event, num)","'https://relevantxkcd.appspot.com/process?%s' % urllib.parse.urlencode({'action': 'xkcd', 'query': ' '.join(terms)})","[""f'https://relevantxkcd.appspot.com/process?{urllib.parse.urlencode({'action': 'xkcd', 'query': ' '.join(terms)})}'""]",no_found,0,,,,
hangoutsbot,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/xkcd.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/xkcd.py,,"def _search_comic(bot, event, terms):
    request = (yield from aiohttp.request('get', 'https://relevantxkcd.appspot.com/process?%s' % urllib.parse.urlencode({'action': 'xkcd', 'query': ' '.join(terms)})))
    raw = (yield from request.read())
    values = [row.strip().split(' ')[0] for row in raw.decode().strip().split('\n')]
    weight = float(values.pop(0))
    values.pop(0)
    comics = [int(i) for i in values]
    num = comics.pop(0)
    msg = 'Most relevant xkcd: #%d (relevance: %.2f%%)\nOther relevant comics: %s' % (num, weight * 100, ', '.join(('#%d' % i for i in comics)))
    yield from _get_comic(bot, num)
    yield from bot.coro_send_message(event.conv.id_, msg)
    yield from _print_comic(bot, event, num)",'#%d' % i,["f'#{i}'"],no_found,0,,,,
imgaug,https://github.com/aleju/imgaug/tree/master/imgaug/augmenters/arithmetic.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgaug/imgaug/augmenters/arithmetic.py,,"def _multiply_elementwise_to_non_uint8(image, multipliers):
    input_dtype = image.dtype
    mul_min = np.min(multipliers)
    mul_max = np.max(multipliers)
    itemsize = max(image.dtype.itemsize, 2 if multipliers.dtype.kind == 'f' else 1)
    dtype_target = np.dtype('%s%d' % (multipliers.dtype.kind, itemsize))
    multipliers = iadt.clip_to_dtype_value_range_(multipliers, dtype_target, validate=True, validate_values=(mul_min, mul_max))
    if multipliers.shape[2] == 1:
        nb_channels = image.shape[-1]
        multipliers = np.tile(multipliers, (1, 1, nb_channels))
    (image, multipliers) = iadt.promote_array_dtypes_([image, multipliers], dtypes=[image, dtype_target], increase_itemsize_factor=1)
    image = np.multiply(image, multipliers, out=image, casting='no')
    return iadt.restore_dtypes_(image, input_dtype)","'%s%d' % (multipliers.dtype.kind, itemsize)",["f'{multipliers.dtype.kind}{itemsize}'"],no_found,0,,,,
thonny,https://github.com/thonny/thonny/tree/master/misc/mp/pyboard.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/thonny/misc/mp/pyboard.py,Pyboard,"def fs_cat(self, src, chunk_size=256):
    cmd = ""with open('%s') as f:\n while 1:\n  b=f.read(%u)\n  if not b:break\n  print(b,end='')"" % (src, chunk_size)
    self.exec_(cmd, data_consumer=stdout_write_bytes)","""with open('%s') as f:\n while 1:\n  b=f.read(%u)\n  if not b:break\n  print(b,end='')"" % (src, chunk_size)","['f""with open(\'{src}\') as f:\\n while 1:\\n  b=f.read({chunk_size})\\n  if not b:break\\n  print(b,end=\'\')""']",no_found,1,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_split_op.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_split_op.py,TestSplitOp_AxisTensor,"def setUp(self):
    self._set_op_type()
    self.dtype = self.get_dtype()
    self.init_data()
    self.inputs = {'X': self.x, 'AxisTensor': np.array([self.axis]).astype('int32')}
    self.attrs = {'sections': self.sections, 'num': self.num}
    out = np.split(self.x, self.indices_or_sections, self.axis)
    self.outputs = {'Out': [('out%d' % i, out[i]) for i in range(len(out))]}",'out%d' % i,["f'out{i}'"],no_found,0,,,,
MxShop,https://github.com/derek-zhang123/MxShop/tree/master/extra_apps/rest_framework/renderers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MxShop/extra_apps/rest_framework/renderers.py,BrowsableAPIRenderer,"def get_content(self, renderer, data, accepted_media_type, renderer_context):
    """"""
        Get the content as if it had been rendered by the default
        non-documenting renderer.
        """"""
    if not renderer:
        return '[No renderers were found]'
    renderer_context['indent'] = 4
    content = renderer.render(data, accepted_media_type, renderer_context)
    render_style = getattr(renderer, 'render_style', 'text')
    assert render_style in ['text', 'binary'], 'Expected .render_style ""text"" or ""binary"", but got ""%s""' % render_style
    if render_style == 'binary':
        return '[%d bytes of binary content]' % len(content)
    return content","'Expected .render_style ""text"" or ""binary"", but got ""%s""' % render_style","['f\'Expected .render_style ""text"" or ""binary"", but got ""{render_style}""\'']",no_found,0,,,,
MxShop,https://github.com/derek-zhang123/MxShop/tree/master/extra_apps/rest_framework/renderers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MxShop/extra_apps/rest_framework/renderers.py,BrowsableAPIRenderer,"def get_content(self, renderer, data, accepted_media_type, renderer_context):
    """"""
        Get the content as if it had been rendered by the default
        non-documenting renderer.
        """"""
    if not renderer:
        return '[No renderers were found]'
    renderer_context['indent'] = 4
    content = renderer.render(data, accepted_media_type, renderer_context)
    render_style = getattr(renderer, 'render_style', 'text')
    assert render_style in ['text', 'binary'], 'Expected .render_style ""text"" or ""binary"", but got ""%s""' % render_style
    if render_style == 'binary':
        return '[%d bytes of binary content]' % len(content)
    return content",'[%d bytes of binary content]' % len(content),["f'[{len(content)} bytes of binary content]'"],no_found,0,,,,
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/Utils/GameModel.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/Utils/GameModel.py,GameModel,"def acceptReceived(self, player, offer):
    log.debug('GameModel.acceptReceived: accepter=%s %s' % (repr(player), offer))
    if player == self.players[WHITE]:
        opPlayer = self.players[BLACK]
    else:
        opPlayer = self.players[WHITE]
    if offer in self.offers and self.offers[offer] == opPlayer:
        if offer.type == DRAW_OFFER:
            self.end(DRAW, DRAW_AGREE)
        elif offer.type == TAKEBACK_OFFER:
            log.debug('GameModel.acceptReceived: undoMoves(%s)' % offer.param)
            self.undoMoves(offer.param)
        elif offer.type == ADJOURN_OFFER:
            self.end(ADJOURNED, ADJOURNED_AGREEMENT)
        elif offer.type == ABORT_OFFER:
            self.end(ABORTED, ABORTED_AGREEMENT)
        elif offer.type == PAUSE_OFFER:
            self.pause()
        elif offer.type == RESUME_OFFER:
            self.resume()
        del self.offers[offer]
    else:
        player.offerError(offer, ACTION_ERROR_NONE_TO_ACCEPT)","'GameModel.acceptReceived: accepter=%s %s' % (repr(player), offer)",f'GameModel.acceptReceived: accepter={repr(player)} {offer}',f'GameModel.acceptReceived: accepter={repr(player)} {offer}',1,,,,
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/Utils/GameModel.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/Utils/GameModel.py,GameModel,"def acceptReceived(self, player, offer):
    log.debug('GameModel.acceptReceived: accepter=%s %s' % (repr(player), offer))
    if player == self.players[WHITE]:
        opPlayer = self.players[BLACK]
    else:
        opPlayer = self.players[WHITE]
    if offer in self.offers and self.offers[offer] == opPlayer:
        if offer.type == DRAW_OFFER:
            self.end(DRAW, DRAW_AGREE)
        elif offer.type == TAKEBACK_OFFER:
            log.debug('GameModel.acceptReceived: undoMoves(%s)' % offer.param)
            self.undoMoves(offer.param)
        elif offer.type == ADJOURN_OFFER:
            self.end(ADJOURNED, ADJOURNED_AGREEMENT)
        elif offer.type == ABORT_OFFER:
            self.end(ABORTED, ABORTED_AGREEMENT)
        elif offer.type == PAUSE_OFFER:
            self.pause()
        elif offer.type == RESUME_OFFER:
            self.resume()
        del self.offers[offer]
    else:
        player.offerError(offer, ACTION_ERROR_NONE_TO_ACCEPT)",'GameModel.acceptReceived: undoMoves(%s)' % offer.param,f'GameModel.acceptReceived: undoMoves({offer.param})',f'GameModel.acceptReceived: undoMoves({offer.param})',1,,,,
django-wiki,https://github.com/django-wiki/django-wiki/tree/master/tests/plugins/images/test_markdown.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-wiki/tests/plugins/images/test_markdown.py,ImageMarkdownTests,"def check_escape(self, text_to_escape):
    md = markdown.ArticleMarkdown(article=self.root_article)
    md_text = md.convert('`%s`' % text_to_escape)
    self.assertNotIn('<figure', md_text)
    self.assertIn(text_to_escape, md_text)",'`%s`' % text_to_escape,f'`{text_to_escape}`',f'`{text_to_escape}`',1,,,,
EasyTransfer,https://github.com/alibaba/EasyTransfer/tree/master/scripts/fashion_bert/image_feature_extract.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyTransfer/scripts/fashion_bert/image_feature_extract.py,PredictorImpl,"def search_pb(self, directory):
    """"""
    search pb file recursively, if multiple pb files exist, exception will be
    raised

    Returns:
      directory contain pb file
    """"""
    dir_list = []
    for (root, dirs, files) in tf.gfile.Walk(directory):
        for f in files:
            (_, ext) = os.path.splitext(f)
            if ext == '.pb':
                dir_list.append(root)
    if len(dir_list) == 0:
        raise ValueError('savedmodel is not found in directory %s' % directory)
    elif len(dir_list) > 1:
        raise ValueError('multiple saved model found in directory %s' % directory)
    return dir_list[0]",'savedmodel is not found in directory %s' % directory,f'savedmodel is not found in directory {directory}',f'savedmodel is not found in directory {directory}',1,,,,
EasyTransfer,https://github.com/alibaba/EasyTransfer/tree/master/scripts/fashion_bert/image_feature_extract.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyTransfer/scripts/fashion_bert/image_feature_extract.py,PredictorImpl,"def search_pb(self, directory):
    """"""
    search pb file recursively, if multiple pb files exist, exception will be
    raised

    Returns:
      directory contain pb file
    """"""
    dir_list = []
    for (root, dirs, files) in tf.gfile.Walk(directory):
        for f in files:
            (_, ext) = os.path.splitext(f)
            if ext == '.pb':
                dir_list.append(root)
    if len(dir_list) == 0:
        raise ValueError('savedmodel is not found in directory %s' % directory)
    elif len(dir_list) > 1:
        raise ValueError('multiple saved model found in directory %s' % directory)
    return dir_list[0]",'multiple saved model found in directory %s' % directory,f'multiple saved model found in directory {directory}',f'multiple saved model found in directory {directory}',1,,,,
Misago,https://github.com/rafalp/Misago/tree/master/misago/themes/uploadto.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Misago/misago/themes/uploadto.py,,"def upload_media_to(instance, filename):
    filename = add_hash_to_filename(instance.hash, filename)
    return 'themes/%s/media/%s' % (instance.theme.dirname, filename)","'themes/%s/media/%s' % (instance.theme.dirname, filename)",f'themes/{instance.theme.dirname}/media/{filename}',f'themes/{instance.theme.dirname}/media/{filename}',1,,,,
ansible,https://github.com/ansible/ansible/tree/master/test/support/integration/plugins/inventory/foreman.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/support/integration/plugins/inventory/foreman.py,InventoryModule,"def _get_facts_by_id(self, hid):
    url = '%s/api/v2/hosts/%s/facts' % (self.foreman_url, hid)
    return self._get_json(url)","'%s/api/v2/hosts/%s/facts' % (self.foreman_url, hid)",f'{self.foreman_url}/api/v2/hosts/{hid}/facts',f'{self.foreman_url}/api/v2/hosts/{hid}/facts',1,,,,
youtube-dl,https://github.com/lrvick/youtube-dl/tree/master/youtube_dl/extractor/rtve.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/youtube-dl/youtube_dl/extractor/rtve.py,RTVEALaCartaIE,"def _extract_png_formats(self, video_id):
    png = self._download_webpage('http://www.rtve.es/ztnr/movil/thumbnail/%s/videos/%s.png' % (self._manager, video_id), video_id, 'Downloading url information', query={'q': 'v2'})
    q = qualities(['Media', 'Alta', 'HQ', 'HD_READY', 'HD_FULL'])
    formats = []
    for (quality, video_url) in self._decrypt_url(png):
        ext = determine_ext(video_url)
        if ext == 'm3u8':
            formats.extend(self._extract_m3u8_formats(video_url, video_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))
        elif ext == 'mpd':
            formats.extend(self._extract_mpd_formats(video_url, video_id, 'dash', fatal=False))
        else:
            formats.append({'format_id': quality, 'quality': q(quality), 'url': video_url})
    self._sort_formats(formats)
    return formats","'http://www.rtve.es/ztnr/movil/thumbnail/%s/videos/%s.png' % (self._manager, video_id)",f'http://www.rtve.es/ztnr/movil/thumbnail/{self._manager}/videos/{video_id}.png',f'http://www.rtve.es/ztnr/movil/thumbnail/{self._manager}/videos/{video_id}.png',1,,,,
sympy,https://github.com/sympy/sympy/tree/master/sympy/printing/julia.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/printing/julia.py,JuliaCodePrinter,"def _print_Indexed(self, expr):
    inds = [self._print(i) for i in expr.indices]
    return '%s[%s]' % (self._print(expr.base.label), ','.join(inds))","'%s[%s]' % (self._print(expr.base.label), ','.join(inds))","f""{self._print(expr.base.label)}[{','.join(inds)}]""","f""{self._print(expr.base.label)}[{','.join(inds)}]""",1,,,,
glance,https://github.com/openstack/glance/tree/master/glance/tests/functional/db/base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/glance/glance/tests/functional/db/base.py,DriverTests,"def test_image_member_create(self, mock_utcnow):
    mock_utcnow.return_value = datetime.datetime.utcnow()
    memberships = self.db_api.image_member_find(self.context)
    self.assertEqual([], memberships)
    TENANT1 = str(uuid.uuid4())
    self.context.auth_token = 'user:%s:user' % TENANT1
    self.db_api.image_member_create(self.context, {'member': TENANT1, 'image_id': UUID1})
    memberships = self.db_api.image_member_find(self.context)
    self.assertEqual(1, len(memberships))
    actual = memberships[0]
    self.assertIsNotNone(actual['created_at'])
    self.assertIsNotNone(actual['updated_at'])
    actual.pop('id')
    actual.pop('created_at')
    actual.pop('updated_at')
    expected = {'member': TENANT1, 'image_id': UUID1, 'can_share': False, 'status': 'pending', 'deleted': False}
    self.assertEqual(expected, actual)",'user:%s:user' % TENANT1,f'user:{TENANT1}:user',f'user:{TENANT1}:user',1,,,,
swift,https://github.com/openstack/swift/tree/master/test/unit/common/middleware/test_keystoneauth.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/common/middleware/test_keystoneauth.py,TestAuthorizeReaderSystem,"def test_reader_put_elsewhere_fails(self):
    roles = operator_roles(self.test_auth) + [self.system_reader_role_1]
    identity = self._get_identity(roles=roles)
    account = '%s%s' % (self._get_account(identity), '2')
    self._check_authenticate(exception=HTTP_FORBIDDEN, identity=identity, account=account, env={'REQUEST_METHOD': 'PUT'})","'%s%s' % (self._get_account(identity), '2')",f'{self._get_account(identity)}2',f'{self._get_account(identity)}2',1,,,,
numpy,https://github.com/numpy/numpy/tree/master/numpy/distutils/command/install.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/distutils/command/install.py,install,"def run(self):
    if not have_setuptools:
        r = old_install.run(self)
    else:
        r = self.setuptools_run()
    if self.record:
        with open(self.record, 'r') as f:
            lines = []
            need_rewrite = False
            for l in f:
                l = l.rstrip()
                if ' ' in l:
                    need_rewrite = True
                    l = '""%s""' % l
                lines.append(l)
        if need_rewrite:
            self.execute(write_file, (self.record, lines), ""re-writing list of installed files to '%s'"" % self.record)
    return r",'"%s"' % l,f'"{l}"',f'"{l}"',1,,,,
tensorflow-ocr,https://github.com/pannous/tensorflow-ocr/tree/master//net.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-ocr//net.py,net,"def buildDenseConv(self, nBlocks=3, nChannels=64, magic_factor=0):
    if magic_factor:
        print('magic_factor DEPRECATED!')
    depth = 3 * nBlocks + 4
    if (depth - 4) % 3:
        raise Exception('Depth must be 3N + 4! (4,7,10,...) ')
    N = (depth - 4) // 3
    print('N=%d' % N)
    do_dropout = True
    growthRate = 12
    self.conv([3, 3, 1, nChannels])
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.addTransition(nChannels, nChannels, do_dropout)
    for i in range(N):
        self.addLayer(nChannels, growthRate, do_dropout)
        nChannels += growthRate
    self.batchnorm()
    self.add(tf.nn.relu(self.last_layer))
    self.add(tf.nn.max_pool(self.last_layer, ksize=[1, 4, 4, 1], strides=[1, 2, 2, 1], padding='SAME'))
    shape = self.last_layer.get_shape()
    nBytes = shape[1] * shape[2] * shape[3]
    self.reshape([-1, int(nBytes)])",'N=%d' % N,f'N={N}',f'N={N}',1,,,,
aliyun-openapi-python-sdk,https://github.com/aliyun/aliyun-openapi-python-sdk/tree/master/aliyun-python-sdk-core-v3/aliyunsdkcore/acs_exception/exceptions.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aliyun-openapi-python-sdk/aliyun-python-sdk-core-v3/aliyunsdkcore/acs_exception/exceptions.py,ServerException,"def __str__(self):
    return 'HTTP Status: %s Error:%s %s RequestID: %s' % (str(self.http_status), self.error_code, self.message, self.request_id)","'HTTP Status: %s Error:%s %s RequestID: %s' % (str(self.http_status), self.error_code, self.message, self.request_id)",f'HTTP Status: {str(self.http_status)} Error:{self.error_code} {self.message} RequestID: {self.request_id}',f'HTTP Status: {str(self.http_status)} Error:{self.error_code} {self.message} RequestID: {self.request_id}',1,,,,
clusterfuzz,https://github.com/google/clusterfuzz/tree/master/src/appengine/handlers/jobs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/appengine/handlers/jobs.py,DeleteJobHandler,"def post(self):
    """"""Handle a post request.""""""
    key = helpers.get_integer_key(request)
    job = ndb.Key(data_types.Job, key).get()
    if not job:
        raise helpers.EarlyExitException('Job not found.', 400)
    for fuzzer in ndb_utils.get_all_from_model(data_types.Fuzzer):
        if job.name in fuzzer.jobs:
            fuzzer.jobs.remove(job.name)
            fuzzer.put()
    query = data_types.FuzzerJob.query()
    query = query.filter(data_types.FuzzerJob.job == job.name)
    for mapping in ndb_utils.get_all_from_query(query):
        mapping.key.delete()
    job.key.delete()
    helpers.log('Deleted job %s' % job.name, helpers.MODIFY_OPERATION)
    return self.redirect('/jobs')",'Deleted job %s' % job.name,f'Deleted job {job.name}',f'Deleted job {job.name}',1,,,,
ParlAI,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/eli5/data_creation/download_reddit_qalist.py,,"def download_and_process_comments(post_ids, st_time):
    lines = defaultdict(list)
    subreddit_names = set()
    for post_id in post_ids:
        for (i, (name, l)) in enumerate(get_comments_from_post(post_id)):
            if i % 1000000 == 0:
                print('read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()])), time() - st_time)
            lines[name] += [l.strip()]
            subreddit_names.add(name)
    print('tokenizing and selecting specific posts %2f' % (time() - st_time))
    processed_items = dict([(name, []) for name in subreddit_names])
    key_list = ['id', 'link_id', 'parent_id', 'score', 'body']
    for name in subreddit_names:
        for line in lines[name]:
            reddit_dct = json.loads(line)
            if valid_comment(reddit_dct):
                reddit_res = {}
                for k in key_list:
                    if k == 'body':
                        if reddit_dct[k].lower() in ['[removed]', '[deleted]']:
                            reddit_dct[k] = ''
                        (txt, url_list) = word_url_tokenize(reddit_dct[k])
                        reddit_res[k] = (' '.join(txt.split()), url_list)
                    else:
                        reddit_res[k] = reddit_dct[k]
                processed_items[name] += [reddit_res]
    print('Total found %d' % len(processed_items), time() - st_time)
    return (subreddit_names, processed_items)","'read %d lines, found %d' % (i, sum([len(ls) for ls in lines.values()]))","f'read {i} lines, found {sum([len(ls) for ls in lines.values()])}'","f'read {i} lines, found {sum([len(ls) for ls in lines.values()])}'",1,,,,
yt-dlp,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/common.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlp/yt_dlp/extractor/common.py,SearchInfoExtractor,"def _real_extract(self, query):
    mobj = re.match(self._make_valid_url(), query)
    if mobj is None:
        raise ExtractorError('Invalid search query ""%s""' % query)
    prefix = mobj.group('prefix')
    query = mobj.group('query')
    if prefix == '':
        return self._get_n_results(query, 1)
    elif prefix == 'all':
        return self._get_n_results(query, self._MAX_RESULTS)
    else:
        n = int(prefix)
        if n <= 0:
            raise ExtractorError('invalid download number %s for query ""%s""' % (n, query))
        elif n > self._MAX_RESULTS:
            self.report_warning('%s returns max %i results (you requested %i)' % (self._SEARCH_KEY, self._MAX_RESULTS, n))
            n = self._MAX_RESULTS
        return self._get_n_results(query, n)",'Invalid search query "%s"' % query,f'Invalid search query "{query}"',f'Invalid search query "{query}"',1,,,,
yt-dlp,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/common.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlp/yt_dlp/extractor/common.py,SearchInfoExtractor,"def _real_extract(self, query):
    mobj = re.match(self._make_valid_url(), query)
    if mobj is None:
        raise ExtractorError('Invalid search query ""%s""' % query)
    prefix = mobj.group('prefix')
    query = mobj.group('query')
    if prefix == '':
        return self._get_n_results(query, 1)
    elif prefix == 'all':
        return self._get_n_results(query, self._MAX_RESULTS)
    else:
        n = int(prefix)
        if n <= 0:
            raise ExtractorError('invalid download number %s for query ""%s""' % (n, query))
        elif n > self._MAX_RESULTS:
            self.report_warning('%s returns max %i results (you requested %i)' % (self._SEARCH_KEY, self._MAX_RESULTS, n))
            n = self._MAX_RESULTS
        return self._get_n_results(query, n)","'invalid download number %s for query ""%s""' % (n, query)",f'invalid download number {n} for query "{query}"',f'invalid download number {n} for query "{query}"',1,,,,
yt-dlp,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/common.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlp/yt_dlp/extractor/common.py,SearchInfoExtractor,"def _real_extract(self, query):
    mobj = re.match(self._make_valid_url(), query)
    if mobj is None:
        raise ExtractorError('Invalid search query ""%s""' % query)
    prefix = mobj.group('prefix')
    query = mobj.group('query')
    if prefix == '':
        return self._get_n_results(query, 1)
    elif prefix == 'all':
        return self._get_n_results(query, self._MAX_RESULTS)
    else:
        n = int(prefix)
        if n <= 0:
            raise ExtractorError('invalid download number %s for query ""%s""' % (n, query))
        elif n > self._MAX_RESULTS:
            self.report_warning('%s returns max %i results (you requested %i)' % (self._SEARCH_KEY, self._MAX_RESULTS, n))
            n = self._MAX_RESULTS
        return self._get_n_results(query, n)","'%s returns max %i results (you requested %i)' % (self._SEARCH_KEY, self._MAX_RESULTS, n)",f'{self._SEARCH_KEY} returns max {self._MAX_RESULTS} results (you requested {n})',f'{self._SEARCH_KEY} returns max {self._MAX_RESULTS} results (you requested {n})',1,,,,
WeasyPrint,https://github.com/Kozea/WeasyPrint/tree/master/tests/layout/test_block.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WeasyPrint/tests/layout/test_block.py,,"def test_vertical_space_6(margin_1, margin_2, result):
    (page,) = render_pages('\n      <style>\n        p { font: 20px/1 serif } /* block height == 20px */\n        #div1 { margin-top: %s; overflow: hidden }\n        #div2 { margin-top: %s }\n      </style>\n      <p>Lorem ipsum\n      <div id=div1>\n        <div id=div2>\n          <p id=p2>dolor sit amet\n        </div>\n      </div>\n    ' % (margin_1, margin_2))
    (html,) = page.children
    (body,) = html.children
    (p1, div1) = body.children
    (div2,) = div1.children
    (p2,) = div2.children
    p1_bottom = p1.content_box_y() + p1.height
    p2_top = p2.content_box_y()
    assert p2_top - p1_bottom == result","'\n      <style>\n        p { font: 20px/1 serif } /* block height == 20px */\n        #div1 { margin-top: %s; overflow: hidden }\n        #div2 { margin-top: %s }\n      </style>\n      <p>Lorem ipsum\n      <div id=div1>\n        <div id=div2>\n          <p id=p2>dolor sit amet\n        </div>\n      </div>\n    ' % (margin_1, margin_2)",f'\n      <style>\n        p {{ font: 20px/1 serif }} /* block height == 20px */\n        #div1 {{ margin-top: {margin_1}; overflow: hidden }}\n        #div2 {{ margin-top: {margin_2} }}\n      </style>\n      <p>Lorem ipsum\n      <div id=div1>\n        <div id=div2>\n          <p id=p2>dolor sit amet\n        </div>\n      </div>\n    ',f'\n      <style>\n        p {{ font: 20px/1 serif }} /* block height == 20px */\n        #div1 {{ margin-top: {margin_1}; overflow: hidden }}\n        #div2 {{ margin-top: {margin_2} }}\n      </style>\n      <p>Lorem ipsum\n      <div id=div1>\n        <div id=div2>\n          <p id=p2>dolor sit amet\n        </div>\n      </div>\n    ',1,,,,
flask-peewee,https://github.com/coleifer/flask-peewee/tree/master/flask_peewee/tests/rest.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flask-peewee/flask_peewee/tests/rest.py,RestApiOwnerAuthTestCase,"def test_detail_get(self):
    resp = self.app.get('/api/message/1/')
    self.assertEqual(resp.status_code, 404)
    self.create_messages()
    resp = self.app.get('/api/message/%s/' % self.normal_message.id)
    resp_json = self.response_json(resp)
    self.assertAPIMessage(resp_json, self.normal_message)",'/api/message/%s/' % self.normal_message.id,f'/api/message/{self.normal_message.id}/',f'/api/message/{self.normal_message.id}/',1,,,,
cobbler,https://github.com/cobbler/cobbler/tree/master/cobbler/cli.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cobbler/cobbler/cli.py,CobblerCLI,"def start_task(self, name: str, options: dict) -> str:
    """"""
        Start an asynchronous task in the background.

        :param name: ""background\\_"" % name function must exist in remote.py. This function will be called in a
                      subthread.
        :param options: Dictionary of options passed to the newly started thread
        :return: Id of the newly started task
        """"""
    options = utils.strip_none(vars(options), omit_none=True)
    fn = getattr(self.remote, 'background_%s' % name)
    return fn(options, self.token)",'background_%s' % name,f'background_{name}',f'background_{name}',1,,,,
circus,https://github.com/circus-tent/circus/tree/master/circus/plugins/statsd.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/circus/circus/plugins/statsd.py,StatsdClient,"def timed(self, bucket, value):
    self.send(bucket, '%s|ms' % value)",'%s|ms' % value,f'{value}|ms',f'{value}|ms',1,,,,
pony,https://github.com/ponyorm/pony/tree/master/pony/orm/tests/testutils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pony/pony/orm/tests/testutils.py,,"def decorator(func):

    def wrapper(test_case, *args, **kwargs):
        try:
            func(test_case, *args, **kwargs)
            test_case.fail(""Expected exception %s wasn't raised"" % exc_class.__name__)
        except exc_class as e:
            if not e.args:
                test_case.assertEqual(test_msg, None)
            else:
                test_exception_msg(test_case, str(e), test_msg)
    wrapper.__name__ = func.__name__
    return wrapper","""Expected exception %s wasn't raised"" % exc_class.__name__",f"Expected exception {exc_class.__name__} wasn't raised",f"Expected exception {exc_class.__name__} wasn't raised",1,,,,
vispy,https://github.com/vispy/vispy/tree/master/codegen/createglapi.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vispy/codegen/createglapi.py,ProxyApiGenerator,"def _add_function(self, des):
    ret = self._returns(des)
    prefix = 'return ' if ret else ''
    argstr = ', '.join(des.args)
    self.lines.append('    def %s(self, %s):' % (des.apiname, argstr))
    self.lines.append('        %sself(""%s"", %r, %s)' % (prefix, apiname(des.name), ret, argstr))","'    def %s(self, %s):' % (des.apiname, argstr)","f'    def {des.apiname}(self, {argstr}):'","f'    def {des.apiname}(self, {argstr}):'",1,,,,
vispy,https://github.com/vispy/vispy/tree/master/codegen/createglapi.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vispy/codegen/createglapi.py,ProxyApiGenerator,"def _add_function(self, des):
    ret = self._returns(des)
    prefix = 'return ' if ret else ''
    argstr = ', '.join(des.args)
    self.lines.append('    def %s(self, %s):' % (des.apiname, argstr))
    self.lines.append('        %sself(""%s"", %r, %s)' % (prefix, apiname(des.name), ret, argstr))","'        %sself(""%s"", %r, %s)' % (prefix, apiname(des.name), ret, argstr)","f'        {prefix}self(""{apiname(des.name)}"", {ret}, {argstr})'","f'        {prefix}self(""{apiname(des.name)}"", {ret}, {argstr})'",1,,,,
angr,https://github.com/angr/angr/tree/master/angr/knowledge_plugins/variables/variable_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/knowledge_plugins/variables/variable_manager.py,VariableManagerInternal,"def assign_variable_names(self, labels=None, types=None):
    """"""
        Assign default names to all SSA variables.

        :param labels:  Known labels in the binary.
        :return:        None
        """"""
    for var in self._variables:
        if (types is None or SimStackVariable in types) and isinstance(var, SimStackVariable):
            if var.name is not None:
                continue
            if var.ident.startswith('iarg'):
                var.name = 'arg_%x' % var.offset
            else:
                var.name = 's_%x' % -var.offset
        elif (types is None or SimRegisterVariable in types) and isinstance(var, SimRegisterVariable):
            if var.name is not None:
                continue
            var.name = var.ident
        elif (types is None or SimMemoryVariable in types) and isinstance(var, SimMemoryVariable):
            if var.name is not None:
                continue
            if labels is not None and var.addr in labels:
                var.name = labels[var.addr]
                if '@@' in var.name:
                    var.name = var.name[:var.name.index('@@')]
            elif isinstance(var.addr, int):
                var.name = 'g_%x' % var.addr
            elif var.ident is not None:
                var.name = var.ident
            else:
                var.name = 'g_%s' % var.addr",'arg_%x' % var.offset,f'arg_{var.offset:x}',f'arg_{var.offset:x}',1,,,,
angr,https://github.com/angr/angr/tree/master/angr/knowledge_plugins/variables/variable_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/knowledge_plugins/variables/variable_manager.py,VariableManagerInternal,"def assign_variable_names(self, labels=None, types=None):
    """"""
        Assign default names to all SSA variables.

        :param labels:  Known labels in the binary.
        :return:        None
        """"""
    for var in self._variables:
        if (types is None or SimStackVariable in types) and isinstance(var, SimStackVariable):
            if var.name is not None:
                continue
            if var.ident.startswith('iarg'):
                var.name = 'arg_%x' % var.offset
            else:
                var.name = 's_%x' % -var.offset
        elif (types is None or SimRegisterVariable in types) and isinstance(var, SimRegisterVariable):
            if var.name is not None:
                continue
            var.name = var.ident
        elif (types is None or SimMemoryVariable in types) and isinstance(var, SimMemoryVariable):
            if var.name is not None:
                continue
            if labels is not None and var.addr in labels:
                var.name = labels[var.addr]
                if '@@' in var.name:
                    var.name = var.name[:var.name.index('@@')]
            elif isinstance(var.addr, int):
                var.name = 'g_%x' % var.addr
            elif var.ident is not None:
                var.name = var.ident
            else:
                var.name = 'g_%s' % var.addr",'s_%x' % -var.offset,f's_{-var.offset:x}',f's_{-var.offset:x}',1,,,,
angr,https://github.com/angr/angr/tree/master/angr/knowledge_plugins/variables/variable_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/knowledge_plugins/variables/variable_manager.py,VariableManagerInternal,"def assign_variable_names(self, labels=None, types=None):
    """"""
        Assign default names to all SSA variables.

        :param labels:  Known labels in the binary.
        :return:        None
        """"""
    for var in self._variables:
        if (types is None or SimStackVariable in types) and isinstance(var, SimStackVariable):
            if var.name is not None:
                continue
            if var.ident.startswith('iarg'):
                var.name = 'arg_%x' % var.offset
            else:
                var.name = 's_%x' % -var.offset
        elif (types is None or SimRegisterVariable in types) and isinstance(var, SimRegisterVariable):
            if var.name is not None:
                continue
            var.name = var.ident
        elif (types is None or SimMemoryVariable in types) and isinstance(var, SimMemoryVariable):
            if var.name is not None:
                continue
            if labels is not None and var.addr in labels:
                var.name = labels[var.addr]
                if '@@' in var.name:
                    var.name = var.name[:var.name.index('@@')]
            elif isinstance(var.addr, int):
                var.name = 'g_%x' % var.addr
            elif var.ident is not None:
                var.name = var.ident
            else:
                var.name = 'g_%s' % var.addr",'g_%x' % var.addr,f'g_{var.addr:x}',f'g_{var.addr:x}',1,,,,
angr,https://github.com/angr/angr/tree/master/angr/knowledge_plugins/variables/variable_manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/knowledge_plugins/variables/variable_manager.py,VariableManagerInternal,"def assign_variable_names(self, labels=None, types=None):
    """"""
        Assign default names to all SSA variables.

        :param labels:  Known labels in the binary.
        :return:        None
        """"""
    for var in self._variables:
        if (types is None or SimStackVariable in types) and isinstance(var, SimStackVariable):
            if var.name is not None:
                continue
            if var.ident.startswith('iarg'):
                var.name = 'arg_%x' % var.offset
            else:
                var.name = 's_%x' % -var.offset
        elif (types is None or SimRegisterVariable in types) and isinstance(var, SimRegisterVariable):
            if var.name is not None:
                continue
            var.name = var.ident
        elif (types is None or SimMemoryVariable in types) and isinstance(var, SimMemoryVariable):
            if var.name is not None:
                continue
            if labels is not None and var.addr in labels:
                var.name = labels[var.addr]
                if '@@' in var.name:
                    var.name = var.name[:var.name.index('@@')]
            elif isinstance(var.addr, int):
                var.name = 'g_%x' % var.addr
            elif var.ident is not None:
                var.name = var.ident
            else:
                var.name = 'g_%s' % var.addr",'g_%s' % var.addr,f'g_{var.addr}',f'g_{var.addr}',1,,,,
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/blocks/plan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/blocks/plan.py,TrainPlan,"def _run(self, supervisor, session):
    train_feed_dict = self.train_feeds.copy()
    train_fetches = {'train_op': self.train_op, 'loss': self.loss_total, 'step': self.global_step}
    if self.compute_summaries:
        train_fetches['summaries'] = self.summaries
    if self.examples:
        (epochs, train_size) = self._by_feed_dict(train_feed_dict)
    else:
        (epochs, train_size) = self._by_input_tensor(train_feed_dict)
    if self.dev_examples:
        gen_dev_batches = util.epochs(((len(batch), self.compiler.build_feed_dict(batch)) for batch in util.group_by_batches(self.dev_examples, self.batch_size)), shuffle=False)
        ckpt = tf.train.get_checkpoint_state(self.logdir)
        if ckpt and ckpt.model_checkpoint_path:
            (_, self._best_loss, _) = self._eval_batches(supervisor, session, next(gen_dev_batches), None, is_dev=True)
            if self._best_loss is None:
                return
    for (epoch, batches) in enumerate(epochs, 1):
        train_loss = 0.0
        for _ in batches:
            if self._should_stop(supervisor):
                return
            results = session.run(train_fetches, train_feed_dict)
            train_loss += results['loss']
            if self.compute_summaries:
                supervisor.summary_computed(session, results['summaries'], results['step'])
        if train_size == 0:
            raise ValueError('examples must be non-empty')
        if self.exact_batch_sizes and epoch == 1:
            if train_size < self.batch_size:
                raise ValueError('when exact_batch_sizes is true, examples must have at least batch_size items; %s vs. %s' % (train_size, self.batch_size))
            train_size -= train_size % self.batch_size
        train_loss /= train_size
        self.report_loss(results['step'], train_loss)
        log_str = 'epoch:%5d train[loss: %.3e]' % (epoch, train_loss)
        if self.dev_examples:
            (dev_size, dev_loss, dev_metrics) = self._eval_batches(supervisor, session, next(gen_dev_batches), results['step'], is_dev=True)
            if dev_size is None:
                return
            if epoch == 1:
                self.log_and_print('train_size: %d dev_size: %d' % (train_size, dev_size))
            log_str += ' dev[%s]' % _eval_str(dev_size, dev_loss, dev_metrics)
            self.log_and_print(log_str)
            self._save_best(session, supervisor.saver, dev_loss, results['step'])
        else:
            if epoch == 1:
                self.log_and_print('train_size: %d' % train_size)
            self.log_and_print(log_str)
    if not self.dev_examples and self.is_chief_trainer:
        save_path = os.path.join(self.logdir, 'model.ckpt')
        save_fname = supervisor.saver.save(session, save_path, global_step=results['step'])
        self.log_and_print('final model saved in file: %s' % save_fname)","'epoch:%5d train[loss: %.3e]' % (epoch, train_loss)",f'epoch:{epoch:5d} train[loss: {train_loss:.3e}]',f'epoch:{epoch:5d} train[loss: {train_loss:.3e}]',1,,,,
fold,https://github.com/tensorflow/fold/tree/master/tensorflow_fold/blocks/plan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fold/tensorflow_fold/blocks/plan.py,TrainPlan,"def _run(self, supervisor, session):
    train_feed_dict = self.train_feeds.copy()
    train_fetches = {'train_op': self.train_op, 'loss': self.loss_total, 'step': self.global_step}
    if self.compute_summaries:
        train_fetches['summaries'] = self.summaries
    if self.examples:
        (epochs, train_size) = self._by_feed_dict(train_feed_dict)
    else:
        (epochs, train_size) = self._by_input_tensor(train_feed_dict)
    if self.dev_examples:
        gen_dev_batches = util.epochs(((len(batch), self.compiler.build_feed_dict(batch)) for batch in util.group_by_batches(self.dev_examples, self.batch_size)), shuffle=False)
        ckpt = tf.train.get_checkpoint_state(self.logdir)
        if ckpt and ckpt.model_checkpoint_path:
            (_, self._best_loss, _) = self._eval_batches(supervisor, session, next(gen_dev_batches), None, is_dev=True)
            if self._best_loss is None:
                return
    for (epoch, batches) in enumerate(epochs, 1):
        train_loss = 0.0
        for _ in batches:
            if self._should_stop(supervisor):
                return
            results = session.run(train_fetches, train_feed_dict)
            train_loss += results['loss']
            if self.compute_summaries:
                supervisor.summary_computed(session, results['summaries'], results['step'])
        if train_size == 0:
            raise ValueError('examples must be non-empty')
        if self.exact_batch_sizes and epoch == 1:
            if train_size < self.batch_size:
                raise ValueError('when exact_batch_sizes is true, examples must have at least batch_size items; %s vs. %s' % (train_size, self.batch_size))
            train_size -= train_size % self.batch_size
        train_loss /= train_size
        self.report_loss(results['step'], train_loss)
        log_str = 'epoch:%5d train[loss: %.3e]' % (epoch, train_loss)
        if self.dev_examples:
            (dev_size, dev_loss, dev_metrics) = self._eval_batches(supervisor, session, next(gen_dev_batches), results['step'], is_dev=True)
            if dev_size is None:
                return
            if epoch == 1:
                self.log_and_print('train_size: %d dev_size: %d' % (train_size, dev_size))
            log_str += ' dev[%s]' % _eval_str(dev_size, dev_loss, dev_metrics)
            self.log_and_print(log_str)
            self._save_best(session, supervisor.saver, dev_loss, results['step'])
        else:
            if epoch == 1:
                self.log_and_print('train_size: %d' % train_size)
            self.log_and_print(log_str)
    if not self.dev_examples and self.is_chief_trainer:
        save_path = os.path.join(self.logdir, 'model.ckpt')
        save_fname = supervisor.saver.save(session, save_path, global_step=results['step'])
        self.log_and_print('final model saved in file: %s' % save_fname)","'train_size: %d dev_size: %d' % (train_size, dev_size)",f'train_size: {train_size} dev_size: {dev_size}',f'train_size: {train_size} dev_size: {dev_size}',1,,,,
oppia,https://github.com/oppia/oppia/tree/master/core/controllers/blog_homepage_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/controllers/blog_homepage_test.py,BlogPostDataHandlerTest,"def test_raise_exception_if_blog_post_does_not_exists(self) -> None:
    self.login(self.user_email)
    blog_post = blog_services.get_blog_post_by_id(self.blog_post_one.id)
    self.get_json('%s/%s' % (feconf.BLOG_HOMEPAGE_DATA_URL, blog_post.url_fragment))
    blog_services.delete_blog_post(blog_post.id)
    self.get_json('%s/%s' % (feconf.BLOG_HOMEPAGE_DATA_URL, blog_post.url_fragment), expected_status_int=404)","'%s/%s' % (feconf.BLOG_HOMEPAGE_DATA_URL, blog_post.url_fragment)",f'{feconf.BLOG_HOMEPAGE_DATA_URL}/{blog_post.url_fragment}',f'{feconf.BLOG_HOMEPAGE_DATA_URL}/{blog_post.url_fragment}',1,,,,
oppia,https://github.com/oppia/oppia/tree/master/core/controllers/blog_homepage_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/controllers/blog_homepage_test.py,BlogPostDataHandlerTest,"def test_raise_exception_if_blog_post_does_not_exists(self) -> None:
    self.login(self.user_email)
    blog_post = blog_services.get_blog_post_by_id(self.blog_post_one.id)
    self.get_json('%s/%s' % (feconf.BLOG_HOMEPAGE_DATA_URL, blog_post.url_fragment))
    blog_services.delete_blog_post(blog_post.id)
    self.get_json('%s/%s' % (feconf.BLOG_HOMEPAGE_DATA_URL, blog_post.url_fragment), expected_status_int=404)","'%s/%s' % (feconf.BLOG_HOMEPAGE_DATA_URL, blog_post.url_fragment)",f'{feconf.BLOG_HOMEPAGE_DATA_URL}/{blog_post.url_fragment}',f'{feconf.BLOG_HOMEPAGE_DATA_URL}/{blog_post.url_fragment}',1,,,,
pyqtgraph,https://github.com/pyqtgraph/pyqtgraph/tree/master/pyqtgraph/examples/ScatterPlotSpeedTest.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyqtgraph/pyqtgraph/examples/ScatterPlotSpeedTest.py,,"def update():
    global ptr, lastTime, fps
    mode = param['mode']
    if mode == 'newItem':
        mkItem()
    elif mode == 'reuseItem':
        item.setData(**getData())
    elif mode == 'panZoom':
        item.viewTransformChanged()
        item.update()
    elif mode == 'hover':
        pts = item.points()
        old = pts[(ptr - 1) % len(pts)]
        new = pts[ptr % len(pts)]
        item.pointsAt(new.pos())
        old.resetBrush()
        new.setBrush(hoverBrush)
    ptr += 1
    now = perf_counter()
    dt = now - lastTime
    lastTime = now
    if fps is None:
        fps = 1.0 / dt
    else:
        s = np.clip(dt * 3.0, 0, 1)
        fps = fps * (1 - s) + 1.0 / dt * s
    p.setTitle('%0.2f fps' % fps)
    p.repaint()",'%0.2f fps' % fps,f'{fps:.2f} fps',f'{fps:.2f} fps',1,,,,
shuup,https://github.com/shuup/shuup/tree/master/shuup/admin/modules/products/views/edit_media.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/admin/modules/products/views/edit_media.py,ProductMediaBulkAdderView,"def post(self, *args, **kwargs):
    ids = self.request.POST.getlist('file_ids')
    shop_product_id = kwargs.pop('pk')
    kind = self.request.POST.get('kind')
    shop = self.request.shop
    shop_id = self.request.POST.get('shop_id', shop.pk)
    if not ids or not shop_product_id:
        return JsonResponse({'response': 'error', 'message': 'Error! Bad request.'}, status=400)
    if not Shop.objects.filter(pk=shop_id).exists():
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop id `%s`.' % shop_id}, status=400)
    shop_product = ShopProduct.objects.filter(pk=shop_product_id, shop_id=shop_id).first()
    if not shop_product:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop product id `%s`.' % shop_product_id}, status=400)
    if kind == 'images':
        kind = ProductMediaKind.IMAGE
    elif kind == 'media':
        kind = ProductMediaKind.GENERIC_FILE
    else:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid file kind `%s`.' % kind}, status=400)
    for file_id in ids:
        if not File.objects.filter(id=file_id).exists():
            return JsonResponse({'response': 'error', 'message': 'Error! Invalid file id `%s`.' % file_id}, status=400)
    added = []
    for file_id in ids:
        if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
            image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
            image.shops.add(shop_id)
            added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})
    return JsonResponse({'response': 'success', 'added': added, 'message': force_text(_('Files added to the product.'))})",'Error! Invalid shop id `%s`.' % shop_id,f'Error! Invalid shop id `{shop_id}`.',f'Error! Invalid shop id `{shop_id}`.',1,,,,
shuup,https://github.com/shuup/shuup/tree/master/shuup/admin/modules/products/views/edit_media.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/admin/modules/products/views/edit_media.py,ProductMediaBulkAdderView,"def post(self, *args, **kwargs):
    ids = self.request.POST.getlist('file_ids')
    shop_product_id = kwargs.pop('pk')
    kind = self.request.POST.get('kind')
    shop = self.request.shop
    shop_id = self.request.POST.get('shop_id', shop.pk)
    if not ids or not shop_product_id:
        return JsonResponse({'response': 'error', 'message': 'Error! Bad request.'}, status=400)
    if not Shop.objects.filter(pk=shop_id).exists():
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop id `%s`.' % shop_id}, status=400)
    shop_product = ShopProduct.objects.filter(pk=shop_product_id, shop_id=shop_id).first()
    if not shop_product:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop product id `%s`.' % shop_product_id}, status=400)
    if kind == 'images':
        kind = ProductMediaKind.IMAGE
    elif kind == 'media':
        kind = ProductMediaKind.GENERIC_FILE
    else:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid file kind `%s`.' % kind}, status=400)
    for file_id in ids:
        if not File.objects.filter(id=file_id).exists():
            return JsonResponse({'response': 'error', 'message': 'Error! Invalid file id `%s`.' % file_id}, status=400)
    added = []
    for file_id in ids:
        if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
            image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
            image.shops.add(shop_id)
            added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})
    return JsonResponse({'response': 'success', 'added': added, 'message': force_text(_('Files added to the product.'))})",'Error! Invalid shop product id `%s`.' % shop_product_id,f'Error! Invalid shop product id `{shop_product_id}`.',f'Error! Invalid shop product id `{shop_product_id}`.',1,,,,
shuup,https://github.com/shuup/shuup/tree/master/shuup/admin/modules/products/views/edit_media.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shuup/shuup/admin/modules/products/views/edit_media.py,ProductMediaBulkAdderView,"def post(self, *args, **kwargs):
    ids = self.request.POST.getlist('file_ids')
    shop_product_id = kwargs.pop('pk')
    kind = self.request.POST.get('kind')
    shop = self.request.shop
    shop_id = self.request.POST.get('shop_id', shop.pk)
    if not ids or not shop_product_id:
        return JsonResponse({'response': 'error', 'message': 'Error! Bad request.'}, status=400)
    if not Shop.objects.filter(pk=shop_id).exists():
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop id `%s`.' % shop_id}, status=400)
    shop_product = ShopProduct.objects.filter(pk=shop_product_id, shop_id=shop_id).first()
    if not shop_product:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid shop product id `%s`.' % shop_product_id}, status=400)
    if kind == 'images':
        kind = ProductMediaKind.IMAGE
    elif kind == 'media':
        kind = ProductMediaKind.GENERIC_FILE
    else:
        return JsonResponse({'response': 'error', 'message': 'Error! Invalid file kind `%s`.' % kind}, status=400)
    for file_id in ids:
        if not File.objects.filter(id=file_id).exists():
            return JsonResponse({'response': 'error', 'message': 'Error! Invalid file id `%s`.' % file_id}, status=400)
    added = []
    for file_id in ids:
        if not ProductMedia.objects.filter(product_id=shop_product.product_id, file_id=file_id, kind=kind, shops__in=[shop_id]).exists():
            image = ProductMedia.objects.create(product_id=shop_product.product_id, file_id=file_id, kind=kind)
            image.shops.add(shop_id)
            added.append({'product': image.product_id, 'file': int(file_id), 'kind': kind.value, 'product_media': image.pk})
    return JsonResponse({'response': 'success', 'added': added, 'message': force_text(_('Files added to the product.'))})",'Error! Invalid file kind `%s`.' % kind,f'Error! Invalid file kind `{kind}`.',f'Error! Invalid file kind `{kind}`.',1,,,,
barman,https://github.com/EnterpriseDB/barman/tree/master/barman/server.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/barman/barman/server.py,Server,"def check_backup_validity(self, check_strategy):
    """"""
        Check if backup validity requirements are satisfied

        :param CheckStrategy check_strategy: the strategy for the management
             of the results of the various checks
        """"""
    check_strategy.init_check('backup maximum age')
    if self.config.last_backup_maximum_age is not None:
        backup_age = self.backup_manager.validate_last_backup_maximum_age(self.config.last_backup_maximum_age)
        check_strategy.result(self.config.name, backup_age[0], hint='interval provided: %s, latest backup age: %s' % (human_readable_timedelta(self.config.last_backup_maximum_age), backup_age[1]))
    else:
        check_strategy.result(self.config.name, True, hint='no last_backup_maximum_age provided')
    check_strategy.init_check('backup minimum size')
    if self.config.last_backup_minimum_size is not None:
        backup_size = self.backup_manager.validate_last_backup_min_size(self.config.last_backup_minimum_size)
        gtlt = '>' if backup_size[0] else '<'
        check_strategy.result(self.config.name, backup_size[0], hint='last backup size %s %s %s minimum' % (pretty_size(backup_size[1]), gtlt, pretty_size(self.config.last_backup_minimum_size)), perfdata=backup_size[1])
    else:
        backup_size = self.backup_manager.validate_last_backup_min_size(0)
        check_strategy.result(self.config.name, True, hint=pretty_size(backup_size[1]), perfdata=backup_size[1])","'interval provided: %s, latest backup age: %s' % (human_readable_timedelta(self.config.last_backup_maximum_age), backup_age[1])","f'interval provided: {human_readable_timedelta(self.config.last_backup_maximum_age)}, latest backup age: {backup_age[1]}'","f'interval provided: {human_readable_timedelta(self.config.last_backup_maximum_age)}, latest backup age: {backup_age[1]}'",1,,,,
barman,https://github.com/EnterpriseDB/barman/tree/master/barman/server.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/barman/barman/server.py,Server,"def check_backup_validity(self, check_strategy):
    """"""
        Check if backup validity requirements are satisfied

        :param CheckStrategy check_strategy: the strategy for the management
             of the results of the various checks
        """"""
    check_strategy.init_check('backup maximum age')
    if self.config.last_backup_maximum_age is not None:
        backup_age = self.backup_manager.validate_last_backup_maximum_age(self.config.last_backup_maximum_age)
        check_strategy.result(self.config.name, backup_age[0], hint='interval provided: %s, latest backup age: %s' % (human_readable_timedelta(self.config.last_backup_maximum_age), backup_age[1]))
    else:
        check_strategy.result(self.config.name, True, hint='no last_backup_maximum_age provided')
    check_strategy.init_check('backup minimum size')
    if self.config.last_backup_minimum_size is not None:
        backup_size = self.backup_manager.validate_last_backup_min_size(self.config.last_backup_minimum_size)
        gtlt = '>' if backup_size[0] else '<'
        check_strategy.result(self.config.name, backup_size[0], hint='last backup size %s %s %s minimum' % (pretty_size(backup_size[1]), gtlt, pretty_size(self.config.last_backup_minimum_size)), perfdata=backup_size[1])
    else:
        backup_size = self.backup_manager.validate_last_backup_min_size(0)
        check_strategy.result(self.config.name, True, hint=pretty_size(backup_size[1]), perfdata=backup_size[1])","'last backup size %s %s %s minimum' % (pretty_size(backup_size[1]), gtlt, pretty_size(self.config.last_backup_minimum_size))",f'last backup size {pretty_size(backup_size[1])} {gtlt} {pretty_size(self.config.last_backup_minimum_size)} minimum',f'last backup size {pretty_size(backup_size[1])} {gtlt} {pretty_size(self.config.last_backup_minimum_size)} minimum',1,,,,
yamllint,https://github.com/adrienverge/yamllint/tree/master/yamllint/cli.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yamllint/yamllint/cli.py,Format,"def standard_color(problem, filename):
    line = '  \x1b[2m%d:%d\x1b[0m' % (problem.line, problem.column)
    line += max(20 - len(line), 0) * ' '
    if problem.level == 'warning':
        line += '\x1b[33m%s\x1b[0m' % problem.level
    else:
        line += '\x1b[31m%s\x1b[0m' % problem.level
    line += max(38 - len(line), 0) * ' '
    line += problem.desc
    if problem.rule:
        line += '  \x1b[2m(%s)\x1b[0m' % problem.rule
    return line","'  \x1b[2m%d:%d\x1b[0m' % (problem.line, problem.column)",f'  \x1b[2m{problem.line}:{problem.column}\x1b[0m',f'  \x1b[2m{problem.line}:{problem.column}\x1b[0m',1,,,,
yamllint,https://github.com/adrienverge/yamllint/tree/master/yamllint/cli.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yamllint/yamllint/cli.py,Format,"def standard_color(problem, filename):
    line = '  \x1b[2m%d:%d\x1b[0m' % (problem.line, problem.column)
    line += max(20 - len(line), 0) * ' '
    if problem.level == 'warning':
        line += '\x1b[33m%s\x1b[0m' % problem.level
    else:
        line += '\x1b[31m%s\x1b[0m' % problem.level
    line += max(38 - len(line), 0) * ' '
    line += problem.desc
    if problem.rule:
        line += '  \x1b[2m(%s)\x1b[0m' % problem.rule
    return line",'\x1b[33m%s\x1b[0m' % problem.level,f'\x1b[33m{problem.level}\x1b[0m',f'\x1b[33m{problem.level}\x1b[0m',1,,,,
yamllint,https://github.com/adrienverge/yamllint/tree/master/yamllint/cli.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yamllint/yamllint/cli.py,Format,"def standard_color(problem, filename):
    line = '  \x1b[2m%d:%d\x1b[0m' % (problem.line, problem.column)
    line += max(20 - len(line), 0) * ' '
    if problem.level == 'warning':
        line += '\x1b[33m%s\x1b[0m' % problem.level
    else:
        line += '\x1b[31m%s\x1b[0m' % problem.level
    line += max(38 - len(line), 0) * ' '
    line += problem.desc
    if problem.rule:
        line += '  \x1b[2m(%s)\x1b[0m' % problem.rule
    return line",'\x1b[31m%s\x1b[0m' % problem.level,f'\x1b[31m{problem.level}\x1b[0m',f'\x1b[31m{problem.level}\x1b[0m',1,,,,
yamllint,https://github.com/adrienverge/yamllint/tree/master/yamllint/cli.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yamllint/yamllint/cli.py,Format,"def standard_color(problem, filename):
    line = '  \x1b[2m%d:%d\x1b[0m' % (problem.line, problem.column)
    line += max(20 - len(line), 0) * ' '
    if problem.level == 'warning':
        line += '\x1b[33m%s\x1b[0m' % problem.level
    else:
        line += '\x1b[31m%s\x1b[0m' % problem.level
    line += max(38 - len(line), 0) * ' '
    line += problem.desc
    if problem.rule:
        line += '  \x1b[2m(%s)\x1b[0m' % problem.rule
    return line",'  \x1b[2m(%s)\x1b[0m' % problem.rule,f'  \x1b[2m({problem.rule})\x1b[0m',f'  \x1b[2m({problem.rule})\x1b[0m',1,,,,
pytorch3d,https://github.com/facebookresearch/pytorch3d/tree/master/projects/nerf/nerf/stats.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch3d/projects/nerf/nerf/stats.py,Stats,"def reset(self) -> None:
    """"""
        Called before an epoch to clear current epoch buffers.
        """"""
    stat_sets = list(self.stats.keys())
    if self.verbose:
        print('stats: epoch %d - reset' % self.epoch)
    self.it = {k: -1 for k in stat_sets}
    for stat_set in stat_sets:
        for stat in self.stats[stat_set]:
            self.stats[stat_set][stat].reset()
    self._epoch_start = time.time()",'stats: epoch %d - reset' % self.epoch,f'stats: epoch {self.epoch} - reset',f'stats: epoch {self.epoch} - reset',1,,,,
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/Players/PyChess.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/Players/PyChess.py,PyChess,"def __remainingMovesA(self):
    ply_count = self.board.plyCount
    remaining = -1.71086e-12 * ply_count ** 6 + 1.69103e-09 * ply_count ** 5 - 6.00801e-07 * ply_count ** 4 + 8.17741e-05 * ply_count ** 3 + 0.000291858 * ply_count ** 2 - 0.94497 * ply_count + 78.8979
    self.print('# remaining moves estimate=%s' % remaining)
    return remaining",'# remaining moves estimate=%s' % remaining,f'# remaining moves estimate={remaining}',f'# remaining moves estimate={remaining}',1,,,,
oppia,https://github.com/oppia/oppia/tree/master/core/domain/rte_component_registry_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/rte_component_registry_test.py,RteComponentUnitTests,"def test_image_thumbnails_for_rte_components(self) -> None:
    """"""Test the thumbnails for the RTE component icons.""""""
    rte_components = rte_component_registry.Registry.get_all_rte_components()
    for (component_name, component_specs) in rte_components.items():
        generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
        relative_icon_data_url = component_specs['icon_data_url'][1:]
        defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
        self.assertEqual(generated_image_filepath, defined_image_filepath)
        with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
            img_data = f.read()
            (width, height) = struct.unpack('>LL', img_data[16:24])
            self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
            self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)",'%s.png' % component_name,f'{component_name}.png',f'{component_name}.png',1,,,,
ansible-modules-core,https://github.com/ansible/ansible-modules-core/tree/master/network/netvisor/pn_vlag.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-core/network/netvisor/pn_vlag.py,,"def pn_cli(module):
    """"""
    This method is to generate the cli portion to launch the Netvisor cli.
    It parses the username, password, switch parameters from module.
    :param module: The Ansible module to fetch username, password and switch
    :return: returns the cli string for further processing
    """"""
    username = module.params['pn_cliusername']
    password = module.params['pn_clipassword']
    cliswitch = module.params['pn_cliswitch']
    if username and password:
        cli = '/usr/bin/cli --quiet --user %s:%s ' % (username, password)
    else:
        cli = '/usr/bin/cli --quiet '
    if cliswitch == 'local':
        cli += ' switch-local '
    else:
        cli += ' switch ' + cliswitch
    return cli","'/usr/bin/cli --quiet --user %s:%s ' % (username, password)",f'/usr/bin/cli --quiet --user {username}:{password} ',f'/usr/bin/cli --quiet --user {username}:{password} ',1,,,,
NeoVintageous,https://github.com/NeoVintageous/NeoVintageous/tree/master/nv/shell_windows.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NeoVintageous/nv/shell_windows.py,,"def filter_region(view, txt: str, cmd: str) -> str:
    try:
        contents = tempfile.NamedTemporaryFile(suffix='.txt', delete=False)
        contents.write(txt.encode('utf-8'))
        contents.close()
        script = tempfile.NamedTemporaryFile(suffix='.bat', delete=False)
        script.write(('@echo off\ntype %s | %s' % (contents.name, cmd)).encode('utf-8'))
        script.close()
        p = subprocess.Popen([script.name], stdout=subprocess.PIPE, stderr=subprocess.PIPE, startupinfo=_get_startup_info())
        (out, err) = p.communicate()
        if out:
            return _translate_newlines(out.decode(_get_encoding()))
        if err:
            return _translate_newlines(err.decode(_get_encoding()))
        return ''
    finally:
        os.remove(script.name)
        os.remove(contents.name)","'@echo off\ntype %s | %s' % (contents.name, cmd)",f'@echo off\ntype {contents.name} | {cmd}',f'@echo off\ntype {contents.name} | {cmd}',1,,,,
pyannote-audio,https://github.com/pyannote/pyannote-audio/tree/master/pyannote/audio/_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyannote-audio/pyannote/audio/_version.py,,"def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):
    if not os.path.exists(os.path.join(root, '.git')):
        if verbose:
            print('no .git in %s' % root)
        raise NotThisMethod('no .git directory')
    GITS = ['git']
    if sys.platform == 'win32':
        GITS = ['git.cmd', 'git.exe']
    describe_out = run_command(GITS, ['describe', '--tags', '--dirty', '--always', '--long'], cwd=root)
    if describe_out is None:
        raise NotThisMethod(""'git describe' failed"")
    describe_out = describe_out.strip()
    full_out = run_command(GITS, ['rev-parse', 'HEAD'], cwd=root)
    if full_out is None:
        raise NotThisMethod(""'git rev-parse' failed"")
    full_out = full_out.strip()
    pieces = {}
    pieces['long'] = full_out
    pieces['short'] = full_out[:7]
    pieces['error'] = None
    git_describe = describe_out
    dirty = git_describe.endswith('-dirty')
    pieces['dirty'] = dirty
    if dirty:
        git_describe = git_describe[:git_describe.rindex('-dirty')]
    if '-' in git_describe:
        mo = re.search('^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)
        if not mo:
            pieces['error'] = ""unable to parse git-describe output: '%s'"" % describe_out
            return pieces
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = ""tag '%s' doesn't start with prefix '%s'""
                print(fmt % (full_tag, tag_prefix))
            pieces['error'] = ""tag '%s' doesn't start with prefix '%s'"" % (full_tag, tag_prefix)
            return pieces
        pieces['closest-tag'] = full_tag[len(tag_prefix):]
        pieces['distance'] = int(mo.group(2))
        pieces['short'] = mo.group(3)
    else:
        pieces['closest-tag'] = None
        count_out = run_command(GITS, ['rev-list', 'HEAD', '--count'], cwd=root)
        pieces['distance'] = int(count_out)
    return pieces",'no .git in %s' % root,f'no .git in {root}',f'no .git in {root}',1,,,,
pyannote-audio,https://github.com/pyannote/pyannote-audio/tree/master/pyannote/audio/_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyannote-audio/pyannote/audio/_version.py,,"def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):
    if not os.path.exists(os.path.join(root, '.git')):
        if verbose:
            print('no .git in %s' % root)
        raise NotThisMethod('no .git directory')
    GITS = ['git']
    if sys.platform == 'win32':
        GITS = ['git.cmd', 'git.exe']
    describe_out = run_command(GITS, ['describe', '--tags', '--dirty', '--always', '--long'], cwd=root)
    if describe_out is None:
        raise NotThisMethod(""'git describe' failed"")
    describe_out = describe_out.strip()
    full_out = run_command(GITS, ['rev-parse', 'HEAD'], cwd=root)
    if full_out is None:
        raise NotThisMethod(""'git rev-parse' failed"")
    full_out = full_out.strip()
    pieces = {}
    pieces['long'] = full_out
    pieces['short'] = full_out[:7]
    pieces['error'] = None
    git_describe = describe_out
    dirty = git_describe.endswith('-dirty')
    pieces['dirty'] = dirty
    if dirty:
        git_describe = git_describe[:git_describe.rindex('-dirty')]
    if '-' in git_describe:
        mo = re.search('^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)
        if not mo:
            pieces['error'] = ""unable to parse git-describe output: '%s'"" % describe_out
            return pieces
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = ""tag '%s' doesn't start with prefix '%s'""
                print(fmt % (full_tag, tag_prefix))
            pieces['error'] = ""tag '%s' doesn't start with prefix '%s'"" % (full_tag, tag_prefix)
            return pieces
        pieces['closest-tag'] = full_tag[len(tag_prefix):]
        pieces['distance'] = int(mo.group(2))
        pieces['short'] = mo.group(3)
    else:
        pieces['closest-tag'] = None
        count_out = run_command(GITS, ['rev-list', 'HEAD', '--count'], cwd=root)
        pieces['distance'] = int(count_out)
    return pieces","""unable to parse git-describe output: '%s'"" % describe_out",f"unable to parse git-describe output: '{describe_out}'",f"unable to parse git-describe output: '{describe_out}'",1,,,,
FedML,https://github.com/FedML-AI/FedML/tree/master/fedml_api/model/cv/efficientnet_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FedML/fedml_api/model/cv/efficientnet_utils.py,BlockDecoder,"def _encode_block_string(block):
    """"""Encode a block to a string.
        Args:
            block (namedtuple): A BlockArgs type argument.
        Returns:
            block_string: A String form of BlockArgs.
        """"""
    args = ['r%d' % block.num_repeat, 'k%d' % block.kernel_size, 's%d%d' % (block.strides[0], block.strides[1]), 'e%s' % block.expand_ratio, 'i%d' % block.input_filters, 'o%d' % block.output_filters]
    if 0 < block.se_ratio <= 1:
        args.append('se%s' % block.se_ratio)
    if block.id_skip is False:
        args.append('noskip')
    return '_'.join(args)",'r%d' % block.num_repeat,f'r{block.num_repeat}',f'r{block.num_repeat}',1,,,,
FedML,https://github.com/FedML-AI/FedML/tree/master/fedml_api/model/cv/efficientnet_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FedML/fedml_api/model/cv/efficientnet_utils.py,BlockDecoder,"def _encode_block_string(block):
    """"""Encode a block to a string.
        Args:
            block (namedtuple): A BlockArgs type argument.
        Returns:
            block_string: A String form of BlockArgs.
        """"""
    args = ['r%d' % block.num_repeat, 'k%d' % block.kernel_size, 's%d%d' % (block.strides[0], block.strides[1]), 'e%s' % block.expand_ratio, 'i%d' % block.input_filters, 'o%d' % block.output_filters]
    if 0 < block.se_ratio <= 1:
        args.append('se%s' % block.se_ratio)
    if block.id_skip is False:
        args.append('noskip')
    return '_'.join(args)","'s%d%d' % (block.strides[0], block.strides[1])",f's{block.strides[0]}{block.strides[1]}',f's{block.strides[0]}{block.strides[1]}',1,,,,
FedML,https://github.com/FedML-AI/FedML/tree/master/fedml_api/model/cv/efficientnet_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FedML/fedml_api/model/cv/efficientnet_utils.py,BlockDecoder,"def _encode_block_string(block):
    """"""Encode a block to a string.
        Args:
            block (namedtuple): A BlockArgs type argument.
        Returns:
            block_string: A String form of BlockArgs.
        """"""
    args = ['r%d' % block.num_repeat, 'k%d' % block.kernel_size, 's%d%d' % (block.strides[0], block.strides[1]), 'e%s' % block.expand_ratio, 'i%d' % block.input_filters, 'o%d' % block.output_filters]
    if 0 < block.se_ratio <= 1:
        args.append('se%s' % block.se_ratio)
    if block.id_skip is False:
        args.append('noskip')
    return '_'.join(args)",'e%s' % block.expand_ratio,f'e{block.expand_ratio}',f'e{block.expand_ratio}',1,,,,
FedML,https://github.com/FedML-AI/FedML/tree/master/fedml_api/model/cv/efficientnet_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FedML/fedml_api/model/cv/efficientnet_utils.py,BlockDecoder,"def _encode_block_string(block):
    """"""Encode a block to a string.
        Args:
            block (namedtuple): A BlockArgs type argument.
        Returns:
            block_string: A String form of BlockArgs.
        """"""
    args = ['r%d' % block.num_repeat, 'k%d' % block.kernel_size, 's%d%d' % (block.strides[0], block.strides[1]), 'e%s' % block.expand_ratio, 'i%d' % block.input_filters, 'o%d' % block.output_filters]
    if 0 < block.se_ratio <= 1:
        args.append('se%s' % block.se_ratio)
    if block.id_skip is False:
        args.append('noskip')
    return '_'.join(args)",'i%d' % block.input_filters,f'i{block.input_filters}',f'i{block.input_filters}',1,,,,
FedML,https://github.com/FedML-AI/FedML/tree/master/fedml_api/model/cv/efficientnet_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FedML/fedml_api/model/cv/efficientnet_utils.py,BlockDecoder,"def _encode_block_string(block):
    """"""Encode a block to a string.
        Args:
            block (namedtuple): A BlockArgs type argument.
        Returns:
            block_string: A String form of BlockArgs.
        """"""
    args = ['r%d' % block.num_repeat, 'k%d' % block.kernel_size, 's%d%d' % (block.strides[0], block.strides[1]), 'e%s' % block.expand_ratio, 'i%d' % block.input_filters, 'o%d' % block.output_filters]
    if 0 < block.se_ratio <= 1:
        args.append('se%s' % block.se_ratio)
    if block.id_skip is False:
        args.append('noskip')
    return '_'.join(args)",'o%d' % block.output_filters,f'o{block.output_filters}',f'o{block.output_filters}',1,,,,
FedML,https://github.com/FedML-AI/FedML/tree/master/fedml_api/model/cv/efficientnet_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FedML/fedml_api/model/cv/efficientnet_utils.py,BlockDecoder,"def _encode_block_string(block):
    """"""Encode a block to a string.
        Args:
            block (namedtuple): A BlockArgs type argument.
        Returns:
            block_string: A String form of BlockArgs.
        """"""
    args = ['r%d' % block.num_repeat, 'k%d' % block.kernel_size, 's%d%d' % (block.strides[0], block.strides[1]), 'e%s' % block.expand_ratio, 'i%d' % block.input_filters, 'o%d' % block.output_filters]
    if 0 < block.se_ratio <= 1:
        args.append('se%s' % block.se_ratio)
    if block.id_skip is False:
        args.append('noskip')
    return '_'.join(args)",'se%s' % block.se_ratio,f'se{block.se_ratio}',f'se{block.se_ratio}',1,,,,
TensorFlow-and-DeepLearning-Tutorial,https://github.com/CreatCodeBuild/TensorFlow-and-DeepLearning-Tutorial/tree/master/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorFlow-and-DeepLearning-Tutorial/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,Network,"def run(self, data_iterator, train_samples, train_labels, test_samples, test_labels):
    """"""
        Session
        :data_iterator: a function that yields chuck of data
        """"""

    def print_confusion_matrix(confusionMatrix):
        print('Confusion    Matrix:')
        for (i, line) in enumerate(confusionMatrix):
            print(line, line[i] / np.sum(line))
        a = 0
        for (i, column) in enumerate(np.transpose(confusionMatrix, (1, 0))):
            a += column[i] / np.sum(column) * (np.sum(column) / 26000)
            print(column[i] / np.sum(column))
        print('\n', np.sum(confusionMatrix), a)
    self.writer = tf.summary.FileWriter('./board', tf.get_default_graph())
    with tf.Session(graph=tf.get_default_graph()) as session:
        tf.initialize_all_variables().run()
        print('Start Training')
        for (i, samples, labels) in data_iterator(train_samples, train_labels, self.train_batch_size):
            (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
            self.writer.add_summary(summary, i)
            (accuracy, _) = self.accuracy(predictions, labels)
            if i % 50 == 0:
                print('Minibatch loss at step %d: %f' % (i, l))
                print('Minibatch accuracy: %.1f%%' % accuracy)
        accuracies = []
        confusionMatrices = []
        for (i, samples, labels) in data_iterator(test_samples, test_labels, self.test_batch_size):
            print('samples shape', samples.shape)
            (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
            self.writer.add_summary(summary, i)
            (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
            accuracies.append(accuracy)
            confusionMatrices.append(cm)
            print('Test Accuracy: %.1f%%' % accuracy)
        print(' Average  Accuracy:', np.average(accuracies))
        print('Standard Deviation:', np.std(accuracies))
        print_confusion_matrix(np.add.reduce(confusionMatrices))","'Minibatch loss at step %d: %f' % (i, l)",f'Minibatch loss at step {i}: {l}',f'Minibatch loss at step {i}: {l}',1,,,,
TensorFlow-and-DeepLearning-Tutorial,https://github.com/CreatCodeBuild/TensorFlow-and-DeepLearning-Tutorial/tree/master/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorFlow-and-DeepLearning-Tutorial/Season1_Tensorflow1.1_Python3.5/12-15/dp_refined_api.py,Network,"def run(self, data_iterator, train_samples, train_labels, test_samples, test_labels):
    """"""
        Session
        :data_iterator: a function that yields chuck of data
        """"""

    def print_confusion_matrix(confusionMatrix):
        print('Confusion    Matrix:')
        for (i, line) in enumerate(confusionMatrix):
            print(line, line[i] / np.sum(line))
        a = 0
        for (i, column) in enumerate(np.transpose(confusionMatrix, (1, 0))):
            a += column[i] / np.sum(column) * (np.sum(column) / 26000)
            print(column[i] / np.sum(column))
        print('\n', np.sum(confusionMatrix), a)
    self.writer = tf.summary.FileWriter('./board', tf.get_default_graph())
    with tf.Session(graph=tf.get_default_graph()) as session:
        tf.initialize_all_variables().run()
        print('Start Training')
        for (i, samples, labels) in data_iterator(train_samples, train_labels, self.train_batch_size):
            (_, l, predictions, summary) = session.run([self.optimizer, self.loss, self.train_prediction, self.merged_train_summary], feed_dict={self.tf_train_samples: samples, self.tf_train_labels: labels})
            self.writer.add_summary(summary, i)
            (accuracy, _) = self.accuracy(predictions, labels)
            if i % 50 == 0:
                print('Minibatch loss at step %d: %f' % (i, l))
                print('Minibatch accuracy: %.1f%%' % accuracy)
        accuracies = []
        confusionMatrices = []
        for (i, samples, labels) in data_iterator(test_samples, test_labels, self.test_batch_size):
            print('samples shape', samples.shape)
            (result, summary) = session.run([self.test_prediction, self.merged_test_summary], feed_dict={self.tf_test_samples: samples})
            self.writer.add_summary(summary, i)
            (accuracy, cm) = self.accuracy(result, labels, need_confusion_matrix=True)
            accuracies.append(accuracy)
            confusionMatrices.append(cm)
            print('Test Accuracy: %.1f%%' % accuracy)
        print(' Average  Accuracy:', np.average(accuracies))
        print('Standard Deviation:', np.std(accuracies))
        print_confusion_matrix(np.add.reduce(confusionMatrices))",'Minibatch accuracy: %.1f%%' % accuracy,f'Minibatch accuracy: {accuracy:.1f}%',f'Minibatch accuracy: {accuracy:.1f}%',1,,,,
brave,https://github.com/bbc/brave/tree/master/brave/connections/connection_to_mixer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brave/brave/connections/connection_to_mixer.py,ConnectionToMixer,"def _remove_from_mix(self, audio_or_video):
    if not self._mix_request_pad[audio_or_video].is_linked():
        self.logger.info('Attempted to remove from %s mix but not currently mixed' % audio_or_video)
        return
    if audio_or_video in self._mix_request_pad:
        self._mix_request_pad[audio_or_video].get_peer().unlink(self._mix_request_pad[audio_or_video])
        self.dest.mixer_element[audio_or_video].release_request_pad(self._mix_request_pad[audio_or_video])
        del self._mix_request_pad[audio_or_video]",'Attempted to remove from %s mix but not currently mixed' % audio_or_video,f'Attempted to remove from {audio_or_video} mix but not currently mixed',f'Attempted to remove from {audio_or_video} mix but not currently mixed',1,,,,
anaconda,https://github.com/DamnWidget/anaconda/tree/master/anaconda_lib/parso/pgen2/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anaconda/anaconda_lib/parso/pgen2/generator.py,,"def _calculate_tree_traversal(nonterminal_to_dfas):
    """"""
    By this point we know how dfas can move around within a stack node, but we
    don't know how we can add a new stack node (nonterminal transitions).
    """"""
    first_plans = {}
    nonterminals = list(nonterminal_to_dfas.keys())
    nonterminals.sort()
    for nonterminal in nonterminals:
        if nonterminal not in first_plans:
            _calculate_first_plans(nonterminal_to_dfas, first_plans, nonterminal)
    for dfas in nonterminal_to_dfas.values():
        for dfa_state in dfas:
            transitions = dfa_state.transitions
            for (nonterminal, next_dfa) in dfa_state.nonterminal_arcs.items():
                for (transition, pushes) in first_plans[nonterminal].items():
                    if transition in transitions:
                        prev_plan = transitions[transition]
                        choices = sorted([prev_plan.dfa_pushes[0].from_rule if prev_plan.dfa_pushes else prev_plan.next_dfa.from_rule, pushes[0].from_rule if pushes else next_dfa.from_rule])
                        raise ValueError(""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices)))
                    transitions[transition] = DFAPlan(next_dfa, pushes)","""Rule %s is ambiguous; given a %s token, we can't determine if we should evaluate %s or %s."" % ((dfa_state.from_rule, transition) + tuple(choices))","f""Rule {dfa_state.from_rule} is ambiguous; given a {transition} token, we can't determine if we should evaluate {choices[0]} or {choices[1]}.""","f""Rule {dfa_state.from_rule} is ambiguous; given a {transition} token, we can't determine if we should evaluate {choices[0]} or {choices[1]}.""",1,,,,
dirbot,https://github.com/scrapy/dirbot/tree/master/dirbot/pipelines.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dirbot/dirbot/pipelines.py,FilterWordsPipeline,"def process_item(self, item, spider):
    for word in self.words_to_filter:
        if word in item['description'].lower():
            raise DropItem('Contains forbidden word: %s' % word)
    else:
        return item",'Contains forbidden word: %s' % word,f'Contains forbidden word: {word}',f'Contains forbidden word: {word}',1,,,,
mypy,https://github.com/python/mypy/tree/master/mypy/suggestions.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mypy/mypy/suggestions.py,SuggestionEngine,"def find_node(self, key: str) -> Tuple[str, str, FuncDef]:
    """"""From a target name, return module/target names and the func def.

        The 'key' argument can be in one of two formats:
        * As the function full name, e.g., package.module.Cls.method
        * As the function location as file and line separated by column,
          e.g., path/to/file.py:42
        """"""
    node: Optional[SymbolNode] = None
    if ':' in key:
        if key.count(':') > 1:
            raise SuggestionFailure('Malformed location for function: {}. Must be either package.module.Class.method or path/to/file.py:line'.format(key))
        (file, line) = key.split(':')
        if not line.isdigit():
            raise SuggestionFailure('Line number must be a number. Got {}'.format(line))
        line_number = int(line)
        (modname, node) = self.find_node_by_file_and_line(file, line_number)
        tail = node.fullname[len(modname) + 1:]
    else:
        target = split_target(self.fgmanager.graph, key)
        if not target:
            raise SuggestionFailure('Cannot find module for %s' % (key,))
        (modname, tail) = target
        node = self.find_node_by_module_and_name(modname, tail)
    if isinstance(node, Decorator):
        node = self.extract_from_decorator(node)
        if not node:
            raise SuggestionFailure(""Object %s is a decorator we can't handle"" % key)
    if not isinstance(node, FuncDef):
        raise SuggestionFailure('Object %s is not a function' % key)
    return (modname, tail, node)","'Cannot find module for %s' % (key,)",f'Cannot find module for {key}',f'Cannot find module for {key}',1,,,,
mypy,https://github.com/python/mypy/tree/master/mypy/suggestions.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mypy/mypy/suggestions.py,SuggestionEngine,"def find_node(self, key: str) -> Tuple[str, str, FuncDef]:
    """"""From a target name, return module/target names and the func def.

        The 'key' argument can be in one of two formats:
        * As the function full name, e.g., package.module.Cls.method
        * As the function location as file and line separated by column,
          e.g., path/to/file.py:42
        """"""
    node: Optional[SymbolNode] = None
    if ':' in key:
        if key.count(':') > 1:
            raise SuggestionFailure('Malformed location for function: {}. Must be either package.module.Class.method or path/to/file.py:line'.format(key))
        (file, line) = key.split(':')
        if not line.isdigit():
            raise SuggestionFailure('Line number must be a number. Got {}'.format(line))
        line_number = int(line)
        (modname, node) = self.find_node_by_file_and_line(file, line_number)
        tail = node.fullname[len(modname) + 1:]
    else:
        target = split_target(self.fgmanager.graph, key)
        if not target:
            raise SuggestionFailure('Cannot find module for %s' % (key,))
        (modname, tail) = target
        node = self.find_node_by_module_and_name(modname, tail)
    if isinstance(node, Decorator):
        node = self.extract_from_decorator(node)
        if not node:
            raise SuggestionFailure(""Object %s is a decorator we can't handle"" % key)
    if not isinstance(node, FuncDef):
        raise SuggestionFailure('Object %s is not a function' % key)
    return (modname, tail, node)","""Object %s is a decorator we can't handle"" % key",f"Object {key} is a decorator we can't handle",f"Object {key} is a decorator we can't handle",1,,,,
mypy,https://github.com/python/mypy/tree/master/mypy/suggestions.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mypy/mypy/suggestions.py,SuggestionEngine,"def find_node(self, key: str) -> Tuple[str, str, FuncDef]:
    """"""From a target name, return module/target names and the func def.

        The 'key' argument can be in one of two formats:
        * As the function full name, e.g., package.module.Cls.method
        * As the function location as file and line separated by column,
          e.g., path/to/file.py:42
        """"""
    node: Optional[SymbolNode] = None
    if ':' in key:
        if key.count(':') > 1:
            raise SuggestionFailure('Malformed location for function: {}. Must be either package.module.Class.method or path/to/file.py:line'.format(key))
        (file, line) = key.split(':')
        if not line.isdigit():
            raise SuggestionFailure('Line number must be a number. Got {}'.format(line))
        line_number = int(line)
        (modname, node) = self.find_node_by_file_and_line(file, line_number)
        tail = node.fullname[len(modname) + 1:]
    else:
        target = split_target(self.fgmanager.graph, key)
        if not target:
            raise SuggestionFailure('Cannot find module for %s' % (key,))
        (modname, tail) = target
        node = self.find_node_by_module_and_name(modname, tail)
    if isinstance(node, Decorator):
        node = self.extract_from_decorator(node)
        if not node:
            raise SuggestionFailure(""Object %s is a decorator we can't handle"" % key)
    if not isinstance(node, FuncDef):
        raise SuggestionFailure('Object %s is not a function' % key)
    return (modname, tail, node)",'Object %s is not a function' % key,f'Object {key} is not a function',f'Object {key} is not a function',1,,,,
python-jose,https://github.com/mpdavis/python-jose/tree/master/jose/backends/cryptography_backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-jose/jose/backends/cryptography_backend.py,CryptographyECKey,"def sign(self, msg):
    if self.hash_alg.digest_size * 8 > self.prepared_key.curve.key_size:
        raise TypeError('this curve (%s) is too short for your digest (%d)' % (self.prepared_key.curve.name, 8 * self.hash_alg.digest_size))
    signature = self.prepared_key.sign(msg, ec.ECDSA(self.hash_alg()))
    return self._der_to_raw(signature)","'this curve (%s) is too short for your digest (%d)' % (self.prepared_key.curve.name, 8 * self.hash_alg.digest_size)",f'this curve ({self.prepared_key.curve.name}) is too short for your digest ({8 * self.hash_alg.digest_size})',f'this curve ({self.prepared_key.curve.name}) is too short for your digest ({8 * self.hash_alg.digest_size})',1,,,,
conan,https://github.com/conan-io/conan/tree/master/conans/client/build/meson.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan/conans/client/build/meson.py,Meson,"def _run_meson_targets(self, args=None, build_dir=None, targets=None):
    args = args or []
    build_dir = build_dir or self.build_dir or self._conanfile.build_folder
    arg_list = join_arguments(['-C ""%s""' % build_dir, args_to_string(args), args_to_string(targets)])
    command = 'ninja' if self.backend == 'ninja' else 'meson compile'
    self._run('%s %s' % (command, arg_list))",'-C "%s"' % build_dir,f'-C "{build_dir}"',f'-C "{build_dir}"',1,,,,
conan,https://github.com/conan-io/conan/tree/master/conans/client/build/meson.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan/conans/client/build/meson.py,Meson,"def _run_meson_targets(self, args=None, build_dir=None, targets=None):
    args = args or []
    build_dir = build_dir or self.build_dir or self._conanfile.build_folder
    arg_list = join_arguments(['-C ""%s""' % build_dir, args_to_string(args), args_to_string(targets)])
    command = 'ninja' if self.backend == 'ninja' else 'meson compile'
    self._run('%s %s' % (command, arg_list))","'%s %s' % (command, arg_list)",f'{command} {arg_list}',f'{command} {arg_list}',1,,,,
bt-speaker,https://github.com/lukasjapan/bt-speaker/tree/master/bt_manager/audio.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bt-speaker/bt_manager/audio.py,SBCAudioSink,"def _notify_media_transport_available(self, path, transport):
    """"""
        Called by the endpoint when a new media transport is
        available
        """"""
    print('Transport available! transport=%s' % transport)
    self.source = BTMediaTransport(transport)
    self.state = 'idle'
    self.source.add_signal_receiver(self._property_change_event_handler, BTAudioSource.SIGNAL_PROPERTY_CHANGED, transport)",'Transport available! transport=%s' % transport,f'Transport available! transport={transport}',f'Transport available! transport={transport}',1,,,,
lore,https://github.com/instacart/lore/tree/master/lore/encoders.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lore/lore/encoders.py,NestedUnique,"def unnest(self, data, fit=False):
    """"""
        :param data: a dataframe containing a column to be unnested
        :param fit: if True, self.sequence_length will exactly accomodate the largest sequence length
        :return: 1D array of values with length = rows * sequence_length
        """"""
    with timer('unnest %s' % self.name, logging.DEBUG):
        raw = self.series(data)
        lengths = [0 if x is None or (isinstance(x, float) and numpy.isnan(x)) else len(x) for x in raw.values]
        if fit and self.sequence_length is None:
            self.sequence_length = numpy.max(lengths)

        def fill_x(x, length):
            x_new = numpy.empty(length, dtype='O')
            if x is None or (isinstance(x, float) and numpy.isnan(x)):
                return x_new
            fill_length = min(len(x), length)
            x_new[0:fill_length] = x[0:fill_length]
            return x_new
        same_size = [fill_x(x, self.sequence_length) for x in raw.values]
        flattened = [item for sublist in same_size for item in sublist]
        return pandas.DataFrame({self.column: flattened})",'unnest %s' % self.name,f'unnest {self.name}',f'unnest {self.name}',1,,,,
django-mysql,https://github.com/adamchainz/django-mysql/tree/master/src/django_mysql/models/fields/lists.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-mysql/src/django_mysql/models/fields/lists.py,ListFieldMixin,"def description(self) -> Any:
    return _('List of %(base_description)s') % {'base_description': self.base_field.description}",_('List of %(base_description)s') % {'base_description': self.base_field.description},f'List of {self.base_field.description}',f'List of {self.base_field.description}',1,-1,-1,1,
gentle,https://github.com/lowerquality/gentle/tree/master/gentle/rpc.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gentle/gentle/rpc.py,RPCError,"def __str__(self):
    return 'standard_kaldi: error %d: %s' % (self.status, self.why)","'standard_kaldi: error %d: %s' % (self.status, self.why)",f'standard_kaldi: error {self.status}: {self.why}',f'standard_kaldi: error {self.status}: {self.why}',1,,,,
synapse,https://github.com/matrix-org/synapse/tree/master/tests/rest/admin/test_device.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/synapse/tests/rest/admin/test_device.py,DeviceRestTestCase,"def test_user_is_not_local(self, method: str) -> None:
    """"""
        Tests that a lookup for a user that is not a local returns a 400
        """"""
    url = '/_synapse/admin/v2/users/@unknown_person:unknown_domain/devices/%s' % self.other_user_device_id
    channel = self.make_request(method, url, access_token=self.admin_user_tok)
    self.assertEqual(400, channel.code, msg=channel.json_body)
    self.assertEqual('Can only lookup local users', channel.json_body['error'])",'/_synapse/admin/v2/users/@unknown_person:unknown_domain/devices/%s' % self.other_user_device_id,f'/_synapse/admin/v2/users/@unknown_person:unknown_domain/devices/{self.other_user_device_id}',f'/_synapse/admin/v2/users/@unknown_person:unknown_domain/devices/{self.other_user_device_id}',1,,,,
geneva,https://github.com/Kkevsterrr/geneva/tree/master/actions/trace.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/geneva/actions/trace.py,TraceAction,"def __str__(self):
    """"""
        Returns a string representation.
        """"""
    s = Action.__str__(self)
    s += '{%d:%d}' % (self.start_ttl, self.end_ttl)
    return s","'{%d:%d}' % (self.start_ttl, self.end_ttl)",f'{{{self.start_ttl}:{self.end_ttl}}}',f'{{{self.start_ttl}:{self.end_ttl}}}',1,,,,
s3cmd,https://github.com/s3tools/s3cmd/tree/master/S3/S3Uri.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/s3cmd/S3/S3Uri.py,S3UriS3,"def compose_uri(bucket, object=''):
    return u's3://%s/%s' % (bucket, object)","u's3://%s/%s' % (bucket, object)",f's3://{bucket}/{object}',f's3://{bucket}/{object}',1,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/tvnow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/tvnow.py,TVNowShowIE,"def _real_extract(self, url):
    (base_url, show_id) = re.match(self._VALID_URL, url).groups()
    result = self._call_api('teaserrow/format/navigation/' + show_id, show_id)
    items = result['items']
    entries = []
    navigation = result.get('navigationType')
    if navigation == 'annual':
        for item in items:
            if not isinstance(item, dict):
                continue
            year = int_or_none(item.get('year'))
            if year is None:
                continue
            months = item.get('months')
            if not isinstance(months, list):
                continue
            for month_dict in months:
                if not isinstance(month_dict, dict) or not month_dict:
                    continue
                month_number = int_or_none(list(month_dict.keys())[0])
                if month_number is None:
                    continue
                entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))
    elif navigation == 'season':
        for item in items:
            if not isinstance(item, dict):
                continue
            season_number = int_or_none(item.get('season'))
            if season_number is None:
                continue
            entries.append(self.url_result('%s/staffel-%d' % (base_url, season_number), ie=TVNowSeasonIE.ie_key()))
    else:
        raise ExtractorError('Unknown navigationType')
    return self.playlist_result(entries, show_id)","'%s/%04d-%02d' % (base_url, year, month_number)",f'{base_url}/{year:04d}-{month_number:02d}',f'{base_url}/{year:04d}-{month_number:02d}',1,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/tvnow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/tvnow.py,TVNowShowIE,"def _real_extract(self, url):
    (base_url, show_id) = re.match(self._VALID_URL, url).groups()
    result = self._call_api('teaserrow/format/navigation/' + show_id, show_id)
    items = result['items']
    entries = []
    navigation = result.get('navigationType')
    if navigation == 'annual':
        for item in items:
            if not isinstance(item, dict):
                continue
            year = int_or_none(item.get('year'))
            if year is None:
                continue
            months = item.get('months')
            if not isinstance(months, list):
                continue
            for month_dict in months:
                if not isinstance(month_dict, dict) or not month_dict:
                    continue
                month_number = int_or_none(list(month_dict.keys())[0])
                if month_number is None:
                    continue
                entries.append(self.url_result('%s/%04d-%02d' % (base_url, year, month_number), ie=TVNowAnnualIE.ie_key()))
    elif navigation == 'season':
        for item in items:
            if not isinstance(item, dict):
                continue
            season_number = int_or_none(item.get('season'))
            if season_number is None:
                continue
            entries.append(self.url_result('%s/staffel-%d' % (base_url, season_number), ie=TVNowSeasonIE.ie_key()))
    else:
        raise ExtractorError('Unknown navigationType')
    return self.playlist_result(entries, show_id)","'%s/staffel-%d' % (base_url, season_number)",f'{base_url}/staffel-{season_number}',f'{base_url}/staffel-{season_number}',1,,,,
RedditDownloader,https://github.com/shadowmoose/RedditDownloader/tree/master/redditdownloader/tools/win_file_fixer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RedditDownloader/redditdownloader/tools/win_file_fixer.py,,"if __name__ == '__main__':
    if os.name != 'nt':
        print('This is only for Windows, as it is the only platform to experience the bug this fixes.')
        import sys
        sys.exit(1)
    print('\nThis is a mini program to (attempt to) repair buggy Windows directories.')
    print('It scans all the subdirectories in the directory you specify,\nincluding the selected directory, and attempts to rename them to valid Windows names.')
    print(""This should ONLY be run if you're stuck with some directories that can't be removed or renamed."")
    print('\nAdditionally, this only works on Windows platforms, and has only been tested on Win10.')
    print(""\tIf you're having issues with directories on other Operating Systems, it's not a bug this can fix."")
    print()
    if 'y' in input('Are you sure you want to run this? (y/n): ').lower():
        targ = input('Enter the EXACT PATH to the base directory you want scanned: ')
        targ = os.path.abspath(targ)
        if 'y' in input('Is the path ""%s"" correct? (y/n): ' % targ).lower():
            repair_subdirs(targ)
        else:
            print('Aborted run.')
        print('Finished.')
    else:
        print('Not running.')",'Is the path "%s" correct? (y/n): ' % targ,f'Is the path "{targ}" correct? (y/n): ',f'Is the path "{targ}" correct? (y/n): ',1,,,,
Remarkable,https://github.com/jamiemcg/Remarkable/tree/master/remarkable_lib/Builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Remarkable/remarkable_lib/Builder.py,,"def auto_connect_by_name(callback_obj, builder):
    """"""finds handlers like on_<widget_name>_<signal> and connects them

    i.e. find widget,signal pair in builder and call
    widget.connect(signal, on_<widget_name>_<signal>)""""""
    callback_handler_dict = dict_from_callback_obj(callback_obj)
    for item in builder.widgets.items():
        (widget_name, widget) = item
        signal_ids = []
        try:
            widget_type = type(widget)
            while widget_type:
                signal_ids.extend(GObject.signal_list_ids(widget_type))
                widget_type = GObject.type_parent(widget_type)
        except RuntimeError:
            pass
        signal_names = [GObject.signal_name(sid) for sid in signal_ids]
        for sig in signal_names:
            sig = sig.replace('-', '_')
            handler_names = ['on_%s_%s' % (widget_name, sig)]
            if widget is callback_obj:
                handler_names.append('on_%s' % sig)
            do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)
    log_unconnected_functions(callback_handler_dict, builder.connections)","'on_%s_%s' % (widget_name, sig)",f'on_{widget_name}_{sig}',f'on_{widget_name}_{sig}',1,,,,
Remarkable,https://github.com/jamiemcg/Remarkable/tree/master/remarkable_lib/Builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Remarkable/remarkable_lib/Builder.py,,"def auto_connect_by_name(callback_obj, builder):
    """"""finds handlers like on_<widget_name>_<signal> and connects them

    i.e. find widget,signal pair in builder and call
    widget.connect(signal, on_<widget_name>_<signal>)""""""
    callback_handler_dict = dict_from_callback_obj(callback_obj)
    for item in builder.widgets.items():
        (widget_name, widget) = item
        signal_ids = []
        try:
            widget_type = type(widget)
            while widget_type:
                signal_ids.extend(GObject.signal_list_ids(widget_type))
                widget_type = GObject.type_parent(widget_type)
        except RuntimeError:
            pass
        signal_names = [GObject.signal_name(sid) for sid in signal_ids]
        for sig in signal_names:
            sig = sig.replace('-', '_')
            handler_names = ['on_%s_%s' % (widget_name, sig)]
            if widget is callback_obj:
                handler_names.append('on_%s' % sig)
            do_connect(item, sig, handler_names, callback_handler_dict, builder.connections)
    log_unconnected_functions(callback_handler_dict, builder.connections)",'on_%s' % sig,f'on_{sig}',f'on_{sig}',1,,,,
hyper,https://github.com/python-hyper/hyper/tree/master/test/test_integration.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hyper/test/test_integration.py,TestRequestsAdapter,"def test_adapter_close_context_manager(self):
    self.set_up(secure=False)

    def socket_handler(listener):
        sock = listener.accept()[0]
        data = b''
        while not data.endswith(b'\r\n\r\n'):
            data += sock.recv(65535)
        resp = b'HTTP/1.1 201 No Content\r\nServer: socket-level-server\r\nContent-Length: 0\r\nConnection: close\r\n\r\n'
        sock.send(resp)
        sock.close()
    self._start_server(socket_handler)
    with requests.Session() as s:
        a = HTTP20Adapter()
        s.mount('http://', a)
        r = s.get('http://%s:%s' % (self.host, self.port))
        connections_before_close = list(a.connections.values())
        assert connections_before_close
    assert not a.connections
    assert all((conn._sock is None for conn in connections_before_close))
    assert r.status_code == 201
    assert len(r.headers) == 3
    assert r.headers['server'] == 'socket-level-server'
    assert r.headers['content-length'] == '0'
    assert r.headers['connection'] == 'close'
    assert r.content == b''
    self.tear_down()","'http://%s:%s' % (self.host, self.port)",f'http://{self.host}:{self.port}',f'http://{self.host}:{self.port}',1,,,,
recordlinkage,https://github.com/J535D165/recordlinkage/tree/master//versioneer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/recordlinkage//versioneer.py,cmd_version,"def run(self):
    vers = get_versions(verbose=True)
    print('Version: %s' % vers['version'])
    print(' full-revisionid: %s' % vers.get('full-revisionid'))
    print(' dirty: %s' % vers.get('dirty'))
    if vers['error']:
        print(' error: %s' % vers['error'])",'Version: %s' % vers['version'],f"Version: {vers['version']}",f"Version: {vers['version']}",1,,,,
recordlinkage,https://github.com/J535D165/recordlinkage/tree/master//versioneer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/recordlinkage//versioneer.py,cmd_version,"def run(self):
    vers = get_versions(verbose=True)
    print('Version: %s' % vers['version'])
    print(' full-revisionid: %s' % vers.get('full-revisionid'))
    print(' dirty: %s' % vers.get('dirty'))
    if vers['error']:
        print(' error: %s' % vers['error'])",' dirty: %s' % vers.get('dirty'),f" dirty: {vers.get('dirty')}",f" dirty: {vers.get('dirty')}",1,,,,
MeshCNN,https://github.com/ranahanocka/MeshCNN/tree/master/util/util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MeshCNN/util/util.py,,"def print_network(net):
    """"""Print the total number of parameters in the network
    Parameters:
        network
    """"""
    print('---------- Network initialized -------------')
    num_params = 0
    for param in net.parameters():
        num_params += param.numel()
    print('[Network] Total number of parameters : %.3f M' % (num_params / 1000000.0))
    print('-----------------------------------------------')",'[Network] Total number of parameters : %.3f M' % (num_params / 1000000.0),f'[Network] Total number of parameters : {num_params / 1000000.0:.3f} M',f'[Network] Total number of parameters : {num_params / 1000000.0:.3f} M',1,,,,
PyDrive,https://github.com/googlearchive/PyDrive/tree/master/pydrive/auth.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyDrive/pydrive/auth.py,GoogleAuth,"def Refresh(self):
    """"""Refreshes the access_token.

    :raises: RefreshError
    """"""
    if self.credentials is None:
        raise RefreshError('No credential to refresh.')
    if self.credentials.refresh_token is None:
        raise RefreshError('No refresh_token found.Please set access_type of OAuth to offline.')
    if self.http is None:
        self.http = httplib2.Http(timeout=self.http_timeout)
    try:
        self.credentials.refresh(self.http)
    except AccessTokenRefreshError as error:
        raise RefreshError('Access token refresh failed: %s' % error)",'Access token refresh failed: %s' % error,f'Access token refresh failed: {error}',f'Access token refresh failed: {error}',1,,,,
sympy,https://github.com/sympy/sympy/tree/master/sympy/printing/c.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/printing/c.py,C89CodePrinter,"def _print_Min(self, expr):
    if 'Min' in self.known_functions:
        return self._print_Function(expr)

    def inner_print_min(args):
        if len(args) == 1:
            return self._print(args[0])
        half = len(args) // 2
        return '((%(a)s < %(b)s) ? %(a)s : %(b)s)' % {'a': inner_print_min(args[:half]), 'b': inner_print_min(args[half:])}
    return inner_print_min(expr.args)","'((%(a)s < %(b)s) ? %(a)s : %(b)s)' % {'a': inner_print_min(args[:half]), 'b': inner_print_min(args[half:])}",f'(({inner_print_min(args[:half])} < {inner_print_min(args[half:])}) ? {inner_print_min(args[:half])} : {inner_print_min(args[half:])})',f'(({inner_print_min(args[:half])} < {inner_print_min(args[half:])}) ? {inner_print_min(args[:half])} : {inner_print_min(args[half:])})',1,,,,
dcc,https://github.com/amimo/dcc/tree/master/androguard/decompiler/dad/instruction.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dcc/androguard/decompiler/dad/instruction.py,CastExpression,"def __str__(self):
    return 'CAST_%s(%s)' % (self.op, self.var_map[self.arg])","'CAST_%s(%s)' % (self.op, self.var_map[self.arg])",f'CAST_{self.op}({self.var_map[self.arg]})',f'CAST_{self.op}({self.var_map[self.arg]})',1,,,,
deep_sort_yolov3,https://github.com/Qidian213/deep_sort_yolov3/tree/master/tools/generate_detections.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_sort_yolov3/tools/generate_detections.py,ImageEncoder,"def __init__(self, checkpoint_filename, input_name='images', output_name='features'):
    self.session = tf.Session()
    with tf.gfile.GFile(checkpoint_filename, 'rb') as file_handle:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(file_handle.read())
    tf.import_graph_def(graph_def, name='net')
    self.input_var = tf.get_default_graph().get_tensor_by_name('net/%s:0' % input_name)
    self.output_var = tf.get_default_graph().get_tensor_by_name('net/%s:0' % output_name)
    assert len(self.output_var.get_shape()) == 2
    assert len(self.input_var.get_shape()) == 4
    self.feature_dim = self.output_var.get_shape().as_list()[-1]
    self.image_shape = self.input_var.get_shape().as_list()[1:]",'net/%s:0' % input_name,f'net/{input_name}:0',f'net/{input_name}:0',1,,,,
deep_sort_yolov3,https://github.com/Qidian213/deep_sort_yolov3/tree/master/tools/generate_detections.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_sort_yolov3/tools/generate_detections.py,ImageEncoder,"def __init__(self, checkpoint_filename, input_name='images', output_name='features'):
    self.session = tf.Session()
    with tf.gfile.GFile(checkpoint_filename, 'rb') as file_handle:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(file_handle.read())
    tf.import_graph_def(graph_def, name='net')
    self.input_var = tf.get_default_graph().get_tensor_by_name('net/%s:0' % input_name)
    self.output_var = tf.get_default_graph().get_tensor_by_name('net/%s:0' % output_name)
    assert len(self.output_var.get_shape()) == 2
    assert len(self.input_var.get_shape()) == 4
    self.feature_dim = self.output_var.get_shape().as_list()[-1]
    self.image_shape = self.input_var.get_shape().as_list()[1:]",'net/%s:0' % output_name,f'net/{output_name}:0',f'net/{output_name}:0',1,,,,
neutron,https://github.com/openstack/neutron/tree/master/neutron/tests/unit/common/ovn/test_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/tests/unit/common/ovn/test_utils.py,TestOvsdbClientCommand,"def test_run_northbound_with_ssl(self):
    private_key = 'north_pk'
    certificate = 'north_cert'
    ca_auth = 'north_ca_auth'
    ovn_conf.cfg.CONF.set_default('ovn_nb_private_key', private_key, group='ovn')
    ovn_conf.cfg.CONF.set_default('ovn_nb_certificate', certificate, group='ovn')
    ovn_conf.cfg.CONF.set_default('ovn_nb_ca_cert', ca_auth, group='ovn')
    expected = 'ovsdb-client %s %s --timeout 180 -p %s -c %s -C %s \'[""OVN_Northbound"", ""foo""]\'' % (self.OvsdbClientTestCommand.COMMAND, self.nb_connection, private_key, certificate, ca_auth)
    self.OvsdbClientTestCommand.run(['OVN_Northbound', 'foo'])
    self.assert_exec_call(expected)","'ovsdb-client %s %s --timeout 180 -p %s -c %s -C %s \'[""OVN_Northbound"", ""foo""]\'' % (self.OvsdbClientTestCommand.COMMAND, self.nb_connection, private_key, certificate, ca_auth)","f""""""ovsdb-client {self.OvsdbClientTestCommand.COMMAND} {self.nb_connection} --timeout 180 -p {private_key} -c {certificate} -C {ca_auth} '[""OVN_Northbound"", ""foo""]'""""""","f""""""ovsdb-client {self.OvsdbClientTestCommand.COMMAND} {self.nb_connection} --timeout 180 -p {private_key} -c {certificate} -C {ca_auth} '[""OVN_Northbound"", ""foo""]'""""""",1,,,,
Transformer-TTS,https://github.com/soobinseo/Transformer-TTS/tree/master//train_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer-TTS//train_transformer.py,,"def main():
    dataset = get_dataset()
    global_step = 0
    m = nn.DataParallel(Model().cuda())
    m.train()
    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)
    pos_weight = t.FloatTensor([5.0]).cuda()
    writer = SummaryWriter()
    for epoch in range(hp.epochs):
        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)
        pbar = tqdm(dataloader)
        for (i, data) in enumerate(pbar):
            pbar.set_description('Processing at epoch %d' % epoch)
            global_step += 1
            if global_step < 400000:
                adjust_learning_rate(optimizer, global_step)
            (character, mel, mel_input, pos_text, pos_mel, _) = data
            stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)
            character = character.cuda()
            mel = mel.cuda()
            mel_input = mel_input.cuda()
            pos_text = pos_text.cuda()
            pos_mel = pos_mel.cuda()
            (mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec) = m.forward(character, mel_input, pos_text, pos_mel)
            mel_loss = nn.L1Loss()(mel_pred, mel)
            post_mel_loss = nn.L1Loss()(postnet_pred, mel)
            loss = mel_loss + post_mel_loss
            writer.add_scalars('training_loss', {'mel_loss': mel_loss, 'post_mel_loss': post_mel_loss}, global_step)
            writer.add_scalars('alphas', {'encoder_alpha': m.module.encoder.alpha.data, 'decoder_alpha': m.module.decoder.alpha.data}, global_step)
            if global_step % hp.image_step == 1:
                for (i, prob) in enumerate(attn_probs):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_enc):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_enc_%d_0' % global_step, x, i * 4 + j)
                for (i, prob) in enumerate(attns_dec):
                    num_h = prob.size(0)
                    for j in range(4):
                        x = vutils.make_grid(prob[j * 16] * 255)
                        writer.add_image('Attention_dec_%d_0' % global_step, x, i * 4 + j)
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(m.parameters(), 1.0)
            optimizer.step()
            if global_step % hp.save_step == 0:
                t.save({'model': m.state_dict(), 'optimizer': optimizer.state_dict()}, os.path.join(hp.checkpoint_path, 'checkpoint_transformer_%d.pth.tar' % global_step))",'Processing at epoch %d' % epoch,f'Processing at epoch {epoch}',f'Processing at epoch {epoch}',1,,,,
yt-dlp,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/imdb.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlp/yt_dlp/extractor/imdb.py,ImdbListIE,"def _real_extract(self, url):
    list_id = self._match_id(url)
    webpage = self._download_webpage(url, list_id)
    entries = [self.url_result('http://www.imdb.com' + m, 'Imdb') for m in re.findall('href=""(/list/ls%s/videoplayer/vi[^""]+)""' % list_id, webpage)]
    list_title = self._html_search_regex('<h1[^>]+class=""[^""]*header[^""]*""[^>]*>(.*?)</h1>', webpage, 'list title')
    list_description = self._html_search_regex('<div[^>]+class=""[^""]*list-description[^""]*""[^>]*><p>(.*?)</p>', webpage, 'list description')
    return self.playlist_result(entries, list_id, list_title, list_description)",'href="(/list/ls%s/videoplayer/vi[^"]+)"' % list_id,f'href="(/list/ls{list_id}/videoplayer/vi[^"]+)"',f'href="(/list/ls{list_id}/videoplayer/vi[^"]+)"',1,,,,
GFM,https://github.com/JizhiziLi/GFM/tree/master/core/data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GFM/core/data.py,MattingTransform,"def __init__(self):
    super(MattingTransform, self).__init__()","super(MattingTransform, self).__init__()",super().__init__(),find_wrong,2,,,,
python-firebase,https://github.com/ozgur/python-firebase/tree/master/firebase/firebase_token_generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-firebase/firebase/firebase_token_generator.py,FirebaseTokenGenerator,"def _encode(self, bytes):
    encoded = base64.urlsafe_b64encode(bytes)
    return encoded.decode('utf-8').replace('=', '')","encoded.decode('utf-8').replace('=', '')",f"{encoded.decode('utf-8').rstrip('=')}",find_wrong,2,,,,
Cura,https://github.com/Ultimaker/Cura/tree/master/plugins/PostProcessingPlugin/scripts/ChangeAtZ.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Cura/plugins/PostProcessingPlugin/scripts/ChangeAtZ.py,ChangeAtZ,"def execute(self, data):
    caz_instance = ChangeAtZProcessor()
    caz_instance.targetValues = {}
    self.setIntSettingIfEnabled(caz_instance, 'e1_Change_speed', 'speed', 'e2_speed')
    self.setIntSettingIfEnabled(caz_instance, 'f1_Change_printspeed', 'printspeed', 'f2_printspeed')
    self.setIntSettingIfEnabled(caz_instance, 'g1_Change_flowrate', 'flowrate', 'g2_flowrate')
    self.setIntSettingIfEnabled(caz_instance, 'g3_Change_flowrateOne', 'flowrateOne', 'g4_flowrateOne')
    self.setIntSettingIfEnabled(caz_instance, 'g5_Change_flowrateTwo', 'flowrateTwo', 'g6_flowrateTwo')
    self.setFloatSettingIfEnabled(caz_instance, 'h1_Change_bedTemp', 'bedTemp', 'h2_bedTemp')
    self.setFloatSettingIfEnabled(caz_instance, 'h1_Change_buildVolumeTemperature', 'buildVolumeTemperature', 'h2_buildVolumeTemperature')
    self.setFloatSettingIfEnabled(caz_instance, 'i1_Change_extruderOne', 'extruderOne', 'i2_extruderOne')
    self.setFloatSettingIfEnabled(caz_instance, 'i3_Change_extruderTwo', 'extruderTwo', 'i4_extruderTwo')
    self.setIntSettingIfEnabled(caz_instance, 'j1_Change_fanSpeed', 'fanSpeed', 'j2_fanSpeed')
    self.setFloatSettingIfEnabled(caz_instance, 'caz_change_retractfeedrate', 'retractfeedrate', 'caz_retractfeedrate')
    self.setFloatSettingIfEnabled(caz_instance, 'caz_change_retractlength', 'retractlength', 'caz_retractlength')
    caz_instance.enabled = self.getSettingValueByKey('caz_enabled')
    caz_instance.displayChangesToLcd = self.getSettingValueByKey('caz_output_to_display')
    caz_instance.linearRetraction = self.getSettingValueByKey('caz_retractstyle') == 'linear'
    caz_instance.applyToSingleLayer = self.getSettingValueByKey('c_behavior') == 'single_layer'
    caz_instance.targetByLayer = self.getSettingValueByKey('a_trigger') == 'layer_no'
    caz_instance.targetLayer = self.getIntSettingByKey('b_targetL', None)
    caz_instance.targetZ = self.getFloatSettingByKey('b_targetZ', None)
    return caz_instance.execute(data)","('e1_Change_speed', 'speed', 'e2_speed')
('f1_Change_printspeed', 'printspeed', 'f2_printspeed')
('g1_Change_flowrate', 'flowrate', 'g2_flowrate')
('g3_Change_flowrateOne', 'flowrateOne', 'g4_flowrateOne')
('g5_Change_flowrateTwo', 'flowrateTwo', 'g6_flowrateTwo')
('h1_Change_bedTemp', 'bedTemp', 'h2_bedTemp')
('h1_Change_buildVolumeTemperature', 'buildVolumeTemperature', 'h2_buildVolumeTemperature')
('i1_Change_extruderOne', 'extruderOne', 'i2_extruderOne')
('i3_Change_extruderTwo', 'extruderTwo', 'i4_extruderTwo')
('j1_Change_fanSpeed', 'fanSpeed', 'j2_fanSpeed')
('caz_change_retractfeedrate', 'retractfeedrate', 'caz_retractfeedrate')
('caz_change_retractlength', 'retractlength', 'caz_retractlength')","'e1_Change_speed', 'speed', 'e2_speed' -> f""e1_Change_{speed}_e2_speed""
'f1_Change_printspeed', 'printspeed', 'f2_printspeed' -> f""f1_Change_{printspeed}_f2_printspeed""
'g1_Change_flowrate', 'flowrate', 'g2_flowrate' -> f""g1_Change_{flowrate}_g2_flowrate""
'g3_Change_flowrateOne', 'flowrateOne', 'g4_flowrateOne' -> f""g3_Change_{flowrateOne}_g4_flowrateOne""
'g5_Change_flowrateTwo', 'flowrateTwo', 'g6_flowrateTwo' -> f""g5_Change_{flowrateTwo}_g6_flowrateTwo""
'h1_Change_bedTemp', 'bedTemp', 'h2_bedTemp' -> f""h1_Change_{bedTemp}_h2_bedTemp""
'h1_Change_buildVolumeTemperature', 'buildVolumeTemperature', 'h2_buildVolumeTemperature' -> f""h1_Change_{buildVolumeTemperature}_h2_buildVolumeTemperature""
'i1_Change_extruderOne', 'extruderOne', 'i2_extruderOne' -> f""i1_Change_{extruderOne}_i2_extruderOne""
'i3_Change_extruderTwo', 'extruderTwo', 'i4_extruderTwo' -> f""i3_Change_{extruderTwo}_i4_extruderTwo""
'j1_Change_fanSpeed', 'fanSpeed', 'j2_fanSpeed' -> f""j1_Change_{fanSpeed}_j2_fanSpeed""
'caz_change_retractfeedrate', 'retractfeedrate', 'caz_retractfeedrate' -> f""caz_change_{retractfeedrate}_caz_retractfeedrate""
'caz_change_retractlength', 'retractlength', 'caz_retractlength' -> f""caz_change_{retractlength}_caz_retractlength""",find_wrong,2,,,,
detection-rules,https://github.com/elastic/detection-rules/tree/master/detection_rules/devtools.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/detection-rules/detection_rules/devtools.py,,"def kibana_commit(ctx, local_repo: str, github_repo: str, ssh: bool, kibana_directory: str, base_branch: str, branch_name: Optional[str], message: Optional[str], push: bool) -> (str, str):
    """"""Prep a commit and push to Kibana.""""""
    package_name = Package.load_configs()['name']
    release_dir = os.path.join(RELEASE_DIR, package_name)
    message = message or f'[Detection Rules] Add {package_name} rules'
    if not os.path.exists(release_dir):
        click.secho(""Release directory doesn't exist."", fg='red', err=True)
        click.echo(f""Run {click.style('python -m detection_rules dev build-release', bold=True)} to populate"", err=True)
        ctx.exit(1)
    git = utils.make_git('-C', local_repo)
    rules_git = utils.make_git('-C', utils.get_path())
    long_commit_hash = rules_git('rev-parse', 'HEAD')
    short_commit_hash = rules_git('rev-parse', '--short', 'HEAD')
    try:
        if not os.path.exists(local_repo):
            click.echo(f""Kibana repository doesn't exist at {local_repo}. Cloning..."")
            url = f'git@github.com:{github_repo}.git' if ssh else f'https://github.com/{github_repo}.git'
            utils.make_git()('clone', url, local_repo, '--depth', '1')
        else:
            git('checkout', base_branch)
        branch_name = branch_name or f'detection-rules/{package_name}-{short_commit_hash}'
        git('checkout', '-b', branch_name, print_output=True)
        git('rm', '-r', kibana_directory)
        source_dir = os.path.join(release_dir, 'rules')
        target_dir = os.path.join(local_repo, kibana_directory)
        os.makedirs(target_dir)
        for name in os.listdir(source_dir):
            (_, ext) = os.path.splitext(name)
            path = os.path.join(source_dir, name)
            if ext in ('.ts', '.json'):
                shutil.copyfile(path, os.path.join(target_dir, name))
        git('add', kibana_directory)
        git('commit', '--no-verify', '-m', message)
        git('status', print_output=True)
        if push:
            git('push', 'origin', branch_name)
        click.echo(f'Kibana repository {local_repo} prepped. Push changes when ready')
        click.secho(f'cd {local_repo}', bold=True)
        return (branch_name, long_commit_hash)
    except subprocess.CalledProcessError as e:
        client_error(str(e), e, ctx=ctx)",f'[Detection Rules] Add {package_name} rules','[Detection Rules] Add {} rules'.format(package_name),find_wrong,2,,,,
detection-rules,https://github.com/elastic/detection-rules/tree/master/detection_rules/devtools.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/detection-rules/detection_rules/devtools.py,,"def kibana_commit(ctx, local_repo: str, github_repo: str, ssh: bool, kibana_directory: str, base_branch: str, branch_name: Optional[str], message: Optional[str], push: bool) -> (str, str):
    """"""Prep a commit and push to Kibana.""""""
    package_name = Package.load_configs()['name']
    release_dir = os.path.join(RELEASE_DIR, package_name)
    message = message or f'[Detection Rules] Add {package_name} rules'
    if not os.path.exists(release_dir):
        click.secho(""Release directory doesn't exist."", fg='red', err=True)
        click.echo(f""Run {click.style('python -m detection_rules dev build-release', bold=True)} to populate"", err=True)
        ctx.exit(1)
    git = utils.make_git('-C', local_repo)
    rules_git = utils.make_git('-C', utils.get_path())
    long_commit_hash = rules_git('rev-parse', 'HEAD')
    short_commit_hash = rules_git('rev-parse', '--short', 'HEAD')
    try:
        if not os.path.exists(local_repo):
            click.echo(f""Kibana repository doesn't exist at {local_repo}. Cloning..."")
            url = f'git@github.com:{github_repo}.git' if ssh else f'https://github.com/{github_repo}.git'
            utils.make_git()('clone', url, local_repo, '--depth', '1')
        else:
            git('checkout', base_branch)
        branch_name = branch_name or f'detection-rules/{package_name}-{short_commit_hash}'
        git('checkout', '-b', branch_name, print_output=True)
        git('rm', '-r', kibana_directory)
        source_dir = os.path.join(release_dir, 'rules')
        target_dir = os.path.join(local_repo, kibana_directory)
        os.makedirs(target_dir)
        for name in os.listdir(source_dir):
            (_, ext) = os.path.splitext(name)
            path = os.path.join(source_dir, name)
            if ext in ('.ts', '.json'):
                shutil.copyfile(path, os.path.join(target_dir, name))
        git('add', kibana_directory)
        git('commit', '--no-verify', '-m', message)
        git('status', print_output=True)
        if push:
            git('push', 'origin', branch_name)
        click.echo(f'Kibana repository {local_repo} prepped. Push changes when ready')
        click.secho(f'cd {local_repo}', bold=True)
        return (branch_name, long_commit_hash)
    except subprocess.CalledProcessError as e:
        client_error(str(e), e, ctx=ctx)",f"Kibana repository doesn't exist at {local_repo}. Cloning...","""Kibana repository doesn't exist at {}. Cloning..."".format(local_repo)",find_wrong,2,,,,
detection-rules,https://github.com/elastic/detection-rules/tree/master/detection_rules/devtools.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/detection-rules/detection_rules/devtools.py,,"def kibana_commit(ctx, local_repo: str, github_repo: str, ssh: bool, kibana_directory: str, base_branch: str, branch_name: Optional[str], message: Optional[str], push: bool) -> (str, str):
    """"""Prep a commit and push to Kibana.""""""
    package_name = Package.load_configs()['name']
    release_dir = os.path.join(RELEASE_DIR, package_name)
    message = message or f'[Detection Rules] Add {package_name} rules'
    if not os.path.exists(release_dir):
        click.secho(""Release directory doesn't exist."", fg='red', err=True)
        click.echo(f""Run {click.style('python -m detection_rules dev build-release', bold=True)} to populate"", err=True)
        ctx.exit(1)
    git = utils.make_git('-C', local_repo)
    rules_git = utils.make_git('-C', utils.get_path())
    long_commit_hash = rules_git('rev-parse', 'HEAD')
    short_commit_hash = rules_git('rev-parse', '--short', 'HEAD')
    try:
        if not os.path.exists(local_repo):
            click.echo(f""Kibana repository doesn't exist at {local_repo}. Cloning..."")
            url = f'git@github.com:{github_repo}.git' if ssh else f'https://github.com/{github_repo}.git'
            utils.make_git()('clone', url, local_repo, '--depth', '1')
        else:
            git('checkout', base_branch)
        branch_name = branch_name or f'detection-rules/{package_name}-{short_commit_hash}'
        git('checkout', '-b', branch_name, print_output=True)
        git('rm', '-r', kibana_directory)
        source_dir = os.path.join(release_dir, 'rules')
        target_dir = os.path.join(local_repo, kibana_directory)
        os.makedirs(target_dir)
        for name in os.listdir(source_dir):
            (_, ext) = os.path.splitext(name)
            path = os.path.join(source_dir, name)
            if ext in ('.ts', '.json'):
                shutil.copyfile(path, os.path.join(target_dir, name))
        git('add', kibana_directory)
        git('commit', '--no-verify', '-m', message)
        git('status', print_output=True)
        if push:
            git('push', 'origin', branch_name)
        click.echo(f'Kibana repository {local_repo} prepped. Push changes when ready')
        click.secho(f'cd {local_repo}', bold=True)
        return (branch_name, long_commit_hash)
    except subprocess.CalledProcessError as e:
        client_error(str(e), e, ctx=ctx)",branch_name = branch_name or f'detection-rules/{package_name}-{short_commit_hash}',"branch_name = branch_name or 'detection-rules/{}-{}'.format(package_name, short_commit_hash)",find_wrong,2,,,,
bertviz,https://github.com/jessevig/bertviz/tree/master/bertviz/transformers_neuron_view/modeling_openai.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/transformers_neuron_view/modeling_openai.py,,"def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):
    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)
    """"""
    import re
    import numpy as np
    if '.ckpt' in openai_checkpoint_folder_path:
        openai_checkpoint_folder_path = os.path.dirname(openai_checkpoint_folder_path)
    logger.info('Loading weights from {}'.format(openai_checkpoint_folder_path))
    names = json.load(open(openai_checkpoint_folder_path + '/parameters_names.json', 'r', encoding='utf-8'))
    shapes = json.load(open(openai_checkpoint_folder_path + '/params_shapes.json', 'r', encoding='utf-8'))
    offsets = np.cumsum([np.prod(shape) for shape in shapes])
    init_params = [np.load(openai_checkpoint_folder_path + '/params_{}.npy'.format(n)) for n in range(10)]
    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]
    init_params = [param.reshape(shape) for (param, shape) in zip(init_params, shapes)]
    init_params = [arr.squeeze() for arr in init_params]
    try:
        assert model.tokens_embed.weight.shape == init_params[1].shape
        assert model.positions_embed.weight.shape == init_params[0].shape
    except AssertionError as e:
        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)
        e.args += (model.positions_embed.weight.shape, init_params[0].shape)
        raise
    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])
    model.positions_embed.weight.data = torch.from_numpy(init_params[0])
    names.pop(0)
    init_params.pop(0)
    init_params.pop(0)
    for (name, array) in zip(names, init_params):
        name = name[6:]
        assert name[-2:] == ':0'
        name = name[:-2]
        name = name.split('/')
        pointer = model
        for m_name in name:
            if re.fullmatch('[A-Za-z]+\\d+', m_name):
                l = re.split('(\\d+)', m_name)
            else:
                l = [m_name]
            if l[0] == 'g':
                pointer = getattr(pointer, 'weight')
            elif l[0] == 'b':
                pointer = getattr(pointer, 'bias')
            elif l[0] == 'w':
                pointer = getattr(pointer, 'weight')
            else:
                pointer = getattr(pointer, l[0])
            if len(l) >= 2:
                num = int(l[1])
                pointer = pointer[num]
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        logger.info('Initialize PyTorch weight {}'.format(name))
        pointer.data = torch.from_numpy(array)
    return model",logger.info('Loading weights from {}'.format(openai_checkpoint_folder_path)),logger.info(f'Loading weights from {openai_checkpoint_folder_path}'),find_wrong,,1,3,,3
bertviz,https://github.com/jessevig/bertviz/tree/master/bertviz/transformers_neuron_view/modeling_openai.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/transformers_neuron_view/modeling_openai.py,,"def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):
    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)
    """"""
    import re
    import numpy as np
    if '.ckpt' in openai_checkpoint_folder_path:
        openai_checkpoint_folder_path = os.path.dirname(openai_checkpoint_folder_path)
    logger.info('Loading weights from {}'.format(openai_checkpoint_folder_path))
    names = json.load(open(openai_checkpoint_folder_path + '/parameters_names.json', 'r', encoding='utf-8'))
    shapes = json.load(open(openai_checkpoint_folder_path + '/params_shapes.json', 'r', encoding='utf-8'))
    offsets = np.cumsum([np.prod(shape) for shape in shapes])
    init_params = [np.load(openai_checkpoint_folder_path + '/params_{}.npy'.format(n)) for n in range(10)]
    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]
    init_params = [param.reshape(shape) for (param, shape) in zip(init_params, shapes)]
    init_params = [arr.squeeze() for arr in init_params]
    try:
        assert model.tokens_embed.weight.shape == init_params[1].shape
        assert model.positions_embed.weight.shape == init_params[0].shape
    except AssertionError as e:
        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)
        e.args += (model.positions_embed.weight.shape, init_params[0].shape)
        raise
    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])
    model.positions_embed.weight.data = torch.from_numpy(init_params[0])
    names.pop(0)
    init_params.pop(0)
    init_params.pop(0)
    for (name, array) in zip(names, init_params):
        name = name[6:]
        assert name[-2:] == ':0'
        name = name[:-2]
        name = name.split('/')
        pointer = model
        for m_name in name:
            if re.fullmatch('[A-Za-z]+\\d+', m_name):
                l = re.split('(\\d+)', m_name)
            else:
                l = [m_name]
            if l[0] == 'g':
                pointer = getattr(pointer, 'weight')
            elif l[0] == 'b':
                pointer = getattr(pointer, 'bias')
            elif l[0] == 'w':
                pointer = getattr(pointer, 'weight')
            else:
                pointer = getattr(pointer, l[0])
            if len(l) >= 2:
                num = int(l[1])
                pointer = pointer[num]
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        logger.info('Initialize PyTorch weight {}'.format(name))
        pointer.data = torch.from_numpy(array)
    return model",'/params_{}.npy'.format(n),f'/params_{n}.npy',find_wrong,,1,3,,3
bertviz,https://github.com/jessevig/bertviz/tree/master/bertviz/transformers_neuron_view/modeling_openai.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bertviz/bertviz/transformers_neuron_view/modeling_openai.py,,"def load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):
    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)
    """"""
    import re
    import numpy as np
    if '.ckpt' in openai_checkpoint_folder_path:
        openai_checkpoint_folder_path = os.path.dirname(openai_checkpoint_folder_path)
    logger.info('Loading weights from {}'.format(openai_checkpoint_folder_path))
    names = json.load(open(openai_checkpoint_folder_path + '/parameters_names.json', 'r', encoding='utf-8'))
    shapes = json.load(open(openai_checkpoint_folder_path + '/params_shapes.json', 'r', encoding='utf-8'))
    offsets = np.cumsum([np.prod(shape) for shape in shapes])
    init_params = [np.load(openai_checkpoint_folder_path + '/params_{}.npy'.format(n)) for n in range(10)]
    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]
    init_params = [param.reshape(shape) for (param, shape) in zip(init_params, shapes)]
    init_params = [arr.squeeze() for arr in init_params]
    try:
        assert model.tokens_embed.weight.shape == init_params[1].shape
        assert model.positions_embed.weight.shape == init_params[0].shape
    except AssertionError as e:
        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)
        e.args += (model.positions_embed.weight.shape, init_params[0].shape)
        raise
    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])
    model.positions_embed.weight.data = torch.from_numpy(init_params[0])
    names.pop(0)
    init_params.pop(0)
    init_params.pop(0)
    for (name, array) in zip(names, init_params):
        name = name[6:]
        assert name[-2:] == ':0'
        name = name[:-2]
        name = name.split('/')
        pointer = model
        for m_name in name:
            if re.fullmatch('[A-Za-z]+\\d+', m_name):
                l = re.split('(\\d+)', m_name)
            else:
                l = [m_name]
            if l[0] == 'g':
                pointer = getattr(pointer, 'weight')
            elif l[0] == 'b':
                pointer = getattr(pointer, 'bias')
            elif l[0] == 'w':
                pointer = getattr(pointer, 'weight')
            else:
                pointer = getattr(pointer, l[0])
            if len(l) >= 2:
                num = int(l[1])
                pointer = pointer[num]
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        try:
            assert pointer.shape == array.shape
        except AssertionError as e:
            e.args += (pointer.shape, array.shape)
            raise
        logger.info('Initialize PyTorch weight {}'.format(name))
        pointer.data = torch.from_numpy(array)
    return model",logger.info('Initialize PyTorch weight {}'.format(name)),logger.info(f'Initialize PyTorch weight {name}'),find_wrong,,1,3,,4
Python-Backdoor,https://github.com/xp4xbox/Python-Backdoor/tree/master/src/archive/old_keylogger/source.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Python-Backdoor/src/archive/old_keylogger/source.py,,"def CheckForOption():
    global strLogs
    while True:
        time.sleep(0.5)
        if os.path.isfile(strSettings) and os.path.getsize(strSettings) > 0 and (os.path.getsize(strSettings) < 20):
            txtData = open(strSettings, 'r')
            txt = txtData.read()
            if txt == 'stop':
                txtData.close()
                open(strSettings, 'w').close()
                break
            elif txt == 'dump':
                txtData = open(TMP + '/spblog.txt', 'w')
                txtData.write(strLogs)
                txtData.close()
                strLogs = ''
                open(strSettings, 'w').close()
    os._exit(0)","open(TMP + '/spblog.txt', 'w')","open(f'{TMP}/spblog.txt', 'w')",find_wrong,,1,3,,
pandapower,https://github.com/e2nIEE/pandapower/tree/master/pandapower/test/shortcircuit/test_1ph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/test/shortcircuit/test_1ph.py,,"def test_1ph_shortcircuit_min():
    results = {'Yy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Yyn': [0.52209346201, 2.4135757259, 1.545054139, 0.99373917957], 'Yd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'YNy': [0.62316686505, 0.66632662571, 0.66756160176, 0.72517293174], 'YNyn': [0.620287259, 2.9155736491, 1.7561556936, 1.0807305212], 'YNd': [0.75434229157, 0.66632662571, 0.66756160176, 0.72517293174], 'Dy': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174], 'Dyn': [0.52209346201, 3.4393798093, 1.9535982949, 1.1558364456], 'Dd': [0.52209346201, 0.66632662571, 0.66756160176, 0.72517293174]}
    for inv_y in (False, True):
        for (vc, result) in results.items():
            net = pp.create_empty_network(sn_mva=16)
            add_network(net, vc)
            try:
                sc.calc_sc(net, fault='1ph', case='min', inverse_y=inv_y)
            except Exception as e:
                raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}')
            check_results(net, vc, result)",raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}'),raise UserWarning(f'{str(e)}: Did not converge after adding transformer with vector group {vc}'),find_wrong,2,,,,
HUNT,https://github.com/bugcrowd/HUNT/tree/master/Burp/lib/issues.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HUNT/Burp/lib/issues.py,Issues,"def create_scanner_issues(self, view, callbacks, helpers, vuln_parameters, request_response):
    issues = self.issues
    json = self.json
    for vuln_parameter in vuln_parameters:
        issue_name = vuln_parameter['vuln_name']
        vuln_param = vuln_parameter['vuln_param']
        param_name = vuln_parameter['param']
        param_value = vuln_parameter['value']
        url = helpers.analyzeRequest(request_response).getUrl()
        url = urlparse.urlsplit(str(url))
        hostname = url.hostname
        path = url.path
        url = url.scheme + '://' + url.hostname + url.path
        http_service = request_response.getHttpService()
        http_messages = [callbacks.applyMarkers(request_response, None, None)]
        detail = json['issues'][issue_name]['detail']
        severity = 'Medium'
        scanner_issue = ScannerIssue(url, issue_name, param_name, vuln_param, param_value, hostname, path, http_service, http_messages, detail, severity, request_response)
        is_scanner_issue_dupe = self.check_duplicate_issue(scanner_issue)
        if is_scanner_issue_dupe:
            continue
        else:
            self.set_scanner_issues(scanner_issue)
        issue_count = self.set_issue_count(issue_name, vuln_param)
        total_count = self.total_count[issue_name]
        view.set_scanner_count(issue_name, vuln_param, issue_count, total_count)
        view.scanner_table_models.set_scanner_table_model(scanner_issue, issue_name, param_name, vuln_param)",url = url.scheme + '://' + url.hostname + url.path,url = f'{url.scheme}://{url.hostname}{url.path}',find_wrong,,1,3,,
HUNT,https://github.com/bugcrowd/HUNT/tree/master/Burp/lib/issues.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HUNT/Burp/lib/issues.py,Issues,"def create_scanner_issues(self, view, callbacks, helpers, vuln_parameters, request_response):
    issues = self.issues
    json = self.json
    for vuln_parameter in vuln_parameters:
        issue_name = vuln_parameter['vuln_name']
        vuln_param = vuln_parameter['vuln_param']
        param_name = vuln_parameter['param']
        param_value = vuln_parameter['value']
        url = helpers.analyzeRequest(request_response).getUrl()
        url = urlparse.urlsplit(str(url))
        hostname = url.hostname
        path = url.path
        url = url.scheme + '://' + url.hostname + url.path
        http_service = request_response.getHttpService()
        http_messages = [callbacks.applyMarkers(request_response, None, None)]
        detail = json['issues'][issue_name]['detail']
        severity = 'Medium'
        scanner_issue = ScannerIssue(url, issue_name, param_name, vuln_param, param_value, hostname, path, http_service, http_messages, detail, severity, request_response)
        is_scanner_issue_dupe = self.check_duplicate_issue(scanner_issue)
        if is_scanner_issue_dupe:
            continue
        else:
            self.set_scanner_issues(scanner_issue)
        issue_count = self.set_issue_count(issue_name, vuln_param)
        total_count = self.total_count[issue_name]
        view.set_scanner_count(issue_name, vuln_param, issue_count, total_count)
        view.scanner_table_models.set_scanner_table_model(scanner_issue, issue_name, param_name, vuln_param)",severity = 'Medium',severity = f'Medium',find_wrong,2,,,,
d2l-en,https://github.com/d2l-ai/d2l-en/tree/master/d2l/tensorflow.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/d2l-en/d2l/tensorflow.py,,"def train_ch11(trainer_fn, states, hyperparams, data_iter, feature_dim, num_epochs=2):
    """"""Defined in :numref:`sec_minibatches`""""""
    w = tf.Variable(tf.random.normal(shape=(feature_dim, 1), mean=0, stddev=0.01), trainable=True)
    b = tf.Variable(tf.zeros(1), trainable=True)
    (net, loss) = (lambda X: d2l.linreg(X, w, b), d2l.squared_loss)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[0, num_epochs], ylim=[0.22, 0.35])
    (n, timer) = (0, d2l.Timer())
    for _ in range(num_epochs):
        for (X, y) in data_iter:
            with tf.GradientTape() as g:
                l = tf.math.reduce_mean(loss(net(X), y))
            (dw, db) = g.gradient(l, [w, b])
            trainer_fn([w, b], [dw, db], states, hyperparams)
            n += X.shape[0]
            if n % 200 == 0:
                timer.stop()
                p = n / X.shape[0]
                q = p / tf.data.experimental.cardinality(data_iter).numpy()
                r = (d2l.evaluate_loss(net, data_iter, loss),)
                animator.add(q, r)
                timer.start()
    print(f'loss: {animator.Y[0][-1]:.3f}, {timer.sum() / num_epochs:.3f} sec/epoch')
    return (timer.cumsum(), animator.Y[0])","""""""loss: {animator.Y[0][-1]:.3f}, {timer.sum() / num_epochs:.3f} sec/epoch""""""","f'loss: {animator.Y[0][-1]:.3f}, {timer.sum() / num_epochs:.3f} sec/epoch'",find_wrong,2,,,,
faust,https://github.com/robinhood/faust/tree/master/t/stress/producer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/faust/t/stress/producer.py,,"def install_produce_command(app) -> None:

    @app.command(option('--max-latency', type=float, default=0.5, envvar='PRODUCE_LATENCY', help='Add delay of (at most) n seconds between publishing.'), option('--max-messages', type=int, default=None, help='Send at most N messages or 0 for infinity.'))
    async def produce(self, max_latency: float, max_messages: int):
        """"""Produce example events.""""""
        prods = {aiter(p(max_messages)) for p in app.stress_producers}
        i = 0
        while not app.should_stop:
            to_remove: Set[Any] = set()
            for producer in prods:
                i += 1
                try:
                    await anext(producer)
                except StopAsyncIteration:
                    to_remove.add(producer)
                if not max_latency:
                    if not i % 10000:
                        self.say(f'+SEND {i}')
                elif not i % 10:
                    self.say(f'+SEND {i}')
            if not prods:
                await asyncio.sleep(1.0)
            if max_latency:
                await asyncio.sleep(random.uniform(0, max_latency))
            for producer in to_remove:
                prods.discard(producer)
        print('No more producers - exiting', file=sys.stderr)",f'+SEND {i}',"f'+SEND {i}' (no change needed, already using fstring)",find_wrong,2,,,,
curator,https://github.com/elastic/curator/tree/master/curator/indexlist.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/curator/curator/indexlist.py,IndexList,"def filter_by_size(self, size_threshold=None, threshold_behavior='greater_than', exclude=False, size_behavior='primary'):
    """"""
        Remove indices from the actionable list based on index size.

        `threshold_behavior`, when set to `greater_than` (default), includes if it the index
        tests to be larger than `size_threshold`. When set to `less_than`, it includes if
        the index is smaller than `size_threshold`

        :arg size_threshold: Filter indices over *n* gigabytes
        :arg threshold_behavior: Size to filter, either ``greater_than`` or ``less_than``. Defaults
            to ``greater_than`` to preserve backwards compatability.
        :arg size_behavior: Size that used to filter, either ``primary`` or ``total``. Defaults to ``primary``
        :arg exclude: If `exclude` is `True`, this filter will remove matching
            indices from `indices`. If `exclude` is `False`, then only matching
            indices will be kept in `indices`.
            Default is `False`
        """"""
    self.loggit.debug('Filtering indices by index size')
    if not size_threshold:
        raise exceptions.MissingArgument('No value for ""size_threshold"" provided')
    if size_behavior not in ['primary', 'total']:
        raise ValueError('Invalid value for ""size_behavior"": {0}'.format(size_behavior))
    if threshold_behavior not in ['greater_than', 'less_than']:
        raise ValueError('Invalid value for ""threshold_behavior"": {0}'.format(threshold_behavior))
    index_size_limit = float(size_threshold) * 2 ** 30
    self.loggit.debug('Cannot get disk usage info from closed indices.  Omitting any closed indices.')
    self.filter_closed()
    working_list = self.working_list()
    for index in working_list:
        if size_behavior == 'primary':
            index_size = self.index_info[index]['primary_size_in_bytes']
        else:
            index_size = self.index_info[index]['size_in_bytes']
        msg = '{0}, index size is {1} and size limit is {2}.'.format(index, utils.byte_size(index_size), utils.byte_size(index_size_limit))
        if threshold_behavior == 'greater_than':
            self.__excludify(index_size > index_size_limit, exclude, index, msg)
        elif threshold_behavior == 'less_than':
            self.__excludify(index_size < index_size_limit, exclude, index, msg)","'{0}, index size is {1} and size limit is {2}.'.format(index, utils.byte_size(index_size), utils.byte_size(index_size_limit))","f'{index}, index size is {utils.byte_size(index_size)} and size limit is {utils.byte_size(index_size_limit)}.'",find_wrong,,1,3,,
Machine-Learning-Collection,https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/others/default_setups/CV - Image Classification/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Machine-Learning-Collection/ML/Pytorch/others/default_setups/CV - Image Classification/utils.py,,"def check_accuracy(loader, model, device='cuda'):
    num_correct = 0
    num_samples = 0
    model.eval()
    with torch.no_grad():
        for (x, y) in loader:
            x = x.to(device=device)
            y = y.to(device=device)
            scores = torch.sigmoid(model(x))
            predictions = (scores > 0.5).float()
            num_correct += (predictions == y).sum()
            num_samples += predictions.shape[0]
        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}')
    model.train()","'Got {} / {} with accuracy {:.2f}'.format(num_correct, num_samples, float(num_correct) / float(num_samples) * 100)",f'Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}',find_wrong,,1,3,,
PowerDNS-Admin,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/routes/index.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PowerDNS-Admin/powerdnsadmin/routes/index.py,,"def dyndns_update():
    hostname = request.args.get('hostname')
    myip = request.args.get('myip')
    if not hostname:
        history = History(msg='DynDNS update: missing hostname parameter', created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    try:
        if current_user.role.name in ['Administrator', 'Operator']:
            domains = Domain.query.all()
        else:
            domains = db.session.query(Domain).outerjoin(DomainUser, Domain.id == DomainUser.domain_id).outerjoin(Account, Domain.account_id == Account.id).outerjoin(AccountUser, Account.id == AccountUser.account_id).filter(db.or_(DomainUser.user_id == current_user.id, AccountUser.user_id == current_user.id)).all()
    except Exception as e:
        current_app.logger.error('DynDNS Error: {0}'.format(e))
        current_app.logger.debug(traceback.format_exc())
        return (render_template('dyndns.html', response='911'), 200)
    domain = None
    domain_segments = hostname.split('.')
    for _index in range(len(domain_segments)):
        full_domain = '.'.join(domain_segments)
        potential_domain = Domain.query.filter(Domain.name == full_domain).first()
        if potential_domain in domains:
            domain = potential_domain
            break
        domain_segments.pop(0)
    if not domain:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    myip_addr = []
    if myip:
        for address in myip.split(','):
            myip_addr += utils.validate_ipaddress(address)
    remote_addr = utils.validate_ipaddress(request.headers.get('X-Forwarded-For', request.remote_addr).split(', ')[0])
    response = 'nochg'
    for ip in myip_addr or remote_addr:
        if isinstance(ip, ipaddress.IPv4Address):
            rtype = 'A'
        else:
            rtype = 'AAAA'
        r = Record(name=hostname, type=rtype)
        if r.exists(domain.name) and r.is_allowed_edit():
            if r.data == str(ip):
                history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
                history.add()
            else:
                oldip = r.data
                result = r.update(domain.name, str(ip))
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
                else:
                    response = '911'
                    break
        elif r.is_allowed_edit():
            ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
            if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
                rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
                rrset = {'rrsets': rrset_data}
                result = Record().add(domain.name, rrset)
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
        else:
            history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
            history.add()
    return (render_template('dyndns.html', response=response), 200)",'DynDNS Error: {0}'.format(e),f'DynDNS Error: {e}',find_wrong,,1,3,,
PowerDNS-Admin,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/routes/index.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PowerDNS-Admin/powerdnsadmin/routes/index.py,,"def dyndns_update():
    hostname = request.args.get('hostname')
    myip = request.args.get('myip')
    if not hostname:
        history = History(msg='DynDNS update: missing hostname parameter', created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    try:
        if current_user.role.name in ['Administrator', 'Operator']:
            domains = Domain.query.all()
        else:
            domains = db.session.query(Domain).outerjoin(DomainUser, Domain.id == DomainUser.domain_id).outerjoin(Account, Domain.account_id == Account.id).outerjoin(AccountUser, Account.id == AccountUser.account_id).filter(db.or_(DomainUser.user_id == current_user.id, AccountUser.user_id == current_user.id)).all()
    except Exception as e:
        current_app.logger.error('DynDNS Error: {0}'.format(e))
        current_app.logger.debug(traceback.format_exc())
        return (render_template('dyndns.html', response='911'), 200)
    domain = None
    domain_segments = hostname.split('.')
    for _index in range(len(domain_segments)):
        full_domain = '.'.join(domain_segments)
        potential_domain = Domain.query.filter(Domain.name == full_domain).first()
        if potential_domain in domains:
            domain = potential_domain
            break
        domain_segments.pop(0)
    if not domain:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    myip_addr = []
    if myip:
        for address in myip.split(','):
            myip_addr += utils.validate_ipaddress(address)
    remote_addr = utils.validate_ipaddress(request.headers.get('X-Forwarded-For', request.remote_addr).split(', ')[0])
    response = 'nochg'
    for ip in myip_addr or remote_addr:
        if isinstance(ip, ipaddress.IPv4Address):
            rtype = 'A'
        else:
            rtype = 'AAAA'
        r = Record(name=hostname, type=rtype)
        if r.exists(domain.name) and r.is_allowed_edit():
            if r.data == str(ip):
                history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
                history.add()
            else:
                oldip = r.data
                result = r.update(domain.name, str(ip))
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
                else:
                    response = '911'
                    break
        elif r.is_allowed_edit():
            ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
            if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
                rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
                rrset = {'rrsets': rrset_data}
                result = Record().add(domain.name, rrset)
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
        else:
            history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
            history.add()
    return (render_template('dyndns.html', response=response), 200)",'DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname),f'DynDNS update: attempted update of {hostname} but it does not exist for this user',find_wrong,,1,3,,
PowerDNS-Admin,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/routes/index.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PowerDNS-Admin/powerdnsadmin/routes/index.py,,"def dyndns_update():
    hostname = request.args.get('hostname')
    myip = request.args.get('myip')
    if not hostname:
        history = History(msg='DynDNS update: missing hostname parameter', created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    try:
        if current_user.role.name in ['Administrator', 'Operator']:
            domains = Domain.query.all()
        else:
            domains = db.session.query(Domain).outerjoin(DomainUser, Domain.id == DomainUser.domain_id).outerjoin(Account, Domain.account_id == Account.id).outerjoin(AccountUser, Account.id == AccountUser.account_id).filter(db.or_(DomainUser.user_id == current_user.id, AccountUser.user_id == current_user.id)).all()
    except Exception as e:
        current_app.logger.error('DynDNS Error: {0}'.format(e))
        current_app.logger.debug(traceback.format_exc())
        return (render_template('dyndns.html', response='911'), 200)
    domain = None
    domain_segments = hostname.split('.')
    for _index in range(len(domain_segments)):
        full_domain = '.'.join(domain_segments)
        potential_domain = Domain.query.filter(Domain.name == full_domain).first()
        if potential_domain in domains:
            domain = potential_domain
            break
        domain_segments.pop(0)
    if not domain:
        history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
        history.add()
        return (render_template('dyndns.html', response='nohost'), 200)
    myip_addr = []
    if myip:
        for address in myip.split(','):
            myip_addr += utils.validate_ipaddress(address)
    remote_addr = utils.validate_ipaddress(request.headers.get('X-Forwarded-For', request.remote_addr).split(', ')[0])
    response = 'nochg'
    for ip in myip_addr or remote_addr:
        if isinstance(ip, ipaddress.IPv4Address):
            rtype = 'A'
        else:
            rtype = 'AAAA'
        r = Record(name=hostname, type=rtype)
        if r.exists(domain.name) and r.is_allowed_edit():
            if r.data == str(ip):
                history = History(msg='DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname), created_by=current_user.username, domain_id=domain.id)
                history.add()
            else:
                oldip = r.data
                result = r.update(domain.name, str(ip))
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: updated {} successfully'.format(hostname), detail=json.dumps({'domain': domain.name, 'record': hostname, 'type': rtype, 'old_value': oldip, 'new_value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
                else:
                    response = '911'
                    break
        elif r.is_allowed_edit():
            ondemand_creation = DomainSetting.query.filter(DomainSetting.domain == domain).filter(DomainSetting.setting == 'create_via_dyndns').first()
            if ondemand_creation is not None and strtobool(ondemand_creation.value) == True:
                rrset_data = [{'changetype': 'REPLACE', 'name': hostname + '.', 'ttl': 3600, 'type': rtype, 'records': [{'content': str(ip), 'disabled': False}], 'comments': []}]
                rrset = {'rrsets': rrset_data}
                result = Record().add(domain.name, rrset)
                if result['status'] == 'ok':
                    history = History(msg='DynDNS update: created record {0} in zone {1} successfully'.format(hostname, domain.name, str(ip)), detail=json.dumps({'domain': domain.name, 'record': hostname, 'value': str(ip)}), created_by=current_user.username, domain_id=domain.id)
                    history.add()
                    response = 'good'
        else:
            history = History(msg='DynDNS update: attempted update of {0} but it does not exist for this user'.format(hostname), created_by=current_user.username)
            history.add()
    return (render_template('dyndns.html', response=response), 200)",'DynDNS update: attempted update of {0} but record already up-to-date'.format(hostname),f'DynDNS update: attempted update of {hostname} but record already up-to-date',find_wrong,,1,3,,
PGL,https://github.com/PaddlePaddle/PGL/tree/master/apps/Graph4Rec/env_run/src/datasets/ego_graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/apps/Graph4Rec/env_run/src/datasets/ego_graph.py,EgoGraphGenerator,"def __init__(self, config, graph, **kwargs):
    self.config = config
    self.graph = graph
    self.rank = kwargs.get('rank', 0)
    self.nrank = kwargs.get('nrank', 1)
    self.kwargs = kwargs
    self.edge_types = self.graph.get_edge_types()
    self.sample_num_list = kwargs.get('sample_list', self.config.sample_num_list)
    log.info('sample_num_list is %s' % repr(self.sample_num_list))",log.info('sample_num_list is %s' % repr(self.sample_num_list)),log.info(f'sample_num_list is {repr(self.sample_num_list)}'),find_wrong,,,,,
kamene,https://github.com/phaethon/kamene/tree/master/kamene/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/utils.py,,"def hexdiff(x, y):
    """"""Show differences between 2 binary strings""""""
    x = any2b(x)[::-1]
    y = any2b(y)[::-1]
    SUBST = 1
    INSERT = 1
    d = {}
    d[-1, -1] = (0, (-1, -1))
    for j in range(len(y)):
        d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))
    for i in range(len(x)):
        d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))
    for j in range(len(y)):
        for i in range(len(x)):
            d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))
    backtrackx = []
    backtracky = []
    i = len(x) - 1
    j = len(y) - 1
    while not i == j == -1:
        (i2, j2) = d[i, j][1]
        backtrackx.append(x[i2 + 1:i + 1])
        backtracky.append(y[j2 + 1:j + 1])
        (i, j) = (i2, j2)
    x = y = i = 0
    colorize = {0: lambda x: x, -1: conf.color_theme.left, 1: conf.color_theme.right}
    dox = 1
    doy = 0
    l = len(backtrackx)
    while i < l:
        separate = 0
        linex = backtrackx[i:i + 16]
        liney = backtracky[i:i + 16]
        xx = sum((len(k) for k in linex))
        yy = sum((len(k) for k in liney))
        if dox and (not xx):
            dox = 0
            doy = 1
        if dox and linex == liney:
            doy = 1
        if dox:
            xd = y
            j = 0
            while not linex[j]:
                j += 1
                xd -= 1
            print(colorize[doy - dox]('%04x' % xd), end=' ')
            x += xx
            line = linex
        else:
            print('    ', end=' ')
        if doy:
            yd = y
            j = 0
            while not liney[j]:
                j += 1
                yd -= 1
            print(colorize[doy - dox]('%04x' % yd), end=' ')
            y += yy
            line = liney
        else:
            print('    ', end=' ')
        print(' ', end=' ')
        cl = ''
        for j in range(16):
            if i + j < l:
                if line[j]:
                    col = colorize[(linex[j] != liney[j]) * (doy - dox)]
                    print(col('%02X' % line[j][0]), end=' ')
                    if linex[j] == liney[j]:
                        cl += sane_color(line[j])
                    else:
                        cl += col(sane(line[j]))
                else:
                    print('  ', end=' ')
                    cl += ' '
            else:
                print('  ', end=' ')
            if j == 7:
                print('', end=' ')
        print(' ', cl)
        if doy or not yy:
            doy = 0
            dox = 1
            i += 16
        elif yy:
            dox = 0
            doy = 1
        else:
            i += 16",%04x' % xd,f'{xd:04x}',find_wrong,,1,3,1,
kamene,https://github.com/phaethon/kamene/tree/master/kamene/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/utils.py,,"def hexdiff(x, y):
    """"""Show differences between 2 binary strings""""""
    x = any2b(x)[::-1]
    y = any2b(y)[::-1]
    SUBST = 1
    INSERT = 1
    d = {}
    d[-1, -1] = (0, (-1, -1))
    for j in range(len(y)):
        d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))
    for i in range(len(x)):
        d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))
    for j in range(len(y)):
        for i in range(len(x)):
            d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))
    backtrackx = []
    backtracky = []
    i = len(x) - 1
    j = len(y) - 1
    while not i == j == -1:
        (i2, j2) = d[i, j][1]
        backtrackx.append(x[i2 + 1:i + 1])
        backtracky.append(y[j2 + 1:j + 1])
        (i, j) = (i2, j2)
    x = y = i = 0
    colorize = {0: lambda x: x, -1: conf.color_theme.left, 1: conf.color_theme.right}
    dox = 1
    doy = 0
    l = len(backtrackx)
    while i < l:
        separate = 0
        linex = backtrackx[i:i + 16]
        liney = backtracky[i:i + 16]
        xx = sum((len(k) for k in linex))
        yy = sum((len(k) for k in liney))
        if dox and (not xx):
            dox = 0
            doy = 1
        if dox and linex == liney:
            doy = 1
        if dox:
            xd = y
            j = 0
            while not linex[j]:
                j += 1
                xd -= 1
            print(colorize[doy - dox]('%04x' % xd), end=' ')
            x += xx
            line = linex
        else:
            print('    ', end=' ')
        if doy:
            yd = y
            j = 0
            while not liney[j]:
                j += 1
                yd -= 1
            print(colorize[doy - dox]('%04x' % yd), end=' ')
            y += yy
            line = liney
        else:
            print('    ', end=' ')
        print(' ', end=' ')
        cl = ''
        for j in range(16):
            if i + j < l:
                if line[j]:
                    col = colorize[(linex[j] != liney[j]) * (doy - dox)]
                    print(col('%02X' % line[j][0]), end=' ')
                    if linex[j] == liney[j]:
                        cl += sane_color(line[j])
                    else:
                        cl += col(sane(line[j]))
                else:
                    print('  ', end=' ')
                    cl += ' '
            else:
                print('  ', end=' ')
            if j == 7:
                print('', end=' ')
        print(' ', cl)
        if doy or not yy:
            doy = 0
            dox = 1
            i += 16
        elif yy:
            dox = 0
            doy = 1
        else:
            i += 16",%04x' % yd,f'{yd:04x}',find_wrong,,1,3,1,
kamene,https://github.com/phaethon/kamene/tree/master/kamene/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/utils.py,,"def hexdiff(x, y):
    """"""Show differences between 2 binary strings""""""
    x = any2b(x)[::-1]
    y = any2b(y)[::-1]
    SUBST = 1
    INSERT = 1
    d = {}
    d[-1, -1] = (0, (-1, -1))
    for j in range(len(y)):
        d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))
    for i in range(len(x)):
        d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))
    for j in range(len(y)):
        for i in range(len(x)):
            d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))
    backtrackx = []
    backtracky = []
    i = len(x) - 1
    j = len(y) - 1
    while not i == j == -1:
        (i2, j2) = d[i, j][1]
        backtrackx.append(x[i2 + 1:i + 1])
        backtracky.append(y[j2 + 1:j + 1])
        (i, j) = (i2, j2)
    x = y = i = 0
    colorize = {0: lambda x: x, -1: conf.color_theme.left, 1: conf.color_theme.right}
    dox = 1
    doy = 0
    l = len(backtrackx)
    while i < l:
        separate = 0
        linex = backtrackx[i:i + 16]
        liney = backtracky[i:i + 16]
        xx = sum((len(k) for k in linex))
        yy = sum((len(k) for k in liney))
        if dox and (not xx):
            dox = 0
            doy = 1
        if dox and linex == liney:
            doy = 1
        if dox:
            xd = y
            j = 0
            while not linex[j]:
                j += 1
                xd -= 1
            print(colorize[doy - dox]('%04x' % xd), end=' ')
            x += xx
            line = linex
        else:
            print('    ', end=' ')
        if doy:
            yd = y
            j = 0
            while not liney[j]:
                j += 1
                yd -= 1
            print(colorize[doy - dox]('%04x' % yd), end=' ')
            y += yy
            line = liney
        else:
            print('    ', end=' ')
        print(' ', end=' ')
        cl = ''
        for j in range(16):
            if i + j < l:
                if line[j]:
                    col = colorize[(linex[j] != liney[j]) * (doy - dox)]
                    print(col('%02X' % line[j][0]), end=' ')
                    if linex[j] == liney[j]:
                        cl += sane_color(line[j])
                    else:
                        cl += col(sane(line[j]))
                else:
                    print('  ', end=' ')
                    cl += ' '
            else:
                print('  ', end=' ')
            if j == 7:
                print('', end=' ')
        print(' ', cl)
        if doy or not yy:
            doy = 0
            dox = 1
            i += 16
        elif yy:
            dox = 0
            doy = 1
        else:
            i += 16",%02X' % line[j][0],f'{line[j][0]:02X}',find_wrong,,1,3,1,
youtube-dl,https://github.com/lrvick/youtube-dl/tree/master/youtube_dl/extractor/rtve.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/youtube-dl/youtube_dl/extractor/rtve.py,RTVEALaCartaIE,"def _extract_png_formats(self, video_id):
    png = self._download_webpage('http://www.rtve.es/ztnr/movil/thumbnail/%s/videos/%s.png' % (self._manager, video_id), video_id, 'Downloading url information', query={'q': 'v2'})
    q = qualities(['Media', 'Alta', 'HQ', 'HD_READY', 'HD_FULL'])
    formats = []
    for (quality, video_url) in self._decrypt_url(png):
        ext = determine_ext(video_url)
        if ext == 'm3u8':
            formats.extend(self._extract_m3u8_formats(video_url, video_id, 'mp4', 'm3u8_native', m3u8_id='hls', fatal=False))
        elif ext == 'mpd':
            formats.extend(self._extract_mpd_formats(video_url, video_id, 'dash', fatal=False))
        else:
            formats.append({'format_id': quality, 'quality': q(quality), 'url': video_url})
    self._sort_formats(formats)
    return formats","formats.append({'format_id': quality, 'quality': q(quality), 'url': video_url})","formats.append({'format_id': quality, 'quality': q(quality), 'url': f'{video_url}'})",find_wrong,2,,,,
frankmocap,https://github.com/facebookresearch/frankmocap/tree/master/mocap_utils/compare_results.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frankmocap/mocap_utils/compare_results.py,,"def main():
    dir_list = ['samples/output/body/third_view_thresh_0.3_distance_2.0', 'samples/output/body/third_view_thresh_0.5_distance_1.5', 'samples/output/body/third_view_thresh_0.7_distance_1.0']
    dir1 = dir_list[0]
    keywords = ['cj_dance', 'body_capture']
    res_dir = 'samples/output/body/third_view_compare'
    res_dir = osp.join(res_dir, '_&&_'.join(['_'.join(item.split('/')[-1:]) for item in dir_list]))
    for subdir in os.listdir(dir1):
        if osp.isdir(osp.join(dir1, subdir)):
            if check_keywords(subdir, keywords):
                dir_path1 = osp.join(dir1, subdir)
                for img_name in ry_utils.get_all_files(dir_path1, ('.jpg', '.png'), 'name_only'):
                    img_list = list()
                    for dir in dir_list:
                        dir_path = dir_path1.replace(dir1, dir)
                        img_path = osp.join(dir_path, img_name)
                        img = cv2.imread(img_path)
                        img_list.append(img)
                        if img_path.find(dir1) >= 0:
                            res_img_path = img_path.replace(dir1, res_dir)
                    if any([img is None for img in img_list]):
                        continue
                    res_img = np.concatenate(img_list, axis=0)
                    (h, w) = res_img.shape[:2]
                    res_img = cv2.resize(res_img, (int(w * 0.7), int(h * 0.7)))
                    res_img_path = res_img_path.replace('.png', '.jpg')
                    ry_utils.make_subdir(res_img_path)
                    cv2.imwrite(res_img_path, res_img)
                    print(res_img_path)","res_dir = 'samples/output/body/third_view_compare'
res_dir = osp.join(res_dir, '_&&_'.join(['_'.join(item.split('/')[-1:]) for item in dir_list]))",res_dir = f"samples/output/body/third_view_compare/_&&_{'_'.join((item.split('/')[-1] for item in dir_list))}",find_wrong,2,,,,
django,https://github.com/django/django/tree/master/tests/gis_tests/test_geoip2.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django/tests/gis_tests/test_geoip2.py,GeoIPTest,"def test_repr(self):
    path = settings.GEOIP_PATH
    g = GeoIP2(path=path)
    meta = g._reader.metadata()
    version = '%s.%s' % (meta.binary_format_major_version, meta.binary_format_minor_version)
    country_path = g._country_file
    city_path = g._city_file
    expected = '<GeoIP2 [v%(version)s] _country_file=""%(country)s"", _city_file=""%(city)s"">' % {'version': version, 'country': country_path, 'city': city_path}
    self.assertEqual(repr(g), expected)","version = '%s.%s' % (meta.binary_format_major_version, meta.binary_format_minor_version)
expected = '<GeoIP2 [v%(version)s] _country_file=""%(country)s"", _city_file=""%(city)s"">' % {'version': version, 'country': country_path, 'city': city_path}","version = f'{meta.binary_format_major_version}.{meta.binary_format_minor_version}'
expected = f'<GeoIP2 [v{version}] _country_file=""{country_path}"", _city_file=""{city_path}"">'",find_wrong,,,,,
pyquil,https://github.com/rigetti/pyquil/tree/master/pyquil/paulis.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/paulis.py,PauliTerm,"def __repr__(self) -> str:
    term_strs = []
    for index in self._ops.keys():
        term_strs.append('%s%s' % (self[index], index))
    if len(term_strs) == 0:
        term_strs.append('I')
    out = '%s*%s' % (self.coefficient, '*'.join(term_strs))
    return out","term_strs.append('%s%s' % (self[index], index))",term_strs.append(f'{self[index]}{index}'),find_wrong,,,,,
pyquil,https://github.com/rigetti/pyquil/tree/master/pyquil/paulis.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyquil/pyquil/paulis.py,PauliTerm,"def __repr__(self) -> str:
    term_strs = []
    for index in self._ops.keys():
        term_strs.append('%s%s' % (self[index], index))
    if len(term_strs) == 0:
        term_strs.append('I')
    out = '%s*%s' % (self.coefficient, '*'.join(term_strs))
    return out","out = '%s*%s' % (self.coefficient, '*'.join(term_strs))",out = f"{self.coefficient}*{'*'.join(term_strs)}",find_wrong,,,,,
q,https://github.com/zestyping/q/tree/master/test/test_suite.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/q/test/test_suite.py,ParsingModeTests,"def test_strict_mode_too_small_specific_column_count(self):
    tmpfile = self.create_file_with_data(sample_data_no_header)
    cmd = Q_EXECUTABLE + ' -d , -m strict -c 2 ""select count(*) from %s""' % tmpfile.name
    (retcode, o, e) = run_command(cmd)
    self.assertNotEqual(retcode, 0)
    self.assertEqual(len(o), 0)
    self.assertEqual(len(e), 1)
    self.assertEqual(e[0], six.b('Strict mode. Column count is expected to be 2 but is 3'))
    self.cleanup(tmpfile)",select count(*) from %s"' % tmpfile.name,f'select count(*) from {tmpfile.name}"',find_wrong,,,,,
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/gluon/gluoncv2/models/resdropresnet_cifar.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/gluon/gluoncv2/models/resdropresnet_cifar.py,,"def _test():
    import numpy as np
    import mxnet as mx
    pretrained = False
    models = [(resdropresnet20_cifar10, 10), (resdropresnet20_cifar100, 100), (resdropresnet20_svhn, 10)]
    for (model, classes) in models:
        net = model(pretrained=pretrained)
        ctx = mx.cpu()
        if not pretrained:
            net.initialize(ctx=ctx)
        net_params = net.collect_params()
        weight_count = 0
        for param in net_params.values():
            if param.shape is None or not param._differentiable:
                continue
            weight_count += np.prod(param.shape)
        print('m={}, {}'.format(model.__name__, weight_count))
        assert model != resdropresnet20_cifar10 or weight_count == 272474
        assert model != resdropresnet20_cifar100 or weight_count == 278324
        assert model != resdropresnet20_svhn or weight_count == 272474
        x = mx.nd.zeros((14, 3, 32, 32), ctx=ctx)
        y = net(x)
        assert y.shape == (14, classes)","'m={}, {}'.format(model.__name__, weight_count)","f'm={model.__name__}, {weight_count}'",find_wrong,,1,3,,
pefile,https://github.com/erocarrera/pefile/tree/master//pefile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pefile//pefile.py,PE,"def parse_sections(self, offset):
    """"""Fetch the PE file sections.

        The sections will be readily available in the ""sections"" attribute.
        Its attributes will contain all the section information plus ""data""
        a buffer containing the section's data.

        The ""Characteristics"" member will be processed and attributes
        representing the section characteristics (with the 'IMAGE_SCN_'
        string trimmed from the constant's names) will be added to the
        section instance.

        Refer to the SectionStructure class for additional info.
        """"""
    self.sections = []
    MAX_SIMULTANEOUS_ERRORS = 3
    for i in range(self.FILE_HEADER.NumberOfSections):
        if i >= MAX_SECTIONS:
            self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
            break
        simultaneous_errors = 0
        section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
        if not section:
            break
        section_offset = offset + section.sizeof() * i
        section.set_file_offset(section_offset)
        section_data = self.__data__[section_offset:section_offset + section.sizeof()]
        if count_zeroes(section_data) == section.sizeof():
            self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
            break
        if not section_data:
            self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
            break
        section.__unpack__(section_data)
        self.__structures__.append(section)
        if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
        if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
        if section.Misc_VirtualSize > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
        if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
        if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
        if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
            self.__warnings.append('Too many warnings parsing section. Aborting.')
            break
        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
        set_flags(section, section.Characteristics, section_flags)
        if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
            if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
                pass
            else:
                self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
        self.sections.append(section)
    self.sections.sort(key=lambda a: a.VirtualAddress)
    for (idx, section) in enumerate(self.sections):
        if idx == len(self.sections) - 1:
            section.next_section_virtual_address = None
        else:
            section.next_section_virtual_address = self.sections[idx + 1].VirtualAddress
    if self.FILE_HEADER.NumberOfSections > 0 and self.sections:
        return offset + self.sections[0].sizeof() * self.FILE_HEADER.NumberOfSections
    else:
        return offset","'Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS)",f'Too many sections {self.FILE_HEADER.NumberOfSections} (>= {MAX_SECTIONS})',find_wrong,,1,3,,
pefile,https://github.com/erocarrera/pefile/tree/master//pefile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pefile//pefile.py,PE,"def parse_sections(self, offset):
    """"""Fetch the PE file sections.

        The sections will be readily available in the ""sections"" attribute.
        Its attributes will contain all the section information plus ""data""
        a buffer containing the section's data.

        The ""Characteristics"" member will be processed and attributes
        representing the section characteristics (with the 'IMAGE_SCN_'
        string trimmed from the constant's names) will be added to the
        section instance.

        Refer to the SectionStructure class for additional info.
        """"""
    self.sections = []
    MAX_SIMULTANEOUS_ERRORS = 3
    for i in range(self.FILE_HEADER.NumberOfSections):
        if i >= MAX_SECTIONS:
            self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
            break
        simultaneous_errors = 0
        section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
        if not section:
            break
        section_offset = offset + section.sizeof() * i
        section.set_file_offset(section_offset)
        section_data = self.__data__[section_offset:section_offset + section.sizeof()]
        if count_zeroes(section_data) == section.sizeof():
            self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
            break
        if not section_data:
            self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
            break
        section.__unpack__(section_data)
        self.__structures__.append(section)
        if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
        if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
        if section.Misc_VirtualSize > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
        if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
        if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
        if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
            self.__warnings.append('Too many warnings parsing section. Aborting.')
            break
        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
        set_flags(section, section.Characteristics, section_flags)
        if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
            if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
                pass
            else:
                self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
        self.sections.append(section)
    self.sections.sort(key=lambda a: a.VirtualAddress)
    for (idx, section) in enumerate(self.sections):
        if idx == len(self.sections) - 1:
            section.next_section_virtual_address = None
        else:
            section.next_section_virtual_address = self.sections[idx + 1].VirtualAddress
    if self.FILE_HEADER.NumberOfSections > 0 and self.sections:
        return offset + self.sections[0].sizeof() * self.FILE_HEADER.NumberOfSections
    else:
        return offset","""""""Invalid section {i}. Contents are null-bytes.""""""",f'Invalid section {i}. Contents are null-bytes.',find_wrong,2,,,,
pefile,https://github.com/erocarrera/pefile/tree/master//pefile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pefile//pefile.py,PE,"def parse_sections(self, offset):
    """"""Fetch the PE file sections.

        The sections will be readily available in the ""sections"" attribute.
        Its attributes will contain all the section information plus ""data""
        a buffer containing the section's data.

        The ""Characteristics"" member will be processed and attributes
        representing the section characteristics (with the 'IMAGE_SCN_'
        string trimmed from the constant's names) will be added to the
        section instance.

        Refer to the SectionStructure class for additional info.
        """"""
    self.sections = []
    MAX_SIMULTANEOUS_ERRORS = 3
    for i in range(self.FILE_HEADER.NumberOfSections):
        if i >= MAX_SECTIONS:
            self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
            break
        simultaneous_errors = 0
        section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
        if not section:
            break
        section_offset = offset + section.sizeof() * i
        section.set_file_offset(section_offset)
        section_data = self.__data__[section_offset:section_offset + section.sizeof()]
        if count_zeroes(section_data) == section.sizeof():
            self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
            break
        if not section_data:
            self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
            break
        section.__unpack__(section_data)
        self.__structures__.append(section)
        if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
        if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
        if section.Misc_VirtualSize > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
        if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
        if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
        if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
            self.__warnings.append('Too many warnings parsing section. Aborting.')
            break
        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
        set_flags(section, section.Characteristics, section_flags)
        if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
            if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
                pass
            else:
                self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
        self.sections.append(section)
    self.sections.sort(key=lambda a: a.VirtualAddress)
    for (idx, section) in enumerate(self.sections):
        if idx == len(self.sections) - 1:
            section.next_section_virtual_address = None
        else:
            section.next_section_virtual_address = self.sections[idx + 1].VirtualAddress
    if self.FILE_HEADER.NumberOfSections > 0 and self.sections:
        return offset + self.sections[0].sizeof() * self.FILE_HEADER.NumberOfSections
    else:
        return offset","""""""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?).""""""",f"Invalid section {i}. No data in the file (is this corkami's virtsectblXP?).",find_wrong,2,,,,
pefile,https://github.com/erocarrera/pefile/tree/master//pefile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pefile//pefile.py,PE,"def parse_sections(self, offset):
    """"""Fetch the PE file sections.

        The sections will be readily available in the ""sections"" attribute.
        Its attributes will contain all the section information plus ""data""
        a buffer containing the section's data.

        The ""Characteristics"" member will be processed and attributes
        representing the section characteristics (with the 'IMAGE_SCN_'
        string trimmed from the constant's names) will be added to the
        section instance.

        Refer to the SectionStructure class for additional info.
        """"""
    self.sections = []
    MAX_SIMULTANEOUS_ERRORS = 3
    for i in range(self.FILE_HEADER.NumberOfSections):
        if i >= MAX_SECTIONS:
            self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
            break
        simultaneous_errors = 0
        section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
        if not section:
            break
        section_offset = offset + section.sizeof() * i
        section.set_file_offset(section_offset)
        section_data = self.__data__[section_offset:section_offset + section.sizeof()]
        if count_zeroes(section_data) == section.sizeof():
            self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
            break
        if not section_data:
            self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
            break
        section.__unpack__(section_data)
        self.__structures__.append(section)
        if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
        if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
        if section.Misc_VirtualSize > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
        if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
        if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
        if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
            self.__warnings.append('Too many warnings parsing section. Aborting.')
            break
        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
        set_flags(section, section.Characteristics, section_flags)
        if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
            if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
                pass
            else:
                self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
        self.sections.append(section)
    self.sections.sort(key=lambda a: a.VirtualAddress)
    for (idx, section) in enumerate(self.sections):
        if idx == len(self.sections) - 1:
            section.next_section_virtual_address = None
        else:
            section.next_section_virtual_address = self.sections[idx + 1].VirtualAddress
    if self.FILE_HEADER.NumberOfSections > 0 and self.sections:
        return offset + self.sections[0].sizeof() * self.FILE_HEADER.NumberOfSections
    else:
        return offset","""""""Error parsing section {i}. SizeOfRawData is larger than file.""""""",f'Error parsing section {i}. SizeOfRawData is larger than file.',find_wrong,2,,,,
pefile,https://github.com/erocarrera/pefile/tree/master//pefile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pefile//pefile.py,PE,"def parse_sections(self, offset):
    """"""Fetch the PE file sections.

        The sections will be readily available in the ""sections"" attribute.
        Its attributes will contain all the section information plus ""data""
        a buffer containing the section's data.

        The ""Characteristics"" member will be processed and attributes
        representing the section characteristics (with the 'IMAGE_SCN_'
        string trimmed from the constant's names) will be added to the
        section instance.

        Refer to the SectionStructure class for additional info.
        """"""
    self.sections = []
    MAX_SIMULTANEOUS_ERRORS = 3
    for i in range(self.FILE_HEADER.NumberOfSections):
        if i >= MAX_SECTIONS:
            self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
            break
        simultaneous_errors = 0
        section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
        if not section:
            break
        section_offset = offset + section.sizeof() * i
        section.set_file_offset(section_offset)
        section_data = self.__data__[section_offset:section_offset + section.sizeof()]
        if count_zeroes(section_data) == section.sizeof():
            self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
            break
        if not section_data:
            self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
            break
        section.__unpack__(section_data)
        self.__structures__.append(section)
        if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
        if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
        if section.Misc_VirtualSize > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
        if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
        if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
        if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
            self.__warnings.append('Too many warnings parsing section. Aborting.')
            break
        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
        set_flags(section, section.Characteristics, section_flags)
        if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
            if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
                pass
            else:
                self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
        self.sections.append(section)
    self.sections.sort(key=lambda a: a.VirtualAddress)
    for (idx, section) in enumerate(self.sections):
        if idx == len(self.sections) - 1:
            section.next_section_virtual_address = None
        else:
            section.next_section_virtual_address = self.sections[idx + 1].VirtualAddress
    if self.FILE_HEADER.NumberOfSections > 0 and self.sections:
        return offset + self.sections[0].sizeof() * self.FILE_HEADER.NumberOfSections
    else:
        return offset","""""""Error parsing section {i}. PointerToRawData points beyond the end of the file.""""""",f'Error parsing section {i}. PointerToRawData points beyond the end of the file.',find_wrong,2,,,,
pefile,https://github.com/erocarrera/pefile/tree/master//pefile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pefile//pefile.py,PE,"def parse_sections(self, offset):
    """"""Fetch the PE file sections.

        The sections will be readily available in the ""sections"" attribute.
        Its attributes will contain all the section information plus ""data""
        a buffer containing the section's data.

        The ""Characteristics"" member will be processed and attributes
        representing the section characteristics (with the 'IMAGE_SCN_'
        string trimmed from the constant's names) will be added to the
        section instance.

        Refer to the SectionStructure class for additional info.
        """"""
    self.sections = []
    MAX_SIMULTANEOUS_ERRORS = 3
    for i in range(self.FILE_HEADER.NumberOfSections):
        if i >= MAX_SECTIONS:
            self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
            break
        simultaneous_errors = 0
        section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
        if not section:
            break
        section_offset = offset + section.sizeof() * i
        section.set_file_offset(section_offset)
        section_data = self.__data__[section_offset:section_offset + section.sizeof()]
        if count_zeroes(section_data) == section.sizeof():
            self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
            break
        if not section_data:
            self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
            break
        section.__unpack__(section_data)
        self.__structures__.append(section)
        if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
        if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
        if section.Misc_VirtualSize > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
        if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
        if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
        if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
            self.__warnings.append('Too many warnings parsing section. Aborting.')
            break
        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
        set_flags(section, section.Characteristics, section_flags)
        if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
            if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
                pass
            else:
                self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
        self.sections.append(section)
    self.sections.sort(key=lambda a: a.VirtualAddress)
    for (idx, section) in enumerate(self.sections):
        if idx == len(self.sections) - 1:
            section.next_section_virtual_address = None
        else:
            section.next_section_virtual_address = self.sections[idx + 1].VirtualAddress
    if self.FILE_HEADER.NumberOfSections > 0 and self.sections:
        return offset + self.sections[0].sizeof() * self.FILE_HEADER.NumberOfSections
    else:
        return offset","""""""Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.""""""",f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.',find_wrong,2,,,,
pefile,https://github.com/erocarrera/pefile/tree/master//pefile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pefile//pefile.py,PE,"def parse_sections(self, offset):
    """"""Fetch the PE file sections.

        The sections will be readily available in the ""sections"" attribute.
        Its attributes will contain all the section information plus ""data""
        a buffer containing the section's data.

        The ""Characteristics"" member will be processed and attributes
        representing the section characteristics (with the 'IMAGE_SCN_'
        string trimmed from the constant's names) will be added to the
        section instance.

        Refer to the SectionStructure class for additional info.
        """"""
    self.sections = []
    MAX_SIMULTANEOUS_ERRORS = 3
    for i in range(self.FILE_HEADER.NumberOfSections):
        if i >= MAX_SECTIONS:
            self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
            break
        simultaneous_errors = 0
        section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
        if not section:
            break
        section_offset = offset + section.sizeof() * i
        section.set_file_offset(section_offset)
        section_data = self.__data__[section_offset:section_offset + section.sizeof()]
        if count_zeroes(section_data) == section.sizeof():
            self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
            break
        if not section_data:
            self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
            break
        section.__unpack__(section_data)
        self.__structures__.append(section)
        if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
        if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
        if section.Misc_VirtualSize > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
        if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
        if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
        if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
            self.__warnings.append('Too many warnings parsing section. Aborting.')
            break
        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
        set_flags(section, section.Characteristics, section_flags)
        if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
            if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
                pass
            else:
                self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
        self.sections.append(section)
    self.sections.sort(key=lambda a: a.VirtualAddress)
    for (idx, section) in enumerate(self.sections):
        if idx == len(self.sections) - 1:
            section.next_section_virtual_address = None
        else:
            section.next_section_virtual_address = self.sections[idx + 1].VirtualAddress
    if self.FILE_HEADER.NumberOfSections > 0 and self.sections:
        return offset + self.sections[0].sizeof() * self.FILE_HEADER.NumberOfSections
    else:
        return offset","""""""Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.""""""",f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.',find_wrong,2,,,,
pefile,https://github.com/erocarrera/pefile/tree/master//pefile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pefile//pefile.py,PE,"def parse_sections(self, offset):
    """"""Fetch the PE file sections.

        The sections will be readily available in the ""sections"" attribute.
        Its attributes will contain all the section information plus ""data""
        a buffer containing the section's data.

        The ""Characteristics"" member will be processed and attributes
        representing the section characteristics (with the 'IMAGE_SCN_'
        string trimmed from the constant's names) will be added to the
        section instance.

        Refer to the SectionStructure class for additional info.
        """"""
    self.sections = []
    MAX_SIMULTANEOUS_ERRORS = 3
    for i in range(self.FILE_HEADER.NumberOfSections):
        if i >= MAX_SECTIONS:
            self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
            break
        simultaneous_errors = 0
        section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
        if not section:
            break
        section_offset = offset + section.sizeof() * i
        section.set_file_offset(section_offset)
        section_data = self.__data__[section_offset:section_offset + section.sizeof()]
        if count_zeroes(section_data) == section.sizeof():
            self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
            break
        if not section_data:
            self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
            break
        section.__unpack__(section_data)
        self.__structures__.append(section)
        if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
        if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
        if section.Misc_VirtualSize > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
        if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
        if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
        if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
            self.__warnings.append('Too many warnings parsing section. Aborting.')
            break
        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
        set_flags(section, section.Characteristics, section_flags)
        if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
            if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
                pass
            else:
                self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
        self.sections.append(section)
    self.sections.sort(key=lambda a: a.VirtualAddress)
    for (idx, section) in enumerate(self.sections):
        if idx == len(self.sections) - 1:
            section.next_section_virtual_address = None
        else:
            section.next_section_virtual_address = self.sections[idx + 1].VirtualAddress
    if self.FILE_HEADER.NumberOfSections > 0 and self.sections:
        return offset + self.sections[0].sizeof() * self.FILE_HEADER.NumberOfSections
    else:
        return offset","""""""Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.""""""","f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.'",find_wrong,2,,,,
pefile,https://github.com/erocarrera/pefile/tree/master//pefile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pefile//pefile.py,PE,"def parse_sections(self, offset):
    """"""Fetch the PE file sections.

        The sections will be readily available in the ""sections"" attribute.
        Its attributes will contain all the section information plus ""data""
        a buffer containing the section's data.

        The ""Characteristics"" member will be processed and attributes
        representing the section characteristics (with the 'IMAGE_SCN_'
        string trimmed from the constant's names) will be added to the
        section instance.

        Refer to the SectionStructure class for additional info.
        """"""
    self.sections = []
    MAX_SIMULTANEOUS_ERRORS = 3
    for i in range(self.FILE_HEADER.NumberOfSections):
        if i >= MAX_SECTIONS:
            self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
            break
        simultaneous_errors = 0
        section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
        if not section:
            break
        section_offset = offset + section.sizeof() * i
        section.set_file_offset(section_offset)
        section_data = self.__data__[section_offset:section_offset + section.sizeof()]
        if count_zeroes(section_data) == section.sizeof():
            self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
            break
        if not section_data:
            self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
            break
        section.__unpack__(section_data)
        self.__structures__.append(section)
        if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
        if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
        if section.Misc_VirtualSize > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
        if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
        if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
        if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
            self.__warnings.append('Too many warnings parsing section. Aborting.')
            break
        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
        set_flags(section, section.Characteristics, section_flags)
        if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
            if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
                pass
            else:
                self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
        self.sections.append(section)
    self.sections.sort(key=lambda a: a.VirtualAddress)
    for (idx, section) in enumerate(self.sections):
        if idx == len(self.sections) - 1:
            section.next_section_virtual_address = None
        else:
            section.next_section_virtual_address = self.sections[idx + 1].VirtualAddress
    if self.FILE_HEADER.NumberOfSections > 0 and self.sections:
        return offset + self.sections[0].sizeof() * self.FILE_HEADER.NumberOfSections
    else:
        return offset","""""""Too many warnings parsing section. Aborting.""""""","""""""Too many warnings parsing section. Aborting.""""""",find_wrong,2,,,,
pefile,https://github.com/erocarrera/pefile/tree/master//pefile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pefile//pefile.py,PE,"def parse_sections(self, offset):
    """"""Fetch the PE file sections.

        The sections will be readily available in the ""sections"" attribute.
        Its attributes will contain all the section information plus ""data""
        a buffer containing the section's data.

        The ""Characteristics"" member will be processed and attributes
        representing the section characteristics (with the 'IMAGE_SCN_'
        string trimmed from the constant's names) will be added to the
        section instance.

        Refer to the SectionStructure class for additional info.
        """"""
    self.sections = []
    MAX_SIMULTANEOUS_ERRORS = 3
    for i in range(self.FILE_HEADER.NumberOfSections):
        if i >= MAX_SECTIONS:
            self.__warnings.append('Too many sections {0} (>={1})'.format(self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))
            break
        simultaneous_errors = 0
        section = SectionStructure(self.__IMAGE_SECTION_HEADER_format__, pe=self)
        if not section:
            break
        section_offset = offset + section.sizeof() * i
        section.set_file_offset(section_offset)
        section_data = self.__data__[section_offset:section_offset + section.sizeof()]
        if count_zeroes(section_data) == section.sizeof():
            self.__warnings.append(f'Invalid section {i}. Contents are null-bytes.')
            break
        if not section_data:
            self.__warnings.append(f""Invalid section {i}. No data in the file (is this corkami's virtsectblXP?)."")
            break
        section.__unpack__(section_data)
        self.__structures__.append(section)
        if section.SizeOfRawData + section.PointerToRawData > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. SizeOfRawData is larger than file.')
        if self.adjust_FileAlignment(section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment) > len(self.__data__):
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData points beyond the end of the file.')
        if section.Misc_VirtualSize > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualSize is extremely large > 256MiB.')
        if self.adjust_SectionAlignment(section.VirtualAddress, self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment) > 268435456:
            simultaneous_errors += 1
            self.__warnings.append(f'Suspicious value found parsing section {i}. VirtualAddress is beyond 0x10000000.')
        if self.OPTIONAL_HEADER.FileAlignment != 0 and section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment != 0:
            simultaneous_errors += 1
            self.__warnings.append(f'Error parsing section {i}. PointerToRawData should normally be a multiple of FileAlignment, this might imply the file is trying to confuse tools which parse this incorrectly.')
        if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:
            self.__warnings.append('Too many warnings parsing section. Aborting.')
            break
        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')
        set_flags(section, section.Characteristics, section_flags)
        if section.__dict__.get('IMAGE_SCN_MEM_WRITE', False) and section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False):
            if section.Name.rstrip(b'\x00') == b'PAGE' and self.is_driver():
                pass
            else:
                self.__warnings.append(f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.')
        self.sections.append(section)
    self.sections.sort(key=lambda a: a.VirtualAddress)
    for (idx, section) in enumerate(self.sections):
        if idx == len(self.sections) - 1:
            section.next_section_virtual_address = None
        else:
            section.next_section_virtual_address = self.sections[idx + 1].VirtualAddress
    if self.FILE_HEADER.NumberOfSections > 0 and self.sections:
        return offset + self.sections[0].sizeof() * self.FILE_HEADER.NumberOfSections
    else:
        return offset","""""""Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.""""""",f'Suspicious flags set for section {i}. Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. This might indicate a packed executable.',find_wrong,2,,,,
Mycodo,https://github.com/kizniche/Mycodo/tree/master/mycodo/outputs/base_output.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/outputs/base_output.py,AbstractOutput,"def check_triggers(self, output_id, amount=None, output_channel=0):
    """"""
        This function is executed whenever an output is turned on or off
        It is responsible for executing Output Triggers
        """"""
    output_channel_dev = db_retrieve_table_daemon(OutputChannel).filter(and_(OutputChannel.output_id == output_id, OutputChannel.channel == output_channel)).first()
    if output_channel_dev is None:
        self.logger.error('Could not find channel in database')
        return
    trigger_output = db_retrieve_table_daemon(Trigger)
    trigger_output = trigger_output.filter(Trigger.trigger_type == 'trigger_output')
    trigger_output = trigger_output.filter(Trigger.unique_id_1 == output_id)
    trigger_output = trigger_output.filter(Trigger.unique_id_2 == output_channel_dev.unique_id)
    trigger_output = trigger_output.filter(Trigger.is_activated.is_(True))
    if self.is_on(output_channel):
        trigger_output = trigger_output.filter(or_(Trigger.output_state == 'on_duration_none', Trigger.output_state == 'on_duration_any', Trigger.output_state == 'on_duration_none_any', Trigger.output_state == 'on_duration_equal', Trigger.output_state == 'on_duration_greater_than', Trigger.output_state == 'on_duration_equal_greater_than', Trigger.output_state == 'on_duration_less_than', Trigger.output_state == 'on_duration_equal_less_than'))
        on_duration_none = and_(Trigger.output_state == 'on_duration_none', amount == 0.0)
        on_duration_any = and_(Trigger.output_state == 'on_duration_any', bool(amount))
        on_duration_none_any = Trigger.output_state == 'on_duration_none_any'
        on_duration_equal = and_(Trigger.output_state == 'on_duration_equal', Trigger.output_duration == amount)
        on_duration_greater_than = and_(Trigger.output_state == 'on_duration_greater_than', amount > Trigger.output_duration)
        on_duration_equal_greater_than = and_(Trigger.output_state == 'on_duration_equal_greater_than', amount >= Trigger.output_duration)
        on_duration_less_than = and_(Trigger.output_state == 'on_duration_less_than', amount < Trigger.output_duration)
        on_duration_equal_less_than = and_(Trigger.output_state == 'on_duration_equal_less_than', amount <= Trigger.output_duration)
        trigger_output = trigger_output.filter(or_(on_duration_none, on_duration_any, on_duration_none_any, on_duration_equal, on_duration_greater_than, on_duration_equal_greater_than, on_duration_less_than, on_duration_equal_less_than))
    else:
        trigger_output = trigger_output.filter(Trigger.output_state == 'off')
    for each_trigger in trigger_output.all():
        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
        message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} {each_trigger.output_state}""
        self.control.trigger_all_actions(each_trigger.unique_id, message=message)
    trigger_output_pwm = db_retrieve_table_daemon(Trigger)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.trigger_type == 'trigger_output_pwm')
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.unique_id_1 == output_id)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.unique_id_2 == output_channel_dev.unique_id)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.is_activated.is_(True))
    for each_trigger in trigger_output_pwm.all():
        trigger_trigger = False
        duty_cycle = self.output_state(output_channel)
        if duty_cycle == 'off':
            if each_trigger.output_state == 'equal' and each_trigger.output_duty_cycle == 0 or (each_trigger.output_state == 'below' and each_trigger.output_duty_cycle != 0):
                trigger_trigger = True
        elif each_trigger.output_state == 'above' and duty_cycle > each_trigger.output_duty_cycle or (each_trigger.output_state == 'below' and duty_cycle < each_trigger.output_duty_cycle) or (each_trigger.output_state == 'equal' and duty_cycle == each_trigger.output_duty_cycle):
            trigger_trigger = True
        if not trigger_trigger:
            continue
        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
        message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} Duty Cycle {duty_cycle} {each_trigger.output_state} {each_trigger.output_duty_cycle}""
        self.control.trigger_all_actions(each_trigger.unique_id, message=message)","message = '{timestamp}\n[Trigger {} ({})] Output {} CH{} {}'.format(each_trigger.unique_id.split('-')[0], each_trigger.name, output_id, output_channel, each_trigger.output_state)",message = f"{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} {each_trigger.output_state}",find_wrong,,1,3,,
Mycodo,https://github.com/kizniche/Mycodo/tree/master/mycodo/outputs/base_output.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/outputs/base_output.py,AbstractOutput,"def check_triggers(self, output_id, amount=None, output_channel=0):
    """"""
        This function is executed whenever an output is turned on or off
        It is responsible for executing Output Triggers
        """"""
    output_channel_dev = db_retrieve_table_daemon(OutputChannel).filter(and_(OutputChannel.output_id == output_id, OutputChannel.channel == output_channel)).first()
    if output_channel_dev is None:
        self.logger.error('Could not find channel in database')
        return
    trigger_output = db_retrieve_table_daemon(Trigger)
    trigger_output = trigger_output.filter(Trigger.trigger_type == 'trigger_output')
    trigger_output = trigger_output.filter(Trigger.unique_id_1 == output_id)
    trigger_output = trigger_output.filter(Trigger.unique_id_2 == output_channel_dev.unique_id)
    trigger_output = trigger_output.filter(Trigger.is_activated.is_(True))
    if self.is_on(output_channel):
        trigger_output = trigger_output.filter(or_(Trigger.output_state == 'on_duration_none', Trigger.output_state == 'on_duration_any', Trigger.output_state == 'on_duration_none_any', Trigger.output_state == 'on_duration_equal', Trigger.output_state == 'on_duration_greater_than', Trigger.output_state == 'on_duration_equal_greater_than', Trigger.output_state == 'on_duration_less_than', Trigger.output_state == 'on_duration_equal_less_than'))
        on_duration_none = and_(Trigger.output_state == 'on_duration_none', amount == 0.0)
        on_duration_any = and_(Trigger.output_state == 'on_duration_any', bool(amount))
        on_duration_none_any = Trigger.output_state == 'on_duration_none_any'
        on_duration_equal = and_(Trigger.output_state == 'on_duration_equal', Trigger.output_duration == amount)
        on_duration_greater_than = and_(Trigger.output_state == 'on_duration_greater_than', amount > Trigger.output_duration)
        on_duration_equal_greater_than = and_(Trigger.output_state == 'on_duration_equal_greater_than', amount >= Trigger.output_duration)
        on_duration_less_than = and_(Trigger.output_state == 'on_duration_less_than', amount < Trigger.output_duration)
        on_duration_equal_less_than = and_(Trigger.output_state == 'on_duration_equal_less_than', amount <= Trigger.output_duration)
        trigger_output = trigger_output.filter(or_(on_duration_none, on_duration_any, on_duration_none_any, on_duration_equal, on_duration_greater_than, on_duration_equal_greater_than, on_duration_less_than, on_duration_equal_less_than))
    else:
        trigger_output = trigger_output.filter(Trigger.output_state == 'off')
    for each_trigger in trigger_output.all():
        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
        message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} {each_trigger.output_state}""
        self.control.trigger_all_actions(each_trigger.unique_id, message=message)
    trigger_output_pwm = db_retrieve_table_daemon(Trigger)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.trigger_type == 'trigger_output_pwm')
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.unique_id_1 == output_id)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.unique_id_2 == output_channel_dev.unique_id)
    trigger_output_pwm = trigger_output_pwm.filter(Trigger.is_activated.is_(True))
    for each_trigger in trigger_output_pwm.all():
        trigger_trigger = False
        duty_cycle = self.output_state(output_channel)
        if duty_cycle == 'off':
            if each_trigger.output_state == 'equal' and each_trigger.output_duty_cycle == 0 or (each_trigger.output_state == 'below' and each_trigger.output_duty_cycle != 0):
                trigger_trigger = True
        elif each_trigger.output_state == 'above' and duty_cycle > each_trigger.output_duty_cycle or (each_trigger.output_state == 'below' and duty_cycle < each_trigger.output_duty_cycle) or (each_trigger.output_state == 'equal' and duty_cycle == each_trigger.output_duty_cycle):
            trigger_trigger = True
        if not trigger_trigger:
            continue
        timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')
        message = f""{timestamp}\n[Trigger {each_trigger.unique_id.split('-')[0]} ({each_trigger.name})] Output {output_id} CH{output_channel} Duty Cycle {duty_cycle} {each_trigger.output_state} {each_trigger.output_duty_cycle}""
        self.control.trigger_all_actions(each_trigger.unique_id, message=message)",timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'),timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),find_wrong,2,,,,
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/chainer_/chainercv2/models/resnet_cub.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/chainer_/chainercv2/models/resnet_cub.py,,"def _test():
    import numpy as np
    import chainer
    chainer.global_config.train = False
    pretrained = False
    models = [resnet10_cub, resnet12_cub, resnet14_cub, resnetbc14b_cub, resnet16_cub, resnet18_cub, resnet26_cub, resnetbc26b_cub, resnet34_cub, resnetbc38b_cub, resnet50_cub, resnet50b_cub, resnet101_cub, resnet101b_cub, resnet152_cub, resnet152b_cub, resnet200_cub, resnet200b_cub]
    for model in models:
        net = model(pretrained=pretrained)
        weight_count = net.count_params()
        print('m={}, {}'.format(model.__name__, weight_count))
        assert model != resnet10_cub or weight_count == 5008392
        assert model != resnet12_cub or weight_count == 5082376
        assert model != resnet14_cub or weight_count == 5377800
        assert model != resnetbc14b_cub or weight_count == 8425736
        assert model != resnet16_cub or weight_count == 6558472
        assert model != resnet18_cub or weight_count == 11279112
        assert model != resnet26_cub or weight_count == 17549832
        assert model != resnetbc26b_cub or weight_count == 14355976
        assert model != resnet34_cub or weight_count == 21387272
        assert model != resnetbc38b_cub or weight_count == 20286216
        assert model != resnet50_cub or weight_count == 23917832
        assert model != resnet50b_cub or weight_count == 23917832
        assert model != resnet101_cub or weight_count == 42909960
        assert model != resnet101b_cub or weight_count == 42909960
        assert model != resnet152_cub or weight_count == 58553608
        assert model != resnet152b_cub or weight_count == 58553608
        assert model != resnet200_cub or weight_count == 63034632
        assert model != resnet200b_cub or weight_count == 63034632
        x = np.zeros((1, 3, 224, 224), np.float32)
        y = net(x)
        assert y.shape == (1, 200)","'m={}, {}'.format(model.__name__, weight_count)","f'm={model.__name__}, {weight_count}'",find_wrong,,1,3,,
dupeguru,https://github.com/arsenetar/dupeguru/tree/master/hscommon/pygettext.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dupeguru/hscommon/pygettext.py,,"def main(source_files, outpath, keywords=None):
    global default_keywords

    class Options:
        GNU = 1
        SOLARIS = 2
        extractall = 0
        escape = 0
        keywords = []
        outfile = 'messages.pot'
        writelocations = 1
        locationstyle = GNU
        verbose = 0
        width = 78
        excludefilename = ''
        docstrings = 0
        nodocstrings = {}
    options = Options()
    options.outfile = outpath
    if keywords:
        options.keywords = keywords
    make_escapes(options.escape)
    options.keywords.extend(default_keywords)
    if options.excludefilename:
        try:
            fp = open(options.excludefilename, encoding='utf-8')
            options.toexclude = fp.readlines()
            fp.close()
        except IOError:
            print(""Can't read --exclude-file: %s"" % options.excludefilename, file=sys.stderr)
            sys.exit(1)
    else:
        options.toexclude = []
    eater = TokenEater(options)
    for filename in source_files:
        if options.verbose:
            print('Working on %s' % filename)
        fp = open(filename, encoding='utf-8')
        closep = 1
        try:
            eater.set_filename(filename)
            try:
                tokens = tokenize.generate_tokens(fp.readline)
                for _token in tokens:
                    eater(*_token)
            except tokenize.TokenError as e:
                print('%s: %s, line %d, column %d' % (e.args[0], filename, e.args[1][0], e.args[1][1]), file=sys.stderr)
        finally:
            if closep:
                fp.close()
    fp = open(options.outfile, 'w', encoding='utf-8')
    closep = 1
    try:
        eater.write(fp)
    finally:
        if closep:
            fp.close()","""Can't read --exclude-file: %s"" % options.excludefilename",f"Can't read --exclude-file: {options.excludefilename}",find_wrong,,1,3,1,
thonny,https://github.com/thonny/thonny/tree/master/misc/mp/pyboard.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/thonny/misc/mp/pyboard.py,Pyboard,"def fs_cat(self, src, chunk_size=256):
    cmd = ""with open('%s') as f:\n while 1:\n  b=f.read(%u)\n  if not b:break\n  print(b,end='')"" % (src, chunk_size)
    self.exec_(cmd, data_consumer=stdout_write_bytes)","cmd = ""with open('%s') as f:\n while 1:\n  b=f.read(%u)\n  if not b:break\n  print(b,end='')"" % (src, chunk_size)","cmd = f""with open('{src}') as f:\n while 1:\n  b=f.read({chunk_size})\n  if not b:break\n  print(b,end='')""",find_wrong,,,,,
stable-baselines3,https://github.com/DLR-RM/stable-baselines3/tree/master/stable_baselines3/common/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stable-baselines3/stable_baselines3/common/utils.py,,"def is_vectorized_dict_observation(observation: np.ndarray, observation_space: gym.spaces.Dict) -> bool:
    """"""
    For dict observation type, detects and validates the shape,
    then returns whether or not the observation is vectorized.

    :param observation: the input observation to validate
    :param observation_space: the observation space
    :return: whether the given observation is vectorized or not
    """"""
    all_non_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape != subspace.shape:
            all_non_vectorized = False
            break
    if all_non_vectorized:
        return False
    all_vectorized = True
    for (key, subspace) in observation_space.spaces.items():
        if observation[key].shape[1:] != subspace.shape:
            all_vectorized = False
            break
    if all_vectorized:
        return True
    else:
        error_msg = ''
        try:
            is_vectorized_observation(observation[key], observation_space.spaces[key])
        except ValueError as e:
            error_msg = f'{e}'
        raise ValueError(f'There seems to be a mix of vectorized and non-vectorized observations. Unexpected observation shape {observation[key].shape} for key {key} of type {observation_space.spaces[key]}. {error_msg}')",f'{e}',str(e),find_wrong,2,,,,
muffin,https://github.com/klen/muffin/tree/master/muffin/manage.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muffin/muffin/manage.py,Manager,"def process_arg(name, *, value=..., **opts):
    argname = name.lower()
    arghelp = docs.get(name, '')
    if value is ...:
        return parser.add_argument(argname, help=arghelp, **opts)
    argname = argname.replace('_', '-')
    if isinstance(value, bool):
        if value:
            return parser.add_argument('--no-' + argname, dest=name, action='store_false', help=arghelp or f'Disable {name}')
        return parser.add_argument('--' + argname, dest=name, action='store_true', help=arghelp or f'Enable {name}')
    if isinstance(value, list):
        return parser.add_argument('--' + argname, action='append', default=value, help=arghelp)
    return parser.add_argument('--' + argname, type=anns.get(name, type(value)), default=value, help=arghelp + ' [%s]' % repr(value))","arghelp or f'Disable {name}'
arghelp or f'Enable {name}'
arghelp + ' [%s]' % repr(value)","f'{arghelp}Disable {name}'
f'{arghelp}Enable {name}'
f'{arghelp} [{repr(value)}]'",find_wrong,,,,,
luigi,https://github.com/spotify/luigi/tree/master/luigi/contrib/scalding.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/luigi/luigi/contrib/scalding.py,ScaldingJobRunner,"def get_job_class(self, source):
    job_name = os.path.splitext(os.path.basename(source))[0]
    package = None
    job_class = None
    for line in open(source).readlines():
        p = re.search('package\\s+([^\\s\\(]+)', line)
        if p:
            package = p.groups()[0]
        p = re.search('class\\s+([^\\s\\(]+).*extends\\s+.*Job', line)
        if p:
            job_class = p.groups()[0]
            if job_class == job_name:
                break
    if job_class:
        if package:
            job_class = package + '.' + job_class
        logger.debug('Found scalding job class: %s', job_class)
        return job_class
    else:
        raise luigi.contrib.hadoop.HadoopJobError('Coudl not find scalding job class.')","logger.debug('Found scalding job class: %s', job_class)",logger.debug(f'Found scalding job class: {job_class}'),find_wrong,2,,,,
hangoutsbot,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/lottery.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/lottery.py,,"def perform_drawing(bot, event, *args):
    """"""draw handling:
        /me draw[s] [a[n]] number[s] => draws from ""number"", ""numbers"" or ""numberes""
        /me draw[s] [a[n]] sticks[s] => draws from ""stick"", ""sticks"" or ""stickses""
        /me draws[s]<unrecognised> => draws from ""default""

        note: to prepare lotteries/drawings, see /bot prepare ...

        XXX: check is for singular, plural ""-s"" and plural ""-es""
    """"""
    draw_lists = _load_lottery_state(bot)
    pattern = re.compile('.+ draws?( +(a +|an +|from +)?([a-z0-9\\-_]+))?$', re.IGNORECASE)
    if pattern.match(event.text):
        listname = 'default'
        matches = pattern.search(event.text)
        groups = matches.groups()
        if groups[2] is not None:
            listname = groups[2]
        if listname.endswith('s'):
            _plurality = (listname[:-1], listname, listname + 'es')
        else:
            _plurality = (listname, listname + 's', listname + 'es')
        global_draw_name = None
        _test_name = None
        for word in _plurality:
            _test_name = _get_global_lottery_name(bot, event.conv.id_, word)
            if _test_name in draw_lists:
                global_draw_name = _test_name
                logger.debug('{} is valid lottery'.format(global_draw_name))
                break
        if global_draw_name is not None:
            if len(draw_lists[global_draw_name]['box']) > 0:
                if event.user.id_.chat_id in draw_lists[global_draw_name]['users']:
                    yield from bot.coro_send_message(event.conv, _('<b>{}</b>, you have already drawn <b>{}</b> from the <b>{}</b> box').format(event.user.full_name, draw_lists[global_draw_name]['users'][event.user.id_.chat_id], word))
                else:
                    _thing = str(draw_lists[global_draw_name]['box'].pop())
                    text_drawn = _('<b>{}</b> draws <b>{}</b> from the <b>{}</b> box. ').format(event.user.full_name, _thing, word)
                    if len(draw_lists[global_draw_name]['box']) == 0:
                        text_drawn = text_drawn + _('...AAAAAND its all gone! The <b>{}</b> lottery is over folks.').format(word)
                    yield from bot.coro_send_message(event.conv, text_drawn)
                    draw_lists[global_draw_name]['users'][event.user.id_.chat_id] = _thing
            else:
                text_finished = _('<b>{}</b>, the <b>{}</b> lottery is over. ').format(event.user.full_name, word)
                if event.user.id_.chat_id in draw_lists[global_draw_name]['users']:
                    text_finished = _('You drew a {} previously.').format(draw_lists[global_draw_name]['users'][event.user.id_.chat_id])
                yield from bot.coro_send_message(event.conv, text_finished)
    _save_lottery_state(bot, draw_lists)","'<b>{}</b>, you have already drawn <b>{}</b> from the <b>{}</b> box'.format(event.user.full_name, draw_lists[global_draw_name]['users'][event.user.id_.chat_id], word)","f""<b>{event.user.full_name}</b>, you have already drawn <b>{draw_lists[global_draw_name]['users'][event.user.id_.chat_id]}</b> from the <b>{word}</b> box""",find_wrong,,1,3,,
hangoutsbot,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/lottery.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/lottery.py,,"def perform_drawing(bot, event, *args):
    """"""draw handling:
        /me draw[s] [a[n]] number[s] => draws from ""number"", ""numbers"" or ""numberes""
        /me draw[s] [a[n]] sticks[s] => draws from ""stick"", ""sticks"" or ""stickses""
        /me draws[s]<unrecognised> => draws from ""default""

        note: to prepare lotteries/drawings, see /bot prepare ...

        XXX: check is for singular, plural ""-s"" and plural ""-es""
    """"""
    draw_lists = _load_lottery_state(bot)
    pattern = re.compile('.+ draws?( +(a +|an +|from +)?([a-z0-9\\-_]+))?$', re.IGNORECASE)
    if pattern.match(event.text):
        listname = 'default'
        matches = pattern.search(event.text)
        groups = matches.groups()
        if groups[2] is not None:
            listname = groups[2]
        if listname.endswith('s'):
            _plurality = (listname[:-1], listname, listname + 'es')
        else:
            _plurality = (listname, listname + 's', listname + 'es')
        global_draw_name = None
        _test_name = None
        for word in _plurality:
            _test_name = _get_global_lottery_name(bot, event.conv.id_, word)
            if _test_name in draw_lists:
                global_draw_name = _test_name
                logger.debug('{} is valid lottery'.format(global_draw_name))
                break
        if global_draw_name is not None:
            if len(draw_lists[global_draw_name]['box']) > 0:
                if event.user.id_.chat_id in draw_lists[global_draw_name]['users']:
                    yield from bot.coro_send_message(event.conv, _('<b>{}</b>, you have already drawn <b>{}</b> from the <b>{}</b> box').format(event.user.full_name, draw_lists[global_draw_name]['users'][event.user.id_.chat_id], word))
                else:
                    _thing = str(draw_lists[global_draw_name]['box'].pop())
                    text_drawn = _('<b>{}</b> draws <b>{}</b> from the <b>{}</b> box. ').format(event.user.full_name, _thing, word)
                    if len(draw_lists[global_draw_name]['box']) == 0:
                        text_drawn = text_drawn + _('...AAAAAND its all gone! The <b>{}</b> lottery is over folks.').format(word)
                    yield from bot.coro_send_message(event.conv, text_drawn)
                    draw_lists[global_draw_name]['users'][event.user.id_.chat_id] = _thing
            else:
                text_finished = _('<b>{}</b>, the <b>{}</b> lottery is over. ').format(event.user.full_name, word)
                if event.user.id_.chat_id in draw_lists[global_draw_name]['users']:
                    text_finished = _('You drew a {} previously.').format(draw_lists[global_draw_name]['users'][event.user.id_.chat_id])
                yield from bot.coro_send_message(event.conv, text_finished)
    _save_lottery_state(bot, draw_lists)","_('<b>{}</b> draws <b>{}</b> from the <b>{}</b> box. ').format(event.user.full_name, _thing, word)",f'<b>{event.user.full_name}</b> draws <b>{_thing}</b> from the <b>{word}</b> box. ',find_wrong,,1,3,,
hangoutsbot,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/lottery.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/lottery.py,,"def perform_drawing(bot, event, *args):
    """"""draw handling:
        /me draw[s] [a[n]] number[s] => draws from ""number"", ""numbers"" or ""numberes""
        /me draw[s] [a[n]] sticks[s] => draws from ""stick"", ""sticks"" or ""stickses""
        /me draws[s]<unrecognised> => draws from ""default""

        note: to prepare lotteries/drawings, see /bot prepare ...

        XXX: check is for singular, plural ""-s"" and plural ""-es""
    """"""
    draw_lists = _load_lottery_state(bot)
    pattern = re.compile('.+ draws?( +(a +|an +|from +)?([a-z0-9\\-_]+))?$', re.IGNORECASE)
    if pattern.match(event.text):
        listname = 'default'
        matches = pattern.search(event.text)
        groups = matches.groups()
        if groups[2] is not None:
            listname = groups[2]
        if listname.endswith('s'):
            _plurality = (listname[:-1], listname, listname + 'es')
        else:
            _plurality = (listname, listname + 's', listname + 'es')
        global_draw_name = None
        _test_name = None
        for word in _plurality:
            _test_name = _get_global_lottery_name(bot, event.conv.id_, word)
            if _test_name in draw_lists:
                global_draw_name = _test_name
                logger.debug('{} is valid lottery'.format(global_draw_name))
                break
        if global_draw_name is not None:
            if len(draw_lists[global_draw_name]['box']) > 0:
                if event.user.id_.chat_id in draw_lists[global_draw_name]['users']:
                    yield from bot.coro_send_message(event.conv, _('<b>{}</b>, you have already drawn <b>{}</b> from the <b>{}</b> box').format(event.user.full_name, draw_lists[global_draw_name]['users'][event.user.id_.chat_id], word))
                else:
                    _thing = str(draw_lists[global_draw_name]['box'].pop())
                    text_drawn = _('<b>{}</b> draws <b>{}</b> from the <b>{}</b> box. ').format(event.user.full_name, _thing, word)
                    if len(draw_lists[global_draw_name]['box']) == 0:
                        text_drawn = text_drawn + _('...AAAAAND its all gone! The <b>{}</b> lottery is over folks.').format(word)
                    yield from bot.coro_send_message(event.conv, text_drawn)
                    draw_lists[global_draw_name]['users'][event.user.id_.chat_id] = _thing
            else:
                text_finished = _('<b>{}</b>, the <b>{}</b> lottery is over. ').format(event.user.full_name, word)
                if event.user.id_.chat_id in draw_lists[global_draw_name]['users']:
                    text_finished = _('You drew a {} previously.').format(draw_lists[global_draw_name]['users'][event.user.id_.chat_id])
                yield from bot.coro_send_message(event.conv, text_finished)
    _save_lottery_state(bot, draw_lists)",_('...AAAAAND its all gone! The <b>{}</b> lottery is over folks.').format(word),f'...AAAAAND its all gone! The <b>{word}</b> lottery is over folks.',find_wrong,,1,3,1,
hangoutsbot,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/lottery.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/lottery.py,,"def perform_drawing(bot, event, *args):
    """"""draw handling:
        /me draw[s] [a[n]] number[s] => draws from ""number"", ""numbers"" or ""numberes""
        /me draw[s] [a[n]] sticks[s] => draws from ""stick"", ""sticks"" or ""stickses""
        /me draws[s]<unrecognised> => draws from ""default""

        note: to prepare lotteries/drawings, see /bot prepare ...

        XXX: check is for singular, plural ""-s"" and plural ""-es""
    """"""
    draw_lists = _load_lottery_state(bot)
    pattern = re.compile('.+ draws?( +(a +|an +|from +)?([a-z0-9\\-_]+))?$', re.IGNORECASE)
    if pattern.match(event.text):
        listname = 'default'
        matches = pattern.search(event.text)
        groups = matches.groups()
        if groups[2] is not None:
            listname = groups[2]
        if listname.endswith('s'):
            _plurality = (listname[:-1], listname, listname + 'es')
        else:
            _plurality = (listname, listname + 's', listname + 'es')
        global_draw_name = None
        _test_name = None
        for word in _plurality:
            _test_name = _get_global_lottery_name(bot, event.conv.id_, word)
            if _test_name in draw_lists:
                global_draw_name = _test_name
                logger.debug('{} is valid lottery'.format(global_draw_name))
                break
        if global_draw_name is not None:
            if len(draw_lists[global_draw_name]['box']) > 0:
                if event.user.id_.chat_id in draw_lists[global_draw_name]['users']:
                    yield from bot.coro_send_message(event.conv, _('<b>{}</b>, you have already drawn <b>{}</b> from the <b>{}</b> box').format(event.user.full_name, draw_lists[global_draw_name]['users'][event.user.id_.chat_id], word))
                else:
                    _thing = str(draw_lists[global_draw_name]['box'].pop())
                    text_drawn = _('<b>{}</b> draws <b>{}</b> from the <b>{}</b> box. ').format(event.user.full_name, _thing, word)
                    if len(draw_lists[global_draw_name]['box']) == 0:
                        text_drawn = text_drawn + _('...AAAAAND its all gone! The <b>{}</b> lottery is over folks.').format(word)
                    yield from bot.coro_send_message(event.conv, text_drawn)
                    draw_lists[global_draw_name]['users'][event.user.id_.chat_id] = _thing
            else:
                text_finished = _('<b>{}</b>, the <b>{}</b> lottery is over. ').format(event.user.full_name, word)
                if event.user.id_.chat_id in draw_lists[global_draw_name]['users']:
                    text_finished = _('You drew a {} previously.').format(draw_lists[global_draw_name]['users'][event.user.id_.chat_id])
                yield from bot.coro_send_message(event.conv, text_finished)
    _save_lottery_state(bot, draw_lists)","_('<b>{}</b>, the <b>{}</b> lottery is over. ').format(event.user.full_name, word)","f'<b>{event.user.full_name}</b>, the <b>{word}</b> lottery is over. '",find_wrong,,1,3,1,
hangoutsbot,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/lottery.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/lottery.py,,"def perform_drawing(bot, event, *args):
    """"""draw handling:
        /me draw[s] [a[n]] number[s] => draws from ""number"", ""numbers"" or ""numberes""
        /me draw[s] [a[n]] sticks[s] => draws from ""stick"", ""sticks"" or ""stickses""
        /me draws[s]<unrecognised> => draws from ""default""

        note: to prepare lotteries/drawings, see /bot prepare ...

        XXX: check is for singular, plural ""-s"" and plural ""-es""
    """"""
    draw_lists = _load_lottery_state(bot)
    pattern = re.compile('.+ draws?( +(a +|an +|from +)?([a-z0-9\\-_]+))?$', re.IGNORECASE)
    if pattern.match(event.text):
        listname = 'default'
        matches = pattern.search(event.text)
        groups = matches.groups()
        if groups[2] is not None:
            listname = groups[2]
        if listname.endswith('s'):
            _plurality = (listname[:-1], listname, listname + 'es')
        else:
            _plurality = (listname, listname + 's', listname + 'es')
        global_draw_name = None
        _test_name = None
        for word in _plurality:
            _test_name = _get_global_lottery_name(bot, event.conv.id_, word)
            if _test_name in draw_lists:
                global_draw_name = _test_name
                logger.debug('{} is valid lottery'.format(global_draw_name))
                break
        if global_draw_name is not None:
            if len(draw_lists[global_draw_name]['box']) > 0:
                if event.user.id_.chat_id in draw_lists[global_draw_name]['users']:
                    yield from bot.coro_send_message(event.conv, _('<b>{}</b>, you have already drawn <b>{}</b> from the <b>{}</b> box').format(event.user.full_name, draw_lists[global_draw_name]['users'][event.user.id_.chat_id], word))
                else:
                    _thing = str(draw_lists[global_draw_name]['box'].pop())
                    text_drawn = _('<b>{}</b> draws <b>{}</b> from the <b>{}</b> box. ').format(event.user.full_name, _thing, word)
                    if len(draw_lists[global_draw_name]['box']) == 0:
                        text_drawn = text_drawn + _('...AAAAAND its all gone! The <b>{}</b> lottery is over folks.').format(word)
                    yield from bot.coro_send_message(event.conv, text_drawn)
                    draw_lists[global_draw_name]['users'][event.user.id_.chat_id] = _thing
            else:
                text_finished = _('<b>{}</b>, the <b>{}</b> lottery is over. ').format(event.user.full_name, word)
                if event.user.id_.chat_id in draw_lists[global_draw_name]['users']:
                    text_finished = _('You drew a {} previously.').format(draw_lists[global_draw_name]['users'][event.user.id_.chat_id])
                yield from bot.coro_send_message(event.conv, text_finished)
    _save_lottery_state(bot, draw_lists)",_('You drew a {} previously.').format(draw_lists[global_draw_name]['users'][event.user.id_.chat_id]),f"You drew a {draw_lists[global_draw_name]['users'][event.user.id_.chat_id]} previously.",find_wrong,,1,3,,
dask,https://github.com/dask/dask/tree/master/dask/array/svg.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dask/dask/array/svg.py,,"def svg_nd(chunks, size=200):
    if len(chunks) % 3 == 1:
        chunks = ((1,),) + chunks
    shape = tuple(map(sum, chunks))
    sizes = draw_sizes(shape, size=size)
    chunks2 = chunks
    sizes2 = sizes
    out = []
    left = 0
    total_height = 0
    while chunks2:
        n = len(chunks2) % 3 or 3
        o = svg(chunks2[:n], sizes=sizes2[:n], offset=(left, 0))
        chunks2 = chunks2[n:]
        sizes2 = sizes2[n:]
        lines = o.split('\n')
        header = lines[0]
        height = float(re.search('height=""(\\d*\\.?\\d*)""', header).groups()[0])
        total_height = max(total_height, height)
        width = float(re.search('width=""(\\d*\\.?\\d*)""', header).groups()[0])
        left += width + 10
        o = '\n'.join(lines[1:-1])
        out.append(o)
    header = '<svg width=""%d"" height=""%d"" style=""stroke:rgb(0,0,0);stroke-width:1"" >\n' % (left, total_height)
    footer = '\n</svg>'
    return header + '\n\n'.join(out) + footer","<svg width=""%d"" height=""%d"" style=""stroke:rgb(0,0,0);stroke-width:1"" >\n' % (left, total_height)","f'<svg width=""{left}"" height=""{total_height}"" style=""stroke:rgb(0,0,0);stroke-width:1"" >\n'",find_wrong,,1,3,1,
micropython-lib,https://github.com/micropython/micropython-lib/tree/master/python-stdlib/quopri/quopri.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/micropython-lib/python-stdlib/quopri/quopri.py,,"def main():
    import sys
    import getopt
    try:
        (opts, args) = getopt.getopt(sys.argv[1:], 'td')
    except getopt.error as msg:
        sys.stdout = sys.stderr
        print(msg)
        print('usage: quopri [-t | -d] [file] ...')
        print('-t: quote tabs')
        print('-d: decode; default encode')
        sys.exit(2)
    deco = 0
    tabs = 0
    for (o, a) in opts:
        if o == '-t':
            tabs = 1
        if o == '-d':
            deco = 1
    if tabs and deco:
        sys.stdout = sys.stderr
        print('-t and -d are mutually exclusive')
        sys.exit(2)
    if not args:
        args = ['-']
    sts = 0
    for file in args:
        if file == '-':
            fp = sys.stdin.buffer
        else:
            try:
                fp = open(file, 'rb')
            except IOError as msg:
                sys.stderr.write(""%s: can't open (%s)\n"" % (file, msg))
                sts = 1
                continue
        try:
            if deco:
                decode(fp, sys.stdout.buffer)
            else:
                encode(fp, sys.stdout.buffer, tabs)
        finally:
            if file != '-':
                fp.close()
    if sts:
        sys.exit(sts)","sys.stderr.write(""%s: can't open (%s)\n"" % (file, msg))","print(f""{file}: can't open ({msg})"", file=sys.stderr)",find_wrong,,1,3,1,
micropython-lib,https://github.com/micropython/micropython-lib/tree/master/python-stdlib/quopri/quopri.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/micropython-lib/python-stdlib/quopri/quopri.py,,"def main():
    import sys
    import getopt
    try:
        (opts, args) = getopt.getopt(sys.argv[1:], 'td')
    except getopt.error as msg:
        sys.stdout = sys.stderr
        print(msg)
        print('usage: quopri [-t | -d] [file] ...')
        print('-t: quote tabs')
        print('-d: decode; default encode')
        sys.exit(2)
    deco = 0
    tabs = 0
    for (o, a) in opts:
        if o == '-t':
            tabs = 1
        if o == '-d':
            deco = 1
    if tabs and deco:
        sys.stdout = sys.stderr
        print('-t and -d are mutually exclusive')
        sys.exit(2)
    if not args:
        args = ['-']
    sts = 0
    for file in args:
        if file == '-':
            fp = sys.stdin.buffer
        else:
            try:
                fp = open(file, 'rb')
            except IOError as msg:
                sys.stderr.write(""%s: can't open (%s)\n"" % (file, msg))
                sts = 1
                continue
        try:
            if deco:
                decode(fp, sys.stdout.buffer)
            else:
                encode(fp, sys.stdout.buffer, tabs)
        finally:
            if file != '-':
                fp.close()
    if sts:
        sys.exit(sts)",print('-t and -d are mutually exclusive'),"print('-t and -d are mutually exclusive', file=sys.stderr)",find_wrong,2,,,,
hydra,https://github.com/facebookresearch/hydra/tree/master/hydra/_internal/defaults_list.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydra/hydra/_internal/defaults_list.py,,"def _update_overrides(defaults_list: List[InputDefault], overrides: Overrides, parent: InputDefault, interpolated_subtree: bool) -> None:
    seen_override = False
    last_override_seen = None
    for d in defaults_list:
        if d.is_self():
            continue
        d.update_parent(parent.get_group_path(), parent.get_final_package())
        legacy_hydra_override = False
        if isinstance(d, GroupDefault):
            assert d.group is not None
            if not version.base_at_least('1.2'):
                legacy_hydra_override = not d.is_override() and d.group.startswith('hydra/')
        if seen_override and (not (d.is_override() or d.is_external_append() or legacy_hydra_override)):
            assert isinstance(last_override_seen, GroupDefault)
            pcp = parent.get_config_path()
            okey = last_override_seen.get_override_key()
            oval = last_override_seen.get_name()
            raise ConfigCompositionException(dedent(f""                    In {pcp}: Override '{okey} : {oval}' is defined before '{d.get_override_key()}: {d.get_name()}'.\n                    Overrides must be at the end of the defaults list""))
        if isinstance(d, GroupDefault):
            if legacy_hydra_override:
                d.override = True
                url = 'https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override'
                msg = dedent(f""                    In {parent.get_config_path()}: Invalid overriding of {d.group}:\n                    Default list overrides requires 'override' keyword.\n                    See {url} for more information.\n                    "")
                deprecation_warning(msg)
            if d.override:
                if not legacy_hydra_override:
                    seen_override = True
                last_override_seen = d
                if interpolated_subtree:
                    raise ConfigCompositionException(dedent(f'                            {parent.get_config_path()}: Default List Overrides are not allowed in the subtree\n                            of an in interpolated config group (override {d.get_override_key()}={d.get_name()}).\n                            '))
                overrides.add_override(parent.get_config_path(), d)",dedent(f"                    In {pcp}: Override '{okey} : {oval}' is defined before '{d.get_override_key()}: {d.get_name()}'.\n                    Overrides must be at the end of the defaults list"),f"                    In {pcp}: Override '{okey} : {oval}' is defined before '{d.get_override_key()}: {d.get_name()}'.\n                    Overrides must be at the end of the defaults list",find_wrong,2,,,,
hydra,https://github.com/facebookresearch/hydra/tree/master/hydra/_internal/defaults_list.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydra/hydra/_internal/defaults_list.py,,"def _update_overrides(defaults_list: List[InputDefault], overrides: Overrides, parent: InputDefault, interpolated_subtree: bool) -> None:
    seen_override = False
    last_override_seen = None
    for d in defaults_list:
        if d.is_self():
            continue
        d.update_parent(parent.get_group_path(), parent.get_final_package())
        legacy_hydra_override = False
        if isinstance(d, GroupDefault):
            assert d.group is not None
            if not version.base_at_least('1.2'):
                legacy_hydra_override = not d.is_override() and d.group.startswith('hydra/')
        if seen_override and (not (d.is_override() or d.is_external_append() or legacy_hydra_override)):
            assert isinstance(last_override_seen, GroupDefault)
            pcp = parent.get_config_path()
            okey = last_override_seen.get_override_key()
            oval = last_override_seen.get_name()
            raise ConfigCompositionException(dedent(f""                    In {pcp}: Override '{okey} : {oval}' is defined before '{d.get_override_key()}: {d.get_name()}'.\n                    Overrides must be at the end of the defaults list""))
        if isinstance(d, GroupDefault):
            if legacy_hydra_override:
                d.override = True
                url = 'https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/defaults_list_override'
                msg = dedent(f""                    In {parent.get_config_path()}: Invalid overriding of {d.group}:\n                    Default list overrides requires 'override' keyword.\n                    See {url} for more information.\n                    "")
                deprecation_warning(msg)
            if d.override:
                if not legacy_hydra_override:
                    seen_override = True
                last_override_seen = d
                if interpolated_subtree:
                    raise ConfigCompositionException(dedent(f'                            {parent.get_config_path()}: Default List Overrides are not allowed in the subtree\n                            of an in interpolated config group (override {d.get_override_key()}={d.get_name()}).\n                            '))
                overrides.add_override(parent.get_config_path(), d)",dedent(f"                    In {parent.get_config_path()}: Invalid overriding of {d.group}:\n                    Default list overrides requires 'override' keyword.\n                    See {url} for more information.\n                    "),f"                    In {parent.get_config_path()}: Invalid overriding of {d.group}:\n                    Default list overrides requires 'override' keyword.\n                    See {url} for more information.\n                    ",find_wrong,2,,,,
s3fs,https://github.com/fsspec/s3fs/tree/master/s3fs/tests/test_s3fs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/s3fs/s3fs/tests/test_s3fs.py,,"def test_s3_big_ls(s3):
    for x in range(1200):
        s3.touch(test_bucket_name + '/thousand/%i.part' % x)
    assert len(s3.find(test_bucket_name)) > 1200
    s3.rm(test_bucket_name + '/thousand/', recursive=True)
    assert len(s3.find(test_bucket_name + '/thousand/')) == 0",thousand/%i.part' % x,f'thousand/{x}.part',find_wrong,,,,,
QRec,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,CUNE_MF,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","print('Kind Note: This method will probably take much time.')
print('Building collaborative user network...')
print('Generating random deep walks...')
print('Generating user embedding...')
print('User embedding generated.')
print('Constructing similarity matrix...')
print('Similarity matrix finished.')
print('Decomposing...')","print(f'Kind Note: This method will probably take much time.')
print(f'Building collaborative user network...')
print(f'Generating random deep walks...')
print(f'Generating user embedding...')
print(f'User embedding generated.')
print(f'Constructing similarity matrix...')
print(f'Similarity matrix finished.')
print(f'Decomposing...')",find_wrong,2,,,,
brozzler,https://github.com/internetarchive/brozzler/tree/master/tests/test_cluster.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brozzler/tests/test_cluster.py,,"def test_brozzle_site(httpd):
    test_id = 'test_brozzle_site-%s' % datetime.datetime.utcnow().isoformat()
    rr = doublethink.Rethinker('localhost', db='brozzler')
    site = brozzler.Site(rr, {'seed': make_url(httpd, '/site1/'), 'warcprox_meta': {'captures-table-extra-fields': {'test_id': test_id}}})
    page1 = make_url(httpd, '/site1/')
    page2 = make_url(httpd, '/site1/file1.txt')
    robots = make_url(httpd, '/robots.txt')
    try:
        stop_service('brozzler-worker')
        assert site.id is None
        frontier = brozzler.RethinkDbFrontier(rr)
        brozzler.new_site(frontier, site)
        assert site.id is not None
        assert len(list(frontier.site_pages(site.id))) == 1
    finally:
        start_service('brozzler-worker')
    start = time.time()
    while site.status != 'FINISHED' and time.time() - start < 300:
        time.sleep(0.5)
        site.refresh()
    assert site.status == 'FINISHED'
    pages = list(frontier.site_pages(site.id))
    assert len(pages) == 2
    assert {page.url for page in pages} == {make_url(httpd, '/site1/'), make_url(httpd, '/site1/file1.txt')}
    time.sleep(2)
    captures = rr.table('captures').filter({'test_id': test_id}).run()
    captures_by_url = {c['url']: c for c in captures if c['http_method'] != 'HEAD'}
    assert robots in captures_by_url
    assert page1 in captures_by_url
    assert page2 in captures_by_url
    assert 'screenshot:%s' % page1 in captures_by_url
    assert 'thumbnail:%s' % page1 in captures_by_url
    t14 = captures_by_url[page2]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, page2)
    expected_payload = open(os.path.join(os.path.dirname(__file__), 'htdocs', 'site1', 'file1.txt'), 'rb').read()
    assert requests.get(wb_url).content == expected_payload
    url = 'screenshot:%s' % page1
    t14 = captures_by_url[url]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, url)
    response = requests.get(wb_url)
    assert response.status_code == 200
    assert response.headers['content-type'] == 'image/jpeg'
    url = 'thumbnail:%s' % page1
    t14 = captures_by_url[url]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, url)
    response = requests.get(wb_url)
    assert response.status_code == 200
    assert response.headers['content-type'] == 'image/jpeg'",test_brozzle_site-%s' % datetime.datetime.utcnow().isoformat(),f'test_brozzle_site-{datetime.datetime.utcnow().isoformat()}',find_wrong,,1,3,,
brozzler,https://github.com/internetarchive/brozzler/tree/master/tests/test_cluster.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brozzler/tests/test_cluster.py,,"def test_brozzle_site(httpd):
    test_id = 'test_brozzle_site-%s' % datetime.datetime.utcnow().isoformat()
    rr = doublethink.Rethinker('localhost', db='brozzler')
    site = brozzler.Site(rr, {'seed': make_url(httpd, '/site1/'), 'warcprox_meta': {'captures-table-extra-fields': {'test_id': test_id}}})
    page1 = make_url(httpd, '/site1/')
    page2 = make_url(httpd, '/site1/file1.txt')
    robots = make_url(httpd, '/robots.txt')
    try:
        stop_service('brozzler-worker')
        assert site.id is None
        frontier = brozzler.RethinkDbFrontier(rr)
        brozzler.new_site(frontier, site)
        assert site.id is not None
        assert len(list(frontier.site_pages(site.id))) == 1
    finally:
        start_service('brozzler-worker')
    start = time.time()
    while site.status != 'FINISHED' and time.time() - start < 300:
        time.sleep(0.5)
        site.refresh()
    assert site.status == 'FINISHED'
    pages = list(frontier.site_pages(site.id))
    assert len(pages) == 2
    assert {page.url for page in pages} == {make_url(httpd, '/site1/'), make_url(httpd, '/site1/file1.txt')}
    time.sleep(2)
    captures = rr.table('captures').filter({'test_id': test_id}).run()
    captures_by_url = {c['url']: c for c in captures if c['http_method'] != 'HEAD'}
    assert robots in captures_by_url
    assert page1 in captures_by_url
    assert page2 in captures_by_url
    assert 'screenshot:%s' % page1 in captures_by_url
    assert 'thumbnail:%s' % page1 in captures_by_url
    t14 = captures_by_url[page2]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, page2)
    expected_payload = open(os.path.join(os.path.dirname(__file__), 'htdocs', 'site1', 'file1.txt'), 'rb').read()
    assert requests.get(wb_url).content == expected_payload
    url = 'screenshot:%s' % page1
    t14 = captures_by_url[url]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, url)
    response = requests.get(wb_url)
    assert response.status_code == 200
    assert response.headers['content-type'] == 'image/jpeg'
    url = 'thumbnail:%s' % page1
    t14 = captures_by_url[url]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, url)
    response = requests.get(wb_url)
    assert response.status_code == 200
    assert response.headers['content-type'] == 'image/jpeg'","http://localhost:8880/brozzler/%s/%s' % (t14, page2)",f'http://localhost:8880/brozzler/{t14}/{page2}',find_wrong,,1,3,,
brozzler,https://github.com/internetarchive/brozzler/tree/master/tests/test_cluster.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brozzler/tests/test_cluster.py,,"def test_brozzle_site(httpd):
    test_id = 'test_brozzle_site-%s' % datetime.datetime.utcnow().isoformat()
    rr = doublethink.Rethinker('localhost', db='brozzler')
    site = brozzler.Site(rr, {'seed': make_url(httpd, '/site1/'), 'warcprox_meta': {'captures-table-extra-fields': {'test_id': test_id}}})
    page1 = make_url(httpd, '/site1/')
    page2 = make_url(httpd, '/site1/file1.txt')
    robots = make_url(httpd, '/robots.txt')
    try:
        stop_service('brozzler-worker')
        assert site.id is None
        frontier = brozzler.RethinkDbFrontier(rr)
        brozzler.new_site(frontier, site)
        assert site.id is not None
        assert len(list(frontier.site_pages(site.id))) == 1
    finally:
        start_service('brozzler-worker')
    start = time.time()
    while site.status != 'FINISHED' and time.time() - start < 300:
        time.sleep(0.5)
        site.refresh()
    assert site.status == 'FINISHED'
    pages = list(frontier.site_pages(site.id))
    assert len(pages) == 2
    assert {page.url for page in pages} == {make_url(httpd, '/site1/'), make_url(httpd, '/site1/file1.txt')}
    time.sleep(2)
    captures = rr.table('captures').filter({'test_id': test_id}).run()
    captures_by_url = {c['url']: c for c in captures if c['http_method'] != 'HEAD'}
    assert robots in captures_by_url
    assert page1 in captures_by_url
    assert page2 in captures_by_url
    assert 'screenshot:%s' % page1 in captures_by_url
    assert 'thumbnail:%s' % page1 in captures_by_url
    t14 = captures_by_url[page2]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, page2)
    expected_payload = open(os.path.join(os.path.dirname(__file__), 'htdocs', 'site1', 'file1.txt'), 'rb').read()
    assert requests.get(wb_url).content == expected_payload
    url = 'screenshot:%s' % page1
    t14 = captures_by_url[url]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, url)
    response = requests.get(wb_url)
    assert response.status_code == 200
    assert response.headers['content-type'] == 'image/jpeg'
    url = 'thumbnail:%s' % page1
    t14 = captures_by_url[url]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, url)
    response = requests.get(wb_url)
    assert response.status_code == 200
    assert response.headers['content-type'] == 'image/jpeg'",'screenshot:%s' % page1,f'screenshot:{page1}',find_wrong,,1,3,,
brozzler,https://github.com/internetarchive/brozzler/tree/master/tests/test_cluster.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brozzler/tests/test_cluster.py,,"def test_brozzle_site(httpd):
    test_id = 'test_brozzle_site-%s' % datetime.datetime.utcnow().isoformat()
    rr = doublethink.Rethinker('localhost', db='brozzler')
    site = brozzler.Site(rr, {'seed': make_url(httpd, '/site1/'), 'warcprox_meta': {'captures-table-extra-fields': {'test_id': test_id}}})
    page1 = make_url(httpd, '/site1/')
    page2 = make_url(httpd, '/site1/file1.txt')
    robots = make_url(httpd, '/robots.txt')
    try:
        stop_service('brozzler-worker')
        assert site.id is None
        frontier = brozzler.RethinkDbFrontier(rr)
        brozzler.new_site(frontier, site)
        assert site.id is not None
        assert len(list(frontier.site_pages(site.id))) == 1
    finally:
        start_service('brozzler-worker')
    start = time.time()
    while site.status != 'FINISHED' and time.time() - start < 300:
        time.sleep(0.5)
        site.refresh()
    assert site.status == 'FINISHED'
    pages = list(frontier.site_pages(site.id))
    assert len(pages) == 2
    assert {page.url for page in pages} == {make_url(httpd, '/site1/'), make_url(httpd, '/site1/file1.txt')}
    time.sleep(2)
    captures = rr.table('captures').filter({'test_id': test_id}).run()
    captures_by_url = {c['url']: c for c in captures if c['http_method'] != 'HEAD'}
    assert robots in captures_by_url
    assert page1 in captures_by_url
    assert page2 in captures_by_url
    assert 'screenshot:%s' % page1 in captures_by_url
    assert 'thumbnail:%s' % page1 in captures_by_url
    t14 = captures_by_url[page2]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, page2)
    expected_payload = open(os.path.join(os.path.dirname(__file__), 'htdocs', 'site1', 'file1.txt'), 'rb').read()
    assert requests.get(wb_url).content == expected_payload
    url = 'screenshot:%s' % page1
    t14 = captures_by_url[url]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, url)
    response = requests.get(wb_url)
    assert response.status_code == 200
    assert response.headers['content-type'] == 'image/jpeg'
    url = 'thumbnail:%s' % page1
    t14 = captures_by_url[url]['timestamp'].strftime('%Y%m%d%H%M%S')
    wb_url = 'http://localhost:8880/brozzler/%s/%s' % (t14, url)
    response = requests.get(wb_url)
    assert response.status_code == 200
    assert response.headers['content-type'] == 'image/jpeg'",thumbnail:%s' % page1,f'thumbnail:{page1}',find_wrong,,1,3,1,
metaworld,https://github.com/rlworkgroup/metaworld/tree/master/metaworld/envs/mujoco/env_dict.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/metaworld/metaworld/envs/mujoco/env_dict.py,,"def create_hidden_goal_envs():
    hidden_goal_envs = {}
    for (env_name, env_cls) in ALL_V2_ENVIRONMENTS.items():
        d = {}

        def initialize(env, seed=None):
            if seed is not None:
                st0 = np.random.get_state()
                np.random.seed(seed)
            super(type(env), env).__init__()
            env._partially_observable = True
            env._freeze_rand_vec = False
            env._set_task_called = True
            env.reset()
            env._freeze_rand_vec = True
            if seed is not None:
                env.seed(seed)
                np.random.set_state(st0)
        d['__init__'] = initialize
        hg_env_name = re.sub('(^|[-])\\s*([a-zA-Z])', lambda p: p.group(0).upper(), env_name)
        hg_env_name = hg_env_name.replace('-', '')
        hg_env_key = '{}-goal-hidden'.format(env_name)
        hg_env_name = '{}GoalHidden'.format(hg_env_name)
        HiddenGoalEnvCls = type(hg_env_name, (env_cls,), d)
        hidden_goal_envs[hg_env_key] = HiddenGoalEnvCls
    return OrderedDict(hidden_goal_envs)","hg_env_key = '{}-goal-hidden'.format(env_name)
hg_env_name = '{}GoalHidden'.format(hg_env_name)","hg_env_key = f'{env_name}-goal-hidden'
hg_env_name = f'{hg_env_name}GoalHidden'",find_wrong,,1,3,,
KILT,https://github.com/facebookresearch/KILT/tree/master/kilt/readers/t5/base_transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/KILT/kilt/readers/t5/base_transformer.py,LoggingCallback,"def on_validation_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):
    logger.info('***** Validation results *****')
    if pl_module.is_logger():
        metrics = trainer.callback_metrics
        for key in sorted(metrics):
            if key not in ['log', 'progress_bar']:
                logger.info('{} = {}\n'.format(key, str(metrics[key])))","logger.info('{} = {}\n'.format(key, str(metrics[key])))",logger.info(f'{key} = {metrics[key]}\n'),find_wrong,,1,3,,
slither,https://github.com/crytic/slither/tree/master/slither/printers/call/call_graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/slither/slither/printers/call/call_graph.py,PrinterCallGraph,"def output(self, filename):
    """"""
        Output the graph in filename
        Args:
            filename(string)
        """"""
    all_contracts_filename = ''
    if not filename.endswith('.dot'):
        all_contracts_filename = f'{filename}.all_contracts.call-graph.dot'
    if filename == '.dot':
        all_contracts_filename = 'all_contracts.dot'
    info = ''
    results = []
    with open(all_contracts_filename, 'w', encoding='utf8') as f:
        info += f'Call Graph: {all_contracts_filename}\n'
        all_functionss = [compilation_unit.functions for compilation_unit in self.slither.compilation_units]
        all_functions = [item for sublist in all_functionss for item in sublist]
        all_functions_as_dict = {function.canonical_name: function for function in all_functions}
        content = '\n'.join(['strict digraph {'] + [_process_functions(all_functions_as_dict.values())] + ['}'])
        f.write(content)
        results.append((all_contracts_filename, content))
    for derived_contract in self.slither.contracts_derived:
        derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot'
        with open(derived_output_filename, 'w', encoding='utf8') as f:
            info += f'Call Graph: {derived_output_filename}\n'
            content = '\n'.join(['strict digraph {'] + [_process_functions(derived_contract.functions)] + ['}'])
            f.write(content)
            results.append((derived_output_filename, content))
    self.info(info)
    res = self.generate_output(info)
    for (filename_result, content) in results:
        res.add_file(filename_result, content)
    return res",all_contracts_filename = f'{filename}.all_contracts.call-graph.dot',all_contracts_filename = f'{filename}.all_contracts.call-graph.dot' if not filename.endswith('.dot') else '',find_wrong,2,,,,
slither,https://github.com/crytic/slither/tree/master/slither/printers/call/call_graph.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/slither/slither/printers/call/call_graph.py,PrinterCallGraph,"def output(self, filename):
    """"""
        Output the graph in filename
        Args:
            filename(string)
        """"""
    all_contracts_filename = ''
    if not filename.endswith('.dot'):
        all_contracts_filename = f'{filename}.all_contracts.call-graph.dot'
    if filename == '.dot':
        all_contracts_filename = 'all_contracts.dot'
    info = ''
    results = []
    with open(all_contracts_filename, 'w', encoding='utf8') as f:
        info += f'Call Graph: {all_contracts_filename}\n'
        all_functionss = [compilation_unit.functions for compilation_unit in self.slither.compilation_units]
        all_functions = [item for sublist in all_functionss for item in sublist]
        all_functions_as_dict = {function.canonical_name: function for function in all_functions}
        content = '\n'.join(['strict digraph {'] + [_process_functions(all_functions_as_dict.values())] + ['}'])
        f.write(content)
        results.append((all_contracts_filename, content))
    for derived_contract in self.slither.contracts_derived:
        derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot'
        with open(derived_output_filename, 'w', encoding='utf8') as f:
            info += f'Call Graph: {derived_output_filename}\n'
            content = '\n'.join(['strict digraph {'] + [_process_functions(derived_contract.functions)] + ['}'])
            f.write(content)
            results.append((derived_output_filename, content))
    self.info(info)
    res = self.generate_output(info)
    for (filename_result, content) in results:
        res.add_file(filename_result, content)
    return res",derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot',derived_output_filename = f'{filename}.{derived_contract.name}.call-graph.dot',find_wrong,2,,,,
flair,https://github.com/flairNLP/flair/tree/master/flair/datasets/sequence_labeling.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flair/flair/datasets/sequence_labeling.py,NER_ENGLISH_STACKOVERFLOW,"def __init__(self, base_path: Union[str, Path]=None, in_memory: bool=True, **corpusargs):
    """"""
        Initialize the STACKOVERFLOW_NER corpus. The first time you call this constructor it will automatically
        download the dataset.
        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this
        to point to a different folder but typically this should not be necessary.
        POS tags instead
        :param in_memory: If True, keeps dataset in memory giving speedups in training.
        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object
        """"""
    if not base_path:
        base_path = flair.cache_root / 'datasets'
    else:
        base_path = Path(base_path)
    '\n        The Datasets are represented in the Conll format.\n           In this format each line of the Dataset is in the following format:\n           <word>+""\t""+<NE>""\t""+<word>+""\t""<markdown>\n           The end of sentence is marked with an empty line.\n           In each line NE represented the human annotated named entity\n           and <markdown> represented the code tags provided by the users who wrote the posts.\n           '
    columns = {0: 'word', 1: 'ner', 3: 'markdown'}
    entity_mapping = {'Library_Function': 'Function', 'Function_Name': 'Function', 'Class_Name': 'Class', 'Library_Class': 'Class', 'Organization': 'Website', 'Library_Variable': 'Variable', 'Variable_Name': 'Variable', 'Error_Name': 'O', 'Keyboard_IP': 'O', 'Value': 'O', 'Output_Block': 'O'}
    dataset_name = self.__class__.__name__.lower()
    data_folder = base_path / dataset_name
    STACKOVERFLOW_NER_path = 'https://raw.githubusercontent.com/jeniyat/StackOverflowNER/master/resources/annotated_ner_data/StackOverflow/'
    banned_sentences = ['code omitted for annotation', 'omitted for annotation', 'CODE_BLOCK :', 'OP_BLOCK :', 'Question_URL :', 'Question_ID :']
    files = ['train', 'test', 'dev']
    for file in files:
        questions = 0
        answers = 0
        cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
        for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
            if line.startswith('Question_ID'):
                questions += 1
            if line.startswith('Answer_to_Question_ID'):
                answers += 1
        log.info(f'File {file} has {questions} questions and {answers} answers.')
    super(NER_ENGLISH_STACKOVERFLOW, self).__init__(data_folder, columns, train_file='train.txt', test_file='test.txt', dev_file='dev.txt', encoding='utf-8', banned_sentences=banned_sentences, in_memory=in_memory, label_name_map=entity_mapping, **corpusargs)",f'{STACKOVERFLOW_NER_path}{file}.txt',f'{STACKOVERFLOW_NER_path}{file}.txt',find_wrong,2,,,,
zvt,https://github.com/zvtvz/zvt/tree/master/zvt/recorders/sina/quotes/sina_etf_kdata_recorder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/zvt/recorders/sina/quotes/sina_etf_kdata_recorder.py,ChinaETFDayKdataRecorder,"def on_finish_entity(self, entity):
    kdatas = get_kdata(entity_id=entity.id, level=IntervalLevel.LEVEL_1DAY.value, order=Etf1dKdata.timestamp.asc(), return_type='domain', session=self.session, filters=[Etf1dKdata.cumulative_net_value.is_(None)])
    if kdatas and len(kdatas) > 0:
        start = kdatas[0].timestamp
        end = kdatas[-1].timestamp
        df = self.fetch_cumulative_net_value(entity, start, end)
        if df is not None and (not df.empty):
            for kdata in kdatas:
                if kdata.timestamp in df.index:
                    kdata.cumulative_net_value = df.loc[kdata.timestamp, 'LJJZ']
                    kdata.change_pct = df.loc[kdata.timestamp, 'JZZZL']
            self.session.commit()
            self.logger.info(f'{entity.code} - {entity.name}...')",self.logger.info(f'{entity.code} - {entity.name}...'),self.logger.info(f'{entity.code} - {entity.name}...'),find_wrong,2,,,,
Mobile-Security-Framework-MobSF,https://github.com/MobSF/Mobile-Security-Framework-MobSF/tree/master/mobsf/MobSF/views/apk_downloader.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mobile-Security-Framework-MobSF/mobsf/MobSF/views/apk_downloader.py,,"def try_provider(package, provider, domain):
    """"""Try using a provider.""""""
    downloaded_file = None
    data = None
    apk_name = f'{package}.apk'
    temp_file = Path(gettempdir()) / apk_name
    link = find_apk_link(provider, domain)
    if link:
        downloaded_file = download_file(link, temp_file)
    if downloaded_file:
        data = add_apk(downloaded_file, apk_name)
    if data:
        return data
    return None",apk_name = f'{package}.apk',apk_name = f'{package}.apk',find_wrong,,1,3,,
integrations-core,https://github.com/DataDog/integrations-core/tree/master/docs/developer/.scripts/33_render_status.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/docs/developer/.scripts/33_render_status.py,,"def render_process_signatures_progress():
    valid_checks = sorted([c for c in get_valid_checks() if c not in PROCESS_SIGNATURE_EXCLUDE])
    total_checks = len(valid_checks)
    checks_with_ps = 0
    lines = ['## Process signatures', '', None, '', '??? check ""Completed""']
    for check in valid_checks:
        if has_process_signature(check):
            status = 'X'
            checks_with_ps += 1
        else:
            status = ' '
        lines.append(f'    - [{status}] {check}')
    percent = checks_with_ps / total_checks * 100
    formatted_percent = f'{percent:.2f}'
    lines[2] = f'[={formatted_percent}% ""{formatted_percent}%""]'
    lines[4] = f'??? check ""Completed {checks_with_ps}/{total_checks}""'
    return lines",formatted_percent = '{:.2f}'.format(percent),formatted_percent = f'{percent:.2f}',find_wrong,,1,3,,
integrations-core,https://github.com/DataDog/integrations-core/tree/master/docs/developer/.scripts/33_render_status.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/docs/developer/.scripts/33_render_status.py,,"def render_process_signatures_progress():
    valid_checks = sorted([c for c in get_valid_checks() if c not in PROCESS_SIGNATURE_EXCLUDE])
    total_checks = len(valid_checks)
    checks_with_ps = 0
    lines = ['## Process signatures', '', None, '', '??? check ""Completed""']
    for check in valid_checks:
        if has_process_signature(check):
            status = 'X'
            checks_with_ps += 1
        else:
            status = ' '
        lines.append(f'    - [{status}] {check}')
    percent = checks_with_ps / total_checks * 100
    formatted_percent = f'{percent:.2f}'
    lines[2] = f'[={formatted_percent}% ""{formatted_percent}%""]'
    lines[4] = f'??? check ""Completed {checks_with_ps}/{total_checks}""'
    return lines","lines.append('    - [{status}] {check}'.format(status=status, check=check))",lines.append(f'    - [{status}] {check}'),find_wrong,,1,3,,
rally,https://github.com/elastic/rally/tree/master/it/tracker_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rally/it/tracker_test.py,,"def test_create_track(cfg, tmp_path, test_cluster):
    cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track=geonames --challenge=append-no-conflicts-index-only --track-params=""ingest_percentage:0.05"" --on-error=abort --include-tasks=""delete-index,create-index,check-cluster-health,index-append"" --quiet'
    assert it.race(cfg, cmd) == 0
    track_name = f'test-track-{uuid.uuid4()}'
    track_path = tmp_path / track_name
    assert it.esrally(cfg, f'create-track --target-hosts=127.0.0.1:{test_cluster.http_port} --indices=geonames --track={track_name} --output-path={tmp_path}') == 0
    base_generated_corpora = 'geonames-documents'
    expected_files = ['track.json', 'geonames.json', f'{base_generated_corpora}-1k.json', f'{base_generated_corpora}.json', f'{base_generated_corpora}-1k.json.bz2', f'{base_generated_corpora}.json.bz2']
    for f in expected_files:
        full_path = track_path / f
        assert full_path.exists(), f'Expected file to exist at path [{full_path}]'
    with open(track_path / f'{base_generated_corpora}-1k.json', 'rt') as f:
        num_lines = sum((1 for line in f))
    assert num_lines == 1000, f'Corpora [{base_generated_corpora}-1k.json] used by test-mode is [{num_lines}] lines but should be 1000 lines'
    cmd = f'--test-mode --pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}'
    assert it.race(cfg, cmd) == 0
    cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}'
    assert it.race(cfg, cmd) == 0","cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track=geonames --challenge=append-no-conflicts-index-only --track-params=""ingest_percentage:0.05"" --on-error=abort --include-tasks=""delete-index,create-index,check-cluster-health,index-append"" --quiet'","cmd = f""--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track=geonames --challenge=append-no-conflicts-index-only --track-params='ingest_percentage:0.05' --on-error=abort --include-tasks='delete-index,create-index,check-cluster-health,index-append' --quiet""",find_wrong,2,,,,
rally,https://github.com/elastic/rally/tree/master/it/tracker_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rally/it/tracker_test.py,,"def test_create_track(cfg, tmp_path, test_cluster):
    cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track=geonames --challenge=append-no-conflicts-index-only --track-params=""ingest_percentage:0.05"" --on-error=abort --include-tasks=""delete-index,create-index,check-cluster-health,index-append"" --quiet'
    assert it.race(cfg, cmd) == 0
    track_name = f'test-track-{uuid.uuid4()}'
    track_path = tmp_path / track_name
    assert it.esrally(cfg, f'create-track --target-hosts=127.0.0.1:{test_cluster.http_port} --indices=geonames --track={track_name} --output-path={tmp_path}') == 0
    base_generated_corpora = 'geonames-documents'
    expected_files = ['track.json', 'geonames.json', f'{base_generated_corpora}-1k.json', f'{base_generated_corpora}.json', f'{base_generated_corpora}-1k.json.bz2', f'{base_generated_corpora}.json.bz2']
    for f in expected_files:
        full_path = track_path / f
        assert full_path.exists(), f'Expected file to exist at path [{full_path}]'
    with open(track_path / f'{base_generated_corpora}-1k.json', 'rt') as f:
        num_lines = sum((1 for line in f))
    assert num_lines == 1000, f'Corpora [{base_generated_corpora}-1k.json] used by test-mode is [{num_lines}] lines but should be 1000 lines'
    cmd = f'--test-mode --pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}'
    assert it.race(cfg, cmd) == 0
    cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}'
    assert it.race(cfg, cmd) == 0",track_name = f'test-track-{uuid.uuid4()}',track_name = f'test-track-{uuid.uuid4()}',find_wrong,2,,,,
rally,https://github.com/elastic/rally/tree/master/it/tracker_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rally/it/tracker_test.py,,"def test_create_track(cfg, tmp_path, test_cluster):
    cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track=geonames --challenge=append-no-conflicts-index-only --track-params=""ingest_percentage:0.05"" --on-error=abort --include-tasks=""delete-index,create-index,check-cluster-health,index-append"" --quiet'
    assert it.race(cfg, cmd) == 0
    track_name = f'test-track-{uuid.uuid4()}'
    track_path = tmp_path / track_name
    assert it.esrally(cfg, f'create-track --target-hosts=127.0.0.1:{test_cluster.http_port} --indices=geonames --track={track_name} --output-path={tmp_path}') == 0
    base_generated_corpora = 'geonames-documents'
    expected_files = ['track.json', 'geonames.json', f'{base_generated_corpora}-1k.json', f'{base_generated_corpora}.json', f'{base_generated_corpora}-1k.json.bz2', f'{base_generated_corpora}.json.bz2']
    for f in expected_files:
        full_path = track_path / f
        assert full_path.exists(), f'Expected file to exist at path [{full_path}]'
    with open(track_path / f'{base_generated_corpora}-1k.json', 'rt') as f:
        num_lines = sum((1 for line in f))
    assert num_lines == 1000, f'Corpora [{base_generated_corpora}-1k.json] used by test-mode is [{num_lines}] lines but should be 1000 lines'
    cmd = f'--test-mode --pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}'
    assert it.race(cfg, cmd) == 0
    cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}'
    assert it.race(cfg, cmd) == 0","expected_files = ['track.json', 'geonames.json', f'{base_generated_corpora}-1k.json', f'{base_generated_corpora}.json', f'{base_generated_corpora}-1k.json.bz2', f'{base_generated_corpora}.json.bz2']","expected_files = ['track.json', 'geonames.json', f'{base_generated_corpora}-1k.json', f'{base_generated_corpora}.json', f'{base_generated_corpora}-1k.json.bz2', f'{base_generated_corpora}.json.bz2']",find_wrong,2,,,,
rally,https://github.com/elastic/rally/tree/master/it/tracker_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rally/it/tracker_test.py,,"def test_create_track(cfg, tmp_path, test_cluster):
    cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track=geonames --challenge=append-no-conflicts-index-only --track-params=""ingest_percentage:0.05"" --on-error=abort --include-tasks=""delete-index,create-index,check-cluster-health,index-append"" --quiet'
    assert it.race(cfg, cmd) == 0
    track_name = f'test-track-{uuid.uuid4()}'
    track_path = tmp_path / track_name
    assert it.esrally(cfg, f'create-track --target-hosts=127.0.0.1:{test_cluster.http_port} --indices=geonames --track={track_name} --output-path={tmp_path}') == 0
    base_generated_corpora = 'geonames-documents'
    expected_files = ['track.json', 'geonames.json', f'{base_generated_corpora}-1k.json', f'{base_generated_corpora}.json', f'{base_generated_corpora}-1k.json.bz2', f'{base_generated_corpora}.json.bz2']
    for f in expected_files:
        full_path = track_path / f
        assert full_path.exists(), f'Expected file to exist at path [{full_path}]'
    with open(track_path / f'{base_generated_corpora}-1k.json', 'rt') as f:
        num_lines = sum((1 for line in f))
    assert num_lines == 1000, f'Corpora [{base_generated_corpora}-1k.json] used by test-mode is [{num_lines}] lines but should be 1000 lines'
    cmd = f'--test-mode --pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}'
    assert it.race(cfg, cmd) == 0
    cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}'
    assert it.race(cfg, cmd) == 0",cmd = f'--test-mode --pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}',cmd = f'--test-mode --pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}',find_wrong,2,,,,
rally,https://github.com/elastic/rally/tree/master/it/tracker_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rally/it/tracker_test.py,,"def test_create_track(cfg, tmp_path, test_cluster):
    cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track=geonames --challenge=append-no-conflicts-index-only --track-params=""ingest_percentage:0.05"" --on-error=abort --include-tasks=""delete-index,create-index,check-cluster-health,index-append"" --quiet'
    assert it.race(cfg, cmd) == 0
    track_name = f'test-track-{uuid.uuid4()}'
    track_path = tmp_path / track_name
    assert it.esrally(cfg, f'create-track --target-hosts=127.0.0.1:{test_cluster.http_port} --indices=geonames --track={track_name} --output-path={tmp_path}') == 0
    base_generated_corpora = 'geonames-documents'
    expected_files = ['track.json', 'geonames.json', f'{base_generated_corpora}-1k.json', f'{base_generated_corpora}.json', f'{base_generated_corpora}-1k.json.bz2', f'{base_generated_corpora}.json.bz2']
    for f in expected_files:
        full_path = track_path / f
        assert full_path.exists(), f'Expected file to exist at path [{full_path}]'
    with open(track_path / f'{base_generated_corpora}-1k.json', 'rt') as f:
        num_lines = sum((1 for line in f))
    assert num_lines == 1000, f'Corpora [{base_generated_corpora}-1k.json] used by test-mode is [{num_lines}] lines but should be 1000 lines'
    cmd = f'--test-mode --pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}'
    assert it.race(cfg, cmd) == 0
    cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}'
    assert it.race(cfg, cmd) == 0",cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}',cmd = f'--pipeline=benchmark-only --target-hosts=127.0.0.1:{test_cluster.http_port} --track-path={track_path}',find_wrong,2,,,,
pywikibot,https://github.com/wikimedia/pywikibot/tree/master/tests/wikistats_tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pywikibot/tests/wikistats_tests.py,WikiStatsTestCase,"def test_sorting_order(self):
    """"""Test sorting order of languages_by_size.""""""
    family = 'wikipedia'
    ws = WikiStats()
    data = ws.get_dict(family)
    last = sys.maxsize
    last_code = ''
    for code in ws.languages_by_size(family):
        curr = int(data[code]['good'])
        self.assertGreaterEqual(last, curr, '{} ({}) is greater than {} ({}).'.format(code, curr, last_code, last))
        last = curr
        last_code = code","'{} ({}) is greater than {} ({}).'.format(code, curr, last_code, last)",f'{code} ({curr}) is greater than {last_code} ({last}).',find_wrong,,1,3,,
sentry,https://github.com/getsentry/sentry/tree/master/src/sentry/tagstore/snuba/backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/tagstore/snuba/backend.py,SnubaTagStorage,"def __get_tag_keys_for_projects(self, projects, group, environments, start, end, limit=1000, keys=None, include_values_seen=True, use_cache=False, include_transactions=False, denylist=None, **kwargs):
    """"""Query snuba for tag keys based on projects

        When use_cache is passed, we'll attempt to use the cache. There's an exception if group_id was passed
        which refines the query enough caching isn't required.
        The cache key is based on the filters being passed so that different queries don't hit the same cache, with
        exceptions for start and end dates. Since even a microsecond passing would result in a different caching
        key, which means always missing the cache.
        Instead, to keep the cache key the same for a short period we append the duration, and the end time rounded
        with a certain jitter to the cache key.
        This jitter is based on the hash of the key before duration/end time is added for consistency per query.
        The jitter's intent is to avoid a dogpile effect of many queries being invalidated at the same time.
        This is done by changing the rounding of the end key to a random offset. See snuba.quantize_time for
        further explanation of how that is done.
        """"""
    (default_start, default_end) = default_start_end_dates()
    if start is None:
        start = default_start
    if end is None:
        end = default_end
    dataset = Dataset.Events
    if include_transactions:
        dataset = Dataset.Discover
    conditions = []
    aggregations = [['count()', '', 'count']]
    filters = {'project_id': sorted(projects)}
    if environments:
        filters['environment'] = sorted(environments)
    if group is not None:
        (dataset, conditions, filters) = self.apply_group_filters_conditions(group, conditions, filters)
    if keys is not None:
        filters['tags_key'] = sorted(keys)
    if include_values_seen:
        aggregations.append(['uniq', 'tags_value', 'values_seen'])
    should_cache = use_cache and group is None
    result = None
    cache_key = None
    if should_cache:
        filtering_strings = [f'{key}={value}' for (key, value) in filters.items()]
        filtering_strings.append(f'dataset={dataset.name}')
        cache_key = 'tagstore.__get_tag_keys:{}'.format(md5_text(*filtering_strings).hexdigest())
        key_hash = hash(cache_key)
        duration = (end - start).total_seconds()
        end = snuba.quantize_time(end, key_hash)
        cache_key += f':{duration}@{end.isoformat()}'
        result = cache.get(cache_key, None)
        if result is not None:
            metrics.incr('testing.tagstore.cache_tag_key.hit')
        else:
            metrics.incr('testing.tagstore.cache_tag_key.miss')
    if result is None:
        result = snuba.query(dataset=dataset, start=start, end=end, groupby=['tags_key'], conditions=conditions, filter_keys=filters, aggregations=aggregations, limit=limit, orderby='-count', referrer='tagstore.__get_tag_keys', **kwargs)
        if should_cache:
            cache.set(cache_key, result, 300)
            metrics.incr('testing.tagstore.cache_tag_key.len', amount=len(result))
    if group is None:
        ctor = TagKey
    else:
        ctor = functools.partial(GroupTagKey, group_id=group.id)
    results = set()
    for (key, data) in result.items():
        if denylist is not None and key in denylist:
            continue
        params = {'key': key}
        if include_values_seen:
            params['values_seen'] = data['values_seen']
            params['count'] = data['count']
        else:
            params['count'] = data
        results.add(ctor(**params))
    return results",cache_key = 'tagstore.__get_tag_keys:{}'.format(md5_text(*filtering_strings).hexdigest()),cache_key = f'tagstore.__get_tag_keys:{md5_text(*filtering_strings).hexdigest()}',find_wrong,,1,3,,
sentry,https://github.com/getsentry/sentry/tree/master/src/sentry/tagstore/snuba/backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/tagstore/snuba/backend.py,SnubaTagStorage,"def __get_tag_keys_for_projects(self, projects, group, environments, start, end, limit=1000, keys=None, include_values_seen=True, use_cache=False, include_transactions=False, denylist=None, **kwargs):
    """"""Query snuba for tag keys based on projects

        When use_cache is passed, we'll attempt to use the cache. There's an exception if group_id was passed
        which refines the query enough caching isn't required.
        The cache key is based on the filters being passed so that different queries don't hit the same cache, with
        exceptions for start and end dates. Since even a microsecond passing would result in a different caching
        key, which means always missing the cache.
        Instead, to keep the cache key the same for a short period we append the duration, and the end time rounded
        with a certain jitter to the cache key.
        This jitter is based on the hash of the key before duration/end time is added for consistency per query.
        The jitter's intent is to avoid a dogpile effect of many queries being invalidated at the same time.
        This is done by changing the rounding of the end key to a random offset. See snuba.quantize_time for
        further explanation of how that is done.
        """"""
    (default_start, default_end) = default_start_end_dates()
    if start is None:
        start = default_start
    if end is None:
        end = default_end
    dataset = Dataset.Events
    if include_transactions:
        dataset = Dataset.Discover
    conditions = []
    aggregations = [['count()', '', 'count']]
    filters = {'project_id': sorted(projects)}
    if environments:
        filters['environment'] = sorted(environments)
    if group is not None:
        (dataset, conditions, filters) = self.apply_group_filters_conditions(group, conditions, filters)
    if keys is not None:
        filters['tags_key'] = sorted(keys)
    if include_values_seen:
        aggregations.append(['uniq', 'tags_value', 'values_seen'])
    should_cache = use_cache and group is None
    result = None
    cache_key = None
    if should_cache:
        filtering_strings = [f'{key}={value}' for (key, value) in filters.items()]
        filtering_strings.append(f'dataset={dataset.name}')
        cache_key = 'tagstore.__get_tag_keys:{}'.format(md5_text(*filtering_strings).hexdigest())
        key_hash = hash(cache_key)
        duration = (end - start).total_seconds()
        end = snuba.quantize_time(end, key_hash)
        cache_key += f':{duration}@{end.isoformat()}'
        result = cache.get(cache_key, None)
        if result is not None:
            metrics.incr('testing.tagstore.cache_tag_key.hit')
        else:
            metrics.incr('testing.tagstore.cache_tag_key.miss')
    if result is None:
        result = snuba.query(dataset=dataset, start=start, end=end, groupby=['tags_key'], conditions=conditions, filter_keys=filters, aggregations=aggregations, limit=limit, orderby='-count', referrer='tagstore.__get_tag_keys', **kwargs)
        if should_cache:
            cache.set(cache_key, result, 300)
            metrics.incr('testing.tagstore.cache_tag_key.len', amount=len(result))
    if group is None:
        ctor = TagKey
    else:
        ctor = functools.partial(GroupTagKey, group_id=group.id)
    results = set()
    for (key, data) in result.items():
        if denylist is not None and key in denylist:
            continue
        params = {'key': key}
        if include_values_seen:
            params['values_seen'] = data['values_seen']
            params['count'] = data['count']
        else:
            params['count'] = data
        results.add(ctor(**params))
    return results",filtering_strings.append(f'dataset={dataset.name}'),filtering_strings.append(f'dataset={dataset.name!s}'),find_wrong,2,,,,
bumblebee-status,https://github.com/tobi-wan-kenobi/bumblebee-status/tree/master/bumblebee_status/modules/contrib/sensors.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bumblebee-status/bumblebee_status/modules/contrib/sensors.py,Module,"def _get_temp_from_sensors(self):
    if self._json == True:
        try:
            output = json.loads(util.cli.execute('sensors -j'))
            for key in self.parameter('path').split('/'):
                output = output[key]
            return int(float(output))
        except Exception as e:
            logging.error('unable to read sensors: {}'.format(str(e)))
            return 'unknown'
    else:
        output = util.cli.execute('sensors -u')
        if self._match_pattern:
            temp_pattern = self.parameter('match', 'temp1_input')
            match = re.search('{}.+{}:\\s*([\\d.]+)$'.format(self._match_pattern, temp_pattern), output.replace('\n', ''))
            if match:
                return int(float(match.group(1)))
            else:
                return 'unknown'
        match = self._pattern.findall(output)
        if match:
            return int(float(match[self._match_number]))
    return 'unknown'",'unable to read sensors: {}'.format(str(e)),f'unable to read sensors: {str(e)}',find_wrong,,1,3,,
bumblebee-status,https://github.com/tobi-wan-kenobi/bumblebee-status/tree/master/bumblebee_status/modules/contrib/sensors.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bumblebee-status/bumblebee_status/modules/contrib/sensors.py,Module,"def _get_temp_from_sensors(self):
    if self._json == True:
        try:
            output = json.loads(util.cli.execute('sensors -j'))
            for key in self.parameter('path').split('/'):
                output = output[key]
            return int(float(output))
        except Exception as e:
            logging.error('unable to read sensors: {}'.format(str(e)))
            return 'unknown'
    else:
        output = util.cli.execute('sensors -u')
        if self._match_pattern:
            temp_pattern = self.parameter('match', 'temp1_input')
            match = re.search('{}.+{}:\\s*([\\d.]+)$'.format(self._match_pattern, temp_pattern), output.replace('\n', ''))
            if match:
                return int(float(match.group(1)))
            else:
                return 'unknown'
        match = self._pattern.findall(output)
        if match:
            return int(float(match[self._match_number]))
    return 'unknown'","'{}.+{}:\\s*([\\d.]+)$'.format(self._match_pattern, temp_pattern)",f'{self._match_pattern}.+{temp_pattern}:\\s*([\\d.]+)$',find_wrong,,1,3,,
airflow,https://github.com/apache/airflow/tree/master/airflow/providers/mysql/transfers/vertica_to_mysql.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/providers/mysql/transfers/vertica_to_mysql.py,VerticaToMySqlOperator,"def execute(self, context):
    vertica = VerticaHook(vertica_conn_id=self.vertica_conn_id)
    mysql = MySqlHook(mysql_conn_id=self.mysql_conn_id)
    tmpfile = None
    result = None
    selected_columns = []
    count = 0
    with closing(vertica.get_conn()) as conn:
        with closing(conn.cursor()) as cursor:
            cursor.execute(self.sql)
            selected_columns = [d.name for d in cursor.description]
            if self.bulk_load:
                with NamedTemporaryFile('w') as tmpfile:
                    self.log.info('Selecting rows from Vertica to local file %s...', tmpfile.name)
                    self.log.info(self.sql)
                    csv_writer = csv.writer(tmpfile, delimiter='\t', encoding='utf-8')
                    for row in cursor.iterate():
                        csv_writer.writerow(row)
                        count += 1
                    tmpfile.flush()
            else:
                self.log.info('Selecting rows from Vertica...')
                self.log.info(self.sql)
                result = cursor.fetchall()
                count = len(result)
            self.log.info('Selected rows from Vertica %s', count)
    if self.mysql_preoperator:
        self.log.info('Running MySQL preoperator...')
        mysql.run(self.mysql_preoperator)
    try:
        if self.bulk_load:
            self.log.info('Bulk inserting rows into MySQL...')
            with closing(mysql.get_conn()) as conn:
                with closing(conn.cursor()) as cursor:
                    cursor.execute(f""LOAD DATA LOCAL INFILE '{tmpfile.name}' INTO TABLE {self.mysql_table} LINES TERMINATED BY '\r\n' ({', '.join(selected_columns)})"")
                    conn.commit()
            tmpfile.close()
        else:
            self.log.info('Inserting rows into MySQL...')
            mysql.insert_rows(table=self.mysql_table, rows=result, target_fields=selected_columns)
        self.log.info('Inserted rows into MySQL %s', count)
    except (MySQLdb.Error, MySQLdb.Warning):
        self.log.info('Inserted rows into MySQL 0')
        raise
    if self.mysql_postoperator:
        self.log.info('Running MySQL postoperator...')
        mysql.run(self.mysql_postoperator)
    self.log.info('Done')","f""LOAD DATA LOCAL INFILE '{tmpfile.name}' INTO TABLE {self.mysql_table} LINES TERMINATED BY '\r\n' ({', '.join(selected_columns)})""","f""LOAD DATA LOCAL INFILE '{tmpfile.name}' INTO TABLE {self.mysql_table} LINES TERMINATED BY '\r\n' ({', '.join(selected_columns)})""",find_wrong,2,,,,
jcvi,https://github.com/tanghaibao/jcvi/tree/master/jcvi/variation/delly.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jcvi/jcvi/variation/delly.py,,"def mito(args):
    """"""
    %prog mito chrM.fa input.bam

    Identify mitochondrial deletions.
    """"""
    p = OptionParser(mito.__doc__)
    p.set_aws_opts(store='hli-mv-data-science/htang/mito-deletions')
    p.add_option('--realignonly', default=False, action='store_true', help='Realign only')
    p.add_option('--svonly', default=False, action='store_true', help='Run Realign => SV calls only')
    p.add_option('--support', default=1, type='int', help='Minimum number of supporting reads')
    p.set_home('speedseq', default='/mnt/software/speedseq/bin')
    p.set_cpus()
    (opts, args) = p.parse_args(args)
    if len(args) != 2:
        sys.exit(not p.print_help())
    (chrMfa, bamfile) = args
    store = opts.output_path
    cleanup = not opts.nocleanup
    if not op.exists(chrMfa):
        logging.debug('File `{}` missing. Exiting.'.format(chrMfa))
        return
    chrMfai = chrMfa + '.fai'
    if not op.exists(chrMfai):
        cmd = 'samtools index {}'.format(chrMfa)
        sh(cmd)
    if not bamfile.endswith('.bam'):
        bamfiles = [x.strip() for x in open(bamfile)]
    else:
        bamfiles = [bamfile]
    if store:
        computed = ls_s3(store)
        computed = [op.basename(x).split('.')[0] for x in computed if x.endswith('.depth')]
        remaining_samples = [x for x in bamfiles if op.basename(x).split('.')[0] not in computed]
        logging.debug('Already computed on `{}`: {}'.format(store, len(bamfiles) - len(remaining_samples)))
        bamfiles = remaining_samples
    logging.debug('Total samples: {}'.format(len(bamfiles)))
    for bamfile in bamfiles:
        run_mito(chrMfa, bamfile, opts, realignonly=opts.realignonly, svonly=opts.svonly, store=store, cleanup=cleanup)",'File `{}` missing. Exiting.'.format(chrMfa),f'File `{chrMfa}` missing. Exiting.',find_wrong,,1,3,,
edx-platform,https://github.com/edx/edx-platform/tree/master/openedx/core/djangoapps/programs/tests/test_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/openedx/core/djangoapps/programs/tests/test_tasks.py,UpdateCertificateAvailableDateOnCourseUpdateTestCase,"def test_update_certificate_available_date_with_expect_no_update(self):
    """"""
        This test verifies that we do _not_ queue a task to update the course certificate configuration in Credentials
        if the course-run does not meet the required criteria.
        """"""
    self.credentials_api_config.enabled = True
    self.credentials_api_config.enable_learner_issuance = True
    available_date = datetime.now(pytz.UTC) + timedelta(days=1)
    course = CourseOverviewFactory.create(self_paced=False, certificate_available_date=available_date, certificates_display_behavior=CertificatesDisplayBehaviors.EARLY_NO_INFO)
    expected_message = f'Skipping update of the `certificate_available_date` for course {course.id} in the Credentials service. This course-run does not meet the required criteria for an update.'
    with LogCapture(level=logging.WARNING) as log_capture:
        tasks.update_certificate_available_date_on_course_update(course.id)
    assert len(log_capture.records) == 1
    assert log_capture.records[0].getMessage() == expected_message",expected_message = f'Skipping update of the `certificate_available_date` for course {course.id} in the Credentials service. This course-run does not meet the required criteria for an update.',expected_message = f"Skipping update of the 'certificate_available_date' for course {course.id} in the Credentials service. This course-run does not meet the required criteria for an update.",find_wrong,2,,,,
spiderfoot,https://github.com/smicallef/spiderfoot/tree/master/test/unit/modules/test_sfp_phone.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spiderfoot/test/unit/modules/test_sfp_phone.py,TestModulePhone,"def test_handleEvent_phone_number_event_data_containing_phone_string_should_return_provider_telco_event(self):
    sf = SpiderFoot(self.default_options)
    module = sfp_phone()
    module.setup(sf, dict())
    target_value = 'spiderfoot.net'
    target_type = 'INTERNET_NAME'
    target = SpiderFootTarget(target_value, target_type)
    module.setTarget(target)

    def new_notifyListeners(self, event):
        expected = 'PROVIDER_TELCO'
        if str(event.eventType) != expected:
            raise Exception(f'{event.eventType} != {expected}')
        expected = 'Swisscom'
        if str(event.data) != expected:
            raise Exception(f'{event.data} != {expected}')
        raise Exception('OK')
    module.notifyListeners = new_notifyListeners.__get__(module, sfp_phone)
    event_type = 'ROOT'
    event_data = 'example data'
    event_module = ''
    source_event = ''
    evt = SpiderFootEvent(event_type, event_data, event_module, source_event)
    event_type = 'PHONE_NUMBER'
    event_data = '+41798765432'
    event_module = 'example module'
    source_event = evt
    evt = SpiderFootEvent(event_type, event_data, event_module, source_event)
    with self.assertRaises(Exception) as cm:
        module.handleEvent(evt)
    self.assertEqual('OK', str(cm.exception))",raise Exception(f'{event.eventType} != {expected}'),raise Exception(f'{event.eventType} != {expected}'),find_wrong,2,,,,
napalm,https://github.com/napalm-automation/napalm/tree/master/napalm/base/helpers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/napalm/napalm/base/helpers.py,,"def find_txt(xml_tree, path, default='', namespaces=None):
    """"""
    Extracts the text value from an XML tree, using XPath.
    In case of error or text element unavailability, will return a default value.

    :param xml_tree:   the XML Tree object. Assumed is <type 'lxml.etree._Element'>.
    :param path:       XPath to be applied, in order to extract the desired data.
    :param default:    Value to be returned in case of error.
    :param namespaces: prefix-namespace mappings to process XPath
    :return: a str value.
    """"""
    value = ''
    try:
        xpath_applied = xml_tree.xpath(path, namespaces=namespaces)
        xpath_length = len(xpath_applied)
        if xpath_length and xpath_applied[0] is not None:
            xpath_result = xpath_applied[0]
            if isinstance(xpath_result, type(xml_tree)):
                if xpath_result.text:
                    value = xpath_result.text.strip()
                else:
                    value = default
            else:
                value = xpath_result
        elif xpath_applied == '':
            logger.debug('Unable to find the specified-text-element/XML path: %s in                          the XML tree provided. Total Items in XML tree: %d ' % (path, xpath_length))
    except Exception as findTxtErr01:
        logger.error(findTxtErr01)
        value = default
    return str(value)","Unable to find the specified-text-element/XML path: %s in the XML tree provided. Total Items in XML tree: %d ' % (path, xpath_length)",f'Unable to find the specified-text-element/XML path: {path} in the XML tree provided. Total Items in XML tree: {xpath_length}',find_wrong,2,,,,
cantools,https://github.com/cantools/cantools/tree/master/cantools/database/can/c_source.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cantools/cantools/database/can/c_source.py,,"def _generate_declarations(database_name, messages, floating_point_numbers):
    declarations = []
    for message in messages:
        signal_declarations = []
        for signal in message.signals:
            signal_declaration = ''
            if floating_point_numbers:
                signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declarations.append(signal_declaration)
        declaration = DECLARATION_FMT.format(database_name=database_name, database_message_name=message.name, message_name=message.snake_name)
        if signal_declarations:
            declaration += '\n' + '\n'.join(signal_declarations)
        declarations.append(declaration)
    return '\n'.join(declarations)","SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)",f'{database_name}_{message.snake_name}_{signal.snake_name}_{signal.type_name}_encode_decode',find_wrong,2,,,,
cantools,https://github.com/cantools/cantools/tree/master/cantools/database/can/c_source.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cantools/cantools/database/can/c_source.py,,"def _generate_declarations(database_name, messages, floating_point_numbers):
    declarations = []
    for message in messages:
        signal_declarations = []
        for signal in message.signals:
            signal_declaration = ''
            if floating_point_numbers:
                signal_declaration = SIGNAL_DECLARATION_ENCODE_DECODE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declaration += SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)
            signal_declarations.append(signal_declaration)
        declaration = DECLARATION_FMT.format(database_name=database_name, database_message_name=message.name, message_name=message.snake_name)
        if signal_declarations:
            declaration += '\n' + '\n'.join(signal_declarations)
        declarations.append(declaration)
    return '\n'.join(declarations)","SIGNAL_DECLARATION_IS_IN_RANGE_FMT.format(database_name=database_name, message_name=message.snake_name, signal_name=signal.snake_name, type_name=signal.type_name)",f'{database_name}_{message.snake_name}_{signal.snake_name}_{signal.type_name}_is_in_range',find_wrong,2,,,,
mitmproxy,https://github.com/mitmproxy/mitmproxy/tree/master/mitmproxy/addons/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mitmproxy/mitmproxy/addons/core.py,Core,"def revert(self, flows: typing.Sequence[flow.Flow]) -> None:
    """"""
            Revert flow changes.
        """"""
    updated = []
    for f in flows:
        if f.modified():
            f.revert()
            updated.append(f)
    ctx.log.alert('Reverted %s flows.' % len(updated))
    ctx.master.addons.trigger(hooks.UpdateHook(updated))",ctx.log.alert('Reverted %s flows.' % len(updated)),ctx.log.alert(f'Reverted {len(updated)} flows.'),find_wrong,,,,,
Pinout.xyz,https://github.com/Gadgetoid/Pinout.xyz/tree/master//generate-json.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pinout.xyz//generate-json.py,,"def load_md(filename):
    filename = 'src/{}/{}'.format(lang, filename)
    try:
        html = markdown.markdown(open(filename).read(), extensions=['fenced_code'])
        return html
    except IOError:
        print('Unable to load markdown from {}'.format(filename))
        return ''",'Unable to load markdown from {}'.format(filename),f'Unable to load markdown from {filename}',find_wrong,,1,3,,
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/deploy/emulator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/deploy/emulator.py,VirtualBoxEmulator,"def serial(self):
    """"""
        Returns:
            list[str]: Such as ['127.0.0.1:62001', '127.0.0.1:62025']
        """"""
    vbox = []
    for (path, folders, files) in os.walk(os.path.join(self.root, self.vbox_path)):
        for file in files:
            if re.match(self.vbox_name, file):
                file = os.path.join(path, file)
                vbox.append(file)
    serial = []
    for file in vbox:
        with open(file, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f.readlines():
                res = re.search('<*?hostport=""(.*?)"".*?guestport=""5555""/>', line)
                if res:
                    serial.append(f'127.0.0.1:{res.group(1)}')
    return serial",serial.append(f'127.0.0.1:{res.group(1)}'),serial.append(f'127.0.0.1:{res.group(1)}'),find_wrong,2,,,,
webterminal,https://github.com/jimmy201602/webterminal/tree/master/elfinder/views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/webterminal/elfinder/views.py,ElfinderConnectorView,"def post(self, request, *args, **kwargs):
    """"""
        called in post method calls.
        It only allows for the 'upload' command
        """"""
    u_id = str(uuid.uuid4())
    kwargs['u_id'] = u_id
    loginuser = kwargs.get('loginuser', None)
    if kwargs['optionset'] == 'sftp':
        server_object = get_object_or_404(ServerInfor, id=kwargs['start_path'])
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}-{1}'.format(server_object.name, server_object.ip)
        key_label = '%s::%s' % (server_object.ip, loginuser)
        port = None
        method = None
        key = None
        password = None
        for credential in server_object.credentials.all():
            if credential.username == loginuser:
                port = credential.port
                method = credential.method
                if method == 'password':
                    password = credential.password
                else:
                    password = credential.password
                    key = credential.key
        if method == 'password':
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'password': password, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        else:
            private_key = StringIO(key)
            if 'RSA' in key:
                private_key = paramiko.RSAKey.from_private_key(private_key, password=password)
            elif 'DSA' in key:
                private_key = paramiko.DSSKey.from_private_key(private_key, password=password)
            elif 'EC' in key:
                private_key = paramiko.ECDSAKey.from_private_key(private_key, password=password)
            elif 'OPENSSH' in key:
                private_key = paramiko.Ed25519Key.from_private_key(private_key, password=password)
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'pkey': private_key, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    else:
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}_tmp_dir'.format(request.user.username)
        optinon_sets['roots'][u_id][0]['path'] = os.path.join(settings.MEDIA_ROOT, request.user.username, 'Download')
        optinon_sets['roots'][u_id][0]['URL'] = '{0}{1}/{2}/'.format(settings.MEDIA_URL, request.user.username, 'Download')
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    cmd = self.get_command(request.POST)
    if not cmd in ['upload']:
        self.render_to_response({'error': self.elfinder.error(ElfinderErrorMessages.ERROR_UPLOAD, ElfinderErrorMessages.ERROR_UPLOAD_TOTAL_SIZE)})
    return self.output(cmd, request.POST)","'{0}-{1}'.format(server_object.name, server_object.ip)",f'{server_object.name}-{server_object.ip}',find_wrong,,1,3,,
webterminal,https://github.com/jimmy201602/webterminal/tree/master/elfinder/views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/webterminal/elfinder/views.py,ElfinderConnectorView,"def post(self, request, *args, **kwargs):
    """"""
        called in post method calls.
        It only allows for the 'upload' command
        """"""
    u_id = str(uuid.uuid4())
    kwargs['u_id'] = u_id
    loginuser = kwargs.get('loginuser', None)
    if kwargs['optionset'] == 'sftp':
        server_object = get_object_or_404(ServerInfor, id=kwargs['start_path'])
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}-{1}'.format(server_object.name, server_object.ip)
        key_label = '%s::%s' % (server_object.ip, loginuser)
        port = None
        method = None
        key = None
        password = None
        for credential in server_object.credentials.all():
            if credential.username == loginuser:
                port = credential.port
                method = credential.method
                if method == 'password':
                    password = credential.password
                else:
                    password = credential.password
                    key = credential.key
        if method == 'password':
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'password': password, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        else:
            private_key = StringIO(key)
            if 'RSA' in key:
                private_key = paramiko.RSAKey.from_private_key(private_key, password=password)
            elif 'DSA' in key:
                private_key = paramiko.DSSKey.from_private_key(private_key, password=password)
            elif 'EC' in key:
                private_key = paramiko.ECDSAKey.from_private_key(private_key, password=password)
            elif 'OPENSSH' in key:
                private_key = paramiko.Ed25519Key.from_private_key(private_key, password=password)
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'pkey': private_key, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    else:
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}_tmp_dir'.format(request.user.username)
        optinon_sets['roots'][u_id][0]['path'] = os.path.join(settings.MEDIA_ROOT, request.user.username, 'Download')
        optinon_sets['roots'][u_id][0]['URL'] = '{0}{1}/{2}/'.format(settings.MEDIA_URL, request.user.username, 'Download')
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    cmd = self.get_command(request.POST)
    if not cmd in ['upload']:
        self.render_to_response({'error': self.elfinder.error(ElfinderErrorMessages.ERROR_UPLOAD, ElfinderErrorMessages.ERROR_UPLOAD_TOTAL_SIZE)})
    return self.output(cmd, request.POST)","%s::%s' % (server_object.ip, loginuser)",f'{server_object.ip}::{loginuser}',find_wrong,,1,3,1,
webterminal,https://github.com/jimmy201602/webterminal/tree/master/elfinder/views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/webterminal/elfinder/views.py,ElfinderConnectorView,"def post(self, request, *args, **kwargs):
    """"""
        called in post method calls.
        It only allows for the 'upload' command
        """"""
    u_id = str(uuid.uuid4())
    kwargs['u_id'] = u_id
    loginuser = kwargs.get('loginuser', None)
    if kwargs['optionset'] == 'sftp':
        server_object = get_object_or_404(ServerInfor, id=kwargs['start_path'])
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}-{1}'.format(server_object.name, server_object.ip)
        key_label = '%s::%s' % (server_object.ip, loginuser)
        port = None
        method = None
        key = None
        password = None
        for credential in server_object.credentials.all():
            if credential.username == loginuser:
                port = credential.port
                method = credential.method
                if method == 'password':
                    password = credential.password
                else:
                    password = credential.password
                    key = credential.key
        if method == 'password':
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'password': password, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        else:
            private_key = StringIO(key)
            if 'RSA' in key:
                private_key = paramiko.RSAKey.from_private_key(private_key, password=password)
            elif 'DSA' in key:
                private_key = paramiko.DSSKey.from_private_key(private_key, password=password)
            elif 'EC' in key:
                private_key = paramiko.ECDSAKey.from_private_key(private_key, password=password)
            elif 'OPENSSH' in key:
                private_key = paramiko.Ed25519Key.from_private_key(private_key, password=password)
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'pkey': private_key, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    else:
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}_tmp_dir'.format(request.user.username)
        optinon_sets['roots'][u_id][0]['path'] = os.path.join(settings.MEDIA_ROOT, request.user.username, 'Download')
        optinon_sets['roots'][u_id][0]['URL'] = '{0}{1}/{2}/'.format(settings.MEDIA_URL, request.user.username, 'Download')
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    cmd = self.get_command(request.POST)
    if not cmd in ['upload']:
        self.render_to_response({'error': self.elfinder.error(ElfinderErrorMessages.ERROR_UPLOAD, ElfinderErrorMessages.ERROR_UPLOAD_TOTAL_SIZE)})
    return self.output(cmd, request.POST)",'{0}_tmp_dir'.format(request.user.username),f'{request.user.username}_tmp_dir',find_wrong,,1,3,,
webterminal,https://github.com/jimmy201602/webterminal/tree/master/elfinder/views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/webterminal/elfinder/views.py,ElfinderConnectorView,"def post(self, request, *args, **kwargs):
    """"""
        called in post method calls.
        It only allows for the 'upload' command
        """"""
    u_id = str(uuid.uuid4())
    kwargs['u_id'] = u_id
    loginuser = kwargs.get('loginuser', None)
    if kwargs['optionset'] == 'sftp':
        server_object = get_object_or_404(ServerInfor, id=kwargs['start_path'])
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}-{1}'.format(server_object.name, server_object.ip)
        key_label = '%s::%s' % (server_object.ip, loginuser)
        port = None
        method = None
        key = None
        password = None
        for credential in server_object.credentials.all():
            if credential.username == loginuser:
                port = credential.port
                method = credential.method
                if method == 'password':
                    password = credential.password
                else:
                    password = credential.password
                    key = credential.key
        if method == 'password':
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'password': password, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        else:
            private_key = StringIO(key)
            if 'RSA' in key:
                private_key = paramiko.RSAKey.from_private_key(private_key, password=password)
            elif 'DSA' in key:
                private_key = paramiko.DSSKey.from_private_key(private_key, password=password)
            elif 'EC' in key:
                private_key = paramiko.ECDSAKey.from_private_key(private_key, password=password)
            elif 'OPENSSH' in key:
                private_key = paramiko.Ed25519Key.from_private_key(private_key, password=password)
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'pkey': private_key, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    else:
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}_tmp_dir'.format(request.user.username)
        optinon_sets['roots'][u_id][0]['path'] = os.path.join(settings.MEDIA_ROOT, request.user.username, 'Download')
        optinon_sets['roots'][u_id][0]['URL'] = '{0}{1}/{2}/'.format(settings.MEDIA_URL, request.user.username, 'Download')
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    cmd = self.get_command(request.POST)
    if not cmd in ['upload']:
        self.render_to_response({'error': self.elfinder.error(ElfinderErrorMessages.ERROR_UPLOAD, ElfinderErrorMessages.ERROR_UPLOAD_TOTAL_SIZE)})
    return self.output(cmd, request.POST)","{0}{1}/{2}/'.format(settings.MEDIA_URL, request.user.username, 'Download')",f'{settings.MEDIA_URL}{request.user.username}/Download/',find_wrong,,1,3,,
keystone,https://github.com/openstack/keystone/tree/master/keystone/tests/unit/test_v3_resource.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keystone/keystone/tests/unit/test_v3_resource.py,ResourceTestCase,"def test_get_project_with_subtree_as_list_and_subtree_as_ids(self):
    """"""Attempt to get a project subtree as both a list and as IDs.

        This uses ``GET /projects/{project_id}?subtree_as_list&subtree_as_ids``
        which should fail with a bad request due to the conflicting query
        strings.

        """"""
    projects = self._create_projects_hierarchy(hierarchy_size=2)
    self.get('/projects/%(project_id)s?subtree_as_list&subtree_as_ids' % {'project_id': projects[1]['project']['id']}, expected_status=http.client.BAD_REQUEST)","self.get('/projects/%(project_id)s?subtree_as_list&subtree_as_ids' % {'project_id': projects[1]['project']['id']}, expected_status=http.client.BAD_REQUEST)","self.get(f""/projects/{projects[1]['project']['id']}?subtree_as_list&subtree_as_ids"", expected_status=http.client.BAD_REQUEST)",find_wrong,,,,,
pyNES,https://github.com/gutomaia/pyNES/tree/master/pynes/tests/image_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyNES/pynes/tests/image_test.py,ImageTest,"def test_export_namespace(self):
    try:
        os.remove('/tmp/level.png')
    except:
        pass
    self.assertFileNotExists('/tmp/level.png')
    image.export_nametable('fixtures/nerdynights/scrolling/SMBlevel.bin', 'fixtures/nerdynights/scrolling/mario.chr', '/tmp/level.png')
    self.assertFileExists('/tmp/level.png')
    img = Image.open('/tmp/level.png')
    (sprs, indexes) = image.acquire_chr(img, optimize_repeated=False)
    sprite.length(sprs)
    self.assertEquals(1024, sprite.length(sprs))
    return
    nt_file = open('fixtures/nerdynights/scrolling/SMBlevel.bin')
    nt = nt_file.read()
    nt_file.close()
    nts = [ord(n) for n in nt]
    mario = sprite.load_sprites('fixtures/nerdynights/scrolling/mario.chr')
    for i in range(32):
        for j in range(32):
            self.assertSpriteEquals(sprite.get_sprite(nts[i * j] + 256, mario), sprite.get_sprite(i * j, sprs))
    os.remove('/tmp/level.png')","""""""/tmp/level.png""""""","""""""/tmp/level.png""""""",find_wrong,2,,,,
GPT2-chitchat,https://github.com/yangjianxin1/GPT2-chitchat/tree/master//train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GPT2-chitchat//train.py,,"def train(model, logger, train_dataset, validate_dataset, args):
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn, drop_last=True)
    validate_dataloader = DataLoader(validate_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn, drop_last=True)
    early_stopping = EarlyStopping(args.patience, verbose=True, save_path=args.save_model_path)
    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.epochs
    optimizer = transformers.AdamW(model.parameters(), lr=args.lr, eps=args.eps)
    scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)
    logger.info('starting training')
    (train_losses, validate_losses) = ([], [])
    best_val_loss = 10000
    for epoch in range(args.epochs):
        train_loss = train_epoch(model=model, train_dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler, logger=logger, epoch=epoch, args=args)
        train_losses.append(train_loss)
        validate_loss = validate_epoch(model=model, validate_dataloader=validate_dataloader, logger=logger, epoch=epoch, args=args)
        validate_losses.append(validate_loss)
        if validate_loss < best_val_loss:
            best_val_loss = validate_loss
            logger.info('saving current best model for epoch {}'.format(epoch + 1))
            model_path = join(args.save_model_path, 'min_ppl_model'.format(epoch + 1))
            if not os.path.exists(model_path):
                os.mkdir(model_path)
            model_to_save = model.module if hasattr(model, 'module') else model
            model_to_save.save_pretrained(model_path)
        if args.patience == 0:
            continue
        early_stopping(validate_loss, model)
        if early_stopping.early_stop:
            logger.info('Early stopping')
            break
    logger.info('training finished')
    logger.info('train_losses:{}'.format(train_losses))
    logger.info('validate_losses:{}'.format(validate_losses))",'saving current best model for epoch {}'.format(epoch + 1),f'saving current best model for epoch {epoch + 1}',find_wrong,,1,3,,
GPT2-chitchat,https://github.com/yangjianxin1/GPT2-chitchat/tree/master//train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GPT2-chitchat//train.py,,"def train(model, logger, train_dataset, validate_dataset, args):
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn, drop_last=True)
    validate_dataloader = DataLoader(validate_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn, drop_last=True)
    early_stopping = EarlyStopping(args.patience, verbose=True, save_path=args.save_model_path)
    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.epochs
    optimizer = transformers.AdamW(model.parameters(), lr=args.lr, eps=args.eps)
    scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)
    logger.info('starting training')
    (train_losses, validate_losses) = ([], [])
    best_val_loss = 10000
    for epoch in range(args.epochs):
        train_loss = train_epoch(model=model, train_dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler, logger=logger, epoch=epoch, args=args)
        train_losses.append(train_loss)
        validate_loss = validate_epoch(model=model, validate_dataloader=validate_dataloader, logger=logger, epoch=epoch, args=args)
        validate_losses.append(validate_loss)
        if validate_loss < best_val_loss:
            best_val_loss = validate_loss
            logger.info('saving current best model for epoch {}'.format(epoch + 1))
            model_path = join(args.save_model_path, 'min_ppl_model'.format(epoch + 1))
            if not os.path.exists(model_path):
                os.mkdir(model_path)
            model_to_save = model.module if hasattr(model, 'module') else model
            model_to_save.save_pretrained(model_path)
        if args.patience == 0:
            continue
        early_stopping(validate_loss, model)
        if early_stopping.early_stop:
            logger.info('Early stopping')
            break
    logger.info('training finished')
    logger.info('train_losses:{}'.format(train_losses))
    logger.info('validate_losses:{}'.format(validate_losses))",'train_losses:{}'.format(train_losses),f'train_losses:{train_losses}',find_wrong,,1,3,,
GPT2-chitchat,https://github.com/yangjianxin1/GPT2-chitchat/tree/master//train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GPT2-chitchat//train.py,,"def train(model, logger, train_dataset, validate_dataset, args):
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn, drop_last=True)
    validate_dataloader = DataLoader(validate_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn, drop_last=True)
    early_stopping = EarlyStopping(args.patience, verbose=True, save_path=args.save_model_path)
    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.epochs
    optimizer = transformers.AdamW(model.parameters(), lr=args.lr, eps=args.eps)
    scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)
    logger.info('starting training')
    (train_losses, validate_losses) = ([], [])
    best_val_loss = 10000
    for epoch in range(args.epochs):
        train_loss = train_epoch(model=model, train_dataloader=train_dataloader, optimizer=optimizer, scheduler=scheduler, logger=logger, epoch=epoch, args=args)
        train_losses.append(train_loss)
        validate_loss = validate_epoch(model=model, validate_dataloader=validate_dataloader, logger=logger, epoch=epoch, args=args)
        validate_losses.append(validate_loss)
        if validate_loss < best_val_loss:
            best_val_loss = validate_loss
            logger.info('saving current best model for epoch {}'.format(epoch + 1))
            model_path = join(args.save_model_path, 'min_ppl_model'.format(epoch + 1))
            if not os.path.exists(model_path):
                os.mkdir(model_path)
            model_to_save = model.module if hasattr(model, 'module') else model
            model_to_save.save_pretrained(model_path)
        if args.patience == 0:
            continue
        early_stopping(validate_loss, model)
        if early_stopping.early_stop:
            logger.info('Early stopping')
            break
    logger.info('training finished')
    logger.info('train_losses:{}'.format(train_losses))
    logger.info('validate_losses:{}'.format(validate_losses))",'validate_losses:{}'.format(validate_losses),f'validate_losses:{validate_losses}',find_wrong,,1,3,,
oppia,https://github.com/oppia/oppia/tree/master/core/domain/rte_component_registry_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/rte_component_registry_test.py,RteComponentUnitTests,"def test_image_thumbnails_for_rte_components(self) -> None:
    """"""Test the thumbnails for the RTE component icons.""""""
    rte_components = rte_component_registry.Registry.get_all_rte_components()
    for (component_name, component_specs) in rte_components.items():
        generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
        relative_icon_data_url = component_specs['icon_data_url'][1:]
        defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
        self.assertEqual(generated_image_filepath, defined_image_filepath)
        with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
            img_data = f.read()
            (width, height) = struct.unpack('>LL', img_data[16:24])
            self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
            self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)","os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)","os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, f'{component_name}.png')",find_wrong,2,,,,
oppia,https://github.com/oppia/oppia/tree/master/core/domain/rte_component_registry_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/rte_component_registry_test.py,RteComponentUnitTests,"def test_image_thumbnails_for_rte_components(self) -> None:
    """"""Test the thumbnails for the RTE component icons.""""""
    rte_components = rte_component_registry.Registry.get_all_rte_components()
    for (component_name, component_specs) in rte_components.items():
        generated_image_filepath = os.path.join(os.getcwd(), feconf.RTE_EXTENSIONS_DIR, component_name, '%s.png' % component_name)
        relative_icon_data_url = component_specs['icon_data_url'][1:]
        defined_image_filepath = os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)
        self.assertEqual(generated_image_filepath, defined_image_filepath)
        with utils.open_file(generated_image_filepath, 'rb', encoding=None) as f:
            img_data = f.read()
            (width, height) = struct.unpack('>LL', img_data[16:24])
            self.assertEqual(int(width), RTE_THUMBNAIL_WIDTH_PX)
            self.assertEqual(int(height), RTE_THUMBNAIL_HEIGHT_PX)","os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', relative_icon_data_url)","os.path.join(os.getcwd(), feconf.EXTENSIONS_DIR_PREFIX, 'extensions', f'{relative_icon_data_url}')",find_wrong,2,,,,
ansible-modules-core,https://github.com/ansible/ansible-modules-core/tree/master/network/netvisor/pn_vlag.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-core/network/netvisor/pn_vlag.py,,"def pn_cli(module):
    """"""
    This method is to generate the cli portion to launch the Netvisor cli.
    It parses the username, password, switch parameters from module.
    :param module: The Ansible module to fetch username, password and switch
    :return: returns the cli string for further processing
    """"""
    username = module.params['pn_cliusername']
    password = module.params['pn_clipassword']
    cliswitch = module.params['pn_cliswitch']
    if username and password:
        cli = '/usr/bin/cli --quiet --user %s:%s ' % (username, password)
    else:
        cli = '/usr/bin/cli --quiet '
    if cliswitch == 'local':
        cli += ' switch-local '
    else:
        cli += ' switch ' + cliswitch
    return cli",cli += ' switch ' + cliswitch,cli += f' switch {cliswitch}',find_wrong,,1,3,,
maltrail,https://github.com/stamparm/maltrail/tree/master/trails/feeds/360necurs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/maltrail/trails/feeds/360necurs.py,,"def fetch():
    retval = {}
    content = retrieve_content(__url__)
    if __check__ in content:
        for match in re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content):
            retval[match.group(1)] = (__info__, __reference__)
    return retval","""""""(?m)^([\\w.-]+)\\s+2\\d{3}\\-""""""",f'(?m)^([\\w.-]+)\\s+2\\d{{3}}-',find_wrong,2,,,,
justpy,https://github.com/elimintz/justpy/tree/master/justpy/htmlcomponents.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/justpy/justpy/htmlcomponents.py,Div,"def to_html(self, indent=0, indent_step=0, format=True):
    block_indent = ' ' * indent
    if format:
        ws = '\n'
    else:
        ws = ''
    s = f'{block_indent}<{self.html_tag} '
    d = self.convert_object_to_dict()
    for (attr, value) in d['attrs'].items():
        if value:
            s = f'{s}{attr}=""{value}"" '
    if self.style:
        s = f'{s}style=""{self.style}""'
    if self.classes:
        s = f'{s}class=""{self.classes}"">{ws}'
    else:
        s = f'{s}>{ws}'
    if self.inner_html:
        s = f'{s}{self.inner_html}</{self.html_tag}>{ws}'
        return s
    try:
        s = f'{s}{self.text}{ws}'
    except:
        pass
    for c in self.components:
        s = f'{s}{c.to_html(indent + indent_step, indent_step, format)}'
    s = f'{s}{block_indent}</{self.html_tag}>{ws}'
    return s","s = f'{block_indent}<{self.html_tag} '
s = f'{s}{attr}=""{value}"" '
s = f'{s}style=""{self.style}""'
s = f'{s}class=""{self.classes}"">{ws}'
s = f'{s}{self.inner_html}</{self.html_tag}>{ws}'
s = f'{s}{self.text}{ws}'
s = f'{s}{c.to_html(indent + indent_step, indent_step, format)}'
s = f'{s}{block_indent}</{self.html_tag}>{ws}'","s = f'{block_indent}<{self.html_tag} {"" "".join([f""{attr}=\""{value}\"""" for attr, value in d[""attrs""].items() if value])}'
s = f'{s} {""style=\""{self.style}\"""" if self.style else """"}'
s = f'{s} {""class=\""{self.classes}\"""" if self.classes else """"}>{ws}'
s = f'{s}{self.inner_html}</{self.html_tag}>{ws}' if self.inner_html else f'{s}{self.text}{ws}'
s = f'{s}{"""".join([c.to_html(indent + indent_step, indent_step, format) for c in self.components])}'
s = f'{s}{block_indent}</{self.html_tag}>{ws}'",find_wrong,2,,,,
pyannote-audio,https://github.com/pyannote/pyannote-audio/tree/master/pyannote/audio/_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyannote-audio/pyannote/audio/_version.py,,"def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):
    if not os.path.exists(os.path.join(root, '.git')):
        if verbose:
            print('no .git in %s' % root)
        raise NotThisMethod('no .git directory')
    GITS = ['git']
    if sys.platform == 'win32':
        GITS = ['git.cmd', 'git.exe']
    describe_out = run_command(GITS, ['describe', '--tags', '--dirty', '--always', '--long'], cwd=root)
    if describe_out is None:
        raise NotThisMethod(""'git describe' failed"")
    describe_out = describe_out.strip()
    full_out = run_command(GITS, ['rev-parse', 'HEAD'], cwd=root)
    if full_out is None:
        raise NotThisMethod(""'git rev-parse' failed"")
    full_out = full_out.strip()
    pieces = {}
    pieces['long'] = full_out
    pieces['short'] = full_out[:7]
    pieces['error'] = None
    git_describe = describe_out
    dirty = git_describe.endswith('-dirty')
    pieces['dirty'] = dirty
    if dirty:
        git_describe = git_describe[:git_describe.rindex('-dirty')]
    if '-' in git_describe:
        mo = re.search('^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)
        if not mo:
            pieces['error'] = ""unable to parse git-describe output: '%s'"" % describe_out
            return pieces
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = ""tag '%s' doesn't start with prefix '%s'""
                print(fmt % (full_tag, tag_prefix))
            pieces['error'] = ""tag '%s' doesn't start with prefix '%s'"" % (full_tag, tag_prefix)
            return pieces
        pieces['closest-tag'] = full_tag[len(tag_prefix):]
        pieces['distance'] = int(mo.group(2))
        pieces['short'] = mo.group(3)
    else:
        pieces['closest-tag'] = None
        count_out = run_command(GITS, ['rev-list', 'HEAD', '--count'], cwd=root)
        pieces['distance'] = int(count_out)
    return pieces","fmt % (full_tag, tag_prefix)",f"{tag_prefix} doesn't start with prefix '{full_tag}'",find_wrong,,-1,3,1,
pyannote-audio,https://github.com/pyannote/pyannote-audio/tree/master/pyannote/audio/_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyannote-audio/pyannote/audio/_version.py,,"def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):
    if not os.path.exists(os.path.join(root, '.git')):
        if verbose:
            print('no .git in %s' % root)
        raise NotThisMethod('no .git directory')
    GITS = ['git']
    if sys.platform == 'win32':
        GITS = ['git.cmd', 'git.exe']
    describe_out = run_command(GITS, ['describe', '--tags', '--dirty', '--always', '--long'], cwd=root)
    if describe_out is None:
        raise NotThisMethod(""'git describe' failed"")
    describe_out = describe_out.strip()
    full_out = run_command(GITS, ['rev-parse', 'HEAD'], cwd=root)
    if full_out is None:
        raise NotThisMethod(""'git rev-parse' failed"")
    full_out = full_out.strip()
    pieces = {}
    pieces['long'] = full_out
    pieces['short'] = full_out[:7]
    pieces['error'] = None
    git_describe = describe_out
    dirty = git_describe.endswith('-dirty')
    pieces['dirty'] = dirty
    if dirty:
        git_describe = git_describe[:git_describe.rindex('-dirty')]
    if '-' in git_describe:
        mo = re.search('^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)
        if not mo:
            pieces['error'] = ""unable to parse git-describe output: '%s'"" % describe_out
            return pieces
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = ""tag '%s' doesn't start with prefix '%s'""
                print(fmt % (full_tag, tag_prefix))
            pieces['error'] = ""tag '%s' doesn't start with prefix '%s'"" % (full_tag, tag_prefix)
            return pieces
        pieces['closest-tag'] = full_tag[len(tag_prefix):]
        pieces['distance'] = int(mo.group(2))
        pieces['short'] = mo.group(3)
    else:
        pieces['closest-tag'] = None
        count_out = run_command(GITS, ['rev-list', 'HEAD', '--count'], cwd=root)
        pieces['distance'] = int(count_out)
    return pieces","fmt % (full_tag, tag_prefix)",f"{tag_prefix} doesn't start with prefix '{full_tag}'",find_wrong,,-1,3,1,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/tools/sampcd_processor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/tools/sampcd_processor.py,,"def insert_codes_into_codeblock(codeblock, apiname='not-specified'):
    """"""
    insert some codes in the frontend and backend into the code-block.
    """"""
    global ENV_KEY_CODES_FRONTEND, GPU_ID, RUN_ON_DEVICE
    inserted_codes_f = ''
    inserted_codes_b = ''
    if ENV_KEY_CODES_FRONTEND in os.environ and os.environ[ENV_KEY_CODES_FRONTEND]:
        inserted_codes_f = os.environ[ENV_KEY_CODES_FRONTEND]
    else:
        cpu_str = '\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = """"\n'
        gpu_str = '\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""{}""\n'.format(GPU_ID)
        if 'required' in codeblock and codeblock['required']:
            if codeblock['required'] == 'cpu':
                inserted_codes_f = cpu_str
            elif codeblock['required'] == 'gpu':
                inserted_codes_f = gpu_str
        elif RUN_ON_DEVICE == 'cpu':
            inserted_codes_f = cpu_str
        elif RUN_ON_DEVICE == 'gpu':
            inserted_codes_f = gpu_str
    inserted_codes_b = '\nprint(""{}\'s sample code (name:{}, id:{}) is executed successfully!"")'.format(apiname, codeblock['name'], codeblock['id'])
    cb = codeblock['codes']
    last_future_line_end = find_last_future_line_end(cb)
    if last_future_line_end:
        return cb[:last_future_line_end] + inserted_codes_f + cb[last_future_line_end:] + inserted_codes_b
    else:
        return inserted_codes_f + cb + inserted_codes_b",'\nimport os\nos.environ["CUDA_VISIBLE_DEVICES"] = "{}"\n'.format(GPU_ID),f'\nimport os\nos.environ["CUDA_VISIBLE_DEVICES"] = "{GPU_ID}"\n',find_wrong,,1,3,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/tools/sampcd_processor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/tools/sampcd_processor.py,,"def insert_codes_into_codeblock(codeblock, apiname='not-specified'):
    """"""
    insert some codes in the frontend and backend into the code-block.
    """"""
    global ENV_KEY_CODES_FRONTEND, GPU_ID, RUN_ON_DEVICE
    inserted_codes_f = ''
    inserted_codes_b = ''
    if ENV_KEY_CODES_FRONTEND in os.environ and os.environ[ENV_KEY_CODES_FRONTEND]:
        inserted_codes_f = os.environ[ENV_KEY_CODES_FRONTEND]
    else:
        cpu_str = '\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = """"\n'
        gpu_str = '\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""{}""\n'.format(GPU_ID)
        if 'required' in codeblock and codeblock['required']:
            if codeblock['required'] == 'cpu':
                inserted_codes_f = cpu_str
            elif codeblock['required'] == 'gpu':
                inserted_codes_f = gpu_str
        elif RUN_ON_DEVICE == 'cpu':
            inserted_codes_f = cpu_str
        elif RUN_ON_DEVICE == 'gpu':
            inserted_codes_f = gpu_str
    inserted_codes_b = '\nprint(""{}\'s sample code (name:{}, id:{}) is executed successfully!"")'.format(apiname, codeblock['name'], codeblock['id'])
    cb = codeblock['codes']
    last_future_line_end = find_last_future_line_end(cb)
    if last_future_line_end:
        return cb[:last_future_line_end] + inserted_codes_f + cb[last_future_line_end:] + inserted_codes_b
    else:
        return inserted_codes_f + cb + inserted_codes_b","'\nprint(""{}\'s sample code (name:{}, id:{}) is executed successfully!"")'.format(apiname, codeblock['name'], codeblock['id'])","f'''\nprint(""{apiname}'s sample code (name:{codeblock['name']}, id:{codeblock['id']}) is executed successfully!"")'''",find_wrong,,1,3,,
FedML,https://github.com/FedML-AI/FedML/tree/master/fedml_api/model/cv/efficientnet_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FedML/fedml_api/model/cv/efficientnet_utils.py,BlockDecoder,"def _encode_block_string(block):
    """"""Encode a block to a string.
        Args:
            block (namedtuple): A BlockArgs type argument.
        Returns:
            block_string: A String form of BlockArgs.
        """"""
    args = ['r%d' % block.num_repeat, 'k%d' % block.kernel_size, 's%d%d' % (block.strides[0], block.strides[1]), 'e%s' % block.expand_ratio, 'i%d' % block.input_filters, 'o%d' % block.output_filters]
    if 0 < block.se_ratio <= 1:
        args.append('se%s' % block.se_ratio)
    if block.id_skip is False:
        args.append('noskip')
    return '_'.join(args)","""""""noskip""""""","""""""noskip""""""",find_wrong,2,,,,
nnFormer,https://github.com/282857341/nnFormer/tree/master/nnformer/preprocessing/preprocessing.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nnFormer/nnformer/preprocessing/preprocessing.py,GenericPreprocessor,"def resample_and_normalize(self, data, target_spacing, properties, seg=None, force_separate_z=None):
    """"""
        data and seg must already have been transposed by transpose_forward. properties are the un-transposed values
        (spacing etc)
        :param data:
        :param target_spacing:
        :param properties:
        :param seg:
        :param force_separate_z:
        :return:
        """"""
    original_spacing_transposed = np.array(properties['original_spacing'])[self.transpose_forward]
    before = {'spacing': properties['original_spacing'], 'spacing_transposed': original_spacing_transposed, 'data.shape (data is transposed)': data.shape}
    data[np.isnan(data)] = 0
    (data, seg) = resample_patient(data, seg, np.array(original_spacing_transposed), target_spacing, 3, 1, force_separate_z=force_separate_z, order_z_data=0, order_z_seg=0, separate_z_anisotropy_threshold=self.resample_separate_z_anisotropy_threshold)
    after = {'spacing': target_spacing, 'data.shape (data is resampled)': data.shape}
    print('before:', before, '\nafter: ', after, '\n')
    if seg is not None:
        seg[seg < -1] = 0
    properties['size_after_resampling'] = data[0].shape
    properties['spacing_after_resampling'] = target_spacing
    use_nonzero_mask = self.use_nonzero_mask
    assert len(self.normalization_scheme_per_modality) == len(data), 'self.normalization_scheme_per_modality must have as many entries as data has modalities'
    assert len(self.use_nonzero_mask) == len(data), 'self.use_nonzero_mask must have as many entries as data has modalities'
    for c in range(len(data)):
        scheme = self.normalization_scheme_per_modality[c]
        if scheme == 'CT':
            assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
            mean_intensity = self.intensityproperties[c]['mean']
            std_intensity = self.intensityproperties[c]['sd']
            lower_bound = self.intensityproperties[c]['percentile_00_5']
            upper_bound = self.intensityproperties[c]['percentile_99_5']
            data[c] = np.clip(data[c], lower_bound, upper_bound)
            data[c] = (data[c] - mean_intensity) / std_intensity
            if use_nonzero_mask[c]:
                data[c][seg[-1] < 0] = 0
        elif scheme == 'CT2':
            assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
            lower_bound = self.intensityproperties[c]['percentile_00_5']
            upper_bound = self.intensityproperties[c]['percentile_99_5']
            mask = (data[c] > lower_bound) & (data[c] < upper_bound)
            data[c] = np.clip(data[c], lower_bound, upper_bound)
            mn = data[c][mask].mean()
            sd = data[c][mask].std()
            data[c] = (data[c] - mn) / sd
            if use_nonzero_mask[c]:
                data[c][seg[-1] < 0] = 0
        else:
            if use_nonzero_mask[c]:
                mask = seg[-1] >= 0
            else:
                mask = np.ones(seg.shape[1:], dtype=bool)
            data[c][mask] = (data[c][mask] - data[c][mask].mean()) / (data[c][mask].std() + 1e-08)
            data[c][mask == 0] = 0
    return (data, seg, properties)",'data.shape (data is transposed)': data.shape,f'data.shape (data is transposed): {data.shape}',find_wrong,2,,,,
nnFormer,https://github.com/282857341/nnFormer/tree/master/nnformer/preprocessing/preprocessing.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nnFormer/nnformer/preprocessing/preprocessing.py,GenericPreprocessor,"def resample_and_normalize(self, data, target_spacing, properties, seg=None, force_separate_z=None):
    """"""
        data and seg must already have been transposed by transpose_forward. properties are the un-transposed values
        (spacing etc)
        :param data:
        :param target_spacing:
        :param properties:
        :param seg:
        :param force_separate_z:
        :return:
        """"""
    original_spacing_transposed = np.array(properties['original_spacing'])[self.transpose_forward]
    before = {'spacing': properties['original_spacing'], 'spacing_transposed': original_spacing_transposed, 'data.shape (data is transposed)': data.shape}
    data[np.isnan(data)] = 0
    (data, seg) = resample_patient(data, seg, np.array(original_spacing_transposed), target_spacing, 3, 1, force_separate_z=force_separate_z, order_z_data=0, order_z_seg=0, separate_z_anisotropy_threshold=self.resample_separate_z_anisotropy_threshold)
    after = {'spacing': target_spacing, 'data.shape (data is resampled)': data.shape}
    print('before:', before, '\nafter: ', after, '\n')
    if seg is not None:
        seg[seg < -1] = 0
    properties['size_after_resampling'] = data[0].shape
    properties['spacing_after_resampling'] = target_spacing
    use_nonzero_mask = self.use_nonzero_mask
    assert len(self.normalization_scheme_per_modality) == len(data), 'self.normalization_scheme_per_modality must have as many entries as data has modalities'
    assert len(self.use_nonzero_mask) == len(data), 'self.use_nonzero_mask must have as many entries as data has modalities'
    for c in range(len(data)):
        scheme = self.normalization_scheme_per_modality[c]
        if scheme == 'CT':
            assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
            mean_intensity = self.intensityproperties[c]['mean']
            std_intensity = self.intensityproperties[c]['sd']
            lower_bound = self.intensityproperties[c]['percentile_00_5']
            upper_bound = self.intensityproperties[c]['percentile_99_5']
            data[c] = np.clip(data[c], lower_bound, upper_bound)
            data[c] = (data[c] - mean_intensity) / std_intensity
            if use_nonzero_mask[c]:
                data[c][seg[-1] < 0] = 0
        elif scheme == 'CT2':
            assert self.intensityproperties is not None, 'ERROR: if there is a CT then we need intensity properties'
            lower_bound = self.intensityproperties[c]['percentile_00_5']
            upper_bound = self.intensityproperties[c]['percentile_99_5']
            mask = (data[c] > lower_bound) & (data[c] < upper_bound)
            data[c] = np.clip(data[c], lower_bound, upper_bound)
            mn = data[c][mask].mean()
            sd = data[c][mask].std()
            data[c] = (data[c] - mn) / sd
            if use_nonzero_mask[c]:
                data[c][seg[-1] < 0] = 0
        else:
            if use_nonzero_mask[c]:
                mask = seg[-1] >= 0
            else:
                mask = np.ones(seg.shape[1:], dtype=bool)
            data[c][mask] = (data[c][mask] - data[c][mask].mean()) / (data[c][mask].std() + 1e-08)
            data[c][mask == 0] = 0
    return (data, seg, properties)","('before:', before, '\nafter: ', after, '\n')",f'before: {before}\nafter: {after}\n',find_wrong,2,,,,
meshio,https://github.com/nschloe/meshio/tree/master/src/meshio/svg/_svg.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meshio/src/meshio/svg/_svg.py,,"def write(filename, mesh, float_fmt: str='.3f', stroke_width: str | None=None, image_width: int | float | None=100, fill: str='#c8c5bd', stroke: str='#000080'):
    if mesh.points.shape[1] == 3 and (not np.allclose(mesh.points[:, 2], 0.0, rtol=0.0, atol=1e-14)):
        raise WriteError(f'SVG can only handle flat 2D meshes (shape: {mesh.points.shape})')
    pts = mesh.points[:, :2].copy()
    min_x = np.min(pts[:, 0]) if len(pts) > 0 else 0.0
    max_x = np.max(pts[:, 0]) if len(pts) > 0 else 0.0
    min_y = np.min(pts[:, 1]) if len(pts) > 0 else 0.0
    max_y = np.max(pts[:, 1]) if len(pts) > 0 else 0.0
    pts[:, 1] = max_y + min_y - pts[:, 1]
    width = max_x - min_x
    height = max_y - min_y
    if image_width is not None and width != 0:
        scaling_factor = image_width / width
        min_x *= scaling_factor
        min_y *= scaling_factor
        width *= scaling_factor
        height *= scaling_factor
        pts *= scaling_factor
    if stroke_width is None:
        stroke_width = str(width / 100)
    fmt = ' '.join(4 * [f'{{:{float_fmt}}}'])
    svg = ET.Element('svg', xmlns='http://www.w3.org/2000/svg', version='1.1', viewBox=fmt.format(min_x, min_y, width, height))
    style = ET.SubElement(svg, 'style')
    opts = [f'fill: {fill}', f'stroke: {stroke}', f'stroke-width: {stroke_width}', 'stroke-linejoin:bevel']
    style.text = 'path {' + '; '.join(opts) + '}'
    for cell_block in mesh.cells:
        if cell_block.type not in ['line', 'triangle', 'quad']:
            continue
        if cell_block.type == 'line':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}'
        elif cell_block.type == 'triangle':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        elif cell_block.type == 'quad':
            fmt = f'M {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + f'L {{:{float_fmt}}} {{:{float_fmt}}}' + 'Z'
        for cell in cell_block.data:
            ET.SubElement(svg, 'path', d=fmt.format(*pts[cell].flatten()))
    tree = ET.ElementTree(svg)
    tree.write(filename)",fmt = ' '.join(4 * [f'{{:{float_fmt}}}']),fmt = f'{{:{float_fmt}}} {{:{float_fmt}}} {{:{float_fmt}}} {{:{float_fmt}}}',find_wrong,2,,,,
text_classification,https://github.com/brightmart/text_classification/tree/master/aa1_data_util/data_util_zhihu.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/text_classification/aa1_data_util/data_util_zhihu.py,,"def create_voabulary_label(voabulary_label='train-zhihu4-only-title-all.txt', name_scope='', use_seq2seq=False):
    print('create_voabulary_label_sorted.started.traning_data_path:', voabulary_label)
    cache_path = 'cache_vocabulary_label_pik/' + name_scope + '_label_voabulary.pik'
    if os.path.exists(cache_path):
        with open(cache_path, 'r') as data_f:
            (vocabulary_word2index_label, vocabulary_index2word_label) = pickle.load(data_f)
            return (vocabulary_word2index_label, vocabulary_index2word_label)
    else:
        zhihu_f_train = codecs.open(voabulary_label, 'r', 'utf8')
        lines = zhihu_f_train.readlines()
        count = 0
        vocabulary_word2index_label = {}
        vocabulary_index2word_label = {}
        vocabulary_label_count_dict = {}
        for (i, line) in enumerate(lines):
            if '__label__' in line:
                label = line[line.index('__label__') + len('__label__'):].strip().replace('\n', '')
                if vocabulary_label_count_dict.get(label, None) is not None:
                    vocabulary_label_count_dict[label] = vocabulary_label_count_dict[label] + 1
                else:
                    vocabulary_label_count_dict[label] = 1
        list_label = sort_by_value(vocabulary_label_count_dict)
        print('length of list_label:', len(list_label))
        countt = 0
        if use_seq2seq:
            i_list = [0, 1, 2]
            label_special_list = [_GO, _END, _PAD]
            for (i, label) in zip(i_list, label_special_list):
                vocabulary_word2index_label[label] = i
                vocabulary_index2word_label[i] = label
        for (i, label) in enumerate(list_label):
            if i < 10:
                count_value = vocabulary_label_count_dict[label]
                print('label:', label, 'count_value:', count_value)
                countt = countt + count_value
            indexx = i + 3 if use_seq2seq else i
            vocabulary_word2index_label[label] = indexx
            vocabulary_index2word_label[indexx] = label
        print('count top10:', countt)
        if not os.path.exists(cache_path):
            with open(cache_path, 'a') as data_f:
                pickle.dump((vocabulary_word2index_label, vocabulary_index2word_label), data_f)
    print('create_voabulary_label_sorted.ended.len of vocabulary_label:', len(vocabulary_index2word_label))
    return (vocabulary_word2index_label, vocabulary_index2word_label)","('create_voabulary_label_sorted.started.traning_data_path:', voabulary_label)",f'create_voabulary_label_sorted.started.traning_data_path: {voabulary_label}',find_wrong,2,,,,
text_classification,https://github.com/brightmart/text_classification/tree/master/aa1_data_util/data_util_zhihu.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/text_classification/aa1_data_util/data_util_zhihu.py,,"def create_voabulary_label(voabulary_label='train-zhihu4-only-title-all.txt', name_scope='', use_seq2seq=False):
    print('create_voabulary_label_sorted.started.traning_data_path:', voabulary_label)
    cache_path = 'cache_vocabulary_label_pik/' + name_scope + '_label_voabulary.pik'
    if os.path.exists(cache_path):
        with open(cache_path, 'r') as data_f:
            (vocabulary_word2index_label, vocabulary_index2word_label) = pickle.load(data_f)
            return (vocabulary_word2index_label, vocabulary_index2word_label)
    else:
        zhihu_f_train = codecs.open(voabulary_label, 'r', 'utf8')
        lines = zhihu_f_train.readlines()
        count = 0
        vocabulary_word2index_label = {}
        vocabulary_index2word_label = {}
        vocabulary_label_count_dict = {}
        for (i, line) in enumerate(lines):
            if '__label__' in line:
                label = line[line.index('__label__') + len('__label__'):].strip().replace('\n', '')
                if vocabulary_label_count_dict.get(label, None) is not None:
                    vocabulary_label_count_dict[label] = vocabulary_label_count_dict[label] + 1
                else:
                    vocabulary_label_count_dict[label] = 1
        list_label = sort_by_value(vocabulary_label_count_dict)
        print('length of list_label:', len(list_label))
        countt = 0
        if use_seq2seq:
            i_list = [0, 1, 2]
            label_special_list = [_GO, _END, _PAD]
            for (i, label) in zip(i_list, label_special_list):
                vocabulary_word2index_label[label] = i
                vocabulary_index2word_label[i] = label
        for (i, label) in enumerate(list_label):
            if i < 10:
                count_value = vocabulary_label_count_dict[label]
                print('label:', label, 'count_value:', count_value)
                countt = countt + count_value
            indexx = i + 3 if use_seq2seq else i
            vocabulary_word2index_label[label] = indexx
            vocabulary_index2word_label[indexx] = label
        print('count top10:', countt)
        if not os.path.exists(cache_path):
            with open(cache_path, 'a') as data_f:
                pickle.dump((vocabulary_word2index_label, vocabulary_index2word_label), data_f)
    print('create_voabulary_label_sorted.ended.len of vocabulary_label:', len(vocabulary_index2word_label))
    return (vocabulary_word2index_label, vocabulary_index2word_label)","print('length of list_label:', len(list_label))",print(f'length of list_label: {len(list_label)}'),find_wrong,2,,,,
text_classification,https://github.com/brightmart/text_classification/tree/master/aa1_data_util/data_util_zhihu.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/text_classification/aa1_data_util/data_util_zhihu.py,,"def create_voabulary_label(voabulary_label='train-zhihu4-only-title-all.txt', name_scope='', use_seq2seq=False):
    print('create_voabulary_label_sorted.started.traning_data_path:', voabulary_label)
    cache_path = 'cache_vocabulary_label_pik/' + name_scope + '_label_voabulary.pik'
    if os.path.exists(cache_path):
        with open(cache_path, 'r') as data_f:
            (vocabulary_word2index_label, vocabulary_index2word_label) = pickle.load(data_f)
            return (vocabulary_word2index_label, vocabulary_index2word_label)
    else:
        zhihu_f_train = codecs.open(voabulary_label, 'r', 'utf8')
        lines = zhihu_f_train.readlines()
        count = 0
        vocabulary_word2index_label = {}
        vocabulary_index2word_label = {}
        vocabulary_label_count_dict = {}
        for (i, line) in enumerate(lines):
            if '__label__' in line:
                label = line[line.index('__label__') + len('__label__'):].strip().replace('\n', '')
                if vocabulary_label_count_dict.get(label, None) is not None:
                    vocabulary_label_count_dict[label] = vocabulary_label_count_dict[label] + 1
                else:
                    vocabulary_label_count_dict[label] = 1
        list_label = sort_by_value(vocabulary_label_count_dict)
        print('length of list_label:', len(list_label))
        countt = 0
        if use_seq2seq:
            i_list = [0, 1, 2]
            label_special_list = [_GO, _END, _PAD]
            for (i, label) in zip(i_list, label_special_list):
                vocabulary_word2index_label[label] = i
                vocabulary_index2word_label[i] = label
        for (i, label) in enumerate(list_label):
            if i < 10:
                count_value = vocabulary_label_count_dict[label]
                print('label:', label, 'count_value:', count_value)
                countt = countt + count_value
            indexx = i + 3 if use_seq2seq else i
            vocabulary_word2index_label[label] = indexx
            vocabulary_index2word_label[indexx] = label
        print('count top10:', countt)
        if not os.path.exists(cache_path):
            with open(cache_path, 'a') as data_f:
                pickle.dump((vocabulary_word2index_label, vocabulary_index2word_label), data_f)
    print('create_voabulary_label_sorted.ended.len of vocabulary_label:', len(vocabulary_index2word_label))
    return (vocabulary_word2index_label, vocabulary_index2word_label)","print('label:', label, 'count_value:', count_value)",print(f'label: {label} count_value: {count_value}'),find_wrong,2,,,,
hyperpose,https://github.com/tensorlayer/hyperpose/tree/master/hyperpose/Model/pifpaf/eval.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hyperpose/hyperpose/Model/pifpaf/eval.py,,"def visualize(img, img_id, processed_img, pd_pif_maps, pd_paf_maps, humans, stride=8, save_dir='./save_dir'):
    print(f'{len(humans)} human found!')
    print('visualizing...')
    os.makedirs(save_dir, exist_ok=True)
    ori_img = np.clip(img * 255.0, 0.0, 255.0).astype(np.uint8)
    processed_img = np.clip(processed_img * 255.0, 0.0, 255.0).astype(np.uint8)
    vis_img = ori_img.copy()
    for human in humans:
        vis_img = human.draw_human(vis_img)
    (pd_pif_conf, pd_pif_vec, _, pd_pif_scale) = pd_pif_maps
    (pd_paf_conf, pd_paf_src_vec, pd_paf_dst_vec, _, _, _, _) = pd_paf_maps
    pd_pif_conf_show = np.amax(pd_pif_conf, axis=0)
    pd_pif_hr_conf_show = np.amax(get_hr_conf(pd_pif_conf, pd_pif_vec, pd_pif_scale, stride=stride, thresh=0.1), axis=0)
    pd_paf_conf_show = np.amax(pd_paf_conf, axis=0)
    pd_paf_vec_show = np.zeros(shape=(pd_pif_hr_conf_show.shape[0], pd_pif_hr_conf_show.shape[1], 3)).astype(np.int8)
    pd_paf_vec_show = get_arrow_map(pd_paf_vec_show, pd_paf_conf, pd_paf_src_vec, pd_paf_dst_vec, thresh=0.1)
    fig = plt.figure(figsize=(12, 12))
    a = fig.add_subplot(3, 3, 1)
    a.set_title('input image')
    plt.imshow(ori_img)
    a = fig.add_subplot(3, 3, 3)
    a.set_title('output result')
    plt.imshow(vis_img)
    a = fig.add_subplot(3, 3, 4)
    a.set_title('processed image')
    plt.imshow(processed_img)
    a = fig.add_subplot(3, 3, 5)
    a.set_title('pif_conf_map')
    plt.imshow(pd_pif_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 6)
    a.set_title('pif_hr_conf_map')
    plt.imshow(pd_pif_hr_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 7)
    a.set_title('processed image')
    plt.imshow(processed_img)
    a = fig.add_subplot(3, 3, 8)
    a.set_title('paf_conf_map')
    plt.imshow(pd_paf_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 9)
    a.set_title('paf_vec_map')
    plt.imshow(pd_paf_vec_show, alpha=0.8)
    plt.colorbar()
    plt.savefig(os.path.join(save_dir, f'{img_id}_visualize.png'))
    plt.close()",print(f'{len(humans)} human found!'),print(f'{len(humans)} human found!'),find_wrong,2,,,,
hyperpose,https://github.com/tensorlayer/hyperpose/tree/master/hyperpose/Model/pifpaf/eval.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hyperpose/hyperpose/Model/pifpaf/eval.py,,"def visualize(img, img_id, processed_img, pd_pif_maps, pd_paf_maps, humans, stride=8, save_dir='./save_dir'):
    print(f'{len(humans)} human found!')
    print('visualizing...')
    os.makedirs(save_dir, exist_ok=True)
    ori_img = np.clip(img * 255.0, 0.0, 255.0).astype(np.uint8)
    processed_img = np.clip(processed_img * 255.0, 0.0, 255.0).astype(np.uint8)
    vis_img = ori_img.copy()
    for human in humans:
        vis_img = human.draw_human(vis_img)
    (pd_pif_conf, pd_pif_vec, _, pd_pif_scale) = pd_pif_maps
    (pd_paf_conf, pd_paf_src_vec, pd_paf_dst_vec, _, _, _, _) = pd_paf_maps
    pd_pif_conf_show = np.amax(pd_pif_conf, axis=0)
    pd_pif_hr_conf_show = np.amax(get_hr_conf(pd_pif_conf, pd_pif_vec, pd_pif_scale, stride=stride, thresh=0.1), axis=0)
    pd_paf_conf_show = np.amax(pd_paf_conf, axis=0)
    pd_paf_vec_show = np.zeros(shape=(pd_pif_hr_conf_show.shape[0], pd_pif_hr_conf_show.shape[1], 3)).astype(np.int8)
    pd_paf_vec_show = get_arrow_map(pd_paf_vec_show, pd_paf_conf, pd_paf_src_vec, pd_paf_dst_vec, thresh=0.1)
    fig = plt.figure(figsize=(12, 12))
    a = fig.add_subplot(3, 3, 1)
    a.set_title('input image')
    plt.imshow(ori_img)
    a = fig.add_subplot(3, 3, 3)
    a.set_title('output result')
    plt.imshow(vis_img)
    a = fig.add_subplot(3, 3, 4)
    a.set_title('processed image')
    plt.imshow(processed_img)
    a = fig.add_subplot(3, 3, 5)
    a.set_title('pif_conf_map')
    plt.imshow(pd_pif_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 6)
    a.set_title('pif_hr_conf_map')
    plt.imshow(pd_pif_hr_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 7)
    a.set_title('processed image')
    plt.imshow(processed_img)
    a = fig.add_subplot(3, 3, 8)
    a.set_title('paf_conf_map')
    plt.imshow(pd_paf_conf_show, alpha=0.8)
    plt.colorbar()
    a = fig.add_subplot(3, 3, 9)
    a.set_title('paf_vec_map')
    plt.imshow(pd_paf_vec_show, alpha=0.8)
    plt.colorbar()
    plt.savefig(os.path.join(save_dir, f'{img_id}_visualize.png'))
    plt.close()","plt.savefig(os.path.join(save_dir, f'{img_id}_visualize.png'))","plt.savefig(os.path.join(save_dir, f'{img_id}_visualize.png'))",find_wrong,2,,,,
tern,https://github.com/tern-tools/tern/tree/master/tern/formats/default/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tern/tern/formats/default/generator.py,,"def print_layer_report(layer):
    """"""Generate a report for a given layer""""""
    notes = get_layer_notices(layer)
    (filelicenses, pkgs_table) = get_layer_info_list(layer)
    notes += formats.layer_file_licenses_list.format(list=filelicenses)
    if pkgs_table:
        notes += formats.layer_packages_header.format('\n')
        for line in pkgs_table.splitlines():
            notes += '\t' + line + '\n'
    else:
        notes += formats.layer_packages_header.format('None\n')
    return notes",notes += formats.layer_file_licenses_list.format(list=filelicenses),notes += f'{formats.layer_file_licenses_list.format(list=filelicenses)}',find_wrong,2,,,,
tern,https://github.com/tern-tools/tern/tree/master/tern/formats/default/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tern/tern/formats/default/generator.py,,"def print_layer_report(layer):
    """"""Generate a report for a given layer""""""
    notes = get_layer_notices(layer)
    (filelicenses, pkgs_table) = get_layer_info_list(layer)
    notes += formats.layer_file_licenses_list.format(list=filelicenses)
    if pkgs_table:
        notes += formats.layer_packages_header.format('\n')
        for line in pkgs_table.splitlines():
            notes += '\t' + line + '\n'
    else:
        notes += formats.layer_packages_header.format('None\n')
    return notes",notes += formats.layer_packages_header.format('\n'),notes += f"{formats.layer_packages_header.format('\n')}",find_wrong,2,,,,
tern,https://github.com/tern-tools/tern/tree/master/tern/formats/default/generator.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tern/tern/formats/default/generator.py,,"def print_layer_report(layer):
    """"""Generate a report for a given layer""""""
    notes = get_layer_notices(layer)
    (filelicenses, pkgs_table) = get_layer_info_list(layer)
    notes += formats.layer_file_licenses_list.format(list=filelicenses)
    if pkgs_table:
        notes += formats.layer_packages_header.format('\n')
        for line in pkgs_table.splitlines():
            notes += '\t' + line + '\n'
    else:
        notes += formats.layer_packages_header.format('None\n')
    return notes",notes += '\t' + line + '\n',notes += f'\t{line}\n',find_wrong,,1,3,,
capa,https://github.com/mandiant/capa/tree/master/scripts/profile-time.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capa/scripts/profile-time.py,,"def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    label = subprocess.run('git show --pretty=oneline --abbrev-commit | head -n 1', shell=True, capture_output=True, text=True).stdout.strip()
    is_dirty = subprocess.run(""git status | grep 'modified: ' | grep -v 'rules' | grep -v 'tests/data'"", shell=True, capture_output=True, text=True).stdout != ''
    if is_dirty:
        label += ' (dirty)'
    parser = argparse.ArgumentParser(description='Profile capa performance')
    capa.main.install_common_args(parser, wanted={'format', 'sample', 'signatures', 'rules'})
    parser.add_argument('--number', type=int, default=3, help='batch size of profile collection')
    parser.add_argument('--repeat', type=int, default=30, help='batch count of profile collection')
    parser.add_argument('--label', type=str, default=label, help='description of the profile collection')
    args = parser.parse_args(args=argv)
    capa.main.handle_common_args(args)
    try:
        taste = capa.helpers.get_file_taste(args.sample)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        with capa.main.timing('load rules'):
            rules = capa.rules.RuleSet(capa.main.get_rules(args.rules, disable_progress=True))
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    try:
        sig_paths = capa.main.get_signatures(args.signatures)
    except IOError as e:
        logger.error('%s', str(e))
        return -1
    if args.format == 'freeze' or (args.format == 'auto' and capa.features.freeze.is_freeze(taste)):
        with open(args.sample, 'rb') as f:
            extractor = capa.features.freeze.load(f.read())
    else:
        extractor = capa.main.get_extractor(args.sample, args.format, capa.main.BACKEND_VIV, sig_paths, should_save_workspace=False)
    with tqdm.tqdm(total=args.number * args.repeat) as pbar:

        def do_iteration():
            capa.perf.reset()
            capa.main.find_capabilities(rules, extractor, disable_progress=True)
            pbar.update(1)
        samples = timeit.repeat(do_iteration, number=args.number, repeat=args.repeat)
    logger.debug('perf: find capabilities: min: %0.2fs' % (min(samples) / float(args.number)))
    logger.debug('perf: find capabilities: avg: %0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)))
    logger.debug('perf: find capabilities: max: %0.2fs' % (max(samples) / float(args.number)))
    for (counter, count) in capa.perf.counters.most_common():
        logger.debug('perf: counter: {:}: {:,}'.format(counter, count))
    print(tabulate.tabulate([(args.label, '{:,}'.format(capa.perf.counters['evaluate.feature']), '%0.2fs' % (min(samples) / float(args.number)), '%0.2fs' % (sum(samples) / float(args.repeat) / float(args.number)), '%0.2fs' % (max(samples) / float(args.number)))], headers=['label', 'count(evaluations)', 'min(time)', 'avg(time)', 'max(time)'], tablefmt='github'))
    return 0","'{:,}'.format(capa.perf.counters['evaluate.feature'])","f""{capa.perf.counters['evaluate.feature']:,}""",find_wrong,,1,3,,
IPGeoLocation,https://github.com/maldevel/IPGeoLocation/tree/master/core/IpGeoLocationLib.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/IPGeoLocation/core/IpGeoLocationLib.py,IpGeoLocationLib,"def __loadUserAgents(self):
    """"""Load user-agent strings from file""""""
    if not self.UserAgentFile:
        raise UserAgentFileNotSpecifiedError()
    self.UserAgents = [line.strip() for line in open(self.UserAgentFile, 'r') if line.strip()]
    self.Logger.Print('{} User-Agent strings loaded.'.format(len(self.UserAgents)))
    if len(self.UserAgents) == 0:
        raise UserAgentFileEmptyError()",'{} User-Agent strings loaded.'.format(len(self.UserAgents)),f'{len(self.UserAgents)} User-Agent strings loaded.',find_wrong,,1,3,,
wig,https://github.com/jekyc/wig/tree/master/wig/classes/request2.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wig/wig/classes/request2.py,Response,"def __repr__(self):

    def get_string(r):
        string = r.url + '\n'
        string += '%s %s\n' % (r.status['code'], r.status['text'])
        string += '\n'.join([header + ': ' + r.headers[header] for header in r.headers])
        string += '\n\n'
        string += 'MD5:            ' + self.md5 + '\n'
        string += 'MD5 Error page: ' + self.md5_404 + '\n'
        return string
    return get_string(self)","string += '%s %s\n' % (r.status['code'], r.status['text'])",string += f"{r.status['code']} {r.status['text']}\n",find_wrong,,,,,
DevOps-Python-tools,https://github.com/HariSekhon/DevOps-Python-tools/tree/master//ambari_trigger_service_checks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DevOps-Python-tools//ambari_trigger_service_checks.py,AmbariTriggerServiceChecks,"def gen_payload(self, services=None):
    log.debug('generating payload for services: %s', services)
    if services is None or services == 'all':
        services = self.get_services()
    if not isList(services):
        code_error('non-list passed to gen_payload')
    payload = [{'RequestSchedule': {'batch': [{'requests': []}, {'batch_settings': {'batch_separation_in_seconds': 1, 'task_failure_tolerance': 1}}]}}]
    service_count = len(services)
    for index in range(service_count):
        service = services[index]
        index += 1
        commandData = ''
        if service.upper() == 'ZOOKEEPER':
            commandData = '{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper())
        else:
            commandData = '{service}_SERVICE_CHECK'.format(service=service.upper())
        payload[0]['RequestSchedule']['batch'][0]['requests'].append({'order_id': index, 'type': 'POST', 'uri': '/api/v1/clusters/{0}/requests'.format(self.cluster), 'RequestBodyInfo': {'RequestInfo': {'command': '{commandData}'.format(commandData=commandData), 'context': '{service} Service Check (batch {index} of {total})'.format(service=service, index=index, total=service_count)}, 'Requests/resource_filters': [{'service_name': service.upper()}]}})
    payload_str = json.dumps(payload)
    if log.isEnabledFor(logging.DEBUG):
        log.debug('generated payload:\n%s', jsonpp(payload_str))
    return payload_str",'{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper()),f'{service.upper()}_QUORUM_SERVICE_CHECK',find_wrong,,1,3,,
DevOps-Python-tools,https://github.com/HariSekhon/DevOps-Python-tools/tree/master//ambari_trigger_service_checks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DevOps-Python-tools//ambari_trigger_service_checks.py,AmbariTriggerServiceChecks,"def gen_payload(self, services=None):
    log.debug('generating payload for services: %s', services)
    if services is None or services == 'all':
        services = self.get_services()
    if not isList(services):
        code_error('non-list passed to gen_payload')
    payload = [{'RequestSchedule': {'batch': [{'requests': []}, {'batch_settings': {'batch_separation_in_seconds': 1, 'task_failure_tolerance': 1}}]}}]
    service_count = len(services)
    for index in range(service_count):
        service = services[index]
        index += 1
        commandData = ''
        if service.upper() == 'ZOOKEEPER':
            commandData = '{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper())
        else:
            commandData = '{service}_SERVICE_CHECK'.format(service=service.upper())
        payload[0]['RequestSchedule']['batch'][0]['requests'].append({'order_id': index, 'type': 'POST', 'uri': '/api/v1/clusters/{0}/requests'.format(self.cluster), 'RequestBodyInfo': {'RequestInfo': {'command': '{commandData}'.format(commandData=commandData), 'context': '{service} Service Check (batch {index} of {total})'.format(service=service, index=index, total=service_count)}, 'Requests/resource_filters': [{'service_name': service.upper()}]}})
    payload_str = json.dumps(payload)
    if log.isEnabledFor(logging.DEBUG):
        log.debug('generated payload:\n%s', jsonpp(payload_str))
    return payload_str",'{service}_SERVICE_CHECK'.format(service=service.upper()),f'{service.upper()}_SERVICE_CHECK',find_wrong,,1,3,,
DevOps-Python-tools,https://github.com/HariSekhon/DevOps-Python-tools/tree/master//ambari_trigger_service_checks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DevOps-Python-tools//ambari_trigger_service_checks.py,AmbariTriggerServiceChecks,"def gen_payload(self, services=None):
    log.debug('generating payload for services: %s', services)
    if services is None or services == 'all':
        services = self.get_services()
    if not isList(services):
        code_error('non-list passed to gen_payload')
    payload = [{'RequestSchedule': {'batch': [{'requests': []}, {'batch_settings': {'batch_separation_in_seconds': 1, 'task_failure_tolerance': 1}}]}}]
    service_count = len(services)
    for index in range(service_count):
        service = services[index]
        index += 1
        commandData = ''
        if service.upper() == 'ZOOKEEPER':
            commandData = '{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper())
        else:
            commandData = '{service}_SERVICE_CHECK'.format(service=service.upper())
        payload[0]['RequestSchedule']['batch'][0]['requests'].append({'order_id': index, 'type': 'POST', 'uri': '/api/v1/clusters/{0}/requests'.format(self.cluster), 'RequestBodyInfo': {'RequestInfo': {'command': '{commandData}'.format(commandData=commandData), 'context': '{service} Service Check (batch {index} of {total})'.format(service=service, index=index, total=service_count)}, 'Requests/resource_filters': [{'service_name': service.upper()}]}})
    payload_str = json.dumps(payload)
    if log.isEnabledFor(logging.DEBUG):
        log.debug('generated payload:\n%s', jsonpp(payload_str))
    return payload_str",'/api/v1/clusters/{0}/requests'.format(self.cluster),f'/api/v1/clusters/{self.cluster}/requests',find_wrong,,1,3,,
DevOps-Python-tools,https://github.com/HariSekhon/DevOps-Python-tools/tree/master//ambari_trigger_service_checks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DevOps-Python-tools//ambari_trigger_service_checks.py,AmbariTriggerServiceChecks,"def gen_payload(self, services=None):
    log.debug('generating payload for services: %s', services)
    if services is None or services == 'all':
        services = self.get_services()
    if not isList(services):
        code_error('non-list passed to gen_payload')
    payload = [{'RequestSchedule': {'batch': [{'requests': []}, {'batch_settings': {'batch_separation_in_seconds': 1, 'task_failure_tolerance': 1}}]}}]
    service_count = len(services)
    for index in range(service_count):
        service = services[index]
        index += 1
        commandData = ''
        if service.upper() == 'ZOOKEEPER':
            commandData = '{service}_QUORUM_SERVICE_CHECK'.format(service=service.upper())
        else:
            commandData = '{service}_SERVICE_CHECK'.format(service=service.upper())
        payload[0]['RequestSchedule']['batch'][0]['requests'].append({'order_id': index, 'type': 'POST', 'uri': '/api/v1/clusters/{0}/requests'.format(self.cluster), 'RequestBodyInfo': {'RequestInfo': {'command': '{commandData}'.format(commandData=commandData), 'context': '{service} Service Check (batch {index} of {total})'.format(service=service, index=index, total=service_count)}, 'Requests/resource_filters': [{'service_name': service.upper()}]}})
    payload_str = json.dumps(payload)
    if log.isEnabledFor(logging.DEBUG):
        log.debug('generated payload:\n%s', jsonpp(payload_str))
    return payload_str",'{commandData}'.format(commandData=commandData),f'{commandData}',find_wrong,,1,3,,
galaxy,https://github.com/ansible/galaxy/tree/master/lib/tool_shed/dependencies/repository/relation_builder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/tool_shed/dependencies/repository/relation_builder.py,RelationBuilder,"def get_updated_changeset_revisions_for_repository_dependencies(self, key_rd_dicts):
    updated_key_rd_dicts = []
    for key_rd_dict in key_rd_dicts:
        key = next(iter(key_rd_dict))
        repository_dependency = key_rd_dict[key]
        (rd_toolshed, rd_name, rd_owner, rd_changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td) = common_util.parse_repository_dependency_tuple(repository_dependency)
        if suc.tool_shed_is_this_tool_shed(rd_toolshed):
            repository = tool_shed.util.repository_util.get_repository_by_name_and_owner(self.app, rd_name, rd_owner)
            if repository:
                repository_id = self.app.security.encode_id(repository.id)
                repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, rd_changeset_revision)
                if repository_metadata:
                    new_key_rd_dict = {}
                    new_key_rd_dict[key] = repository_dependency
                    updated_key_rd_dicts.append(key_rd_dict)
                else:
                    changeset_revision = metadata_util.get_next_downloadable_changeset_revision(self.app, repository, rd_changeset_revision)
                    if changeset_revision != rd_changeset_revision:
                        repository_metadata = metadata_util.get_repository_metadata_by_repository_id_changeset_revision(self.app, repository_id, changeset_revision)
                    if repository_metadata:
                        new_key_rd_dict = {}
                        new_key_rd_dict[key] = [rd_toolshed, rd_name, rd_owner, repository_metadata.changeset_revision, rd_prior_installation_required, rd_only_if_compiling_contained_td]
                        updated_key_rd_dicts.append(new_key_rd_dict)
                    else:
                        repository_components_tuple = container_util.get_components_from_key(key)
                        components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                        (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                        if len(components_list) in (4, 5):
                            rd_only_if_compiling_contained_td = 'False'
                        message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                        message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                        log.debug(message)
            else:
                repository_components_tuple = container_util.get_components_from_key(key)
                components_list = tool_shed.util.repository_util.extract_components_from_tuple(repository_components_tuple)
                (toolshed, repository_name, repository_owner, repository_changeset_revision) = components_list[0:4]
                message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
                message += f'dependencies defined for repository {str(repository_name)} will be ignored.'
                log.debug(message)
    return updated_key_rd_dicts","message = 'The revision %s defined for repository %s owned by %s is invalid, so repository ' % (str(rd_changeset_revision), str(rd_name), str(rd_owner))
message += f'dependencies defined for repository {str(repository_name)} will be ignored.'","message = f'The revision {rd_changeset_revision} defined for repository {rd_name} owned by {rd_owner} is invalid, so repository dependencies defined for repository {repository_name} will be ignored.'",find_wrong,,,,,
social_mapper,https://github.com/Greenwolf/social_mapper/tree/master//social_mapper.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/social_mapper//social_mapper.py,,"def fill_linkedin(peoplelist):
    LinkedinfinderObject = linkedinfinder.Linkedinfinder(showbrowser)
    LinkedinfinderObject.doLogin(linkedin_username, linkedin_password)
    if args.waitafterlogin:
        input('Press Enter to continue after verifying you are logged in...')
    count = 1
    ammount = len(peoplelist)
    for person in peoplelist:
        if args.vv == True or args.debug == True:
            print('LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name))
        else:
            sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name))
            sys.stdout.flush()
        count = count + 1
        if person.person_image:
            try:
                target_image = face_recognition.load_image_file(person.person_image)
                target_encoding = face_recognition.face_encodings(target_image)[0]
                profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
                if args.debug == True:
                    print(profilelist)
            except:
                continue
        else:
            continue
        early_break = False
        updatedlist = []
        for (profilelink, profilepic, distance) in profilelist:
            try:
                os.remove('potential_target_image.jpg')
            except:
                pass
            if early_break:
                break
            image_link = profilepic
            if image_link:
                try:
                    urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                    potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                    try:
                        potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                    except:
                        continue
                    results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                    for result in results:
                        if args.mode == 'fast':
                            if result < threshold:
                                person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                                person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                                if args.vv == True:
                                    print('\tMatch found: ' + person.full_name)
                                    print('\tLinkedIn: ' + person.linkedin)
                                early_break = True
                                break
                        elif args.mode == 'accurate':
                            if result < threshold:
                                updatedlist.append([profilelink, image_link, result])
                except Exception as e:
                    print(e)
        if args.mode == 'accurate':
            highestdistance = 1.0
            bestprofilelink = ''
            bestimagelink = ''
            for (profilelink, image_link, distance) in updatedlist:
                if distance < highestdistance:
                    highestdistance = distance
                    bestprofilelink = profilelink
                    bestimagelink = image_link
            if highestdistance < threshold:
                person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
                person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
                if args.vv == True:
                    print('\tMatch found: ' + person.full_name)
                    print('\tLinkedIn: ' + person.linkedin)
    try:
        LinkedinfinderObject.kill()
    except:
        print('Error Killing LinkedIn Selenium instance')
    return peoplelist","'LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name)",f'LinkedIn Check {count}/{ammount} : {person.full_name}',find_wrong,,1,3,1,
social_mapper,https://github.com/Greenwolf/social_mapper/tree/master//social_mapper.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/social_mapper//social_mapper.py,,"def fill_linkedin(peoplelist):
    LinkedinfinderObject = linkedinfinder.Linkedinfinder(showbrowser)
    LinkedinfinderObject.doLogin(linkedin_username, linkedin_password)
    if args.waitafterlogin:
        input('Press Enter to continue after verifying you are logged in...')
    count = 1
    ammount = len(peoplelist)
    for person in peoplelist:
        if args.vv == True or args.debug == True:
            print('LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name))
        else:
            sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name))
            sys.stdout.flush()
        count = count + 1
        if person.person_image:
            try:
                target_image = face_recognition.load_image_file(person.person_image)
                target_encoding = face_recognition.face_encodings(target_image)[0]
                profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
                if args.debug == True:
                    print(profilelist)
            except:
                continue
        else:
            continue
        early_break = False
        updatedlist = []
        for (profilelink, profilepic, distance) in profilelist:
            try:
                os.remove('potential_target_image.jpg')
            except:
                pass
            if early_break:
                break
            image_link = profilepic
            if image_link:
                try:
                    urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                    potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                    try:
                        potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                    except:
                        continue
                    results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                    for result in results:
                        if args.mode == 'fast':
                            if result < threshold:
                                person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                                person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                                if args.vv == True:
                                    print('\tMatch found: ' + person.full_name)
                                    print('\tLinkedIn: ' + person.linkedin)
                                early_break = True
                                break
                        elif args.mode == 'accurate':
                            if result < threshold:
                                updatedlist.append([profilelink, image_link, result])
                except Exception as e:
                    print(e)
        if args.mode == 'accurate':
            highestdistance = 1.0
            bestprofilelink = ''
            bestimagelink = ''
            for (profilelink, image_link, distance) in updatedlist:
                if distance < highestdistance:
                    highestdistance = distance
                    bestprofilelink = profilelink
                    bestimagelink = image_link
            if highestdistance < threshold:
                person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
                person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
                if args.vv == True:
                    print('\tMatch found: ' + person.full_name)
                    print('\tLinkedIn: ' + person.linkedin)
    try:
        LinkedinfinderObject.kill()
    except:
        print('Error Killing LinkedIn Selenium instance')
    return peoplelist","\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name)",f'\rLinkedIn Check {count}/{ammount} : {person.full_name:<32}',find_wrong,2,,,,
nltk-trainer,https://github.com/japerk/nltk-trainer/tree/master/nltk_trainer/featx/phonetics.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk-trainer/nltk_trainer/featx/phonetics.py,,"def metaphone(term):
    """"""returns metaphone code for a given string""""""
    code = ''
    i = 0
    term_length = len(term)
    if term_length == 0:
        return code
    term = string.lower(term)
    term = re.sub('[^a-z]', '', term)
    if len(term) == 0:
        return code
    firstChar = term[0]
    str2 = firstChar
    for x in term:
        if x != str2[-1]:
            str2 = str2 + x
    firstChar = str2[0]
    str3 = firstChar
    for x in str2[1:]:
        if re.search('[^aeiou]', x):
            str3 = str3 + x
    term = str3
    term_length = len(term)
    if term_length == 0:
        return code
    if term_length > 1:
        first_chars = term[0:2]
        table = {'ae': 'e', 'gn': 'n', 'kn': 'n', 'pn': 'n', 'wr': 'n', 'wh': 'w'}
        if first_chars in table.keys():
            term = term[2:]
            code = table[first_chars]
            term_length = len(term)
    elif term[0] == 'x':
        term = ''
        code = 's'
        term_length = 0
    st_trans = {'b': 'b', 'c': 'k', 'd': 't', 'g': 'k', 'h': 'h', 'k': 'k', 'p': 'p', 'q': 'k', 's': 's', 't': 't', 'v': 'f', 'w': 'w', 'x': 'ks', 'y': 'y', 'z': 's'}
    i = 0
    while i < term_length:
        add_char = ''
        part_n_2 = ''
        part_n_3 = ''
        part_n_4 = ''
        part_c_2 = ''
        part_c_3 = ''
        if i < term_length - 1:
            part_n_2 = term[i:i + 2]
            if i > 0:
                part_c_2 = term[i - 1:i + 1]
                part_c_3 = term[i - 1:i + 2]
        if i < term_length - 2:
            part_n_3 = term[i:i + 3]
        if i < term_length - 3:
            part_n_4 = term[i:i + 4]
        if term[i] == 'b':
            add_char = st_trans['b']
            if i == term_length - 1:
                if i > 0:
                    if term[i - 1] == 'm':
                        add_char = ''
        elif term[i] == 'c':
            add_char = st_trans['c']
            if part_n_2 == 'ch':
                add_char = 'x'
            elif re.search('c[iey]', part_n_2):
                add_char = 's'
            if part_n_3 == 'cia':
                add_char = 'x'
            if re.search('sc[iey]', part_c_3):
                add_char = ''
        elif term[i] == 'd':
            add_char = st_trans['d']
            if re.search('dg[eyi]', part_n_3):
                add_char = 'j'
        elif term[i] == 'g':
            add_char = st_trans['g']
            if part_n_2 == 'gh':
                if i == term_length - 2:
                    add_char = ''
            elif re.search('gh[aeiouy]', part_n_3):
                add_char = ''
            elif part_n_2 == 'gn':
                add_char = ''
            elif part_n_4 == 'gned':
                add_char = ''
            elif re.search('dg[eyi]', part_c_3):
                add_char = ''
            elif part_n_2 == 'gi':
                if part_c_3 != 'ggi':
                    add_char = 'j'
            elif part_n_2 == 'ge':
                if part_c_3 != 'gge':
                    add_char = 'j'
            elif part_n_2 == 'gy':
                if part_c_3 != 'ggy':
                    add_char = 'j'
            elif part_n_2 == 'gg':
                add_char = ''
        elif term[i] == 'h':
            add_char = st_trans['h']
            if re.search('[aeiouy]h[^aeiouy]', part_c_3):
                add_char = ''
            elif re.search('[csptg]h', part_c_2):
                add_char = ''
        elif term[i] == 'k':
            add_char = st_trans['k']
            if part_c_2 == 'ck':
                add_char = ''
        elif term[i] == 'p':
            add_char = st_trans['p']
            if part_n_2 == 'ph':
                add_char = 'f'
        elif term[i] == 'q':
            add_char = st_trans['q']
        elif term[i] == 's':
            add_char = st_trans['s']
            if part_n_2 == 'sh':
                add_char = 'x'
            if re.search('si[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 't':
            add_char = st_trans['t']
            if part_n_2 == 'th':
                add_char = '0'
            if re.search('ti[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 'v':
            add_char = st_trans['v']
        elif term[i] == 'w':
            add_char = st_trans['w']
            if re.search('w[^aeiouy]', part_n_2):
                add_char = ''
        elif term[i] == 'x':
            add_char = st_trans['x']
        elif term[i] == 'y':
            add_char = st_trans['y']
        elif term[i] == 'z':
            add_char = st_trans['z']
        else:
            add_char = term[i]
        code = code + add_char
        i += 1
    return code",term = string.lower(term),term = term.lower(),find_wrong,2,,,,
nltk-trainer,https://github.com/japerk/nltk-trainer/tree/master/nltk_trainer/featx/phonetics.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk-trainer/nltk_trainer/featx/phonetics.py,,"def metaphone(term):
    """"""returns metaphone code for a given string""""""
    code = ''
    i = 0
    term_length = len(term)
    if term_length == 0:
        return code
    term = string.lower(term)
    term = re.sub('[^a-z]', '', term)
    if len(term) == 0:
        return code
    firstChar = term[0]
    str2 = firstChar
    for x in term:
        if x != str2[-1]:
            str2 = str2 + x
    firstChar = str2[0]
    str3 = firstChar
    for x in str2[1:]:
        if re.search('[^aeiou]', x):
            str3 = str3 + x
    term = str3
    term_length = len(term)
    if term_length == 0:
        return code
    if term_length > 1:
        first_chars = term[0:2]
        table = {'ae': 'e', 'gn': 'n', 'kn': 'n', 'pn': 'n', 'wr': 'n', 'wh': 'w'}
        if first_chars in table.keys():
            term = term[2:]
            code = table[first_chars]
            term_length = len(term)
    elif term[0] == 'x':
        term = ''
        code = 's'
        term_length = 0
    st_trans = {'b': 'b', 'c': 'k', 'd': 't', 'g': 'k', 'h': 'h', 'k': 'k', 'p': 'p', 'q': 'k', 's': 's', 't': 't', 'v': 'f', 'w': 'w', 'x': 'ks', 'y': 'y', 'z': 's'}
    i = 0
    while i < term_length:
        add_char = ''
        part_n_2 = ''
        part_n_3 = ''
        part_n_4 = ''
        part_c_2 = ''
        part_c_3 = ''
        if i < term_length - 1:
            part_n_2 = term[i:i + 2]
            if i > 0:
                part_c_2 = term[i - 1:i + 1]
                part_c_3 = term[i - 1:i + 2]
        if i < term_length - 2:
            part_n_3 = term[i:i + 3]
        if i < term_length - 3:
            part_n_4 = term[i:i + 4]
        if term[i] == 'b':
            add_char = st_trans['b']
            if i == term_length - 1:
                if i > 0:
                    if term[i - 1] == 'm':
                        add_char = ''
        elif term[i] == 'c':
            add_char = st_trans['c']
            if part_n_2 == 'ch':
                add_char = 'x'
            elif re.search('c[iey]', part_n_2):
                add_char = 's'
            if part_n_3 == 'cia':
                add_char = 'x'
            if re.search('sc[iey]', part_c_3):
                add_char = ''
        elif term[i] == 'd':
            add_char = st_trans['d']
            if re.search('dg[eyi]', part_n_3):
                add_char = 'j'
        elif term[i] == 'g':
            add_char = st_trans['g']
            if part_n_2 == 'gh':
                if i == term_length - 2:
                    add_char = ''
            elif re.search('gh[aeiouy]', part_n_3):
                add_char = ''
            elif part_n_2 == 'gn':
                add_char = ''
            elif part_n_4 == 'gned':
                add_char = ''
            elif re.search('dg[eyi]', part_c_3):
                add_char = ''
            elif part_n_2 == 'gi':
                if part_c_3 != 'ggi':
                    add_char = 'j'
            elif part_n_2 == 'ge':
                if part_c_3 != 'gge':
                    add_char = 'j'
            elif part_n_2 == 'gy':
                if part_c_3 != 'ggy':
                    add_char = 'j'
            elif part_n_2 == 'gg':
                add_char = ''
        elif term[i] == 'h':
            add_char = st_trans['h']
            if re.search('[aeiouy]h[^aeiouy]', part_c_3):
                add_char = ''
            elif re.search('[csptg]h', part_c_2):
                add_char = ''
        elif term[i] == 'k':
            add_char = st_trans['k']
            if part_c_2 == 'ck':
                add_char = ''
        elif term[i] == 'p':
            add_char = st_trans['p']
            if part_n_2 == 'ph':
                add_char = 'f'
        elif term[i] == 'q':
            add_char = st_trans['q']
        elif term[i] == 's':
            add_char = st_trans['s']
            if part_n_2 == 'sh':
                add_char = 'x'
            if re.search('si[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 't':
            add_char = st_trans['t']
            if part_n_2 == 'th':
                add_char = '0'
            if re.search('ti[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 'v':
            add_char = st_trans['v']
        elif term[i] == 'w':
            add_char = st_trans['w']
            if re.search('w[^aeiouy]', part_n_2):
                add_char = ''
        elif term[i] == 'x':
            add_char = st_trans['x']
        elif term[i] == 'y':
            add_char = st_trans['y']
        elif term[i] == 'z':
            add_char = st_trans['z']
        else:
            add_char = term[i]
        code = code + add_char
        i += 1
    return code",var = 'arg_%x' % var.offset,var = f'arg_{var.offset:x}',find_wrong,2,,,,
nltk-trainer,https://github.com/japerk/nltk-trainer/tree/master/nltk_trainer/featx/phonetics.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk-trainer/nltk_trainer/featx/phonetics.py,,"def metaphone(term):
    """"""returns metaphone code for a given string""""""
    code = ''
    i = 0
    term_length = len(term)
    if term_length == 0:
        return code
    term = string.lower(term)
    term = re.sub('[^a-z]', '', term)
    if len(term) == 0:
        return code
    firstChar = term[0]
    str2 = firstChar
    for x in term:
        if x != str2[-1]:
            str2 = str2 + x
    firstChar = str2[0]
    str3 = firstChar
    for x in str2[1:]:
        if re.search('[^aeiou]', x):
            str3 = str3 + x
    term = str3
    term_length = len(term)
    if term_length == 0:
        return code
    if term_length > 1:
        first_chars = term[0:2]
        table = {'ae': 'e', 'gn': 'n', 'kn': 'n', 'pn': 'n', 'wr': 'n', 'wh': 'w'}
        if first_chars in table.keys():
            term = term[2:]
            code = table[first_chars]
            term_length = len(term)
    elif term[0] == 'x':
        term = ''
        code = 's'
        term_length = 0
    st_trans = {'b': 'b', 'c': 'k', 'd': 't', 'g': 'k', 'h': 'h', 'k': 'k', 'p': 'p', 'q': 'k', 's': 's', 't': 't', 'v': 'f', 'w': 'w', 'x': 'ks', 'y': 'y', 'z': 's'}
    i = 0
    while i < term_length:
        add_char = ''
        part_n_2 = ''
        part_n_3 = ''
        part_n_4 = ''
        part_c_2 = ''
        part_c_3 = ''
        if i < term_length - 1:
            part_n_2 = term[i:i + 2]
            if i > 0:
                part_c_2 = term[i - 1:i + 1]
                part_c_3 = term[i - 1:i + 2]
        if i < term_length - 2:
            part_n_3 = term[i:i + 3]
        if i < term_length - 3:
            part_n_4 = term[i:i + 4]
        if term[i] == 'b':
            add_char = st_trans['b']
            if i == term_length - 1:
                if i > 0:
                    if term[i - 1] == 'm':
                        add_char = ''
        elif term[i] == 'c':
            add_char = st_trans['c']
            if part_n_2 == 'ch':
                add_char = 'x'
            elif re.search('c[iey]', part_n_2):
                add_char = 's'
            if part_n_3 == 'cia':
                add_char = 'x'
            if re.search('sc[iey]', part_c_3):
                add_char = ''
        elif term[i] == 'd':
            add_char = st_trans['d']
            if re.search('dg[eyi]', part_n_3):
                add_char = 'j'
        elif term[i] == 'g':
            add_char = st_trans['g']
            if part_n_2 == 'gh':
                if i == term_length - 2:
                    add_char = ''
            elif re.search('gh[aeiouy]', part_n_3):
                add_char = ''
            elif part_n_2 == 'gn':
                add_char = ''
            elif part_n_4 == 'gned':
                add_char = ''
            elif re.search('dg[eyi]', part_c_3):
                add_char = ''
            elif part_n_2 == 'gi':
                if part_c_3 != 'ggi':
                    add_char = 'j'
            elif part_n_2 == 'ge':
                if part_c_3 != 'gge':
                    add_char = 'j'
            elif part_n_2 == 'gy':
                if part_c_3 != 'ggy':
                    add_char = 'j'
            elif part_n_2 == 'gg':
                add_char = ''
        elif term[i] == 'h':
            add_char = st_trans['h']
            if re.search('[aeiouy]h[^aeiouy]', part_c_3):
                add_char = ''
            elif re.search('[csptg]h', part_c_2):
                add_char = ''
        elif term[i] == 'k':
            add_char = st_trans['k']
            if part_c_2 == 'ck':
                add_char = ''
        elif term[i] == 'p':
            add_char = st_trans['p']
            if part_n_2 == 'ph':
                add_char = 'f'
        elif term[i] == 'q':
            add_char = st_trans['q']
        elif term[i] == 's':
            add_char = st_trans['s']
            if part_n_2 == 'sh':
                add_char = 'x'
            if re.search('si[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 't':
            add_char = st_trans['t']
            if part_n_2 == 'th':
                add_char = '0'
            if re.search('ti[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 'v':
            add_char = st_trans['v']
        elif term[i] == 'w':
            add_char = st_trans['w']
            if re.search('w[^aeiouy]', part_n_2):
                add_char = ''
        elif term[i] == 'x':
            add_char = st_trans['x']
        elif term[i] == 'y':
            add_char = st_trans['y']
        elif term[i] == 'z':
            add_char = st_trans['z']
        else:
            add_char = term[i]
        code = code + add_char
        i += 1
    return code",code = code + add_char,code += add_char,find_wrong,2,,,,
PaddleSlim,https://github.com/PaddlePaddle/PaddleSlim/tree/master/paddleslim/dygraph/prune/filter_pruner.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSlim/paddleslim/dygraph/prune/filter_pruner.py,FilterPruner,"def prune_var(self, var_name, pruned_axis, pruned_ratio, apply='impretive'):
    """"""
        Pruning a variable.
        Parameters:
            var_name(str): The name of variable.
            pruned_axis(int): The axis to be pruned. For convolution with format [out_c, in_c, k, k],
                             'axis=0' means pruning filters.
            pruned_ratio(float): The ratio of pruned values in one variable.
            apply(str): How to apply pruning plan to graph. It can be 'impretive', 'lazy' or None. None
                        means just returning an instance of 'PruningPlan' but not applying it to graph.

        Returns:
            plan: An instance of PruningPlan that can be applied on model by calling 'plan.apply(model)'.

        """"""
    pruned_axis = pruned_axis[0] if isinstance(pruned_axis, list) else pruned_axis
    assert isinstance(pruned_axis, int)
    if var_name in self.skip_vars:
        _logger.warn(f'{var_name} is skiped beacause it is not supported for pruning directly.')
        return
    collection = self.collections.find_collection_by_master(var_name, pruned_axis)
    plan = PruningPlan(self.model.full_name)
    if collection is None:
        _logger.debug(f""Can not find collection with master ['name': {var_name}, 'axis': {pruned_axis}]"")
        return plan
    _logger.info(f'Pruning variable [{var_name}] and its relatives {list(collection.variables())}')
    mask = self.cal_mask(pruned_ratio, collection, num_head=self.num_head)
    for _detail in collection.all_pruning_details():
        src_mask = copy.deepcopy(mask)
        var_shape = _detail.var.shape()
        for tran in _detail.transform:
            src_mask = self._transform_mask(src_mask, tran)
        current_mask = src_mask
        groups = _detail.op.attr('groups')
        if groups is None or groups == 1:
            assert len(current_mask) == var_shape[_detail.axis], f'The length of current_mask must be equal to the size of dimension to be pruned on. But get: len(current_mask): {len(current_mask)}; var_shape: {var_shape}; axis: {_detail.axis}; var name: {_detail.name}; len(mask): {len(mask)}'
        plan.add(_detail.name, PruningMask(_detail.axis, current_mask, pruned_ratio, _detail.op))
    if apply == 'lazy':
        plan.apply(self.model, lazy=True)
    elif apply == 'impretive':
        plan.apply(self.model, lazy=False, opt=self.opt, prune_type=self.prune_type)
    return plan",_logger.warn(f'{var_name} is skiped beacause it is not supported for pruning directly.'),_logger.warn(f'{var_name} is skipped because it is not supported for pruning directly.'),find_wrong,2,,,,
WhatWaf,https://github.com/Ekultek/WhatWaf/tree/master/content/tampers/enclosebrackets.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WhatWaf/content/tampers/enclosebrackets.py,,"def tamper(payload, **kwargs):
    payload = str(payload)
    to_enclose = string.digits
    if not any((i in list(payload) for i in to_enclose)):
        return payload
    retval = ''
    for char in payload:
        if char in to_enclose:
            retval += '[{}]'.format(char)
        else:
            retval += char
    return retval",retval += '[{}]'.format(char),retval += f'[{char}]',find_wrong,,1,3,,
MarkdownEditing,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/references.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MarkdownEditing/plugins/references.py,MdeConvertInlineLinkToReferenceCommand,"def run(self, edit, name=None):
    """"""Run command callback.""""""
    view = self.view
    pattern = '\\[([^\\]]+)\\]\\((?!#)([^\\)]+)\\)'
    whitespace_at_end = view.find('\\s*\\z', 0)
    view.replace(edit, whitespace_at_end, '\n')
    if not view.find('\\n\\s*\\[[^\\]]*\\]:.*\\s*\\z', 0):
        view.insert(edit, view.size(), '\n')
    link_spans = []
    for sel in view.sel():
        if not view.match_selector(sel.b, 'meta.link.inline'):
            continue
        start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
        end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
        text = view.substr(sublime.Region(start, end))
        m = re.match(pattern, text)
        if m is None:
            continue
        text = m.group(1)
        link = m.group(2)
        link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
        if is_url(link):
            link = mangle_url(link)
        if len(link) > 0:
            if name is None:
                suggested_name = check_for_link(view, link)
                if suggested_name is None:
                    is_image = view.substr(start - 1) == '!' if start > 0 else False
                    suggested_name = suggest_default_link_name(text, link, is_image)
            _name = name if name is not None else suggested_name
            link_spans.append((link_span, _name, _name == text))
    offset = 0
    for link_span in link_spans:
        _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
        offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])","""""""\\[([^\\]]+)\\]\\((?!#)([^\\)]+)\\)""""""",f'\\[([^\\]]+)\\]\\((?!#)({[^\\)]+})\\)',find_wrong,2,,,,
MarkdownEditing,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/references.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MarkdownEditing/plugins/references.py,MdeConvertInlineLinkToReferenceCommand,"def run(self, edit, name=None):
    """"""Run command callback.""""""
    view = self.view
    pattern = '\\[([^\\]]+)\\]\\((?!#)([^\\)]+)\\)'
    whitespace_at_end = view.find('\\s*\\z', 0)
    view.replace(edit, whitespace_at_end, '\n')
    if not view.find('\\n\\s*\\[[^\\]]*\\]:.*\\s*\\z', 0):
        view.insert(edit, view.size(), '\n')
    link_spans = []
    for sel in view.sel():
        if not view.match_selector(sel.b, 'meta.link.inline'):
            continue
        start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
        end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
        text = view.substr(sublime.Region(start, end))
        m = re.match(pattern, text)
        if m is None:
            continue
        text = m.group(1)
        link = m.group(2)
        link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
        if is_url(link):
            link = mangle_url(link)
        if len(link) > 0:
            if name is None:
                suggested_name = check_for_link(view, link)
                if suggested_name is None:
                    is_image = view.substr(start - 1) == '!' if start > 0 else False
                    suggested_name = suggest_default_link_name(text, link, is_image)
            _name = name if name is not None else suggested_name
            link_spans.append((link_span, _name, _name == text))
    offset = 0
    for link_span in link_spans:
        _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
        offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])","""""""\\n\\s*\\[[^\\]]*\\]:.*\\s*\\z""""""",f'\\n\\s*\\[[^\\]]*\\]:.*\\s*$',find_wrong,2,,,,
MarkdownEditing,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/references.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MarkdownEditing/plugins/references.py,MdeConvertInlineLinkToReferenceCommand,"def run(self, edit, name=None):
    """"""Run command callback.""""""
    view = self.view
    pattern = '\\[([^\\]]+)\\]\\((?!#)([^\\)]+)\\)'
    whitespace_at_end = view.find('\\s*\\z', 0)
    view.replace(edit, whitespace_at_end, '\n')
    if not view.find('\\n\\s*\\[[^\\]]*\\]:.*\\s*\\z', 0):
        view.insert(edit, view.size(), '\n')
    link_spans = []
    for sel in view.sel():
        if not view.match_selector(sel.b, 'meta.link.inline'):
            continue
        start = findScopeFrom(view, sel.b, marker_begin_scope_name, backwards=True)
        end = findScopeFrom(view, sel.b, marker_end_scope_name) + 1
        text = view.substr(sublime.Region(start, end))
        m = re.match(pattern, text)
        if m is None:
            continue
        text = m.group(1)
        link = m.group(2)
        link_span = sublime.Region(start + m.span(2)[0] - 1, start + m.span(2)[1] + 1)
        if is_url(link):
            link = mangle_url(link)
        if len(link) > 0:
            if name is None:
                suggested_name = check_for_link(view, link)
                if suggested_name is None:
                    is_image = view.substr(start - 1) == '!' if start > 0 else False
                    suggested_name = suggest_default_link_name(text, link, is_image)
            _name = name if name is not None else suggested_name
            link_spans.append((link_span, _name, _name == text))
    offset = 0
    for link_span in link_spans:
        _link_span = sublime.Region(link_span[0].a + offset, link_span[0].b + offset)
        offset -= convert2ref(view, edit, _link_span, link_span[1], link_span[2])","""""""!""""""","'!' in view.substr(sublime.Region(start - 1, start))",find_wrong,2,,,,
mars,https://github.com/mars-project/mars/tree/master/mars/lib/filesystem/oss.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mars/mars/lib/filesystem/oss.py,OSSFileSystem,"def ls(self, path: path_type) -> List[path_type]:
    file_list = []
    file_entry = oc.OSSFileEntry(path)
    if not file_entry.is_dir():
        raise OSError('ls for file is not supported')
    else:
        (bucket, key, access_key_id, access_key_secret, end_point) = oc.parse_osspath(path)
        oss_bucket = oss2.Bucket(auth=oss2.Auth(access_key_id=access_key_id, access_key_secret=access_key_secret), endpoint=end_point, bucket_name=bucket, connect_timeout=_oss_time_out)
        for obj in oss2.ObjectIteratorV2(oss_bucket, prefix=key):
            if obj.key.endswith('/'):
                continue
            obj_path = f'oss://{bucket}/{obj.key}'
            file_list.append(obj_path)
    return file_list","obj_path = 'oss://{}/{}'.format(bucket, obj.key)",obj_path = f'oss://{bucket}/{obj.key}',find_wrong,,1,3,,
mypy,https://github.com/python/mypy/tree/master/mypy/suggestions.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mypy/mypy/suggestions.py,SuggestionEngine,"def find_node(self, key: str) -> Tuple[str, str, FuncDef]:
    """"""From a target name, return module/target names and the func def.

        The 'key' argument can be in one of two formats:
        * As the function full name, e.g., package.module.Cls.method
        * As the function location as file and line separated by column,
          e.g., path/to/file.py:42
        """"""
    node: Optional[SymbolNode] = None
    if ':' in key:
        if key.count(':') > 1:
            raise SuggestionFailure('Malformed location for function: {}. Must be either package.module.Class.method or path/to/file.py:line'.format(key))
        (file, line) = key.split(':')
        if not line.isdigit():
            raise SuggestionFailure('Line number must be a number. Got {}'.format(line))
        line_number = int(line)
        (modname, node) = self.find_node_by_file_and_line(file, line_number)
        tail = node.fullname[len(modname) + 1:]
    else:
        target = split_target(self.fgmanager.graph, key)
        if not target:
            raise SuggestionFailure('Cannot find module for %s' % (key,))
        (modname, tail) = target
        node = self.find_node_by_module_and_name(modname, tail)
    if isinstance(node, Decorator):
        node = self.extract_from_decorator(node)
        if not node:
            raise SuggestionFailure(""Object %s is a decorator we can't handle"" % key)
    if not isinstance(node, FuncDef):
        raise SuggestionFailure('Object %s is not a function' % key)
    return (modname, tail, node)",'Malformed location for function: {}. Must be either package.module.Class.method or path/to/file.py:line'.format(key),f'Malformed location for function: {key}. Must be either package.module.Class.method or path/to/file.py:line',find_wrong,2,,,,
mypy,https://github.com/python/mypy/tree/master/mypy/suggestions.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mypy/mypy/suggestions.py,SuggestionEngine,"def find_node(self, key: str) -> Tuple[str, str, FuncDef]:
    """"""From a target name, return module/target names and the func def.

        The 'key' argument can be in one of two formats:
        * As the function full name, e.g., package.module.Cls.method
        * As the function location as file and line separated by column,
          e.g., path/to/file.py:42
        """"""
    node: Optional[SymbolNode] = None
    if ':' in key:
        if key.count(':') > 1:
            raise SuggestionFailure('Malformed location for function: {}. Must be either package.module.Class.method or path/to/file.py:line'.format(key))
        (file, line) = key.split(':')
        if not line.isdigit():
            raise SuggestionFailure('Line number must be a number. Got {}'.format(line))
        line_number = int(line)
        (modname, node) = self.find_node_by_file_and_line(file, line_number)
        tail = node.fullname[len(modname) + 1:]
    else:
        target = split_target(self.fgmanager.graph, key)
        if not target:
            raise SuggestionFailure('Cannot find module for %s' % (key,))
        (modname, tail) = target
        node = self.find_node_by_module_and_name(modname, tail)
    if isinstance(node, Decorator):
        node = self.extract_from_decorator(node)
        if not node:
            raise SuggestionFailure(""Object %s is a decorator we can't handle"" % key)
    if not isinstance(node, FuncDef):
        raise SuggestionFailure('Object %s is not a function' % key)
    return (modname, tail, node)",'Line number must be a number. Got {}'.format(line),f'Line number must be a number. Got {line}',find_wrong,,1,3,,
pyglossary,https://github.com/ilius/pyglossary/tree/master/pyglossary/ui/ui_cmd_interactive.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglossary/pyglossary/ui/ui_cmd_interactive.py,UI,"def __init__(self):
    self._inputFilename = ''
    self._outputFilename = ''
    self._inputFormat = ''
    self._outputFormat = ''
    self.config = None
    self._readOptions = None
    self._writeOptions = None
    self._convertOptions = None
    ui_cmd.UI.__init__(self)
    self.ls_parser = argparse.ArgumentParser(add_help=False)
    self.ls_parser.add_argument('-l', '--long', action='store_true', dest='long', help='use a long listing format')
    self.ls_parser.add_argument('--help', action='store_true', dest='help', help='display help')
    self.ls_usage = 'Usage: !ls [--help] [-l] [FILE/DIRECTORY]...\n\noptional arguments:\n    --help      show this help message and exit\n    -l, --long  use a long listing format\n'
    self._fsActions = OrderedDict([('!pwd', (self.fs_pwd, '')), ('!ls', (self.fs_ls, self.ls_usage)), ('!..', (self.fs_cd_parent, '')), ('!cd', (self.fs_cd, ''))])
    self._finalActions = OrderedDict([('formats', self.askFormats), ('read-options', self.askReadOptions), ('write-options', self.askWriteOptions), ('reset-read-options', self.resetReadOptions), ('reset-write-options', self.resetWriteOptions), ('config', self.askConfig), ('indirect', self.setIndirect), ('sqlite', self.setSQLite), ('no-progressbar', self.setNoProgressbar), ('sort', self.setSort), ('sort-key', self.setSortKey), ('show-options', self.showOptions), ('back', None)])","self.ls_usage = 'Usage: !ls [--help] [-l] [FILE/DIRECTORY]...\n\noptional arguments:\n    --help      show this help message and exit\n    -l, --long  use a long listing format\n'","self.ls_usage = f'Usage: !ls [--help] [-l] [FILE/DIRECTORY]...\n\noptional arguments:\n    --help      show this help message and exit\n    -l, --long  use a long listing format\n'",find_wrong,2,,,,
WatchAD,https://github.com/Qianlitp/WatchAD/tree/master/models/Log.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WatchAD/models/Log.py,TargetInfo,"def __init__(self, event_data):
    self.domain_name = None
    self.user_name = None
    self.user_sid = None
    self.logon_id = None
    self.info = None
    self.server_name = None
    self.sid = None
    self.full_user_name = None
    self._field_map = {'TargetDomainName': 'domain_name', 'TargetUserName': 'user_name', 'TargetUserSid': 'user_sid', 'TargetSid': 'sid', 'TargetLogonId': 'logon_id', 'TargetInfo': 'info', 'TargetServerName': 'server_name'}
    for (key, value) in self._field_map.items():
        if key not in event_data:
            continue
        if key == 'TargetUserName':
            if '@' in event_data[key]:
                user_name = event_data[key].split('@')[0]
                self.__dict__.update({value: user_name})
            else:
                self.__dict__.update({value: event_data[key]})
            self.__dict__.update({'full_user_name': event_data[key]})
        elif key in event_data:
            self.__dict__.update({value: event_data[key]})","""""""user_name = event_data[key]
self.__dict__.update({value: user_name})""""""",f'self.__dict__.update({value}: {event_data[key]})',find_wrong,2,,,,
mindsdb,https://github.com/mindsdb/mindsdb/tree/master/mindsdb/api/mysql/mysql_proxy/classes/sql_query.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mindsdb/mindsdb/api/mysql/mysql_proxy/classes/sql_query.py,SQLQuery,"def _fetch_dataframe_step(self, step):
    dn = self.datahub.get(step.integration)
    query = step.query
    if dn is None:
        raise SqlApiUnknownError(f'Unknown integration name: {step.integration}')
    if query is None:
        table_alias = (self.database, 'result', 'result')
        (data, columns_info) = dn.query(native_query=step.raw_query, session=self.session)
    else:
        table_alias = get_table_alias(step.query.from_table, self.database)
        (data, columns_info) = dn.query(query=query, session=self.session)
    if isinstance(data, ASTNode):
        subquery = SQLQuery(data, session=self.session)
        return subquery.fetched_data
    columns = [(column['name'], column['name']) for column in columns_info]
    data = [{(key, key): value for (key, value) in row.items()} for row in data]
    data = [{table_alias: x} for x in data]
    col_types = {column['name']: column['type'] for column in columns_info}
    columns_collection = ColumnsCollection()
    for column in columns:
        columns_collection.add(table_alias, column)
    data = {'values': data, 'columns': columns_collection, 'tables': [table_alias], 'types': {table_alias: col_types}}
    return data",f'Unknown integration name: {step.integration}','Unknown integration name: {}'.format(step.integration),find_wrong,2,,,,
mindsdb,https://github.com/mindsdb/mindsdb/tree/master/mindsdb/api/mysql/mysql_proxy/classes/sql_query.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mindsdb/mindsdb/api/mysql/mysql_proxy/classes/sql_query.py,SQLQuery,"def _fetch_dataframe_step(self, step):
    dn = self.datahub.get(step.integration)
    query = step.query
    if dn is None:
        raise SqlApiUnknownError(f'Unknown integration name: {step.integration}')
    if query is None:
        table_alias = (self.database, 'result', 'result')
        (data, columns_info) = dn.query(native_query=step.raw_query, session=self.session)
    else:
        table_alias = get_table_alias(step.query.from_table, self.database)
        (data, columns_info) = dn.query(query=query, session=self.session)
    if isinstance(data, ASTNode):
        subquery = SQLQuery(data, session=self.session)
        return subquery.fetched_data
    columns = [(column['name'], column['name']) for column in columns_info]
    data = [{(key, key): value for (key, value) in row.items()} for row in data]
    data = [{table_alias: x} for x in data]
    col_types = {column['name']: column['type'] for column in columns_info}
    columns_collection = ColumnsCollection()
    for column in columns:
        columns_collection.add(table_alias, column)
    data = {'values': data, 'columns': columns_collection, 'tables': [table_alias], 'types': {table_alias: col_types}}
    return data",f'{table_alias: x}','{:x}'.format(table_alias),find_wrong,,1,3,,
geopy,https://github.com/geopy/geopy/tree/master/geopy/geocoders/smartystreets.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/geopy/geopy/geocoders/smartystreets.py,LiveAddress,"def __init__(self, auth_id, auth_token, *, timeout=DEFAULT_SENTINEL, proxies=DEFAULT_SENTINEL, user_agent=None, ssl_context=DEFAULT_SENTINEL, adapter_factory=None):
    """"""

        :param str auth_id: Valid `Auth ID` from SmartyStreets.

        :param str auth_token: Valid `Auth Token` from SmartyStreets.

        :param int timeout:
            See :attr:`geopy.geocoders.options.default_timeout`.

        :param dict proxies:
            See :attr:`geopy.geocoders.options.default_proxies`.

        :param str user_agent:
            See :attr:`geopy.geocoders.options.default_user_agent`.

        :type ssl_context: :class:`ssl.SSLContext`
        :param ssl_context:
            See :attr:`geopy.geocoders.options.default_ssl_context`.

        :param callable adapter_factory:
            See :attr:`geopy.geocoders.options.default_adapter_factory`.

            .. versionadded:: 2.0
        """"""
    super().__init__(scheme='https', timeout=timeout, proxies=proxies, user_agent=user_agent, ssl_context=ssl_context, adapter_factory=adapter_factory)
    self.auth_id = auth_id
    self.auth_token = auth_token
    domain = 'api.smartystreets.com'
    self.api = '%s://%s%s' % (self.scheme, domain, self.geocode_path)","self.api = '%s://%s%s' % (self.scheme, domain, self.geocode_path)",self.api = f'{self.scheme}://{domain}{self.geocode_path}',find_wrong,,,,,
docassemble,https://github.com/jhpyle/docassemble/tree/master/tests/features/steps/docassemble.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/tests/features/steps/docassemble.py,,"def select_nth_option(step, value, ordinal, label):
    try:
        elem = world.browser.find_element_by_id(world.browser.find_element_by_xpath('(//label[text()=""' + label + '""])[' + str(1 + 2 * (number_from_ordinal[ordinal] - 1)) + ']').get_attribute('for'))
    except:
        label += '\xa0'
        elem = world.browser.find_element_by_id(world.browser.find_element_by_xpath('(//label[text()=""' + label + '""])[' + str(1 + 2 * (number_from_ordinal[ordinal] - 1)) + ']').get_attribute('for'))
    found = False
    for option in elem.find_elements_by_tag_name('option'):
        if option.text == value:
            found = True
            option.click()
            break
    assert found",'(//label[text()="' + label + '"])[' + str(1 + 2 * (number_from_ordinal[ordinal] - 1)) + ']',f'(//label[text()="{label}"])[{1 + 2 * (number_from_ordinal[ordinal] - 1)}]',find_wrong,,1,3,,
nlp-recipes,https://github.com/microsoft/nlp-recipes/tree/master/examples/sentence_similarity/gensen_train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp-recipes/examples/sentence_similarity/gensen_train.py,,"def train(config, data_folder, learning_rate=0.0001, max_epoch=None):
    """""" Train the Gensen model.

    Args:
        max_epoch(int): Limit training to specified number of epochs.
        config(dict): Loaded json file as a python object.
        data_folder(str): Path to the folder containing the data.
        learning_rate(float): Learning rate for the model.
    """"""
    owd = os.getcwd()
    os.chdir(data_folder)
    try:
        with mlflow.start_run():
            save_dir = config['data']['save_dir']
            if not os.path.exists('./log'):
                os.makedirs('./log')
            os.makedirs(save_dir, exist_ok=True)
            setup_logging(config)
            batch_size = config['training']['batch_size']
            src_vocab_size = config['model']['n_words_src']
            trg_vocab_size = config['model']['n_words_trg']
            max_len_src = config['data']['max_src_length']
            max_len_trg = config['data']['max_trg_length']
            model_state = {}
            train_src = [item['train_src'] for item in config['data']['paths']]
            train_trg = [item['train_trg'] for item in config['data']['paths']]
            tasknames = [item['taskname'] for item in config['data']['paths']]
            if 'skipthought_next' in tasknames and 'skipthought_previous' in tasknames:
                skipthought_idx = tasknames.index('skipthought_next')
                skipthought_backward_idx = tasknames.index('skipthought_previous')
                paired_tasks = {skipthought_idx: skipthought_backward_idx, skipthought_backward_idx: skipthought_idx}
            else:
                paired_tasks = None
                skipthought_idx = None
                skipthought_backward_idx = None
            train_iterator = BufferedDataIterator(train_src, train_trg, src_vocab_size, trg_vocab_size, tasknames, save_dir, buffer_size=1000000.0, lowercase=True, seed=(hvd.rank() + 1) * 12345)
            nli_iterator = NLIIterator(train=config['data']['nli_train'], dev=config['data']['nli_dev'], test=config['data']['nli_test'], vocab_size=-1, vocab=os.path.join(save_dir, 'src_vocab.pkl'), seed=(hvd.rank() + 1) * 12345)
            src_vocab_size = len(train_iterator.src[0]['word2id'])
            trg_vocab_size = len(train_iterator.trg[0]['word2id'])
            logging.info('Finished creating iterator ...')
            log_config(config)
            logging.info('Found %d words in source : ' % len(train_iterator.src[0]['id2word']))
            for (idx, taskname) in enumerate(tasknames):
                logging.info('Found %d target words in task %s ' % (len(train_iterator.trg[idx]['id2word']), taskname))
            logging.info('Found %d words in src ' % src_vocab_size)
            logging.info('Found %d words in trg ' % trg_vocab_size)
            weight_mask = torch.ones(trg_vocab_size).cuda()
            weight_mask[train_iterator.trg[0]['word2id']['<pad>']] = 0
            loss_criterion = nn.CrossEntropyLoss(weight=weight_mask).cuda()
            nli_criterion = nn.CrossEntropyLoss().cuda()
            model = MultitaskModel(src_emb_dim=config['model']['dim_word_src'], trg_emb_dim=config['model']['dim_word_trg'], src_vocab_size=src_vocab_size, trg_vocab_size=trg_vocab_size, src_hidden_dim=config['model']['dim_src'], trg_hidden_dim=config['model']['dim_trg'], bidirectional=config['model']['bidirectional'], pad_token_src=train_iterator.src[0]['word2id']['<pad>'], pad_token_trg=train_iterator.trg[0]['word2id']['<pad>'], nlayers_src=config['model']['n_layers_src'], dropout=config['model']['dropout'], num_tasks=len(train_iterator.src), paired_tasks=paired_tasks).cuda()
            optimizer = setup_horovod(model, learning_rate=learning_rate)
            logging.info(model)
            n_gpus = config['training']['n_gpus']
            model = torch.nn.DataParallel(model, device_ids=range(n_gpus))
            task_losses = [[] for _ in tasknames]
            task_idxs = [0 for _ in tasknames]
            nli_losses = []
            updates = 0
            nli_ctr = 0
            nli_epoch = 0
            monitor_epoch = 0
            nli_mbatch_ctr = 0
            mbatch_times = []
            min_val_loss = 10000000
            min_val_loss_epoch = -1
            rng_num_tasks = len(tasknames) - 1 if paired_tasks else len(tasknames)
            logging.info('OS Environ: \n {} \n\n'.format(os.environ))
            mlflow.log_param('learning_rate', learning_rate)
            logging.info('Commencing Training ...')
            start = time.time()
            while True:
                batch_start_time = time.time()
                if nli_ctr % 10 == 0:
                    minibatch = nli_iterator.get_parallel_minibatch(nli_mbatch_ctr, batch_size * n_gpus)
                    optimizer.zero_grad()
                    class_logits = model(minibatch, -1, return_hidden=False, paired_trg=None)
                    loss = nli_criterion(class_logits.contiguous().view(-1, class_logits.size(1)), minibatch['labels'].contiguous().view(-1))
                    nli_losses.append(loss.item())
                    loss.backward()
                    torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)
                    optimizer.step()
                    nli_mbatch_ctr += batch_size * n_gpus
                    if nli_mbatch_ctr >= len(nli_iterator.train_lines):
                        nli_mbatch_ctr = 0
                        nli_epoch += 1
                else:
                    task_idx = np.random.randint(low=0, high=rng_num_tasks)
                    minibatch = train_iterator.get_parallel_minibatch(task_idx, task_idxs[task_idx], batch_size * n_gpus, max_len_src, max_len_trg)
                    'Increment pointer into task and if current buffer is\n                    exhausted, fetch new buffer. '
                    task_idxs[task_idx] += batch_size * n_gpus
                    if task_idxs[task_idx] >= train_iterator.buffer_size:
                        train_iterator.fetch_buffer(task_idx)
                        task_idxs[task_idx] = 0
                    if task_idx == skipthought_idx:
                        minibatch_back = train_iterator.get_parallel_minibatch(skipthought_backward_idx, task_idxs[skipthought_backward_idx], batch_size * n_gpus, max_len_src, max_len_trg)
                        task_idxs[skipthought_backward_idx] += batch_size * n_gpus
                        if task_idxs[skipthought_backward_idx] >= train_iterator.buffer_size:
                            train_iterator.fetch_buffer(skipthought_backward_idx)
                            task_idxs[skipthought_backward_idx] = 0
                        optimizer.zero_grad()
                        (decoder_logit, decoder_logit_2) = model(minibatch, task_idx, paired_trg=minibatch_back['input_trg'])
                        loss_f = loss_criterion(decoder_logit.contiguous().view(-1, decoder_logit.size(2)), minibatch['output_trg'].contiguous().view(-1))
                        loss_b = loss_criterion(decoder_logit_2.contiguous().view(-1, decoder_logit_2.size(2)), minibatch_back['output_trg'].contiguous().view(-1))
                        task_losses[task_idx].append(loss_f.data[0])
                        task_losses[skipthought_backward_idx].append(loss_b.data[0])
                        loss = loss_f + loss_b
                    else:
                        optimizer.zero_grad()
                        decoder_logit = model(minibatch, task_idx)
                        loss = loss_criterion(decoder_logit.contiguous().view(-1, decoder_logit.size(2)), minibatch['output_trg'].contiguous().view(-1))
                        task_losses[task_idx].append(loss.item())
                    loss.backward()
                    optimizer.synchronize()
                    torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)
                    optimizer.step()
                end = time.time()
                mbatch_times.append(end - batch_start_time)
                if updates % config['management']['monitor_loss'] == 0 and updates != 0:
                    monitor_epoch += 1
                    for (idx, task) in enumerate(tasknames):
                        logging.info('Seq2Seq Examples Processed : %d %s Loss : %.5f Num %s minibatches : %d' % (updates, task, np.mean(task_losses[idx]), task, len(task_losses[idx])))
                        mlflow.log_metric('validation_loss', np.mean(task_losses[idx]), step=monitor_epoch)
                    logging.info('Round: %d NLI Epoch : %d NLI Examples Processed : %d NLI Loss : %.5f ' % (nli_ctr, nli_epoch, nli_mbatch_ctr, np.mean(nli_losses)))
                    mlflow.log_metric('nli_loss', np.mean(nli_losses), step=nli_epoch)
                    logging.info('Average time per minibatch : %.5f' % np.mean(mbatch_times))
                    mlflow.log_metric('minibatch_avg_duration', np.mean(mbatch_times))
                    task_losses = [[] for _ in tasknames]
                    mbatch_times = []
                    nli_losses = []
                    logging.info('############################')
                    logging.info('##### Evaluating model #####')
                    logging.info('############################')
                    (training_complete, min_val_loss_epoch, min_val_loss, model_state) = evaluate(config=config, train_iterator=train_iterator, model=model, loss_criterion=loss_criterion, monitor_epoch=monitor_epoch, min_val_loss=min_val_loss, min_val_loss_epoch=min_val_loss_epoch, save_dir=save_dir, starting_time=start, model_state=model_state, max_epoch=max_epoch)
                    if training_complete:
                        mlflow.log_metric('min_val_loss', float(min_val_loss))
                        mlflow.log_metric('learning_rate', learning_rate)
                        break
                    logging.info('Evaluating on NLI')
                    evaluate_nli(nli_iterator=nli_iterator, model=model, n_gpus=n_gpus, batch_size=batch_size)
                updates += batch_size * n_gpus
                nli_ctr += 1
                logging.info('Updates: %d' % updates)
    finally:
        os.chdir(owd)","Found %d target words in task %s ' % (len(train_iterator.trg[idx]['id2word']), taskname)",f"Found {len(train_iterator.trg[idx]['id2word'])} target words in task {taskname}",find_wrong,,1,3,1,
nlp-recipes,https://github.com/microsoft/nlp-recipes/tree/master/examples/sentence_similarity/gensen_train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp-recipes/examples/sentence_similarity/gensen_train.py,,"def train(config, data_folder, learning_rate=0.0001, max_epoch=None):
    """""" Train the Gensen model.

    Args:
        max_epoch(int): Limit training to specified number of epochs.
        config(dict): Loaded json file as a python object.
        data_folder(str): Path to the folder containing the data.
        learning_rate(float): Learning rate for the model.
    """"""
    owd = os.getcwd()
    os.chdir(data_folder)
    try:
        with mlflow.start_run():
            save_dir = config['data']['save_dir']
            if not os.path.exists('./log'):
                os.makedirs('./log')
            os.makedirs(save_dir, exist_ok=True)
            setup_logging(config)
            batch_size = config['training']['batch_size']
            src_vocab_size = config['model']['n_words_src']
            trg_vocab_size = config['model']['n_words_trg']
            max_len_src = config['data']['max_src_length']
            max_len_trg = config['data']['max_trg_length']
            model_state = {}
            train_src = [item['train_src'] for item in config['data']['paths']]
            train_trg = [item['train_trg'] for item in config['data']['paths']]
            tasknames = [item['taskname'] for item in config['data']['paths']]
            if 'skipthought_next' in tasknames and 'skipthought_previous' in tasknames:
                skipthought_idx = tasknames.index('skipthought_next')
                skipthought_backward_idx = tasknames.index('skipthought_previous')
                paired_tasks = {skipthought_idx: skipthought_backward_idx, skipthought_backward_idx: skipthought_idx}
            else:
                paired_tasks = None
                skipthought_idx = None
                skipthought_backward_idx = None
            train_iterator = BufferedDataIterator(train_src, train_trg, src_vocab_size, trg_vocab_size, tasknames, save_dir, buffer_size=1000000.0, lowercase=True, seed=(hvd.rank() + 1) * 12345)
            nli_iterator = NLIIterator(train=config['data']['nli_train'], dev=config['data']['nli_dev'], test=config['data']['nli_test'], vocab_size=-1, vocab=os.path.join(save_dir, 'src_vocab.pkl'), seed=(hvd.rank() + 1) * 12345)
            src_vocab_size = len(train_iterator.src[0]['word2id'])
            trg_vocab_size = len(train_iterator.trg[0]['word2id'])
            logging.info('Finished creating iterator ...')
            log_config(config)
            logging.info('Found %d words in source : ' % len(train_iterator.src[0]['id2word']))
            for (idx, taskname) in enumerate(tasknames):
                logging.info('Found %d target words in task %s ' % (len(train_iterator.trg[idx]['id2word']), taskname))
            logging.info('Found %d words in src ' % src_vocab_size)
            logging.info('Found %d words in trg ' % trg_vocab_size)
            weight_mask = torch.ones(trg_vocab_size).cuda()
            weight_mask[train_iterator.trg[0]['word2id']['<pad>']] = 0
            loss_criterion = nn.CrossEntropyLoss(weight=weight_mask).cuda()
            nli_criterion = nn.CrossEntropyLoss().cuda()
            model = MultitaskModel(src_emb_dim=config['model']['dim_word_src'], trg_emb_dim=config['model']['dim_word_trg'], src_vocab_size=src_vocab_size, trg_vocab_size=trg_vocab_size, src_hidden_dim=config['model']['dim_src'], trg_hidden_dim=config['model']['dim_trg'], bidirectional=config['model']['bidirectional'], pad_token_src=train_iterator.src[0]['word2id']['<pad>'], pad_token_trg=train_iterator.trg[0]['word2id']['<pad>'], nlayers_src=config['model']['n_layers_src'], dropout=config['model']['dropout'], num_tasks=len(train_iterator.src), paired_tasks=paired_tasks).cuda()
            optimizer = setup_horovod(model, learning_rate=learning_rate)
            logging.info(model)
            n_gpus = config['training']['n_gpus']
            model = torch.nn.DataParallel(model, device_ids=range(n_gpus))
            task_losses = [[] for _ in tasknames]
            task_idxs = [0 for _ in tasknames]
            nli_losses = []
            updates = 0
            nli_ctr = 0
            nli_epoch = 0
            monitor_epoch = 0
            nli_mbatch_ctr = 0
            mbatch_times = []
            min_val_loss = 10000000
            min_val_loss_epoch = -1
            rng_num_tasks = len(tasknames) - 1 if paired_tasks else len(tasknames)
            logging.info('OS Environ: \n {} \n\n'.format(os.environ))
            mlflow.log_param('learning_rate', learning_rate)
            logging.info('Commencing Training ...')
            start = time.time()
            while True:
                batch_start_time = time.time()
                if nli_ctr % 10 == 0:
                    minibatch = nli_iterator.get_parallel_minibatch(nli_mbatch_ctr, batch_size * n_gpus)
                    optimizer.zero_grad()
                    class_logits = model(minibatch, -1, return_hidden=False, paired_trg=None)
                    loss = nli_criterion(class_logits.contiguous().view(-1, class_logits.size(1)), minibatch['labels'].contiguous().view(-1))
                    nli_losses.append(loss.item())
                    loss.backward()
                    torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)
                    optimizer.step()
                    nli_mbatch_ctr += batch_size * n_gpus
                    if nli_mbatch_ctr >= len(nli_iterator.train_lines):
                        nli_mbatch_ctr = 0
                        nli_epoch += 1
                else:
                    task_idx = np.random.randint(low=0, high=rng_num_tasks)
                    minibatch = train_iterator.get_parallel_minibatch(task_idx, task_idxs[task_idx], batch_size * n_gpus, max_len_src, max_len_trg)
                    'Increment pointer into task and if current buffer is\n                    exhausted, fetch new buffer. '
                    task_idxs[task_idx] += batch_size * n_gpus
                    if task_idxs[task_idx] >= train_iterator.buffer_size:
                        train_iterator.fetch_buffer(task_idx)
                        task_idxs[task_idx] = 0
                    if task_idx == skipthought_idx:
                        minibatch_back = train_iterator.get_parallel_minibatch(skipthought_backward_idx, task_idxs[skipthought_backward_idx], batch_size * n_gpus, max_len_src, max_len_trg)
                        task_idxs[skipthought_backward_idx] += batch_size * n_gpus
                        if task_idxs[skipthought_backward_idx] >= train_iterator.buffer_size:
                            train_iterator.fetch_buffer(skipthought_backward_idx)
                            task_idxs[skipthought_backward_idx] = 0
                        optimizer.zero_grad()
                        (decoder_logit, decoder_logit_2) = model(minibatch, task_idx, paired_trg=minibatch_back['input_trg'])
                        loss_f = loss_criterion(decoder_logit.contiguous().view(-1, decoder_logit.size(2)), minibatch['output_trg'].contiguous().view(-1))
                        loss_b = loss_criterion(decoder_logit_2.contiguous().view(-1, decoder_logit_2.size(2)), minibatch_back['output_trg'].contiguous().view(-1))
                        task_losses[task_idx].append(loss_f.data[0])
                        task_losses[skipthought_backward_idx].append(loss_b.data[0])
                        loss = loss_f + loss_b
                    else:
                        optimizer.zero_grad()
                        decoder_logit = model(minibatch, task_idx)
                        loss = loss_criterion(decoder_logit.contiguous().view(-1, decoder_logit.size(2)), minibatch['output_trg'].contiguous().view(-1))
                        task_losses[task_idx].append(loss.item())
                    loss.backward()
                    optimizer.synchronize()
                    torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)
                    optimizer.step()
                end = time.time()
                mbatch_times.append(end - batch_start_time)
                if updates % config['management']['monitor_loss'] == 0 and updates != 0:
                    monitor_epoch += 1
                    for (idx, task) in enumerate(tasknames):
                        logging.info('Seq2Seq Examples Processed : %d %s Loss : %.5f Num %s minibatches : %d' % (updates, task, np.mean(task_losses[idx]), task, len(task_losses[idx])))
                        mlflow.log_metric('validation_loss', np.mean(task_losses[idx]), step=monitor_epoch)
                    logging.info('Round: %d NLI Epoch : %d NLI Examples Processed : %d NLI Loss : %.5f ' % (nli_ctr, nli_epoch, nli_mbatch_ctr, np.mean(nli_losses)))
                    mlflow.log_metric('nli_loss', np.mean(nli_losses), step=nli_epoch)
                    logging.info('Average time per minibatch : %.5f' % np.mean(mbatch_times))
                    mlflow.log_metric('minibatch_avg_duration', np.mean(mbatch_times))
                    task_losses = [[] for _ in tasknames]
                    mbatch_times = []
                    nli_losses = []
                    logging.info('############################')
                    logging.info('##### Evaluating model #####')
                    logging.info('############################')
                    (training_complete, min_val_loss_epoch, min_val_loss, model_state) = evaluate(config=config, train_iterator=train_iterator, model=model, loss_criterion=loss_criterion, monitor_epoch=monitor_epoch, min_val_loss=min_val_loss, min_val_loss_epoch=min_val_loss_epoch, save_dir=save_dir, starting_time=start, model_state=model_state, max_epoch=max_epoch)
                    if training_complete:
                        mlflow.log_metric('min_val_loss', float(min_val_loss))
                        mlflow.log_metric('learning_rate', learning_rate)
                        break
                    logging.info('Evaluating on NLI')
                    evaluate_nli(nli_iterator=nli_iterator, model=model, n_gpus=n_gpus, batch_size=batch_size)
                updates += batch_size * n_gpus
                nli_ctr += 1
                logging.info('Updates: %d' % updates)
    finally:
        os.chdir(owd)","Seq2Seq Examples Processed : %d %s Loss : %.5f Num %s minibatches : %d' % (updates, task, np.mean(task_losses[idx]), task, len(task_losses[idx])))",f'Seq2Seq Examples Processed : {updates} {task} Loss : {np.mean(task_losses[idx]):.5f} Num {task} minibatches : {len(task_losses[idx])}',find_wrong,,1,3,1,
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data",function = f'{__name__}.{sys._getframe().f_code.co_name}',function = f'{__name__}.{sys._getframe().f_code.co_name}',find_wrong,2,,,,
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data",log_data['message'] = 'Skipping task: An identical task is currently running',log_data['message'] = f'Skipping task: An identical task is currently running',find_wrong,2,,,,
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data","log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})","log.debug({**log_data, 'message': f'`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})",find_wrong,2,,,,
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data","config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz')","config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', default='account_resource_cache/cache_all_roles_v1.json.gz')",find_wrong,2,,,,
consoleme,https://github.com/Netflix/consoleme/tree/master/consoleme/celery_tasks/celery_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consoleme/consoleme/celery_tasks/celery_tasks.py,,"def cache_iam_resources_across_accounts(run_subtasks: bool=True, wait_for_subtask_completion: bool=True) -> Dict:
    function = f'{__name__}.{sys._getframe().f_code.co_name}'
    cache_keys = {'iam_roles': {'cache_key': config.get('aws.iamroles_redis_key', 'IAM_ROLE_CACHE'), 'temp_cache_key': config.get('aws.iamroles_redis_key_temp', 'IAM_ROLE_CACHE_TEMP')}, 'iam_users': {'cache_key': config.get('aws.iamusers_redis_key', 'IAM_USER_CACHE'), 'temp_cache_key': config.get('aws.iamusers_redis_key_temp', 'IAM_USER_CACHE_TEMP')}, 'iam_groups': {'cache_key': config.get('aws.iamgroups_redis_key', 'IAM_GROUP_CACHE'), 'temp_cache_key': config.get('aws.iamgroups_redis_key_temp', 'IAM_GROUP_CACHE_TEMP')}, 'iam_policies': {'cache_key': config.get('aws.iampolicies_redis_key', 'IAM_POLICY_CACHE'), 'temp_cache_key': config.get('aws.iampolicies_redis_key_temp', 'IAM_POLICIES_CACHE_TEMP')}}
    log_data = {'function': function, 'cache_keys': cache_keys}
    if is_task_already_running(function, []):
        log_data['message'] = 'Skipping task: An identical task is currently running'
        log.debug(log_data)
        return log_data
    if run_subtasks and wait_for_subtask_completion:
        for (k, v) in cache_keys.items():
            temp_cache_key = v['temp_cache_key']
            red.delete(temp_cache_key)
    accounts_d: Dict[str, str] = async_to_sync(get_account_id_to_name_mapping)()
    tasks = []
    if config.region == config.get('celery.active_region', config.region) or config.get('environment') in ['dev']:
        for account_id in accounts_d.keys():
            if config.get('environment') in ['prod', 'dev']:
                tasks.append(cache_iam_resources_for_account.s(account_id))
            else:
                log.debug({**log_data, 'message': '`environment` configuration is not set. Only running tasks for accounts in configuration key `celery.test_account_ids`'})
                if account_id in config.get('celery.test_account_ids', []):
                    tasks.append(cache_iam_resources_for_account.s(account_id))
        if run_subtasks:
            results = group(*tasks).apply_async()
            if wait_for_subtask_completion:
                results.join(disable_sync_subtasks=False)
    else:
        log.debug({**log_data, 'message': 'Running in non-active region. Caching roles from DynamoDB and not directly from AWS'})
        dynamo = IAMRoleDynamoHandler()
        roles = dynamo.fetch_all_roles()
        for role_entry in roles:
            _add_role_to_redis(cache_keys['iam_roles']['cache_key'], role_entry)
    all_roles = red.hgetall(cache_keys['iam_roles']['cache_key'])
    roles_to_delete_from_cache = []
    for (arn, role_entry_j) in all_roles.items():
        role_entry = json.loads(role_entry_j)
        if datetime.fromtimestamp(role_entry['ttl']) < datetime.utcnow():
            roles_to_delete_from_cache.append(arn)
    if roles_to_delete_from_cache:
        red.hdel(cache_keys['iam_roles']['cache_key'], *roles_to_delete_from_cache)
        for arn in roles_to_delete_from_cache:
            all_roles.pop(arn, None)
    log_data['num_iam_roles'] = len(all_roles)
    if all_roles:
        async_to_sync(store_json_results_in_redis_and_s3)(all_roles, redis_key=cache_keys['iam_roles']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_roles_combined.s3.file', 'account_resource_cache/cache_all_roles_v1.json.gz'))
    all_iam_users = red.hgetall(cache_keys['iam_users']['temp_cache_key'])
    log_data['num_iam_users'] = len(all_iam_users)
    if all_iam_users:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_users, redis_key=cache_keys['iam_users']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_users_combined.s3.file', 'account_resource_cache/cache_all_users_v1.json.gz'))
    all_iam_groups = red.hgetall(cache_keys['iam_groups']['temp_cache_key'])
    log_data['num_iam_groups'] = len(all_iam_groups)
    if all_iam_groups:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_groups, redis_key=cache_keys['iam_groups']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_groups_combined.s3.file', 'account_resource_cache/cache_all_groups_v1.json.gz'))
    all_iam_policies = red.hgetall(cache_keys['iam_policies']['temp_cache_key'])
    log_data['num_iam_policies'] = len(all_iam_groups)
    if all_iam_policies:
        async_to_sync(store_json_results_in_redis_and_s3)(all_iam_policies, redis_key=cache_keys['iam_policies']['cache_key'], redis_data_type='hash', s3_bucket=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.bucket'), s3_key=config.get('cache_iam_resources_across_accounts.all_policies_combined.s3.file', 'account_resource_cache/cache_all_policies_v1.json.gz'))
    for (k, v) in cache_keys.items():
        temp_cache_key = v['temp_cache_key']
        red.delete(temp_cache_key)
    stats.count(f'{function}.success')
    log_data['num_accounts'] = len(accounts_d)
    log.debug(log_data)
    return log_data",f'{function}.success',f'{function}.success',find_wrong,2,,,,
PaddleX,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex/cv/models/base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex/cv/models/base.py,BaseAPI,"def save_model(self, save_dir):
    if not osp.isdir(save_dir):
        if osp.exists(save_dir):
            os.remove(save_dir)
        os.makedirs(save_dir)
    if self.train_prog is not None:
        fluid.save(self.train_prog, osp.join(save_dir, 'model'))
    else:
        fluid.save(self.test_prog, osp.join(save_dir, 'model'))
    model_info = self.get_model_info()
    model_info['status'] = self.status
    with open(osp.join(save_dir, 'model.yml'), encoding='utf-8', mode='w') as f:
        yaml.dump(model_info, f)
    if hasattr(self, 'eval_details'):
        with open(osp.join(save_dir, 'eval_details.json'), 'w') as f:
            json.dump(self.eval_details, f)
    if self.status == 'Prune':
        shapes = {}
        for block in self.train_prog.blocks:
            for param in block.all_parameters():
                pd_var = fluid.global_scope().find_var(param.name)
                pd_param = pd_var.get_tensor()
                shapes[param.name] = np.array(pd_param).shape
        with open(osp.join(save_dir, 'prune.yml'), encoding='utf-8', mode='w') as f:
            yaml.dump(shapes, f)
    open(osp.join(save_dir, '.success'), 'w').close()
    logging.info('Model saved in {}.'.format(save_dir))",'Model saved in {}.'.format(save_dir),f'Model saved in {save_dir}.',find_wrong,,1,3,,
pgadmin4,https://github.com/postgres/pgadmin4/tree/master/web/pgadmin/browser/server_groups/servers/databases/schemas/tables/indexes/tests/test_indexes_get_nodes.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgadmin4/web/pgadmin/browser/server_groups/servers/databases/schemas/tables/indexes/tests/test_indexes_get_nodes.py,IndexesGetTestCase,"def setUp(self):
    """""" Creating index required in further steps""""""
    self.db_name = parent_node_dict['database'][-1]['db_name']
    schema_info = parent_node_dict['schema'][-1]
    self.server_id = schema_info['server_id']
    self.db_id = schema_info['db_id']
    db_con = database_utils.connect_database(self, utils.SERVER_GROUP, self.server_id, self.db_id)
    if not db_con['data']['connected']:
        raise Exception('Could not connect to database to add a table.')
    self.schema_id = schema_info['schema_id']
    self.schema_name = schema_info['schema_name']
    schema_response = schema_utils.verify_schemas(self.server, self.db_name, self.schema_name)
    if not schema_response:
        raise Exception('Could not find the schema to add a table.')
    self.table_name = 'table_column_%s' % str(uuid.uuid4())[1:8]
    self.table_id = tables_utils.create_table(self.server, self.db_name, self.schema_name, self.table_name)
    self.column_name = 'test_column_delete_%s' % str(uuid.uuid4())[1:8]
    self.column_id = columns_utils.create_column(self.server, self.db_name, self.schema_name, self.table_name, self.column_name)
    self.index_name = 'test_index_delete_%s' % str(uuid.uuid4())[1:8]
    self.index_id = indexes_utils.create_index(self.server, self.db_name, self.schema_name, self.table_name, self.index_name, self.column_name)
    if self.is_list:
        self.index_name_1 = 'test_index_delete_%s' % str(uuid.uuid4())[1:8]
        self.index_ids = [self.index_id, indexes_utils.create_index(self.server, self.db_name, self.schema_name, self.table_name, self.index_name_1, self.column_name)]",self.table_name = 'table_column_%s' % str(uuid.uuid4())[1:8],self.table_name = f'table_column_{str(uuid.uuid4())[1:8]}',find_wrong,2,,,,
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/map_detection/grid_predictor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/map_detection/grid_predictor.py,GridPredictor,"def predict_enemy_genre(self):
    image_dic = {}
    scaling_dic = self.config.MAP_ENEMY_GENRE_DETECTION_SCALING
    for (name, template) in self.template_enemy_genre.items():
        if template is None:
            logger.warning(f'Enemy detection template not found: {name}')
            logger.warning('Please create it with dev_tools/relative_record.py or dev_tools/relative_crop.py, then place it under ./assets/<server>/template')
            raise ScriptError(f'Enemy detection template not found: {name}')
        short_name = name[6:] if name.startswith('Siren_') else name
        scaling = scaling_dic.get(short_name, 1)
        scaling = (scaling,) if not isinstance(scaling, tuple) else scaling
        for scale in scaling:
            if scale not in image_dic:
                shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
                image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
            if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
                return name
    return None",logger.warning(f'Enemy detection template not found: {name}'),logger.warning('Enemy detection template not found: {}'.format(name)),find_wrong,2,,,,
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/map_detection/grid_predictor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/map_detection/grid_predictor.py,GridPredictor,"def predict_enemy_genre(self):
    image_dic = {}
    scaling_dic = self.config.MAP_ENEMY_GENRE_DETECTION_SCALING
    for (name, template) in self.template_enemy_genre.items():
        if template is None:
            logger.warning(f'Enemy detection template not found: {name}')
            logger.warning('Please create it with dev_tools/relative_record.py or dev_tools/relative_crop.py, then place it under ./assets/<server>/template')
            raise ScriptError(f'Enemy detection template not found: {name}')
        short_name = name[6:] if name.startswith('Siren_') else name
        scaling = scaling_dic.get(short_name, 1)
        scaling = (scaling,) if not isinstance(scaling, tuple) else scaling
        for scale in scaling:
            if scale not in image_dic:
                shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))
                image_dic[scale] = rgb2gray(self.relative_crop((-0.5, -1, 0.5, 0), shape=shape))
            if template.match(image_dic[scale], similarity=self.config.MAP_ENEMY_GENRE_SIMILARITY):
                return name
    return None","shape = tuple(np.round(np.array((60, 60)) * scale).astype(int))","shape = tuple(np.round(np.array((60, 60)) * scale).astype(int)).__format__('d')",find_wrong,2,,,,
PathPicker,https://github.com/facebook/PathPicker/tree/master/src/tests/test_parsing.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PathPicker/src/tests/test_parsing.py,TestParseFunction,"def test_all_input_matches(self) -> None:
    for test_case in ALL_INPUT_TEST_CASES:
        result = parse.match_line(test_case.test_input, False, True)
        if not result:
            self.assertTrue(test_case.match is None, f'Expected a match ""{test_case.match}"" where one did not occur.')
            continue
        (match, _, _) = result
        self.assertEqual(match, test_case.match, f'Line ""{test_case.test_input}"" did not match.')
    print(f'Tested {len(ALL_INPUT_TEST_CASES)} cases for all-input matching.')","""""""Expected a match ""{test_case.match}"" where one did not occur.""""""",f'Expected a match "{test_case.match}" where one did not occur.',find_wrong,2,,,,
PathPicker,https://github.com/facebook/PathPicker/tree/master/src/tests/test_parsing.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PathPicker/src/tests/test_parsing.py,TestParseFunction,"def test_all_input_matches(self) -> None:
    for test_case in ALL_INPUT_TEST_CASES:
        result = parse.match_line(test_case.test_input, False, True)
        if not result:
            self.assertTrue(test_case.match is None, f'Expected a match ""{test_case.match}"" where one did not occur.')
            continue
        (match, _, _) = result
        self.assertEqual(match, test_case.match, f'Line ""{test_case.test_input}"" did not match.')
    print(f'Tested {len(ALL_INPUT_TEST_CASES)} cases for all-input matching.')","""""""Line ""{test_case.test_input}"" did not match.""""""",f'Line "{test_case.test_input}" did not match.',find_wrong,2,,,,
PathPicker,https://github.com/facebook/PathPicker/tree/master/src/tests/test_parsing.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PathPicker/src/tests/test_parsing.py,TestParseFunction,"def test_all_input_matches(self) -> None:
    for test_case in ALL_INPUT_TEST_CASES:
        result = parse.match_line(test_case.test_input, False, True)
        if not result:
            self.assertTrue(test_case.match is None, f'Expected a match ""{test_case.match}"" where one did not occur.')
            continue
        (match, _, _) = result
        self.assertEqual(match, test_case.match, f'Line ""{test_case.test_input}"" did not match.')
    print(f'Tested {len(ALL_INPUT_TEST_CASES)} cases for all-input matching.')",print(f'Tested {len(ALL_INPUT_TEST_CASES)} cases for all-input matching.'),print(f'Tested {len(ALL_INPUT_TEST_CASES):d} cases for all-input matching.'),find_wrong,2,,,,
python-telegram-bot,https://github.com/python-telegram-bot/python-telegram-bot/tree/master/tests/test_inlinequeryresultcacheddocument.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-telegram-bot/tests/test_inlinequeryresultcacheddocument.py,TestInlineQueryResultCachedDocument,"def test_slot_behaviour(self, inline_query_result_cached_document, mro_slots):
    inst = inline_query_result_cached_document
    for attr in inst.__slots__:
        assert getattr(inst, attr, 'err') != 'err', f""got extra slot '{attr}'""
    assert len(mro_slots(inst)) == len(set(mro_slots(inst))), 'duplicate slot'","assert getattr(inst, attr, 'err') != 'err', f""got extra slot '{attr}'""","assert getattr(inst, attr, 'err') != 'err', f""got extra slot 'attr={attr!r}'""",find_wrong,2,,,,
eo-learn,https://github.com/sentinel-hub/eo-learn/tree/master/core/eolearn/tests/test_eodata_io.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/eo-learn/core/eolearn/tests/test_eodata_io.py,TestEOPatchIO,"def test_saving_in_empty_folder(self):
    for fs_loader in self.filesystem_loaders:
        with fs_loader() as temp_fs:
            if isinstance(temp_fs, TempFS):
                self.eopatch.save(temp_fs.root_path)
            else:
                self.eopatch.save('/', filesystem=temp_fs)
            self.assertTrue(temp_fs.exists('/data_timeless/mask.npy'))
            subfolder = 'new-subfolder'
            self.eopatch.save('new-subfolder', filesystem=temp_fs)
            self.assertTrue(temp_fs.exists(f'/{subfolder}/bbox.pkl'))",f'/{subfolder}/bbox.pkl',f'/{subfolder}/bbox.pkl',find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",message_input_model['text'] = 'testString',message_input_model['text'] = f'testString',find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",runtime_intent_model['intent'] = 'testString',runtime_intent_model['intent'] = f'testString',find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",capture_group_model['group'] = 'testString',capture_group_model['group'] = f'testString',find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",runtime_entity_interpretation_model['calendar_type'] = 'testString',runtime_entity_interpretation_model['calendar_type'] = f'testString',find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",runtime_entity_alternative_model['value'] = 'testString',runtime_entity_alternative_model['value'] = f'testString',find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",runtime_entity_role_model['type'] = 'date_from',runtime_entity_role_model['type'] = f'date_from',find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",runtime_entity_model['entity'] = 'testString',runtime_entity_model['entity'] = f'testString',find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",runtime_entity_model['value'] = 'testString',runtime_entity_model['value'] = f'testString',find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",log_message_model['msg'] = 'testString',log_message_model['msg'] = f'testString',find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",dialog_node_output_options_element_model['label'] = 'testString',dialog_node_output_options_element_model['label'] = f'testString',find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",runtime_response_generic_model['title'] = 'testString',runtime_response_generic_model['title'] = f'testString',find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",runtime_response_generic_model['description'] = 'testString',runtime_response_generic_model['description'] = f'testString',find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",output_data_model['text'] = ['testString'],output_data_model['text'] = [f'testString'],find_wrong,2,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json",message_response_model_json['user_id'] = 'testString',message_response_model_json['user_id'] = f'testString',find_wrong,2,,,,
AzurLaneAutoScript,https://github.com/LmeSzinc/AzurLaneAutoScript/tree/master/module/os/operation_siren.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AzurLaneAutoScript/module/os/operation_siren.py,OperationSiren,"def _os_explore(self):
    """"""
        Explore all dangerous zones at the beginning of month.
        """"""

    def end():
        logger.info('OS explore finished, delay to next reset')
        next_reset = get_os_next_reset()
        logger.attr('OpsiNextReset', next_reset)
        logger.info('To run again, clear OpsiExplore.Scheduler.NextRun and set OpsiExplore.OpsiExplore.LastZone=0')
        with self.config.multi_set():
            self.config.OpsiExplore_LastZone = 0
            self.config.task_delay(target=next_reset)
            self.config.task_call('OpsiDaily', force_call=False)
            self.config.task_call('OpsiShop', force_call=False)
        self.config.task_stop()
    logger.hr('OS explore', level=1)
    order = [int(f.strip(' \t\r\n')) for f in self.config.OS_EXPLORE_FILTER.split('>')]
    try:
        last_zone = self.name_to_zone(self.config.OpsiExplore_LastZone).zone_id
    except ScriptError:
        logger.warning(f'Invalid OpsiExplore_LastZone={self.config.OpsiExplore_LastZone}, re-explore')
        last_zone = 0
    if last_zone in order:
        order = order[order.index(last_zone) + 1:]
        logger.info(f'Last zone: {self.name_to_zone(last_zone)}, next zone: {order[:1]}')
    elif last_zone == 0:
        logger.info(f'First run, next zone: {order[:1]}')
    else:
        raise ScriptError(f'Invalid last_zone: {last_zone}')
    if not len(order):
        end()
    for zone in order:
        if not self.globe_goto(zone, stop_if_safe=True):
            logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
            self.config.OpsiExplore_LastZone = zone
            continue
        logger.hr(f'OS explore {zone}', level=1)
        if not self.config.OpsiExplore_SpecialRadar:
            self.tuning_sample_use()
        self.fleet_set(self.config.OpsiFleet_Fleet)
        self.os_order_execute(recon_scan=not self.config.OpsiExplore_SpecialRadar, submarine_call=self.config.OpsiFleet_Submarine)
        self._os_explore_task_delay()
        self.run_auto_search()
        self.config.OpsiExplore_LastZone = zone
        logger.info(f'Zone cleared: {self.name_to_zone(zone)}')
        self.handle_after_auto_search()
        self.config.check_task_switch()
        if zone == order[-1]:
            end()","logger.warning(f'Invalid OpsiExplore_LastZone={self.config.OpsiExplore_LastZone}, re-explore')","logger.warning(f'Invalid OpsiExplore_LastZone={self.config.OpsiExplore_LastZone!r}, re-explore')",find_wrong,2,,,,
evalml,https://github.com/alteryx/evalml/tree/master/evalml/utils/gen_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/evalml/evalml/utils/gen_utils.py,,"def _file_path_check(filepath=None, format='png', interactive=False, is_plotly=False):
    """"""Helper function to check the filepath being passed.

    Args:
        filepath (str or Path, optional): Location to save file.
        format (str): Extension for figure to be saved as. Defaults to 'png'.
        interactive (bool, optional): If True and fig is of type plotly.Figure, sets the format to 'html'.
        is_plotly (bool, optional): Check to see if the fig being passed is of type plotly.Figure.

    Returns:
        String representing the final filepath the image will be saved to.
    """"""
    if filepath:
        filepath = str(filepath)
        (path_and_name, extension) = os.path.splitext(filepath)
        extension = extension[1:].lower() if extension else None
        if is_plotly and interactive:
            format_ = 'html'
        elif not extension and (not interactive):
            format_ = format
        else:
            format_ = extension
        filepath = f'{path_and_name}.{format_}'
        try:
            f = open(filepath, 'w')
            f.close()
        except (IOError, FileNotFoundError):
            raise ValueError('Specified filepath is not writeable: {}'.format(filepath))
    return filepath","filepath = '{0}.{1}'.format(path_and_name, format_)",filepath = f'{path_and_name}.{format_}',find_wrong,,1,3,,
evalml,https://github.com/alteryx/evalml/tree/master/evalml/utils/gen_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/evalml/evalml/utils/gen_utils.py,,"def _file_path_check(filepath=None, format='png', interactive=False, is_plotly=False):
    """"""Helper function to check the filepath being passed.

    Args:
        filepath (str or Path, optional): Location to save file.
        format (str): Extension for figure to be saved as. Defaults to 'png'.
        interactive (bool, optional): If True and fig is of type plotly.Figure, sets the format to 'html'.
        is_plotly (bool, optional): Check to see if the fig being passed is of type plotly.Figure.

    Returns:
        String representing the final filepath the image will be saved to.
    """"""
    if filepath:
        filepath = str(filepath)
        (path_and_name, extension) = os.path.splitext(filepath)
        extension = extension[1:].lower() if extension else None
        if is_plotly and interactive:
            format_ = 'html'
        elif not extension and (not interactive):
            format_ = format
        else:
            format_ = extension
        filepath = f'{path_and_name}.{format_}'
        try:
            f = open(filepath, 'w')
            f.close()
        except (IOError, FileNotFoundError):
            raise ValueError('Specified filepath is not writeable: {}'.format(filepath))
    return filepath",raise ValueError('Specified filepath is not writeable: {}'.format(filepath)),raise ValueError(f'Specified filepath is not writeable: {filepath}'),find_wrong,,1,3,,
cement,https://github.com/datafolklabs/cement/tree/master/cement/cli/contrib/yaml/emitter.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cement/cement/cli/contrib/yaml/emitter.py,Emitter,"def write_plain(self, text, split=True):
    if self.root_context:
        self.open_ended = True
    if not text:
        return
    if not self.whitespace:
        data = ' '
        self.column += len(data)
        if self.encoding:
            data = data.encode(self.encoding)
        self.stream.write(data)
    self.whitespace = False
    self.indention = False
    spaces = False
    breaks = False
    start = end = 0
    while end <= len(text):
        ch = None
        if end < len(text):
            ch = text[end]
        if spaces:
            if ch != ' ':
                if start + 1 == end and self.column > self.best_width and split:
                    self.write_indent()
                    self.whitespace = False
                    self.indention = False
                else:
                    data = text[start:end]
                    self.column += len(data)
                    if self.encoding:
                        data = data.encode(self.encoding)
                    self.stream.write(data)
                start = end
        elif breaks:
            if ch not in '\n\x85\u2028\u2029':
                if text[start] == '\n':
                    self.write_line_break()
                for br in text[start:end]:
                    if br == '\n':
                        self.write_line_break()
                    else:
                        self.write_line_break(br)
                self.write_indent()
                self.whitespace = False
                self.indention = False
                start = end
        elif ch is None or ch in ' \n\x85\u2028\u2029':
            data = text[start:end]
            self.column += len(data)
            if self.encoding:
                data = data.encode(self.encoding)
            self.stream.write(data)
            start = end
        if ch is not None:
            spaces = ch == ' '
            breaks = ch in '\n\x85\u2028\u2029'
        end += 1","data = ' '
...
data = text[start:end]","data = f' '
...
data = f'{text[start:end]}'",find_wrong,2,,,,
magenta,https://github.com/magenta/magenta/tree/master/magenta/models/image_stylization/image_stylization_convert_tflite.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/magenta/magenta/models/image_stylization/image_stylization_convert_tflite.py,,"def _convert_to_tflite(saved_model_dir, num_styles, image_size, quantize, output_model):
    """"""Convert a image stylization saved model to TensorFlow Lite format.""""""
    if tf.io.gfile.isdir(output_model):
        if quantize:
            filename = 'stylize_quantized.tflite'
        else:
            filename = 'stylize.tflite'
        output_model = os.path.join(output_model, filename)
    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=saved_model_dir, input_shapes={'input_image': [None, image_size, image_size, 3], 'style_weights': num_styles})
    if quantize:
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()
    with tf.io.gfile.GFile(output_model, 'wb') as f:
        f.write(tflite_model)
    tf.logging.info('Converted to TF Lite model: %s; Size: %d KB.' % (output_model, len(tflite_model) / 1024))","TF Lite model: %s; Size: %d KB.' % (output_model, len(tflite_model) / 1024)",f'TF Lite model: {output_model}; Size: {len(tflite_model) / 1024} KB.',find_wrong,,,,,
jetson_stats,https://github.com/rbonghi/jetson_stats/tree/master/jtop/tests/test_fan.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jetson_stats/jtop/tests/test_fan.py,,"def copyDirectory(src, dest):
    try:
        shutil.copytree(src, dest)
    except shutil.Error as e:
        print('Directory not copied. Error: %s' % e)
    except OSError as e:
        print('Directory not copied. Error: %s' % e)",print('Directory not copied. Error: %s' % e),print(f'Directory not copied. Error: {e}'),find_wrong,2,,,,
torba,https://github.com/lbryio/torba/tree/master/torba/server/coins.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torba/torba/server/coins.py,Namecoin,"def split_name_script(cls, script):
    from torba.server.script import _match_ops, Script, ScriptError
    try:
        ops = Script.get_ops(script)
    except ScriptError:
        return (None, script)
    match = _match_ops
    OP_NAME_NEW = OpCodes.OP_1
    OP_NAME_FIRSTUPDATE = OpCodes.OP_2
    OP_NAME_UPDATE = OpCodes.OP_3
    NAME_NEW_OPS = [OP_NAME_NEW, -1, OpCodes.OP_2DROP]
    NAME_FIRSTUPDATE_OPS = [OP_NAME_FIRSTUPDATE, -1, -1, -1, OpCodes.OP_2DROP, OpCodes.OP_2DROP]
    NAME_UPDATE_OPS = [OP_NAME_UPDATE, -1, -1, OpCodes.OP_2DROP, OpCodes.OP_DROP]
    name_script_op_count = None
    name_pushdata = None
    if match(ops[:len(NAME_NEW_OPS)], NAME_NEW_OPS):
        name_script_op_count = len(NAME_NEW_OPS)
    elif match(ops[:len(NAME_FIRSTUPDATE_OPS)], NAME_FIRSTUPDATE_OPS):
        name_script_op_count = len(NAME_FIRSTUPDATE_OPS)
        name_pushdata = ops[1]
    elif match(ops[:len(NAME_UPDATE_OPS)], NAME_UPDATE_OPS):
        name_script_op_count = len(NAME_UPDATE_OPS)
        name_pushdata = ops[1]
    if name_script_op_count is None:
        return (None, script)
    n = 0
    for i in range(name_script_op_count):
        op = script[n]
        n += 1
        if op <= OpCodes.OP_PUSHDATA4:
            if op < OpCodes.OP_PUSHDATA1:
                dlen = op
            elif op == OpCodes.OP_PUSHDATA1:
                dlen = script[n]
                n += 1
            elif op == OpCodes.OP_PUSHDATA2:
                (dlen,) = struct.unpack('<H', script[n:n + 2])
                n += 2
            else:
                (dlen,) = struct.unpack('<I', script[n:n + 4])
                n += 4
            if n + dlen > len(script):
                raise IndexError
            op = (op, script[n:n + dlen])
            n += dlen
    address_script = script[n:]
    if name_pushdata is None:
        return (None, address_script)
    normalized_name_op_script = bytearray()
    normalized_name_op_script.append(OP_NAME_UPDATE)
    normalized_name_op_script.extend(Script.push_data(name_pushdata[1]))
    normalized_name_op_script.extend(Script.push_data(bytes([])))
    normalized_name_op_script.append(OpCodes.OP_2DROP)
    normalized_name_op_script.append(OpCodes.OP_DROP)
    normalized_name_op_script.append(OpCodes.OP_RETURN)
    return (bytes(normalized_name_op_script), address_script)",normalized_name_op_script.extend(Script.push_data(name_pushdata[1])),normalized_name_op_script.extend(Script.push_data(f'{name_pushdata[1]}')),find_wrong,2,,,,
ctci-solutions,https://github.com/w-hat/ctci-solutions/tree/master/ch-08-recursion-and-dynamic-programming/12-eight-queens.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ctci-solutions/ch-08-recursion-and-dynamic-programming/12-eight-queens.py,,"def show(placement):
    parts = ['\n+-----------------+\n']
    for row in xrange(8):
        parts.append('| ')
        for col in xrange(8):
            bit = 1 << row * 8 + col
            if bit & placement:
                parts.append('Q ')
            else:
                parts.append('  ')
        parts.append('|\n')
    parts.append('+-----------------+\n')
    return ''.join(parts)","""""""| """"""
'Q '
'  '
'|\n'
'+-----------------+\n'","'| {' '}'
'Q '
'  '
'{' '|\n'}'
'+-----------------+\n'",find_wrong,2,,,,
SDV,https://github.com/sdv-dev/SDV/tree/master//tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SDV//tasks.py,,"def install_minimum(c):
    with open('setup.py', 'r') as setup_py:
        lines = setup_py.read().splitlines()
    versions = []
    started = False
    for line in lines:
        if started:
            if line == ']':
                started = False
                continue
            line = line.strip()
            if _validate_python_version(line):
                requirement = re.match('[^>]*', line).group(0)
                requirement = re.sub('[\'"",]', '', requirement)
                version = re.search('>=?[^(,|#)]*', line).group(0)
                if version:
                    version = re.sub('>=?', '==', version)
                    version = re.sub('[\'"",]', '', version)
                    requirement += version
                versions.append(requirement)
        elif line.startswith('install_requires = [') or line.startswith('pomegranate_requires = ['):
            started = True
    c.run(f""python -m pip install {' '.join(versions)}"")",c.run(f"python -m pip install {' '.join(versions)}"),c.run(f"python -m pip install {' '.join(versions)}"),find_wrong,2,,,,
enumerate-iam,https://github.com/andresriancho/enumerate-iam/tree/master/enumerate_iam/generate_bruteforce_tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/enumerate-iam/enumerate_iam/generate_bruteforce_tests.py,,"def main():
    bruteforce_tests = dict()
    for filename in os.listdir(API_DEFINITIONS):
        if not filename.endswith('.min.json'):
            continue
        api_json_data = open(os.path.join(API_DEFINITIONS, filename)).read()
        api_json = json.loads(api_json_data)
        service_name = extract_service_name(filename, api_json)
        if service_name is None:
            print('%s does not define a service name' % filename)
            continue
        operations = extract_operations(api_json)
        if not operations:
            continue
        if service_name in bruteforce_tests:
            bruteforce_tests[service_name].extend(operations)
        else:
            bruteforce_tests[service_name] = operations
    output = OUTPUT_FMT % json.dumps(bruteforce_tests, indent=4, sort_keys=True)
    open(OUTPUT_FILE, 'w').write(output)",print('%s does not define a service name' % filename),print(f'{filename} does not define a service name'),find_wrong,,1,3,,
enumerate-iam,https://github.com/andresriancho/enumerate-iam/tree/master/enumerate_iam/generate_bruteforce_tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/enumerate-iam/enumerate_iam/generate_bruteforce_tests.py,,"def main():
    bruteforce_tests = dict()
    for filename in os.listdir(API_DEFINITIONS):
        if not filename.endswith('.min.json'):
            continue
        api_json_data = open(os.path.join(API_DEFINITIONS, filename)).read()
        api_json = json.loads(api_json_data)
        service_name = extract_service_name(filename, api_json)
        if service_name is None:
            print('%s does not define a service name' % filename)
            continue
        operations = extract_operations(api_json)
        if not operations:
            continue
        if service_name in bruteforce_tests:
            bruteforce_tests[service_name].extend(operations)
        else:
            bruteforce_tests[service_name] = operations
    output = OUTPUT_FMT % json.dumps(bruteforce_tests, indent=4, sort_keys=True)
    open(OUTPUT_FILE, 'w').write(output)","output = OUTPUT_FMT % json.dumps(bruteforce_tests, indent=4, sort_keys=True)","output = f'{OUTPUT_FMT.format(json.dumps(bruteforce_tests, indent=4, sort_keys=True))}'",find_wrong,2,,,,
TFSegmentation,https://github.com/MSiam/TFSegmentation/tree/master/data/preprocess_npy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TFSegmentation/data/preprocess_npy.py,,"def write_image_annotation_pairs(filename_pairs, path, split):
    counter = 0
    imgs = []
    labels = []
    for (img_path, annotation_path) in tqdm(filename_pairs):
        img = misc.imread(img_path)
        img = misc.imresize(img, SIZE)
        imgs.append(img)
        annotation = misc.imread(annotation_path)
        annotation[annotation <= 128] = 0
        annotation[annotation > 128] = 1
        annotation = misc.imresize(annotation, SIZE, 'nearest')
        labels.append(annotation)
    np.save(path + '/X_' + split + '.npy', imgs)
    np.save(path + '/Y_' + split + '.npy', labels)
    if split == 'train':
        mean = np.mean(np.asarray(imgs), axis=0)
        np.save(path + '/mean.npy', mean)
        weights = get_weights(2, labels)
        np.save(path + '/weights.npy', weights)",path + '/X_' + split + '.npy',f'{path}/X_{split}.npy',find_wrong,,1,3,,
TFSegmentation,https://github.com/MSiam/TFSegmentation/tree/master/data/preprocess_npy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TFSegmentation/data/preprocess_npy.py,,"def write_image_annotation_pairs(filename_pairs, path, split):
    counter = 0
    imgs = []
    labels = []
    for (img_path, annotation_path) in tqdm(filename_pairs):
        img = misc.imread(img_path)
        img = misc.imresize(img, SIZE)
        imgs.append(img)
        annotation = misc.imread(annotation_path)
        annotation[annotation <= 128] = 0
        annotation[annotation > 128] = 1
        annotation = misc.imresize(annotation, SIZE, 'nearest')
        labels.append(annotation)
    np.save(path + '/X_' + split + '.npy', imgs)
    np.save(path + '/Y_' + split + '.npy', labels)
    if split == 'train':
        mean = np.mean(np.asarray(imgs), axis=0)
        np.save(path + '/mean.npy', mean)
        weights = get_weights(2, labels)
        np.save(path + '/weights.npy', weights)",path + '/Y_' + split + '.npy',f'{path}/Y_{split}.npy',find_wrong,,1,3,,
TFSegmentation,https://github.com/MSiam/TFSegmentation/tree/master/data/preprocess_npy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TFSegmentation/data/preprocess_npy.py,,"def write_image_annotation_pairs(filename_pairs, path, split):
    counter = 0
    imgs = []
    labels = []
    for (img_path, annotation_path) in tqdm(filename_pairs):
        img = misc.imread(img_path)
        img = misc.imresize(img, SIZE)
        imgs.append(img)
        annotation = misc.imread(annotation_path)
        annotation[annotation <= 128] = 0
        annotation[annotation > 128] = 1
        annotation = misc.imresize(annotation, SIZE, 'nearest')
        labels.append(annotation)
    np.save(path + '/X_' + split + '.npy', imgs)
    np.save(path + '/Y_' + split + '.npy', labels)
    if split == 'train':
        mean = np.mean(np.asarray(imgs), axis=0)
        np.save(path + '/mean.npy', mean)
        weights = get_weights(2, labels)
        np.save(path + '/weights.npy', weights)",path + '/mean.npy',f'{path}/mean.npy',find_wrong,,1,3,,
TFSegmentation,https://github.com/MSiam/TFSegmentation/tree/master/data/preprocess_npy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TFSegmentation/data/preprocess_npy.py,,"def write_image_annotation_pairs(filename_pairs, path, split):
    counter = 0
    imgs = []
    labels = []
    for (img_path, annotation_path) in tqdm(filename_pairs):
        img = misc.imread(img_path)
        img = misc.imresize(img, SIZE)
        imgs.append(img)
        annotation = misc.imread(annotation_path)
        annotation[annotation <= 128] = 0
        annotation[annotation > 128] = 1
        annotation = misc.imresize(annotation, SIZE, 'nearest')
        labels.append(annotation)
    np.save(path + '/X_' + split + '.npy', imgs)
    np.save(path + '/Y_' + split + '.npy', labels)
    if split == 'train':
        mean = np.mean(np.asarray(imgs), axis=0)
        np.save(path + '/mean.npy', mean)
        weights = get_weights(2, labels)
        np.save(path + '/weights.npy', weights)",path + '/weights.npy',f'{path}/weights.npy',find_wrong,,1,3,,
pyhanlp,https://github.com/hankcs/pyhanlp/tree/master/tests/book/ch10/demo_clustering_f.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyhanlp/tests/book/ch10/demo_clustering_f.py,,"if __name__ == '__main__':
    for algorithm in ('kmeans', 'repeated bisection'):
        print('%s F1=%.2f\n' % (algorithm, ClusterAnalyzer.evaluate(sogou_corpus_path, algorithm) * 100))","print('%s F1=%.2f\n' % (algorithm, ClusterAnalyzer.evaluate(sogou_corpus_path, algorithm) * 100))","print(f'{algorithm} F1={ClusterAnalyzer.evaluate(sogou_corpus_path, algorithm) * 100:.2f}\n')",find_wrong,,,,,
kale,https://github.com/kubeflow-kale/kale/tree/master/backend/kale/kfserving/transformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kale/backend/kale/kfserving/transformer.py,KaleTransformer,"def _load_transformer_assets(self):
    marshal.set_data_dir(serveutils.TRANSFORMER_ASSETS_DIR)
    log.info('Loading transformer function...')
    _fn = marshal.load(serveutils.TRANSFORMER_FN_ASSET_NAME)
    self.fn = types.FunctionType(_fn.__code__, globals(), _fn.__name__, _fn.__defaults__, _fn.__closure__)
    log.info('Processing source notebook for imports and functions...')
    processor = NotebookProcessor(nb_path=os.path.join(serveutils.TRANSFORMER_ASSETS_DIR, serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME), skip_validation=True)
    self.init_code = processor.get_imports_and_functions()
    log.info('Initialization code:\n%s' % self.init_code)
    log.info('Running initialization code...')
    exec(self.init_code, globals())
    log.info(""Loading transformer's assets..."")
    for file in os.listdir(serveutils.TRANSFORMER_ASSETS_DIR):
        if file in [serveutils.TRANSFORMER_SRC_NOTEBOOK_NAME, serveutils.TRANSFORMER_FN_ASSET_NAME]:
            continue
        basename = os.path.splitext(file)[0]
        self.assets[basename] = marshal.load(basename)
    log.info('Assets successfully loaded: %s' % self.assets.keys())
    log.info('Initializing assets...')
    for (asset_name, asset_value) in self.assets.items():
        globals()[asset_name] = asset_value","""""""Loading transformer function...""""""
'Processing source notebook for imports and functions...'
'Initialization code:\n%s' % self.init_code
'Running initialization code...'
""Loading transformer's assets...""
'Assets successfully loaded: %s' % self.assets.keys()
'Initializing assets...'","""""""Loading transformer function...""""""
f'Processing source notebook for imports and functions...'
f'Initialization code:\n{self.init_code}'
'Running initialization code...'
""Loading transformer's assets...""
f'Assets successfully loaded: {list(self.assets.keys())}'
'Initializing assets...'",find_wrong,2,,,,
zao-,https://github.com/qiucheng025/zao-/tree/master/lib/gui/stats.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zao-/lib/gui/stats.py,SessionsSummary,"def format_stats(compiled_stats):
    """""" Format for display """"""
    logger.debug('Formatting stats')
    for summary in compiled_stats:
        (hrs, mins, secs) = convert_time(summary['elapsed'])
        summary['start'] = time.strftime('%x %X', time.gmtime(summary['start']))
        summary['end'] = time.strftime('%x %X', time.gmtime(summary['end']))
        summary['elapsed'] = '{}:{}:{}'.format(hrs, mins, secs)
        summary['rate'] = '{0:.1f}'.format(summary['rate'])
    return compiled_stats","summary['elapsed'] = '{}:{}:{}'.format(hrs, mins, secs)",summary['elapsed'] = f'{hrs}:{mins}:{secs}',find_wrong,,1,3,,
zao-,https://github.com/qiucheng025/zao-/tree/master/lib/gui/stats.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zao-/lib/gui/stats.py,SessionsSummary,"def format_stats(compiled_stats):
    """""" Format for display """"""
    logger.debug('Formatting stats')
    for summary in compiled_stats:
        (hrs, mins, secs) = convert_time(summary['elapsed'])
        summary['start'] = time.strftime('%x %X', time.gmtime(summary['start']))
        summary['end'] = time.strftime('%x %X', time.gmtime(summary['end']))
        summary['elapsed'] = '{}:{}:{}'.format(hrs, mins, secs)
        summary['rate'] = '{0:.1f}'.format(summary['rate'])
    return compiled_stats",summary['rate'] = '{0:.1f}'.format(summary['rate']),summary['rate'] = f"{summary['rate']:.1f}",find_wrong,,1,3,,
aws-cli,https://github.com/aws/aws-cli/tree/master/awscli/customizations/cloudtrail/validation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-cli/awscli/customizations/cloudtrail/validation.py,CloudTrailValidateLogs,"def _write_summary_text(self):
    if not self._is_last_status_double_space:
        sys.stdout.write('\n')
    sys.stdout.write('Results requested for %s to %s\n' % (format_display_date(self.start_time), format_display_date(self.end_time)))
    if not self._valid_digests and (not self._invalid_digests):
        sys.stdout.write('No digests found\n')
        return
    if not self._found_start_time or not self._found_end_time:
        sys.stdout.write('No valid digests found in range\n')
    else:
        sys.stdout.write('Results found for %s to %s:\n' % (format_display_date(self._found_start_time), format_display_date(self._found_end_time)))
    self._write_ratio(self._valid_digests, self._invalid_digests, 'digest')
    self._write_ratio(self._valid_logs, self._invalid_logs, 'log')
    sys.stdout.write('\n')","sys.stdout.write('Results requested for %s to %s\n' % (format_display_date(self.start_time), format_display_date(self.end_time)))",print(f'Results requested for {format_display_date(self.start_time)} to {format_display_date(self.end_time)}'),find_wrong,,,,,
aws-cli,https://github.com/aws/aws-cli/tree/master/awscli/customizations/cloudtrail/validation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-cli/awscli/customizations/cloudtrail/validation.py,CloudTrailValidateLogs,"def _write_summary_text(self):
    if not self._is_last_status_double_space:
        sys.stdout.write('\n')
    sys.stdout.write('Results requested for %s to %s\n' % (format_display_date(self.start_time), format_display_date(self.end_time)))
    if not self._valid_digests and (not self._invalid_digests):
        sys.stdout.write('No digests found\n')
        return
    if not self._found_start_time or not self._found_end_time:
        sys.stdout.write('No valid digests found in range\n')
    else:
        sys.stdout.write('Results found for %s to %s:\n' % (format_display_date(self._found_start_time), format_display_date(self._found_end_time)))
    self._write_ratio(self._valid_digests, self._invalid_digests, 'digest')
    self._write_ratio(self._valid_logs, self._invalid_logs, 'log')
    sys.stdout.write('\n')","sys.stdout.write('Results found for %s to %s:\n' % (format_display_date(self._found_start_time), format_display_date(self._found_end_time)))",print(f'Results found for {format_display_date(self._found_start_time)} to {format_display_date(self._found_end_time)}:'),find_wrong,,,,,
airflow,https://github.com/apache/airflow/tree/master/airflow/models/dag.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/models/dag.py,DAG,"def __init__(self, dag_id: str, description: Optional[str]=None, schedule_interval: ScheduleIntervalArg=ScheduleIntervalArgNotSet, timetable: Optional[Timetable]=None, start_date: Optional[datetime]=None, end_date: Optional[datetime]=None, full_filepath: Optional[str]=None, template_searchpath: Optional[Union[str, Iterable[str]]]=None, template_undefined: Type[jinja2.StrictUndefined]=jinja2.StrictUndefined, user_defined_macros: Optional[Dict]=None, user_defined_filters: Optional[Dict]=None, default_args: Optional[Dict]=None, concurrency: Optional[int]=None, max_active_tasks: int=conf.getint('core', 'max_active_tasks_per_dag'), max_active_runs: int=conf.getint('core', 'max_active_runs_per_dag'), dagrun_timeout: Optional[timedelta]=None, sla_miss_callback: Optional[Callable[['DAG', str, str, List[str], List[TaskInstance]], None]]=None, default_view: str=conf.get('webserver', 'dag_default_view').lower(), orientation: str=conf.get('webserver', 'dag_orientation'), catchup: bool=conf.getboolean('scheduler', 'catchup_by_default'), on_success_callback: Optional[DagStateChangeCallback]=None, on_failure_callback: Optional[DagStateChangeCallback]=None, doc_md: Optional[str]=None, params: Optional[Dict]=None, access_control: Optional[Dict]=None, is_paused_upon_creation: Optional[bool]=None, jinja_environment_kwargs: Optional[Dict]=None, render_template_as_native_obj: bool=False, tags: Optional[List[str]]=None):
    from airflow.utils.task_group import TaskGroup
    self.user_defined_macros = user_defined_macros
    self.user_defined_filters = user_defined_filters
    self.default_args = copy.deepcopy(default_args or {})
    self.params = params or {}
    if 'params' in self.default_args:
        self.params.update(self.default_args['params'])
        del self.default_args['params']
    self.params = ParamsDict(self.params)
    if full_filepath:
        warnings.warn('Passing full_filepath to DAG() is deprecated and has no effect', DeprecationWarning, stacklevel=2)
    validate_key(dag_id)
    self._dag_id = dag_id
    if concurrency:
        warnings.warn(""The 'concurrency' parameter is deprecated. Please use 'max_active_tasks'."", DeprecationWarning, stacklevel=2)
        max_active_tasks = concurrency
    self._max_active_tasks = max_active_tasks
    self._pickle_id: Optional[int] = None
    self._description = description
    back = sys._getframe().f_back
    self.fileloc = back.f_code.co_filename if back else ''
    self.task_dict: Dict[str, BaseOperator] = {}
    if start_date and start_date.tzinfo:
        self.timezone = start_date.tzinfo
    elif 'start_date' in self.default_args and self.default_args['start_date']:
        if isinstance(self.default_args['start_date'], str):
            self.default_args['start_date'] = timezone.parse(self.default_args['start_date'])
        self.timezone = self.default_args['start_date'].tzinfo
    if not hasattr(self, 'timezone') or not self.timezone:
        self.timezone = settings.TIMEZONE
    if 'end_date' in self.default_args and self.default_args['end_date']:
        if isinstance(self.default_args['end_date'], str):
            self.default_args['end_date'] = timezone.parse(self.default_args['end_date'], timezone=self.timezone)
    self.start_date = timezone.convert_to_utc(start_date)
    self.end_date = timezone.convert_to_utc(end_date)
    if 'start_date' in self.default_args:
        self.default_args['start_date'] = timezone.convert_to_utc(self.default_args['start_date'])
    if 'end_date' in self.default_args:
        self.default_args['end_date'] = timezone.convert_to_utc(self.default_args['end_date'])
    if timetable is None:
        self.timetable = create_timetable(schedule_interval, self.timezone)
        if schedule_interval is ScheduleIntervalArgNotSet:
            schedule_interval = DEFAULT_SCHEDULE_INTERVAL
        self.schedule_interval: ScheduleInterval = schedule_interval
    elif schedule_interval is ScheduleIntervalArgNotSet:
        self.timetable = timetable
        self.schedule_interval = self.timetable.summary
    else:
        raise TypeError(""cannot specify both 'schedule_interval' and 'timetable'"")
    if isinstance(template_searchpath, str):
        template_searchpath = [template_searchpath]
    self.template_searchpath = template_searchpath
    self.template_undefined = template_undefined
    self.parent_dag: Optional[DAG] = None
    self.last_loaded = timezone.utcnow()
    self.safe_dag_id = dag_id.replace('.', '__dot__')
    self.max_active_runs = max_active_runs
    self.dagrun_timeout = dagrun_timeout
    self.sla_miss_callback = sla_miss_callback
    if default_view in DEFAULT_VIEW_PRESETS:
        self._default_view: str = default_view
    else:
        raise AirflowException(f'Invalid values of dag.default_view: only support {DEFAULT_VIEW_PRESETS}, but get {default_view}')
    if orientation in ORIENTATION_PRESETS:
        self.orientation = orientation
    else:
        raise AirflowException(f'Invalid values of dag.orientation: only support {ORIENTATION_PRESETS}, but get {orientation}')
    self.catchup = catchup
    self.is_subdag = False
    self.partial = False
    self.on_success_callback = on_success_callback
    self.on_failure_callback = on_failure_callback
    self.edge_info: Dict[str, Dict[str, EdgeInfoType]] = {}
    self.has_on_success_callback = self.on_success_callback is not None
    self.has_on_failure_callback = self.on_failure_callback is not None
    self.doc_md = doc_md
    self._access_control = DAG._upgrade_outdated_dag_access_control(access_control)
    self.is_paused_upon_creation = is_paused_upon_creation
    self.jinja_environment_kwargs = jinja_environment_kwargs
    self.render_template_as_native_obj = render_template_as_native_obj
    self.tags = tags
    self._task_group = TaskGroup.create_root(self)
    self.validate_schedule_and_params()","""""""dag.default_view: only support {DEFAULT_VIEW_PRESETS}, but get {default_view}""""""","f'dag.default_view: only support {DEFAULT_VIEW_PRESETS}, but get {default_view}'",find_wrong,2,,,,
JBOPS,https://github.com/blacktwin/JBOPS/tree/master/fun/plexapi_haiku.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/JBOPS/fun/plexapi_haiku.py,,"def sylco(word):
    word = word.lower()
    exception_add = ['serious', 'crucial']
    exception_del = ['fortunately', 'unfortunately']
    co_one = ['cool', 'coach', 'coat', 'coal', 'count', 'coin', 'coarse', 'coup', 'coif', 'cook', 'coign', 'coiffe', 'coof', 'court']
    co_two = ['coapt', 'coed', 'coinci']
    pre_one = ['preach']
    syls = 0
    disc = 0
    if len(word) <= 3:
        syls = 1
        return syls
    if word[-2:] == 'es' or word[-2:] == 'ed':
        doubleAndtripple_1 = len(re.findall('[eaoui][eaoui]', word))
        if doubleAndtripple_1 > 1 or len(re.findall('[eaoui][^eaoui]', word)) > 1:
            if word[-3:] == 'ted' or word[-3:] == 'tes' or word[-3:] == 'ses' or (word[-3:] == 'ied') or (word[-3:] == 'ies'):
                pass
            else:
                disc += 1
    le_except = ['whole', 'mobile', 'pole', 'male', 'female', 'hale', 'pale', 'tale', 'sale', 'aisle', 'whale', 'while']
    if word[-1:] == 'e':
        if word[-2:] == 'le' and word not in le_except:
            pass
        else:
            disc += 1
    doubleAndtripple = len(re.findall('[eaoui][eaoui]', word))
    tripple = len(re.findall('[eaoui][eaoui][eaoui]', word))
    disc += doubleAndtripple + tripple
    numVowels = len(re.findall('[eaoui]', word))
    if word[:2] == 'mc':
        syls += 1
    if word[-1:] == 'y' and word[-2] not in 'aeoui':
        syls += 1
    for (i, j) in enumerate(word):
        if j == 'y':
            if i != 0 and i != len(word) - 1:
                if word[i - 1] not in 'aeoui' and word[i + 1] not in 'aeoui':
                    syls += 1
    if word[:3] == 'tri' and word[3] in 'aeoui':
        syls += 1
    if word[:2] == 'bi' and word[2] in 'aeoui':
        syls += 1
    if word[-3:] == 'ian':
        if word[-4:] == 'cian' or word[-4:] == 'tian':
            pass
        else:
            syls += 1
    if word[:2] == 'co' and word[2] in 'eaoui':
        if word[:4] in co_two or word[:5] in co_two or word[:6] in co_two:
            syls += 1
        elif word[:4] in co_one or word[:5] in co_one or word[:6] in co_one:
            pass
        else:
            syls += 1
    if word[:3] == 'pre' and word[3] in 'eaoui':
        if word[:6] in pre_one:
            pass
        else:
            syls += 1
    negative = [""doesn't"", ""isn't"", ""shouldn't"", ""couldn't"", ""wouldn't""]
    if word[-3:] == ""n't"":
        if word in negative:
            syls += 1
        else:
            pass
    if word in exception_del:
        disc += 1
    if word in exception_add:
        syls += 1
    return numVowels - disc + syls",if word[-2:] == 'es' or word[-2:] == 'ed':,"if word.endswith(('es', 'ed')):",find_wrong,2,,,,
JBOPS,https://github.com/blacktwin/JBOPS/tree/master/fun/plexapi_haiku.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/JBOPS/fun/plexapi_haiku.py,,"def sylco(word):
    word = word.lower()
    exception_add = ['serious', 'crucial']
    exception_del = ['fortunately', 'unfortunately']
    co_one = ['cool', 'coach', 'coat', 'coal', 'count', 'coin', 'coarse', 'coup', 'coif', 'cook', 'coign', 'coiffe', 'coof', 'court']
    co_two = ['coapt', 'coed', 'coinci']
    pre_one = ['preach']
    syls = 0
    disc = 0
    if len(word) <= 3:
        syls = 1
        return syls
    if word[-2:] == 'es' or word[-2:] == 'ed':
        doubleAndtripple_1 = len(re.findall('[eaoui][eaoui]', word))
        if doubleAndtripple_1 > 1 or len(re.findall('[eaoui][^eaoui]', word)) > 1:
            if word[-3:] == 'ted' or word[-3:] == 'tes' or word[-3:] == 'ses' or (word[-3:] == 'ied') or (word[-3:] == 'ies'):
                pass
            else:
                disc += 1
    le_except = ['whole', 'mobile', 'pole', 'male', 'female', 'hale', 'pale', 'tale', 'sale', 'aisle', 'whale', 'while']
    if word[-1:] == 'e':
        if word[-2:] == 'le' and word not in le_except:
            pass
        else:
            disc += 1
    doubleAndtripple = len(re.findall('[eaoui][eaoui]', word))
    tripple = len(re.findall('[eaoui][eaoui][eaoui]', word))
    disc += doubleAndtripple + tripple
    numVowels = len(re.findall('[eaoui]', word))
    if word[:2] == 'mc':
        syls += 1
    if word[-1:] == 'y' and word[-2] not in 'aeoui':
        syls += 1
    for (i, j) in enumerate(word):
        if j == 'y':
            if i != 0 and i != len(word) - 1:
                if word[i - 1] not in 'aeoui' and word[i + 1] not in 'aeoui':
                    syls += 1
    if word[:3] == 'tri' and word[3] in 'aeoui':
        syls += 1
    if word[:2] == 'bi' and word[2] in 'aeoui':
        syls += 1
    if word[-3:] == 'ian':
        if word[-4:] == 'cian' or word[-4:] == 'tian':
            pass
        else:
            syls += 1
    if word[:2] == 'co' and word[2] in 'eaoui':
        if word[:4] in co_two or word[:5] in co_two or word[:6] in co_two:
            syls += 1
        elif word[:4] in co_one or word[:5] in co_one or word[:6] in co_one:
            pass
        else:
            syls += 1
    if word[:3] == 'pre' and word[3] in 'eaoui':
        if word[:6] in pre_one:
            pass
        else:
            syls += 1
    negative = [""doesn't"", ""isn't"", ""shouldn't"", ""couldn't"", ""wouldn't""]
    if word[-3:] == ""n't"":
        if word in negative:
            syls += 1
        else:
            pass
    if word in exception_del:
        disc += 1
    if word in exception_add:
        syls += 1
    return numVowels - disc + syls",if word[-1:] == 'e':,if word.endswith('e'):,find_wrong,2,,,,
JBOPS,https://github.com/blacktwin/JBOPS/tree/master/fun/plexapi_haiku.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/JBOPS/fun/plexapi_haiku.py,,"def sylco(word):
    word = word.lower()
    exception_add = ['serious', 'crucial']
    exception_del = ['fortunately', 'unfortunately']
    co_one = ['cool', 'coach', 'coat', 'coal', 'count', 'coin', 'coarse', 'coup', 'coif', 'cook', 'coign', 'coiffe', 'coof', 'court']
    co_two = ['coapt', 'coed', 'coinci']
    pre_one = ['preach']
    syls = 0
    disc = 0
    if len(word) <= 3:
        syls = 1
        return syls
    if word[-2:] == 'es' or word[-2:] == 'ed':
        doubleAndtripple_1 = len(re.findall('[eaoui][eaoui]', word))
        if doubleAndtripple_1 > 1 or len(re.findall('[eaoui][^eaoui]', word)) > 1:
            if word[-3:] == 'ted' or word[-3:] == 'tes' or word[-3:] == 'ses' or (word[-3:] == 'ied') or (word[-3:] == 'ies'):
                pass
            else:
                disc += 1
    le_except = ['whole', 'mobile', 'pole', 'male', 'female', 'hale', 'pale', 'tale', 'sale', 'aisle', 'whale', 'while']
    if word[-1:] == 'e':
        if word[-2:] == 'le' and word not in le_except:
            pass
        else:
            disc += 1
    doubleAndtripple = len(re.findall('[eaoui][eaoui]', word))
    tripple = len(re.findall('[eaoui][eaoui][eaoui]', word))
    disc += doubleAndtripple + tripple
    numVowels = len(re.findall('[eaoui]', word))
    if word[:2] == 'mc':
        syls += 1
    if word[-1:] == 'y' and word[-2] not in 'aeoui':
        syls += 1
    for (i, j) in enumerate(word):
        if j == 'y':
            if i != 0 and i != len(word) - 1:
                if word[i - 1] not in 'aeoui' and word[i + 1] not in 'aeoui':
                    syls += 1
    if word[:3] == 'tri' and word[3] in 'aeoui':
        syls += 1
    if word[:2] == 'bi' and word[2] in 'aeoui':
        syls += 1
    if word[-3:] == 'ian':
        if word[-4:] == 'cian' or word[-4:] == 'tian':
            pass
        else:
            syls += 1
    if word[:2] == 'co' and word[2] in 'eaoui':
        if word[:4] in co_two or word[:5] in co_two or word[:6] in co_two:
            syls += 1
        elif word[:4] in co_one or word[:5] in co_one or word[:6] in co_one:
            pass
        else:
            syls += 1
    if word[:3] == 'pre' and word[3] in 'eaoui':
        if word[:6] in pre_one:
            pass
        else:
            syls += 1
    negative = [""doesn't"", ""isn't"", ""shouldn't"", ""couldn't"", ""wouldn't""]
    if word[-3:] == ""n't"":
        if word in negative:
            syls += 1
        else:
            pass
    if word in exception_del:
        disc += 1
    if word in exception_add:
        syls += 1
    return numVowels - disc + syls",if word[-1:] == 'y' and word[-2] not in 'aeoui':,if word.endswith('y') and word[-2] not in 'aeoui':,find_wrong,2,,,,
JBOPS,https://github.com/blacktwin/JBOPS/tree/master/fun/plexapi_haiku.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/JBOPS/fun/plexapi_haiku.py,,"def sylco(word):
    word = word.lower()
    exception_add = ['serious', 'crucial']
    exception_del = ['fortunately', 'unfortunately']
    co_one = ['cool', 'coach', 'coat', 'coal', 'count', 'coin', 'coarse', 'coup', 'coif', 'cook', 'coign', 'coiffe', 'coof', 'court']
    co_two = ['coapt', 'coed', 'coinci']
    pre_one = ['preach']
    syls = 0
    disc = 0
    if len(word) <= 3:
        syls = 1
        return syls
    if word[-2:] == 'es' or word[-2:] == 'ed':
        doubleAndtripple_1 = len(re.findall('[eaoui][eaoui]', word))
        if doubleAndtripple_1 > 1 or len(re.findall('[eaoui][^eaoui]', word)) > 1:
            if word[-3:] == 'ted' or word[-3:] == 'tes' or word[-3:] == 'ses' or (word[-3:] == 'ied') or (word[-3:] == 'ies'):
                pass
            else:
                disc += 1
    le_except = ['whole', 'mobile', 'pole', 'male', 'female', 'hale', 'pale', 'tale', 'sale', 'aisle', 'whale', 'while']
    if word[-1:] == 'e':
        if word[-2:] == 'le' and word not in le_except:
            pass
        else:
            disc += 1
    doubleAndtripple = len(re.findall('[eaoui][eaoui]', word))
    tripple = len(re.findall('[eaoui][eaoui][eaoui]', word))
    disc += doubleAndtripple + tripple
    numVowels = len(re.findall('[eaoui]', word))
    if word[:2] == 'mc':
        syls += 1
    if word[-1:] == 'y' and word[-2] not in 'aeoui':
        syls += 1
    for (i, j) in enumerate(word):
        if j == 'y':
            if i != 0 and i != len(word) - 1:
                if word[i - 1] not in 'aeoui' and word[i + 1] not in 'aeoui':
                    syls += 1
    if word[:3] == 'tri' and word[3] in 'aeoui':
        syls += 1
    if word[:2] == 'bi' and word[2] in 'aeoui':
        syls += 1
    if word[-3:] == 'ian':
        if word[-4:] == 'cian' or word[-4:] == 'tian':
            pass
        else:
            syls += 1
    if word[:2] == 'co' and word[2] in 'eaoui':
        if word[:4] in co_two or word[:5] in co_two or word[:6] in co_two:
            syls += 1
        elif word[:4] in co_one or word[:5] in co_one or word[:6] in co_one:
            pass
        else:
            syls += 1
    if word[:3] == 'pre' and word[3] in 'eaoui':
        if word[:6] in pre_one:
            pass
        else:
            syls += 1
    negative = [""doesn't"", ""isn't"", ""shouldn't"", ""couldn't"", ""wouldn't""]
    if word[-3:] == ""n't"":
        if word in negative:
            syls += 1
        else:
            pass
    if word in exception_del:
        disc += 1
    if word in exception_add:
        syls += 1
    return numVowels - disc + syls",if word[-3:] == "n't":,if word.endswith("n't"):,find_wrong,2,,,,
speechbrain,https://github.com/speechbrain/speechbrain/tree/master/recipes/KsponSpeech/ASR/transformer/train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/speechbrain/recipes/KsponSpeech/ASR/transformer/train.py,,"if __name__ == '__main__':
    (hparams_file, run_opts, overrides) = sb.parse_arguments(sys.argv[1:])
    with open(hparams_file) as fin:
        hparams = load_hyperpyyaml(fin, overrides)
    sb.utils.distributed.ddp_init_group(run_opts)
    from ksponspeech_prepare import prepare_ksponspeech
    sb.create_experiment_directory(experiment_directory=hparams['output_folder'], hyperparams_to_save=hparams_file, overrides=overrides)
    run_on_main(prepare_ksponspeech, kwargs={'data_folder': hparams['data_folder'], 'tr_splits': hparams['train_splits'], 'dev_splits': hparams['dev_splits'], 'te_splits': hparams['test_splits'], 'save_folder': hparams['data_folder'], 'merge_lst': hparams['train_splits'], 'merge_name': hparams['train_csv'], 'skip_prep': hparams['skip_prep']})
    (train_data, valid_data, test_datasets, tokenizer) = dataio_prepare(hparams)
    run_on_main(hparams['pretrainer'].collect_files)
    hparams['pretrainer'].load_collected(device=run_opts['device'])
    asr_brain = ASR(modules=hparams['modules'], opt_class=hparams['Adam'], hparams=hparams, run_opts=run_opts, checkpointer=hparams['checkpointer'])
    asr_brain.tokenizer = hparams['tokenizer']
    asr_brain.fit(asr_brain.hparams.epoch_counter, train_data, valid_data, train_loader_kwargs=hparams['train_dataloader_opts'], valid_loader_kwargs=hparams['valid_dataloader_opts'])
    for k in test_datasets.keys():
        asr_brain.hparams.wer_file = os.path.join(hparams['output_folder'], 'wer_{}.txt'.format(k))
        asr_brain.evaluate(test_datasets[k], max_key='ACC', test_loader_kwargs=hparams['test_dataloader_opts'])",'wer_{}.txt'.format(k),f'wer_{k}.txt',find_wrong,,1,3,,
erpnext,https://github.com/frappe/erpnext/tree/master/erpnext/loan_management/doctype/loan_interest_accrual/loan_interest_accrual.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/loan_management/doctype/loan_interest_accrual/loan_interest_accrual.py,,"def make_accrual_interest_entry_for_term_loans(posting_date, process_loan_interest, term_loan=None, loan_type=None, accrual_type='Regular'):
    curr_date = posting_date or add_days(nowdate(), 1)
    term_loans = get_term_loans(curr_date, term_loan, loan_type)
    accrued_entries = []
    for loan in term_loans:
        accrued_entries.append(loan.payment_entry)
        args = frappe._dict({'loan': loan.name, 'applicant_type': loan.applicant_type, 'applicant': loan.applicant, 'interest_income_account': loan.interest_income_account, 'loan_account': loan.loan_account, 'interest_amount': loan.interest_amount, 'payable_principal': loan.principal_amount, 'process_loan_interest': process_loan_interest, 'repayment_schedule_name': loan.payment_entry, 'posting_date': posting_date, 'accrual_type': accrual_type})
        make_loan_interest_accrual_entry(args)
    if accrued_entries:
        frappe.db.sql('UPDATE `tabRepayment Schedule`\n\t\t\tSET is_accrued = 1 where name in (%s)' % ', '.join(['%s'] * len(accrued_entries)), tuple(accrued_entries))","UPDATE `tabRepayment Schedule`\n\t\t\tSET is_accrued = 1 where name in (%s)' % ', '.join(['%s'] * len(accrued_entries))","f""UPDATE `tabRepayment Schedule`\n\t\t\tSET is_accrued = 1 where name in ({', '.join(['%s'] * len(accrued_entries))})""",find_wrong,,1,3,1,
openpilot,https://github.com/commaai/openpilot/tree/master/tools/sim/lib/manual_ctrl.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openpilot/tools/sim/lib/manual_ctrl.py,,"def wheel_poll_thread(q: 'Queue[str]') -> NoReturn:
    fn = '/dev/input/js0'
    print('Opening %s...' % fn)
    jsdev = open(fn, 'rb')
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2147510803 + 65536 * len(buf), buf)
    js_name = buf.tobytes().rstrip(b'\x00').decode('utf-8')
    print('Device name: %s' % js_name)
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576337, buf)
    num_axes = buf[0]
    buf = array.array('B', [0])
    ioctl(jsdev, 2147576338, buf)
    num_buttons = buf[0]
    buf = array.array('B', [0] * 64)
    ioctl(jsdev, 2151705138, buf)
    for _axis in buf[:num_axes]:
        axis_name = axis_names.get(_axis, 'unknown(0x%02x)' % _axis)
        axis_map.append(axis_name)
        axis_states[axis_name] = 0.0
    buf = array.array('H', [0] * 200)
    ioctl(jsdev, 2151705140, buf)
    for btn in buf[:num_buttons]:
        btn_name = button_names.get(btn, 'unknown(0x%03x)' % btn)
        button_map.append(btn_name)
        button_states[btn_name] = 0
    print('%d axes found: %s' % (num_axes, ', '.join(axis_map)))
    print('%d buttons found: %s' % (num_buttons, ', '.join(button_map)))
    import evdev
    from evdev import ecodes, InputDevice
    device = evdev.list_devices()[0]
    evtdev = InputDevice(device)
    val = 24000
    evtdev.write(ecodes.EV_FF, ecodes.FF_AUTOCENTER, val)
    while True:
        evbuf = jsdev.read(8)
        (value, mtype, number) = struct.unpack('4xhBB', evbuf)
        if mtype & 2:
            axis = axis_map[number]
            if axis == 'z':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('throttle_%f' % normalized)
            elif axis == 'rz':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = (1 - fvalue) * 50
                q.put('brake_%f' % normalized)
            elif axis == 'x':
                fvalue = value / 32767.0
                axis_states[axis] = fvalue
                normalized = fvalue
                q.put('steer_%f' % normalized)
        elif mtype & 1:
            if value == 1:
                if number in [0, 19]:
                    q.put('cruise_down')
                elif number in [3, 18]:
                    q.put('cruise_up')
                elif number in [1, 6]:
                    q.put('cruise_cancel')
                elif number in [10, 21]:
                    q.put('reverse_switch')",Opening %s...' % fn,f'Opening {fn}...',find_wrong,,1,3,,
myscan,https://github.com/amcai/myscan/tree/master/myscan/reverse/reverse_dns.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/myscan/myscan/reverse/reverse_dns.py,SinDNSAnswer,"def __init__(self, ip):
    self.type = 1
    self.classify = 1
    self.name = 49164
    self.datalength = 4
    self.timetolive = 190
    self.ip = ip",self.name = 49164,self.name = f'{49164}',find_wrong,2,,,,
InstaPy,https://github.com/InstaPy/InstaPy/tree/master/instapy/instapy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/InstaPy/instapy/instapy.py,InstaPy,"def interact_user_likers(self, usernames: list, posts_grab_amount: int=3, interact_likers_per_post: int=3, randomize: bool=False):
    """"""
        Interact with the likers of given user's posts.

        set_do_comment, set_do_follow and set_do_like are applicable.

        :param usernames: List of users with whose likers to interact.
        :param posts_grab_amount: Amount of posts to get the likers from per given user.
        :param interact_likers_per_post: Amount of likers to be interacted with per post.
        :param randomize: If followers should be chosen randomly.
        """"""
    if self.aborting:
        return self
    if self.do_follow is not True and self.do_like is not True:
        self.logger.info('Please enable following or liking in settings in order to do interactions.')
        return self
    elif self.user_interact_amount <= 0:
        self.logger.info('Please choose an amount higher than zero in `set_user_interact` in order to do interactions.')
        return self
    if not isinstance(usernames, list):
        usernames = [usernames]
    if posts_grab_amount > 12:
        self.logger.info('Sorry, you can only grab likers from first 12 posts for given username now.\n')
        posts_grab_amount = 12
    interacted_all = 0
    not_valid_users = 0
    simulated_unfollow = 0
    liked_init = self.liked_img
    already_liked_init = self.already_liked
    commented_init = self.commented
    followed_init = self.followed
    inap_img_init = self.inap_img
    self.quotient_breach = False
    for (index, username) in enumerate(usernames):
        if self.quotient_breach:
            break
        self.logger.info(""User '{}' [{}/{}]"".format(username, index + 1, len(usernames)))
        try:
            post_urls = get_photo_urls_from_profile(self.browser, username, posts_grab_amount, randomize, self.logger)
            if not isinstance(post_urls, list):
                post_urls = [post_urls]
        except (TypeError, RuntimeWarning) as err:
            if isinstance(err, RuntimeWarning):
                self.logger.warning('Warning: {} , skipping to next user'.format(err))
                continue
            else:
                self.logger.error('Sorry, an error occurred: {}'.format(err))
                self.aborting = True
                return self
        print('')
        self.logger.info(""Grabbed {} posts from '{}'s profile to do interaction."".format(len(post_urls), username))
        interacted_personal = 0
        for (post_index, post_url) in enumerate(post_urls):
            if self.quotient_breach:
                break
            likers = users_liked(self.browser, post_url, interact_likers_per_post, self.logger)
            random.shuffle(likers)
            self.logger.info(""Post '{}' [{}/{}]"".format(post_url, post_index + 1, len(post_urls)))
            for (liker_index, person) in enumerate(likers):
                if self.quotient_breach:
                    self.logger.warning('--> Like quotient reached its peak!\t~leaving Interact-Likers activity\n')
                    break
                self.logger.info(""Liker '{}' [{}/{}]"".format(person, liker_index + 1, len(likers)))
                (validation, details) = self.validate_user_call(person)
                if not validation:
                    self.logger.info(details)
                    not_valid_users += 1
                    continue
                do_interact = random.randint(0, 100) <= self.user_interact_percentage
                if not do_interact:
                    self.logger.info(""Skipping user '{}' due to the interaction percentage of {}"".format(person, self.user_interact_percentage))
                    continue
                else:
                    interacted_all += 1
                    interacted_personal += 1
                    self.logger.info('Interaction [{}/{}]  |  Total Interaction: {}'.format(interacted_personal, len(likers), interacted_all))
                    with self.feature_in_feature('interact_by_users', False):
                        self.interact_by_users(person, self.user_interact_amount, self.user_interact_random, self.user_interact_media)
                    if self.aborting:
                        return self
                    sleep(1)
    self.logger.info(""Finished interacting {} people from {} users' `Followers`! xD\n"".format(interacted_all, len(usernames)))
    liked = self.liked_img - liked_init
    already_liked = self.already_liked - already_liked_init
    commented = self.commented - commented_init
    followed = self.followed - followed_init
    inap_img = self.inap_img - inap_img_init
    self.logger.info('Liked: {}'.format(liked))
    self.logger.info('Already Liked: {}'.format(already_liked))
    self.logger.info('Commented: {}'.format(commented))
    self.logger.info('Followed: {}'.format(followed))
    self.logger.info('Inappropriate: {}'.format(inap_img))
    self.logger.info('Not valid users: {}\n'.format(not_valid_users))
    self.not_valid_users += not_valid_users
    return self","self.logger.info(""User '{}' [{}/{}]"".format(username, index + 1, len(usernames)))",self.logger.info(f"User '{username}' [{index + 1}/{len(usernames)}]"),find_wrong,,1,3,,
InstaPy,https://github.com/InstaPy/InstaPy/tree/master/instapy/instapy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/InstaPy/instapy/instapy.py,InstaPy,"def interact_user_likers(self, usernames: list, posts_grab_amount: int=3, interact_likers_per_post: int=3, randomize: bool=False):
    """"""
        Interact with the likers of given user's posts.

        set_do_comment, set_do_follow and set_do_like are applicable.

        :param usernames: List of users with whose likers to interact.
        :param posts_grab_amount: Amount of posts to get the likers from per given user.
        :param interact_likers_per_post: Amount of likers to be interacted with per post.
        :param randomize: If followers should be chosen randomly.
        """"""
    if self.aborting:
        return self
    if self.do_follow is not True and self.do_like is not True:
        self.logger.info('Please enable following or liking in settings in order to do interactions.')
        return self
    elif self.user_interact_amount <= 0:
        self.logger.info('Please choose an amount higher than zero in `set_user_interact` in order to do interactions.')
        return self
    if not isinstance(usernames, list):
        usernames = [usernames]
    if posts_grab_amount > 12:
        self.logger.info('Sorry, you can only grab likers from first 12 posts for given username now.\n')
        posts_grab_amount = 12
    interacted_all = 0
    not_valid_users = 0
    simulated_unfollow = 0
    liked_init = self.liked_img
    already_liked_init = self.already_liked
    commented_init = self.commented
    followed_init = self.followed
    inap_img_init = self.inap_img
    self.quotient_breach = False
    for (index, username) in enumerate(usernames):
        if self.quotient_breach:
            break
        self.logger.info(""User '{}' [{}/{}]"".format(username, index + 1, len(usernames)))
        try:
            post_urls = get_photo_urls_from_profile(self.browser, username, posts_grab_amount, randomize, self.logger)
            if not isinstance(post_urls, list):
                post_urls = [post_urls]
        except (TypeError, RuntimeWarning) as err:
            if isinstance(err, RuntimeWarning):
                self.logger.warning('Warning: {} , skipping to next user'.format(err))
                continue
            else:
                self.logger.error('Sorry, an error occurred: {}'.format(err))
                self.aborting = True
                return self
        print('')
        self.logger.info(""Grabbed {} posts from '{}'s profile to do interaction."".format(len(post_urls), username))
        interacted_personal = 0
        for (post_index, post_url) in enumerate(post_urls):
            if self.quotient_breach:
                break
            likers = users_liked(self.browser, post_url, interact_likers_per_post, self.logger)
            random.shuffle(likers)
            self.logger.info(""Post '{}' [{}/{}]"".format(post_url, post_index + 1, len(post_urls)))
            for (liker_index, person) in enumerate(likers):
                if self.quotient_breach:
                    self.logger.warning('--> Like quotient reached its peak!\t~leaving Interact-Likers activity\n')
                    break
                self.logger.info(""Liker '{}' [{}/{}]"".format(person, liker_index + 1, len(likers)))
                (validation, details) = self.validate_user_call(person)
                if not validation:
                    self.logger.info(details)
                    not_valid_users += 1
                    continue
                do_interact = random.randint(0, 100) <= self.user_interact_percentage
                if not do_interact:
                    self.logger.info(""Skipping user '{}' due to the interaction percentage of {}"".format(person, self.user_interact_percentage))
                    continue
                else:
                    interacted_all += 1
                    interacted_personal += 1
                    self.logger.info('Interaction [{}/{}]  |  Total Interaction: {}'.format(interacted_personal, len(likers), interacted_all))
                    with self.feature_in_feature('interact_by_users', False):
                        self.interact_by_users(person, self.user_interact_amount, self.user_interact_random, self.user_interact_media)
                    if self.aborting:
                        return self
                    sleep(1)
    self.logger.info(""Finished interacting {} people from {} users' `Followers`! xD\n"".format(interacted_all, len(usernames)))
    liked = self.liked_img - liked_init
    already_liked = self.already_liked - already_liked_init
    commented = self.commented - commented_init
    followed = self.followed - followed_init
    inap_img = self.inap_img - inap_img_init
    self.logger.info('Liked: {}'.format(liked))
    self.logger.info('Already Liked: {}'.format(already_liked))
    self.logger.info('Commented: {}'.format(commented))
    self.logger.info('Followed: {}'.format(followed))
    self.logger.info('Inappropriate: {}'.format(inap_img))
    self.logger.info('Not valid users: {}\n'.format(not_valid_users))
    self.not_valid_users += not_valid_users
    return self","self.logger.info(""Grabbed {} posts from '{}'s profile to do interaction."".format(len(post_urls), username))",self.logger.info(f"Grabbed {len(post_urls)} posts from '{username}'s profile to do interaction."),find_wrong,,1,3,,
InstaPy,https://github.com/InstaPy/InstaPy/tree/master/instapy/instapy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/InstaPy/instapy/instapy.py,InstaPy,"def interact_user_likers(self, usernames: list, posts_grab_amount: int=3, interact_likers_per_post: int=3, randomize: bool=False):
    """"""
        Interact with the likers of given user's posts.

        set_do_comment, set_do_follow and set_do_like are applicable.

        :param usernames: List of users with whose likers to interact.
        :param posts_grab_amount: Amount of posts to get the likers from per given user.
        :param interact_likers_per_post: Amount of likers to be interacted with per post.
        :param randomize: If followers should be chosen randomly.
        """"""
    if self.aborting:
        return self
    if self.do_follow is not True and self.do_like is not True:
        self.logger.info('Please enable following or liking in settings in order to do interactions.')
        return self
    elif self.user_interact_amount <= 0:
        self.logger.info('Please choose an amount higher than zero in `set_user_interact` in order to do interactions.')
        return self
    if not isinstance(usernames, list):
        usernames = [usernames]
    if posts_grab_amount > 12:
        self.logger.info('Sorry, you can only grab likers from first 12 posts for given username now.\n')
        posts_grab_amount = 12
    interacted_all = 0
    not_valid_users = 0
    simulated_unfollow = 0
    liked_init = self.liked_img
    already_liked_init = self.already_liked
    commented_init = self.commented
    followed_init = self.followed
    inap_img_init = self.inap_img
    self.quotient_breach = False
    for (index, username) in enumerate(usernames):
        if self.quotient_breach:
            break
        self.logger.info(""User '{}' [{}/{}]"".format(username, index + 1, len(usernames)))
        try:
            post_urls = get_photo_urls_from_profile(self.browser, username, posts_grab_amount, randomize, self.logger)
            if not isinstance(post_urls, list):
                post_urls = [post_urls]
        except (TypeError, RuntimeWarning) as err:
            if isinstance(err, RuntimeWarning):
                self.logger.warning('Warning: {} , skipping to next user'.format(err))
                continue
            else:
                self.logger.error('Sorry, an error occurred: {}'.format(err))
                self.aborting = True
                return self
        print('')
        self.logger.info(""Grabbed {} posts from '{}'s profile to do interaction."".format(len(post_urls), username))
        interacted_personal = 0
        for (post_index, post_url) in enumerate(post_urls):
            if self.quotient_breach:
                break
            likers = users_liked(self.browser, post_url, interact_likers_per_post, self.logger)
            random.shuffle(likers)
            self.logger.info(""Post '{}' [{}/{}]"".format(post_url, post_index + 1, len(post_urls)))
            for (liker_index, person) in enumerate(likers):
                if self.quotient_breach:
                    self.logger.warning('--> Like quotient reached its peak!\t~leaving Interact-Likers activity\n')
                    break
                self.logger.info(""Liker '{}' [{}/{}]"".format(person, liker_index + 1, len(likers)))
                (validation, details) = self.validate_user_call(person)
                if not validation:
                    self.logger.info(details)
                    not_valid_users += 1
                    continue
                do_interact = random.randint(0, 100) <= self.user_interact_percentage
                if not do_interact:
                    self.logger.info(""Skipping user '{}' due to the interaction percentage of {}"".format(person, self.user_interact_percentage))
                    continue
                else:
                    interacted_all += 1
                    interacted_personal += 1
                    self.logger.info('Interaction [{}/{}]  |  Total Interaction: {}'.format(interacted_personal, len(likers), interacted_all))
                    with self.feature_in_feature('interact_by_users', False):
                        self.interact_by_users(person, self.user_interact_amount, self.user_interact_random, self.user_interact_media)
                    if self.aborting:
                        return self
                    sleep(1)
    self.logger.info(""Finished interacting {} people from {} users' `Followers`! xD\n"".format(interacted_all, len(usernames)))
    liked = self.liked_img - liked_init
    already_liked = self.already_liked - already_liked_init
    commented = self.commented - commented_init
    followed = self.followed - followed_init
    inap_img = self.inap_img - inap_img_init
    self.logger.info('Liked: {}'.format(liked))
    self.logger.info('Already Liked: {}'.format(already_liked))
    self.logger.info('Commented: {}'.format(commented))
    self.logger.info('Followed: {}'.format(followed))
    self.logger.info('Inappropriate: {}'.format(inap_img))
    self.logger.info('Not valid users: {}\n'.format(not_valid_users))
    self.not_valid_users += not_valid_users
    return self","self.logger.info(""Post '{}' [{}/{}]"".format(post_url, post_index + 1, len(post_urls)))",self.logger.info(f"Post '{post_url}' [{post_index + 1}/{len(post_urls)}]"),find_wrong,,1,3,,
InstaPy,https://github.com/InstaPy/InstaPy/tree/master/instapy/instapy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/InstaPy/instapy/instapy.py,InstaPy,"def interact_user_likers(self, usernames: list, posts_grab_amount: int=3, interact_likers_per_post: int=3, randomize: bool=False):
    """"""
        Interact with the likers of given user's posts.

        set_do_comment, set_do_follow and set_do_like are applicable.

        :param usernames: List of users with whose likers to interact.
        :param posts_grab_amount: Amount of posts to get the likers from per given user.
        :param interact_likers_per_post: Amount of likers to be interacted with per post.
        :param randomize: If followers should be chosen randomly.
        """"""
    if self.aborting:
        return self
    if self.do_follow is not True and self.do_like is not True:
        self.logger.info('Please enable following or liking in settings in order to do interactions.')
        return self
    elif self.user_interact_amount <= 0:
        self.logger.info('Please choose an amount higher than zero in `set_user_interact` in order to do interactions.')
        return self
    if not isinstance(usernames, list):
        usernames = [usernames]
    if posts_grab_amount > 12:
        self.logger.info('Sorry, you can only grab likers from first 12 posts for given username now.\n')
        posts_grab_amount = 12
    interacted_all = 0
    not_valid_users = 0
    simulated_unfollow = 0
    liked_init = self.liked_img
    already_liked_init = self.already_liked
    commented_init = self.commented
    followed_init = self.followed
    inap_img_init = self.inap_img
    self.quotient_breach = False
    for (index, username) in enumerate(usernames):
        if self.quotient_breach:
            break
        self.logger.info(""User '{}' [{}/{}]"".format(username, index + 1, len(usernames)))
        try:
            post_urls = get_photo_urls_from_profile(self.browser, username, posts_grab_amount, randomize, self.logger)
            if not isinstance(post_urls, list):
                post_urls = [post_urls]
        except (TypeError, RuntimeWarning) as err:
            if isinstance(err, RuntimeWarning):
                self.logger.warning('Warning: {} , skipping to next user'.format(err))
                continue
            else:
                self.logger.error('Sorry, an error occurred: {}'.format(err))
                self.aborting = True
                return self
        print('')
        self.logger.info(""Grabbed {} posts from '{}'s profile to do interaction."".format(len(post_urls), username))
        interacted_personal = 0
        for (post_index, post_url) in enumerate(post_urls):
            if self.quotient_breach:
                break
            likers = users_liked(self.browser, post_url, interact_likers_per_post, self.logger)
            random.shuffle(likers)
            self.logger.info(""Post '{}' [{}/{}]"".format(post_url, post_index + 1, len(post_urls)))
            for (liker_index, person) in enumerate(likers):
                if self.quotient_breach:
                    self.logger.warning('--> Like quotient reached its peak!\t~leaving Interact-Likers activity\n')
                    break
                self.logger.info(""Liker '{}' [{}/{}]"".format(person, liker_index + 1, len(likers)))
                (validation, details) = self.validate_user_call(person)
                if not validation:
                    self.logger.info(details)
                    not_valid_users += 1
                    continue
                do_interact = random.randint(0, 100) <= self.user_interact_percentage
                if not do_interact:
                    self.logger.info(""Skipping user '{}' due to the interaction percentage of {}"".format(person, self.user_interact_percentage))
                    continue
                else:
                    interacted_all += 1
                    interacted_personal += 1
                    self.logger.info('Interaction [{}/{}]  |  Total Interaction: {}'.format(interacted_personal, len(likers), interacted_all))
                    with self.feature_in_feature('interact_by_users', False):
                        self.interact_by_users(person, self.user_interact_amount, self.user_interact_random, self.user_interact_media)
                    if self.aborting:
                        return self
                    sleep(1)
    self.logger.info(""Finished interacting {} people from {} users' `Followers`! xD\n"".format(interacted_all, len(usernames)))
    liked = self.liked_img - liked_init
    already_liked = self.already_liked - already_liked_init
    commented = self.commented - commented_init
    followed = self.followed - followed_init
    inap_img = self.inap_img - inap_img_init
    self.logger.info('Liked: {}'.format(liked))
    self.logger.info('Already Liked: {}'.format(already_liked))
    self.logger.info('Commented: {}'.format(commented))
    self.logger.info('Followed: {}'.format(followed))
    self.logger.info('Inappropriate: {}'.format(inap_img))
    self.logger.info('Not valid users: {}\n'.format(not_valid_users))
    self.not_valid_users += not_valid_users
    return self","self.logger.info(""Liker '{}' [{}/{}]"".format(person, liker_index + 1, len(likers)))",self.logger.info(f"Liker '{person}' [{liker_index + 1}/{len(likers)}]"),find_wrong,,1,3,,
freeipa,https://github.com/freeipa/freeipa/tree/master/ipaserver/plugins/host.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/plugins/host.py,host,"def get_managed_hosts(self, dn):
    host_filter = 'managedBy=%s' % dn
    host_attrs = ['fqdn']
    ldap = self.api.Backend.ldap2
    managed_hosts = []
    try:
        (hosts, _truncated) = ldap.find_entries(base_dn=DN(self.container_dn, api.env.basedn), filter=host_filter, attrs_list=host_attrs)
        for host in hosts:
            managed_hosts.append(host.dn)
    except errors.NotFound:
        return []
    return managed_hosts",host_filter = 'managedBy=%s' % dn,host_filter = f'managedBy={dn}',find_wrong,,,,,
RigNet,https://github.com/zhan-xu/RigNet/tree/master/utils/rig_parser.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RigNet/utils/rig_parser.py,Skel,"def save(self, filename):
    fout = open(filename, 'w')
    this_level = [self.root]
    hier_level = 1
    while this_level:
        next_level = []
        for p_node in this_level:
            pos = p_node.pos
            parent = p_node.parent.name if p_node.parent is not None else 'None'
            line = '{0} {1} {2:8f} {3:8f} {4:8f} {5}\n'.format(hier_level, p_node.name, pos[0], pos[1], pos[2], parent)
            fout.write(line)
            for c_node in p_node.children:
                next_level.append(c_node)
        this_level = next_level
        hier_level += 1
    fout.close()","'{0} {1} {2:8f} {3:8f} {4:8f} {5}\n'.format(hier_level, p_node.name, pos[0], pos[1], pos[2], parent)",f'{hier_level} {p_node.name} {pos[0]:8f} {pos[1]:8f} {pos[2]:8f} {parent}\n',find_wrong,,1,3,,
espresso,https://github.com/freewym/espresso/tree/master/examples/discriminative_reranking_nmt/criterions/discriminative_reranking_criterion.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/examples/discriminative_reranking_nmt/criterions/discriminative_reranking_criterion.py,KLDivergenceRerankingCriterion,"def forward(self, model, sample, reduce=True):
    """"""Compute the loss for the given sample.

        Returns a tuple with three elements:
        1) the loss
        2) the sample size, which is used as the denominator for the gradient
        3) logging outputs to display while training
        """"""
    sample_size = sample['id'].numel()
    assert sample_size % self.task.cfg.mt_beam == 0, f'sample_size ({sample_size}) cannot be divided by beam size ({self.task.cfg.mt_beam}).Please set --required-batch-size-multiple={self.task.cfg.mt_beam}.'
    batch_out = []
    for i in range(0, sample_size, self.forward_batch_size):
        j = min(i + self.forward_batch_size, sample_size)
        out = model(src_tokens=sample['net_input']['src_tokens'][i:j, :], src_lengths=sample['net_input']['src_lengths'][i:j])
        batch_out.append(model.sentence_forward(out, sample['net_input']['src_tokens'][i:j, :]))
    batch_out = torch.cat(batch_out, dim=0).view(self.task.cfg.mt_beam, sample_size // self.task.cfg.mt_beam, -1)
    if model.joint_classification == 'sent':
        batch_out = model.joint_forward(batch_out)
    scores = model.classification_forward(batch_out.view(sample_size, 1, -1)).view(-1, self.task.cfg.mt_beam)
    loss = self.compute_kl_loss(scores, sample['target'][:, 0].view(-1, self.task.cfg.mt_beam))
    sample_size = sample_size // self.task.cfg.mt_beam
    logging_output = {'loss': loss.detach(), 'ntokens': sample['ntokens'], 'nsentences': sample_size * self.task.cfg.mt_beam, 'sample_size': sample_size, 'scores': scores.detach()}
    return (loss, sample_size, logging_output)","""""""sample_size ({sample_size}) cannot be divided by beam size ({self.task.cfg.mt_beam}).Please set --required-batch-size-multiple={self.task.cfg.mt_beam}.""""""",f'sample_size ({sample_size}) cannot be divided by beam size ({self.task.cfg.mt_beam}).Please set --required-batch-size-multiple={self.task.cfg.mt_beam}.',find_wrong,2,,,,
FARM,https://github.com/deepset-ai/FARM/tree/master/farm/modeling/biadaptive_model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FARM/farm/modeling/biadaptive_model.py,BaseBiAdaptiveModel,"def connect_heads_with_processor(self, tasks, require_labels=True):
    """"""
        Populates prediction head with information coming from tasks.

        :param tasks: A dictionary where the keys are the names of the tasks and the values are the details of the task (e.g. label_list, metric, tensor name)
        :param require_labels: If True, an error will be thrown when a task is not supplied with labels)
        :return:
        """"""
    for head in self.prediction_heads:
        head.label_tensor_name = tasks[head.task_name]['label_tensor_name']
        label_list = tasks[head.task_name]['label_list']
        if not label_list and require_labels:
            raise Exception(f""The task '{head.task_name}' is missing a valid set of labels"")
        label_list = tasks[head.task_name]['label_list']
        head.label_list = label_list
        num_labels = len(label_list)
        head.metric = tasks[head.task_name]['metric']","""""""The task '{head.task_name}' is missing a valid set of labels""""""",f"The task '{head.task_name}' is missing a valid set of labels",find_wrong,2,,,,
data-driven-web-apps-with-flask,https://github.com/talkpython/data-driven-web-apps-with-flask/tree/master/app/ch12-forms/starter/pypi_org/bin/load_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-driven-web-apps-with-flask/app/ch12-forms/starter/pypi_org/bin/load_data.py,,"def do_import_languages(file_data: List[dict]):
    imported = set()
    print('Importing languages ... ', flush=True)
    with progressbar.ProgressBar(max_value=len(file_data)) as bar:
        for (idx, p) in enumerate(file_data):
            info = p.get('info')
            classifiers = info.get('classifiers')
            for c in classifiers:
                if 'Programming Language' not in c:
                    continue
                original = c
                c = c.replace('Implementation ::', '').replace('::', ':')
                text = c
                parts = c.split(':')
                if len(parts) > 1:
                    text = ' '.join(parts[-2:]).strip().replace('  ', ' ')
                if text not in imported:
                    imported.add(text)
                    session = db_session.create_session()
                    lang = ProgrammingLanguage()
                    lang.description = original
                    lang.id = text
                    session.add(lang)
                    session.commit()
            bar.update(idx)
    sys.stderr.flush()
    sys.stdout.flush()","text = ' '.join(parts[-2:]).strip().replace('  ', ' ')","text = f""{' '.join(parts[-2:]).strip().replace('  ', ' ')}""",find_wrong,,1,3,,
tvm,https://github.com/apache/tvm/tree/master/vta/tests/python/unittest/test_vta_insn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/vta/tests/python/unittest/test_vta_insn.py,,"def _run(env, remote):

    def check_alu(tvm_op, np_op=None, use_imm=False, test_name=None):
        """"""Test ALU""""""
        m = 8
        n = 8
        imm = np.random.randint(1, 5)
        a = te.placeholder((m, n, env.BATCH, env.BLOCK_OUT), name='a', dtype=env.acc_dtype)
        a_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: a(*i), 'a_buf')
        if use_imm:
            res_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: tvm_op(a_buf(*i), imm), 'res_buf')
        else:
            b = te.placeholder((m, n, env.BATCH, env.BLOCK_OUT), name='b', dtype=env.acc_dtype)
            b_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: b(*i), 'b_buf')
            res_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: tvm_op(a_buf(*i), b_buf(*i)), 'res_buf')
        res = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: res_buf(*i).astype(env.inp_dtype), 'res')
        s = te.create_schedule(res.op)
        s[a_buf].set_scope(env.acc_scope)
        s[a_buf].pragma(a_buf.op.axis[0], env.dma_copy)
        s[res_buf].set_scope(env.acc_scope)
        s[res_buf].pragma(res_buf.op.axis[0], env.alu)
        s[res].pragma(res.op.axis[0], env.dma_copy)
        if not use_imm:
            s[b_buf].set_scope(env.acc_scope)
            s[b_buf].pragma(b_buf.op.axis[0], env.dma_copy)
        if not remote:
            return
        with vta.build_config():
            if use_imm:
                mod = vta.build(s, [a, res], tvm.target.Target('ext_dev', host=env.target_host))
            else:
                mod = vta.build(s, [a, b, res], tvm.target.Target('ext_dev', host=env.target_host))
        temp = utils.tempdir()
        mod.save(temp.relpath('load_act.o'))
        remote.upload(temp.relpath('load_act.o'))
        f = remote.load_module('load_act.o')
        dev = remote.ext_dev(0)
        a_np = np.random.randint(-16, 16, size=(m, n, env.BATCH, env.BLOCK_OUT)).astype(a.dtype)
        if use_imm:
            res_np = np_op(a_np, imm) if np_op else tvm_op(a_np, imm)
        else:
            b_np = np.random.randint(-16, 16, size=(m, n, env.BATCH, env.BLOCK_OUT)).astype(b.dtype)
            res_np = np_op(a_np, b_np) if np_op else tvm_op(a_np, b_np)
        res_np = res_np.astype(res.dtype)
        a_nd = tvm.nd.array(a_np, dev)
        res_nd = tvm.nd.array(np.zeros((m, n, env.BATCH, env.BLOCK_OUT)).astype(res.dtype), dev)
        if env.TARGET in ['sim', 'tsim']:
            simulator.clear_stats()
        if use_imm:
            f(a_nd, res_nd)
        else:
            b_nd = tvm.nd.array(b_np, dev)
            f(a_nd, b_nd, res_nd)
        np.testing.assert_equal(res_np, res_nd.numpy())
        if env.TARGET in ['sim', 'tsim']:
            sim_stats = simulator.stats()
            print('ALU {} execution statistics:'.format(test_name))
            for (k, v) in sim_stats.items():
                print('\t{:<16}: {:>16}'.format(k, v))
    check_alu(lambda x, y: x << y, np.left_shift, use_imm=True, test_name='SHL')
    check_alu(tvm.te.max, np.maximum, use_imm=True, test_name='MAX')
    check_alu(tvm.te.max, np.maximum, test_name='MAX')
    check_alu(lambda x, y: x + y, use_imm=True, test_name='ADD')
    check_alu(lambda x, y: x + y, test_name='ADD')
    check_alu(lambda x, y: x >> y, np.right_shift, use_imm=True, test_name='SHR')",'ALU {} execution statistics:'.format(test_name),f'ALU {test_name} execution statistics:',find_wrong,,1,3,,
taiga-back,https://github.com/taigaio/taiga-back/tree/master/tests/integration/test_milestones.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taiga-back/tests/integration/test_milestones.py,,"def test_api_filter_by_milestone__estimated_start_and_end(client, field_name):
    user = f.UserFactory.create()
    project = f.ProjectFactory.create(owner=user)
    role = f.RoleFactory.create(project=project)
    f.MembershipFactory.create(project=project, user=user, role=role, is_admin=True)
    milestone = f.MilestoneFactory.create(project=project, owner=user)
    assert hasattr(milestone, field_name)
    date = getattr(milestone, field_name)
    before = (date - timedelta(days=1)).isoformat()
    after = (date + timedelta(days=1)).isoformat()
    client.login(milestone.owner)
    expections = {field_name + '__gte=' + quote(before): 1, field_name + '__gte=' + quote(after): 0, field_name + '__lte=' + quote(before): 0, field_name + '__lte=' + quote(after): 1}
    for (param, expection) in expections.items():
        url = reverse('milestones-list') + '?' + param
        response = client.get(url)
        number_of_milestones = len(response.data)
        assert response.status_code == 200
        assert number_of_milestones == expection, param
        if number_of_milestones > 0:
            assert response.data[0]['slug'] == milestone.slug",field_name + '__gte=' + quote(before),f'{field_name}__gte={quote(before)}',find_wrong,,1,3,,
taiga-back,https://github.com/taigaio/taiga-back/tree/master/tests/integration/test_milestones.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taiga-back/tests/integration/test_milestones.py,,"def test_api_filter_by_milestone__estimated_start_and_end(client, field_name):
    user = f.UserFactory.create()
    project = f.ProjectFactory.create(owner=user)
    role = f.RoleFactory.create(project=project)
    f.MembershipFactory.create(project=project, user=user, role=role, is_admin=True)
    milestone = f.MilestoneFactory.create(project=project, owner=user)
    assert hasattr(milestone, field_name)
    date = getattr(milestone, field_name)
    before = (date - timedelta(days=1)).isoformat()
    after = (date + timedelta(days=1)).isoformat()
    client.login(milestone.owner)
    expections = {field_name + '__gte=' + quote(before): 1, field_name + '__gte=' + quote(after): 0, field_name + '__lte=' + quote(before): 0, field_name + '__lte=' + quote(after): 1}
    for (param, expection) in expections.items():
        url = reverse('milestones-list') + '?' + param
        response = client.get(url)
        number_of_milestones = len(response.data)
        assert response.status_code == 200
        assert number_of_milestones == expection, param
        if number_of_milestones > 0:
            assert response.data[0]['slug'] == milestone.slug",field_name + '__gte=' + quote(after),f'{field_name}__gte={quote(after)}',find_wrong,,1,3,,
taiga-back,https://github.com/taigaio/taiga-back/tree/master/tests/integration/test_milestones.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taiga-back/tests/integration/test_milestones.py,,"def test_api_filter_by_milestone__estimated_start_and_end(client, field_name):
    user = f.UserFactory.create()
    project = f.ProjectFactory.create(owner=user)
    role = f.RoleFactory.create(project=project)
    f.MembershipFactory.create(project=project, user=user, role=role, is_admin=True)
    milestone = f.MilestoneFactory.create(project=project, owner=user)
    assert hasattr(milestone, field_name)
    date = getattr(milestone, field_name)
    before = (date - timedelta(days=1)).isoformat()
    after = (date + timedelta(days=1)).isoformat()
    client.login(milestone.owner)
    expections = {field_name + '__gte=' + quote(before): 1, field_name + '__gte=' + quote(after): 0, field_name + '__lte=' + quote(before): 0, field_name + '__lte=' + quote(after): 1}
    for (param, expection) in expections.items():
        url = reverse('milestones-list') + '?' + param
        response = client.get(url)
        number_of_milestones = len(response.data)
        assert response.status_code == 200
        assert number_of_milestones == expection, param
        if number_of_milestones > 0:
            assert response.data[0]['slug'] == milestone.slug",field_name + '__lte=' + quote(before),f'{field_name}__lte={quote(before)}',find_wrong,,1,3,,
taiga-back,https://github.com/taigaio/taiga-back/tree/master/tests/integration/test_milestones.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taiga-back/tests/integration/test_milestones.py,,"def test_api_filter_by_milestone__estimated_start_and_end(client, field_name):
    user = f.UserFactory.create()
    project = f.ProjectFactory.create(owner=user)
    role = f.RoleFactory.create(project=project)
    f.MembershipFactory.create(project=project, user=user, role=role, is_admin=True)
    milestone = f.MilestoneFactory.create(project=project, owner=user)
    assert hasattr(milestone, field_name)
    date = getattr(milestone, field_name)
    before = (date - timedelta(days=1)).isoformat()
    after = (date + timedelta(days=1)).isoformat()
    client.login(milestone.owner)
    expections = {field_name + '__gte=' + quote(before): 1, field_name + '__gte=' + quote(after): 0, field_name + '__lte=' + quote(before): 0, field_name + '__lte=' + quote(after): 1}
    for (param, expection) in expections.items():
        url = reverse('milestones-list') + '?' + param
        response = client.get(url)
        number_of_milestones = len(response.data)
        assert response.status_code == 200
        assert number_of_milestones == expection, param
        if number_of_milestones > 0:
            assert response.data[0]['slug'] == milestone.slug",field_name + '__lte=' + quote(after),f'{field_name}__lte={quote(after)}',find_wrong,,1,3,,
django-rest-framework,https://github.com/encode/django-rest-framework/tree/master/rest_framework/viewsets.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-rest-framework/rest_framework/viewsets.py,ViewSetMixin,"def get_extra_action_url_map(self):
    """"""
        Build a map of {names: urls} for the extra actions.

        This method will noop if `detail` was not provided as a view initkwarg.
        """"""
    action_urls = OrderedDict()
    if self.detail is None:
        return action_urls
    actions = [action for action in self.get_extra_actions() if action.detail == self.detail]
    for action in actions:
        try:
            url_name = '%s-%s' % (self.basename, action.url_name)
            url = reverse(url_name, self.args, self.kwargs, request=self.request)
            view = self.__class__(**action.kwargs)
            action_urls[view.get_view_name()] = url
        except NoReverseMatch:
            pass
    return action_urls","url_name = '%s-%s' % (self.basename, action.url_name)",url_name = f'{self.basename}-{action.url_name}',find_wrong,,,,,
doit,https://github.com/pydoit/doit/tree/master/doit/task.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/doit/doit/task.py,Task,"def clean(self, outstream, dryrun):
    """"""Execute task's clean
        @ivar outstream: 'write' output into this stream
        @ivar dryrun (bool): if True clean tasks are not executed
                             (just print out what would be executed)
        """"""
    self.init_options()
    if self._remove_targets is True:
        clean_targets(self, dryrun)
    else:
        for action in self.clean_actions:
            msg = ""%s - executing '%s'\n""
            outstream.write(msg % (self.name, action))
            execute_on_dryrun = False
            if isinstance(action, PythonAction):
                action_sig = inspect.signature(action.py_callable)
                if 'dryrun' in action_sig.parameters:
                    execute_on_dryrun = True
                    action.kwargs['dryrun'] = dryrun
            if not dryrun or execute_on_dryrun:
                result = action.execute(out=outstream)
                if isinstance(result, CatchedException):
                    sys.stderr.write(str(result))","msg = ""%s - executing '%s'\n""
outstream.write(msg % (self.name, action))",outstream.write(f"{self.name} - executing '{action}'\n"),find_wrong,2,1,3,,
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/webapps/reports/controllers/system.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/webapps/reports/controllers/system.py,System,"def deleted_histories(self, trans, **kwd):
    """"""
        The number of histories that were deleted more than the specified number of days ago, but have not yet been purged.
        Also included is the number of datasets associated with the histories.
        """"""
    params = util.Params(kwd)
    message = ''
    if params.deleted_histories_days:
        deleted_histories_days = int(params.deleted_histories_days)
        cutoff_time = datetime.utcnow() - timedelta(days=deleted_histories_days)
        history_count = 0
        dataset_count = 0
        disk_space = 0
        histories = trans.sa_session.query(model.History).filter(and_(model.History.table.c.deleted == true(), model.History.table.c.purged == false(), model.History.update_time < cutoff_time)).options(eagerload('datasets'))
        for history in histories:
            for hda in history.datasets:
                if not hda.dataset.purged:
                    dataset_count += 1
                    try:
                        disk_space += hda.dataset.file_size
                    except Exception:
                        pass
            history_count += 1
        message = '%d histories ( including a total of %d datasets ) were deleted more than %d days ago, but have not yet been purged, disk space: %s.' % (history_count, dataset_count, deleted_histories_days, nice_size(disk_space, True))
    else:
        message = 'Enter the number of days.'
    return (str(deleted_histories_days), message)","%d histories ( including a total of %d datasets ) were deleted more than %d days ago, but have not yet been purged, disk space: %s.' % (history_count, dataset_count, deleted_histories_days, nice_size(disk_space, True))","f'{history_count} histories ( including a total of {dataset_count} datasets ) were deleted more than {deleted_histories_days} days ago, but have not yet been purged, disk space: {nice_size(disk_space, True)}.'",find_wrong,2,1,3,,
tvm,https://github.com/apache/tvm/tree/master/python/tvm/autotvm/tuner/sa_model_optimizer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/autotvm/tuner/sa_model_optimizer.py,SimulatedAnnealingOptimizer,"def find_maximums(self, model, num, exclusive):
    tic = time.time()
    (temp, n_iter, early_stop, log_interval) = (self.temp, self.n_iter, self.early_stop, self.log_interval)
    if self.persistent and self.points is not None:
        points = self.points
    else:
        points = self.task.config_space.sample_ints(self.parallel_size)
    scores = model.predict(points)
    heap_items = [(float('-inf'), -1 - i) for i in range(num)]
    heapq.heapify(heap_items)
    in_heap = set(exclusive)
    in_heap.update([x[1] for x in heap_items])
    for (s, p) in zip(scores, points):
        if s > heap_items[0][0] and p not in in_heap:
            pop = heapq.heapreplace(heap_items, (s, p))
            in_heap.remove(pop[1])
            in_heap.add(p)
    k = 0
    k_last_modify = 0
    if isinstance(temp, (tuple, list, np.ndarray)):
        t = temp[0]
        cool = 1.0 * (temp[0] - temp[1]) / (n_iter + 1)
    else:
        t = temp
        cool = 0
    while k < n_iter and k < k_last_modify + early_stop:
        new_points = np.empty_like(points)
        for (i, p) in enumerate(points):
            new_points[i] = self.task.config_space.random_walk(p)
        new_scores = model.predict(new_points)
        ac_prob = np.exp(np.minimum((new_scores - scores) / (t + 1e-05), 1))
        ac_index = np.random.random(len(ac_prob)) < ac_prob
        points[ac_index] = new_points[ac_index]
        scores[ac_index] = new_scores[ac_index]
        for (s, p) in zip(new_scores, new_points):
            if s > heap_items[0][0] and p not in in_heap:
                pop = heapq.heapreplace(heap_items, (s, p))
                in_heap.remove(pop[1])
                in_heap.add(p)
                k_last_modify = k
        k += 1
        t -= cool
        if log_interval and k % log_interval == 0:
            t_str = '%.2f' % t
            logger.debug('SA iter: %d\tlast_update: %d\tmax-0: %.2f\tmax-1: %.2f\ttemp: %s\telapsed: %.2f', k, k_last_modify, heap_items[0][0], np.max([v for (v, _) in heap_items]), t_str, time.time() - tic)
    heap_items.sort(key=lambda item: -item[0])
    heap_items = [x for x in heap_items if x[0] >= 0]
    logger.debug('SA iter: %d\tlast_update: %d\telapsed: %.2f', k, k_last_modify, time.time() - tic)
    logger.debug('SA Maximums: %s', heap_items)
    if self.persistent:
        self.points = points
    return [x[1] for x in heap_items]",t_str = '%.2f' % t,t_str = f'{t:.2f}',find_wrong,,,,,
tvm,https://github.com/apache/tvm/tree/master/python/tvm/autotvm/tuner/sa_model_optimizer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/autotvm/tuner/sa_model_optimizer.py,SimulatedAnnealingOptimizer,"def find_maximums(self, model, num, exclusive):
    tic = time.time()
    (temp, n_iter, early_stop, log_interval) = (self.temp, self.n_iter, self.early_stop, self.log_interval)
    if self.persistent and self.points is not None:
        points = self.points
    else:
        points = self.task.config_space.sample_ints(self.parallel_size)
    scores = model.predict(points)
    heap_items = [(float('-inf'), -1 - i) for i in range(num)]
    heapq.heapify(heap_items)
    in_heap = set(exclusive)
    in_heap.update([x[1] for x in heap_items])
    for (s, p) in zip(scores, points):
        if s > heap_items[0][0] and p not in in_heap:
            pop = heapq.heapreplace(heap_items, (s, p))
            in_heap.remove(pop[1])
            in_heap.add(p)
    k = 0
    k_last_modify = 0
    if isinstance(temp, (tuple, list, np.ndarray)):
        t = temp[0]
        cool = 1.0 * (temp[0] - temp[1]) / (n_iter + 1)
    else:
        t = temp
        cool = 0
    while k < n_iter and k < k_last_modify + early_stop:
        new_points = np.empty_like(points)
        for (i, p) in enumerate(points):
            new_points[i] = self.task.config_space.random_walk(p)
        new_scores = model.predict(new_points)
        ac_prob = np.exp(np.minimum((new_scores - scores) / (t + 1e-05), 1))
        ac_index = np.random.random(len(ac_prob)) < ac_prob
        points[ac_index] = new_points[ac_index]
        scores[ac_index] = new_scores[ac_index]
        for (s, p) in zip(new_scores, new_points):
            if s > heap_items[0][0] and p not in in_heap:
                pop = heapq.heapreplace(heap_items, (s, p))
                in_heap.remove(pop[1])
                in_heap.add(p)
                k_last_modify = k
        k += 1
        t -= cool
        if log_interval and k % log_interval == 0:
            t_str = '%.2f' % t
            logger.debug('SA iter: %d\tlast_update: %d\tmax-0: %.2f\tmax-1: %.2f\ttemp: %s\telapsed: %.2f', k, k_last_modify, heap_items[0][0], np.max([v for (v, _) in heap_items]), t_str, time.time() - tic)
    heap_items.sort(key=lambda item: -item[0])
    heap_items = [x for x in heap_items if x[0] >= 0]
    logger.debug('SA iter: %d\tlast_update: %d\telapsed: %.2f', k, k_last_modify, time.time() - tic)
    logger.debug('SA Maximums: %s', heap_items)
    if self.persistent:
        self.points = points
    return [x[1] for x in heap_items]","logger.debug('SA iter: %d\tlast_update: %d\tmax-0: %.2f\tmax-1: %.2f\ttemp: %s\telapsed: %.2f', k, k_last_modify, heap_items[0][0], np.max([v for (v, _) in heap_items]), t_str, time.time() - tic)","logger.debug(f'SA iter: {k}\tlast_update: {k_last_modify}\tmax-0: {heap_items[0][0]:.2f}\tmax-1: {np.max([v for (v, _) in heap_items]):.2f}\ttemp: {t_str}\telapsed: {time.time() - tic:.2f}')",find_wrong,2,,,,
PyQt5-Apps,https://github.com/taseikyo/PyQt5-Apps/tree/master/hust-lib/src/main.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyQt5-Apps/hust-lib/src/main.py,MWin,"def resolveDataDone(self, data):
    self.statusBar.showMessage('...', 1000)
    if self.first_kw:
        self.total_pages = int(data[0])
        self.jump_page.setMaximum(self.total_pages)
        self.page_count.setText(f' {self.total_pages} ')
        self.first_kw = False
    self.current_page_index.setText(f' {self.cur_page} ')
    self.jump_page.setValue(self.cur_page)
    self.pre = data[1]
    self.suf = data[2]
    self.request.pre = self.pre
    self.request.suf = self.suf
    books = data[-1]
    if not self.table_has_header:
        self.table.setColumnCount(2)
        self.table.setHorizontalHeaderLabels(['', ''])
        self.table_has_header = True
    else:
        self.table.clearContents()
        self.table.setRowCount(0)
    lists = []
    for x in books:
        tl = x.find('span', {'class', 'briefcitTitle'}).a
        title = tl.text
        link = tl['href']
        detail = x.find('span', {'class', 'briefcitDetail'}).text.replace('\n', ' ')
        lists.append([title, detail, link])
        row = self.table.rowCount()
        self.table.insertRow(row)
        self.table.setItem(row, 0, QTableWidgetItem(title))
        self.table.setItem(row, 1, QTableWidgetItem(detail))
    self.book_lists = lists",self.page_count.setText(f' {self.total_pages} '),self.page_count.setText(f' {self.total_pages}'),find_wrong,2,,,,
PyQt5-Apps,https://github.com/taseikyo/PyQt5-Apps/tree/master/hust-lib/src/main.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyQt5-Apps/hust-lib/src/main.py,MWin,"def resolveDataDone(self, data):
    self.statusBar.showMessage('...', 1000)
    if self.first_kw:
        self.total_pages = int(data[0])
        self.jump_page.setMaximum(self.total_pages)
        self.page_count.setText(f' {self.total_pages} ')
        self.first_kw = False
    self.current_page_index.setText(f' {self.cur_page} ')
    self.jump_page.setValue(self.cur_page)
    self.pre = data[1]
    self.suf = data[2]
    self.request.pre = self.pre
    self.request.suf = self.suf
    books = data[-1]
    if not self.table_has_header:
        self.table.setColumnCount(2)
        self.table.setHorizontalHeaderLabels(['', ''])
        self.table_has_header = True
    else:
        self.table.clearContents()
        self.table.setRowCount(0)
    lists = []
    for x in books:
        tl = x.find('span', {'class', 'briefcitTitle'}).a
        title = tl.text
        link = tl['href']
        detail = x.find('span', {'class', 'briefcitDetail'}).text.replace('\n', ' ')
        lists.append([title, detail, link])
        row = self.table.rowCount()
        self.table.insertRow(row)
        self.table.setItem(row, 0, QTableWidgetItem(title))
        self.table.setItem(row, 1, QTableWidgetItem(detail))
    self.book_lists = lists",self.current_page_index.setText(f' {self.cur_page} '),self.current_page_index.setText(f' {self.cur_page}'),find_wrong,2,,,,
numpy,https://github.com/numpy/numpy/tree/master/numpy/_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/_version.py,,"def render_pep440_post_branch(pieces):
    """"""TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .

    The "".dev0"" means not master branch.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]
    """"""
    if pieces['closest-tag']:
        rendered = pieces['closest-tag']
        if pieces['distance'] or pieces['dirty']:
            rendered += '.post%d' % pieces['distance']
            if pieces['branch'] != 'master':
                rendered += '.dev0'
            rendered += plus_or_dot(pieces)
            rendered += 'g%s' % pieces['short']
            if pieces['dirty']:
                rendered += '.dirty'
    else:
        rendered = '0.post%d' % pieces['distance']
        if pieces['branch'] != 'master':
            rendered += '.dev0'
        rendered += '+g%s' % pieces['short']
        if pieces['dirty']:
            rendered += '.dirty'
    return rendered","""""""rendered += '.post%d' % pieces['distance']""""""",f"{rendered}.post{pieces['distance']}",find_wrong,,,,,
numpy,https://github.com/numpy/numpy/tree/master/numpy/_version.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy/numpy/_version.py,,"def render_pep440_post_branch(pieces):
    """"""TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .

    The "".dev0"" means not master branch.

    Exceptions:
    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]
    """"""
    if pieces['closest-tag']:
        rendered = pieces['closest-tag']
        if pieces['distance'] or pieces['dirty']:
            rendered += '.post%d' % pieces['distance']
            if pieces['branch'] != 'master':
                rendered += '.dev0'
            rendered += plus_or_dot(pieces)
            rendered += 'g%s' % pieces['short']
            if pieces['dirty']:
                rendered += '.dirty'
    else:
        rendered = '0.post%d' % pieces['distance']
        if pieces['branch'] != 'master':
            rendered += '.dev0'
        rendered += '+g%s' % pieces['short']
        if pieces['dirty']:
            rendered += '.dirty'
    return rendered","""""""rendered += 'g%s' % pieces['short']""""""",f"{rendered}g{pieces['short']}",find_wrong,,,,,
ansible-modules-extras,https://github.com/ansible/ansible-modules-extras/tree/master/database/mssql/mssql_db.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-extras/database/mssql/mssql_db.py,,"def db_import(conn, cursor, module, db, target):
    if os.path.isfile(target):
        backup = open(target, 'r')
        try:
            sqlQuery = 'USE [%s]\n' % db
            for line in backup:
                if line is None:
                    break
                elif line.startswith('GO'):
                    cursor.execute(sqlQuery)
                    sqlQuery = 'USE [%s]\n' % db
                else:
                    sqlQuery += line
            cursor.execute(sqlQuery)
            conn.commit()
        finally:
            backup.close()
        return (0, 'import successful', '')
    else:
        return (1, 'cannot find target file', 'cannot find target file')",sqlQuery = 'USE [%s]\n' % db,sqlQuery = f'USE [{db}]\n',find_wrong,,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]","[_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]","[f""{_('Date')}"", f""{_('Time')}"", f""{_('Check-in list')}"", f""{_('Scan type')}"", f""{_('Order code')}"", f""{_('Position ID')}"", f""{_('Secret')}"", f""{_('Product')}"", f""{_('Name')}"", f""{_('Device')}"", f""{_('Offline override')}"", f""{_('Automatically checked in')}"", f""{_('Gate')}"", f""{_('Result')}"", f""{_('Error message')}"", f""{_('Upload date')}"", f""{_('Upload time')}""]",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]","date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT')","f""{date_format(ci.datetime.astimezone(self.timezone), '%x')}""",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]","date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT')","f""{date_format(ci.datetime.astimezone(self.timezone), '%X')}""",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",str(ci.list),f'{ci.list}',find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",ci.get_type_display(),f'{ci.get_type_display()}',find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",ci.position.order.code if ci.position else '',f"{(ci.position.order.code if ci.position else '')}",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",ci.position.positionid if ci.position else '',f"{(ci.position.positionid if ci.position else '')}",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",ci.raw_barcode or ci.position.secret,f'{ci.raw_barcode or ci.position.secret}',find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '',f"{(str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '')}",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",ci.position.attendee_name or ia.name if ci.position else '',f"{(ci.position.attendee_name or ia.name if ci.position else '')}",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",str(ci.device) if ci.device else '',f"{(str(ci.device) if ci.device else '')}",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",_('Yes') if ci.forced else _('No'),f"{(_('Yes') if ci.forced else _('No'))}",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",_('Yes') if ci.auto_checked_in else _('No'),f"{(_('Yes') if ci.auto_checked_in else _('No'))}",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",str(ci.gate or ''),f"{str(ci.gate or '')}",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",_('OK') if ci.successful else ci.get_error_reason_display(),f"{(_('OK') if ci.successful else ci.get_error_reason_display())}",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]",ci.error_explanation or '',f"{ci.error_explanation or ''}",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]","date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT')","f""{date_format(ci.created.astimezone(self.timezone), '%x')}""",find_wrong,2,,,,
pretix,https://github.com/pretix/pretix/tree/master/src/pretix/plugins/checkinlists/exporters.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pretix/src/pretix/plugins/checkinlists/exporters.py,CheckinLogList,"def iterate_list(self, form_data):
    yield [_('Date'), _('Time'), _('Check-in list'), _('Scan type'), _('Order code'), _('Position ID'), _('Secret'), _('Product'), _('Name'), _('Device'), _('Offline override'), _('Automatically checked in'), _('Gate'), _('Result'), _('Error message'), _('Upload date'), _('Upload time')]
    qs = Checkin.all.filter(list__event=self.event)
    if form_data.get('list'):
        qs = qs.filter(list_id=form_data.get('list'))
    if form_data.get('items'):
        if len(form_data['items']) != self.event.items.count():
            qs = qs.filter(Q(position__item_id__in=form_data['items']) | Q(raw_item_id__in=form_data['items']))
    if form_data.get('successful_only'):
        qs = qs.filter(successful=True)
    yield self.ProgressSetTotal(total=qs.count())
    qs = qs.select_related('position__item', 'position__order', 'position__order__invoice_address', 'position', 'list', 'device', 'raw_item').order_by('datetime')
    for ci in qs.iterator():
        if ci.position:
            try:
                ia = ci.position.order.invoice_address
            except InvoiceAddress.DoesNotExist:
                ia = InvoiceAddress()
        yield [date_format(ci.datetime.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.datetime.astimezone(self.timezone), 'TIME_FORMAT'), str(ci.list), ci.get_type_display(), ci.position.order.code if ci.position else '', ci.position.positionid if ci.position else '', ci.raw_barcode or ci.position.secret, str(ci.position.item) if ci.position else str(ci.raw_item) if ci.raw_item else '', ci.position.attendee_name or ia.name if ci.position else '', str(ci.device) if ci.device else '', _('Yes') if ci.forced else _('No'), _('Yes') if ci.auto_checked_in else _('No'), str(ci.gate or ''), _('OK') if ci.successful else ci.get_error_reason_display(), ci.error_explanation or '', date_format(ci.created.astimezone(self.timezone), 'SHORT_DATE_FORMAT'), date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')]","date_format(ci.created.astimezone(self.timezone), 'TIME_FORMAT')","f""{date_format(ci.created.astimezone(self.timezone), '%X')}""",find_wrong,2,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_dist_base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_dist_base.py,TestDistRunnerBase,"def run_use_fleet_api_trainer(self, args):
    assert args.update_method == 'nccl2' or 'bkcl'
    self.lr = args.lr
    exec_strategy = fluid.ExecutionStrategy()
    exec_strategy.num_threads = 1
    dist_strategy = DistributedStrategy()
    dist_strategy.exec_strategy = exec_strategy
    dist_strategy.fuse_memory_size = 1
    dist_strategy.fuse_laryer_size = 1
    if args.use_local_sgd:
        dist_strategy.use_local_sgd = True
    if args.ut4grad_allreduce:
        dist_strategy._ut4grad_allreduce = True
    if args.sync_batch_norm:
        dist_strategy.sync_batch_norm = True
    role = role_maker.PaddleCloudRoleMaker(is_collective=True)
    fleet.init(role)
    print_to_err('use_fleet', 'fleet.node_num:')
    (test_program, avg_cost, train_reader, test_reader, batch_acc, predict) = self.get_model(batch_size=args.batch_size, dist_strategy=dist_strategy)
    trainer_prog = fleet._origin_program
    dist_prog = fleet.main_program
    if fluid.core.is_compiled_with_cuda():
        device_id = int(os.getenv('FLAGS_selected_gpus', '0'))
        place = fluid.CUDAPlace(device_id)
    elif fluid.core.is_compiled_with_xpu():
        device_id = int(os.getenv('FLAGS_selected_xpus', '0'))
        place = fluid.XPUPlace(device_id)
    else:
        raise ValueError('fleet dygraph api must in paddlepaddle-xpu or paddlepaddle-gpu.')
    exe = fluid.Executor(place)
    exe.run(fluid.default_startup_program())
    eprint(type(self).__name__, 'run worker startup program done.')
    feed_var_list = [var for var in trainer_prog.global_block().vars.values() if var.is_data]
    eprint('feed_var_list:', feed_var_list)
    if feed_var_list[0].name == 'label':
        feed_var_list = feed_var_list[::-1]
    feeder = fluid.DataFeeder(feed_var_list, place)
    reader_generator = train_reader()

    def get_data():
        origin_batch = next(reader_generator)
        if args.update_method != 'local' and args.use_reader_alloc:
            new_batch = []
            for (offset, item) in enumerate(origin_batch):
                if offset % 2 == args.trainer_id:
                    new_batch.append(item)
            return new_batch
        else:
            return origin_batch
    print_to_err(type(self).__name__, 'begin to train on trainer')
    out_losses = []
    for i in range(RUN_STEP):
        (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
        out_losses.append(loss[0])
        print_to_err(type(self).__name__, 'run step %d finished' % i)
    print_to_err(type(self).__name__, 'trainer run finished')
    sys.stdout.buffer.write(pickle.dumps(out_losses))
    if args.save_model:
        model_save_dir = '/tmp'
        if fleet.worker_index() == 0:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer')
        else:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables_2')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables_2')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer_2')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer_2')
        paddle.distributed.io.save_persistables(exe, model_save_dir_fluid, fleet._origin_program)
        fleet.save_persistables(executor=exe, dirname=model_save_dir_fleet)
        feeded_var_names = [var.name for var in feed_var_list]
        fluid.io.save_inference_model(infer_save_dir_fluid, feeded_var_names, [avg_cost], exe, fleet._origin_program)
        fleet.save_inference_model(exe, infer_save_dir_fleet, feeded_var_names, [avg_cost])",arg_%x' % var.offset,f'arg_{var.offset:x}',find_wrong,2,,,,
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))",name = f'{source.__module__}.{source.__name__}',name = f'{source.__module__}.{source.__qualname__}',find_wrong,2,,,,
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))","issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')","issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')",find_wrong,2,,,,
cloud-custodian,https://github.com/cloud-custodian/cloud-custodian/tree/master/tests/test_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tests/test_policy.py,PolicyMetaLint,"def test_deprecation_dates(self):

    def check_deprecations(source):
        issues = set()
        for dep in getattr(source, 'deprecations', ()):
            when = dep.removed_after
            if when is not None:
                name = f'{source.__module__}.{source.__name__}'
                if not isinstance(when, str):
                    issues.add(f'{name}: ""{dep}"", removed_after attribute must be a string')
                    continue
                try:
                    datetime.strptime(when, '%Y-%m-%d')
                except ValueError:
                    issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")
        return issues
    issues = check_deprecations(Policy)
    for (name, cloud) in clouds.items():
        for (resource_name, resource) in cloud.resources.items():
            issues = issues.union(check_deprecations(resource))
            for (fname, f) in resource.filter_registry.items():
                if fname in ('and', 'or', 'not'):
                    continue
                issues = issues.union(check_deprecations(f))
            for (aname, a) in resource.action_registry.items():
                issues = issues.union(check_deprecations(a))
    for (name, mode) in execution.items():
        issues = issues.union(check_deprecations(mode))
    if issues:
        self.fail('Deprecation validation issues with \n\t%s' % '\n\t'.join(sorted(issues)))","issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")","issues.add(f""""""{name}: ""{dep}"", removed_after must be a valid date in the format 'YYYY-MM-DD', got '{when}'"""""")",find_wrong,2,,,,
amundsen,https://github.com/amundsen-io/amundsen/tree/master/common/tests/unit/utils/test_atlas_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/amundsen/common/tests/unit/utils/test_atlas_utils.py,TestAtlasColumnKey,"def test_table_column_key_details_from_qualified_name(self) -> None:
    params = [('db_name.table_name.column_name@cluster_name', dict(cluster='cluster_name', schema='db_name', table='table_name', column='column_name')), ('db_name.table_name.column_name.dot@cluster_name.dot', dict(cluster='cluster_name.dot', schema='db_name', table='table_name', column='column_name.dot'))]
    for (qn, details) in params:
        with self.subTest(f'Test extract details from qualified name: {qn}'):
            result = AtlasColumnKey(qn)
            self.assertEqual(details, result.get_details())","Non-Idiomatic code: 
with self.subTest(f'Test extract details from qualified name: {qn}'):",with self.subTest(f"Test extract details from qualified name: {qn}"):,find_wrong,2,,,,
suzieq,https://github.com/netenglabs/suzieq/tree/master/suzieq/db/parquet/pq_coalesce.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/suzieq/suzieq/db/parquet/pq_coalesce.py,,"def coalesce_resource_table(infolder: str, outfolder: str, archive_folder: str, table: str, state: SqCoalesceState) -> None:
    """"""This routine coalesces all the parquet data in the folder provided

    This function MUST be called with sqPoller as the table the first time to
    build the polling period sample. Without this, its not possible to compute
    the records to be written for a period accurately. The polling periods are
    computed when this function is called the first time with None as the
    state field. This function stuffs the sqPoller timeblocks as the polling
    period in the state block and returns it. The state object returned also
    has some statistics written such as number of files written, number of
    records written and so on.

    :param infolder: str, folder to read data in from
    :param outfolder: str, folder to write data to
    :param archive_folder: str, folder to store the archived files in
    :param table: str, name of table we're coalesceing
    :param state: SqCoalesceState, state about this coalesceion run
    :returns: Nothing
    """"""

    def compute_block_start(start):
        return start - timedelta(seconds=start.timestamp() % state.period.total_seconds())
    partition_cols = ['sqvers', 'namespace']
    dodel = True
    if table == 'sqPoller':
        wr_polling_period = True
        state.poller_periods = set()
    else:
        wr_polling_period = False
    state.wrfile_count = 0
    state.wrrec_count = 0
    state.table_name = table
    schema = state.schema
    if state.schema.type == 'record':
        state.keys = schema.key_fields()
        if state.current_df.empty:
            state.current_df = get_last_update_df(table, outfolder, state)
    dataset = ds.dataset(infolder, partitioning='hive', format='parquet', ignore_prefixes=state.ign_pfx, schema=schema.get_arrow_schema())
    state.logger.info(f'Examining {len(dataset.files)} {table} files for coalescing')
    fdf = get_file_timestamps(dataset.files)
    if fdf.empty:
        if table == 'sqPoller' or not state.poller_periods:
            return
    polled_periods = sorted(state.poller_periods)
    if fdf.empty:
        state.logger.info(f'No updates for {table} to coalesce')
        start = polled_periods[0]
    else:
        start = fdf.timestamp.iloc[0]
    utcnow = datetime.now(timezone.utc)
    if utcnow < start:
        logging.error('ERROR: Something is off, now is earlier than dates on files')
        return
    block_start = compute_block_start(start)
    block_end = block_start + state.period
    if block_end > utcnow:
        return
    readblock = []
    wrfile_count = 0
    if schema.type == 'record':
        for interval in polled_periods:
            if not fdf.empty and block_end < interval:
                break
            pre_block_start = compute_block_start(interval)
            pre_block_end = pre_block_start + state.period
            write_files(table, readblock, infolder, outfolder, partition_cols, state, pre_block_start, pre_block_end)
    for row in fdf.itertuples():
        if block_start <= row.timestamp < block_end:
            readblock.append(row.file)
            continue
        if readblock or (schema.type == 'record' and block_start in state.poller_periods):
            write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            wrfile_count += len(readblock)
        if wr_polling_period and readblock:
            state.poller_periods.add(block_start)
        if readblock:
            archive_coalesced_files(readblock, archive_folder, state, dodel)
        block_start = block_end
        block_end = block_start + state.period
        readblock = []
        if schema.type != 'record':
            block_start = compute_block_start(row.timestamp)
            block_end = block_start + state.period
            if row.timestamp > block_end or block_end > utcnow:
                break
            readblock = [row.file]
            continue
        while row.timestamp > block_end:
            if block_start in state.poller_periods:
                write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            block_start = block_end
            block_end = block_start + state.period
        if block_end > utcnow:
            break
        readblock = [row.file]
    if readblock or (fdf.empty and schema.type == 'record' and (block_start in state.poller_periods)):
        write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        wrfile_count += len(readblock)
        if wr_polling_period:
            state.poller_periods.add(block_start)
        archive_coalesced_files(readblock, archive_folder, state, dodel)
    state.wrfile_count = wrfile_count
    return",state.logger.info(f'Examining {len(dataset.files)} {table} files for coalescing'),"state.logger.info('Examining {} {} files for coalescing'.format(len(dataset.files), table))",find_wrong,2,,,,
suzieq,https://github.com/netenglabs/suzieq/tree/master/suzieq/db/parquet/pq_coalesce.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/suzieq/suzieq/db/parquet/pq_coalesce.py,,"def coalesce_resource_table(infolder: str, outfolder: str, archive_folder: str, table: str, state: SqCoalesceState) -> None:
    """"""This routine coalesces all the parquet data in the folder provided

    This function MUST be called with sqPoller as the table the first time to
    build the polling period sample. Without this, its not possible to compute
    the records to be written for a period accurately. The polling periods are
    computed when this function is called the first time with None as the
    state field. This function stuffs the sqPoller timeblocks as the polling
    period in the state block and returns it. The state object returned also
    has some statistics written such as number of files written, number of
    records written and so on.

    :param infolder: str, folder to read data in from
    :param outfolder: str, folder to write data to
    :param archive_folder: str, folder to store the archived files in
    :param table: str, name of table we're coalesceing
    :param state: SqCoalesceState, state about this coalesceion run
    :returns: Nothing
    """"""

    def compute_block_start(start):
        return start - timedelta(seconds=start.timestamp() % state.period.total_seconds())
    partition_cols = ['sqvers', 'namespace']
    dodel = True
    if table == 'sqPoller':
        wr_polling_period = True
        state.poller_periods = set()
    else:
        wr_polling_period = False
    state.wrfile_count = 0
    state.wrrec_count = 0
    state.table_name = table
    schema = state.schema
    if state.schema.type == 'record':
        state.keys = schema.key_fields()
        if state.current_df.empty:
            state.current_df = get_last_update_df(table, outfolder, state)
    dataset = ds.dataset(infolder, partitioning='hive', format='parquet', ignore_prefixes=state.ign_pfx, schema=schema.get_arrow_schema())
    state.logger.info(f'Examining {len(dataset.files)} {table} files for coalescing')
    fdf = get_file_timestamps(dataset.files)
    if fdf.empty:
        if table == 'sqPoller' or not state.poller_periods:
            return
    polled_periods = sorted(state.poller_periods)
    if fdf.empty:
        state.logger.info(f'No updates for {table} to coalesce')
        start = polled_periods[0]
    else:
        start = fdf.timestamp.iloc[0]
    utcnow = datetime.now(timezone.utc)
    if utcnow < start:
        logging.error('ERROR: Something is off, now is earlier than dates on files')
        return
    block_start = compute_block_start(start)
    block_end = block_start + state.period
    if block_end > utcnow:
        return
    readblock = []
    wrfile_count = 0
    if schema.type == 'record':
        for interval in polled_periods:
            if not fdf.empty and block_end < interval:
                break
            pre_block_start = compute_block_start(interval)
            pre_block_end = pre_block_start + state.period
            write_files(table, readblock, infolder, outfolder, partition_cols, state, pre_block_start, pre_block_end)
    for row in fdf.itertuples():
        if block_start <= row.timestamp < block_end:
            readblock.append(row.file)
            continue
        if readblock or (schema.type == 'record' and block_start in state.poller_periods):
            write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            wrfile_count += len(readblock)
        if wr_polling_period and readblock:
            state.poller_periods.add(block_start)
        if readblock:
            archive_coalesced_files(readblock, archive_folder, state, dodel)
        block_start = block_end
        block_end = block_start + state.period
        readblock = []
        if schema.type != 'record':
            block_start = compute_block_start(row.timestamp)
            block_end = block_start + state.period
            if row.timestamp > block_end or block_end > utcnow:
                break
            readblock = [row.file]
            continue
        while row.timestamp > block_end:
            if block_start in state.poller_periods:
                write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            block_start = block_end
            block_end = block_start + state.period
        if block_end > utcnow:
            break
        readblock = [row.file]
    if readblock or (fdf.empty and schema.type == 'record' and (block_start in state.poller_periods)):
        write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        wrfile_count += len(readblock)
        if wr_polling_period:
            state.poller_periods.add(block_start)
        archive_coalesced_files(readblock, archive_folder, state, dodel)
    state.wrfile_count = wrfile_count
    return","logging.error('ERROR: Something is off, now is earlier than dates on files')","logging.error('ERROR: Something is off, now is earlier than dates on files')",find_wrong,2,,,,
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","""Could not find answer: '%s' vs. '%s'"" % (actual_text, cleaned_answer_text)",f"Could not find answer: '{actual_text}' vs. '{cleaned_answer_text}'",find_wrong,2,,,,
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']","tokenizer.__class__.__name__ in {'RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast'}",find_wrong,2,,,,
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","%sToken' % tokenizer.__class__.__name__.replace('Tokenizer', '').lower()","f""{tokenizer.__class__.__name__.replace('Tokenizer', '').lower()}Token""",find_wrong,2,,,,
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features",'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0),'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and (not encoded_dict['overflowing_tokens'])),find_wrong,2,,,,
routersploit,https://github.com/threat9/routersploit/tree/master/routersploit/modules/exploits/routers/asmax/ar_1004g_password_disclosure.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/routersploit/routersploit/modules/exploits/routers/asmax/ar_1004g_password_disclosure.py,Exploit,"def run(self):
    creds = []
    print_status('Requesting {}'.format(self.get_target_url()))
    response = self.http_request(method='GET', path='/password.cgi')
    if response is None:
        print_error('Exploit failed - empty response')
        return
    tokens = [('admin', ""pwdAdmin = '(.+?)'""), ('support', ""pwdSupport = '(.+?)'""), ('user', ""pwdUser = '(.+?)'"")]
    print_status('Trying to extract credentials')
    for token in tokens:
        res = re.findall(token[1], response.text)
        if res:
            creds.append((token[0], res[0]))
    if creds:
        print_success('Credentials found')
        print_table(('Login', 'Password'), *creds)
    else:
        print_error('Exploit failed - credentials could not be found')",print_status('Requesting {}'.format(self.get_target_url())),print_status(f'Requesting {self.get_target_url()}'),find_wrong,2,3,1,,
routersploit,https://github.com/threat9/routersploit/tree/master/routersploit/modules/exploits/routers/asmax/ar_1004g_password_disclosure.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/routersploit/routersploit/modules/exploits/routers/asmax/ar_1004g_password_disclosure.py,Exploit,"def run(self):
    creds = []
    print_status('Requesting {}'.format(self.get_target_url()))
    response = self.http_request(method='GET', path='/password.cgi')
    if response is None:
        print_error('Exploit failed - empty response')
        return
    tokens = [('admin', ""pwdAdmin = '(.+?)'""), ('support', ""pwdSupport = '(.+?)'""), ('user', ""pwdUser = '(.+?)'"")]
    print_status('Trying to extract credentials')
    for token in tokens:
        res = re.findall(token[1], response.text)
        if res:
            creds.append((token[0], res[0]))
    if creds:
        print_success('Credentials found')
        print_table(('Login', 'Password'), *creds)
    else:
        print_error('Exploit failed - credentials could not be found')","tokens = [('admin', ""pwdAdmin = '(.+?)'""), ('support', ""pwdSupport = '(.+?)'""), ('user', ""pwdUser = '(.+?)'"")]","tokens = [('admin', r""pwdAdmin = '(.+?)'""), ('support', r""pwdSupport = '(.+?)'""), ('user', r""pwdUser = '(.+?)'"")]
Note: The 'r' before the regex string is added to make it a raw string.",find_wrong,2,,,,
yt-dlp,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/imdb.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlp/yt_dlp/extractor/imdb.py,ImdbListIE,"def _real_extract(self, url):
    list_id = self._match_id(url)
    webpage = self._download_webpage(url, list_id)
    entries = [self.url_result('http://www.imdb.com' + m, 'Imdb') for m in re.findall('href=""(/list/ls%s/videoplayer/vi[^""]+)""' % list_id, webpage)]
    list_title = self._html_search_regex('<h1[^>]+class=""[^""]*header[^""]*""[^>]*>(.*?)</h1>', webpage, 'list title')
    list_description = self._html_search_regex('<div[^>]+class=""[^""]*list-description[^""]*""[^>]*><p>(.*?)</p>', webpage, 'list description')
    return self.playlist_result(entries, list_id, list_title, list_description)",http://www.imdb.com' + m,f'http://www.imdb.com{m}',find_wrong,2,1,3,,
pi-timolo,https://github.com/pageauc/pi-timolo/tree/master/source/pi-timolo-67/pi-timolo.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pi-timolo/source/pi-timolo-67/pi-timolo.py,,"def subDirCreate(directory, prefix):
    now = datetime.datetime.now()
    subDirName = '%s%d-%02d-%02d-%02d:%02d' % (prefix, now.year, now.month, now.day, now.hour, now.minute)
    subDirPath = os.path.join(directory, subDirName)
    if not os.path.exists(subDirPath):
        try:
            os.makedirs(subDirPath)
        except OSError as err:
            logging.error('Cannot Create Directory %s - %s, using default location.', subDirPath, err)
            subDirPath = directory
        else:
            logging.info('Created %s', subDirPath)
    else:
        subDirPath = directory
    return subDirPath","subDirName = '%s%d-%02d-%02d-%02d:%02d' % (prefix, now.year, now.month, now.day, now.hour, now.minute)",subDirName = f'{prefix}{now.year}-{now.month:02}-{now.day:02}-{now.hour:02}:{now.minute:02}',find_wrong,,,,,
SDV,https://github.com/sdv-dev/SDV/tree/master/sdv/timeseries/deepecho.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SDV/sdv/timeseries/deepecho.py,DeepEchoModel,"def _fit(self, timeseries_data):
    self._model = self._build_model()
    if self._sequence_index:
        timeseries_data = timeseries_data.rename(columns={self._sequence_index + '.value': self._sequence_index})
    self._output_columns = list(timeseries_data.columns)
    self._data_columns = [column for column in timeseries_data.columns if column not in self._entity_columns + self._context_columns]
    sequences = assemble_sequences(timeseries_data, self._entity_columns, self._context_columns, self._segment_size, self._sequence_index, drop_sequence_index=False)
    data_types = list()
    context_types = list()
    for field in self._output_columns:
        dtype = timeseries_data[field].dtype
        kind = dtype.kind
        if kind in ('i', 'f'):
            data_type = 'continuous'
        elif kind in ('O', 'b'):
            data_type = 'categorical'
        else:
            raise ValueError(f'Unsupported dtype {dtype}')
        if field in self._data_columns:
            data_types.append(data_type)
        elif field in self._context_columns:
            context_types.append(data_type)
    if self._sequence_index:
        self._transform_sequence_index(sequences)
        data_types.append('continuous')
    self._model.fit_sequences(sequences, context_types, data_types)",raise ValueError(f'Unsupported dtype {dtype}'),raise ValueError(f'Unsupported dtype {dtype}'),find_wrong,2,,,,
hummingbot,https://github.com/CoinAlpha/hummingbot/tree/master/hummingbot/strategy/hedge/hedge_config_map.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hummingbot/hummingbot/strategy/hedge/hedge_config_map.py,,"def asset_validate(value: str) -> Optional[str]:
    tokens_list = list()
    if len(value.strip()) == 0:
        return 'Invalid market(s). The given entry is empty.'
    markets = list(value.upper().split(','))
    for market in markets:
        if len(market.strip()) == 0:
            return 'Invalid assets. The given entry contains an empty market.'
        tokens = market.strip().split('-')
        if len(tokens) >= 2:
            return f'Invalid asset. {market} contain more than 1 asset.'
        for token in tokens:
            if len(token.strip()) == 0:
                return f'Invalid market. Ticker {token} has an invalid length.'
            if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
                return f'Invalid market. Ticker {token} contains invalid characters.'
            if token in tokens_list:
                return f'Duplicate market {token}.'
            tokens_list.append(token)","""""""Invalid asset. {market} contain more than 1 asset.""""""",f'Invalid asset. {market} contain more than 1 asset.',find_wrong,2,,,,
hummingbot,https://github.com/CoinAlpha/hummingbot/tree/master/hummingbot/strategy/hedge/hedge_config_map.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hummingbot/hummingbot/strategy/hedge/hedge_config_map.py,,"def asset_validate(value: str) -> Optional[str]:
    tokens_list = list()
    if len(value.strip()) == 0:
        return 'Invalid market(s). The given entry is empty.'
    markets = list(value.upper().split(','))
    for market in markets:
        if len(market.strip()) == 0:
            return 'Invalid assets. The given entry contains an empty market.'
        tokens = market.strip().split('-')
        if len(tokens) >= 2:
            return f'Invalid asset. {market} contain more than 1 asset.'
        for token in tokens:
            if len(token.strip()) == 0:
                return f'Invalid market. Ticker {token} has an invalid length.'
            if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
                return f'Invalid market. Ticker {token} contains invalid characters.'
            if token in tokens_list:
                return f'Duplicate market {token}.'
            tokens_list.append(token)","""""""Invalid market. Ticker {token} has an invalid length.""""""",f'Invalid market. Ticker {token} has an invalid length.',find_wrong,2,,,,
hummingbot,https://github.com/CoinAlpha/hummingbot/tree/master/hummingbot/strategy/hedge/hedge_config_map.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hummingbot/hummingbot/strategy/hedge/hedge_config_map.py,,"def asset_validate(value: str) -> Optional[str]:
    tokens_list = list()
    if len(value.strip()) == 0:
        return 'Invalid market(s). The given entry is empty.'
    markets = list(value.upper().split(','))
    for market in markets:
        if len(market.strip()) == 0:
            return 'Invalid assets. The given entry contains an empty market.'
        tokens = market.strip().split('-')
        if len(tokens) >= 2:
            return f'Invalid asset. {market} contain more than 1 asset.'
        for token in tokens:
            if len(token.strip()) == 0:
                return f'Invalid market. Ticker {token} has an invalid length.'
            if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
                return f'Invalid market. Ticker {token} contains invalid characters.'
            if token in tokens_list:
                return f'Duplicate market {token}.'
            tokens_list.append(token)","""""""Invalid market. Ticker {token} contains invalid characters.""""""",f'Invalid market. Ticker {token} contains invalid characters.',find_wrong,2,,,,
hummingbot,https://github.com/CoinAlpha/hummingbot/tree/master/hummingbot/strategy/hedge/hedge_config_map.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hummingbot/hummingbot/strategy/hedge/hedge_config_map.py,,"def asset_validate(value: str) -> Optional[str]:
    tokens_list = list()
    if len(value.strip()) == 0:
        return 'Invalid market(s). The given entry is empty.'
    markets = list(value.upper().split(','))
    for market in markets:
        if len(market.strip()) == 0:
            return 'Invalid assets. The given entry contains an empty market.'
        tokens = market.strip().split('-')
        if len(tokens) >= 2:
            return f'Invalid asset. {market} contain more than 1 asset.'
        for token in tokens:
            if len(token.strip()) == 0:
                return f'Invalid market. Ticker {token} has an invalid length.'
            if bool(re.search('^[a-zA-Z0-9]*$', token)) is False:
                return f'Invalid market. Ticker {token} contains invalid characters.'
            if token in tokens_list:
                return f'Duplicate market {token}.'
            tokens_list.append(token)","""""""Duplicate market {token}.""""""",f'Duplicate market {token}.',find_wrong,2,,,,
astroquery,https://github.com/astropy/astroquery/tree/master/astroquery/lamda/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astroquery/astroquery/lamda/core.py,,"def parse_lamda_lines(data):
    """"""
    Extract a LAMDA datafile into a dictionary of tables

    (non-pythonic!  more like, fortranic)
    """"""
    meta_rad = {}
    meta_mol = {}
    meta_coll = {}
    levels = []
    radtrans = []
    collider = None
    ncolltrans = None
    for (ii, line) in enumerate(data):
        if line[0] == '!':
            continue
        if 'molecule' not in meta_mol:
            meta_mol['molecule'] = _cln(line)
            continue
        if 'molwt' not in meta_mol:
            meta_mol['molwt'] = float(_cln(line))
            continue
        if 'nenergylevels' not in meta_mol:
            meta_mol['nenergylevels'] = int(_cln(line))
            continue
        if len(levels) < meta_mol['nenergylevels']:
            (lev, en, wt) = _cln(line).split()[:3]
            jul = ' '.join(_cln(line).split()[3:])
            levels.append([int(lev), float(en), int(float(wt)), jul])
            continue
        if 'radtrans' not in meta_rad:
            meta_rad['radtrans'] = int(_cln(line))
            continue
        if len(radtrans) < meta_rad['radtrans']:
            (trans, up, low, aval, freq, eu) = _cln(line).split()[:6]
            radtrans.append([int(trans), int(up), int(low), float(aval), float(freq), float(eu)])
            continue
        if 'ncoll' not in meta_coll:
            meta_coll['ncoll'] = int(_cln(line))
            collrates = {}
            continue
        if collider is None:
            collider = int(line[0])
            collname = collider_ids[collider]
            collrates[collider] = []
            meta_coll[collname] = {'collider': collname, 'collider_id': collider}
            continue
        if ncolltrans is None:
            ncolltrans = int(_cln(line))
            meta_coll[collname]['ntrans'] = ncolltrans
            continue
        if 'ntemp' not in meta_coll[collname]:
            meta_coll[collname]['ntemp'] = int(_cln(line))
            continue
        if 'temperatures' not in meta_coll[collname]:
            meta_coll[collname]['temperatures'] = [int(float(x)) for x in _cln(line).split()]
            continue
        if len(collrates[collider]) < meta_coll[collname]['ntrans']:
            (trans, up, low) = [int(x) for x in _cln(line).split()[:3]]
            temperatures = [float(x) for x in _cln(line).split()[3:]]
            collrates[collider].append([trans, up, low] + temperatures)
        if len(collrates[collider]) == meta_coll[collname]['ntrans']:
            log.debug('{ii} Finished loading collider {0:d}: {1}'.format(collider, collider_ids[collider], ii=ii))
            collider = None
            ncolltrans = None
            if len(collrates) == meta_coll['ncoll']:
                break
    if len(levels[0]) == 4:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J']
    elif len(levels[0]) == 5:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J', 'F']
    else:
        raise ValueError('Unrecognized levels structure.')
    mol_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(mol_table_names, zip(*levels))]
    mol_table = table.Table(data=mol_table_columns, meta=meta_mol)
    rad_table_names = ['Transition', 'Upper', 'Lower', 'EinsteinA', 'Frequency', 'E_u(K)']
    rad_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(rad_table_names, zip(*radtrans))]
    rad_table = table.Table(data=rad_table_columns, meta=meta_rad)
    coll_tables = {collider_ids[collider]: None for collider in collrates}
    for collider in collrates:
        collname = collider_ids[collider]
        coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
        coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
        coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
        coll_tables[collname] = coll_table
    return (coll_tables, rad_table, mol_table)",'C_ij(T={0:d})'.format(tem),f'C_ij(T={tem:d})',find_wrong,2,1,3,,
DeepRobust,https://github.com/DSE-MSU/DeepRobust/tree/master/deeprobust/graph/defense/gcn_preprocess.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRobust/deeprobust/graph/defense/gcn_preprocess.py,,"def dropedge_both(A, iA, jA, features, threshold1=2.5, threshold2=0.01):
    removed_cnt = 0
    for row in range(len(iA) - 1):
        for i in range(iA[row], iA[row + 1]):
            n1 = row
            n2 = jA[i]
            C1 = np.linalg.norm(features[n1] - features[n2])
            (a, b) = (features[n1], features[n2])
            inner_product = (a * b).sum()
            C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06)
            if C1 > threshold1 or threshold2 < 0:
                A[i] = 0
                removed_cnt += 1
    return removed_cnt",C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06),C2 = inner_product / (np.sqrt(np.square(a).sum() + np.square(b).sum()) + 1e-06),find_wrong,2,,,,
tfc,https://github.com/maqp/tfc/tree/master/tests/common/test_crypto.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tfc/tests/common/test_crypto.py,TestArgon2KDF,"def test_argon2_cffi_using_the_official_command_line_utility(self) -> None:
    min_password_length = 1
    max_password_length = 127
    min_salt_length = 8
    min_parallelism = 1
    max_parallelism = multiprocessing.cpu_count()
    min_time_cost = 1
    min_memory_cost = 7
    min_key_length = 4
    max_salt_length = 128
    max_time_cost = 3
    max_memory_cost = 15
    max_key_length = 64
    sys_rand = random.SystemRandom()
    for _ in range(self.number_of_tests):
        len_password = sys_rand.randint(min_password_length, max_password_length)
        len_salt = sys_rand.randint(min_salt_length, max_salt_length)
        parallelism = sys_rand.randint(min_parallelism, max_parallelism)
        time_cost = sys_rand.randint(min_time_cost, max_time_cost)
        memory_cost = sys_rand.randint(min_memory_cost, max_memory_cost)
        key_length = sys_rand.randint(min_key_length, max_key_length)
        password = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_password)])
        salt = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_salt)])
        output = subprocess.check_output(f'echo -n ""{password}"" | ./argon2 {salt} -t {time_cost} -m {memory_cost} -p {parallelism} -l {key_length} -id', shell=True).decode()
        key_test_vector = output.split('\n')[4].split('\t')[-1]
        purported_key = argon2.low_level.hash_secret_raw(secret=password.encode(), salt=salt.encode(), time_cost=time_cost, memory_cost=2 ** memory_cost, parallelism=parallelism, hash_len=key_length, type=argon2.Type.ID).hex()
        self.assertEqual(purported_key, key_test_vector)","""""""echo -n ""{password}"" | ./argon2 {salt} -t {time_cost} -m {memory_cost} -p {parallelism} -l {key_length} -id""""""",f'echo -n "{password}" | ./argon2 {salt} -t {time_cost} -m {memory_cost} -p {parallelism} -l {key_length} -id',find_wrong,2,,,fstring,
