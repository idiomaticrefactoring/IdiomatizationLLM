repo_name,file_path,file_html,class_name,me_name,me_code,old_code,new_code,gd_truth_code,acc,baseline,ours,real
xarray,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xarray/xarray/core/merge.py,https://github.com/pydata/xarray/tree/master/xarray/core/merge.py,,dataset_update_method$961,"def dataset_update_method(dataset: 'Dataset', other: 'CoercibleMapping') -> _MergeResult:
    """"""Guts of the Dataset.update method.

    This drops a duplicated coordinates from `other` if `other` is not an
    `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,
    GH2180).
    """"""
    from .dataarray import DataArray
    from .dataset import Dataset
    if not isinstance(other, Dataset):
        other = dict(other)
        for (key, value) in other.items():
            if isinstance(value, DataArray):
                coord_names = [c for c in value.coords if c not in value.dims and c in dataset.coords]
                if coord_names:
                    other[key] = value.drop_vars(coord_names)
    indexes = {}
    for (key, index) in dataset.xindexes.items():
        if isinstance(index, PandasIndex):
            indexes[key] = dataset.coords[key]
        else:
            indexes[key] = index
    return merge_core([dataset, other], priority_arg=1, indexes=indexes, combine_attrs='override')","for (key, value) in other.items():
    if isinstance(value, DataArray):
        coord_names = [c for c in value.coords if c not in value.dims and c in dataset.coords]
        if coord_names:
            other[key] = value.drop_vars(coord_names)","other = {key: value.drop_vars([c for c in value.coords if c not in value.dims and c in dataset.coords]) if isinstance(value, DataArray) else value for (key, value) in dict(other).items()}",find_wrong,1,1,3,1
mars,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mars/mars/dataframe/plotting/core.py,https://github.com/mars-project/mars/tree/master/mars/dataframe/plotting/core.py,PlotAccessor,__call__$27,"def __call__(self, kind='line', session=None, **kwargs):
    to_executes = OrderedDict()
    to_executes['__object__'] = self._obj
    for (k, v) in kwargs.items():
        if isinstance(v, ENTITY_TYPE):
            to_executes[k] = v
    result = dict()
    executed = ExecutableTuple(to_executes.values()).execute().fetch()
    for (p, v) in zip(to_executes, executed):
        result[p] = v
    data = result.pop('__object__')
    pd_kwargs = kwargs.copy()
    pd_kwargs['kind'] = kind
    pd_kwargs.update(result)
    return data.plot(**pd_kwargs)","to_executes['__object__'] = self._obj
for (k, v) in kwargs.items():
    if isinstance(v, ENTITY_TYPE):
        to_executes[k] = v","to_executes = {'__object__': self._obj, **{k: v for (k, v) in kwargs.items() if isinstance(v, ENTITY_TYPE)}}",find_wrong,1,1,3,1
networkx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/networkx/networkx/algorithms/bipartite/matching.py,https://github.com/networkx/networkx/tree/master/networkx/algorithms/bipartite/matching.py,,eppstein_matching$182,"def eppstein_matching(G, top_nodes=None):
    """"""Returns the maximum cardinality matching of the bipartite graph `G`.

    Parameters
    ----------
    G : NetworkX graph

      Undirected bipartite graph

    top_nodes : container

      Container with all nodes in one bipartite node set. If not supplied
      it will be computed. But if more than one solution exists an exception
      will be raised.

    Returns
    -------
    matches : dictionary

      The matching is returned as a dictionary, `matching`, such that
      ``matching[v] == w`` if node `v` is matched to node `w`. Unmatched
      nodes do not occur as a key in `matching`.

    Raises
    ------
    AmbiguousSolution
      Raised if the input bipartite graph is disconnected and no container
      with all nodes in one bipartite set is provided. When determining
      the nodes in each bipartite set more than one valid solution is
      possible if the input graph is disconnected.

    Notes
    -----
    This function is implemented with David Eppstein's version of the algorithm
    Hopcroft--Karp algorithm (see :func:`hopcroft_karp_matching`), which
    originally appeared in the `Python Algorithms and Data Structures library
    (PADS) <http://www.ics.uci.edu/~eppstein/PADS/ABOUT-PADS.txt>`_.

    See :mod:`bipartite documentation <networkx.algorithms.bipartite>`
    for further details on how bipartite graphs are handled in NetworkX.

    See Also
    --------

    hopcroft_karp_matching

    """"""
    (left, right) = bipartite_sets(G, top_nodes)
    G = nx.DiGraph(G.edges(left))
    matching = {}
    for u in G:
        for v in G[u]:
            if v not in matching:
                matching[v] = u
                break
    while True:
        preds = {}
        unmatched = []
        pred = {u: unmatched for u in G}
        for v in matching:
            del pred[matching[v]]
        layer = list(pred)
        while layer and (not unmatched):
            newLayer = {}
            for u in layer:
                for v in G[u]:
                    if v not in preds:
                        newLayer.setdefault(v, []).append(u)
            layer = []
            for v in newLayer:
                preds[v] = newLayer[v]
                if v in matching:
                    layer.append(matching[v])
                    pred[matching[v]] = v
                else:
                    unmatched.append(v)
        if not unmatched:
            unlayered = {}
            for u in G:
                for v in G[u]:
                    if v not in preds:
                        unlayered[v] = None
            for key in matching.copy():
                matching[matching[key]] = key
            return matching

        def recurse(v):
            if v in preds:
                L = preds.pop(v)
                for u in L:
                    if u in pred:
                        pu = pred.pop(u)
                        if pu is unmatched or recurse(pu):
                            matching[v] = u
                            return True
            return False
        for v in unmatched:
            recurse(v)","for u in G:
    for v in G[u]:
        if v not in matching:
            matching[v] = u
            break",matching = {v: u for u in G for v in G[u] if v not in matching},find_wrong,2,,,
unilm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unilm/unilm-v1/src/biunilm/loader_utils.py,https://github.com/microsoft/unilm/tree/master/unilm-v1/src/biunilm/loader_utils.py,Pipeline,get_masked_pos$146,"def get_masked_pos(self, tokens, n_pred, add_skipgram=False, mask_segment=None, protect_range=None):
    if self.pieces_dir is not None and self.trie is None:
        self.create_trie_tree(self.pieces_dir)
    if self.pre_whole_word:
        if self.trie is not None:
            pieces = self.trie.get_pieces(tokens, 0)
            new_pieces = []
            for piece in pieces:
                if len(new_pieces) > 0 and tokens[piece[0]].startswith('##'):
                    new_pieces[-1].extend(piece)
                else:
                    new_pieces.append(piece)
            del pieces
            pieces = new_pieces
            pre_word_split = list((_[-1] for _ in pieces))
            pre_word_split.append(len(tokens))
        else:
            pre_word_split = _get_word_split_index(tokens, 0, len(tokens))
        index2piece = None
    else:
        pre_word_split = list(range(0, len(tokens) + 1))
        if self.trie is not None:
            pieces = self.trie.get_pieces(tokens, 0)
            index2piece = {}
            for piece in pieces:
                for index in piece:
                    index2piece[index] = (piece[0], piece[-1])
        else:
            index2piece = None
    span_list = list(zip(pre_word_split[:-1], pre_word_split[1:]))
    cand_pos = []
    special_pos = set()
    if mask_segment:
        for (i, sp) in enumerate(span_list):
            (sp_st, sp_end) = sp
            if sp_end - sp_st == 1 and tokens[sp_st].endswith('SEP]'):
                segment_index = i
                break
    for (i, sp) in enumerate(span_list):
        (sp_st, sp_end) = sp
        if sp_end - sp_st == 1 and (tokens[sp_st].endswith('CLS]') or tokens[sp_st].endswith('SEP]')):
            special_pos.add(i)
        elif mask_segment:
            if i < segment_index and 'a' in mask_segment or (i > segment_index and 'b' in mask_segment):
                cand_pos.append(i)
        else:
            cand_pos.append(i)
    shuffle(cand_pos)
    masked_pos = set()
    for i_span in cand_pos:
        if len(masked_pos) >= n_pred:
            break
        (cand_st, cand_end) = span_list[i_span]
        if len(masked_pos) + cand_end - cand_st > n_pred:
            continue
        if any((p in masked_pos for p in range(cand_st, cand_end))):
            continue
        n_span = 1
        if index2piece is not None:
            (p_start, p_end) = index2piece[i_span]
            if p_start < p_end and rand() < self.sp_prob:
                (st_span, end_span) = (p_start, p_end + 1)
            else:
                (st_span, end_span) = (i_span, i_span + 1)
        else:
            rand_skipgram_size = 0
            if self.skipgram_size_geo_list:
                rand_skipgram_size = np.random.choice(len(self.skipgram_size_geo_list), 1, p=self.skipgram_size_geo_list)[0] + 1
            elif add_skipgram and self.skipgram_prb > 0 and (self.skipgram_size >= 2) and (rand() < self.skipgram_prb):
                rand_skipgram_size = min(randint(2, self.skipgram_size), len(span_list) - i_span)
            for n in range(2, rand_skipgram_size + 1):
                (tail_st, tail_end) = span_list[i_span + n - 1]
                if tail_end - tail_st == 1 and tail_st in special_pos:
                    break
                if len(masked_pos) + tail_end - cand_st > n_pred:
                    break
                n_span = n
            (st_span, end_span) = (i_span, i_span + n_span)
        if self.mask_whole_word:
            (st_span, end_span) = _expand_whole_word(tokens, st_span, end_span)
        if self.word_subsample_prb:
            skip_pos = set()
            if self.pre_whole_word:
                w_span_list = span_list[st_span:end_span]
            else:
                split_idx = _get_word_split_index(tokens, st_span, end_span)
                w_span_list = list(zip(split_idx[:-1], split_idx[1:]))
            for (i, sp) in enumerate(w_span_list):
                (sp_st, sp_end) = sp
                if sp_end - sp_st == 1:
                    w_cat = tokens[sp_st]
                else:
                    w_cat = ''.join(tokens[sp_st:sp_end])
                if w_cat in self.word_subsample_prb and rand() < self.word_subsample_prb[w_cat]:
                    for k in range(sp_st, sp_end):
                        skip_pos.add(k)
        else:
            skip_pos = None
        for sp in range(st_span, end_span):
            for mp in range(span_list[sp][0], span_list[sp][1]):
                if not (skip_pos and mp in skip_pos) and mp not in special_pos and (not (protect_range and protect_range[0] <= mp < protect_range[1])):
                    masked_pos.add(mp)
    if len(masked_pos) < n_pred:
        shuffle(cand_pos)
        for pos in cand_pos:
            if len(masked_pos) >= n_pred:
                break
            if pos not in masked_pos:
                masked_pos.add(pos)
    masked_pos = list(masked_pos)
    if len(masked_pos) > n_pred:
        masked_pos = masked_pos[:n_pred]
    return masked_pos","cand_pos = []
for (i, sp) in enumerate(span_list):
    (sp_st, sp_end) = sp
    if sp_end - sp_st == 1 and (tokens[sp_st].endswith('CLS]') or tokens[sp_st].endswith('SEP]')):
        special_pos.add(i)
    elif mask_segment:
        if i < segment_index and 'a' in mask_segment or (i > segment_index and 'b' in mask_segment):
            cand_pos.append(i)
    else:
        cand_pos.append(i)
masked_pos = set()
for i_span in cand_pos:
    if len(masked_pos) >= n_pred:
        break
    (cand_st, cand_end) = span_list[i_span]
    if len(masked_pos) + cand_end - cand_st > n_pred:
        continue
    if any((p in masked_pos for p in range(cand_st, cand_end))):
        continue
    n_span = 1
    if index2piece is not None:
        (p_start, p_end) = index2piece[i_span]
        if p_start < p_end and rand() < self.sp_prob:
            (st_span, end_span) = (p_start, p_end + 1)
        else:
            (st_span, end_span) = (i_span, i_span + 1)
    else:
        rand_skipgram_size = 0
        if self.skipgram_size_geo_list:
            rand_skipgram_size = np.random.choice(len(self.skipgram_size_geo_list), 1, p=self.skipgram_size_geo_list)[0] + 1
        elif add_skipgram and self.skipgram_prb > 0 and (self.skipgram_size >= 2) and (rand() < self.skipgram_prb):
            rand_skipgram_size = min(randint(2, self.skipgram_size), len(span_list) - i_span)
        for n in range(2, rand_skipgram_size + 1):
            (tail_st, tail_end) = span_list[i_span + n - 1]
            if tail_end - tail_st == 1 and tail_st in special_pos:
                break
            if len(masked_pos) + tail_end - cand_st > n_pred:
                break
            n_span = n
        (st_span, end_span) = (i_span, i_span + n_span)
    if self.mask_whole_word:
        (st_span, end_span) = _expand_whole_word(tokens, st_span, end_span)
    if self.word_subsample_prb:
        skip_pos = set()
        if self.pre_whole_word:
            w_span_list = span_list[st_span:end_span]
        else:
            split_idx = _get_word_split_index(tokens, st_span, end_span)
            w_span_list = list(zip(split_idx[:-1], split_idx[1:]))
        for (i, sp) in enumerate(w_span_list):
            (sp_st, sp_end) = sp
            if sp_end - sp_st == 1:
                w_cat = tokens[sp_st]
            else:
                w_cat = ''.join(tokens[sp_st:sp_end])
            if w_cat in self.word_subsample_prb and rand() < self.word_subsample_prb[w_cat]:
                for k in range(sp_st, sp_end):
                    skip_pos.add(k)
        else:
            skip_pos = None
    for sp in range(st_span, end_span):
        for mp in range(span_list[sp][0], span_list[sp][1]):
            if not (skip_pos and mp in skip_pos) and mp not in special_pos and (not (protect_range and protect_range[0] <= mp < protect_range[1])):
                masked_pos.add(mp)
masked_pos = list(masked_pos)
if len(masked_pos) > n_pred:
    masked_pos = masked_pos[:n_pred]","span_list = list(zip(pre_word_split[:-1], pre_word_split[1:]))
cand_pos = [i for (i, sp) in enumerate(span_list) if sp[1] - sp[0] != 1 or (not tokens[sp[0]].endswith('CLS]') and (not tokens[sp[0]].endswith('SEP]')))]
if mask_segment:
    segment_index = next((i for (i, sp) in enumerate(span_list) if sp[1] - sp[0] == 1 and tokens[sp[0]].endswith('SEP]')), None)
    cand_pos = [i for i in cand_pos if i < segment_index and 'a' in mask_segment or (i > segment_index and 'b' in mask_segment)]
masked_pos = set()
for i_span in cand_pos:
    if len(masked_pos) >= n_pred:
        break
    (cand_st, cand_end) = span_list[i_span]
    if len(masked_pos) + cand_end - cand_st > n_pred:
        continue
    if any((p in masked_pos for p in range(cand_st, cand_end))):
        continue
    n_span = 1
    if index2piece is not None:
        (p_start, p_end) = index2piece[i_span]
        if p_start < p_end and rand() < self.sp_prob:
            (st_span, end_span) = (p_start, p_end + 1)
        else:
            (st_span, end_span) = (i_span, i_span + 1)
    else:
        rand_skipgram_size = 0
        if self.skipgram_size_geo_list:
            rand_skipgram_size = np.random.choice(len(self.skipgram_size_geo_list), 1, p=self.skipgram_size_geo_list)[0] + 1
        elif add_skipgram and self.skipgram_prb > 0 and (self.skipgram_size >= 2) and (rand() < self.skipgram_prb):
            rand_skipgram_size = min(randint(2, self.skipgram_size), len(span_list) - i_span)
        for n in range(2, rand_skipgram_size + 1):
            (tail_st, tail_end) = span_list[i_span + n - 1]
            if tail_end - tail_st == 1 and tail_st in special_pos:
                break
            if len(masked_pos) + tail_end - cand_st > n_pred:
                break
            n_span = n
        (st_span, end_span) = (i_span, i_span + n_span)
    if self.mask_whole_word:
        (st_span, end_span) = _expand_whole_word(tokens, st_span, end_span)
    if self.word_subsample_prb:
        skip_pos = set()
        if self.pre_whole_word:
            w_span_list = span_list[st_span:end_span]
        else:
            split_idx = _get_word_split_index(tokens, st_span, end_span)
            w_span_list = list(zip(split_idx[:-1], split_idx[1:]))
        for (i, sp) in enumerate(w_span_list):
            (sp_st, sp_end) = sp
            if sp_end - sp_st == 1:
                w_cat = tokens[sp_st]
            else:
                w_cat = ''.join(tokens[sp_st:sp_end])
            if w_cat in self.word_subsample_prb and rand() < self.word_subsample_prb[w_cat]:
                for k in range(sp_st, sp_end):
                    skip_pos.add(k)
        else:
            skip_pos = None
    for sp in range(st_span, end_span):
        for mp in range(span_list[sp][0], span_list[sp][1]):
            if not (skip_pos and mp in skip_pos) and mp not in special_pos and (not (protect_range and protect_range[0] <= mp < protect_range[1])):
                masked_pos.add(mp)
masked_pos = list(masked_pos)
if len(masked_pos) > n_pred:
    masked_pos = masked_pos[:n_pred]",find_wrong,2,,,
deluge,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deluge/deluge/plugins/Stats/deluge_stats/core.py,https://github.com/deluge-torrent/deluge/tree/master/deluge/plugins/Stats/deluge_stats/core.py,Core,update_stats$122,"def update_stats(self):
    stats = {}
    raw_stats = self.core.get_session_status(self.stat_keys)
    for (name, fn) in self.stat_getters.items():
        stats[name] = fn(raw_stats)
    update_time = time.time()
    self.last_update[1] = update_time
    for (stat, stat_list) in self.stats[1].items():
        if stat in stats:
            stat_list.insert(0, int(stats[stat]))
        else:
            stat_list.insert(0, 0)
        if len(stat_list) > self.length:
            stat_list.pop()

    def update_interval(interval, base, multiplier):
        self.count[interval] = self.count[interval] + 1
        if self.count[interval] >= interval:
            self.last_update[interval] = update_time
            self.count[interval] = 0
            current_stats = self.stats[interval]
            for (stat, stat_list) in self.stats[base].items():
                try:
                    avg = mean(stat_list[0:multiplier])
                except ValueError:
                    avg = 0
                current_stats[stat].insert(0, avg)
                if len(current_stats[stat]) > self.length:
                    current_stats[stat].pop()
    update_interval(5, 1, 5)
    update_interval(30, 5, 6)
    update_interval(300, 30, 10)","raw_stats = self.core.get_session_status(self.stat_keys)
for (name, fn) in self.stat_getters.items():
    stats[name] = fn(raw_stats)","stats = {name: fn(self.core.get_session_status(self.stat_keys)) for (name, fn) in self.stat_getters.items()}",find_wrong,-1,,,
readthedocs.org,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/readthedocs.org/readthedocs/projects/models.py,https://github.com/readthedocs/readthedocs.org/tree/master/readthedocs/projects/models.py,Project,get_downloads$776,"def get_downloads(self):
    downloads = {}
    default_version = self.get_default_version()
    for type_ in ('htmlzip', 'epub', 'pdf'):
        downloads[type_] = self.get_production_media_url(type_, default_version)
    return downloads","default_version = self.get_default_version()
for type_ in ('htmlzip', 'epub', 'pdf'):
    downloads[type_] = self.get_production_media_url(type_, default_version)","downloads = {type_: self.get_production_media_url(type_, default_version) for type_ in ('htmlzip', 'epub', 'pdf')}",find_wrong,-1,,,
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/swift/common/ring/composite_builder.py,https://github.com/openstack/swift/tree/master/swift/common/ring/composite_builder.py,,pre_validate_all_builders$109,"def pre_validate_all_builders(builders):
    """"""
    Pre-validation for all component ring builders that are to be included in
    the composite ring. Checks that all component rings are valid with respect
    to each other.

    :param builders: a list of :class:`swift.common.ring.builder.RingBuilder`
        instances
    :raises ValueError: if the builders are invalid with respect to each other
    """"""
    if len(builders) < 2:
        raise ValueError('Two or more component builders are required.')
    for attr in MUST_MATCH_ATTRS:
        attr_dict = defaultdict(list)
        for (i, builder) in enumerate(builders):
            value = getattr(builder, attr, None)
            attr_dict[value].append(i)
        if len(attr_dict) > 1:
            variations = ['%s=%s found at indexes %s' % (attr, val, indexes) for (val, indexes) in attr_dict.items()]
            raise ValueError('All builders must have same value for %r.\n%s' % (attr, '\n  '.join(variations)))
    errors = []
    for (index, builder) in enumerate(builders):
        if int(builder.replicas) != builder.replicas:
            errors.append('Non integer replica count %s found at index %s' % (builder.replicas, index))
        if builder.devs_changed:
            errors.append('Builder needs rebalance to apply changes at index %s' % index)
    if errors:
        raise ValueError('Problem with builders.\n%s' % '\n  '.join(errors))
    regions_info = {}
    for builder in builders:
        regions_info[builder] = set((dev['region'] for dev in builder._iter_devs()))
    for (first_region_set, second_region_set) in combinations(regions_info.values(), 2):
        inter = first_region_set & second_region_set
        if inter:
            raise ValueError('Same region found in different rings')
    check_for_dev_uniqueness(builders)","for (i, builder) in enumerate(builders):
    value = getattr(builder, attr, None)
    attr_dict[value].append(i)","attr_dict = {value: [i for (i, builder) in enumerate(builders) if getattr(builder, attr, None) == value] for value in set((getattr(builder, attr, None) for builder in builders))}",find_wrong,2,,,
CenterPoint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CenterPoint/det3d/utils/utils.py,https://github.com/tianweiy/CenterPoint/tree/master/det3d/utils/utils.py,,example_to_device$5,"def example_to_device(example, dtype=torch.float32, device=None, non_blocking=True) -> dict:
    device = device or torch.device('cuda:0')
    example_torch = {}
    float_names = ['voxels', 'bev_map']
    for (k, v) in example.items():
        if k in ['anchors', 'reg_targets', 'reg_weights', 'labels', 'anchors_mask']:
            res = []
            for (kk, vv) in v.items():
                vv = [vvv.unsqueeze_(0) for vvv in vv]
                res.append(torch.cat(vv, dim=0).cuda(device, non_blocking=non_blocking))
            example_torch[k] = res
        elif k in ['voxels', 'bev_map', 'coordinates', 'num_points', 'points', 'num_voxels']:
            example_torch[k] = v.cuda(device, non_blocking=non_blocking)
        elif k == 'calib':
            calib = {}
            for (k1, v1) in v.items():
                calib[k1] = v1.cuda(device, non_blocking=non_blocking)
            example_torch[k] = calib
        else:
            example_torch[k] = v
    return example_torch","float_names = ['voxels', 'bev_map']
for (k, v) in example.items():
    if k in ['anchors', 'reg_targets', 'reg_weights', 'labels', 'anchors_mask']:
        res = []
        for (kk, vv) in v.items():
            vv = [vvv.unsqueeze_(0) for vvv in vv]
            res.append(torch.cat(vv, dim=0).cuda(device, non_blocking=non_blocking))
        example_torch[k] = res
    elif k in ['voxels', 'bev_map', 'coordinates', 'num_points', 'points', 'num_voxels']:
        example_torch[k] = v.cuda(device, non_blocking=non_blocking)
    elif k == 'calib':
        calib = {}
        for (k1, v1) in v.items():
            calib[k1] = v1.cuda(device, non_blocking=non_blocking)
        example_torch[k] = calib
    else:
        example_torch[k] = v","example_torch = {k: [torch.cat([vvv.unsqueeze_(0) for vvv in vv], dim=0).cuda(device, non_blocking=non_blocking) for (kk, vv) in v.items()] if k in ['anchors', 'reg_targets', 'reg_weights', 'labels', 'anchors_mask'] else v.cuda(device, non_blocking=non_blocking) if k in ['voxels', 'bev_map', 'coordinates', 'num_points', 'points', 'num_voxels'] else {k1: v1.cuda(device, non_blocking=non_blocking) for (k1, v1) in v.items()} if k == 'calib' else v for (k, v) in example.items()}",find_wrong,1,,,
OpenSfM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenSfM/opensfm/stats.py,https://github.com/mapillary/OpenSfM/tree/master/opensfm/stats.py,,save_residual_grids$858,"def save_residual_grids(data: DataSetBase, tracks_manager: pymap.TracksManager, reconstructions: List[types.Reconstruction], output_path: str, io_handler: io.IoFilesystemBase) -> None:
    all_errors = {}
    scaling = 4
    for rec in reconstructions:
        for camera_id in rec.cameras:
            all_errors[camera_id] = []
    for i in range(len(reconstructions)):
        valid_observations = _get_valid_observations(reconstructions, tracks_manager)(i)
        errors_scaled = _compute_errors(reconstructions, tracks_manager)(i, pymap.ErrorType.Normalized)
        errors_unscaled = _compute_errors(reconstructions, tracks_manager)(i, pymap.ErrorType.Pixel)
        for (shot_id, shot_errors) in errors_scaled.items():
            shot = reconstructions[i].get_shot(shot_id)
            w = shot.camera.width
            h = shot.camera.height
            center = np.array([w / 2.0, h / 2.0])
            normalizer = max(shot.camera.width, shot.camera.height)
            (buckets_x, buckets_y) = _grid_buckets(shot.camera)
            w_bucket = buckets_x / w
            h_bucket = buckets_y / h
            shots_errors = []
            for (error_scaled, error_unscaled, observation) in zip(shot_errors.values(), errors_unscaled[shot_id].values(), valid_observations[shot_id].values()):
                if _norm2d(error_unscaled * normalizer) > RESIDUAL_PIXEL_CUTOFF:
                    continue
                bucket = observation.point * normalizer + center
                x = max([0, min([int(bucket[0] * w_bucket), buckets_x - 1])])
                y = max([0, min([int(bucket[1] * h_bucket), buckets_y - 1])])
                shots_errors.append((x, y, error_scaled))
            all_errors[shot.camera.id] += shots_errors
    for (camera_id, errors) in all_errors.items():
        if not errors:
            continue
        (buckets_x, buckets_y) = _grid_buckets(rec.cameras[camera_id])
        camera_array_res = np.zeros((buckets_y, buckets_x, 2))
        camera_array_count = np.full((buckets_y, buckets_x, 1), 1)
        for (x, y, e) in errors:
            camera_array_res[y, x] += e
            camera_array_count[y, x, 0] += 1
        camera_array_res = np.divide(camera_array_res, camera_array_count)
        camera = rec.get_camera(camera_id)
        (w, h) = (camera.width, camera.height)
        normalizer = max(w, h)
        clamp = 0.1
        res_colors = np.linalg.norm(camera_array_res[:, :, :2], axis=2)
        lowest = np.percentile(res_colors, 0)
        highest = np.percentile(res_colors, 100 * (1 - clamp))
        np.clip(res_colors, lowest, highest, res_colors)
        res_colors /= highest - lowest
        plt.clf()
        plt.figure(figsize=(12, 10))
        Q = plt.quiver(camera_array_res[:, :, 0] * scaling, camera_array_res[:, :, 1] * scaling, res_colors, units='xy', angles='xy', scale_units='xy', scale=1, width=0.1, cmap='viridis_r')
        scale = highest - lowest
        plt.quiverkey(Q, X=0.1, Y=1.04, U=scale * scaling, label=f'Residual grid scale : {scale:.2f}', labelpos='E')
        plt.title('                      ', fontsize='large')
        norm = colors.Normalize(vmin=lowest, vmax=highest)
        cmap = cm.get_cmap('viridis_r')
        sm = cm.ScalarMappable(norm=norm, cmap=cmap)
        sm.set_array([])
        plt.colorbar(mappable=sm, orientation='horizontal', label='Residual Norm', pad=0.08, aspect=40)
        plt.xticks([0, buckets_x / 2, buckets_x], [0, int(w / 2), w], fontsize='x-small')
        plt.yticks([0, buckets_y / 2, buckets_y], [0, int(h / 2), h], fontsize='x-small')
        with io_handler.open(os.path.join(output_path, 'residuals_' + str(camera_id.replace('/', '_')) + '.png'), 'wb') as fwb:
            plt.savefig(fwb, dpi=300, bbox_inches='tight')","scaling = 4
for rec in reconstructions:
    for camera_id in rec.cameras:
        all_errors[camera_id] = []","all_errors = {camera_id: [] for rec in reconstructions for camera_id in rec.cameras}

Note: There is no way to refactor the rest of the code with dictionary comprehension.",find_wrong,-1,,,
autokeras,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/autokeras/autokeras/analysers/input_analysers.py,https://github.com/keras-team/autokeras/tree/master/autokeras/analysers/input_analysers.py,StructuredDataAnalyser,infer_column_types$137,"def infer_column_types(self):
    column_types = {}
    for i in range(self.num_col):
        if self.count_categorical[i] > 0:
            column_types[self.column_names[i]] = CATEGORICAL
        elif len(self.count_unique_numerical[i]) / self.count_numerical[i] < 0.05:
            column_types[self.column_names[i]] = CATEGORICAL
        else:
            column_types[self.column_names[i]] = NUMERICAL
    if self.column_types is None:
        self.column_types = {}
    for (key, value) in column_types.items():
        if key not in self.column_types:
            self.column_types[key] = value","for i in range(self.num_col):
    if self.count_categorical[i] > 0:
        column_types[self.column_names[i]] = CATEGORICAL
    elif len(self.count_unique_numerical[i]) / self.count_numerical[i] < 0.05:
        column_types[self.column_names[i]] = CATEGORICAL
    else:
        column_types[self.column_names[i]] = NUMERICAL
if self.column_types is None:
    self.column_types = {}
for (key, value) in column_types.items():
    if key not in self.column_types:
        self.column_types[key] = value","column_types = {self.column_names[i]: CATEGORICAL if self.count_categorical[i] > 0 or len(self.count_unique_numerical[i]) / self.count_numerical[i] < 0.05 else NUMERICAL for i in range(self.num_col)}
if self.column_types is None:
    self.column_types = {}
self.column_types.update({key: value for (key, value) in column_types.items() if key not in self.column_types})",find_wrong,1,,,
sqlalchemy-mixins,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sqlalchemy-mixins/sqlalchemy_mixins/serialize.py,https://github.com/absent1706/sqlalchemy-mixins/tree/master/sqlalchemy_mixins/serialize.py,SerializeMixin,to_dict$11,"def to_dict(self, nested=False, hybrid_attributes=False, exclude=None):
    """"""Return dict object with model's data.

        :param nested: flag to return nested relationships' data if true
        :type: bool
        :param hybrid_attributes: flag to include hybrid attributes if true
        :type: bool
        :return: dict
        """"""
    result = dict()
    if exclude is None:
        view_cols = self.columns
    else:
        view_cols = filter(lambda e: e not in exclude, self.columns)
    for key in view_cols:
        result[key] = getattr(self, key)
    if hybrid_attributes:
        for key in self.hybrid_properties:
            result[key] = getattr(self, key)
    if nested:
        for key in self.relations:
            obj = getattr(self, key)
            if isinstance(obj, SerializeMixin):
                result[key] = obj.to_dict(hybrid_attributes=hybrid_attributes)
            elif isinstance(obj, Iterable):
                result[key] = [o.to_dict(hybrid_attributes=hybrid_attributes) for o in obj if isinstance(o, SerializeMixin)]
    return result","if exclude is None:
    view_cols = self.columns
else:
    view_cols = filter(lambda e: e not in exclude, self.columns)
for key in view_cols:
    result[key] = getattr(self, key)","view_cols = self.columns if exclude is None else filter(lambda e: e not in exclude, self.columns)
result = {key: getattr(self, key) for key in view_cols}",find_wrong,1,,,
BigGAN-PyTorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BigGAN-PyTorch/TFHub/converter.py,https://github.com/ajbrock/BigGAN-PyTorch/tree/master/TFHub/converter.py,,convert_from_v1$210,"def convert_from_v1(hub_dict, resolution=128):
    weightname_dict = {'weight_u': 'u0', 'weight_bar': 'weight', 'bias': 'bias'}
    convnum_dict = {'conv0': 'conv1', 'conv1': 'conv2', 'conv_sc': 'conv_sc'}
    attention_blocknum = {128: 3, 256: 4, 512: 3}[resolution]
    hub2me = {'linear.weight': 'shared.weight', 'G_linear.module.weight_bar': 'linear.weight', 'G_linear.module.bias': 'linear.bias', 'G_linear.module.weight_u': 'linear.u0', 'ScaledCrossReplicaBN.weight': 'output_layer.0.gain', 'ScaledCrossReplicaBN.bias': 'output_layer.0.bias', 'ScaledCrossReplicaBN.running_mean': 'output_layer.0.stored_mean', 'ScaledCrossReplicaBN.running_var': 'output_layer.0.stored_var', 'colorize.module.weight_bar': 'output_layer.2.weight', 'colorize.module.bias': 'output_layer.2.bias', 'colorize.module.weight_u': 'output_layer.2.u0', 'attention.gamma': 'blocks.%d.1.gamma' % attention_blocknum, 'attention.theta.module.weight_u': 'blocks.%d.1.theta.u0' % attention_blocknum, 'attention.theta.module.weight_bar': 'blocks.%d.1.theta.weight' % attention_blocknum, 'attention.phi.module.weight_u': 'blocks.%d.1.phi.u0' % attention_blocknum, 'attention.phi.module.weight_bar': 'blocks.%d.1.phi.weight' % attention_blocknum, 'attention.g.module.weight_u': 'blocks.%d.1.g.u0' % attention_blocknum, 'attention.g.module.weight_bar': 'blocks.%d.1.g.weight' % attention_blocknum, 'attention.o_conv.module.weight_u': 'blocks.%d.1.o.u0' % attention_blocknum, 'attention.o_conv.module.weight_bar': 'blocks.%d.1.o.weight' % attention_blocknum}
    for name in hub_dict.keys():
        if 'GBlock' in name:
            if 'HyperBN' not in name:
                out = parse.parse('GBlock.{:d}.{}.module.{}', name)
                (blocknum, convnum, weightname) = out
                if weightname not in weightname_dict:
                    continue
                out_name = 'blocks.%d.0.%s.%s' % (blocknum, convnum_dict[convnum], weightname_dict[weightname])
            else:
                BNnum = 2 if 'HyperBN_1' in name else 1
                if 'embed' in name:
                    out = parse.parse('GBlock.{:d}.{}.module.{}', name)
                    (blocknum, gamma_or_beta, weightname) = out
                    if weightname not in weightname_dict:
                        continue
                    out_name = 'blocks.%d.0.bn%d.%s.%s' % (blocknum, BNnum, 'gain' if 'gamma' in gamma_or_beta else 'bias', weightname_dict[weightname])
                else:
                    out = parse.parse('GBlock.{:d}.{}.bn.{}', name)
                    (blocknum, dummy, mean_or_var) = out
                    if 'num_batches_tracked' in mean_or_var:
                        continue
                    out_name = 'blocks.%d.0.bn%d.%s' % (blocknum, BNnum, 'stored_mean' if 'mean' in mean_or_var else 'stored_var')
            hub2me[name] = out_name
    me2hub = {hub2me[item]: item for item in hub2me}
    new_dict = {}
    dimz_dict = {128: 20, 256: 20, 512: 16}
    for item in me2hub:
        if ('bn' in item and 'weight' in item) and ('gain' in item or 'bias' in item) and ('output_layer' not in item):
            new_dict[item] = torch.cat([hub_dict[me2hub[item]][:, -128:], hub_dict[me2hub[item]][:, :dimz_dict[resolution]]], 1)
        elif item == 'linear.weight':
            new_dict[item] = hub_dict[me2hub[item]].contiguous().view(4, 4, 96 * 16, -1).permute(2, 0, 1, 3).contiguous().view(-1, dimz_dict[resolution])
        elif item == 'linear.bias':
            new_dict[item] = hub_dict[me2hub[item]].view(4, 4, 96 * 16).permute(2, 0, 1).contiguous().view(-1)
        elif item == 'linear.u0':
            new_dict[item] = hub_dict[me2hub[item]].view(4, 4, 96 * 16).permute(2, 0, 1).contiguous().view(1, -1)
        elif me2hub[item] == 'linear.weight':
            new_dict[item] = hub_dict[me2hub[item]].t()
        elif 'weight_u' in me2hub[item]:
            new_dict[item] = hub_dict[me2hub[item]].unsqueeze(0)
        else:
            new_dict[item] = hub_dict[me2hub[item]]
    return new_dict","for item in me2hub:
    if ('bn' in item and 'weight' in item) and ('gain' in item or 'bias' in item) and ('output_layer' not in item):
        new_dict[item] = torch.cat([hub_dict[me2hub[item]][:, -128:], hub_dict[me2hub[item]][:, :dimz_dict[resolution]]], 1)","dimz_dict = {128: 20, 256: 20, 512: 16}
new_dict = {item: torch.cat([hub_dict[me2hub[item]][:, -128:], hub_dict[me2hub[item]][:, :dimz_dict[resolution]]], 1) if ('bn' in item and 'weight' in item) and ('gain' in item or 'bias' in item) and ('output_layer' not in item) else hub_dict[me2hub[item]] for item in me2hub}",find_wrong,-1,,,
second.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/second.pytorch/second/data/kitti_common.py,https://github.com/traveller59/second.pytorch/tree/master/second/data/kitti_common.py,,remove_dontcare$331,"def remove_dontcare(image_anno):
    img_filtered_annotations = {}
    relevant_annotation_indices = [i for (i, x) in enumerate(image_anno['name']) if x != 'DontCare']
    for key in image_anno.keys():
        img_filtered_annotations[key] = image_anno[key][relevant_annotation_indices]
    return img_filtered_annotations","relevant_annotation_indices = [i for (i, x) in enumerate(image_anno['name']) if x != 'DontCare']
for key in image_anno.keys():
    img_filtered_annotations[key] = image_anno[key][relevant_annotation_indices]","relevant_annotation_indices = [i for (i, x) in enumerate(image_anno['name']) if x != 'DontCare']
img_filtered_annotations = {key: image_anno[key][relevant_annotation_indices] for key in image_anno.keys()}",find_wrong,1,,,
sweetviz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sweetviz/sweetviz/sv_html.py,https://github.com/fbdesignpro/sweetviz/tree/master/sweetviz/sv_html.py,,load_layout_globals_from_config$29,"def load_layout_globals_from_config():
    jinja2_env.globals['FeatureType'] = FeatureType
    layout_globals = dict()
    general_globals = dict()
    for element in config['Layout']:
        layout_globals[element] = config['Layout'].getint(element)
    general_globals['use_cjk_font'] = config['General'].getint('use_cjk_font')
    general_globals['association_min_to_bold'] = config['General'].getfloat('association_min_to_bold')
    jinja2_env.globals['layout'] = layout_globals
    jinja2_env.globals['general'] = general_globals","for element in config['Layout']:
    layout_globals[element] = config['Layout'].getint(element)
general_globals = dict()
general_globals['use_cjk_font'] = config['General'].getint('use_cjk_font')
general_globals['association_min_to_bold'] = config['General'].getfloat('association_min_to_bold')","layout_globals = {element: config['Layout'].getint(element) for element in config['Layout']}
general_globals = {'use_cjk_font': config['General'].getint('use_cjk_font'), 'association_min_to_bold': config['General'].getfloat('association_min_to_bold')}",find_wrong,1,,,
data-science-competition,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-science-competition/澶╂睜/楂樺痉鍦板浘/mmdetectionForGaode/get_features.py,https://github.com/DLLXW/data-science-competition/tree/master/澶╂睜/楂樺痉鍦板浘/mmdetectionForGaode/get_features.py,,if_main_my$257,"if __name__ == '__main__':
    begin_time = time.time()
    features_name = ['name', 'car_cnt', 'car_scaleSum', 'car_scaleMean', 'car_scaleMax', 'car_xSum', 'car_xMean', 'car_xMax', 'car_xMin', 'car_ySum', 'car_yMean', 'car_yMax', 'car_yMin', 'car_disSum', 'car_dis', 'x_disSum', 'x_disMean', 'x_disMin', 'x_disMax', 'y_disSum', 'y_disMean', 'y_disMin', 'y_disMax', 'roi_car_cnt', 'roi_car_sizeSum', 'roi_car_size', 'roi_car_sizeMax', 'roi_car_sizeMin', 'roi_car_sizeStd', 'roi_car_cnt1', 'roi_car_sizeSum1', 'roi_car_size1', 'roi_car_sizeMax1', 'roi_car_sizeMin1', 'roi_car_sizeStd1', 'roi_car_cnt2', 'roi_car_sizeSum2', 'roi_car_size2', 'roi_car_sizeMax2', 'roi_car_sizeMin2', 'roi_car_sizeStd2', 'roi_car_cnt3', 'roi_car_sizeSum3', 'roi_car_size3', 'roi_car_sizeMax3', 'roi_car_sizeMin3', 'roi_car_sizeStd3', 'roi_car_cnt4', 'roi_car_sizeSum4', 'roi_car_size4', 'roi_car_sizeMax4', 'roi_car_sizeMin4', 'roi_car_sizeStd4']
    features_dic = {}
    fill_null = 0
    for fea in features_name:
        features_dic[fea] = []
    train_image_dir = '/home/admins/qyl/gaode_classify/dataset/amap_traffic_final_train_data'
    train_json_dir = '/home/admins/qyl/gaode_classify/dataset/amap_traffic_final_train_0906.json'
    config = 'configs_raw/cascade_rcnn/cascade_rcnn_r50_fpn_1x_coco.py'
    checkpoint = 'checkpoints/cascade_rcnn_r50_fpn_1x_coco_20200316-3dc56deb.pth'
    model = init_detector(config, checkpoint, device=0)
    img_paths = sorted(glob.glob(train_image_dir + '/*/*'))
    print('data size:', len(img_paths))
    cnt_programe = 0
    for img_path in img_paths:
        cnt_programe += 1
        print(img_path, ' ', cnt_programe)
        seq = img_path.split('/')[-2]
        frame = img_path.split('/')[-1]
        get_frt(img_path, img_name=seq + '_' + frame)
    print(pd.DataFrame(features_dic))
    df = pd.DataFrame(features_dic)
    df.to_csv('train_res2net.csv', index=False)
    print('spend time {}s'.format(time.time() - begin_time))","fill_null = 0
for fea in features_name:
    features_dic[fea] = []",features_dic = {fea: [] for fea in features_name},find_wrong,-1,,,
avalanche,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/avalanche/avalanche/evaluation/metrics/forgetting_bwt.py,https://github.com/ContinualAI/avalanche/tree/master/avalanche/evaluation/metrics/forgetting_bwt.py,Forgetting,result$67,"def result(self, k=None) -> Union[float, None, Dict[int, float]]:
    """"""
        Forgetting is returned only for keys encountered twice.

        :param k: the key for which returning forgetting. If k has not
            updated at least twice it returns None. If k is None,
            forgetting will be returned for all keys encountered at least
            twice.

        :return: the difference between the first and last value encountered
            for k, if k is not None. It returns None if k has not been updated
            at least twice. If k is None, returns a dictionary
            containing keys whose value has been updated at least twice. The
            associated value is the difference between the first and last
            value recorded for that key.
        """"""
    forgetting = {}
    if k is not None:
        if k in self.initial and k in self.last:
            return self.initial[k] - self.last[k]
        else:
            return None
    ik = set(self.initial.keys())
    both_keys = list(ik.intersection(set(self.last.keys())))
    for k in both_keys:
        forgetting[k] = self.initial[k] - self.last[k]
    return forgetting","ik = set(self.initial.keys())
both_keys = list(ik.intersection(set(self.last.keys())))
for k in both_keys:
    forgetting[k] = self.initial[k] - self.last[k]",forgetting = {k: self.initial[k] - self.last[k] for k in set(self.initial.keys()).intersection(set(self.last.keys()))},find_wrong,1,,,
mmpose,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmpose/mmpose/core/fp16/decorators.py,https://github.com/open-mmlab/mmpose/tree/master/mmpose/core/fp16/decorators.py,,new_func$51,"def new_func(*args, **kwargs):
    if not isinstance(args[0], torch.nn.Module):
        raise TypeError('@auto_fp16 can only be used to decorate the method of nn.Module')
    if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
        return old_func(*args, **kwargs)
    args_info = getfullargspec(old_func)
    args_to_cast = args_info.args if apply_to is None else apply_to
    new_args = []
    if args:
        arg_names = args_info.args[:len(args)]
        for (i, arg_name) in enumerate(arg_names):
            if arg_name in args_to_cast:
                new_args.append(cast_tensor_type(args[i], torch.float, torch.half))
            else:
                new_args.append(args[i])
    new_kwargs = {}
    if kwargs:
        for (arg_name, arg_value) in kwargs.items():
            if arg_name in args_to_cast:
                new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
            else:
                new_kwargs[arg_name] = arg_value
    output = old_func(*new_args, **new_kwargs)
    if out_fp32:
        output = cast_tensor_type(output, torch.half, torch.float)
    return output","if kwargs:
    for (arg_name, arg_value) in kwargs.items():
        if arg_name in args_to_cast:
            new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
        else:
            new_kwargs[arg_name] = arg_value","new_kwargs = {arg_name: cast_tensor_type(arg_value, torch.float, torch.half) if arg_name in args_to_cast else arg_value for (arg_name, arg_value) in kwargs.items()}",find_wrong,-1,,,
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/stack_pointer_tracker.py,https://github.com/angr/angr/tree/master/angr/analyses/stack_pointer_tracker.py,,_dict_merge$248,"def _dict_merge(d1, d2):
    all_keys = set(d1.keys()) | set(d2.keys())
    merged = {}
    for k in all_keys:
        if k not in d1 or d1[k] is TOP:
            merged[k] = TOP
        elif k not in d2 or d2[k] is TOP:
            merged[k] = TOP
        elif d1[k] is BOTTOM:
            merged[k] = d2[k]
        elif d2[k] is BOTTOM:
            merged[k] = d1[k]
        elif d1[k] == d2[k]:
            merged[k] = d1[k]
        else:
            merged[k] = TOP
    return merged","merged = {}
for k in all_keys:
    if k not in d1 or d1[k] is TOP:
        merged[k] = TOP
    elif k not in d2 or d2[k] is TOP:
        merged[k] = TOP
    elif d1[k] is BOTTOM:
        merged[k] = d2[k]
    elif d2[k] is BOTTOM:
        merged[k] = d1[k]
    elif d1[k] == d2[k]:
        merged[k] = d1[k]
    else:
        merged[k] = TOP","all_keys = set(d1.keys()) | set(d2.keys())
merged = {k: TOP if k not in d1 or d1[k] is TOP else TOP if k not in d2 or d2[k] is TOP else d2[k] if d1[k] is BOTTOM else d1[k] if d2[k] is BOTTOM else d1[k] if d1[k] == d2[k] else TOP for k in all_keys}",find_wrong,-1,,,
coveragepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coveragepy/coverage/config.py,https://github.com/nedbat/coveragepy/tree/master/coverage/config.py,HandyConfigParser,get_section$64,"def get_section(self, section):
    """"""Get the contents of a section, as a dictionary.""""""
    d = {}
    for opt in self.options(section):
        d[opt] = self.get(section, opt)
    return d","for opt in self.options(section):
    d[opt] = self.get(section, opt)
return d","return {opt: self.get(section, opt) for opt in self.options(section)}",find_wrong,1,,,
shiv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shiv/src/shiv/cli.py,https://github.com/linkedin/shiv/tree/master/src/shiv/cli.py,,main$160,"def main(output_file: str, entry_point: Optional[str], console_script: Optional[str], python: Optional[str], site_packages: Optional[str], build_id: Optional[str], compressed: bool, compile_pyc: bool, extend_pythonpath: bool, reproducible: bool, no_modify: bool, preamble: Optional[str], root: Optional[str], pip_args: List[str]) -> None:
    """"""
    Shiv is a command line utility for building fully self-contained Python zipapps
    as outlined in PEP 441, but with all their dependencies included!
    """"""
    if not pip_args and (not site_packages):
        sys.exit(NO_PIP_ARGS_OR_SITE_PACKAGES)
    if output_file is None:
        sys.exit(NO_OUTFILE)
    for disallowed in DISALLOWED_ARGS:
        for supplied_arg in pip_args:
            if supplied_arg in disallowed:
                sys.exit(DISALLOWED_PIP_ARGS.format(arg=supplied_arg, reason=DISALLOWED_ARGS[disallowed]))
    if build_id is not None:
        click.secho('Warning! You have overridden the default build-id behavior, executables created by shiv must have unique build IDs or unexpected behavior could occur.', fg='yellow')
    sources: List[Path] = []
    with TemporaryDirectory() as tmp_site_packages:
        if site_packages:
            if pip_args:
                for sp in site_packages:
                    copytree(Path(sp), Path(tmp_site_packages))
            else:
                sources.extend([Path(p).expanduser() for p in site_packages])
        if pip_args:
            pip.install(['--target', tmp_site_packages] + list(pip_args))
        if preamble:
            bin_dir = Path(tmp_site_packages, 'bin')
            bin_dir.mkdir(exist_ok=True)
            shutil.copy(Path(preamble).absolute(), bin_dir / Path(preamble).name)
        sources.append(Path(tmp_site_packages).absolute())
        if no_modify:
            hashes = {}
            for source in sources:
                for path in source.rglob('**/*.py'):
                    hashes[str(path.relative_to(source))] = hashlib.sha256(path.read_bytes()).hexdigest()
        if entry_point is None and console_script is not None:
            try:
                entry_point = find_entry_point(sources, console_script)
            except KeyError:
                if not console_script_exists(sources, console_script):
                    sys.exit(NO_ENTRY_POINT.format(entry_point=console_script))
            else:
                console_script = None
        timestamp = int(os.environ.get(SOURCE_DATE_EPOCH_ENV, SOURCE_DATE_EPOCH_DEFAULT if reproducible else time.time()))
        env = Environment(built_at=datetime.utcfromtimestamp(timestamp).strftime(BUILD_AT_TIMESTAMP_FORMAT), build_id=build_id, entry_point=entry_point, script=console_script, compile_pyc=compile_pyc, extend_pythonpath=extend_pythonpath, shiv_version=__version__, no_modify=no_modify, reproducible=reproducible, preamble=Path(preamble).name if preamble else None, root=root)
        if no_modify:
            env.hashes = hashes
        builder.create_archive(sources, target=Path(output_file).expanduser(), interpreter=python or DEFAULT_SHEBANG, main='_bootstrap:bootstrap', env=env, compressed=compressed)","for sp in site_packages:
    copytree(Path(sp), Path(tmp_site_packages))
sources.extend([Path(p).expanduser() for p in site_packages])","sources = [Path(p).expanduser() for p in site_packages]
if site_packages and pip_args:
    for sp in site_packages:
        copytree(Path(sp), Path(tmp_site_packages))
        sources.append(Path(tmp_site_packages).absolute())",find_wrong,2,,,
wharfee,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wharfee/wharfee/formatter.py,https://github.com/j-bennet/wharfee/tree/master/wharfee/formatter.py,,filter_dict$411,"def filter_dict(data, display_keys):
    """"""
    Strip out some of the dictionary fields.
    :param display_keys: set
    :param data: dict
    :return: dict
    """"""
    if data and isinstance(data, list) and isinstance(data[0], dict):
        result = []
        for item in data:
            filtered = {}
            for (k, v) in item.items():
                if k.lower() in display_keys:
                    filtered[k] = v
            result.append(filtered)
        return result
    return data","for item in data:
    filtered = {}
    for (k, v) in item.items():
        if k.lower() in display_keys:
            filtered[k] = v
    result.append(filtered)","result = [{k: v for (k, v) in item.items() if k.lower() in display_keys} for item in data]",find_wrong,1,,,
pygame_tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygame_tutorials/tilemap/part 21/main.py,https://github.com/kidscancode/pygame_tutorials/tree/master/tilemap/part 21/main.py,Game,load_data$64,"def load_data(self):
    game_folder = path.dirname(__file__)
    img_folder = path.join(game_folder, 'img')
    snd_folder = path.join(game_folder, 'snd')
    music_folder = path.join(game_folder, 'music')
    map_folder = path.join(game_folder, 'maps')
    self.title_font = path.join(img_folder, 'ZOMBIE.TTF')
    self.dim_screen = pg.Surface(self.screen.get_size()).convert_alpha()
    self.dim_screen.fill((0, 0, 0, 180))
    self.map = TiledMap(path.join(map_folder, 'level1.tmx'))
    self.map_img = self.map.make_map()
    self.map.rect = self.map_img.get_rect()
    self.player_img = pg.image.load(path.join(img_folder, PLAYER_IMG)).convert_alpha()
    self.bullet_images = {}
    self.bullet_images['lg'] = pg.image.load(path.join(img_folder, BULLET_IMG)).convert_alpha()
    self.bullet_images['sm'] = pg.transform.scale(self.bullet_images['lg'], (10, 10))
    self.mob_img = pg.image.load(path.join(img_folder, MOB_IMG)).convert_alpha()
    self.splat = pg.image.load(path.join(img_folder, SPLAT)).convert_alpha()
    self.splat = pg.transform.scale(self.splat, (64, 64))
    self.gun_flashes = []
    for img in MUZZLE_FLASHES:
        self.gun_flashes.append(pg.image.load(path.join(img_folder, img)).convert_alpha())
    self.item_images = {}
    for item in ITEM_IMAGES:
        self.item_images[item] = pg.image.load(path.join(img_folder, ITEM_IMAGES[item])).convert_alpha()
    pg.mixer.music.load(path.join(music_folder, BG_MUSIC))
    self.effects_sounds = {}
    for type in EFFECTS_SOUNDS:
        self.effects_sounds[type] = pg.mixer.Sound(path.join(snd_folder, EFFECTS_SOUNDS[type]))
    self.weapon_sounds = {}
    for weapon in WEAPON_SOUNDS:
        self.weapon_sounds[weapon] = []
        for snd in WEAPON_SOUNDS[weapon]:
            s = pg.mixer.Sound(path.join(snd_folder, snd))
            s.set_volume(0.3)
            self.weapon_sounds[weapon].append(s)
    self.zombie_moan_sounds = []
    for snd in ZOMBIE_MOAN_SOUNDS:
        s = pg.mixer.Sound(path.join(snd_folder, snd))
        s.set_volume(0.2)
        self.zombie_moan_sounds.append(s)
    self.player_hit_sounds = []
    for snd in PLAYER_HIT_SOUNDS:
        self.player_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))
    self.zombie_hit_sounds = []
    for snd in ZOMBIE_HIT_SOUNDS:
        self.zombie_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))","self.bullet_images['lg'] = pg.image.load(path.join(img_folder, BULLET_IMG)).convert_alpha()
self.bullet_images['sm'] = pg.transform.scale(self.bullet_images['lg'], (10, 10))","self.bullet_images = {'lg': pg.image.load(path.join(img_folder, BULLET_IMG)).convert_alpha(), 'sm': pg.transform.scale(self.bullet_images['lg'], (10, 10))}",find_wrong,2,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/win_status.py,https://github.com/saltstack/salt/tree/master/salt/modules/win_status.py,,meminfo$198,"def meminfo():
    """"""
    Return information about physical and virtual memory on the system

    Returns:
        dict: A dictionary of information about memory on the system

    CLI Example:

    .. code-block:: bash

        salt * status.meminfo
    """"""
    (vm_total, vm_available, vm_percent, vm_used, vm_free) = psutil.virtual_memory()
    (swp_total, swp_used, swp_free, swp_percent, _, _) = psutil.swap_memory()

    def get_unit_value(memory):
        symbols = ('K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')
        prefix = {}
        for (i, s) in enumerate(symbols):
            prefix[s] = 1 << (i + 1) * 10
        for s in reversed(symbols):
            if memory >= prefix[s]:
                value = float(memory) / prefix[s]
                return {'unit': s, 'value': value}
        return {'unit': 'B', 'value': memory}
    return {'VmallocTotal': get_unit_value(vm_total), 'VmallocUsed': get_unit_value(vm_used), 'VmallocFree': get_unit_value(vm_free), 'VmallocAvail': get_unit_value(vm_available), 'SwapTotal': get_unit_value(swp_total), 'SwapUsed': get_unit_value(swp_used), 'SwapFree': get_unit_value(swp_free)}","""VmallocTotal"": get_unit_value(vm_total),
        ""VmallocUsed"": get_unit_value(vm_used),
        ""VmallocFree"": get_unit_value(vm_free),
        ""VmallocAvail"": get_unit_value(vm_available),
        ""SwapTotal"": get_unit_value(swp_total),
        ""SwapUsed"": get_unit_value(swp_used),
        ""SwapFree"": get_unit_value(swp_free),
    }","return {f'{key}': get_unit_value(value) for (key, value) in {'VmallocTotal': vm_total, 'VmallocUsed': vm_used, 'VmallocFree': vm_free, 'VmallocAvail': vm_available, 'SwapTotal': swp_total, 'SwapUsed': swp_used, 'SwapFree': swp_free}.items()}",find_wrong,2,,,
attn2d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/attn2d/fairseq_cli/preprocess.py,https://github.com/elbayadm/attn2d/tree/master/fairseq_cli/preprocess.py,,main$32,"def main(args):
    utils.import_user_module(args)
    os.makedirs(args.destdir, exist_ok=True)
    logger.addHandler(logging.FileHandler(filename=os.path.join(args.destdir, 'preprocess.log')))
    logger.info(args)
    task = tasks.get_task(args.task)

    def train_path(lang):
        return '{}{}'.format(args.trainpref, '.' + lang if lang else '')

    def file_name(prefix, lang):
        fname = prefix
        if lang is not None:
            fname += '.{lang}'.format(lang=lang)
        return fname

    def dest_path(prefix, lang):
        return os.path.join(args.destdir, file_name(prefix, lang))

    def dict_path(lang):
        return dest_path('dict', lang) + '.txt'

    def build_dictionary(filenames, src=False, tgt=False):
        assert src ^ tgt
        return task.build_dictionary(filenames, workers=args.workers, threshold=args.thresholdsrc if src else args.thresholdtgt, nwords=args.nwordssrc if src else args.nwordstgt, padding_factor=args.padding_factor)
    target = not args.only_source
    if not args.srcdict and os.path.exists(dict_path(args.source_lang)):
        raise FileExistsError(dict_path(args.source_lang))
    if target and (not args.tgtdict) and os.path.exists(dict_path(args.target_lang)):
        raise FileExistsError(dict_path(args.target_lang))
    if args.joined_dictionary:
        assert not args.srcdict or not args.tgtdict, 'cannot use both --srcdict and --tgtdict with --joined-dictionary'
        if args.srcdict:
            src_dict = task.load_dictionary(args.srcdict)
        elif args.tgtdict:
            src_dict = task.load_dictionary(args.tgtdict)
        else:
            assert args.trainpref, '--trainpref must be set if --srcdict is not specified'
            src_dict = build_dictionary({train_path(lang) for lang in [args.source_lang, args.target_lang]}, src=True)
        tgt_dict = src_dict
    else:
        if args.srcdict:
            src_dict = task.load_dictionary(args.srcdict)
        else:
            assert args.trainpref, '--trainpref must be set if --srcdict is not specified'
            src_dict = build_dictionary([train_path(args.source_lang)], src=True)
        if target:
            if args.tgtdict:
                tgt_dict = task.load_dictionary(args.tgtdict)
            else:
                assert args.trainpref, '--trainpref must be set if --tgtdict is not specified'
                tgt_dict = build_dictionary([train_path(args.target_lang)], tgt=True)
        else:
            tgt_dict = None
    src_dict.save(dict_path(args.source_lang))
    if target and tgt_dict is not None:
        tgt_dict.save(dict_path(args.target_lang))

    def make_binary_dataset(vocab, input_prefix, output_prefix, lang, num_workers):
        logger.info('[{}] Dictionary: {} types'.format(lang, len(vocab) - 1))
        n_seq_tok = [0, 0]
        replaced = Counter()

        def merge_result(worker_result):
            replaced.update(worker_result['replaced'])
            n_seq_tok[0] += worker_result['nseq']
            n_seq_tok[1] += worker_result['ntok']
        input_file = '{}{}'.format(input_prefix, '.' + lang if lang is not None else '')
        offsets = Binarizer.find_offsets(input_file, num_workers)
        pool = None
        if num_workers > 1:
            pool = Pool(processes=num_workers - 1)
            for worker_id in range(1, num_workers):
                prefix = '{}{}'.format(output_prefix, worker_id)
                pool.apply_async(binarize, (args, input_file, vocab, prefix, lang, offsets[worker_id], offsets[worker_id + 1]), callback=merge_result)
            pool.close()
        ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, lang, 'bin'), impl=args.dataset_impl, vocab_size=len(vocab))
        merge_result(Binarizer.binarize(input_file, vocab, lambda t: ds.add_item(t), offset=0, end=offsets[1]))
        if num_workers > 1:
            pool.join()
            for worker_id in range(1, num_workers):
                prefix = '{}{}'.format(output_prefix, worker_id)
                temp_file_path = dataset_dest_prefix(args, prefix, lang)
                ds.merge_file_(temp_file_path)
                os.remove(indexed_dataset.data_file_path(temp_file_path))
                os.remove(indexed_dataset.index_file_path(temp_file_path))
        ds.finalize(dataset_dest_file(args, output_prefix, lang, 'idx'))
        logger.info('[{}] {}: {} sents, {} tokens, {:.3}% replaced by {}'.format(lang, input_file, n_seq_tok[0], n_seq_tok[1], 100 * sum(replaced.values()) / n_seq_tok[1], vocab.unk_word))

    def make_binary_alignment_dataset(input_prefix, output_prefix, num_workers):
        nseq = [0]

        def merge_result(worker_result):
            nseq[0] += worker_result['nseq']
        input_file = input_prefix
        offsets = Binarizer.find_offsets(input_file, num_workers)
        pool = None
        if num_workers > 1:
            pool = Pool(processes=num_workers - 1)
            for worker_id in range(1, num_workers):
                prefix = '{}{}'.format(output_prefix, worker_id)
                pool.apply_async(binarize_alignments, (args, input_file, utils.parse_alignment, prefix, offsets[worker_id], offsets[worker_id + 1]), callback=merge_result)
            pool.close()
        ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, None, 'bin'), impl=args.dataset_impl)
        merge_result(Binarizer.binarize_alignments(input_file, utils.parse_alignment, lambda t: ds.add_item(t), offset=0, end=offsets[1]))
        if num_workers > 1:
            pool.join()
            for worker_id in range(1, num_workers):
                prefix = '{}{}'.format(output_prefix, worker_id)
                temp_file_path = dataset_dest_prefix(args, prefix, None)
                ds.merge_file_(temp_file_path)
                os.remove(indexed_dataset.data_file_path(temp_file_path))
                os.remove(indexed_dataset.index_file_path(temp_file_path))
        ds.finalize(dataset_dest_file(args, output_prefix, None, 'idx'))
        logger.info('[alignments] {}: parsed {} alignments'.format(input_file, nseq[0]))

    def make_dataset(vocab, input_prefix, output_prefix, lang, num_workers=1):
        if args.dataset_impl == 'raw':
            output_text_file = dest_path(output_prefix + '.{}-{}'.format(args.source_lang, args.target_lang), lang)
            shutil.copyfile(file_name(input_prefix, lang), output_text_file)
        else:
            make_binary_dataset(vocab, input_prefix, output_prefix, lang, num_workers)

    def make_all(lang, vocab):
        if args.trainpref:
            make_dataset(vocab, args.trainpref, 'train', lang, num_workers=args.workers)
        if args.validpref:
            for (k, validpref) in enumerate(args.validpref.split(',')):
                outprefix = 'valid{}'.format(k) if k > 0 else 'valid'
                make_dataset(vocab, validpref, outprefix, lang, num_workers=args.workers)
        if args.testpref:
            for (k, testpref) in enumerate(args.testpref.split(',')):
                outprefix = 'test{}'.format(k) if k > 0 else 'test'
                make_dataset(vocab, testpref, outprefix, lang, num_workers=args.workers)

    def make_all_alignments():
        if args.trainpref and os.path.exists(args.trainpref + '.' + args.align_suffix):
            make_binary_alignment_dataset(args.trainpref + '.' + args.align_suffix, 'train.align', num_workers=args.workers)
        if args.validpref and os.path.exists(args.validpref + '.' + args.align_suffix):
            make_binary_alignment_dataset(args.validpref + '.' + args.align_suffix, 'valid.align', num_workers=args.workers)
        if args.testpref and os.path.exists(args.testpref + '.' + args.align_suffix):
            make_binary_alignment_dataset(args.testpref + '.' + args.align_suffix, 'test.align', num_workers=args.workers)
    make_all(args.source_lang, src_dict)
    if target:
        make_all(args.target_lang, tgt_dict)
    if args.align_suffix:
        make_all_alignments()
    logger.info('Wrote preprocessed data to {}'.format(args.destdir))
    if args.alignfile:
        assert args.trainpref, '--trainpref must be set if --alignfile is specified'
        src_file_name = train_path(args.source_lang)
        tgt_file_name = train_path(args.target_lang)
        freq_map = {}
        with open(args.alignfile, 'r', encoding='utf-8') as align_file:
            with open(src_file_name, 'r', encoding='utf-8') as src_file:
                with open(tgt_file_name, 'r', encoding='utf-8') as tgt_file:
                    for (a, s, t) in zip_longest(align_file, src_file, tgt_file):
                        si = src_dict.encode_line(s, add_if_not_exist=False)
                        ti = tgt_dict.encode_line(t, add_if_not_exist=False)
                        ai = list(map(lambda x: tuple(x.split('-')), a.split()))
                        for (sai, tai) in ai:
                            srcidx = si[int(sai)]
                            tgtidx = ti[int(tai)]
                            if srcidx != src_dict.unk() and tgtidx != tgt_dict.unk():
                                assert srcidx != src_dict.pad()
                                assert srcidx != src_dict.eos()
                                assert tgtidx != tgt_dict.pad()
                                assert tgtidx != tgt_dict.eos()
                                if srcidx not in freq_map:
                                    freq_map[srcidx] = {}
                                if tgtidx not in freq_map[srcidx]:
                                    freq_map[srcidx][tgtidx] = 1
                                else:
                                    freq_map[srcidx][tgtidx] += 1
        align_dict = {}
        for srcidx in freq_map.keys():
            align_dict[srcidx] = max(freq_map[srcidx], key=freq_map[srcidx].get)
        with open(os.path.join(args.destdir, 'alignment.{}-{}.txt'.format(args.source_lang, args.target_lang)), 'w', encoding='utf-8') as f:
            for (k, v) in align_dict.items():
                print('{} {}'.format(src_dict[k], tgt_dict[v]), file=f)","if lang is not None:
    fname += '.{lang}'.format(lang=lang)",fname = f'{prefix}.{lang}' if lang is not None else prefix,find_wrong,2,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/model/dataset_collections/builder.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/model/dataset_collections/builder.py,CollectionBuilder,build_elements$84,"def build_elements(self):
    elements = self._current_elements
    if self._nested_collection:
        new_elements = {}
        for (identifier, element) in elements.items():
            new_elements[identifier] = element.build()
        elements = new_elements
    else:
        self._current_elements = {}
    return elements","for (identifier, element) in elements.items():
    new_elements[identifier] = element.build()
elements = new_elements","elements = {identifier: element.build() for (identifier, element) in elements.items()}",find_wrong,1,,,
zenodo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zenodo/zenodo/modules/spam/views.py,https://github.com/zenodo/zenodo/tree/master/zenodo/modules/spam/views.py,,safelist_admin$199,"def safelist_admin():
    """"""Safelist admin.""""""
    if not Permission(ActionNeed('admin-access')).can():
        abort(403)
    data = request.args.get('data', 'all', type=str)
    data_categories = {'records': data in ('all', 'records'), 'communities': data in ('all', 'communities')}
    from_weeks = request.args.get('from_weeks', 4, type=int)
    to_weeks = request.args.get('to_weeks', 0, type=int)
    max_users = request.args.get('max_users', 1000, type=int)
    include_pending = request.args.get('include_pending', 'include', type=str) == 'include'
    result = {}
    if data_categories['records']:
        search = RecordsSearch(index='records').filter('range', **{'created': {'gte': 'now-{}w'.format(from_weeks), 'lt': 'now-{}w'.format(to_weeks)}}).filter('term', _safelisted=False)
        user_agg = search.aggs.bucket('user', 'terms', field='owners', size=max_users)
        user_agg.metric('records', 'top_hits', size=3, _source=['title', 'description', 'recid'])
        res = search[0:0].execute()
        for user in res.aggregations.user.buckets:
            result[user.key] = {'last_content_titles': ', '.join((r.title for r in user.records)), 'last_content_descriptions': ', '.join((r.description for r in user.records)), 'first_content_url': url_for('invenio_records_ui.recid', pid_value=user.records[0].recid), 'total_content': user.doc_count}
    if data_categories['communities']:
        from_date = datetime.utcnow() - timedelta(weeks=from_weeks)
        to_date = datetime.utcnow() - timedelta(weeks=to_weeks)
        community_users = db.session.query(User.id.label('user_id'), sa.func.count(Community.id).label('count'), sa.func.max(Community.id).label('c_id'), sa.func.max(Community.title).label('title'), sa.func.max(Community.description).label('description')).join(Community).group_by(User.id).filter(Community.created.between(from_date, to_date), Community.deleted_at.is_(None), ~User.safelist.any()).limit(max_users)
        for row in community_users:
            user_data = result.get(row.user_id, {'last_content_titles': '', 'last_content_descriptions': '', 'first_content_url': '', 'total_content': 0})
            if user_data['last_content_titles']:
                user_data['last_content_titles'] += ', '
            user_data['last_content_titles'] += row.title
            if user_data['last_content_descriptions']:
                user_data['last_content_descriptions'] += ', '
            user_data['last_content_descriptions'] += row.description
            user_data['first_content_url'] = url_for('invenio_communities.detail', community_id=row.c_id)
            user_data['total_content'] += row.count
            result[row.user_id] = user_data
    _expand_users_info(result, include_pending)
    return render_template('zenodo_spam/safelist/admin.html', users=result)","'records': data in ('all', 'records'),
    'communities': data in ('all', 'communities'),
}","data_categories = {'records': data in ('all', 'records'), 'communities': data in ('all', 'communities')}",find_wrong,2,,,
pikaur,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pikaur/pikaur/search_cli.py,https://github.com/actionless/pikaur/tree/master/pikaur/search_cli.py,,package_search_thread_aur$45,"def package_search_thread_aur(queries: list[str]) -> list[AURPackageInfo]:
    args = parse_args()
    result = {}
    if queries:
        use_as_filters: list[str] = []
        with ThreadPool() as pool:
            requests = {}
            for query in queries:
                requests[query] = pool.apply_async(aur_rpc_search_name_desc, (query,))
            pool.close()
            for (query, request) in requests.items():
                try:
                    result[query] = request.get()
                except AURError as exc:
                    if exc.error == 'Too many package results.':
                        print_error(translate(""AUR: Too many package results for '{query}'"").format(query=query))
                        use_as_filters.append(query)
                    elif exc.error == 'Query arg too small.':
                        print_error(translate(""AUR: Query arg too small '{query}'"").format(query=query))
                        use_as_filters.append(query)
                    else:
                        raise
            pool.join()
        for query in use_as_filters:
            result = filter_aur_results(result, query)
        if args.namesonly:
            for (subindex, subresult) in result.items():
                result[subindex] = [pkg for pkg in subresult if subindex in pkg.name]
    elif args.quiet:
        result = {'all': [AURPackageInfo(name=name, packagebase=name, version='0') for name in get_all_aur_names()]}
    else:
        result = {'all': get_all_aur_packages()}
    if not args.quiet:
        sys.stderr.write('#')
    return list(join_search_results(list(result.values())))","for (query, request) in requests.items():
    try:
        result[query] = request.get()
    except AURError as exc:
        if exc.error == 'Too many package results.':
            print_error(translate(""AUR: Too many package results for '{query}'"").format(query=query))
            use_as_filters.append(query)
        elif exc.error == 'Query arg too small.':
            print_error(translate(""AUR: Query arg too small '{query}'"").format(query=query))
            use_as_filters.append(query)
        else:
            raise","result = {query: request.get() if exc.error != 'Too many package results.' and exc.error != 'Query arg too small.' else use_as_filters.append(query) for (query, request) in requests.items()}",find_wrong,2,,,
ansible-modules-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-core/cloud/amazon/ec2_ami_find.py,https://github.com/ansible/ansible-modules-core/tree/master/cloud/amazon/ec2_ami_find.py,,get_block_device_mapping$279,"def get_block_device_mapping(image):
    """"""
    Retrieves block device mapping from AMI
    """"""
    bdm_dict = dict()
    bdm = getattr(image, 'block_device_mapping')
    for device_name in bdm.keys():
        bdm_dict[device_name] = {'size': bdm[device_name].size, 'snapshot_id': bdm[device_name].snapshot_id, 'volume_type': bdm[device_name].volume_type, 'encrypted': bdm[device_name].encrypted, 'delete_on_termination': bdm[device_name].delete_on_termination}
    return bdm_dict","bdm = getattr(image, 'block_device_mapping')
for device_name in bdm.keys():
    bdm_dict[device_name] = {'size': bdm[device_name].size, 'snapshot_id': bdm[device_name].snapshot_id, 'volume_type': bdm[device_name].volume_type, 'encrypted': bdm[device_name].encrypted, 'delete_on_termination': bdm[device_name].delete_on_termination}","bdm_dict = {device_name: {'size': bdm[device_name].size, 'snapshot_id': bdm[device_name].snapshot_id, 'volume_type': bdm[device_name].volume_type, 'encrypted': bdm[device_name].encrypted, 'delete_on_termination': bdm[device_name].delete_on_termination} for device_name in bdm.keys()}",find_wrong,2,,,
Keras-TextClassification,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Keras-TextClassification/keras_textclassification/data_preprocess/generator_preprocess.py,https://github.com/yongzhuo/Keras-TextClassification/tree/master/keras_textclassification/data_preprocess/generator_preprocess.py,PreprocessGenerator,prereocess_pred_xid$40,"def prereocess_pred_xid(self, pred):
    if os.path.exists(self.path_fast_text_model_l2i_i2l):
        pred_l2i = {}
        l2i = self.l2i_i2l['l2i']
        for i in range(len(pred)):
            pred_l2i[pred[i]] = l2i[pred[i]]
        pred_l2i_rank = [sorted(pred_l2i.items(), key=lambda k: k[1], reverse=True)]
        return pred_l2i_rank
    else:
        raise RuntimeError('path_fast_text_model_label2index is None')","l2i = self.l2i_i2l['l2i']
for i in range(len(pred)):
    pred_l2i[pred[i]] = l2i[pred[i]]",pred_l2i = {pred[i]: self.l2i_i2l['l2i'][pred[i]] for i in range(len(pred))},find_wrong,-1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/smartos_vmadm.py,https://github.com/saltstack/salt/tree/master/salt/modules/smartos_vmadm.py,,create$701,"def create(from_file=None, **kwargs):
    """"""
    Create a new vm

    from_file : string
        json file to create the vm from -- if present, all other options will be ignored
    kwargs : string|int|...
        options to set for the vm

    CLI Example:

    .. code-block:: bash

        salt '*' vmadm.create from_file=/tmp/new_vm.json
        salt '*' vmadm.create image_uuid='...' alias='...' nics='[{ ""nic_tag"": ""admin"", ""ip"": ""198.51.100.123"", ...}, {...}]' [...]
    """"""
    ret = {}
    vmcfg = {}
    kwargs = salt.utils.args.clean_kwargs(**kwargs)
    for (k, v) in kwargs.items():
        vmcfg[k] = v
    if from_file:
        return _create_update_from_file('create', path=from_file)
    else:
        return _create_update_from_cfg('create', vmcfg=vmcfg)","kwargs = salt.utils.args.clean_kwargs(**kwargs)
for (k, v) in kwargs.items():
    vmcfg[k] = v","vmcfg = {k: v for (k, v) in salt.utils.args.clean_kwargs(**kwargs).items()}",find_wrong,1,,,
flasgger,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flasgger/flasgger/marshmallow_apispec.py,https://github.com/flasgger/flasgger/tree/master/flasgger/marshmallow_apispec.py,SwaggerView,dispatch_request$90,"def dispatch_request(self, *args, **kwargs):
    """"""
        If validation=True perform validation
        """"""
    if self.validation:
        specs = {}
        attrs = flasgger.constants.OPTIONAL_FIELDS + ['parameters', 'definitions', 'responses', 'summary', 'description']
        for attr in attrs:
            specs[attr] = getattr(self, attr)
        definitions = {}
        specs.update(convert_schemas(specs, definitions))
        specs['definitions'] = definitions
        flasgger.utils.validate(specs=specs, validation_function=self.validation_function, validation_error_handler=self.validation_error_handler)
    return super(SwaggerView, self).dispatch_request(*args, **kwargs)","attrs = flasgger.constants.OPTIONAL_FIELDS + ['parameters', 'definitions', 'responses', 'summary', 'description']
for attr in attrs:
    specs[attr] = getattr(self, attr)","specs = {attr: getattr(self, attr) for attr in flasgger.constants.OPTIONAL_FIELDS + ['parameters', 'definitions', 'responses', 'summary', 'description']}",find_wrong,1,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/cwl/util.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/cwl/util.py,,output_to_cwl_json$437,"def output_to_cwl_json(galaxy_output, get_metadata, get_dataset, get_extra_files, pseduo_location=False):
    """"""Convert objects in a Galaxy history into a CWL object.

    Useful in running conformance tests and implementing the cwl-runner
    interface via Galaxy.
    """"""

    def element_to_cwl_json(element):
        object = element['object']
        content_type = object.get('history_content_type')
        metadata = None
        if content_type is None:
            content_type = 'dataset_collection'
            metadata = element['object']
            metadata['history_content_type'] = content_type
        element_output = GalaxyOutput(galaxy_output.history_id, content_type, object['id'], metadata)
        return output_to_cwl_json(element_output, get_metadata, get_dataset, get_extra_files, pseduo_location=pseduo_location)
    output_metadata = galaxy_output.metadata
    if output_metadata is None:
        output_metadata = get_metadata(galaxy_output.history_content_type, galaxy_output.history_content_id)

    def dataset_dict_to_json_content(dataset_dict):
        if 'content' in dataset_dict:
            return json.loads(dataset_dict['content'])
        else:
            with open(dataset_dict['path']) as f:
                return json.load(f)
    if galaxy_output.history_content_type == 'raw_value':
        return galaxy_output.history_content_id
    elif output_metadata['history_content_type'] == 'dataset':
        ext = output_metadata['file_ext']
        if ext == 'expression.json':
            dataset_dict = get_dataset(output_metadata)
            return dataset_dict_to_json_content(dataset_dict)
        else:
            file_or_directory = 'Directory' if ext == 'directory' else 'File'
            secondary_files = []
            if file_or_directory == 'File':
                dataset_dict = get_dataset(output_metadata)
                properties = output_properties(pseduo_location=pseduo_location, **dataset_dict)
                basename = properties['basename']
                extra_files = get_extra_files(output_metadata)
                found_index = False
                for extra_file in extra_files:
                    if extra_file['class'] == 'File':
                        path = extra_file['path']
                        if path == SECONDARY_FILES_INDEX_PATH:
                            found_index = True
                if found_index:
                    ec = get_dataset(output_metadata, filename=SECONDARY_FILES_INDEX_PATH)
                    index = dataset_dict_to_json_content(ec)

                    def dir_listing(dir_path):
                        listing = []
                        for extra_file in extra_files:
                            path = extra_file['path']
                            extra_file_class = extra_file['class']
                            extra_file_basename = os.path.basename(path)
                            if os.path.join(dir_path, extra_file_basename) != path:
                                continue
                            if extra_file_class == 'File':
                                ec = get_dataset(output_metadata, filename=path)
                                ec['basename'] = extra_file_basename
                                ec_properties = output_properties(pseduo_location=pseduo_location, **ec)
                            elif extra_file_class == 'Directory':
                                ec_properties = {}
                                ec_properties['class'] = 'Directory'
                                ec_properties['location'] = ec_basename
                                ec_properties['listing'] = dir_listing(path)
                            else:
                                raise Exception('Unknown output type encountered....')
                            listing.append(ec_properties)
                        return listing
                    for basename in index['order']:
                        for extra_file in extra_files:
                            path = extra_file['path']
                            if path != os.path.join(SECONDARY_FILES_EXTRA_PREFIX, basename or ''):
                                continue
                            extra_file_class = extra_file['class']
                            if not STORE_SECONDARY_FILES_WITH_BASENAME:
                                ec_basename = basename + os.path.basename(path)
                            else:
                                ec_basename = os.path.basename(path)
                            if extra_file_class == 'File':
                                ec = get_dataset(output_metadata, filename=path)
                                ec['basename'] = ec_basename
                                ec_properties = output_properties(pseduo_location=pseduo_location, **ec)
                            elif extra_file_class == 'Directory':
                                ec_properties = {}
                                ec_properties['class'] = 'Directory'
                                ec_properties['location'] = ec_basename
                                ec_properties['listing'] = dir_listing(path)
                            else:
                                raise Exception('Unknown output type encountered....')
                            secondary_files.append(ec_properties)
            else:
                basename = output_metadata.get('created_from_basename')
                if not basename:
                    basename = output_metadata.get('name')
                listing: List[OutputPropertiesType] = []
                properties = {'class': 'Directory', 'basename': basename, 'listing': listing}
                extra_files = get_extra_files(output_metadata)
                for extra_file in extra_files:
                    if extra_file['class'] == 'File':
                        path = extra_file['path']
                        ec = get_dataset(output_metadata, filename=path)
                        ec['basename'] = os.path.basename(path)
                        ec_properties = output_properties(pseduo_location=pseduo_location, **ec)
                        listing.append(ec_properties)
            if secondary_files:
                properties['secondaryFiles'] = secondary_files
            return properties
    elif output_metadata['history_content_type'] == 'dataset_collection':
        collection_type = output_metadata['collection_type'].split(':', 1)[0]
        if collection_type in ['list', 'paired']:
            rval_l = []
            for element in output_metadata['elements']:
                rval_l.append(element_to_cwl_json(element))
            return rval_l
        elif collection_type == 'record':
            rval_d = {}
            for element in output_metadata['elements']:
                rval_d[element['element_identifier']] = element_to_cwl_json(element)
            return rval_d
        return None
    else:
        raise NotImplementedError('Unknown history content type encountered')","for element in output_metadata['elements']:
    rval_l.append(element_to_cwl_json(element))",rval_l = [element_to_cwl_json(element) for element in output_metadata['elements']],find_wrong,2,,,
PyRetri,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyRetri/search/reid_search_index.py,https://github.com/PyRetri/PyRetri/tree/master/search/reid_search_index.py,,main$44,"def main():
    args = parse_args()
    assert args.fea_dir is not None, 'the feature directory must be provided!'
    assert args.search_modules is not None, 'the search modules must be provided!'
    assert args.save_path is not None, 'the save path must be provided!'
    cfg = get_defaults_cfg()
    datasets = load_datasets()
    indexes = importlib.import_module('{}.index_dict'.format(args.search_modules)).indexes
    evaluates = importlib.import_module('{}.index_dict'.format(args.search_modules)).evaluates
    if os.path.exists(args.save_path):
        with open(args.save_path, 'r') as f:
            results = json.load(f)
    else:
        results = list()
    for dir in os.listdir(args.fea_dir):
        for (data_name, data_args) in datasets.items():
            for (index_name, index_args) in indexes.items():
                if data_name in dir:
                    print(dir)
                    (gallery_fea_dir, query_fea_dir, train_fea_dir) = get_dir(args.fea_dir, dir, data_args)
                    evaluate_args = evaluates['reid_overall']
                    for dim_proc in index_args.dim_processors.names:
                        if dim_proc in ['PartPCA', 'PartSVD', 'PCA', 'SVD']:
                            index_args.dim_processors[dim_proc].train_fea_dir = train_fea_dir
                    for fea_name in fea_names:
                        result_dict = get_default_result_dict(dir, data_name, index_name, fea_name)
                        if check_result_exist(result_dict, results):
                            print('[Search Query]: config exists...')
                            continue
                        index_args.feature_names = [fea_name]
                        cfg.index.merge_from_other_cfg(index_args)
                        cfg.evaluate.merge_from_other_cfg(evaluate_args)
                        (query_fea, query_info, _) = feature_loader.load(query_fea_dir, [fea_name])
                        (gallery_fea, gallery_info, _) = feature_loader.load(gallery_fea_dir, [fea_name])
                        index_helper = build_index_helper(cfg.index)
                        (index_result_info, _, _) = index_helper.do_index(query_fea, query_info, gallery_fea)
                        evaluate_helper = build_evaluate_helper(cfg.evaluate)
                        (mAP, recall_at_k) = evaluate_helper.do_eval(index_result_info, gallery_info)
                        to_save_recall = dict()
                        for k in recall_at_k:
                            to_save_recall[str(k)] = recall_at_k[k]
                        result_dict['mAP'] = float(mAP)
                        result_dict['recall_at_k'] = to_save_recall
                        results.append(result_dict)
                        with open(args.save_path, 'w') as f:
                            json.dump(results, f)","for dir in os.listdir(args.fea_dir):
    for (data_name, data_args) in datasets.items():
        for (index_name, index_args) in indexes.items():
            if data_name in dir:
                print(dir)
                (gallery_fea_dir, query_fea_dir, train_fea_dir) = get_dir(args.fea_dir, dir, data_args)
                evaluate_args = evaluates['reid_overall']
                for dim_proc in index_args.dim_processors.names:
                    if dim_proc in ['PartPCA', 'PartSVD', 'PCA', 'SVD']:
                        index_args.dim_processors[dim_proc].train_fea_dir = train_fea_dir
                for fea_name in fea_names:
                    result_dict = get_default_result_dict(dir, data_name, index_name, fea_name)
                    if check_result_exist(result_dict, results):
                        print('[Search Query]: config exists...')
                        continue
                    index_args.feature_names = [fea_name]
                    cfg.index.merge_from_other_cfg(index_args)
                    cfg.evaluate.merge_from_other_cfg(evaluate_args)
                    (query_fea, query_info, _) = feature_loader.load(query_fea_dir, [fea_name])
                    (gallery_fea, gallery_info, _) = feature_loader.load(gallery_fea_dir, [fea_name])
                    index_helper = build_index_helper(cfg.index)
                    (index_result_info, _, _) = index_helper.do_index(query_fea, query_info, gallery_fea)
                    evaluate_helper = build_evaluate_helper(cfg.evaluate)
                    (mAP, recall_at_k) = evaluate_helper.do_eval(index_result_info, gallery_info)
                    to_save_recall = dict()
                    for k in recall_at_k:
                        to_save_recall[str(k)] = recall_at_k[k]
                    result_dict['mAP'] = float(mAP)
                    result_dict['recall_at_k'] = to_save_recall
                    results.append(result_dict)
                    with open(args.save_path, 'w') as f:
                        json.dump(results, f)","results = []
for dir in os.listdir(args.fea_dir):
    for (data_name, data_args) in datasets.items():
        for (index_name, index_args) in indexes.items():
            if data_name in dir:
                print(dir)
                (gallery_fea_dir, query_fea_dir, train_fea_dir) = get_dir(args.fea_dir, dir, data_args)
                evaluate_args = evaluates['reid_overall']
                [index_args.dim_processors.__setitem__(dim_proc, train_fea_dir) for dim_proc in index_args.dim_processors.names if dim_proc in ['PartPCA', 'PartSVD', 'PCA', 'SVD']]
                for fea_name in fea_names:
                    result_dict = get_default_result_dict(dir, data_name, index_name, fea_name)
                    if check_result_exist(result_dict, results):
                        print('[Search Query]: config exists...')
                        continue
                    index_args.feature_names = [fea_name]
                    cfg.index.merge_from_other_cfg(index_args)
                    cfg.evaluate.merge_from_other_cfg(evaluate_args)
                    (query_fea, query_info, _) = feature_loader.load(query_fea_dir, [fea_name])
                    (gallery_fea, gallery_info, _) = feature_loader.load(gallery_fea_dir, [fea_name])
                    index_helper = build_index_helper(cfg.index)
                    (index_result_info, _, _) = index_helper.do_index(query_fea, query_info, gallery_fea)
                    evaluate_helper = build_evaluate_helper(cfg.evaluate)
                    (mAP, recall_at_k) = evaluate_helper.do_eval(index_result_info, gallery_info)
                    to_save_recall = {str(k): recall_at_k[k] for k in recall_at_k}
                    result_dict['mAP'] = float(mAP)
                    result_dict['recall_at_k'] = to_save_recall
                    results.append(result_dict)
                    with open(args.save_path, 'w') as f:
                        json.dump(results, f)",find_wrong,2,,,
nyaa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nyaa/nyaa/utils.py,https://github.com/nyaadevs/nyaa/tree/master/nyaa/utils.py,,flatten_dict$47,"def flatten_dict(d, result=None):
    if result is None:
        result = {}
    for key in d:
        value = d[key]
        if isinstance(value, dict):
            value1 = {}
            for keyIn in value:
                value1['/'.join([key, keyIn])] = value[keyIn]
            flatten_dict(value1, result)
        elif isinstance(value, (list, tuple)):
            for (indexB, element) in enumerate(value):
                if isinstance(element, dict):
                    value1 = {}
                    index = 0
                    for keyIn in element:
                        newkey = '/'.join([key, keyIn])
                        value1[newkey] = value[indexB][keyIn]
                        index += 1
                    for keyA in value1:
                        flatten_dict(value1, result)
        else:
            result[key] = value
    return result","for keyIn in value:
    value1['/'.join([key, keyIn])] = value[keyIn]
flatten_dict(value1, result)","value1 = {'/'.join([key, keyIn]): value[keyIn] for keyIn in value}
flatten_dict(value1, result)",find_wrong,1,,,
animation_nodes,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/animation_nodes/animation_nodes/auto_load.py,https://github.com/JacquesLucke/animation_nodes/tree/master/animation_nodes/auto_load.py,,get_register_deps_dict$73,"def get_register_deps_dict(modules):
    my_classes = set(iter_my_classes(modules))
    my_classes_by_idname = {cls.bl_idname: cls for cls in my_classes if hasattr(cls, 'bl_idname')}
    deps_dict = {}
    for cls in my_classes:
        deps_dict[cls] = set(iter_my_register_deps(cls, my_classes, my_classes_by_idname))
    return deps_dict","for cls in my_classes:
    if hasattr(cls, 'bl_idname'):
        my_classes_by_idname[cls.bl_idname] = cls","my_classes_by_idname = {cls.bl_idname: cls for cls in my_classes if hasattr(cls, 'bl_idname')}",find_wrong,2,,,
pgmpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgmpy/pgmpy/readwrite/UAI.py,https://github.com/pgmpy/pgmpy/tree/master/pgmpy/readwrite/UAI.py,UAIWriter,get_domain$349,"def get_domain(self):
    """"""
        Adds domain of each variable to the network.

        Examples
        --------
        >>> from pgmpy.readwrite import UAIWriter
        >>> writer = UAIWriter(model)
        >>> writer.get_domain()
        """"""
    if isinstance(self.model, BayesianNetwork):
        cpds = self.model.get_cpds()
        cpds.sort(key=lambda x: x.variable)
        domain = {}
        for cpd in cpds:
            domain[cpd.variable] = str(cpd.variable_card)
        return domain
    elif isinstance(self.model, MarkovNetwork):
        factors = self.model.get_factors()
        domain = {}
        for factor in factors:
            variables = factor.variables
            for var in variables:
                if var not in domain:
                    domain[var] = str(factor.get_cardinality([var])[var])
        return domain
    else:
        raise TypeError('Model must be an instance of Markov or Bayesian model.')","cpds = self.model.get_cpds()
    cpds.sort(key=lambda x: x.variable)
    domain = {}
    for cpd in cpds:
        domain[cpd.variable] = str(cpd.variable_card)
    return domain
elif isinstance(self.model, MarkovNetwork):
    factors = self.model.get_factors()
    domain = {}
    for factor in factors:
        variables = factor.variables
        for var in variables:
            if var not in domain:
                domain[var] = str(factor.get_cardinality([var])[var])
    return domain","if isinstance(self.model, BayesianNetwork):
    cpds = self.model.get_cpds()
    cpds.sort(key=lambda x: x.variable)
    return {cpd.variable: str(cpd.variable_card) for cpd in cpds}
elif isinstance(self.model, MarkovNetwork):
    factors = self.model.get_factors()
    return {var: str(factor.get_cardinality([var])[var]) for factor in factors for var in factor.variables if var not in domain}
else:
    raise TypeError('Model must be an instance of Markov or Bayesian model.')",find_wrong,1,,,
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/autotvm/graph_tuner/utils/traverse_graph.py,https://github.com/apache/tvm/tree/master/python/tvm/autotvm/graph_tuner/utils/traverse_graph.py,,get_out_nodes$312,"def get_out_nodes(in_node_dict):
    """"""Create output dictionary from input dictionary.

    Parameters
    ----------
    in_node_dict : dict of int to list of int
        Dictionary maps node index to closest input ancestors.
        It can be created with get_in_nodes.

    Returns
    -------
    out : dict of int to list of int
        Dictionary maps node index to closest output nodes.
    """"""
    out_node_dict = {}
    for key in in_node_dict:
        out_node_dict[key] = []
    for (key, val) in in_node_dict.items():
        for item in val:
            if item in out_node_dict:
                out_node_dict[item].append(key)
            else:
                out_node_dict[item] = [key]
    return out_node_dict","for key in in_node_dict:
    out_node_dict[key] = []
for (key, val) in in_node_dict.items():
    for item in val:
        if item in out_node_dict:
            out_node_dict[item].append(key)
        else:
            out_node_dict[item] = [key]","out_node_dict = {key: [] for key in in_node_dict}
[out_node_dict[item].append(key) if item in out_node_dict else out_node_dict.update({item: [key]}) for (key, val) in in_node_dict.items() for item in val]",find_wrong,1,,,
lxmert,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lxmert/src/lxrt/entry.py,https://github.com/airsplay/lxmert/tree/master/src/lxrt/entry.py,LXRTEncoder,load$126,"def load(self, path):
    print('Load LXMERT pre-trained model from %s' % path)
    state_dict = torch.load('%s_LXRT.pth' % path)
    new_state_dict = {}
    for (key, value) in state_dict.items():
        if key.startswith('module.'):
            new_state_dict[key[len('module.'):]] = value
        else:
            new_state_dict[key] = value
    state_dict = new_state_dict
    load_keys = set(state_dict.keys())
    model_keys = set(self.model.state_dict().keys())
    print()
    print('Weights in loaded but not in model:')
    for key in sorted(load_keys.difference(model_keys)):
        print(key)
    print()
    print('Weights in model but not in loaded:')
    for key in sorted(model_keys.difference(load_keys)):
        print(key)
    print()
    self.model.load_state_dict(state_dict, strict=False)","for (key, value) in state_dict.items():
    if key.startswith('module.'):
        new_state_dict[key[len('module.'):]] = value
    else:
        new_state_dict[key] = value
state_dict = new_state_dict","state_dict = {key[len('module.'):] if key.startswith('module.') else key: value for (key, value) in state_dict.items()}",find_wrong,-1,,,
ByteTrack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ByteTrack/tutorials/cstrack/tracker.py,https://github.com/ifzhang/ByteTrack/tree/master/tutorials/cstrack/tracker.py,,sub_stracks$396,"def sub_stracks(tlista, tlistb):
    stracks = {}
    for t in tlista:
        stracks[t.track_id] = t
    for t in tlistb:
        tid = t.track_id
        if stracks.get(tid, 0):
            del stracks[tid]
    return list(stracks.values())","for t in tlista:
    stracks[t.track_id] = t
for t in tlistb:
    tid = t.track_id
    if stracks.get(tid, 0):
        del stracks[tid]
return list(stracks.values())","stracks = {t.track_id: t for t in tlista}
[stracks.pop(t.track_id, None) for t in tlistb]
return list(stracks.values())",find_wrong,-1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/solarispkg.py,https://github.com/saltstack/salt/tree/master/salt/modules/solarispkg.py,,latest_version$142,"def latest_version(*names, **kwargs):
    """"""
    Return the latest version of the named package available for upgrade or
    installation. If more than one package name is specified, a dict of
    name/version pairs is returned.

    If the latest version of a given package is already installed, an empty
    string will be returned for that package.

    CLI Example:

    .. code-block:: bash

        salt '*' pkg.latest_version <package name>
        salt '*' pkg.latest_version <package1> <package2> <package3> ...

    NOTE: As package repositories are not presently supported for Solaris
    pkgadd, this function will always return an empty string for a given
    package.
    """"""
    kwargs.pop('refresh', True)
    ret = {}
    if not names:
        return ''
    for name in names:
        ret[name] = ''
    if len(names) == 1:
        return ret[names[0]]
    return ret","if not names:
    return ''
for name in names:
    ret[name] = ''
if len(names) == 1:
    return ret[names[0]]
return ret","ret = {name: '' for name in names}
return ret[names[0]] if len(names) == 1 else ret",find_wrong,-1,,,
pass-import,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pass-import/pass_import/managers/lastpass.py,https://github.com/roddhjav/pass-import/tree/master/pass_import/managers/lastpass.py,LastpassCLI,show$91,"def show(self, uid):
    """"""Decrypt a lastpass entry and read the credentials.

        lpass do not show the same data with the --json option and without.
        To retrieve the full entry, both --json and --format option need to
        be used.

        :param str uid: UniqueID to the password entry to decrypt.
        :return dict: Return a dictionary with of the password entry.

        """"""
    entry = {}
    ignores = {'fullname'}
    keys = self.invkeys()
    jsons = self._command(['show', '--json', uid])
    item = json.loads(jsons).pop()
    for (key, value) in item.items():
        if key not in ignores:
            entry[keys.get(key, key)] = value
    entry['group'] = self._path(item['group'])
    ignores = {'Username', 'Password', 'URL'}
    arg = ['show', '--color=never', '--format=%fn|%fv', '--color=never', uid]
    data = self._command(arg).split('\n')
    data.pop()
    data.pop(0)
    for line in data:
        if '|' in line:
            (key, value) = line.split('|', 1)
            if key not in ignores:
                entry[key] = value
    if entry.get('url', '') == 'http://':
        entry['url'] = ''
    return entry","ignores = {'fullname'}
keys = self.invkeys()
jsons = self._command(['show', '--json', uid])
item = json.loads(jsons).pop()
for (key, value) in item.items():
    if key not in ignores:
        entry[keys.get(key, key)] = value
entry['group'] = self._path(item['group'])","ignores = {'fullname'}
keys = self.invkeys()
item = json.loads(self._command(['show', '--json', uid])).pop()
entry = {keys.get(key, key): value for (key, value) in item.items() if key not in ignores}
entry['group'] = self._path(item['group'])",find_wrong,1,,,
ivre,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ivre/ivre/tools/flowcli.py,https://github.com/ivre/ivre/tree/master/ivre/tools/flowcli.py,,main$78,"def main() -> None:
    parser = ArgumentParser(description=__doc__)
    parser.add_argument('--init', '--purgedb', action='store_true', help='Purge or create and initialize the database.')
    parser.add_argument('--ensure-indexes', action='store_true', help='Create missing indexes (will lock the database).')
    parser.add_argument('--node-filters', '-n', nargs='+', metavar='FILTER', help='Filter the results with a list of ivre specific node textual filters (see WebUI doc in FLOW.md).')
    parser.add_argument('--flow-filters', '-f', nargs='+', metavar='FILTER', help='Filter the results with a list of ivre specific flow textual filters (see WebUI doc in FLOW.md).')
    parser.add_argument('--json', '-j', action='store_true', help='Outputs the full json records of results.')
    parser.add_argument('--count', '-c', action='store_true', help='Only return the count of the results.')
    parser.add_argument('--limit', '-l', type=int, default=None, help='Output at most LIMIT results.')
    parser.add_argument('--skip', type=int, default=0, help='Skip first SKIP results.')
    parser.add_argument('--orderby', '-o', help='Order of results (""src"", ""dst"" or ""flow"")')
    parser.add_argument('--separator', '-s', help='Separator string.')
    parser.add_argument('--top', '-t', nargs='+', help='Top flows for a given set of fields, e.g. ""--top src.addr dport"".')
    parser.add_argument('--collect', '-C', nargs='+', help='When using --top, also collect these properties.', default=[])
    parser.add_argument('--sum', '-S', nargs='+', help='When using --top, sum on these properties to order the result.', default=[])
    parser.add_argument('--least', '-L', action='store_true', help='When using --top, sort records by least')
    parser.add_argument('--mode', '-m', help='Query special mode (flow_map, talk_map...)')
    parser.add_argument('--timeline', '-T', action='store_true', help='Retrieves the timeline of each flow')
    parser.add_argument('--flow-daily', action='store_true', help='Flow count per times of the day. If --precision is absent, it will be based on FLOW_TIME_PRECISION (%d)' % config.FLOW_TIME_PRECISION)
    parser.add_argument('--plot', action='store_true', help='Plot data when possible (requires matplotlib).')
    parser.add_argument('--fields', nargs='*', help='Without values, gives the list of available fields. Otherwise, display these fields for each entry.')
    parser.add_argument('--reduce-precision', type=int, metavar='NEW_PRECISION', help='Only with MongoDB backend. Reduce precision to NEW_PRECISION for flows timeslots. Uses precision, before, after and filters.')
    parser.add_argument('--after', '-a', type=str, help='Only with MongoDB backend. Get only flows seen after this date. Date format: YEAR-MONTH-DAY HOUR:MINUTE. Based on timeslots precision. If the given date is in the middle of a timeslot, flows start at the next timeslot.')
    parser.add_argument('--before', '-b', type=str, help='Only with MongoDB backend. Get only flows seen before this date. Date format: YEAR-MONTH-DAY HOUR:MINUTE. Based on timeslots precision. If the given date is in the middle of a timeslot, the whole period is kept even if theoretically some flows may have been seen after the given date.')
    parser.add_argument('--precision', nargs='?', default=None, const=0, help='Only With MongoDB backend. If PRECISION is specified, get only flows with one timeslot of the given precision. Otherwise, list precisions.', type=int)
    parser.add_argument('--host', type=str, metavar='HOST', help='Filter on source OR destination IP. Accepts IP address or CIDR.')
    parser.add_argument('--src', type=str, metavar='SRC', help='Filter on source IP. Accepts IP address or CIDR.')
    parser.add_argument('--dst', type=str, metavar='DST', help='Filter on destination IP. Accepts IP address or CIDR.')
    parser.add_argument('--proto', type=str, metavar='PROTO', help='Filter on transport protocol.')
    parser.add_argument('--tcp', action='store_true', help='Alias to --proto tcp')
    parser.add_argument('--udp', action='store_true', help='Alias to --proto udp')
    parser.add_argument('--port', type=int, metavar='PORT', help='Alias to --dport')
    parser.add_argument('--dport', type=int, metavar='DPORT', help='Filter on destination port.')
    parser.add_argument('--sport', type=int, metavar='SPORT', help='Filter on source port.')
    args = parser.parse_args()
    out = sys.stdout
    if args.plot and plt is None:
        utils.LOGGER.critical('Matplotlib is required for --plot')
        sys.exit(-1)
    if args.init:
        if os.isatty(sys.stdin.fileno()):
            out.write('This will remove any flow result in your database. Process ? [y/N] ')
            ans = input()
            if ans.lower() != 'y':
                sys.exit(-1)
        db.flow.init()
        sys.exit(0)
    if args.ensure_indexes:
        if os.isatty(sys.stdin.fileno()):
            out.write('This will lock your database. Process ? [y/N] ')
            ans = input()
            if ans.lower() != 'y':
                sys.exit(-1)
        db.flow.ensure_indexes()
        sys.exit(0)
    if args.fields is not None and (not args.fields):
        print_fields()
        sys.exit(0)
    elif args.fields is not None:
        for field in args.fields:
            ivre.flow.validate_field(field)
    if args.precision == 0:
        out.writelines(('%d\n' % precision for precision in db.flow.list_precisions()))
        sys.exit(0)
    filters = {'nodes': args.node_filters or [], 'edges': args.flow_filters or []}
    args_dict = vars(args)
    for key in addr_fields:
        if args_dict[key] is not None:
            (flt_t, flt_v) = get_addr_argument(key, args_dict[key])
            filters[flt_t].append(flt_v)
    if args.proto is not None:
        filters['edges'].append('proto = %s' % args.proto)
    for key in ['tcp', 'udp']:
        if args_dict[key]:
            filters['edges'].append('proto = %s' % key)
    for key in ['port', 'dport']:
        if args_dict[key] is not None:
            filters['edges'].append('dport = %d' % args_dict[key])
    if args.sport is not None:
        filters['edges'].append('ANY sports = %d' % args.sport)
    time_args = ['before', 'after']
    time_values = {}
    for arg in time_args:
        time_values[arg] = datetime.datetime.strptime(args_dict[arg], '%Y-%m-%d %H:%M') if args_dict[arg] is not None else None
    query = db.flow.from_filters(filters, limit=args.limit, skip=args.skip, orderby=args.orderby, mode=args.mode, timeline=args.timeline, after=time_values['after'], before=time_values['before'], precision=args.precision)
    if args.reduce_precision:
        if os.isatty(sys.stdin.fileno()):
            out.write('This will permanently reduce the precision of your database. Process ? [y/N] ')
            ans = input()
            if ans.lower() != 'y':
                sys.exit(-1)
        new_precision = args.reduce_precision
        db.flow.reduce_precision(new_precision, flt=query, before=time_values['before'], after=time_values['after'], current_precision=args.precision)
        sys.exit(0)
    sep = args.separator or ' | '
    coma = ' ;' if args.separator else ' ; '
    coma2 = ',' if args.separator else ', '
    if args.count:
        count = db.flow.count(query)
        out.write('%(clients)d clients\n%(servers)d servers\n%(flows)d flows\n' % count)
    elif args.top:
        top = db.flow.topvalues(query, args.top, collect_fields=args.collect, sum_fields=args.sum, topnbr=args.limit, skip=args.skip, least=args.least)
        for rec in top:
            sys.stdout.write('%s%s%s%s%s\n' % ('(' + coma2.join((str(val) for val in rec['fields'])) + ')', sep, rec['count'], sep, coma.join((str('(' + coma2.join((str(val) for val in collected)) + ')') for collected in rec['collected'])) if rec['collected'] else ''))
    elif args.flow_daily:
        precision = args.precision if args.precision is not None else config.FLOW_TIME_PRECISION
        plot_data: Dict[str, Dict[datetime.datetime, int]] = {}
        for rec in db.flow.flow_daily(precision, query, after=time_values['after'], before=time_values['before']):
            out.write(sep.join([rec['time_in_day'].strftime('%T.%f'), ' ; '.join(['(' + x[0] + ', ' + str(x[1]) + ')' for x in rec['flows']])]))
            out.write('\n')
            if args.plot:
                for flw in rec['flows']:
                    t = rec['time_in_day']
                    dt = datetime.datetime(1970, 1, 1, hour=t.hour, minute=t.minute, second=t.second)
                    plot_data.setdefault(flw[0], {})
                    plot_data[flw[0]][dt] = flw[1]
        if args.plot and plot_data:
            t = datetime.datetime(1970, 1, 1, 0, 0, 0)
            t += datetime.timedelta(seconds=config.FLOW_TIME_BASE % precision)
            times = []
            while t < datetime.datetime(1970, 1, 2):
                times.append(t)
                t = t + datetime.timedelta(seconds=precision)
            ax = plt.subplots()[1]
            fmt = matplotlib.dates.DateFormatter('%H:%M:%S')
            for (flow, data) in plot_data.items():
                values = [data[ti] if ti in data else 0 for ti in times]
                plt.step(times, values, '.-', where='post', label=flow)
            plt.legend(loc='best')
            ax.xaxis.set_major_formatter(fmt)
            plt.gcf().autofmt_xdate()
            plt.show()
    else:
        fmt = '%%s%s%%s%s%%s' % (sep, sep)
        node_width = len('XXXX:XXXX:XXXX:XXXX:XXXX:XXXX')
        flow_width = len('tcp/XXXXX')
        for res in db.flow.to_iter(query, limit=args.limit, skip=args.skip, orderby=args.orderby, mode=args.mode, timeline=args.timeline):
            if args.json:
                out.write('%s\n' % res)
            else:
                elts = {}
                for elt in ['src', 'flow', 'dst']:
                    elts[elt] = res[elt]['label']
                    if args.fields:
                        elts[elt] = '%s%s%s' % (elts[elt], coma, coma.join((str(res[elt]['data'].get(field, '')) for field in args.fields)))
                (src, flow, dst) = (elts['src'], elts['flow'], elts['dst'])
                node_width = max(node_width, len(src), len(dst))
                flow_width = max(flow_width, len(flow))
                if not args.separator:
                    fmt = '%%-%ds%s%%-%ds%s%%-%ds' % (node_width, sep, flow_width, sep, node_width)
                out.write(fmt % (src, flow, dst))
                if args.timeline:
                    out.write(sep)
                    try:
                        out.write(coma.join((str(elt) for elt in sorted(res['flow']['data']['meta']['times']))))
                    except KeyError:
                        out.write('?')
                out.write('\n')","args_dict = vars(args)
for key in addr_fields:
    if args_dict[key] is not None:
        (flt_t, flt_v) = get_addr_argument(key, args_dict[key])
        filters[flt_t].append(flt_v)
if args.proto is not None:
    filters['edges'].append('proto = %s' % args.proto)
for key in ['tcp', 'udp']:
    if args_dict[key]:
        filters['edges'].append('proto = %s' % key)
for key in ['port', 'dport']:
    if args_dict[key] is not None:
        filters['edges'].append('dport = %d' % args_dict[key])
if args.sport is not None:
    filters['edges'].append('ANY sports = %d' % args.sport)
time_args = ['before', 'after']
time_values = {}
for arg in time_args:
    time_values[arg] = datetime.datetime.strptime(args_dict[arg], '%Y-%m-%d %H:%M') if args_dict[arg] is not None else None","filters = {""nodes"": args.node_filters or [], ""edges"": args.flow_filters or []}

args_dict = vars(args)

filters[""edges""].extend([""proto = %s"" % args.proto] if args.proto is not None else [])
filters[""edges""].extend([""proto = %s"" % key] for key in [""tcp"", ""udp""] if args_dict[key])
filters[""edges""].extend([""dport",find_wrong,-1,,,
Hierarchical-Localization,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Hierarchical-Localization/hloc/match_features.py,https://github.com/cvg/Hierarchical-Localization/tree/master/hloc/match_features.py,FeaturePairsDataset,__getitem__$104,"def __getitem__(self, idx):
    (name0, name1) = self.pairs[idx]
    data = {}
    with h5py.File(self.feature_path_q, 'r') as fd:
        grp = fd[name0]
        for (k, v) in grp.items():
            data[k + '0'] = torch.from_numpy(v.__array__()).float()
        data['image0'] = torch.empty((1,) + tuple(grp['image_size'])[::-1])
    with h5py.File(self.feature_path_r, 'r') as fd:
        grp = fd[name1]
        for (k, v) in grp.items():
            data[k + '1'] = torch.from_numpy(v.__array__()).float()
        data['image1'] = torch.empty((1,) + tuple(grp['image_size'])[::-1])
    return data","with h5py.File(self.feature_path_q, 'r') as fd:
    grp = fd[name0]
    for (k, v) in grp.items():
        data[k + '0'] = torch.from_numpy(v.__array__()).float()
    data['image0'] = torch.empty((1,) + tuple(grp['image_size'])[::-1])
with h5py.File(self.feature_path_r, 'r') as fd:
    grp = fd[name1]
    for (k, v) in grp.items():
        data[k + '1'] = torch.from_numpy(v.__array__()).float()
    data['image1'] = torch.empty((1,) + tuple(grp['image_size'])[::-1])","data = {k + '0': torch.from_numpy(v.__array__()).float() for (k, v) in h5py.File(self.feature_path_q, 'r')[name0].items()}
data['image0'] = torch.empty((1,) + tuple(h5py.File(self.feature_path_q, 'r')[name0]['image_size'])[::-1])
data.update({k + '1': torch.from_numpy(v.__array__()).float() for (k, v) in h5py.File(self.feature_path_r, 'r')[name1].items()})
data['image1'] = torch.empty((1,) + tuple(h5py.File(self.feature_path_r, 'r')[name1]['image_size'])[::-1])",find_wrong,-1,,,
pyinfra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyinfra/pyinfra/facts/hardware.py,https://github.com/Fizzadar/pyinfra/tree/master/pyinfra/facts/hardware.py,,_parse_regexes$134,"def _parse_regexes(regexes, lines):
    data = {'ipv4': {}, 'ipv6': {}}
    for line in lines:
        for (regex, groups) in regexes:
            matches = re.match(regex, line)
            if matches:
                ip_data = {}
                for (i, group) in enumerate(groups[1:]):
                    ip_data[group] = matches.group(i + 1)
                if 'mask_bits' in ip_data:
                    ip_data['mask_bits'] = int(ip_data['mask_bits'])
                target_group = data[groups[0]]
                if target_group.get('address'):
                    target_group.setdefault('additional_ips', []).append(ip_data)
                else:
                    target_group.update(ip_data)
                break
    return data","""ipv4"": {},
    ""ipv6"": {},
}

for line in lines:
    for regex, groups in regexes:
        matches = re.match(regex, line)
        if matches:
            ip_data = {}

            for i, group in enumerate(groups[1:]):
                ip_data[group] = matches.group(i + 1)

            if ""mask_bits"" in ip_data:
                ip_data[""mask_bits""] = int(ip_data[""mask_bits""])

            target_group = data[groups[0]]
            if target_group.get(""address""):
                target_group.setdefault(""additional_ips"", []).append(ip_data)
            else:
                target_group.update(ip_data)

            break","data = {ip_type: {} for ip_type in ['ipv4', 'ipv6']}
for line in lines:
    matches = [re.match(regex, line) for (regex, groups) in regexes]
    matches = [match for match in matches if match is not None]
    for match in matches:
        ip_data = {group: match.group(i + 1) for (i, group) in enumerate(groups[1:])}
        if 'mask_bits' in ip_data:
            ip_data['mask_bits'] = int(ip_data['mask_bits'])
        target_group = data[groups[0]]
        if target_group.get('address'):
            target_group.setdefault('additional_ips', []).append(ip_data)
        else:
            target_group.update(ip_data)",find_wrong,-1,,,
sentry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/api/serializers/models/alert_rule.py,https://github.com/getsentry/sentry/tree/master/src/sentry/api/serializers/models/alert_rule.py,AlertRuleSerializer,get_attrs$33,"def get_attrs(self, item_list, user, **kwargs):
    alert_rules = {item.id: item for item in item_list}
    prefetch_related_objects(item_list, 'snuba_query__environment')
    result = defaultdict(dict)
    triggers = AlertRuleTrigger.objects.filter(alert_rule__in=item_list).order_by('label')
    serialized_triggers = serialize(list(triggers), **kwargs)
    trigger_actions = AlertRuleTriggerAction.objects.filter(alert_rule_trigger__alert_rule_id__in=alert_rules.keys()).exclude(sentry_app_config__isnull=True, sentry_app_id__isnull=True)
    sentry_app_installations_by_sentry_app_id = SentryAppInstallation.objects.get_related_sentry_app_components(organization_ids={alert_rule.organization_id for alert_rule in alert_rules.values()}, sentry_app_ids=trigger_actions.values_list('sentry_app_id', flat=True), type='alert-rule-action')
    for (trigger, serialized) in zip(triggers, serialized_triggers):
        alert_rule_triggers = result[alert_rules[trigger.alert_rule_id]].setdefault('triggers', [])
        for action in serialized.get('actions', []):
            install = sentry_app_installations_by_sentry_app_id.get(action.get('sentryAppId'))
            if install:
                action['_sentry_app_component'] = install.get('sentry_app_component')
                action['_sentry_app_installation'] = install.get('sentry_app_installation')
                action['sentryAppInstallationUuid'] = install.get('sentry_app_installation').get('uuid')
        alert_rule_triggers.append(serialized)
    alert_rule_projects = AlertRule.objects.filter(id__in=[item.id for item in item_list]).values_list('id', 'snuba_query__subscriptions__project__slug')
    for (alert_rule_id, project_slug) in alert_rule_projects:
        rule_result = result[alert_rules[alert_rule_id]].setdefault('projects', [])
        rule_result.append(project_slug)
    for rule_activity in AlertRuleActivity.objects.filter(alert_rule__in=item_list, type=AlertRuleActivityType.CREATED.value).select_related('alert_rule', 'user'):
        if rule_activity.user:
            user = {'id': rule_activity.user.id, 'name': rule_activity.user.get_display_name(), 'email': rule_activity.user.email}
        else:
            user = None
        result[alert_rules[rule_activity.alert_rule.id]].update({'created_by': user})
    resolved_actors = {}
    owners_by_type = defaultdict(list)
    for item in item_list:
        if item.owner_id is not None:
            owners_by_type[actor_type_to_string(item.owner.type)].append(item.owner_id)
    for (k, v) in ACTOR_TYPES.items():
        resolved_actors[k] = {a.actor_id: a.id for a in actor_type_to_class(v).objects.filter(actor_id__in=owners_by_type[k])}
    for alert_rule in alert_rules.values():
        if alert_rule.owner_id:
            type = actor_type_to_string(alert_rule.owner.type)
            if alert_rule.owner_id in resolved_actors[type]:
                result[alert_rule]['owner'] = f'{type}:{resolved_actors[type][alert_rule.owner_id]}'
    if 'original_alert_rule' in self.expand:
        snapshot_activities = AlertRuleActivity.objects.filter(alert_rule__in=item_list, type=AlertRuleActivityType.SNAPSHOT.value)
        for activity in snapshot_activities:
            result[alert_rules[activity.alert_rule_id]]['originalAlertRuleId'] = activity.previous_alert_rule_id
    if 'latestIncident' in self.expand:
        incident_map = {}
        for incident in Incident.objects.filter(id__in=Incident.objects.filter(alert_rule__in=alert_rules).values('alert_rule_id').annotate(incident_id=Max('id')).values('incident_id')):
            incident_map[incident.alert_rule_id] = serialize(incident, user=user)
        for alert_rule in alert_rules.values():
            result[alert_rule]['latestIncident'] = incident_map.get(alert_rule.id, None)
    return result","for (trigger, serialized) in zip(triggers, serialized_triggers):
    alert_rule_triggers = result[alert_rules[trigger.alert_rule_id]].setdefault('triggers', [])
    for action in serialized.get('actions', []):
        install = sentry_app_installations_by_sentry_app_id.get(action.get('sentryAppId'))
        if install:
            action['_sentry_app_component'] = install.get('sentry_app_component')
            action['_sentry_app_installation'] = install.get('sentry_app_installation')
            action['sentryAppInstallationUuid'] = install.get('sentry_app_installation').get('uuid')
    alert_rule_triggers.append(serialized)","result = {alert_rules[trigger.alert_rule_id]: {'triggers': [dict(action, **{'_sentry_app_component': install.get('sentry_app_component'), '_sentry_app_installation': install.get('sentry_app_installation'), 'sentryAppInstallationUuid': install.get('sentry_app_installation').get('uuid')}) for action in serialized.get('actions', []) if (install := sentry_app_installations_by_sentry_app_id.get(action.get('sentryAppId')))]} for (trigger, serialized) in zip(triggers, serialized_triggers)}",find_wrong,2,,,
class-balanced-loss,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/class-balanced-loss/tpu/models/official/retinanet/retinanet_architecture.py,https://github.com/richardaecn/class-balanced-loss/tree/master/tpu/models/official/retinanet/retinanet_architecture.py,,resnet_fpn$518,"def resnet_fpn(features, min_level=3, max_level=7, resnet_depth=50, is_training_bn=False, use_nearest_upsampling=True):
    """"""ResNet feature pyramid networks.""""""
    with tf.variable_scope('resnet%s' % resnet_depth):
        resnet_fn = resnet_v1(resnet_depth)
        (u2, u3, u4, u5) = resnet_fn(features, is_training_bn)
    feats_bottom_up = {2: u2, 3: u3, 4: u4, 5: u5}
    with tf.variable_scope('resnet_fpn'):
        feats_lateral = {}
        for level in range(min_level, _RESNET_MAX_LEVEL + 1):
            feats_lateral[level] = tf.layers.conv2d(feats_bottom_up[level], filters=256, kernel_size=(1, 1), padding='same', name='l%d' % level)
        feats = {_RESNET_MAX_LEVEL: feats_lateral[_RESNET_MAX_LEVEL]}
        for level in range(_RESNET_MAX_LEVEL - 1, min_level - 1, -1):
            if use_nearest_upsampling:
                feats[level] = nearest_upsampling(feats[level + 1], 2) + feats_lateral[level]
            else:
                feats[level] = resize_bilinear(feats[level + 1], tf.shape(feats_lateral[level])[1:3], feats[level + 1].dtype) + feats_lateral[level]
        for level in range(min_level, _RESNET_MAX_LEVEL + 1):
            feats[level] = tf.layers.conv2d(feats[level], filters=256, strides=(1, 1), kernel_size=(3, 3), padding='same', name='post_hoc_d%d' % level)
        for level in range(_RESNET_MAX_LEVEL + 1, max_level + 1):
            feats_in = feats[level - 1]
            if level > _RESNET_MAX_LEVEL + 1:
                feats_in = tf.nn.relu(feats_in)
            feats[level] = tf.layers.conv2d(feats_in, filters=256, strides=(2, 2), kernel_size=(3, 3), padding='same', name='p%d' % level)
        for level in range(min_level, max_level + 1):
            feats[level] = tf.layers.batch_normalization(inputs=feats[level], momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True, scale=True, training=is_training_bn, fused=True, name='p%d-bn' % level)
    return feats","2: u2,
      3: u3,
      4: u4,
      5: u5,
  }","feats_bottom_up = {level: feats for (level, feats) in zip(range(2, 6), [u2, u3, u4, u5])}",find_wrong,2,,,
OctoPrint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OctoPrint/src/octoprint/cli/client.py,https://github.com/OctoPrint/OctoPrint/tree/master/src/octoprint/cli/client.py,,command$208,"def command(ctx, path, command, str_params, int_params, float_params, bool_params, timeout):
    """"""Sends a JSON command to the specified server path.""""""
    data = {}
    params = str_params + int_params + float_params + bool_params
    for param in params:
        data[param[0]] = param[1]
    r = ctx.obj.client.post_command(path, command, additional=data, timeout=timeout)
    log_response(r, body=False)","params = str_params + int_params + float_params + bool_params
for param in params:
    data[param[0]] = param[1]",data = {param[0]: param[1] for param in str_params + int_params + float_params + bool_params},find_wrong,1,,,
PMapper,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PMapper/principalmapper/querying/query_interface.py,https://github.com/nccgroup/PMapper/tree/master/principalmapper/querying/query_interface.py,,search_authorization_across_accounts$89,"def search_authorization_across_accounts(graph_scp_pairs: List[Tuple[Graph, Optional[List[List[Policy]]]]], inter_account_edges: List[Edge], principal: Node, action_to_check: str, resource_to_check: str, condition_keys_to_check: _UODict, resource_policy: Optional[dict]=None, resource_owner: Optional[str]=None, session_policy: Optional[dict]=None) -> QueryResult:
    """"""Determines if the passed principal, or any principals it can access, can perform a given action for a
    given resource/condition. Handles an optional resource policy, an optional SCP list, and an optional
    session policy. The session policy is discarded after checking if the passed principal has access
    (we assume it is discarded after each pivot, and that it does NOT affect the accessibility of the edges).

    In `local_check_authorization` we usually throw up our hands if the given principal is an admin. But, because of
    how SCPs work (even blocking the root user), we force the full search to give more accurate results. If the
    SCPs param is None, we assume no SCPs are in place and can make the same assumption as in
    `local_check_authorization`.

    If the resource_owner param is not None, and the resource_owner param is None, the `local_check_authorization_full`
    function that gets called will throw a ValueError, so make sure the resource ownership is sorted before calling
    this method.

    The graphs to include in the search have to be passed in tuples. The second element of the tuple is either the SCPs
    that affect that graph or None. If your graph belongs to an organization, remember that you can take the
    OrganizationTree object and produce the applicable SCPs by calling
    principalmapper.querying.query_orgs.produce_scp_list and passing the graph + org-tree objects.""""""
    account_id_graph_scp_pair_map = {}
    for graph_scp_pair in graph_scp_pairs:
        account_id_graph_scp_pair_map[graph_scp_pair[0].metadata['account_id']] = graph_scp_pair
    source_graph_scp_pair = account_id_graph_scp_pair_map[arns.get_account_id(principal.arn)]
    if local_check_authorization_full(principal, action_to_check, resource_to_check, condition_keys_to_check, resource_policy, resource_owner, source_graph_scp_pair[1], session_policy):
        return QueryResult(True, [], principal)
    if source_graph_scp_pair[1] is None and principal.is_admin and (resource_owner == arns.get_account_id(principal.arn)):
        return QueryResult(True, principal, principal)
    for edge_list in query_utils.get_interaccount_search_list([x[0] for x in graph_scp_pairs], inter_account_edges, principal):
        proxy_principal = edge_list[-1].destination
        proxy_principal_scps = account_id_graph_scp_pair_map[arns.get_account_id(proxy_principal.arn)][1]
        if local_check_authorization_full(edge_list[-1].destination, action_to_check, resource_to_check, condition_keys_to_check, resource_policy, resource_owner, proxy_principal_scps, None):
            return QueryResult(True, edge_list, principal)
    return QueryResult(False, [], principal)","for graph_scp_pair in graph_scp_pairs:
    account_id_graph_scp_pair_map[graph_scp_pair[0].metadata['account_id']] = graph_scp_pair
source_graph_scp_pair = account_id_graph_scp_pair_map[arns.get_account_id(principal.arn)]","account_id_graph_scp_pair_map = {graph_scp_pair[0].metadata['account_id']: graph_scp_pair for graph_scp_pair in graph_scp_pairs}
source_graph_scp_pair = account_id_graph_scp_pair_map[arns.get_account_id(principal.arn)]",find_wrong,1,,,
fake-bpy-module,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fake-bpy-module/src/fake_bpy_module/generator.py,https://github.com/nutti/fake-bpy-module/tree/master/src/fake_bpy_module/generator.py,BaseGenerator,_sorted_generation_info$292,"def _sorted_generation_info(self, data: 'GenerationInfoByTarget') -> List[Info]:
    class_data: List[ClassInfo] = []
    function_data: List[FunctionInfo] = []
    constant_data: List[VariableInfo] = []
    high_priority_class_data: List[ClassInfo] = []
    for d in data.data:
        if d.type() == 'class':
            if d.name() in ['bpy_prop_collection', 'bpy_struct']:
                high_priority_class_data.append(d)
            else:
                class_data.append(d)
        elif d.type() == 'function':
            function_data.append(d)
        elif d.type() == 'constant':
            constant_data.append(d)
        else:
            raise ValueError(f'Invalid data type. ({d.type})')
    class_data = high_priority_class_data + sorted(class_data, key=lambda x: x.name())
    graph = DAG()
    class_name_to_nodes = OrderedDict()
    for class_ in class_data:
        class_name_to_nodes[class_.name()] = graph.make_node(class_)
    for class_ in class_data:
        class_node = class_name_to_nodes[class_.name()]
        for base_class in class_.base_classes():
            if base_class.type() == 'MIXIN':
                raise ValueError('DataType of base class must not be MixinDataType.')
            if base_class.type() == 'UNKNOWN':
                continue
            if base_class.data_type() == class_.name():
                output_log(LOG_LEVEL_DEBUG, f'Self dependency {base_class.data_type()} is found.')
                continue
            base_class_node = class_name_to_nodes.get(base_class.data_type())
            if base_class_node:
                graph.make_edge(base_class_node, class_node)
            else:
                output_log(LOG_LEVEL_WARN, f'Base class node (type={base_class.data_type()}) is not found')
    sorted_nodes = topological_sort(graph)
    sorted_class_data = [node.data() for node in sorted_nodes]
    order = {}
    for (i, class_) in enumerate(sorted_class_data):
        order[class_.name()] = i
    for class_ in sorted_class_data:

        def sort_func(x):
            if x.type() == 'UNKNOWN':
                return 0
            if x.data_type() not in order:
                return 0
            return -order[x.data_type()]
        new_base_classes = sorted(class_.base_classes(), key=sort_func)
        for (i, c) in enumerate(new_base_classes):
            class_.set_base_class(i, c)
    sorted_function_data = sorted(function_data, key=lambda x: x.name())
    sorted_constant_data = sorted(constant_data, key=lambda x: x.name())
    sorted_data = sorted_class_data
    sorted_data.extend(sorted_function_data)
    sorted_data.extend(sorted_constant_data)
    return sorted_data","function_data: List[FunctionInfo] = []
constant_data: List[VariableInfo] = []
high_priority_class_data: List[ClassInfo] = []
for d in data.data:
    if d.type() == 'class':
        if d.name() in ['bpy_prop_collection', 'bpy_struct']:
            high_priority_class_data.append(d)
        else:
            class_data.append(d)
    elif d.type() == 'function':
        function_data.append(d)
    elif d.type() == 'constant':
        constant_data.append(d)
    else:
        raise ValueError(f'Invalid data type. ({d.type})')","class_data = [d for d in data.data if d.type() == 'class' and d.name() not in ['bpy_prop_collection', 'bpy_struct']]
high_priority_class_data = [d for d in data.data if d.type() == 'class' and d.name() in ['bpy_prop_collection', 'bpy_struct']]
function_data = [d for d in data.data if d.type() == 'function']
constant_data = [d for d in data.data if d.type() == 'constant']",find_wrong,2,,,
TensorNetwork,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorNetwork/tensornetwork/tn_keras/entangler.py,https://github.com/google/TensorNetwork/tree/master/tensornetwork/tn_keras/entangler.py,DenseEntangler,get_config$192,"def get_config(self) -> dict:
    """"""Returns the config of the layer.

    The same layer can be reinstantiated later
    (without its trained weights) from this configuration.

    Returns:
      Python dictionary containing the configuration of the layer.
    """"""
    config = {}
    args = ['output_dim', 'num_legs', 'num_levels', 'use_bias']
    for arg in args:
        config[arg] = getattr(self, arg)
    config['activation'] = activations.serialize(getattr(self, 'activation'))
    layer_initializers = ['kernel_initializer', 'bias_initializer']
    for initializer_arg in layer_initializers:
        config[initializer_arg] = initializers.serialize(getattr(self, initializer_arg))
    base_config = super().get_config()
    return dict(list(base_config.items()) + list(config.items()))","args = ['output_dim', 'num_legs', 'num_levels', 'use_bias']
for arg in args:
    config[arg] = getattr(self, arg)
config['activation'] = activations.serialize(getattr(self, 'activation'))
layer_initializers = ['kernel_initializer', 'bias_initializer']
for initializer_arg in layer_initializers:
    config[initializer_arg] = initializers.serialize(getattr(self, initializer_arg))","config = {arg: getattr(self, arg) for arg in ['output_dim', 'num_legs', 'num_levels', 'use_bias']}
config['activation'] = activations.serialize(getattr(self, 'activation'))
config.update({initializer_arg: initializers.serialize(getattr(self, initializer_arg)) for initializer_arg in ['kernel_initializer', 'bias_initializer']})",find_wrong,,,,
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/c7n/resources/quotas.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/quotas.py,ServiceQuota,augment$52,"def augment(self, resources):
    client = local_session(self.session_factory).client('service-quotas')
    retry = get_retry(('TooManyRequestsException',))

    def get_quotas(client, s):
        quotas = {}
        token = None
        kwargs = {'ServiceCode': s['ServiceCode'], 'MaxResults': self.batch_size}
        while True:
            if token:
                kwargs['NextToken'] = token
            response = retry(client.list_service_quotas, **kwargs)
            rquotas = {q['QuotaCode']: q for q in response['Quotas']}
            token = response.get('NextToken')
            new = set(rquotas) - set(quotas)
            quotas.update(rquotas)
            if token is None:
                break
            elif token and (not new):
                break
        return quotas.values()
    results = []
    with self.executor_factory(max_workers=self.max_workers) as w:
        futures = {}
        for r in resources:
            futures[w.submit(get_quotas, client, r)] = r
        for f in as_completed(futures):
            if f.exception():
                raise f.exception()
            results.extend(f.result())
    return results","for q in response['Quotas']:
    quotas[q['QuotaCode']] = q
return quotas.values()",return [q for q in response['Quotas']],find_wrong,2,,,
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/appconfig/feature.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/appconfig/feature.py,,list_feature$260,"def list_feature(cmd, feature=None, key=None, name=None, label=None, fields=None, connection_string=None, top=None, all_=False, auth_mode='key', endpoint=None):
    if key and feature:
        logger.warning('Since both `--key` and `--feature` are provided, `--feature` argument will be ignored.')
    if key is not None:
        key_filter = key
    elif feature is not None:
        key_filter = FeatureFlagConstants.FEATURE_FLAG_PREFIX + feature
    else:
        key_filter = FeatureFlagConstants.FEATURE_FLAG_PREFIX + SearchFilterOptions.ANY_KEY
    azconfig_client = get_appconfig_data_client(cmd, name, connection_string, auth_mode, endpoint)
    try:
        retrieved_keyvalues = __list_all_keyvalues(azconfig_client, key_filter=key_filter, label=label if label else SearchFilterOptions.ANY_LABEL)
        retrieved_featureflags = []
        for kv in retrieved_keyvalues:
            retrieved_featureflags.append(map_keyvalue_to_featureflag(keyvalue=kv, show_conditions=True))
        filtered_featureflags = []
        count = 0
        if all_:
            top = len(retrieved_featureflags)
        elif top is None:
            top = 100
        for featureflag in retrieved_featureflags:
            if fields:
                partial_featureflags = {}
                for field in fields:
                    partial_featureflags[field.name.lower()] = getattr(featureflag, field.name.lower())
                filtered_featureflags.append(partial_featureflags)
            else:
                filtered_featureflags.append(featureflag)
            count += 1
            if count >= top:
                break
        return filtered_featureflags
    except Exception as exception:
        raise CLIError(str(exception))","for kv in retrieved_keyvalues:
    retrieved_featureflags.append(map_keyvalue_to_featureflag(keyvalue=kv, show_conditions=True))
filtered_featureflags = []
count = 0
for featureflag in retrieved_featureflags:
    if fields:
        partial_featureflags = {}
        for field in fields:
            partial_featureflags[field.name.lower()] = getattr(featureflag, field.name.lower())
        filtered_featureflags.append(partial_featureflags)
    else:
        filtered_featureflags.append(featureflag)
    count += 1
    if count >= top:
        break","filtered_featureflags = [{field.name.lower(): getattr(featureflag, field.name.lower()) for field in fields} if fields else featureflag for featureflag in retrieved_featureflags[:top]]",find_wrong,-1,,,
Malt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Malt/Malt/Parameter.py,https://github.com/bnpr/Malt/tree/master/Malt/Parameter.py,PythonPipelineGraph,__init__$55,"def __init__(self, pipeline, function_nodes, graph_io_reflection):
    self.pipeline = pipeline
    self.node_instances = {}
    self.nodes = {}
    functions = {}
    for node_class in function_nodes:
        reflection = node_class.reflect()
        functions[reflection['name']] = reflection
        self.nodes[reflection['name']] = node_class
    graph_io = {}
    for node in graph_io_reflection:
        graph_io[node['name']] = node
    super().__init__('Python', '-render_layer.py', functions, {}, graph_io)","for node_class in function_nodes:
    reflection = node_class.reflect()
    functions[reflection['name']] = reflection
    self.nodes[reflection['name']] = node_class
graph_io = {}
for node in graph_io_reflection:
    graph_io[node['name']] = node","functions = {node_class.reflect()['name']: node_class.reflect() for node_class in function_nodes}
self.nodes = {node_class.reflect()['name']: node_class for node_class in function_nodes}
graph_io = {node['name']: node for node in graph_io_reflection}",find_wrong,,,,
django-role-permissions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-role-permissions/rolepermissions/permissions.py,https://github.com/vintasoftware/django-role-permissions/tree/master/rolepermissions/permissions.py,,available_perm_status$42,"def available_perm_status(user):
    """"""
    Get a boolean map of the permissions available to a user
    based on that user's roles.
    """"""
    roles = get_user_roles(user)
    permission_hash = {}
    user_permission_names = set(user.user_permissions.values_list('codename', flat=True))
    for role in roles:
        for permission_name in role.permission_names_list():
            permission_hash[permission_name] = permission_name in user_permission_names
    return permission_hash","user_permission_names = set(user.user_permissions.values_list('codename', flat=True))
for role in roles:
    for permission_name in role.permission_names_list():
        permission_hash[permission_name] = permission_name in user_permission_names","permission_hash = {permission_name: permission_name in set(user.user_permissions.values_list('codename', flat=True)) for role in roles for permission_name in role.permission_names_list()}",find_wrong,1,,,
timesketch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/timesketch/timesketch/lib/aggregators_old.py,https://github.com/google/timesketch/tree/master/timesketch/lib/aggregators_old.py,,heatmap$25,"def heatmap(es_client, sketch_id, query_string, query_filter, query_dsl, indices):
    """"""Aggregate query results into number of events per hour/day.

    Args:
        es_client: Elasticsearch client (instance of ElasticSearchDatastore)
        sketch_id: Integer of sketch primary key
        query_string: Query string
        query_filter: Dictionary containing filters to apply
        query_dsl: Dictionary containing Elasticsearch DSL to apply
        indices: List of indices to query

    returns:
        List of events per hour/day
    """"""
    query_filter.pop('size', None)
    query_filter.pop('from', None)
    result_count = es_client.search(sketch_id, query_string, query_filter, query_dsl, indices, count=True)
    if result_count > MAX_RESULT_LIMIT or result_count == 0:
        return []
    days_map = {'Mon': 1, 'Tue': 2, 'Wed': 3, 'Thu': 4, 'Fri': 5, 'Sat': 6, 'Sun': 7}
    if es_client.version.startswith('5.'):
        source_script = 'new SimpleDateFormat(params.format).format(new Date(doc[""datetime""].value))'
    else:
        source_script = 'doc[""datetime""].value.toString(params.format);'
    aggregation = {'byDay': {'terms': {'script': {'source': source_script, 'lang': 'painless', 'params': {'format': 'E'}}}, 'aggs': {'byHour': {'terms': {'order': {'_term': 'asc'}, 'script': {'source': source_script, 'lang': 'painless', 'params': {'format': 'H'}}, 'size': 24}}}}}
    search_result = es_client.search(sketch_id, query_string, query_filter, query_dsl, indices, aggregations=aggregation, return_fields=None, enable_scroll=False)
    try:
        aggregation_result = search_result['aggregations']
        day_buckets = aggregation_result['byDay']['buckets']
    except KeyError:
        day_buckets = []
    per_hour = {}
    for day in range(1, 8):
        for hour in range(0, 24):
            per_hour[day, hour] = 0
    for day_bucket in day_buckets:
        day = days_map[day_bucket.get('key')]
        day_hours = day_bucket['byHour']['buckets']
        for day_hour in day_hours:
            hour = int(day_hour['key'])
            count = day_hour['doc_count']
            per_hour[day, int(hour)] = count
    return [dict(day=k[0], hour=k[1], count=v) for (k, v) in per_hour.items()]","'Mon': 1,
    'Tue': 2,
    'Wed': 3,
    'Thu': 4,
    'Fri': 5,
    'Sat': 6,
    'Sun': 7,
}","days_map = {day: i + 1 for (i, day) in enumerate(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])}",find_wrong,2,,,
KBQA-BERT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/KBQA-BERT/run_ner.py,https://github.com/WenRichard/KBQA-BERT/tree/master//run_ner.py,,convert_single_example$252,"def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, mode):
    """"""
    灏嗕竴涓鏍锋湰杩涜屽垎鏋愶紝鐒跺悗灏嗗瓧杞鍖栦负id, 鏍囩捐浆鍖栦负id,鐒跺悗缁撴瀯鍖栧埌InputFeatures瀵硅薄涓
    :param ex_index: index
    :param example: 涓涓鏍锋湰
    :param label_list: 鏍囩惧垪琛
    :param max_seq_length:
    :param tokenizer:
    :param mode:
    :return:
    """"""
    label_map = {}
    for (i, label) in enumerate(label_list, 1):
        label_map[label] = i
    if not os.path.exists(os.path.join(FLAGS.output_dir, 'label2id.pkl')):
        with codecs.open(os.path.join(FLAGS.output_dir, 'label2id.pkl'), 'wb') as w:
            pickle.dump(label_map, w)
    textlist = example.text.split(' ')
    labellist = example.label.split(' ')
    tokens = []
    labels = []
    for (i, word) in enumerate(textlist):
        token = tokenizer.tokenize(word)
        tokens.extend(token)
        label_1 = labellist[i]
        for m in range(len(token)):
            if m == 0:
                labels.append(label_1)
            else:
                labels.append('X')
    if len(tokens) >= max_seq_length - 1:
        tokens = tokens[0:max_seq_length - 2]
        labels = labels[0:max_seq_length - 2]
    ntokens = []
    segment_ids = []
    label_ids = []
    ntokens.append('[CLS]')
    segment_ids.append(0)
    label_ids.append(label_map['[CLS]'])
    for (i, token) in enumerate(tokens):
        ntokens.append(token)
        segment_ids.append(0)
        label_ids.append(label_map[labels[i]])
    ntokens.append('[SEP]')
    segment_ids.append(0)
    label_ids.append(label_map['[SEP]'])
    input_ids = tokenizer.convert_tokens_to_ids(ntokens)
    input_mask = [1] * len(input_ids)
    while len(input_ids) < max_seq_length:
        input_ids.append(0)
        input_mask.append(0)
        segment_ids.append(0)
        label_ids.append(0)
        ntokens.append('**NULL**')
    assert len(input_ids) == max_seq_length
    assert len(input_mask) == max_seq_length
    assert len(segment_ids) == max_seq_length
    assert len(label_ids) == max_seq_length
    if ex_index < 5:
        tf.logging.info('*** Example ***')
        tf.logging.info('guid: %s' % example.guid)
        tf.logging.info('tokens: %s' % ' '.join([tokenization.printable_text(x) for x in tokens]))
        tf.logging.info('input_ids: %s' % ' '.join([str(x) for x in input_ids]))
        tf.logging.info('input_mask: %s' % ' '.join([str(x) for x in input_mask]))
        tf.logging.info('segment_ids: %s' % ' '.join([str(x) for x in segment_ids]))
        tf.logging.info('label_ids: %s' % ' '.join([str(x) for x in label_ids]))
    feature = InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_ids=label_ids)
    write_tokens(ntokens, mode)
    return feature","for (i, label) in enumerate(label_list, 1):
    label_map[label] = i","label_map = {label: i for (i, label) in enumerate(label_list, 1)}",find_wrong,1,,,
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/manufacturing/doctype/work_order/test_work_order.py,https://github.com/frappe/erpnext/tree/master/erpnext/manufacturing/doctype/work_order/test_work_order.py,,get_scrap_item_details$1737,"def get_scrap_item_details(bom_no):
    scrap_items = {}
    for item in frappe.db.sql('select item_code, stock_qty from `tabBOM Scrap Item`\n\t\twhere parent = %s', bom_no, as_dict=1):
        scrap_items[item.item_code] = item.stock_qty
    return scrap_items","for item in frappe.db.sql('select item_code, stock_qty from `tabBOM Scrap Item`\n    where parent = %s', bom_no, as_dict=1):
    scrap_items[item.item_code] = item.stock_qty","scrap_items = {item.item_code: item.stock_qty for item in frappe.db.sql('select item_code, stock_qty from `tabBOM Scrap Item` where parent = %s', bom_no, as_dict=1)}",find_wrong,1,,,
EasyMocap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyMocap/easymocap/dataset/base.py,https://github.com/zju3dv/EasyMocap/tree/master/easymocap/dataset/base.py,_VideoBase,vis_smpl$305,"def vis_smpl(self, peopleDict, faces, image, nf, sub_vis=[], mode='smpl', extra_data=[], add_back=True, axis=np.array([1.0, 0.0, 0.0]), degree=0.0, fix_center=None):
    render_data = {}
    for (pid, data) in peopleDict.items():
        render_data[pid] = {'vertices': data.vertices, 'faces': faces, 'vid': pid, 'name': 'human_{}_{}'.format(nf, pid)}
    for (iid, extra) in enumerate(extra_data):
        render_data[10000 + iid] = {'vertices': extra['vertices'], 'faces': extra['faces'], 'colors': extra['colors'], 'name': extra['name']}
    camera = {}
    for key in self.camera.keys():
        camera[key] = self.camera[key][None, :, :]
    if np.abs(degree) > 0.001:
        vertices_all = np.vstack([data.vertices for data in peopleDict.values()])
        if fix_center is None:
            center = np.mean(vertices_all, axis=0, keepdims=True)
            new_center = center.copy()
            new_center[:, 0:2] = 0
        else:
            center = fix_center.copy()
            new_center = fix_center.copy()
            new_center[:, 2] *= 1.5
        direc = np.array(axis)
        (rot, _) = cv2.Rodrigues(direc * degree / 90 * np.pi / 2)
        blank = np.zeros_like(image, dtype=np.uint8) + 255
        images = [image, blank]
        Rnew = camera['R'][0] @ rot
        Tnew = camera['R'][0] @ (new_center.T - rot @ center.T) + camera['T'][0]
        camera['K'] = np.vstack([camera['K'], camera['K']])
        camera['R'] = np.vstack([camera['R'], Rnew[None, :, :]])
        camera['T'] = np.vstack([camera['T'], Tnew[None, :, :]])
    else:
        images = [image]
    self.writer.vis_smpl(render_data, nf, images, camera, mode, add_back=add_back)","for (pid, data) in peopleDict.items():
    render_data[pid] = {'vertices': data.vertices, 'faces': faces, 'vid': pid, 'name': 'human_{}_{}'.format(nf, pid)}
for (iid, extra) in enumerate(extra_data):
    render_data[10000 + iid] = {'vertices': extra['vertices'], 'faces': extra['faces'], 'colors': extra['colors'], 'name': extra['name']}","render_data = {pid: {'vertices': data.vertices, 'faces': faces, 'vid': pid, 'name': 'human_{}_{}'.format(nf, pid)} for (pid, data) in peopleDict.items()}
render_data.update({10000 + iid: {'vertices': extra['vertices'], 'faces': extra['faces'], 'colors': extra['colors'], 'name': extra['name']} for (iid, extra) in enumerate(extra_data)})",find_wrong,,,,
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/core/fp16/decorators.py,https://github.com/poodarchu/Det3D/tree/master/det3d/core/fp16/decorators.py,,auto_fp16$9,"def auto_fp16(apply_to=None, out_fp32=False):
    """"""Decorator to enable fp16 training automatically.
    This decorator is useful when you write custom modules and want to support
    mixed precision training. If inputs arguments are fp32 tensors, they will
    be converted to fp16 automatically. Arguments other than fp32 tensors are
    ignored.
    Args:
        apply_to (Iterable, optional): The argument names to be converted.
            `None` indicates all arguments.
        out_fp32 (bool): Whether to convert the output back to fp32.
    :Example:
        class MyModule1(nn.Module)
            # Convert x and y to fp16
            @auto_fp16()
            def forward(self, x, y):
                pass
        class MyModule2(nn.Module):
            # convert pred to fp16
            @auto_fp16(apply_to=('pred', ))
            def do_something(self, pred, others):
                pass
    """"""

    def auto_fp16_wrapper(old_func):

        @functools.wraps(old_func)
        def new_func(*args, **kwargs):
            if not isinstance(args[0], torch.nn.Module):
                raise TypeError('@auto_fp16 can only be used to decorate the method of nn.Module')
            if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
                return old_func(*args, **kwargs)
            args_info = getfullargspec(old_func)
            args_to_cast = args_info.args if apply_to is None else apply_to
            new_args = []
            if args:
                arg_names = args_info.args[:len(args)]
                for (i, arg_name) in enumerate(arg_names):
                    if arg_name in args_to_cast:
                        new_args.append(cast_tensor_type(args[i], torch.float, torch.half))
                    else:
                        new_args.append(args[i])
            new_kwargs = {}
            if kwargs:
                for (arg_name, arg_value) in kwargs.items():
                    if arg_name in args_to_cast:
                        new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
                    else:
                        new_kwargs[arg_name] = arg_value
            output = old_func(*new_args, **new_kwargs)
            if out_fp32:
                output = cast_tensor_type(output, torch.half, torch.float)
            return output
        return new_func
    return auto_fp16_wrapper","if kwargs:
    for (arg_name, arg_value) in kwargs.items():
        if arg_name in args_to_cast:
            new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
        else:
            new_kwargs[arg_name] = arg_value","new_kwargs = {arg_name: cast_tensor_type(arg_value, torch.float, torch.half) if arg_name in args_to_cast else arg_value for (arg_name, arg_value) in kwargs.items()}",find_wrong,-1,,,
ansible-modules-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-core/cloud/docker/docker_container.py,https://github.com/ansible/ansible-modules-core/tree/master/cloud/docker/docker_container.py,TaskParameters,_host_config$898,"def _host_config(self):
    """"""
        Returns parameters used to create a HostConfig object
        """"""
    host_config_params = dict(port_bindings='published_ports', publish_all_ports='publish_all_ports', links='links', privileged='privileged', dns='dns_servers', dns_search='dns_search_domains', binds='volume_binds', volumes_from='volumes_from', network_mode='network_mode', cap_add='capabilities', extra_hosts='etc_hosts', read_only='read_only', ipc_mode='ipc_mode', security_opt='security_opts', ulimits='ulimits', log_config='log_config', mem_limit='memory', memswap_limit='memory_swap', mem_swappiness='memory_swappiness', oom_score_adj='oom_score_adj', shm_size='shm_size', group_add='groups', devices='devices', pid_mode='pid_mode')
    params = dict()
    for (key, value) in host_config_params.iteritems():
        if getattr(self, value, None) is not None:
            params[key] = getattr(self, value)
    if self.restart_policy:
        params['restart_policy'] = dict(Name=self.restart_policy, MaximumRetryCount=self.restart_retries)
    return self.client.create_host_config(**params)","port_bindings='published_ports',
    publish_all_ports='publish_all_ports',
    links='links',
    privileged='privileged',
    dns='dns_servers',
    dns_search='dns_search_domains',
    binds='volume_binds',
    volumes_from='volumes_from',
    network_mode='network_mode',
    cap_add='capabilities',
    extra_hosts='etc_hosts',
    read_only='read_only',
    ipc_mode='ipc_mode',
    security_opt='security_opts',
    ulimits='ulimits',
    log_config='log_config',
    mem_limit='memory',
    memswap_limit='memory_swap',
    mem_swappiness='memory_swappiness',
    oom_score_adj='oom_score_adj',
    shm_size='shm_size',
    group_add='groups',
    devices='devices',
    pid_mode='pid_mode'
)
params = dict()
for key, value in host_config_params.iteritems():
    if getattr(self, value, None) is not None:
        params[key] = getattr(self, value)","params = {key: getattr(self, value) for (key, value) in host_config_params.items() if getattr(self, value, None) is not None}",find_wrong,2,,,
pysystemtrade,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pysystemtrade/sysobjects/production/timed_storage.py,https://github.com/robcarver17/pysystemtrade/tree/master/sysobjects/production/timed_storage.py,timedEntry,_resolve_args_passed_as_star_args$77,"def _resolve_args_passed_as_star_args(self, args: tuple, date: datetime.datetime) -> dict:
    required_args = self.required_argument_names
    try:
        assert len(required_args) == len(args)
    except BaseException:
        raise Exception('Expecting to be passed arguments of length %d to match %s, instead got %d arguments' % (len(required_args), str(required_args), len(args)))
    args_as_dict = {}
    for (arg_name, arg_value) in zip(required_args, args):
        args_as_dict[arg_name] = arg_value
    args_as_dict[DATE_KEY_NAME] = date
    return args_as_dict","for (arg_name, arg_value) in zip(required_args, args):
    args_as_dict[arg_name] = arg_value
args_as_dict[DATE_KEY_NAME] = date","args_as_dict = {arg_name: arg_value for (arg_name, arg_value) in zip(required_args, args)}
args_as_dict[DATE_KEY_NAME] = date",find_wrong,1,,,
qutip,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutip/qutip/qip/qasm.py,https://github.com/qutip/qutip/tree/master/qutip/qip/qasm.py,QasmProcessor,_custom_gate$320,"def _custom_gate(self, qc_temp, gate_call):
    """"""
        Recursively process a custom-defined gate with specified arguments
        to produce a dummy circuit with all the gates in the custom-defined
        gate.

        Parameters
        ----------

        qc_temp: :class:`.QubitCircuit`
            temporary circuit to process custom gate
        gate_call: list of str
            tokens corresponding to gate signature/call
        """"""
    (gate_name, args, regs) = gate_call
    gate = self.qasm_gates[gate_name]
    args_map = {}
    regs_map = {}
    for (i, arg) in enumerate(gate.gate_args):
        args_map[arg] = eval(str(args[i]))
    for (i, reg) in enumerate(gate.gate_regs):
        regs_map[reg] = regs[i]
    for call in gate.gates_inside:
        (name, com_args, com_regs) = call
        for (arg, real_arg) in args_map.items():
            com_args = [command.replace(arg.strip(), str(real_arg)) for command in com_args]
        for (reg, real_reg) in regs_map.items():
            com_regs = [command.replace(reg.strip(), str(real_reg)) for command in com_regs]
        com_args = [eval(arg) for arg in com_args]
        if name in self.predefined_gates:
            qc_temp.user_gates = _get_qiskit_gates()
            com_regs = [int(reg) for reg in com_regs]
            self._add_predefined_gates(qc_temp, name, com_regs, com_args)
        else:
            self._custom_gate(qc_temp, [name, com_args, com_regs])","for (i, arg) in enumerate(gate.gate_args):
    args_map[arg] = eval(str(args[i]))
regs_map = {}
for (i, reg) in enumerate(gate.gate_regs):
    regs_map[reg] = regs[i]","args_map = {arg: eval(str(args[i])) for (i, arg) in enumerate(gate.gate_args)}
regs_map = {reg: regs[i] for (i, reg) in enumerate(gate.gate_regs)}",find_wrong,,,,
django-autofixture,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-autofixture/autofixture/values.py,https://github.com/gregmuellegger/django-autofixture/tree/master/autofixture/values.py,ValuesMetaclass,__new__$5,"def __new__(mcs, name, bases, attrs):
    parent_value_attrs = {}
    for base in bases[::-1]:
        if hasattr(base, '_value_attrs'):
            parent_value_attrs.update(base._value_attrs)
    defined_value_attrs = {}
    for key in attrs:
        if not key.startswith('__'):
            defined_value_attrs[key] = attrs[key]
    for key in defined_value_attrs:
        del attrs[key]
    attrs['_value_attrs'] = {}
    attrs['_value_attrs'].update(parent_value_attrs)
    attrs['_value_attrs'].update(defined_value_attrs)
    return super(ValuesMetaclass, mcs).__new__(mcs, name, bases, attrs)","for base in bases[::-1]:
    if hasattr(base, '_value_attrs'):
        parent_value_attrs.update(base._value_attrs)","parent_value_attrs = {key: value for base in bases[::-1] if hasattr(base, '_value_attrs') for (key, value) in base._value_attrs.items()}",find_wrong,2,,,
pygame_tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygame_tutorials/tilemap/part 19/main.py,https://github.com/kidscancode/pygame_tutorials/tree/master/tilemap/part 19/main.py,Game,load_data$63,"def load_data(self):
    game_folder = path.dirname(__file__)
    img_folder = path.join(game_folder, 'img')
    snd_folder = path.join(game_folder, 'snd')
    music_folder = path.join(game_folder, 'music')
    map_folder = path.join(game_folder, 'maps')
    self.title_font = path.join(img_folder, 'ZOMBIE.TTF')
    self.dim_screen = pg.Surface(self.screen.get_size()).convert_alpha()
    self.dim_screen.fill((0, 0, 0, 180))
    self.map = TiledMap(path.join(map_folder, 'level1.tmx'))
    self.map_img = self.map.make_map()
    self.map.rect = self.map_img.get_rect()
    self.player_img = pg.image.load(path.join(img_folder, PLAYER_IMG)).convert_alpha()
    self.bullet_img = pg.image.load(path.join(img_folder, BULLET_IMG)).convert_alpha()
    self.mob_img = pg.image.load(path.join(img_folder, MOB_IMG)).convert_alpha()
    self.wall_img = pg.image.load(path.join(img_folder, WALL_IMG)).convert_alpha()
    self.wall_img = pg.transform.scale(self.wall_img, (TILESIZE, TILESIZE))
    self.splat = pg.image.load(path.join(img_folder, SPLAT)).convert_alpha()
    self.splat = pg.transform.scale(self.splat, (64, 64))
    self.gun_flashes = []
    for img in MUZZLE_FLASHES:
        self.gun_flashes.append(pg.image.load(path.join(img_folder, img)).convert_alpha())
    self.item_images = {}
    for item in ITEM_IMAGES:
        self.item_images[item] = pg.image.load(path.join(img_folder, ITEM_IMAGES[item])).convert_alpha()
    pg.mixer.music.load(path.join(music_folder, BG_MUSIC))
    self.effects_sounds = {}
    for type in EFFECTS_SOUNDS:
        self.effects_sounds[type] = pg.mixer.Sound(path.join(snd_folder, EFFECTS_SOUNDS[type]))
    self.weapon_sounds = {}
    self.weapon_sounds['gun'] = []
    for snd in WEAPON_SOUNDS_GUN:
        self.weapon_sounds['gun'].append(pg.mixer.Sound(path.join(snd_folder, snd)))
    self.zombie_moan_sounds = []
    for snd in ZOMBIE_MOAN_SOUNDS:
        s = pg.mixer.Sound(path.join(snd_folder, snd))
        s.set_volume(0.2)
        self.zombie_moan_sounds.append(s)
    self.player_hit_sounds = []
    for snd in PLAYER_HIT_SOUNDS:
        self.player_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))
    self.zombie_hit_sounds = []
    for snd in ZOMBIE_HIT_SOUNDS:
        self.zombie_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))","for img in MUZZLE_FLASHES:
    self.gun_flashes.append(pg.image.load(path.join(img_folder, img)).convert_alpha())","self.gun_flashes = [pg.image.load(path.join(img_folder, img)).convert_alpha() for img in MUZZLE_FLASHES]",find_wrong,2,,,
pygame_tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygame_tutorials/tilemap/part 19/main.py,https://github.com/kidscancode/pygame_tutorials/tree/master/tilemap/part 19/main.py,Game,load_data$63,"def load_data(self):
    game_folder = path.dirname(__file__)
    img_folder = path.join(game_folder, 'img')
    snd_folder = path.join(game_folder, 'snd')
    music_folder = path.join(game_folder, 'music')
    map_folder = path.join(game_folder, 'maps')
    self.title_font = path.join(img_folder, 'ZOMBIE.TTF')
    self.dim_screen = pg.Surface(self.screen.get_size()).convert_alpha()
    self.dim_screen.fill((0, 0, 0, 180))
    self.map = TiledMap(path.join(map_folder, 'level1.tmx'))
    self.map_img = self.map.make_map()
    self.map.rect = self.map_img.get_rect()
    self.player_img = pg.image.load(path.join(img_folder, PLAYER_IMG)).convert_alpha()
    self.bullet_img = pg.image.load(path.join(img_folder, BULLET_IMG)).convert_alpha()
    self.mob_img = pg.image.load(path.join(img_folder, MOB_IMG)).convert_alpha()
    self.wall_img = pg.image.load(path.join(img_folder, WALL_IMG)).convert_alpha()
    self.wall_img = pg.transform.scale(self.wall_img, (TILESIZE, TILESIZE))
    self.splat = pg.image.load(path.join(img_folder, SPLAT)).convert_alpha()
    self.splat = pg.transform.scale(self.splat, (64, 64))
    self.gun_flashes = []
    for img in MUZZLE_FLASHES:
        self.gun_flashes.append(pg.image.load(path.join(img_folder, img)).convert_alpha())
    self.item_images = {}
    for item in ITEM_IMAGES:
        self.item_images[item] = pg.image.load(path.join(img_folder, ITEM_IMAGES[item])).convert_alpha()
    pg.mixer.music.load(path.join(music_folder, BG_MUSIC))
    self.effects_sounds = {}
    for type in EFFECTS_SOUNDS:
        self.effects_sounds[type] = pg.mixer.Sound(path.join(snd_folder, EFFECTS_SOUNDS[type]))
    self.weapon_sounds = {}
    self.weapon_sounds['gun'] = []
    for snd in WEAPON_SOUNDS_GUN:
        self.weapon_sounds['gun'].append(pg.mixer.Sound(path.join(snd_folder, snd)))
    self.zombie_moan_sounds = []
    for snd in ZOMBIE_MOAN_SOUNDS:
        s = pg.mixer.Sound(path.join(snd_folder, snd))
        s.set_volume(0.2)
        self.zombie_moan_sounds.append(s)
    self.player_hit_sounds = []
    for snd in PLAYER_HIT_SOUNDS:
        self.player_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))
    self.zombie_hit_sounds = []
    for snd in ZOMBIE_HIT_SOUNDS:
        self.zombie_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))","for snd in WEAPON_SOUNDS_GUN:
    self.weapon_sounds['gun'].append(pg.mixer.Sound(path.join(snd_folder, snd)))","self.weapon_sounds = {'gun': [pg.mixer.Sound(path.join(snd_folder, snd)) for snd in WEAPON_SOUNDS_GUN]}",find_wrong,2,,,
VideoSuperResolution,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VideoSuperResolution/prepare_data.py,https://github.com/LoSealL/VideoSuperResolution/tree/master//prepare_data.py,,main$157,"def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('filter', help='an re pattern to filter candidates.')
    parser.add_argument('--download_dir', type=str, default=_DEFAULT_DOWNLOAD_DIR, help='Specify download directory. [{}]'.format(_DEFAULT_DOWNLOAD_DIR))
    parser.add_argument('--data_dir', type=str, default=_DEFAULT_DATASET_PATH, help='Specify dataset extracted directory. [{}]'.format(_DEFAULT_DATASET_PATH))
    parser.add_argument('--weights_dir', type=str, default=_DEFAULT_WEIGHTS_DIR, help='Specify weights extracted directory. [{}]'.format(_DEFAULT_WEIGHTS_DIR))
    parser.add_argument('-q', '--quiet', action='store_true', help='download quietly')
    (args, _) = parser.parse_known_args()
    Path(args.download_dir).mkdir(exist_ok=True, parents=True)

    def get_leaf(key: str, node: dict):
        for (k, v) in node.items():
            if isinstance(v, dict):
                for (k2, v2) in get_leaf(k, v):
                    yield (Path(key) / k2, v2)
            else:
                yield (Path(key) / k, v)
    need_to_download = {}
    try:
        Path(args.data_dir).mkdir(exist_ok=True, parents=True)
        for (k, v) in get_leaf(args.data_dir, DATASETS):
            if user_input(k.stem, args.quiet, args.filter):
                need_to_download[k] = v
    except (FileNotFoundError, OSError):
        pass
    from VSR.Backend import BACKEND
    for (k, v) in get_leaf(args.weights_dir, WEIGHTS[BACKEND]):
        if user_input(k.stem, args.quiet, args.filter):
            need_to_download[k] = v
    need_to_extract = {}
    for (k, v) in need_to_download.items():
        if v[:4] == 'http':
            need_to_extract[k] = (k.parent, download(k.name, v, args.download_dir))
        else:
            need_to_extract[k] = (k.parent, drive_download(k.name, v, args.download_dir))
    for (k, v) in need_to_extract.values():
        if v is None:
            continue
        ext = Path(v).suffix
        if ext in ('.tar', '.tgz', '.gz', '.bz'):
            open_fn = tarfile.open
            is_match_fn = tarfile.is_tarfile
        elif ext in ('.zip',):
            open_fn = zipfile.ZipFile
            is_match_fn = zipfile.is_zipfile
        else:

            class copy:

                def __init__(self, src):
                    self.src = src

                def __enter__(self):
                    return self

                def __exit__(self, exc_type, exc_val, exc_tb):
                    return

                def extractall(self, dst):
                    import shutil
                    shutil.copy(self.src, dst)
            is_match_fn = lambda x: True
            open_fn = copy
        if is_match_fn(v):
            with open_fn(v) as fd:
                try:
                    fd.extractall(str(k.resolve()))
                except (tarfile.TarError, RuntimeError, KeyboardInterrupt):
                    pass
        else:
            print('[WARN] {} have to be uncompressed manually.'.format(v))","for (k, v) in get_leaf(args.data_dir, DATASETS):
    if user_input(k.stem, args.quiet, args.filter):
        need_to_download[k] = v
from VSR.Backend import BACKEND
for (k, v) in get_leaf(args.weights_dir, WEIGHTS[BACKEND]):
    if user_input(k.stem, args.quiet, args.filter):
        need_to_download[k] = v","need_to_download = {k: v for (k, v) in get_leaf(args.data_dir, DATASETS) if user_input(k.stem, args.quiet, args.filter)}
need_to_download.update({k: v for (k, v) in get_leaf(args.weights_dir, WEIGHTS[BACKEND]) if user_input(k.stem, args.quiet, args.filter)})",find_wrong,,,,
fiftyone,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fiftyone/fiftyone/core/collections.py,https://github.com/voxel51/fiftyone/tree/master/fiftyone/core/collections.py,SampleCollection,_get_dynamic_field_schema$1139,"def _get_dynamic_field_schema(self, schema, prefix, fields=None):
    if fields is not None:
        if etau.is_str(fields):
            fields = {fields}
        else:
            fields = set(fields)
        schema = {k: v for (k, v) in schema.items() if k in fields}
    aggs = []
    paths = []
    for (name, field) in schema.items():
        if isinstance(field, fof.EmbeddedDocumentField):
            path = name
            aggs.append(foa.Schema(prefix + path, dynamic_only=True))
            paths.append(path)
            if issubclass(field.document_type, fol._LABEL_LIST_FIELDS):
                path = name + '.' + field.document_type._LABEL_LIST_FIELD
                aggs.append(foa.Schema(prefix + path, dynamic_only=True))
                paths.append(path)
    fields = {}
    if aggs:
        results = self.aggregate(aggs)
        for (path, schema) in zip(paths, results):
            for (name, field) in schema.items():
                fields[path + '.' + name] = field
    return fields",,"schema = {k: v for (k, v) in schema.items() if fields is None or k in fields or (isinstance(fields, set) and len(fields.intersection(set([k]))) > 0)}",find_wrong,2,,,
keystone,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keystone/keystone/models/revoke_model.py,https://github.com/openstack/keystone/tree/master/keystone/models/revoke_model.py,,blank_token_data$66,"def blank_token_data(issued_at):
    token_data = dict()
    for name in _NAMES:
        token_data[name] = None
    for name in _TOKEN_KEYS:
        token_data[name] = None
    token_data['issued_at'] = issued_at
    return token_data","for name in _NAMES:
    token_data[name] = None
for name in _TOKEN_KEYS:
    token_data[name] = None
token_data['issued_at'] = issued_at","token_data = {name: None for name in _NAMES + _TOKEN_KEYS}
token_data['issued_at'] = issued_at",find_wrong,,,,
django-wiki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-wiki/src/wiki/forms.py,https://github.com/django-wiki/django-wiki/tree/master/src/wiki/forms.py,EditForm,__init__$244,"def __init__(self, request, current_revision, *args, **kwargs):
    self.request = request
    self.no_clean = kwargs.pop('no_clean', False)
    self.preview = kwargs.pop('preview', False)
    self.initial_revision = current_revision
    self.presumed_revision = None
    if current_revision:
        provided_content = True
        content = kwargs.pop('content', None)
        if content is None:
            provided_content = False
            content = current_revision.content
        initial = {'content': content, 'title': current_revision.title, 'current_revision': current_revision.id}
        initial.update(kwargs.get('initial', {}))
        data = None
        if len(args) > 0:
            data = args[0]
            args = args[1:]
        if data is None:
            data = kwargs.get('data', None)
        if data:
            self.presumed_revision = data.get('current_revision', None)
            if not str(self.presumed_revision) == str(self.initial_revision.id):
                newdata = {}
                for (k, v) in data.items():
                    newdata[k] = v
                newdata['current_revision'] = self.initial_revision.id
                if provided_content:
                    self.presumed_revision = self.initial_revision.id
                else:
                    newdata['content'] = simple_merge(content, data.get('content', ''))
                newdata['title'] = current_revision.title
                kwargs['data'] = newdata
            else:
                kwargs['data'] = data
        kwargs['initial'] = initial
    super().__init__(*args, **kwargs)","""content"": content,
    ""title"": current_revision.title,
    ""current_revision"": current_revision.id,
}
initial.update(kwargs.get(""initial"", {}))","initial = {**{'content': content, 'title': current_revision.title, 'current_revision': current_revision.id}, **kwargs.get('initial', {})}",find_wrong,2,,,
elasticdl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/elasticdl/model_zoo/census_dnn_model/census_feature_columns.py,https://github.com/sql-machine-learning/elasticdl/tree/master/model_zoo/census_dnn_model/census_feature_columns.py,,get_feature_input_layers$56,"def get_feature_input_layers():
    feature_input_layers = {}
    for numeric_feature_key in NUMERIC_FEATURE_KEYS:
        feature_input_layers[numeric_feature_key] = tf.keras.Input(shape=(1,), name=numeric_feature_key, dtype=tf.float32)
    for categorical_feature_key in CATEGORICAL_FEATURE_KEYS:
        feature_input_layers[categorical_feature_key] = tf.keras.Input(shape=(1,), name=categorical_feature_key, dtype=tf.string)
    return feature_input_layers","for numeric_feature_key in NUMERIC_FEATURE_KEYS:
    feature_input_layers[numeric_feature_key] = tf.keras.Input(shape=(1,), name=numeric_feature_key, dtype=tf.float32)
for categorical_feature_key in CATEGORICAL_FEATURE_KEYS:
    feature_input_layers[categorical_feature_key] = tf.keras.Input(shape=(1,), name=categorical_feature_key, dtype=tf.string)","feature_input_layers = {numeric_feature_key: tf.keras.Input(shape=(1,), name=numeric_feature_key, dtype=tf.float32) for numeric_feature_key in NUMERIC_FEATURE_KEYS}
feature_input_layers.update({categorical_feature_key: tf.keras.Input(shape=(1,), name=categorical_feature_key, dtype=tf.string) for categorical_feature_key in CATEGORICAL_FEATURE_KEYS})",find_wrong,,,,
supervisor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/supervisor/supervisor/options.py,https://github.com/home-assistant/supervisor/tree/master/supervisor/options.py,Options,get_plugins$363,"def get_plugins(self, parser, factory_key, section_prefix):
    factories = []
    for section in parser.sections():
        if not section.startswith(section_prefix):
            continue
        name = section.split(':', 1)[1]
        factory_spec = parser.saneget(section, factory_key, None)
        if factory_spec is None:
            raise ValueError('section [%s] does not specify a %s' % (section, factory_key))
        try:
            factory = self.import_spec(factory_spec)
        except ImportError:
            raise ValueError('%s cannot be resolved within [%s]' % (factory_spec, section))
        extras = {}
        for k in parser.options(section):
            if k != factory_key:
                extras[k] = parser.saneget(section, k)
        factories.append((name, factory, extras))
    return factories","for section in parser.sections():
    if not section.startswith(section_prefix):
        continue
    name = section.split(':', 1)[1]
    factory_spec = parser.saneget(section, factory_key, None)
    if factory_spec is None:
        raise ValueError('section [%s] does not specify a %s' % (section, factory_key))
    try:
        factory = self.import_spec(factory_spec)
    except ImportError:
        raise ValueError('%s cannot be resolved within [%s]' % (factory_spec, section))
    extras = {}
    for k in parser.options(section):
        if k != factory_key:
            extras[k] = parser.saneget(section, k)
    factories.append((name, factory, extras))","factories = [(section.split(':', 1)[1], self.import_spec(parser.saneget(section, factory_key, None)), {k: parser.saneget(section, k) for k in parser.options(section) if k != factory_key}) for section in parser.sections() if section.startswith(section_prefix)]",find_wrong,-1,,,
MB-Lab,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MB-Lab/jointscreator.py,https://github.com/animate1978/MB-Lab/tree/master//jointscreator.py,,create_offset_template_file$262,"def create_offset_template_file(filepath):
    file = {}
    for item in skeleton_ops.ik_joints_head:
        file[item] = [0, 0, 0]
    for item in skeleton_ops.ik_joints_tail:
        file[item] = [0, 0, 0]
    with open(filepath, 'w') as j_file:
        json.dump(file, j_file, indent=2)","for item in skeleton_ops.ik_joints_head:
    file[item] = [0, 0, 0]
for item in skeleton_ops.ik_joints_tail:
    file[item] = [0, 0, 0]","file = {item: [0, 0, 0] for item in skeleton_ops.ik_joints_head + skeleton_ops.ik_joints_tail}",find_wrong,2,,,
zato,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zato/code/zato-server/src/zato/server/connection/web_socket/msg.py,https://github.com/zatosource/zato/tree/master/code/zato-server/src/zato/server/connection/web_socket/msg.py,ClientMessage,get_meta$107,"def get_meta(self, attrs=('action', 'service', 'id', 'timestamp', 'cid', 'in_reply_to', 'ext_client_id', 'ext_client_name')):
    out = {}
    for name in attrs:
        out[name] = getattr(self, name)
    return out","for name in attrs:
    out[name] = getattr(self, name)
return out","out = {name: getattr(self, name) for name in attrs}
return out",find_wrong,,,,
rssant,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rssant/rssant_api/models/image.py,https://github.com/anyant/rssant/tree/master/rssant_api/models/image.py,ImageInfo,batch_detect$42,"def batch_detect(cls, url_roots: typing.List[str]) -> dict:
    sql = '\n        SELECT DISTINCT ON (url_root)\n            id, url_root, status_code\n        FROM rssant_api_imageinfo\n        WHERE url_root = ANY(%s) AND (\n            (status_code > 0 AND dt_created > %s) OR\n            (status_code <= 0 AND dt_created > %s)\n        )\n        ORDER BY url_root, dt_created DESC\n        '
    now = timezone.now()
    dt_pos = now - POSITIVE_STATUS_TTL
    dt_neg = now - NEGTIVE_STATUS_TTL
    url_root_map = {}
    rows = cls.objects.raw(sql, [list(url_roots), dt_pos, dt_neg])
    for row in rows:
        url_root_map[row.url_root] = row.status_code
    return url_root_map","rows = cls.objects.raw(sql, [list(url_roots), dt_pos, dt_neg])
for row in rows:
    url_root_map[row.url_root] = row.status_code","url_root_map = {row.url_root: row.status_code for row in cls.objects.raw(sql, [list(url_roots), dt_pos, dt_neg])}",find_wrong,1,,,
pandapower,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/converter/powermodels/to_pm.py,https://github.com/e2nIEE/pandapower/tree/master/pandapower/converter/powermodels/to_pm.py,,add_params_to_pm$477,"def add_params_to_pm(net, pm):
    pd_idxs_br = []
    pm_idxs_br = []
    br_elms = ['line', 'trafo']
    pm['user_defined_params'] = dict()
    for elm in ['bus', 'line', 'gen', 'load', 'trafo', 'sgen']:
        param_cols = [col for col in net[elm].columns if 'pm_param' in col]
        if not param_cols:
            continue
        params = [param_col.split('/')[-1] for param_col in param_cols]
        br_param = list(set(params) - {'side'})
        for (param, param_col) in zip(params, param_cols):
            pd_idxs = net[elm].index[net[elm][param_col].notna()].tolist()
            target_values = net[elm][param_col][pd_idxs].values.tolist()
            if elm in br_elms and param in br_param:
                pd_idxs_br += net[elm].index[net[elm][param_col].notna()].tolist()
                target_values = net[elm][param_col][pd_idxs_br].values.tolist()
            if elm in ['line', 'trafo']:
                (start, end) = net._pd2pm_lookups['branch'][elm]
                pd_pos = [net[elm].index.tolist().index(p) for p in pd_idxs_br]
                pm_idxs = [int(v) + start for v in pd_pos]
            elif elm == 'sgen':
                pm_idxs = [int(v) for v in net._pd2pm_lookups[elm + '_controllable'][pd_idxs]]
                elm = 'gen'
            else:
                pm_idxs = [int(v) for v in net._pd2pm_lookups[elm][pd_idxs]]
            df = pd.DataFrame(index=pm_idxs) if elm not in ['line', 'trafo'] else pd.DataFrame(index=pm_idxs_br)
            df['element_index'] = pm_idxs
            df['element_pp_index'] = pd_idxs if elm not in ['line', 'trafo'] else pd_idxs_br
            df['value'] = target_values
            df['element'] = elm
            pm['user_defined_params'][param] = df.to_dict(into=OrderedDict, orient='index')
        if elm in ['line', 'trafo']:
            for bp in br_param:
                for k in pm['user_defined_params']['side'].keys():
                    side = pm['user_defined_params']['side'][k]['value']
                    side_bus_f = side + '_bus'
                    if elm == 'line':
                        side_bus_t = 'from_bus' if side == 'to' else 'to_bus'
                    if elm == 'trafo':
                        side_bus_t = 'hv_bus' if side == 'lv' else 'lv_bus'
                    pd_idx = pm['user_defined_params']['side'][k]['element_pp_index']
                    ppcidx = net._pd2pm_lookups['branch'][elm][0] - 1 + pd_idx
                    if side in ['from', 'hv']:
                        ppcrow_f = 0
                        ppcrow_t = 1
                    else:
                        ppcrow_f = 1
                        ppcrow_t = 0
                        assert side in ['to', 'lv']
                    pm['user_defined_params'][bp][k]['f_bus'] = int(net._ppc_opf['branch'][ppcidx, ppcrow_f].real) + 1
                    pm['user_defined_params'][bp][k]['t_bus'] = int(net._ppc_opf['branch'][ppcidx, ppcrow_t].real) + 1
    dic = {}
    if 'user_defined_params' in pm.keys():
        for elm in ['gen', 'sgen_controllable']:
            if elm in net._pd2pm_lookups.keys():
                pm_idxs = net._pd2pm_lookups[elm]
                for k in pm_idxs[pm_idxs != -1]:
                    dic[str(k)] = k
        if dic != {}:
            pm['user_defined_params']['gen_and_controllable_sgen'] = dic
    if 'obj_factors' in net.keys():
        assert type(net.obj_factors) == list
        assert sum(net.obj_factors) <= 1
        dic = {}
        for (i, k) in enumerate(net.obj_factors):
            dic['fac_' + str(i + 1)] = k
        pm['user_defined_params']['obj_factors'] = dic
    return pm","...
pm['user_defined_params'][param] = df.to_dict(into=OrderedDict, orient='index')","pm['user_defined_params'] = {param: df.to_dict(into=OrderedDict, orient='index') for (param, df) in zip(params, [net[elm].loc[net[elm][param_col].notna()] for param_col in param_cols])}",find_wrong,2,,,
football,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/football/gfootball/env/football_env.py,https://github.com/google-research/football/tree/master/gfootball/env/football_env.py,FootballEnv,_convert_observations$92,"def _convert_observations(self, original, player, left_player_position, right_player_position):
    """"""Converts generic observations returned by the environment to
       the player specific observations.

    Args:
      original: original observations from the environment.
      player: player for which to generate observations.
      left_player_position: index into observation corresponding to the left
          player.
      right_player_position: index into observation corresponding to the right
          player.
    """"""
    observations = []
    for is_left in [True, False]:
        adopted = original if is_left or player.can_play_right() else observation_rotation.flip_observation(original, self._config)
        prefix = 'left' if is_left or not player.can_play_right() else 'right'
        position = left_player_position if is_left else right_player_position
        for x in range(player.num_controlled_left_players() if is_left else player.num_controlled_right_players()):
            o = {}
            for v in constants.EXPOSED_OBSERVATIONS:
                o[v] = copy.deepcopy(adopted[v])
            assert len(adopted[prefix + '_agent_controlled_player']) == len(adopted[prefix + '_agent_sticky_actions'])
            o['designated'] = adopted[prefix + '_team_designated_player']
            if position + x >= len(adopted[prefix + '_agent_controlled_player']):
                o['active'] = -1
                o['sticky_actions'] = []
            else:
                o['active'] = adopted[prefix + '_agent_controlled_player'][position + x]
                o['sticky_actions'] = np.array(copy.deepcopy(adopted[prefix + '_agent_sticky_actions'][position + x]))
            if is_left and 'frame' in original:
                o['frame'] = original['frame']
            observations.append(o)
    return observations","for is_left in [True, False]:
    adopted = original if is_left or player.can_play_right() else observation_rotation.flip_observation(original, self._config)
    prefix = 'left' if is_left or not player.can_play_right() else 'right'
    position = left_player_position if is_left else right_player_position
    for x in range(player.num_controlled_left_players() if is_left else player.num_controlled_right_players()):
        o = {}
        for v in constants.EXPOSED_OBSERVATIONS:
            o[v] = copy.deepcopy(adopted[v])
        assert len(adopted[prefix + '_agent_controlled_player']) == len(adopted[prefix + '_agent_sticky_actions'])
        o['designated'] = adopted[prefix + '_team_designated_player']
        if position + x >= len(adopted[prefix + '_agent_controlled_player']):
            o['active'] = -1
            o['sticky_actions'] = []
        else:
            o['active'] = adopted[prefix + '_agent_controlled_player'][position + x]
            o['sticky_actions'] = np.array(copy.deepcopy(adopted[prefix + '_agent_sticky_actions'][position + x]))
        if is_left and 'frame' in original:
            o['frame'] = original['frame']
        observations.append(o)","observations = [{v: copy.deepcopy(adopted[v]) for v in constants.EXPOSED_OBSERVATIONS} | {'designated': adopted[prefix + '_team_designated_player'], 'active': adopted[prefix + '_agent_controlled_player'][position + x] if position + x < len(adopted[prefix + '_agent_controlled_player']) else -1, 'sticky_actions': np.array(copy.deepcopy(adopted[prefix + '_agent_sticky_actions'][position + x])) if position + x < len(adopted[prefix + '_agent_controlled_player']) else []} | ({'frame': original['frame']} if is_left and 'frame' in original else {}) for is_left in [True, False] for x in range(player.num_controlled_left_players() if is_left else player.num_controlled_right_players()) for (prefix, position) in [('left', left_player_position) if is_left else ('right', right_player_position)] if is_left or player.can_play_right() or (not is_left)]",find_wrong,-1,,,
gaphor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gaphor/gaphor/UML/profiles/stereotypepropertypages.py,https://github.com/gaphor/gaphor/tree/master/gaphor/UML/profiles/stereotypepropertypages.py,,refresh$76,"def refresh(subject, model):
    stereotypes = UML.model.get_stereotypes(subject)
    instances = subject.appliedStereotype

    def upsert(path, parent, row_data):
        try:
            new_row = model.get_iter(path)
        except ValueError:
            new_row = model.append(parent, row_data)
        else:
            row = model[path]
            row[:] = row_data
        return new_row
    slots = {}
    for applied in instances:
        for slot in applied.slot:
            slots[slot.definingFeature] = slot
    for (st_index, st) in enumerate(stereotypes):
        for applied in instances:
            if st in applied.classifier:
                break
        else:
            applied = None
        parent = upsert(f'{st_index}', None, (st.name, '', bool(applied), True, False, st, None, None))
        for (attr_index, attr) in enumerate((attr for attr in st.ownedAttribute if not attr.association)):
            slot = slots.get(attr)
            value = slot.value if slot else ''
            upsert(f'{st_index}:{attr_index}', parent, (attr.name, value, bool(applied), False, bool(applied), attr, applied, slot))","slots = {}
    for applied in instances:
        for slot in applied.slot:
            slots[slot.definingFeature] = slot","# shortcut map stereotype -> slot (InstanceSpecification)
    slots = {slot.definingFeature: slot for applied in instances for slot in applied.slot}",find_wrong,1,,,
fbchat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fbchat/fbchat/_threads/_abc.py,https://github.com/fbchat-dev/fbchat/tree/master/fbchat/_threads/_abc.py,ThreadABC,_parse_customization_info$746,"def _parse_customization_info(data: Any) -> MutableMapping[str, Any]:
    if not data or not data.get('customization_info'):
        return {'emoji': None, 'color': DEFAULT_COLOR}
    info = data['customization_info']
    rtn = {'emoji': info.get('emoji'), 'color': ThreadABC._parse_color(info.get('outgoing_bubble_color'))}
    if data.get('thread_type') == 'GROUP' or data.get('is_group_thread') or data.get('thread_key', {}).get('thread_fbid'):
        rtn['nicknames'] = {}
        for k in info.get('participant_customizations', []):
            rtn['nicknames'][k['participant_id']] = k.get('nickname')
    elif info.get('participant_customizations'):
        user_id = data.get('thread_key', {}).get('other_user_id') or data.get('id')
        pc = info['participant_customizations']
        if len(pc) > 0:
            if pc[0].get('participant_id') == user_id:
                rtn['nickname'] = pc[0].get('nickname')
            else:
                rtn['own_nickname'] = pc[0].get('nickname')
        if len(pc) > 1:
            if pc[1].get('participant_id') == user_id:
                rtn['nickname'] = pc[1].get('nickname')
            else:
                rtn['own_nickname'] = pc[1].get('nickname')
    return rtn","""emoji"": info.get(""emoji""),
    ""color"": ThreadABC._parse_color(info.get(""outgoing_bubble_color"")),
}
if (
    data.get(""thread_type"") == ""GROUP""
    or data.get(""is_group_thread"")
    or data.get(""thread_key"", {}).get(""thread_fbid"")
):
    rtn[""nicknames""] = {}
    for k in info.get(""participant_customizations"", []):
        rtn[""nicknames""][k[""participant_id""]] = k.get(""nickname"")
elif info.get(""participant_customizations""):
    user_id = data.get(""thread_key"", {}).get(""other_user_id"") or data.get(""id"")
    pc = info[""participant_customizations""]
    if len(pc) > 0:
        if pc[0].get(""participant_id"") == user_id:
            rtn[""nickname""] = pc[0].get(""nickname"")
        else:
            rtn[""own_nickname""] = pc[0].get(""nickname"")
    if len(pc) > 1:
        if pc[1].get(""participant_id"") == user_id:
            rtn[""nickname""] = pc[1].get(""nickname"")
        else:
            rtn[""own_nickname""] = pc[1].get(""nickname"")","rtn = {'emoji': info.get('emoji'), 'color': ThreadABC._parse_color(info.get('outgoing_bubble_color')), 'nicknames': {k['participant_id']: k.get('nickname') for k in info.get('participant_customizations', []) if data.get('thread_type') == 'GROUP' or data.get('is_group_thread') or data.get('thread_key', {}).get('thread_fbid')}, 'nickname': pc[0].get('nickname') if len(pc) > 0 and pc[0].get('participant_id') == user_id else None, 'own_nickname': pc[0].get('nickname') if len(pc) > 0 and pc[0].get('participant_id') != user_id else None, 'nickname': pc[1].get('nickname') if len(pc) > 1 and pc[1].get('participant_id') == user_id else None, 'own_nickname': pc[1].get('nickname') if len(pc) > 1 and pc[1].get('participant_id') != user_id else None}",find_wrong,2,,,
fuel,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fuel/fuel/datasets/hdf5.py,https://github.com/mila-iqia/fuel/tree/master/fuel/datasets/hdf5.py,H5PYDataset,get_axis_labels$390,"def get_axis_labels(h5file):
    """"""Returns axis labels for all sources in an HDF5 dataset.

        Parameters
        ----------
        h5file : HDF5 file handle
            An HDF5 dataset respecting the H5PYDataset interface.

        Returns
        -------
        axis_labels : dict
            Maps source names to a tuple of str representing the axis
            labels.

        """"""
    axis_labels = {}
    vlen_sources = H5PYDataset.get_vlen_sources(h5file)
    for source_name in H5PYDataset.get_all_sources(h5file):
        if source_name in vlen_sources:
            axis_labels[source_name] = (h5file[source_name].dims[0].label,) + tuple((label.decode('utf8') for label in h5file[source_name].dims[0]['shape_labels']))
        else:
            axis_labels[source_name] = tuple((dim.label for dim in h5file[source_name].dims))
    return axis_labels","vlen_sources = H5PYDataset.get_vlen_sources(h5file)
for source_name in H5PYDataset.get_all_sources(h5file):
    if source_name in vlen_sources:
        axis_labels[source_name] = (h5file[source_name].dims[0].label,) + tuple((label.decode('utf8') for label in h5file[source_name].dims[0]['shape_labels']))
    else:
        axis_labels[source_name] = tuple((dim.label for dim in h5file[source_name].dims))","axis_labels = {source_name: (h5file[source_name].dims[0].label,) + tuple((label.decode('utf8') for label in h5file[source_name].dims[0]['shape_labels'])) if source_name in H5PYDataset.get_vlen_sources(h5file) else tuple((dim.label for dim in h5file[source_name].dims)) for source_name in H5PYDataset.get_all_sources(h5file)}",find_wrong,1,,,
synapse,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/synapse/synapse/handlers/auth.py,https://github.com/matrix-org/synapse/tree/master/synapse/handlers/auth.py,,load_single_legacy_password_auth_provider$1855,"def load_single_legacy_password_auth_provider(module: Type, config: JsonDict, api: 'ModuleApi') -> None:
    try:
        provider = module(config=config, account_handler=api)
    except Exception as e:
        logger.error('Error while initializing %r: %s', module, e)
        raise

    def async_wrapper(f: Optional[Callable]) -> Optional[Callable[..., Awaitable]]:
        if f is None:
            return None
        if f.__name__ == 'check_password':

            async def wrapped_check_password(username: str, login_type: str, login_dict: JsonDict) -> Optional[Tuple[str, Optional[Callable]]]:
                assert f is not None
                matrix_user_id = api.get_qualified_user_id(username)
                password = login_dict['password']
                is_valid = await f(matrix_user_id, password)
                if is_valid:
                    return (matrix_user_id, None)
                return None
            return wrapped_check_password
        if f.__name__ == 'check_auth':

            async def wrapped_check_auth(username: str, login_type: str, login_dict: JsonDict) -> Optional[Tuple[str, Optional[Callable]]]:
                assert f is not None
                result = await f(username, login_type, login_dict)
                if isinstance(result, str):
                    return (result, None)
                return result
            return wrapped_check_auth
        if f.__name__ == 'check_3pid_auth':

            async def wrapped_check_3pid_auth(medium: str, address: str, password: str) -> Optional[Tuple[str, Optional[Callable]]]:
                assert f is not None
                result = await f(medium, address, password)
                if isinstance(result, str):
                    return (result, None)
                return result
            return wrapped_check_3pid_auth

        def run(*args: Tuple, **kwargs: Dict) -> Awaitable:
            assert f is not None
            return maybe_awaitable(f(*args, **kwargs))
        return run
    check_3pid_auth_hook: Optional[CHECK_3PID_AUTH_CALLBACK] = async_wrapper(getattr(provider, 'check_3pid_auth', None))
    on_logged_out_hook: Optional[ON_LOGGED_OUT_CALLBACK] = async_wrapper(getattr(provider, 'on_logged_out', None))
    supported_login_types = {}
    g = getattr(provider, 'get_supported_login_types', None)
    if g is not None:
        supported_login_types.update(g())
    auth_checkers = {}
    check_auth = async_wrapper(getattr(provider, 'check_auth', None))
    if check_auth is not None:
        for (login_type, fields) in supported_login_types.items():
            auth_checkers[login_type, tuple(fields)] = check_auth
    check_password = async_wrapper(getattr(provider, 'check_password', None))
    if check_password is not None:
        auth_checkers[LoginType.PASSWORD, ('password',)] = check_password
    api.register_password_auth_provider_callbacks(check_3pid_auth=check_3pid_auth_hook, on_logged_out=on_logged_out_hook, auth_checkers=auth_checkers)","g = getattr(provider, 'get_supported_login_types', None)
if g is not None:
    supported_login_types.update(g())","supported_login_types = g() if (g := getattr(provider, 'get_supported_login_types', None)) is not None else {}",find_wrong,2,,,
crossbar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/crossbar/crossbar/router/longpoll.py,https://github.com/crossbario/crossbar/tree/master/crossbar/router/longpoll.py,WampLongPollResource,__init__$536,"def __init__(self, factory, serializers=None, timeout=10, killAfter=30, queueLimitBytes=128 * 1024, queueLimitMessages=100, debug_transport_id=None, reactor=None):
    """"""
        Create new HTTP WAMP Web resource.

        :param factory: A (router) session factory.
        :type factory: Instance of :class:`autobahn.twisted.wamp.RouterSessionFactory`.
        :param serializers: List of WAMP serializers.
        :type serializers: list of obj (which implement :class:`autobahn.wamp.interfaces.ISerializer`)
        :param timeout: XHR polling timeout in seconds.
        :type timeout: int
        :param killAfter: Kill WAMP session after inactivity in seconds.
        :type killAfter: int
        :param queueLimitBytes: Kill WAMP session after accumulation of this many bytes in send queue (XHR poll).
        :type queueLimitBytes: int
        :param queueLimitMessages: Kill WAMP session after accumulation of this many message in send queue (XHR poll).
        :type queueLimitMessages: int
        :param debug: Enable debug logging.
        :type debug: bool
        :param debug_transport_id: If given, use this fixed transport ID.
        :type debug_transport_id: str
        :param reactor: The Twisted reactor to run under.
        :type reactor: obj
        """"""
    Resource.__init__(self)
    self._factory = factory
    if reactor is None:
        from twisted.internet import reactor
    self.reactor = reactor
    self._debug_transport_id = debug_transport_id
    self._timeout = timeout
    self._killAfter = killAfter
    self._queueLimitBytes = queueLimitBytes
    self._queueLimitMessages = queueLimitMessages
    if serializers is None:
        serializers = []
        try:
            from autobahn.wamp.serializer import CBORSerializer
            serializers.append(CBORSerializer(batched=True))
            serializers.append(CBORSerializer())
        except ImportError:
            pass
        try:
            from autobahn.wamp.serializer import MsgPackSerializer
            serializers.append(MsgPackSerializer(batched=True))
            serializers.append(MsgPackSerializer())
        except ImportError:
            pass
        try:
            from autobahn.wamp.serializer import UBJSONSerializer
            serializers.append(UBJSONSerializer(batched=True))
            serializers.append(UBJSONSerializer())
        except ImportError:
            pass
        try:
            from autobahn.wamp.serializer import JsonSerializer
            serializers.append(JsonSerializer(batched=True))
            serializers.append(JsonSerializer())
        except ImportError:
            pass
        if not serializers:
            raise Exception('could not import any WAMP serializers')
    self._serializers = {}
    for ser in serializers:
        self._serializers[ser.SERIALIZER_ID] = ser
    self._transports = {}
    self.putChild(b'open', WampLongPollResourceOpen(self))
    self.log.debug('WampLongPollResource initialized')","serializers = []

    # try CBOR WAMP serializer
    try:
        from autobahn.wamp.serializer import CBORSerializer
        serializers.append(CBORSerializer(batched=True))
        serializers.append(CBORSerializer())
    except ImportError:
        pass

    # try MsgPack WAMP serializer
    try:
        from autobahn.wamp.serializer import MsgPackSerializer
        serializers.append(MsgPackSerializer(batched=True))
        serializers.append(MsgPackSerializer())
    except ImportError:
        pass

    # try UBJSON WAMP serializer
    try:
        from autobahn.wamp.serializer import UBJSONSerializer
        serializers.append(UBJSONSerializer(batched=True))
        serializers.append(UBJSONSerializer())
    except ImportError:
        pass

    # try JSON WAMP serializer
    try:
        from autobahn.wamp.serializer import JsonSerializer
        serializers.append(JsonSerializer(batched=True))
        serializers.append(JsonSerializer())
    except ImportError:
        pass

    if not serializers:
        raise Exception(""could not import any WAMP serializers"")","serializers = [ser(batched=True) for ser in [CBORSerializer, MsgPackSerializer, UBJSONSerializer, JsonSerializer] if ser.__module__ != 'autobahn.wamp.serializer' or ser.__name__ != 'JsonSerializer']
serializers += [ser() for ser in [CBORSerializer, MsgPackSerializer, UBJSONSerializer, JsonSerializer] if ser.__module__ != 'autobahn.wamp.serializer' or ser.__name__ != 'JsonSerializer']
if not serializers:
    raise Exception('could not import any WAMP serializers')",find_wrong,2,,,
ReAgent,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ReAgent/reagent/preprocessing/sparse_to_dense.py,https://github.com/facebookresearch/ReAgent/tree/master/reagent/preprocessing/sparse_to_dense.py,StringKeySparseToDenseProcessor,process$35,"def process(self, sparse_data: List[Dict[str, float]]) -> Tuple[torch.Tensor, torch.Tensor]:
    sparse_data_int = []
    for sd in sparse_data:
        sd_int = {}
        for (k, v) in sd.items():
            sd_int[int(k)] = v
        sparse_data_int.append(sd_int)
    return self._sparse_to_dense(sparse_data_int)","for sd in sparse_data:
    sd_int = {}
    for (k, v) in sd.items():
        sd_int[int(k)] = v
    sparse_data_int.append(sd_int)","sparse_data_int = [{int(k): v for (k, v) in sd.items()} for sd in sparse_data]",find_wrong,1,,,
unilm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unilm/xtune/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,https://github.com/microsoft/unilm/tree/master/xtune/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,,convert_xlm_checkpoint_to_pytorch$32,"def convert_xlm_checkpoint_to_pytorch(xlm_checkpoint_path, pytorch_dump_folder_path):
    chkpt = torch.load(xlm_checkpoint_path, map_location='cpu')
    state_dict = chkpt['model']
    two_levels_state_dict = {}
    for (k, v) in state_dict.items():
        if 'pred_layer' in k:
            two_levels_state_dict[k] = v
        else:
            two_levels_state_dict['transformer.' + k] = v
    config = chkpt['params']
    config = dict(((n, v) for (n, v) in config.items() if not isinstance(v, (torch.FloatTensor, numpy.ndarray))))
    vocab = chkpt['dico_word2id']
    vocab = dict(((s + '</w>' if s.find('@@') == -1 and i > 13 else s.replace('@@', ''), i) for (s, i) in vocab.items()))
    pytorch_weights_dump_path = pytorch_dump_folder_path + '/' + WEIGHTS_NAME
    pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME
    pytorch_vocab_dump_path = pytorch_dump_folder_path + '/' + VOCAB_FILES_NAMES['vocab_file']
    print('Save PyTorch model to {}'.format(pytorch_weights_dump_path))
    torch.save(two_levels_state_dict, pytorch_weights_dump_path)
    print('Save configuration file to {}'.format(pytorch_config_dump_path))
    with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:
        f.write(json.dumps(config, indent=2) + '\n')
    print('Save vocab file to {}'.format(pytorch_config_dump_path))
    with open(pytorch_vocab_dump_path, 'w', encoding='utf-8') as f:
        f.write(json.dumps(vocab, indent=2) + '\n')","vocab = dict(((s + '</w>' if s.find('@@') == -1 and i > 13 else s.replace('@@', ''), i) for (s, i) in vocab.items()))","config = {n: v for (n, v) in config.items() if not isinstance(v, (torch.FloatTensor, numpy.ndarray))}
vocab = {s + '</w>' if s.find('@@') == -1 and i > 13 else s.replace('@@', ''): i for (s, i) in vocab.items()}",find_wrong,2,,,
oppia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/exp_domain.py,https://github.com/oppia/oppia/tree/master/core/domain/exp_domain.py,Exploration,_remove_unwanted_content_ids_from_translations_and_voiceovers_from_state_v51_or_v52$3089,"def _remove_unwanted_content_ids_from_translations_and_voiceovers_from_state_v51_or_v52(cls, state_dict: state_domain.StateDict, state_schema: int) -> None:
    """"""Helper function to remove the content IDs from the translations
        and voiceovers which are deleted from the state.

        Args:
            state_dict: state_domain.StateDict. The state dictionary.
            state_schema: int. The state schema from which we are using
                this functionality.
        """"""
    interaction = state_dict['interaction']
    content_id_list = [state_dict['content']['content_id']]
    for answer_group in interaction['answer_groups']:
        content_id_list.append(answer_group['outcome']['feedback']['content_id'])
        for rule_spec in answer_group['rule_specs']:
            for (param_name, value) in rule_spec['inputs'].items():
                interaction_id = interaction['id']
                param_type = interaction_registry.Registry.get_interaction_by_id(interaction_id).get_rule_param_type(rule_spec['rule_type'], param_name)
                if issubclass(param_type, objects.BaseTranslatableObject):
                    assert isinstance(value, dict)
                    content_id = value['contentId']
                    assert isinstance(content_id, str)
                    content_id_list.append(content_id)
    default_outcome = interaction['default_outcome']
    if default_outcome:
        content_id_list.append(default_outcome['feedback']['content_id'])
    for hint in interaction['hints']:
        content_id_list.append(hint['hint_content']['content_id'])
    interaction_solution = interaction['solution']
    if interaction_solution:
        content_id_list.append(interaction_solution['explanation']['content_id'])
    if interaction['id'] is not None:
        customisation_args = state_domain.InteractionInstance.convert_customization_args_dict_to_customization_args(interaction['id'], interaction['customization_args'], state_schema_version=state_schema)
        for ca_name in customisation_args:
            content_id_list.extend(customisation_args[ca_name].get_content_ids())
    translations_mapping = state_dict['written_translations']['translations_mapping']
    new_translations_mapping = {content_id: translation_item for (content_id, translation_item) in translations_mapping.items() if content_id in content_id_list}
    state_dict['written_translations']['translations_mapping'] = new_translations_mapping
    voiceovers_mapping = state_dict['recorded_voiceovers']['voiceovers_mapping']
    new_voiceovers_mapping = {}
    for (content_id, voiceover_item) in voiceovers_mapping.items():
        if content_id in content_id_list:
            new_voiceovers_mapping[content_id] = voiceover_item
    state_dict['recorded_voiceovers']['voiceovers_mapping'] = new_voiceovers_mapping","content_id: translation_item for
     content_id, translation_item in translations_mapping.items()
     if content_id in content_id_list
}
state_dict['written_translations']['translations_mapping'] = (
    new_translations_mapping)

new_voiceovers_mapping = {}
for content_id, voiceover_item in voiceovers_mapping.items():
    if content_id in content_id_list:
        new_voiceovers_mapping[content_id] = voiceover_item
state_dict['recorded_voiceovers']['voiceovers_mapping'] = (
    new_voiceovers_mapping)","state_dict['written_translations']['translations_mapping'] = {content_id: translation_item for (content_id, translation_item) in translations_mapping.items() if content_id in content_id_list}
state_dict['recorded_voiceovers']['voiceovers_mapping'] = {content_id: voiceover_item for (content_id, voiceover_item) in voiceovers_mapping.items() if content_id in content_id_list}",find_wrong,-1,,,
horovod,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/horovod/horovod/spark/torch/remote.py,https://github.com/horovod/horovod/tree/master/horovod/spark/torch/remote.py,,get_metric_avgs$543,"def get_metric_avgs(metric_value_groups):
    all_metric_groups_values = []
    for metric_value_group in metric_value_groups:
        metric_avgs = {}
        for metric in metric_value_group:
            metric_avgs[metric.name] = metric.avg.item()
        all_metric_groups_values.append(metric_avgs)
    return all_metric_groups_values","for metric_value_group in metric_value_groups:
    metric_avgs = {}
    for metric in metric_value_group:
        metric_avgs[metric.name] = metric.avg.item()
    all_metric_groups_values.append(metric_avgs)",all_metric_groups_values = [{metric.name: metric.avg.item() for metric in metric_value_group} for metric_value_group in metric_value_groups],find_wrong,1,,,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/dataset/conll05.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/dataset/conll05.py,,load_dict$65,"def load_dict(filename):
    d = dict()
    with open(filename, 'r') as f:
        for (i, line) in enumerate(f):
            d[line.strip()] = i
    return d","with open(filename, 'r') as f:
    for (i, line) in enumerate(f):
        d[line.strip()] = i","with open(filename, 'r') as f:
    d = {line.strip(): i for (i, line) in enumerate(f)}",find_wrong,1,,,
nasbench,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nasbench/nasbench/scripts/generate_cifar10_tfrecords.py,https://github.com/google-research/nasbench/tree/master/nasbench/scripts/generate_cifar10_tfrecords.py,,_get_file_names$61,"def _get_file_names():
    """"""Returns the file names expected to exist in the input_dir.""""""
    file_names = {}
    for i in range(1, 5):
        file_names['train_%d' % i] = 'data_batch_%d' % i
    file_names['validation'] = 'data_batch_5'
    file_names['test'] = 'test_batch'
    return file_names","for i in range(1, 5):
    file_names['train_%d' % i] = 'data_batch_%d' % i
file_names['validation'] = 'data_batch_5'
file_names['test'] = 'test_batch'","file_names = {'train_%d' % i: 'data_batch_%d' % i for i in range(1, 5)}
file_names['validation'] = 'data_batch_5'
file_names['test'] = 'test_batch'",find_wrong,1,,,
zato,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zato/code/zato-common/src/zato/common/ext/configobj_.py,https://github.com/zatosource/zato/tree/master/code/zato-common/src/zato/common/ext/configobj_.py,Builder,build_Dict$190,"def build_Dict(self, o):
    d = {}
    i = iter(map(self.build, o.getChildren()))
    for el in i:
        d[el] = next(i)
    return d","i = iter(map(self.build, o.getChildren()))
for el in i:
    d[el] = next(i)","d = {el: next(i) for el in map(self.build, o.getChildren())}",find_wrong,1,,,
mysql-connector-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mysql-connector-python/lib/mysql/connector/connection_cext.py,https://github.com/mysql/mysql-connector-python/tree/master/lib/mysql/connector/connection_cext.py,CMySQLConnection,prepare_for_mysql$647,"def prepare_for_mysql(self, params):
    """"""Prepare parameters for statements

        This method is use by cursors to prepared parameters found in the
        list (or tuple) params.

        Returns dict.
        """"""
    if isinstance(params, (list, tuple)):
        if self.converter:
            result = [self.converter.quote(self.converter.escape(self.converter.to_mysql(value))) for value in params]
        else:
            result = self._cmysql.convert_to_mysql(*params)
    elif isinstance(params, dict):
        result = {}
        if self.converter:
            for (key, value) in params.items():
                result[key] = self.converter.quote(self.converter.escape(self.converter.to_mysql(value)))
        else:
            for (key, value) in params.items():
                result[key] = self._cmysql.convert_to_mysql(value)[0]
    else:
        raise ValueError('Could not process parameters')
    return result","if self.converter:
    for (key, value) in params.items():
        result[key] = self.converter.quote(self.converter.escape(self.converter.to_mysql(value)))
else:
    for (key, value) in params.items():
        result[key] = self._cmysql.convert_to_mysql(value)[0]","result = {key: self.converter.quote(self.converter.escape(self.converter.to_mysql(value))) if self.converter else self._cmysql.convert_to_mysql(value)[0] for (key, value) in params.items()}",find_wrong,1,,,
anchore-engine,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anchore-engine/anchore_engine/common/helpers.py,https://github.com/anchore/anchore-engine/tree/master/anchore_engine/common/helpers.py,,extract_java_content$284,"def extract_java_content(image_data):
    ret = {}
    if 'pkgs.java' in image_data['imagedata']['analysis_report']['package_list']:
        adata = image_data['imagedata']['analysis_report']['package_list']['pkgs.java']['base']
        for k in list(adata.keys()):
            ret[k] = safe_extract_json_value(adata[k])
    return ret","if 'pkgs.java' in image_data['imagedata']['analysis_report']['package_list']:
    adata = image_data['imagedata']['analysis_report']['package_list']['pkgs.java']['base']
    for k in list(adata.keys()):
        ret[k] = safe_extract_json_value(adata[k])","ret = {k: safe_extract_json_value(adata[k]) for (k, v) in image_data['imagedata']['analysis_report']['package_list'].get('pkgs.java', {}).get('base', {}).items()}",find_wrong,1,,,
orbit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/orbit/orbit/forecaster/full_bayes.py,https://github.com/uber/orbit/tree/master/orbit/forecaster/full_bayes.py,FullBayesianForecaster,predict$89,"def predict(self, df, decompose=False, store_prediction_array=False, seed=None, **kwargs):
    if not self.is_fitted():
        raise ForecasterException('Model is not fitted yet.')
    self._set_prediction_meta(df)
    prediction_meta = self.get_prediction_meta()
    training_meta = self.get_training_meta()
    if seed is not None:
        np.random.seed(seed)
    if self._point_method is None:
        posterior_samples = self._bootstrap(num_samples=self.estimator.num_sample, posterior_samples=self._posterior_samples, n=self._n_bootstrap_draws) if self._n_bootstrap_draws > 1 else self._posterior_samples
        predicted_dict = self._model.predict(posterior_estimates=posterior_samples, df=df, training_meta=training_meta, prediction_meta=prediction_meta, include_error=True, **kwargs)
        if PredictionKeys.PREDICTION.value not in predicted_dict.keys():
            raise ForecasterException(""cannot find the key:'{}' from return of _predict()"".format(PredictionKeys.PREDICTION.value))
        if not decompose:
            predicted_dict = {k: v for (k, v) in predicted_dict.items() if k == PredictionKeys.PREDICTION.value}
        if store_prediction_array:
            self.prediction_array = predicted_dict[PredictionKeys.PREDICTION.value]
        percentiles_dict = compute_percentiles(predicted_dict, self._prediction_percentiles)
        predicted_df = pd.DataFrame(percentiles_dict)
        predicted_df = prepend_date_column(predicted_df, df, self.date_col)
        return predicted_df
    else:
        point_posteriors = self._point_posteriors.get(self._point_method)
        point_predicted_dict = self._model.predict(posterior_estimates=point_posteriors, df=df, training_meta=training_meta, prediction_meta=prediction_meta, include_error=False, **kwargs)
        for (k, v) in point_predicted_dict.items():
            point_predicted_dict[k] = np.squeeze(v, 0)
        if self._n_bootstrap_draws > 0 and len(self._prediction_percentiles) > 1:
            posterior_samples = {}
            for (k, v) in point_posteriors.items():
                posterior_samples[k] = np.repeat(v, self.n_bootstrap_draws, axis=0)
            predicted_dict = self._model.predict(posterior_estimates=posterior_samples, df=df, training_meta=training_meta, prediction_meta=prediction_meta, include_error=True, **kwargs)
            percentiles_dict = compute_percentiles(predicted_dict, self._prediction_percentiles)
            percentiles_dict.update(point_predicted_dict)
            if PredictionKeys.PREDICTION.value not in percentiles_dict.keys():
                raise ForecasterException(""cannot find the key:'{}' from return of _predict()"".format(PredictionKeys.PREDICTION.value))
            if not decompose:
                k = PredictionKeys.PREDICTION.value
                reduced_keys = [k + '_' + str(p) if p != 50 else k for p in self._prediction_percentiles]
                percentiles_dict = {k: v for (k, v) in percentiles_dict.items() if k in reduced_keys}
            predicted_df = pd.DataFrame(percentiles_dict)
        else:
            if not decompose:
                point_predicted_dict = {k: v for (k, v) in point_predicted_dict.items() if k == PredictionKeys.PREDICTION.value}
            predicted_df = pd.DataFrame(point_predicted_dict)
        predicted_df = prepend_date_column(predicted_df, df, self.date_col)
        return predicted_df","k: v
    for k, v in predicted_dict.items()
    if k == PredictionKeys.PREDICTION.value
}","predicted_dict = {k: v for (k, v) in predicted_dict.items() if k == PredictionKeys.PREDICTION.value}",find_wrong,2,,,
model-analysis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/model-analysis/tensorflow_model_analysis/api/verifier_lib.py,https://github.com/tensorflow/model-analysis/tree/master/tensorflow_model_analysis/api/verifier_lib.py,,Validate$26,"def Validate(extracts: beam.pvalue.PCollection, alternatives: Dict[str, beam.PTransform], validators: List[validator.Validator]) -> validator.Validation:
    """"""Performs validation of alternative evaluations.

  Args:
    extracts: PCollection of extracts.
    alternatives: Dict of PTransforms (Extracts -> Evaluation) whose output will
      be compared for validation purposes (e.g. 'baseline' vs 'candidate').
    validators: List of validators for validating the output from running the
      alternatives. The Validation outputs produced by the validators will be
      merged into a single output. If there are overlapping output keys, later
      outputs will replace earlier outputs sharing the same key.

  Returns:
    Validation dict.
  """"""
    evaluations = {}
    for key in alternatives:
        evaluations[key] = extracts | 'Evaluate(%s)' % key >> alternatives[key]
    validation = {}
    for v in validators:
        validation.update(evaluations | v.stage_name >> v.ptransform)
    return validation","for key in alternatives:
    evaluations[key] = extracts | 'Evaluate(%s)' % key >> alternatives[key]
validation = {}
for v in validators:
    validation.update(evaluations | v.stage_name >> v.ptransform)","evaluations = {key: extracts | f'Evaluate({key})' >> alternatives[key] for key in alternatives}
validation = {**evaluations, **{v.stage_name: evaluations | v.stage_name >> v.ptransform for v in validators}}",find_wrong,2,,,
simpleui,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/simpleui/simpleui/templatetags/simpletags.py,https://github.com/newpanjing/simpleui/tree/master/simpleui/templatetags/simpletags.py,,custom_button$393,"def custom_button(context):
    admin = context.get('cl').model_admin
    data = {}
    actions = admin.get_actions(context.request)
    if actions:
        i = 0
        for name in actions:
            values = {}
            fun = actions.get(name)[0]
            for (key, v) in fun.__dict__.items():
                if key != '__len__' and key != '__wrapped__':
                    values[key] = v
            values['eid'] = i
            i += 1
            data[name] = values
    return json.dumps(data, cls=LazyEncoder)","i = 0
    for name in actions:
        values = {}
        fun = actions.get(name)[0]
        for key, v in fun.__dict__.items():
            if key != '__len__' and key != '__wrapped__':
                values[key] = v
        values['eid'] = i
        i += 1
        data[name] = values","data = {name: {key: v for (key, v) in fun.__dict__.items() if key != '__len__' and key != '__wrapped__'} for (i, (name, (fun, _))) in enumerate(actions.items())}",find_wrong,1,,,
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/adapters/heads/base.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/adapters/heads/base.py,ModelWithFlexibleHeadsAdaptersMixin,_get_head_input$748,"def _get_head_input(outputs, cls_out, batch):
    if isinstance(outputs, ModelOutput):
        inputs = {}
        for (key, base_output) in outputs.items():
            if torch.is_tensor(base_output):
                inputs[key] = base_output[batch[0]:batch[-1] + 1]
        inputs = outputs.__class__(**inputs)
    else:
        inputs = tuple()
        for base_output in outputs:
            inputs = inputs + (base_output[batch],)
    if cls_out is not None:
        cls_input = cls_out[batch]
    else:
        cls_input = None
    return (inputs, cls_input)","for (key, base_output) in outputs.items():
    if torch.is_tensor(base_output):
        inputs[key] = base_output[batch[0]:batch[-1] + 1]
inputs = outputs.__class__(**inputs)","inputs = outputs.__class__({key: base_output[batch[0]:batch[-1] + 1] for (key, base_output) in outputs.items() if torch.is_tensor(base_output)})",find_wrong,1,,,
orchest,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/orchest/services/orchest-api/app/app/apis/namespace_jobs.py,https://github.com/orchest/orchest/tree/master/services/orchest-api/app/app/apis/namespace_jobs.py,UpdateDraftJobPipeline,_resolve_environment_variables$1411,"def _resolve_environment_variables(project_env_vars: Dict[str, str], old_pipeline_env_vars: Dict[str, str], new_pipeline_env_vars: Dict[str, str], old_job_env_vars: Dict[str, str]) -> Dict[str, str]:
    """"""Resolves the environment variables to be used for the job.

        When changing the pipeline for a draft job we'd like to carry
        over all work that the user has done w.r.t. setting/changing the
        environment variables of the job. Do to that, we have to
        reconstruct the changes the user has made and resolve
        ambiguities.

        This logic identifies user changes as:
        - removing env vars inherited by the project or pipeline
        - changing env vars inherited by the project or pipeline
        - adding new environment variables

        Ambiguities:
        - an env var inherited by the old pipeline which hasn't been
            changed signals that the user wanted the default value of
            the pipeline env var.  If the new pipeline has such env var,
            use the default value coming from the new pipeline, if it
            doesn't have the env var, ignore the variable, i.e. do not
            include it in the resulting set.
        - an env var inherited by the project wasn't changed, and the
            old pipeline didn't overwrite the value of that variable. If
            the new pipeline has that env var then it will overwrite the
            value.

        """"""
    old_proj_ppl_merge = {**project_env_vars, **old_pipeline_env_vars}
    removed_env_vars = set()
    changed_env_vars = dict()
    added_env_vars = dict()
    for env_var in old_proj_ppl_merge:
        if env_var not in old_job_env_vars:
            removed_env_vars.add(env_var)
        elif old_proj_ppl_merge[env_var] != old_job_env_vars[env_var]:
            changed_env_vars[env_var] = old_job_env_vars[env_var]
    for env_var in old_job_env_vars:
        if env_var not in old_proj_ppl_merge:
            added_env_vars[env_var] = old_job_env_vars[env_var]
    result = {**project_env_vars, **new_pipeline_env_vars, **changed_env_vars, **added_env_vars}
    for env_var in removed_env_vars:
        result.pop(env_var, None)
    return result","**project_env_vars,
            **new_pipeline_env_vars,
            **changed_env_vars,
            **added_env_vars,
        }","result = {**project_env_vars, **new_pipeline_env_vars, **changed_env_vars, **added_env_vars}",find_wrong,2,,,
OpenVINO-YoloV3,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenVINO-YoloV3/utils.py,https://github.com/PINTO0309/OpenVINO-YoloV3/tree/master//utils.py,,load_coco_names$229,"def load_coco_names(file_name):
    names = {}
    with open(file_name) as f:
        for (id, name) in enumerate(f):
            names[id] = name
    return names","with open(file_name) as f:
    for (id, name) in enumerate(f):
        names[id] = name","with open(file_name) as f:
    names = {id: name for (id, name) in enumerate(f)}",find_wrong,1,,,
PGL,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/examples/kddcup2021/WikiKG90M/feature/dump_feat/10_ht_feat.py,https://github.com/PaddlePaddle/PGL/tree/master/examples/kddcup2021/WikiKG90M/feature/dump_feat/10_ht_feat.py,,f$33,"def f(x):
    res = np.zeros_like(x)
    (unique, counts) = np.unique(x, return_counts=True)
    mapper_dict = {}
    for (idx, count) in zip(unique, counts):
        mapper_dict[idx] = count

    def mp(entry):
        return mapper_dict[entry]
    mp = np.vectorize(mp)
    return mp(x)","for (idx, count) in zip(unique, counts):
    mapper_dict[idx] = count

def mp(entry):
    return mapper_dict[entry]
mp = np.vectorize(mp)","mapper_dict = {idx: count for (idx, count) in zip(unique, counts)}
mp = np.vectorize(lambda entry: mapper_dict[entry])",find_wrong,1,,,
Tencent2019_Finals_Rank1st,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Tencent2019_Finals_Rank1st/A/extract_features.py,https://github.com/bettenW/Tencent2019_Finals_Rank1st/tree/master/A/extract_features.py,,request_cont$117,"def request_cont(train_df, test_df, flag):
    print('request_cont')
    request = pd.read_pickle('preprocess_data/aid_request_{}.pkl'.format(flag))
    dic = {}
    for item in request[['aid', 'day', 'request_cont']].values:
        dic[item[0], item[1]] = int(item[2])
    train_df['request_cont'] = train_df[['aid', 'day']].apply(lambda x: dic[tuple(x)], axis=1)
    test_df['request_cont'] = test_df[['aid', 'day']].apply(lambda x: dic[tuple(x)], axis=1)","for item in request[['aid', 'day', 'request_cont']].values:
    dic[item[0], item[1]] = int(item[2])
train_df['request_cont'] = train_df[['aid', 'day']].apply(lambda x: dic[tuple(x)], axis=1)
test_df['request_cont'] = test_df[['aid', 'day']].apply(lambda x: dic[tuple(x)], axis=1)","dic = {(item[0], item[1]): int(item[2]) for item in request[['aid', 'day', 'request_cont']].values}
train_df['request_cont'] = train_df[['aid', 'day']].apply(lambda x: dic[tuple(x)], axis=1)
test_df['request_cont'] = test_df[['aid', 'day']].apply(lambda x: dic[tuple(x)], axis=1)",find_wrong,1,,,
Pyro4,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pyro4/src/Pyro4/naming.py,https://github.com/irmen/Pyro4/tree/master/src/Pyro4/naming.py,NameServer,list$158,"def list(self, prefix=None, regex=None, metadata_all=None, metadata_any=None, return_metadata=False):
    """"""Retrieve the registered items as a dictionary name-to-URI. The URIs
        in the resulting dict are strings, not URI objects.
        You can filter by prefix or by regex or by metadata subset (separately)""""""

    def fix_set(result):
        if return_metadata:
            fixed = {}
            for (name, data) in result.items():
                fixed[name] = (data[0], list(data[1]))
            return fixed
        return result
    if sum((1 for x in [prefix, regex, metadata_all, metadata_any] if x is not None)) > 1:
        raise ValueError('you can only filter on one thing at a time')
    with self.lock:
        if prefix:
            result = self.storage.optimized_prefix_list(prefix, return_metadata)
            if result is not None:
                return fix_set(result)
            result = {}
            for name in self.storage:
                if name.startswith(prefix):
                    result[name] = self.storage[name] if return_metadata else self.storage[name][0]
            return fix_set(result)
        elif regex:
            result = self.storage.optimized_regex_list(regex, return_metadata)
            if result is not None:
                return fix_set(result)
            result = {}
            try:
                regex = re.compile(regex)
            except re.error as x:
                raise NamingError('invalid regex: ' + str(x))
            else:
                for name in self.storage:
                    if regex.match(name):
                        result[name] = self.storage[name] if return_metadata else self.storage[name][0]
                return fix_set(result)
        elif metadata_all:
            if isinstance(metadata_all, basestring):
                raise TypeError('metadata_all should not be a str, but another iterable (set, list, etc)')
            metadata_all and iter(metadata_all)
            result = self.storage.optimized_metadata_search(metadata_all=metadata_all, return_metadata=return_metadata)
            if result is not None:
                return fix_set(result)
            metadata_all = frozenset(metadata_all)
            result = {}
            for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
                if metadata_all.issubset(meta):
                    result[name] = (uri, meta) if return_metadata else uri
            return fix_set(result)
        elif metadata_any:
            if isinstance(metadata_any, basestring):
                raise TypeError('metadata_any should not be a str, but another iterable (set, list, etc)')
            metadata_any and iter(metadata_any)
            result = self.storage.optimized_metadata_search(metadata_any=metadata_any, return_metadata=return_metadata)
            if result is not None:
                return fix_set(result)
            metadata_any = frozenset(metadata_any)
            result = {}
            for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
                if metadata_any & meta:
                    result[name] = (uri, meta) if return_metadata else uri
            return fix_set(result)
        else:
            return fix_set(self.storage.everything(return_metadata))","for name in self.storage:
    if name.startswith(prefix):
        result[name] = self.storage[name] if return_metadata else self.storage[name][0]
return fix_set(result)","result = {name: self.storage[name] if return_metadata else self.storage[name][0] for name in self.storage if name.startswith(prefix)}
return fix_set(result)",find_wrong,2,,,
PyQt5-Apps,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyQt5-Apps/words-recorder/main.py,https://github.com/taseikyo/PyQt5-Apps/tree/master/words-recorder/main.py,MainWindow,connectDatabase$83,"def connectDatabase(self, w):
    """""":author : Tich
        connect to database
        :param w: used for data update in `w.table`
        """"""
    global db, cursor
    if db:
        return
    config = ConfigParser()
    conf = {}
    try:
        config.readfp(open('setting.ini'))
        head = ['host', 'user', 'password', 'db', 'port', 'charset', 'path']
        for x in head:
            conf[x] = config.get('MySQL', x)
        self.outPath = conf['path']
    except:
        self.messageBox(""config the 'setting.ini' file first!"")
        return
    try:
        db = pymysql.connect(host=conf['host'], user=conf['user'], password=conf['password'], db=conf['db'], port=int(conf['port']), charset=conf['charset'])
        cursor = db.cursor()
        self.messageBox('connected to the database!\nthe table will be updated.')
    except Exception as e:
        self.messageBox('database connect error!\nerror msg: %s.                    \n===\nplease check your databse setting \nand restart the app.' % e.args[1])
        return
    self.updateTable(w)","conf = {}
try:
    config.readfp(open('setting.ini'))
    head = ['host', 'user', 'password', 'db', 'port', 'charset', 'path']
    for x in head:
        conf[x] = config.get('MySQL', x)
    self.outPath = conf['path']
except:
    self.messageBox(""config the 'setting.ini' file first!"")
    return","config = ConfigParser()
try:
    config.readfp(open('setting.ini'))
    self.outPath = conf['path'] = {x: config.get('MySQL', x) for x in ['host', 'user', 'password', 'db', 'port', 'charset', 'path']}['path']
except:
    self.messageBox(""config the 'setting.ini' file first!"")
    return",find_wrong,1,,,
unilm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unilm/markuplm/examples/fine_tuning/run_websrc/utils.py,https://github.com/microsoft/unilm/tree/master/markuplm/examples/fine_tuning/run_websrc/utils.py,,write_predictions$727,"def write_predictions(all_examples, all_features, all_results, n_best_size, max_answer_length, do_lower_case, output_prediction_file, output_tag_prediction_file, output_nbest_file, verbose_logging, tokenizer):
    """"""
    Compute and write down the final results, including the n best results.

    Arguments:
        all_examples (list[SRCExample]): all the SRC Example of the dataset; note that we only need it to provide the
                                         mapping from example index to the question-answers id.
        all_features (list[InputFeatures]): all the features for the input doc spans.
        all_results (list[RawResult]): all the results from the models.
        n_best_size (int): the number of the n best buffer and the final n best result saved.
        max_answer_length (int): constrain the model to predict the answer no longer than it.
        do_lower_case (bool): whether the model distinguish upper and lower case of the letters.
        output_prediction_file (str): the file which the best answer text predictions will be written to.
        output_tag_prediction_file (str): the file which the best answer tag predictions will be written to.
        output_nbest_file (str): the file which the n best answer predictions including text, tag, and probabilities
                                 will be written to.
        verbose_logging (bool): if true, all of the warnings related to data processing will be printed.
    """"""
    logger.info('Writing predictions to: %s' % output_prediction_file)
    logger.info('Writing nbest to: %s' % output_nbest_file)
    example_index_to_features = collections.defaultdict(list)
    for feature in all_features:
        example_index_to_features[feature.example_index].append(feature)
    unique_id_to_result = {}
    for result in all_results:
        unique_id_to_result[result.unique_id] = result
    _PrelimPrediction = collections.namedtuple('PrelimPrediction', ['feature_index', 'start_index', 'end_index', 'start_logit', 'end_logit', 'tag_ids'])
    all_predictions = collections.OrderedDict()
    all_tag_predictions = collections.OrderedDict()
    all_nbest_json = collections.OrderedDict()
    for (example_index, example) in enumerate(all_examples):
        features = example_index_to_features[example_index]
        prelim_predictions = []
        for (feature_index, feature) in enumerate(features):
            result = unique_id_to_result[feature.unique_id]
            start_indexes = _get_best_indexes(result.start_logits, n_best_size)
            end_indexes = _get_best_indexes(result.end_logits, n_best_size)
            for start_index in start_indexes:
                for end_index in end_indexes:
                    if start_index >= len(feature.tokens):
                        continue
                    if end_index >= len(feature.tokens):
                        continue
                    if start_index not in feature.token_to_orig_map:
                        continue
                    if end_index not in feature.token_to_orig_map:
                        continue
                    if not feature.token_is_max_context.get(start_index, False):
                        continue
                    if end_index < start_index:
                        continue
                    length = end_index - start_index + 1
                    if length > max_answer_length:
                        continue
                    tag_ids = set(feature.token_to_tag_index[start_index:end_index + 1])
                    prelim_predictions.append(_PrelimPrediction(feature_index=feature_index, start_index=start_index, end_index=end_index, start_logit=result.start_logits[start_index], end_logit=result.end_logits[end_index], tag_ids=list(tag_ids)))
        prelim_predictions = sorted(prelim_predictions, key=lambda x: x.start_logit + x.end_logit, reverse=True)
        _NbestPrediction = collections.namedtuple('NbestPrediction', ['text', 'start_logit', 'end_logit', 'tag_ids'])
        seen_predictions = {}
        nbest = []
        for pred in prelim_predictions:
            if len(nbest) >= n_best_size:
                break
            feature = features[pred.feature_index]
            if pred.start_index > 0:
                tok_tokens = feature.tokens[pred.start_index:pred.end_index + 1]
                orig_doc_start = feature.token_to_orig_map[pred.start_index]
                orig_doc_end = feature.token_to_orig_map[pred.end_index]
                orig_tokens = example.doc_tokens[orig_doc_start:orig_doc_end + 1]
                tok_text = ' '.join(tok_tokens)
                tok_text = tok_text.replace(' ##', '')
                tok_text = tok_text.replace('##', '')
                tok_text = tok_text.strip()
                tok_text = ' '.join(tok_text.split())
                orig_text = ' '.join(orig_tokens)
                final_text = _get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)
                if final_text in seen_predictions:
                    continue
                seen_predictions[final_text] = True
            else:
                final_text = ''
                seen_predictions[final_text] = True
            nbest.append(_NbestPrediction(text=final_text, start_logit=pred.start_logit, end_logit=pred.end_logit, tag_ids=pred.tag_ids))
        if not nbest:
            nbest.append(_NbestPrediction(text='empty', start_logit=0.0, end_logit=0.0, tag_ids=[-1]))
        assert len(nbest) >= 1
        total_scores = []
        best_non_null_entry = None
        for entry in nbest:
            total_scores.append(entry.start_logit + entry.end_logit)
            if not best_non_null_entry:
                if entry.text:
                    best_non_null_entry = entry
        probs = _compute_softmax(total_scores)
        nbest_json = []
        for (i, entry) in enumerate(nbest):
            output = collections.OrderedDict()
            output['text'] = entry.text
            output['probability'] = probs[i]
            output['start_logit'] = entry.start_logit
            output['end_logit'] = entry.end_logit
            output['tag_ids'] = entry.tag_ids
            nbest_json.append(output)
        assert len(nbest_json) >= 1
        best = nbest_json[0]['text'].split()
        best = ' '.join([w for w in best if (w[0] != '<' or w[-1] != '>') and w != '<end-of-node>' and (w != tokenizer.sep_token) and (w != tokenizer.cls_token)])
        all_predictions[example.qas_id] = best
        all_tag_predictions[example.qas_id] = nbest_json[0]['tag_ids']
        all_nbest_json[example.qas_id] = nbest_json
    with open(output_prediction_file, 'w') as writer:
        writer.write(json.dumps(all_predictions, indent=4) + '\n')
    with open(output_nbest_file, 'w') as writer:
        writer.write(json.dumps(all_nbest_json, indent=4) + '\n')
    with open(output_tag_prediction_file, 'w') as writer:
        writer.write(json.dumps(all_tag_predictions, indent=4) + '\n')
    return","for feature in all_features:
    example_index_to_features[feature.example_index].append(feature)
unique_id_to_result = {}
for result in all_results:
    unique_id_to_result[result.unique_id] = result
all_predictions = collections.OrderedDict()
all_tag_predictions = collections.OrderedDict()
all_nbest_json = collections.OrderedDict()
for (example_index, example) in enumerate(all_examples):
    features = example_index_to_features[example_index]","example_index_to_features = {feature.example_index: [] for feature in all_features}
for feature in all_features:
    example_index_to_features[feature.example_index].append(feature)
unique_id_to_result = {result.unique_id: result for result in all_results}
all_predictions = collections.OrderedDict()
all_tag_predictions = collections.OrderedDict()
all_nbest_json = collections.OrderedDict()
for (example_index, example) in enumerate(all_examples):
    features = example_index_to_features.get(example_index, [])",find_wrong,1,,,
data-validation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-validation/tensorflow_data_validation/arrow/arrow_util_test.py,https://github.com/tensorflow/data-validation/tree/master/tensorflow_data_validation/arrow/arrow_util_test.py,ArrowUtilTest,testEnumerateArrays$459,"def testEnumerateArrays(self):
    for (leaves_only, has_weights, wrap_flat_struct_in_list) in itertools.product([True, False], [True, False], [True, False]):
        actual_results = {}
        for (feature_path, feature_array, weights) in arrow_util.enumerate_arrays(_INPUT_RECORD_BATCH, _EXAMPLE_WEIGHT_MAP if has_weights else None, leaves_only, wrap_flat_struct_in_list):
            actual_results[feature_path] = (feature_array, weights)
        expected_results = {}
        for p in [['f1'], ['w'], ['w_override1'], ['w_override2'], ['f2', 'sf1'], ['f2', 'sf2', 'ssf1'], ['f3', 'sf1'], ['f3', 'sf2']]:
            feature_path = types.FeaturePath(p)
            expected_results[feature_path] = (_FEATURES_TO_ARRAYS[feature_path].array, _FEATURES_TO_ARRAYS[feature_path].weights if has_weights else None)
        if not leaves_only:
            for p in [['f2'], ['f2', 'sf2'], ['f3']]:
                feature_path = types.FeaturePath(p)
                expected_array = _FEATURES_TO_ARRAYS[feature_path][0]
                if wrap_flat_struct_in_list and pa.types.is_struct(expected_array.type):
                    expected_array = array_util.ToSingletonListArray(expected_array)
                expected_results[feature_path] = (expected_array, _FEATURES_TO_ARRAYS[feature_path].weights if has_weights else None)
        self.assertLen(actual_results, len(expected_results))
        for (k, v) in six.iteritems(expected_results):
            self.assertIn(k, actual_results)
            actual = actual_results[k]
            self.assertTrue(actual[0].equals(v[0]), 'leaves_only={}; has_weights={}; wrap_flat_struct_in_list={} feature={}; expected: {}; actual: {}'.format(leaves_only, has_weights, wrap_flat_struct_in_list, k, v, actual))
            np.testing.assert_array_equal(actual[1], v[1])","itertools.product([True, False], [True, False], [True, False])):","configs = [{'leaves_only': lo, 'has_weights': hw, 'wrap_flat_struct_in_list': wf} for lo in [True, False] for hw in [True, False] for wf in [True, False]]
for config in configs:
    leaves_only = config['leaves_only']
    has_weights = config['has_weights']
    wrap_flat_struct_in_list = config['wrap_flat_struct_in_list']",find_wrong,2,,,
surreal,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/surreal/surreal/learner/base.py,https://github.com/SurrealAI/surreal/tree/master/surreal/learner/base.py,Learner,generate_tensorplex_report$201,"def generate_tensorplex_report(self):
    """"""
            Adds core and system level tensorplex stats
        """"""
    cur_time = time.time()
    current_iter = self.current_iter
    iter_elapsed = current_iter - self.last_iter
    self.last_iter = current_iter
    time_elapsed = cur_time - self.last_time
    self.last_time = cur_time
    core_metrics = {}
    system_metrics = {}
    learn_time = self.learn_timer.avg + 1e-06
    fetch_timer = self._prefetch_queue.timer
    fetch_time = fetch_timer.avg + 1e-06
    iter_time = self.iter_timer.avg + 1e-06
    publish_time = self.publish_timer.avg + 1e-06
    core_metrics['learn_time_s'] = learn_time
    core_metrics['fetch_time_s'] = fetch_time
    core_metrics['publish_time_s'] = publish_time
    core_metrics['iter_time_s'] = iter_time
    iter_per_s = iter_elapsed / time_elapsed
    system_metrics['iter_per_s'] = iter_per_s
    system_metrics['exp_per_s'] = iter_per_s * self.learner_config.replay.batch_size
    system_metrics['compute_load_percent'] = min(learn_time / iter_time * 100, 100)
    system_metrics['io_fetch_experience_load_percent'] = min(fetch_time / iter_time * 100, 100)
    system_metrics['io_publish_load_percent'] = min(publish_time / iter_time * 100, 100)
    all_metrics = {}
    for k in core_metrics:
        all_metrics['.core/' + k] = core_metrics[k]
    for k in system_metrics:
        all_metrics['.system/' + k] = system_metrics[k]
    self.tensorplex.add_scalars(all_metrics)","for k in core_metrics:
    all_metrics['.core/' + k] = core_metrics[k]
for k in system_metrics:
    all_metrics['.system/' + k] = system_metrics[k]","all_metrics = {'.core/' + k: core_metrics[k] for k in core_metrics}
all_metrics.update({'.system/' + k: system_metrics[k] for k in system_metrics})",find_wrong,,,,
pdfx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pdfx/pdfx/libs/xmp.py,https://github.com/metachris/pdfx/tree/master/pdfx/libs/xmp.py,XmpParser,_parse_value$71,"def _parse_value(self, el):
    """""" Extract the metadata value from an element. """"""
    if el.find(RDF_NS + 'Bag') is not None:
        value = []
        for li in el.findall(RDF_NS + 'Bag/' + RDF_NS + 'li'):
            value.append(li.text)
    elif el.find(RDF_NS + 'Seq') is not None:
        value = []
        for li in el.findall(RDF_NS + 'Seq/' + RDF_NS + 'li'):
            value.append(li.text)
    elif el.find(RDF_NS + 'Alt') is not None:
        value = {}
        for li in el.findall(RDF_NS + 'Alt/' + RDF_NS + 'li'):
            value[li.get(XML_NS + 'lang')] = li.text
    else:
        value = el.text
    return value","value = []
            for li in el.findall(RDF_NS + ""Bag/"" + RDF_NS + ""li""):
                value.append(li.text)
        elif el.find(RDF_NS + ""Seq"") is not None:
            value = []
            for li in el.findall(RDF_NS + ""Seq/"" + RDF_NS + ""li""):
                value.append(li.text)
        elif el.find(RDF_NS + ""Alt"") is not None:
            value = {}
            for li in el.findall(RDF_NS + ""Alt/"" + RDF_NS + ""li""):
                value[li.get(XML_NS + ""lang"")] = li.text
        else:
            value = el.text",value = [li.text for li in el.findall(RDF_NS + 'Bag/' + RDF_NS + 'li')] if el.find(RDF_NS + 'Bag') is not None else [li.text for li in el.findall(RDF_NS + 'Seq/' + RDF_NS + 'li')] if el.find(RDF_NS + 'Seq') is not None else {li.get(XML_NS + 'lang'): li.text for li in el.findall(RDF_NS + 'Alt/' + RDF_NS + 'li')} if el.find(RDF_NS + 'Alt') is not None else el.text,find_wrong,2,,,
EasyMocap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyMocap/easymocap/mytools/file_utils.py,https://github.com/zju3dv/EasyMocap/tree/master/easymocap/mytools/file_utils.py,,select_nf$205,"def select_nf(params_all, nf):
    output = {}
    for key in ['poses', 'Rh', 'Th']:
        output[key] = params_all[key][nf:nf + 1, :]
    if 'expression' in params_all.keys():
        output['expression'] = params_all['expression'][nf:nf + 1, :]
    if params_all['shapes'].shape[0] == 1:
        output['shapes'] = params_all['shapes']
    else:
        output['shapes'] = params_all['shapes'][nf:nf + 1, :]
    return output","for key in ['poses', 'Rh', 'Th']:
    output[key] = params_all[key][nf:nf + 1, :]
if 'expression' in params_all.keys():
    output['expression'] = params_all['expression'][nf:nf + 1, :]
if params_all['shapes'].shape[0] == 1:
    output['shapes'] = params_all['shapes']
else:
    output['shapes'] = params_all['shapes'][nf:nf + 1, :]","output = {key: params_all[key][nf:nf + 1, :] for key in ['poses', 'Rh', 'Th']}
output['expression'] = params_all['expression'][nf:nf + 1, :] if 'expression' in params_all.keys() else None
output['shapes'] = params_all['shapes'] if params_all['shapes'].shape[0] == 1 else params_all['shapes'][nf:nf + 1, :]",find_wrong,1,,,
schema,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/schema/schema.py,https://github.com/keleshev/schema/tree/master//schema.py,Schema,_json_schema$484,"def _json_schema(schema, is_main_schema=True, description=None, allow_reference=True):
    Schema = self.__class__

    def _create_or_use_ref(return_dict):
        """"""If not already seen, return the provided part of the schema unchanged.
                If already seen, give an id to the already seen dict and return a reference to the previous part
                of the schema instead.
                """"""
        if not use_refs or is_main_schema:
            return return_schema
        hashed = hash(repr(sorted(return_dict.items())))
        if hashed not in seen:
            seen[hashed] = return_dict
            return return_dict
        else:
            id_str = '#' + str(hashed)
            seen[hashed]['$id'] = id_str
            return {'$ref': id_str}

    def _get_type_name(python_type):
        """"""Return the JSON schema name for a Python type""""""
        if python_type == str:
            return 'string'
        elif python_type == int:
            return 'integer'
        elif python_type == float:
            return 'number'
        elif python_type == bool:
            return 'boolean'
        elif python_type == list:
            return 'array'
        elif python_type == dict:
            return 'object'
        return 'string'

    def _to_json_type(value):
        """"""Attempt to convert a constant value (for ""const"" and ""default"") to a JSON serializable value""""""
        if value is None or type(value) in (str, int, float, bool, list, dict):
            return value
        if type(value) in (tuple, set, frozenset):
            return list(value)
        if isinstance(value, Literal):
            return value.schema
        return str(value)

    def _to_schema(s, ignore_extra_keys):
        if not isinstance(s, Schema):
            return Schema(s, ignore_extra_keys=ignore_extra_keys)
        return s
    s = schema.schema
    i = schema.ignore_extra_keys
    flavor = _priority(s)
    return_schema = {}
    return_description = description or schema.description
    if return_description:
        return_schema['description'] = return_description
    if allow_reference and schema.as_reference:
        if schema.name not in definitions_by_name:
            definitions_by_name[schema.name] = {}
            definitions_by_name[schema.name] = _json_schema(schema, is_main_schema=False, allow_reference=False)
        return_schema['$ref'] = '#/definitions/' + schema.name
    elif flavor == TYPE:
        return_schema['type'] = _get_type_name(s)
    elif flavor == ITERABLE:
        return_schema['type'] = 'array'
        if len(s) == 1:
            return_schema['items'] = _json_schema(_to_schema(s[0], i), is_main_schema=False)
        elif len(s) > 1:
            return_schema['items'] = _json_schema(Schema(Or(*s)), is_main_schema=False)
    elif isinstance(s, Or):
        if all((priority == COMPARABLE for priority in [_priority(value) for value in s.args])):
            or_values = [str(s) if isinstance(s, Literal) else s for s in s.args]
            if len(or_values) == 1:
                return_schema['const'] = _to_json_type(or_values[0])
                return return_schema
            return_schema['enum'] = or_values
        else:
            any_of_values = []
            for or_key in s.args:
                new_value = _json_schema(_to_schema(or_key, i), is_main_schema=False)
                if new_value != {} and new_value not in any_of_values:
                    any_of_values.append(new_value)
            if len(any_of_values) == 1:
                return_schema.update(any_of_values[0])
            else:
                return_schema['anyOf'] = any_of_values
    elif isinstance(s, And):
        all_of_values = []
        for and_key in s.args:
            new_value = _json_schema(_to_schema(and_key, i), is_main_schema=False)
            if new_value != {} and new_value not in all_of_values:
                all_of_values.append(new_value)
        if len(all_of_values) == 1:
            return_schema.update(all_of_values[0])
        else:
            return_schema['allOf'] = all_of_values
    elif flavor == COMPARABLE:
        return_schema['const'] = _to_json_type(s)
    elif flavor == VALIDATOR and type(s) == Regex:
        return_schema['type'] = 'string'
        return_schema['pattern'] = s.pattern_str
    else:
        if flavor != DICT:
            return return_schema
        required_keys = []
        expanded_schema = {}
        additional_properties = i
        for key in s:
            if isinstance(key, Hook):
                continue

            def _key_allows_additional_properties(key):
                """"""Check if a key is broad enough to allow additional properties""""""
                if isinstance(key, Optional):
                    return _key_allows_additional_properties(key.schema)
                return key == str or key == object

            def _get_key_description(key):
                """"""Get the description associated to a key (as specified in a Literal object). Return None if not a Literal""""""
                if isinstance(key, Optional):
                    return _get_key_description(key.schema)
                if isinstance(key, Literal):
                    return key.description
                return None

            def _get_key_name(key):
                """"""Get the name of a key (as specified in a Literal object). Return the key unchanged if not a Literal""""""
                if isinstance(key, Optional):
                    return _get_key_name(key.schema)
                if isinstance(key, Literal):
                    return key.schema
                return key
            additional_properties = additional_properties or _key_allows_additional_properties(key)
            sub_schema = _to_schema(s[key], ignore_extra_keys=i)
            key_name = _get_key_name(key)
            if isinstance(key_name, str):
                if not isinstance(key, Optional):
                    required_keys.append(key_name)
                expanded_schema[key_name] = _json_schema(sub_schema, is_main_schema=False, description=_get_key_description(key))
                if isinstance(key, Optional) and hasattr(key, 'default'):
                    expanded_schema[key_name]['default'] = _to_json_type(_invoke_with_optional_kwargs(key.default, **kwargs) if callable(key.default) else key.default)
            elif isinstance(key_name, Or):
                for or_key in key_name.args:
                    expanded_schema[_get_key_name(or_key)] = _json_schema(sub_schema, is_main_schema=False, description=_get_key_description(or_key))
        return_schema.update({'type': 'object', 'properties': expanded_schema, 'required': required_keys, 'additionalProperties': additional_properties})
    if is_main_schema:
        return_schema.update({'$id': schema_id, '$schema': 'http://json-schema.org/draft-07/schema#'})
        if self._name:
            return_schema['title'] = self._name
        if definitions_by_name:
            return_schema['definitions'] = {}
            for (definition_name, definition) in definitions_by_name.items():
                return_schema['definitions'][definition_name] = definition
    return _create_or_use_ref(return_schema)","for key in s:
    if isinstance(key, Hook):
        continue

    def _get_key_name(key):
        """"""Get the name of a key (as specified in a Literal object). Return the key unchanged if not a Literal""""""
        if isinstance(key, Optional):
            return _get_key_name(key.schema)
        if isinstance(key, Literal):
            return key.schema
        return key
    key_name = _get_key_name(key)
    if isinstance(key_name, str):
        if not isinstance(key, Optional):
            required_keys.append(key_name)
        expanded_schema[key_name] = _json_schema(sub_schema, is_main_schema=False, description=_get_key_description(key))
    elif isinstance(key_name, Or):
        for or_key in key_name.args:
            expanded_schema[_get_key_name(or_key)] = _json_schema(sub_schema, is_main_schema=False, description=_get_key_description(or_key))","expanded_schema = {key_name: _json_schema(sub_schema, is_main_schema=False, description=_get_key_description(key)) if isinstance(key_name, str) and (not isinstance(key, Optional) or key_name in required_keys) else _json_schema(sub_schema, is_main_schema=False, description=_get_key_description(or_key)) for key in s if not isinstance(key, Hook) for or_key in key.schema.args if isinstance(key.schema, Or) for key_name in [key.schema if not isinstance(key.schema, Or) else or_key]}",find_wrong,2,,,
SegFormer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SegFormer/mmseg/datasets/custom.py,https://github.com/NVlabs/SegFormer/tree/master/mmseg/datasets/custom.py,CustomDataset,evaluate$306,"def evaluate(self, results, metric='mIoU', logger=None, efficient_test=False, **kwargs):
    """"""Evaluate the dataset.

        Args:
            results (list): Testing results of the dataset.
            metric (str | list[str]): Metrics to be evaluated. 'mIoU' and
                'mDice' are supported.
            logger (logging.Logger | None | str): Logger used for printing
                related information during evaluation. Default: None.

        Returns:
            dict[str, float]: Default metrics.
        """"""
    if isinstance(metric, str):
        metric = [metric]
    allowed_metrics = ['mIoU', 'mDice']
    if not set(metric).issubset(set(allowed_metrics)):
        raise KeyError('metric {} is not supported'.format(metric))
    eval_results = {}
    gt_seg_maps = self.get_gt_seg_maps(efficient_test)
    if self.CLASSES is None:
        num_classes = len(reduce(np.union1d, [np.unique(_) for _ in gt_seg_maps]))
    else:
        num_classes = len(self.CLASSES)
    ret_metrics = eval_metrics(results, gt_seg_maps, num_classes, self.ignore_index, metric, label_map=self.label_map, reduce_zero_label=self.reduce_zero_label)
    class_table_data = [['Class'] + [m[1:] for m in metric] + ['Acc']]
    if self.CLASSES is None:
        class_names = tuple(range(num_classes))
    else:
        class_names = self.CLASSES
    ret_metrics_round = [np.round(ret_metric * 100, 2) for ret_metric in ret_metrics]
    for i in range(num_classes):
        class_table_data.append([class_names[i]] + [m[i] for m in ret_metrics_round[2:]] + [ret_metrics_round[1][i]])
    summary_table_data = [['Scope'] + ['m' + head for head in class_table_data[0][1:]] + ['aAcc']]
    ret_metrics_mean = [np.round(np.nanmean(ret_metric) * 100, 2) for ret_metric in ret_metrics]
    summary_table_data.append(['global'] + ret_metrics_mean[2:] + [ret_metrics_mean[1]] + [ret_metrics_mean[0]])
    print_log('per class results:', logger)
    table = AsciiTable(class_table_data)
    print_log('\n' + table.table, logger=logger)
    print_log('Summary:', logger)
    table = AsciiTable(summary_table_data)
    print_log('\n' + table.table, logger=logger)
    for i in range(1, len(summary_table_data[0])):
        eval_results[summary_table_data[0][i]] = summary_table_data[1][i] / 100.0
    if mmcv.is_list_of(results, str):
        for file_name in results:
            os.remove(file_name)
    return eval_results",metric = [metric],"metric = [metric] if isinstance(metric, str) else metric",find_wrong,2,,,
RootTheBox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RootTheBox/libs/BotManager.py,https://github.com/moloch--/RootTheBox/tree/master/libs/BotManager.py,BotManager,count_all_teams$137,"def count_all_teams(self):
    from models.Team import Team
    teams = Team.all()
    botcount = {}
    for team in teams:
        botcount[team.uuid] = 0
    for bot in self.botdb.query(Bot).all():
        botcount[bot.team_uuid] += 1
    return botcount","for team in teams:
    botcount[team.uuid] = 0
for bot in self.botdb.query(Bot).all():
    botcount[bot.team_uuid] += 1","botcount = {team.uuid: 0 for team in teams}
botcount.update({bot.team_uuid: botcount.get(bot.team_uuid, 0) + 1 for bot in self.botdb.query(Bot).all()})",find_wrong,1,,,
python-devtools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-devtools/devtools/debug.py,https://github.com/samuelcolvin/python-devtools/tree/master/devtools/debug.py,Debug,_process_args$200,"def _process_args(self, ex: 'Any', args: 'Any', kwargs: 'Any') -> 'Generator[DebugArgument, None, None]':
    import ast
    func_ast = ex.node
    atok = ex.source.asttokens()
    for (arg, ast_arg) in zip(args, func_ast.args):
        if isinstance(ast_arg, ast.Name):
            yield self.output_class.arg_class(arg, name=ast_arg.id)
        else:
            name = ' '.join(map(str.strip, atok.get_text(ast_arg).splitlines()))
            yield self.output_class.arg_class(arg, name=name)
    kw_arg_names = {}
    for kw in func_ast.keywords:
        if isinstance(kw.value, ast.Name):
            kw_arg_names[kw.arg] = kw.value.id
    for (name, value) in kwargs.items():
        yield self.output_class.arg_class(value, name=name, variable=kw_arg_names.get(name))","for kw in func_ast.keywords:
    if isinstance(kw.value, ast.Name):
        kw_arg_names[kw.arg] = kw.value.id
for (name, value) in kwargs.items():
    yield self.output_class.arg_class(value, name=name, variable=kw_arg_names.get(name))","kw_arg_names = {kw.arg: kw.value.id for kw in func_ast.keywords if isinstance(kw.value, ast.Name)}
yield from (self.output_class.arg_class(value, name=name, variable=kw_arg_names.get(name)) for (name, value) in kwargs.items())",find_wrong,1,,,
virt-manager,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/virt-manager/virtManager/createvm.py,https://github.com/virt-manager/virt-manager/tree/master/virtManager/createvm.py,vmmCreateVM,_start_install$1948,"def _start_install(self, guest, installer):
    """"""
        Launch the async job to start the install
        """"""
    bootstrap_args = {}
    if guest.os.is_container() and self._get_config_oscontainer_bootstrap():
        bootstrap_arg_keys = {'src': self._get_config_oscontainer_source_url, 'dest': self.widget('install-oscontainer-fs').get_text, 'user': self._get_config_oscontainer_source_username, 'passwd': self._get_config_oscontainer_source_password, 'insecure': self._get_config_oscontainer_isecure, 'root_password': self._get_config_oscontainer_root_password}
        for (key, getter) in bootstrap_arg_keys.items():
            bootstrap_args[key] = getter()
    parentobj = self._customize_window or self
    progWin = vmmAsyncJob(self._do_async_install, [guest, installer, bootstrap_args], self._install_finished_cb, [guest, parentobj], _('Creating Virtual Machine'), _('The virtual machine is now being created. Allocation of disk storage and retrieval of the installation images may take a few minutes to complete.'), parentobj.topwin)
    progWin.run()","bootstrap_arg_keys = {'src': self._get_config_oscontainer_source_url, 'dest': self.widget('install-oscontainer-fs').get_text, 'user': self._get_config_oscontainer_source_username, 'passwd': self._get_config_oscontainer_source_password, 'insecure': self._get_config_oscontainer_isecure, 'root_password': self._get_config_oscontainer_root_password}
for (key, getter) in bootstrap_arg_keys.items():
    bootstrap_args[key] = getter()","bootstrap_args = {key: getter() for (key, getter) in {'src': self._get_config_oscontainer_source_url, 'dest': self.widget('install-oscontainer-fs').get_text, 'user': self._get_config_oscontainer_source_username, 'passwd': self._get_config_oscontainer_source_password, 'insecure': self._get_config_oscontainer_isecure, 'root_password': self._get_config_oscontainer_root_password}.items()}",find_wrong,-1,,,
mmcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmcv/mmcv/runner/hooks/logger/base.py,https://github.com/open-mmlab/mmcv/tree/master/mmcv/runner/hooks/logger/base.py,LoggerHook,get_lr_tags$92,"def get_lr_tags(self, runner):
    tags = {}
    lrs = runner.current_lr()
    if isinstance(lrs, dict):
        for (name, value) in lrs.items():
            tags[f'learning_rate/{name}'] = value[0]
    else:
        tags['learning_rate'] = lrs[0]
    return tags","lrs = runner.current_lr()
if isinstance(lrs, dict):
    for (name, value) in lrs.items():
        tags[f'learning_rate/{name}'] = value[0]
else:
    tags['learning_rate'] = lrs[0]","tags = {f'learning_rate/{name}': value[0] for (name, value) in runner.current_lr().items()} if isinstance(runner.current_lr(), dict) else {'learning_rate': runner.current_lr()[0]}",find_wrong,1,,,
flax,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flax/examples/wmt/input_pipeline.py,https://github.com/google/flax/tree/master/examples/wmt/input_pipeline.py,,_pack_with_tf_ops$149,"def _pack_with_tf_ops(dataset: tf.data.Dataset, keys: List[str], key2length: Dict[str, int]) -> tf.data.Dataset:
    """"""Helper-function for packing a dataset which has already been batched.

  Helper for pack_dataset()  Uses tf.while_loop.

  Args:
    dataset: a dataset containing padded batches of examples.
    keys: a list of strings
    key2length: an dict from feature-key to integer

  Returns:
    a dataset.
  """"""
    empty_example = {}
    for k in keys:
        empty_example[k] = tf.zeros([0], dtype=tf.int32)
        empty_example[k + '_position'] = tf.zeros([0], dtype=tf.int32)
    keys_etc = empty_example.keys()

    def write_packed_example(partial, outputs):
        new_partial = empty_example.copy()
        new_outputs = {}
        for k in keys_etc:
            new_outputs[k] = outputs[k].write(outputs[k].size(), tf.pad(partial[k], [[0, key2length[k] - tf.size(partial[k])]]))
        return (new_partial, new_outputs)

    def map_fn(x):
        """"""Internal function to flat_map over.

    Consumes a batch of input examples and produces a variable number of output
    examples.
    Args:
      x: a single example

    Returns:
      a tf.data.Dataset
    """"""
        partial = empty_example.copy()
        i = tf.zeros([], dtype=tf.int32)
        dynamic_batch_size = tf.shape(x[keys[0]])[0]
        outputs = {}
        for k in keys:
            outputs[k] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])
            outputs[k + '_position'] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])

        def body_fn(i, partial, outputs):
            """"""Body function for while_loop.

      Args:
        i: integer scalar
        partial: dictionary of Tensor (partially-constructed example)
        outputs: dictionary of TensorArray

      Returns:
        A triple containing the new values of the inputs.
      """"""
            can_append = True
            one_example = {}
            for k in keys:
                val = tf.cast(x[k][i], tf.int32)
                val = val[:tf.reduce_sum(tf.cast(tf.not_equal(val, 0), tf.int32))]
                one_example[k] = val
            for k in keys:
                can_append = tf.logical_and(can_append, tf.less_equal(tf.size(partial[k]) + tf.size(one_example[k]), key2length[k]))

            def false_fn():
                return write_packed_example(partial, outputs)

            def true_fn():
                return (partial, outputs)
            (partial, outputs) = tf.cond(can_append, true_fn, false_fn)
            new_partial = {}
            for k in keys:
                new_seq = one_example[k][:key2length[k]]
                new_seq_len = tf.size(new_seq)
                new_partial[k] = tf.concat([partial[k], new_seq], 0)
                new_partial[k + '_position'] = tf.concat([partial[k + '_position'], tf.range(new_seq_len)], 0)
            partial = new_partial
            return (i + 1, partial, outputs)
        (i, partial, outputs) = tf.while_loop(cond=lambda *_: True, body=body_fn, loop_vars=(i, partial, outputs), shape_invariants=(tf.TensorShape([]), {k: tf.TensorShape([None]) for k in keys_etc}, {k: tf.TensorShape(None) for k in keys_etc}), maximum_iterations=dynamic_batch_size)
        (_, outputs) = write_packed_example(partial, outputs)
        packed = {k: outputs[k].stack() for k in keys_etc}
        for k in keys:
            packed[k + '_segmentation'] = tf.cumsum(tf.cast(tf.equal(packed[k + '_position'], 0), tf.int32), axis=1) * tf.cast(tf.not_equal(packed[k], 0), tf.int32)
        return packed
    dataset = dataset.map(map_fn, num_parallel_calls=AUTOTUNE)
    return dataset.unbatch()","for k in keys:
    empty_example[k] = tf.zeros([0], dtype=tf.int32)
    empty_example[k + '_position'] = tf.zeros([0], dtype=tf.int32)
keys_etc = empty_example.keys()","empty_example = {k: tf.zeros([0], dtype=tf.int32) for k in keys}
empty_example.update({k + '_position': tf.zeros([0], dtype=tf.int32) for k in keys})
keys_etc = empty_example.keys()",find_wrong,1,,,
nnFormer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nnFormer/nnformer/evaluation/model_selection/rank_candidates_cascade.py,https://github.com/282857341/nnFormer/tree/master/nnformer/evaluation/model_selection/rank_candidates_cascade.py,,if_main_my$20,"if __name__ == '__main__':
    summary_files_dir = join(network_training_output_dir, 'summary_jsons_fold0_new')
    output_file = join(network_training_output_dir, 'summary_cascade.csv')
    folds = (0,)
    folds_str = ''
    for f in folds:
        folds_str += str(f)
    plans = 'nnFormerPlansv2.1'
    overwrite_plans = {'nnFormerTrainerCascadeFullRes': ['nnFormerPlans']}
    trainers = ['nnFormerTrainerCascadeFullRes', 'nnFormerTrainerV2CascadeFullRes_EducatedGuess', 'nnFormerTrainerV2CascadeFullRes_EducatedGuess2', 'nnFormerTrainerV2CascadeFullRes_EducatedGuess3', 'nnFormerTrainerV2CascadeFullRes_lowerLR', 'nnFormerTrainerV2CascadeFullRes', 'nnFormerTrainerV2CascadeFullRes_noConnComp', 'nnFormerTrainerV2CascadeFullRes_shorter_lowerLR', 'nnFormerTrainerV2CascadeFullRes_shorter', 'nnFormerTrainerV2CascadeFullRes_smallerBinStrel']
    datasets = {'Task003_Liver': ('3d_cascade_fullres',), 'Task006_Lung': ('3d_cascade_fullres',), 'Task007_Pancreas': ('3d_cascade_fullres',), 'Task008_HepaticVessel': ('3d_cascade_fullres',), 'Task009_Spleen': ('3d_cascade_fullres',), 'Task010_Colon': ('3d_cascade_fullres',), 'Task017_AbdominalOrganSegmentation': ('3d_cascade_fullres',), 'Task048_KiTS_clean': ('3d_cascade_fullres',), 'Task055_SegTHOR': ('3d_cascade_fullres',), 'Task056_VerSe': ('3d_cascade_fullres',)}
    expected_validation_folder = 'validation_raw'
    alternative_validation_folder = 'validation'
    alternative_alternative_validation_folder = 'validation_tiledTrue_doMirror_True'
    interested_in = 'mean'
    result_per_dataset = {}
    for d in datasets:
        result_per_dataset[d] = {}
        for c in datasets[d]:
            result_per_dataset[d][c] = []
    valid_trainers = []
    all_trainers = []
    with open(output_file, 'w') as f:
        f.write('trainer,')
        for t in datasets.keys():
            s = t[4:7]
            for c in datasets[t]:
                s1 = s + '_' + c[3]
                f.write('%s,' % s1)
        f.write('\n')
        for trainer in trainers:
            trainer_plans = [plans]
            if trainer in overwrite_plans.keys():
                trainer_plans = overwrite_plans[trainer]
            result_per_dataset_here = {}
            for d in datasets:
                result_per_dataset_here[d] = {}
            for p in trainer_plans:
                name = '%s__%s' % (trainer, p)
                all_present = True
                all_trainers.append(name)
                f.write('%s,' % name)
                for dataset in datasets.keys():
                    for configuration in datasets[dataset]:
                        summary_file = join(summary_files_dir, '%s__%s__%s__%s__%s__%s.json' % (dataset, configuration, trainer, p, expected_validation_folder, folds_str))
                        if not isfile(summary_file):
                            summary_file = join(summary_files_dir, '%s__%s__%s__%s__%s__%s.json' % (dataset, configuration, trainer, p, alternative_validation_folder, folds_str))
                            if not isfile(summary_file):
                                summary_file = join(summary_files_dir, '%s__%s__%s__%s__%s__%s.json' % (dataset, configuration, trainer, p, alternative_alternative_validation_folder, folds_str))
                                if not isfile(summary_file):
                                    all_present = False
                                    print(name, dataset, configuration, 'has missing summary file')
                        if isfile(summary_file):
                            result = load_json(summary_file)['results'][interested_in]['mean']['Dice']
                            result_per_dataset_here[dataset][configuration] = result
                            f.write('%02.4f,' % result)
                        else:
                            f.write('NA,')
                            result_per_dataset_here[dataset][configuration] = 0
                f.write('\n')
                if True:
                    valid_trainers.append(name)
                    for d in datasets:
                        for c in datasets[d]:
                            result_per_dataset[d][c].append(result_per_dataset_here[d][c])
    invalid_trainers = [i for i in all_trainers if i not in valid_trainers]
    num_valid = len(valid_trainers)
    num_datasets = len(datasets.keys())
    all_res = np.zeros((num_valid, num_datasets))
    for (j, d) in enumerate(datasets.keys()):
        ks = list(result_per_dataset[d].keys())
        tmp = result_per_dataset[d][ks[0]]
        for k in ks[1:]:
            for i in range(len(tmp)):
                tmp[i] = max(tmp[i], result_per_dataset[d][k][i])
        all_res[:, j] = tmp
    ranks_arr = np.zeros_like(all_res)
    for d in range(ranks_arr.shape[1]):
        temp = np.argsort(all_res[:, d])[::-1]
        ranks = np.empty_like(temp)
        ranks[temp] = np.arange(len(temp))
        ranks_arr[:, d] = ranks
    mn = np.mean(ranks_arr, 1)
    for i in np.argsort(mn):
        print(mn[i], valid_trainers[i])
    print()
    print(valid_trainers[np.argmin(mn)])","for f in folds:
    folds_str += str(f)",folds_str = ''.join((str(f) for f in folds)),find_wrong,2,,,
DetectoRS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DetectoRS/mmdet/core/fp16/decorators.py,https://github.com/joe-siyuan-qiao/DetectoRS/tree/master/mmdet/core/fp16/decorators.py,,auto_fp16$9,"def auto_fp16(apply_to=None, out_fp32=False):
    """"""Decorator to enable fp16 training automatically.

    This decorator is useful when you write custom modules and want to support
    mixed precision training. If inputs arguments are fp32 tensors, they will
    be converted to fp16 automatically. Arguments other than fp32 tensors are
    ignored.

    Args:
        apply_to (Iterable, optional): The argument names to be converted.
            `None` indicates all arguments.
        out_fp32 (bool): Whether to convert the output back to fp32.

    :Example:

        class MyModule1(nn.Module)

            # Convert x and y to fp16
            @auto_fp16()
            def forward(self, x, y):
                pass

        class MyModule2(nn.Module):

            # convert pred to fp16
            @auto_fp16(apply_to=('pred', ))
            def do_something(self, pred, others):
                pass
    """"""

    def auto_fp16_wrapper(old_func):

        @functools.wraps(old_func)
        def new_func(*args, **kwargs):
            if not isinstance(args[0], torch.nn.Module):
                raise TypeError('@auto_fp16 can only be used to decorate the method of nn.Module')
            if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
                return old_func(*args, **kwargs)
            args_info = getfullargspec(old_func)
            args_to_cast = args_info.args if apply_to is None else apply_to
            new_args = []
            if args:
                arg_names = args_info.args[:len(args)]
                for (i, arg_name) in enumerate(arg_names):
                    if arg_name in args_to_cast:
                        new_args.append(cast_tensor_type(args[i], torch.float, torch.half))
                    else:
                        new_args.append(args[i])
            new_kwargs = {}
            if kwargs:
                for (arg_name, arg_value) in kwargs.items():
                    if arg_name in args_to_cast:
                        new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
                    else:
                        new_kwargs[arg_name] = arg_value
            output = old_func(*new_args, **new_kwargs)
            if out_fp32:
                output = cast_tensor_type(output, torch.half, torch.float)
            return output
        return new_func
    return auto_fp16_wrapper","if kwargs:
    for (arg_name, arg_value) in kwargs.items():
        if arg_name in args_to_cast:
            new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
        else:
            new_kwargs[arg_name] = arg_value","new_kwargs = {arg_name: cast_tensor_type(arg_value, torch.float, torch.half) if arg_name in args_to_cast else arg_value for (arg_name, arg_value) in kwargs.items()}",find_wrong,-1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/boto_rds.py,https://github.com/saltstack/salt/tree/master/salt/modules/boto_rds.py,,describe_parameter_group$986,"def describe_parameter_group(name, Filters=None, MaxRecords=None, Marker=None, region=None, key=None, keyid=None, profile=None):
    """"""
    Returns a list of `DBParameterGroup` descriptions.
    CLI example to description of parameter group::

        salt myminion boto_rds.describe_parameter_group parametergroupname            region=us-east-1
    """"""
    res = __salt__['boto_rds.parameter_group_exists'](name, tags=None, region=region, key=key, keyid=keyid, profile=profile)
    if not res.get('exists'):
        return {'exists': bool(res)}
    try:
        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
        if not conn:
            return {'results': bool(conn)}
        kwargs = {}
        for key in ('Marker', 'Filters'):
            if locals()[key] is not None:
                kwargs[key] = str(locals()[key])
        if locals()['MaxRecords'] is not None:
            kwargs['MaxRecords'] = int(locals()['MaxRecords'])
        info = conn.describe_db_parameter_groups(DBParameterGroupName=name, **kwargs)
        if not info:
            return {'results': bool(info), 'message': 'Failed to get RDS description for group {}.'.format(name)}
        return {'results': bool(info), 'message': 'Got RDS descrition for group {}.'.format(name)}
    except ClientError as e:
        return {'error': __utils__['boto3.get_error'](e)}","for key in ('Marker', 'Filters'):
    if locals()[key] is not None:
        kwargs[key] = str(locals()[key])
if locals()['MaxRecords'] is not None:
    kwargs['MaxRecords'] = int(locals()['MaxRecords'])","kwargs = {key: str(locals()[key]) for key in ('Marker', 'Filters') if locals()[key] is not None}
kwargs['MaxRecords'] = int(locals()['MaxRecords']) if locals()['MaxRecords'] is not None else None",find_wrong,1,,,
second.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/second.pytorch/second/data/kitti_common.py,https://github.com/traveller59/second.pytorch/tree/master/second/data/kitti_common.py,,remove_low_score$351,"def remove_low_score(image_anno, thresh):
    img_filtered_annotations = {}
    relevant_annotation_indices = [i for (i, s) in enumerate(image_anno['score']) if s >= thresh]
    for key in image_anno.keys():
        img_filtered_annotations[key] = image_anno[key][relevant_annotation_indices]
    return img_filtered_annotations","relevant_annotation_indices = [i for (i, s) in enumerate(image_anno['score']) if s >= thresh]
for key in image_anno.keys():
    img_filtered_annotations[key] = image_anno[key][relevant_annotation_indices]","relevant_annotation_indices = [i for (i, s) in enumerate(image_anno['score']) if s >= thresh]
img_filtered_annotations = {key: image_anno[key][relevant_annotation_indices] for key in image_anno.keys()}",find_wrong,1,,,
lightning-flash,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lightning-flash/flash/tabular/data.py,https://github.com/PyTorchLightning/lightning-flash/tree/master/flash/tabular/data.py,TabularDeserializer,example_input$213,"def example_input(self) -> str:
    parameters = self.parameters
    row = {}
    for cat_col in parameters['categorical_fields']:
        row[cat_col] = ['test']
    for num_col in parameters['numerical_fields']:
        row[num_col] = [0]
    return str(DataFrame.from_dict(row).to_csv())","for cat_col in parameters['categorical_fields']:
    row[cat_col] = ['test']
for num_col in parameters['numerical_fields']:
    row[num_col] = [0]","row = {cat_col: ['test'] for cat_col in parameters['categorical_fields']}
row.update({num_col: [0] for num_col in parameters['numerical_fields']})",find_wrong,,,,
sentry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/api/serializers/models/deploy.py,https://github.com/getsentry/sentry/tree/master/src/sentry/api/serializers/models/deploy.py,DeploySerializer,get_attrs$7,"def get_attrs(self, item_list, user, **kwargs):
    environments = {id: name for (id, name) in Environment.objects.filter(id__in=[d.environment_id for d in item_list]).values_list('id', 'name')}
    result = {}
    for item in item_list:
        result[item] = {'environment': environments.get(item.environment_id)}
    return result","id: name
    for id, name in Environment.objects.filter(
        id__in=[d.environment_id for d in item_list]
    ).values_list(""id"", ""name"")
}

result = {}
for item in item_list:
    result[item] = {""environment"": environments.get(item.environment_id)}","environments = {id: name for (id, name) in Environment.objects.filter(id__in=[d.environment_id for d in item_list]).values_list('id', 'name')}
result = {item: {'environment': environments.get(item.environment_id)} for item in item_list}",find_wrong,2,,,
celeb-detection-oss,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/celeb-detection-oss/model_training/utils.py,https://github.com/Giphy/celeb-detection-oss/tree/master/model_training/utils.py,,labels_by_name$57,"def labels_by_name(path):
    labels = {}
    with open(path) as f:
        file_reader = csv.reader(f)
        next(file_reader)
        rows = sorted(file_reader, key=lambda x: x[0])
        for (i, row) in enumerate(rows):
            labels[row[0]] = i
    return labels","with open(path) as f:
    file_reader = csv.reader(f)
    next(file_reader)
    rows = sorted(file_reader, key=lambda x: x[0])
    for (i, row) in enumerate(rows):
        labels[row[0]] = i","with open(path) as f:
    file_reader = csv.reader(f)
    next(file_reader)
    labels = {row[0]: i for (i, row) in enumerate(sorted(file_reader, key=lambda x: x[0]))}",find_wrong,1,,,
spaCy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spaCy/spacy/lang/el/get_pos_from_wiktionary.py,https://github.com/explosion/spaCy/tree/master/spacy/lang/el/get_pos_from_wiktionary.py,,get_pos_from_wiktionary$1,"def get_pos_from_wiktionary():
    import re
    from gensim.corpora.wikicorpus import extract_pages
    regex = re.compile('==={{(\\w+)\\|el}}===')
    regex2 = re.compile('==={{(\\w+ \\w+)\\|el}}===')
    expected_parts = ['渭蔚蟿慰蠂萎', '蟻萎渭伪', '蔚蟺委胃蔚蟿慰', '蔚蟺委蟻蟻畏渭伪', '慰蠀蟽喂伪蟽蟿喂魏蠈', '魏蠉蟻喂慰 蠈谓慰渭伪', '维蟻胃蟻慰']
    wiktionary_file_path = '/data/gsoc2018-spacy/spacy/lang/el/res/elwiktionary-latest-pages-articles.xml'
    proper_names_dict = {'慰蠀蟽喂伪蟽蟿喂魏蠈': 'nouns', '蔚蟺委胃蔚蟿慰': 'adjectives', '维蟻胃蟻慰': 'dets', '蔚蟺委蟻蟻畏渭伪': 'adverbs', '魏蠉蟻喂慰 蠈谓慰渭伪': 'proper_names', '渭蔚蟿慰蠂萎': 'participles', '蟻萎渭伪': 'verbs'}
    expected_parts_dict = {}
    for expected_part in expected_parts:
        expected_parts_dict[expected_part] = []
    for (title, text, pageid) in extract_pages(wiktionary_file_path):
        if text.startswith('#REDIRECT'):
            continue
        title = title.lower()
        all_regex = regex.findall(text)
        all_regex.extend(regex2.findall(text))
        for a in all_regex:
            if a in expected_parts:
                expected_parts_dict[a].append(title)
    for i in expected_parts_dict:
        with open('_{0}.py'.format(proper_names_dict[i]), 'w') as f:
            f.write('from __future__ import unicode_literals\n')
            f.write('{} = set(""""""\n'.format(proper_names_dict[i].upper()))
            words = sorted(expected_parts_dict[i])
            line = ''
            to_write = []
            for word in words:
                if len(line + ' ' + word) > 79:
                    to_write.append(line)
                    line = ''
                else:
                    line = line + ' ' + word
            f.write('\n'.join(to_write))
            f.write('\n"""""".split())')","for expected_part in expected_parts:
    expected_parts_dict[expected_part] = []",expected_parts_dict = {expected_part: [] for expected_part in expected_parts},find_wrong,1,,,
kuma,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kuma/kuma/settings/common.py,https://github.com/mdn/kuma/tree/master/kuma/settings/common.py,,_get_locales$156,"def _get_locales():
    """"""
    Load LOCALES data from languages.json

    languages.json is from the product-details project:
    https://product-details.mozilla.org/1.0/languages.json
    """"""
    lang_path = BASE_DIR / 'settings' / 'languages.json'
    with open(lang_path) as lang_file:
        json_locales = json.load(lang_file)
    locales = {}
    _Language = namedtuple('Language', 'english native')
    for (locale, meta) in json_locales.items():
        locales[locale] = _Language(meta['English'], meta['native'])
    return locales","_Language = namedtuple('Language', 'english native')
for (locale, meta) in json_locales.items():
    locales[locale] = _Language(meta['English'], meta['native'])","locales = {locale: _Language(meta['English'], meta['native']) for (locale, meta) in json_locales.items()}",find_wrong,1,,,
pbtk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pbtk/extractors/jar_extract.py,https://github.com/marin-m/pbtk/tree/master/extractors/jar_extract.py,,handle_jar$43,"def handle_jar(path):
    if path.endswith('.jar'):
        yield ('_progress', ('Decompressing JAR...', None))
    else:
        yield ('_progress', ('Converting DEX to JAR...', None))
    with JarWrapper(path) as jar:
        enums = {}
        pkg_to_codedinputstream = OrderedDict()
        pkg_to_codedoutputstream = {}
        map_entry_cls = []
        out_additional_cls = []
        pkg_to_j2me_protobuftype = OrderedDict()
        '\n        First iteration on classes: look for library classes signatures.\n        '
        for (i, cls) in enumerate(jar.classes):
            if i % 10 == 0:
                yield ('_progress', ('Scanning Java package contents...', i / len(jar.classes) * 0.5))
            pkg = cls[:cls.rfind('.')] if '.' in cls else ''
            binr = jar.read(cls)
            raw_cls = cls.replace('.', '/').encode('utf8')
            ""\n            Handle multiple cases:\n            1. CodedInputStream, before it was split out in multiple\n               subclasses (cc8ca5b - oct 2016)\n            2. CodedInputStream, after it was\n            3. CodedInputByteBufferNano\n            4. CodedInputStreamMicro\n            \n            The second case doesn't provide intelligible strings to\n            search for, so we'll use method signatures (the different\n            kinds that can be produced by Proguard) instead.\n            ""
            SIG_NANO = b'([BII)V'
            SIG_NANO_2 = b'([BI)V'
            SIG_DEF = b'([BIIZ)L%s;' % raw_cls
            SIG_DEF_2 = b'([BII)L%s;' % raw_cls
            SIG_CALL = b'([BIIZ)V'
            SIG_CALL_2 = b'([BII)V'
            SIG_CALL_3 = b'([BIIZL'
            has_constructor = SIG_DEF in binr or SIG_DEF_2 in binr
            calls_arraydecoder = SIG_CALL in binr or SIG_CALL_2 in binr or SIG_CALL_3 in binr
            is_legit_class = b'Beginning index' not in binr and b'Number too large' not in binr and (b'a byte array' not in binr)
            has_constructor_nano = SIG_NANO in binr or SIG_NANO_2 in binr
            has_relevant_string = b'message contained an invalid tag' in binr
            has_relevant_string_nano = b'is beyond current' in binr
            has_relevant_string_micro = b""when buffer wasn't empty"" in binr
            '\n            Try to match CodedOutputStream before CodedInputStream, as\n            it may have common points in signatures but always has a\n            recognizable string.\n            '
            has_out_constructor = b'([BII' in binr
            has_out_relevant_string = b'write as much data as' in binr
            has_out_relevant_string_old = b'UTF-8 not supported.' in binr
            has_out_relevant_string_nano = b'Unpaired surrogate at index ' in binr and b'wrap' in binr
            has_out_relevant_string_2 = b'Converting ill-formed UTF-16.' in binr and b'Pos:' not in binr
            is_legit_out_class = b'byte array' not in binr
            if has_out_constructor and ((has_out_relevant_string or has_out_relevant_string_old) and is_legit_out_class or has_out_relevant_string_nano or has_out_relevant_string_2):
                while pkg in pkg_to_codedoutputstream:
                    pkg += '_'
                pkg_to_codedoutputstream[pkg] = cls
            elif has_constructor and is_legit_class and (calls_arraydecoder or has_relevant_string) or (has_constructor_nano and (has_relevant_string_nano or has_relevant_string_micro)):
                while pkg in pkg_to_codedinputstream:
                    pkg += '_'
                pkg_to_codedinputstream[pkg] = cls
            elif b'Generated message class' in binr:
                out_additional_cls.append(cls)
            elif b'is not a primitive type' in binr:
                map_entry_cls.append(cls)
            elif b'Groups are not allowed in maps' in binr or b'a map entry message.' in binr:
                map_entry_cls.append(cls)
            elif b'Unexp.EOF' in binr:
                code = jar.decomp(cls, True).raw
                protobuftype_cls = search('public \\w+\\(([\\w.$]+) \\w+\\)', code).group(1)
                default_consts = {}
                for (prop, const) in findall('(\\w+) = new Boolean\\((\\w+)\\)', code):
                    default_consts[cls + '.' + prop] = const
                while pkg in pkg_to_j2me_protobuftype:
                    pkg += '_'
                pkg_to_j2me_protobuftype[pkg] = (protobuftype_cls, default_consts)
        for pkg in list(pkg_to_codedinputstream):
            if pkg not in pkg_to_codedoutputstream:
                del pkg_to_codedinputstream[pkg]
        '\n        Second iteration on classes: look for generated classes, that\n        contains method call signatures [1] for libraries we found, or\n        other extractible information.\n        \n        [1] https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.3\n        '
        gen_classes = OrderedDict()
        gen_classes_j2me = OrderedDict()
        had_metadata = set()
        for (i, cls) in enumerate(jar.classes):
            if i % 10 == 0:
                yield ('_progress', ('Scanning Java package contents...', i / len(jar.classes) * 0.5 + 0.5))
            binr = jar.read(cls)
            if b'.proto\x12' in binr or b'.protodevel\x12' in binr:
                code = jar.decomp(cls, True).raw
                code = sub('"",\\s+""', '', code, flags=MULTILINE)
                meta = search('""(\\\\n.+?\\.proto.+)""', code)
                if meta:
                    meta = meta.group(1).encode('latin1')
                    meta = meta.decode('unicode_escape').encode('latin1')
                    yield from walk_binary(meta)
                    had_metadata.add(cls)
            for impl in pkg_to_codedinputstream:
                if b'%s' % pkg_to_codedinputstream[impl].replace('.', '/').encode('ascii') in binr and b'(L%s;' % pkg_to_codedoutputstream[impl].replace('.', '/').encode('ascii') in binr and (cls not in (pkg_to_codedinputstream[impl], pkg_to_codedoutputstream[impl])):
                    gen_classes[cls] = (pkg_to_codedinputstream[impl], pkg_to_codedoutputstream[impl])
            for (impl, (protobuftype_cls, consts)) in pkg_to_j2me_protobuftype.items():
                if b'(IILjava/lang/Object;)L%s;' % protobuftype_cls.replace('.', '/').encode('ascii') in binr and cls != protobuftype_cls:
                    gen_classes_j2me[cls] = (protobuftype_cls, consts)
            if b'Ljava/lang/Enum<' in binr[:256]:
                enums[cls] = cls
                if '$' in cls:
                    enums[cls.replace('$', '.')] = cls
                    enums[cls.rsplit('.', 1)[0] + '.' + cls.rsplit('$', 1)[1]] = cls
        gen_classes_nodollar = OrderedDict(gen_classes)
        for (cls, pkg) in OrderedDict(gen_classes_nodollar).items():
            if '$' in cls:
                gen_classes_nodollar[cls.replace('$', '.')] = pkg
                gen_classes_nodollar[cls.rsplit('.', 1)[0] + '.' + cls.rsplit('$', 1)[1]] = pkg
        '\n        Once we know what classes we should look at, do the actual code\n        scraping and extraction work.\n        '
        msg_path_to_obj = OrderedDict()
        msg_to_referrers = defaultdict(list)
        for (i, (cls, (codedinputstream, codedoutputstream))) in enumerate(gen_classes.items()):
            yield ('_progress', ('Extracting %s...' % cls, i / len(gen_classes)))
            if cls.split('$')[0] not in had_metadata:
                extract_lite(jar, cls, enums, gen_classes_nodollar, codedinputstream, codedoutputstream, map_entry_cls, out_additional_cls, msg_path_to_obj, msg_to_referrers)
        for (i, (cls, (protobuftype_cls, consts))) in enumerate(gen_classes_j2me.items()):
            yield ('_progress', ('Extracting %s...' % cls, i / len(gen_classes_j2me)))
            extract_j2me(jar, cls, enums, gen_classes_j2me, protobuftype_cls, consts, msg_path_to_obj, msg_to_referrers)
        yield ('_progress', ('Dumping information to .protos...', None))
        yield from nest_and_print_to_files(msg_path_to_obj, msg_to_referrers)
        yield from jar.bonus_protos.items()","for (i, cls) in enumerate(jar.classes):
    ...
    while pkg in pkg_to_codedinputstream:
        pkg += '_'
    pkg_to_codedinputstream[pkg] = cls","pkg_to_codedinputstream = {pkg: cls for (i, cls) in enumerate(jar.classes) for pkg in [cls[:cls.rfind('.')] if '.' in cls else ''] if pkg not in pkg_to_codedinputstream}",find_wrong,2,,,
CLUEPretrainedModels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUEPretrainedModels/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_utils.py,https://github.com/CLUEbenchmark/CLUEPretrainedModels/tree/master/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_utils.py,PreTrainedTokenizer,_from_pretrained$286,"def _from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs):
    cache_dir = kwargs.pop('cache_dir', None)
    force_download = kwargs.pop('force_download', False)
    proxies = kwargs.pop('proxies', None)
    s3_models = list(cls.max_model_input_sizes.keys())
    vocab_files = {}
    init_configuration = {}
    if pretrained_model_name_or_path in s3_models:
        for (file_id, map_list) in cls.pretrained_vocab_files_map.items():
            vocab_files[file_id] = map_list[pretrained_model_name_or_path]
        if cls.pretrained_init_configuration and pretrained_model_name_or_path in cls.pretrained_init_configuration:
            init_configuration = cls.pretrained_init_configuration[pretrained_model_name_or_path]
    else:
        logger.info(""Model name '{}' not found in model shortcut name list ({}). Assuming '{}' is a path or url to a directory containing tokenizer files."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path))
        for (file_id, file_name) in cls.vocab_files_names.items():
            if os.path.isdir(pretrained_model_name_or_path):
                full_file_name = os.path.join(pretrained_model_name_or_path, file_name)
            else:
                full_file_name = pretrained_model_name_or_path
            if not os.path.exists(full_file_name):
                logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
                full_file_name = None
            vocab_files[file_id] = full_file_name
        additional_files_names = {'added_tokens_file': ADDED_TOKENS_FILE, 'special_tokens_map_file': SPECIAL_TOKENS_MAP_FILE, 'tokenizer_config_file': TOKENIZER_CONFIG_FILE}
        saved_directory = pretrained_model_name_or_path
        if os.path.exists(saved_directory) and (not os.path.isdir(saved_directory)):
            saved_directory = os.path.dirname(saved_directory)
        for (file_id, file_name) in additional_files_names.items():
            full_file_name = os.path.join(saved_directory, file_name)
            if not os.path.exists(full_file_name):
                logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
                full_file_name = None
            vocab_files[file_id] = full_file_name
        if all((full_file_name is None for full_file_name in vocab_files.values())):
            raise EnvironmentError(""Model name '{}' was not found in tokenizers model name list ({}). We assumed '{}' was a path or url to a directory containing vocabulary files named {} but couldn't find such vocabulary files at this path or url."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path, list(cls.vocab_files_names.values())))
    try:
        resolved_vocab_files = {}
        for (file_id, file_path) in vocab_files.items():
            if file_path is None:
                resolved_vocab_files[file_id] = None
            else:
                resolved_vocab_files[file_id] = cached_path(file_path, cache_dir=cache_dir, force_download=force_download, proxies=proxies)
    except EnvironmentError:
        if pretrained_model_name_or_path in s3_models:
            msg = ""Couldn't reach server at '{}' to download vocabulary files.""
        else:
            msg = ""Model name '{}' was not found in tokenizers model name list ({}). We assumed '{}' was a path or url to a directory containing vocabulary files named {}, but couldn't find such vocabulary files at this path or url."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path, list(cls.vocab_files_names.values()))
        raise EnvironmentError(msg)
    for (file_id, file_path) in vocab_files.items():
        if file_path == resolved_vocab_files[file_id]:
            logger.info('loading file {}'.format(file_path))
        else:
            logger.info('loading file {} from cache at {}'.format(file_path, resolved_vocab_files[file_id]))
    tokenizer_config_file = resolved_vocab_files.pop('tokenizer_config_file', None)
    if tokenizer_config_file is not None:
        init_kwargs = json.load(open(tokenizer_config_file, encoding='utf-8'))
        saved_init_inputs = init_kwargs.pop('init_inputs', ())
        if not init_inputs:
            init_inputs = saved_init_inputs
    else:
        init_kwargs = init_configuration
    init_kwargs.update(kwargs)
    if pretrained_model_name_or_path in cls.max_model_input_sizes:
        max_len = cls.max_model_input_sizes[pretrained_model_name_or_path]
        if max_len is not None and isinstance(max_len, (int, float)):
            init_kwargs['max_len'] = min(init_kwargs.get('max_len', int(1000000000000.0)), max_len)
    added_tokens_file = resolved_vocab_files.pop('added_tokens_file', None)
    special_tokens_map_file = resolved_vocab_files.pop('special_tokens_map_file', None)
    for (args_name, file_path) in resolved_vocab_files.items():
        if args_name not in init_kwargs:
            init_kwargs[args_name] = file_path
    if special_tokens_map_file is not None:
        special_tokens_map = json.load(open(special_tokens_map_file, encoding='utf-8'))
        for (key, value) in special_tokens_map.items():
            if key not in init_kwargs:
                init_kwargs[key] = value
    tokenizer = cls(*init_inputs, **init_kwargs)
    tokenizer.init_inputs = init_inputs
    tokenizer.init_kwargs = init_kwargs
    if added_tokens_file is not None:
        added_tok_encoder = json.load(open(added_tokens_file, encoding='utf-8'))
        added_tok_decoder = {v: k for (k, v) in added_tok_encoder.items()}
        tokenizer.added_tokens_encoder.update(added_tok_encoder)
        tokenizer.added_tokens_decoder.update(added_tok_decoder)
    return tokenizer","if pretrained_model_name_or_path in s3_models:
    for (file_id, map_list) in cls.pretrained_vocab_files_map.items():
        vocab_files[file_id] = map_list[pretrained_model_name_or_path]
    if cls.pretrained_init_configuration and pretrained_model_name_or_path in cls.pretrained_init_configuration:
        init_configuration = cls.pretrained_init_configuration[pretrained_model_name_or_path]
else:
    logger.info(""Model name '{}' not found in model shortcut name list ({}). Assuming '{}' is a path or url to a directory containing tokenizer files."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path))
    for (file_id, file_name) in cls.vocab_files_names.items():
        if os.path.isdir(pretrained_model_name_or_path):
            full_file_name = os.path.join(pretrained_model_name_or_path, file_name)
        else:
            full_file_name = pretrained_model_name_or_path
        if not os.path.exists(full_file_name):
            logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
            full_file_name = None
        vocab_files[file_id] = full_file_name
    additional_files_names = {'added_tokens_file': ADDED_TOKENS_FILE, 'special_tokens_map_file': SPECIAL_TOKENS_MAP_FILE, 'tokenizer_config_file': TOKENIZER_CONFIG_FILE}
    saved_directory = pretrained_model_name_or_path
    if os.path.exists(saved_directory) and (not os.path.isdir(saved_directory)):
        saved_directory = os.path.dirname(saved_directory)
    for (file_id, file_name) in additional_files_names.items():
        full_file_name = os.path.join(saved_directory, file_name)
        if not os.path.exists(full_file_name):
            logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
            full_file_name = None
        vocab_files[file_id] = full_file_name
    if all((full_file_name is None for full_file_name in vocab_files.values())):
        raise EnvironmentError(""Model name '{}' was not found in tokenizers model name list ({}). We assumed '{}' was a path or url to a directory containing vocabulary files named {} but couldn't find such vocabulary files at this path or url."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path, list(cls.vocab_files_names.values())))","vocab_files = {file_id: map_list[pretrained_model_name_or_path] if pretrained_model_name_or_path in s3_models else os.path.join(pretrained_model_name_or_path, file_name) if not os.path.isdir(pretrained_model_name_or_path) else os.path.join(pretrained_model_name_or_path, file_name) if os.path.exists(os.path.join(pretrained_model_name_or_path, file_name)) else None for (file_id, map_list) in cls.pretrained_vocab_files_map.items() for (file_id, file_name) in cls.vocab_files_names.items()}",find_wrong,1,,,
random-network-distillation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/random-network-distillation/mpi_util.py,https://github.com/openai/random-network-distillation/tree/master//mpi_util.py,,dict_gather_mean$111,"def dict_gather_mean(comm, d):
    alldicts = comm.allgather(d)
    size = comm.Get_size()
    k2li = defaultdict(list)
    for d in alldicts:
        for (k, v) in d.items():
            k2li[k].append(v)
    k2mean = {}
    for (k, li) in k2li.items():
        k2mean[k] = np.mean(li, axis=0) if len(li) == size else np.nan
    return k2mean","for d in alldicts:
    for (k, v) in d.items():
        k2li[k].append(v)
k2mean = {}
for (k, li) in k2li.items():
    k2mean[k] = np.mean(li, axis=0) if len(li) == size else np.nan","k2li = {k: [d[k] for d in alldicts] for k in d.keys() for d in alldicts}
k2mean = {k: np.mean(li, axis=0) if len(li) == size else np.nan for (k, li) in k2li.items()}",find_wrong,1,,,
causalnex,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/causalnex/causalnex/ebaybbn/bbn.py,https://github.com/quantumblacklabs/causalnex/tree/master/causalnex/ebaybbn/bbn.py,JoinTree,initialize_potentials$202,"def initialize_potentials(self, assignments, bbn, evidence={}):
    for node in self.nodes:
        tt = {}
        vals = []
        variables = node.variable_names
        variables.sort()
        for variable in variables:
            domain = bbn.domains.get(variable, [True, False])
            vals.append(list(product([variable], domain)))
        permutations = product(*vals)
        for permutation in permutations:
            tt[permutation] = 1
        node.potential_tt = tt
    for (clique, bbn_nodes) in assignments.items():
        tt = {}
        vals = []
        variables = list(clique.variable_names)
        variables.sort()
        for variable in variables:
            domain = bbn.domains.get(variable, [True, False])
            vals.append(list(product([variable], domain)))
        permutations = product(*vals)
        for permutation in permutations:
            argvals = dict(permutation)
            potential = 1
            for bbn_node in bbn_nodes:
                bbn_node.clique = clique
                arg_list = []
                for arg_name in get_args(bbn_node.func):
                    arg_list.append(argvals[arg_name])
                potential *= bbn_node.func(*arg_list)
            tt[permutation] = potential
        clique.potential_tt = tt
    if not evidence:
        return
    self.initial_likelihoods(assignments, bbn)
    for (clique, bbn_nodes) in assignments.items():
        for node in bbn_nodes:
            if node.variable_name in evidence:
                for (k, v) in list(clique.potential_tt.items()):
                    for (variable, value) in k:
                        if variable == node.variable_name:
                            if value != evidence[variable]:
                                clique.potential_tt[k] = 0","variables = node.variable_names
variables.sort()
for variable in variables:
    domain = bbn.domains.get(variable, [True, False])
    vals.append(list(product([variable], domain)))
permutations = product(*vals)
for permutation in permutations:
    tt[permutation] = 1","vals = [list(product([variable], bbn.domains.get(variable, [True, False]))) for variable in sorted(node.variable_names)]
permutations = product(*vals)
tt = {permutation: 1 for permutation in permutations}",find_wrong,1,,,
PatrickStar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PatrickStar/patrickstar/core/preprocess.py,https://github.com/Tencent/PatrickStar/tree/master/patrickstar/core/preprocess.py,,cast_forward$89,"def cast_forward(module, dtype):
    if not isinstance(dtype, torch.dtype):
        raise ValueError('dtype should be of torch.dtype.')
    old_forward = module.forward

    def forward(*args, **kwargs):
        casted_args = []
        for arg in args:
            if isinstance(arg, torch.Tensor) and torch.is_floating_point(arg):
                casted_args.append(arg.to(dtype))
            else:
                casted_args.append(arg)
        casted_kwargs = {}
        for (k, v) in kwargs.items():
            if isinstance(v, torch.Tensor) and torch.is_floating_point(v):
                casted_kwargs[k] = v.to(dtype)
            else:
                casted_kwargs[k] = v
        return old_forward(*casted_args, **casted_kwargs)
    module.forward = forward","for arg in args:
    if isinstance(arg, torch.Tensor) and torch.is_floating_point(arg):
        casted_args.append(arg.to(dtype))
    else:
        casted_args.append(arg)
casted_kwargs = {}
for (k, v) in kwargs.items():
    if isinstance(v, torch.Tensor) and torch.is_floating_point(v):
        casted_kwargs[k] = v.to(dtype)
    else:
        casted_kwargs[k] = v","casted_args = [arg.to(dtype) if isinstance(arg, torch.Tensor) and torch.is_floating_point(arg) else arg for arg in args]
casted_kwargs = {k: v.to(dtype) if isinstance(v, torch.Tensor) and torch.is_floating_point(v) else v for (k, v) in kwargs.items()}",find_wrong,1,,,
dcc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dcc/dex2c/graph.py,https://github.com/amimo/dcc/tree/master/dex2c/graph.py,,construct$289,"def construct(start_block):
    bfs_blocks = bfs(start_block)
    graph = Graph()
    block_to_node = {}
    node_to_landing_pad = {}
    exception_to_landing_pad = {}
    for block in bfs_blocks:
        node = block_to_node.get(block)
        if node is None:
            node = IrBasicBlock(block)
            block_to_node[block] = node
        graph.add_node(node)
        if block.exception_analysis:
            if block.exception_analysis in exception_to_landing_pad:
                landing_pad = exception_to_landing_pad[block.exception_analysis]
            else:
                landing_pad = LandingPad(node)
                graph.add_landing_pad(landing_pad)
                exception_to_landing_pad[block.exception_analysis] = landing_pad
                for (_type, _, exception_target) in block.exception_analysis.exceptions:
                    catch_node = block_to_node.get(exception_target)
                    if catch_node is None:
                        catch_node = IrBasicBlock(exception_target)
                        block_to_node[exception_target] = catch_node
                    catch_node.set_catch_type(_type)
                    catch_node.in_catch = True
                    landing_pad.add_catch_handle(_type, catch_node)
            for (_type, _, exception_target) in block.exception_analysis.exceptions:
                catch_node = block_to_node.get(exception_target)
                assert catch_node is not None
                node.add_catch_successor(catch_node)
                graph.add_catch_edge(node, catch_node)
            node_to_landing_pad[node] = landing_pad
        for (_, _, child_block) in block.childs:
            child_node = block_to_node.get(child_block)
            if child_node is None:
                child_node = IrBasicBlock(child_block)
                block_to_node[child_block] = child_node
            graph.add_edge(node, child_node)
    graph.entry = block_to_node[start_block]
    graph.compute_rpo()
    offset_to_node = {}
    for node in graph.rpo:
        if node.start >= 0:
            offset_to_node[node.start] = node
    graph.node_to_landing_pad = node_to_landing_pad
    graph.offset_to_node = offset_to_node
    for node in graph.rpo:
        preds = [pred for pred in graph.all_preds(node) if pred.num < node.num]
        if preds and all((pred.in_catch for pred in preds)):
            node.in_catch = True
    return graph","for block in bfs_blocks:
    node = block_to_node.get(block)
    if node is None:
        node = IrBasicBlock(block)
        block_to_node[block] = node",block_to_node = {block: IrBasicBlock(block) for block in bfs_blocks if block not in block_to_node},find_wrong,1,,,
server-tools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/server-tools/sql_export/wizard/wizard_file.py,https://github.com/OCA/server-tools/tree/master/sql_export/wizard/wizard_file.py,SqlFileWizard,export_sql$56,"def export_sql(self):
    self.ensure_one()
    sql_export = self.sql_export_id
    variable_dict = {}
    now_tz = fields.Datetime.context_timestamp(sql_export, datetime.now())
    date = now_tz.strftime(DEFAULT_SERVER_DATETIME_FORMAT)
    if sql_export.field_ids:
        for field in sql_export.field_ids:
            if field.ttype == 'many2one':
                variable_dict[field.name] = self[field.name].id
            elif field.ttype == 'many2many':
                variable_dict[field.name] = tuple(self[field.name].ids)
            else:
                variable_dict[field.name] = self[field.name]
    if '%(company_id)s' in sql_export.query:
        company_id = self.env.company.id
        variable_dict['company_id'] = company_id
    if '%(user_id)s' in sql_export.query:
        user_id = self.env.user.id
        variable_dict['user_id'] = user_id
    method_name = '%s_get_data_from_query' % sql_export.file_format
    data = getattr(sql_export, method_name)(variable_dict)
    extension = sql_export._get_file_extension()
    self.write({'binary_file': data, 'file_name': '%(name)s_%(date)s.%(extension)s' % {'name': sql_export.name, 'date': date, 'extension': extension}})
    return {'view_mode': 'form', 'res_model': 'sql.file.wizard', 'res_id': self.id, 'type': 'ir.actions.act_window', 'target': 'new', 'context': self.env.context, 'nodestroy': True}","if sql_export.field_ids:
    for field in sql_export.field_ids:
        if field.ttype == 'many2one':
            variable_dict[field.name] = self[field.name].id
        elif field.ttype == 'many2many':
            variable_dict[field.name] = tuple(self[field.name].ids)
        else:
            variable_dict[field.name] = self[field.name]
if '%(company_id)s' in sql_export.query:
    company_id = self.env.company.id
    variable_dict['company_id'] = company_id
if '%(user_id)s' in sql_export.query:
    user_id = self.env.user.id
    variable_dict['user_id'] = user_id","variable_dict = {field.name: self[field.name].id if field.ttype == 'many2one' else tuple(self[field.name].ids) if field.ttype == 'many2many' else self[field.name] for field in sql_export.field_ids}
variable_dict.update({'company_id': self.env.company.id} if '%(company_id)s' in sql_export.query else {})
variable_dict.update({'user_id': self.env.user.id} if '%(user_id)s' in sql_export.query else {})",find_wrong,1,,,
orator,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/orator/orator/orm/builder.py,https://github.com/sdispater/orator/tree/master/orator/orm/builder.py,Builder,_nested_relations$518,"def _nested_relations(self, relation):
    """"""
        Get the deeply nested relations for a given top-level relation.

        :rtype: dict
        """"""
    nested = {}
    for (name, constraints) in self._eager_load.items():
        if self._is_nested(name, relation):
            nested[name[len(relation + '.'):]] = constraints
    return nested","for (name, constraints) in self._eager_load.items():
    if self._is_nested(name, relation):
        nested[name[len(relation + '.'):]] = constraints
return nested","nested = {name[len(relation + '.'):]: constraints for (name, constraints) in self._eager_load.items() if self._is_nested(name, relation)}",find_wrong,-1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/cloud/clouds/msazure.py,https://github.com/saltstack/salt/tree/master/salt/cloud/clouds/msazure.py,,avail_locations$152,"def avail_locations(conn=None, call=None):
    """"""
    List available locations for Azure
    """"""
    if call == 'action':
        raise SaltCloudSystemExit('The avail_locations function must be called with -f or --function, or with the --list-locations option')
    if not conn:
        conn = get_conn()
    ret = {}
    locations = conn.list_locations()
    for location in locations:
        ret[location.name] = {'name': location.name, 'display_name': location.display_name, 'available_services': location.available_services}
    return ret","locations = conn.list_locations()
for location in locations:
    ret[location.name] = {'name': location.name, 'display_name': location.display_name, 'available_services': location.available_services}","ret = {location.name: {'name': location.name, 'display_name': location.display_name, 'available_services': location.available_services} for location in conn.list_locations()}",find_wrong,1,,,
edx-platform,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/lms/djangoapps/course_api/blocks/transformers/tests/test_video_urls.py,https://github.com/edx/edx-platform/tree/master/lms/djangoapps/course_api/blocks/transformers/tests/test_video_urls.py,TestVideoBlockURLTransformer,change_encoded_videos_presentation$34,"def change_encoded_videos_presentation(self, encoded_videos):
    """"""
        Relocate url data in new dictionary for pre & post transformation data comparison.
        """"""
    video_urls = {}
    for (video_format, video_data) in encoded_videos.items():
        video_urls[video_format] = video_data['url']
    return video_urls","for (video_format, video_data) in encoded_videos.items():
    video_urls[video_format] = video_data['url']
return video_urls","video_urls = {video_format: video_data['url'] for (video_format, video_data) in encoded_videos.items()}
return video_urls",find_wrong,1,,,
PGL,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/redis_graph.py,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/redis_graph.py,RedisGraph,subgraph$412,"def subgraph(self, nodes, eid, edges=None):
    """"""Generate subgraph with nodes and edge ids.

        This function will generate a :code:`pgl.graph.Subgraph` object and
        copy all corresponding node and edge features. Nodes and edges will
        be reindex from 0.

        WARNING: ALL NODES IN EID MUST BE INCLUDED BY NODES

        Args:
            nodes: Node ids which will be included in the subgraph.

            eid: Edge ids which will be included in the subgraph.

        Return:
            A :code:`pgl.graph.Subgraph` object.
        """"""
    reindex = {}
    for (ind, node) in enumerate(nodes):
        reindex[node] = ind
    if edges is None:
        edges = self.get_edges_by_id(eid)
    else:
        edges = np.array(edges, dtype='int64')
    sub_edges = graph_kernel.map_edges(np.arange(len(edges), dtype='int64'), edges, reindex)
    sub_edge_feat = {}
    for (key, _, _) in self.edge_feat_info():
        sub_edge_feat[key] = self.get_edge_feat_by_id(key, eid)
    sub_node_feat = {}
    for (key, _, _) in self.node_feat_info():
        sub_node_feat[key] = self.get_node_feat_by_id(key, nodes)
    subgraph = pgraph.SubGraph(num_nodes=len(nodes), edges=sub_edges, node_feat=sub_node_feat, edge_feat=sub_edge_feat, reindex=reindex)
    return subgraph","for (key, _, _) in self.edge_feat_info():
    sub_edge_feat[key] = self.get_edge_feat_by_id(key, eid)
sub_node_feat = {}
for (key, _, _) in self.node_feat_info():
    sub_node_feat[key] = self.get_node_feat_by_id(key, nodes)","sub_edge_feat = {key: self.get_edge_feat_by_id(key, eid) for (key, _, _) in self.edge_feat_info()}
sub_node_feat = {key: self.get_node_feat_by_id(key, nodes) for (key, _, _) in self.node_feat_info()}",find_wrong,,,,
tapiriik,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tapiriik/tapiriik/web/views/ab.py,https://github.com/cpfair/tapiriik/tree/master/tapiriik/web/views/ab.py,,ab_experiment_context$43,"def ab_experiment_context(request):
    context = {}
    if request.user:
        for key in _experiments.keys():
            context['ab_%s_%s' % (key, ab_select_variant(key, request.user['_id']))] = True
    return context","if request.user:
    for key in _experiments.keys():
        context['ab_%s_%s' % (key, ab_select_variant(key, request.user['_id']))] = True","context = {'ab_%s_%s' % (key, ab_select_variant(key, request.user['_id'])): True for key in _experiments.keys() if request.user}",find_wrong,1,,,
Open3D-ML,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Open3D-ML/scripts/preprocess_scannet.py,https://github.com/isl-org/Open3D-ML/tree/master/scripts/preprocess_scannet.py,ScannetProcess,read_label_mapping$188,"def read_label_mapping(filename, label_from='raw_category', label_to='nyu40id'):
    assert os.path.isfile(filename)
    mapping = dict()
    with open(filename) as csvfile:
        reader = csv.DictReader(csvfile, delimiter='\t')
        for row in reader:
            mapping[row[label_from]] = int(row[label_to])
    if represents_int(list(mapping.keys())[0]):
        mapping = {int(k): v for (k, v) in mapping.items()}
    return mapping","with open(filename) as csvfile:
    reader = csv.DictReader(csvfile, delimiter='\t')
    for row in reader:
        mapping[row[label_from]] = int(row[label_to])","with open(filename) as csvfile:
    reader = csv.DictReader(csvfile, delimiter='\t')
    mapping = {row[label_from]: int(row[label_to]) for row in reader}",find_wrong,1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/utils/network.py,https://github.com/saltstack/salt/tree/master/salt/utils/network.py,,_subnets$1302,"def _subnets(proto='inet', interfaces_=None):
    """"""
    Returns a list of subnets to which the host belongs
    """"""
    if interfaces_ is None:
        ifaces = interfaces()
    elif isinstance(interfaces_, list):
        ifaces = {}
        for (key, value) in interfaces().items():
            if key in interfaces_:
                ifaces[key] = value
    else:
        ifaces = {interfaces_: interfaces().get(interfaces_, {})}
    ret = set()
    if proto == 'inet':
        subnet = 'netmask'
        dflt_cidr = 32
    elif proto == 'inet6':
        subnet = 'prefixlen'
        dflt_cidr = 128
    else:
        log.error('Invalid proto %s calling subnets()', proto)
        return
    for ip_info in ifaces.values():
        addrs = ip_info.get(proto, [])
        addrs.extend([addr for addr in ip_info.get('secondary', []) if addr.get('type') == proto])
        for intf in addrs:
            if subnet in intf:
                intf = ipaddress.ip_interface('{}/{}'.format(intf['address'], intf[subnet]))
            else:
                intf = ipaddress.ip_interface('{}/{}'.format(intf['address'], dflt_cidr))
            if not intf.is_loopback:
                ret.add(intf.network)
    return [str(net) for net in sorted(ret)]","if interfaces_ is None:
    ifaces = interfaces()
elif isinstance(interfaces_, list):
    for (key, value) in interfaces().items():
        if key in interfaces_:
            ifaces[key] = value
else:
    ifaces = {interfaces_: interfaces().get(interfaces_, {})}","ifaces = interfaces() if interfaces_ is None else {key: value for (key, value) in interfaces().items() if key in interfaces_} if isinstance(interfaces_, list) else {interfaces_: interfaces().get(interfaces_, {})}",find_wrong,1,,,
not-youtube-dl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/naver.py,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/naver.py,NaverBaseIE,_extract_video_info$21,"def _extract_video_info(self, video_id, vid, key):
    video_data = self._download_json('http://play.rmcnmv.naver.com/vod/play/v2.0/' + vid, video_id, query={'key': key})
    meta = video_data['meta']
    title = meta['subject']
    formats = []
    get_list = lambda x: try_get(video_data, lambda y: y[x + 's']['list'], list) or []

    def extract_formats(streams, stream_type, query={}):
        for stream in streams:
            stream_url = stream.get('source')
            if not stream_url:
                continue
            stream_url = update_url_query(stream_url, query)
            encoding_option = stream.get('encodingOption', {})
            bitrate = stream.get('bitrate', {})
            formats.append({'format_id': '%s_%s' % (stream.get('type') or stream_type, dict_get(encoding_option, ('name', 'id'))), 'url': stream_url, 'width': int_or_none(encoding_option.get('width')), 'height': int_or_none(encoding_option.get('height')), 'vbr': int_or_none(bitrate.get('video')), 'abr': int_or_none(bitrate.get('audio')), 'filesize': int_or_none(stream.get('size')), 'protocol': 'm3u8_native' if stream_type == 'HLS' else None})
    extract_formats(get_list('video'), 'H264')
    for stream_set in video_data.get('streams', []):
        query = {}
        for param in stream_set.get('keys', []):
            query[param['name']] = param['value']
        stream_type = stream_set.get('type')
        videos = stream_set.get('videos')
        if videos:
            extract_formats(videos, stream_type, query)
        elif stream_type == 'HLS':
            stream_url = stream_set.get('source')
            if not stream_url:
                continue
            formats.extend(self._extract_m3u8_formats(update_url_query(stream_url, query), video_id, 'mp4', 'm3u8_native', m3u8_id=stream_type, fatal=False))
    self._sort_formats(formats)
    replace_ext = lambda x, y: re.sub(self._CAPTION_EXT_RE, '.' + y, x)

    def get_subs(caption_url):
        if re.search(self._CAPTION_EXT_RE, caption_url):
            return [{'url': replace_ext(caption_url, 'ttml')}, {'url': replace_ext(caption_url, 'vtt')}]
        else:
            return [{'url': caption_url}]
    automatic_captions = {}
    subtitles = {}
    for caption in get_list('caption'):
        caption_url = caption.get('source')
        if not caption_url:
            continue
        sub_dict = automatic_captions if caption.get('type') == 'auto' else subtitles
        sub_dict.setdefault(dict_get(caption, ('locale', 'language')), []).extend(get_subs(caption_url))
    user = meta.get('user', {})
    return {'id': video_id, 'title': title, 'formats': formats, 'subtitles': subtitles, 'automatic_captions': automatic_captions, 'thumbnail': try_get(meta, lambda x: x['cover']['source']), 'view_count': int_or_none(meta.get('count')), 'uploader_id': user.get('id'), 'uploader': user.get('name'), 'uploader_url': user.get('url')}","get_list = lambda x: try_get(video_data, lambda y: y[x + 's']['list'], list) or []

        def extract_formats(streams, stream_type, query={}):
            for stream in streams:
                stream_url = stream.get('source')
                if not stream_url:
                    continue
                stream_url = update_url_query(stream_url, query)
                encoding_option = stream.get('encodingOption', {})
                bitrate = stream.get('bitrate', {})
                formats.append({
                    'format_id': '%s_%s' % (stream.get('type') or stream_type, dict_get(encoding_option, ('name', 'id'))),
                    'url': stream_url,
                    'width': int_or_none(encoding_option.get('width')),
                    'height': int_or_none(encoding_option.get('height')),
                    'vbr': int_or_none(bitrate.get('video')),
                    'abr': int_or_none(bitrate.get('audio')),
                    'filesize': int_or_none(stream.get('size')),
                    'protocol': 'm3u8_native' if stream_type == 'HLS' else None,
                })

        extract_formats(get_list('video'), 'H264')
        for stream_set in video_data.get('streams', []):
            query = {}
            for param in stream_set.get('keys', []):
                query[param['name']] = param['value']
            stream_type = stream_set.get('type')
            videos = stream_set.get('videos')
            if videos:
                extract_formats(videos, stream_type, query)
            elif stream_type == 'HLS':
                stream_url = stream_set.get('source')
                if not stream_url:
                    continue
                formats.extend(self._extract_m3u8_formats(
                    update_url_query(stream_url, query), video_id,
                    'mp4', 'm3u8_native', m3u8_id=stream_type, fatal=False))
        self._sort_formats(formats)","formats = [{'format_id': '%s_%s' % (stream.get('type') or stream_type, dict_get(encoding_option, ('name', 'id'))), 'url': update_url_query(stream.get('source'), query), 'width': int_or_none(encoding_option.get('width')), 'height': int_or_none(encoding_option.get('height')), 'vbr': int_or_none(bitrate.get('video')), 'abr': int_or_none(bitrate.get('audio')), 'filesize': int_or_none(stream.get('size')), 'protocol': 'm3u8_native' if stream_type == 'HLS' else None} for stream_set in video_data.get('streams', []) for query in [{param['name']: param['value']} for param in stream_set.get('keys', [])] + [{}] for stream_type in [stream_set.get('type')] + (['H264'] if not stream_set.get('type') else []) for streams in [stream_set.get('videos')] + ([get_list('video')] if not stream_set.get('videos') and stream_set.get('type') == 'HLS' else []) for stream in streams if stream.get('source') is not None for encoding_option in [stream.get('encodingOption', {})] for bitrate in [stream.get('bitrate', {})]]
self._sort_formats(formats)",find_wrong,-1,,,
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/torchie/apis/train.py,https://github.com/poodarchu/Det3D/tree/master/det3d/torchie/apis/train.py,,example_convert_to_torch$24,"def example_convert_to_torch(example, dtype=torch.float32, device=None) -> dict:
    assert device is not None
    example_torch = {}
    float_names = ['voxels', 'bev_map']
    for (k, v) in example.items():
        if k in ['anchors', 'reg_targets', 'reg_weights']:
            res = []
            for (kk, vv) in v.items():
                res.append(torch.tensor(vv).to(device, non_blocking=True))
            example_torch[k] = res
        elif k in float_names:
            example_torch[k] = v.to(device, non_blocking=True)
        elif k in ['coordinates', 'num_points']:
            example_torch[k] = v.to(device, non_blocking=True)
        elif k == 'labels':
            res = []
            for (kk, vv) in v.items():
                res.append(torch.tensor(vv).to(device, non_blocking=True))
            example_torch[k] = res
        elif k == 'points':
            example_torch[k] = v.to(device, non_blocking=True)
        elif k in ['anchors_mask']:
            res = []
            for (kk, vv) in v.items():
                res.append(torch.tensor(vv).to(device, non_blocking=True))
            example_torch[k] = res
        elif k == 'calib':
            calib = {}
            for (k1, v1) in v.items():
                calib[k1] = torch.tensor(v1).to(device, non_blocking=True)
            example_torch[k] = calib
        elif k == 'num_voxels':
            example_torch[k] = v.to(device, non_blocking=True)
        else:
            example_torch[k] = v
    return example_torch","float_names = ['voxels', 'bev_map']
for (k, v) in example.items():
    if k in ['anchors', 'reg_targets', 'reg_weights']:
        res = []
        for (kk, vv) in v.items():
            res.append(torch.tensor(vv).to(device, non_blocking=True))
        example_torch[k] = res
    elif k in float_names:
        example_torch[k] = v.to(device, non_blocking=True)
    elif k in ['coordinates', 'num_points']:
        example_torch[k] = v.to(device, non_blocking=True)
    elif k == 'labels':
        res = []
        for (kk, vv) in v.items():
            res.append(torch.tensor(vv).to(device, non_blocking=True))
        example_torch[k] = res
    elif k == 'points':
        example_torch[k] = v.to(device, non_blocking=True)
    elif k in ['anchors_mask']:
        res = []
        for (kk, vv) in v.items():
            res.append(torch.tensor(vv).to(device, non_blocking=True))
        example_torch[k] = res
    elif k == 'calib':
        calib = {}
        for (k1, v1) in v.items():
            calib[k1] = torch.tensor(v1).to(device, non_blocking=True)
        example_torch[k] = calib
    elif k == 'num_voxels':
        example_torch[k] = v.to(device, non_blocking=True)
    else:
        example_torch[k] = v","example_torch = {k: [torch.tensor(vv).to(device, non_blocking=True) for (kk, vv) in v.items()] if k in ['anchors', 'reg_targets', 'reg_weights'] else v.to(device, non_blocking=True) if k in float_names else v.to(device, non_blocking=True) if k in ['coordinates', 'num_points'] else [torch.tensor(vv).to(device, non_blocking=True) for (kk, vv) in v.items()] if k == 'labels' else v.to(device, non_blocking=True) if k == 'points' else [torch.tensor(vv).to(device, non_blocking=True) for (kk, vv) in v.items()] if k in ['anchors_mask'] else {k1: torch.tensor(v1).to(device, non_blocking=True) for (k1, v1) in v.items()} if k == 'calib' else v.to(device, non_blocking=True) if k == 'num_voxels' else v for (k, v) in example.items()}",find_wrong,1,,,
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/functional/tests.py,https://github.com/openstack/swift/tree/master/test/functional/tests.py,TestFile,testContentTypeGuessing$2056,"def testContentTypeGuessing(self):
    file_types = {'wav': 'audio/x-wav', 'txt': 'text/plain', 'zip': 'application/zip'}
    container = self.env.account.container(Utils.create_name())
    self.assertTrue(container.create())
    for i in file_types.keys():
        file_item = container.file(Utils.create_name() + '.' + i)
        file_item.write(b'', cfg={'no_content_type': True})
    file_types_read = {}
    for i in container.files(parms={'format': 'json'}):
        file_types_read[i['name'].split('.')[1]] = i['content_type']
    self.assertEqual(file_types, file_types_read)","for i in container.files(parms={'format': 'json'}):
    file_types_read[i['name'].split('.')[1]] = i['content_type']",file_types_read = {i['name'].split('.')[1]: i['content_type'] for i in container.files(parms={'format': 'json'})},file_types_read = {i['name'].split('.')[1]: i['content_type'] for i in container.files(parms={'format': 'json'})},1,,,
dephell,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dephell/dephell/commands/deps_install.py,https://github.com/dephell/dephell/tree/master/dephell/commands/deps_install.py,DepsInstallCommand,_get_install_remove$68,"def _get_install_remove(self, graph, python) -> Tuple[list, list]:
    installed_root = InstalledConverter().load(paths=python.lib_paths)
    installed = dict()
    for dep in installed_root.dependencies:
        installed[dep.name] = [c.strip('=') for c in str(dep.constraint).split(' || ')]
    install = []
    remove = []
    reqs = Requirement.from_graph(graph=graph, lock=True)
    for req in reqs:
        if req.name not in installed:
            install.append(req)
            continue
        if not req.version:
            continue
        version = req.version.strip('=')
        if version in installed[req.name]:
            continue
        self.logger.debug('dependency will be updated', extra=dict(dependency=req.name, old=installed[req.name], new=version))
        remove.append(req)
        install.append(req)
    if self.sync:
        names = set(installed) - {req.name for req in reqs}
        remove.extend((SimpleNamespace(name=name) for name in names))
    return (install, remove)","for dep in installed_root.dependencies:
    installed[dep.name] = [c.strip('=') for c in str(dep.constraint).split(' || ')]",installed = {dep.name: [c.strip('=') for c in str(dep.constraint).split(' || ')] for dep in installed_root.dependencies},installed = {dep.name: [c.strip('=') for c in str(dep.constraint).split(' || ')] for dep in installed_root.dependencies},1,,,
pywb,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pywb/pywb/recorder/recorderapp.py,https://github.com/webrecorder/pywb/tree/master/pywb/recorder/recorderapp.py,ReqWrapper,__init__$328,"def __init__(self, stream, req_headers, params, create_func):
    super(ReqWrapper, self).__init__(stream, params, create_func)
    self.headers = {}
    for n in six.iterkeys(req_headers):
        if n.upper().startswith('WARC-'):
            self.headers[n] = req_headers[n]","for n in six.iterkeys(req_headers):
    if n.upper().startswith('WARC-'):
        self.headers[n] = req_headers[n]",self.headers = {n: req_headers[n] for n in six.iterkeys(req_headers) if n.upper().startswith('WARC-')},self.headers = {n: req_headers[n] for n in six.iterkeys(req_headers) if n.upper().startswith('WARC-')},1,,,
DeepRec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRec/models/item_ranking/cdae.py,https://github.com/cheungdaven/DeepRec/tree/master/models/item_ranking/cdae.py,CDAE,_get_neg_items$134,"def _get_neg_items(self, data):
    neg_items = {}
    for u in range(self.num_user):
        neg_items[u] = [k for (k, i) in enumerate(data[u]) if data[u][k] == 0]
    return neg_items","for u in range(self.num_user):
    neg_items[u] = [k for (k, i) in enumerate(data[u]) if data[u][k] == 0]","neg_items = {u: [k for (k, i) in enumerate(data[u]) if i == 0] for u in range(self.num_user)}","neg_items = {u: [k for (k, i) in enumerate(data[u]) if i == 0] for u in range(self.num_user)}",1,,,
sentry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/integrations/aws_lambda/integration.py,https://github.com/getsentry/sentry/tree/master/src/sentry/integrations/aws_lambda/integration.py,AwsLambdaListFunctionsPipelineView,dispatch$316,"def dispatch(self, request: Request, pipeline) -> Response:
    if request.method == 'POST':
        raw_data = request.POST
        data = {}
        for (key, val) in raw_data.items():
            data[key] = val == 'true'
        pipeline.bind_state('enabled_lambdas', data)
        return pipeline.next_step()
    account_number = pipeline.fetch_state('account_number')
    region = pipeline.fetch_state('region')
    aws_external_id = pipeline.fetch_state('aws_external_id')
    lambda_client = gen_aws_client(account_number, region, aws_external_id)
    lambda_functions = get_supported_functions(lambda_client)
    curr_step = 2 if pipeline.fetch_state('skipped_project_select') else 3
    return self.render_react_view(request, 'awsLambdaFunctionSelect', {'lambdaFunctions': lambda_functions, 'initialStepNumber': curr_step})","for (key, val) in raw_data.items():
    data[key] = val == 'true'","data = {key: val == 'true' for (key, val) in raw_data.items()}","data = {key: val == 'true' for (key, val) in raw_data.items()}",1,,,
BlenderProc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BlenderProc/blenderproc/python/utility/Utility.py,https://github.com/DLR-RM/BlenderProc/tree/master/blenderproc/python/utility/Utility.py,Utility,build_provider_based_on_config$402,"def build_provider_based_on_config(config: Config) -> 'Provider':
    """""" Builds up the provider using the parameters described in the given config.

        The given config should follow the following scheme:

        .. code-block:: yaml

            {
              ""provider"": ""<name of provider class>""
              ""parameters"": {
                <provider parameters>
              }
            }

        :param config: A Configuration object or a dict containing the configuration data.
        :return: The constructed provider.
        """"""
    if isinstance(config, dict):
        config = Config(config)
    parameters = {}
    for key in config.data.keys():
        if key != 'provider':
            parameters[key] = config.data[key]
    if not config.has_param('provider'):
        raise RuntimeError(f'Each provider needs a provider label, this one does not contain one: {config.data}')
    return Utility.build_provider(config.get_string('provider'), parameters)","for key in config.data.keys():
    if key != 'provider':
        parameters[key] = config.data[key]",parameters = {key: config.data[key] for key in config.data.keys() if key != 'provider'},parameters = {key: config.data[key] for key in config.data.keys() if key != 'provider'},1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/xbpspkg.py,https://github.com/saltstack/salt/tree/master/salt/modules/xbpspkg.py,,latest_version$161,"def latest_version(*names, **kwargs):
    """"""
    Return the latest version of the named package available for upgrade or
    installation. If more than one package name is specified, a dict of
    name/version pairs is returned.

    If the latest version of a given package is already installed, an empty
    string will be returned for that package.

    CLI Example:

    .. code-block:: bash

        salt '*' pkg.latest_version <package name>
        salt '*' pkg.latest_version <package1> <package2> <package3> ...
    """"""
    refresh = salt.utils.data.is_true(kwargs.pop('refresh', True))
    if len(names) == 0:
        return ''
    if refresh:
        refresh_db()
    ret = {}
    for name in names:
        ret[name] = ''
    cmd = ['xbps-install', '-un']
    cmd.extend(names)
    out = __salt__['cmd.run'](cmd, ignore_retcode=True, output_loglevel='trace')
    for line in out.splitlines():
        if not line:
            continue
        if line.find(' is up to date.') != -1:
            continue
        try:
            (pkg, ver) = line.split()[0].rsplit('-', 1)
        except (ValueError, IndexError):
            log.error('xbps-query: Unexpected formatting in line: ""%s""', line)
            continue
        log.trace('pkg=%s version=%s', pkg, ver)
        if pkg in names:
            ret[pkg] = ver
    if len(names) == 1:
        return ret[names[0]]
    return ret","for name in names:
    ret[name] = ''",ret = {name: '' for name in names},ret = {name: '' for name in names},1,,,
soynlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/soynlp/soynlp/noun/_noun_ver1.py,https://github.com/lovit/soynlp/tree/master/soynlp/noun/_noun_ver1.py,LRNounExtractor,_to_NounScore$272,"def _to_NounScore(self, nouns):
    noun_frequencies = {}
    for word in sorted(nouns, key=lambda x: -len(x)):
        r_count = self.lrgraph.get_r(word, -1)
        noun_frequencies[word] = sum((c for (w, c) in r_count))
        for (r, count) in r_count:
            self.lrgraph.remove_eojeol(word + r, count)
    self.lrgraph.reset_lrgraph()
    nouns_ = {}
    for (word, score) in nouns.items():
        nouns_[word] = NounScore_v1(noun_frequencies[word], score[0], score[1])
    return nouns_","for (word, score) in nouns.items():
    nouns_[word] = NounScore_v1(noun_frequencies[word], score[0], score[1])","nouns_ = {word: NounScore_v1(noun_frequencies[word], score[0], score[1]) for (word, score) in nouns.items()}","nouns_ = {word: NounScore_v1(noun_frequencies[word], score[0], score[1]) for (word, score) in nouns.items()}",1,,,
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/polys/solvers.py,https://github.com/sympy/sympy/tree/master/sympy/polys/solvers.py,,_solve_lin_sys$299,"def _solve_lin_sys(eqs_coeffs, eqs_rhs, ring):
    """"""Solve a linear system from dict of PolynomialRing coefficients

    Explanation
    ===========

    This is an **internal** function used by :func:`solve_lin_sys` after the
    equations have been preprocessed. The role of this function is to split
    the system into connected components and pass those to
    :func:`_solve_lin_sys_component`.

    Examples
    ========

    Setup a system for $x-y=0$ and $x+y=2$ and solve:

    >>> from sympy import symbols, sring
    >>> from sympy.polys.solvers import _solve_lin_sys
    >>> x, y = symbols('x, y')
    >>> R, (xr, yr) = sring([x, y], [x, y])
    >>> eqs = [{xr:R.one, yr:-R.one}, {xr:R.one, yr:R.one}]
    >>> eqs_rhs = [R.zero, -2*R.one]
    >>> _solve_lin_sys(eqs, eqs_rhs, R)
    {y: 1, x: 1}

    See also
    ========

    solve_lin_sys: This function is used internally by :func:`solve_lin_sys`.
    """"""
    V = ring.gens
    E = []
    for eq_coeffs in eqs_coeffs:
        syms = list(eq_coeffs)
        E.extend(zip(syms[:-1], syms[1:]))
    G = (V, E)
    components = connected_components(G)
    sym2comp = {}
    for (n, component) in enumerate(components):
        for sym in component:
            sym2comp[sym] = n
    subsystems = [([], []) for _ in range(len(components))]
    for (eq_coeff, eq_rhs) in zip(eqs_coeffs, eqs_rhs):
        sym = next(iter(eq_coeff), None)
        (sub_coeff, sub_rhs) = subsystems[sym2comp[sym]]
        sub_coeff.append(eq_coeff)
        sub_rhs.append(eq_rhs)
    sol = {}
    for subsystem in subsystems:
        subsol = _solve_lin_sys_component(subsystem[0], subsystem[1], ring)
        if subsol is None:
            return None
        sol.update(subsol)
    return sol","for (n, component) in enumerate(components):
    for sym in component:
        sym2comp[sym] = n","sym2comp = {sym: n for (n, component) in enumerate(components) for sym in component}","sym2comp = {sym: n for (n, component) in enumerate(components) for sym in component}",1,,,
scenic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scenic/scenic/model_lib/base_models/classification_model.py,https://github.com/google-research/scenic/tree/master/scenic/model_lib/base_models/classification_model.py,,classification_metrics_function$36,"def classification_metrics_function(logits: jnp.ndarray, batch: base_model.Batch, target_is_onehot: bool=False, metrics: base_model.MetricNormalizerFnDict=_CLASSIFICATION_METRICS, axis_name: Union[str, Tuple[str, ...]]='batch') -> Dict[str, Tuple[float, int]]:
    """"""Calculates metrics for the classification task.


  Currently we assume each metric_fn has the API:
    ```metric_fn(logits, targets, weights)```
  and returns an array of shape [batch_size]. We also assume that to compute
  the aggregate metric, one should sum across all batches, then divide by the
  total samples seen. In this way we currently only support metrics of the 1/N
  sum f(inputs, targets). Note, the caller is responsible for dividing by
  the normalizer when computing the mean of each metric.

  Args:
   logits: Output of model in shape [batch, length, num_classes].
   batch: Batch of data that has 'label' and optionally 'batch_mask'.
   target_is_onehot: If the target is a one-hot vector.
   metrics: The classification metrics to evaluate. The key is the name of the
     metric, and the value is the metrics function.
   axis_name: List of axes on which we run the pmsum.

  Returns:
    A dict of metrics, in which keys are metrics name and values are tuples of
    (metric, normalizer).
  """"""
    if target_is_onehot:
        one_hot_targets = batch['label']
    else:
        one_hot_targets = common_utils.onehot(batch['label'], logits.shape[-1])
    weights = batch.get('batch_mask')
    evaluated_metrics = {}
    for (key, val) in metrics.items():
        evaluated_metrics[key] = model_utils.psum_metric_normalizer((val[0](logits, one_hot_targets, weights), val[1](logits, one_hot_targets, weights)), axis_name=axis_name)
    return evaluated_metrics","for (key, val) in metrics.items():
    evaluated_metrics[key] = model_utils.psum_metric_normalizer((val[0](logits, one_hot_targets, weights), val[1](logits, one_hot_targets, weights)), axis_name=axis_name)","evaluated_metrics = {key: model_utils.psum_metric_normalizer((val[0](logits, one_hot_targets, weights), val[1](logits, one_hot_targets, weights)), axis_name=axis_name) for (key, val) in metrics.items()}","evaluated_metrics = {key: model_utils.psum_metric_normalizer((val[0](logits, one_hot_targets, weights), val[1](logits, one_hot_targets, weights)), axis_name=axis_name) for (key, val) in metrics.items()}",1,,,
X-Temporal,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/X-Temporal/tools/gen_label_kinetics.py,https://github.com/Sense-X/X-Temporal/tree/master/tools/gen_label_kinetics.py,,if_main_my$7,"if __name__ == '__main__':
    with open('kinetics_label_map.txt') as f:
        categories = f.readlines()
        categories = [c.strip().replace(' ', '_').replace('""', '').replace('(', '').replace(')', '').replace(""'"", '') for c in categories]
    assert len(set(categories)) == 600
    dict_categories = {}
    for (i, category) in enumerate(categories):
        dict_categories[category] = i
    print(dict_categories)
    files_input = ['kinetics-600_val.csv', 'kinetics-600_train.csv']
    files_output = ['val_videofolder.txt', 'train_videofolder.txt']
    for (filename_input, filename_output) in zip(files_input, files_output):
        count_cat = {k: 0 for k in dict_categories.keys()}
        with open(os.path.join(label_path, filename_input)) as f:
            lines = f.readlines()[1:]
        folders = []
        idx_categories = []
        categories_list = []
        for line in lines:
            line = line.rstrip()
            items = line.split(',')
            st = int(items[2])
            et = int(items[3])
            folders.append(items[1] + '_' + '%06d' % st + '_' + '%06d' % et)
            this_catergory = items[0].replace(' ', '_').replace('""', '').replace('(', '').replace(')', '').replace(""'"", '')
            categories_list.append(this_catergory)
            idx_categories.append(dict_categories[this_catergory])
            count_cat[this_catergory] += 1
        print(max(count_cat.values()))
        assert len(idx_categories) == len(folders)
        missing_folders = []
        output = []
        for i in range(len(folders)):
            curFolder = folders[i]
            curIDX = idx_categories[i]
            img_dir = os.path.join(dataset_path, categories_list[i], curFolder)
            if not os.path.exists(img_dir):
                missing_folders.append(img_dir)
            else:
                dir_files = os.listdir(img_dir)
                output.append('%s %d %d' % (os.path.join(categories_list[i], curFolder), len(dir_files), curIDX))
            print('%d/%d, missing %d' % (i, len(folders), len(missing_folders)))
        with open(os.path.join(label_path, filename_output), 'w') as f:
            f.write('\n'.join(output))
        with open(os.path.join(label_path, 'missing_' + filename_output), 'w') as f:
            f.write('\n'.join(missing_folders))","for (i, category) in enumerate(categories):
    dict_categories[category] = i","dict_categories = {category: i for (i, category) in enumerate(categories)}","dict_categories = {category: i for (i, category) in enumerate(categories)}",1,,,
OpenWPM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenWPM/test/test_extension.py,https://github.com/openwpm/OpenWPM/tree/master/test/test_extension.py,TestExtension,test_extension_gets_correct_visit_id$288,"def test_extension_gets_correct_visit_id(self) -> None:
    url_a = utilities.BASE_TEST_URL + '/simple_a.html'
    url_b = utilities.BASE_TEST_URL + '/simple_b.html'
    self.visit(url_a)
    db = self.visit(url_b)
    qry_res = db_utils.query_db(db, 'SELECT visit_id, site_url FROM site_visits')
    visit_ids = dict()
    for row in qry_res:
        visit_ids[row[1]] = row[0]
    simple_a_visit_id = db_utils.query_db(db, 'SELECT visit_id FROM javascript WHERE symbol=?', ('window.navigator.userAgent',))
    simple_b_visit_id = db_utils.query_db(db, 'SELECT visit_id FROM javascript WHERE symbol=?', ('window.navigator.platform',))
    assert visit_ids[url_a] == simple_a_visit_id[0][0]
    assert visit_ids[url_b] == simple_b_visit_id[0][0]","for row in qry_res:
    visit_ids[row[1]] = row[0]",visit_ids = {row[1]: row[0] for row in qry_res},visit_ids = {row[1]: row[0] for row in qry_res},1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/cloud/clouds/packet.py,https://github.com/saltstack/salt/tree/master/salt/cloud/clouds/packet.py,,list_nodes_min$482,"def list_nodes_min(call=None):
    """"""
    Return a list of the VMs that are on the provider. Only a list of VM names and
    their state is returned. This is the minimum amount of information needed to
    check for existing VMs.

    .. versionadded:: 2015.8.0

    CLI Example:

    .. code-block:: bash

        salt-cloud -f list_nodes_min packet-provider
        salt-cloud --function list_nodes_min packet-provider
    """"""
    if call == 'action':
        raise SaltCloudSystemExit('The list_nodes_min function must be called with -f or --function.')
    ret = {}
    for device in get_devices_by_token():
        ret[device.hostname] = {'id': device.id, 'state': device.state}
    return ret","for device in get_devices_by_token():
    ret[device.hostname] = {'id': device.id, 'state': device.state}","ret = {device.hostname: {'id': device.id, 'state': device.state} for device in get_devices_by_token()}","ret = {device.hostname: {'id': device.id, 'state': device.state} for device in get_devices_by_token()}",1,,,
quantstats,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quantstats/quantstats/stats.py,https://github.com/ranaroussi/quantstats/tree/master/quantstats/stats.py,,win_rate$171,"def win_rate(returns, aggregate=None, compounded=True, prepare_returns=True):
    """"""Calculates the win ratio for a period""""""

    def _win_rate(series):
        try:
            return len(series[series > 0]) / len(series[series != 0])
        except Exception:
            return 0.0
    if prepare_returns:
        returns = _utils._prepare_returns(returns)
    if aggregate:
        returns = _utils.aggregate_returns(returns, aggregate, compounded)
    if isinstance(returns, _pd.DataFrame):
        _df = {}
        for col in returns.columns:
            _df[col] = _win_rate(returns[col])
        return _pd.Series(_df)
    return _win_rate(returns)","for col in returns.columns:
    _df[col] = _win_rate(returns[col])",_df = {col: _win_rate(returns[col]) for col in returns.columns},_df = {col: _win_rate(returns[col]) for col in returns.columns},1,,,
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/google/cloud/forseti/services/explain/service.py,https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/services/explain/service.py,GrpcExplainer,_get_handle$41,"def _get_handle(self, context):
    """"""Return the handle associated with the gRPC call.

        Args:
            context (object): gRPC context

        Returns:
            str: handle of the GRPC call
        """"""
    metadata = context.invocation_metadata()
    metadata_dict = {}
    for (key, value) in metadata:
        metadata_dict[key] = value
    return metadata_dict[self.HANDLE_KEY]","for (key, value) in metadata:
    metadata_dict[key] = value","metadata_dict = {key: value for (key, value) in metadata}","metadata_dict = {key: value for (key, value) in metadata}",1,,,
StyleGAN-nada,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/StyleGAN-nada/convert_weight.py,https://github.com/rinongal/StyleGAN-nada/tree/master//convert_weight.py,,convert_torgb$59,"def convert_torgb(vars, source_name, target_name):
    weight = vars[source_name + '/weight'].value().eval()
    mod_weight = vars[source_name + '/mod_weight'].value().eval()
    mod_bias = vars[source_name + '/mod_bias'].value().eval()
    bias = vars[source_name + '/bias'].value().eval()
    dic = {'conv.weight': np.expand_dims(weight.transpose((3, 2, 0, 1)), 0), 'conv.modulation.weight': mod_weight.transpose((1, 0)), 'conv.modulation.bias': mod_bias + 1, 'bias': bias.reshape((1, 3, 1, 1))}
    dic_torch = {}
    for (k, v) in dic.items():
        dic_torch[target_name + '.' + k] = torch.from_numpy(v)
    return dic_torch","for (k, v) in dic.items():
    dic_torch[target_name + '.' + k] = torch.from_numpy(v)","dic_torch = {target_name + '.' + k: torch.from_numpy(v) for (k, v) in dic.items()}","dic_torch = {target_name + '.' + k: torch.from_numpy(v) for (k, v) in dic.items()}",1,,,
qutebrowser,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutebrowser/qutebrowser/config/websettings.py,https://github.com/qutebrowser/qutebrowser/tree/master/qutebrowser/config/websettings.py,UserAgent,parse$52,"def parse(cls, ua: str) -> 'UserAgent':
    """"""Parse a user agent string into its components.""""""
    comment_matches = re.finditer('\\(([^)]*)\\)', ua)
    os_info = list(comment_matches)[0].group(1)
    version_matches = re.finditer('(\\S+)/(\\S+)', ua)
    versions = {}
    for match in version_matches:
        versions[match.group(1)] = match.group(2)
    webkit_version = versions['AppleWebKit']
    if 'Chrome' in versions:
        upstream_browser_key = 'Chrome'
        qt_key = 'QtWebEngine'
    elif 'Version' in versions:
        upstream_browser_key = 'Version'
        qt_key = 'Qt'
    else:
        raise ValueError('Invalid upstream browser key: {}'.format(ua))
    upstream_browser_version = versions[upstream_browser_key]
    qt_version = versions.get(qt_key)
    return cls(os_info=os_info, webkit_version=webkit_version, upstream_browser_key=upstream_browser_key, upstream_browser_version=upstream_browser_version, qt_key=qt_key, qt_version=qt_version)","for match in version_matches:
    versions[match.group(1)] = match.group(2)",versions = {match.group(1): match.group(2) for match in version_matches},versions = {match.group(1): match.group(2) for match in version_matches},1,,,
GlobaLeaks,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GlobaLeaks/backend/globaleaks/handlers/admin/https.py,https://github.com/globaleaks/GlobaLeaks/tree/master/backend/globaleaks/handlers/admin/https.py,,db_serialize_https_config_summary$130,"def db_serialize_https_config_summary(session, tid):
    config = ConfigFactory(session, tid)
    file_summaries = {}
    for (key, file_res_cls) in FileHandler.mapped_resources.items():
        file_summaries[key] = file_res_cls.db_serialize(session, tid)
    return {'enabled': config.get_val('https_enabled'), 'files': file_summaries, 'acme': config.get_val('acme')}","for (key, file_res_cls) in FileHandler.mapped_resources.items():
    file_summaries[key] = file_res_cls.db_serialize(session, tid)","file_summaries = {key: file_res_cls.db_serialize(session, tid) for (key, file_res_cls) in FileHandler.mapped_resources.items()}","file_summaries = {key: file_res_cls.db_serialize(session, tid) for (key, file_res_cls) in FileHandler.mapped_resources.items()}",1,,,
rllab,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rllab/rllab/algos/cma_es_lib.py,https://github.com/rll/rllab/tree/master/rllab/algos/cma_es_lib.py,CMADataLogger,data$5712,"def data(self):
    """"""return dictionary with data.

        If data entries are None or incomplete, consider calling
        ``.load().data()`` to (re-)load the data from files first.

        """"""
    d = {}
    for name in self.key_names:
        d[name] = self.__dict__.get(name, None)
    return d","for name in self.key_names:
    d[name] = self.__dict__.get(name, None)","d = {name: self.__dict__.get(name, None) for name in self.key_names}","d = {name: self.__dict__.get(name, None) for name in self.key_names}",1,,,
ROMP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ROMP/romp/lib/utils/util.py,https://github.com/Arthur151/ROMP/tree/master/romp/lib/utils/util.py,,read_h5$196,"def read_h5(name):
    if name[-3:] != '.h5':
        name += '.h5'
    f = h5py.File(name, 'r')
    info = {}
    for (item, value) in f.items():
        info[item] = np.array(value)
    f.close()
    return info","for (item, value) in f.items():
    info[item] = np.array(value)","info = {item: np.array(value) for (item, value) in f.items()}","info = {item: np.array(value) for (item, value) in f.items()}",1,,,
texar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/texar/texar/tf/data/data/dataset_utils.py,https://github.com/asyml/texar/tree/master/texar/tf/data/data/dataset_utils.py,_DataSpec,get_ith_data_spec$65,"def get_ith_data_spec(self, i):
    """"""Returns an instance of :class:`_DataSpec` that contains the
        `i`-th specifications.
        """"""
    kwargs = {}
    for (k, v) in six.iteritems(self.__dict__):
        kwargs[k] = v[i] if isinstance(v, (tuple, list)) else v
    return _DataSpec(**kwargs)","for (k, v) in six.iteritems(self.__dict__):
    kwargs[k] = v[i] if isinstance(v, (tuple, list)) else v","kwargs = {k: v[i] if isinstance(v, (tuple, list)) else v for (k, v) in six.iteritems(self.__dict__)}","kwargs = {k: v[i] if isinstance(v, (tuple, list)) else v for (k, v) in six.iteritems(self.__dict__)}",1,,,
sfepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sfepy/sfepy/base/conf.py,https://github.com/sfepy/sfepy/tree/master/sfepy/base/conf.py,ProblemConf,setup$432,"def setup(self, define_dict=None, funmod=None, filename=None, required=None, other=None):
    define_dict = get_default(define_dict, self.__dict__)
    self._filename = filename
    self.validate(required=required, other=other)
    self.transform_input_trivial()
    self._raw = {}
    for (key, val) in six.iteritems(define_dict):
        if isinstance(val, dict):
            self._raw[key] = copy(val)
    self.transform_input()
    self.funmod = funmod","for (key, val) in six.iteritems(define_dict):
    if isinstance(val, dict):
        self._raw[key] = copy(val)","self._raw = {key: copy(val) for (key, val) in six.iteritems(define_dict) if isinstance(val, dict)}","self._raw = {key: copy(val) for (key, val) in six.iteritems(define_dict) if isinstance(val, dict)}",1,,,
maro,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/maro/maro/simulator/scenarios/citi_bike/decision_strategy.py,https://github.com/microsoft/maro/tree/master/maro/simulator/scenarios/citi_bike/decision_strategy.py,DistanceFilter,filter$27,"def filter(self, station_idx: int, deision_type: DecisionType, source: Dict[int, int]) -> Dict[int, int]:
    """"""Filter neighbors by distance from neareat to farest.

        Args:
            station_idx (int): Station index to sort its neighbors.
            deision_type (DecisionType): Current decision type.
            source (dict): Input neighbors for sorting.

        Returns:
            dict: N nearest neighbors.
        """"""
    output_num = min(self._output_num, len(source))
    neighbors = self._strategy._get_neighbors(station_idx)
    output_neighbors = neighbors[0:output_num]
    result = {}
    for (neighbor_idx, _) in output_neighbors:
        result[neighbor_idx] = source[neighbor_idx]
    return result","for (neighbor_idx, _) in output_neighbors:
    result[neighbor_idx] = source[neighbor_idx]","result = {neighbor_idx: source[neighbor_idx] for (neighbor_idx, _) in output_neighbors}","result = {neighbor_idx: source[neighbor_idx] for (neighbor_idx, _) in output_neighbors}",1,,,
core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/script/hassfest/requirements.py,https://github.com/home-assistant/core/tree/master/script/hassfest/requirements.py,,ensure_cache$169,"def ensure_cache():
    """"""Ensure we have a cache of pipdeptree.

    {
        ""flake8-docstring"": {
            ""key"": ""flake8-docstrings"",
            ""package_name"": ""flake8-docstrings"",
            ""installed_version"": ""1.5.0""
            ""dependencies"": {""flake8""}
        }
    }
    """"""
    global PIPDEPTREE_CACHE
    if PIPDEPTREE_CACHE is not None:
        return
    cache = {}
    for item in json.loads(subprocess.run(['pipdeptree', '-w', 'silence', '--json'], check=True, capture_output=True, text=True).stdout):
        cache[item['package']['key']] = {**item['package'], 'dependencies': {dep['key'] for dep in item['dependencies']}}
    PIPDEPTREE_CACHE = cache","for item in json.loads(subprocess.run(['pipdeptree', '-w', 'silence', '--json'], check=True, capture_output=True, text=True).stdout):
    cache[item['package']['key']] = {**item['package'], 'dependencies': {dep['key'] for dep in item['dependencies']}}","cache = {item['package']['key']: {**item['package'], 'dependencies': {dep['key'] for dep in item['dependencies']}} for item in json.loads(subprocess.run(['pipdeptree', '-w', 'silence', '--json'], check=True, capture_output=True, text=True).stdout)}","cache = {item['package']['key']: {**item['package'], 'dependencies': {dep['key'] for dep in item['dependencies']}} for item in json.loads(subprocess.run(['pipdeptree', '-w', 'silence', '--json'], check=True, capture_output=True, text=True).stdout)}",1,,,
nlp-architect,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp-architect/nlp_architect/data/cdc_resources/gen_scripts/create_reference_dict_dump.py,https://github.com/IntelLabs/nlp-architect/tree/master/nlp_architect/data/cdc_resources/gen_scripts/create_reference_dict_dump.py,,ref_dict_dump$39,"def ref_dict_dump():
    logger.info('Extracting referent dict dump, this may take a while...')
    ref_dict_file = args.ref_dict
    out_file = args.output
    mentions_entity_gold_file = [args.mentions]
    vocab = load_mentions_vocab_from_files(mentions_entity_gold_file, True)
    ref_dict = ReferentDictRelationExtraction.load_reference_dict(ref_dict_file)
    ref_dict_for_vocab = {}
    for word in vocab:
        if word in ref_dict:
            ref_dict_for_vocab[word] = ref_dict[word]
    logger.info('Found %d words from vocabulary', len(ref_dict_for_vocab.keys()))
    logger.info('Preparing to save refDict output file')
    with open(out_file, 'w') as f:
        json.dump(ref_dict_for_vocab, f)
    logger.info('Done saved to-%s', out_file)","for word in vocab:
    if word in ref_dict:
        ref_dict_for_vocab[word] = ref_dict[word]",ref_dict_for_vocab = {word: ref_dict[word] for word in vocab if word in ref_dict},ref_dict_for_vocab = {word: ref_dict[word] for word in vocab if word in ref_dict},1,,,
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/obj/test_replicator.py,https://github.com/openstack/swift/tree/master/test/unit/obj/test_replicator.py,TestObjectReplicator,test_collect_jobs$503,"def test_collect_jobs(self):
    jobs = self.replicator.collect_jobs()
    jobs_to_delete = [j for j in jobs if j['delete']]
    jobs_by_pol_part = {}
    for job in jobs:
        jobs_by_pol_part[str(int(job['policy'])) + job['partition']] = job
    self.assertEqual(len(jobs_to_delete), 2)
    self.assertEqual('1', jobs_to_delete[0]['partition'])
    self.assertEqual([node['id'] for node in jobs_by_pol_part['00']['nodes']], [1, 2])
    self.assertEqual([node['id'] for node in jobs_by_pol_part['01']['nodes']], [1, 2, 3])
    self.assertEqual([node['id'] for node in jobs_by_pol_part['02']['nodes']], [2, 3])
    self.assertEqual([node['id'] for node in jobs_by_pol_part['03']['nodes']], [3, 1])
    self.assertEqual([node['id'] for node in jobs_by_pol_part['10']['nodes']], [1, 2])
    self.assertEqual([node['id'] for node in jobs_by_pol_part['11']['nodes']], [1, 2, 3])
    self.assertEqual([node['id'] for node in jobs_by_pol_part['12']['nodes']], [2, 3])
    self.assertEqual([node['id'] for node in jobs_by_pol_part['13']['nodes']], [3, 1])
    for part in ['00', '01', '02', '03']:
        for node in jobs_by_pol_part[part]['nodes']:
            self.assertEqual(node['device'], 'sda')
        self.assertEqual(jobs_by_pol_part[part]['path'], os.path.join(self.objects, part[1:]))
    for part in ['10', '11', '12', '13']:
        for node in jobs_by_pol_part[part]['nodes']:
            self.assertEqual(node['device'], 'sda')
        self.assertEqual(jobs_by_pol_part[part]['path'], os.path.join(self.objects_1, part[1:]))","for job in jobs:
    jobs_by_pol_part[str(int(job['policy'])) + job['partition']] = job",jobs_by_pol_part = {str(int(job['policy'])) + job['partition']: job for job in jobs},jobs_by_pol_part = {str(int(job['policy'])) + job['partition']: job for job in jobs},1,,,
deep-learning-for-image-processing,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep-learning-for-image-processing/pytorch_segmentation/lraspp/train_utils/train_and_eval.py,https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_segmentation/lraspp/train_utils/train_and_eval.py,,criterion$6,"def criterion(inputs, target):
    losses = {}
    for (name, x) in inputs.items():
        losses[name] = nn.functional.cross_entropy(x, target, ignore_index=255)
    if len(losses) == 1:
        return losses['out']
    return losses['out'] + 0.5 * losses['aux']","for (name, x) in inputs.items():
    losses[name] = nn.functional.cross_entropy(x, target, ignore_index=255)","losses = {name: nn.functional.cross_entropy(x, target, ignore_index=255) for (name, x) in inputs.items()}","losses = {name: nn.functional.cross_entropy(x, target, ignore_index=255) for (name, x) in inputs.items()}",1,,,
deeplab2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deeplab2/trainer/evaluator_test.py,https://github.com/google-research/deeplab2/tree/master/trainer/evaluator_test.py,RealDataEvaluatorTest,test_evaluates_max_deeplab_model$124,"def test_evaluates_max_deeplab_model(self):
    tf.random.set_seed(0)
    np.random.seed(0)
    small_instances = {'threshold': 4096, 'weight': 1.0}
    generator = sample_generator.PanopticSampleGenerator(dataset.CITYSCAPES_PANOPTIC_INFORMATION._asdict(), focus_small_instances=small_instances, is_training=False, crop_size=[769, 769], thing_id_mask_annotations=True)
    input_sample = {'image': self._rgb_image, 'image_name': 'test_image', 'label': self._label, 'height': 800, 'width': 800}
    sample = generator(input_sample)
    experiment_options_textproto = '\n      experiment_name: ""evaluation_test""\n      eval_dataset_options {\n        dataset: ""cityscapes_panoptic""\n        file_pattern: ""EMPTY""\n        batch_size: 1\n        crop_size: 769\n        crop_size: 769\n        thing_id_mask_annotations: true\n      }\n      evaluator_options {\n        continuous_eval_timeout: -1\n        stuff_area_limit: 2048\n        center_score_threshold: 0.1\n        nms_kernel: 13\n        save_predictions: true\n        save_raw_predictions: false\n      }\n    '
    config = text_format.Parse(experiment_options_textproto, config_pb2.ExperimentOptions())
    model_proto_filename = os.path.join(_CONFIG_PATH, 'example_coco_max_deeplab.textproto')
    model_config = _read_proto_file(model_proto_filename, config_pb2.ExperimentOptions())
    config.model_options.CopyFrom(model_config.model_options)
    config.model_options.max_deeplab.auxiliary_semantic_head.output_channels = 19
    model = deeplab.DeepLab(config, dataset.CITYSCAPES_PANOPTIC_INFORMATION)
    pool_size = (49, 49)
    model.set_pool_size(pool_size)
    loss_layer = _create_max_deeplab_loss(dataset.CITYSCAPES_PANOPTIC_INFORMATION)
    global_step = tf.Variable(initial_value=0, dtype=tf.int64)
    batched_sample = {}
    for (key, value) in sample.items():
        batched_sample[key] = tf.expand_dims(value, axis=0)
    real_data = [batched_sample]
    with tempfile.TemporaryDirectory() as model_dir:
        with mock.patch.object(runner_utils, 'create_dataset'):
            ev = evaluator.Evaluator(config, model, loss_layer, global_step, model_dir)
            state = ev.eval_begin()
            self.assertTrue(os.path.isdir(os.path.join(model_dir, 'vis')))
            step_outputs = ev.eval_step(iter(real_data))
            state = ev.eval_reduce(state, step_outputs)
            result = ev.eval_end(state)
    expected_metric_keys = {'losses/eval_' + common.TOTAL_LOSS, 'losses/eval_' + common.SEMANTIC_LOSS, 'losses/eval_' + common.PQ_STYLE_LOSS_CLASS_TERM, 'losses/eval_' + common.PQ_STYLE_LOSS_MASK_DICE_TERM, 'losses/eval_' + common.MASK_ID_CROSS_ENTROPY_LOSS, 'losses/eval_' + common.INSTANCE_DISCRIMINATION_LOSS, 'evaluation/iou/IoU', 'evaluation/pq/PQ', 'evaluation/pq/SQ', 'evaluation/pq/RQ', 'evaluation/pq/TP', 'evaluation/pq/FN', 'evaluation/pq/FP', 'evaluation/ap/AP_Mask'}
    self.assertCountEqual(result.keys(), expected_metric_keys)
    self.assertSequenceEqual(result['losses/eval_total_loss'].shape, ())","for (key, value) in sample.items():
    batched_sample[key] = tf.expand_dims(value, axis=0)","batched_sample = {key: tf.expand_dims(value, axis=0) for (key, value) in sample.items()}","batched_sample = {key: tf.expand_dims(value, axis=0) for (key, value) in sample.items()}",1,,,
glance,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/glance/glance/api/property_protections.py,https://github.com/openstack/glance/tree/master/glance/api/property_protections.py,ExtraPropertiesProxy,__init__$83,"def __init__(self, context, extra_props, property_rules):
    self.context = context
    self.property_rules = property_rules
    extra_properties = {}
    for key in extra_props.keys():
        if self.property_rules.check_property_rules(key, 'read', self.context):
            extra_properties[key] = extra_props[key]
    super(ExtraPropertiesProxy, self).__init__(extra_properties)","for key in extra_props.keys():
    if self.property_rules.check_property_rules(key, 'read', self.context):
        extra_properties[key] = extra_props[key]","extra_properties = {key: extra_props[key] for key in extra_props.keys() if self.property_rules.check_property_rules(key, 'read', self.context)}","extra_properties = {key: extra_props[key] for key in extra_props.keys() if self.property_rules.check_property_rules(key, 'read', self.context)}",1,,,
OpenSfM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenSfM/opensfm/io.py,https://github.com/mapillary/OpenSfM/tree/master/opensfm/io.py,,rig_cameras_from_json$220,"def rig_cameras_from_json(obj: Dict[str, Any]) -> Dict[str, pymap.RigCamera]:
    """"""
    Read rig cameras from a json object
    """"""
    rig_cameras = {}
    for (key, value) in obj.items():
        rig_cameras[key] = rig_camera_from_json(key, value)
    return rig_cameras","for (key, value) in obj.items():
    rig_cameras[key] = rig_camera_from_json(key, value)","rig_cameras = {key: rig_camera_from_json(key, value) for (key, value) in obj.items()}","rig_cameras = {key: rig_camera_from_json(key, value) for (key, value) in obj.items()}",1,,,
indico,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/indico/indico/modules/users/ext.py,https://github.com/indico/indico/tree/master/indico/modules/users/ext.py,ExtraUserPreferences,process_form_data$46,"def process_form_data(self, data):
    """"""Process and save submitted data.

        This modifies `data` so the core code doesn't receive any extra
        data it doesn't expect.
        """"""
    local_data = {}
    for key in self.fields:
        local_data[key] = data.pop(self._prefix + key)
    self.save(local_data)","for key in self.fields:
    local_data[key] = data.pop(self._prefix + key)",local_data = {key: data.pop(self._prefix + key) for key in self.fields},local_data = {key: data.pop(self._prefix + key) for key in self.fields},1,,,
dynaconf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dynaconf/dynaconf/vendor_src/box/box.py,https://github.com/rochacbruno/dynaconf/tree/master/dynaconf/vendor_src/box/box.py,Box,from_json$597,"def from_json(cls, json_string: str=None, filename: Union[str, Path]=None, encoding: str='utf-8', errors: str='strict', **kwargs):
    """"""
        Transform a json object string into a Box object. If the incoming
        json is a list, you must use BoxList.from_json.

        :param json_string: string to pass to `json.loads`
        :param filename: filename to open and pass to `json.load`
        :param encoding: File encoding
        :param errors: How to handle encoding errors
        :param kwargs: parameters to pass to `Box()` or `json.loads`
        :return: Box object from json data
        """"""
    box_args = {}
    for arg in kwargs.copy():
        if arg in BOX_PARAMETERS:
            box_args[arg] = kwargs.pop(arg)
    data = _from_json(json_string, filename=filename, encoding=encoding, errors=errors, **kwargs)
    if not isinstance(data, dict):
        raise BoxError(f'json data not returned as a dictionary, but rather a {type(data).__name__}')
    return cls(data, **box_args)","for arg in kwargs.copy():
    if arg in BOX_PARAMETERS:
        box_args[arg] = kwargs.pop(arg)",box_args = {arg: kwargs.pop(arg) for arg in kwargs.copy() if arg in BOX_PARAMETERS},box_args = {arg: kwargs.pop(arg) for arg in kwargs.copy() if arg in BOX_PARAMETERS},1,,,
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/run_classifier.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//run_classifier.py,,file_based_convert_examples_to_features$426,"def file_based_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_file):
    """"""Convert a set of `InputExample`s to a TFRecord file.""""""
    writer = tf.python_io.TFRecordWriter(output_file)
    label_map = {}
    for (i, label) in enumerate(sorted(label_list)):
        label_map[label] = i
    for (ex_index, example) in enumerate(examples):
        if ex_index % 10000 == 0:
            tf.logging.info('Writing example %d of %d' % (ex_index, len(examples)))
        feature = convert_single_example(ex_index, example, label_map, max_seq_length, tokenizer)

        def create_int_feature(values):
            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))
            return f
        features = collections.OrderedDict()
        features['input_ids'] = create_int_feature(feature.input_ids)
        features['input_mask'] = create_int_feature(feature.input_mask)
        features['segment_ids'] = create_int_feature(feature.segment_ids)
        features['label_ids'] = create_int_feature([feature.label_id])
        tf_example = tf.train.Example(features=tf.train.Features(feature=features))
        writer.write(tf_example.SerializeToString())
    return label_map","for (i, label) in enumerate(sorted(label_list)):
    label_map[label] = i","label_map = {label: i for (i, label) in enumerate(sorted(label_list))}","label_map = {label: i for (i, label) in enumerate(sorted(label_list))}",1,,,
chisel,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chisel/commands/FBDebugCommands.py,https://github.com/facebook/chisel/tree/master/commands/FBDebugCommands.py,FBHeapFromCommand,run$502,"def run(self, arguments, options):
    var = self.context.frame.var(arguments[0])
    if not var or not var.IsValid():
        self.result.SetError('No variable named ""{}""'.format(arguments[0]))
        return
    root = var.GetNonSyntheticValue()
    leafs = []
    queue = [root]
    while queue:
        node = queue.pop(0)
        if node.num_children == 0:
            leafs.append(node)
        else:
            queue += [node.GetChildAtIndex(i) for i in range(node.num_children)]
    pointers = {}
    for node in leafs:
        if node.addr and (not node.value):
            pointers[node.load_addr] = node.path
    options = lldb.SBExpressionOptions()
    options.SetLanguage(lldb.eLanguageTypeC)

    def isHeap(addr):
        lookup = '(int)malloc_size({})'.format(addr)
        return self.context.frame.EvaluateExpression(lookup, options).unsigned != 0
    allocations = (addr for addr in pointers if isHeap(addr))
    for addr in allocations:
        print('0x{addr:x} {path}'.format(addr=addr, path=pointers[addr]), file=self.result)
    if not allocations:
        print('No heap addresses found', file=self.result)","for node in leafs:
    if node.addr and (not node.value):
        pointers[node.load_addr] = node.path",pointers = {node.load_addr: node.path for node in leafs if node.addr and (not node.value)},pointers = {node.load_addr: node.path for node in leafs if node.addr and (not node.value)},1,,,
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/tools/c7n_org/c7n_org/cli.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/tools/c7n_org/c7n_org/cli.py,,run_script$481,"def run_script(config, output_dir, accounts, tags, region, echo, serial, script_args):
    """"""run an aws/azure/gcp script across accounts""""""
    (accounts_config, custodian_config, executor) = init(config, None, serial, True, accounts, tags, (), ())
    if echo:
        print('command to run: `%s`' % ' '.join(script_args))
        return
    if len(script_args) == 1 and ' ' in script_args[0]:
        script_args = script_args[0].split()
    success = True
    with executor(max_workers=WORKER_COUNT) as w:
        futures = {}
        for a in accounts_config.get('accounts', ()):
            for r in resolve_regions(region or a.get('regions', ()), a):
                futures[w.submit(run_account_script, a, r, output_dir, serial, script_args)] = (a, r)
        for f in as_completed(futures):
            (a, r) = futures[f]
            if f.exception():
                if serial:
                    raise
                log.warning('Error running script in %s @ %s exception: %s', a['name'], r, f.exception())
                success = False
            exit_code = f.result()
            if exit_code == 0:
                log.info('ran script on account:%s region:%s script: `%s`', a['name'], r, ' '.join(script_args))
            else:
                log.info('error running script on account:%s region:%s script: `%s`', a['name'], r, ' '.join(script_args))
                success = False
    if not success:
        sys.exit(1)","for a in accounts_config.get('accounts', ()):
    for r in resolve_regions(region or a.get('regions', ()), a):
        futures[w.submit(run_account_script, a, r, output_dir, serial, script_args)] = (a, r)","futures = {w.submit(run_account_script, a, r, output_dir, serial, script_args): (a, r) for a in accounts_config.get('accounts', ()) for r in resolve_regions(region or a.get('regions', ()), a)}","futures = {w.submit(run_account_script, a, r, output_dir, serial, script_args): (a, r) for a in accounts_config.get('accounts', ()) for r in resolve_regions(region or a.get('regions', ()), a)}",1,,,
MxShop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MxShop/extra_apps/rest_framework/routers.py,https://github.com/derek-zhang123/MxShop/tree/master/extra_apps/rest_framework/routers.py,SimpleRouter,get_method_map$215,"def get_method_map(self, viewset, method_map):
    """"""
        Given a viewset, and a mapping of http methods to actions,
        return a new mapping which only includes any mappings that
        are actually implemented by the viewset.
        """"""
    bound_methods = {}
    for (method, action) in method_map.items():
        if hasattr(viewset, action):
            bound_methods[method] = action
    return bound_methods","for (method, action) in method_map.items():
    if hasattr(viewset, action):
        bound_methods[method] = action","bound_methods = {method: action for (method, action) in method_map.items() if hasattr(viewset, action)}","bound_methods = {method: action for (method, action) in method_map.items() if hasattr(viewset, action)}",1,,,
dpark,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dpark/dpark/cache.py,https://github.com/douban/dpark/tree/master/dpark/cache.py,CacheTrackerServer,getLocationsSnapshot$224,"def getLocationsSnapshot(self):
    result = {}
    for (rdd_id, partitions) in self.rdds.items():
        result[rdd_id] = [self.locs.get('cache:%s-%s' % (rdd_id, index), []) for index in range(partitions)]
    return result","for (rdd_id, partitions) in self.rdds.items():
    result[rdd_id] = [self.locs.get('cache:%s-%s' % (rdd_id, index), []) for index in range(partitions)]","result = {rdd_id: [self.locs.get('cache:%s-%s' % (rdd_id, index), []) for index in range(partitions)] for (rdd_id, partitions) in self.rdds.items()}","result = {rdd_id: [self.locs.get('cache:%s-%s' % (rdd_id, index), []) for index in range(partitions)] for (rdd_id, partitions) in self.rdds.items()}",1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/neutronng.py,https://github.com/saltstack/salt/tree/master/salt/modules/neutronng.py,,compare_changes$52,"def compare_changes(obj, **kwargs):
    """"""
    Compare two dicts returning only keys that exist in the first dict and are
    different in the second one
    """"""
    changes = {}
    for (key, value) in obj.items():
        if key in kwargs:
            if value != kwargs[key]:
                changes[key] = kwargs[key]
    return changes","for (key, value) in obj.items():
    if key in kwargs:
        if value != kwargs[key]:
            changes[key] = kwargs[key]","changes = {key: kwargs[key] for (key, value) in obj.items() if key in kwargs and value != kwargs[key]}","changes = {key: kwargs[key] for (key, value) in obj.items() if key in kwargs and value != kwargs[key]}",1,,,
Minecraft-Overviewer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Minecraft-Overviewer/test/test_tileset.py,https://github.com/overviewer/Minecraft-Overviewer/tree/master/test/test_tileset.py,TilesetTest,test_update_chunk2$214,"def test_update_chunk2(self):
    """"""Same as above but with a different set of chunks
        """"""
    chunks = list(self.rs.chunks.keys())
    self.r.shuffle(chunks)
    updated_chunks = {}
    for key in chunks[:3]:
        updated_chunks[key] = 6
    self.rs.chunks.update(updated_chunks)
    ts = self.get_tileset({'renderchecks': 0}, self.get_outputdir(), lambda ts: setattr(ts, 'last_rendertime', 5))
    self.compare_iterate_to_expected(ts, updated_chunks)","for key in chunks[:3]:
    updated_chunks[key] = 6",updated_chunks = {key: 6 for key in chunks[:3]},updated_chunks = {key: 6 for key in chunks[:3]},1,,,
site,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/site/pydis_site/apps/content/utils.py,https://github.com/python-discord/site/tree/master/pydis_site/apps/content/utils.py,,get_category_pages$298,"def get_category_pages(path: Path) -> dict[str, dict]:
    """"""Get all page names and their metadata at a category path.""""""
    if path == Path(__file__).parent / 'resources/tags':
        return get_tag_category(collapse_groups=True)
    pages = {}
    for item in path.glob('*.md'):
        if item.is_file() and (not item.with_suffix('').is_dir()):
            pages[item.stem] = frontmatter.load(item).metadata
    return pages","for item in path.glob('*.md'):
    if item.is_file() and (not item.with_suffix('').is_dir()):
        pages[item.stem] = frontmatter.load(item).metadata",pages = {item.stem: frontmatter.load(item).metadata for item in path.glob('*.md') if item.is_file() and (not item.with_suffix('').is_dir())},pages = {item.stem: frontmatter.load(item).metadata for item in path.glob('*.md') if item.is_file() and (not item.with_suffix('').is_dir())},1,,,
knowledge-repo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/knowledge-repo/knowledge_repo/app/routes/debug.py,https://github.com/airbnb/knowledge-repo/tree/master/knowledge_repo/app/routes/debug.py,,show_views$80,"def show_views():
    output = []
    for rule in current_app.url_map.iter_rules():
        options = {}
        for arg in rule.arguments:
            options[arg] = f'[{arg}]'
        methods = ','.join(rule.methods)
        url = url_for(rule.endpoint, **options)
        line = unquote(f'{rule.endpoint:50s} {methods:20s} {url}')
        output.append(line)
    return '<br />'.join(sorted(output))","for arg in rule.arguments:
    options[arg] = f'[{arg}]'",options = {arg: f'[{arg}]' for arg in rule.arguments},options = {arg: f'[{arg}]' for arg in rule.arguments},1,,,
alfred-workflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/alfred-workflow/tests/test_workflow_xml.py,https://github.com/deanishe/alfred-workflow/tree/master/tests/test_workflow_xml.py,,test_item_creation_with_modifiers$92,"def test_item_creation_with_modifiers(wf):
    """"""XML generation (with modifiers).""""""
    mod_subs = {}
    for mod in ('cmd', 'ctrl', 'alt', 'shift', 'fn'):
        mod_subs[mod] = mod
    wf.add_item('title', 'subtitle', mod_subs, arg='arg', autocomplete='autocomplete', valid=True, uid='uid', icon='icon.png', icontype='fileicon', type='file')
    with stdout() as sio:
        wf.send_feedback()
        output = sio.getvalue()
    root = ET.fromstring(output)
    item = list(root)[0]
    assert item.attrib['uid'] == 'uid'
    assert item.attrib['autocomplete'] == 'autocomplete'
    assert item.attrib['valid'] == 'yes'
    assert item.attrib['uid'] == 'uid'
    (title, subtitle, sub_cmd, sub_ctrl, sub_alt, sub_shift, sub_fn, arg, icon) = list(item)
    assert title.text == 'title'
    assert title.tag == 'title'
    assert subtitle.text == 'subtitle'
    assert sub_cmd.text == 'cmd'
    assert sub_cmd.attrib['mod'] == 'cmd'
    assert sub_ctrl.text == 'ctrl'
    assert sub_ctrl.attrib['mod'] == 'ctrl'
    assert sub_alt.text == 'alt'
    assert sub_alt.attrib['mod'] == 'alt'
    assert sub_shift.text == 'shift'
    assert sub_shift.attrib['mod'] == 'shift'
    assert sub_fn.text == 'fn'
    assert sub_fn.attrib['mod'] == 'fn'
    assert subtitle.tag == 'subtitle'
    assert arg.text == 'arg'
    assert arg.tag == 'arg'
    assert icon.text == 'icon.png'
    assert icon.tag == 'icon'
    assert icon.attrib['type'] == 'fileicon'","for mod in ('cmd', 'ctrl', 'alt', 'shift', 'fn'):
    mod_subs[mod] = mod","mod_subs = {mod: mod for mod in ('cmd', 'ctrl', 'alt', 'shift', 'fn')}","mod_subs = {mod: mod for mod in ('cmd', 'ctrl', 'alt', 'shift', 'fn')}",1,,,
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,LoadSpeakerDetails$197,"def LoadSpeakerDetails(speaker_details):
    spk_details = {}
    spk_file = open(speaker_details, 'r', encoding='utf-8')
    content = spk_file.read()
    last_names = re.findall('\\\\last_name (.*?)\\n', content, re.S)
    first_names = re.findall('\\\\first_name (.*?)\\n', content, re.S)
    codes = re.findall('\\\\code (.*?)\\n', content, re.S)
    assert len(last_names) == len(first_names) == len(codes)
    for (last, first, code) in zip(last_names, first_names, codes):
        spk_details['%s %s' % (' '.join(first.split()), ' '.join(last.split()))] = code
    return spk_details","for (last, first, code) in zip(last_names, first_names, codes):
    spk_details['%s %s' % (' '.join(first.split()), ' '.join(last.split()))] = code","spk_details = {'%s %s' % (' '.join(first.split()), ' '.join(last.split())): code for (last, first, code) in zip(last_names, first_names, codes)}","spk_details = {'%s %s' % (' '.join(first.split()), ' '.join(last.split())): code for (last, first, code) in zip(last_names, first_names, codes)}",1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/cloud/clouds/qingcloud.py,https://github.com/saltstack/salt/tree/master/salt/cloud/clouds/qingcloud.py,,avail_sizes$399,"def avail_sizes(kwargs=None, call=None):
    """"""
    Return a list of the instance sizes that are on the provider.

    CLI Examples:

    .. code-block:: bash

        salt-cloud --list-sizes my-qingcloud
        salt-cloud -f avail_sizes my-qingcloud zone=pek2
    """"""
    if call == 'action':
        raise SaltCloudSystemExit('The avail_sizes function must be called with -f or --function, or with the --list-sizes option')
    zone = _get_specified_zone(kwargs, get_configured_provider())
    result = {}
    for size_key in QINGCLOUD_SIZES[zone]:
        result[size_key] = {}
        for attribute_key in QINGCLOUD_SIZES[zone][size_key]:
            result[size_key][attribute_key] = QINGCLOUD_SIZES[zone][size_key][attribute_key]
    return result","for size_key in QINGCLOUD_SIZES[zone]:
    result[size_key] = {}
    for attribute_key in QINGCLOUD_SIZES[zone][size_key]:
        result[size_key][attribute_key] = QINGCLOUD_SIZES[zone][size_key][attribute_key]",result = {size_key: {attribute_key: QINGCLOUD_SIZES[zone][size_key][attribute_key] for attribute_key in QINGCLOUD_SIZES[zone][size_key]} for size_key in QINGCLOUD_SIZES[zone]},result = {size_key: {attribute_key: QINGCLOUD_SIZES[zone][size_key][attribute_key] for attribute_key in QINGCLOUD_SIZES[zone][size_key]} for size_key in QINGCLOUD_SIZES[zone]},1,,,
pyopencl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyopencl/aksetup_helper.py,https://github.com/inducer/pyopencl/tree/master//aksetup_helper.py,ConfigSchema,read_config_from_pyfile$284,"def read_config_from_pyfile(self, filename):
    result = {}
    filevars = {}
    infile = open(filename, 'r')
    try:
        contents = infile.read()
    finally:
        infile.close()
    exec(compile(contents, filename, 'exec'), filevars)
    for (key, value) in filevars.items():
        if key in self.optdict:
            result[key] = value
    return result","for (key, value) in filevars.items():
    if key in self.optdict:
        result[key] = value","result = {key: value for (key, value) in filevars.items() if key in self.optdict}","result = {key: value for (key, value) in filevars.items() if key in self.optdict}",1,,,
EasyClangComplete,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyClangComplete/plugin/clang/cindex.py,https://github.com/niosus/EasyClangComplete/tree/master/plugin/clang/cindex.py,CursorKind,name$337,"def name(self):
    """"""Get the enumeration name of this cursor kind.""""""
    if self._name_map is None:
        self._name_map = {}
        for (key, value) in CursorKind.__dict__.items():
            if isinstance(value, CursorKind):
                self._name_map[value] = key
    return self._name_map[self]","for (key, value) in CursorKind.__dict__.items():
    if isinstance(value, CursorKind):
        self._name_map[value] = key","self._name_map = {value: key for (key, value) in CursorKind.__dict__.items() if isinstance(value, CursorKind)}","self._name_map = {value: key for (key, value) in CursorKind.__dict__.items() if isinstance(value, CursorKind)}",1,,,
beets,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/beets/test/test_autotag.py,https://github.com/beetbox/beets/tree/master/test/test_autotag.py,ApplyTestUtil,_apply$605,"def _apply(self, info=None, per_disc_numbering=False, artist_credit=False):
    info = info or self.info
    mapping = {}
    for (i, t) in zip(self.items, info.tracks):
        mapping[i] = t
    config['per_disc_numbering'] = per_disc_numbering
    config['artist_credit'] = artist_credit
    autotag.apply_metadata(info, mapping)","for (i, t) in zip(self.items, info.tracks):
    mapping[i] = t","mapping = {i: t for (i, t) in zip(self.items, info.tracks)}","mapping = {i: t for (i, t) in zip(self.items, info.tracks)}",1,,,
PaddleDetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleDetection/deploy/pptracking/python/mot/mtmct/postprocess.py,https://github.com/PaddlePaddle/PaddleDetection/tree/master/deploy/pptracking/python/mot/mtmct/postprocess.py,,sub_cluster$93,"def sub_cluster(cid_tid_dict, scene_cluster, use_ff=True, use_rerank=True, use_camera=False, use_st_filter=False):
    """"""
    cid_tid_dict: all camera_id and track_id
    scene_cluster: like [41, 42, 43, 44, 45, 46] in AIC21 MTMCT S06 test videos
    """"""
    assert len(scene_cluster) != 0, 'Error: scene_cluster length equals 0'
    cid_tids = sorted([key for key in cid_tid_dict.keys() if key[0] in scene_cluster])
    if use_camera:
        clu = get_labels_with_camera(cid_tid_dict, cid_tids, use_ff=use_ff, use_rerank=use_rerank, use_st_filter=use_st_filter)
    else:
        clu = get_labels(cid_tid_dict, cid_tids, use_ff=use_ff, use_rerank=use_rerank, use_st_filter=use_st_filter)
    new_clu = list()
    for c_list in clu:
        if len(c_list) <= 1:
            continue
        cam_list = [cid_tids[c][0] for c in c_list]
        if len(cam_list) != len(set(cam_list)):
            continue
        new_clu.append([cid_tids[c] for c in c_list])
    all_clu = new_clu
    cid_tid_label = dict()
    for (i, c_list) in enumerate(all_clu):
        for c in c_list:
            cid_tid_label[c] = i + 1
    return cid_tid_label","for (i, c_list) in enumerate(all_clu):
    for c in c_list:
        cid_tid_label[c] = i + 1","cid_tid_label = {c: i + 1 for (i, c_list) in enumerate(all_clu) for c in c_list}","cid_tid_label = {c: i + 1 for (i, c_list) in enumerate(all_clu) for c in c_list}",1,,,
DeepRec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRec/utils/load_data/load_data_ranking.py,https://github.com/cheungdaven/DeepRec/tree/master/utils/load_data/load_data_ranking.py,,load_data_separately$109,"def load_data_separately(path_train=None, path_test=None, path_val=None, header=['user_id', 'item_id', 'rating'], sep=' ', n_users=0, n_items=0):
    n_users = n_users
    n_items = n_items
    print('start')
    train_matrix = None
    if path_train is not None:
        train_data = pd.read_csv(path_train, sep=sep, names=header, engine='python')
        print('Load data finished. Number of users:', n_users, 'Number of items:', n_items)
        train_row = []
        train_col = []
        train_rating = []
        for line in train_data.itertuples():
            u = line[1]
            i = line[2]
            train_row.append(u)
            train_col.append(i)
            train_rating.append(1)
        train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))
    print('Load data finished. Number of users:', n_users, 'Number of items:', n_items)
    test_dict = None
    if path_test is not None:
        test_data = pd.read_csv(path_test, sep=sep, names=header, engine='python')
        test_row = []
        test_col = []
        test_rating = []
        for line in test_data.itertuples():
            test_row.append(line[1])
            i = line[2]
            test_col.append(i)
            test_rating.append(1)
        test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))
        test_dict = {}
        for u in range(n_users):
            test_dict[u] = test_matrix.getrow(u).nonzero()[1]
    all_items = set(np.arange(n_items))
    train_interaction_matrix = []
    for u in range(n_users):
        train_interaction_matrix.append(list(train_matrix.getrow(u).toarray()[0]))
    if path_val is not None:
        val_data = pd.read_csv(path_val, sep=sep, names=header, engine='python')
    print('end')
    return (train_interaction_matrix, test_dict, n_users, n_items)","for u in range(n_users):
    test_dict[u] = test_matrix.getrow(u).nonzero()[1]",test_dict = {u: test_matrix.getrow(u).nonzero()[1] for u in range(n_users)},test_dict = {u: test_matrix.getrow(u).nonzero()[1] for u in range(n_users)},1,,,
django-treebeard,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-treebeard/treebeard/models.py,https://github.com/django-treebeard/django-treebeard/tree/master/treebeard/models.py,Node,get_foreign_keys$39,"def get_foreign_keys(cls):
    """"""Get foreign keys and models they refer to, so we can pre-process
        the data for load_bulk
        """"""
    foreign_keys = {}
    for field in cls._meta.fields:
        if field.get_internal_type() == 'ForeignKey' and field.name != 'parent':
            foreign_keys[field.name] = field.remote_field.model
    return foreign_keys","for field in cls._meta.fields:
    if field.get_internal_type() == 'ForeignKey' and field.name != 'parent':
        foreign_keys[field.name] = field.remote_field.model",foreign_keys = {field.name: field.remote_field.model for field in cls._meta.fields if field.get_internal_type() == 'ForeignKey' and field.name != 'parent'},foreign_keys = {field.name: field.remote_field.model for field in cls._meta.fields if field.get_internal_type() == 'ForeignKey' and field.name != 'parent'},1,,,
smplx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/smplx/smplx/joint_names.py,https://github.com/vchoutas/smplx/tree/master/smplx/joint_names.py,Body,__init__$277,"def __init__(self, joints, joint_names):
    assert joints.ndim > 1
    assert joints.shape[0] == len(joint_names)
    self.joints = {}
    for (i, j) in enumerate(joint_names):
        self.joints[j] = joints[i]","for (i, j) in enumerate(joint_names):
    self.joints[j] = joints[i]","self.joints = {j: joints[i] for (i, j) in enumerate(joint_names)}","self.joints = {j: joints[i] for (i, j) in enumerate(joint_names)}",1,,,
NVTabular,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NVTabular/nvtabular/ops/moments.py,https://github.com/NVIDIA-Merlin/NVTabular/tree/master/nvtabular/ops/moments.py,,_custom_moments$28,"def _custom_moments(ddf, split_every=32):
    dsk = {}
    token = tokenize(ddf)
    tree_reduce_name = 'chunkwise-moments-' + token
    result_name = 'global-moments-' + token
    for p in range(ddf.npartitions):
        dsk[tree_reduce_name, p, 0] = (_chunkwise_moments, (ddf._name, p))
    parts = ddf.npartitions
    widths = [parts]
    while parts > 1:
        parts = math.ceil(parts / split_every)
        widths.append(parts)
    height = len(widths)
    for depth in range(1, height):
        for group in range(widths[depth]):
            p_max = widths[depth - 1]
            lstart = split_every * group
            lstop = min(lstart + split_every, p_max)
            node_list = [(tree_reduce_name, p, depth - 1) for p in range(lstart, lstop)]
            dsk[tree_reduce_name, group, depth] = (_tree_node_moments, node_list)
    dsk[result_name] = (_finalize_moments, (tree_reduce_name, 0, height - 1))
    graph = HighLevelGraph.from_collections(result_name, dsk, dependencies=[ddf])
    return Delayed(result_name, graph)","for p in range(ddf.npartitions):
    dsk[tree_reduce_name, p, 0] = (_chunkwise_moments, (ddf._name, p))","dsk = {(tree_reduce_name, p, 0): (_chunkwise_moments, (ddf._name, p)) for p in range(ddf.npartitions)}","dsk = {(tree_reduce_name, p, 0): (_chunkwise_moments, (ddf._name, p)) for p in range(ddf.npartitions)}",1,,,
pyyaml,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyyaml/lib/yaml/representer.py,https://github.com/yaml/pyyaml/tree/master/lib/yaml/representer.py,SafeRepresenter,represent_set$209,"def represent_set(self, data):
    value = {}
    for key in data:
        value[key] = None
    return self.represent_mapping('tag:yaml.org,2002:set', value)","for key in data:
    value[key] = None",value = {key: None for key in data},value = {key: None for key in data},1,,,
moto,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/ecr/models.py,https://github.com/spulec/moto/tree/master/moto/ecr/models.py,BaseObject,gen_response_object$51,"def gen_response_object(self):
    response_object = dict()
    for (key, value) in self.__dict__.items():
        if '_' in key:
            response_object[self.camelCase(key)] = value
        else:
            response_object[key] = value
    return response_object","for (key, value) in self.__dict__.items():
    if '_' in key:
        response_object[self.camelCase(key)] = value
    else:
        response_object[key] = value","response_object = {self.camelCase(key): value if '_' in key else value for (key, value) in self.__dict__.items()}","response_object = {self.camelCase(key): value if '_' in key else value for (key, value) in self.__dict__.items()}",1,,,
OctoPrint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OctoPrint/src/octoprint/server/api/slicing.py,https://github.com/OctoPrint/OctoPrint/tree/master/src/octoprint/server/api/slicing.py,,_getSlicingProfilesData$263,"def _getSlicingProfilesData(slicer, require_configured=False):
    profiles = slicingManager.all_profiles(slicer, require_configured=require_configured)
    result = {}
    for (name, profile) in profiles.items():
        result[name] = _getSlicingProfileData(slicer, name, profile)
    return result","for (name, profile) in profiles.items():
    result[name] = _getSlicingProfileData(slicer, name, profile)","result = {name: _getSlicingProfileData(slicer, name, profile) for (name, profile) in profiles.items()}","result = {name: _getSlicingProfileData(slicer, name, profile) for (name, profile) in profiles.items()}",1,,,
object_detector_app,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/object_detector_app/object_detection/utils/label_map_util.py,https://github.com/datitran/object_detector_app/tree/master/object_detection/utils/label_map_util.py,,create_category_index$25,"def create_category_index(categories):
    """"""Creates dictionary of COCO compatible categories keyed by category id.

  Args:
    categories: a list of dicts, each of which has the following keys:
      'id': (required) an integer id uniquely identifying this category.
      'name': (required) string representing category name
        e.g., 'cat', 'dog', 'pizza'.

  Returns:
    category_index: a dict containing the same entries as categories, but keyed
      by the 'id' field of each category.
  """"""
    category_index = {}
    for cat in categories:
        category_index[cat['id']] = cat
    return category_index","for cat in categories:
    category_index[cat['id']] = cat",category_index = {cat['id']: cat for cat in categories},category_index = {cat['id']: cat for cat in categories},1,,,
moto,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/config/models.py,https://github.com/spulec/moto/tree/master/moto/config/models.py,,convert_to_class_args$171,"def convert_to_class_args(dict_arg):
    """"""Return dict that can be used to instantiate it's representative class.

    Given a dictionary in the incoming API request, convert the keys to
    snake case to use as arguments when instatiating the representative
    class's __init__().
    """"""
    class_args = {}
    for (key, value) in dict_arg.items():
        class_args[CAMEL_TO_SNAKE_REGEX.sub('_', key).lower()] = value
    return class_args","for (key, value) in dict_arg.items():
    class_args[CAMEL_TO_SNAKE_REGEX.sub('_', key).lower()] = value","class_args = {CAMEL_TO_SNAKE_REGEX.sub('_', key).lower(): value for (key, value) in dict_arg.items()}","class_args = {CAMEL_TO_SNAKE_REGEX.sub('_', key).lower(): value for (key, value) in dict_arg.items()}",1,,,
PaddleSlim,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSlim/tests/test_slim_prune.py,https://github.com/PaddlePaddle/PaddleSlim/tree/master/tests/test_slim_prune.py,TestPrune,test_prune$24,"def test_prune(self):
    main_program = paddle.static.Program()
    startup_program = paddle.static.Program()
    with paddle.static.program_guard(main_program, startup_program):
        input = paddle.static.data(name='image', shape=[None, 3, 16, 16])
        conv1 = conv_bn_layer(input, 8, 3, 'conv1')
        conv2 = conv_bn_layer(conv1, 8, 3, 'conv2')
        sum1 = conv1 + conv2
        conv3 = conv_bn_layer(sum1, 8, 3, 'conv3')
        conv4 = conv_bn_layer(conv3, 8, 3, 'conv4')
        sum2 = conv4 + sum1
        conv5 = conv_bn_layer(sum2, 8, 3, 'conv5')
        conv6 = conv_bn_layer(conv5, 8, 3, 'conv6')
    shapes = {}
    for param in main_program.global_block().all_parameters():
        shapes[param.name] = param.shape
    place = paddle.CPUPlace()
    exe = paddle.static.Executor(place)
    scope = paddle.static.Scope()
    exe.run(startup_program, scope=scope)
    criterion = 'bn_scale'
    pruner = Pruner(criterion)
    (main_program, _, _) = pruner.prune(main_program, scope, params=['conv4_weights'], ratios=[0.5], place=place, lazy=False, only_graph=False, param_backup=None, param_shape_backup=None)
    shapes = {'conv1_weights': (4, 3, 3, 3), 'conv2_weights': (4, 4, 3, 3), 'conv3_weights': (8, 4, 3, 3), 'conv4_weights': (4, 8, 3, 3), 'conv5_weights': (8, 4, 3, 3), 'conv6_weights': (8, 8, 3, 3)}
    for param in main_program.global_block().all_parameters():
        if 'weights' in param.name:
            print('param: {}; param shape: {}'.format(param.name, param.shape))
            self.assertTrue(param.shape == shapes[param.name])","for param in main_program.global_block().all_parameters():
    shapes[param.name] = param.shape",shapes = {param.name: param.shape for param in main_program.global_block().all_parameters()},shapes = {param.name: param.shape for param in main_program.global_block().all_parameters()},1,,,
tensorpack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorpack/tensorpack/tfutils/collection.py,https://github.com/tensorpack/tensorpack/tree/master/tensorpack/tfutils/collection.py,,backup_collection$19,"def backup_collection(keys=None):
    """"""
    Args:
        keys (list): list of collection keys to backup.
            Defaults to all keys in the graph.

    Returns:
        dict: the backup
    """"""
    if keys is None:
        keys = tf.get_default_graph().get_all_collection_keys()
    ret = {}
    assert isinstance(keys, (list, tuple, set))
    for k in keys:
        ret[k] = copy(tf.get_collection(k))
    return ret","for k in keys:
    ret[k] = copy(tf.get_collection(k))",ret = {k: copy(tf.get_collection(k)) for k in keys},ret = {k: copy(tf.get_collection(k)) for k in keys},1,,,
model-analysis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/model-analysis/tensorflow_model_analysis/evaluators/metrics_plots_and_validations_evaluator_test.py,https://github.com/tensorflow/model-analysis/tree/master/tensorflow_model_analysis/evaluators/metrics_plots_and_validations_evaluator_test.py,MetricsPlotsAndValidationsEvaluatorTest,check_metrics$992,"def check_metrics(got):
    try:
        self.assertLen(got, 3)
        slices = {}
        for (slice_key, value) in got:
            slices[slice_key] = value
        overall_slice = ()
        fixed_string1_slice = (('fixed_string', 'fixed_string1'),)
        fixed_string2_slice = (('fixed_string', 'fixed_string2'),)
        self.assertCountEqual(list(slices.keys()), [overall_slice, fixed_string1_slice, fixed_string2_slice])
        example_count_key = metric_types.MetricKey(name='example_count')
        weighted_example_count_key = metric_types.MetricKey(name='weighted_example_count', example_weighted=True)
        label_key = metric_types.MetricKey(name='mean_label', example_weighted=True)
        pred_key = metric_types.MetricKey(name='mean_prediction', example_weighted=True)
        self.assertLen(slices[overall_slice], 4)
        self.assertDictElementsAlmostEqual(slices[overall_slice], {example_count_key: 3000, weighted_example_count_key: 4000.0})
        self.assertDictElementsWithTDistributionAlmostEqual(slices[overall_slice], {label_key: (1.0 + 0.0 + 2 * 0.0) / (1.0 + 1.0 + 2.0), pred_key: (0.2 + 0.8 + 2 * 0.5) / (1.0 + 1.0 + 2.0)})
        self.assertDictElementsAlmostEqual(slices[fixed_string1_slice], {weighted_example_count_key: 2000.0})
        self.assertDictElementsWithTDistributionAlmostEqual(slices[fixed_string1_slice], {label_key: (1.0 + 0.0) / (1.0 + 1.0), pred_key: (0.2 + 0.8) / (1.0 + 1.0)})
        self.assertDictElementsAlmostEqual(slices[fixed_string2_slice], {weighted_example_count_key: 2000.0})
        self.assertDictElementsWithTDistributionAlmostEqual(slices[fixed_string2_slice], {label_key: 2 * 0.0 / 2.0, pred_key: 2 * 0.5 / 2.0})
    except AssertionError as err:
        raise util.BeamAssertException(err)","for (slice_key, value) in got:
    slices[slice_key] = value","slices = {slice_key: value for (slice_key, value) in got}","slices = {slice_key: value for (slice_key, value) in got}",1,,,
gradient-checkpointing,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gradient-checkpointing/test/util.py,https://github.com/cybertronai/gradient-checkpointing/tree/master/test/util.py,,sort$353,"def sort(nodes, total_order, dedup=False):
    """"""Sorts nodes according to order provided.
  
  Args:
    nodes: nodes to sort
    total_order: list of nodes in correct order
    dedup: if True, also discards duplicates in nodes

  Returns:
    Iterable of nodes in sorted order.
  """"""
    total_order_idx = {}
    for (i, node) in enumerate(total_order):
        total_order_idx[node] = i
    if dedup:
        nodes = set(nodes)
    return sorted(nodes, key=lambda n: total_order_idx[n])","for (i, node) in enumerate(total_order):
    total_order_idx[node] = i","total_order_idx = {node: i for (i, node) in enumerate(total_order)}","total_order_idx = {node: i for (i, node) in enumerate(total_order)}",1,,,
faraday,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/faraday/faraday/server/utils/web.py,https://github.com/infobyte/faraday/tree/master/faraday/server/utils/web.py,,filter_request_args$34,"def filter_request_args(*filter_out_args):
    filtered_args = {}
    for arg in request.args:
        if arg not in filter_out_args:
            filtered_args[arg] = request.args.get(arg)
    return filtered_args","for arg in request.args:
    if arg not in filter_out_args:
        filtered_args[arg] = request.args.get(arg)",filtered_args = {arg: request.args.get(arg) for arg in request.args if arg not in filter_out_args},filtered_args = {arg: request.args.get(arg) for arg in request.args if arg not in filter_out_args},1,,,
Sark,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Sark/sark/code/switch.py,https://github.com/tmr232/Sark/tree/master/sark/code/switch.py,Switch,_build_map$48,"def _build_map(self, results):
    switch_map = {}
    for (cases, target) in zip(results.cases, results.targets):
        for case in cases:
            switch_map[case] = target
    return switch_map","for (cases, target) in zip(results.cases, results.targets):
    for case in cases:
        switch_map[case] = target","switch_map = {case: target for (cases, target) in zip(results.cases, results.targets) for case in cases}","switch_map = {case: target for (cases, target) in zip(results.cases, results.targets) for case in cases}",1,,,
pysystemtrade,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pysystemtrade/systems/system_cache.py,https://github.com/robcarver17/pysystemtrade/tree/master/systems/system_cache.py,systemCache,as_dict$220,"def as_dict(self):
    self_as_dict = {}
    for ref_name in self.get_items_with_data():
        self_as_dict[ref_name] = self[ref_name]
    return self_as_dict","for ref_name in self.get_items_with_data():
    self_as_dict[ref_name] = self[ref_name]",self_as_dict = {ref_name: self[ref_name] for ref_name in self.get_items_with_data()},self_as_dict = {ref_name: self[ref_name] for ref_name in self.get_items_with_data()},1,,,
neon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neon/neon/layers/layer.py,https://github.com/NervanaSystems/neon/tree/master/neon/layers/layer.py,BatchNorm,get_description$2529,"def get_description(self, get_weights=False, keep_states=True):
    """"""
        Get layer parameters.

        Arguments:
            get_weights (bool, optional): Control whether all parameters are returned or
                                          just weights for serialization.
            keep_states (bool, optional): Controls whether the states should be returned
        """"""
    serial_dict = super(BatchNorm, self).get_description()
    if get_weights:
        serial_dict['params'] = {}
        for key in ['beta', 'gamma', 'gmean', 'gvar']:
            serial_dict['params'][key] = getattr(self, key).get()
        if keep_states:
            serial_dict['states'] = [[s.get() for s in slist] for slist in self.states]
    return serial_dict","for key in ['beta', 'gamma', 'gmean', 'gvar']:
    serial_dict['params'][key] = getattr(self, key).get()","serial_dict['params'] = {key: getattr(self, key).get() for key in ['beta', 'gamma', 'gmean', 'gvar']}","serial_dict['params'] = {key: getattr(self, key).get() for key in ['beta', 'gamma', 'gmean', 'gvar']}",1,,,
alibi-detect,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/alibi-detect/alibi_detect/datasets.py,https://github.com/SeldonIO/alibi-detect/tree/master/alibi_detect/datasets.py,,fetch_kdd$24,"def fetch_kdd(target: list=['dos', 'r2l', 'u2r', 'probe'], keep_cols: list=['srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate'], percent10: bool=True, return_X_y: bool=False) -> Union[Bunch, Tuple[np.ndarray, np.ndarray]]:
    """"""
    KDD Cup '99 dataset. Detect computer network intrusions.

    Parameters
    ----------
    target
        List with attack types to detect.
    keep_cols
        List with columns to keep. Defaults to continuous features.
    percent10
        Bool, whether to only return 10% of the data.
    return_X_y
        Bool, whether to only return the data and target values or a Bunch object.

    Returns
    -------
    Bunch
        Dataset and outlier labels (0 means 'normal' and 1 means 'outlier').
    (data, target)
        Tuple if 'return_X_y' equals True.
    """"""
    data_raw = fetch_kddcup99(subset=None, data_home=None, percent10=percent10)
    cols = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']
    data = pd.DataFrame(data=data_raw['data'], columns=cols)
    data['attack_type'] = data_raw['target']
    attack_list = np.unique(data['attack_type'])
    attack_category = ['dos', 'u2r', 'r2l', 'r2l', 'r2l', 'probe', 'dos', 'u2r', 'r2l', 'dos', 'probe', 'normal', 'u2r', 'r2l', 'dos', 'probe', 'u2r', 'probe', 'dos', 'r2l', 'dos', 'r2l', 'r2l']
    attack_types = {}
    for (i, j) in zip(attack_list, attack_category):
        attack_types[i] = j
    data['attack_category'] = 'normal'
    for (k, v) in attack_types.items():
        data['attack_category'][data['attack_type'] == k] = v
    data['target'] = 0
    for t in target:
        data['target'][data['attack_category'] == t] = 1
    is_outlier = data['target'].values
    drop_cols = []
    for col in data.columns.values:
        if col not in keep_cols:
            drop_cols.append(col)
    if drop_cols != []:
        data.drop(columns=drop_cols, inplace=True)
    if return_X_y:
        return (data.values, is_outlier)
    return Bunch(data=data.values, target=is_outlier, target_names=['normal', 'outlier'], feature_names=keep_cols)","for (i, j) in zip(attack_list, attack_category):
    attack_types[i] = j","attack_types = {i: j for (i, j) in zip(attack_list, attack_category)}","attack_types = {i: j for (i, j) in zip(attack_list, attack_category)}",1,,,
mlrun,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mlrun/tests/test_requirements.py,https://github.com/mlrun/mlrun/tree/master/tests/test_requirements.py,,test_requirement_specifiers_inconsistencies$151,"def test_requirement_specifiers_inconsistencies():
    requirement_specifiers_map = _generate_all_requirement_specifiers_map()
    inconsistent_specifiers_map = {}
    print(requirement_specifiers_map)
    for (requirement_name, requirement_specifiers) in requirement_specifiers_map.items():
        if not len(requirement_specifiers) == 1:
            inconsistent_specifiers_map[requirement_name] = requirement_specifiers
    ignored_inconsistencies_map = {'python-dotenv': {'', '~=0.17.0'}}
    for (inconsistent_requirement_name, inconsistent_specifiers) in ignored_inconsistencies_map.items():
        if inconsistent_requirement_name in inconsistent_specifiers_map:
            diff = deepdiff.DeepDiff(inconsistent_specifiers_map[inconsistent_requirement_name], inconsistent_specifiers, ignore_order=True)
            if diff == {}:
                del inconsistent_specifiers_map[inconsistent_requirement_name]
    assert inconsistent_specifiers_map == {}","for (requirement_name, requirement_specifiers) in requirement_specifiers_map.items():
    if not len(requirement_specifiers) == 1:
        inconsistent_specifiers_map[requirement_name] = requirement_specifiers","inconsistent_specifiers_map = {requirement_name: requirement_specifiers for (requirement_name, requirement_specifiers) in requirement_specifiers_map.items() if not len(requirement_specifiers) == 1}","inconsistent_specifiers_map = {requirement_name: requirement_specifiers for (requirement_name, requirement_specifiers) in requirement_specifiers_map.items() if not len(requirement_specifiers) == 1}",1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/ext/tornado/test/locale_test.py,https://github.com/saltstack/salt/tree/master/salt/ext/tornado/test/locale_test.py,TranslationLoaderTest,setUp$23,"def setUp(self):
    self.saved = {}
    for var in TranslationLoaderTest.SAVE_VARS:
        self.saved[var] = getattr(salt.ext.tornado.locale, var)
    self.clear_locale_cache()","for var in TranslationLoaderTest.SAVE_VARS:
    self.saved[var] = getattr(salt.ext.tornado.locale, var)","self.saved = {var: getattr(salt.ext.tornado.locale, var) for var in TranslationLoaderTest.SAVE_VARS}","self.saved = {var: getattr(salt.ext.tornado.locale, var) for var in TranslationLoaderTest.SAVE_VARS}",1,,,
SMARTS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMARTS/smarts/core/sensors.py,https://github.com/huawei-noah/SMARTS/tree/master/smarts/core/sensors.py,RoadWaypointsSensor,__call__$976,"def __call__(self):
    lp = self._road_network.lanepoints.closest_lanepoint(self._vehicle.pose)
    road_edges = self._road_network.road_edge_data_for_lane_id(lp.lane_id)
    lane_paths = {}
    for edge in road_edges.forward_edges + road_edges.oncoming_edges:
        for lane in edge.getLanes():
            lane_paths[lane.getID()] = self.paths_for_lane(lane)
    route_waypoints = self.route_waypoints()
    return RoadWaypoints(lanes=lane_paths, route_waypoints=route_waypoints)","for edge in road_edges.forward_edges + road_edges.oncoming_edges:
    for lane in edge.getLanes():
        lane_paths[lane.getID()] = self.paths_for_lane(lane)",lane_paths = {lane.getID(): self.paths_for_lane(lane) for edge in road_edges.forward_edges + road_edges.oncoming_edges for lane in edge.getLanes()},lane_paths = {lane.getID(): self.paths_for_lane(lane) for edge in road_edges.forward_edges + road_edges.oncoming_edges for lane in edge.getLanes()},1,,,
maltrail,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/maltrail/trails/feeds/360gameover.py,https://github.com/stamparm/maltrail/tree/master/trails/feeds/360gameover.py,,fetch$17,"def fetch():
    retval = {}
    content = retrieve_content(__url__)
    if __check__ in content:
        for match in re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content):
            retval[match.group(1)] = (__info__, __reference__)
    return retval","for match in re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content):
    retval[match.group(1)] = (__info__, __reference__)","retval = {match.group(1): (__info__, __reference__) for match in re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content)}","retval = {match.group(1): (__info__, __reference__) for match in re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content)}",1,,,
model-analysis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/model-analysis/tensorflow_model_analysis/evaluators/metrics_plots_and_validations_evaluator_test.py,https://github.com/tensorflow/model-analysis/tree/master/tensorflow_model_analysis/evaluators/metrics_plots_and_validations_evaluator_test.py,MetricsPlotsAndValidationsEvaluatorTest,testEvaluateWithEvalSavedModel$2109,"def testEvaluateWithEvalSavedModel(self):
    temp_export_dir = self._getExportDir()
    (_, export_dir) = linear_classifier.simple_linear_classifier(None, temp_export_dir)
    eval_config = config_pb2.EvalConfig(model_specs=[config_pb2.ModelSpec(signature_name='eval')], slicing_specs=[config_pb2.SlicingSpec(), config_pb2.SlicingSpec(feature_keys=['slice_key'])])
    eval_shared_model = self.createTestEvalSharedModel(eval_saved_model_path=export_dir, add_metrics_callbacks=[_addExampleCountMetricCallback])
    extractors = [legacy_predict_extractor.PredictExtractor(eval_shared_model, eval_config=eval_config), unbatch_extractor.UnbatchExtractor(), slice_key_extractor.SliceKeyExtractor(eval_config=eval_config)]
    evaluators = [metrics_plots_and_validations_evaluator.MetricsPlotsAndValidationsEvaluator(eval_config=eval_config, eval_shared_model=eval_shared_model)]
    examples = [self._makeExample(age=3.0, language='english', label=1.0, slice_key='first_slice'), self._makeExample(age=3.0, language='chinese', label=0.0, slice_key='first_slice'), self._makeExample(age=4.0, language='english', label=0.0, slice_key='second_slice'), self._makeExample(age=5.0, language='chinese', label=1.0, slice_key='second_slice'), self._makeExample(age=5.0, language='chinese', label=1.0, slice_key='second_slice')]
    tfx_io = raw_tf_record.RawBeamRecordTFXIO(physical_format='inmemory', raw_record_column_name=constants.ARROW_INPUT_COLUMN, telemetry_descriptors=['TFMATest'])
    with beam.Pipeline() as pipeline:
        metrics = pipeline | 'Create' >> beam.Create([e.SerializeToString() for e in examples]) | 'BatchExamples' >> tfx_io.BeamSource() | 'InputsToExtracts' >> model_eval_lib.BatchedInputsToExtracts() | 'ExtractAndEvaluate' >> model_eval_lib.ExtractAndEvaluate(extractors=extractors, evaluators=evaluators)

        def check_metrics(got):
            try:
                self.assertLen(got, 3)
                slices = {}
                for (slice_key, value) in got:
                    slices[slice_key] = value
                overall_slice = ()
                first_slice = (('slice_key', 'first_slice'),)
                second_slice = (('slice_key', 'second_slice'),)
                self.assertCountEqual(list(slices.keys()), [overall_slice, first_slice, second_slice])
                self.assertDictElementsAlmostEqual(slices[overall_slice], {metric_types.MetricKey(name='accuracy', example_weighted=None): 0.4, metric_types.MetricKey(name='label/mean', example_weighted=None): 0.6, metric_types.MetricKey(name='my_mean_age', example_weighted=None): 4.0, metric_types.MetricKey(name='my_mean_age_times_label', example_weighted=None): 2.6, metric_types.MetricKey(name='added_example_count', example_weighted=None): 5.0})
                self.assertDictElementsAlmostEqual(slices[first_slice], {metric_types.MetricKey(name='accuracy', example_weighted=None): 1.0, metric_types.MetricKey(name='label/mean', example_weighted=None): 0.5, metric_types.MetricKey(name='my_mean_age', example_weighted=None): 3.0, metric_types.MetricKey(name='my_mean_age_times_label', example_weighted=None): 1.5, metric_types.MetricKey(name='added_example_count', example_weighted=None): 2.0})
                self.assertDictElementsAlmostEqual(slices[second_slice], {metric_types.MetricKey(name='accuracy', example_weighted=None): 0.0, metric_types.MetricKey(name='label/mean', example_weighted=None): 2.0 / 3.0, metric_types.MetricKey(name='my_mean_age', example_weighted=None): 14.0 / 3.0, metric_types.MetricKey(name='my_mean_age_times_label', example_weighted=None): 10.0 / 3.0, metric_types.MetricKey(name='added_example_count', example_weighted=None): 3.0})
            except AssertionError as err:
                raise util.BeamAssertException(err)
        util.assert_that(metrics[constants.METRICS_KEY], check_metrics, label='metrics')","for (slice_key, value) in got:
    slices[slice_key] = value","slices = {slice_key: value for (slice_key, value) in got}","slices = {slice_key: value for (slice_key, value) in got}",1,,,
unilm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unilm/xtune/src/pequod/data/utils_squad_evaluate.py,https://github.com/microsoft/unilm/tree/master/xtune/src/pequod/data/utils_squad_evaluate.py,,make_qid_to_has_ans$50,"def make_qid_to_has_ans(dataset):
    qid_to_has_ans = {}
    for article in dataset:
        for p in article['paragraphs']:
            for qa in p['qas']:
                qid_to_has_ans[qa['id']] = bool(qa['answers'])
    return qid_to_has_ans","for article in dataset:
    for p in article['paragraphs']:
        for qa in p['qas']:
            qid_to_has_ans[qa['id']] = bool(qa['answers'])",qid_to_has_ans = {qa['id']: bool(qa['answers']) for article in dataset for p in article['paragraphs'] for qa in p['qas']},qid_to_has_ans = {qa['id']: bool(qa['answers']) for article in dataset for p in article['paragraphs'] for qa in p['qas']},1,,,
nematus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nematus/nematus/transformer_inference.py,https://github.com/EdinburghNLP/nematus/tree/master/nematus/transformer_inference.py,ModelAdapter,generate_initial_memories$127,"def generate_initial_memories(self, batch_size, beam_size):
    with tf.compat.v1.name_scope(self._scope):
        state_size = self.config.state_size
        memories = {}
        for layer_id in range(1, self.config.transformer_dec_depth + 1):
            memories['layer_{:d}'.format(layer_id)] = {'keys': tf.tile(tf.zeros([batch_size, 0, state_size]), [beam_size, 1, 1]), 'values': tf.tile(tf.zeros([batch_size, 0, state_size]), [beam_size, 1, 1])}
        return memories","for layer_id in range(1, self.config.transformer_dec_depth + 1):
    memories['layer_{:d}'.format(layer_id)] = {'keys': tf.tile(tf.zeros([batch_size, 0, state_size]), [beam_size, 1, 1]), 'values': tf.tile(tf.zeros([batch_size, 0, state_size]), [beam_size, 1, 1])}","memories = {'layer_{:d}'.format(layer_id): {'keys': tf.tile(tf.zeros([batch_size, 0, state_size]), [beam_size, 1, 1]), 'values': tf.tile(tf.zeros([batch_size, 0, state_size]), [beam_size, 1, 1])} for layer_id in range(1, self.config.transformer_dec_depth + 1)}","memories = {'layer_{:d}'.format(layer_id): {'keys': tf.tile(tf.zeros([batch_size, 0, state_size]), [beam_size, 1, 1]), 'values': tf.tile(tf.zeros([batch_size, 0, state_size]), [beam_size, 1, 1])} for layer_id in range(1, self.config.transformer_dec_depth + 1)}",1,,,
mindmeld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mindmeld/mindmeld/active_learning/data_loading.py,https://github.com/cisco/mindmeld/tree/master/mindmeld/active_learning/data_loading.py,LabelMap,_reverse_nested_dict$108,"def _reverse_nested_dict(dictionary: Dict[str, Dict[str, int]]):
    """"""
        Returns:
            reversed_dict (dict): Reversed dictionary.
        """"""
    reversed_dict = {}
    for (parent_key, parent_value) in dictionary.items():
        reversed_dict[parent_key] = LabelMap._reverse_dict(parent_value)
    return reversed_dict","for (parent_key, parent_value) in dictionary.items():
    reversed_dict[parent_key] = LabelMap._reverse_dict(parent_value)","reversed_dict = {parent_key: LabelMap._reverse_dict(parent_value) for (parent_key, parent_value) in dictionary.items()}","reversed_dict = {parent_key: LabelMap._reverse_dict(parent_value) for (parent_key, parent_value) in dictionary.items()}",1,,,
python-elgato-streamdeck,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-elgato-streamdeck/src/example_animated.py,https://github.com/abcminiuser/python-elgato-streamdeck/tree/master/src/example_animated.py,,if_main_my$71,"if __name__ == '__main__':
    streamdecks = DeviceManager().enumerate()
    print('Found {} Stream Deck(s).\n'.format(len(streamdecks)))
    for (index, deck) in enumerate(streamdecks):
        if not deck.is_visual():
            continue
        deck.open()
        deck.reset()
        print(""Opened '{}' device (serial number: '{}')"".format(deck.deck_type(), deck.get_serial_number()))
        deck.set_brightness(30)
        print('Loading animations...')
        animations = [create_animation_frames(deck, 'Elephant_Walking_animated.gif'), create_animation_frames(deck, 'RGB_color_space_animated_view.gif'), create_animation_frames(deck, 'Simple_CV_Joint_animated.gif')]
        print('Ready.')
        key_images = dict()
        for k in range(deck.key_count()):
            key_images[k] = itertools.cycle(animations[k % len(animations)])

        def animate(fps):
            frame_time = Fraction(1, fps)
            next_frame = Fraction(time.monotonic())
            while deck.is_open():
                try:
                    with deck:
                        for (key, frames) in key_images.items():
                            deck.set_key_image(key, next(frames))
                except TransportError as err:
                    print('TransportError: {0}'.format(err))
                    break
                next_frame += frame_time
                sleep_interval = float(next_frame) - time.monotonic()
                if sleep_interval >= 0:
                    time.sleep(sleep_interval)
        threading.Thread(target=animate, args=[FRAMES_PER_SECOND]).start()
        deck.set_key_callback(key_change_callback)
        for t in threading.enumerate():
            try:
                t.join()
            except RuntimeError:
                pass","for k in range(deck.key_count()):
    key_images[k] = itertools.cycle(animations[k % len(animations)])",key_images = {k: itertools.cycle(animations[k % len(animations)]) for k in range(deck.key_count())},key_images = {k: itertools.cycle(animations[k % len(animations)]) for k in range(deck.key_count())},1,,,
zato,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zato/code/zato-common/src/zato/common/util/api.py,https://github.com/zatosource/zato/tree/master/code/zato-common/src/zato/common/util/api.py,,get_engine_url$1527,"def get_engine_url(args):
    attrs = {}
    is_sqlite = False
    is_django = 'NAME' in args
    has_get = getattr(args, 'get', False)
    odb_type = getattr(args, 'odb_type', None)
    if odb_type:
        is_sqlite = odb_type == 'sqlite'
    else:
        is_sqlite = args.get('engine') == 'sqlite' or args.get('db_type') == 'sqlite'
    names = ('engine', 'username', 'password', 'host', 'port', 'name', 'db_name', 'db_type', 'sqlite_path', 'odb_type', 'odb_user', 'odb_password', 'odb_host', 'odb_port', 'odb_db_name', 'odb_type', 'ENGINE', 'NAME', 'HOST', 'USER', 'PASSWORD', 'PORT')
    for name in names:
        if has_get:
            attrs[name] = args.get(name, '')
        else:
            attrs[name] = getattr(args, name, '')
    if is_django:
        for name in django_sa_mappings:
            value = attrs.get(name, ZATO_NOT_GIVEN)
            if value != ZATO_NOT_GIVEN:
                if not value and (name in 'db_type', 'odb_type'):
                    continue
                attrs[django_sa_mappings[name]] = value
    if not attrs.get('engine'):
        for name in cli_sa_mappings:
            value = attrs.get(name, ZATO_NOT_GIVEN)
            if value != ZATO_NOT_GIVEN:
                attrs[cli_sa_mappings[name]] = value
    if attrs['engine'] == 'sqlite':
        db_name = attrs.get('db_name')
        sqlite_path = attrs.get('sqlite_path')
        if db_name:
            attrs['sqlite_path'] = db_name
        if sqlite_path:
            attrs['db_name'] = sqlite_path
    return (engine_def_sqlite if is_sqlite else engine_def).format(**attrs)","for name in names:
    if has_get:
        attrs[name] = args.get(name, '')
    else:
        attrs[name] = getattr(args, name, '')","attrs = {name: args.get(name, '') if has_get else getattr(args, name, '') for name in names}","attrs = {name: args.get(name, '') if has_get else getattr(args, name, '') for name in names}",1,,,
ichnaea,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ichnaea/ichnaea/taskapp/task.py,https://github.com/mozilla/ichnaea/tree/master/ichnaea/taskapp/task.py,BaseTask,beat_config$45,"def beat_config(cls):
    """"""
        Returns the beat schedule for this task, taking into account
        the optional shard_model to create multiple schedule entries.
        """"""
    if cls._shard_model is None:
        return {cls.shortname(): {'task': cls.name, 'schedule': cls._schedule}}
    result = {}
    for shard_id in cls._shard_model.shards().keys():
        result[cls.shortname() + '_' + shard_id] = {'task': cls.name, 'schedule': cls._schedule, 'kwargs': {'shard_id': shard_id}}
    return result","for shard_id in cls._shard_model.shards().keys():
    result[cls.shortname() + '_' + shard_id] = {'task': cls.name, 'schedule': cls._schedule, 'kwargs': {'shard_id': shard_id}}","result = {cls.shortname() + '_' + shard_id: {'task': cls.name, 'schedule': cls._schedule, 'kwargs': {'shard_id': shard_id}} for shard_id in cls._shard_model.shards().keys()}","result = {cls.shortname() + '_' + shard_id: {'task': cls.name, 'schedule': cls._schedule, 'kwargs': {'shard_id': shard_id}} for shard_id in cls._shard_model.shards().keys()}",1,,,
jasmin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jasmin/jasmin/protocols/cli/morouterm.py,https://github.com/jookies/jasmin/tree/master/jasmin/protocols/cli/morouterm.py,,parse_args_and_call_with_instance$42,"def parse_args_and_call_with_instance(self, *args, **kwargs):
    cmd = args[0]
    arg = args[1]
    if cmd is None:
        return self.protocol.sendData()
    if cmd == 'ok':
        if len(self.sessBuffer) - 2 < len(self.protocol.sessionCompletitions):
            return self.protocol.sendData('You must set these options before saving: %s' % ', '.join(self.protocol.sessionCompletitions))
        route = {}
        for (key, value) in self.sessBuffer.items():
            if key not in ['order', 'type', 'route_class', 'route_args']:
                route[key] = value
        try:
            RouteInstance = self.sessBuffer['route_class'](**route)
            return fCallback(self, self.sessBuffer['order'], RouteInstance)
        except Exception as e:
            return self.protocol.sendData('Error: %s' % str(e))
    else:
        ra = []
        if 'route_args' in self.sessBuffer:
            ra = self.sessBuffer['route_args']
        if cmd not in MORouteKeyMap and cmd not in ra:
            return self.protocol.sendData('Unknown Route key: %s' % cmd)
        if cmd == 'type':
            _type = None
            for route in MOROUTES:
                if arg.lower() == route.lower():
                    _type = route
                    break
            if _type is None:
                return self.protocol.sendData('Unknown MO Route type:%s, available types: %s' % (arg, ', '.join(MOROUTES)))
            elif _type == 'DefaultRoute':
                self.sessBuffer['order'] = 0
            if 'order' in self.sessBuffer:
                self.sessBuffer = {'order': self.sessBuffer['order']}
            else:
                self.sessBuffer = {}
            self.sessBuffer['type'] = _type
            self.sessBuffer['route_class'] = globals()[_type]
            RouteClassArgs = inspect.getfullargspec(self.sessBuffer['route_class'].__init__).args
            if 'self' in RouteClassArgs:
                RouteClassArgs.remove('self')
            if 'rate' in RouteClassArgs:
                RouteClassArgs.remove('rate')
            self.sessBuffer['route_args'] = RouteClassArgs
            if len(RouteClassArgs) > 0:
                self.protocol.sessionCompletitions = list(MORouteKeyMap) + RouteClassArgs
                return self.protocol.sendData('%s arguments:\n%s' % (self.sessBuffer['route_class'], ', '.join(RouteClassArgs)))
        else:
            if cmd == 'order':
                if arg != '0' and 'type' in self.sessBuffer and (self.sessBuffer['type'] == 'DefaultRoute'):
                    self.sessBuffer['order'] = 0
                    return self.protocol.sendData('Route order forced to 0 since it is a DefaultRoute')
                elif arg == '0' and 'type' in self.sessBuffer and (self.sessBuffer['type'] != 'DefaultRoute'):
                    return self.protocol.sendData('This route order (0) is reserved for DefaultRoute only')
                elif not arg.isdigit() or int(arg) < 0:
                    return self.protocol.sendData('Route order must be a positive integer')
                else:
                    arg = int(arg)
            if cmd == 'connector':
                try:
                    (ctype, cid) = validate_typed_connector_id(arg)
                    if ctype == 'http':
                        if cid not in self.protocol.managers['httpccm'].httpccs:
                            raise Exception('Unknown http cid: %s' % cid)
                        arg = self.protocol.managers['httpccm'].httpccs[cid]
                    elif ctype == 'smpps':
                        arg = SmppServerSystemIdConnector(cid)
                    else:
                        raise NotImplementedError('Not implemented yet !')
                except Exception as e:
                    return self.protocol.sendData(str(e))
            if cmd == 'connectors':
                CIDs = arg.split(';')
                if len(CIDs) == 1:
                    return self.protocol.sendData('%s option value must contain a minimum of 2 connector IDs separated with "";"".' % cmd)
                arg = []
                for typed_cid in CIDs:
                    try:
                        (ctype, cid) = validate_typed_connector_id(typed_cid)
                        if ctype == 'http':
                            if cid not in self.protocol.managers['httpccm'].httpccs:
                                raise Exception('Unknown http cid: %s' % cid)
                            arg.append(self.protocol.managers['httpccm'].httpccs[cid])
                        elif ctype == 'smpps':
                            arg.append(SmppServerSystemIdConnector(cid))
                        else:
                            raise NotImplementedError('Not implemented yet !')
                    except Exception as e:
                        return self.protocol.sendData(str(e))
            if cmd == 'filters':
                FIDs = arg.split(';')
                arg = []
                for fid in FIDs:
                    if fid not in self.protocol.managers['filter'].filters:
                        return self.protocol.sendData('Unknown fid: %s' % fid)
                    else:
                        _Filter = self.protocol.managers['filter'].filters[fid]
                        if _Filter.__class__.__name__ not in MOFILTERS:
                            return self.protocol.sendData('%s#%s is not a valid filter for MORoute (not in MOFILTERS)' % (_Filter.__class__.__name__, fid))
                        else:
                            arg.append(_Filter)
            if cmd not in ra:
                RouteKey = MORouteKeyMap[cmd]
            else:
                RouteKey = cmd
            self.sessBuffer[RouteKey] = arg
        return self.protocol.sendData()","for (key, value) in self.sessBuffer.items():
    if key not in ['order', 'type', 'route_class', 'route_args']:
        route[key] = value","route = {key: value for (key, value) in self.sessBuffer.items() if key not in ['order', 'type', 'route_class', 'route_args']}","route = {key: value for (key, value) in self.sessBuffer.items() if key not in ['order', 'type', 'route_class', 'route_args']}",1,,,
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/integrals/tests/test_meijerint.py,https://github.com/sympy/sympy/tree/master/sympy/integrals/tests/test_meijerint.py,,test_lookup_table$340,"def test_lookup_table():
    from sympy.core.random import uniform, randrange
    from sympy.core.add import Add
    from sympy.integrals.meijerint import z as z_dummy
    table = {}
    _create_lookup_table(table)
    for (_, l) in sorted(table.items()):
        for (formula, terms, cond, hint) in sorted(l, key=default_sort_key):
            subs = {}
            for ai in list(formula.free_symbols) + [z_dummy]:
                if hasattr(ai, 'properties') and ai.properties:
                    subs[ai] = randrange(1, 10)
                else:
                    subs[ai] = uniform(1.5, 2.0)
            if not isinstance(terms, list):
                terms = terms(subs)
            expanded = [hyperexpand(g) for (_, g) in terms]
            assert all((x.is_Piecewise or not x.has(meijerg) for x in expanded))
            expanded = Add(*[f * x for (f, x) in terms])
            (a, b) = (formula.n(subs=subs), expanded.n(subs=subs))
            r = min(abs(a), abs(b))
            if r < 1:
                assert abs(a - b).n() <= 1e-10
            else:
                assert (abs(a - b) / r).n() <= 1e-10","for ai in list(formula.free_symbols) + [z_dummy]:
    if hasattr(ai, 'properties') and ai.properties:
        subs[ai] = randrange(1, 10)
    else:
        subs[ai] = uniform(1.5, 2.0)","subs = {ai: randrange(1, 10) if hasattr(ai, 'properties') and ai.properties else uniform(1.5, 2.0) for ai in list(formula.free_symbols) + [z_dummy]}","subs = {ai: randrange(1, 10) if hasattr(ai, 'properties') and ai.properties else uniform(1.5, 2.0) for ai in list(formula.free_symbols) + [z_dummy]}",1,,,
pywb,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pywb/pywb/warcserver/access_checker.py,https://github.com/webrecorder/pywb/tree/master/pywb/warcserver/access_checker.py,AccessChecker,create_access_aggregator$176,"def create_access_aggregator(self, source_files):
    """"""Creates a new AccessRulesAggregator using the supplied list
        of access control file names

        :param list[str] source_files: The list of access control file names
        :return: The created AccessRulesAggregator
        :rtype: AccessRulesAggregator
        """"""
    sources = {}
    for filename in source_files:
        sources[filename] = self.create_access_source(filename)
    aggregator = AccessRulesAggregator(sources)
    return aggregator","for filename in source_files:
    sources[filename] = self.create_access_source(filename)",sources = {filename: self.create_access_source(filename) for filename in source_files},sources = {filename: self.create_access_source(filename) for filename in source_files},1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/cloud/clouds/ec2.py,https://github.com/saltstack/salt/tree/master/salt/cloud/clouds/ec2.py,,list_nodes$3683,"def list_nodes(call=None):
    """"""
    Return a list of the VMs that are on the provider
    """"""
    if call == 'action':
        raise SaltCloudSystemExit('The list_nodes function must be called with -f or --function.')
    ret = {}
    nodes = list_nodes_full(get_location())
    if 'error' in nodes:
        raise SaltCloudSystemExit('An error occurred while listing nodes: {}'.format(nodes['error']['Errors']['Error']['Message']))
    for node in nodes:
        ret[node] = {'id': nodes[node]['id'], 'image': nodes[node]['image'], 'name': nodes[node]['name'], 'size': nodes[node]['size'], 'state': nodes[node]['state'], 'private_ips': nodes[node]['private_ips'], 'public_ips': nodes[node]['public_ips']}
    return ret","for node in nodes:
    ret[node] = {'id': nodes[node]['id'], 'image': nodes[node]['image'], 'name': nodes[node]['name'], 'size': nodes[node]['size'], 'state': nodes[node]['state'], 'private_ips': nodes[node]['private_ips'], 'public_ips': nodes[node]['public_ips']}","ret = {node: {'id': nodes[node]['id'], 'image': nodes[node]['image'], 'name': nodes[node]['name'], 'size': nodes[node]['size'], 'state': nodes[node]['state'], 'private_ips': nodes[node]['private_ips'], 'public_ips': nodes[node]['public_ips']} for node in nodes}","ret = {node: {'id': nodes[node]['id'], 'image': nodes[node]['image'], 'name': nodes[node]['name'], 'size': nodes[node]['size'], 'state': nodes[node]['state'], 'private_ips': nodes[node]['private_ips'], 'public_ips': nodes[node]['public_ips']} for node in nodes}",1,,,
pychess,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/perspectives/fics/PlayerListPanel.py,https://github.com/pychess/pychess/tree/master/lib/pychess/perspectives/fics/PlayerListPanel.py,Sidepanel,onPlayerAdded$102,"def onPlayerAdded(self, players, new_players):
    np = {}
    for player in new_players:
        np[player] = (player, player.getIcon(), player.name + player.display_titles(), player.blitz, player.standard, player.lightning, player.display_status, get_player_tooltip_text(player))

    def do_onPlayerAdded(players, new_players, np):
        for player in new_players:
            if player in self.players:
                continue
            if player not in np:
                continue
            self.players[player] = {}
            self.players[player]['ti'] = self.store.append(np[player])
            self.players[player]['status'] = player.connect('notify::status', self.status_changed)
            self.players[player]['game'] = player.connect('notify::game', self.status_changed)
            self.players[player]['titles'] = player.connect('notify::titles', self.titles_changed)
            if player.game:
                self.players[player]['private'] = player.game.connect('notify::private', self.private_changed, player)
            self.players[player]['ratings'] = player.connect('ratings_changed', self.elo_changed, player)
        count = len(self.players)
        self.widgets['playersOnlineLabel'].set_text(_('Players: %d') % count)
        return False
    GLib.idle_add(do_onPlayerAdded, players, new_players, np, priority=GLib.PRIORITY_LOW)","for player in new_players:
    np[player] = (player, player.getIcon(), player.name + player.display_titles(), player.blitz, player.standard, player.lightning, player.display_status, get_player_tooltip_text(player))","np = {player: (player, player.getIcon(), player.name + player.display_titles(), player.blitz, player.standard, player.lightning, player.display_status, get_player_tooltip_text(player)) for player in new_players}","np = {player: (player, player.getIcon(), player.name + player.display_titles(), player.blitz, player.standard, player.lightning, player.display_status, get_player_tooltip_text(player)) for player in new_players}",1,,,
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/qiskit/algorithms/algorithm_result.py,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/algorithms/algorithm_result.py,AlgorithmResult,__str__$25,"def __str__(self) -> str:
    result = {}
    for (name, value) in inspect.getmembers(self):
        if not name.startswith('_') and (not inspect.ismethod(value)) and (not inspect.isfunction(value)) and hasattr(self, name):
            result[name] = value
    return pprint.pformat(result, indent=4)","for (name, value) in inspect.getmembers(self):
    if not name.startswith('_') and (not inspect.ismethod(value)) and (not inspect.isfunction(value)) and hasattr(self, name):
        result[name] = value","result = {name: value for (name, value) in inspect.getmembers(self) if not name.startswith('_') and (not inspect.ismethod(value)) and (not inspect.isfunction(value)) and hasattr(self, name)}","result = {name: value for (name, value) in inspect.getmembers(self) if not name.startswith('_') and (not inspect.ismethod(value)) and (not inspect.isfunction(value)) and hasattr(self, name)}",1,,,
consensus-specs,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/consensus-specs/tests/core/pyspec/eth2spec/test/context.py,https://github.com/ethereum/consensus-specs/tree/master/tests/core/pyspec/eth2spec/test/context.py,,_run_test_case_with_phases$472,"def _run_test_case_with_phases(fn, phases, other_phases, kw, args, is_fork_transition=False):
    run_phases = _get_run_phases(phases, kw)
    if len(run_phases) == 0:
        if not is_fork_transition:
            dump_skipping_message('none of the recognized phases are executable, skipping test.')
        return None
    available_phases = _get_available_phases(run_phases, other_phases)
    targets = _get_preset_targets(kw)
    phase_dir = {}
    for phase in available_phases:
        phase_dir[phase] = targets[phase]
    for phase in run_phases:
        ret = fn(*args, spec=targets[phase], phases=phase_dir, **kw)
    return ret","for phase in available_phases:
    phase_dir[phase] = targets[phase]",phase_dir = {phase: targets[phase] for phase in available_phases},phase_dir = {phase: targets[phase] for phase in available_phases},1,,,
xmnlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xmnlp/xmnlp/summary/textrank.py,https://github.com/SeanLee97/xmnlp/tree/master/xmnlp/summary/textrank.py,KeywordTextRank,topk$66,"def topk(self, k):
    word_pr = {}
    for i in range(len(self.PR)):
        word_pr[self.idx_dict[i]] = self.PR[i][0]
    res = sorted(word_pr.items(), key=lambda x: x[1], reverse=True)
    return res[:k]","for i in range(len(self.PR)):
    word_pr[self.idx_dict[i]] = self.PR[i][0]",word_pr = {self.idx_dict[i]: self.PR[i][0] for i in range(len(self.PR))},word_pr = {self.idx_dict[i]: self.PR[i][0] for i in range(len(self.PR))},1,,,
objax,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/objax/examples/fixmatch/libml/augment/ctaugment.py,https://github.com/google/objax/tree/master/examples/fixmatch/libml/augment/ctaugment.py,CTAugment,__init__$46,"def __init__(self, depth: int=2, th: float=0.85, decay: float=0.99):
    self.decay = decay
    self.depth = depth
    self.th = th
    self.rates = {}
    for (k, op) in OPS.items():
        self.rates[k] = tuple([np.ones(x, 'f') for x in op.bins])","for (k, op) in OPS.items():
    self.rates[k] = tuple([np.ones(x, 'f') for x in op.bins])","self.rates = {k: tuple([np.ones(x, 'f') for x in op.bins]) for (k, op) in OPS.items()}","self.rates = {k: tuple([np.ones(x, 'f') for x in op.bins]) for (k, op) in OPS.items()}",1,,,
CLUEPretrainedModels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUEPretrainedModels/baselines/models/bert/tokenization_test.py,https://github.com/CLUEbenchmark/CLUEPretrainedModels/tree/master/baselines/models/bert/tokenization_test.py,TokenizationTest,test_wordpiece_tokenizer$73,"def test_wordpiece_tokenizer(self):
    vocab_tokens = ['[UNK]', '[CLS]', '[SEP]', 'want', '##want', '##ed', 'wa', 'un', 'runn', '##ing']
    vocab = {}
    for (i, token) in enumerate(vocab_tokens):
        vocab[token] = i
    tokenizer = tokenization.WordpieceTokenizer(vocab=vocab)
    self.assertAllEqual(tokenizer.tokenize(''), [])
    self.assertAllEqual(tokenizer.tokenize('unwanted running'), ['un', '##want', '##ed', 'runn', '##ing'])
    self.assertAllEqual(tokenizer.tokenize('unwantedX running'), ['[UNK]', 'runn', '##ing'])","for (i, token) in enumerate(vocab_tokens):
    vocab[token] = i","vocab = {token: i for (i, token) in enumerate(vocab_tokens)}","vocab = {token: i for (i, token) in enumerate(vocab_tokens)}",1,,,
nltk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/parse/pchart.py,https://github.com/nltk/nltk/tree/master/nltk/parse/pchart.py,BottomUpProbabilisticChartParser,parse$221,"def parse(self, tokens):
    self._grammar.check_coverage(tokens)
    chart = Chart(list(tokens))
    grammar = self._grammar
    bu_init = ProbabilisticBottomUpInitRule()
    bu = ProbabilisticBottomUpPredictRule()
    fr = SingleEdgeProbabilisticFundamentalRule()
    queue = []
    for edge in bu_init.apply(chart, grammar):
        if self._trace > 1:
            print('  %-50s [%s]' % (chart.pretty_format_edge(edge, width=2), edge.prob()))
        queue.append(edge)
    while len(queue) > 0:
        self.sort_queue(queue, chart)
        if self.beam_size:
            self._prune(queue, chart)
        edge = queue.pop()
        if self._trace > 0:
            print('  %-50s [%s]' % (chart.pretty_format_edge(edge, width=2), edge.prob()))
        queue.extend(bu.apply(chart, grammar, edge))
        queue.extend(fr.apply(chart, grammar, edge))
    parses = list(chart.parses(grammar.start(), ProbabilisticTree))
    prod_probs = {}
    for prod in grammar.productions():
        prod_probs[prod.lhs(), prod.rhs()] = prod.prob()
    for parse in parses:
        self._setprob(parse, prod_probs)
    parses.sort(reverse=True, key=lambda tree: tree.prob())
    return iter(parses)","for prod in grammar.productions():
    prod_probs[prod.lhs(), prod.rhs()] = prod.prob()","prod_probs = {(prod.lhs(), prod.rhs()): prod.prob() for prod in grammar.productions()}","prod_probs = {(prod.lhs(), prod.rhs()): prod.prob() for prod in grammar.productions()}",1,,,
kivy-designer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy-designer/designer/components/buildozer_spec_editor.py,https://github.com/kivy/kivy-designer/tree/master/designer/components/buildozer_spec_editor.py,BuildozerSpecEditor,create_json_panel$185,"def create_json_panel(self, title, config, filename=None, data=None):
    """"""Override the original method to use the custom SpecSettingsPanel
        """"""
    if filename is None and data is None:
        raise Exception('You must specify either the filename or data')
    if filename is not None:
        with open(filename, 'r', encoding='utf-8') as fd:
            data = json.loads(fd.read())
    else:
        data = json.loads(data)
    if type(data) != list:
        raise ValueError('The first element must be a list')
    panel = SpecSettingsPanel(title=title, settings=self, config=config)
    for setting in data:
        if not 'type' in setting:
            raise ValueError('One setting are missing the ""type"" element')
        ttype = setting['type']
        cls = self._types.get(ttype)
        if cls is None:
            raise ValueError('No class registered to handle the <%s> type' % setting['type'])
        del setting['type']
        str_settings = {}
        for (key, item) in setting.items():
            str_settings[str(key)] = item
        instance = cls(panel=panel, **str_settings)
        panel.add_widget(instance)
    return panel","for (key, item) in setting.items():
    str_settings[str(key)] = item","str_settings = {str(key): item for (key, item) in setting.items()}","str_settings = {str(key): item for (key, item) in setting.items()}",1,,,
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/relay/frontend/tensorflow2.py,https://github.com/apache/tvm/tree/master/python/tvm/relay/frontend/tensorflow2.py,GraphProto,_func$397,"def _func(self, graph, outputs):
    out = []
    if outputs is None:
        last_node = graph.node[-1]
        op = self._nodes[last_node.name.split(':')[0]]
        if last_node.op == 'Exit':
            out = [op[0].tuple_value]
        else:
            out = op
    else:
        for out_name in outputs:
            if ':' in out_name:
                out_name = out_name.split(':')
                (out_name, out_num) = (out_name[0], out_name[-1])
                out_num = int(out_num)
                out.append(self._nodes[out_name][out_num])
            else:
                out.append(self._nodes[out_name][0])
    if isinstance(out, _expr.TupleWrapper):
        out = out.astuple()
    else:
        out = out[0] if len(out) == 1 else _expr.Tuple(out)
    fvars = analysis.free_vars(out)
    func = _function.Function(fvars, out)
    final_params = {}
    for fv in fvars:
        if fv.name_hint in self._params:
            final_params[fv.name_hint] = self._params[fv.name_hint]
    self._params = final_params
    return func","for fv in fvars:
    if fv.name_hint in self._params:
        final_params[fv.name_hint] = self._params[fv.name_hint]",final_params = {fv.name_hint: self._params[fv.name_hint] for fv in fvars if fv.name_hint in self._params},final_params = {fv.name_hint: self._params[fv.name_hint] for fv in fvars if fv.name_hint in self._params},1,,,
fava,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fava/src/fava/core/charts.py,https://github.com/beancount/fava/tree/master/src/fava/core/charts.py,ChartModule,interval_totals$149,"def interval_totals(self, filtered: FilteredLedger, interval: Interval, accounts: str | tuple[str], conversion: str, invert: bool=False) -> Generator[DateAndBalanceWithBudget, None, None]:
    """"""Renders totals for account (or accounts) in the intervals.

        Args:
            interval: An interval.
            accounts: A single account (str) or a tuple of accounts.
            conversion: The conversion to use.
            invert: invert all numbers.
        """"""
    price_map = self.ledger.price_map
    for (begin, end) in pairwise(filtered.interval_ends(interval)):
        inventory = Inventory()
        entries = iter_entry_dates(filtered.entries, begin, end)
        account_inventories = {}
        for entry in (e for e in entries if isinstance(e, Transaction)):
            for posting in entry.postings:
                if posting.account.startswith(accounts):
                    if posting.account not in account_inventories:
                        account_inventories[posting.account] = Inventory()
                    account_inventories[posting.account].add_position(posting)
                    inventory.add_position(posting)
        balance = cost_or_value(inventory, conversion, price_map, end - ONE_DAY)
        account_balances = {}
        for (account, acct_value) in account_inventories.items():
            account_balances[account] = cost_or_value(acct_value, conversion, price_map, end - ONE_DAY)
        budgets = {}
        if isinstance(accounts, str):
            budgets = self.ledger.budgets.calculate_children(accounts, begin, end)
        if invert:
            balance = -balance
            budgets = {k: -v for (k, v) in budgets.items()}
            account_balances = {k: -v for (k, v) in account_balances.items()}
        yield DateAndBalanceWithBudget(begin, balance, account_balances, budgets)","for (account, acct_value) in account_inventories.items():
    account_balances[account] = cost_or_value(acct_value, conversion, price_map, end - ONE_DAY)","account_balances = {account: cost_or_value(acct_value, conversion, price_map, end - ONE_DAY) for (account, acct_value) in account_inventories.items()}","account_balances = {account: cost_or_value(acct_value, conversion, price_map, end - ONE_DAY) for (account, acct_value) in account_inventories.items()}",1,,,
pygame_tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygame_tutorials/tilemap/part 19/main.py,https://github.com/kidscancode/pygame_tutorials/tree/master/tilemap/part 19/main.py,Game,load_data$63,"def load_data(self):
    game_folder = path.dirname(__file__)
    img_folder = path.join(game_folder, 'img')
    snd_folder = path.join(game_folder, 'snd')
    music_folder = path.join(game_folder, 'music')
    map_folder = path.join(game_folder, 'maps')
    self.title_font = path.join(img_folder, 'ZOMBIE.TTF')
    self.dim_screen = pg.Surface(self.screen.get_size()).convert_alpha()
    self.dim_screen.fill((0, 0, 0, 180))
    self.map = TiledMap(path.join(map_folder, 'level1.tmx'))
    self.map_img = self.map.make_map()
    self.map.rect = self.map_img.get_rect()
    self.player_img = pg.image.load(path.join(img_folder, PLAYER_IMG)).convert_alpha()
    self.bullet_img = pg.image.load(path.join(img_folder, BULLET_IMG)).convert_alpha()
    self.mob_img = pg.image.load(path.join(img_folder, MOB_IMG)).convert_alpha()
    self.wall_img = pg.image.load(path.join(img_folder, WALL_IMG)).convert_alpha()
    self.wall_img = pg.transform.scale(self.wall_img, (TILESIZE, TILESIZE))
    self.splat = pg.image.load(path.join(img_folder, SPLAT)).convert_alpha()
    self.splat = pg.transform.scale(self.splat, (64, 64))
    self.gun_flashes = []
    for img in MUZZLE_FLASHES:
        self.gun_flashes.append(pg.image.load(path.join(img_folder, img)).convert_alpha())
    self.item_images = {}
    for item in ITEM_IMAGES:
        self.item_images[item] = pg.image.load(path.join(img_folder, ITEM_IMAGES[item])).convert_alpha()
    pg.mixer.music.load(path.join(music_folder, BG_MUSIC))
    self.effects_sounds = {}
    for type in EFFECTS_SOUNDS:
        self.effects_sounds[type] = pg.mixer.Sound(path.join(snd_folder, EFFECTS_SOUNDS[type]))
    self.weapon_sounds = {}
    self.weapon_sounds['gun'] = []
    for snd in WEAPON_SOUNDS_GUN:
        self.weapon_sounds['gun'].append(pg.mixer.Sound(path.join(snd_folder, snd)))
    self.zombie_moan_sounds = []
    for snd in ZOMBIE_MOAN_SOUNDS:
        s = pg.mixer.Sound(path.join(snd_folder, snd))
        s.set_volume(0.2)
        self.zombie_moan_sounds.append(s)
    self.player_hit_sounds = []
    for snd in PLAYER_HIT_SOUNDS:
        self.player_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))
    self.zombie_hit_sounds = []
    for snd in ZOMBIE_HIT_SOUNDS:
        self.zombie_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))","for item in ITEM_IMAGES:
    self.item_images[item] = pg.image.load(path.join(img_folder, ITEM_IMAGES[item])).convert_alpha()","self.item_images = {item: pg.image.load(path.join(img_folder, ITEM_IMAGES[item])).convert_alpha() for item in ITEM_IMAGES}","self.item_images = {item: pg.image.load(path.join(img_folder, ITEM_IMAGES[item])).convert_alpha() for item in ITEM_IMAGES}",1,,,
pygame_tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygame_tutorials/tilemap/part 19/main.py,https://github.com/kidscancode/pygame_tutorials/tree/master/tilemap/part 19/main.py,Game,load_data$63,"def load_data(self):
    game_folder = path.dirname(__file__)
    img_folder = path.join(game_folder, 'img')
    snd_folder = path.join(game_folder, 'snd')
    music_folder = path.join(game_folder, 'music')
    map_folder = path.join(game_folder, 'maps')
    self.title_font = path.join(img_folder, 'ZOMBIE.TTF')
    self.dim_screen = pg.Surface(self.screen.get_size()).convert_alpha()
    self.dim_screen.fill((0, 0, 0, 180))
    self.map = TiledMap(path.join(map_folder, 'level1.tmx'))
    self.map_img = self.map.make_map()
    self.map.rect = self.map_img.get_rect()
    self.player_img = pg.image.load(path.join(img_folder, PLAYER_IMG)).convert_alpha()
    self.bullet_img = pg.image.load(path.join(img_folder, BULLET_IMG)).convert_alpha()
    self.mob_img = pg.image.load(path.join(img_folder, MOB_IMG)).convert_alpha()
    self.wall_img = pg.image.load(path.join(img_folder, WALL_IMG)).convert_alpha()
    self.wall_img = pg.transform.scale(self.wall_img, (TILESIZE, TILESIZE))
    self.splat = pg.image.load(path.join(img_folder, SPLAT)).convert_alpha()
    self.splat = pg.transform.scale(self.splat, (64, 64))
    self.gun_flashes = []
    for img in MUZZLE_FLASHES:
        self.gun_flashes.append(pg.image.load(path.join(img_folder, img)).convert_alpha())
    self.item_images = {}
    for item in ITEM_IMAGES:
        self.item_images[item] = pg.image.load(path.join(img_folder, ITEM_IMAGES[item])).convert_alpha()
    pg.mixer.music.load(path.join(music_folder, BG_MUSIC))
    self.effects_sounds = {}
    for type in EFFECTS_SOUNDS:
        self.effects_sounds[type] = pg.mixer.Sound(path.join(snd_folder, EFFECTS_SOUNDS[type]))
    self.weapon_sounds = {}
    self.weapon_sounds['gun'] = []
    for snd in WEAPON_SOUNDS_GUN:
        self.weapon_sounds['gun'].append(pg.mixer.Sound(path.join(snd_folder, snd)))
    self.zombie_moan_sounds = []
    for snd in ZOMBIE_MOAN_SOUNDS:
        s = pg.mixer.Sound(path.join(snd_folder, snd))
        s.set_volume(0.2)
        self.zombie_moan_sounds.append(s)
    self.player_hit_sounds = []
    for snd in PLAYER_HIT_SOUNDS:
        self.player_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))
    self.zombie_hit_sounds = []
    for snd in ZOMBIE_HIT_SOUNDS:
        self.zombie_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))","for type in EFFECTS_SOUNDS:
    self.effects_sounds[type] = pg.mixer.Sound(path.join(snd_folder, EFFECTS_SOUNDS[type]))","self.effects_sounds = {type: pg.mixer.Sound(path.join(snd_folder, EFFECTS_SOUNDS[type])) for type in EFFECTS_SOUNDS}","self.effects_sounds = {type: pg.mixer.Sound(path.join(snd_folder, EFFECTS_SOUNDS[type])) for type in EFFECTS_SOUNDS}",1,,,
oncall,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oncall/src/oncall/user_sync/ldap_sync.py,https://github.com/linkedin/oncall/tree/master/src/oncall/user_sync/ldap_sync.py,,get_modes$209,"def get_modes(engine):
    engine.execute('SELECT `name`, `id` FROM `contact_mode`')
    modes = {}
    for row in engine.fetchall():
        modes[row['name']] = row['id']
    return modes","for row in engine.fetchall():
    modes[row['name']] = row['id']",modes = {row['name']: row['id'] for row in engine.fetchall()},modes = {row['name']: row['id'] for row in engine.fetchall()},1,,,
MACHIN3tools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MACHIN3tools/utils/modifier.py,https://github.com/machin3io/MACHIN3tools/tree/master/utils/modifier.py,,get_mods_as_dict$89,"def get_mods_as_dict(obj, types=[], skip_show_expanded=False):
    mods = []
    for mod in obj.modifiers:
        if types:
            if mod.type in types:
                mods.append(mod)
        else:
            mods.append(mod)
    modsdict = {}
    for mod in mods:
        modsdict[mod.name] = get_mod_as_dict(mod, skip_show_expanded=skip_show_expanded)
    return modsdict","for mod in mods:
    modsdict[mod.name] = get_mod_as_dict(mod, skip_show_expanded=skip_show_expanded)","modsdict = {mod.name: get_mod_as_dict(mod, skip_show_expanded=skip_show_expanded) for mod in mods}","modsdict = {mod.name: get_mod_as_dict(mod, skip_show_expanded=skip_show_expanded) for mod in mods}",1,,,
sfepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sfepy/sfepy/discrete/problem.py,https://github.com/sfepy/sfepy/tree/master/sfepy/discrete/problem.py,Problem,__init__$244,"def __init__(self, name, conf=None, functions=None, domain=None, fields=None, equations=None, auto_conf=True, active_only=True):
    self.active_only = active_only
    self.name = name
    self.conf = conf
    self.functions = functions
    self.reset()
    self.ls_conf = self.nls_conf = self.ts_conf = None
    self.conf_variables = self.conf_materials = None
    if auto_conf:
        if equations is None:
            raise ValueError('missing equations in auto_conf mode!')
        if fields is None:
            variables = equations.variables
            fields = {}
            for field in [var.get_field() for var in variables]:
                fields[field.name] = field
        if domain is None:
            domain = list(fields.values())[0].domain
        if conf is None:
            self.conf = Struct(options={}, ics={}, ebcs={}, epbcs={}, lcbcs={}, materials={})
    self.equations = equations
    self.fields = fields
    self.domain = domain
    if auto_conf:
        self.set_ics(self.conf.ics)
    self.setup_output()","for field in [var.get_field() for var in variables]:
    fields[field.name] = field",fields = {field.name: field for field in [var.get_field() for var in variables]},fields = {field.name: field for field in [var.get_field() for var in variables]},1,,,
ReAgent,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ReAgent/serving/scripts/rasp_to_model.py,https://github.com/facebookresearch/ReAgent/tree/master/serving/scripts/rasp_to_model.py,,keys_to_int$14,"def keys_to_int(d: Dict[str, Any]) -> Dict[int, Any]:
    new_d = {}
    for k in d:
        new_d[int(k)] = d[k]
    return new_d","for k in d:
    new_d[int(k)] = d[k]",new_d = {int(k): d[k] for k in d},new_d = {int(k): d[k] for k in d},1,,,
pyspider,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyspider/pyspider/libs/base_handler.py,https://github.com/binux/pyspider/tree/master/pyspider/libs/base_handler.py,BaseHandler,_crawl$255,"def _crawl(self, url, **kwargs):
    """"""
        real crawl API

        checking kwargs, and repack them to each sub-dict
        """"""
    task = {}
    assert len(url) < 1024, 'Maximum (1024) URL length error.'
    if kwargs.get('callback'):
        callback = kwargs['callback']
        if isinstance(callback, six.string_types) and hasattr(self, callback):
            func = getattr(self, callback)
        elif six.callable(callback) and six.get_method_self(callback) is self:
            func = callback
            kwargs['callback'] = func.__name__
        elif six.callable(callback) and hasattr(self, callback.__name__):
            func = getattr(self, callback.__name__)
            kwargs['callback'] = func.__name__
        else:
            raise NotImplementedError('self.%s() not implemented!' % callback)
        if hasattr(func, '_config'):
            for (k, v) in iteritems(func._config):
                if isinstance(v, dict) and isinstance(kwargs.get(k), dict):
                    kwargs[k].update(v)
                else:
                    kwargs.setdefault(k, v)
    url = quote_chinese(_build_url(url.strip(), kwargs.pop('params', None)))
    if kwargs.get('files'):
        assert isinstance(kwargs.get('data', {}), dict), 'data must be a dict when using with files!'
        (content_type, data) = _encode_multipart_formdata(kwargs.pop('data', {}), kwargs.pop('files', {}))
        kwargs.setdefault('headers', {})
        kwargs['headers']['Content-Type'] = content_type
        kwargs['data'] = data
    if kwargs.get('data'):
        kwargs['data'] = _encode_params(kwargs['data'])
    if kwargs.get('data'):
        kwargs.setdefault('method', 'POST')
    if kwargs.get('user_agent'):
        kwargs.setdefault('headers', {})
        kwargs['headers']['User-Agent'] = kwargs.get('user_agent')
    schedule = {}
    for key in self.schedule_fields:
        if key in kwargs:
            schedule[key] = kwargs.pop(key)
        elif key in self.crawl_config:
            schedule[key] = self.crawl_config[key]
    task['schedule'] = schedule
    fetch = {}
    for key in self.fetch_fields:
        if key in kwargs:
            fetch[key] = kwargs.pop(key)
    task['fetch'] = fetch
    process = {}
    for key in self.process_fields:
        if key in kwargs:
            process[key] = kwargs.pop(key)
    task['process'] = process
    task['project'] = self.project_name
    task['url'] = url
    if 'taskid' in kwargs:
        task['taskid'] = kwargs.pop('taskid')
    else:
        task['taskid'] = self.get_taskid(task)
    if kwargs:
        raise TypeError('crawl() got unexpected keyword argument: %s' % kwargs.keys())
    if self.is_debugger():
        task = self.task_join_crawl_config(task, self.crawl_config)
    cache_key = '%(project)s:%(taskid)s' % task
    if cache_key not in self._follows_keys:
        self._follows_keys.add(cache_key)
        self._follows.append(task)
    return task","for key in self.schedule_fields:
    if key in kwargs:
        schedule[key] = kwargs.pop(key)
    elif key in self.crawl_config:
        schedule[key] = self.crawl_config[key]",schedule = {key: kwargs.pop(key) if key in kwargs else self.crawl_config[key] for key in self.schedule_fields if key in kwargs or key in self.crawl_config},schedule = {key: kwargs.pop(key) if key in kwargs else self.crawl_config[key] for key in self.schedule_fields if key in kwargs or key in self.crawl_config},1,,,
ReAgent,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ReAgent/reagent/preprocessing/transforms.py,https://github.com/facebookresearch/ReAgent/tree/master/reagent/preprocessing/transforms.py,,_build_id_2_hashing$148,"def _build_id_2_hashing(keys: List[str], feature_configs: List[List[rlt.BaseDataClass]], id_mapping_configs: List[Dict[str, rlt.IdMappingConfig]]):
    """"""Sparse feature id -> hashing boolean in corresponding id_mapping_config""""""
    id_2_hashing = {}
    for (key, feature_config, id_mapping_config) in zip(keys, feature_configs, id_mapping_configs):
        id_2_hashing[key] = {config.feature_id: id_mapping_config[config.id_mapping_name].hashing for config in feature_config}
    return id_2_hashing","for (key, feature_config, id_mapping_config) in zip(keys, feature_configs, id_mapping_configs):
    id_2_hashing[key] = {config.feature_id: id_mapping_config[config.id_mapping_name].hashing for config in feature_config}","id_2_hashing = {key: {config.feature_id: id_mapping_config[config.id_mapping_name].hashing for config in feature_config} for (key, feature_config, id_mapping_config) in zip(keys, feature_configs, id_mapping_configs)}","id_2_hashing = {key: {config.feature_id: id_mapping_config[config.id_mapping_name].hashing for config in feature_config} for (key, feature_config, id_mapping_config) in zip(keys, feature_configs, id_mapping_configs)}",1,,,
aio-pika,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aio-pika/aio_pika/connection.py,https://github.com/mosquito/aio-pika/tree/master/aio_pika/connection.py,Connection,_parse_kwargs$36,"def _parse_kwargs(cls, kwargs):
    result = {}
    for (key, parser, default) in cls.KWARGS_TYPES:
        result[key] = parser(kwargs.get(key, default))
    return result","for (key, parser, default) in cls.KWARGS_TYPES:
    result[key] = parser(kwargs.get(key, default))","result = {key: parser(kwargs.get(key, default)) for (key, parser, default) in cls.KWARGS_TYPES}","result = {key: parser(kwargs.get(key, default)) for (key, parser, default) in cls.KWARGS_TYPES}",1,,,
ros_comm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros_comm/test/test_rosmaster/test/client_verification/test_slave_api.py,https://github.com/ros/ros_comm/tree/master/test/test_rosmaster/test/client_verification/test_slave_api.py,TopicDescriptionList,as_dict$78,"def as_dict(self):
    d = {}
    for t in self.topics:
        d[t.topic_name] = t.topic_type
    return d","for t in self.topics:
    d[t.topic_name] = t.topic_type",d = {t.topic_name: t.topic_type for t in self.topics},d = {t.topic_name: t.topic_type for t in self.topics},1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/proxy/cimc.py,https://github.com/saltstack/salt/tree/master/salt/proxy/cimc.py,,get_config_resolver_class$184,"def get_config_resolver_class(cid=None, hierarchical=False):
    """"""
    The configResolveClass method returns requested managed object in a given class.
    """"""
    ret = {}
    cookie = logon()
    h = 'false'
    if hierarchical is True:
        h = 'true'
    payload = '<configResolveClass cookie=""{}"" inHierarchical=""{}"" classId=""{}""/>'.format(cookie, h, cid)
    r = __utils__['http.query'](DETAILS['url'], data=payload, method='POST', decode_type='plain', decode=True, verify_ssl=DETAILS['verify_ssl'], raise_error=True, status=True, headers=DETAILS['headers'])
    _validate_response_code(r['status'], cookie)
    answer = re.findall('(<[\\s\\S.]*>)', r['text'])[0]
    items = ET.fromstring(answer)
    logout(cookie)
    for item in items:
        ret[item.tag] = prepare_return(item)
    return ret","for item in items:
    ret[item.tag] = prepare_return(item)",ret = {item.tag: prepare_return(item) for item in items},ret = {item.tag: prepare_return(item) for item in items},1,,,
zvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/zvt/utils/utils.py,https://github.com/zvtvz/zvt/tree/master/zvt/utils/utils.py,,fill_domain_from_dict$78,"def fill_domain_from_dict(the_domain, the_dict: dict, the_map: dict=None, default_func=lambda x: x):
    """"""
    use field map and related func to fill properties from the dict to the domain


    :param the_domain:
    :type the_domain: DeclarativeMeta
    :param the_dict:
    :type the_dict: dict
    :param the_map:
    :type the_map: dict
    :param default_func:
    :type default_func: function
    """"""
    if not the_map:
        the_map = {}
        for k in the_dict:
            the_map[k] = (k, default_func)
    for (k, v) in the_map.items():
        if isinstance(v, tuple):
            field_in_dict = v[0]
            the_func = v[1]
        else:
            field_in_dict = v
            the_func = default_func
        the_value = the_dict.get(field_in_dict)
        if the_value is not None:
            to_value = the_value
            if to_value in none_values:
                setattr(the_domain, k, None)
            else:
                result_value = the_func(to_value)
                setattr(the_domain, k, result_value)
                exec('the_domain.{}=result_value'.format(k))","for k in the_dict:
    the_map[k] = (k, default_func)","the_map = {k: (k, default_func) for k in the_dict}","the_map = {k: (k, default_func) for k in the_dict}",1,,,
biobert,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/biobert/run_ner.py,https://github.com/dmis-lab/biobert/tree/master//run_ner.py,,convert_single_example$213,"def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, mode):
    label_map = {}
    for (i, label) in enumerate(label_list):
        label_map[label] = i
    with open(os.path.join(FLAGS.output_dir, 'label2id.pkl'), 'wb') as w:
        pickle.dump(label_map, w)
    textlist = example.text.split()
    labellist = example.label.split()
    tokens = []
    labels = []
    for (i, word) in enumerate(textlist):
        token = tokenizer.tokenize(word)
        tokens.extend(token)
        label_1 = labellist[i]
        for (m, tok) in enumerate(token):
            if m == 0:
                labels.append(label_1)
            else:
                labels.append('X')
    if len(tokens) >= max_seq_length - 1:
        tokens = tokens[0:max_seq_length - 2]
        labels = labels[0:max_seq_length - 2]
    ntokens = []
    segment_ids = []
    label_ids = []
    ntokens.append('[CLS]')
    segment_ids.append(0)
    label_ids.append(label_map['[CLS]'])
    for (i, token) in enumerate(tokens):
        ntokens.append(token)
        segment_ids.append(0)
        label_ids.append(label_map[labels[i]])
    ntokens.append('[SEP]')
    segment_ids.append(0)
    label_ids.append(label_map['[SEP]'])
    input_ids = tokenizer.convert_tokens_to_ids(ntokens)
    input_mask = [1] * len(input_ids)
    while len(input_ids) < max_seq_length:
        input_ids.append(0)
        input_mask.append(0)
        segment_ids.append(0)
        label_ids.append(0)
        ntokens.append('[PAD]')
    assert len(input_ids) == max_seq_length
    assert len(input_mask) == max_seq_length
    assert len(segment_ids) == max_seq_length
    assert len(label_ids) == max_seq_length
    if ex_index < 4:
        tf.logging.info('*** Example ***')
        tf.logging.info('guid: %s' % example.guid)
        tf.logging.info('tokens: %s' % ' '.join([tokenization.printable_text(x) for x in tokens]))
        tf.logging.info('input_ids: %s' % ' '.join([str(x) for x in input_ids]))
        tf.logging.info('input_mask: %s' % ' '.join([str(x) for x in input_mask]))
        tf.logging.info('segment_ids: %s' % ' '.join([str(x) for x in segment_ids]))
        tf.logging.info('label_ids: %s' % ' '.join([str(x) for x in label_ids]))
    feature = InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_ids=label_ids)
    write_tokens(ntokens, mode)
    return feature","for (i, label) in enumerate(label_list):
    label_map[label] = i","label_map = {label: i for (i, label) in enumerate(label_list)}","label_map = {label: i for (i, label) in enumerate(label_list)}",1,,,
maro,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/maro/maro/cli/data_pipeline/vm_scheduling.py,https://github.com/microsoft/maro/tree/master/maro/cli/data_pipeline/vm_scheduling.py,VmSchedulingPipeline,_filter_out_vmid$280,"def _filter_out_vmid(self, vm_table: pd.DataFrame, vm_id_map: dict) -> dict:
    new_id_map = {}
    for (key, value) in vm_id_map.items():
        if value in vm_table.vmid.values:
            new_id_map[key] = value
    return new_id_map","for (key, value) in vm_id_map.items():
    if value in vm_table.vmid.values:
        new_id_map[key] = value","new_id_map = {key: value for (key, value) in vm_id_map.items() if value in vm_table.vmid.values}","new_id_map = {key: value for (key, value) in vm_id_map.items() if value in vm_table.vmid.values}",1,,,
transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transformers/examples/research_projects/pplm/run_pplm_discrim_train.py,https://github.com/huggingface/transformers/tree/master/examples/research_projects/pplm/run_pplm_discrim_train.py,,collate_fn$101,"def collate_fn(data):

    def pad_sequences(sequences):
        lengths = [len(seq) for seq in sequences]
        padded_sequences = torch.zeros(len(sequences), max(lengths)).long()
        for (i, seq) in enumerate(sequences):
            end = lengths[i]
            padded_sequences[i, :end] = seq[:end]
        return (padded_sequences, lengths)
    item_info = {}
    for key in data[0].keys():
        item_info[key] = [d[key] for d in data]
    (x_batch, _) = pad_sequences(item_info['X'])
    y_batch = torch.tensor(item_info['y'], dtype=torch.long)
    return (x_batch, y_batch)","for key in data[0].keys():
    item_info[key] = [d[key] for d in data]",item_info = {key: [d[key] for d in data] for key in data[0].keys()},item_info = {key: [d[key] for d in data] for key in data[0].keys()},1,,,
fedlearner,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fedlearner/web_console_v2/api/fedlearner_webconsole/project/add_on.py,https://github.com/bytedance/fedlearner/tree/master/web_console_v2/api/fedlearner_webconsole/project/add_on.py,,parse_certificates$31,"def parse_certificates(encoded_gz):
    """"""
    Parse certificates from base64-encoded string to a dict
    Args:
        encoded_gz: A base64-encoded string from a `.gz` file.
    Returns:
        dict: key is the file name, value is the content
    """"""
    binary_gz = io.BytesIO(b64decode(encoded_gz))
    with tarfile.open(fileobj=binary_gz) as gz:
        certificates = {}
        for file in gz.getmembers():
            if file.isfile():
                certificates[file.name.split('/', 1)[-1]] = str(b64encode(gz.extractfile(file).read()), encoding='utf-8')
    return certificates","for file in gz.getmembers():
    if file.isfile():
        certificates[file.name.split('/', 1)[-1]] = str(b64encode(gz.extractfile(file).read()), encoding='utf-8')","certificates = {file.name.split('/', 1)[-1]: str(b64encode(gz.extractfile(file).read()), encoding='utf-8') for file in gz.getmembers() if file.isfile()}","certificates = {file.name.split('/', 1)[-1]: str(b64encode(gz.extractfile(file).read()), encoding='utf-8') for file in gz.getmembers() if file.isfile()}",1,,,
SiCKRAGE,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SiCKRAGE/sickrage/libs/rtorrentlib/torrent.py,https://github.com/SiCKRAGE/SiCKRAGE/tree/master/sickrage/libs/rtorrentlib/torrent.py,Torrent,get_files$118,"def get_files(self):
    """"""Get list of File instances for given torrent.

        @return: L{File} instances
        @rtype: list

        @note: also assigns return value to self.files
        """"""
    self.files = []
    retriever_methods = [m for m in rtorrentlib.file.methods if m.is_retriever() and m.is_available(self._rt_obj)]
    m = rtorrentlib.rpc.Multicall(self)
    m.add('f.multicall', self.info_hash, '', *[method.rpc_call + '=' for method in retriever_methods])
    results = m.call()[0]
    offset_method_index = retriever_methods.index(rtorrentlib.rpc.find_method('f.offset'))
    offset_list = sorted([r[offset_method_index] for r in results])
    for result in results:
        results_dict = {}
        for (m, r) in zip(retriever_methods, result):
            results_dict[m.varname] = rtorrentlib.rpc.process_result(m, r)
        f_index = offset_list.index(results_dict['offset'])
        self.files.append(File(self._rt_obj, self.info_hash, f_index, **results_dict))
    return self.files","for (m, r) in zip(retriever_methods, result):
    results_dict[m.varname] = rtorrentlib.rpc.process_result(m, r)","results_dict = {m.varname: rtorrentlib.rpc.process_result(m, r) for (m, r) in zip(retriever_methods, result)}","results_dict = {m.varname: rtorrentlib.rpc.process_result(m, r) for (m, r) in zip(retriever_methods, result)}",1,,,
nucypher,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nucypher/examples/heartbeat_demo/doctor_keys.py,https://github.com/nucypher/nucypher/tree/master/examples/heartbeat_demo/doctor_keys.py,,_get_keys$49,"def _get_keys(file, key_class):
    if not file.exists():
        generate_doctor_keys()
    with open(file) as f:
        stored_keys = json.load(f)
    keys = dict()
    for (key_type, key_str) in stored_keys.items():
        keys[key_type] = key_class.from_bytes(bytes.fromhex(key_str))
    return keys","for (key_type, key_str) in stored_keys.items():
    keys[key_type] = key_class.from_bytes(bytes.fromhex(key_str))","keys = {key_type: key_class.from_bytes(bytes.fromhex(key_str)) for (key_type, key_str) in stored_keys.items()}","keys = {key_type: key_class.from_bytes(bytes.fromhex(key_str)) for (key_type, key_str) in stored_keys.items()}",1,,,
BERT-flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-flow/tokenization_test.py,https://github.com/bohanli/BERT-flow/tree/master//tokenization_test.py,TokenizationTest,test_convert_tokens_to_ids$93,"def test_convert_tokens_to_ids(self):
    vocab_tokens = ['[UNK]', '[CLS]', '[SEP]', 'want', '##want', '##ed', 'wa', 'un', 'runn', '##ing']
    vocab = {}
    for (i, token) in enumerate(vocab_tokens):
        vocab[token] = i
    self.assertAllEqual(tokenization.convert_tokens_to_ids(vocab, ['un', '##want', '##ed', 'runn', '##ing']), [7, 4, 5, 8, 9])","for (i, token) in enumerate(vocab_tokens):
    vocab[token] = i","vocab = {token: i for (i, token) in enumerate(vocab_tokens)}","vocab = {token: i for (i, token) in enumerate(vocab_tokens)}",1,,,
nni,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nni/examples/trials/benchmarking/automlbenchmark/parse_result_csv.py,https://github.com/microsoft/nni/tree/master/examples/trials/benchmarking/automlbenchmark/parse_result_csv.py,,generate_perf_report$10,"def generate_perf_report(result_file_name):
    """"""
    Generate a performance report. 
    The input result_file_name should be the path of the ""results.csv"" generated by automlbenchmark.
    This function outputs 1) a formatted report named ""performances.txt"" in the ""reports/"" directory 
    located in the same parent directory as ""results.csv"" and 2) a report named ""rankings.txt"" in the
    same directory ranking the tuners contained in ""results.csv"". 
    """"""
    result = pd.read_csv(result_file_name)
    task_ids = result['id'].unique()
    tuners = result['framework'].unique()
    metric_types = ['rmse', 'auc', 'logloss']
    metric2taskres = {}
    for m in metric_types:
        metric2taskres[m] = []
    keep_parameters = ['framework', 'constraint', 'result', 'metric', 'params', 'utc', 'duration'] + list(result.columns[16:])
    with open(result_file_name.replace('results.csv', 'reports/performances.txt'), 'w') as out_f:
        for task_id in task_ids:
            task_results = result[result['id'] == task_id]
            task_name = task_results.task.unique()[0]
            out_f.write('====================================================\n')
            out_f.write('Task ID: {}\n'.format(task_id))
            out_f.write('Task Name: {}\n'.format(task_name))
            folds = task_results['fold'].unique()
            for fold in folds:
                out_f.write('Fold {}:\n'.format(fold))
                res = task_results[task_results['fold'] == fold][keep_parameters]
                out_f.write(res.to_string())
                out_f.write('\n')
                res_list = []
                for (_, row) in res.iterrows():
                    res_list.append([row['framework'], row['result']])
                metric2taskres[res['metric'].unique()[0]].append(res_list)
            out_f.write('\n')
    with open(result_file_name.replace('results.csv', 'reports/rankings.txt'), 'w') as out_f:
        ranking_aggs = {}
        for metric_type in metric_types:
            sorted_lists = []
            if metric_type in ['auc']:
                for l in metric2taskres[metric_type]:
                    l_sorted = sorted(l, key=lambda x: x[-1], reverse=True)
                    l_sorted = [[x[0], x[1], i + 1] for (i, x) in enumerate(l_sorted)]
                    sorted_lists.append(l_sorted)
            elif metric_type in ['rmse', 'logloss']:
                for l in metric2taskres[metric_type]:
                    l_sorted = sorted(l, key=lambda x: x[-1])
                    l_sorted = [[x[0], x[1], i + 1] for (i, x) in enumerate(l_sorted)]
                    sorted_lists.append(l_sorted)
            metric2taskres[metric_type] = sorted_lists
            out_f.write('====================================================\n')
            out_f.write('Average rankings for metric {}:\n'.format(metric_type))
            ranking_agg = [[t, 0] for t in tuners]
            for (i, tuner) in enumerate(tuners):
                for trial_res in metric2taskres[metric_type]:
                    for (t, s, r) in trial_res:
                        if t == tuner:
                            ranking_agg[i][-1] += r
            ranking_agg = [[x[0], x[1] / float(len(metric2taskres[metric_type]))] for x in ranking_agg]
            ranking_agg = sorted(ranking_agg, key=lambda x: x[-1])
            for (t, r) in ranking_agg:
                out_f.write('{:<12} {:.2f}\n'.format(t, r))
            ranking_aggs[metric_type] = ranking_agg
            out_f.write('\n')
        out_f.write('====================================================\n')
        out_f.write('Average rankings for tuners:\n')
        header_string = '{:<12}'
        for _ in metric_types:
            header_string += ' {:<12}'
        header_string += '\n'
        out_f.write(header_string.format('Tuner', *metric_types))
        for tuner in tuners:
            tuner_ranks = []
            for m in metric_types:
                for (t, r) in ranking_aggs[m]:
                    if t == tuner:
                        tuner_ranks.append('{:.2f}'.format(r))
                        break
            out_f.write(header_string.format(tuner, *tuner_ranks))
        out_f.write('\n')","for m in metric_types:
    metric2taskres[m] = []",metric2taskres = {m: [] for m in metric_types},metric2taskres = {m: [] for m in metric_types},1,,,
maltrail,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/maltrail/trails/feeds/360bigviktor.py,https://github.com/stamparm/maltrail/tree/master/trails/feeds/360bigviktor.py,,fetch$17,"def fetch():
    retval = {}
    content = retrieve_content(__url__)
    if __check__ in content:
        for match in re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content):
            retval[match.group(1)] = (__info__, __reference__)
    return retval","for match in re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content):
    retval[match.group(1)] = (__info__, __reference__)","retval = {match.group(1): (__info__, __reference__) for match in re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content)}","retval = {match.group(1): (__info__, __reference__) for match in re.finditer('(?m)^([\\w.-]+)\\s+2\\d{3}\\-', content)}",1,,,
toil,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/toil/src/toil/lib/ec2.py,https://github.com/DataBiosphere/toil/tree/master/src/toil/lib/ec2.py,,prune$272,"def prune(bushy: dict) -> dict:
    """"""
    Prune entries in the given dict with false-y values.
    Boto3 may not like None and instead wants no key.
    """"""
    pruned = dict()
    for key in bushy:
        if bushy[key]:
            pruned[key] = bushy[key]
    return pruned","for key in bushy:
    if bushy[key]:
        pruned[key] = bushy[key]",pruned = {key: bushy[key] for key in bushy if bushy[key]},pruned = {key: bushy[key] for key in bushy if bushy[key]},1,,,
networkx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/networkx/networkx/algorithms/centrality/voterank_alg.py,https://github.com/networkx/networkx/tree/master/networkx/algorithms/centrality/voterank_alg.py,,voterank$6,"def voterank(G, number_of_nodes=None):
    """"""Select a list of influential nodes in a graph using VoteRank algorithm

    VoteRank [1]_ computes a ranking of the nodes in a graph G based on a
    voting scheme. With VoteRank, all nodes vote for each of its in-neighbours
    and the node with the highest votes is elected iteratively. The voting
    ability of out-neighbors of elected nodes is decreased in subsequent turns.

    Note: We treat each edge independently in case of multigraphs.

    Parameters
    ----------
    G : graph
        A NetworkX graph.

    number_of_nodes : integer, optional
        Number of ranked nodes to extract (default all nodes).

    Returns
    -------
    voterank : list
        Ordered list of computed seeds.
        Only nodes with positive number of votes are returned.

    References
    ----------
    .. [1] Zhang, J.-X. et al. (2016).
        Identifying a set of influential spreaders in complex networks.
        Sci. Rep. 6, 27823; doi: 10.1038/srep27823.
    """"""
    influential_nodes = []
    voterank = {}
    if len(G) == 0:
        return influential_nodes
    if number_of_nodes is None or number_of_nodes > len(G):
        number_of_nodes = len(G)
    if G.is_directed():
        avgDegree = sum((deg for (_, deg) in G.out_degree())) / len(G)
    else:
        avgDegree = sum((deg for (_, deg) in G.degree())) / len(G)
    for n in G.nodes():
        voterank[n] = [0, 1]
    for _ in range(number_of_nodes):
        for n in G.nodes():
            voterank[n][0] = 0
        for (n, nbr) in G.edges():
            voterank[n][0] += voterank[nbr][1]
            if not G.is_directed():
                voterank[nbr][0] += voterank[n][1]
        for n in influential_nodes:
            voterank[n][0] = 0
        n = max(G.nodes, key=lambda x: voterank[x][0])
        if voterank[n][0] == 0:
            return influential_nodes
        influential_nodes.append(n)
        voterank[n] = [0, 0]
        for (_, nbr) in G.edges(n):
            voterank[nbr][1] -= 1 / avgDegree
            voterank[nbr][1] = max(voterank[nbr][1], 0)
    return influential_nodes","for n in G.nodes():
    voterank[n] = [0, 1]","voterank = {n: [0, 1] for n in G.nodes()}","voterank = {n: [0, 1] for n in G.nodes()}",1,,,
opt_einsum,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/opt_einsum/opt_einsum/tests/test_paths.py,https://github.com/dgasmith/opt_einsum/tree/master/opt_einsum/tests/test_paths.py,,test_size_by_dict$85,"def test_size_by_dict():
    sizes_dict = {}
    for (ind, val) in zip('abcdez', [2, 5, 9, 11, 13, 0]):
        sizes_dict[ind] = val
    path_func = oe.helpers.compute_size_by_dict
    assert 1 == path_func('', sizes_dict)
    assert 2 == path_func('a', sizes_dict)
    assert 5 == path_func('b', sizes_dict)
    assert 0 == path_func('z', sizes_dict)
    assert 0 == path_func('az', sizes_dict)
    assert 0 == path_func('zbc', sizes_dict)
    assert 104 == path_func('aaae', sizes_dict)
    assert 12870 == path_func('abcde', sizes_dict)","for (ind, val) in zip('abcdez', [2, 5, 9, 11, 13, 0]):
    sizes_dict[ind] = val","sizes_dict = {ind: val for (ind, val) in zip('abcdez', [2, 5, 9, 11, 13, 0])}","sizes_dict = {ind: val for (ind, val) in zip('abcdez', [2, 5, 9, 11, 13, 0])}",1,,,
fonttools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/ttLib/tables/_c_m_a_p.py,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/ttLib/tables/_c_m_a_p.py,cmap_format_12_or_13,compile$946,"def compile(self, ttFont):
    if self.data:
        return struct.pack('>HHLLL', self.format, self.reserved, self.length, self.language, self.nGroups) + self.data
    charCodes = list(self.cmap.keys())
    names = list(self.cmap.values())
    nameMap = ttFont.getReverseGlyphMap()
    try:
        gids = [nameMap[name] for name in names]
    except KeyError:
        nameMap = ttFont.getReverseGlyphMap(rebuild=True)
        try:
            gids = [nameMap[name] for name in names]
        except KeyError:
            gids = []
            for name in names:
                try:
                    gid = nameMap[name]
                except KeyError:
                    try:
                        if name[:3] == 'gid':
                            gid = int(name[3:])
                        else:
                            gid = ttFont.getGlyphID(name)
                    except:
                        raise KeyError(name)
                gids.append(gid)
    cmap = {}
    for (code, gid) in zip(charCodes, gids):
        cmap[code] = gid
    charCodes.sort()
    index = 0
    startCharCode = charCodes[0]
    startGlyphID = cmap[startCharCode]
    lastGlyphID = startGlyphID - self._format_step
    lastCharCode = startCharCode - 1
    nGroups = 0
    dataList = []
    maxIndex = len(charCodes)
    for index in range(maxIndex):
        charCode = charCodes[index]
        glyphID = cmap[charCode]
        if not self._IsInSameRun(glyphID, lastGlyphID, charCode, lastCharCode):
            dataList.append(struct.pack('>LLL', startCharCode, lastCharCode, startGlyphID))
            startCharCode = charCode
            startGlyphID = glyphID
            nGroups = nGroups + 1
        lastGlyphID = glyphID
        lastCharCode = charCode
    dataList.append(struct.pack('>LLL', startCharCode, lastCharCode, startGlyphID))
    nGroups = nGroups + 1
    data = bytesjoin(dataList)
    lengthSubtable = len(data) + 16
    assert len(data) == nGroups * 12 == lengthSubtable - 16
    return struct.pack('>HHLLL', self.format, self.reserved, lengthSubtable, self.language, nGroups) + data","for (code, gid) in zip(charCodes, gids):
    cmap[code] = gid","cmap = {code: gid for (code, gid) in zip(charCodes, gids)}","cmap = {code: gid for (code, gid) in zip(charCodes, gids)}",1,,,
Kats,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Kats/kats/utils/multi_objective_hpt.py,https://github.com/facebookresearch/Kats/tree/master/kats/utils/multi_objective_hpt.py,HPT_Problem,__init__$42,"def __init__(self, search_grid: TimeSeriesParameterTuning, data_df: pd.DataFrame, objectives_and_goals: Dict[str, str]):
    self._validate_objectives_and_goals(objectives_and_goals)
    self.objectives_and_goals = objectives_and_goals
    self.objectives = list(objectives_and_goals.keys())
    tunable_parameters = search_grid.get_search_space().tunable_parameters
    self.par_to_val = {}
    for par in tunable_parameters:
        self.par_to_val[par] = tunable_parameters[par].values
    self.tunable_parameters = list(tunable_parameters.keys())
    (self.lower_limits, self.upper_limits) = self.get_upper_and_lower_limits()
    self.n_vars = len(tunable_parameters)
    self.all_solutions = {}
    self.data_df = data_df
    super().__init__(n_var=self.n_vars, n_obj=len(self.objectives), n_constr=0, xl=np.array(self.lower_limits), xu=np.array(self.upper_limits), type_var=int, elementwise_evaluation=True)
    self.turing_model = changepoint_evaluator.TuringEvaluator(is_detector_model=True, detector=cusum_model.CUSUMDetectorModel)","for par in tunable_parameters:
    self.par_to_val[par] = tunable_parameters[par].values",self.par_to_val = {par: tunable_parameters[par].values for par in tunable_parameters},self.par_to_val = {par: tunable_parameters[par].values for par in tunable_parameters},1,,,
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/translate.py,https://github.com/frappe/frappe/tree/master/frappe/translate.py,,update_translations$1076,"def update_translations(lang, untranslated_file, translated_file, app='_ALL_APPS'):
    """"""Update translations from a source and target file for a given language.

	:param lang: Language code (e.g. `en`).
	:param untranslated_file: File path with the messages in English.
	:param translated_file: File path with messages in language to be updated.""""""
    clear_cache()
    full_dict = get_all_translations(lang)

    def restore_newlines(s):
        return s.replace('|||||', '\\\n').replace('| | | | |', '\\\n').replace('||||', '\\n').replace('| | | |', '\\n').replace('|||', '\n').replace('| | |', '\n')
    translation_dict = {}
    for (key, value) in zip(frappe.get_file_items(untranslated_file, ignore_empty_lines=False), frappe.get_file_items(translated_file, ignore_empty_lines=False)):
        translation_dict[restore_newlines(key)] = restore_newlines(value)
    full_dict.update(translation_dict)
    apps = frappe.get_all_apps(True)
    if app != '_ALL_APPS':
        if app not in apps:
            print(f'Application {app} not found!')
            return
        apps = [app]
    for app_name in apps:
        write_translations_file(app_name, lang, full_dict)","for (key, value) in zip(frappe.get_file_items(untranslated_file, ignore_empty_lines=False), frappe.get_file_items(translated_file, ignore_empty_lines=False)):
    translation_dict[restore_newlines(key)] = restore_newlines(value)","translation_dict = {restore_newlines(key): restore_newlines(value) for (key, value) in zip(frappe.get_file_items(untranslated_file, ignore_empty_lines=False), frappe.get_file_items(translated_file, ignore_empty_lines=False))}","translation_dict = {restore_newlines(key): restore_newlines(value) for (key, value) in zip(frappe.get_file_items(untranslated_file, ignore_empty_lines=False), frappe.get_file_items(translated_file, ignore_empty_lines=False))}",1,,,
Deep-Reinforcement-Learning-Hands-On,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Deep-Reinforcement-Learning-Hands-On/Chapter18/telegram-bot.py,https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/tree/master/Chapter18/telegram-bot.py,PlayerBot,_read_models$92,"def _read_models(self, models_dir):
    result = {}
    for (idx, name) in enumerate(sorted(glob.glob(os.path.join(models_dir, '*.dat')))):
        result[idx] = name
    return result","for (idx, name) in enumerate(sorted(glob.glob(os.path.join(models_dir, '*.dat')))):
    result[idx] = name","result = {idx: name for (idx, name) in enumerate(sorted(glob.glob(os.path.join(models_dir, '*.dat'))))}","result = {idx: name for (idx, name) in enumerate(sorted(glob.glob(os.path.join(models_dir, '*.dat'))))}",1,,,
CLUEPretrainedModels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUEPretrainedModels/baselines/models/roberta_wwm_large_ext/run_ner.py,https://github.com/CLUEbenchmark/CLUEPretrainedModels/tree/master/baselines/models/roberta_wwm_large_ext/run_ner.py,,convert_single_example$356,"def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, output_dir, mode):
    label_map = {}
    for (i, label) in enumerate(label_list, 1):
        label_map[label] = i
    if not os.path.exists(os.path.join(output_dir, 'label2id.pkl')):
        with open(os.path.join(output_dir, 'label2id.pkl'), 'wb') as w:
            pickle.dump(label_map, w)
    textlist = example.text.split(' ')
    labellist = example.label.split(' ')
    tokens = []
    labels = []
    label_mask = []
    for (i, word) in enumerate(textlist):
        token = tokenizer.tokenize(word)
        tokens.extend(token)
        label_1 = labellist[i]
        for m in range(len(token)):
            if m == 0:
                labels.append(label_1)
            else:
                labels.append('X')
    if len(tokens) >= max_seq_length - 1:
        tokens = tokens[0:max_seq_length - 2]
        labels = labels[0:max_seq_length - 2]
    ntokens = []
    segment_ids = []
    label_ids = []
    ntokens.append('[CLS]')
    segment_ids.append(0)
    label_ids.append(label_map['[CLS]'])
    label_mask.append(0)
    for (i, token) in enumerate(tokens):
        ntokens.append(token)
        segment_ids.append(0)
        label_ids.append(label_map[labels[i]])
        if labels[i] == 'X':
            label_mask.append(0)
        else:
            label_mask.append(1)
    ntokens.append('[SEP]')
    segment_ids.append(0)
    label_mask.append(0)
    label_ids.append(label_map['[SEP]'])
    input_ids = tokenizer.convert_tokens_to_ids(ntokens)
    input_mask = [1] * len(input_ids)
    while len(input_ids) < max_seq_length:
        input_ids.append(0)
        input_mask.append(0)
        segment_ids.append(0)
        label_ids.append(0)
        ntokens.append('**NULL**')
        label_mask.append(0)
    assert len(input_ids) == max_seq_length
    assert len(input_mask) == max_seq_length
    assert len(segment_ids) == max_seq_length
    assert len(label_ids) == max_seq_length
    assert len(label_mask) == max_seq_length
    if ex_index < 5:
        tf.logging.info('*** Example ***')
        tf.logging.info('guid: %s' % example.guid)
        tf.logging.info('tokens: %s' % ' '.join([tokenization.printable_text(x) for x in tokens]))
        tf.logging.info('input_ids: %s' % ' '.join([str(x) for x in input_ids]))
        tf.logging.info('input_mask: %s' % ' '.join([str(x) for x in input_mask]))
        tf.logging.info('segment_ids: %s' % ' '.join([str(x) for x in segment_ids]))
        tf.logging.info('label_ids: %s' % ' '.join([str(x) for x in label_ids]))
        tf.logging.info('label_mask: %s' % ' '.join([str(x) for x in label_mask]))
    feature = InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_ids=label_ids, label_mask=label_mask)
    write_tokens(ntokens, mode)
    return feature","for (i, label) in enumerate(label_list, 1):
    label_map[label] = i","label_map = {label: i for (i, label) in enumerate(label_list, 1)}","label_map = {label: i for (i, label) in enumerate(label_list, 1)}",1,,,
jasmin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jasmin/jasmin/protocols/cli/groupsm.py,https://github.com/jookies/jasmin/tree/master/jasmin/protocols/cli/groupsm.py,,parse_args_and_call_with_instance$12,"def parse_args_and_call_with_instance(self, *args, **kwargs):
    cmd = args[0]
    arg = args[1]
    if cmd is None:
        return self.protocol.sendData()
    if cmd == 'ok':
        if len(self.sessBuffer) != 1:
            return self.protocol.sendData('You must set Group id (gid) before saving !')
        group = {}
        for (key, value) in self.sessBuffer.items():
            group[key] = value
        try:
            GroupInstance = Group(**group)
            return fCallback(self, GroupInstance)
        except Exception as e:
            return self.protocol.sendData('Error: %s' % str(e))
    else:
        if cmd not in GroupKeyMap:
            return self.protocol.sendData('Unknown Group key: %s' % cmd)
        GroupKey = GroupKeyMap[cmd]
        self.sessBuffer[GroupKey] = arg
        return self.protocol.sendData()","for (key, value) in self.sessBuffer.items():
    group[key] = value","group = {key: value for (key, value) in self.sessBuffer.items()}","group = {key: value for (key, value) in self.sessBuffer.items()}",1,,,
integrations-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/vsphere/tests/legacy/test_mor_cache.py,https://github.com/DataDog/integrations-core/tree/master/vsphere/tests/legacy/test_mor_cache.py,,test_purge$169,"def test_purge(cache):
    cache._mor['foo_instance'] = {}
    for i in range(3):
        cache._mor['foo_instance'][i] = {'creation_time': 0}
    cache._mor['foo_instance']['hero'] = {'creation_time': time.time()}
    cache.purge('foo_instance', 60)
    assert len(cache._mor['foo_instance']) == 1
    assert 'hero' in cache._mor['foo_instance']","for i in range(3):
    cache._mor['foo_instance'][i] = {'creation_time': 0}",cache._mor['foo_instance'] = {i: {'creation_time': 0} for i in range(3)},cache._mor['foo_instance'] = {i: {'creation_time': 0} for i in range(3)},1,,,
fiftyone,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fiftyone/tests/unittests/similarity_tests.py,https://github.com/voxel51/fiftyone/tree/master/tests/unittests/similarity_tests.py,SimilarityTests,test_object_similarity$64,"def test_object_similarity(self):
    dataset = fo.Dataset()
    dataset.add_samples([fo.Sample(filepath='image1.png', ground_truth=fo.Detections(detections=[fo.Detection(label='cat'), fo.Detection(label='dog'), fo.Detection(label='rabbit'), fo.Detection(label='squirrel')])), fo.Sample(filepath='image2.png', ground_truth=fo.Detections(detections=[fo.Detection(label='cat'), fo.Detection(label='dog')]))])
    embeddings = {}
    for sample in dataset:
        embeddings[sample.id] = np.random.randn(len(sample.ground_truth.detections), 4)
    fob.compute_similarity(dataset, patches_field='ground_truth', embeddings=embeddings, brain_key='object_similarity')
    query_id = dataset.first().ground_truth.detections[0].id
    view = dataset.sort_by_similarity(query_id, k=3, brain_key='object_similarity')
    self.assertEqual(view.count('ground_truth.detections'), 3)
    patches = dataset.to_patches('ground_truth')
    view1 = patches.sort_by_similarity(query_id)
    view2 = patches.sort_by_similarity(query_id, reverse=True)
    self.assertEqual(view1.values('id'), list(reversed(view2.values('id'))))
    view3 = patches.sort_by_similarity(query_id, k=4)
    self.assertEqual(len(view3), 4)
    view4 = patches.sort_by_similarity(query_id, brain_key='object_similarity')
    self.assertEqual(view1.values('id'), view4.values('id'))","for sample in dataset:
    embeddings[sample.id] = np.random.randn(len(sample.ground_truth.detections), 4)","embeddings = {sample.id: np.random.randn(len(sample.ground_truth.detections), 4) for sample in dataset}","embeddings = {sample.id: np.random.randn(len(sample.ground_truth.detections), 4) for sample in dataset}",1,,,
autogluon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/autogluon/features/src/autogluon/features/generators/memory_minimize.py,https://github.com/awslabs/autogluon/tree/master/features/src/autogluon/features/generators/memory_minimize.py,CategoryMemoryMinimizeFeatureGenerator,_minimize_categorical_memory_usage$40,"def _minimize_categorical_memory_usage(self, X: DataFrame):
    if self._category_maps:
        X_renamed = dict()
        for column in self._category_maps:
            X_renamed[column] = X[column].cat.rename_categories(self._category_maps[column])
        X = DataFrame(X_renamed)
    return X","for column in self._category_maps:
    X_renamed[column] = X[column].cat.rename_categories(self._category_maps[column])",X_renamed = {column: X[column].cat.rename_categories(self._category_maps[column]) for column in self._category_maps},X_renamed = {column: X[column].cat.rename_categories(self._category_maps[column]) for column in self._category_maps},1,,,
fedlearner,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fedlearner/test/trainer/test_data_visitor.py,https://github.com/bytedance/fedlearner/tree/master/test/trainer/test_data_visitor.py,TestLeaderDataVisitor,test_dump_restore$67,"def test_dump_restore(self):
    epoch_num = 5
    output = {}
    for i in range(epoch_num):
        output[i + 1] = set()
    visitor = _DataVisitor(self._datablocks, epoch_num, shuffle=True)
    for i in range(len(self._datablocks) * 3 + 2):
        b = next(visitor)
        output[b.epoch].add(b.id)
    buff = visitor.dump()
    data = visitor._try_parse_v2(buff)
    print(data)
    visitor2 = _DataVisitor(self._datablocks, epoch_num)
    visitor2.restore(buff)
    try:
        while True:
            b = next(visitor2)
            output[b.epoch].add(b.id)
    except StopIteration:
        pass
    for i in range(epoch_num):
        for (j, id) in enumerate(sorted(output[i + 1])):
            assert self._datablocks[j].id == id","for i in range(epoch_num):
    output[i + 1] = set()",output = {i + 1: set() for i in range(epoch_num)},output = {i + 1: set() for i in range(epoch_num)},1,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/parser/cwl.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/parser/cwl.py,CwlToolSource,parse_outputs$115,"def parse_outputs(self, tool):
    output_instances = self.tool_proxy.output_instances()
    outputs = {}
    output_defs = []
    for output_instance in output_instances:
        output_defs.append(self._parse_output(tool, output_instance))
    for output_def in output_defs:
        outputs[output_def.name] = output_def
    return (outputs, {})","for output_def in output_defs:
    outputs[output_def.name] = output_def",outputs = {output_def.name: output_def for output_def in output_defs},outputs = {output_def.name: output_def for output_def in output_defs},1,,,
blender_mmd_tools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/blender_mmd_tools/mmd_tools/core/vmd/importer.py,https://github.com/sugiany/blender_mmd_tools/tree/master/mmd_tools/core/vmd/importer.py,VMDImporter,__assignToMesh$155,"def __assignToMesh(self, meshObj, action_name=None):
    if action_name is not None:
        act = bpy.data.actions.new(name=action_name)
        a = meshObj.data.shape_keys.animation_data_create()
        a.action = act
    shapeKeyAnim = self.__vmdFile.shapeKeyAnimation
    shapeKeyDict = {}
    for i in meshObj.data.shape_keys.key_blocks:
        shapeKeyDict[i.name] = i
    for (name, keyFrames) in shapeKeyAnim.items():
        if name not in shapeKeyDict:
            print('WARNING: not found shape key %s' % str(name))
            continue
        shapeKey = shapeKeyDict[name]
        for i in keyFrames:
            shapeKey.value = i.weight
            shapeKey.keyframe_insert(data_path='value', group=name, frame=i.frame_number + self.__frame_margin)","for i in meshObj.data.shape_keys.key_blocks:
    shapeKeyDict[i.name] = i",shapeKeyDict = {i.name: i for i in meshObj.data.shape_keys.key_blocks},shapeKeyDict = {i.name: i for i in meshObj.data.shape_keys.key_blocks},1,,,
onnx-tensorflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/onnx-tensorflow/example/train_onnx_model.py,https://github.com/onnx/onnx-tensorflow/tree/master/example/train_onnx_model.py,,save_trained_onnx$41,"def save_trained_onnx(tensor_dict, onnx_model, sess):
    print('Update onnx model....')
    retrained_params = {}
    for (name, tensor) in tensor_dict.items():
        if isinstance(tensor, tf.Variable):
            retrained_params[name] = sess.run(tensor)
    for tensor in onnx_model.graph.initializer:
        if tensor.name in retrained_params:
            print('Updating {}.'.format(tensor.name))
            assert tensor.HasField('raw_data')
            tensor.raw_data = retrained_params[tensor.name].tobytes()
    onnx.save(onnx_model, trained_onnx_model)
    print('Save trained onnx model {}'.format(trained_onnx_model))","for (name, tensor) in tensor_dict.items():
    if isinstance(tensor, tf.Variable):
        retrained_params[name] = sess.run(tensor)","retrained_params = {name: sess.run(tensor) for (name, tensor) in tensor_dict.items() if isinstance(tensor, tf.Variable)}","retrained_params = {name: sess.run(tensor) for (name, tensor) in tensor_dict.items() if isinstance(tensor, tf.Variable)}",1,,,
powerline,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/powerline/tests/modules/vim.py,https://github.com/powerline/powerline/tree/master/tests/modules/vim.py,,_init$633,"def _init():
    global _dict
    if _dict:
        return _dict
    _dict = {}
    for (varname, value) in globals().items():
        if varname[0] != '_':
            _dict[varname] = value
    _tabnew()
    return _dict","for (varname, value) in globals().items():
    if varname[0] != '_':
        _dict[varname] = value","_dict = {varname: value for (varname, value) in globals().items() if varname[0] != '_'}","_dict = {varname: value for (varname, value) in globals().items() if varname[0] != '_'}",1,,,
ludwig,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ludwig/ludwig/combiners/combiners.py,https://github.com/ludwig-ai/ludwig/tree/master/ludwig/combiners/combiners.py,Combiner,output_shape$96,"def output_shape(self) -> torch.Size:
    pseudo_input = {}
    for k in self.input_features:
        pseudo_input[k] = {'encoder_output': torch.rand(2, *self.input_features[k].output_shape, dtype=self.input_dtype)}
    output_tensor = self.forward(pseudo_input)
    return output_tensor['combiner_output'].size()[1:]","for k in self.input_features:
    pseudo_input[k] = {'encoder_output': torch.rand(2, *self.input_features[k].output_shape, dtype=self.input_dtype)}","pseudo_input = {k: {'encoder_output': torch.rand(2, *self.input_features[k].output_shape, dtype=self.input_dtype)} for k in self.input_features}","pseudo_input = {k: {'encoder_output': torch.rand(2, *self.input_features[k].output_shape, dtype=self.input_dtype)} for k in self.input_features}",1,,,
django-treebeard,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-treebeard/treebeard/forms.py,https://github.com/django-treebeard/django-treebeard/tree/master/treebeard/forms.py,MoveNodeForm,save$137,"def save(self, commit=True):
    (position_type, reference_node_id) = self._clean_cleaned_data()
    if self.instance._state.adding:
        cl_data = {}
        for field in self.cleaned_data:
            if not isinstance(self.cleaned_data[field], (list, QuerySet)):
                cl_data[field] = self.cleaned_data[field]
        if reference_node_id:
            reference_node = self._meta.model.objects.get(pk=reference_node_id)
            self.instance = reference_node.add_child(**cl_data)
            self.instance.move(reference_node, pos=position_type)
        else:
            self.instance = self._meta.model.add_root(**cl_data)
    else:
        self.instance.save()
        if reference_node_id:
            reference_node = self._meta.model.objects.get(pk=reference_node_id)
            self.instance.move(reference_node, pos=position_type)
        else:
            if self.is_sorted:
                pos = 'sorted-sibling'
            else:
                pos = 'first-sibling'
            self.instance.move(self._meta.model.get_first_root_node(), pos)
    self.instance.refresh_from_db()
    super().save(commit=commit)
    return self.instance","for field in self.cleaned_data:
    if not isinstance(self.cleaned_data[field], (list, QuerySet)):
        cl_data[field] = self.cleaned_data[field]","cl_data = {field: self.cleaned_data[field] for field in self.cleaned_data if not isinstance(self.cleaned_data[field], (list, QuerySet))}","cl_data = {field: self.cleaned_data[field] for field in self.cleaned_data if not isinstance(self.cleaned_data[field], (list, QuerySet))}",1,,,
learning-tools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/learning-tools/ansible/kubeadm-etcd-template/inventory/ec2.py,https://github.com/scottslowe/learning-tools/tree/master/ansible/kubeadm-etcd-template/inventory/ec2.py,Ec2Inventory,get_instances_by_region$571,"def get_instances_by_region(self, region):
    """""" Makes an AWS EC2 API call to the list of instances in a particular
        region """"""
    try:
        conn = self.connect(region)
        reservations = []
        if self.ec2_instance_filters:
            if self.stack_filters:
                filters_dict = {}
                for (filter_key, filter_values) in self.ec2_instance_filters.items():
                    filters_dict[filter_key] = filter_values
                reservations.extend(conn.get_all_instances(filters=filters_dict))
            else:
                for (filter_key, filter_values) in self.ec2_instance_filters.items():
                    reservations.extend(conn.get_all_instances(filters={filter_key: filter_values}))
        else:
            reservations = conn.get_all_instances()
        instance_ids = []
        for reservation in reservations:
            instance_ids.extend([instance.id for instance in reservation.instances])
        max_filter_value = 199
        tags = []
        for i in range(0, len(instance_ids), max_filter_value):
            tags.extend(conn.get_all_tags(filters={'resource-type': 'instance', 'resource-id': instance_ids[i:i + max_filter_value]}))
        tags_by_instance_id = defaultdict(dict)
        for tag in tags:
            tags_by_instance_id[tag.res_id][tag.name] = tag.value
        if not self.aws_account_id and reservations:
            self.aws_account_id = reservations[0].owner_id
        for reservation in reservations:
            for instance in reservation.instances:
                instance.tags = tags_by_instance_id[instance.id]
                self.add_instance(instance, region)
    except boto.exception.BotoServerError as e:
        if e.error_code == 'AuthFailure':
            error = self.get_auth_error_message()
        else:
            backend = 'Eucalyptus' if self.eucalyptus else 'AWS'
            error = 'Error connecting to %s backend.\n%s' % (backend, e.message)
        self.fail_with_error(error, 'getting EC2 instances')","for (filter_key, filter_values) in self.ec2_instance_filters.items():
    filters_dict[filter_key] = filter_values","filters_dict = {filter_key: filter_values for (filter_key, filter_values) in self.ec2_instance_filters.items()}","filters_dict = {filter_key: filter_values for (filter_key, filter_values) in self.ec2_instance_filters.items()}",1,,,
facenet-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/facenet-pytorch/models/utils/tensorflow2pytorch.py,https://github.com/timesler/facenet-pytorch/tree/master/models/utils/tensorflow2pytorch.py,,get_layer_indices$48,"def get_layer_indices(layer_lookup, tf_layers):
    """"""Giving a lookup of model layer attribute names and tensorflow variable names,
    find matching parameters.
    
    Arguments:
        layer_lookup {dict} -- Dictionary mapping pytorch attribute names to (partial)
            tensorflow variable names. Expects dict of the form {'attr': ['tf_name', ...]}
            where the '...'s are ignored.
        tf_layers {list} -- List of tensorflow variable names.
    
    Returns:
        list -- The input dictionary with the list of matching inds appended to each item.
    """"""
    layer_inds = {}
    for (name, value) in layer_lookup.items():
        layer_inds[name] = value + [[i for (i, n) in enumerate(tf_layers) if value[0] in n]]
    return layer_inds","for (name, value) in layer_lookup.items():
    layer_inds[name] = value + [[i for (i, n) in enumerate(tf_layers) if value[0] in n]]","layer_inds = {name: value + [[i for (i, n) in enumerate(tf_layers) if value[0] in n]] for (name, value) in layer_lookup.items()}","layer_inds = {name: value + [[i for (i, n) in enumerate(tf_layers) if value[0] in n]] for (name, value) in layer_lookup.items()}",1,,,
sparrow-wifi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sparrow-wifi/sparrowbluetooth.py,https://github.com/ghostop14/sparrow-wifi/tree/master//sparrowbluetooth.py,SparrowBluetooth,__init__$549,"def __init__(self):
    self.spectrum = {}
    for i in range(2402, 2495):
        self.spectrum[i] = -100
    self.spectrumLock = Lock()
    self.deviceLock = Lock()
    self.spectrumScanThread = None
    self.blueHydraProc = None
    self.btmonThread = None
    self.devices = {}
    self.scanType = SparrowBluetooth.SCANTYPE_BLUEHYDRA
    self.beaconActive = False
    self.hasBluetooth = False
    self.hasUbertooth = False
    self.hasBlueHydra = False
    numBtAdapters = len(SparrowBluetooth.getBluetoothInterfaces())
    if numBtAdapters > 0:
        self.hasBluetooth = True
    if SparrowBluetooth.getNumUbertoothDevices() > 0:
        (errcode, errmsg) = SparrowBluetooth.hasUbertoothTools()
        if errcode == 0:
            self.hasUbertooth = True
    if os.path.isfile('/opt/bluetooth/blue_hydra/bin/blue_hydra'):
        self.hasBlueHydra = True","for i in range(2402, 2495):
    self.spectrum[i] = -100","self.spectrum = {i: -100 for i in range(2402, 2495)}","self.spectrum = {i: -100 for i in range(2402, 2495)}",1,,,
jasmin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jasmin/jasmin/protocols/cli/usersm.py,https://github.com/jookies/jasmin/tree/master/jasmin/protocols/cli/usersm.py,,UserBuild$123,"def UserBuild(fCallback):
    """"""Parse args and try to build a jasmin.routing.jasminApi.User instance to pass it to fCallback""""""

    def parse_args_and_call_with_instance(self, *args, **kwargs):
        cmd = args[0]
        arg = args[1]
        if cmd is None:
            return self.protocol.sendData()
        if cmd == 'ok':
            if 'uid' not in self.sessBuffer or 'group' not in self.sessBuffer or 'username' not in self.sessBuffer or ('password' not in self.sessBuffer):
                return self.protocol.sendData('You must set User id (uid), group (gid), username and password before saving !')
            if 'mt_credential' not in self.sessBuffer:
                self.sessBuffer[UserKeyMap['mt_messaging_cred']['keyMapValue']] = globals()[UserKeyMap['mt_messaging_cred']['class']]()
            if 'smpps_credential' not in self.sessBuffer:
                self.sessBuffer[UserKeyMap['smpps_cred']['keyMapValue']] = globals()[UserKeyMap['smpps_cred']['class']]()
            user = {}
            for (key, value) in self.sessBuffer.items():
                user[key] = value
            try:
                UserInstance = User(**user)
                return fCallback(self, UserInstance)
            except Exception as e:
                return self.protocol.sendData('Error: %s' % str(e))
        else:
            if cmd not in UserKeyMap:
                return self.protocol.sendData('Unknown User key: %s' % cmd)
            if isinstance(UserKeyMap[cmd], dict):
                subKeyMap = UserKeyMap[cmd]
                _r = re.match('^(\\S+) (\\S+) (\\S+.*$)', arg)
                if not _r:
                    return self.protocol.sendData('Error: expected syntax: %s section key value' % cmd)
                section = _r.group(1).lower()
                key = _r.group(2).lower()
                value = _r.group(3)
                possible_values = list(subKeyMap)
                possible_values.remove('class')
                possible_values.remove('keyMapValue')
                valid_section = False
                for pv in possible_values:
                    if section == pv.lower():
                        section = pv
                        valid_section = True
                        break
                if not valid_section:
                    return self.protocol.sendData('Error: invalid section name: %s, possible values: %s' % (section, ', '.join(possible_values)))
                if key not in list(subKeyMap[section]):
                    return self.protocol.sendData('Error: invalid key: %s, possible keys: %s' % (key, ', '.join(list(subKeyMap[section]))))
                SectionKey = subKeyMap[section][key]
                try:
                    SectionValue = castToBuiltCorrectCredType(subKeyMap['class'], section, SectionKey, value)
                    if subKeyMap['keyMapValue'] not in self.sessBuffer:
                        self.sessBuffer[subKeyMap['keyMapValue']] = globals()[subKeyMap['class']]()
                    getattr(self.sessBuffer[subKeyMap['keyMapValue']], 'set%s' % section)(SectionKey, SectionValue)
                except (jasminApiCredentialError, ValueError) as e:
                    return self.protocol.sendData('Error: %s' % str(e))
            elif cmd == 'gid':
                group = self.pb['router'].getGroup(arg)
                if group is None:
                    return self.protocol.sendData('Unknown Group gid:%s, you must first create the Group' % arg)
                self.sessBuffer['group'] = group
            else:
                UserKey = UserKeyMap[cmd]
                if UserKey not in UserConfigStringKeys:
                    self.sessBuffer[UserKey] = str2num(arg)
                else:
                    self.sessBuffer[UserKey] = arg
            return self.protocol.sendData()
    return parse_args_and_call_with_instance","for (key, value) in self.sessBuffer.items():
    user[key] = value","user = {key: value for (key, value) in self.sessBuffer.items()}","user = {key: value for (key, value) in self.sessBuffer.items()}",1,,,
aiobotocore,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aiobotocore/aiobotocore/client.py,https://github.com/aio-libs/aiobotocore/tree/master/aiobotocore/client.py,AioBaseClient,get_waiter$293,"def get_waiter(self, waiter_name):
    """"""Returns an object that can wait for some condition.

        :type waiter_name: str
        :param waiter_name: The name of the waiter to get. See the waiters
            section of the service docs for a list of available waiters.

        :returns: The specified waiter object.
        :rtype: botocore.waiter.Waiter
        """"""
    config = self._get_waiter_config()
    if not config:
        raise ValueError('Waiter does not exist: %s' % waiter_name)
    model = waiter.WaiterModel(config)
    mapping = {}
    for name in model.waiter_names:
        mapping[xform_name(name)] = name
    if waiter_name not in mapping:
        raise ValueError('Waiter does not exist: %s' % waiter_name)
    return waiter.create_waiter_with_client(mapping[waiter_name], model, self)","for name in model.waiter_names:
    mapping[xform_name(name)] = name",mapping = {xform_name(name): name for name in model.waiter_names},mapping = {xform_name(name): name for name in model.waiter_names},1,,,
brian2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brian2/brian2/tests/test_parsing.py,https://github.com/brian-team/brian2/tree/master/brian2/tests/test_parsing.py,,test_parse_expression_unit$301,"def test_parse_expression_unit(expect, expr):
    Var = namedtuple('Var', ['dim', 'dtype'])
    variables = {'a': Var(dim=(volt * amp).dim, dtype=np.float64), 'b': Var(dim=volt.dim, dtype=np.float64), 'c': Var(dim=amp.dim, dtype=np.float64)}
    group = SimpleGroup(namespace={}, variables=variables)
    all_variables = {}
    for name in get_identifiers(expr):
        if name in variables:
            all_variables[name] = variables[name]
        else:
            all_variables[name] = group._resolve(name, {})
    if isinstance(expect, type) and issubclass(expect, Exception):
        with pytest.raises(expect):
            parse_expression_dimensions(expr, all_variables)
    else:
        u = parse_expression_dimensions(expr, all_variables)
        assert have_same_dimensions(u, expect)","for name in get_identifiers(expr):
    if name in variables:
        all_variables[name] = variables[name]
    else:
        all_variables[name] = group._resolve(name, {})","all_variables = {name: variables[name] if name in variables else group._resolve(name, {}) for name in get_identifiers(expr)}","all_variables = {name: variables[name] if name in variables else group._resolve(name, {}) for name in get_identifiers(expr)}",1,,,
jcvi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jcvi/jcvi/assembly/allmaps.py,https://github.com/tanghaibao/jcvi/tree/master/jcvi/assembly/allmaps.py,Map,compute_lengths$567,"def compute_lengths(self, function):
    lengths = {}
    for (mlg, v) in self.ranks.items():
        lengths[mlg] = max((function(x) for x in v))
    return lengths","for (mlg, v) in self.ranks.items():
    lengths[mlg] = max((function(x) for x in v))","lengths = {mlg: max((function(x) for x in v)) for (mlg, v) in self.ranks.items()}","lengths = {mlg: max((function(x) for x in v)) for (mlg, v) in self.ranks.items()}",1,,,
luminol,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/luminol/src/luminol/algorithms/anomaly_detector_algorithms/bitmap_detector.py,https://github.com/linkedin/luminol/tree/master/src/luminol/algorithms/anomaly_detector_algorithms/bitmap_detector.py,BitmapDetector,_set_scores$196,"def _set_scores(self):
    """"""
        Compute anomaly scores for the time series by sliding both lagging window and future window.
        """"""
    anom_scores = {}
    self._generate_SAX()
    self._construct_all_SAX_chunk_dict()
    length = self.time_series_length
    lws = self.lag_window_size
    fws = self.future_window_size
    for (i, timestamp) in enumerate(self.time_series.timestamps):
        if i < lws or i > length - fws:
            anom_scores[timestamp] = 0
        else:
            anom_scores[timestamp] = self._compute_anom_score_between_two_windows(i)
    self.anom_scores = TimeSeries(self._denoise_scores(anom_scores))","for (i, timestamp) in enumerate(self.time_series.timestamps):
    if i < lws or i > length - fws:
        anom_scores[timestamp] = 0
    else:
        anom_scores[timestamp] = self._compute_anom_score_between_two_windows(i)","anom_scores = {timestamp: 0 if i < lws or i > length - fws else self._compute_anom_score_between_two_windows(i) for (i, timestamp) in enumerate(self.time_series.timestamps)}","anom_scores = {timestamp: 0 if i < lws or i > length - fws else self._compute_anom_score_between_two_windows(i) for (i, timestamp) in enumerate(self.time_series.timestamps)}",1,,,
viztracer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/viztracer/tests/test_cmdline.py,https://github.com/gaogaotiantian/viztracer/tree/master/tests/test_cmdline.py,TestCommandLineBasic,option_to_file$481,"def option_to_file(options, filename='.viztracerrc', section='default'):
    parser = configparser.ConfigParser()
    parser[section] = {}
    for (key, val) in options.items():
        parser[section][key] = val
    with open(filename, 'w') as f:
        parser.write(f)
    try:
        yield
    finally:
        os.remove(filename)","for (key, val) in options.items():
    parser[section][key] = val","parser[section] = {key: val for (key, val) in options.items()}","parser[section] = {key: val for (key, val) in options.items()}",1,,,
noto-emoji,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/noto-emoji/check_emoji_sequences.py,https://github.com/googlefonts/noto-emoji/tree/master//check_emoji_sequences.py,,run_check$398,"def run_check(dirs, names, prefix, suffix, exclude, unicode_version, coverage):
    msg = ''
    if unicode_version:
        msg = ' (%3.1f)' % unicode_version
    if names and dirs:
        sys.exit('Please only provide a directory or a list of names')
    elif names:
        name_to_dirpath = {}
        for name in names:
            name_to_dirpath[name] = ''
    elif dirs:
        print(f'Checking files with prefix ""{prefix}"" and suffix ""{suffix}""{msg} in: {dirs}')
        name_to_dirpath = collect_name_to_dirpath_with_override(dirs, prefix=prefix, suffix=suffix, exclude=exclude)
    print(f'checking {len(name_to_dirpath)} names')
    seq_to_filepath = create_sequence_to_filepath(name_to_dirpath, prefix, suffix)
    print(f'checking {len(seq_to_filepath)} sequences')
    check_sequence_to_filepath(seq_to_filepath, unicode_version, coverage)
    print('Done running checks')","for name in names:
    name_to_dirpath[name] = ''",name_to_dirpath = {name: '' for name in names},name_to_dirpath = {name: '' for name in names},1,,,
yowsup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yowsup/yowsup/layers/axolotl/protocolentities/iq_keys_set.py,https://github.com/tgalal/yowsup/tree/master/yowsup/layers/axolotl/protocolentities/iq_keys_set.py,SetKeysIqProtocolEntity,fromProtocolTreeNode$20,"def fromProtocolTreeNode(node):
    entity = IqProtocolEntity.fromProtocolTreeNode(node)
    entity.__class__ = SetKeysIqProtocolEntity
    regVal = node.getChild('registration').data
    typeVal = node.getChild('type').data
    idVal = node.getChild('identity').data
    preKeys = {}
    for keyNode in node.getChild('list').getAllChildren():
        preKeys[keyNode.getChild('id').data] = keyNode.getChild('value').data
    skeyNode = node.getChild('skey')
    entity.setProps(idVal, (skeyNode.getChild('id').data, skeyNode.getChild('value').data, skeyNode.getChild('signature').data), preKeys, typeVal, regVal)
    return entity","for keyNode in node.getChild('list').getAllChildren():
    preKeys[keyNode.getChild('id').data] = keyNode.getChild('value').data",preKeys = {keyNode.getChild('id').data: keyNode.getChild('value').data for keyNode in node.getChild('list').getAllChildren()},preKeys = {keyNode.getChild('id').data: keyNode.getChild('value').data for keyNode in node.getChild('list').getAllChildren()},1,,,
pymdptoolbox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pymdptoolbox/src/tests/test_utils.py,https://github.com/sawcordwell/pymdptoolbox/tree/master/src/tests/test_utils.py,,test_check_P_square_stochastic_nonnegative_dict_array$75,"def test_check_P_square_stochastic_nonnegative_dict_array():
    P = {}
    R = np.random.rand(STATES, ACTIONS)
    for a in range(ACTIONS):
        P[a] = np.eye(STATES)
    assert mdptoolbox.util.check(P, R) is None","for a in range(ACTIONS):
    P[a] = np.eye(STATES)",P = {a: np.eye(STATES) for a in range(ACTIONS)},P = {a: np.eye(STATES) for a in range(ACTIONS)},1,,,
allennlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/allennlp/tests/training/metrics/sequence_accuracy_test.py,https://github.com/allenai/allennlp/tree/master/tests/training/metrics/sequence_accuracy_test.py,,multiple_runs$126,"def multiple_runs(global_rank: int, world_size: int, gpu_id: Union[int, torch.device], metric: SequenceAccuracy, metric_kwargs: Dict[str, List[Any]], desired_values: Dict[str, Any], exact: Union[bool, Tuple[float, float]]=True):
    kwargs = {}
    for argname in metric_kwargs:
        kwargs[argname] = metric_kwargs[argname][global_rank]
    for i in range(200):
        metric(**kwargs)
    assert desired_values['accuracy'] == metric.get_metric()['accuracy']","for argname in metric_kwargs:
    kwargs[argname] = metric_kwargs[argname][global_rank]",kwargs = {argname: metric_kwargs[argname][global_rank] for argname in metric_kwargs},kwargs = {argname: metric_kwargs[argname][global_rank] for argname in metric_kwargs},1,,,
awx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/awx/awx/main/utils/formatters.py,https://github.com/ansible/awx/tree/master/awx/main/utils/formatters.py,LogstashFormatterBase,get_extra_fields$81,"def get_extra_fields(self, record):
    skip_list = ('args', 'asctime', 'created', 'exc_info', 'exc_text', 'filename', 'funcName', 'id', 'levelname', 'levelno', 'lineno', 'module', 'msecs', 'msecs', 'message', 'msg', 'name', 'pathname', 'process', 'processName', 'relativeCreated', 'thread', 'threadName', 'extra')
    easy_types = (str, bool, dict, float, int, list, type(None))
    fields = {}
    for (key, value) in record.__dict__.items():
        if key not in skip_list:
            if isinstance(value, easy_types):
                fields[key] = value
            else:
                fields[key] = repr(value)
    return fields","for (key, value) in record.__dict__.items():
    if key not in skip_list:
        if isinstance(value, easy_types):
            fields[key] = value
        else:
            fields[key] = repr(value)","fields = {key: value if isinstance(value, easy_types) else repr(value) for (key, value) in record.__dict__.items() if key not in skip_list}","fields = {key: value if isinstance(value, easy_types) else repr(value) for (key, value) in record.__dict__.items() if key not in skip_list}",1,,,
Twitch-Chat-Downloader,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Twitch-Chat-Downloader/tcd/downloader.py,https://github.com/PetterKraabol/Twitch-Chat-Downloader/tree/master/tcd/downloader.py,Downloader,video$68,"def video(self, video: Video) -> None:
    """"""
        Download chat from video
        :param video: Video object
        :return: None
        """"""
    regex = re.compile('((?P<hours>\\d+?)h)?((?P<minutes>\\d+?)m)?((?P<seconds>\\d+?)s)?')
    parts = regex.match(video.duration).groupdict()
    time_params = {}
    for (name, param) in parts.items():
        if param:
            time_params[name] = int(param)
    video_duration = datetime.timedelta(**time_params)
    formatter = Formatter(video)
    if 'json' in self.formats:
        Logger().log('Downloading JSON data', Log.VERBOSE)
        output: str = Pipe(Settings().config['formats']['json']['output']).output(video.data)
        os.makedirs(os.path.dirname(output), exist_ok=True)
        data: dict = {'video': video.data, 'comments': []}
        for comment in video.comments:
            if Arguments().users and comment.commenter.name.lower() not in Arguments().users:
                continue
            if Arguments().includes and Arguments().includes not in comment.message.body.lower():
                continue
            data['comments'].append(comment.data)
            if Settings().config['formats']['json'].get('comments', {}).get('ignore_new_comments', False):
                comment_date = dateutil.parser.parse(comment.created_at)
                vod_finish_date = dateutil.parser.parse(video.created_at) + video_duration
                if comment_date > vod_finish_date:
                    continue
            if Logger().should_print_type(Log.PROGRESS):
                self.draw_progress(current=comment.content_offset_seconds, end=video_duration.seconds, description='json')
        with open(output, 'w', encoding='utf-8') as file:
            json.dump(data, file, indent=4, sort_keys=True)
        Logger().log(f'[json] {output}')
    for format_name in [x for x in self.formats if x not in ['json']]:
        Logger().log(f'Formatting chat using: {format_name}', Log.VERBOSE)
        (comment_tuple, output) = formatter.use(format_name)
        os.makedirs(os.path.dirname(output), exist_ok=True)
        with open(output, '+w', encoding='utf-8') as file:
            for (formatted_comment, comment) in comment_tuple:
                if Arguments().users and comment.commenter.name.lower() not in Arguments().users:
                    continue
                if Arguments().includes and Arguments().includes.lower() not in comment.message.body.lower():
                    continue
                if Settings().config['formats'][format_name].get('comments', {}).get('ignore_new_comments', False):
                    comment_date = dateutil.parser.parse(comment.created_at)
                    vod_finish_date = dateutil.parser.parse(video.created_at) + video_duration
                    if comment_date > vod_finish_date:
                        continue
                if comment and Logger().should_print_type(Log.PROGRESS):
                    self.draw_progress(current=comment.content_offset_seconds, end=video_duration.seconds, description=format_name)
                Logger().log(formatted_comment, Log.PREVIEW)
                file.write('{}\n'.format(formatted_comment))
        Logger().log('[{}] {}'.format(format_name, output))","for (name, param) in parts.items():
    if param:
        time_params[name] = int(param)","time_params = {name: int(param) for (name, param) in parts.items() if param}","time_params = {name: int(param) for (name, param) in parts.items() if param}",1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/fileserver/hgfs.py,https://github.com/saltstack/salt/tree/master/salt/fileserver/hgfs.py,,init$193,"def init():
    """"""
    Return a list of hglib objects for the various hgfs remotes
    """"""
    bp_ = os.path.join(__opts__['cachedir'], 'hgfs')
    new_remote = False
    repos = []
    per_remote_defaults = {}
    for param in PER_REMOTE_OVERRIDES:
        per_remote_defaults[param] = str(__opts__['hgfs_{}'.format(param)])
    for remote in __opts__['hgfs_remotes']:
        repo_conf = copy.deepcopy(per_remote_defaults)
        if isinstance(remote, dict):
            repo_url = next(iter(remote))
            per_remote_conf = {key: str(val) for (key, val) in salt.utils.data.repack_dictlist(remote[repo_url]).items()}
            if not per_remote_conf:
                log.error('Invalid per-remote configuration for hgfs remote %s. If no per-remote parameters are being specified, there may be a trailing colon after the URL, which should be removed. Check the master configuration file.', repo_url)
                _failhard()
            branch_method = per_remote_conf.get('branch_method', per_remote_defaults['branch_method'])
            if branch_method not in VALID_BRANCH_METHODS:
                log.error(""Invalid branch_method '%s' for remote %s. Valid branch methods are: %s. This remote will be ignored."", branch_method, repo_url, ', '.join(VALID_BRANCH_METHODS))
                _failhard()
            per_remote_errors = False
            for param in (x for x in per_remote_conf if x not in PER_REMOTE_OVERRIDES):
                log.error(""Invalid configuration parameter '%s' for remote %s. Valid parameters are: %s. See the documentation for further information."", param, repo_url, ', '.join(PER_REMOTE_OVERRIDES))
                per_remote_errors = True
            if per_remote_errors:
                _failhard()
            repo_conf.update(per_remote_conf)
        else:
            repo_url = remote
        if not isinstance(repo_url, str):
            log.error('Invalid hgfs remote %s. Remotes must be strings, you may need to enclose the URL in quotes', repo_url)
            _failhard()
        try:
            repo_conf['mountpoint'] = salt.utils.url.strip_proto(repo_conf['mountpoint'])
        except TypeError:
            pass
        hash_type = getattr(hashlib, __opts__.get('hash_type', 'md5'))
        repo_hash = hash_type(repo_url.encode('utf-8')).hexdigest()
        rp_ = os.path.join(bp_, repo_hash)
        if not os.path.isdir(rp_):
            os.makedirs(rp_)
        if not os.listdir(rp_):
            hglib.init(rp_)
            new_remote = True
        try:
            repo = hglib.open(rp_)
        except hglib.error.ServerError:
            log.error('Cache path %s (corresponding remote: %s) exists but is not a valid mercurial repository. You will need to manually delete this directory on the master to continue to use this hgfs remote.', rp_, repo_url)
            _failhard()
        except Exception as exc:
            log.error(""Exception '%s' encountered while initializing hgfs remote %s"", exc, repo_url)
            _failhard()
        try:
            refs = repo.config(names=b'paths')
        except hglib.error.CommandError:
            refs = None
        if not refs:
            hgconfpath = os.path.join(rp_, '.hg', 'hgrc')
            with salt.utils.files.fopen(hgconfpath, 'w+') as hgconfig:
                hgconfig.write('[paths]\n')
                hgconfig.write(salt.utils.stringutils.to_str('default = {}\n'.format(repo_url)))
        repo_conf.update({'repo': repo, 'url': repo_url, 'hash': repo_hash, 'cachedir': rp_, 'lockfile': os.path.join(__opts__['cachedir'], 'hgfs', '{}.update.lk'.format(repo_hash))})
        repos.append(repo_conf)
        repo.close()
    if new_remote:
        remote_map = os.path.join(__opts__['cachedir'], 'hgfs/remote_map.txt')
        try:
            with salt.utils.files.fopen(remote_map, 'w+') as fp_:
                timestamp = datetime.now().strftime('%d %b %Y %H:%M:%S.%f')
                fp_.write('# hgfs_remote map as of {}\n'.format(timestamp))
                for repo in repos:
                    fp_.write(salt.utils.stringutils.to_str('{} = {}\n'.format(repo['hash'], repo['url'])))
        except OSError:
            pass
        else:
            log.info('Wrote new hgfs_remote map to %s', remote_map)
    return repos","for param in PER_REMOTE_OVERRIDES:
    per_remote_defaults[param] = str(__opts__['hgfs_{}'.format(param)])",per_remote_defaults = {param: str(__opts__['hgfs_{}'.format(param)]) for param in PER_REMOTE_OVERRIDES},per_remote_defaults = {param: str(__opts__['hgfs_{}'.format(param)]) for param in PER_REMOTE_OVERRIDES},1,,,
rotki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rotki/rotkehlchen/db/dbhandler.py,https://github.com/rotki/rotki/tree/master/rotkehlchen/db/dbhandler.py,DBHandler,get_settings$569,"def get_settings(self, cursor: 'DBCursor', have_premium: bool=False) -> DBSettings:
    """"""Aggregates settings from DB and from the given args and returns the settings object""""""
    cursor.execute('SELECT name, value FROM settings;')
    settings_dict = {}
    for q in cursor:
        settings_dict[q[0]] = q[1]
    settings_dict['have_premium'] = have_premium
    return db_settings_from_dict(settings_dict, self.msg_aggregator)","for q in cursor:
    settings_dict[q[0]] = q[1]",settings_dict = {q[0]: q[1] for q in cursor},settings_dict = {q[0]: q[1] for q in cursor},1,,,
PaddleSlim,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSlim/demo/dygraph/pruning/export_model.py,https://github.com/PaddlePaddle/PaddleSlim/tree/master/demo/dygraph/pruning/export_model.py,,export$67,"def export(args):
    paddle.set_device('cpu')
    test_reader = None
    if args.data == 'cifar10':
        class_dim = 10
        image_shape = [3, 224, 224]
    elif args.data == 'imagenet':
        class_dim = 1000
        image_shape = [3, 224, 224]
    else:
        raise ValueError('{} is not supported.'.format(args.data))
    assert args.model in model_list, '{} is not in lists: {}'.format(args.model, model_list)
    net = models.__dict__[args.model](pretrained=False, num_classes=class_dim)
    pruner = paddleslim.dygraph.L1NormFilterPruner(net, [1] + image_shape)
    params = get_pruned_params(args, net)
    ratios = {}
    for param in params:
        ratios[param] = args.pruned_ratio
    print('ratios: {}'.format(ratios))
    pruner.prune_vars(ratios, [0])
    param_state_dict = paddle.load(args.checkpoint + '.pdparams')
    net.set_dict(param_state_dict)
    net.eval()
    model = to_static(net, input_spec=[paddle.static.InputSpec(shape=[None] + image_shape, dtype='float32', name='image')])
    paddle.jit.save(net, args.output_path)","for param in params:
    ratios[param] = args.pruned_ratio",ratios = {param: args.pruned_ratio for param in params},ratios = {param: args.pruned_ratio for param in params},1,,,
cornac,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cornac/cornac/models/recommender.py,https://github.com/PreferredAI/cornac/tree/master/cornac/models/recommender.py,Recommender,clone$79,"def clone(self, new_params=None):
    """"""Clone an instance of the model object.

        Parameters
        ----------
        new_params: dict, optional, default: None
            New parameters for the cloned instance.

        Returns
        -------
        object: :obj:`cornac.models.Recommender`
        """"""
    new_params = {} if new_params is None else new_params
    init_params = {}
    for name in self._get_init_params():
        init_params[name] = new_params.get(name, copy.deepcopy(getattr(self, name)))
    return self.__class__(**init_params)","for name in self._get_init_params():
    init_params[name] = new_params.get(name, copy.deepcopy(getattr(self, name)))","init_params = {name: new_params.get(name, copy.deepcopy(getattr(self, name))) for name in self._get_init_params()}","init_params = {name: new_params.get(name, copy.deepcopy(getattr(self, name))) for name in self._get_init_params()}",1,,,
PaddleSlim,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSlim/demo/dygraph/pruning/train.py,https://github.com/PaddlePaddle/PaddleSlim/tree/master/demo/dygraph/pruning/train.py,,compress$113,"def compress(args):
    num_workers = 4
    shuffle = True
    if args.ce_test:
        seed = 111
        paddle.seed(seed)
        np.random.seed(seed)
        random.seed(seed)
        num_workers = 0
        shuffle = False
    paddle.set_device('gpu' if args.use_gpu else 'cpu')
    train_reader = None
    test_reader = None
    if args.data == 'cifar10':
        transform = T.Compose([T.Transpose(), T.Normalize([127.5], [127.5])])
        train_dataset = paddle.vision.datasets.Cifar10(mode='train', backend='cv2', transform=transform)
        val_dataset = paddle.vision.datasets.Cifar10(mode='test', backend='cv2', transform=transform)
        class_dim = 10
        image_shape = [3, 32, 32]
        pretrain = False
    elif args.data == 'imagenet':
        train_dataset = ImageNetDataset('data/ILSVRC2012', mode='train', image_size=224, resize_short_size=256)
        val_dataset = ImageNetDataset('data/ILSVRC2012', mode='val', image_size=224, resize_short_size=256)
        class_dim = 1000
        image_shape = [3, 224, 224]
        pretrain = True
    else:
        raise ValueError('{} is not supported.'.format(args.data))
    assert args.model in model_list, '{} is not in lists: {}'.format(args.model, model_list)
    inputs = [Input([None] + image_shape, 'float32', name='image')]
    labels = [Input([None, 1], 'int64', name='label')]
    net = models.__dict__[args.model](pretrained=pretrain, num_classes=class_dim)
    _logger.info('FLOPs before pruning: {}GFLOPs'.format(flops(net, [1] + image_shape) / 1000))
    net.eval()
    if args.criterion == 'fpgm':
        pruner = paddleslim.dygraph.FPGMFilterPruner(net, [1] + image_shape)
    elif args.criterion == 'l1_norm':
        pruner = paddleslim.dygraph.L1NormFilterPruner(net, [1] + image_shape)
    params = get_pruned_params(args, net)
    ratios = {}
    for param in params:
        ratios[param] = args.pruned_ratio
    plan = pruner.prune_vars(ratios, [0])
    _logger.info('FLOPs after pruning: {}GFLOPs; pruned ratio: {}'.format(flops(net, [1] + image_shape) / 1000, plan.pruned_flops))
    for param in net.parameters():
        if 'conv2d' in param.name:
            print('{}\t{}'.format(param.name, param.shape))
    net.train()
    model = paddle.Model(net, inputs, labels)
    steps_per_epoch = int(np.ceil(len(train_dataset) * 1.0 / args.batch_size))
    opt = create_optimizer(args, net.parameters(), steps_per_epoch)
    model.prepare(opt, paddle.nn.CrossEntropyLoss(), paddle.metric.Accuracy(topk=(1, 5)))
    if args.checkpoint is not None:
        model.load(args.checkpoint)
    model.fit(train_data=train_dataset, eval_data=val_dataset, epochs=args.num_epochs, batch_size=args.batch_size // ParallelEnv().nranks, verbose=1, save_dir=args.model_path, num_workers=num_workers, shuffle=shuffle)","for param in params:
    ratios[param] = args.pruned_ratio",ratios = {param: args.pruned_ratio for param in params},ratios = {param: args.pruned_ratio for param in params},1,,,
traitlets,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/traitlets/traitlets/traitlets.py,https://github.com/ipython/traitlets/tree/master/traitlets/traitlets.py,HasTraits,trait_defaults$1567,"def trait_defaults(self, *names, **metadata):
    """"""Return a trait's default value or a dictionary of them

        Notes
        -----
        Dynamically generated default values may
        depend on the current state of the object.""""""
    for n in names:
        if not self.has_trait(n):
            raise TraitError(""'%s' is not a trait of '%s' instances"" % (n, type(self).__name__))
    if len(names) == 1 and len(metadata) == 0:
        return self._get_trait_default_generator(names[0])(self)
    trait_names = self.trait_names(**metadata)
    trait_names.extend(names)
    defaults = {}
    for n in trait_names:
        defaults[n] = self._get_trait_default_generator(n)(self)
    return defaults","for n in trait_names:
    defaults[n] = self._get_trait_default_generator(n)(self)",defaults = {n: self._get_trait_default_generator(n)(self) for n in trait_names},defaults = {n: self._get_trait_default_generator(n)(self) for n in trait_names},1,,,
flare-emu,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flare-emu/flare_emu.py,https://github.com/mandiant/flare-emu/tree/master//flare_emu.py,EmuHelper,getPaths$996,"def getPaths(self, fva, maxPaths=MAXCODEPATHS, maxNodes=MAXNODESEARCH):
    flowchart = self.analysisHelper.getFlowChart(fva)
    term_bbs_ids = [bb.id for bb in self.analysisHelper.getTerminatingBBs(flowchart)]
    start_bb = self.analysisHelper.getStartBB(fva, flowchart)
    if term_bbs_ids != [0]:
        if self.verbose > 0:
            self.logger.debug('exploring function with %d blocks' % len(flowchart))
        graph = self._explore(start_bb)
        if graph is None:
            self.logger.debug('graph for target %s could not be traversed, skipping' % self.hexString(fva))
            return (None, None)
        if self.verbose > 0:
            self.logger.debug('graph for target:\n%s' % repr(graph))
        path = [0]
        paths = []
        self._findPathsFromGraph(paths, path, graph, 0, term_bbs_ids, maxPaths, 0, maxNodes)
        if len(paths) == 0:
            self.logger.debug('paths for target %s could not be discovered, skipping' % self.hexString(fva))
            return (None, None)
    else:
        paths = [[0]]
    if self.verbose > 0:
        self.logger.debug('code paths to target: %s' % repr(paths))
    flow = {}
    for bb in flowchart:
        flow[bb.id] = (bb.start_ea, self.analysisHelper.getBlockEndInsnAddr(bb.start_ea, flowchart))
    return (flow, paths)","for bb in flowchart:
    flow[bb.id] = (bb.start_ea, self.analysisHelper.getBlockEndInsnAddr(bb.start_ea, flowchart))","flow = {bb.id: (bb.start_ea, self.analysisHelper.getBlockEndInsnAddr(bb.start_ea, flowchart)) for bb in flowchart}","flow = {bb.id: (bb.start_ea, self.analysisHelper.getBlockEndInsnAddr(bb.start_ea, flowchart)) for bb in flowchart}",1,,,
code-catalog-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/code-catalog-python/catalog/suggested/graphs/floyd_warshall.py,https://github.com/jwasham/code-catalog-python/tree/master/catalog/suggested/graphs/floyd_warshall.py,,floyd_warshall$11,"def floyd_warshall(graph):
    nodes = graph.keys()
    ' distance[][] will be the output matrix that will finally\n        have the shortest distances between every pair of vertices '
    distance = {}
    for n in nodes:
        distance[n] = {}
        for k in nodes:
            distance[n][k] = graph[n][k]
    ' Add all vertices one by one to the set of intermediate\n     vertices.\n\n         ---> Before start of a iteration, we have shortest distances\n         between all pairs of vertices such that the shortest\n         distances consider only the vertices in set\n         {0, 1, 2, .. k-1} as intermediate vertices.\n\n          ----> After the end of a iteration, vertex no. k is\n         added to the set of intermediate vertices and the\n         set becomes {0, 1, 2, .. k}\n    '
    for k in nodes:
        for i in nodes:
            for j in nodes:
                distance[i][j] = min(distance[i][j], distance[i][k] + distance[k][j])
    return distance","for n in nodes:
    distance[n] = {}
    for k in nodes:
        distance[n][k] = graph[n][k]",distance = {n: {k: graph[n][k] for k in nodes} for n in nodes},distance = {n: {k: graph[n][k] for k in nodes} for n in nodes},1,,,
thetagang,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/thetagang/thetagang/util.py,https://github.com/brndnmtthws/thetagang/tree/master/thetagang/util.py,,account_summary_to_dict$15,"def account_summary_to_dict(account_summary):
    d = dict()
    for s in account_summary:
        d[s.tag] = s
    return d","for s in account_summary:
    d[s.tag] = s",d = {s.tag: s for s in account_summary},d = {s.tag: s for s in account_summary},1,,,
moto,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/rds/responses.py,https://github.com/spulec/moto/tree/master/moto/rds/responses.py,RDSResponse,unpack_complex_list_params$59,"def unpack_complex_list_params(self, label, names):
    unpacked_list = list()
    count = 1
    while self._get_param('{0}.{1}.{2}'.format(label, count, names[0])):
        param = dict()
        for i in range(len(names)):
            param[names[i]] = self._get_param('{0}.{1}.{2}'.format(label, count, names[i]))
        unpacked_list.append(param)
        count += 1
    return unpacked_list","for i in range(len(names)):
    param[names[i]] = self._get_param('{0}.{1}.{2}'.format(label, count, names[i]))","param = {names[i]: self._get_param('{0}.{1}.{2}'.format(label, count, names[i])) for i in range(len(names))}","param = {names[i]: self._get_param('{0}.{1}.{2}'.format(label, count, names[i])) for i in range(len(names))}",1,,,
DeepRec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRec/models/item_ranking/cdae.py,https://github.com/cheungdaven/DeepRec/tree/master/models/item_ranking/cdae.py,ICDAE,_get_neg_items$262,"def _get_neg_items(self, data):
    neg_items = {}
    for u in range(self.num_user):
        neg_items[u] = [k for (k, i) in enumerate(data[u]) if data[u][k] == 0]
    return neg_items","for u in range(self.num_user):
    neg_items[u] = [k for (k, i) in enumerate(data[u]) if data[u][k] == 0]","neg_items = {u: [k for (k, i) in enumerate(data[u]) if i == 0] for u in range(self.num_user)}","neg_items = {u: [k for (k, i) in enumerate(data[u]) if i == 0] for u in range(self.num_user)}",1,,,
pytorch3d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch3d/pytorch3d/datasets/utils.py,https://github.com/facebookresearch/pytorch3d/tree/master/pytorch3d/datasets/utils.py,,collate_batched_meshes$13,"def collate_batched_meshes(batch: List[Dict]):
    """"""
    Take a list of objects in the form of dictionaries and merge them
    into a single dictionary. This function can be used with a Dataset
    object to create a torch.utils.data.Dataloader which directly
    returns Meshes objects.
    TODO: Add support for textures.

    Args:
        batch: List of dictionaries containing information about objects
            in the dataset.

    Returns:
        collated_dict: Dictionary of collated lists. If batch contains both
            verts and faces, a collated mesh batch is also returned.
    """"""
    if batch is None or len(batch) == 0:
        return None
    collated_dict = {}
    for k in batch[0].keys():
        collated_dict[k] = [d[k] for d in batch]
    collated_dict['mesh'] = None
    if {'verts', 'faces'}.issubset(collated_dict.keys()):
        textures = None
        if 'textures' in collated_dict:
            textures = TexturesAtlas(atlas=collated_dict['textures'])
        collated_dict['mesh'] = Meshes(verts=collated_dict['verts'], faces=collated_dict['faces'], textures=textures)
    return collated_dict","for k in batch[0].keys():
    collated_dict[k] = [d[k] for d in batch]",collated_dict = {k: [d[k] for d in batch] for k in batch[0].keys()},collated_dict = {k: [d[k] for d in batch] for k in batch[0].keys()},1,,,
deit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deit/cait_models.py,https://github.com/facebookresearch/deit/tree/master//cait_models.py,,cait_XS24$342,"def cait_XS24(pretrained=False, **kwargs):
    model = cait_models(img_size=384, patch_size=16, embed_dim=288, depth=24, num_heads=6, mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-06), init_scale=1e-05, depth_token_only=2, **kwargs)
    model.default_cfg = _cfg()
    if pretrained:
        checkpoint = torch.hub.load_state_dict_from_url(url='https://dl.fbaipublicfiles.com/deit/XS24_384.pth', map_location='cpu', check_hash=True)
        checkpoint_no_module = {}
        for k in model.state_dict().keys():
            checkpoint_no_module[k] = checkpoint['model']['module.' + k]
        model.load_state_dict(checkpoint_no_module)
    return model","for k in model.state_dict().keys():
    checkpoint_no_module[k] = checkpoint['model']['module.' + k]",checkpoint_no_module = {k: checkpoint['model']['module.' + k] for k in model.state_dict().keys()},checkpoint_no_module = {k: checkpoint['model']['module.' + k] for k in model.state_dict().keys()},1,,,
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/printing/codeprinter.py,https://github.com/sympy/sympy/tree/master/sympy/printing/codeprinter.py,CodePrinter,_sort_optimized$263,"def _sort_optimized(self, indices, expr):
    from sympy.tensor.indexed import Indexed
    if not indices:
        return []
    score_table = {}
    for i in indices:
        score_table[i] = 0
    arrays = expr.atoms(Indexed)
    for arr in arrays:
        for (p, ind) in enumerate(arr.indices):
            try:
                score_table[ind] += self._rate_index_position(p)
            except KeyError:
                pass
    return sorted(indices, key=lambda x: score_table[x])","for i in indices:
    score_table[i] = 0",score_table = {i: 0 for i in indices},score_table = {i: 0 for i in indices},1,,,
planet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/planet/planet/control/wrappers.py,https://github.com/google-research/planet/tree/master/planet/control/wrappers.py,DeepMindWrapper,observation_space$285,"def observation_space(self):
    components = {}
    for (key, value) in self._env.observation_spec().items():
        components[key] = gym.spaces.Box(-np.inf, np.inf, value.shape, dtype=np.float32)
    return gym.spaces.Dict(components)","for (key, value) in self._env.observation_spec().items():
    components[key] = gym.spaces.Box(-np.inf, np.inf, value.shape, dtype=np.float32)","components = {key: gym.spaces.Box(-np.inf, np.inf, value.shape, dtype=np.float32) for (key, value) in self._env.observation_spec().items()}","components = {key: gym.spaces.Box(-np.inf, np.inf, value.shape, dtype=np.float32) for (key, value) in self._env.observation_spec().items()}",1,,,
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/core/doctype/system_settings/system_settings.py,https://github.com/frappe/frappe/tree/master/frappe/core/doctype/system_settings/system_settings.py,,load$91,"def load():
    if not 'System Manager' in frappe.get_roles():
        frappe.throw(_('Not permitted'), frappe.PermissionError)
    all_defaults = frappe.db.get_defaults()
    defaults = {}
    for df in frappe.get_meta('System Settings').get('fields'):
        if df.fieldtype in ('Select', 'Data'):
            defaults[df.fieldname] = all_defaults.get(df.fieldname)
    return {'timezones': get_all_timezones(), 'defaults': defaults}","for df in frappe.get_meta('System Settings').get('fields'):
    if df.fieldtype in ('Select', 'Data'):
        defaults[df.fieldname] = all_defaults.get(df.fieldname)","defaults = {df.fieldname: all_defaults.get(df.fieldname) for df in frappe.get_meta('System Settings').get('fields') if df.fieldtype in ('Select', 'Data')}","defaults = {df.fieldname: all_defaults.get(df.fieldname) for df in frappe.get_meta('System Settings').get('fields') if df.fieldtype in ('Select', 'Data')}",1,,,
uis-rnn,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/uis-rnn/uisrnn/evals.py,https://github.com/google/uis-rnn/tree/master/uisrnn/evals.py,,get_list_inverse_index$20,"def get_list_inverse_index(unique_ids):
    """"""Get value to position index from a list of unique ids.

  Args:
    unique_ids: A list of unique integers of strings.

  Returns:
    result: a dict from value to position

  Raises:
    TypeError: If unique_ids is not a list.
  """"""
    if not isinstance(unique_ids, list):
        raise TypeError('unique_ids must be a list')
    result = dict()
    for (i, unique_id) in enumerate(unique_ids):
        result[unique_id] = i
    return result","for (i, unique_id) in enumerate(unique_ids):
    result[unique_id] = i","result = {unique_id: i for (i, unique_id) in enumerate(unique_ids)}","result = {unique_id: i for (i, unique_id) in enumerate(unique_ids)}",1,,,
lifelines,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lifelines/lifelines/utils/sklearn_adapter.py,https://github.com/CamDavidsonPilon/lifelines/tree/master/lifelines/utils/sklearn_adapter.py,_SklearnModel,get_params$62,"def get_params(self, deep=True):
    out = {}
    for (name, p) in inspect.signature(self.lifelines_model.__init__).parameters.items():
        if p.kind < 4:
            out[name] = getattr(self.lifelines_model, name)
    return out","for (name, p) in inspect.signature(self.lifelines_model.__init__).parameters.items():
    if p.kind < 4:
        out[name] = getattr(self.lifelines_model, name)","out = {name: getattr(self.lifelines_model, name) for (name, p) in inspect.signature(self.lifelines_model.__init__).parameters.items() if p.kind < 4}","out = {name: getattr(self.lifelines_model, name) for (name, p) in inspect.signature(self.lifelines_model.__init__).parameters.items() if p.kind < 4}",1,,,
freeipa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/plugins/servicedelegation.py,https://github.com/freeipa/freeipa/tree/master/ipaserver/plugins/servicedelegation.py,servicedelegationtarget_find,pre_callback$516,"def pre_callback(self, ldap, filters, attrs_list, base_dn, scope, term=None, **options):
    """"""
        Exclude rules from the search output. A target contains a subset
        of a rule objectclass.
        """"""
    search_kw = self.args_options_2_entry(**options)
    search_kw['objectclass'] = self.obj.object_class
    attr_filter = ldap.make_filter(search_kw, rules=ldap.MATCH_ALL)
    rule_kw = {'objectclass': 'ipakrb5delegationacl'}
    target_filter = ldap.make_filter(rule_kw, rules=ldap.MATCH_NONE)
    attr_filter = ldap.combine_filters((target_filter, attr_filter), rules=ldap.MATCH_ALL)
    search_kw = {}
    for a in self.obj.default_attributes:
        search_kw[a] = term
    term_filter = ldap.make_filter(search_kw, exact=False)
    sfilter = ldap.combine_filters((term_filter, attr_filter), rules=ldap.MATCH_ALL)
    return (sfilter, base_dn, ldap.SCOPE_ONELEVEL)","for a in self.obj.default_attributes:
    search_kw[a] = term",search_kw = {a: term for a in self.obj.default_attributes},search_kw = {a: term for a in self.obj.default_attributes},1,,,
OctoPrint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OctoPrint/src/octoprint/server/api/printer_profiles.py,https://github.com/OctoPrint/OctoPrint/tree/master/src/octoprint/server/api/printer_profiles.py,,_convert_profiles$180,"def _convert_profiles(profiles):
    result = {}
    for (identifier, profile) in profiles.items():
        result[identifier] = _convert_profile(profile)
    return result","for (identifier, profile) in profiles.items():
    result[identifier] = _convert_profile(profile)","result = {identifier: _convert_profile(profile) for (identifier, profile) in profiles.items()}","result = {identifier: _convert_profile(profile) for (identifier, profile) in profiles.items()}",1,,,
platformio-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/platformio-core/platformio/platform/board.py,https://github.com/platformio/platformio-core/tree/master/platformio/platform/board.py,PlatformBoardConfig,get_debug_data$105,"def get_debug_data(self):
    if not self._manifest.get('debug', {}).get('tools'):
        return None
    tools = {}
    for (name, options) in self._manifest['debug']['tools'].items():
        tools[name] = {}
        for (key, value) in options.items():
            if key in ('default', 'onboard') and value:
                tools[name][key] = value
    return {'tools': tools}","for (name, options) in self._manifest['debug']['tools'].items():
    tools[name] = {}
    for (key, value) in options.items():
        if key in ('default', 'onboard') and value:
            tools[name][key] = value","tools = {name: {key: value for (key, value) in options.items() if key in ('default', 'onboard') and value} for (name, options) in self._manifest['debug']['tools'].items()}","tools = {name: {key: value for (key, value) in options.items() if key in ('default', 'onboard') and value} for (name, options) in self._manifest['debug']['tools'].items()}",1,,,
kge,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kge/kge/dataset.py,https://github.com/uma-pi1/kge/tree/master/kge/dataset.py,Dataset,save_to$164,"def save_to(self, checkpoint: Dict, meta_keys: Optional[List[str]]=None) -> Dict:
    """"""Adds meta data to a checkpoint""""""
    dataset_checkpoint = {'num_entities': self.num_entities(), 'num_relations': self.num_relations()}
    checkpoint['dataset'] = dataset_checkpoint
    if meta_keys is None:
        return checkpoint
    meta_checkpoint = {}
    for key in meta_keys:
        meta_checkpoint[key] = self.map_indexes(None, key)
    checkpoint['dataset']['meta'] = meta_checkpoint
    return checkpoint","for key in meta_keys:
    meta_checkpoint[key] = self.map_indexes(None, key)","meta_checkpoint = {key: self.map_indexes(None, key) for key in meta_keys}","meta_checkpoint = {key: self.map_indexes(None, key) for key in meta_keys}",1,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_webapp/docassemble/webapp/server.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_webapp/docassemble/webapp/server.py,,redirect_to_interview_in_package$12148,"def redirect_to_interview_in_package(package, filename):
    if COOKIELESS_SESSIONS:
        return html_index()
    arguments = dict()
    for arg in request.args:
        arguments[arg] = request.args[arg]
    if re.search('playground[0-9]', package):
        arguments['i'] = 'docassemble.' + package + ':' + filename + '.yml'
    else:
        arguments['i'] = 'docassemble.' + package + ':data/questions/' + filename + '.yml'
    if 'session' not in arguments:
        arguments['new_session'] = '1'
    request.args = arguments
    return index(refer=['start', package, filename])","for arg in request.args:
    arguments[arg] = request.args[arg]",arguments = {arg: request.args[arg] for arg in request.args},arguments = {arg: request.args[arg] for arg in request.args},1,,,
anchore-engine,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anchore-engine/tests/integration/services/policy_engine/conftest.py,https://github.com/anchore/anchore-engine/tree/master/tests/integration/services/policy_engine/conftest.py,,run_legacy_sync$88,"def run_legacy_sync(test_env: LocalTestDataEnvironment, to_sync: List[str]) -> List[FeedSyncResult]:
    DataFeeds.__scratch_dir__ = '/tmp'
    feed_url = os.getenv('ANCHORE_GRYPE_DB_URL', 'https://ancho.re/v1/service/feeds')
    data_clause = {}
    for feed_name in to_sync:
        data_clause[feed_name] = {'enabled': True, 'url': feed_url}
    config = {'provider': 'legacy', 'sync': {'enabled': os.getenv('ANCHORE_FEEDS_ENABLED', True), 'ssl_verify': os.getenv('ANCHORE_FEEDS_SSL_VERIFY', True), 'connection_timeout_seconds': 3, 'read_timeout_seconds': 60, 'data': data_clause}}
    vulnerabilities_provider = LegacyProvider()
    default_sync_config = vulnerabilities_provider.get_default_sync_config()
    sync_configs = compute_selected_configs_to_sync(provider='legacy', vulnerabilities_config=config, default_provider_sync_config=default_sync_config)
    sync_utils = vulnerabilities_provider.get_sync_util_provider(sync_configs)
    sync_utils.get_client = MagicMock(return_value=test_env.feed_client)
    return DataFeeds.sync(sync_util_provider=sync_utils)","for feed_name in to_sync:
    data_clause[feed_name] = {'enabled': True, 'url': feed_url}","data_clause = {feed_name: {'enabled': True, 'url': feed_url} for feed_name in to_sync}","data_clause = {feed_name: {'enabled': True, 'url': feed_url} for feed_name in to_sync}",1,,,
dockerscan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dockerscan/dockerscan/actions/image/docker_api.py,https://github.com/cr0hn/dockerscan/tree/master/dockerscan/actions/image/docker_api.py,,open_docker_image$68,"def open_docker_image(image_path: str):
    """"""
    This function is a context manager that allow to open a docker image and
    return their layers and the layers metadata.

    yields img:TarFile, first_layer, image_and_tag, manifest

    >>> with open_docker_image(""~/images/nginx:latest"") as (img, first_layer, image_and_tag, manifest):
            print(img)
            print(first_layer)
            print(image_and_tag)
            print(manifest)
    <tarfile.TarFile object at 0x10464be48>
    '2dc9f5ef4d45394b3bfedbe23950de81cabd941519f59e163d243a7d4f859622'
    {'nginx': 'latest'}
    [{'Layers': ['8327c7df0d8cfe8652fc4be305e15e516b1b5bb48e13bb39780a87a58316c522/layer.tar', '076538d7e850181c3cccbdbce3a0811698efad376e2c99a72a203493739c2bf2/layer.tar', '2dc9f5ef4d45394b3bfedbe23950de81cabd941519f59e163d243a7d4f859622/layer.tar'], 'RepoTags': ['nginx:latest'], 'Config': 'db079554b4d2f7c65c4df3adae88cb72d051c8c3b8613eb44e86f60c945b1ca7.json'}]

    """"""
    tmp_image = os.path.basename(image_path)
    if ':' in tmp_image:
        (image, tag, *_) = tmp_image.split(':', maxsplit=1)
    else:
        (image, tag) = (tmp_image, 'latest')
    image_layers_tags = {}
    with tarfile.open(image_path, 'r') as img:
        manifest_content = read_file_from_image(img, 'manifest.json')
        if hasattr(manifest_content, 'decode'):
            manifest_content = manifest_content.decode()
        manifest_content = json.loads(manifest_content)
        repo_content = read_file_from_image(img, 'repositories')
        if hasattr(repo_content, 'decode'):
            repo_content = repo_content.decode()
        repos_info = json.loads(repo_content)
        for (name, tags) in repos_info.items():
            image_layers_tags[name] = ' '.join(tags)
        try:
            top_layers = repos_info[image][tag]
        except KeyError:
            image = list(image_layers_tags.keys())[0]
            tag = list(repos_info[image].keys())[0]
            top_layers = repos_info[image][tag]
        yield (img, top_layers, image_layers_tags, manifest_content)","for (name, tags) in repos_info.items():
    image_layers_tags[name] = ' '.join(tags)","image_layers_tags = {name: ' '.join(tags) for (name, tags) in repos_info.items()}","image_layers_tags = {name: ' '.join(tags) for (name, tags) in repos_info.items()}",1,,,
kappa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kappa/kappa/restapi.py,https://github.com/garnaat/kappa/tree/master/kappa/restapi.py,RestApi,_get_resources$89,"def _get_resources(self):
    if self._resources is None:
        try:
            response = self._apigateway_client.call('get_resources', restApiId=self.api_id)
            LOG.debug(response)
            self._resources = {}
            for item in response['items']:
                self._resources[item['path']] = item
        except Exception:
            LOG.exception('Unable to find resources for: %s', self.api_name)
    return self._resources","for item in response['items']:
    self._resources[item['path']] = item",self._resources = {item['path']: item for item in response['items']},self._resources = {item['path']: item for item in response['items']},1,,,
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/solvers/solvers.py,https://github.com/sympy/sympy/tree/master/sympy/solvers/solvers.py,,solve_linear_system_LU$2362,"def solve_linear_system_LU(matrix, syms):
    """"""
    Solves the augmented matrix system using ``LUsolve`` and returns a
    dictionary in which solutions are keyed to the symbols of *syms* as ordered.

    Explanation
    ===========

    The matrix must be invertible.

    Examples
    ========

    >>> from sympy import Matrix
    >>> from sympy.abc import x, y, z
    >>> from sympy.solvers.solvers import solve_linear_system_LU

    >>> solve_linear_system_LU(Matrix([
    ... [1, 2, 0, 1],
    ... [3, 2, 2, 1],
    ... [2, 0, 0, 1]]), [x, y, z])
    {x: 1/2, y: 1/4, z: -1/2}

    See Also
    ========

    LUsolve

    """"""
    if matrix.rows != matrix.cols - 1:
        raise ValueError('Rows should be equal to columns - 1')
    A = matrix[:matrix.rows, :matrix.rows]
    b = matrix[:, matrix.cols - 1:]
    soln = A.LUsolve(b)
    solutions = {}
    for i in range(soln.rows):
        solutions[syms[i]] = soln[i, 0]
    return solutions","for i in range(soln.rows):
    solutions[syms[i]] = soln[i, 0]","solutions = {syms[i]: soln[i, 0] for i in range(soln.rows)}","solutions = {syms[i]: soln[i, 0] for i in range(soln.rows)}",1,,,
ERNIE,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ERNIE/ernie-gram/mrc/mrc_metrics.py,https://github.com/thunlp/ERNIE/tree/master/ernie-gram/mrc/mrc_metrics.py,,_get_final_text$257,"def _get_final_text(pred_text, orig_text, tokenizer):
    """"""Project the tokenized prediction back to the original text.""""""

    def _strip_spaces(text):
        ns_chars = []
        ns_to_s_map = collections.OrderedDict()
        for (i, c) in enumerate(text):
            if c == ' ':
                continue
            ns_to_s_map[len(ns_chars)] = i
            ns_chars.append(c)
        ns_text = ''.join(ns_chars)
        return (ns_text, ns_to_s_map)
    tok_text = ' '.join(tokenizer.tokenize(orig_text))
    start_position = tok_text.find(pred_text)
    if start_position == -1:
        return orig_text
    end_position = start_position + len(pred_text) - 1
    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)
    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)
    if len(orig_ns_text) != len(tok_ns_text):
        return orig_text
    tok_s_to_ns_map = {}
    for (i, tok_index) in six.iteritems(tok_ns_to_s_map):
        tok_s_to_ns_map[tok_index] = i
    orig_start_position = None
    if start_position in tok_s_to_ns_map:
        ns_start_position = tok_s_to_ns_map[start_position]
        if ns_start_position in orig_ns_to_s_map:
            orig_start_position = orig_ns_to_s_map[ns_start_position]
    if orig_start_position is None:
        return orig_text
    orig_end_position = None
    if end_position in tok_s_to_ns_map:
        ns_end_position = tok_s_to_ns_map[end_position]
        if ns_end_position in orig_ns_to_s_map:
            orig_end_position = orig_ns_to_s_map[ns_end_position]
    if orig_end_position is None:
        return orig_text
    output_text = orig_text[orig_start_position:orig_end_position + 1]
    return output_text","for (i, tok_index) in six.iteritems(tok_ns_to_s_map):
    tok_s_to_ns_map[tok_index] = i","tok_s_to_ns_map = {tok_index: i for (i, tok_index) in six.iteritems(tok_ns_to_s_map)}","tok_s_to_ns_map = {tok_index: i for (i, tok_index) in six.iteritems(tok_ns_to_s_map)}",1,,,
neutron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/tests/unit/plugins/ml2/drivers/l2pop/rpc_manager/test_l2population_rpc.py,https://github.com/openstack/neutron/tree/master/neutron/tests/unit/plugins/ml2/drivers/l2pop/rpc_manager/test_l2population_rpc.py,TestL2populationRpcCallBackTunnelMixin,test_get_agent_ports_no_agent_ports$50,"def test_get_agent_ports_no_agent_ports(self):
    results = {}
    self.fdb_entries1[self.lvms[1].net]['ports'] = {}
    for (lvm, agent_ports) in self.fakeagent.get_agent_ports(self.fdb_entries1):
        results[lvm] = agent_ports
    expected = {self.lvm0: {self.ports[0].ip: [(self.lvms[0].mac, self.lvms[0].ip)], self.local_ip: []}, self.lvm1: {}, self.lvm2: {self.ports[2].ip: [(self.lvms[2].mac, self.lvms[2].ip)], self.local_ip: []}}
    self.assertEqual(expected, results)","for (lvm, agent_ports) in self.fakeagent.get_agent_ports(self.fdb_entries1):
    results[lvm] = agent_ports","results = {lvm: agent_ports for (lvm, agent_ports) in self.fakeagent.get_agent_ports(self.fdb_entries1)}","results = {lvm: agent_ports for (lvm, agent_ports) in self.fakeagent.get_agent_ports(self.fdb_entries1)}",1,,,
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/lib/ansible/galaxy/api.py,https://github.com/ansible/ansible/tree/master/lib/ansible/galaxy/api.py,GalaxyAPI,get_collection_metadata$732,"def get_collection_metadata(self, namespace, name):
    """"""
        Gets the collection information from the Galaxy server about a specific Collection.

        :param namespace: The collection namespace.
        :param name: The collection name.
        return: CollectionMetadata about the collection.
        """"""
    if 'v3' in self.available_api_versions:
        api_path = self.available_api_versions['v3']
        field_map = [('created_str', 'created_at'), ('modified_str', 'updated_at')]
    else:
        api_path = self.available_api_versions['v2']
        field_map = [('created_str', 'created'), ('modified_str', 'modified')]
    info_url = _urljoin(self.api_server, api_path, 'collections', namespace, name, '/')
    error_context_msg = 'Error when getting the collection info for %s.%s from %s (%s)' % (namespace, name, self.name, self.api_server)
    data = self._call_galaxy(info_url, error_context_msg=error_context_msg)
    metadata = {}
    for (name, api_field) in field_map:
        metadata[name] = data.get(api_field, None)
    return CollectionMetadata(namespace, name, **metadata)","for (name, api_field) in field_map:
    metadata[name] = data.get(api_field, None)","metadata = {name: data.get(api_field, None) for (name, api_field) in field_map}","metadata = {name: data.get(api_field, None) for (name, api_field) in field_map}",1,,,
moto,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/packages/httpretty/core.py,https://github.com/spulec/moto/tree/master/moto/packages/httpretty/core.py,HTTPrettyRequest,parse_querystring$205,"def parse_querystring(self, qs):
    expanded = unquote_utf8(qs)
    parsed = parse_qs(expanded)
    result = {}
    for k in parsed:
        result[k] = list(map(decode_utf8, parsed[k]))
    return result","for k in parsed:
    result[k] = list(map(decode_utf8, parsed[k]))","result = {k: list(map(decode_utf8, parsed[k])) for k in parsed}","result = {k: list(map(decode_utf8, parsed[k])) for k in parsed}",1,,,
nnabla,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nnabla/build-tools/code_formatter/copyright_checker.py,https://github.com/sony/nnabla/tree/master/build-tools/code_formatter/copyright_checker.py,,main$219,"def main(args):
    types = {'script': (['.py', '.cfg', '.ini', '.sh', '.mk', '.pyx', '.cmake', 'CMakeLists.txt', 'Dockerfile', '.pxd'], '#'), 'c': (['.c', '.cpp', '.h', '.hpp', '.cu', '.cuh', '.proto'], '//'), 'bat': (['.bat'], 'REM')}
    checkers = {}
    for (k, v) in types.items():
        checkers[k] = Checker(*v)
    files = list_up_files(args.rootdir, checkers)
    for (fn, c) in tqdm(files):
        if c.type == 'unknown':
            if not c.has_shebang():
                continue
        new_header = c.create_file_header(fn)
        if new_header is None:
            continue
        with c.read_file(str(fn)) as f:
            if f:
                old_header = c.extract_file_header()
                if new_header == old_header:
                    continue
                with open(str(fn), 'w', encoding='utf-8') as fh:
                    fh.write(c.replace_file_header(old_header, new_header))","for (k, v) in types.items():
    checkers[k] = Checker(*v)","checkers = {k: Checker(*v) for (k, v) in types.items()}","checkers = {k: Checker(*v) for (k, v) in types.items()}",1,,,
rqalpha,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rqalpha/rqalpha/utils/repr.py,https://github.com/ricequant/rqalpha/tree/master/rqalpha/utils/repr.py,,slots$84,"def slots(inst):
    result = {}
    for slot in inst.__slots__:
        result[slot] = getattr(inst, slot)
    return result","for slot in inst.__slots__:
    result[slot] = getattr(inst, slot)","result = {slot: getattr(inst, slot) for slot in inst.__slots__}","result = {slot: getattr(inst, slot) for slot in inst.__slots__}",1,,,
EasyClangComplete,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyClangComplete/plugin/clang/cindex39.py,https://github.com/niosus/EasyClangComplete/tree/master/plugin/clang/cindex39.py,RefQualifierKind,name$1988,"def name(self):
    """"""Get the enumeration name of this kind.""""""
    if self._name_map is None:
        self._name_map = {}
        for (key, value) in RefQualifierKind.__dict__.items():
            if isinstance(value, RefQualifierKind):
                self._name_map[value] = key
    return self._name_map[self]","for (key, value) in RefQualifierKind.__dict__.items():
    if isinstance(value, RefQualifierKind):
        self._name_map[value] = key","self._name_map = {value: key for (key, value) in RefQualifierKind.__dict__.items() if isinstance(value, RefQualifierKind)}","self._name_map = {value: key for (key, value) in RefQualifierKind.__dict__.items() if isinstance(value, RefQualifierKind)}",1,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/tools/filters/gff/gtf_filter_by_attribute_values_list.py,https://github.com/ansible/galaxy/tree/master/tools/filters/gff/gtf_filter_by_attribute_values_list.py,,gff_filter$49,"def gff_filter(gff_file, attribute_name, ids_file, output_file):
    ids_dict = {}
    for line in open(ids_file):
        ids_dict[line.split('\t')[0].strip()] = True
    with open(output_file, 'w') as output, open(gff_file) as ingff:
        for line in ingff:
            if not line or line.startswith('#'):
                output.write(line)
                continue
            fields = line.split('\t')
            attributes = parse_gff_attributes(fields[8])
            if attribute_name in attributes and attributes[attribute_name] in ids_dict:
                output.write(line)","for line in open(ids_file):
    ids_dict[line.split('\t')[0].strip()] = True",ids_dict = {line.split('\t')[0].strip(): True for line in open(ids_file)},ids_dict = {line.split('\t')[0].strip(): True for line in open(ids_file)},1,,,
intelmq,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/intelmq/intelmq/lib/pipeline.py,https://github.com/certtools/intelmq/tree/master/intelmq/lib/pipeline.py,Pythonlist,count_queued_messages$383,"def count_queued_messages(self, *queues) -> dict:
    """"""Returns the amount of queued messages
           over all given queue names.
        """"""
    if not self.state:
        self.set_queues(None, 'source')
        self.connect()
    qdict = {}
    for queue in queues:
        qdict[queue] = len(self.state.get(queue, []))
    return qdict","for queue in queues:
    qdict[queue] = len(self.state.get(queue, []))","qdict = {queue: len(self.state.get(queue, [])) for queue in queues}","qdict = {queue: len(self.state.get(queue, [])) for queue in queues}",1,,,
keystone,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keystone/keystone/identity/core.py,https://github.com/openstack/keystone/tree/master/keystone/identity/core.py,Manager,_set_domain_id_and_mapping_for_list$646,"def _set_domain_id_and_mapping_for_list(self, ref_list, domain_id, driver, entity_type, conf):
    """"""Set domain id and mapping for a list of refs.

        The method modifies refs in-place.
        """"""
    if not ref_list:
        return []
    if not domain_id:
        domain_id = CONF.identity.default_domain_id
    if not driver.is_domain_aware():
        for ref in ref_list:
            ref['domain_id'] = domain_id
    if not self._is_mapping_needed(driver):
        return ref_list
    refs_map = {}
    for r in ref_list:
        refs_map[r['id'], entity_type, r['domain_id']] = r
    domain_mappings = PROVIDERS.id_mapping_api.get_domain_mapping_list(domain_id, entity_type=entity_type)
    for _mapping in domain_mappings:
        idx = (_mapping.local_id, _mapping.entity_type, _mapping.domain_id)
        try:
            ref = refs_map.pop(idx)
            ref['id'] = _mapping.public_id
        except KeyError:
            pass
    for ref in refs_map.values():
        local_entity = {'domain_id': ref['domain_id'], 'local_id': ref['id'], 'entity_type': entity_type}
        self._insert_new_public_id(local_entity, ref, driver)
    return ref_list","for r in ref_list:
    refs_map[r['id'], entity_type, r['domain_id']] = r","refs_map = {(r['id'], entity_type, r['domain_id']): r for r in ref_list}","refs_map = {(r['id'], entity_type, r['domain_id']): r for r in ref_list}",1,,,
stocktalk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stocktalk/stocktalk/scripts/streaming.py,https://github.com/anfederico/stocktalk/tree/master/stocktalk/scripts/streaming.py,,get_reverse$22,"def get_reverse(queries):
    reverse = {}
    for query in queries:
        for keyword in queries[query]:
            reverse[keyword] = query
    return reverse","for query in queries:
    for keyword in queries[query]:
        reverse[keyword] = query",reverse = {keyword: query for query in queries for keyword in queries[query]},reverse = {keyword: query for query in queries for keyword in queries[query]},1,,,
mljar-supervised,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mljar-supervised/supervised/preprocessing/loo_encoder.py,https://github.com/mljar/mljar-supervised/tree/master/supervised/preprocessing/loo_encoder.py,LooEncoder,from_json$49,"def from_json(self, data_json):
    self.enc.cols = data_json.get('cols')
    self.enc._dim = data_json.get('dim')
    self.enc._mean = data_json.get('mean')
    self.enc.feature_names = data_json.get('feature_names')
    self.enc.mapping = {}
    for (k, v) in data_json.get('mapping', {}).items():
        self.enc.mapping[k] = pd.DataFrame(json.loads(v))","for (k, v) in data_json.get('mapping', {}).items():
    self.enc.mapping[k] = pd.DataFrame(json.loads(v))","self.enc.mapping = {k: pd.DataFrame(json.loads(v)) for (k, v) in data_json.get('mapping', {}).items()}","self.enc.mapping = {k: pd.DataFrame(json.loads(v)) for (k, v) in data_json.get('mapping', {}).items()}",1,,,
kafka-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kafka-python/test/test_assignors.py,https://github.com/dpkp/kafka-python/tree/master/test/test_assignors.py,,make_member_metadata$788,"def make_member_metadata(subscriptions):
    member_metadata = {}
    for (member, topics) in six.iteritems(subscriptions):
        member_metadata[member] = StickyPartitionAssignor._metadata(topics, [])
    return member_metadata","for (member, topics) in six.iteritems(subscriptions):
    member_metadata[member] = StickyPartitionAssignor._metadata(topics, [])","member_metadata = {member: StickyPartitionAssignor._metadata(topics, []) for (member, topics) in six.iteritems(subscriptions)}","member_metadata = {member: StickyPartitionAssignor._metadata(topics, []) for (member, topics) in six.iteritems(subscriptions)}",1,,,
spaCy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spaCy/spacy/cli/project/remote_storage.py,https://github.com/explosion/spaCy/tree/master/spacy/cli/project/remote_storage.py,,get_env_hash$163,"def get_env_hash(env: Dict[str, str]) -> str:
    """"""Construct a hash of the environment variables that will be passed into
    the commands.

    Values in the env dict may be references to the current os.environ, using
    the syntax $ENV_VAR to mean os.environ[ENV_VAR]
    """"""
    env_vars = {}
    for (key, value) in env.items():
        if value.startswith('$'):
            env_vars[key] = os.environ.get(value[1:], '')
        else:
            env_vars[key] = value
    return get_hash(env_vars)","for (key, value) in env.items():
    if value.startswith('$'):
        env_vars[key] = os.environ.get(value[1:], '')
    else:
        env_vars[key] = value","env_vars = {key: os.environ.get(value[1:], '') if value.startswith('$') else value for (key, value) in env.items()}","env_vars = {key: os.environ.get(value[1:], '') if value.startswith('$') else value for (key, value) in env.items()}",1,,,
omegaconf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/omegaconf/tests/test_basic_ops_dict.py,https://github.com/omry/omegaconf/tree/master/tests/test_basic_ops_dict.py,,test_iterate_dictionary$418,"def test_iterate_dictionary() -> None:
    c = OmegaConf.create({'a': 1, 'b': 2})
    m2 = {}
    for key in c:
        m2[key] = c[key]
    assert m2 == c","for key in c:
    m2[key] = c[key]",m2 = {key: c[key] for key in c},m2 = {key: c[key] for key in c},1,,,
FairMOT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FairMOT/src/lib/models/networks/pose_dla_conv.py,https://github.com/ifzhang/FairMOT/tree/master/src/lib/models/networks/pose_dla_conv.py,DLASeg,forward$470,"def forward(self, x):
    x = self.base(x)
    x = self.dla_up(x)
    y = []
    for i in range(self.last_level - self.first_level):
        y.append(x[i].clone())
    self.ida_up(y, 0, len(y))
    z = {}
    for head in self.heads:
        z[head] = self.__getattr__(head)(y[-1])
    return [z]","for head in self.heads:
    z[head] = self.__getattr__(head)(y[-1])",z = {head: self.__getattr__(head)(y[-1]) for head in self.heads},z = {head: self.__getattr__(head)(y[-1]) for head in self.heads},1,,,
PaddleClas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleClas/ppcls/metric/metrics.py,https://github.com/PaddlePaddle/PaddleClas/tree/master/ppcls/metric/metrics.py,Precisionk,forward$276,"def forward(self, similarities_matrix, query_img_id, gallery_img_id, keep_mask):
    metric_dict = dict()
    choosen_indices = paddle.argsort(similarities_matrix, axis=1, descending=self.descending)
    gallery_labels_transpose = paddle.transpose(gallery_img_id, [1, 0])
    gallery_labels_transpose = paddle.broadcast_to(gallery_labels_transpose, shape=[choosen_indices.shape[0], gallery_labels_transpose.shape[1]])
    choosen_label = paddle.index_sample(gallery_labels_transpose, choosen_indices)
    equal_flag = paddle.equal(choosen_label, query_img_id)
    if keep_mask is not None:
        keep_mask = paddle.index_sample(keep_mask.astype('float32'), choosen_indices)
        equal_flag = paddle.logical_and(equal_flag, keep_mask.astype('bool'))
    equal_flag = paddle.cast(equal_flag, 'float32')
    Ns = paddle.arange(gallery_img_id.shape[0]) + 1
    equal_flag_cumsum = paddle.cumsum(equal_flag, axis=1)
    Precision_at_k = (paddle.mean(equal_flag_cumsum, axis=0) / Ns).numpy()
    for k in self.topk:
        metric_dict['precision@{}'.format(k)] = Precision_at_k[k - 1]
    return metric_dict","for k in self.topk:
    metric_dict['precision@{}'.format(k)] = Precision_at_k[k - 1]",metric_dict = {'precision@{}'.format(k): Precision_at_k[k - 1] for k in self.topk},metric_dict = {'precision@{}'.format(k): Precision_at_k[k - 1] for k in self.topk},1,,,
devpi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/devpi/server/devpi_server/model.py,https://github.com/devpi/devpi/tree/master/server/devpi_server/model.py,LinkStore,metadata$1564,"def metadata(self):
    metadata = {}
    for (k, v) in get_mutable_deepcopy(self.verdata).items():
        if not k.startswith('+'):
            metadata[k] = v
    return metadata","for (k, v) in get_mutable_deepcopy(self.verdata).items():
    if not k.startswith('+'):
        metadata[k] = v","metadata = {k: v for (k, v) in get_mutable_deepcopy(self.verdata).items() if not k.startswith('+')}","metadata = {k: v for (k, v) in get_mutable_deepcopy(self.verdata).items() if not k.startswith('+')}",1,,,
healthchecks,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/healthchecks/hc/api/views.py,https://github.com/healthchecks/healthchecks/tree/master/hc/api/views.py,,badges$494,"def badges(request):
    tags = set(['*'])
    for check in Check.objects.filter(project=request.project):
        tags.update(check.tags_list())
    key = request.project.badge_key
    badges = {}
    for tag in tags:
        badges[tag] = {'svg': get_badge_url(key, tag), 'svg3': get_badge_url(key, tag, with_late=True), 'json': get_badge_url(key, tag, fmt='json'), 'json3': get_badge_url(key, tag, fmt='json', with_late=True), 'shields': get_badge_url(key, tag, fmt='shields'), 'shields3': get_badge_url(key, tag, fmt='shields', with_late=True)}
    return JsonResponse({'badges': badges})","for tag in tags:
    badges[tag] = {'svg': get_badge_url(key, tag), 'svg3': get_badge_url(key, tag, with_late=True), 'json': get_badge_url(key, tag, fmt='json'), 'json3': get_badge_url(key, tag, fmt='json', with_late=True), 'shields': get_badge_url(key, tag, fmt='shields'), 'shields3': get_badge_url(key, tag, fmt='shields', with_late=True)}","badges = {tag: {'svg': get_badge_url(key, tag), 'svg3': get_badge_url(key, tag, with_late=True), 'json': get_badge_url(key, tag, fmt='json'), 'json3': get_badge_url(key, tag, fmt='json', with_late=True), 'shields': get_badge_url(key, tag, fmt='shields'), 'shields3': get_badge_url(key, tag, fmt='shields', with_late=True)} for tag in tags}","badges = {tag: {'svg': get_badge_url(key, tag), 'svg3': get_badge_url(key, tag, with_late=True), 'json': get_badge_url(key, tag, fmt='json'), 'json3': get_badge_url(key, tag, fmt='json', with_late=True), 'shields': get_badge_url(key, tag, fmt='shields'), 'shields3': get_badge_url(key, tag, fmt='shields', with_late=True)} for tag in tags}",1,,,
mars,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mars/mars/core/operand/base.py,https://github.com/mars-project/mars/tree/master/mars/core/operand/base.py,Operand,_extract_scheduling_hint$157,"def _extract_scheduling_hint(cls, kwargs: Dict[str, Any]):
    if 'scheduling_hint' in kwargs:
        return
    scheduling_hint_kwargs = dict()
    for hint_name in SchedulingHint.all_hint_names:
        if hint_name in kwargs:
            scheduling_hint_kwargs[hint_name] = kwargs.pop(hint_name)
    if scheduling_hint_kwargs:
        kwargs['scheduling_hint'] = SchedulingHint(**scheduling_hint_kwargs)","for hint_name in SchedulingHint.all_hint_names:
    if hint_name in kwargs:
        scheduling_hint_kwargs[hint_name] = kwargs.pop(hint_name)",scheduling_hint_kwargs = {hint_name: kwargs.pop(hint_name) for hint_name in SchedulingHint.all_hint_names if hint_name in kwargs},scheduling_hint_kwargs = {hint_name: kwargs.pop(hint_name) for hint_name in SchedulingHint.all_hint_names if hint_name in kwargs},1,,,
xarray,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xarray/xarray/core/merge.py,https://github.com/pydata/xarray/tree/master/xarray/core/merge.py,,dataset_update_method$961,"def dataset_update_method(dataset: 'Dataset', other: 'CoercibleMapping') -> _MergeResult:
    """"""Guts of the Dataset.update method.

    This drops a duplicated coordinates from `other` if `other` is not an
    `xarray.Dataset`, e.g., if it's a dict with DataArray values (GH2068,
    GH2180).
    """"""
    from .dataarray import DataArray
    from .dataset import Dataset
    if not isinstance(other, Dataset):
        other = dict(other)
        for (key, value) in other.items():
            if isinstance(value, DataArray):
                coord_names = [c for c in value.coords if c not in value.dims and c in dataset.coords]
                if coord_names:
                    other[key] = value.drop_vars(coord_names)
    indexes = {}
    for (key, index) in dataset.xindexes.items():
        if isinstance(index, PandasIndex):
            indexes[key] = dataset.coords[key]
        else:
            indexes[key] = index
    return merge_core([dataset, other], priority_arg=1, indexes=indexes, combine_attrs='override')","for (key, index) in dataset.xindexes.items():
    if isinstance(index, PandasIndex):
        indexes[key] = dataset.coords[key]
    else:
        indexes[key] = index",XXX,no_found,0,,,
mars,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mars/mars/dataframe/plotting/core.py,https://github.com/mars-project/mars/tree/master/mars/dataframe/plotting/core.py,PlotAccessor,__call__$27,"def __call__(self, kind='line', session=None, **kwargs):
    to_executes = OrderedDict()
    to_executes['__object__'] = self._obj
    for (k, v) in kwargs.items():
        if isinstance(v, ENTITY_TYPE):
            to_executes[k] = v
    result = dict()
    executed = ExecutableTuple(to_executes.values()).execute().fetch()
    for (p, v) in zip(to_executes, executed):
        result[p] = v
    data = result.pop('__object__')
    pd_kwargs = kwargs.copy()
    pd_kwargs['kind'] = kind
    pd_kwargs.update(result)
    return data.plot(**pd_kwargs)","for (p, v) in zip(to_executes, executed):
    result[p] = v",XXX,no_found,0,,,
rlkit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rlkit/rlkit/data_management/simple_replay_buffer.py,https://github.com/rail-berkeley/rlkit/tree/master/rlkit/data_management/simple_replay_buffer.py,SimpleReplayBuffer,__init__$11,"def __init__(self, max_replay_buffer_size, observation_dim, action_dim, env_info_sizes, replace=True):
    self._observation_dim = observation_dim
    self._action_dim = action_dim
    self._max_replay_buffer_size = max_replay_buffer_size
    self._observations = np.zeros((max_replay_buffer_size, observation_dim))
    self._next_obs = np.zeros((max_replay_buffer_size, observation_dim))
    self._actions = np.zeros((max_replay_buffer_size, action_dim))
    self._rewards = np.zeros((max_replay_buffer_size, 1))
    self._terminals = np.zeros((max_replay_buffer_size, 1), dtype='uint8')
    self._env_infos = {}
    for (key, size) in env_info_sizes.items():
        self._env_infos[key] = np.zeros((max_replay_buffer_size, size))
    self._env_info_keys = list(env_info_sizes.keys())
    self._replace = replace
    self._top = 0
    self._size = 0","for (key, size) in env_info_sizes.items():
    self._env_infos[key] = np.zeros((max_replay_buffer_size, size))",XXX,no_found,0,,,
torchdrug,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torchdrug/torchdrug/core/core.py,https://github.com/DeepGraphLearning/torchdrug/tree/master/torchdrug/core/core.py,_Configurable,__new__$271,"def __new__(typ, *args, **kwargs):
    cls = type.__new__(typ, *args, **kwargs)

    @decorator
    def wrapper(init, self, *args, **kwargs):
        sig = inspect.signature(init)
        func = sig.bind(self, *args, **kwargs)
        func.apply_defaults()
        config = {}
        keys = list(sig.parameters.keys())
        for (k, v) in zip(keys[1:], func.args[1:]):
            config[k] = v
        config.update(func.kwargs)
        for k in getattr(self, '_ignore_args', {}):
            config.pop(k)
        self._config = dict(config)
        return init(self, *args, **kwargs)

    def get_function(method):
        if isinstance(method, types.MethodType):
            return method.__func__
        return method
    if isinstance(cls.__init__, types.FunctionType):
        cls.__init__ = wrapper(cls.__init__)
        custom_load_func = hasattr(cls, 'load_config_dict') and get_function(cls.load_config_dict) != get_function(typ.load_config_dict)
        custom_config_func = hasattr(cls, 'config_dict') and get_function(cls.config_dict) != get_function(typ.config_dict)
        if not custom_load_func:
            cls.load_config_dict = _Configurable.load_config_dict
        if not custom_config_func:
            cls.config_dict = _Configurable.config_dict
    return cls","for (k, v) in zip(keys[1:], func.args[1:]):
    config[k] = v",XXX,no_found,0,,,
networkx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/networkx/networkx/algorithms/bipartite/matching.py,https://github.com/networkx/networkx/tree/master/networkx/algorithms/bipartite/matching.py,,eppstein_matching$182,"def eppstein_matching(G, top_nodes=None):
    """"""Returns the maximum cardinality matching of the bipartite graph `G`.

    Parameters
    ----------
    G : NetworkX graph

      Undirected bipartite graph

    top_nodes : container

      Container with all nodes in one bipartite node set. If not supplied
      it will be computed. But if more than one solution exists an exception
      will be raised.

    Returns
    -------
    matches : dictionary

      The matching is returned as a dictionary, `matching`, such that
      ``matching[v] == w`` if node `v` is matched to node `w`. Unmatched
      nodes do not occur as a key in `matching`.

    Raises
    ------
    AmbiguousSolution
      Raised if the input bipartite graph is disconnected and no container
      with all nodes in one bipartite set is provided. When determining
      the nodes in each bipartite set more than one valid solution is
      possible if the input graph is disconnected.

    Notes
    -----
    This function is implemented with David Eppstein's version of the algorithm
    Hopcroft--Karp algorithm (see :func:`hopcroft_karp_matching`), which
    originally appeared in the `Python Algorithms and Data Structures library
    (PADS) <http://www.ics.uci.edu/~eppstein/PADS/ABOUT-PADS.txt>`_.

    See :mod:`bipartite documentation <networkx.algorithms.bipartite>`
    for further details on how bipartite graphs are handled in NetworkX.

    See Also
    --------

    hopcroft_karp_matching

    """"""
    (left, right) = bipartite_sets(G, top_nodes)
    G = nx.DiGraph(G.edges(left))
    matching = {}
    for u in G:
        for v in G[u]:
            if v not in matching:
                matching[v] = u
                break
    while True:
        preds = {}
        unmatched = []
        pred = {u: unmatched for u in G}
        for v in matching:
            del pred[matching[v]]
        layer = list(pred)
        while layer and (not unmatched):
            newLayer = {}
            for u in layer:
                for v in G[u]:
                    if v not in preds:
                        newLayer.setdefault(v, []).append(u)
            layer = []
            for v in newLayer:
                preds[v] = newLayer[v]
                if v in matching:
                    layer.append(matching[v])
                    pred[matching[v]] = v
                else:
                    unmatched.append(v)
        if not unmatched:
            unlayered = {}
            for u in G:
                for v in G[u]:
                    if v not in preds:
                        unlayered[v] = None
            for key in matching.copy():
                matching[matching[key]] = key
            return matching

        def recurse(v):
            if v in preds:
                L = preds.pop(v)
                for u in L:
                    if u in pred:
                        pu = pred.pop(u)
                        if pu is unmatched or recurse(pu):
                            matching[v] = u
                            return True
            return False
        for v in unmatched:
            recurse(v)","for u in G:
    for v in G[u]:
        if v not in preds:
            unlayered[v] = None",XXX,no_found,0,,,
unilm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unilm/unilm-v1/src/biunilm/loader_utils.py,https://github.com/microsoft/unilm/tree/master/unilm-v1/src/biunilm/loader_utils.py,Pipeline,get_masked_pos$146,"def get_masked_pos(self, tokens, n_pred, add_skipgram=False, mask_segment=None, protect_range=None):
    if self.pieces_dir is not None and self.trie is None:
        self.create_trie_tree(self.pieces_dir)
    if self.pre_whole_word:
        if self.trie is not None:
            pieces = self.trie.get_pieces(tokens, 0)
            new_pieces = []
            for piece in pieces:
                if len(new_pieces) > 0 and tokens[piece[0]].startswith('##'):
                    new_pieces[-1].extend(piece)
                else:
                    new_pieces.append(piece)
            del pieces
            pieces = new_pieces
            pre_word_split = list((_[-1] for _ in pieces))
            pre_word_split.append(len(tokens))
        else:
            pre_word_split = _get_word_split_index(tokens, 0, len(tokens))
        index2piece = None
    else:
        pre_word_split = list(range(0, len(tokens) + 1))
        if self.trie is not None:
            pieces = self.trie.get_pieces(tokens, 0)
            index2piece = {}
            for piece in pieces:
                for index in piece:
                    index2piece[index] = (piece[0], piece[-1])
        else:
            index2piece = None
    span_list = list(zip(pre_word_split[:-1], pre_word_split[1:]))
    cand_pos = []
    special_pos = set()
    if mask_segment:
        for (i, sp) in enumerate(span_list):
            (sp_st, sp_end) = sp
            if sp_end - sp_st == 1 and tokens[sp_st].endswith('SEP]'):
                segment_index = i
                break
    for (i, sp) in enumerate(span_list):
        (sp_st, sp_end) = sp
        if sp_end - sp_st == 1 and (tokens[sp_st].endswith('CLS]') or tokens[sp_st].endswith('SEP]')):
            special_pos.add(i)
        elif mask_segment:
            if i < segment_index and 'a' in mask_segment or (i > segment_index and 'b' in mask_segment):
                cand_pos.append(i)
        else:
            cand_pos.append(i)
    shuffle(cand_pos)
    masked_pos = set()
    for i_span in cand_pos:
        if len(masked_pos) >= n_pred:
            break
        (cand_st, cand_end) = span_list[i_span]
        if len(masked_pos) + cand_end - cand_st > n_pred:
            continue
        if any((p in masked_pos for p in range(cand_st, cand_end))):
            continue
        n_span = 1
        if index2piece is not None:
            (p_start, p_end) = index2piece[i_span]
            if p_start < p_end and rand() < self.sp_prob:
                (st_span, end_span) = (p_start, p_end + 1)
            else:
                (st_span, end_span) = (i_span, i_span + 1)
        else:
            rand_skipgram_size = 0
            if self.skipgram_size_geo_list:
                rand_skipgram_size = np.random.choice(len(self.skipgram_size_geo_list), 1, p=self.skipgram_size_geo_list)[0] + 1
            elif add_skipgram and self.skipgram_prb > 0 and (self.skipgram_size >= 2) and (rand() < self.skipgram_prb):
                rand_skipgram_size = min(randint(2, self.skipgram_size), len(span_list) - i_span)
            for n in range(2, rand_skipgram_size + 1):
                (tail_st, tail_end) = span_list[i_span + n - 1]
                if tail_end - tail_st == 1 and tail_st in special_pos:
                    break
                if len(masked_pos) + tail_end - cand_st > n_pred:
                    break
                n_span = n
            (st_span, end_span) = (i_span, i_span + n_span)
        if self.mask_whole_word:
            (st_span, end_span) = _expand_whole_word(tokens, st_span, end_span)
        if self.word_subsample_prb:
            skip_pos = set()
            if self.pre_whole_word:
                w_span_list = span_list[st_span:end_span]
            else:
                split_idx = _get_word_split_index(tokens, st_span, end_span)
                w_span_list = list(zip(split_idx[:-1], split_idx[1:]))
            for (i, sp) in enumerate(w_span_list):
                (sp_st, sp_end) = sp
                if sp_end - sp_st == 1:
                    w_cat = tokens[sp_st]
                else:
                    w_cat = ''.join(tokens[sp_st:sp_end])
                if w_cat in self.word_subsample_prb and rand() < self.word_subsample_prb[w_cat]:
                    for k in range(sp_st, sp_end):
                        skip_pos.add(k)
        else:
            skip_pos = None
        for sp in range(st_span, end_span):
            for mp in range(span_list[sp][0], span_list[sp][1]):
                if not (skip_pos and mp in skip_pos) and mp not in special_pos and (not (protect_range and protect_range[0] <= mp < protect_range[1])):
                    masked_pos.add(mp)
    if len(masked_pos) < n_pred:
        shuffle(cand_pos)
        for pos in cand_pos:
            if len(masked_pos) >= n_pred:
                break
            if pos not in masked_pos:
                masked_pos.add(pos)
    masked_pos = list(masked_pos)
    if len(masked_pos) > n_pred:
        masked_pos = masked_pos[:n_pred]
    return masked_pos","for piece in pieces:
    for index in piece:
        index2piece[index] = (piece[0], piece[-1])",XXX,no_found,0,,,
deluge,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deluge/deluge/plugins/Stats/deluge_stats/core.py,https://github.com/deluge-torrent/deluge/tree/master/deluge/plugins/Stats/deluge_stats/core.py,Core,update_stats$122,"def update_stats(self):
    stats = {}
    raw_stats = self.core.get_session_status(self.stat_keys)
    for (name, fn) in self.stat_getters.items():
        stats[name] = fn(raw_stats)
    update_time = time.time()
    self.last_update[1] = update_time
    for (stat, stat_list) in self.stats[1].items():
        if stat in stats:
            stat_list.insert(0, int(stats[stat]))
        else:
            stat_list.insert(0, 0)
        if len(stat_list) > self.length:
            stat_list.pop()

    def update_interval(interval, base, multiplier):
        self.count[interval] = self.count[interval] + 1
        if self.count[interval] >= interval:
            self.last_update[interval] = update_time
            self.count[interval] = 0
            current_stats = self.stats[interval]
            for (stat, stat_list) in self.stats[base].items():
                try:
                    avg = mean(stat_list[0:multiplier])
                except ValueError:
                    avg = 0
                current_stats[stat].insert(0, avg)
                if len(current_stats[stat]) > self.length:
                    current_stats[stat].pop()
    update_interval(5, 1, 5)
    update_interval(30, 5, 6)
    update_interval(300, 30, 10)","for (name, fn) in self.stat_getters.items():
    stats[name] = fn(raw_stats)",XXX,no_found,,,,
readthedocs.org,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/readthedocs.org/readthedocs/projects/models.py,https://github.com/readthedocs/readthedocs.org/tree/master/readthedocs/projects/models.py,Project,get_downloads$776,"def get_downloads(self):
    downloads = {}
    default_version = self.get_default_version()
    for type_ in ('htmlzip', 'epub', 'pdf'):
        downloads[type_] = self.get_production_media_url(type_, default_version)
    return downloads","for type_ in ('htmlzip', 'epub', 'pdf'):
    downloads[type_] = self.get_production_media_url(type_, default_version)",XXX,no_found,,,,
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/swift/common/ring/composite_builder.py,https://github.com/openstack/swift/tree/master/swift/common/ring/composite_builder.py,,pre_validate_all_builders$109,"def pre_validate_all_builders(builders):
    """"""
    Pre-validation for all component ring builders that are to be included in
    the composite ring. Checks that all component rings are valid with respect
    to each other.

    :param builders: a list of :class:`swift.common.ring.builder.RingBuilder`
        instances
    :raises ValueError: if the builders are invalid with respect to each other
    """"""
    if len(builders) < 2:
        raise ValueError('Two or more component builders are required.')
    for attr in MUST_MATCH_ATTRS:
        attr_dict = defaultdict(list)
        for (i, builder) in enumerate(builders):
            value = getattr(builder, attr, None)
            attr_dict[value].append(i)
        if len(attr_dict) > 1:
            variations = ['%s=%s found at indexes %s' % (attr, val, indexes) for (val, indexes) in attr_dict.items()]
            raise ValueError('All builders must have same value for %r.\n%s' % (attr, '\n  '.join(variations)))
    errors = []
    for (index, builder) in enumerate(builders):
        if int(builder.replicas) != builder.replicas:
            errors.append('Non integer replica count %s found at index %s' % (builder.replicas, index))
        if builder.devs_changed:
            errors.append('Builder needs rebalance to apply changes at index %s' % index)
    if errors:
        raise ValueError('Problem with builders.\n%s' % '\n  '.join(errors))
    regions_info = {}
    for builder in builders:
        regions_info[builder] = set((dev['region'] for dev in builder._iter_devs()))
    for (first_region_set, second_region_set) in combinations(regions_info.values(), 2):
        inter = first_region_set & second_region_set
        if inter:
            raise ValueError('Same region found in different rings')
    check_for_dev_uniqueness(builders)","for builder in builders:
    regions_info[builder] = set((dev['region'] for dev in builder._iter_devs()))",XXX,no_found,0,,,
CenterPoint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CenterPoint/det3d/utils/utils.py,https://github.com/tianweiy/CenterPoint/tree/master/det3d/utils/utils.py,,example_to_device$5,"def example_to_device(example, dtype=torch.float32, device=None, non_blocking=True) -> dict:
    device = device or torch.device('cuda:0')
    example_torch = {}
    float_names = ['voxels', 'bev_map']
    for (k, v) in example.items():
        if k in ['anchors', 'reg_targets', 'reg_weights', 'labels', 'anchors_mask']:
            res = []
            for (kk, vv) in v.items():
                vv = [vvv.unsqueeze_(0) for vvv in vv]
                res.append(torch.cat(vv, dim=0).cuda(device, non_blocking=non_blocking))
            example_torch[k] = res
        elif k in ['voxels', 'bev_map', 'coordinates', 'num_points', 'points', 'num_voxels']:
            example_torch[k] = v.cuda(device, non_blocking=non_blocking)
        elif k == 'calib':
            calib = {}
            for (k1, v1) in v.items():
                calib[k1] = v1.cuda(device, non_blocking=non_blocking)
            example_torch[k] = calib
        else:
            example_torch[k] = v
    return example_torch","for (k, v) in example.items():
    if k in ['anchors', 'reg_targets', 'reg_weights', 'labels', 'anchors_mask']:
        res = []
        for (kk, vv) in v.items():
            vv = [vvv.unsqueeze_(0) for vvv in vv]
            res.append(torch.cat(vv, dim=0).cuda(device, non_blocking=non_blocking))
        example_torch[k] = res
    elif k in ['voxels', 'bev_map', 'coordinates', 'num_points', 'points', 'num_voxels']:
        example_torch[k] = v.cuda(device, non_blocking=non_blocking)
    elif k == 'calib':
        calib = {}
        for (k1, v1) in v.items():
            calib[k1] = v1.cuda(device, non_blocking=non_blocking)
        example_torch[k] = calib
    else:
        example_torch[k] = v",XXX,no_found,,,,
CenterPoint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CenterPoint/det3d/utils/utils.py,https://github.com/tianweiy/CenterPoint/tree/master/det3d/utils/utils.py,,example_to_device$5,"def example_to_device(example, dtype=torch.float32, device=None, non_blocking=True) -> dict:
    device = device or torch.device('cuda:0')
    example_torch = {}
    float_names = ['voxels', 'bev_map']
    for (k, v) in example.items():
        if k in ['anchors', 'reg_targets', 'reg_weights', 'labels', 'anchors_mask']:
            res = []
            for (kk, vv) in v.items():
                vv = [vvv.unsqueeze_(0) for vvv in vv]
                res.append(torch.cat(vv, dim=0).cuda(device, non_blocking=non_blocking))
            example_torch[k] = res
        elif k in ['voxels', 'bev_map', 'coordinates', 'num_points', 'points', 'num_voxels']:
            example_torch[k] = v.cuda(device, non_blocking=non_blocking)
        elif k == 'calib':
            calib = {}
            for (k1, v1) in v.items():
                calib[k1] = v1.cuda(device, non_blocking=non_blocking)
            example_torch[k] = calib
        else:
            example_torch[k] = v
    return example_torch","for (k1, v1) in v.items():
    calib[k1] = v1.cuda(device, non_blocking=non_blocking)",XXX,no_found,0,,,
OpenSfM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenSfM/opensfm/stats.py,https://github.com/mapillary/OpenSfM/tree/master/opensfm/stats.py,,save_residual_grids$858,"def save_residual_grids(data: DataSetBase, tracks_manager: pymap.TracksManager, reconstructions: List[types.Reconstruction], output_path: str, io_handler: io.IoFilesystemBase) -> None:
    all_errors = {}
    scaling = 4
    for rec in reconstructions:
        for camera_id in rec.cameras:
            all_errors[camera_id] = []
    for i in range(len(reconstructions)):
        valid_observations = _get_valid_observations(reconstructions, tracks_manager)(i)
        errors_scaled = _compute_errors(reconstructions, tracks_manager)(i, pymap.ErrorType.Normalized)
        errors_unscaled = _compute_errors(reconstructions, tracks_manager)(i, pymap.ErrorType.Pixel)
        for (shot_id, shot_errors) in errors_scaled.items():
            shot = reconstructions[i].get_shot(shot_id)
            w = shot.camera.width
            h = shot.camera.height
            center = np.array([w / 2.0, h / 2.0])
            normalizer = max(shot.camera.width, shot.camera.height)
            (buckets_x, buckets_y) = _grid_buckets(shot.camera)
            w_bucket = buckets_x / w
            h_bucket = buckets_y / h
            shots_errors = []
            for (error_scaled, error_unscaled, observation) in zip(shot_errors.values(), errors_unscaled[shot_id].values(), valid_observations[shot_id].values()):
                if _norm2d(error_unscaled * normalizer) > RESIDUAL_PIXEL_CUTOFF:
                    continue
                bucket = observation.point * normalizer + center
                x = max([0, min([int(bucket[0] * w_bucket), buckets_x - 1])])
                y = max([0, min([int(bucket[1] * h_bucket), buckets_y - 1])])
                shots_errors.append((x, y, error_scaled))
            all_errors[shot.camera.id] += shots_errors
    for (camera_id, errors) in all_errors.items():
        if not errors:
            continue
        (buckets_x, buckets_y) = _grid_buckets(rec.cameras[camera_id])
        camera_array_res = np.zeros((buckets_y, buckets_x, 2))
        camera_array_count = np.full((buckets_y, buckets_x, 1), 1)
        for (x, y, e) in errors:
            camera_array_res[y, x] += e
            camera_array_count[y, x, 0] += 1
        camera_array_res = np.divide(camera_array_res, camera_array_count)
        camera = rec.get_camera(camera_id)
        (w, h) = (camera.width, camera.height)
        normalizer = max(w, h)
        clamp = 0.1
        res_colors = np.linalg.norm(camera_array_res[:, :, :2], axis=2)
        lowest = np.percentile(res_colors, 0)
        highest = np.percentile(res_colors, 100 * (1 - clamp))
        np.clip(res_colors, lowest, highest, res_colors)
        res_colors /= highest - lowest
        plt.clf()
        plt.figure(figsize=(12, 10))
        Q = plt.quiver(camera_array_res[:, :, 0] * scaling, camera_array_res[:, :, 1] * scaling, res_colors, units='xy', angles='xy', scale_units='xy', scale=1, width=0.1, cmap='viridis_r')
        scale = highest - lowest
        plt.quiverkey(Q, X=0.1, Y=1.04, U=scale * scaling, label=f'Residual grid scale : {scale:.2f}', labelpos='E')
        plt.title('                      ', fontsize='large')
        norm = colors.Normalize(vmin=lowest, vmax=highest)
        cmap = cm.get_cmap('viridis_r')
        sm = cm.ScalarMappable(norm=norm, cmap=cmap)
        sm.set_array([])
        plt.colorbar(mappable=sm, orientation='horizontal', label='Residual Norm', pad=0.08, aspect=40)
        plt.xticks([0, buckets_x / 2, buckets_x], [0, int(w / 2), w], fontsize='x-small')
        plt.yticks([0, buckets_y / 2, buckets_y], [0, int(h / 2), h], fontsize='x-small')
        with io_handler.open(os.path.join(output_path, 'residuals_' + str(camera_id.replace('/', '_')) + '.png'), 'wb') as fwb:
            plt.savefig(fwb, dpi=300, bbox_inches='tight')","for rec in reconstructions:
    for camera_id in rec.cameras:
        all_errors[camera_id] = []",XXX,no_found,,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/xbpspkg.py,https://github.com/saltstack/salt/tree/master/salt/modules/xbpspkg.py,,latest_version$161,"def latest_version(*names, **kwargs):
    """"""
    Return the latest version of the named package available for upgrade or
    installation. If more than one package name is specified, a dict of
    name/version pairs is returned.

    If the latest version of a given package is already installed, an empty
    string will be returned for that package.

    CLI Example:

    .. code-block:: bash

        salt '*' pkg.latest_version <package name>
        salt '*' pkg.latest_version <package1> <package2> <package3> ...
    """"""
    refresh = salt.utils.data.is_true(kwargs.pop('refresh', True))
    if len(names) == 0:
        return ''
    if refresh:
        refresh_db()
    ret = {}
    for name in names:
        ret[name] = ''
    cmd = ['xbps-install', '-un']
    cmd.extend(names)
    out = __salt__['cmd.run'](cmd, ignore_retcode=True, output_loglevel='trace')
    for line in out.splitlines():
        if not line:
            continue
        if line.find(' is up to date.') != -1:
            continue
        try:
            (pkg, ver) = line.split()[0].rsplit('-', 1)
        except (ValueError, IndexError):
            log.error('xbps-query: Unexpected formatting in line: ""%s""', line)
            continue
        log.trace('pkg=%s version=%s', pkg, ver)
        if pkg in names:
            ret[pkg] = ver
    if len(names) == 1:
        return ret[names[0]]
    return ret","for line in out.splitlines():
    if not line:
        continue
    if line.find(' is up to date.') != -1:
        continue
    try:
        (pkg, ver) = line.split()[0].rsplit('-', 1)
    except (ValueError, IndexError):
        log.error('xbps-query: Unexpected formatting in line: ""%s""', line)
        continue
    log.trace('pkg=%s version=%s', pkg, ver)
    if pkg in names:
        ret[pkg] = ver",XXX,no_found,0,,,
autokeras,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/autokeras/autokeras/analysers/input_analysers.py,https://github.com/keras-team/autokeras/tree/master/autokeras/analysers/input_analysers.py,StructuredDataAnalyser,infer_column_types$137,"def infer_column_types(self):
    column_types = {}
    for i in range(self.num_col):
        if self.count_categorical[i] > 0:
            column_types[self.column_names[i]] = CATEGORICAL
        elif len(self.count_unique_numerical[i]) / self.count_numerical[i] < 0.05:
            column_types[self.column_names[i]] = CATEGORICAL
        else:
            column_types[self.column_names[i]] = NUMERICAL
    if self.column_types is None:
        self.column_types = {}
    for (key, value) in column_types.items():
        if key not in self.column_types:
            self.column_types[key] = value","for i in range(self.num_col):
    if self.count_categorical[i] > 0:
        column_types[self.column_names[i]] = CATEGORICAL
    elif len(self.count_unique_numerical[i]) / self.count_numerical[i] < 0.05:
        column_types[self.column_names[i]] = CATEGORICAL
    else:
        column_types[self.column_names[i]] = NUMERICAL",XXX,no_found,,,,
autokeras,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/autokeras/autokeras/analysers/input_analysers.py,https://github.com/keras-team/autokeras/tree/master/autokeras/analysers/input_analysers.py,StructuredDataAnalyser,infer_column_types$137,"def infer_column_types(self):
    column_types = {}
    for i in range(self.num_col):
        if self.count_categorical[i] > 0:
            column_types[self.column_names[i]] = CATEGORICAL
        elif len(self.count_unique_numerical[i]) / self.count_numerical[i] < 0.05:
            column_types[self.column_names[i]] = CATEGORICAL
        else:
            column_types[self.column_names[i]] = NUMERICAL
    if self.column_types is None:
        self.column_types = {}
    for (key, value) in column_types.items():
        if key not in self.column_types:
            self.column_types[key] = value","for (key, value) in column_types.items():
    if key not in self.column_types:
        self.column_types[key] = value",XXX,no_found,0,,,
sqlalchemy-mixins,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sqlalchemy-mixins/sqlalchemy_mixins/serialize.py,https://github.com/absent1706/sqlalchemy-mixins/tree/master/sqlalchemy_mixins/serialize.py,SerializeMixin,to_dict$11,"def to_dict(self, nested=False, hybrid_attributes=False, exclude=None):
    """"""Return dict object with model's data.

        :param nested: flag to return nested relationships' data if true
        :type: bool
        :param hybrid_attributes: flag to include hybrid attributes if true
        :type: bool
        :return: dict
        """"""
    result = dict()
    if exclude is None:
        view_cols = self.columns
    else:
        view_cols = filter(lambda e: e not in exclude, self.columns)
    for key in view_cols:
        result[key] = getattr(self, key)
    if hybrid_attributes:
        for key in self.hybrid_properties:
            result[key] = getattr(self, key)
    if nested:
        for key in self.relations:
            obj = getattr(self, key)
            if isinstance(obj, SerializeMixin):
                result[key] = obj.to_dict(hybrid_attributes=hybrid_attributes)
            elif isinstance(obj, Iterable):
                result[key] = [o.to_dict(hybrid_attributes=hybrid_attributes) for o in obj if isinstance(o, SerializeMixin)]
    return result","for key in view_cols:
    result[key] = getattr(self, key)",XXX,no_found,,,,
sqlalchemy-mixins,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sqlalchemy-mixins/sqlalchemy_mixins/serialize.py,https://github.com/absent1706/sqlalchemy-mixins/tree/master/sqlalchemy_mixins/serialize.py,SerializeMixin,to_dict$11,"def to_dict(self, nested=False, hybrid_attributes=False, exclude=None):
    """"""Return dict object with model's data.

        :param nested: flag to return nested relationships' data if true
        :type: bool
        :param hybrid_attributes: flag to include hybrid attributes if true
        :type: bool
        :return: dict
        """"""
    result = dict()
    if exclude is None:
        view_cols = self.columns
    else:
        view_cols = filter(lambda e: e not in exclude, self.columns)
    for key in view_cols:
        result[key] = getattr(self, key)
    if hybrid_attributes:
        for key in self.hybrid_properties:
            result[key] = getattr(self, key)
    if nested:
        for key in self.relations:
            obj = getattr(self, key)
            if isinstance(obj, SerializeMixin):
                result[key] = obj.to_dict(hybrid_attributes=hybrid_attributes)
            elif isinstance(obj, Iterable):
                result[key] = [o.to_dict(hybrid_attributes=hybrid_attributes) for o in obj if isinstance(o, SerializeMixin)]
    return result","for key in self.hybrid_properties:
    result[key] = getattr(self, key)",XXX,no_found,0,,,
sqlalchemy-mixins,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sqlalchemy-mixins/sqlalchemy_mixins/serialize.py,https://github.com/absent1706/sqlalchemy-mixins/tree/master/sqlalchemy_mixins/serialize.py,SerializeMixin,to_dict$11,"def to_dict(self, nested=False, hybrid_attributes=False, exclude=None):
    """"""Return dict object with model's data.

        :param nested: flag to return nested relationships' data if true
        :type: bool
        :param hybrid_attributes: flag to include hybrid attributes if true
        :type: bool
        :return: dict
        """"""
    result = dict()
    if exclude is None:
        view_cols = self.columns
    else:
        view_cols = filter(lambda e: e not in exclude, self.columns)
    for key in view_cols:
        result[key] = getattr(self, key)
    if hybrid_attributes:
        for key in self.hybrid_properties:
            result[key] = getattr(self, key)
    if nested:
        for key in self.relations:
            obj = getattr(self, key)
            if isinstance(obj, SerializeMixin):
                result[key] = obj.to_dict(hybrid_attributes=hybrid_attributes)
            elif isinstance(obj, Iterable):
                result[key] = [o.to_dict(hybrid_attributes=hybrid_attributes) for o in obj if isinstance(o, SerializeMixin)]
    return result","for key in self.relations:
    obj = getattr(self, key)
    if isinstance(obj, SerializeMixin):
        result[key] = obj.to_dict(hybrid_attributes=hybrid_attributes)
    elif isinstance(obj, Iterable):
        result[key] = [o.to_dict(hybrid_attributes=hybrid_attributes) for o in obj if isinstance(o, SerializeMixin)]",XXX,no_found,0,,,
ParlAI,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/light_genderation_bias/agents.py,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/light_genderation_bias/agents.py,,read_gender_tsv$93,"def read_gender_tsv(path, remove_verbs=True):
    """"""
    Load TSV of gendered word lists and return a dict.
    """"""
    gender_dct = {}
    with PathManager.open(path) as tsvfile:
        reader = list(csv.reader(tsvfile, delimiter='\t'))
        title_lst = reader[0]
        title_dict = {}
        for (idx, title) in enumerate(title_lst):
            title_dict[idx] = title
        for i in range(1, len(reader)):
            row = reader[i]
            word = row[0].lower()
            gender_dct[word] = {}
            for (j, category) in enumerate(row[1:]):
                gender_dct[word][title_dict[j + 1]] = category
    if remove_verbs:
        return {k: v for (k, v) in gender_dct.items() if v['syncategory'] != 'verb'}
    return gender_dct","for (idx, title) in enumerate(title_lst):
    title_dict[idx] = title",XXX,no_found,0,,,
ParlAI,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/light_genderation_bias/agents.py,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/light_genderation_bias/agents.py,,read_gender_tsv$93,"def read_gender_tsv(path, remove_verbs=True):
    """"""
    Load TSV of gendered word lists and return a dict.
    """"""
    gender_dct = {}
    with PathManager.open(path) as tsvfile:
        reader = list(csv.reader(tsvfile, delimiter='\t'))
        title_lst = reader[0]
        title_dict = {}
        for (idx, title) in enumerate(title_lst):
            title_dict[idx] = title
        for i in range(1, len(reader)):
            row = reader[i]
            word = row[0].lower()
            gender_dct[word] = {}
            for (j, category) in enumerate(row[1:]):
                gender_dct[word][title_dict[j + 1]] = category
    if remove_verbs:
        return {k: v for (k, v) in gender_dct.items() if v['syncategory'] != 'verb'}
    return gender_dct","for (j, category) in enumerate(row[1:]):
    gender_dct[word][title_dict[j + 1]] = category",XXX,no_found,0,,,
redis-py-cluster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/redis-py-cluster/tests/test_cluster_obj.py,https://github.com/Grokzen/redis-py-cluster/tree/master/tests/test_cluster_obj.py,,test_refresh_using_specific_nodes$479,"def test_refresh_using_specific_nodes(r):
    """"""
    Test making calls on specific nodes when the cluster has failed over to
    another node
    """"""
    with patch.object(RedisCluster, 'parse_response') as parse_response_mock:
        with patch.object(NodeManager, 'initialize', autospec=True) as init_mock:

            def side_effect(self, *args, **kwargs):
                if self.port == 7006:
                    parse_response_mock.failed_calls += 1
                    raise ClusterDownError('CLUSTERDOWN The cluster is down. Use CLUSTER INFO for more information')
                elif self.port == 7007:
                    parse_response_mock.successful_calls += 1

            def side_effect_rebuild_slots_cache(self):
                self.nodes = {'127.0.0.1:7006': {'host': '127.0.0.1', 'server_type': 'master', 'port': 7006, 'name': '127.0.0.1:7006'}}
                self.slots = {}
                for i in range(0, 16383):
                    self.slots[i] = [{'host': '127.0.0.1', 'server_type': 'master', 'port': 7006, 'name': '127.0.0.1:7006'}]

                def map_7007(self):
                    self.nodes = {'127.0.0.1:7007': {'host': '127.0.0.1', 'server_type': 'master', 'port': 7007, 'name': '127.0.0.1:7007'}}
                    self.slots = {}
                    for i in range(0, 16383):
                        self.slots[i] = [{'host': '127.0.0.1', 'server_type': 'master', 'port': 7007, 'name': '127.0.0.1:7007'}]
                init_mock.side_effect = map_7007
            parse_response_mock.side_effect = side_effect
            parse_response_mock.successful_calls = 0
            parse_response_mock.failed_calls = 0
            init_mock.side_effect = side_effect_rebuild_slots_cache
            rc = RedisCluster(host='127.0.0.1', port=7006)
            assert len(rc.connection_pool.nodes.nodes) == 1
            assert '127.0.0.1:7006' in rc.connection_pool.nodes.nodes
            rc.ping()
            assert len(rc.connection_pool.nodes.nodes) == 1
            assert '127.0.0.1:7007' in rc.connection_pool.nodes.nodes
            assert parse_response_mock.failed_calls == 1
            assert parse_response_mock.successful_calls == 1","for i in range(0, 16383):
    self.slots[i] = [{'host': '127.0.0.1', 'server_type': 'master', 'port': 7006, 'name': '127.0.0.1:7006'}]",XXX,no_found,0,,,
redis-py-cluster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/redis-py-cluster/tests/test_cluster_obj.py,https://github.com/Grokzen/redis-py-cluster/tree/master/tests/test_cluster_obj.py,,test_refresh_using_specific_nodes$479,"def test_refresh_using_specific_nodes(r):
    """"""
    Test making calls on specific nodes when the cluster has failed over to
    another node
    """"""
    with patch.object(RedisCluster, 'parse_response') as parse_response_mock:
        with patch.object(NodeManager, 'initialize', autospec=True) as init_mock:

            def side_effect(self, *args, **kwargs):
                if self.port == 7006:
                    parse_response_mock.failed_calls += 1
                    raise ClusterDownError('CLUSTERDOWN The cluster is down. Use CLUSTER INFO for more information')
                elif self.port == 7007:
                    parse_response_mock.successful_calls += 1

            def side_effect_rebuild_slots_cache(self):
                self.nodes = {'127.0.0.1:7006': {'host': '127.0.0.1', 'server_type': 'master', 'port': 7006, 'name': '127.0.0.1:7006'}}
                self.slots = {}
                for i in range(0, 16383):
                    self.slots[i] = [{'host': '127.0.0.1', 'server_type': 'master', 'port': 7006, 'name': '127.0.0.1:7006'}]

                def map_7007(self):
                    self.nodes = {'127.0.0.1:7007': {'host': '127.0.0.1', 'server_type': 'master', 'port': 7007, 'name': '127.0.0.1:7007'}}
                    self.slots = {}
                    for i in range(0, 16383):
                        self.slots[i] = [{'host': '127.0.0.1', 'server_type': 'master', 'port': 7007, 'name': '127.0.0.1:7007'}]
                init_mock.side_effect = map_7007
            parse_response_mock.side_effect = side_effect
            parse_response_mock.successful_calls = 0
            parse_response_mock.failed_calls = 0
            init_mock.side_effect = side_effect_rebuild_slots_cache
            rc = RedisCluster(host='127.0.0.1', port=7006)
            assert len(rc.connection_pool.nodes.nodes) == 1
            assert '127.0.0.1:7006' in rc.connection_pool.nodes.nodes
            rc.ping()
            assert len(rc.connection_pool.nodes.nodes) == 1
            assert '127.0.0.1:7007' in rc.connection_pool.nodes.nodes
            assert parse_response_mock.failed_calls == 1
            assert parse_response_mock.successful_calls == 1","for i in range(0, 16383):
    self.slots[i] = [{'host': '127.0.0.1', 'server_type': 'master', 'port': 7007, 'name': '127.0.0.1:7007'}]",XXX,no_found,0,,,
second.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/second.pytorch/second/data/kitti_common.py,https://github.com/traveller59/second.pytorch/tree/master/second/data/kitti_common.py,,remove_dontcare$331,"def remove_dontcare(image_anno):
    img_filtered_annotations = {}
    relevant_annotation_indices = [i for (i, x) in enumerate(image_anno['name']) if x != 'DontCare']
    for key in image_anno.keys():
        img_filtered_annotations[key] = image_anno[key][relevant_annotation_indices]
    return img_filtered_annotations","for key in image_anno.keys():
    img_filtered_annotations[key] = image_anno[key][relevant_annotation_indices]",XXX,no_found,,,,
sweetviz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sweetviz/sweetviz/sv_html.py,https://github.com/fbdesignpro/sweetviz/tree/master/sweetviz/sv_html.py,,load_layout_globals_from_config$29,"def load_layout_globals_from_config():
    jinja2_env.globals['FeatureType'] = FeatureType
    layout_globals = dict()
    general_globals = dict()
    for element in config['Layout']:
        layout_globals[element] = config['Layout'].getint(element)
    general_globals['use_cjk_font'] = config['General'].getint('use_cjk_font')
    general_globals['association_min_to_bold'] = config['General'].getfloat('association_min_to_bold')
    jinja2_env.globals['layout'] = layout_globals
    jinja2_env.globals['general'] = general_globals","for element in config['Layout']:
    layout_globals[element] = config['Layout'].getint(element)",XXX,no_found,,,,
data-science-competition,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-science-competition/婵犮垹鐏堥弲婊堟儓/婵°倕鍊归敋闁汇儱鎰鎹堕柡澶嬪х粋/mmdetectionForGaode/get_features.py,https://github.com/DLLXW/data-science-competition/tree/master/婵犮垹鐏堥弲婊堟儓/婵°倕鍊归敋闁汇儱鎰鎹堕柡澶嬪х粋/mmdetectionForGaode/get_features.py,,if_main_my$257,"if __name__ == '__main__':
    begin_time = time.time()
    features_name = ['name', 'car_cnt', 'car_scaleSum', 'car_scaleMean', 'car_scaleMax', 'car_xSum', 'car_xMean', 'car_xMax', 'car_xMin', 'car_ySum', 'car_yMean', 'car_yMax', 'car_yMin', 'car_disSum', 'car_dis', 'x_disSum', 'x_disMean', 'x_disMin', 'x_disMax', 'y_disSum', 'y_disMean', 'y_disMin', 'y_disMax', 'roi_car_cnt', 'roi_car_sizeSum', 'roi_car_size', 'roi_car_sizeMax', 'roi_car_sizeMin', 'roi_car_sizeStd', 'roi_car_cnt1', 'roi_car_sizeSum1', 'roi_car_size1', 'roi_car_sizeMax1', 'roi_car_sizeMin1', 'roi_car_sizeStd1', 'roi_car_cnt2', 'roi_car_sizeSum2', 'roi_car_size2', 'roi_car_sizeMax2', 'roi_car_sizeMin2', 'roi_car_sizeStd2', 'roi_car_cnt3', 'roi_car_sizeSum3', 'roi_car_size3', 'roi_car_sizeMax3', 'roi_car_sizeMin3', 'roi_car_sizeStd3', 'roi_car_cnt4', 'roi_car_sizeSum4', 'roi_car_size4', 'roi_car_sizeMax4', 'roi_car_sizeMin4', 'roi_car_sizeStd4']
    features_dic = {}
    fill_null = 0
    for fea in features_name:
        features_dic[fea] = []
    train_image_dir = '/home/admins/qyl/gaode_classify/dataset/amap_traffic_final_train_data'
    train_json_dir = '/home/admins/qyl/gaode_classify/dataset/amap_traffic_final_train_0906.json'
    config = 'configs_raw/cascade_rcnn/cascade_rcnn_r50_fpn_1x_coco.py'
    checkpoint = 'checkpoints/cascade_rcnn_r50_fpn_1x_coco_20200316-3dc56deb.pth'
    model = init_detector(config, checkpoint, device=0)
    img_paths = sorted(glob.glob(train_image_dir + '/*/*'))
    print('data size:', len(img_paths))
    cnt_programe = 0
    for img_path in img_paths:
        cnt_programe += 1
        print(img_path, ' ', cnt_programe)
        seq = img_path.split('/')[-2]
        frame = img_path.split('/')[-1]
        get_frt(img_path, img_name=seq + '_' + frame)
    print(pd.DataFrame(features_dic))
    df = pd.DataFrame(features_dic)
    df.to_csv('train_res2net.csv', index=False)
    print('spend time {}s'.format(time.time() - begin_time))","for fea in features_name:
    features_dic[fea] = []",XXX,no_found,,,,
avalanche,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/avalanche/avalanche/evaluation/metrics/forgetting_bwt.py,https://github.com/ContinualAI/avalanche/tree/master/avalanche/evaluation/metrics/forgetting_bwt.py,Forgetting,result$67,"def result(self, k=None) -> Union[float, None, Dict[int, float]]:
    """"""
        Forgetting is returned only for keys encountered twice.

        :param k: the key for which returning forgetting. If k has not
            updated at least twice it returns None. If k is None,
            forgetting will be returned for all keys encountered at least
            twice.

        :return: the difference between the first and last value encountered
            for k, if k is not None. It returns None if k has not been updated
            at least twice. If k is None, returns a dictionary
            containing keys whose value has been updated at least twice. The
            associated value is the difference between the first and last
            value recorded for that key.
        """"""
    forgetting = {}
    if k is not None:
        if k in self.initial and k in self.last:
            return self.initial[k] - self.last[k]
        else:
            return None
    ik = set(self.initial.keys())
    both_keys = list(ik.intersection(set(self.last.keys())))
    for k in both_keys:
        forgetting[k] = self.initial[k] - self.last[k]
    return forgetting","for k in both_keys:
    forgetting[k] = self.initial[k] - self.last[k]",XXX,no_found,,,,
mmpose,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmpose/mmpose/core/fp16/decorators.py,https://github.com/open-mmlab/mmpose/tree/master/mmpose/core/fp16/decorators.py,,new_func$51,"def new_func(*args, **kwargs):
    if not isinstance(args[0], torch.nn.Module):
        raise TypeError('@auto_fp16 can only be used to decorate the method of nn.Module')
    if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
        return old_func(*args, **kwargs)
    args_info = getfullargspec(old_func)
    args_to_cast = args_info.args if apply_to is None else apply_to
    new_args = []
    if args:
        arg_names = args_info.args[:len(args)]
        for (i, arg_name) in enumerate(arg_names):
            if arg_name in args_to_cast:
                new_args.append(cast_tensor_type(args[i], torch.float, torch.half))
            else:
                new_args.append(args[i])
    new_kwargs = {}
    if kwargs:
        for (arg_name, arg_value) in kwargs.items():
            if arg_name in args_to_cast:
                new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
            else:
                new_kwargs[arg_name] = arg_value
    output = old_func(*new_args, **new_kwargs)
    if out_fp32:
        output = cast_tensor_type(output, torch.half, torch.float)
    return output","for (arg_name, arg_value) in kwargs.items():
    if arg_name in args_to_cast:
        new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
    else:
        new_kwargs[arg_name] = arg_value",XXX,no_found,,,,
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/stack_pointer_tracker.py,https://github.com/angr/angr/tree/master/angr/analyses/stack_pointer_tracker.py,,_dict_merge$248,"def _dict_merge(d1, d2):
    all_keys = set(d1.keys()) | set(d2.keys())
    merged = {}
    for k in all_keys:
        if k not in d1 or d1[k] is TOP:
            merged[k] = TOP
        elif k not in d2 or d2[k] is TOP:
            merged[k] = TOP
        elif d1[k] is BOTTOM:
            merged[k] = d2[k]
        elif d2[k] is BOTTOM:
            merged[k] = d1[k]
        elif d1[k] == d2[k]:
            merged[k] = d1[k]
        else:
            merged[k] = TOP
    return merged","for k in all_keys:
    if k not in d1 or d1[k] is TOP:
        merged[k] = TOP
    elif k not in d2 or d2[k] is TOP:
        merged[k] = TOP
    elif d1[k] is BOTTOM:
        merged[k] = d2[k]
    elif d2[k] is BOTTOM:
        merged[k] = d1[k]
    elif d1[k] == d2[k]:
        merged[k] = d1[k]
    else:
        merged[k] = TOP",XXX,no_found,,,,
coveragepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coveragepy/coverage/config.py,https://github.com/nedbat/coveragepy/tree/master/coverage/config.py,HandyConfigParser,get_section$64,"def get_section(self, section):
    """"""Get the contents of a section, as a dictionary.""""""
    d = {}
    for opt in self.options(section):
        d[opt] = self.get(section, opt)
    return d","for opt in self.options(section):
    d[opt] = self.get(section, opt)",XXX,no_found,,,,
shiv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shiv/src/shiv/cli.py,https://github.com/linkedin/shiv/tree/master/src/shiv/cli.py,,main$160,"def main(output_file: str, entry_point: Optional[str], console_script: Optional[str], python: Optional[str], site_packages: Optional[str], build_id: Optional[str], compressed: bool, compile_pyc: bool, extend_pythonpath: bool, reproducible: bool, no_modify: bool, preamble: Optional[str], root: Optional[str], pip_args: List[str]) -> None:
    """"""
    Shiv is a command line utility for building fully self-contained Python zipapps
    as outlined in PEP 441, but with all their dependencies included!
    """"""
    if not pip_args and (not site_packages):
        sys.exit(NO_PIP_ARGS_OR_SITE_PACKAGES)
    if output_file is None:
        sys.exit(NO_OUTFILE)
    for disallowed in DISALLOWED_ARGS:
        for supplied_arg in pip_args:
            if supplied_arg in disallowed:
                sys.exit(DISALLOWED_PIP_ARGS.format(arg=supplied_arg, reason=DISALLOWED_ARGS[disallowed]))
    if build_id is not None:
        click.secho('Warning! You have overridden the default build-id behavior, executables created by shiv must have unique build IDs or unexpected behavior could occur.', fg='yellow')
    sources: List[Path] = []
    with TemporaryDirectory() as tmp_site_packages:
        if site_packages:
            if pip_args:
                for sp in site_packages:
                    copytree(Path(sp), Path(tmp_site_packages))
            else:
                sources.extend([Path(p).expanduser() for p in site_packages])
        if pip_args:
            pip.install(['--target', tmp_site_packages] + list(pip_args))
        if preamble:
            bin_dir = Path(tmp_site_packages, 'bin')
            bin_dir.mkdir(exist_ok=True)
            shutil.copy(Path(preamble).absolute(), bin_dir / Path(preamble).name)
        sources.append(Path(tmp_site_packages).absolute())
        if no_modify:
            hashes = {}
            for source in sources:
                for path in source.rglob('**/*.py'):
                    hashes[str(path.relative_to(source))] = hashlib.sha256(path.read_bytes()).hexdigest()
        if entry_point is None and console_script is not None:
            try:
                entry_point = find_entry_point(sources, console_script)
            except KeyError:
                if not console_script_exists(sources, console_script):
                    sys.exit(NO_ENTRY_POINT.format(entry_point=console_script))
            else:
                console_script = None
        timestamp = int(os.environ.get(SOURCE_DATE_EPOCH_ENV, SOURCE_DATE_EPOCH_DEFAULT if reproducible else time.time()))
        env = Environment(built_at=datetime.utcfromtimestamp(timestamp).strftime(BUILD_AT_TIMESTAMP_FORMAT), build_id=build_id, entry_point=entry_point, script=console_script, compile_pyc=compile_pyc, extend_pythonpath=extend_pythonpath, shiv_version=__version__, no_modify=no_modify, reproducible=reproducible, preamble=Path(preamble).name if preamble else None, root=root)
        if no_modify:
            env.hashes = hashes
        builder.create_archive(sources, target=Path(output_file).expanduser(), interpreter=python or DEFAULT_SHEBANG, main='_bootstrap:bootstrap', env=env, compressed=compressed)","for source in sources:
    for path in source.rglob('**/*.py'):
        hashes[str(path.relative_to(source))] = hashlib.sha256(path.read_bytes()).hexdigest()",XXX,no_found,0,,,
pony,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pony/pony/thirdparty/compiler/pyassem.py,https://github.com/ponyorm/pony/tree/master/pony/thirdparty/compiler/pyassem.py,PyFlowGraph,computeStackDepth$336,"def computeStackDepth(self):
    """"""Compute the max stack depth.

        Approach is to compute the stack effect of each basic block.
        Then find the path through the code with the largest total
        effect.
        """"""
    depth = {}
    exit = None
    for b in self.getBlocks():
        depth[b] = findDepth(b.getInstructions())
    seen = {}

    def max_depth(b, d):
        if b in seen:
            return d
        seen[b] = 1
        d = d + depth[b]
        children = b.get_children()
        if children:
            return max([max_depth(c, d) for c in children])
        elif not b.label == 'exit':
            return max_depth(self.exit, d)
        else:
            return d
    self.stacksize = max_depth(self.entry, 0)","for b in self.getBlocks():
    depth[b] = findDepth(b.getInstructions())",XXX,no_found,0,,,
wharfee,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wharfee/wharfee/formatter.py,https://github.com/j-bennet/wharfee/tree/master/wharfee/formatter.py,,filter_dict$411,"def filter_dict(data, display_keys):
    """"""
    Strip out some of the dictionary fields.
    :param display_keys: set
    :param data: dict
    :return: dict
    """"""
    if data and isinstance(data, list) and isinstance(data[0], dict):
        result = []
        for item in data:
            filtered = {}
            for (k, v) in item.items():
                if k.lower() in display_keys:
                    filtered[k] = v
            result.append(filtered)
        return result
    return data","for (k, v) in item.items():
    if k.lower() in display_keys:
        filtered[k] = v",XXX,no_found,,,,
pygame_tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygame_tutorials/tilemap/part 21/main.py,https://github.com/kidscancode/pygame_tutorials/tree/master/tilemap/part 21/main.py,Game,load_data$64,"def load_data(self):
    game_folder = path.dirname(__file__)
    img_folder = path.join(game_folder, 'img')
    snd_folder = path.join(game_folder, 'snd')
    music_folder = path.join(game_folder, 'music')
    map_folder = path.join(game_folder, 'maps')
    self.title_font = path.join(img_folder, 'ZOMBIE.TTF')
    self.dim_screen = pg.Surface(self.screen.get_size()).convert_alpha()
    self.dim_screen.fill((0, 0, 0, 180))
    self.map = TiledMap(path.join(map_folder, 'level1.tmx'))
    self.map_img = self.map.make_map()
    self.map.rect = self.map_img.get_rect()
    self.player_img = pg.image.load(path.join(img_folder, PLAYER_IMG)).convert_alpha()
    self.bullet_images = {}
    self.bullet_images['lg'] = pg.image.load(path.join(img_folder, BULLET_IMG)).convert_alpha()
    self.bullet_images['sm'] = pg.transform.scale(self.bullet_images['lg'], (10, 10))
    self.mob_img = pg.image.load(path.join(img_folder, MOB_IMG)).convert_alpha()
    self.splat = pg.image.load(path.join(img_folder, SPLAT)).convert_alpha()
    self.splat = pg.transform.scale(self.splat, (64, 64))
    self.gun_flashes = []
    for img in MUZZLE_FLASHES:
        self.gun_flashes.append(pg.image.load(path.join(img_folder, img)).convert_alpha())
    self.item_images = {}
    for item in ITEM_IMAGES:
        self.item_images[item] = pg.image.load(path.join(img_folder, ITEM_IMAGES[item])).convert_alpha()
    pg.mixer.music.load(path.join(music_folder, BG_MUSIC))
    self.effects_sounds = {}
    for type in EFFECTS_SOUNDS:
        self.effects_sounds[type] = pg.mixer.Sound(path.join(snd_folder, EFFECTS_SOUNDS[type]))
    self.weapon_sounds = {}
    for weapon in WEAPON_SOUNDS:
        self.weapon_sounds[weapon] = []
        for snd in WEAPON_SOUNDS[weapon]:
            s = pg.mixer.Sound(path.join(snd_folder, snd))
            s.set_volume(0.3)
            self.weapon_sounds[weapon].append(s)
    self.zombie_moan_sounds = []
    for snd in ZOMBIE_MOAN_SOUNDS:
        s = pg.mixer.Sound(path.join(snd_folder, snd))
        s.set_volume(0.2)
        self.zombie_moan_sounds.append(s)
    self.player_hit_sounds = []
    for snd in PLAYER_HIT_SOUNDS:
        self.player_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))
    self.zombie_hit_sounds = []
    for snd in ZOMBIE_HIT_SOUNDS:
        self.zombie_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))","for item in ITEM_IMAGES:
    self.item_images[item] = pg.image.load(path.join(img_folder, ITEM_IMAGES[item])).convert_alpha()",XXX,no_found,0,,,
pygame_tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygame_tutorials/tilemap/part 21/main.py,https://github.com/kidscancode/pygame_tutorials/tree/master/tilemap/part 21/main.py,Game,load_data$64,"def load_data(self):
    game_folder = path.dirname(__file__)
    img_folder = path.join(game_folder, 'img')
    snd_folder = path.join(game_folder, 'snd')
    music_folder = path.join(game_folder, 'music')
    map_folder = path.join(game_folder, 'maps')
    self.title_font = path.join(img_folder, 'ZOMBIE.TTF')
    self.dim_screen = pg.Surface(self.screen.get_size()).convert_alpha()
    self.dim_screen.fill((0, 0, 0, 180))
    self.map = TiledMap(path.join(map_folder, 'level1.tmx'))
    self.map_img = self.map.make_map()
    self.map.rect = self.map_img.get_rect()
    self.player_img = pg.image.load(path.join(img_folder, PLAYER_IMG)).convert_alpha()
    self.bullet_images = {}
    self.bullet_images['lg'] = pg.image.load(path.join(img_folder, BULLET_IMG)).convert_alpha()
    self.bullet_images['sm'] = pg.transform.scale(self.bullet_images['lg'], (10, 10))
    self.mob_img = pg.image.load(path.join(img_folder, MOB_IMG)).convert_alpha()
    self.splat = pg.image.load(path.join(img_folder, SPLAT)).convert_alpha()
    self.splat = pg.transform.scale(self.splat, (64, 64))
    self.gun_flashes = []
    for img in MUZZLE_FLASHES:
        self.gun_flashes.append(pg.image.load(path.join(img_folder, img)).convert_alpha())
    self.item_images = {}
    for item in ITEM_IMAGES:
        self.item_images[item] = pg.image.load(path.join(img_folder, ITEM_IMAGES[item])).convert_alpha()
    pg.mixer.music.load(path.join(music_folder, BG_MUSIC))
    self.effects_sounds = {}
    for type in EFFECTS_SOUNDS:
        self.effects_sounds[type] = pg.mixer.Sound(path.join(snd_folder, EFFECTS_SOUNDS[type]))
    self.weapon_sounds = {}
    for weapon in WEAPON_SOUNDS:
        self.weapon_sounds[weapon] = []
        for snd in WEAPON_SOUNDS[weapon]:
            s = pg.mixer.Sound(path.join(snd_folder, snd))
            s.set_volume(0.3)
            self.weapon_sounds[weapon].append(s)
    self.zombie_moan_sounds = []
    for snd in ZOMBIE_MOAN_SOUNDS:
        s = pg.mixer.Sound(path.join(snd_folder, snd))
        s.set_volume(0.2)
        self.zombie_moan_sounds.append(s)
    self.player_hit_sounds = []
    for snd in PLAYER_HIT_SOUNDS:
        self.player_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))
    self.zombie_hit_sounds = []
    for snd in ZOMBIE_HIT_SOUNDS:
        self.zombie_hit_sounds.append(pg.mixer.Sound(path.join(snd_folder, snd)))","for type in EFFECTS_SOUNDS:
    self.effects_sounds[type] = pg.mixer.Sound(path.join(snd_folder, EFFECTS_SOUNDS[type]))",XXX,no_found,0,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/win_status.py,https://github.com/saltstack/salt/tree/master/salt/modules/win_status.py,,meminfo$198,"def meminfo():
    """"""
    Return information about physical and virtual memory on the system

    Returns:
        dict: A dictionary of information about memory on the system

    CLI Example:

    .. code-block:: bash

        salt * status.meminfo
    """"""
    (vm_total, vm_available, vm_percent, vm_used, vm_free) = psutil.virtual_memory()
    (swp_total, swp_used, swp_free, swp_percent, _, _) = psutil.swap_memory()

    def get_unit_value(memory):
        symbols = ('K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y')
        prefix = {}
        for (i, s) in enumerate(symbols):
            prefix[s] = 1 << (i + 1) * 10
        for s in reversed(symbols):
            if memory >= prefix[s]:
                value = float(memory) / prefix[s]
                return {'unit': s, 'value': value}
        return {'unit': 'B', 'value': memory}
    return {'VmallocTotal': get_unit_value(vm_total), 'VmallocUsed': get_unit_value(vm_used), 'VmallocFree': get_unit_value(vm_free), 'VmallocAvail': get_unit_value(vm_available), 'SwapTotal': get_unit_value(swp_total), 'SwapUsed': get_unit_value(swp_used), 'SwapFree': get_unit_value(swp_free)}","for (i, s) in enumerate(symbols):
    prefix[s] = 1 << (i + 1) * 10",XXX,no_found,0,,,
attn2d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/attn2d/fairseq_cli/preprocess.py,https://github.com/elbayadm/attn2d/tree/master/fairseq_cli/preprocess.py,,main$32,"def main(args):
    utils.import_user_module(args)
    os.makedirs(args.destdir, exist_ok=True)
    logger.addHandler(logging.FileHandler(filename=os.path.join(args.destdir, 'preprocess.log')))
    logger.info(args)
    task = tasks.get_task(args.task)

    def train_path(lang):
        return '{}{}'.format(args.trainpref, '.' + lang if lang else '')

    def file_name(prefix, lang):
        fname = prefix
        if lang is not None:
            fname += '.{lang}'.format(lang=lang)
        return fname

    def dest_path(prefix, lang):
        return os.path.join(args.destdir, file_name(prefix, lang))

    def dict_path(lang):
        return dest_path('dict', lang) + '.txt'

    def build_dictionary(filenames, src=False, tgt=False):
        assert src ^ tgt
        return task.build_dictionary(filenames, workers=args.workers, threshold=args.thresholdsrc if src else args.thresholdtgt, nwords=args.nwordssrc if src else args.nwordstgt, padding_factor=args.padding_factor)
    target = not args.only_source
    if not args.srcdict and os.path.exists(dict_path(args.source_lang)):
        raise FileExistsError(dict_path(args.source_lang))
    if target and (not args.tgtdict) and os.path.exists(dict_path(args.target_lang)):
        raise FileExistsError(dict_path(args.target_lang))
    if args.joined_dictionary:
        assert not args.srcdict or not args.tgtdict, 'cannot use both --srcdict and --tgtdict with --joined-dictionary'
        if args.srcdict:
            src_dict = task.load_dictionary(args.srcdict)
        elif args.tgtdict:
            src_dict = task.load_dictionary(args.tgtdict)
        else:
            assert args.trainpref, '--trainpref must be set if --srcdict is not specified'
            src_dict = build_dictionary({train_path(lang) for lang in [args.source_lang, args.target_lang]}, src=True)
        tgt_dict = src_dict
    else:
        if args.srcdict:
            src_dict = task.load_dictionary(args.srcdict)
        else:
            assert args.trainpref, '--trainpref must be set if --srcdict is not specified'
            src_dict = build_dictionary([train_path(args.source_lang)], src=True)
        if target:
            if args.tgtdict:
                tgt_dict = task.load_dictionary(args.tgtdict)
            else:
                assert args.trainpref, '--trainpref must be set if --tgtdict is not specified'
                tgt_dict = build_dictionary([train_path(args.target_lang)], tgt=True)
        else:
            tgt_dict = None
    src_dict.save(dict_path(args.source_lang))
    if target and tgt_dict is not None:
        tgt_dict.save(dict_path(args.target_lang))

    def make_binary_dataset(vocab, input_prefix, output_prefix, lang, num_workers):
        logger.info('[{}] Dictionary: {} types'.format(lang, len(vocab) - 1))
        n_seq_tok = [0, 0]
        replaced = Counter()

        def merge_result(worker_result):
            replaced.update(worker_result['replaced'])
            n_seq_tok[0] += worker_result['nseq']
            n_seq_tok[1] += worker_result['ntok']
        input_file = '{}{}'.format(input_prefix, '.' + lang if lang is not None else '')
        offsets = Binarizer.find_offsets(input_file, num_workers)
        pool = None
        if num_workers > 1:
            pool = Pool(processes=num_workers - 1)
            for worker_id in range(1, num_workers):
                prefix = '{}{}'.format(output_prefix, worker_id)
                pool.apply_async(binarize, (args, input_file, vocab, prefix, lang, offsets[worker_id], offsets[worker_id + 1]), callback=merge_result)
            pool.close()
        ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, lang, 'bin'), impl=args.dataset_impl, vocab_size=len(vocab))
        merge_result(Binarizer.binarize(input_file, vocab, lambda t: ds.add_item(t), offset=0, end=offsets[1]))
        if num_workers > 1:
            pool.join()
            for worker_id in range(1, num_workers):
                prefix = '{}{}'.format(output_prefix, worker_id)
                temp_file_path = dataset_dest_prefix(args, prefix, lang)
                ds.merge_file_(temp_file_path)
                os.remove(indexed_dataset.data_file_path(temp_file_path))
                os.remove(indexed_dataset.index_file_path(temp_file_path))
        ds.finalize(dataset_dest_file(args, output_prefix, lang, 'idx'))
        logger.info('[{}] {}: {} sents, {} tokens, {:.3}% replaced by {}'.format(lang, input_file, n_seq_tok[0], n_seq_tok[1], 100 * sum(replaced.values()) / n_seq_tok[1], vocab.unk_word))

    def make_binary_alignment_dataset(input_prefix, output_prefix, num_workers):
        nseq = [0]

        def merge_result(worker_result):
            nseq[0] += worker_result['nseq']
        input_file = input_prefix
        offsets = Binarizer.find_offsets(input_file, num_workers)
        pool = None
        if num_workers > 1:
            pool = Pool(processes=num_workers - 1)
            for worker_id in range(1, num_workers):
                prefix = '{}{}'.format(output_prefix, worker_id)
                pool.apply_async(binarize_alignments, (args, input_file, utils.parse_alignment, prefix, offsets[worker_id], offsets[worker_id + 1]), callback=merge_result)
            pool.close()
        ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, None, 'bin'), impl=args.dataset_impl)
        merge_result(Binarizer.binarize_alignments(input_file, utils.parse_alignment, lambda t: ds.add_item(t), offset=0, end=offsets[1]))
        if num_workers > 1:
            pool.join()
            for worker_id in range(1, num_workers):
                prefix = '{}{}'.format(output_prefix, worker_id)
                temp_file_path = dataset_dest_prefix(args, prefix, None)
                ds.merge_file_(temp_file_path)
                os.remove(indexed_dataset.data_file_path(temp_file_path))
                os.remove(indexed_dataset.index_file_path(temp_file_path))
        ds.finalize(dataset_dest_file(args, output_prefix, None, 'idx'))
        logger.info('[alignments] {}: parsed {} alignments'.format(input_file, nseq[0]))

    def make_dataset(vocab, input_prefix, output_prefix, lang, num_workers=1):
        if args.dataset_impl == 'raw':
            output_text_file = dest_path(output_prefix + '.{}-{}'.format(args.source_lang, args.target_lang), lang)
            shutil.copyfile(file_name(input_prefix, lang), output_text_file)
        else:
            make_binary_dataset(vocab, input_prefix, output_prefix, lang, num_workers)

    def make_all(lang, vocab):
        if args.trainpref:
            make_dataset(vocab, args.trainpref, 'train', lang, num_workers=args.workers)
        if args.validpref:
            for (k, validpref) in enumerate(args.validpref.split(',')):
                outprefix = 'valid{}'.format(k) if k > 0 else 'valid'
                make_dataset(vocab, validpref, outprefix, lang, num_workers=args.workers)
        if args.testpref:
            for (k, testpref) in enumerate(args.testpref.split(',')):
                outprefix = 'test{}'.format(k) if k > 0 else 'test'
                make_dataset(vocab, testpref, outprefix, lang, num_workers=args.workers)

    def make_all_alignments():
        if args.trainpref and os.path.exists(args.trainpref + '.' + args.align_suffix):
            make_binary_alignment_dataset(args.trainpref + '.' + args.align_suffix, 'train.align', num_workers=args.workers)
        if args.validpref and os.path.exists(args.validpref + '.' + args.align_suffix):
            make_binary_alignment_dataset(args.validpref + '.' + args.align_suffix, 'valid.align', num_workers=args.workers)
        if args.testpref and os.path.exists(args.testpref + '.' + args.align_suffix):
            make_binary_alignment_dataset(args.testpref + '.' + args.align_suffix, 'test.align', num_workers=args.workers)
    make_all(args.source_lang, src_dict)
    if target:
        make_all(args.target_lang, tgt_dict)
    if args.align_suffix:
        make_all_alignments()
    logger.info('Wrote preprocessed data to {}'.format(args.destdir))
    if args.alignfile:
        assert args.trainpref, '--trainpref must be set if --alignfile is specified'
        src_file_name = train_path(args.source_lang)
        tgt_file_name = train_path(args.target_lang)
        freq_map = {}
        with open(args.alignfile, 'r', encoding='utf-8') as align_file:
            with open(src_file_name, 'r', encoding='utf-8') as src_file:
                with open(tgt_file_name, 'r', encoding='utf-8') as tgt_file:
                    for (a, s, t) in zip_longest(align_file, src_file, tgt_file):
                        si = src_dict.encode_line(s, add_if_not_exist=False)
                        ti = tgt_dict.encode_line(t, add_if_not_exist=False)
                        ai = list(map(lambda x: tuple(x.split('-')), a.split()))
                        for (sai, tai) in ai:
                            srcidx = si[int(sai)]
                            tgtidx = ti[int(tai)]
                            if srcidx != src_dict.unk() and tgtidx != tgt_dict.unk():
                                assert srcidx != src_dict.pad()
                                assert srcidx != src_dict.eos()
                                assert tgtidx != tgt_dict.pad()
                                assert tgtidx != tgt_dict.eos()
                                if srcidx not in freq_map:
                                    freq_map[srcidx] = {}
                                if tgtidx not in freq_map[srcidx]:
                                    freq_map[srcidx][tgtidx] = 1
                                else:
                                    freq_map[srcidx][tgtidx] += 1
        align_dict = {}
        for srcidx in freq_map.keys():
            align_dict[srcidx] = max(freq_map[srcidx], key=freq_map[srcidx].get)
        with open(os.path.join(args.destdir, 'alignment.{}-{}.txt'.format(args.source_lang, args.target_lang)), 'w', encoding='utf-8') as f:
            for (k, v) in align_dict.items():
                print('{} {}'.format(src_dict[k], tgt_dict[v]), file=f)","for srcidx in freq_map.keys():
    align_dict[srcidx] = max(freq_map[srcidx], key=freq_map[srcidx].get)",XXX,no_found,0,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/model/dataset_collections/builder.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/model/dataset_collections/builder.py,CollectionBuilder,build_elements$84,"def build_elements(self):
    elements = self._current_elements
    if self._nested_collection:
        new_elements = {}
        for (identifier, element) in elements.items():
            new_elements[identifier] = element.build()
        elements = new_elements
    else:
        self._current_elements = {}
    return elements","for (identifier, element) in elements.items():
    new_elements[identifier] = element.build()",XXX,no_found,,,,
zenodo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zenodo/zenodo/modules/spam/views.py,https://github.com/zenodo/zenodo/tree/master/zenodo/modules/spam/views.py,,safelist_admin$199,"def safelist_admin():
    """"""Safelist admin.""""""
    if not Permission(ActionNeed('admin-access')).can():
        abort(403)
    data = request.args.get('data', 'all', type=str)
    data_categories = {'records': data in ('all', 'records'), 'communities': data in ('all', 'communities')}
    from_weeks = request.args.get('from_weeks', 4, type=int)
    to_weeks = request.args.get('to_weeks', 0, type=int)
    max_users = request.args.get('max_users', 1000, type=int)
    include_pending = request.args.get('include_pending', 'include', type=str) == 'include'
    result = {}
    if data_categories['records']:
        search = RecordsSearch(index='records').filter('range', **{'created': {'gte': 'now-{}w'.format(from_weeks), 'lt': 'now-{}w'.format(to_weeks)}}).filter('term', _safelisted=False)
        user_agg = search.aggs.bucket('user', 'terms', field='owners', size=max_users)
        user_agg.metric('records', 'top_hits', size=3, _source=['title', 'description', 'recid'])
        res = search[0:0].execute()
        for user in res.aggregations.user.buckets:
            result[user.key] = {'last_content_titles': ', '.join((r.title for r in user.records)), 'last_content_descriptions': ', '.join((r.description for r in user.records)), 'first_content_url': url_for('invenio_records_ui.recid', pid_value=user.records[0].recid), 'total_content': user.doc_count}
    if data_categories['communities']:
        from_date = datetime.utcnow() - timedelta(weeks=from_weeks)
        to_date = datetime.utcnow() - timedelta(weeks=to_weeks)
        community_users = db.session.query(User.id.label('user_id'), sa.func.count(Community.id).label('count'), sa.func.max(Community.id).label('c_id'), sa.func.max(Community.title).label('title'), sa.func.max(Community.description).label('description')).join(Community).group_by(User.id).filter(Community.created.between(from_date, to_date), Community.deleted_at.is_(None), ~User.safelist.any()).limit(max_users)
        for row in community_users:
            user_data = result.get(row.user_id, {'last_content_titles': '', 'last_content_descriptions': '', 'first_content_url': '', 'total_content': 0})
            if user_data['last_content_titles']:
                user_data['last_content_titles'] += ', '
            user_data['last_content_titles'] += row.title
            if user_data['last_content_descriptions']:
                user_data['last_content_descriptions'] += ', '
            user_data['last_content_descriptions'] += row.description
            user_data['first_content_url'] = url_for('invenio_communities.detail', community_id=row.c_id)
            user_data['total_content'] += row.count
            result[row.user_id] = user_data
    _expand_users_info(result, include_pending)
    return render_template('zenodo_spam/safelist/admin.html', users=result)","for user in res.aggregations.user.buckets:
    result[user.key] = {'last_content_titles': ', '.join((r.title for r in user.records)), 'last_content_descriptions': ', '.join((r.description for r in user.records)), 'first_content_url': url_for('invenio_records_ui.recid', pid_value=user.records[0].recid), 'total_content': user.doc_count}",XXX,no_found,0,,,
pikaur,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pikaur/pikaur/search_cli.py,https://github.com/actionless/pikaur/tree/master/pikaur/search_cli.py,,package_search_thread_aur$45,"def package_search_thread_aur(queries: list[str]) -> list[AURPackageInfo]:
    args = parse_args()
    result = {}
    if queries:
        use_as_filters: list[str] = []
        with ThreadPool() as pool:
            requests = {}
            for query in queries:
                requests[query] = pool.apply_async(aur_rpc_search_name_desc, (query,))
            pool.close()
            for (query, request) in requests.items():
                try:
                    result[query] = request.get()
                except AURError as exc:
                    if exc.error == 'Too many package results.':
                        print_error(translate(""AUR: Too many package results for '{query}'"").format(query=query))
                        use_as_filters.append(query)
                    elif exc.error == 'Query arg too small.':
                        print_error(translate(""AUR: Query arg too small '{query}'"").format(query=query))
                        use_as_filters.append(query)
                    else:
                        raise
            pool.join()
        for query in use_as_filters:
            result = filter_aur_results(result, query)
        if args.namesonly:
            for (subindex, subresult) in result.items():
                result[subindex] = [pkg for pkg in subresult if subindex in pkg.name]
    elif args.quiet:
        result = {'all': [AURPackageInfo(name=name, packagebase=name, version='0') for name in get_all_aur_names()]}
    else:
        result = {'all': get_all_aur_packages()}
    if not args.quiet:
        sys.stderr.write('#')
    return list(join_search_results(list(result.values())))","for query in queries:
    requests[query] = pool.apply_async(aur_rpc_search_name_desc, (query,))",XXX,no_found,0,,,
pikaur,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pikaur/pikaur/search_cli.py,https://github.com/actionless/pikaur/tree/master/pikaur/search_cli.py,,package_search_thread_aur$45,"def package_search_thread_aur(queries: list[str]) -> list[AURPackageInfo]:
    args = parse_args()
    result = {}
    if queries:
        use_as_filters: list[str] = []
        with ThreadPool() as pool:
            requests = {}
            for query in queries:
                requests[query] = pool.apply_async(aur_rpc_search_name_desc, (query,))
            pool.close()
            for (query, request) in requests.items():
                try:
                    result[query] = request.get()
                except AURError as exc:
                    if exc.error == 'Too many package results.':
                        print_error(translate(""AUR: Too many package results for '{query}'"").format(query=query))
                        use_as_filters.append(query)
                    elif exc.error == 'Query arg too small.':
                        print_error(translate(""AUR: Query arg too small '{query}'"").format(query=query))
                        use_as_filters.append(query)
                    else:
                        raise
            pool.join()
        for query in use_as_filters:
            result = filter_aur_results(result, query)
        if args.namesonly:
            for (subindex, subresult) in result.items():
                result[subindex] = [pkg for pkg in subresult if subindex in pkg.name]
    elif args.quiet:
        result = {'all': [AURPackageInfo(name=name, packagebase=name, version='0') for name in get_all_aur_names()]}
    else:
        result = {'all': get_all_aur_packages()}
    if not args.quiet:
        sys.stderr.write('#')
    return list(join_search_results(list(result.values())))","for (subindex, subresult) in result.items():
    result[subindex] = [pkg for pkg in subresult if subindex in pkg.name]",XXX,no_found,0,,,
ansible-modules-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-core/cloud/amazon/ec2_ami_find.py,https://github.com/ansible/ansible-modules-core/tree/master/cloud/amazon/ec2_ami_find.py,,get_block_device_mapping$279,"def get_block_device_mapping(image):
    """"""
    Retrieves block device mapping from AMI
    """"""
    bdm_dict = dict()
    bdm = getattr(image, 'block_device_mapping')
    for device_name in bdm.keys():
        bdm_dict[device_name] = {'size': bdm[device_name].size, 'snapshot_id': bdm[device_name].snapshot_id, 'volume_type': bdm[device_name].volume_type, 'encrypted': bdm[device_name].encrypted, 'delete_on_termination': bdm[device_name].delete_on_termination}
    return bdm_dict","for device_name in bdm.keys():
    bdm_dict[device_name] = {'size': bdm[device_name].size, 'snapshot_id': bdm[device_name].snapshot_id, 'volume_type': bdm[device_name].volume_type, 'encrypted': bdm[device_name].encrypted, 'delete_on_termination': bdm[device_name].delete_on_termination}",XXX,no_found,0,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/cloud/clouds/parallels.py,https://github.com/saltstack/salt/tree/master/salt/cloud/clouds/parallels.py,,show_instance$468,"def show_instance(name, call=None):
    """"""
    Show the details from Parallels concerning an instance
    """"""
    if call != 'action':
        raise SaltCloudSystemExit('The show_instance action must be called with -a or --action.')
    items = query(action='ve', command=name)
    ret = {}
    for item in items:
        if 'text' in item.__dict__:
            ret[item.tag] = item.text
        else:
            ret[item.tag] = item.attrib
        if item._children:
            ret[item.tag] = {}
            children = item._children
            for child in children:
                ret[item.tag][child.tag] = child.attrib
    __utils__['cloud.cache_node'](ret, _get_active_provider_name(), __opts__)
    return ret","for child in children:
    ret[item.tag][child.tag] = child.attrib",XXX,no_found,0,,,
Keras-TextClassification,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Keras-TextClassification/keras_textclassification/data_preprocess/generator_preprocess.py,https://github.com/yongzhuo/Keras-TextClassification/tree/master/keras_textclassification/data_preprocess/generator_preprocess.py,PreprocessGenerator,prereocess_pred_xid$40,"def prereocess_pred_xid(self, pred):
    if os.path.exists(self.path_fast_text_model_l2i_i2l):
        pred_l2i = {}
        l2i = self.l2i_i2l['l2i']
        for i in range(len(pred)):
            pred_l2i[pred[i]] = l2i[pred[i]]
        pred_l2i_rank = [sorted(pred_l2i.items(), key=lambda k: k[1], reverse=True)]
        return pred_l2i_rank
    else:
        raise RuntimeError('path_fast_text_model_label2index is None')","for i in range(len(pred)):
    pred_l2i[pred[i]] = l2i[pred[i]]",XXX,no_found,,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/smartos_vmadm.py,https://github.com/saltstack/salt/tree/master/salt/modules/smartos_vmadm.py,,create$701,"def create(from_file=None, **kwargs):
    """"""
    Create a new vm

    from_file : string
        json file to create the vm from -- if present, all other options will be ignored
    kwargs : string|int|...
        options to set for the vm

    CLI Example:

    .. code-block:: bash

        salt '*' vmadm.create from_file=/tmp/new_vm.json
        salt '*' vmadm.create image_uuid='...' alias='...' nics='[{ ""nic_tag"": ""admin"", ""ip"": ""198.51.100.123"", ...}, {...}]' [...]
    """"""
    ret = {}
    vmcfg = {}
    kwargs = salt.utils.args.clean_kwargs(**kwargs)
    for (k, v) in kwargs.items():
        vmcfg[k] = v
    if from_file:
        return _create_update_from_file('create', path=from_file)
    else:
        return _create_update_from_cfg('create', vmcfg=vmcfg)","for (k, v) in kwargs.items():
    vmcfg[k] = v",XXX,no_found,,,,
flasgger,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flasgger/flasgger/marshmallow_apispec.py,https://github.com/flasgger/flasgger/tree/master/flasgger/marshmallow_apispec.py,SwaggerView,dispatch_request$90,"def dispatch_request(self, *args, **kwargs):
    """"""
        If validation=True perform validation
        """"""
    if self.validation:
        specs = {}
        attrs = flasgger.constants.OPTIONAL_FIELDS + ['parameters', 'definitions', 'responses', 'summary', 'description']
        for attr in attrs:
            specs[attr] = getattr(self, attr)
        definitions = {}
        specs.update(convert_schemas(specs, definitions))
        specs['definitions'] = definitions
        flasgger.utils.validate(specs=specs, validation_function=self.validation_function, validation_error_handler=self.validation_error_handler)
    return super(SwaggerView, self).dispatch_request(*args, **kwargs)","for attr in attrs:
    specs[attr] = getattr(self, attr)",XXX,no_found,,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/cwl/util.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/cwl/util.py,,output_to_cwl_json$437,"def output_to_cwl_json(galaxy_output, get_metadata, get_dataset, get_extra_files, pseduo_location=False):
    """"""Convert objects in a Galaxy history into a CWL object.

    Useful in running conformance tests and implementing the cwl-runner
    interface via Galaxy.
    """"""

    def element_to_cwl_json(element):
        object = element['object']
        content_type = object.get('history_content_type')
        metadata = None
        if content_type is None:
            content_type = 'dataset_collection'
            metadata = element['object']
            metadata['history_content_type'] = content_type
        element_output = GalaxyOutput(galaxy_output.history_id, content_type, object['id'], metadata)
        return output_to_cwl_json(element_output, get_metadata, get_dataset, get_extra_files, pseduo_location=pseduo_location)
    output_metadata = galaxy_output.metadata
    if output_metadata is None:
        output_metadata = get_metadata(galaxy_output.history_content_type, galaxy_output.history_content_id)

    def dataset_dict_to_json_content(dataset_dict):
        if 'content' in dataset_dict:
            return json.loads(dataset_dict['content'])
        else:
            with open(dataset_dict['path']) as f:
                return json.load(f)
    if galaxy_output.history_content_type == 'raw_value':
        return galaxy_output.history_content_id
    elif output_metadata['history_content_type'] == 'dataset':
        ext = output_metadata['file_ext']
        if ext == 'expression.json':
            dataset_dict = get_dataset(output_metadata)
            return dataset_dict_to_json_content(dataset_dict)
        else:
            file_or_directory = 'Directory' if ext == 'directory' else 'File'
            secondary_files = []
            if file_or_directory == 'File':
                dataset_dict = get_dataset(output_metadata)
                properties = output_properties(pseduo_location=pseduo_location, **dataset_dict)
                basename = properties['basename']
                extra_files = get_extra_files(output_metadata)
                found_index = False
                for extra_file in extra_files:
                    if extra_file['class'] == 'File':
                        path = extra_file['path']
                        if path == SECONDARY_FILES_INDEX_PATH:
                            found_index = True
                if found_index:
                    ec = get_dataset(output_metadata, filename=SECONDARY_FILES_INDEX_PATH)
                    index = dataset_dict_to_json_content(ec)

                    def dir_listing(dir_path):
                        listing = []
                        for extra_file in extra_files:
                            path = extra_file['path']
                            extra_file_class = extra_file['class']
                            extra_file_basename = os.path.basename(path)
                            if os.path.join(dir_path, extra_file_basename) != path:
                                continue
                            if extra_file_class == 'File':
                                ec = get_dataset(output_metadata, filename=path)
                                ec['basename'] = extra_file_basename
                                ec_properties = output_properties(pseduo_location=pseduo_location, **ec)
                            elif extra_file_class == 'Directory':
                                ec_properties = {}
                                ec_properties['class'] = 'Directory'
                                ec_properties['location'] = ec_basename
                                ec_properties['listing'] = dir_listing(path)
                            else:
                                raise Exception('Unknown output type encountered....')
                            listing.append(ec_properties)
                        return listing
                    for basename in index['order']:
                        for extra_file in extra_files:
                            path = extra_file['path']
                            if path != os.path.join(SECONDARY_FILES_EXTRA_PREFIX, basename or ''):
                                continue
                            extra_file_class = extra_file['class']
                            if not STORE_SECONDARY_FILES_WITH_BASENAME:
                                ec_basename = basename + os.path.basename(path)
                            else:
                                ec_basename = os.path.basename(path)
                            if extra_file_class == 'File':
                                ec = get_dataset(output_metadata, filename=path)
                                ec['basename'] = ec_basename
                                ec_properties = output_properties(pseduo_location=pseduo_location, **ec)
                            elif extra_file_class == 'Directory':
                                ec_properties = {}
                                ec_properties['class'] = 'Directory'
                                ec_properties['location'] = ec_basename
                                ec_properties['listing'] = dir_listing(path)
                            else:
                                raise Exception('Unknown output type encountered....')
                            secondary_files.append(ec_properties)
            else:
                basename = output_metadata.get('created_from_basename')
                if not basename:
                    basename = output_metadata.get('name')
                listing: List[OutputPropertiesType] = []
                properties = {'class': 'Directory', 'basename': basename, 'listing': listing}
                extra_files = get_extra_files(output_metadata)
                for extra_file in extra_files:
                    if extra_file['class'] == 'File':
                        path = extra_file['path']
                        ec = get_dataset(output_metadata, filename=path)
                        ec['basename'] = os.path.basename(path)
                        ec_properties = output_properties(pseduo_location=pseduo_location, **ec)
                        listing.append(ec_properties)
            if secondary_files:
                properties['secondaryFiles'] = secondary_files
            return properties
    elif output_metadata['history_content_type'] == 'dataset_collection':
        collection_type = output_metadata['collection_type'].split(':', 1)[0]
        if collection_type in ['list', 'paired']:
            rval_l = []
            for element in output_metadata['elements']:
                rval_l.append(element_to_cwl_json(element))
            return rval_l
        elif collection_type == 'record':
            rval_d = {}
            for element in output_metadata['elements']:
                rval_d[element['element_identifier']] = element_to_cwl_json(element)
            return rval_d
        return None
    else:
        raise NotImplementedError('Unknown history content type encountered')","for element in output_metadata['elements']:
    rval_d[element['element_identifier']] = element_to_cwl_json(element)",XXX,no_found,0,,,
PyRetri,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyRetri/search/reid_search_index.py,https://github.com/PyRetri/PyRetri/tree/master/search/reid_search_index.py,,main$44,"def main():
    args = parse_args()
    assert args.fea_dir is not None, 'the feature directory must be provided!'
    assert args.search_modules is not None, 'the search modules must be provided!'
    assert args.save_path is not None, 'the save path must be provided!'
    cfg = get_defaults_cfg()
    datasets = load_datasets()
    indexes = importlib.import_module('{}.index_dict'.format(args.search_modules)).indexes
    evaluates = importlib.import_module('{}.index_dict'.format(args.search_modules)).evaluates
    if os.path.exists(args.save_path):
        with open(args.save_path, 'r') as f:
            results = json.load(f)
    else:
        results = list()
    for dir in os.listdir(args.fea_dir):
        for (data_name, data_args) in datasets.items():
            for (index_name, index_args) in indexes.items():
                if data_name in dir:
                    print(dir)
                    (gallery_fea_dir, query_fea_dir, train_fea_dir) = get_dir(args.fea_dir, dir, data_args)
                    evaluate_args = evaluates['reid_overall']
                    for dim_proc in index_args.dim_processors.names:
                        if dim_proc in ['PartPCA', 'PartSVD', 'PCA', 'SVD']:
                            index_args.dim_processors[dim_proc].train_fea_dir = train_fea_dir
                    for fea_name in fea_names:
                        result_dict = get_default_result_dict(dir, data_name, index_name, fea_name)
                        if check_result_exist(result_dict, results):
                            print('[Search Query]: config exists...')
                            continue
                        index_args.feature_names = [fea_name]
                        cfg.index.merge_from_other_cfg(index_args)
                        cfg.evaluate.merge_from_other_cfg(evaluate_args)
                        (query_fea, query_info, _) = feature_loader.load(query_fea_dir, [fea_name])
                        (gallery_fea, gallery_info, _) = feature_loader.load(gallery_fea_dir, [fea_name])
                        index_helper = build_index_helper(cfg.index)
                        (index_result_info, _, _) = index_helper.do_index(query_fea, query_info, gallery_fea)
                        evaluate_helper = build_evaluate_helper(cfg.evaluate)
                        (mAP, recall_at_k) = evaluate_helper.do_eval(index_result_info, gallery_info)
                        to_save_recall = dict()
                        for k in recall_at_k:
                            to_save_recall[str(k)] = recall_at_k[k]
                        result_dict['mAP'] = float(mAP)
                        result_dict['recall_at_k'] = to_save_recall
                        results.append(result_dict)
                        with open(args.save_path, 'w') as f:
                            json.dump(results, f)","for k in recall_at_k:
    to_save_recall[str(k)] = recall_at_k[k]",XXX,no_found,0,,,
nyaa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nyaa/nyaa/utils.py,https://github.com/nyaadevs/nyaa/tree/master/nyaa/utils.py,,flatten_dict$47,"def flatten_dict(d, result=None):
    if result is None:
        result = {}
    for key in d:
        value = d[key]
        if isinstance(value, dict):
            value1 = {}
            for keyIn in value:
                value1['/'.join([key, keyIn])] = value[keyIn]
            flatten_dict(value1, result)
        elif isinstance(value, (list, tuple)):
            for (indexB, element) in enumerate(value):
                if isinstance(element, dict):
                    value1 = {}
                    index = 0
                    for keyIn in element:
                        newkey = '/'.join([key, keyIn])
                        value1[newkey] = value[indexB][keyIn]
                        index += 1
                    for keyA in value1:
                        flatten_dict(value1, result)
        else:
            result[key] = value
    return result","for keyIn in value:
    value1['/'.join([key, keyIn])] = value[keyIn]",XXX,no_found,,,,
animation_nodes,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/animation_nodes/animation_nodes/auto_load.py,https://github.com/JacquesLucke/animation_nodes/tree/master/animation_nodes/auto_load.py,,get_register_deps_dict$73,"def get_register_deps_dict(modules):
    my_classes = set(iter_my_classes(modules))
    my_classes_by_idname = {cls.bl_idname: cls for cls in my_classes if hasattr(cls, 'bl_idname')}
    deps_dict = {}
    for cls in my_classes:
        deps_dict[cls] = set(iter_my_register_deps(cls, my_classes, my_classes_by_idname))
    return deps_dict","for cls in my_classes:
    deps_dict[cls] = set(iter_my_register_deps(cls, my_classes, my_classes_by_idname))",XXX,no_found,0,,,
pgmpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgmpy/pgmpy/readwrite/UAI.py,https://github.com/pgmpy/pgmpy/tree/master/pgmpy/readwrite/UAI.py,UAIWriter,get_domain$349,"def get_domain(self):
    """"""
        Adds domain of each variable to the network.

        Examples
        --------
        >>> from pgmpy.readwrite import UAIWriter
        >>> writer = UAIWriter(model)
        >>> writer.get_domain()
        """"""
    if isinstance(self.model, BayesianNetwork):
        cpds = self.model.get_cpds()
        cpds.sort(key=lambda x: x.variable)
        domain = {}
        for cpd in cpds:
            domain[cpd.variable] = str(cpd.variable_card)
        return domain
    elif isinstance(self.model, MarkovNetwork):
        factors = self.model.get_factors()
        domain = {}
        for factor in factors:
            variables = factor.variables
            for var in variables:
                if var not in domain:
                    domain[var] = str(factor.get_cardinality([var])[var])
        return domain
    else:
        raise TypeError('Model must be an instance of Markov or Bayesian model.')","for cpd in cpds:
    domain[cpd.variable] = str(cpd.variable_card)",XXX,no_found,,,,
pgmpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgmpy/pgmpy/readwrite/UAI.py,https://github.com/pgmpy/pgmpy/tree/master/pgmpy/readwrite/UAI.py,UAIWriter,get_domain$349,"def get_domain(self):
    """"""
        Adds domain of each variable to the network.

        Examples
        --------
        >>> from pgmpy.readwrite import UAIWriter
        >>> writer = UAIWriter(model)
        >>> writer.get_domain()
        """"""
    if isinstance(self.model, BayesianNetwork):
        cpds = self.model.get_cpds()
        cpds.sort(key=lambda x: x.variable)
        domain = {}
        for cpd in cpds:
            domain[cpd.variable] = str(cpd.variable_card)
        return domain
    elif isinstance(self.model, MarkovNetwork):
        factors = self.model.get_factors()
        domain = {}
        for factor in factors:
            variables = factor.variables
            for var in variables:
                if var not in domain:
                    domain[var] = str(factor.get_cardinality([var])[var])
        return domain
    else:
        raise TypeError('Model must be an instance of Markov or Bayesian model.')","for factor in factors:
    variables = factor.variables
    for var in variables:
        if var not in domain:
            domain[var] = str(factor.get_cardinality([var])[var])",XXX,no_found,0,,,
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/autotvm/graph_tuner/utils/traverse_graph.py,https://github.com/apache/tvm/tree/master/python/tvm/autotvm/graph_tuner/utils/traverse_graph.py,,get_out_nodes$312,"def get_out_nodes(in_node_dict):
    """"""Create output dictionary from input dictionary.

    Parameters
    ----------
    in_node_dict : dict of int to list of int
        Dictionary maps node index to closest input ancestors.
        It can be created with get_in_nodes.

    Returns
    -------
    out : dict of int to list of int
        Dictionary maps node index to closest output nodes.
    """"""
    out_node_dict = {}
    for key in in_node_dict:
        out_node_dict[key] = []
    for (key, val) in in_node_dict.items():
        for item in val:
            if item in out_node_dict:
                out_node_dict[item].append(key)
            else:
                out_node_dict[item] = [key]
    return out_node_dict","for key in in_node_dict:
    out_node_dict[key] = []",XXX,no_found,0,,,
tvm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/autotvm/graph_tuner/utils/traverse_graph.py,https://github.com/apache/tvm/tree/master/python/tvm/autotvm/graph_tuner/utils/traverse_graph.py,,get_out_nodes$312,"def get_out_nodes(in_node_dict):
    """"""Create output dictionary from input dictionary.

    Parameters
    ----------
    in_node_dict : dict of int to list of int
        Dictionary maps node index to closest input ancestors.
        It can be created with get_in_nodes.

    Returns
    -------
    out : dict of int to list of int
        Dictionary maps node index to closest output nodes.
    """"""
    out_node_dict = {}
    for key in in_node_dict:
        out_node_dict[key] = []
    for (key, val) in in_node_dict.items():
        for item in val:
            if item in out_node_dict:
                out_node_dict[item].append(key)
            else:
                out_node_dict[item] = [key]
    return out_node_dict","for (key, val) in in_node_dict.items():
    for item in val:
        if item in out_node_dict:
            out_node_dict[item].append(key)
        else:
            out_node_dict[item] = [key]",XXX,no_found,,,,
lxmert,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lxmert/src/lxrt/entry.py,https://github.com/airsplay/lxmert/tree/master/src/lxrt/entry.py,LXRTEncoder,load$126,"def load(self, path):
    print('Load LXMERT pre-trained model from %s' % path)
    state_dict = torch.load('%s_LXRT.pth' % path)
    new_state_dict = {}
    for (key, value) in state_dict.items():
        if key.startswith('module.'):
            new_state_dict[key[len('module.'):]] = value
        else:
            new_state_dict[key] = value
    state_dict = new_state_dict
    load_keys = set(state_dict.keys())
    model_keys = set(self.model.state_dict().keys())
    print()
    print('Weights in loaded but not in model:')
    for key in sorted(load_keys.difference(model_keys)):
        print(key)
    print()
    print('Weights in model but not in loaded:')
    for key in sorted(model_keys.difference(load_keys)):
        print(key)
    print()
    self.model.load_state_dict(state_dict, strict=False)","for (key, value) in state_dict.items():
    if key.startswith('module.'):
        new_state_dict[key[len('module.'):]] = value
    else:
        new_state_dict[key] = value",XXX,no_found,,,,
ByteTrack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ByteTrack/tutorials/cstrack/tracker.py,https://github.com/ifzhang/ByteTrack/tree/master/tutorials/cstrack/tracker.py,,sub_stracks$396,"def sub_stracks(tlista, tlistb):
    stracks = {}
    for t in tlista:
        stracks[t.track_id] = t
    for t in tlistb:
        tid = t.track_id
        if stracks.get(tid, 0):
            del stracks[tid]
    return list(stracks.values())","for t in tlista:
    stracks[t.track_id] = t",XXX,no_found,,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/solarispkg.py,https://github.com/saltstack/salt/tree/master/salt/modules/solarispkg.py,,latest_version$142,"def latest_version(*names, **kwargs):
    """"""
    Return the latest version of the named package available for upgrade or
    installation. If more than one package name is specified, a dict of
    name/version pairs is returned.

    If the latest version of a given package is already installed, an empty
    string will be returned for that package.

    CLI Example:

    .. code-block:: bash

        salt '*' pkg.latest_version <package name>
        salt '*' pkg.latest_version <package1> <package2> <package3> ...

    NOTE: As package repositories are not presently supported for Solaris
    pkgadd, this function will always return an empty string for a given
    package.
    """"""
    kwargs.pop('refresh', True)
    ret = {}
    if not names:
        return ''
    for name in names:
        ret[name] = ''
    if len(names) == 1:
        return ret[names[0]]
    return ret","for name in names:
    ret[name] = ''",XXX,no_found,,,,
pass-import,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pass-import/pass_import/managers/lastpass.py,https://github.com/roddhjav/pass-import/tree/master/pass_import/managers/lastpass.py,LastpassCLI,show$91,"def show(self, uid):
    """"""Decrypt a lastpass entry and read the credentials.

        lpass do not show the same data with the --json option and without.
        To retrieve the full entry, both --json and --format option need to
        be used.

        :param str uid: UniqueID to the password entry to decrypt.
        :return dict: Return a dictionary with of the password entry.

        """"""
    entry = {}
    ignores = {'fullname'}
    keys = self.invkeys()
    jsons = self._command(['show', '--json', uid])
    item = json.loads(jsons).pop()
    for (key, value) in item.items():
        if key not in ignores:
            entry[keys.get(key, key)] = value
    entry['group'] = self._path(item['group'])
    ignores = {'Username', 'Password', 'URL'}
    arg = ['show', '--color=never', '--format=%fn|%fv', '--color=never', uid]
    data = self._command(arg).split('\n')
    data.pop()
    data.pop(0)
    for line in data:
        if '|' in line:
            (key, value) = line.split('|', 1)
            if key not in ignores:
                entry[key] = value
    if entry.get('url', '') == 'http://':
        entry['url'] = ''
    return entry","for (key, value) in item.items():
    if key not in ignores:
        entry[keys.get(key, key)] = value",XXX,no_found,,,,
pass-import,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pass-import/pass_import/managers/lastpass.py,https://github.com/roddhjav/pass-import/tree/master/pass_import/managers/lastpass.py,LastpassCLI,show$91,"def show(self, uid):
    """"""Decrypt a lastpass entry and read the credentials.

        lpass do not show the same data with the --json option and without.
        To retrieve the full entry, both --json and --format option need to
        be used.

        :param str uid: UniqueID to the password entry to decrypt.
        :return dict: Return a dictionary with of the password entry.

        """"""
    entry = {}
    ignores = {'fullname'}
    keys = self.invkeys()
    jsons = self._command(['show', '--json', uid])
    item = json.loads(jsons).pop()
    for (key, value) in item.items():
        if key not in ignores:
            entry[keys.get(key, key)] = value
    entry['group'] = self._path(item['group'])
    ignores = {'Username', 'Password', 'URL'}
    arg = ['show', '--color=never', '--format=%fn|%fv', '--color=never', uid]
    data = self._command(arg).split('\n')
    data.pop()
    data.pop(0)
    for line in data:
        if '|' in line:
            (key, value) = line.split('|', 1)
            if key not in ignores:
                entry[key] = value
    if entry.get('url', '') == 'http://':
        entry['url'] = ''
    return entry","for line in data:
    if '|' in line:
        (key, value) = line.split('|', 1)
        if key not in ignores:
            entry[key] = value",XXX,no_found,0,,,
ivre,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ivre/ivre/tools/flowcli.py,https://github.com/ivre/ivre/tree/master/ivre/tools/flowcli.py,,main$78,"def main() -> None:
    parser = ArgumentParser(description=__doc__)
    parser.add_argument('--init', '--purgedb', action='store_true', help='Purge or create and initialize the database.')
    parser.add_argument('--ensure-indexes', action='store_true', help='Create missing indexes (will lock the database).')
    parser.add_argument('--node-filters', '-n', nargs='+', metavar='FILTER', help='Filter the results with a list of ivre specific node textual filters (see WebUI doc in FLOW.md).')
    parser.add_argument('--flow-filters', '-f', nargs='+', metavar='FILTER', help='Filter the results with a list of ivre specific flow textual filters (see WebUI doc in FLOW.md).')
    parser.add_argument('--json', '-j', action='store_true', help='Outputs the full json records of results.')
    parser.add_argument('--count', '-c', action='store_true', help='Only return the count of the results.')
    parser.add_argument('--limit', '-l', type=int, default=None, help='Output at most LIMIT results.')
    parser.add_argument('--skip', type=int, default=0, help='Skip first SKIP results.')
    parser.add_argument('--orderby', '-o', help='Order of results (""src"", ""dst"" or ""flow"")')
    parser.add_argument('--separator', '-s', help='Separator string.')
    parser.add_argument('--top', '-t', nargs='+', help='Top flows for a given set of fields, e.g. ""--top src.addr dport"".')
    parser.add_argument('--collect', '-C', nargs='+', help='When using --top, also collect these properties.', default=[])
    parser.add_argument('--sum', '-S', nargs='+', help='When using --top, sum on these properties to order the result.', default=[])
    parser.add_argument('--least', '-L', action='store_true', help='When using --top, sort records by least')
    parser.add_argument('--mode', '-m', help='Query special mode (flow_map, talk_map...)')
    parser.add_argument('--timeline', '-T', action='store_true', help='Retrieves the timeline of each flow')
    parser.add_argument('--flow-daily', action='store_true', help='Flow count per times of the day. If --precision is absent, it will be based on FLOW_TIME_PRECISION (%d)' % config.FLOW_TIME_PRECISION)
    parser.add_argument('--plot', action='store_true', help='Plot data when possible (requires matplotlib).')
    parser.add_argument('--fields', nargs='*', help='Without values, gives the list of available fields. Otherwise, display these fields for each entry.')
    parser.add_argument('--reduce-precision', type=int, metavar='NEW_PRECISION', help='Only with MongoDB backend. Reduce precision to NEW_PRECISION for flows timeslots. Uses precision, before, after and filters.')
    parser.add_argument('--after', '-a', type=str, help='Only with MongoDB backend. Get only flows seen after this date. Date format: YEAR-MONTH-DAY HOUR:MINUTE. Based on timeslots precision. If the given date is in the middle of a timeslot, flows start at the next timeslot.')
    parser.add_argument('--before', '-b', type=str, help='Only with MongoDB backend. Get only flows seen before this date. Date format: YEAR-MONTH-DAY HOUR:MINUTE. Based on timeslots precision. If the given date is in the middle of a timeslot, the whole period is kept even if theoretically some flows may have been seen after the given date.')
    parser.add_argument('--precision', nargs='?', default=None, const=0, help='Only With MongoDB backend. If PRECISION is specified, get only flows with one timeslot of the given precision. Otherwise, list precisions.', type=int)
    parser.add_argument('--host', type=str, metavar='HOST', help='Filter on source OR destination IP. Accepts IP address or CIDR.')
    parser.add_argument('--src', type=str, metavar='SRC', help='Filter on source IP. Accepts IP address or CIDR.')
    parser.add_argument('--dst', type=str, metavar='DST', help='Filter on destination IP. Accepts IP address or CIDR.')
    parser.add_argument('--proto', type=str, metavar='PROTO', help='Filter on transport protocol.')
    parser.add_argument('--tcp', action='store_true', help='Alias to --proto tcp')
    parser.add_argument('--udp', action='store_true', help='Alias to --proto udp')
    parser.add_argument('--port', type=int, metavar='PORT', help='Alias to --dport')
    parser.add_argument('--dport', type=int, metavar='DPORT', help='Filter on destination port.')
    parser.add_argument('--sport', type=int, metavar='SPORT', help='Filter on source port.')
    args = parser.parse_args()
    out = sys.stdout
    if args.plot and plt is None:
        utils.LOGGER.critical('Matplotlib is required for --plot')
        sys.exit(-1)
    if args.init:
        if os.isatty(sys.stdin.fileno()):
            out.write('This will remove any flow result in your database. Process ? [y/N] ')
            ans = input()
            if ans.lower() != 'y':
                sys.exit(-1)
        db.flow.init()
        sys.exit(0)
    if args.ensure_indexes:
        if os.isatty(sys.stdin.fileno()):
            out.write('This will lock your database. Process ? [y/N] ')
            ans = input()
            if ans.lower() != 'y':
                sys.exit(-1)
        db.flow.ensure_indexes()
        sys.exit(0)
    if args.fields is not None and (not args.fields):
        print_fields()
        sys.exit(0)
    elif args.fields is not None:
        for field in args.fields:
            ivre.flow.validate_field(field)
    if args.precision == 0:
        out.writelines(('%d\n' % precision for precision in db.flow.list_precisions()))
        sys.exit(0)
    filters = {'nodes': args.node_filters or [], 'edges': args.flow_filters or []}
    args_dict = vars(args)
    for key in addr_fields:
        if args_dict[key] is not None:
            (flt_t, flt_v) = get_addr_argument(key, args_dict[key])
            filters[flt_t].append(flt_v)
    if args.proto is not None:
        filters['edges'].append('proto = %s' % args.proto)
    for key in ['tcp', 'udp']:
        if args_dict[key]:
            filters['edges'].append('proto = %s' % key)
    for key in ['port', 'dport']:
        if args_dict[key] is not None:
            filters['edges'].append('dport = %d' % args_dict[key])
    if args.sport is not None:
        filters['edges'].append('ANY sports = %d' % args.sport)
    time_args = ['before', 'after']
    time_values = {}
    for arg in time_args:
        time_values[arg] = datetime.datetime.strptime(args_dict[arg], '%Y-%m-%d %H:%M') if args_dict[arg] is not None else None
    query = db.flow.from_filters(filters, limit=args.limit, skip=args.skip, orderby=args.orderby, mode=args.mode, timeline=args.timeline, after=time_values['after'], before=time_values['before'], precision=args.precision)
    if args.reduce_precision:
        if os.isatty(sys.stdin.fileno()):
            out.write('This will permanently reduce the precision of your database. Process ? [y/N] ')
            ans = input()
            if ans.lower() != 'y':
                sys.exit(-1)
        new_precision = args.reduce_precision
        db.flow.reduce_precision(new_precision, flt=query, before=time_values['before'], after=time_values['after'], current_precision=args.precision)
        sys.exit(0)
    sep = args.separator or ' | '
    coma = ' ;' if args.separator else ' ; '
    coma2 = ',' if args.separator else ', '
    if args.count:
        count = db.flow.count(query)
        out.write('%(clients)d clients\n%(servers)d servers\n%(flows)d flows\n' % count)
    elif args.top:
        top = db.flow.topvalues(query, args.top, collect_fields=args.collect, sum_fields=args.sum, topnbr=args.limit, skip=args.skip, least=args.least)
        for rec in top:
            sys.stdout.write('%s%s%s%s%s\n' % ('(' + coma2.join((str(val) for val in rec['fields'])) + ')', sep, rec['count'], sep, coma.join((str('(' + coma2.join((str(val) for val in collected)) + ')') for collected in rec['collected'])) if rec['collected'] else ''))
    elif args.flow_daily:
        precision = args.precision if args.precision is not None else config.FLOW_TIME_PRECISION
        plot_data: Dict[str, Dict[datetime.datetime, int]] = {}
        for rec in db.flow.flow_daily(precision, query, after=time_values['after'], before=time_values['before']):
            out.write(sep.join([rec['time_in_day'].strftime('%T.%f'), ' ; '.join(['(' + x[0] + ', ' + str(x[1]) + ')' for x in rec['flows']])]))
            out.write('\n')
            if args.plot:
                for flw in rec['flows']:
                    t = rec['time_in_day']
                    dt = datetime.datetime(1970, 1, 1, hour=t.hour, minute=t.minute, second=t.second)
                    plot_data.setdefault(flw[0], {})
                    plot_data[flw[0]][dt] = flw[1]
        if args.plot and plot_data:
            t = datetime.datetime(1970, 1, 1, 0, 0, 0)
            t += datetime.timedelta(seconds=config.FLOW_TIME_BASE % precision)
            times = []
            while t < datetime.datetime(1970, 1, 2):
                times.append(t)
                t = t + datetime.timedelta(seconds=precision)
            ax = plt.subplots()[1]
            fmt = matplotlib.dates.DateFormatter('%H:%M:%S')
            for (flow, data) in plot_data.items():
                values = [data[ti] if ti in data else 0 for ti in times]
                plt.step(times, values, '.-', where='post', label=flow)
            plt.legend(loc='best')
            ax.xaxis.set_major_formatter(fmt)
            plt.gcf().autofmt_xdate()
            plt.show()
    else:
        fmt = '%%s%s%%s%s%%s' % (sep, sep)
        node_width = len('XXXX:XXXX:XXXX:XXXX:XXXX:XXXX')
        flow_width = len('tcp/XXXXX')
        for res in db.flow.to_iter(query, limit=args.limit, skip=args.skip, orderby=args.orderby, mode=args.mode, timeline=args.timeline):
            if args.json:
                out.write('%s\n' % res)
            else:
                elts = {}
                for elt in ['src', 'flow', 'dst']:
                    elts[elt] = res[elt]['label']
                    if args.fields:
                        elts[elt] = '%s%s%s' % (elts[elt], coma, coma.join((str(res[elt]['data'].get(field, '')) for field in args.fields)))
                (src, flow, dst) = (elts['src'], elts['flow'], elts['dst'])
                node_width = max(node_width, len(src), len(dst))
                flow_width = max(flow_width, len(flow))
                if not args.separator:
                    fmt = '%%-%ds%s%%-%ds%s%%-%ds' % (node_width, sep, flow_width, sep, node_width)
                out.write(fmt % (src, flow, dst))
                if args.timeline:
                    out.write(sep)
                    try:
                        out.write(coma.join((str(elt) for elt in sorted(res['flow']['data']['meta']['times']))))
                    except KeyError:
                        out.write('?')
                out.write('\n')","for arg in time_args:
    time_values[arg] = datetime.datetime.strptime(args_dict[arg], '%Y-%m-%d %H:%M') if args_dict[arg] is not None else None",XXX,no_found,,,,
ivre,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ivre/ivre/tools/flowcli.py,https://github.com/ivre/ivre/tree/master/ivre/tools/flowcli.py,,main$78,"def main() -> None:
    parser = ArgumentParser(description=__doc__)
    parser.add_argument('--init', '--purgedb', action='store_true', help='Purge or create and initialize the database.')
    parser.add_argument('--ensure-indexes', action='store_true', help='Create missing indexes (will lock the database).')
    parser.add_argument('--node-filters', '-n', nargs='+', metavar='FILTER', help='Filter the results with a list of ivre specific node textual filters (see WebUI doc in FLOW.md).')
    parser.add_argument('--flow-filters', '-f', nargs='+', metavar='FILTER', help='Filter the results with a list of ivre specific flow textual filters (see WebUI doc in FLOW.md).')
    parser.add_argument('--json', '-j', action='store_true', help='Outputs the full json records of results.')
    parser.add_argument('--count', '-c', action='store_true', help='Only return the count of the results.')
    parser.add_argument('--limit', '-l', type=int, default=None, help='Output at most LIMIT results.')
    parser.add_argument('--skip', type=int, default=0, help='Skip first SKIP results.')
    parser.add_argument('--orderby', '-o', help='Order of results (""src"", ""dst"" or ""flow"")')
    parser.add_argument('--separator', '-s', help='Separator string.')
    parser.add_argument('--top', '-t', nargs='+', help='Top flows for a given set of fields, e.g. ""--top src.addr dport"".')
    parser.add_argument('--collect', '-C', nargs='+', help='When using --top, also collect these properties.', default=[])
    parser.add_argument('--sum', '-S', nargs='+', help='When using --top, sum on these properties to order the result.', default=[])
    parser.add_argument('--least', '-L', action='store_true', help='When using --top, sort records by least')
    parser.add_argument('--mode', '-m', help='Query special mode (flow_map, talk_map...)')
    parser.add_argument('--timeline', '-T', action='store_true', help='Retrieves the timeline of each flow')
    parser.add_argument('--flow-daily', action='store_true', help='Flow count per times of the day. If --precision is absent, it will be based on FLOW_TIME_PRECISION (%d)' % config.FLOW_TIME_PRECISION)
    parser.add_argument('--plot', action='store_true', help='Plot data when possible (requires matplotlib).')
    parser.add_argument('--fields', nargs='*', help='Without values, gives the list of available fields. Otherwise, display these fields for each entry.')
    parser.add_argument('--reduce-precision', type=int, metavar='NEW_PRECISION', help='Only with MongoDB backend. Reduce precision to NEW_PRECISION for flows timeslots. Uses precision, before, after and filters.')
    parser.add_argument('--after', '-a', type=str, help='Only with MongoDB backend. Get only flows seen after this date. Date format: YEAR-MONTH-DAY HOUR:MINUTE. Based on timeslots precision. If the given date is in the middle of a timeslot, flows start at the next timeslot.')
    parser.add_argument('--before', '-b', type=str, help='Only with MongoDB backend. Get only flows seen before this date. Date format: YEAR-MONTH-DAY HOUR:MINUTE. Based on timeslots precision. If the given date is in the middle of a timeslot, the whole period is kept even if theoretically some flows may have been seen after the given date.')
    parser.add_argument('--precision', nargs='?', default=None, const=0, help='Only With MongoDB backend. If PRECISION is specified, get only flows with one timeslot of the given precision. Otherwise, list precisions.', type=int)
    parser.add_argument('--host', type=str, metavar='HOST', help='Filter on source OR destination IP. Accepts IP address or CIDR.')
    parser.add_argument('--src', type=str, metavar='SRC', help='Filter on source IP. Accepts IP address or CIDR.')
    parser.add_argument('--dst', type=str, metavar='DST', help='Filter on destination IP. Accepts IP address or CIDR.')
    parser.add_argument('--proto', type=str, metavar='PROTO', help='Filter on transport protocol.')
    parser.add_argument('--tcp', action='store_true', help='Alias to --proto tcp')
    parser.add_argument('--udp', action='store_true', help='Alias to --proto udp')
    parser.add_argument('--port', type=int, metavar='PORT', help='Alias to --dport')
    parser.add_argument('--dport', type=int, metavar='DPORT', help='Filter on destination port.')
    parser.add_argument('--sport', type=int, metavar='SPORT', help='Filter on source port.')
    args = parser.parse_args()
    out = sys.stdout
    if args.plot and plt is None:
        utils.LOGGER.critical('Matplotlib is required for --plot')
        sys.exit(-1)
    if args.init:
        if os.isatty(sys.stdin.fileno()):
            out.write('This will remove any flow result in your database. Process ? [y/N] ')
            ans = input()
            if ans.lower() != 'y':
                sys.exit(-1)
        db.flow.init()
        sys.exit(0)
    if args.ensure_indexes:
        if os.isatty(sys.stdin.fileno()):
            out.write('This will lock your database. Process ? [y/N] ')
            ans = input()
            if ans.lower() != 'y':
                sys.exit(-1)
        db.flow.ensure_indexes()
        sys.exit(0)
    if args.fields is not None and (not args.fields):
        print_fields()
        sys.exit(0)
    elif args.fields is not None:
        for field in args.fields:
            ivre.flow.validate_field(field)
    if args.precision == 0:
        out.writelines(('%d\n' % precision for precision in db.flow.list_precisions()))
        sys.exit(0)
    filters = {'nodes': args.node_filters or [], 'edges': args.flow_filters or []}
    args_dict = vars(args)
    for key in addr_fields:
        if args_dict[key] is not None:
            (flt_t, flt_v) = get_addr_argument(key, args_dict[key])
            filters[flt_t].append(flt_v)
    if args.proto is not None:
        filters['edges'].append('proto = %s' % args.proto)
    for key in ['tcp', 'udp']:
        if args_dict[key]:
            filters['edges'].append('proto = %s' % key)
    for key in ['port', 'dport']:
        if args_dict[key] is not None:
            filters['edges'].append('dport = %d' % args_dict[key])
    if args.sport is not None:
        filters['edges'].append('ANY sports = %d' % args.sport)
    time_args = ['before', 'after']
    time_values = {}
    for arg in time_args:
        time_values[arg] = datetime.datetime.strptime(args_dict[arg], '%Y-%m-%d %H:%M') if args_dict[arg] is not None else None
    query = db.flow.from_filters(filters, limit=args.limit, skip=args.skip, orderby=args.orderby, mode=args.mode, timeline=args.timeline, after=time_values['after'], before=time_values['before'], precision=args.precision)
    if args.reduce_precision:
        if os.isatty(sys.stdin.fileno()):
            out.write('This will permanently reduce the precision of your database. Process ? [y/N] ')
            ans = input()
            if ans.lower() != 'y':
                sys.exit(-1)
        new_precision = args.reduce_precision
        db.flow.reduce_precision(new_precision, flt=query, before=time_values['before'], after=time_values['after'], current_precision=args.precision)
        sys.exit(0)
    sep = args.separator or ' | '
    coma = ' ;' if args.separator else ' ; '
    coma2 = ',' if args.separator else ', '
    if args.count:
        count = db.flow.count(query)
        out.write('%(clients)d clients\n%(servers)d servers\n%(flows)d flows\n' % count)
    elif args.top:
        top = db.flow.topvalues(query, args.top, collect_fields=args.collect, sum_fields=args.sum, topnbr=args.limit, skip=args.skip, least=args.least)
        for rec in top:
            sys.stdout.write('%s%s%s%s%s\n' % ('(' + coma2.join((str(val) for val in rec['fields'])) + ')', sep, rec['count'], sep, coma.join((str('(' + coma2.join((str(val) for val in collected)) + ')') for collected in rec['collected'])) if rec['collected'] else ''))
    elif args.flow_daily:
        precision = args.precision if args.precision is not None else config.FLOW_TIME_PRECISION
        plot_data: Dict[str, Dict[datetime.datetime, int]] = {}
        for rec in db.flow.flow_daily(precision, query, after=time_values['after'], before=time_values['before']):
            out.write(sep.join([rec['time_in_day'].strftime('%T.%f'), ' ; '.join(['(' + x[0] + ', ' + str(x[1]) + ')' for x in rec['flows']])]))
            out.write('\n')
            if args.plot:
                for flw in rec['flows']:
                    t = rec['time_in_day']
                    dt = datetime.datetime(1970, 1, 1, hour=t.hour, minute=t.minute, second=t.second)
                    plot_data.setdefault(flw[0], {})
                    plot_data[flw[0]][dt] = flw[1]
        if args.plot and plot_data:
            t = datetime.datetime(1970, 1, 1, 0, 0, 0)
            t += datetime.timedelta(seconds=config.FLOW_TIME_BASE % precision)
            times = []
            while t < datetime.datetime(1970, 1, 2):
                times.append(t)
                t = t + datetime.timedelta(seconds=precision)
            ax = plt.subplots()[1]
            fmt = matplotlib.dates.DateFormatter('%H:%M:%S')
            for (flow, data) in plot_data.items():
                values = [data[ti] if ti in data else 0 for ti in times]
                plt.step(times, values, '.-', where='post', label=flow)
            plt.legend(loc='best')
            ax.xaxis.set_major_formatter(fmt)
            plt.gcf().autofmt_xdate()
            plt.show()
    else:
        fmt = '%%s%s%%s%s%%s' % (sep, sep)
        node_width = len('XXXX:XXXX:XXXX:XXXX:XXXX:XXXX')
        flow_width = len('tcp/XXXXX')
        for res in db.flow.to_iter(query, limit=args.limit, skip=args.skip, orderby=args.orderby, mode=args.mode, timeline=args.timeline):
            if args.json:
                out.write('%s\n' % res)
            else:
                elts = {}
                for elt in ['src', 'flow', 'dst']:
                    elts[elt] = res[elt]['label']
                    if args.fields:
                        elts[elt] = '%s%s%s' % (elts[elt], coma, coma.join((str(res[elt]['data'].get(field, '')) for field in args.fields)))
                (src, flow, dst) = (elts['src'], elts['flow'], elts['dst'])
                node_width = max(node_width, len(src), len(dst))
                flow_width = max(flow_width, len(flow))
                if not args.separator:
                    fmt = '%%-%ds%s%%-%ds%s%%-%ds' % (node_width, sep, flow_width, sep, node_width)
                out.write(fmt % (src, flow, dst))
                if args.timeline:
                    out.write(sep)
                    try:
                        out.write(coma.join((str(elt) for elt in sorted(res['flow']['data']['meta']['times']))))
                    except KeyError:
                        out.write('?')
                out.write('\n')","for elt in ['src', 'flow', 'dst']:
    elts[elt] = res[elt]['label']
    if args.fields:
        elts[elt] = '%s%s%s' % (elts[elt], coma, coma.join((str(res[elt]['data'].get(field, '')) for field in args.fields)))",XXX,no_found,0,,,
Hierarchical-Localization,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Hierarchical-Localization/hloc/match_features.py,https://github.com/cvg/Hierarchical-Localization/tree/master/hloc/match_features.py,FeaturePairsDataset,__getitem__$104,"def __getitem__(self, idx):
    (name0, name1) = self.pairs[idx]
    data = {}
    with h5py.File(self.feature_path_q, 'r') as fd:
        grp = fd[name0]
        for (k, v) in grp.items():
            data[k + '0'] = torch.from_numpy(v.__array__()).float()
        data['image0'] = torch.empty((1,) + tuple(grp['image_size'])[::-1])
    with h5py.File(self.feature_path_r, 'r') as fd:
        grp = fd[name1]
        for (k, v) in grp.items():
            data[k + '1'] = torch.from_numpy(v.__array__()).float()
        data['image1'] = torch.empty((1,) + tuple(grp['image_size'])[::-1])
    return data","for (k, v) in grp.items():
    data[k + '0'] = torch.from_numpy(v.__array__()).float()",XXX,no_found,,,,
Hierarchical-Localization,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Hierarchical-Localization/hloc/match_features.py,https://github.com/cvg/Hierarchical-Localization/tree/master/hloc/match_features.py,FeaturePairsDataset,__getitem__$104,"def __getitem__(self, idx):
    (name0, name1) = self.pairs[idx]
    data = {}
    with h5py.File(self.feature_path_q, 'r') as fd:
        grp = fd[name0]
        for (k, v) in grp.items():
            data[k + '0'] = torch.from_numpy(v.__array__()).float()
        data['image0'] = torch.empty((1,) + tuple(grp['image_size'])[::-1])
    with h5py.File(self.feature_path_r, 'r') as fd:
        grp = fd[name1]
        for (k, v) in grp.items():
            data[k + '1'] = torch.from_numpy(v.__array__()).float()
        data['image1'] = torch.empty((1,) + tuple(grp['image_size'])[::-1])
    return data","for (k, v) in grp.items():
    data[k + '1'] = torch.from_numpy(v.__array__()).float()",XXX,no_found,0,,,
pyinfra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyinfra/pyinfra/facts/hardware.py,https://github.com/Fizzadar/pyinfra/tree/master/pyinfra/facts/hardware.py,,_parse_regexes$134,"def _parse_regexes(regexes, lines):
    data = {'ipv4': {}, 'ipv6': {}}
    for line in lines:
        for (regex, groups) in regexes:
            matches = re.match(regex, line)
            if matches:
                ip_data = {}
                for (i, group) in enumerate(groups[1:]):
                    ip_data[group] = matches.group(i + 1)
                if 'mask_bits' in ip_data:
                    ip_data['mask_bits'] = int(ip_data['mask_bits'])
                target_group = data[groups[0]]
                if target_group.get('address'):
                    target_group.setdefault('additional_ips', []).append(ip_data)
                else:
                    target_group.update(ip_data)
                break
    return data","for (i, group) in enumerate(groups[1:]):
    ip_data[group] = matches.group(i + 1)",XXX,no_found,,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/cloud/clouds/qingcloud.py,https://github.com/saltstack/salt/tree/master/salt/cloud/clouds/qingcloud.py,,avail_sizes$399,"def avail_sizes(kwargs=None, call=None):
    """"""
    Return a list of the instance sizes that are on the provider.

    CLI Examples:

    .. code-block:: bash

        salt-cloud --list-sizes my-qingcloud
        salt-cloud -f avail_sizes my-qingcloud zone=pek2
    """"""
    if call == 'action':
        raise SaltCloudSystemExit('The avail_sizes function must be called with -f or --function, or with the --list-sizes option')
    zone = _get_specified_zone(kwargs, get_configured_provider())
    result = {}
    for size_key in QINGCLOUD_SIZES[zone]:
        result[size_key] = {}
        for attribute_key in QINGCLOUD_SIZES[zone][size_key]:
            result[size_key][attribute_key] = QINGCLOUD_SIZES[zone][size_key][attribute_key]
    return result","for attribute_key in QINGCLOUD_SIZES[zone][size_key]:
    result[size_key][attribute_key] = QINGCLOUD_SIZES[zone][size_key][attribute_key]",XXX,no_found,0,,,
cogdl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cogdl/cogdl/models/emb/dngr.py,https://github.com/THUDM/cogdl/tree/master/cogdl/models/emb/dngr.py,DNGR,forward$126,"def forward(self, graph, return_dict=False):
    device = 'cuda' if torch.cuda.is_available() and (not self.cpu) else 'cpu'
    G = graph.to_networkx()
    self.num_node = G.number_of_nodes()
    A = nx.adjacency_matrix(G).todense()
    PPMI = self.get_ppmi_matrix(A)
    print('PPMI matrix compute done')
    input_mat = torch.from_numpy(self.get_denoised_matrix(PPMI).astype(np.float32))
    model = DNGR_layer(self.num_node, self.hidden_size1, self.hidden_size2)
    input_mat = input_mat.to(device)
    model = model.to(device)
    opt = torch.optim.Adam(model.parameters(), lr=self.lr)
    loss_func = nn.MSELoss()
    epoch_iter = tqdm(range(self.epochs))
    for epoch in epoch_iter:
        opt.zero_grad()
        (encoded, decoded) = model.forward(input_mat)
        Loss = loss_func(decoded, input_mat)
        Loss.backward()
        epoch_iter.set_description(f'Epoch: {epoch:03d},  Loss: {Loss:.8f}')
        opt.step()
    (embeddings, _) = model.forward(input_mat)
    embeddings = embeddings.detach().cpu().numpy()
    if return_dict:
        features_matrix = dict()
        for (vid, node) in enumerate(G.nodes()):
            features_matrix[node] = embeddings[vid]
    else:
        features_matrix = np.zeros((graph.num_nodes, embeddings.shape[1]))
        nx_nodes = G.nodes()
        features_matrix[nx_nodes] = embeddings[np.arange(graph.num_nodes)]
    return features_matrix","for (vid, node) in enumerate(G.nodes()):
    features_matrix[node] = embeddings[vid]",XXX,no_found,0,,,
TSD,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TSD/mmdet/core/fp16/decorators.py,https://github.com/Sense-X/TSD/tree/master/mmdet/core/fp16/decorators.py,,force_fp32$88,"def force_fp32(apply_to=None, out_fp16=False):
    """"""Decorator to convert input arguments to fp32 in force.

    This decorator is useful when you write custom modules and want to support
    mixed precision training. If there are some inputs that must be processed
    in fp32 mode, then this decorator can handle it. If inputs arguments are
    fp16 tensors, they will be converted to fp32 automatically. Arguments other
    than fp16 tensors are ignored.

    Args:
        apply_to (Iterable, optional): The argument names to be converted.
            `None` indicates all arguments.
        out_fp16 (bool): Whether to convert the output back to fp16.

    :Example:

        class MyModule1(nn.Module)

            # Convert x and y to fp32
            @force_fp32()
            def loss(self, x, y):
                pass

        class MyModule2(nn.Module):

            # convert pred to fp32
            @force_fp32(apply_to=('pred', ))
            def post_process(self, pred, others):
                pass
    """"""

    def force_fp32_wrapper(old_func):

        @functools.wraps(old_func)
        def new_func(*args, **kwargs):
            if not isinstance(args[0], torch.nn.Module):
                raise TypeError('@force_fp32 can only be used to decorate the method of nn.Module')
            if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
                return old_func(*args, **kwargs)
            args_info = getfullargspec(old_func)
            args_to_cast = args_info.args if apply_to is None else apply_to
            new_args = []
            if args:
                arg_names = args_info.args[:len(args)]
                for (i, arg_name) in enumerate(arg_names):
                    if arg_name in args_to_cast:
                        new_args.append(cast_tensor_type(args[i], torch.half, torch.float))
                    else:
                        new_args.append(args[i])
            new_kwargs = dict()
            if kwargs:
                for (arg_name, arg_value) in kwargs.items():
                    if arg_name in args_to_cast:
                        new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.half, torch.float)
                    else:
                        new_kwargs[arg_name] = arg_value
            output = old_func(*new_args, **new_kwargs)
            if out_fp16:
                output = cast_tensor_type(output, torch.float, torch.half)
            return output
        return new_func
    return force_fp32_wrapper","for (arg_name, arg_value) in kwargs.items():
    if arg_name in args_to_cast:
        new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.half, torch.float)
    else:
        new_kwargs[arg_name] = arg_value",XXX,no_found,,,,
NVTabular,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NVTabular/nvtabular/ops/moments.py,https://github.com/NVIDIA-Merlin/NVTabular/tree/master/nvtabular/ops/moments.py,,_custom_moments$28,"def _custom_moments(ddf, split_every=32):
    dsk = {}
    token = tokenize(ddf)
    tree_reduce_name = 'chunkwise-moments-' + token
    result_name = 'global-moments-' + token
    for p in range(ddf.npartitions):
        dsk[tree_reduce_name, p, 0] = (_chunkwise_moments, (ddf._name, p))
    parts = ddf.npartitions
    widths = [parts]
    while parts > 1:
        parts = math.ceil(parts / split_every)
        widths.append(parts)
    height = len(widths)
    for depth in range(1, height):
        for group in range(widths[depth]):
            p_max = widths[depth - 1]
            lstart = split_every * group
            lstop = min(lstart + split_every, p_max)
            node_list = [(tree_reduce_name, p, depth - 1) for p in range(lstart, lstop)]
            dsk[tree_reduce_name, group, depth] = (_tree_node_moments, node_list)
    dsk[result_name] = (_finalize_moments, (tree_reduce_name, 0, height - 1))
    graph = HighLevelGraph.from_collections(result_name, dsk, dependencies=[ddf])
    return Delayed(result_name, graph)","for depth in range(1, height):
    for group in range(widths[depth]):
        p_max = widths[depth - 1]
        lstart = split_every * group
        lstop = min(lstart + split_every, p_max)
        node_list = [(tree_reduce_name, p, depth - 1) for p in range(lstart, lstop)]
        dsk[tree_reduce_name, group, depth] = (_tree_node_moments, node_list)",XXX,no_found,0,,,
sentry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/api/serializers/models/alert_rule.py,https://github.com/getsentry/sentry/tree/master/src/sentry/api/serializers/models/alert_rule.py,AlertRuleSerializer,get_attrs$33,"def get_attrs(self, item_list, user, **kwargs):
    alert_rules = {item.id: item for item in item_list}
    prefetch_related_objects(item_list, 'snuba_query__environment')
    result = defaultdict(dict)
    triggers = AlertRuleTrigger.objects.filter(alert_rule__in=item_list).order_by('label')
    serialized_triggers = serialize(list(triggers), **kwargs)
    trigger_actions = AlertRuleTriggerAction.objects.filter(alert_rule_trigger__alert_rule_id__in=alert_rules.keys()).exclude(sentry_app_config__isnull=True, sentry_app_id__isnull=True)
    sentry_app_installations_by_sentry_app_id = SentryAppInstallation.objects.get_related_sentry_app_components(organization_ids={alert_rule.organization_id for alert_rule in alert_rules.values()}, sentry_app_ids=trigger_actions.values_list('sentry_app_id', flat=True), type='alert-rule-action')
    for (trigger, serialized) in zip(triggers, serialized_triggers):
        alert_rule_triggers = result[alert_rules[trigger.alert_rule_id]].setdefault('triggers', [])
        for action in serialized.get('actions', []):
            install = sentry_app_installations_by_sentry_app_id.get(action.get('sentryAppId'))
            if install:
                action['_sentry_app_component'] = install.get('sentry_app_component')
                action['_sentry_app_installation'] = install.get('sentry_app_installation')
                action['sentryAppInstallationUuid'] = install.get('sentry_app_installation').get('uuid')
        alert_rule_triggers.append(serialized)
    alert_rule_projects = AlertRule.objects.filter(id__in=[item.id for item in item_list]).values_list('id', 'snuba_query__subscriptions__project__slug')
    for (alert_rule_id, project_slug) in alert_rule_projects:
        rule_result = result[alert_rules[alert_rule_id]].setdefault('projects', [])
        rule_result.append(project_slug)
    for rule_activity in AlertRuleActivity.objects.filter(alert_rule__in=item_list, type=AlertRuleActivityType.CREATED.value).select_related('alert_rule', 'user'):
        if rule_activity.user:
            user = {'id': rule_activity.user.id, 'name': rule_activity.user.get_display_name(), 'email': rule_activity.user.email}
        else:
            user = None
        result[alert_rules[rule_activity.alert_rule.id]].update({'created_by': user})
    resolved_actors = {}
    owners_by_type = defaultdict(list)
    for item in item_list:
        if item.owner_id is not None:
            owners_by_type[actor_type_to_string(item.owner.type)].append(item.owner_id)
    for (k, v) in ACTOR_TYPES.items():
        resolved_actors[k] = {a.actor_id: a.id for a in actor_type_to_class(v).objects.filter(actor_id__in=owners_by_type[k])}
    for alert_rule in alert_rules.values():
        if alert_rule.owner_id:
            type = actor_type_to_string(alert_rule.owner.type)
            if alert_rule.owner_id in resolved_actors[type]:
                result[alert_rule]['owner'] = f'{type}:{resolved_actors[type][alert_rule.owner_id]}'
    if 'original_alert_rule' in self.expand:
        snapshot_activities = AlertRuleActivity.objects.filter(alert_rule__in=item_list, type=AlertRuleActivityType.SNAPSHOT.value)
        for activity in snapshot_activities:
            result[alert_rules[activity.alert_rule_id]]['originalAlertRuleId'] = activity.previous_alert_rule_id
    if 'latestIncident' in self.expand:
        incident_map = {}
        for incident in Incident.objects.filter(id__in=Incident.objects.filter(alert_rule__in=alert_rules).values('alert_rule_id').annotate(incident_id=Max('id')).values('incident_id')):
            incident_map[incident.alert_rule_id] = serialize(incident, user=user)
        for alert_rule in alert_rules.values():
            result[alert_rule]['latestIncident'] = incident_map.get(alert_rule.id, None)
    return result","for (k, v) in ACTOR_TYPES.items():
    resolved_actors[k] = {a.actor_id: a.id for a in actor_type_to_class(v).objects.filter(actor_id__in=owners_by_type[k])}",XXX,no_found,0,,,
sentry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/api/serializers/models/alert_rule.py,https://github.com/getsentry/sentry/tree/master/src/sentry/api/serializers/models/alert_rule.py,AlertRuleSerializer,get_attrs$33,"def get_attrs(self, item_list, user, **kwargs):
    alert_rules = {item.id: item for item in item_list}
    prefetch_related_objects(item_list, 'snuba_query__environment')
    result = defaultdict(dict)
    triggers = AlertRuleTrigger.objects.filter(alert_rule__in=item_list).order_by('label')
    serialized_triggers = serialize(list(triggers), **kwargs)
    trigger_actions = AlertRuleTriggerAction.objects.filter(alert_rule_trigger__alert_rule_id__in=alert_rules.keys()).exclude(sentry_app_config__isnull=True, sentry_app_id__isnull=True)
    sentry_app_installations_by_sentry_app_id = SentryAppInstallation.objects.get_related_sentry_app_components(organization_ids={alert_rule.organization_id for alert_rule in alert_rules.values()}, sentry_app_ids=trigger_actions.values_list('sentry_app_id', flat=True), type='alert-rule-action')
    for (trigger, serialized) in zip(triggers, serialized_triggers):
        alert_rule_triggers = result[alert_rules[trigger.alert_rule_id]].setdefault('triggers', [])
        for action in serialized.get('actions', []):
            install = sentry_app_installations_by_sentry_app_id.get(action.get('sentryAppId'))
            if install:
                action['_sentry_app_component'] = install.get('sentry_app_component')
                action['_sentry_app_installation'] = install.get('sentry_app_installation')
                action['sentryAppInstallationUuid'] = install.get('sentry_app_installation').get('uuid')
        alert_rule_triggers.append(serialized)
    alert_rule_projects = AlertRule.objects.filter(id__in=[item.id for item in item_list]).values_list('id', 'snuba_query__subscriptions__project__slug')
    for (alert_rule_id, project_slug) in alert_rule_projects:
        rule_result = result[alert_rules[alert_rule_id]].setdefault('projects', [])
        rule_result.append(project_slug)
    for rule_activity in AlertRuleActivity.objects.filter(alert_rule__in=item_list, type=AlertRuleActivityType.CREATED.value).select_related('alert_rule', 'user'):
        if rule_activity.user:
            user = {'id': rule_activity.user.id, 'name': rule_activity.user.get_display_name(), 'email': rule_activity.user.email}
        else:
            user = None
        result[alert_rules[rule_activity.alert_rule.id]].update({'created_by': user})
    resolved_actors = {}
    owners_by_type = defaultdict(list)
    for item in item_list:
        if item.owner_id is not None:
            owners_by_type[actor_type_to_string(item.owner.type)].append(item.owner_id)
    for (k, v) in ACTOR_TYPES.items():
        resolved_actors[k] = {a.actor_id: a.id for a in actor_type_to_class(v).objects.filter(actor_id__in=owners_by_type[k])}
    for alert_rule in alert_rules.values():
        if alert_rule.owner_id:
            type = actor_type_to_string(alert_rule.owner.type)
            if alert_rule.owner_id in resolved_actors[type]:
                result[alert_rule]['owner'] = f'{type}:{resolved_actors[type][alert_rule.owner_id]}'
    if 'original_alert_rule' in self.expand:
        snapshot_activities = AlertRuleActivity.objects.filter(alert_rule__in=item_list, type=AlertRuleActivityType.SNAPSHOT.value)
        for activity in snapshot_activities:
            result[alert_rules[activity.alert_rule_id]]['originalAlertRuleId'] = activity.previous_alert_rule_id
    if 'latestIncident' in self.expand:
        incident_map = {}
        for incident in Incident.objects.filter(id__in=Incident.objects.filter(alert_rule__in=alert_rules).values('alert_rule_id').annotate(incident_id=Max('id')).values('incident_id')):
            incident_map[incident.alert_rule_id] = serialize(incident, user=user)
        for alert_rule in alert_rules.values():
            result[alert_rule]['latestIncident'] = incident_map.get(alert_rule.id, None)
    return result","for incident in Incident.objects.filter(id__in=Incident.objects.filter(alert_rule__in=alert_rules).values('alert_rule_id').annotate(incident_id=Max('id')).values('incident_id')):
    incident_map[incident.alert_rule_id] = serialize(incident, user=user)",XXX,no_found,0,,,
horizon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/horizon/openstack_dashboard/test/helpers.py,https://github.com/openstack/horizon/tree/master/openstack_dashboard/test/helpers.py,,_apply_panel_mocks$143,"def _apply_panel_mocks(patchers=None):
    """"""Global mocks on panels that get called on all views.""""""
    if patchers is None:
        patchers = {}
    mocked_methods = settings.TEST_GLOBAL_MOCKS_ON_PANELS
    for (name, mock_config) in mocked_methods.items():
        method = mock_config['method']
        mock_params = {}
        for param in ['return_value', 'side_effect']:
            if param in mock_config:
                mock_params[param] = mock_config[param]
        patcher = mock.patch(method, **mock_params)
        patcher.start()
        patchers[name] = patcher
    return patchers","for param in ['return_value', 'side_effect']:
    if param in mock_config:
        mock_params[param] = mock_config[param]",XXX,no_found,0,,,
class-balanced-loss,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/class-balanced-loss/tpu/models/official/retinanet/retinanet_architecture.py,https://github.com/richardaecn/class-balanced-loss/tree/master/tpu/models/official/retinanet/retinanet_architecture.py,,resnet_fpn$518,"def resnet_fpn(features, min_level=3, max_level=7, resnet_depth=50, is_training_bn=False, use_nearest_upsampling=True):
    """"""ResNet feature pyramid networks.""""""
    with tf.variable_scope('resnet%s' % resnet_depth):
        resnet_fn = resnet_v1(resnet_depth)
        (u2, u3, u4, u5) = resnet_fn(features, is_training_bn)
    feats_bottom_up = {2: u2, 3: u3, 4: u4, 5: u5}
    with tf.variable_scope('resnet_fpn'):
        feats_lateral = {}
        for level in range(min_level, _RESNET_MAX_LEVEL + 1):
            feats_lateral[level] = tf.layers.conv2d(feats_bottom_up[level], filters=256, kernel_size=(1, 1), padding='same', name='l%d' % level)
        feats = {_RESNET_MAX_LEVEL: feats_lateral[_RESNET_MAX_LEVEL]}
        for level in range(_RESNET_MAX_LEVEL - 1, min_level - 1, -1):
            if use_nearest_upsampling:
                feats[level] = nearest_upsampling(feats[level + 1], 2) + feats_lateral[level]
            else:
                feats[level] = resize_bilinear(feats[level + 1], tf.shape(feats_lateral[level])[1:3], feats[level + 1].dtype) + feats_lateral[level]
        for level in range(min_level, _RESNET_MAX_LEVEL + 1):
            feats[level] = tf.layers.conv2d(feats[level], filters=256, strides=(1, 1), kernel_size=(3, 3), padding='same', name='post_hoc_d%d' % level)
        for level in range(_RESNET_MAX_LEVEL + 1, max_level + 1):
            feats_in = feats[level - 1]
            if level > _RESNET_MAX_LEVEL + 1:
                feats_in = tf.nn.relu(feats_in)
            feats[level] = tf.layers.conv2d(feats_in, filters=256, strides=(2, 2), kernel_size=(3, 3), padding='same', name='p%d' % level)
        for level in range(min_level, max_level + 1):
            feats[level] = tf.layers.batch_normalization(inputs=feats[level], momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True, scale=True, training=is_training_bn, fused=True, name='p%d-bn' % level)
    return feats","for level in range(min_level, _RESNET_MAX_LEVEL + 1):
    feats_lateral[level] = tf.layers.conv2d(feats_bottom_up[level], filters=256, kernel_size=(1, 1), padding='same', name='l%d' % level)",XXX,no_found,0,,,
data-validation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-validation/tensorflow_data_validation/utils/display_util.py,https://github.com/tensorflow/data-validation/tree/master/tensorflow_data_validation/utils/display_util.py,,get_natural_language_statistics_dataframes$458,"def get_natural_language_statistics_dataframes(lhs_statistics: statistics_pb2.DatasetFeatureStatisticsList, rhs_statistics: Optional[statistics_pb2.DatasetFeatureStatisticsList]=None, lhs_name: Text='lhs_statistics', rhs_name: Text='rhs_statistics', allowlist_features: Optional[List[types.FeaturePath]]=None, denylist_features: Optional[List[types.FeaturePath]]=None) -> Optional[Dict[str, Dict[Union[int, str], Union[Dict[str, pd.DataFrame], pd.DataFrame]]]]:
    """"""Gets the `NaturalLanguageStatistics` as a dict of pandas.DataFrame.

  Each pd.DataFrame can be fed into a plot with little to no manipulation.

  For example, to plot the `token_length_histogram` in plot.ly:
  ```
  import pandas a pd
  import plotly
  import tensorflow_data_validation as tfdv
  from tensorflow_data_validation.utils import display_util as tfdv_display_util

  data = pd.DataFrame.from_dict({""col"": [1, 2, 3]})
  statistics = tfdv.generate_statistics_from_dataframe(data)

  df = tfdv_display_util.get_natural_language_statistics_dataframes(statistics)
  hist, bin_edges = np.histogram(df[ds_name][feature_name][
                      'token_length_histogram']['high_values'])
  fig = plotly.graph_objs.Figure(data=[
      plotly.graph_objs.Bar(x=bin_edges, y=hist, name='Histogram'),
  ])
  ```

  The resulting dict contains `token_length_histogram` and each token name as
  its keys. For each token, the data frame represents a list of stats as well
  as the token's positions histogram.

  Args:
    lhs_statistics: A DatasetFeatureStatisticsList protocol buffer.
    rhs_statistics: An optional DatasetFeatureStatisticsList protocol buffer to
      compare with lhs_statistics.
    lhs_name: Name of the lhs_statistics dataset.
    rhs_name: Name of the rhs_statistics dataset.
    allowlist_features: Set of features to be visualized.
    denylist_features: Set of features to ignore for visualization.

  Returns:
    A dict of pandas data frames. Returns None if natural language statistics
    does not exist in the statistics proto.
  """"""
    combined_statistics = _get_combined_statistics(lhs_statistics, rhs_statistics, lhs_name, rhs_name, allowlist_features, denylist_features)
    nlp_stats = _get_natural_language_statistics(combined_statistics)
    if not nlp_stats:
        return None
    result = {}
    for (ds_name, features_dict) in nlp_stats.items():
        result[ds_name] = {}
        for (feature_name, nlp_stat) in features_dict.items():
            result[ds_name][feature_name] = {'token_length_histogram': _get_histogram_dataframe(nlp_stat.token_length_histogram), 'token_statistics': _get_token_statistics(list(nlp_stat.token_statistics))}
    return result","for (feature_name, nlp_stat) in features_dict.items():
    result[ds_name][feature_name] = {'token_length_histogram': _get_histogram_dataframe(nlp_stat.token_length_histogram), 'token_statistics': _get_token_statistics(list(nlp_stat.token_statistics))}",XXX,no_found,0,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/glusterfs.py,https://github.com/saltstack/salt/tree/master/salt/modules/glusterfs.py,,info$400,"def info(name=None):
    """"""
    .. versionadded:: 2015.8.4

    Return gluster volume info.

    name
        Optional name to retrieve only information of one volume

    CLI Example:

    .. code-block:: bash

        salt '*' glusterfs.info
    """"""
    cmd = 'volume info'
    if name is not None:
        cmd += ' ' + name
    root = _gluster_xml(cmd)
    if not _gluster_ok(root):
        return None
    ret = {}
    for volume in _iter(root, 'volume'):
        name = volume.find('name').text
        ret[name] = _etree_to_dict(volume)
        bricks = {}
        for (i, brick) in enumerate(_iter(volume, 'brick'), start=1):
            brickkey = 'brick{}'.format(i)
            bricks[brickkey] = {'path': brick.text}
            for child in brick:
                if not child.tag == 'name':
                    bricks[brickkey].update({child.tag: child.text})
            for (k, v) in brick.items():
                bricks[brickkey][k] = v
        ret[name]['bricks'] = bricks
        options = {}
        for option in _iter(volume, 'option'):
            options[option.find('name').text] = option.find('value').text
        ret[name]['options'] = options
    return ret","for (i, brick) in enumerate(_iter(volume, 'brick'), start=1):
    brickkey = 'brick{}'.format(i)
    bricks[brickkey] = {'path': brick.text}
    for child in brick:
        if not child.tag == 'name':
            bricks[brickkey].update({child.tag: child.text})
    for (k, v) in brick.items():
        bricks[brickkey][k] = v",XXX,no_found,0,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/glusterfs.py,https://github.com/saltstack/salt/tree/master/salt/modules/glusterfs.py,,info$400,"def info(name=None):
    """"""
    .. versionadded:: 2015.8.4

    Return gluster volume info.

    name
        Optional name to retrieve only information of one volume

    CLI Example:

    .. code-block:: bash

        salt '*' glusterfs.info
    """"""
    cmd = 'volume info'
    if name is not None:
        cmd += ' ' + name
    root = _gluster_xml(cmd)
    if not _gluster_ok(root):
        return None
    ret = {}
    for volume in _iter(root, 'volume'):
        name = volume.find('name').text
        ret[name] = _etree_to_dict(volume)
        bricks = {}
        for (i, brick) in enumerate(_iter(volume, 'brick'), start=1):
            brickkey = 'brick{}'.format(i)
            bricks[brickkey] = {'path': brick.text}
            for child in brick:
                if not child.tag == 'name':
                    bricks[brickkey].update({child.tag: child.text})
            for (k, v) in brick.items():
                bricks[brickkey][k] = v
        ret[name]['bricks'] = bricks
        options = {}
        for option in _iter(volume, 'option'):
            options[option.find('name').text] = option.find('value').text
        ret[name]['options'] = options
    return ret","for option in _iter(volume, 'option'):
    options[option.find('name').text] = option.find('value').text",XXX,no_found,0,,,
OctoPrint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OctoPrint/src/octoprint/cli/client.py,https://github.com/OctoPrint/OctoPrint/tree/master/src/octoprint/cli/client.py,,command$208,"def command(ctx, path, command, str_params, int_params, float_params, bool_params, timeout):
    """"""Sends a JSON command to the specified server path.""""""
    data = {}
    params = str_params + int_params + float_params + bool_params
    for param in params:
        data[param[0]] = param[1]
    r = ctx.obj.client.post_command(path, command, additional=data, timeout=timeout)
    log_response(r, body=False)","for param in params:
    data[param[0]] = param[1]",XXX,no_found,,,,
PMapper,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PMapper/principalmapper/querying/query_interface.py,https://github.com/nccgroup/PMapper/tree/master/principalmapper/querying/query_interface.py,,search_authorization_across_accounts$89,"def search_authorization_across_accounts(graph_scp_pairs: List[Tuple[Graph, Optional[List[List[Policy]]]]], inter_account_edges: List[Edge], principal: Node, action_to_check: str, resource_to_check: str, condition_keys_to_check: _UODict, resource_policy: Optional[dict]=None, resource_owner: Optional[str]=None, session_policy: Optional[dict]=None) -> QueryResult:
    """"""Determines if the passed principal, or any principals it can access, can perform a given action for a
    given resource/condition. Handles an optional resource policy, an optional SCP list, and an optional
    session policy. The session policy is discarded after checking if the passed principal has access
    (we assume it is discarded after each pivot, and that it does NOT affect the accessibility of the edges).

    In `local_check_authorization` we usually throw up our hands if the given principal is an admin. But, because of
    how SCPs work (even blocking the root user), we force the full search to give more accurate results. If the
    SCPs param is None, we assume no SCPs are in place and can make the same assumption as in
    `local_check_authorization`.

    If the resource_owner param is not None, and the resource_owner param is None, the `local_check_authorization_full`
    function that gets called will throw a ValueError, so make sure the resource ownership is sorted before calling
    this method.

    The graphs to include in the search have to be passed in tuples. The second element of the tuple is either the SCPs
    that affect that graph or None. If your graph belongs to an organization, remember that you can take the
    OrganizationTree object and produce the applicable SCPs by calling
    principalmapper.querying.query_orgs.produce_scp_list and passing the graph + org-tree objects.""""""
    account_id_graph_scp_pair_map = {}
    for graph_scp_pair in graph_scp_pairs:
        account_id_graph_scp_pair_map[graph_scp_pair[0].metadata['account_id']] = graph_scp_pair
    source_graph_scp_pair = account_id_graph_scp_pair_map[arns.get_account_id(principal.arn)]
    if local_check_authorization_full(principal, action_to_check, resource_to_check, condition_keys_to_check, resource_policy, resource_owner, source_graph_scp_pair[1], session_policy):
        return QueryResult(True, [], principal)
    if source_graph_scp_pair[1] is None and principal.is_admin and (resource_owner == arns.get_account_id(principal.arn)):
        return QueryResult(True, principal, principal)
    for edge_list in query_utils.get_interaccount_search_list([x[0] for x in graph_scp_pairs], inter_account_edges, principal):
        proxy_principal = edge_list[-1].destination
        proxy_principal_scps = account_id_graph_scp_pair_map[arns.get_account_id(proxy_principal.arn)][1]
        if local_check_authorization_full(edge_list[-1].destination, action_to_check, resource_to_check, condition_keys_to_check, resource_policy, resource_owner, proxy_principal_scps, None):
            return QueryResult(True, edge_list, principal)
    return QueryResult(False, [], principal)","for graph_scp_pair in graph_scp_pairs:
    account_id_graph_scp_pair_map[graph_scp_pair[0].metadata['account_id']] = graph_scp_pair",XXX,no_found,,,,
fake-bpy-module,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fake-bpy-module/src/fake_bpy_module/generator.py,https://github.com/nutti/fake-bpy-module/tree/master/src/fake_bpy_module/generator.py,BaseGenerator,_sorted_generation_info$292,"def _sorted_generation_info(self, data: 'GenerationInfoByTarget') -> List[Info]:
    class_data: List[ClassInfo] = []
    function_data: List[FunctionInfo] = []
    constant_data: List[VariableInfo] = []
    high_priority_class_data: List[ClassInfo] = []
    for d in data.data:
        if d.type() == 'class':
            if d.name() in ['bpy_prop_collection', 'bpy_struct']:
                high_priority_class_data.append(d)
            else:
                class_data.append(d)
        elif d.type() == 'function':
            function_data.append(d)
        elif d.type() == 'constant':
            constant_data.append(d)
        else:
            raise ValueError(f'Invalid data type. ({d.type})')
    class_data = high_priority_class_data + sorted(class_data, key=lambda x: x.name())
    graph = DAG()
    class_name_to_nodes = OrderedDict()
    for class_ in class_data:
        class_name_to_nodes[class_.name()] = graph.make_node(class_)
    for class_ in class_data:
        class_node = class_name_to_nodes[class_.name()]
        for base_class in class_.base_classes():
            if base_class.type() == 'MIXIN':
                raise ValueError('DataType of base class must not be MixinDataType.')
            if base_class.type() == 'UNKNOWN':
                continue
            if base_class.data_type() == class_.name():
                output_log(LOG_LEVEL_DEBUG, f'Self dependency {base_class.data_type()} is found.')
                continue
            base_class_node = class_name_to_nodes.get(base_class.data_type())
            if base_class_node:
                graph.make_edge(base_class_node, class_node)
            else:
                output_log(LOG_LEVEL_WARN, f'Base class node (type={base_class.data_type()}) is not found')
    sorted_nodes = topological_sort(graph)
    sorted_class_data = [node.data() for node in sorted_nodes]
    order = {}
    for (i, class_) in enumerate(sorted_class_data):
        order[class_.name()] = i
    for class_ in sorted_class_data:

        def sort_func(x):
            if x.type() == 'UNKNOWN':
                return 0
            if x.data_type() not in order:
                return 0
            return -order[x.data_type()]
        new_base_classes = sorted(class_.base_classes(), key=sort_func)
        for (i, c) in enumerate(new_base_classes):
            class_.set_base_class(i, c)
    sorted_function_data = sorted(function_data, key=lambda x: x.name())
    sorted_constant_data = sorted(constant_data, key=lambda x: x.name())
    sorted_data = sorted_class_data
    sorted_data.extend(sorted_function_data)
    sorted_data.extend(sorted_constant_data)
    return sorted_data","for (i, class_) in enumerate(sorted_class_data):
    order[class_.name()] = i",XXX,no_found,0,,,
TensorNetwork,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorNetwork/tensornetwork/tn_keras/entangler.py,https://github.com/google/TensorNetwork/tree/master/tensornetwork/tn_keras/entangler.py,DenseEntangler,get_config$192,"def get_config(self) -> dict:
    """"""Returns the config of the layer.

    The same layer can be reinstantiated later
    (without its trained weights) from this configuration.

    Returns:
      Python dictionary containing the configuration of the layer.
    """"""
    config = {}
    args = ['output_dim', 'num_legs', 'num_levels', 'use_bias']
    for arg in args:
        config[arg] = getattr(self, arg)
    config['activation'] = activations.serialize(getattr(self, 'activation'))
    layer_initializers = ['kernel_initializer', 'bias_initializer']
    for initializer_arg in layer_initializers:
        config[initializer_arg] = initializers.serialize(getattr(self, initializer_arg))
    base_config = super().get_config()
    return dict(list(base_config.items()) + list(config.items()))","for arg in args:
    config[arg] = getattr(self, arg)",XXX,no_found,1,,,
TensorNetwork,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorNetwork/tensornetwork/tn_keras/entangler.py,https://github.com/google/TensorNetwork/tree/master/tensornetwork/tn_keras/entangler.py,DenseEntangler,get_config$192,"def get_config(self) -> dict:
    """"""Returns the config of the layer.

    The same layer can be reinstantiated later
    (without its trained weights) from this configuration.

    Returns:
      Python dictionary containing the configuration of the layer.
    """"""
    config = {}
    args = ['output_dim', 'num_legs', 'num_levels', 'use_bias']
    for arg in args:
        config[arg] = getattr(self, arg)
    config['activation'] = activations.serialize(getattr(self, 'activation'))
    layer_initializers = ['kernel_initializer', 'bias_initializer']
    for initializer_arg in layer_initializers:
        config[initializer_arg] = initializers.serialize(getattr(self, initializer_arg))
    base_config = super().get_config()
    return dict(list(base_config.items()) + list(config.items()))","for initializer_arg in layer_initializers:
    config[initializer_arg] = initializers.serialize(getattr(self, initializer_arg))",XXX,no_found,1,,,
cloud-custodian,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud-custodian/c7n/resources/quotas.py,https://github.com/cloud-custodian/cloud-custodian/tree/master/c7n/resources/quotas.py,ServiceQuota,augment$52,"def augment(self, resources):
    client = local_session(self.session_factory).client('service-quotas')
    retry = get_retry(('TooManyRequestsException',))

    def get_quotas(client, s):
        quotas = {}
        token = None
        kwargs = {'ServiceCode': s['ServiceCode'], 'MaxResults': self.batch_size}
        while True:
            if token:
                kwargs['NextToken'] = token
            response = retry(client.list_service_quotas, **kwargs)
            rquotas = {q['QuotaCode']: q for q in response['Quotas']}
            token = response.get('NextToken')
            new = set(rquotas) - set(quotas)
            quotas.update(rquotas)
            if token is None:
                break
            elif token and (not new):
                break
        return quotas.values()
    results = []
    with self.executor_factory(max_workers=self.max_workers) as w:
        futures = {}
        for r in resources:
            futures[w.submit(get_quotas, client, r)] = r
        for f in as_completed(futures):
            if f.exception():
                raise f.exception()
            results.extend(f.result())
    return results","for r in resources:
    futures[w.submit(get_quotas, client, r)] = r",XXX,no_found,0,,,
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/appconfig/feature.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/appconfig/feature.py,,list_feature$260,"def list_feature(cmd, feature=None, key=None, name=None, label=None, fields=None, connection_string=None, top=None, all_=False, auth_mode='key', endpoint=None):
    if key and feature:
        logger.warning('Since both `--key` and `--feature` are provided, `--feature` argument will be ignored.')
    if key is not None:
        key_filter = key
    elif feature is not None:
        key_filter = FeatureFlagConstants.FEATURE_FLAG_PREFIX + feature
    else:
        key_filter = FeatureFlagConstants.FEATURE_FLAG_PREFIX + SearchFilterOptions.ANY_KEY
    azconfig_client = get_appconfig_data_client(cmd, name, connection_string, auth_mode, endpoint)
    try:
        retrieved_keyvalues = __list_all_keyvalues(azconfig_client, key_filter=key_filter, label=label if label else SearchFilterOptions.ANY_LABEL)
        retrieved_featureflags = []
        for kv in retrieved_keyvalues:
            retrieved_featureflags.append(map_keyvalue_to_featureflag(keyvalue=kv, show_conditions=True))
        filtered_featureflags = []
        count = 0
        if all_:
            top = len(retrieved_featureflags)
        elif top is None:
            top = 100
        for featureflag in retrieved_featureflags:
            if fields:
                partial_featureflags = {}
                for field in fields:
                    partial_featureflags[field.name.lower()] = getattr(featureflag, field.name.lower())
                filtered_featureflags.append(partial_featureflags)
            else:
                filtered_featureflags.append(featureflag)
            count += 1
            if count >= top:
                break
        return filtered_featureflags
    except Exception as exception:
        raise CLIError(str(exception))","for field in fields:
    partial_featureflags[field.name.lower()] = getattr(featureflag, field.name.lower())",XXX,no_found,,,,
xonsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xonsh/xonsh/ply/ply/lex.py,https://github.com/xonsh/xonsh/tree/master/xonsh/ply/ply/lex.py,Lexer,clone$146,"def clone(self, object=None):
    c = copy.copy(self)
    if object:
        newtab = {}
        for (key, ritem) in self.lexstatere.items():
            newre = []
            for (cre, findex) in ritem:
                newfindex = []
                for f in findex:
                    if not f or not f[0]:
                        newfindex.append(f)
                        continue
                    newfindex.append((getattr(object, f[0].__name__), f[1]))
            newre.append((cre, newfindex))
            newtab[key] = newre
        c.lexstatere = newtab
        c.lexstateerrorf = {}
        for (key, ef) in self.lexstateerrorf.items():
            c.lexstateerrorf[key] = getattr(object, ef.__name__)
        c.lexmodule = object
    return c","for (key, ritem) in self.lexstatere.items():
    newre = []
    for (cre, findex) in ritem:
        newfindex = []
        for f in findex:
            if not f or not f[0]:
                newfindex.append(f)
                continue
            newfindex.append((getattr(object, f[0].__name__), f[1]))
    newre.append((cre, newfindex))
    newtab[key] = newre",XXX,no_found,0,,,
xonsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xonsh/xonsh/ply/ply/lex.py,https://github.com/xonsh/xonsh/tree/master/xonsh/ply/ply/lex.py,Lexer,clone$146,"def clone(self, object=None):
    c = copy.copy(self)
    if object:
        newtab = {}
        for (key, ritem) in self.lexstatere.items():
            newre = []
            for (cre, findex) in ritem:
                newfindex = []
                for f in findex:
                    if not f or not f[0]:
                        newfindex.append(f)
                        continue
                    newfindex.append((getattr(object, f[0].__name__), f[1]))
            newre.append((cre, newfindex))
            newtab[key] = newre
        c.lexstatere = newtab
        c.lexstateerrorf = {}
        for (key, ef) in self.lexstateerrorf.items():
            c.lexstateerrorf[key] = getattr(object, ef.__name__)
        c.lexmodule = object
    return c","for (key, ef) in self.lexstateerrorf.items():
    c.lexstateerrorf[key] = getattr(object, ef.__name__)",XXX,no_found,0,,,
Malt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Malt/Malt/Parameter.py,https://github.com/bnpr/Malt/tree/master/Malt/Parameter.py,PythonPipelineGraph,__init__$55,"def __init__(self, pipeline, function_nodes, graph_io_reflection):
    self.pipeline = pipeline
    self.node_instances = {}
    self.nodes = {}
    functions = {}
    for node_class in function_nodes:
        reflection = node_class.reflect()
        functions[reflection['name']] = reflection
        self.nodes[reflection['name']] = node_class
    graph_io = {}
    for node in graph_io_reflection:
        graph_io[node['name']] = node
    super().__init__('Python', '-render_layer.py', functions, {}, graph_io)","for node_class in function_nodes:
    reflection = node_class.reflect()
    functions[reflection['name']] = reflection
    self.nodes[reflection['name']] = node_class",XXX,no_found,1,,,
Malt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Malt/Malt/Parameter.py,https://github.com/bnpr/Malt/tree/master/Malt/Parameter.py,PythonPipelineGraph,__init__$55,"def __init__(self, pipeline, function_nodes, graph_io_reflection):
    self.pipeline = pipeline
    self.node_instances = {}
    self.nodes = {}
    functions = {}
    for node_class in function_nodes:
        reflection = node_class.reflect()
        functions[reflection['name']] = reflection
        self.nodes[reflection['name']] = node_class
    graph_io = {}
    for node in graph_io_reflection:
        graph_io[node['name']] = node
    super().__init__('Python', '-render_layer.py', functions, {}, graph_io)","for node in graph_io_reflection:
    graph_io[node['name']] = node",XXX,no_found,1,,,
django-role-permissions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-role-permissions/rolepermissions/permissions.py,https://github.com/vintasoftware/django-role-permissions/tree/master/rolepermissions/permissions.py,,available_perm_status$42,"def available_perm_status(user):
    """"""
    Get a boolean map of the permissions available to a user
    based on that user's roles.
    """"""
    roles = get_user_roles(user)
    permission_hash = {}
    user_permission_names = set(user.user_permissions.values_list('codename', flat=True))
    for role in roles:
        for permission_name in role.permission_names_list():
            permission_hash[permission_name] = permission_name in user_permission_names
    return permission_hash","for role in roles:
    for permission_name in role.permission_names_list():
        permission_hash[permission_name] = permission_name in user_permission_names",XXX,no_found,,,,
timesketch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/timesketch/timesketch/lib/aggregators_old.py,https://github.com/google/timesketch/tree/master/timesketch/lib/aggregators_old.py,,heatmap$25,"def heatmap(es_client, sketch_id, query_string, query_filter, query_dsl, indices):
    """"""Aggregate query results into number of events per hour/day.

    Args:
        es_client: Elasticsearch client (instance of ElasticSearchDatastore)
        sketch_id: Integer of sketch primary key
        query_string: Query string
        query_filter: Dictionary containing filters to apply
        query_dsl: Dictionary containing Elasticsearch DSL to apply
        indices: List of indices to query

    returns:
        List of events per hour/day
    """"""
    query_filter.pop('size', None)
    query_filter.pop('from', None)
    result_count = es_client.search(sketch_id, query_string, query_filter, query_dsl, indices, count=True)
    if result_count > MAX_RESULT_LIMIT or result_count == 0:
        return []
    days_map = {'Mon': 1, 'Tue': 2, 'Wed': 3, 'Thu': 4, 'Fri': 5, 'Sat': 6, 'Sun': 7}
    if es_client.version.startswith('5.'):
        source_script = 'new SimpleDateFormat(params.format).format(new Date(doc[""datetime""].value))'
    else:
        source_script = 'doc[""datetime""].value.toString(params.format);'
    aggregation = {'byDay': {'terms': {'script': {'source': source_script, 'lang': 'painless', 'params': {'format': 'E'}}}, 'aggs': {'byHour': {'terms': {'order': {'_term': 'asc'}, 'script': {'source': source_script, 'lang': 'painless', 'params': {'format': 'H'}}, 'size': 24}}}}}
    search_result = es_client.search(sketch_id, query_string, query_filter, query_dsl, indices, aggregations=aggregation, return_fields=None, enable_scroll=False)
    try:
        aggregation_result = search_result['aggregations']
        day_buckets = aggregation_result['byDay']['buckets']
    except KeyError:
        day_buckets = []
    per_hour = {}
    for day in range(1, 8):
        for hour in range(0, 24):
            per_hour[day, hour] = 0
    for day_bucket in day_buckets:
        day = days_map[day_bucket.get('key')]
        day_hours = day_bucket['byHour']['buckets']
        for day_hour in day_hours:
            hour = int(day_hour['key'])
            count = day_hour['doc_count']
            per_hour[day, int(hour)] = count
    return [dict(day=k[0], hour=k[1], count=v) for (k, v) in per_hour.items()]","for day in range(1, 8):
    for hour in range(0, 24):
        per_hour[day, hour] = 0",XXX,no_found,0,,,
timesketch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/timesketch/timesketch/lib/aggregators_old.py,https://github.com/google/timesketch/tree/master/timesketch/lib/aggregators_old.py,,heatmap$25,"def heatmap(es_client, sketch_id, query_string, query_filter, query_dsl, indices):
    """"""Aggregate query results into number of events per hour/day.

    Args:
        es_client: Elasticsearch client (instance of ElasticSearchDatastore)
        sketch_id: Integer of sketch primary key
        query_string: Query string
        query_filter: Dictionary containing filters to apply
        query_dsl: Dictionary containing Elasticsearch DSL to apply
        indices: List of indices to query

    returns:
        List of events per hour/day
    """"""
    query_filter.pop('size', None)
    query_filter.pop('from', None)
    result_count = es_client.search(sketch_id, query_string, query_filter, query_dsl, indices, count=True)
    if result_count > MAX_RESULT_LIMIT or result_count == 0:
        return []
    days_map = {'Mon': 1, 'Tue': 2, 'Wed': 3, 'Thu': 4, 'Fri': 5, 'Sat': 6, 'Sun': 7}
    if es_client.version.startswith('5.'):
        source_script = 'new SimpleDateFormat(params.format).format(new Date(doc[""datetime""].value))'
    else:
        source_script = 'doc[""datetime""].value.toString(params.format);'
    aggregation = {'byDay': {'terms': {'script': {'source': source_script, 'lang': 'painless', 'params': {'format': 'E'}}}, 'aggs': {'byHour': {'terms': {'order': {'_term': 'asc'}, 'script': {'source': source_script, 'lang': 'painless', 'params': {'format': 'H'}}, 'size': 24}}}}}
    search_result = es_client.search(sketch_id, query_string, query_filter, query_dsl, indices, aggregations=aggregation, return_fields=None, enable_scroll=False)
    try:
        aggregation_result = search_result['aggregations']
        day_buckets = aggregation_result['byDay']['buckets']
    except KeyError:
        day_buckets = []
    per_hour = {}
    for day in range(1, 8):
        for hour in range(0, 24):
            per_hour[day, hour] = 0
    for day_bucket in day_buckets:
        day = days_map[day_bucket.get('key')]
        day_hours = day_bucket['byHour']['buckets']
        for day_hour in day_hours:
            hour = int(day_hour['key'])
            count = day_hour['doc_count']
            per_hour[day, int(hour)] = count
    return [dict(day=k[0], hour=k[1], count=v) for (k, v) in per_hour.items()]","for day_bucket in day_buckets:
    day = days_map[day_bucket.get('key')]
    day_hours = day_bucket['byHour']['buckets']
    for day_hour in day_hours:
        hour = int(day_hour['key'])
        count = day_hour['doc_count']
        per_hour[day, int(hour)] = count",XXX,no_found,0,,,
KBQA-BERT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/KBQA-BERT/run_ner.py,https://github.com/WenRichard/KBQA-BERT/tree/master//run_ner.py,,convert_single_example$252,"def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer, mode):
    """"""
    闁诲繐绻愬Λ鏃傜博鐎涙尖枖闂佸搫绉归弨鍗烇耿閻楀牊浜ゆ繛鎴炵矊閼婚柛銊ラ叄瀵鎼佸箛閸撲胶澶愭煟閹烘洘纭鹃柟濡ゅ啩鐒婇柛鈩冨啯鎲婚柡澶屽仱瀹曠娀寮介敃浣哄獢d, 闂佸搫绉村ú銊╁箲閹扮粊闂佸憡鐗楅悧鏃傛嫻閻掓矟,闂佺粯甯熷▔娑㈠箖濡ゅ啰纾奸柟鎯ф悂鏌涢弽銊уⅹ闁糕晛鐡nputFeatures闁诲海鏁搁崨鐗堟緰婵
    :param ex_index: index
    :param example: 婵炴垶鎸荤粙澶愭煛瀹ュ嬫暠婵
    :param label_list: 闂佸搫绉村ú銊╁箚瑜嶉悘娆撴偠
    :param max_seq_length:
    :param tokenizer:
    :param mode:
    :return:
    """"""
    label_map = {}
    for (i, label) in enumerate(label_list, 1):
        label_map[label] = i
    if not os.path.exists(os.path.join(FLAGS.output_dir, 'label2id.pkl')):
        with codecs.open(os.path.join(FLAGS.output_dir, 'label2id.pkl'), 'wb') as w:
            pickle.dump(label_map, w)
    textlist = example.text.split(' ')
    labellist = example.label.split(' ')
    tokens = []
    labels = []
    for (i, word) in enumerate(textlist):
        token = tokenizer.tokenize(word)
        tokens.extend(token)
        label_1 = labellist[i]
        for m in range(len(token)):
            if m == 0:
                labels.append(label_1)
            else:
                labels.append('X')
    if len(tokens) >= max_seq_length - 1:
        tokens = tokens[0:max_seq_length - 2]
        labels = labels[0:max_seq_length - 2]
    ntokens = []
    segment_ids = []
    label_ids = []
    ntokens.append('[CLS]')
    segment_ids.append(0)
    label_ids.append(label_map['[CLS]'])
    for (i, token) in enumerate(tokens):
        ntokens.append(token)
        segment_ids.append(0)
        label_ids.append(label_map[labels[i]])
    ntokens.append('[SEP]')
    segment_ids.append(0)
    label_ids.append(label_map['[SEP]'])
    input_ids = tokenizer.convert_tokens_to_ids(ntokens)
    input_mask = [1] * len(input_ids)
    while len(input_ids) < max_seq_length:
        input_ids.append(0)
        input_mask.append(0)
        segment_ids.append(0)
        label_ids.append(0)
        ntokens.append('**NULL**')
    assert len(input_ids) == max_seq_length
    assert len(input_mask) == max_seq_length
    assert len(segment_ids) == max_seq_length
    assert len(label_ids) == max_seq_length
    if ex_index < 5:
        tf.logging.info('*** Example ***')
        tf.logging.info('guid: %s' % example.guid)
        tf.logging.info('tokens: %s' % ' '.join([tokenization.printable_text(x) for x in tokens]))
        tf.logging.info('input_ids: %s' % ' '.join([str(x) for x in input_ids]))
        tf.logging.info('input_mask: %s' % ' '.join([str(x) for x in input_mask]))
        tf.logging.info('segment_ids: %s' % ' '.join([str(x) for x in segment_ids]))
        tf.logging.info('label_ids: %s' % ' '.join([str(x) for x in label_ids]))
    feature = InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_ids=label_ids)
    write_tokens(ntokens, mode)
    return feature","for (i, label) in enumerate(label_list, 1):
    label_map[label] = i",XXX,no_found,,,,
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/manufacturing/doctype/work_order/test_work_order.py,https://github.com/frappe/erpnext/tree/master/erpnext/manufacturing/doctype/work_order/test_work_order.py,,get_scrap_item_details$1737,"def get_scrap_item_details(bom_no):
    scrap_items = {}
    for item in frappe.db.sql('select item_code, stock_qty from `tabBOM Scrap Item`\n\t\twhere parent = %s', bom_no, as_dict=1):
        scrap_items[item.item_code] = item.stock_qty
    return scrap_items","for item in frappe.db.sql('select item_code, stock_qty from `tabBOM Scrap Item`\n\t\twhere parent = %s', bom_no, as_dict=1):
    scrap_items[item.item_code] = item.stock_qty",XXX,no_found,,,,
EasyMocap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyMocap/easymocap/dataset/base.py,https://github.com/zju3dv/EasyMocap/tree/master/easymocap/dataset/base.py,_VideoBase,vis_smpl$305,"def vis_smpl(self, peopleDict, faces, image, nf, sub_vis=[], mode='smpl', extra_data=[], add_back=True, axis=np.array([1.0, 0.0, 0.0]), degree=0.0, fix_center=None):
    render_data = {}
    for (pid, data) in peopleDict.items():
        render_data[pid] = {'vertices': data.vertices, 'faces': faces, 'vid': pid, 'name': 'human_{}_{}'.format(nf, pid)}
    for (iid, extra) in enumerate(extra_data):
        render_data[10000 + iid] = {'vertices': extra['vertices'], 'faces': extra['faces'], 'colors': extra['colors'], 'name': extra['name']}
    camera = {}
    for key in self.camera.keys():
        camera[key] = self.camera[key][None, :, :]
    if np.abs(degree) > 0.001:
        vertices_all = np.vstack([data.vertices for data in peopleDict.values()])
        if fix_center is None:
            center = np.mean(vertices_all, axis=0, keepdims=True)
            new_center = center.copy()
            new_center[:, 0:2] = 0
        else:
            center = fix_center.copy()
            new_center = fix_center.copy()
            new_center[:, 2] *= 1.5
        direc = np.array(axis)
        (rot, _) = cv2.Rodrigues(direc * degree / 90 * np.pi / 2)
        blank = np.zeros_like(image, dtype=np.uint8) + 255
        images = [image, blank]
        Rnew = camera['R'][0] @ rot
        Tnew = camera['R'][0] @ (new_center.T - rot @ center.T) + camera['T'][0]
        camera['K'] = np.vstack([camera['K'], camera['K']])
        camera['R'] = np.vstack([camera['R'], Rnew[None, :, :]])
        camera['T'] = np.vstack([camera['T'], Tnew[None, :, :]])
    else:
        images = [image]
    self.writer.vis_smpl(render_data, nf, images, camera, mode, add_back=add_back)","for (pid, data) in peopleDict.items():
    render_data[pid] = {'vertices': data.vertices, 'faces': faces, 'vid': pid, 'name': 'human_{}_{}'.format(nf, pid)}",XXX,no_found,1,,,
EasyMocap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyMocap/easymocap/dataset/base.py,https://github.com/zju3dv/EasyMocap/tree/master/easymocap/dataset/base.py,_VideoBase,vis_smpl$305,"def vis_smpl(self, peopleDict, faces, image, nf, sub_vis=[], mode='smpl', extra_data=[], add_back=True, axis=np.array([1.0, 0.0, 0.0]), degree=0.0, fix_center=None):
    render_data = {}
    for (pid, data) in peopleDict.items():
        render_data[pid] = {'vertices': data.vertices, 'faces': faces, 'vid': pid, 'name': 'human_{}_{}'.format(nf, pid)}
    for (iid, extra) in enumerate(extra_data):
        render_data[10000 + iid] = {'vertices': extra['vertices'], 'faces': extra['faces'], 'colors': extra['colors'], 'name': extra['name']}
    camera = {}
    for key in self.camera.keys():
        camera[key] = self.camera[key][None, :, :]
    if np.abs(degree) > 0.001:
        vertices_all = np.vstack([data.vertices for data in peopleDict.values()])
        if fix_center is None:
            center = np.mean(vertices_all, axis=0, keepdims=True)
            new_center = center.copy()
            new_center[:, 0:2] = 0
        else:
            center = fix_center.copy()
            new_center = fix_center.copy()
            new_center[:, 2] *= 1.5
        direc = np.array(axis)
        (rot, _) = cv2.Rodrigues(direc * degree / 90 * np.pi / 2)
        blank = np.zeros_like(image, dtype=np.uint8) + 255
        images = [image, blank]
        Rnew = camera['R'][0] @ rot
        Tnew = camera['R'][0] @ (new_center.T - rot @ center.T) + camera['T'][0]
        camera['K'] = np.vstack([camera['K'], camera['K']])
        camera['R'] = np.vstack([camera['R'], Rnew[None, :, :]])
        camera['T'] = np.vstack([camera['T'], Tnew[None, :, :]])
    else:
        images = [image]
    self.writer.vis_smpl(render_data, nf, images, camera, mode, add_back=add_back)","for (iid, extra) in enumerate(extra_data):
    render_data[10000 + iid] = {'vertices': extra['vertices'], 'faces': extra['faces'], 'colors': extra['colors'], 'name': extra['name']}",XXX,no_found,1,,,
EasyMocap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyMocap/easymocap/dataset/base.py,https://github.com/zju3dv/EasyMocap/tree/master/easymocap/dataset/base.py,_VideoBase,vis_smpl$305,"def vis_smpl(self, peopleDict, faces, image, nf, sub_vis=[], mode='smpl', extra_data=[], add_back=True, axis=np.array([1.0, 0.0, 0.0]), degree=0.0, fix_center=None):
    render_data = {}
    for (pid, data) in peopleDict.items():
        render_data[pid] = {'vertices': data.vertices, 'faces': faces, 'vid': pid, 'name': 'human_{}_{}'.format(nf, pid)}
    for (iid, extra) in enumerate(extra_data):
        render_data[10000 + iid] = {'vertices': extra['vertices'], 'faces': extra['faces'], 'colors': extra['colors'], 'name': extra['name']}
    camera = {}
    for key in self.camera.keys():
        camera[key] = self.camera[key][None, :, :]
    if np.abs(degree) > 0.001:
        vertices_all = np.vstack([data.vertices for data in peopleDict.values()])
        if fix_center is None:
            center = np.mean(vertices_all, axis=0, keepdims=True)
            new_center = center.copy()
            new_center[:, 0:2] = 0
        else:
            center = fix_center.copy()
            new_center = fix_center.copy()
            new_center[:, 2] *= 1.5
        direc = np.array(axis)
        (rot, _) = cv2.Rodrigues(direc * degree / 90 * np.pi / 2)
        blank = np.zeros_like(image, dtype=np.uint8) + 255
        images = [image, blank]
        Rnew = camera['R'][0] @ rot
        Tnew = camera['R'][0] @ (new_center.T - rot @ center.T) + camera['T'][0]
        camera['K'] = np.vstack([camera['K'], camera['K']])
        camera['R'] = np.vstack([camera['R'], Rnew[None, :, :]])
        camera['T'] = np.vstack([camera['T'], Tnew[None, :, :]])
    else:
        images = [image]
    self.writer.vis_smpl(render_data, nf, images, camera, mode, add_back=add_back)","for key in self.camera.keys():
    camera[key] = self.camera[key][None, :, :]",XXX,no_found,0,,,
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/core/fp16/decorators.py,https://github.com/poodarchu/Det3D/tree/master/det3d/core/fp16/decorators.py,,auto_fp16$9,"def auto_fp16(apply_to=None, out_fp32=False):
    """"""Decorator to enable fp16 training automatically.
    This decorator is useful when you write custom modules and want to support
    mixed precision training. If inputs arguments are fp32 tensors, they will
    be converted to fp16 automatically. Arguments other than fp32 tensors are
    ignored.
    Args:
        apply_to (Iterable, optional): The argument names to be converted.
            `None` indicates all arguments.
        out_fp32 (bool): Whether to convert the output back to fp32.
    :Example:
        class MyModule1(nn.Module)
            # Convert x and y to fp16
            @auto_fp16()
            def forward(self, x, y):
                pass
        class MyModule2(nn.Module):
            # convert pred to fp16
            @auto_fp16(apply_to=('pred', ))
            def do_something(self, pred, others):
                pass
    """"""

    def auto_fp16_wrapper(old_func):

        @functools.wraps(old_func)
        def new_func(*args, **kwargs):
            if not isinstance(args[0], torch.nn.Module):
                raise TypeError('@auto_fp16 can only be used to decorate the method of nn.Module')
            if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
                return old_func(*args, **kwargs)
            args_info = getfullargspec(old_func)
            args_to_cast = args_info.args if apply_to is None else apply_to
            new_args = []
            if args:
                arg_names = args_info.args[:len(args)]
                for (i, arg_name) in enumerate(arg_names):
                    if arg_name in args_to_cast:
                        new_args.append(cast_tensor_type(args[i], torch.float, torch.half))
                    else:
                        new_args.append(args[i])
            new_kwargs = {}
            if kwargs:
                for (arg_name, arg_value) in kwargs.items():
                    if arg_name in args_to_cast:
                        new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
                    else:
                        new_kwargs[arg_name] = arg_value
            output = old_func(*new_args, **new_kwargs)
            if out_fp32:
                output = cast_tensor_type(output, torch.half, torch.float)
            return output
        return new_func
    return auto_fp16_wrapper","for (arg_name, arg_value) in kwargs.items():
    if arg_name in args_to_cast:
        new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
    else:
        new_kwargs[arg_name] = arg_value",XXX,no_found,,,,
ansible-modules-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible-modules-core/cloud/docker/docker_container.py,https://github.com/ansible/ansible-modules-core/tree/master/cloud/docker/docker_container.py,TaskParameters,_host_config$898,"def _host_config(self):
    """"""
        Returns parameters used to create a HostConfig object
        """"""
    host_config_params = dict(port_bindings='published_ports', publish_all_ports='publish_all_ports', links='links', privileged='privileged', dns='dns_servers', dns_search='dns_search_domains', binds='volume_binds', volumes_from='volumes_from', network_mode='network_mode', cap_add='capabilities', extra_hosts='etc_hosts', read_only='read_only', ipc_mode='ipc_mode', security_opt='security_opts', ulimits='ulimits', log_config='log_config', mem_limit='memory', memswap_limit='memory_swap', mem_swappiness='memory_swappiness', oom_score_adj='oom_score_adj', shm_size='shm_size', group_add='groups', devices='devices', pid_mode='pid_mode')
    params = dict()
    for (key, value) in host_config_params.iteritems():
        if getattr(self, value, None) is not None:
            params[key] = getattr(self, value)
    if self.restart_policy:
        params['restart_policy'] = dict(Name=self.restart_policy, MaximumRetryCount=self.restart_retries)
    return self.client.create_host_config(**params)","for (key, value) in host_config_params.iteritems():
    if getattr(self, value, None) is not None:
        params[key] = getattr(self, value)",XXX,no_found,0,,,
pysystemtrade,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pysystemtrade/sysobjects/production/timed_storage.py,https://github.com/robcarver17/pysystemtrade/tree/master/sysobjects/production/timed_storage.py,timedEntry,_resolve_args_passed_as_star_args$77,"def _resolve_args_passed_as_star_args(self, args: tuple, date: datetime.datetime) -> dict:
    required_args = self.required_argument_names
    try:
        assert len(required_args) == len(args)
    except BaseException:
        raise Exception('Expecting to be passed arguments of length %d to match %s, instead got %d arguments' % (len(required_args), str(required_args), len(args)))
    args_as_dict = {}
    for (arg_name, arg_value) in zip(required_args, args):
        args_as_dict[arg_name] = arg_value
    args_as_dict[DATE_KEY_NAME] = date
    return args_as_dict","for (arg_name, arg_value) in zip(required_args, args):
    args_as_dict[arg_name] = arg_value",XXX,no_found,,,,
qutip,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutip/qutip/qip/qasm.py,https://github.com/qutip/qutip/tree/master/qutip/qip/qasm.py,QasmProcessor,_custom_gate$320,"def _custom_gate(self, qc_temp, gate_call):
    """"""
        Recursively process a custom-defined gate with specified arguments
        to produce a dummy circuit with all the gates in the custom-defined
        gate.

        Parameters
        ----------

        qc_temp: :class:`.QubitCircuit`
            temporary circuit to process custom gate
        gate_call: list of str
            tokens corresponding to gate signature/call
        """"""
    (gate_name, args, regs) = gate_call
    gate = self.qasm_gates[gate_name]
    args_map = {}
    regs_map = {}
    for (i, arg) in enumerate(gate.gate_args):
        args_map[arg] = eval(str(args[i]))
    for (i, reg) in enumerate(gate.gate_regs):
        regs_map[reg] = regs[i]
    for call in gate.gates_inside:
        (name, com_args, com_regs) = call
        for (arg, real_arg) in args_map.items():
            com_args = [command.replace(arg.strip(), str(real_arg)) for command in com_args]
        for (reg, real_reg) in regs_map.items():
            com_regs = [command.replace(reg.strip(), str(real_reg)) for command in com_regs]
        com_args = [eval(arg) for arg in com_args]
        if name in self.predefined_gates:
            qc_temp.user_gates = _get_qiskit_gates()
            com_regs = [int(reg) for reg in com_regs]
            self._add_predefined_gates(qc_temp, name, com_regs, com_args)
        else:
            self._custom_gate(qc_temp, [name, com_args, com_regs])","for (i, reg) in enumerate(gate.gate_regs):
    regs_map[reg] = regs[i]",XXX,no_found,0,,,
django-autofixture,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-autofixture/autofixture/values.py,https://github.com/gregmuellegger/django-autofixture/tree/master/autofixture/values.py,ValuesMetaclass,__new__$5,"def __new__(mcs, name, bases, attrs):
    parent_value_attrs = {}
    for base in bases[::-1]:
        if hasattr(base, '_value_attrs'):
            parent_value_attrs.update(base._value_attrs)
    defined_value_attrs = {}
    for key in attrs:
        if not key.startswith('__'):
            defined_value_attrs[key] = attrs[key]
    for key in defined_value_attrs:
        del attrs[key]
    attrs['_value_attrs'] = {}
    attrs['_value_attrs'].update(parent_value_attrs)
    attrs['_value_attrs'].update(defined_value_attrs)
    return super(ValuesMetaclass, mcs).__new__(mcs, name, bases, attrs)","for key in attrs:
    if not key.startswith('__'):
        defined_value_attrs[key] = attrs[key]",XXX,no_found,0,,,
cityscapesScripts,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cityscapesScripts/cityscapesscripts/evaluation/evalInstanceLevelSemanticLabeling.py,https://github.com/mcordts/cityscapesScripts/tree/master/cityscapesscripts/evaluation/evalInstanceLevelSemanticLabeling.py,,assignGt2Preds$254,"def assignGt2Preds(gtInstancesOrig, gtImage, predInfo, args):
    predInstances = {}
    predInstCount = 0
    for label in args.instLabels:
        predInstances[label] = []
    gtInstances = deepcopy(gtInstancesOrig)
    for label in gtInstances:
        for gt in gtInstances[label]:
            gt['matchedPred'] = []
    gtNp = np.array(gtImage)
    voidLabelIDList = []
    for label in labels:
        if label.ignoreInEval:
            voidLabelIDList.append(label.id)
    boolVoid = np.in1d(gtNp, voidLabelIDList).reshape(gtNp.shape)
    for predImageFile in predInfo:
        labelID = predInfo[predImageFile]['labelID']
        predConf = predInfo[predImageFile]['conf']
        labelName = id2label[int(labelID)].name
        if not labelName in args.instLabels:
            continue
        predImage = Image.open(predImageFile)
        predImage = predImage.convert('L')
        predNp = np.array(predImage)
        boolPredInst = predNp != 0
        predPixelCount = np.count_nonzero(boolPredInst)
        if not predPixelCount:
            continue
        predInstance = {}
        predInstance['imgName'] = predImageFile
        predInstance['predID'] = predInstCount
        predInstance['labelID'] = int(labelID)
        predInstance['pixelCount'] = predPixelCount
        predInstance['confidence'] = predConf
        predInstance['voidIntersection'] = np.count_nonzero(np.logical_and(boolVoid, boolPredInst))
        matchedGt = []
        for (gtNum, gtInstance) in enumerate(gtInstancesOrig[labelName]):
            intersection = np.count_nonzero(np.logical_and(gtNp == gtInstance['instID'], boolPredInst))
            if intersection > 0:
                gtCopy = gtInstance.copy()
                predCopy = predInstance.copy()
                gtCopy['intersection'] = intersection
                predCopy['intersection'] = intersection
                matchedGt.append(gtCopy)
                gtInstances[labelName][gtNum]['matchedPred'].append(predCopy)
        predInstance['matchedGt'] = matchedGt
        predInstCount += 1
        predInstances[labelName].append(predInstance)
    return (gtInstances, predInstances)","for label in args.instLabels:
    predInstances[label] = []",XXX,no_found,0,,,
VideoSuperResolution,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VideoSuperResolution/prepare_data.py,https://github.com/LoSealL/VideoSuperResolution/tree/master//prepare_data.py,,main$157,"def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('filter', help='an re pattern to filter candidates.')
    parser.add_argument('--download_dir', type=str, default=_DEFAULT_DOWNLOAD_DIR, help='Specify download directory. [{}]'.format(_DEFAULT_DOWNLOAD_DIR))
    parser.add_argument('--data_dir', type=str, default=_DEFAULT_DATASET_PATH, help='Specify dataset extracted directory. [{}]'.format(_DEFAULT_DATASET_PATH))
    parser.add_argument('--weights_dir', type=str, default=_DEFAULT_WEIGHTS_DIR, help='Specify weights extracted directory. [{}]'.format(_DEFAULT_WEIGHTS_DIR))
    parser.add_argument('-q', '--quiet', action='store_true', help='download quietly')
    (args, _) = parser.parse_known_args()
    Path(args.download_dir).mkdir(exist_ok=True, parents=True)

    def get_leaf(key: str, node: dict):
        for (k, v) in node.items():
            if isinstance(v, dict):
                for (k2, v2) in get_leaf(k, v):
                    yield (Path(key) / k2, v2)
            else:
                yield (Path(key) / k, v)
    need_to_download = {}
    try:
        Path(args.data_dir).mkdir(exist_ok=True, parents=True)
        for (k, v) in get_leaf(args.data_dir, DATASETS):
            if user_input(k.stem, args.quiet, args.filter):
                need_to_download[k] = v
    except (FileNotFoundError, OSError):
        pass
    from VSR.Backend import BACKEND
    for (k, v) in get_leaf(args.weights_dir, WEIGHTS[BACKEND]):
        if user_input(k.stem, args.quiet, args.filter):
            need_to_download[k] = v
    need_to_extract = {}
    for (k, v) in need_to_download.items():
        if v[:4] == 'http':
            need_to_extract[k] = (k.parent, download(k.name, v, args.download_dir))
        else:
            need_to_extract[k] = (k.parent, drive_download(k.name, v, args.download_dir))
    for (k, v) in need_to_extract.values():
        if v is None:
            continue
        ext = Path(v).suffix
        if ext in ('.tar', '.tgz', '.gz', '.bz'):
            open_fn = tarfile.open
            is_match_fn = tarfile.is_tarfile
        elif ext in ('.zip',):
            open_fn = zipfile.ZipFile
            is_match_fn = zipfile.is_zipfile
        else:

            class copy:

                def __init__(self, src):
                    self.src = src

                def __enter__(self):
                    return self

                def __exit__(self, exc_type, exc_val, exc_tb):
                    return

                def extractall(self, dst):
                    import shutil
                    shutil.copy(self.src, dst)
            is_match_fn = lambda x: True
            open_fn = copy
        if is_match_fn(v):
            with open_fn(v) as fd:
                try:
                    fd.extractall(str(k.resolve()))
                except (tarfile.TarError, RuntimeError, KeyboardInterrupt):
                    pass
        else:
            print('[WARN] {} have to be uncompressed manually.'.format(v))","for (k, v) in get_leaf(args.weights_dir, WEIGHTS[BACKEND]):
    if user_input(k.stem, args.quiet, args.filter):
        need_to_download[k] = v",XXX,no_found,1,,,
VideoSuperResolution,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VideoSuperResolution/prepare_data.py,https://github.com/LoSealL/VideoSuperResolution/tree/master//prepare_data.py,,main$157,"def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('filter', help='an re pattern to filter candidates.')
    parser.add_argument('--download_dir', type=str, default=_DEFAULT_DOWNLOAD_DIR, help='Specify download directory. [{}]'.format(_DEFAULT_DOWNLOAD_DIR))
    parser.add_argument('--data_dir', type=str, default=_DEFAULT_DATASET_PATH, help='Specify dataset extracted directory. [{}]'.format(_DEFAULT_DATASET_PATH))
    parser.add_argument('--weights_dir', type=str, default=_DEFAULT_WEIGHTS_DIR, help='Specify weights extracted directory. [{}]'.format(_DEFAULT_WEIGHTS_DIR))
    parser.add_argument('-q', '--quiet', action='store_true', help='download quietly')
    (args, _) = parser.parse_known_args()
    Path(args.download_dir).mkdir(exist_ok=True, parents=True)

    def get_leaf(key: str, node: dict):
        for (k, v) in node.items():
            if isinstance(v, dict):
                for (k2, v2) in get_leaf(k, v):
                    yield (Path(key) / k2, v2)
            else:
                yield (Path(key) / k, v)
    need_to_download = {}
    try:
        Path(args.data_dir).mkdir(exist_ok=True, parents=True)
        for (k, v) in get_leaf(args.data_dir, DATASETS):
            if user_input(k.stem, args.quiet, args.filter):
                need_to_download[k] = v
    except (FileNotFoundError, OSError):
        pass
    from VSR.Backend import BACKEND
    for (k, v) in get_leaf(args.weights_dir, WEIGHTS[BACKEND]):
        if user_input(k.stem, args.quiet, args.filter):
            need_to_download[k] = v
    need_to_extract = {}
    for (k, v) in need_to_download.items():
        if v[:4] == 'http':
            need_to_extract[k] = (k.parent, download(k.name, v, args.download_dir))
        else:
            need_to_extract[k] = (k.parent, drive_download(k.name, v, args.download_dir))
    for (k, v) in need_to_extract.values():
        if v is None:
            continue
        ext = Path(v).suffix
        if ext in ('.tar', '.tgz', '.gz', '.bz'):
            open_fn = tarfile.open
            is_match_fn = tarfile.is_tarfile
        elif ext in ('.zip',):
            open_fn = zipfile.ZipFile
            is_match_fn = zipfile.is_zipfile
        else:

            class copy:

                def __init__(self, src):
                    self.src = src

                def __enter__(self):
                    return self

                def __exit__(self, exc_type, exc_val, exc_tb):
                    return

                def extractall(self, dst):
                    import shutil
                    shutil.copy(self.src, dst)
            is_match_fn = lambda x: True
            open_fn = copy
        if is_match_fn(v):
            with open_fn(v) as fd:
                try:
                    fd.extractall(str(k.resolve()))
                except (tarfile.TarError, RuntimeError, KeyboardInterrupt):
                    pass
        else:
            print('[WARN] {} have to be uncompressed manually.'.format(v))","for (k, v) in need_to_download.items():
    if v[:4] == 'http':
        need_to_extract[k] = (k.parent, download(k.name, v, args.download_dir))
    else:
        need_to_extract[k] = (k.parent, drive_download(k.name, v, args.download_dir))",XXX,no_found,0,,,
VideoSuperResolution,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VideoSuperResolution/prepare_data.py,https://github.com/LoSealL/VideoSuperResolution/tree/master//prepare_data.py,,main$157,"def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('filter', help='an re pattern to filter candidates.')
    parser.add_argument('--download_dir', type=str, default=_DEFAULT_DOWNLOAD_DIR, help='Specify download directory. [{}]'.format(_DEFAULT_DOWNLOAD_DIR))
    parser.add_argument('--data_dir', type=str, default=_DEFAULT_DATASET_PATH, help='Specify dataset extracted directory. [{}]'.format(_DEFAULT_DATASET_PATH))
    parser.add_argument('--weights_dir', type=str, default=_DEFAULT_WEIGHTS_DIR, help='Specify weights extracted directory. [{}]'.format(_DEFAULT_WEIGHTS_DIR))
    parser.add_argument('-q', '--quiet', action='store_true', help='download quietly')
    (args, _) = parser.parse_known_args()
    Path(args.download_dir).mkdir(exist_ok=True, parents=True)

    def get_leaf(key: str, node: dict):
        for (k, v) in node.items():
            if isinstance(v, dict):
                for (k2, v2) in get_leaf(k, v):
                    yield (Path(key) / k2, v2)
            else:
                yield (Path(key) / k, v)
    need_to_download = {}
    try:
        Path(args.data_dir).mkdir(exist_ok=True, parents=True)
        for (k, v) in get_leaf(args.data_dir, DATASETS):
            if user_input(k.stem, args.quiet, args.filter):
                need_to_download[k] = v
    except (FileNotFoundError, OSError):
        pass
    from VSR.Backend import BACKEND
    for (k, v) in get_leaf(args.weights_dir, WEIGHTS[BACKEND]):
        if user_input(k.stem, args.quiet, args.filter):
            need_to_download[k] = v
    need_to_extract = {}
    for (k, v) in need_to_download.items():
        if v[:4] == 'http':
            need_to_extract[k] = (k.parent, download(k.name, v, args.download_dir))
        else:
            need_to_extract[k] = (k.parent, drive_download(k.name, v, args.download_dir))
    for (k, v) in need_to_extract.values():
        if v is None:
            continue
        ext = Path(v).suffix
        if ext in ('.tar', '.tgz', '.gz', '.bz'):
            open_fn = tarfile.open
            is_match_fn = tarfile.is_tarfile
        elif ext in ('.zip',):
            open_fn = zipfile.ZipFile
            is_match_fn = zipfile.is_zipfile
        else:

            class copy:

                def __init__(self, src):
                    self.src = src

                def __enter__(self):
                    return self

                def __exit__(self, exc_type, exc_val, exc_tb):
                    return

                def extractall(self, dst):
                    import shutil
                    shutil.copy(self.src, dst)
            is_match_fn = lambda x: True
            open_fn = copy
        if is_match_fn(v):
            with open_fn(v) as fd:
                try:
                    fd.extractall(str(k.resolve()))
                except (tarfile.TarError, RuntimeError, KeyboardInterrupt):
                    pass
        else:
            print('[WARN] {} have to be uncompressed manually.'.format(v))","for (k, v) in get_leaf(args.data_dir, DATASETS):
    if user_input(k.stem, args.quiet, args.filter):
        need_to_download[k] = v",XXX,no_found,1,,,
fiftyone,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fiftyone/fiftyone/core/collections.py,https://github.com/voxel51/fiftyone/tree/master/fiftyone/core/collections.py,SampleCollection,_get_dynamic_field_schema$1139,"def _get_dynamic_field_schema(self, schema, prefix, fields=None):
    if fields is not None:
        if etau.is_str(fields):
            fields = {fields}
        else:
            fields = set(fields)
        schema = {k: v for (k, v) in schema.items() if k in fields}
    aggs = []
    paths = []
    for (name, field) in schema.items():
        if isinstance(field, fof.EmbeddedDocumentField):
            path = name
            aggs.append(foa.Schema(prefix + path, dynamic_only=True))
            paths.append(path)
            if issubclass(field.document_type, fol._LABEL_LIST_FIELDS):
                path = name + '.' + field.document_type._LABEL_LIST_FIELD
                aggs.append(foa.Schema(prefix + path, dynamic_only=True))
                paths.append(path)
    fields = {}
    if aggs:
        results = self.aggregate(aggs)
        for (path, schema) in zip(paths, results):
            for (name, field) in schema.items():
                fields[path + '.' + name] = field
    return fields","for (path, schema) in zip(paths, results):
    for (name, field) in schema.items():
        fields[path + '.' + name] = field",XXX,no_found,0,,,
keystone,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keystone/keystone/models/revoke_model.py,https://github.com/openstack/keystone/tree/master/keystone/models/revoke_model.py,,blank_token_data$66,"def blank_token_data(issued_at):
    token_data = dict()
    for name in _NAMES:
        token_data[name] = None
    for name in _TOKEN_KEYS:
        token_data[name] = None
    token_data['issued_at'] = issued_at
    return token_data","for name in _NAMES:
    token_data[name] = None",XXX,no_found,1,,,
keystone,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keystone/keystone/models/revoke_model.py,https://github.com/openstack/keystone/tree/master/keystone/models/revoke_model.py,,blank_token_data$66,"def blank_token_data(issued_at):
    token_data = dict()
    for name in _NAMES:
        token_data[name] = None
    for name in _TOKEN_KEYS:
        token_data[name] = None
    token_data['issued_at'] = issued_at
    return token_data","for name in _TOKEN_KEYS:
    token_data[name] = None",XXX,no_found,1,,,
elasticdl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/elasticdl/model_zoo/census_dnn_model/census_feature_columns.py,https://github.com/sql-machine-learning/elasticdl/tree/master/model_zoo/census_dnn_model/census_feature_columns.py,,get_feature_input_layers$56,"def get_feature_input_layers():
    feature_input_layers = {}
    for numeric_feature_key in NUMERIC_FEATURE_KEYS:
        feature_input_layers[numeric_feature_key] = tf.keras.Input(shape=(1,), name=numeric_feature_key, dtype=tf.float32)
    for categorical_feature_key in CATEGORICAL_FEATURE_KEYS:
        feature_input_layers[categorical_feature_key] = tf.keras.Input(shape=(1,), name=categorical_feature_key, dtype=tf.string)
    return feature_input_layers","for numeric_feature_key in NUMERIC_FEATURE_KEYS:
    feature_input_layers[numeric_feature_key] = tf.keras.Input(shape=(1,), name=numeric_feature_key, dtype=tf.float32)",XXX,no_found,1,,,
elasticdl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/elasticdl/model_zoo/census_dnn_model/census_feature_columns.py,https://github.com/sql-machine-learning/elasticdl/tree/master/model_zoo/census_dnn_model/census_feature_columns.py,,get_feature_input_layers$56,"def get_feature_input_layers():
    feature_input_layers = {}
    for numeric_feature_key in NUMERIC_FEATURE_KEYS:
        feature_input_layers[numeric_feature_key] = tf.keras.Input(shape=(1,), name=numeric_feature_key, dtype=tf.float32)
    for categorical_feature_key in CATEGORICAL_FEATURE_KEYS:
        feature_input_layers[categorical_feature_key] = tf.keras.Input(shape=(1,), name=categorical_feature_key, dtype=tf.string)
    return feature_input_layers","for categorical_feature_key in CATEGORICAL_FEATURE_KEYS:
    feature_input_layers[categorical_feature_key] = tf.keras.Input(shape=(1,), name=categorical_feature_key, dtype=tf.string)",XXX,no_found,1,,,
supervisor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/supervisor/supervisor/options.py,https://github.com/home-assistant/supervisor/tree/master/supervisor/options.py,Options,get_plugins$363,"def get_plugins(self, parser, factory_key, section_prefix):
    factories = []
    for section in parser.sections():
        if not section.startswith(section_prefix):
            continue
        name = section.split(':', 1)[1]
        factory_spec = parser.saneget(section, factory_key, None)
        if factory_spec is None:
            raise ValueError('section [%s] does not specify a %s' % (section, factory_key))
        try:
            factory = self.import_spec(factory_spec)
        except ImportError:
            raise ValueError('%s cannot be resolved within [%s]' % (factory_spec, section))
        extras = {}
        for k in parser.options(section):
            if k != factory_key:
                extras[k] = parser.saneget(section, k)
        factories.append((name, factory, extras))
    return factories","for k in parser.options(section):
    if k != factory_key:
        extras[k] = parser.saneget(section, k)",XXX,no_found,,,,
pyspider,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyspider/pyspider/libs/base_handler.py,https://github.com/binux/pyspider/tree/master/pyspider/libs/base_handler.py,BaseHandler,_crawl$255,"def _crawl(self, url, **kwargs):
    """"""
        real crawl API

        checking kwargs, and repack them to each sub-dict
        """"""
    task = {}
    assert len(url) < 1024, 'Maximum (1024) URL length error.'
    if kwargs.get('callback'):
        callback = kwargs['callback']
        if isinstance(callback, six.string_types) and hasattr(self, callback):
            func = getattr(self, callback)
        elif six.callable(callback) and six.get_method_self(callback) is self:
            func = callback
            kwargs['callback'] = func.__name__
        elif six.callable(callback) and hasattr(self, callback.__name__):
            func = getattr(self, callback.__name__)
            kwargs['callback'] = func.__name__
        else:
            raise NotImplementedError('self.%s() not implemented!' % callback)
        if hasattr(func, '_config'):
            for (k, v) in iteritems(func._config):
                if isinstance(v, dict) and isinstance(kwargs.get(k), dict):
                    kwargs[k].update(v)
                else:
                    kwargs.setdefault(k, v)
    url = quote_chinese(_build_url(url.strip(), kwargs.pop('params', None)))
    if kwargs.get('files'):
        assert isinstance(kwargs.get('data', {}), dict), 'data must be a dict when using with files!'
        (content_type, data) = _encode_multipart_formdata(kwargs.pop('data', {}), kwargs.pop('files', {}))
        kwargs.setdefault('headers', {})
        kwargs['headers']['Content-Type'] = content_type
        kwargs['data'] = data
    if kwargs.get('data'):
        kwargs['data'] = _encode_params(kwargs['data'])
    if kwargs.get('data'):
        kwargs.setdefault('method', 'POST')
    if kwargs.get('user_agent'):
        kwargs.setdefault('headers', {})
        kwargs['headers']['User-Agent'] = kwargs.get('user_agent')
    schedule = {}
    for key in self.schedule_fields:
        if key in kwargs:
            schedule[key] = kwargs.pop(key)
        elif key in self.crawl_config:
            schedule[key] = self.crawl_config[key]
    task['schedule'] = schedule
    fetch = {}
    for key in self.fetch_fields:
        if key in kwargs:
            fetch[key] = kwargs.pop(key)
    task['fetch'] = fetch
    process = {}
    for key in self.process_fields:
        if key in kwargs:
            process[key] = kwargs.pop(key)
    task['process'] = process
    task['project'] = self.project_name
    task['url'] = url
    if 'taskid' in kwargs:
        task['taskid'] = kwargs.pop('taskid')
    else:
        task['taskid'] = self.get_taskid(task)
    if kwargs:
        raise TypeError('crawl() got unexpected keyword argument: %s' % kwargs.keys())
    if self.is_debugger():
        task = self.task_join_crawl_config(task, self.crawl_config)
    cache_key = '%(project)s:%(taskid)s' % task
    if cache_key not in self._follows_keys:
        self._follows_keys.add(cache_key)
        self._follows.append(task)
    return task","for key in self.fetch_fields:
    if key in kwargs:
        fetch[key] = kwargs.pop(key)",XXX,no_found,0,,,
pyspider,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyspider/pyspider/libs/base_handler.py,https://github.com/binux/pyspider/tree/master/pyspider/libs/base_handler.py,BaseHandler,_crawl$255,"def _crawl(self, url, **kwargs):
    """"""
        real crawl API

        checking kwargs, and repack them to each sub-dict
        """"""
    task = {}
    assert len(url) < 1024, 'Maximum (1024) URL length error.'
    if kwargs.get('callback'):
        callback = kwargs['callback']
        if isinstance(callback, six.string_types) and hasattr(self, callback):
            func = getattr(self, callback)
        elif six.callable(callback) and six.get_method_self(callback) is self:
            func = callback
            kwargs['callback'] = func.__name__
        elif six.callable(callback) and hasattr(self, callback.__name__):
            func = getattr(self, callback.__name__)
            kwargs['callback'] = func.__name__
        else:
            raise NotImplementedError('self.%s() not implemented!' % callback)
        if hasattr(func, '_config'):
            for (k, v) in iteritems(func._config):
                if isinstance(v, dict) and isinstance(kwargs.get(k), dict):
                    kwargs[k].update(v)
                else:
                    kwargs.setdefault(k, v)
    url = quote_chinese(_build_url(url.strip(), kwargs.pop('params', None)))
    if kwargs.get('files'):
        assert isinstance(kwargs.get('data', {}), dict), 'data must be a dict when using with files!'
        (content_type, data) = _encode_multipart_formdata(kwargs.pop('data', {}), kwargs.pop('files', {}))
        kwargs.setdefault('headers', {})
        kwargs['headers']['Content-Type'] = content_type
        kwargs['data'] = data
    if kwargs.get('data'):
        kwargs['data'] = _encode_params(kwargs['data'])
    if kwargs.get('data'):
        kwargs.setdefault('method', 'POST')
    if kwargs.get('user_agent'):
        kwargs.setdefault('headers', {})
        kwargs['headers']['User-Agent'] = kwargs.get('user_agent')
    schedule = {}
    for key in self.schedule_fields:
        if key in kwargs:
            schedule[key] = kwargs.pop(key)
        elif key in self.crawl_config:
            schedule[key] = self.crawl_config[key]
    task['schedule'] = schedule
    fetch = {}
    for key in self.fetch_fields:
        if key in kwargs:
            fetch[key] = kwargs.pop(key)
    task['fetch'] = fetch
    process = {}
    for key in self.process_fields:
        if key in kwargs:
            process[key] = kwargs.pop(key)
    task['process'] = process
    task['project'] = self.project_name
    task['url'] = url
    if 'taskid' in kwargs:
        task['taskid'] = kwargs.pop('taskid')
    else:
        task['taskid'] = self.get_taskid(task)
    if kwargs:
        raise TypeError('crawl() got unexpected keyword argument: %s' % kwargs.keys())
    if self.is_debugger():
        task = self.task_join_crawl_config(task, self.crawl_config)
    cache_key = '%(project)s:%(taskid)s' % task
    if cache_key not in self._follows_keys:
        self._follows_keys.add(cache_key)
        self._follows.append(task)
    return task","for key in self.process_fields:
    if key in kwargs:
        process[key] = kwargs.pop(key)",XXX,no_found,0,,,
MB-Lab,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MB-Lab/jointscreator.py,https://github.com/animate1978/MB-Lab/tree/master//jointscreator.py,,create_offset_template_file$262,"def create_offset_template_file(filepath):
    file = {}
    for item in skeleton_ops.ik_joints_head:
        file[item] = [0, 0, 0]
    for item in skeleton_ops.ik_joints_tail:
        file[item] = [0, 0, 0]
    with open(filepath, 'w') as j_file:
        json.dump(file, j_file, indent=2)","for item in skeleton_ops.ik_joints_head:
    file[item] = [0, 0, 0]",XXX,no_found,1,,,
MB-Lab,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MB-Lab/jointscreator.py,https://github.com/animate1978/MB-Lab/tree/master//jointscreator.py,,create_offset_template_file$262,"def create_offset_template_file(filepath):
    file = {}
    for item in skeleton_ops.ik_joints_head:
        file[item] = [0, 0, 0]
    for item in skeleton_ops.ik_joints_tail:
        file[item] = [0, 0, 0]
    with open(filepath, 'w') as j_file:
        json.dump(file, j_file, indent=2)","for item in skeleton_ops.ik_joints_tail:
    file[item] = [0, 0, 0]",XXX,no_found,1,,,
zato,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zato/code/zato-server/src/zato/server/connection/web_socket/msg.py,https://github.com/zatosource/zato/tree/master/code/zato-server/src/zato/server/connection/web_socket/msg.py,ClientMessage,get_meta$107,"def get_meta(self, attrs=('action', 'service', 'id', 'timestamp', 'cid', 'in_reply_to', 'ext_client_id', 'ext_client_name')):
    out = {}
    for name in attrs:
        out[name] = getattr(self, name)
    return out","for name in attrs:
    out[name] = getattr(self, name)",XXX,no_found,1,,,
rssant,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rssant/rssant_api/models/image.py,https://github.com/anyant/rssant/tree/master/rssant_api/models/image.py,ImageInfo,batch_detect$42,"def batch_detect(cls, url_roots: typing.List[str]) -> dict:
    sql = '\n        SELECT DISTINCT ON (url_root)\n            id, url_root, status_code\n        FROM rssant_api_imageinfo\n        WHERE url_root = ANY(%s) AND (\n            (status_code > 0 AND dt_created > %s) OR\n            (status_code <= 0 AND dt_created > %s)\n        )\n        ORDER BY url_root, dt_created DESC\n        '
    now = timezone.now()
    dt_pos = now - POSITIVE_STATUS_TTL
    dt_neg = now - NEGTIVE_STATUS_TTL
    url_root_map = {}
    rows = cls.objects.raw(sql, [list(url_roots), dt_pos, dt_neg])
    for row in rows:
        url_root_map[row.url_root] = row.status_code
    return url_root_map","for row in rows:
    url_root_map[row.url_root] = row.status_code",XXX,no_found,,,,
pandapower,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/converter/powermodels/to_pm.py,https://github.com/e2nIEE/pandapower/tree/master/pandapower/converter/powermodels/to_pm.py,,add_params_to_pm$477,"def add_params_to_pm(net, pm):
    pd_idxs_br = []
    pm_idxs_br = []
    br_elms = ['line', 'trafo']
    pm['user_defined_params'] = dict()
    for elm in ['bus', 'line', 'gen', 'load', 'trafo', 'sgen']:
        param_cols = [col for col in net[elm].columns if 'pm_param' in col]
        if not param_cols:
            continue
        params = [param_col.split('/')[-1] for param_col in param_cols]
        br_param = list(set(params) - {'side'})
        for (param, param_col) in zip(params, param_cols):
            pd_idxs = net[elm].index[net[elm][param_col].notna()].tolist()
            target_values = net[elm][param_col][pd_idxs].values.tolist()
            if elm in br_elms and param in br_param:
                pd_idxs_br += net[elm].index[net[elm][param_col].notna()].tolist()
                target_values = net[elm][param_col][pd_idxs_br].values.tolist()
            if elm in ['line', 'trafo']:
                (start, end) = net._pd2pm_lookups['branch'][elm]
                pd_pos = [net[elm].index.tolist().index(p) for p in pd_idxs_br]
                pm_idxs = [int(v) + start for v in pd_pos]
            elif elm == 'sgen':
                pm_idxs = [int(v) for v in net._pd2pm_lookups[elm + '_controllable'][pd_idxs]]
                elm = 'gen'
            else:
                pm_idxs = [int(v) for v in net._pd2pm_lookups[elm][pd_idxs]]
            df = pd.DataFrame(index=pm_idxs) if elm not in ['line', 'trafo'] else pd.DataFrame(index=pm_idxs_br)
            df['element_index'] = pm_idxs
            df['element_pp_index'] = pd_idxs if elm not in ['line', 'trafo'] else pd_idxs_br
            df['value'] = target_values
            df['element'] = elm
            pm['user_defined_params'][param] = df.to_dict(into=OrderedDict, orient='index')
        if elm in ['line', 'trafo']:
            for bp in br_param:
                for k in pm['user_defined_params']['side'].keys():
                    side = pm['user_defined_params']['side'][k]['value']
                    side_bus_f = side + '_bus'
                    if elm == 'line':
                        side_bus_t = 'from_bus' if side == 'to' else 'to_bus'
                    if elm == 'trafo':
                        side_bus_t = 'hv_bus' if side == 'lv' else 'lv_bus'
                    pd_idx = pm['user_defined_params']['side'][k]['element_pp_index']
                    ppcidx = net._pd2pm_lookups['branch'][elm][0] - 1 + pd_idx
                    if side in ['from', 'hv']:
                        ppcrow_f = 0
                        ppcrow_t = 1
                    else:
                        ppcrow_f = 1
                        ppcrow_t = 0
                        assert side in ['to', 'lv']
                    pm['user_defined_params'][bp][k]['f_bus'] = int(net._ppc_opf['branch'][ppcidx, ppcrow_f].real) + 1
                    pm['user_defined_params'][bp][k]['t_bus'] = int(net._ppc_opf['branch'][ppcidx, ppcrow_t].real) + 1
    dic = {}
    if 'user_defined_params' in pm.keys():
        for elm in ['gen', 'sgen_controllable']:
            if elm in net._pd2pm_lookups.keys():
                pm_idxs = net._pd2pm_lookups[elm]
                for k in pm_idxs[pm_idxs != -1]:
                    dic[str(k)] = k
        if dic != {}:
            pm['user_defined_params']['gen_and_controllable_sgen'] = dic
    if 'obj_factors' in net.keys():
        assert type(net.obj_factors) == list
        assert sum(net.obj_factors) <= 1
        dic = {}
        for (i, k) in enumerate(net.obj_factors):
            dic['fac_' + str(i + 1)] = k
        pm['user_defined_params']['obj_factors'] = dic
    return pm","for elm in ['gen', 'sgen_controllable']:
    if elm in net._pd2pm_lookups.keys():
        pm_idxs = net._pd2pm_lookups[elm]
        for k in pm_idxs[pm_idxs != -1]:
            dic[str(k)] = k",XXX,no_found,0,,,
pandapower,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandapower/pandapower/converter/powermodels/to_pm.py,https://github.com/e2nIEE/pandapower/tree/master/pandapower/converter/powermodels/to_pm.py,,add_params_to_pm$477,"def add_params_to_pm(net, pm):
    pd_idxs_br = []
    pm_idxs_br = []
    br_elms = ['line', 'trafo']
    pm['user_defined_params'] = dict()
    for elm in ['bus', 'line', 'gen', 'load', 'trafo', 'sgen']:
        param_cols = [col for col in net[elm].columns if 'pm_param' in col]
        if not param_cols:
            continue
        params = [param_col.split('/')[-1] for param_col in param_cols]
        br_param = list(set(params) - {'side'})
        for (param, param_col) in zip(params, param_cols):
            pd_idxs = net[elm].index[net[elm][param_col].notna()].tolist()
            target_values = net[elm][param_col][pd_idxs].values.tolist()
            if elm in br_elms and param in br_param:
                pd_idxs_br += net[elm].index[net[elm][param_col].notna()].tolist()
                target_values = net[elm][param_col][pd_idxs_br].values.tolist()
            if elm in ['line', 'trafo']:
                (start, end) = net._pd2pm_lookups['branch'][elm]
                pd_pos = [net[elm].index.tolist().index(p) for p in pd_idxs_br]
                pm_idxs = [int(v) + start for v in pd_pos]
            elif elm == 'sgen':
                pm_idxs = [int(v) for v in net._pd2pm_lookups[elm + '_controllable'][pd_idxs]]
                elm = 'gen'
            else:
                pm_idxs = [int(v) for v in net._pd2pm_lookups[elm][pd_idxs]]
            df = pd.DataFrame(index=pm_idxs) if elm not in ['line', 'trafo'] else pd.DataFrame(index=pm_idxs_br)
            df['element_index'] = pm_idxs
            df['element_pp_index'] = pd_idxs if elm not in ['line', 'trafo'] else pd_idxs_br
            df['value'] = target_values
            df['element'] = elm
            pm['user_defined_params'][param] = df.to_dict(into=OrderedDict, orient='index')
        if elm in ['line', 'trafo']:
            for bp in br_param:
                for k in pm['user_defined_params']['side'].keys():
                    side = pm['user_defined_params']['side'][k]['value']
                    side_bus_f = side + '_bus'
                    if elm == 'line':
                        side_bus_t = 'from_bus' if side == 'to' else 'to_bus'
                    if elm == 'trafo':
                        side_bus_t = 'hv_bus' if side == 'lv' else 'lv_bus'
                    pd_idx = pm['user_defined_params']['side'][k]['element_pp_index']
                    ppcidx = net._pd2pm_lookups['branch'][elm][0] - 1 + pd_idx
                    if side in ['from', 'hv']:
                        ppcrow_f = 0
                        ppcrow_t = 1
                    else:
                        ppcrow_f = 1
                        ppcrow_t = 0
                        assert side in ['to', 'lv']
                    pm['user_defined_params'][bp][k]['f_bus'] = int(net._ppc_opf['branch'][ppcidx, ppcrow_f].real) + 1
                    pm['user_defined_params'][bp][k]['t_bus'] = int(net._ppc_opf['branch'][ppcidx, ppcrow_t].real) + 1
    dic = {}
    if 'user_defined_params' in pm.keys():
        for elm in ['gen', 'sgen_controllable']:
            if elm in net._pd2pm_lookups.keys():
                pm_idxs = net._pd2pm_lookups[elm]
                for k in pm_idxs[pm_idxs != -1]:
                    dic[str(k)] = k
        if dic != {}:
            pm['user_defined_params']['gen_and_controllable_sgen'] = dic
    if 'obj_factors' in net.keys():
        assert type(net.obj_factors) == list
        assert sum(net.obj_factors) <= 1
        dic = {}
        for (i, k) in enumerate(net.obj_factors):
            dic['fac_' + str(i + 1)] = k
        pm['user_defined_params']['obj_factors'] = dic
    return pm","for (i, k) in enumerate(net.obj_factors):
    dic['fac_' + str(i + 1)] = k",XXX,no_found,0,,,
football,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/football/gfootball/env/football_env.py,https://github.com/google-research/football/tree/master/gfootball/env/football_env.py,FootballEnv,_convert_observations$92,"def _convert_observations(self, original, player, left_player_position, right_player_position):
    """"""Converts generic observations returned by the environment to
       the player specific observations.

    Args:
      original: original observations from the environment.
      player: player for which to generate observations.
      left_player_position: index into observation corresponding to the left
          player.
      right_player_position: index into observation corresponding to the right
          player.
    """"""
    observations = []
    for is_left in [True, False]:
        adopted = original if is_left or player.can_play_right() else observation_rotation.flip_observation(original, self._config)
        prefix = 'left' if is_left or not player.can_play_right() else 'right'
        position = left_player_position if is_left else right_player_position
        for x in range(player.num_controlled_left_players() if is_left else player.num_controlled_right_players()):
            o = {}
            for v in constants.EXPOSED_OBSERVATIONS:
                o[v] = copy.deepcopy(adopted[v])
            assert len(adopted[prefix + '_agent_controlled_player']) == len(adopted[prefix + '_agent_sticky_actions'])
            o['designated'] = adopted[prefix + '_team_designated_player']
            if position + x >= len(adopted[prefix + '_agent_controlled_player']):
                o['active'] = -1
                o['sticky_actions'] = []
            else:
                o['active'] = adopted[prefix + '_agent_controlled_player'][position + x]
                o['sticky_actions'] = np.array(copy.deepcopy(adopted[prefix + '_agent_sticky_actions'][position + x]))
            if is_left and 'frame' in original:
                o['frame'] = original['frame']
            observations.append(o)
    return observations","for v in constants.EXPOSED_OBSERVATIONS:
    o[v] = copy.deepcopy(adopted[v])",XXX,no_found,,,,
ViLT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ViLT/vilt/utils/write_vqa.py,https://github.com/dandelin/ViLT/tree/master/vilt/utils/write_vqa.py,,make_arrow$52,"def make_arrow(root, dataset_root):
    with open(f'{root}/v2_OpenEnded_mscoco_train2014_questions.json', 'r') as fp:
        questions_train2014 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_val2014_questions.json', 'r') as fp:
        questions_val2014 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_test2015_questions.json', 'r') as fp:
        questions_test2015 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_test-dev2015_questions.json', 'r') as fp:
        questions_test_dev2015 = json.load(fp)['questions']
    with open(f'{root}/v2_mscoco_train2014_annotations.json', 'r') as fp:
        annotations_train2014 = json.load(fp)['annotations']
    with open(f'{root}/v2_mscoco_val2014_annotations.json', 'r') as fp:
        annotations_val2014 = json.load(fp)['annotations']
    annotations = dict()
    for (split, questions) in zip(['train', 'val', 'test', 'test-dev'], [questions_train2014, questions_val2014, questions_test2015, questions_test_dev2015]):
        _annot = defaultdict(dict)
        for q in tqdm(questions):
            _annot[q['image_id']][q['question_id']] = [q['question']]
        annotations[split] = _annot
    all_major_answers = list()
    for (split, annots) in zip(['train', 'val'], [annotations_train2014, annotations_val2014]):
        _annot = annotations[split]
        for q in tqdm(annots):
            all_major_answers.append(q['multiple_choice_answer'])
    all_major_answers = [normalize_word(word) for word in tqdm(all_major_answers)]
    counter = {k: v for (k, v) in Counter(all_major_answers).items() if v >= 9}
    ans2label = {k: i for (i, k) in enumerate(counter.keys())}
    label2ans = list(counter.keys())
    for (split, annots) in zip(['train', 'val'], [annotations_train2014, annotations_val2014]):
        _annot = annotations[split]
        for q in tqdm(annots):
            answers = q['answers']
            answer_count = {}
            for answer in answers:
                answer_ = answer['answer']
                answer_count[answer_] = answer_count.get(answer_, 0) + 1
            labels = []
            scores = []
            for answer in answer_count:
                if answer not in ans2label:
                    continue
                labels.append(ans2label[answer])
                score = get_score(answer_count[answer])
                scores.append(score)
            _annot[q['image_id']][q['question_id']].append({'labels': labels, 'scores': scores})
    for split in ['train', 'val']:
        filtered_annot = dict()
        for (ik, iv) in annotations[split].items():
            new_q = dict()
            for (qk, qv) in iv.items():
                if len(qv[1]['labels']) != 0:
                    new_q[qk] = qv
            if len(new_q) != 0:
                filtered_annot[ik] = new_q
        annotations[split] = filtered_annot
    for split in ['train', 'val', 'test', 'test-dev']:
        annot = annotations[split]
        split_name = {'train': 'train2014', 'val': 'val2014', 'test': 'test2015', 'test-dev': 'test2015'}[split]
        paths = list(glob(f'{root}/{split_name}/*.jpg'))
        random.shuffle(paths)
        annot_paths = [path for path in paths if int(path.split('/')[-1].split('_')[-1][:-4]) in annot]
        if len(paths) == len(annot_paths):
            print('all images have caption annotations')
        else:
            print('not all images have caption annotations')
        print(len(paths), len(annot_paths), len(annot))
        bs = [path2rest(path, split, annotations, label2ans) for path in tqdm(annot_paths)]
        dataframe = pd.DataFrame(bs, columns=['image', 'questions', 'answers', 'answer_labels', 'answer_scores', 'image_id', 'question_id', 'split'])
        table = pa.Table.from_pandas(dataframe)
        os.makedirs(dataset_root, exist_ok=True)
        with pa.OSFile(f'{dataset_root}/vqav2_{split}.arrow', 'wb') as sink:
            with pa.RecordBatchFileWriter(sink, table.schema) as writer:
                writer.write_table(table)
    table = pa.ipc.RecordBatchFileReader(pa.memory_map(f'{dataset_root}/vqav2_val.arrow', 'r')).read_all()
    pdtable = table.to_pandas()
    df1 = pdtable[:-1000]
    df2 = pdtable[-1000:]
    df1 = pa.Table.from_pandas(df1)
    df2 = pa.Table.from_pandas(df2)
    with pa.OSFile(f'{dataset_root}/vqav2_trainable_val.arrow', 'wb') as sink:
        with pa.RecordBatchFileWriter(sink, df1.schema) as writer:
            writer.write_table(df1)
    with pa.OSFile(f'{dataset_root}/vqav2_rest_val.arrow', 'wb') as sink:
        with pa.RecordBatchFileWriter(sink, df2.schema) as writer:
            writer.write_table(df2)","for split in ['train', 'val']:
    filtered_annot = dict()
    for (ik, iv) in annotations[split].items():
        new_q = dict()
        for (qk, qv) in iv.items():
            if len(qv[1]['labels']) != 0:
                new_q[qk] = qv
        if len(new_q) != 0:
            filtered_annot[ik] = new_q
    annotations[split] = filtered_annot",XXX,no_found,0,,,
ViLT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ViLT/vilt/utils/write_vqa.py,https://github.com/dandelin/ViLT/tree/master/vilt/utils/write_vqa.py,,make_arrow$52,"def make_arrow(root, dataset_root):
    with open(f'{root}/v2_OpenEnded_mscoco_train2014_questions.json', 'r') as fp:
        questions_train2014 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_val2014_questions.json', 'r') as fp:
        questions_val2014 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_test2015_questions.json', 'r') as fp:
        questions_test2015 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_test-dev2015_questions.json', 'r') as fp:
        questions_test_dev2015 = json.load(fp)['questions']
    with open(f'{root}/v2_mscoco_train2014_annotations.json', 'r') as fp:
        annotations_train2014 = json.load(fp)['annotations']
    with open(f'{root}/v2_mscoco_val2014_annotations.json', 'r') as fp:
        annotations_val2014 = json.load(fp)['annotations']
    annotations = dict()
    for (split, questions) in zip(['train', 'val', 'test', 'test-dev'], [questions_train2014, questions_val2014, questions_test2015, questions_test_dev2015]):
        _annot = defaultdict(dict)
        for q in tqdm(questions):
            _annot[q['image_id']][q['question_id']] = [q['question']]
        annotations[split] = _annot
    all_major_answers = list()
    for (split, annots) in zip(['train', 'val'], [annotations_train2014, annotations_val2014]):
        _annot = annotations[split]
        for q in tqdm(annots):
            all_major_answers.append(q['multiple_choice_answer'])
    all_major_answers = [normalize_word(word) for word in tqdm(all_major_answers)]
    counter = {k: v for (k, v) in Counter(all_major_answers).items() if v >= 9}
    ans2label = {k: i for (i, k) in enumerate(counter.keys())}
    label2ans = list(counter.keys())
    for (split, annots) in zip(['train', 'val'], [annotations_train2014, annotations_val2014]):
        _annot = annotations[split]
        for q in tqdm(annots):
            answers = q['answers']
            answer_count = {}
            for answer in answers:
                answer_ = answer['answer']
                answer_count[answer_] = answer_count.get(answer_, 0) + 1
            labels = []
            scores = []
            for answer in answer_count:
                if answer not in ans2label:
                    continue
                labels.append(ans2label[answer])
                score = get_score(answer_count[answer])
                scores.append(score)
            _annot[q['image_id']][q['question_id']].append({'labels': labels, 'scores': scores})
    for split in ['train', 'val']:
        filtered_annot = dict()
        for (ik, iv) in annotations[split].items():
            new_q = dict()
            for (qk, qv) in iv.items():
                if len(qv[1]['labels']) != 0:
                    new_q[qk] = qv
            if len(new_q) != 0:
                filtered_annot[ik] = new_q
        annotations[split] = filtered_annot
    for split in ['train', 'val', 'test', 'test-dev']:
        annot = annotations[split]
        split_name = {'train': 'train2014', 'val': 'val2014', 'test': 'test2015', 'test-dev': 'test2015'}[split]
        paths = list(glob(f'{root}/{split_name}/*.jpg'))
        random.shuffle(paths)
        annot_paths = [path for path in paths if int(path.split('/')[-1].split('_')[-1][:-4]) in annot]
        if len(paths) == len(annot_paths):
            print('all images have caption annotations')
        else:
            print('not all images have caption annotations')
        print(len(paths), len(annot_paths), len(annot))
        bs = [path2rest(path, split, annotations, label2ans) for path in tqdm(annot_paths)]
        dataframe = pd.DataFrame(bs, columns=['image', 'questions', 'answers', 'answer_labels', 'answer_scores', 'image_id', 'question_id', 'split'])
        table = pa.Table.from_pandas(dataframe)
        os.makedirs(dataset_root, exist_ok=True)
        with pa.OSFile(f'{dataset_root}/vqav2_{split}.arrow', 'wb') as sink:
            with pa.RecordBatchFileWriter(sink, table.schema) as writer:
                writer.write_table(table)
    table = pa.ipc.RecordBatchFileReader(pa.memory_map(f'{dataset_root}/vqav2_val.arrow', 'r')).read_all()
    pdtable = table.to_pandas()
    df1 = pdtable[:-1000]
    df2 = pdtable[-1000:]
    df1 = pa.Table.from_pandas(df1)
    df2 = pa.Table.from_pandas(df2)
    with pa.OSFile(f'{dataset_root}/vqav2_trainable_val.arrow', 'wb') as sink:
        with pa.RecordBatchFileWriter(sink, df1.schema) as writer:
            writer.write_table(df1)
    with pa.OSFile(f'{dataset_root}/vqav2_rest_val.arrow', 'wb') as sink:
        with pa.RecordBatchFileWriter(sink, df2.schema) as writer:
            writer.write_table(df2)","for (ik, iv) in annotations[split].items():
    new_q = dict()
    for (qk, qv) in iv.items():
        if len(qv[1]['labels']) != 0:
            new_q[qk] = qv
    if len(new_q) != 0:
        filtered_annot[ik] = new_q",XXX,no_found,0,,,
ViLT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ViLT/vilt/utils/write_vqa.py,https://github.com/dandelin/ViLT/tree/master/vilt/utils/write_vqa.py,,make_arrow$52,"def make_arrow(root, dataset_root):
    with open(f'{root}/v2_OpenEnded_mscoco_train2014_questions.json', 'r') as fp:
        questions_train2014 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_val2014_questions.json', 'r') as fp:
        questions_val2014 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_test2015_questions.json', 'r') as fp:
        questions_test2015 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_test-dev2015_questions.json', 'r') as fp:
        questions_test_dev2015 = json.load(fp)['questions']
    with open(f'{root}/v2_mscoco_train2014_annotations.json', 'r') as fp:
        annotations_train2014 = json.load(fp)['annotations']
    with open(f'{root}/v2_mscoco_val2014_annotations.json', 'r') as fp:
        annotations_val2014 = json.load(fp)['annotations']
    annotations = dict()
    for (split, questions) in zip(['train', 'val', 'test', 'test-dev'], [questions_train2014, questions_val2014, questions_test2015, questions_test_dev2015]):
        _annot = defaultdict(dict)
        for q in tqdm(questions):
            _annot[q['image_id']][q['question_id']] = [q['question']]
        annotations[split] = _annot
    all_major_answers = list()
    for (split, annots) in zip(['train', 'val'], [annotations_train2014, annotations_val2014]):
        _annot = annotations[split]
        for q in tqdm(annots):
            all_major_answers.append(q['multiple_choice_answer'])
    all_major_answers = [normalize_word(word) for word in tqdm(all_major_answers)]
    counter = {k: v for (k, v) in Counter(all_major_answers).items() if v >= 9}
    ans2label = {k: i for (i, k) in enumerate(counter.keys())}
    label2ans = list(counter.keys())
    for (split, annots) in zip(['train', 'val'], [annotations_train2014, annotations_val2014]):
        _annot = annotations[split]
        for q in tqdm(annots):
            answers = q['answers']
            answer_count = {}
            for answer in answers:
                answer_ = answer['answer']
                answer_count[answer_] = answer_count.get(answer_, 0) + 1
            labels = []
            scores = []
            for answer in answer_count:
                if answer not in ans2label:
                    continue
                labels.append(ans2label[answer])
                score = get_score(answer_count[answer])
                scores.append(score)
            _annot[q['image_id']][q['question_id']].append({'labels': labels, 'scores': scores})
    for split in ['train', 'val']:
        filtered_annot = dict()
        for (ik, iv) in annotations[split].items():
            new_q = dict()
            for (qk, qv) in iv.items():
                if len(qv[1]['labels']) != 0:
                    new_q[qk] = qv
            if len(new_q) != 0:
                filtered_annot[ik] = new_q
        annotations[split] = filtered_annot
    for split in ['train', 'val', 'test', 'test-dev']:
        annot = annotations[split]
        split_name = {'train': 'train2014', 'val': 'val2014', 'test': 'test2015', 'test-dev': 'test2015'}[split]
        paths = list(glob(f'{root}/{split_name}/*.jpg'))
        random.shuffle(paths)
        annot_paths = [path for path in paths if int(path.split('/')[-1].split('_')[-1][:-4]) in annot]
        if len(paths) == len(annot_paths):
            print('all images have caption annotations')
        else:
            print('not all images have caption annotations')
        print(len(paths), len(annot_paths), len(annot))
        bs = [path2rest(path, split, annotations, label2ans) for path in tqdm(annot_paths)]
        dataframe = pd.DataFrame(bs, columns=['image', 'questions', 'answers', 'answer_labels', 'answer_scores', 'image_id', 'question_id', 'split'])
        table = pa.Table.from_pandas(dataframe)
        os.makedirs(dataset_root, exist_ok=True)
        with pa.OSFile(f'{dataset_root}/vqav2_{split}.arrow', 'wb') as sink:
            with pa.RecordBatchFileWriter(sink, table.schema) as writer:
                writer.write_table(table)
    table = pa.ipc.RecordBatchFileReader(pa.memory_map(f'{dataset_root}/vqav2_val.arrow', 'r')).read_all()
    pdtable = table.to_pandas()
    df1 = pdtable[:-1000]
    df2 = pdtable[-1000:]
    df1 = pa.Table.from_pandas(df1)
    df2 = pa.Table.from_pandas(df2)
    with pa.OSFile(f'{dataset_root}/vqav2_trainable_val.arrow', 'wb') as sink:
        with pa.RecordBatchFileWriter(sink, df1.schema) as writer:
            writer.write_table(df1)
    with pa.OSFile(f'{dataset_root}/vqav2_rest_val.arrow', 'wb') as sink:
        with pa.RecordBatchFileWriter(sink, df2.schema) as writer:
            writer.write_table(df2)","for answer in answers:
    answer_ = answer['answer']
    answer_count[answer_] = answer_count.get(answer_, 0) + 1",XXX,no_found,0,,,
ViLT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ViLT/vilt/utils/write_vqa.py,https://github.com/dandelin/ViLT/tree/master/vilt/utils/write_vqa.py,,make_arrow$52,"def make_arrow(root, dataset_root):
    with open(f'{root}/v2_OpenEnded_mscoco_train2014_questions.json', 'r') as fp:
        questions_train2014 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_val2014_questions.json', 'r') as fp:
        questions_val2014 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_test2015_questions.json', 'r') as fp:
        questions_test2015 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_test-dev2015_questions.json', 'r') as fp:
        questions_test_dev2015 = json.load(fp)['questions']
    with open(f'{root}/v2_mscoco_train2014_annotations.json', 'r') as fp:
        annotations_train2014 = json.load(fp)['annotations']
    with open(f'{root}/v2_mscoco_val2014_annotations.json', 'r') as fp:
        annotations_val2014 = json.load(fp)['annotations']
    annotations = dict()
    for (split, questions) in zip(['train', 'val', 'test', 'test-dev'], [questions_train2014, questions_val2014, questions_test2015, questions_test_dev2015]):
        _annot = defaultdict(dict)
        for q in tqdm(questions):
            _annot[q['image_id']][q['question_id']] = [q['question']]
        annotations[split] = _annot
    all_major_answers = list()
    for (split, annots) in zip(['train', 'val'], [annotations_train2014, annotations_val2014]):
        _annot = annotations[split]
        for q in tqdm(annots):
            all_major_answers.append(q['multiple_choice_answer'])
    all_major_answers = [normalize_word(word) for word in tqdm(all_major_answers)]
    counter = {k: v for (k, v) in Counter(all_major_answers).items() if v >= 9}
    ans2label = {k: i for (i, k) in enumerate(counter.keys())}
    label2ans = list(counter.keys())
    for (split, annots) in zip(['train', 'val'], [annotations_train2014, annotations_val2014]):
        _annot = annotations[split]
        for q in tqdm(annots):
            answers = q['answers']
            answer_count = {}
            for answer in answers:
                answer_ = answer['answer']
                answer_count[answer_] = answer_count.get(answer_, 0) + 1
            labels = []
            scores = []
            for answer in answer_count:
                if answer not in ans2label:
                    continue
                labels.append(ans2label[answer])
                score = get_score(answer_count[answer])
                scores.append(score)
            _annot[q['image_id']][q['question_id']].append({'labels': labels, 'scores': scores})
    for split in ['train', 'val']:
        filtered_annot = dict()
        for (ik, iv) in annotations[split].items():
            new_q = dict()
            for (qk, qv) in iv.items():
                if len(qv[1]['labels']) != 0:
                    new_q[qk] = qv
            if len(new_q) != 0:
                filtered_annot[ik] = new_q
        annotations[split] = filtered_annot
    for split in ['train', 'val', 'test', 'test-dev']:
        annot = annotations[split]
        split_name = {'train': 'train2014', 'val': 'val2014', 'test': 'test2015', 'test-dev': 'test2015'}[split]
        paths = list(glob(f'{root}/{split_name}/*.jpg'))
        random.shuffle(paths)
        annot_paths = [path for path in paths if int(path.split('/')[-1].split('_')[-1][:-4]) in annot]
        if len(paths) == len(annot_paths):
            print('all images have caption annotations')
        else:
            print('not all images have caption annotations')
        print(len(paths), len(annot_paths), len(annot))
        bs = [path2rest(path, split, annotations, label2ans) for path in tqdm(annot_paths)]
        dataframe = pd.DataFrame(bs, columns=['image', 'questions', 'answers', 'answer_labels', 'answer_scores', 'image_id', 'question_id', 'split'])
        table = pa.Table.from_pandas(dataframe)
        os.makedirs(dataset_root, exist_ok=True)
        with pa.OSFile(f'{dataset_root}/vqav2_{split}.arrow', 'wb') as sink:
            with pa.RecordBatchFileWriter(sink, table.schema) as writer:
                writer.write_table(table)
    table = pa.ipc.RecordBatchFileReader(pa.memory_map(f'{dataset_root}/vqav2_val.arrow', 'r')).read_all()
    pdtable = table.to_pandas()
    df1 = pdtable[:-1000]
    df2 = pdtable[-1000:]
    df1 = pa.Table.from_pandas(df1)
    df2 = pa.Table.from_pandas(df2)
    with pa.OSFile(f'{dataset_root}/vqav2_trainable_val.arrow', 'wb') as sink:
        with pa.RecordBatchFileWriter(sink, df1.schema) as writer:
            writer.write_table(df1)
    with pa.OSFile(f'{dataset_root}/vqav2_rest_val.arrow', 'wb') as sink:
        with pa.RecordBatchFileWriter(sink, df2.schema) as writer:
            writer.write_table(df2)","for (qk, qv) in iv.items():
    if len(qv[1]['labels']) != 0:
        new_q[qk] = qv",XXX,no_found,0,,,
fbchat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fbchat/fbchat/_threads/_abc.py,https://github.com/fbchat-dev/fbchat/tree/master/fbchat/_threads/_abc.py,ThreadABC,_parse_customization_info$746,"def _parse_customization_info(data: Any) -> MutableMapping[str, Any]:
    if not data or not data.get('customization_info'):
        return {'emoji': None, 'color': DEFAULT_COLOR}
    info = data['customization_info']
    rtn = {'emoji': info.get('emoji'), 'color': ThreadABC._parse_color(info.get('outgoing_bubble_color'))}
    if data.get('thread_type') == 'GROUP' or data.get('is_group_thread') or data.get('thread_key', {}).get('thread_fbid'):
        rtn['nicknames'] = {}
        for k in info.get('participant_customizations', []):
            rtn['nicknames'][k['participant_id']] = k.get('nickname')
    elif info.get('participant_customizations'):
        user_id = data.get('thread_key', {}).get('other_user_id') or data.get('id')
        pc = info['participant_customizations']
        if len(pc) > 0:
            if pc[0].get('participant_id') == user_id:
                rtn['nickname'] = pc[0].get('nickname')
            else:
                rtn['own_nickname'] = pc[0].get('nickname')
        if len(pc) > 1:
            if pc[1].get('participant_id') == user_id:
                rtn['nickname'] = pc[1].get('nickname')
            else:
                rtn['own_nickname'] = pc[1].get('nickname')
    return rtn","for k in info.get('participant_customizations', []):
    rtn['nicknames'][k['participant_id']] = k.get('nickname')",XXX,no_found,0,,,
fuel,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fuel/fuel/datasets/hdf5.py,https://github.com/mila-iqia/fuel/tree/master/fuel/datasets/hdf5.py,H5PYDataset,get_axis_labels$390,"def get_axis_labels(h5file):
    """"""Returns axis labels for all sources in an HDF5 dataset.

        Parameters
        ----------
        h5file : HDF5 file handle
            An HDF5 dataset respecting the H5PYDataset interface.

        Returns
        -------
        axis_labels : dict
            Maps source names to a tuple of str representing the axis
            labels.

        """"""
    axis_labels = {}
    vlen_sources = H5PYDataset.get_vlen_sources(h5file)
    for source_name in H5PYDataset.get_all_sources(h5file):
        if source_name in vlen_sources:
            axis_labels[source_name] = (h5file[source_name].dims[0].label,) + tuple((label.decode('utf8') for label in h5file[source_name].dims[0]['shape_labels']))
        else:
            axis_labels[source_name] = tuple((dim.label for dim in h5file[source_name].dims))
    return axis_labels","for source_name in H5PYDataset.get_all_sources(h5file):
    if source_name in vlen_sources:
        axis_labels[source_name] = (h5file[source_name].dims[0].label,) + tuple((label.decode('utf8') for label in h5file[source_name].dims[0]['shape_labels']))
    else:
        axis_labels[source_name] = tuple((dim.label for dim in h5file[source_name].dims))",XXX,no_found,,,,
synapse,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/synapse/synapse/handlers/auth.py,https://github.com/matrix-org/synapse/tree/master/synapse/handlers/auth.py,,load_single_legacy_password_auth_provider$1855,"def load_single_legacy_password_auth_provider(module: Type, config: JsonDict, api: 'ModuleApi') -> None:
    try:
        provider = module(config=config, account_handler=api)
    except Exception as e:
        logger.error('Error while initializing %r: %s', module, e)
        raise

    def async_wrapper(f: Optional[Callable]) -> Optional[Callable[..., Awaitable]]:
        if f is None:
            return None
        if f.__name__ == 'check_password':

            async def wrapped_check_password(username: str, login_type: str, login_dict: JsonDict) -> Optional[Tuple[str, Optional[Callable]]]:
                assert f is not None
                matrix_user_id = api.get_qualified_user_id(username)
                password = login_dict['password']
                is_valid = await f(matrix_user_id, password)
                if is_valid:
                    return (matrix_user_id, None)
                return None
            return wrapped_check_password
        if f.__name__ == 'check_auth':

            async def wrapped_check_auth(username: str, login_type: str, login_dict: JsonDict) -> Optional[Tuple[str, Optional[Callable]]]:
                assert f is not None
                result = await f(username, login_type, login_dict)
                if isinstance(result, str):
                    return (result, None)
                return result
            return wrapped_check_auth
        if f.__name__ == 'check_3pid_auth':

            async def wrapped_check_3pid_auth(medium: str, address: str, password: str) -> Optional[Tuple[str, Optional[Callable]]]:
                assert f is not None
                result = await f(medium, address, password)
                if isinstance(result, str):
                    return (result, None)
                return result
            return wrapped_check_3pid_auth

        def run(*args: Tuple, **kwargs: Dict) -> Awaitable:
            assert f is not None
            return maybe_awaitable(f(*args, **kwargs))
        return run
    check_3pid_auth_hook: Optional[CHECK_3PID_AUTH_CALLBACK] = async_wrapper(getattr(provider, 'check_3pid_auth', None))
    on_logged_out_hook: Optional[ON_LOGGED_OUT_CALLBACK] = async_wrapper(getattr(provider, 'on_logged_out', None))
    supported_login_types = {}
    g = getattr(provider, 'get_supported_login_types', None)
    if g is not None:
        supported_login_types.update(g())
    auth_checkers = {}
    check_auth = async_wrapper(getattr(provider, 'check_auth', None))
    if check_auth is not None:
        for (login_type, fields) in supported_login_types.items():
            auth_checkers[login_type, tuple(fields)] = check_auth
    check_password = async_wrapper(getattr(provider, 'check_password', None))
    if check_password is not None:
        auth_checkers[LoginType.PASSWORD, ('password',)] = check_password
    api.register_password_auth_provider_callbacks(check_3pid_auth=check_3pid_auth_hook, on_logged_out=on_logged_out_hook, auth_checkers=auth_checkers)","for (login_type, fields) in supported_login_types.items():
    auth_checkers[login_type, tuple(fields)] = check_auth",XXX,no_found,0,,,
crossbar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/crossbar/crossbar/router/longpoll.py,https://github.com/crossbario/crossbar/tree/master/crossbar/router/longpoll.py,WampLongPollResource,__init__$536,"def __init__(self, factory, serializers=None, timeout=10, killAfter=30, queueLimitBytes=128 * 1024, queueLimitMessages=100, debug_transport_id=None, reactor=None):
    """"""
        Create new HTTP WAMP Web resource.

        :param factory: A (router) session factory.
        :type factory: Instance of :class:`autobahn.twisted.wamp.RouterSessionFactory`.
        :param serializers: List of WAMP serializers.
        :type serializers: list of obj (which implement :class:`autobahn.wamp.interfaces.ISerializer`)
        :param timeout: XHR polling timeout in seconds.
        :type timeout: int
        :param killAfter: Kill WAMP session after inactivity in seconds.
        :type killAfter: int
        :param queueLimitBytes: Kill WAMP session after accumulation of this many bytes in send queue (XHR poll).
        :type queueLimitBytes: int
        :param queueLimitMessages: Kill WAMP session after accumulation of this many message in send queue (XHR poll).
        :type queueLimitMessages: int
        :param debug: Enable debug logging.
        :type debug: bool
        :param debug_transport_id: If given, use this fixed transport ID.
        :type debug_transport_id: str
        :param reactor: The Twisted reactor to run under.
        :type reactor: obj
        """"""
    Resource.__init__(self)
    self._factory = factory
    if reactor is None:
        from twisted.internet import reactor
    self.reactor = reactor
    self._debug_transport_id = debug_transport_id
    self._timeout = timeout
    self._killAfter = killAfter
    self._queueLimitBytes = queueLimitBytes
    self._queueLimitMessages = queueLimitMessages
    if serializers is None:
        serializers = []
        try:
            from autobahn.wamp.serializer import CBORSerializer
            serializers.append(CBORSerializer(batched=True))
            serializers.append(CBORSerializer())
        except ImportError:
            pass
        try:
            from autobahn.wamp.serializer import MsgPackSerializer
            serializers.append(MsgPackSerializer(batched=True))
            serializers.append(MsgPackSerializer())
        except ImportError:
            pass
        try:
            from autobahn.wamp.serializer import UBJSONSerializer
            serializers.append(UBJSONSerializer(batched=True))
            serializers.append(UBJSONSerializer())
        except ImportError:
            pass
        try:
            from autobahn.wamp.serializer import JsonSerializer
            serializers.append(JsonSerializer(batched=True))
            serializers.append(JsonSerializer())
        except ImportError:
            pass
        if not serializers:
            raise Exception('could not import any WAMP serializers')
    self._serializers = {}
    for ser in serializers:
        self._serializers[ser.SERIALIZER_ID] = ser
    self._transports = {}
    self.putChild(b'open', WampLongPollResourceOpen(self))
    self.log.debug('WampLongPollResource initialized')","for ser in serializers:
    self._serializers[ser.SERIALIZER_ID] = ser",XXX,no_found,0,,,
mmcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmcv/mmcv/runner/fp16_utils.py,https://github.com/open-mmlab/mmcv/tree/master/mmcv/runner/fp16_utils.py,,auto_fp16_wrapper$88,"def auto_fp16_wrapper(old_func):

    @functools.wraps(old_func)
    def new_func(*args, **kwargs):
        if not isinstance(args[0], torch.nn.Module):
            raise TypeError('@auto_fp16 can only be used to decorate the method of nn.Module')
        if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
            return old_func(*args, **kwargs)
        args_info = getfullargspec(old_func)
        args_to_cast = args_info.args if apply_to is None else apply_to
        new_args = []
        if args:
            arg_names = args_info.args[:len(args)]
            for (i, arg_name) in enumerate(arg_names):
                if arg_name in args_to_cast:
                    new_args.append(cast_tensor_type(args[i], torch.float, torch.half))
                else:
                    new_args.append(args[i])
        new_kwargs = {}
        if kwargs:
            for (arg_name, arg_value) in kwargs.items():
                if arg_name in args_to_cast:
                    new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
                else:
                    new_kwargs[arg_name] = arg_value
        if TORCH_VERSION != 'parrots' and digit_version(TORCH_VERSION) >= digit_version('1.6.0'):
            with autocast(enabled=True):
                output = old_func(*new_args, **new_kwargs)
        else:
            output = old_func(*new_args, **new_kwargs)
        if out_fp32:
            output = cast_tensor_type(output, torch.half, torch.float)
        return output
    return new_func","for (arg_name, arg_value) in kwargs.items():
    if arg_name in args_to_cast:
        new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
    else:
        new_kwargs[arg_name] = arg_value",XXX,no_found,0,,,
ReAgent,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ReAgent/reagent/preprocessing/sparse_to_dense.py,https://github.com/facebookresearch/ReAgent/tree/master/reagent/preprocessing/sparse_to_dense.py,StringKeySparseToDenseProcessor,process$35,"def process(self, sparse_data: List[Dict[str, float]]) -> Tuple[torch.Tensor, torch.Tensor]:
    sparse_data_int = []
    for sd in sparse_data:
        sd_int = {}
        for (k, v) in sd.items():
            sd_int[int(k)] = v
        sparse_data_int.append(sd_int)
    return self._sparse_to_dense(sparse_data_int)","for (k, v) in sd.items():
    sd_int[int(k)] = v",XXX,no_found,,,,
unilm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unilm/xtune/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,https://github.com/microsoft/unilm/tree/master/xtune/src/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py,,convert_xlm_checkpoint_to_pytorch$32,"def convert_xlm_checkpoint_to_pytorch(xlm_checkpoint_path, pytorch_dump_folder_path):
    chkpt = torch.load(xlm_checkpoint_path, map_location='cpu')
    state_dict = chkpt['model']
    two_levels_state_dict = {}
    for (k, v) in state_dict.items():
        if 'pred_layer' in k:
            two_levels_state_dict[k] = v
        else:
            two_levels_state_dict['transformer.' + k] = v
    config = chkpt['params']
    config = dict(((n, v) for (n, v) in config.items() if not isinstance(v, (torch.FloatTensor, numpy.ndarray))))
    vocab = chkpt['dico_word2id']
    vocab = dict(((s + '</w>' if s.find('@@') == -1 and i > 13 else s.replace('@@', ''), i) for (s, i) in vocab.items()))
    pytorch_weights_dump_path = pytorch_dump_folder_path + '/' + WEIGHTS_NAME
    pytorch_config_dump_path = pytorch_dump_folder_path + '/' + CONFIG_NAME
    pytorch_vocab_dump_path = pytorch_dump_folder_path + '/' + VOCAB_FILES_NAMES['vocab_file']
    print('Save PyTorch model to {}'.format(pytorch_weights_dump_path))
    torch.save(two_levels_state_dict, pytorch_weights_dump_path)
    print('Save configuration file to {}'.format(pytorch_config_dump_path))
    with open(pytorch_config_dump_path, 'w', encoding='utf-8') as f:
        f.write(json.dumps(config, indent=2) + '\n')
    print('Save vocab file to {}'.format(pytorch_config_dump_path))
    with open(pytorch_vocab_dump_path, 'w', encoding='utf-8') as f:
        f.write(json.dumps(vocab, indent=2) + '\n')","for (k, v) in state_dict.items():
    if 'pred_layer' in k:
        two_levels_state_dict[k] = v
    else:
        two_levels_state_dict['transformer.' + k] = v",XXX,no_found,0,,,
oppia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/exp_domain.py,https://github.com/oppia/oppia/tree/master/core/domain/exp_domain.py,Exploration,_remove_unwanted_content_ids_from_translations_and_voiceovers_from_state_v51_or_v52$3089,"def _remove_unwanted_content_ids_from_translations_and_voiceovers_from_state_v51_or_v52(cls, state_dict: state_domain.StateDict, state_schema: int) -> None:
    """"""Helper function to remove the content IDs from the translations
        and voiceovers which are deleted from the state.

        Args:
            state_dict: state_domain.StateDict. The state dictionary.
            state_schema: int. The state schema from which we are using
                this functionality.
        """"""
    interaction = state_dict['interaction']
    content_id_list = [state_dict['content']['content_id']]
    for answer_group in interaction['answer_groups']:
        content_id_list.append(answer_group['outcome']['feedback']['content_id'])
        for rule_spec in answer_group['rule_specs']:
            for (param_name, value) in rule_spec['inputs'].items():
                interaction_id = interaction['id']
                param_type = interaction_registry.Registry.get_interaction_by_id(interaction_id).get_rule_param_type(rule_spec['rule_type'], param_name)
                if issubclass(param_type, objects.BaseTranslatableObject):
                    assert isinstance(value, dict)
                    content_id = value['contentId']
                    assert isinstance(content_id, str)
                    content_id_list.append(content_id)
    default_outcome = interaction['default_outcome']
    if default_outcome:
        content_id_list.append(default_outcome['feedback']['content_id'])
    for hint in interaction['hints']:
        content_id_list.append(hint['hint_content']['content_id'])
    interaction_solution = interaction['solution']
    if interaction_solution:
        content_id_list.append(interaction_solution['explanation']['content_id'])
    if interaction['id'] is not None:
        customisation_args = state_domain.InteractionInstance.convert_customization_args_dict_to_customization_args(interaction['id'], interaction['customization_args'], state_schema_version=state_schema)
        for ca_name in customisation_args:
            content_id_list.extend(customisation_args[ca_name].get_content_ids())
    translations_mapping = state_dict['written_translations']['translations_mapping']
    new_translations_mapping = {content_id: translation_item for (content_id, translation_item) in translations_mapping.items() if content_id in content_id_list}
    state_dict['written_translations']['translations_mapping'] = new_translations_mapping
    voiceovers_mapping = state_dict['recorded_voiceovers']['voiceovers_mapping']
    new_voiceovers_mapping = {}
    for (content_id, voiceover_item) in voiceovers_mapping.items():
        if content_id in content_id_list:
            new_voiceovers_mapping[content_id] = voiceover_item
    state_dict['recorded_voiceovers']['voiceovers_mapping'] = new_voiceovers_mapping","for (content_id, voiceover_item) in voiceovers_mapping.items():
    if content_id in content_id_list:
        new_voiceovers_mapping[content_id] = voiceover_item",XXX,no_found,,,,
horovod,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/horovod/horovod/spark/torch/remote.py,https://github.com/horovod/horovod/tree/master/horovod/spark/torch/remote.py,,get_metric_avgs$543,"def get_metric_avgs(metric_value_groups):
    all_metric_groups_values = []
    for metric_value_group in metric_value_groups:
        metric_avgs = {}
        for metric in metric_value_group:
            metric_avgs[metric.name] = metric.avg.item()
        all_metric_groups_values.append(metric_avgs)
    return all_metric_groups_values","for metric in metric_value_group:
    metric_avgs[metric.name] = metric.avg.item()",XXX,no_found,,,,
tensorpack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorpack/tensorpack/models/tflayer.py,https://github.com/tensorpack/tensorpack/tree/master/tensorpack/models/tflayer.py,,convert_to_tflayer_args$33,"def convert_to_tflayer_args(args_names, name_mapping):
    """"""
    After applying this decorator:
    1. data_format becomes tf.layers style
    2. nl becomes activation
    3. initializers are renamed
    4. positional args are transformed to corresponding kwargs, according to args_names
    5. kwargs are mapped to tf.layers names if needed, by name_mapping
    """"""

    def decorator(func):

        @functools.wraps(func)
        def decorated_func(inputs, *args, **kwargs):
            kwargs = map_common_tfargs(kwargs)
            posarg_dic = {}
            assert len(args) <= len(args_names), 'Please use kwargs instead of positional args to call this model, except for the following arguments: {}'.format(', '.join(args_names))
            for (pos_arg, name) in zip(args, args_names):
                posarg_dic[name] = pos_arg
            ret = {}
            for (name, arg) in six.iteritems(kwargs):
                newname = name_mapping.get(name, None)
                if newname is not None:
                    assert newname not in kwargs, 'Argument {} and {} conflicts!'.format(name, newname)
                else:
                    newname = name
                ret[newname] = arg
            ret.update(posarg_dic)
            return func(inputs, **ret)
        return decorated_func
    return decorator","for (pos_arg, name) in zip(args, args_names):
    posarg_dic[name] = pos_arg",XXX,no_found,0,,,
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/dataset/conll05.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/dataset/conll05.py,,load_dict$65,"def load_dict(filename):
    d = dict()
    with open(filename, 'r') as f:
        for (i, line) in enumerate(f):
            d[line.strip()] = i
    return d","for (i, line) in enumerate(f):
    d[line.strip()] = i",XXX,no_found,,,,
nasbench,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nasbench/nasbench/scripts/generate_cifar10_tfrecords.py,https://github.com/google-research/nasbench/tree/master/nasbench/scripts/generate_cifar10_tfrecords.py,,_get_file_names$61,"def _get_file_names():
    """"""Returns the file names expected to exist in the input_dir.""""""
    file_names = {}
    for i in range(1, 5):
        file_names['train_%d' % i] = 'data_batch_%d' % i
    file_names['validation'] = 'data_batch_5'
    file_names['test'] = 'test_batch'
    return file_names","for i in range(1, 5):
    file_names['train_%d' % i] = 'data_batch_%d' % i",XXX,no_found,,,,
mysql-connector-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mysql-connector-python/lib/mysql/connector/connection_cext.py,https://github.com/mysql/mysql-connector-python/tree/master/lib/mysql/connector/connection_cext.py,CMySQLConnection,prepare_for_mysql$647,"def prepare_for_mysql(self, params):
    """"""Prepare parameters for statements

        This method is use by cursors to prepared parameters found in the
        list (or tuple) params.

        Returns dict.
        """"""
    if isinstance(params, (list, tuple)):
        if self.converter:
            result = [self.converter.quote(self.converter.escape(self.converter.to_mysql(value))) for value in params]
        else:
            result = self._cmysql.convert_to_mysql(*params)
    elif isinstance(params, dict):
        result = {}
        if self.converter:
            for (key, value) in params.items():
                result[key] = self.converter.quote(self.converter.escape(self.converter.to_mysql(value)))
        else:
            for (key, value) in params.items():
                result[key] = self._cmysql.convert_to_mysql(value)[0]
    else:
        raise ValueError('Could not process parameters')
    return result","for (key, value) in params.items():
    result[key] = self.converter.quote(self.converter.escape(self.converter.to_mysql(value)))",XXX,no_found,,,,
mysql-connector-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mysql-connector-python/lib/mysql/connector/connection_cext.py,https://github.com/mysql/mysql-connector-python/tree/master/lib/mysql/connector/connection_cext.py,CMySQLConnection,prepare_for_mysql$647,"def prepare_for_mysql(self, params):
    """"""Prepare parameters for statements

        This method is use by cursors to prepared parameters found in the
        list (or tuple) params.

        Returns dict.
        """"""
    if isinstance(params, (list, tuple)):
        if self.converter:
            result = [self.converter.quote(self.converter.escape(self.converter.to_mysql(value))) for value in params]
        else:
            result = self._cmysql.convert_to_mysql(*params)
    elif isinstance(params, dict):
        result = {}
        if self.converter:
            for (key, value) in params.items():
                result[key] = self.converter.quote(self.converter.escape(self.converter.to_mysql(value)))
        else:
            for (key, value) in params.items():
                result[key] = self._cmysql.convert_to_mysql(value)[0]
    else:
        raise ValueError('Could not process parameters')
    return result","for (key, value) in params.items():
    result[key] = self._cmysql.convert_to_mysql(value)[0]",XXX,no_found,0,,,
anchore-engine,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anchore-engine/anchore_engine/common/helpers.py,https://github.com/anchore/anchore-engine/tree/master/anchore_engine/common/helpers.py,,extract_java_content$284,"def extract_java_content(image_data):
    ret = {}
    if 'pkgs.java' in image_data['imagedata']['analysis_report']['package_list']:
        adata = image_data['imagedata']['analysis_report']['package_list']['pkgs.java']['base']
        for k in list(adata.keys()):
            ret[k] = safe_extract_json_value(adata[k])
    return ret","for k in list(adata.keys()):
    ret[k] = safe_extract_json_value(adata[k])",XXX,no_found,,,,
orbit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/orbit/orbit/forecaster/full_bayes.py,https://github.com/uber/orbit/tree/master/orbit/forecaster/full_bayes.py,FullBayesianForecaster,predict$89,"def predict(self, df, decompose=False, store_prediction_array=False, seed=None, **kwargs):
    if not self.is_fitted():
        raise ForecasterException('Model is not fitted yet.')
    self._set_prediction_meta(df)
    prediction_meta = self.get_prediction_meta()
    training_meta = self.get_training_meta()
    if seed is not None:
        np.random.seed(seed)
    if self._point_method is None:
        posterior_samples = self._bootstrap(num_samples=self.estimator.num_sample, posterior_samples=self._posterior_samples, n=self._n_bootstrap_draws) if self._n_bootstrap_draws > 1 else self._posterior_samples
        predicted_dict = self._model.predict(posterior_estimates=posterior_samples, df=df, training_meta=training_meta, prediction_meta=prediction_meta, include_error=True, **kwargs)
        if PredictionKeys.PREDICTION.value not in predicted_dict.keys():
            raise ForecasterException(""cannot find the key:'{}' from return of _predict()"".format(PredictionKeys.PREDICTION.value))
        if not decompose:
            predicted_dict = {k: v for (k, v) in predicted_dict.items() if k == PredictionKeys.PREDICTION.value}
        if store_prediction_array:
            self.prediction_array = predicted_dict[PredictionKeys.PREDICTION.value]
        percentiles_dict = compute_percentiles(predicted_dict, self._prediction_percentiles)
        predicted_df = pd.DataFrame(percentiles_dict)
        predicted_df = prepend_date_column(predicted_df, df, self.date_col)
        return predicted_df
    else:
        point_posteriors = self._point_posteriors.get(self._point_method)
        point_predicted_dict = self._model.predict(posterior_estimates=point_posteriors, df=df, training_meta=training_meta, prediction_meta=prediction_meta, include_error=False, **kwargs)
        for (k, v) in point_predicted_dict.items():
            point_predicted_dict[k] = np.squeeze(v, 0)
        if self._n_bootstrap_draws > 0 and len(self._prediction_percentiles) > 1:
            posterior_samples = {}
            for (k, v) in point_posteriors.items():
                posterior_samples[k] = np.repeat(v, self.n_bootstrap_draws, axis=0)
            predicted_dict = self._model.predict(posterior_estimates=posterior_samples, df=df, training_meta=training_meta, prediction_meta=prediction_meta, include_error=True, **kwargs)
            percentiles_dict = compute_percentiles(predicted_dict, self._prediction_percentiles)
            percentiles_dict.update(point_predicted_dict)
            if PredictionKeys.PREDICTION.value not in percentiles_dict.keys():
                raise ForecasterException(""cannot find the key:'{}' from return of _predict()"".format(PredictionKeys.PREDICTION.value))
            if not decompose:
                k = PredictionKeys.PREDICTION.value
                reduced_keys = [k + '_' + str(p) if p != 50 else k for p in self._prediction_percentiles]
                percentiles_dict = {k: v for (k, v) in percentiles_dict.items() if k in reduced_keys}
            predicted_df = pd.DataFrame(percentiles_dict)
        else:
            if not decompose:
                point_predicted_dict = {k: v for (k, v) in point_predicted_dict.items() if k == PredictionKeys.PREDICTION.value}
            predicted_df = pd.DataFrame(point_predicted_dict)
        predicted_df = prepend_date_column(predicted_df, df, self.date_col)
        return predicted_df","for (k, v) in point_posteriors.items():
    posterior_samples[k] = np.repeat(v, self.n_bootstrap_draws, axis=0)",XXX,no_found,0,,,
simpleui,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/simpleui/simpleui/templatetags/simpletags.py,https://github.com/newpanjing/simpleui/tree/master/simpleui/templatetags/simpletags.py,,custom_button$393,"def custom_button(context):
    admin = context.get('cl').model_admin
    data = {}
    actions = admin.get_actions(context.request)
    if actions:
        i = 0
        for name in actions:
            values = {}
            fun = actions.get(name)[0]
            for (key, v) in fun.__dict__.items():
                if key != '__len__' and key != '__wrapped__':
                    values[key] = v
            values['eid'] = i
            i += 1
            data[name] = values
    return json.dumps(data, cls=LazyEncoder)","for name in actions:
    values = {}
    fun = actions.get(name)[0]
    for (key, v) in fun.__dict__.items():
        if key != '__len__' and key != '__wrapped__':
            values[key] = v
    values['eid'] = i
    i += 1
    data[name] = values",XXX,no_found,,,,
simpleui,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/simpleui/simpleui/templatetags/simpletags.py,https://github.com/newpanjing/simpleui/tree/master/simpleui/templatetags/simpletags.py,,custom_button$393,"def custom_button(context):
    admin = context.get('cl').model_admin
    data = {}
    actions = admin.get_actions(context.request)
    if actions:
        i = 0
        for name in actions:
            values = {}
            fun = actions.get(name)[0]
            for (key, v) in fun.__dict__.items():
                if key != '__len__' and key != '__wrapped__':
                    values[key] = v
            values['eid'] = i
            i += 1
            data[name] = values
    return json.dumps(data, cls=LazyEncoder)","for (key, v) in fun.__dict__.items():
    if key != '__len__' and key != '__wrapped__':
        values[key] = v",XXX,no_found,0,,,
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/adapters/heads/base.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/adapters/heads/base.py,ModelWithFlexibleHeadsAdaptersMixin,_get_head_input$748,"def _get_head_input(outputs, cls_out, batch):
    if isinstance(outputs, ModelOutput):
        inputs = {}
        for (key, base_output) in outputs.items():
            if torch.is_tensor(base_output):
                inputs[key] = base_output[batch[0]:batch[-1] + 1]
        inputs = outputs.__class__(**inputs)
    else:
        inputs = tuple()
        for base_output in outputs:
            inputs = inputs + (base_output[batch],)
    if cls_out is not None:
        cls_input = cls_out[batch]
    else:
        cls_input = None
    return (inputs, cls_input)","for (key, base_output) in outputs.items():
    if torch.is_tensor(base_output):
        inputs[key] = base_output[batch[0]:batch[-1] + 1]",XXX,no_found,,,,
orchest,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/orchest/services/orchest-api/app/app/apis/namespace_jobs.py,https://github.com/orchest/orchest/tree/master/services/orchest-api/app/app/apis/namespace_jobs.py,UpdateDraftJobPipeline,_resolve_environment_variables$1411,"def _resolve_environment_variables(project_env_vars: Dict[str, str], old_pipeline_env_vars: Dict[str, str], new_pipeline_env_vars: Dict[str, str], old_job_env_vars: Dict[str, str]) -> Dict[str, str]:
    """"""Resolves the environment variables to be used for the job.

        When changing the pipeline for a draft job we'd like to carry
        over all work that the user has done w.r.t. setting/changing the
        environment variables of the job. Do to that, we have to
        reconstruct the changes the user has made and resolve
        ambiguities.

        This logic identifies user changes as:
        - removing env vars inherited by the project or pipeline
        - changing env vars inherited by the project or pipeline
        - adding new environment variables

        Ambiguities:
        - an env var inherited by the old pipeline which hasn't been
            changed signals that the user wanted the default value of
            the pipeline env var.  If the new pipeline has such env var,
            use the default value coming from the new pipeline, if it
            doesn't have the env var, ignore the variable, i.e. do not
            include it in the resulting set.
        - an env var inherited by the project wasn't changed, and the
            old pipeline didn't overwrite the value of that variable. If
            the new pipeline has that env var then it will overwrite the
            value.

        """"""
    old_proj_ppl_merge = {**project_env_vars, **old_pipeline_env_vars}
    removed_env_vars = set()
    changed_env_vars = dict()
    added_env_vars = dict()
    for env_var in old_proj_ppl_merge:
        if env_var not in old_job_env_vars:
            removed_env_vars.add(env_var)
        elif old_proj_ppl_merge[env_var] != old_job_env_vars[env_var]:
            changed_env_vars[env_var] = old_job_env_vars[env_var]
    for env_var in old_job_env_vars:
        if env_var not in old_proj_ppl_merge:
            added_env_vars[env_var] = old_job_env_vars[env_var]
    result = {**project_env_vars, **new_pipeline_env_vars, **changed_env_vars, **added_env_vars}
    for env_var in removed_env_vars:
        result.pop(env_var, None)
    return result","for env_var in old_proj_ppl_merge:
    if env_var not in old_job_env_vars:
        removed_env_vars.add(env_var)
    elif old_proj_ppl_merge[env_var] != old_job_env_vars[env_var]:
        changed_env_vars[env_var] = old_job_env_vars[env_var]",XXX,no_found,0,,,
orchest,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/orchest/services/orchest-api/app/app/apis/namespace_jobs.py,https://github.com/orchest/orchest/tree/master/services/orchest-api/app/app/apis/namespace_jobs.py,UpdateDraftJobPipeline,_resolve_environment_variables$1411,"def _resolve_environment_variables(project_env_vars: Dict[str, str], old_pipeline_env_vars: Dict[str, str], new_pipeline_env_vars: Dict[str, str], old_job_env_vars: Dict[str, str]) -> Dict[str, str]:
    """"""Resolves the environment variables to be used for the job.

        When changing the pipeline for a draft job we'd like to carry
        over all work that the user has done w.r.t. setting/changing the
        environment variables of the job. Do to that, we have to
        reconstruct the changes the user has made and resolve
        ambiguities.

        This logic identifies user changes as:
        - removing env vars inherited by the project or pipeline
        - changing env vars inherited by the project or pipeline
        - adding new environment variables

        Ambiguities:
        - an env var inherited by the old pipeline which hasn't been
            changed signals that the user wanted the default value of
            the pipeline env var.  If the new pipeline has such env var,
            use the default value coming from the new pipeline, if it
            doesn't have the env var, ignore the variable, i.e. do not
            include it in the resulting set.
        - an env var inherited by the project wasn't changed, and the
            old pipeline didn't overwrite the value of that variable. If
            the new pipeline has that env var then it will overwrite the
            value.

        """"""
    old_proj_ppl_merge = {**project_env_vars, **old_pipeline_env_vars}
    removed_env_vars = set()
    changed_env_vars = dict()
    added_env_vars = dict()
    for env_var in old_proj_ppl_merge:
        if env_var not in old_job_env_vars:
            removed_env_vars.add(env_var)
        elif old_proj_ppl_merge[env_var] != old_job_env_vars[env_var]:
            changed_env_vars[env_var] = old_job_env_vars[env_var]
    for env_var in old_job_env_vars:
        if env_var not in old_proj_ppl_merge:
            added_env_vars[env_var] = old_job_env_vars[env_var]
    result = {**project_env_vars, **new_pipeline_env_vars, **changed_env_vars, **added_env_vars}
    for env_var in removed_env_vars:
        result.pop(env_var, None)
    return result","for env_var in old_job_env_vars:
    if env_var not in old_proj_ppl_merge:
        added_env_vars[env_var] = old_job_env_vars[env_var]",XXX,no_found,0,,,
OpenVINO-YoloV3,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenVINO-YoloV3/utils.py,https://github.com/PINTO0309/OpenVINO-YoloV3/tree/master//utils.py,,load_coco_names$229,"def load_coco_names(file_name):
    names = {}
    with open(file_name) as f:
        for (id, name) in enumerate(f):
            names[id] = name
    return names","for (id, name) in enumerate(f):
    names[id] = name",XXX,no_found,,,,
PGL,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/examples/kddcup2021/WikiKG90M/feature/dump_feat/10_ht_feat.py,https://github.com/PaddlePaddle/PGL/tree/master/examples/kddcup2021/WikiKG90M/feature/dump_feat/10_ht_feat.py,,f$33,"def f(x):
    res = np.zeros_like(x)
    (unique, counts) = np.unique(x, return_counts=True)
    mapper_dict = {}
    for (idx, count) in zip(unique, counts):
        mapper_dict[idx] = count

    def mp(entry):
        return mapper_dict[entry]
    mp = np.vectorize(mp)
    return mp(x)","for (idx, count) in zip(unique, counts):
    mapper_dict[idx] = count",XXX,no_found,,,,
Tencent2019_Finals_Rank1st,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Tencent2019_Finals_Rank1st/A/extract_features.py,https://github.com/bettenW/Tencent2019_Finals_Rank1st/tree/master/A/extract_features.py,,request_cont$117,"def request_cont(train_df, test_df, flag):
    print('request_cont')
    request = pd.read_pickle('preprocess_data/aid_request_{}.pkl'.format(flag))
    dic = {}
    for item in request[['aid', 'day', 'request_cont']].values:
        dic[item[0], item[1]] = int(item[2])
    train_df['request_cont'] = train_df[['aid', 'day']].apply(lambda x: dic[tuple(x)], axis=1)
    test_df['request_cont'] = test_df[['aid', 'day']].apply(lambda x: dic[tuple(x)], axis=1)","for item in request[['aid', 'day', 'request_cont']].values:
    dic[item[0], item[1]] = int(item[2])",XXX,no_found,,,,
Pyro4,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pyro4/src/Pyro4/naming.py,https://github.com/irmen/Pyro4/tree/master/src/Pyro4/naming.py,NameServer,list$158,"def list(self, prefix=None, regex=None, metadata_all=None, metadata_any=None, return_metadata=False):
    """"""Retrieve the registered items as a dictionary name-to-URI. The URIs
        in the resulting dict are strings, not URI objects.
        You can filter by prefix or by regex or by metadata subset (separately)""""""

    def fix_set(result):
        if return_metadata:
            fixed = {}
            for (name, data) in result.items():
                fixed[name] = (data[0], list(data[1]))
            return fixed
        return result
    if sum((1 for x in [prefix, regex, metadata_all, metadata_any] if x is not None)) > 1:
        raise ValueError('you can only filter on one thing at a time')
    with self.lock:
        if prefix:
            result = self.storage.optimized_prefix_list(prefix, return_metadata)
            if result is not None:
                return fix_set(result)
            result = {}
            for name in self.storage:
                if name.startswith(prefix):
                    result[name] = self.storage[name] if return_metadata else self.storage[name][0]
            return fix_set(result)
        elif regex:
            result = self.storage.optimized_regex_list(regex, return_metadata)
            if result is not None:
                return fix_set(result)
            result = {}
            try:
                regex = re.compile(regex)
            except re.error as x:
                raise NamingError('invalid regex: ' + str(x))
            else:
                for name in self.storage:
                    if regex.match(name):
                        result[name] = self.storage[name] if return_metadata else self.storage[name][0]
                return fix_set(result)
        elif metadata_all:
            if isinstance(metadata_all, basestring):
                raise TypeError('metadata_all should not be a str, but another iterable (set, list, etc)')
            metadata_all and iter(metadata_all)
            result = self.storage.optimized_metadata_search(metadata_all=metadata_all, return_metadata=return_metadata)
            if result is not None:
                return fix_set(result)
            metadata_all = frozenset(metadata_all)
            result = {}
            for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
                if metadata_all.issubset(meta):
                    result[name] = (uri, meta) if return_metadata else uri
            return fix_set(result)
        elif metadata_any:
            if isinstance(metadata_any, basestring):
                raise TypeError('metadata_any should not be a str, but another iterable (set, list, etc)')
            metadata_any and iter(metadata_any)
            result = self.storage.optimized_metadata_search(metadata_any=metadata_any, return_metadata=return_metadata)
            if result is not None:
                return fix_set(result)
            metadata_any = frozenset(metadata_any)
            result = {}
            for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
                if metadata_any & meta:
                    result[name] = (uri, meta) if return_metadata else uri
            return fix_set(result)
        else:
            return fix_set(self.storage.everything(return_metadata))","for (name, data) in result.items():
    fixed[name] = (data[0], list(data[1]))",XXX,no_found,0,,,
Pyro4,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pyro4/src/Pyro4/naming.py,https://github.com/irmen/Pyro4/tree/master/src/Pyro4/naming.py,NameServer,list$158,"def list(self, prefix=None, regex=None, metadata_all=None, metadata_any=None, return_metadata=False):
    """"""Retrieve the registered items as a dictionary name-to-URI. The URIs
        in the resulting dict are strings, not URI objects.
        You can filter by prefix or by regex or by metadata subset (separately)""""""

    def fix_set(result):
        if return_metadata:
            fixed = {}
            for (name, data) in result.items():
                fixed[name] = (data[0], list(data[1]))
            return fixed
        return result
    if sum((1 for x in [prefix, regex, metadata_all, metadata_any] if x is not None)) > 1:
        raise ValueError('you can only filter on one thing at a time')
    with self.lock:
        if prefix:
            result = self.storage.optimized_prefix_list(prefix, return_metadata)
            if result is not None:
                return fix_set(result)
            result = {}
            for name in self.storage:
                if name.startswith(prefix):
                    result[name] = self.storage[name] if return_metadata else self.storage[name][0]
            return fix_set(result)
        elif regex:
            result = self.storage.optimized_regex_list(regex, return_metadata)
            if result is not None:
                return fix_set(result)
            result = {}
            try:
                regex = re.compile(regex)
            except re.error as x:
                raise NamingError('invalid regex: ' + str(x))
            else:
                for name in self.storage:
                    if regex.match(name):
                        result[name] = self.storage[name] if return_metadata else self.storage[name][0]
                return fix_set(result)
        elif metadata_all:
            if isinstance(metadata_all, basestring):
                raise TypeError('metadata_all should not be a str, but another iterable (set, list, etc)')
            metadata_all and iter(metadata_all)
            result = self.storage.optimized_metadata_search(metadata_all=metadata_all, return_metadata=return_metadata)
            if result is not None:
                return fix_set(result)
            metadata_all = frozenset(metadata_all)
            result = {}
            for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
                if metadata_all.issubset(meta):
                    result[name] = (uri, meta) if return_metadata else uri
            return fix_set(result)
        elif metadata_any:
            if isinstance(metadata_any, basestring):
                raise TypeError('metadata_any should not be a str, but another iterable (set, list, etc)')
            metadata_any and iter(metadata_any)
            result = self.storage.optimized_metadata_search(metadata_any=metadata_any, return_metadata=return_metadata)
            if result is not None:
                return fix_set(result)
            metadata_any = frozenset(metadata_any)
            result = {}
            for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
                if metadata_any & meta:
                    result[name] = (uri, meta) if return_metadata else uri
            return fix_set(result)
        else:
            return fix_set(self.storage.everything(return_metadata))","for name in self.storage:
    if name.startswith(prefix):
        result[name] = self.storage[name] if return_metadata else self.storage[name][0]",XXX,no_found,0,,,
Pyro4,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pyro4/src/Pyro4/naming.py,https://github.com/irmen/Pyro4/tree/master/src/Pyro4/naming.py,NameServer,list$158,"def list(self, prefix=None, regex=None, metadata_all=None, metadata_any=None, return_metadata=False):
    """"""Retrieve the registered items as a dictionary name-to-URI. The URIs
        in the resulting dict are strings, not URI objects.
        You can filter by prefix or by regex or by metadata subset (separately)""""""

    def fix_set(result):
        if return_metadata:
            fixed = {}
            for (name, data) in result.items():
                fixed[name] = (data[0], list(data[1]))
            return fixed
        return result
    if sum((1 for x in [prefix, regex, metadata_all, metadata_any] if x is not None)) > 1:
        raise ValueError('you can only filter on one thing at a time')
    with self.lock:
        if prefix:
            result = self.storage.optimized_prefix_list(prefix, return_metadata)
            if result is not None:
                return fix_set(result)
            result = {}
            for name in self.storage:
                if name.startswith(prefix):
                    result[name] = self.storage[name] if return_metadata else self.storage[name][0]
            return fix_set(result)
        elif regex:
            result = self.storage.optimized_regex_list(regex, return_metadata)
            if result is not None:
                return fix_set(result)
            result = {}
            try:
                regex = re.compile(regex)
            except re.error as x:
                raise NamingError('invalid regex: ' + str(x))
            else:
                for name in self.storage:
                    if regex.match(name):
                        result[name] = self.storage[name] if return_metadata else self.storage[name][0]
                return fix_set(result)
        elif metadata_all:
            if isinstance(metadata_all, basestring):
                raise TypeError('metadata_all should not be a str, but another iterable (set, list, etc)')
            metadata_all and iter(metadata_all)
            result = self.storage.optimized_metadata_search(metadata_all=metadata_all, return_metadata=return_metadata)
            if result is not None:
                return fix_set(result)
            metadata_all = frozenset(metadata_all)
            result = {}
            for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
                if metadata_all.issubset(meta):
                    result[name] = (uri, meta) if return_metadata else uri
            return fix_set(result)
        elif metadata_any:
            if isinstance(metadata_any, basestring):
                raise TypeError('metadata_any should not be a str, but another iterable (set, list, etc)')
            metadata_any and iter(metadata_any)
            result = self.storage.optimized_metadata_search(metadata_any=metadata_any, return_metadata=return_metadata)
            if result is not None:
                return fix_set(result)
            metadata_any = frozenset(metadata_any)
            result = {}
            for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
                if metadata_any & meta:
                    result[name] = (uri, meta) if return_metadata else uri
            return fix_set(result)
        else:
            return fix_set(self.storage.everything(return_metadata))","for name in self.storage:
    if regex.match(name):
        result[name] = self.storage[name] if return_metadata else self.storage[name][0]",XXX,no_found,0,,,
Pyro4,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pyro4/src/Pyro4/naming.py,https://github.com/irmen/Pyro4/tree/master/src/Pyro4/naming.py,NameServer,list$158,"def list(self, prefix=None, regex=None, metadata_all=None, metadata_any=None, return_metadata=False):
    """"""Retrieve the registered items as a dictionary name-to-URI. The URIs
        in the resulting dict are strings, not URI objects.
        You can filter by prefix or by regex or by metadata subset (separately)""""""

    def fix_set(result):
        if return_metadata:
            fixed = {}
            for (name, data) in result.items():
                fixed[name] = (data[0], list(data[1]))
            return fixed
        return result
    if sum((1 for x in [prefix, regex, metadata_all, metadata_any] if x is not None)) > 1:
        raise ValueError('you can only filter on one thing at a time')
    with self.lock:
        if prefix:
            result = self.storage.optimized_prefix_list(prefix, return_metadata)
            if result is not None:
                return fix_set(result)
            result = {}
            for name in self.storage:
                if name.startswith(prefix):
                    result[name] = self.storage[name] if return_metadata else self.storage[name][0]
            return fix_set(result)
        elif regex:
            result = self.storage.optimized_regex_list(regex, return_metadata)
            if result is not None:
                return fix_set(result)
            result = {}
            try:
                regex = re.compile(regex)
            except re.error as x:
                raise NamingError('invalid regex: ' + str(x))
            else:
                for name in self.storage:
                    if regex.match(name):
                        result[name] = self.storage[name] if return_metadata else self.storage[name][0]
                return fix_set(result)
        elif metadata_all:
            if isinstance(metadata_all, basestring):
                raise TypeError('metadata_all should not be a str, but another iterable (set, list, etc)')
            metadata_all and iter(metadata_all)
            result = self.storage.optimized_metadata_search(metadata_all=metadata_all, return_metadata=return_metadata)
            if result is not None:
                return fix_set(result)
            metadata_all = frozenset(metadata_all)
            result = {}
            for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
                if metadata_all.issubset(meta):
                    result[name] = (uri, meta) if return_metadata else uri
            return fix_set(result)
        elif metadata_any:
            if isinstance(metadata_any, basestring):
                raise TypeError('metadata_any should not be a str, but another iterable (set, list, etc)')
            metadata_any and iter(metadata_any)
            result = self.storage.optimized_metadata_search(metadata_any=metadata_any, return_metadata=return_metadata)
            if result is not None:
                return fix_set(result)
            metadata_any = frozenset(metadata_any)
            result = {}
            for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
                if metadata_any & meta:
                    result[name] = (uri, meta) if return_metadata else uri
            return fix_set(result)
        else:
            return fix_set(self.storage.everything(return_metadata))","for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
    if metadata_all.issubset(meta):
        result[name] = (uri, meta) if return_metadata else uri",XXX,no_found,0,,,
Pyro4,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pyro4/src/Pyro4/naming.py,https://github.com/irmen/Pyro4/tree/master/src/Pyro4/naming.py,NameServer,list$158,"def list(self, prefix=None, regex=None, metadata_all=None, metadata_any=None, return_metadata=False):
    """"""Retrieve the registered items as a dictionary name-to-URI. The URIs
        in the resulting dict are strings, not URI objects.
        You can filter by prefix or by regex or by metadata subset (separately)""""""

    def fix_set(result):
        if return_metadata:
            fixed = {}
            for (name, data) in result.items():
                fixed[name] = (data[0], list(data[1]))
            return fixed
        return result
    if sum((1 for x in [prefix, regex, metadata_all, metadata_any] if x is not None)) > 1:
        raise ValueError('you can only filter on one thing at a time')
    with self.lock:
        if prefix:
            result = self.storage.optimized_prefix_list(prefix, return_metadata)
            if result is not None:
                return fix_set(result)
            result = {}
            for name in self.storage:
                if name.startswith(prefix):
                    result[name] = self.storage[name] if return_metadata else self.storage[name][0]
            return fix_set(result)
        elif regex:
            result = self.storage.optimized_regex_list(regex, return_metadata)
            if result is not None:
                return fix_set(result)
            result = {}
            try:
                regex = re.compile(regex)
            except re.error as x:
                raise NamingError('invalid regex: ' + str(x))
            else:
                for name in self.storage:
                    if regex.match(name):
                        result[name] = self.storage[name] if return_metadata else self.storage[name][0]
                return fix_set(result)
        elif metadata_all:
            if isinstance(metadata_all, basestring):
                raise TypeError('metadata_all should not be a str, but another iterable (set, list, etc)')
            metadata_all and iter(metadata_all)
            result = self.storage.optimized_metadata_search(metadata_all=metadata_all, return_metadata=return_metadata)
            if result is not None:
                return fix_set(result)
            metadata_all = frozenset(metadata_all)
            result = {}
            for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
                if metadata_all.issubset(meta):
                    result[name] = (uri, meta) if return_metadata else uri
            return fix_set(result)
        elif metadata_any:
            if isinstance(metadata_any, basestring):
                raise TypeError('metadata_any should not be a str, but another iterable (set, list, etc)')
            metadata_any and iter(metadata_any)
            result = self.storage.optimized_metadata_search(metadata_any=metadata_any, return_metadata=return_metadata)
            if result is not None:
                return fix_set(result)
            metadata_any = frozenset(metadata_any)
            result = {}
            for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
                if metadata_any & meta:
                    result[name] = (uri, meta) if return_metadata else uri
            return fix_set(result)
        else:
            return fix_set(self.storage.everything(return_metadata))","for (name, (uri, meta)) in self.storage.everything(return_metadata=True).items():
    if metadata_any & meta:
        result[name] = (uri, meta) if return_metadata else uri",XXX,no_found,0,,,
CellProfiler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CellProfiler/tests/modules/test_exporttospreadsheet.py,https://github.com/CellProfiler/CellProfiler/tree/master/tests/modules/test_exporttospreadsheet.py,,test_missing_measurements$2100,"def test_missing_measurements(output_dir):
    """"""Make sure ExportToSpreadsheet can continue when measurements are missing

    Regression test of IMG-361
    Take measurements for 3 image sets, some measurements missing
    from the middle one.
    """"""
    path = os.path.join(output_dir, 'my_file.csv')
    module = cellprofiler.modules.exporttospreadsheet.ExportToSpreadsheet()
    module.set_module_num(1)
    module.wants_everything.value = False
    module.wants_prefix.value = False
    module.object_groups[0].name.value = 'my_objects'
    module.object_groups[0].file_name.value = path
    module.object_groups[0].wants_automatic_file_name.value = False
    module.add_metadata.value = True
    m = cellprofiler_core.measurement.Measurements()
    numpy.random.seed(0)
    mvalues = numpy.random.uniform(size=(3, 4))
    for image_idx in range(mvalues.shape[0]):
        if image_idx:
            m.next_image_set()
        m.add_image_measurement('Count_my_objects', mvalues.shape[1])
        if image_idx != 1:
            m.add_image_measurement('my_measurement', 100)
            m.add_measurement('my_objects', 'my_measurement', mvalues[image_idx, :])
    image_set_list = cellprofiler_core.image.ImageSetList()
    image_set = image_set_list.get_image_set(0)
    object_set = cellprofiler_core.object.ObjectSet()
    object_set.add_objects(cellprofiler_core.object.Objects(), 'my_objects')
    workspace = cellprofiler_core.workspace.Workspace(make_measurements_pipeline(m), module, image_set, object_set, m, image_set_list)
    module.post_run(workspace)
    try:
        fd = open(path, 'r')
        reader = csv.reader(fd, delimiter=module.delimiter_char)
        header = next(reader)
        assert len(header) == 3
        d = {}
        for (index, column) in enumerate(header):
            d[column] = index
        assert 'my_measurement' in d
        for image_idx in range(3):
            for object_idx in range(mvalues.shape[1]):
                row = next(reader)
                assert len(row) == 3
                if image_idx == 1:
                    assert row[d['my_measurement']] == str(numpy.NAN)
                else:
                    assert round(abs(float(row[d['my_measurement']]) - mvalues[image_idx, object_idx]), 4) == 0
        with pytest.raises(StopIteration):
            reader.__next__()
    finally:
        fd.close()","for (index, column) in enumerate(header):
    d[column] = index",XXX,no_found,0,,,
PyQt5-Apps,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyQt5-Apps/words-recorder/main.py,https://github.com/taseikyo/PyQt5-Apps/tree/master/words-recorder/main.py,MainWindow,connectDatabase$83,"def connectDatabase(self, w):
    """""":author : Tich
        connect to database
        :param w: used for data update in `w.table`
        """"""
    global db, cursor
    if db:
        return
    config = ConfigParser()
    conf = {}
    try:
        config.readfp(open('setting.ini'))
        head = ['host', 'user', 'password', 'db', 'port', 'charset', 'path']
        for x in head:
            conf[x] = config.get('MySQL', x)
        self.outPath = conf['path']
    except:
        self.messageBox(""config the 'setting.ini' file first!"")
        return
    try:
        db = pymysql.connect(host=conf['host'], user=conf['user'], password=conf['password'], db=conf['db'], port=int(conf['port']), charset=conf['charset'])
        cursor = db.cursor()
        self.messageBox('connected to the database!\nthe table will be updated.')
    except Exception as e:
        self.messageBox('database connect error!\nerror msg: %s.                    \n===\nplease check your databse setting \nand restart the app.' % e.args[1])
        return
    self.updateTable(w)","for x in head:
    conf[x] = config.get('MySQL', x)",XXX,no_found,,,,
neutron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_db_sync.py,https://github.com/openstack/neutron/tree/master/neutron/plugins/ml2/drivers/ovn/mech_driver/ovsdb/ovn_db_sync.py,OvnNbSynchronizer,sync_routers_and_rports$437,"def sync_routers_and_rports(self, ctx):
    """"""Sync Routers between neutron and NB.

        @param ctx: neutron_lib.context
        @type  ctx: object of type neutron_lib.context.Context
        @var   db_routers: List of Routers from neutron DB
        @var   db_router_ports: List of Router ports from neutron DB
        @var   lrouters: NB dictionary of logical routers and
               the corresponding logical router ports.
               vs list-of-acls
        @var   del_lrouters_list: List of Routers that need to be
               deleted from NB
        @var   del_lrouter_ports_list: List of Router ports that need to be
               deleted from NB
        @return: Nothing
        """"""
    if not utils.is_ovn_l3(self.l3_plugin):
        LOG.debug('OVN L3 mode is disabled, skipping sync routers and router ports')
        return
    LOG.debug('OVN-NB Sync Routers and Router ports started @ %s', str(datetime.now()))
    db_routers = {}
    db_extends = {}
    db_router_ports = {}
    for router in self.l3_plugin.get_routers(ctx):
        db_routers[router['id']] = router
        db_extends[router['id']] = {}
        db_extends[router['id']]['routes'] = []
        db_extends[router['id']]['snats'] = []
        db_extends[router['id']]['fips'] = []
        db_extends[router['id']]['fips_pfs'] = []
        if not router.get(l3.EXTERNAL_GW_INFO):
            continue
        gateways = self._ovn_client._get_gw_info(ctx, router)
        for gw_info in gateways:
            prefix = constants.IPv4_ANY if gw_info.ip_version == constants.IP_VERSION_4 else constants.IPv6_ANY
            if gw_info.gateway_ip:
                db_extends[router['id']]['routes'].append({'destination': prefix, 'nexthop': gw_info.gateway_ip, 'external_ids': {ovn_const.OVN_ROUTER_IS_EXT_GW: 'true', ovn_const.OVN_SUBNET_EXT_ID_KEY: gw_info.subnet_id}})
            if gw_info.ip_version == constants.IP_VERSION_6:
                continue
            if gw_info.router_ip and utils.is_snat_enabled(router):
                networks = self._ovn_client._get_v4_network_of_all_router_ports(ctx, router['id'])
                for network in networks:
                    db_extends[router['id']]['snats'].append({'logical_ip': network, 'external_ip': gw_info.router_ip, 'type': 'snat'})
    fips = self.l3_plugin.get_floatingips(ctx, {'router_id': list(db_routers.keys())})
    for fip in fips:
        db_extends[fip['router_id']]['fips'].append(fip)
        if self.pf_plugin:
            fip_pfs = self.pf_plugin.get_floatingip_port_forwardings(ctx, fip['id'])
            for fip_pf in fip_pfs:
                db_extends[fip['router_id']]['fips_pfs'].append(fip_pf)
    interfaces = self.l3_plugin._get_sync_interfaces(ctx, list(db_routers.keys()), [constants.DEVICE_OWNER_ROUTER_INTF, constants.DEVICE_OWNER_ROUTER_GW, constants.DEVICE_OWNER_DVR_INTERFACE, constants.DEVICE_OWNER_ROUTER_HA_INTF, constants.DEVICE_OWNER_HA_REPLICATED_INT])
    for interface in interfaces:
        db_router_ports[interface['id']] = interface
    lrouters = self.ovn_api.get_all_logical_routers_with_rports()
    del_lrouters_list = []
    del_lrouter_ports_list = []
    update_sroutes_list = []
    update_lrport_list = []
    update_snats_list = []
    update_fips_list = []
    update_pfs_list = []
    for lrouter in lrouters:
        ovn_rtr_lb_pfs = self.ovn_api.get_router_floatingip_lbs(utils.ovn_name(lrouter['name']))
        if lrouter['name'] in db_routers:
            for (lrport, lrport_nets) in lrouter['ports'].items():
                if lrport in db_router_ports:
                    update_lrport_list.append(db_router_ports[lrport])
                    del db_router_ports[lrport]
                else:
                    del_lrouter_ports_list.append({'port': lrport, 'lrouter': lrouter['name']})
            if 'routes' in db_routers[lrouter['name']]:
                db_routes = db_routers[lrouter['name']]['routes']
            else:
                db_routes = []
            if 'routes' in db_extends[lrouter['name']]:
                db_routes.extend(db_extends[lrouter['name']]['routes'])
            ovn_routes = lrouter['static_routes']
            (add_routes, del_routes) = self._calculate_routes_differences(ovn_routes, db_routes)
            update_sroutes_list.append({'id': lrouter['name'], 'add': add_routes, 'del': del_routes})
            ovn_fips = lrouter['dnat_and_snats']
            db_fips = db_extends[lrouter['name']]['fips']
            (add_fips, del_fips) = self._calculate_fips_differences(ovn_fips, ovn_rtr_lb_pfs, db_fips)
            update_fips_list.append({'id': lrouter['name'], 'add': add_fips, 'del': del_fips})
            db_fips_pfs = db_extends[lrouter['name']]['fips_pfs']
            (add_fip_pfs, del_fip_pfs) = self._calculate_fip_pfs_differences(ovn_rtr_lb_pfs, db_fips_pfs)
            update_pfs_list.append({'id': lrouter['name'], 'add': add_fip_pfs, 'del': del_fip_pfs})
            ovn_nats = lrouter['snats']
            db_snats = db_extends[lrouter['name']]['snats']
            (add_snats, del_snats) = helpers.diff_list_of_dict(ovn_nats, db_snats)
            update_snats_list.append({'id': lrouter['name'], 'add': add_snats, 'del': del_snats})
        else:
            del_lrouters_list.append(lrouter)
    lrouters_names = {lr['name'] for lr in lrouters}
    for (r_id, router) in db_routers.items():
        if r_id in lrouters_names:
            continue
        LOG.warning('Router found in Neutron but not in OVN DB, router id=%s', router['id'])
        if self.mode == SYNC_MODE_REPAIR:
            try:
                LOG.warning('Creating the router %s in OVN NB DB', router['id'])
                self._ovn_client.create_router(ctx, router, add_external_gateway=False)
                if 'routes' in router:
                    update_sroutes_list.append({'id': router['id'], 'add': router['routes'], 'del': []})
                if 'routes' in db_extends[router['id']]:
                    update_sroutes_list.append({'id': router['id'], 'add': db_extends[router['id']]['routes'], 'del': []})
                if 'snats' in db_extends[router['id']]:
                    update_snats_list.append({'id': router['id'], 'add': db_extends[router['id']]['snats'], 'del': []})
                if 'fips' in db_extends[router['id']]:
                    update_fips_list.append({'id': router['id'], 'add': db_extends[router['id']]['fips'], 'del': []})
                if 'fips_pfs' in db_extends[router['id']]:
                    add_fip_pfs = {db_pf['floatingip_id'] for db_pf in db_extends[router['id']]['fips_pfs']}
                    update_pfs_list.append({'id': router['id'], 'add': list(add_fip_pfs), 'del': []})
            except RuntimeError:
                LOG.warning('Create router in OVN NB failed for router %s', router['id'])
    for (rp_id, rrport) in db_router_ports.items():
        LOG.warning('Router Port found in Neutron but not in OVN DB, router port_id=%s', rrport['id'])
        if self.mode == SYNC_MODE_REPAIR:
            try:
                LOG.warning('Creating the router port %s in OVN NB DB', rrport['id'])
                router = db_routers[rrport['device_id']]
                self._ovn_client._create_lrouter_port(ctx, router, rrport)
            except RuntimeError:
                LOG.warning('Create router port in OVN NB failed for router port %s', rrport['id'])
    for rport in update_lrport_list:
        LOG.warning('Router Port port_id=%s needs to be updated for networks changed', rport['id'])
        if self.mode == SYNC_MODE_REPAIR:
            try:
                LOG.warning('Updating networks on router port %s in OVN NB DB', rport['id'])
                self._ovn_client.update_router_port(ctx, rport)
            except RuntimeError:
                LOG.warning('Update router port networks in OVN NB failed for router port %s', rport['id'])
    with self.ovn_api.transaction(check_error=True) as txn:
        for lrouter in del_lrouters_list:
            LOG.warning('Router found in OVN but not in Neutron, router id=%s', lrouter['name'])
            if self.mode == SYNC_MODE_REPAIR:
                LOG.warning('Deleting the router %s from OVN NB DB', lrouter['name'])
                txn.add(self.ovn_api.delete_lrouter(utils.ovn_name(lrouter['name'])))
        for lrport_info in del_lrouter_ports_list:
            LOG.warning('Router Port found in OVN but not in Neutron, port_id=%s', lrport_info['port'])
            if self.mode == SYNC_MODE_REPAIR:
                LOG.warning('Deleting the port %s from OVN NB DB', lrport_info['port'])
                txn.add(self.ovn_api.delete_lrouter_port(utils.ovn_lrouter_port_name(lrport_info['port']), utils.ovn_name(lrport_info['lrouter']), if_exists=False))
        for sroute in update_sroutes_list:
            if sroute['add']:
                LOG.warning('Router %(id)s static routes %(route)s found in Neutron but not in OVN', {'id': sroute['id'], 'route': sroute['add']})
                if self.mode == SYNC_MODE_REPAIR:
                    LOG.warning('Add static routes %s to OVN NB DB', sroute['add'])
                    for route in sroute['add']:
                        columns = {}
                        if 'external_ids' in route:
                            columns['external_ids'] = route['external_ids']
                        txn.add(self.ovn_api.add_static_route(utils.ovn_name(sroute['id']), ip_prefix=route['destination'], nexthop=route['nexthop'], **columns))
            if sroute['del']:
                LOG.warning('Router %(id)s static routes %(route)s found in OVN but not in Neutron', {'id': sroute['id'], 'route': sroute['del']})
                if self.mode == SYNC_MODE_REPAIR:
                    LOG.warning('Delete static routes %s from OVN NB DB', sroute['del'])
                    for route in sroute['del']:
                        txn.add(self.ovn_api.delete_static_route(utils.ovn_name(sroute['id']), ip_prefix=route['destination'], nexthop=route['nexthop']))
        for fip in update_fips_list:
            if fip['del']:
                LOG.warning('Router %(id)s floating ips %(fip)s found in OVN but not in Neutron', {'id': fip['id'], 'fip': fip['del']})
                if self.mode == SYNC_MODE_REPAIR:
                    LOG.warning('Delete floating ips %s from OVN NB DB', fip['del'])
                    for nat in fip['del']:
                        self._ovn_client._delete_floatingip(nat, utils.ovn_name(fip['id']), txn=txn)
            if fip['add']:
                LOG.warning('Router %(id)s floating ips %(fip)s found in Neutron but not in OVN', {'id': fip['id'], 'fip': fip['add']})
                if self.mode == SYNC_MODE_REPAIR:
                    LOG.warning('Add floating ips %s to OVN NB DB', fip['add'])
                    for nat in fip['add']:
                        self._ovn_client._create_or_update_floatingip(nat, txn=txn)
        for pf in update_pfs_list:
            if pf['del']:
                LOG.warning('Router %(id)s port forwarding for floating ips %(fip)s found in OVN but not in Neutron', {'id': pf['id'], 'fip': pf['del']})
                if self.mode == SYNC_MODE_REPAIR:
                    LOG.warning('Delete port forwarding for fips %s from OVN NB DB', pf['del'])
                    for pf_id in pf['del']:
                        self._delete_floatingip_pfs(ctx, pf_id, txn)
            if pf['add']:
                LOG.warning('Router %(id)s port forwarding for floating ips %(fip)s Neutron out of sync or missing in OVN', {'id': pf['id'], 'fip': pf['add']})
                if self.mode == SYNC_MODE_REPAIR:
                    LOG.warning('Add port forwarding for fips %s to OVN NB DB', pf['add'])
                    for pf_fip_id in pf['add']:
                        self._create_or_update_floatingip_pfs(ctx, pf_fip_id, txn)
        for snat in update_snats_list:
            if snat['del']:
                LOG.warning('Router %(id)s snat %(snat)s found in OVN but not in Neutron', {'id': snat['id'], 'snat': snat['del']})
                if self.mode == SYNC_MODE_REPAIR:
                    LOG.warning('Delete snats %s from OVN NB DB', snat['del'])
                    for nat in snat['del']:
                        txn.add(self.ovn_api.delete_nat_rule_in_lrouter(utils.ovn_name(snat['id']), logical_ip=nat['logical_ip'], external_ip=nat['external_ip'], type='snat'))
            if snat['add']:
                LOG.warning('Router %(id)s snat %(snat)s found in Neutron but not in OVN', {'id': snat['id'], 'snat': snat['add']})
                if self.mode == SYNC_MODE_REPAIR:
                    LOG.warning('Add snats %s to OVN NB DB', snat['add'])
                    for nat in snat['add']:
                        txn.add(self.ovn_api.add_nat_rule_in_lrouter(utils.ovn_name(snat['id']), logical_ip=nat['logical_ip'], external_ip=nat['external_ip'], type='snat'))
    LOG.debug('OVN-NB Sync routers and router ports finished %s', str(datetime.now()))","for interface in interfaces:
    db_router_ports[interface['id']] = interface",XXX,no_found,0,,,
MultiQC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MultiQC/multiqc/modules/mirtrace/mirtrace.py,https://github.com/ewels/MultiQC/tree/master/multiqc/modules/mirtrace/mirtrace.py,MultiqcModule,parse_complexity$178,"def parse_complexity(self, f):
    header = []
    body = {}
    lines = f['f'].splitlines()
    for l in lines:
        s = l.split('\t')
        if len(header) == 0:
            if s[0] != 'DISTINCT_MIRNA_HAIRPINS_ACCUMULATED_COUNT':
                log.debug('No valid data {} for miRNA complexity'.format(f['fn']))
                return None
            header = s[1:]
        else:
            body[s[0]] = s[1:len(s)]
    for record in header[0:len(header)]:
        s_name = self.clean_s_name(record, f)
        parsed_data = {}
        idx = header[0:len(header)].index(record)
        for depth in body:
            parsed_data[depth] = int(body[depth][idx]) if body[depth][idx] else 0
        if s_name in self.complexity_data:
            log.debug('Duplicate sample name found! Overwriting: {}'.format(s_name))
        self.add_data_source(f, s_name)
        self.complexity_data[s_name] = parsed_data","for depth in body:
    parsed_data[depth] = int(body[depth][idx]) if body[depth][idx] else 0",XXX,no_found,0,,,
unilm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unilm/markuplm/examples/fine_tuning/run_websrc/utils.py,https://github.com/microsoft/unilm/tree/master/markuplm/examples/fine_tuning/run_websrc/utils.py,,write_predictions$727,"def write_predictions(all_examples, all_features, all_results, n_best_size, max_answer_length, do_lower_case, output_prediction_file, output_tag_prediction_file, output_nbest_file, verbose_logging, tokenizer):
    """"""
    Compute and write down the final results, including the n best results.

    Arguments:
        all_examples (list[SRCExample]): all the SRC Example of the dataset; note that we only need it to provide the
                                         mapping from example index to the question-answers id.
        all_features (list[InputFeatures]): all the features for the input doc spans.
        all_results (list[RawResult]): all the results from the models.
        n_best_size (int): the number of the n best buffer and the final n best result saved.
        max_answer_length (int): constrain the model to predict the answer no longer than it.
        do_lower_case (bool): whether the model distinguish upper and lower case of the letters.
        output_prediction_file (str): the file which the best answer text predictions will be written to.
        output_tag_prediction_file (str): the file which the best answer tag predictions will be written to.
        output_nbest_file (str): the file which the n best answer predictions including text, tag, and probabilities
                                 will be written to.
        verbose_logging (bool): if true, all of the warnings related to data processing will be printed.
    """"""
    logger.info('Writing predictions to: %s' % output_prediction_file)
    logger.info('Writing nbest to: %s' % output_nbest_file)
    example_index_to_features = collections.defaultdict(list)
    for feature in all_features:
        example_index_to_features[feature.example_index].append(feature)
    unique_id_to_result = {}
    for result in all_results:
        unique_id_to_result[result.unique_id] = result
    _PrelimPrediction = collections.namedtuple('PrelimPrediction', ['feature_index', 'start_index', 'end_index', 'start_logit', 'end_logit', 'tag_ids'])
    all_predictions = collections.OrderedDict()
    all_tag_predictions = collections.OrderedDict()
    all_nbest_json = collections.OrderedDict()
    for (example_index, example) in enumerate(all_examples):
        features = example_index_to_features[example_index]
        prelim_predictions = []
        for (feature_index, feature) in enumerate(features):
            result = unique_id_to_result[feature.unique_id]
            start_indexes = _get_best_indexes(result.start_logits, n_best_size)
            end_indexes = _get_best_indexes(result.end_logits, n_best_size)
            for start_index in start_indexes:
                for end_index in end_indexes:
                    if start_index >= len(feature.tokens):
                        continue
                    if end_index >= len(feature.tokens):
                        continue
                    if start_index not in feature.token_to_orig_map:
                        continue
                    if end_index not in feature.token_to_orig_map:
                        continue
                    if not feature.token_is_max_context.get(start_index, False):
                        continue
                    if end_index < start_index:
                        continue
                    length = end_index - start_index + 1
                    if length > max_answer_length:
                        continue
                    tag_ids = set(feature.token_to_tag_index[start_index:end_index + 1])
                    prelim_predictions.append(_PrelimPrediction(feature_index=feature_index, start_index=start_index, end_index=end_index, start_logit=result.start_logits[start_index], end_logit=result.end_logits[end_index], tag_ids=list(tag_ids)))
        prelim_predictions = sorted(prelim_predictions, key=lambda x: x.start_logit + x.end_logit, reverse=True)
        _NbestPrediction = collections.namedtuple('NbestPrediction', ['text', 'start_logit', 'end_logit', 'tag_ids'])
        seen_predictions = {}
        nbest = []
        for pred in prelim_predictions:
            if len(nbest) >= n_best_size:
                break
            feature = features[pred.feature_index]
            if pred.start_index > 0:
                tok_tokens = feature.tokens[pred.start_index:pred.end_index + 1]
                orig_doc_start = feature.token_to_orig_map[pred.start_index]
                orig_doc_end = feature.token_to_orig_map[pred.end_index]
                orig_tokens = example.doc_tokens[orig_doc_start:orig_doc_end + 1]
                tok_text = ' '.join(tok_tokens)
                tok_text = tok_text.replace(' ##', '')
                tok_text = tok_text.replace('##', '')
                tok_text = tok_text.strip()
                tok_text = ' '.join(tok_text.split())
                orig_text = ' '.join(orig_tokens)
                final_text = _get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)
                if final_text in seen_predictions:
                    continue
                seen_predictions[final_text] = True
            else:
                final_text = ''
                seen_predictions[final_text] = True
            nbest.append(_NbestPrediction(text=final_text, start_logit=pred.start_logit, end_logit=pred.end_logit, tag_ids=pred.tag_ids))
        if not nbest:
            nbest.append(_NbestPrediction(text='empty', start_logit=0.0, end_logit=0.0, tag_ids=[-1]))
        assert len(nbest) >= 1
        total_scores = []
        best_non_null_entry = None
        for entry in nbest:
            total_scores.append(entry.start_logit + entry.end_logit)
            if not best_non_null_entry:
                if entry.text:
                    best_non_null_entry = entry
        probs = _compute_softmax(total_scores)
        nbest_json = []
        for (i, entry) in enumerate(nbest):
            output = collections.OrderedDict()
            output['text'] = entry.text
            output['probability'] = probs[i]
            output['start_logit'] = entry.start_logit
            output['end_logit'] = entry.end_logit
            output['tag_ids'] = entry.tag_ids
            nbest_json.append(output)
        assert len(nbest_json) >= 1
        best = nbest_json[0]['text'].split()
        best = ' '.join([w for w in best if (w[0] != '<' or w[-1] != '>') and w != '<end-of-node>' and (w != tokenizer.sep_token) and (w != tokenizer.cls_token)])
        all_predictions[example.qas_id] = best
        all_tag_predictions[example.qas_id] = nbest_json[0]['tag_ids']
        all_nbest_json[example.qas_id] = nbest_json
    with open(output_prediction_file, 'w') as writer:
        writer.write(json.dumps(all_predictions, indent=4) + '\n')
    with open(output_nbest_file, 'w') as writer:
        writer.write(json.dumps(all_nbest_json, indent=4) + '\n')
    with open(output_tag_prediction_file, 'w') as writer:
        writer.write(json.dumps(all_tag_predictions, indent=4) + '\n')
    return","for result in all_results:
    unique_id_to_result[result.unique_id] = result",XXX,no_found,,,,
data-validation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-validation/tensorflow_data_validation/arrow/arrow_util_test.py,https://github.com/tensorflow/data-validation/tree/master/tensorflow_data_validation/arrow/arrow_util_test.py,ArrowUtilTest,testEnumerateArrays$459,"def testEnumerateArrays(self):
    for (leaves_only, has_weights, wrap_flat_struct_in_list) in itertools.product([True, False], [True, False], [True, False]):
        actual_results = {}
        for (feature_path, feature_array, weights) in arrow_util.enumerate_arrays(_INPUT_RECORD_BATCH, _EXAMPLE_WEIGHT_MAP if has_weights else None, leaves_only, wrap_flat_struct_in_list):
            actual_results[feature_path] = (feature_array, weights)
        expected_results = {}
        for p in [['f1'], ['w'], ['w_override1'], ['w_override2'], ['f2', 'sf1'], ['f2', 'sf2', 'ssf1'], ['f3', 'sf1'], ['f3', 'sf2']]:
            feature_path = types.FeaturePath(p)
            expected_results[feature_path] = (_FEATURES_TO_ARRAYS[feature_path].array, _FEATURES_TO_ARRAYS[feature_path].weights if has_weights else None)
        if not leaves_only:
            for p in [['f2'], ['f2', 'sf2'], ['f3']]:
                feature_path = types.FeaturePath(p)
                expected_array = _FEATURES_TO_ARRAYS[feature_path][0]
                if wrap_flat_struct_in_list and pa.types.is_struct(expected_array.type):
                    expected_array = array_util.ToSingletonListArray(expected_array)
                expected_results[feature_path] = (expected_array, _FEATURES_TO_ARRAYS[feature_path].weights if has_weights else None)
        self.assertLen(actual_results, len(expected_results))
        for (k, v) in six.iteritems(expected_results):
            self.assertIn(k, actual_results)
            actual = actual_results[k]
            self.assertTrue(actual[0].equals(v[0]), 'leaves_only={}; has_weights={}; wrap_flat_struct_in_list={} feature={}; expected: {}; actual: {}'.format(leaves_only, has_weights, wrap_flat_struct_in_list, k, v, actual))
            np.testing.assert_array_equal(actual[1], v[1])","for (feature_path, feature_array, weights) in arrow_util.enumerate_arrays(_INPUT_RECORD_BATCH, _EXAMPLE_WEIGHT_MAP if has_weights else None, leaves_only, wrap_flat_struct_in_list):
    actual_results[feature_path] = (feature_array, weights)",XXX,no_found,0,,,
data-validation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-validation/tensorflow_data_validation/arrow/arrow_util_test.py,https://github.com/tensorflow/data-validation/tree/master/tensorflow_data_validation/arrow/arrow_util_test.py,ArrowUtilTest,testEnumerateArrays$459,"def testEnumerateArrays(self):
    for (leaves_only, has_weights, wrap_flat_struct_in_list) in itertools.product([True, False], [True, False], [True, False]):
        actual_results = {}
        for (feature_path, feature_array, weights) in arrow_util.enumerate_arrays(_INPUT_RECORD_BATCH, _EXAMPLE_WEIGHT_MAP if has_weights else None, leaves_only, wrap_flat_struct_in_list):
            actual_results[feature_path] = (feature_array, weights)
        expected_results = {}
        for p in [['f1'], ['w'], ['w_override1'], ['w_override2'], ['f2', 'sf1'], ['f2', 'sf2', 'ssf1'], ['f3', 'sf1'], ['f3', 'sf2']]:
            feature_path = types.FeaturePath(p)
            expected_results[feature_path] = (_FEATURES_TO_ARRAYS[feature_path].array, _FEATURES_TO_ARRAYS[feature_path].weights if has_weights else None)
        if not leaves_only:
            for p in [['f2'], ['f2', 'sf2'], ['f3']]:
                feature_path = types.FeaturePath(p)
                expected_array = _FEATURES_TO_ARRAYS[feature_path][0]
                if wrap_flat_struct_in_list and pa.types.is_struct(expected_array.type):
                    expected_array = array_util.ToSingletonListArray(expected_array)
                expected_results[feature_path] = (expected_array, _FEATURES_TO_ARRAYS[feature_path].weights if has_weights else None)
        self.assertLen(actual_results, len(expected_results))
        for (k, v) in six.iteritems(expected_results):
            self.assertIn(k, actual_results)
            actual = actual_results[k]
            self.assertTrue(actual[0].equals(v[0]), 'leaves_only={}; has_weights={}; wrap_flat_struct_in_list={} feature={}; expected: {}; actual: {}'.format(leaves_only, has_weights, wrap_flat_struct_in_list, k, v, actual))
            np.testing.assert_array_equal(actual[1], v[1])","for p in [['f1'], ['w'], ['w_override1'], ['w_override2'], ['f2', 'sf1'], ['f2', 'sf2', 'ssf1'], ['f3', 'sf1'], ['f3', 'sf2']]:
    feature_path = types.FeaturePath(p)
    expected_results[feature_path] = (_FEATURES_TO_ARRAYS[feature_path].array, _FEATURES_TO_ARRAYS[feature_path].weights if has_weights else None)",XXX,no_found,0,,,
data-validation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-validation/tensorflow_data_validation/arrow/arrow_util_test.py,https://github.com/tensorflow/data-validation/tree/master/tensorflow_data_validation/arrow/arrow_util_test.py,ArrowUtilTest,testEnumerateArrays$459,"def testEnumerateArrays(self):
    for (leaves_only, has_weights, wrap_flat_struct_in_list) in itertools.product([True, False], [True, False], [True, False]):
        actual_results = {}
        for (feature_path, feature_array, weights) in arrow_util.enumerate_arrays(_INPUT_RECORD_BATCH, _EXAMPLE_WEIGHT_MAP if has_weights else None, leaves_only, wrap_flat_struct_in_list):
            actual_results[feature_path] = (feature_array, weights)
        expected_results = {}
        for p in [['f1'], ['w'], ['w_override1'], ['w_override2'], ['f2', 'sf1'], ['f2', 'sf2', 'ssf1'], ['f3', 'sf1'], ['f3', 'sf2']]:
            feature_path = types.FeaturePath(p)
            expected_results[feature_path] = (_FEATURES_TO_ARRAYS[feature_path].array, _FEATURES_TO_ARRAYS[feature_path].weights if has_weights else None)
        if not leaves_only:
            for p in [['f2'], ['f2', 'sf2'], ['f3']]:
                feature_path = types.FeaturePath(p)
                expected_array = _FEATURES_TO_ARRAYS[feature_path][0]
                if wrap_flat_struct_in_list and pa.types.is_struct(expected_array.type):
                    expected_array = array_util.ToSingletonListArray(expected_array)
                expected_results[feature_path] = (expected_array, _FEATURES_TO_ARRAYS[feature_path].weights if has_weights else None)
        self.assertLen(actual_results, len(expected_results))
        for (k, v) in six.iteritems(expected_results):
            self.assertIn(k, actual_results)
            actual = actual_results[k]
            self.assertTrue(actual[0].equals(v[0]), 'leaves_only={}; has_weights={}; wrap_flat_struct_in_list={} feature={}; expected: {}; actual: {}'.format(leaves_only, has_weights, wrap_flat_struct_in_list, k, v, actual))
            np.testing.assert_array_equal(actual[1], v[1])","for p in [['f2'], ['f2', 'sf2'], ['f3']]:
    feature_path = types.FeaturePath(p)
    expected_array = _FEATURES_TO_ARRAYS[feature_path][0]
    if wrap_flat_struct_in_list and pa.types.is_struct(expected_array.type):
        expected_array = array_util.ToSingletonListArray(expected_array)
    expected_results[feature_path] = (expected_array, _FEATURES_TO_ARRAYS[feature_path].weights if has_weights else None)",XXX,no_found,0,,,
exbert,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/exbert/server/transformers/templates/adding_a_new_example_script/utils_xxx.py,https://github.com/bhoov/exbert/tree/master/server/transformers/templates/adding_a_new_example_script/utils_xxx.py,,get_final_text$879,"def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):
    """"""Project the tokenized prediction back to the original text.""""""

    def _strip_spaces(text):
        ns_chars = []
        ns_to_s_map = collections.OrderedDict()
        for (i, c) in enumerate(text):
            if c == ' ':
                continue
            ns_to_s_map[len(ns_chars)] = i
            ns_chars.append(c)
        ns_text = ''.join(ns_chars)
        return (ns_text, ns_to_s_map)
    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)
    tok_text = ' '.join(tokenizer.tokenize(orig_text))
    start_position = tok_text.find(pred_text)
    if start_position == -1:
        if verbose_logging:
            logger.info(""Unable to find text: '%s' in '%s'"" % (pred_text, orig_text))
        return orig_text
    end_position = start_position + len(pred_text) - 1
    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)
    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)
    if len(orig_ns_text) != len(tok_ns_text):
        if verbose_logging:
            logger.info(""Length not equal after stripping spaces: '%s' vs '%s'"", orig_ns_text, tok_ns_text)
        return orig_text
    tok_s_to_ns_map = {}
    for (i, tok_index) in tok_ns_to_s_map.items():
        tok_s_to_ns_map[tok_index] = i
    orig_start_position = None
    if start_position in tok_s_to_ns_map:
        ns_start_position = tok_s_to_ns_map[start_position]
        if ns_start_position in orig_ns_to_s_map:
            orig_start_position = orig_ns_to_s_map[ns_start_position]
    if orig_start_position is None:
        if verbose_logging:
            logger.info(""Couldn't map start position"")
        return orig_text
    orig_end_position = None
    if end_position in tok_s_to_ns_map:
        ns_end_position = tok_s_to_ns_map[end_position]
        if ns_end_position in orig_ns_to_s_map:
            orig_end_position = orig_ns_to_s_map[ns_end_position]
    if orig_end_position is None:
        if verbose_logging:
            logger.info(""Couldn't map end position"")
        return orig_text
    output_text = orig_text[orig_start_position:orig_end_position + 1]
    return output_text","for (i, tok_index) in tok_ns_to_s_map.items():
    tok_s_to_ns_map[tok_index] = i",XXX,no_found,0,,,
surreal,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/surreal/surreal/learner/base.py,https://github.com/SurrealAI/surreal/tree/master/surreal/learner/base.py,Learner,generate_tensorplex_report$201,"def generate_tensorplex_report(self):
    """"""
            Adds core and system level tensorplex stats
        """"""
    cur_time = time.time()
    current_iter = self.current_iter
    iter_elapsed = current_iter - self.last_iter
    self.last_iter = current_iter
    time_elapsed = cur_time - self.last_time
    self.last_time = cur_time
    core_metrics = {}
    system_metrics = {}
    learn_time = self.learn_timer.avg + 1e-06
    fetch_timer = self._prefetch_queue.timer
    fetch_time = fetch_timer.avg + 1e-06
    iter_time = self.iter_timer.avg + 1e-06
    publish_time = self.publish_timer.avg + 1e-06
    core_metrics['learn_time_s'] = learn_time
    core_metrics['fetch_time_s'] = fetch_time
    core_metrics['publish_time_s'] = publish_time
    core_metrics['iter_time_s'] = iter_time
    iter_per_s = iter_elapsed / time_elapsed
    system_metrics['iter_per_s'] = iter_per_s
    system_metrics['exp_per_s'] = iter_per_s * self.learner_config.replay.batch_size
    system_metrics['compute_load_percent'] = min(learn_time / iter_time * 100, 100)
    system_metrics['io_fetch_experience_load_percent'] = min(fetch_time / iter_time * 100, 100)
    system_metrics['io_publish_load_percent'] = min(publish_time / iter_time * 100, 100)
    all_metrics = {}
    for k in core_metrics:
        all_metrics['.core/' + k] = core_metrics[k]
    for k in system_metrics:
        all_metrics['.system/' + k] = system_metrics[k]
    self.tensorplex.add_scalars(all_metrics)","for k in core_metrics:
    all_metrics['.core/' + k] = core_metrics[k]",XXX,no_found,1,,,
surreal,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/surreal/surreal/learner/base.py,https://github.com/SurrealAI/surreal/tree/master/surreal/learner/base.py,Learner,generate_tensorplex_report$201,"def generate_tensorplex_report(self):
    """"""
            Adds core and system level tensorplex stats
        """"""
    cur_time = time.time()
    current_iter = self.current_iter
    iter_elapsed = current_iter - self.last_iter
    self.last_iter = current_iter
    time_elapsed = cur_time - self.last_time
    self.last_time = cur_time
    core_metrics = {}
    system_metrics = {}
    learn_time = self.learn_timer.avg + 1e-06
    fetch_timer = self._prefetch_queue.timer
    fetch_time = fetch_timer.avg + 1e-06
    iter_time = self.iter_timer.avg + 1e-06
    publish_time = self.publish_timer.avg + 1e-06
    core_metrics['learn_time_s'] = learn_time
    core_metrics['fetch_time_s'] = fetch_time
    core_metrics['publish_time_s'] = publish_time
    core_metrics['iter_time_s'] = iter_time
    iter_per_s = iter_elapsed / time_elapsed
    system_metrics['iter_per_s'] = iter_per_s
    system_metrics['exp_per_s'] = iter_per_s * self.learner_config.replay.batch_size
    system_metrics['compute_load_percent'] = min(learn_time / iter_time * 100, 100)
    system_metrics['io_fetch_experience_load_percent'] = min(fetch_time / iter_time * 100, 100)
    system_metrics['io_publish_load_percent'] = min(publish_time / iter_time * 100, 100)
    all_metrics = {}
    for k in core_metrics:
        all_metrics['.core/' + k] = core_metrics[k]
    for k in system_metrics:
        all_metrics['.system/' + k] = system_metrics[k]
    self.tensorplex.add_scalars(all_metrics)","for k in system_metrics:
    all_metrics['.system/' + k] = system_metrics[k]",XXX,no_found,1,,,
yarGen,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yarGen/yarGen.py,https://github.com/Neo23x0/yarGen/tree/master//yarGen.py,,generate_rules$1044,"def generate_rules(file_strings, file_opcodes, super_rules, file_info, inverse_stats):
    if args.o:
        try:
            fh = open(args.o, 'w')
        except Exception as e:
            traceback.print_exc()
    general_info = '/*\n'
    general_info += '   YARA Rule Set\n'
    general_info += '   Author: {0}\n'.format(args.a)
    general_info += '   Date: {0}\n'.format(get_timestamp_basic())
    general_info += '   Identifier: {0}\n'.format(identifier)
    general_info += '   Reference: {0}\n'.format(reference)
    if args.l != '':
        general_info += '   License: {0}\n'.format(args.l)
    general_info += '*/\n\n'
    fh.write(general_info)
    if args.globalrule:
        (condition, pe_module_necessary) = generate_general_condition(file_info)
        if condition != '':
            global_rule = '/* Global Rule -------------------------------------------------------------- */\n'
            global_rule += '/* Will be evaluated first, speeds up scanning process, remove at will */\n\n'
            global_rule += 'global private rule gen_characteristics {\n'
            global_rule += '   condition:\n'
            global_rule += '      {0}\n'.format(condition)
            global_rule += '}\n\n'
            if args.o:
                fh.write(global_rule)
    rules = ''
    printed_rules = {}
    opcodes_to_add = []
    rule_count = 0
    inverse_rule_count = 0
    super_rule_count = 0
    pe_module_necessary = False
    if not args.inverse:
        print('[+] Generating Simple Rules ...')
        print('[-] Applying intelligent filters to string findings ...')
        for filePath in file_strings:
            print('[-] Filtering string set for %s ...' % filePath)
            string_set = file_strings[filePath]
            file_strings[filePath] = []
            file_strings[filePath] = filter_string_set(string_set)
            if filePath not in file_opcodes:
                file_opcodes[filePath] = []
            else:
                print('[-] Filtering opcode set for %s ...' % filePath)
            opcode_set = file_opcodes[filePath]
            file_opcodes[filePath] = []
            file_opcodes[filePath] = filter_opcode_set(opcode_set)
        fh.write('/* Rule Set ----------------------------------------------------------------- */\n\n')
        for filePath in file_strings:
            if len(file_strings[filePath]) == 0:
                print('[W] Not enough high scoring strings to create a rule. (Try -z 0 to reduce the min score or --opcodes to include opcodes) FILE: %s' % filePath)
                continue
            elif len(file_strings[filePath]) == 0 and len(file_opcodes[filePath]) == 0:
                print('[W] Not enough high scoring strings and opcodes to create a rule. (Try -z 0 to reduce the min score) FILE: %s' % filePath)
                continue
            try:
                rule = ''
                (path, file) = os.path.split(filePath)
                fileBase = os.path.splitext(file)[0]
                cleanedName = fileBase
                if len(fileBase) < 8:
                    cleanedName = path.split('\\')[-1:][0] + '_' + cleanedName
                if re.search('^[0-9]', cleanedName):
                    cleanedName = 'sig_' + cleanedName
                cleanedName = re.sub('[^\\w]', '_', cleanedName)
                if cleanedName in printed_rules:
                    printed_rules[cleanedName] += 1
                    cleanedName = cleanedName + '_' + str(printed_rules[cleanedName])
                else:
                    printed_rules[cleanedName] = 1
                rule += 'rule %s {\n' % cleanedName
                rule += '   meta:\n'
                rule += '      description = ""%s - file %s""\n' % (prefix, file)
                rule += '      author = ""%s""\n' % args.a
                rule += '      reference = ""%s""\n' % reference
                rule += '      date = ""%s""\n' % get_timestamp_basic()
                rule += '      hash1 = ""%s""\n' % file_info[filePath]['hash']
                rule += '   strings:\n'
                (rule_strings, opcodes_included, string_rule_count, high_scoring_strings) = get_rule_strings(file_strings[filePath], file_opcodes[filePath])
                rule += rule_strings
                if args.strings:
                    strings = get_strings(file_strings[filePath])
                    write_strings(filePath, strings, args.e, args.score)
                conditions = []
                subconditions = []
                condition_pe = []
                condition_pe_part1 = []
                condition_pe_part2 = []
                if not args.noextras and file_info[filePath]['magic'] == 'MZ':
                    if file_info[filePath]['imphash'] not in good_imphashes_db and file_info[filePath]['imphash'] != '':
                        imphash = file_info[filePath]['imphash']
                        comment = ''
                        if imphash in KNOWN_IMPHASHES:
                            comment = ' /* {0} */'.format(KNOWN_IMPHASHES[imphash])
                        condition_pe_part1.append('pe.imphash() == ""{0}""{1}'.format(imphash, comment))
                        pe_module_necessary = True
                    if file_info[filePath]['exports']:
                        e_count = 0
                        for export in file_info[filePath]['exports']:
                            if export not in good_exports_db:
                                condition_pe_part2.append('pe.exports(""{0}"")'.format(export))
                                e_count += 1
                                pe_module_necessary = True
                            if e_count > 5:
                                break
                basic_conditions = []
                if not args.nofilesize:
                    basic_conditions.insert(0, get_file_range(file_info[filePath]['size']))
                if file_info[filePath]['magic'] != '':
                    uint_string = get_uint_string(file_info[filePath]['magic'])
                    basic_conditions.insert(0, uint_string)
                if len(basic_conditions):
                    conditions.append(' and '.join(basic_conditions))
                pe_conditions_add = False
                if condition_pe_part1 or condition_pe_part2:
                    if len(condition_pe_part1) == 1:
                        condition_pe.append(condition_pe_part1[0])
                    elif len(condition_pe_part1) > 1:
                        condition_pe.append('( %s )' % ' or '.join(condition_pe_part1))
                    if len(condition_pe_part2) == 1:
                        condition_pe.append(condition_pe_part2[0])
                    elif len(condition_pe_part2) > 1:
                        condition_pe.append('( %s )' % ' and '.join(condition_pe_part2))
                    pe_conditions_add = True
                    subconditions.append(' and '.join(condition_pe))
                cond_op = ''
                cond_hs = ''
                cond_ls = ''
                low_scoring_strings = string_rule_count - high_scoring_strings
                if high_scoring_strings > 0:
                    cond_hs = '1 of ($x*)'
                if low_scoring_strings > 0:
                    if low_scoring_strings > 10:
                        if high_scoring_strings > 0:
                            cond_ls = '4 of them'
                        else:
                            cond_ls = '8 of them'
                    else:
                        cond_ls = 'all of them'
                cond_combined = 'all of them'
                needs_brackets = False
                if low_scoring_strings > 0 and high_scoring_strings > 0:
                    if pe_conditions_add:
                        cond_combined = '{0} or {1}'.format(cond_hs, cond_ls)
                        needs_brackets = True
                    else:
                        cond_combined = '{0} and {1}'.format(cond_hs, cond_ls)
                elif low_scoring_strings > 0 and (not high_scoring_strings > 0):
                    cond_combined = '{0}'.format(cond_ls)
                elif not low_scoring_strings > 0 and high_scoring_strings > 0:
                    cond_combined = '{0}'.format(cond_hs)
                if opcodes_included:
                    cond_op = ' and all of ($op*)'
                if cond_op or needs_brackets:
                    subconditions.append('( {0}{1} )'.format(cond_combined, cond_op))
                else:
                    subconditions.append(cond_combined)
                if len(subconditions) == 1:
                    conditions.append(subconditions[0])
                elif len(subconditions) > 1:
                    conditions.append('( %s )' % ' or '.join(subconditions))
                condition_string = ' and\n      '.join(conditions)
                rule += '   condition:\n'
                rule += '      %s\n' % condition_string
                rule += '}\n\n'
                rules += rule
                rule_count += 1
            except Exception as e:
                traceback.print_exc()
    if not nosuper and (not args.inverse):
        rules += '/* Super Rules ------------------------------------------------------------- */\n\n'
        super_rule_names = []
        print('[+] Generating Super Rules ...')
        printed_combi = {}
        for super_rule in super_rules:
            try:
                rule = ''
                rule_name = ''
                file_list = []
                imphashes = Counter()
                for filePath in super_rule['files']:
                    (path, file) = os.path.split(filePath)
                    file_list.append(file)
                    fileBase = os.path.splitext(file)[0]
                    cleanedName = fileBase
                    rule_name += '_' + cleanedName
                    imphash = file_info[filePath]['imphash']
                    if imphash != '-' and imphash != '':
                        imphashes.update([imphash])
                if len(imphashes) == 1:
                    unique_imphash = imphashes.items()[0][0]
                    if unique_imphash in good_imphashes_db:
                        unique_imphash = ''
                rule_name = rule_name[:124]
                if rule_name not in super_rule_names:
                    rule_name = '%s_%s' % (rule_name, super_rule_count)
                super_rule_names.append(rule_name)
                file_listing = ', '.join(file_list)
                if re.search('^[0-9]', rule_name):
                    rule_name = 'sig_' + rule_name
                rule_name = re.sub('[^\\w]', '_', rule_name)
                if rule_name in printed_rules:
                    printed_combi[rule_name] += 1
                    rule_name = rule_name + '_' + str(printed_combi[rule_name])
                else:
                    printed_combi[rule_name] = 1
                rule += 'rule %s {\n' % rule_name
                rule += '   meta:\n'
                rule += '      description = ""%s - from files %s""\n' % (prefix, file_listing)
                rule += '      author = ""%s""\n' % args.a
                rule += '      reference = ""%s""\n' % reference
                rule += '      date = ""%s""\n' % get_timestamp_basic()
                for (i, filePath) in enumerate(super_rule['files']):
                    rule += '      hash%s = ""%s""\n' % (str(i + 1), file_info[filePath]['hash'])
                rule += '   strings:\n'
                if file_opcodes.get(filePath) is None:
                    tmp_file_opcodes = {}
                else:
                    tmp_file_opcodes = file_opcodes.get(filePath)
                (rule_strings, opcodes_included, string_rule_count, high_scoring_strings) = get_rule_strings(super_rule['strings'], tmp_file_opcodes)
                rule += rule_strings
                conditions = []
                file_info_super = {}
                for filePath in super_rule['files']:
                    file_info_super[filePath] = file_info[filePath]
                (condition_strings, pe_module_necessary_gen) = generate_general_condition(file_info_super)
                if pe_module_necessary_gen:
                    pe_module_necessary = True
                cond_op = ''
                cond_hs = ''
                cond_ls = ''
                low_scoring_strings = string_rule_count - high_scoring_strings
                if high_scoring_strings > 0:
                    cond_hs = '1 of ($x*)'
                if low_scoring_strings > 0:
                    if low_scoring_strings > 10:
                        if high_scoring_strings > 0:
                            cond_ls = '4 of them'
                        else:
                            cond_ls = '8 of them'
                    else:
                        cond_ls = 'all of them'
                cond_combined = 'all of them'
                if low_scoring_strings > 0 and high_scoring_strings > 0:
                    cond_combined = '{0} and {1}'.format(cond_hs, cond_ls)
                elif low_scoring_strings > 0 and (not high_scoring_strings > 0):
                    cond_combined = '{0}'.format(cond_ls)
                elif not low_scoring_strings > 0 and high_scoring_strings > 0:
                    cond_combined = '{0}'.format(cond_hs)
                if opcodes_included:
                    cond_op = ' and all of ($op*)'
                condition2 = '( {0} ){1}'.format(cond_combined, cond_op)
                conditions.append(' and '.join([condition_strings, condition2]))
                condition_pe = 'all of them'
                conditions.append(condition_pe)
                condition_string = '\n      ) or ( '.join(conditions)
                rule += '   condition:\n'
                rule += '      ( %s )\n' % condition_string
                rule += '}\n\n'
                rules += rule
                super_rule_count += 1
            except Exception as e:
                traceback.print_exc()
    try:
        if not args.noextras:
            if pe_module_necessary:
                fh.write('import ""pe""\n\n')
        if args.o:
            fh.write(rules)
    except Exception as e:
        traceback.print_exc()
    if args.inverse:
        print('[+] Generating inverse rules ...')
        inverse_rules = ''
        print('[+] Applying intelligent filters to string findings ...')
        for fileName in inverse_stats:
            print('[-] Filtering string set for %s ...' % fileName)
            string_set = inverse_stats[fileName]
            inverse_stats[fileName] = []
            inverse_stats[fileName] = filter_string_set(string_set)
            if fileName not in file_opcodes:
                file_opcodes[fileName] = {}
        fh.write('/* Inverse Rules ------------------------------------------------------------- */\n\n')
        for fileName in inverse_stats:
            try:
                rule = ''
                cleanedName = fileName.replace('.', '_')
                cleanedName += '_ANOMALY'
                if re.search('^[0-9]', cleanedName):
                    cleanedName = 'sig_' + cleanedName
                cleanedName = re.sub('[^\\w]', '_', cleanedName)
                if cleanedName in printed_rules:
                    printed_rules[cleanedName] += 1
                    cleanedName = cleanedName + '_' + str(printed_rules[cleanedName])
                else:
                    printed_rules[cleanedName] = 1
                rule += 'rule %s {\n' % cleanedName
                rule += '   meta:\n'
                rule += '      description = ""%s for anomaly detection - file %s""\n' % (prefix, fileName)
                rule += '      author = ""%s""\n' % args.a
                rule += '      reference = ""%s""\n' % reference
                rule += '      date = ""%s""\n' % get_timestamp_basic()
                for (i, hash) in enumerate(file_info[fileName]['hashes']):
                    rule += '      hash%s = ""%s""\n' % (str(i + 1), hash)
                rule += '   strings:\n'
                (rule_strings, opcodes_included, string_rule_count, high_scoring_strings) = get_rule_strings(inverse_stats[fileName], file_opcodes[fileName])
                rule += rule_strings
                folderNames = ''
                if not args.nodirname:
                    folderNames += 'and ( filepath matches /'
                    folderNames += '$/ or filepath matches /'.join(file_info[fileName]['folder_names'])
                    folderNames += '$/ )'
                condition = 'filename == ""%s"" %s and not ( all of them )' % (fileName, folderNames)
                rule += '   condition:\n'
                rule += '      %s\n' % condition
                rule += '}\n\n'
                inverse_rules += rule
            except Exception as e:
                traceback.print_exc()
        try:
            if args.o:
                fh.write(inverse_rules)
            inverse_rule_count += 1
        except Exception as e:
            traceback.print_exc()
    if args.o:
        try:
            fh.close()
        except Exception as e:
            traceback.print_exc()
    if args.debug:
        print(rules)
    return (rule_count, inverse_rule_count, super_rule_count)","for filePath in file_strings:
    print('[-] Filtering string set for %s ...' % filePath)
    string_set = file_strings[filePath]
    file_strings[filePath] = []
    file_strings[filePath] = filter_string_set(string_set)
    if filePath not in file_opcodes:
        file_opcodes[filePath] = []
    else:
        print('[-] Filtering opcode set for %s ...' % filePath)
    opcode_set = file_opcodes[filePath]
    file_opcodes[filePath] = []
    file_opcodes[filePath] = filter_opcode_set(opcode_set)",XXX,no_found,0,,,
yarGen,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yarGen/yarGen.py,https://github.com/Neo23x0/yarGen/tree/master//yarGen.py,,generate_rules$1044,"def generate_rules(file_strings, file_opcodes, super_rules, file_info, inverse_stats):
    if args.o:
        try:
            fh = open(args.o, 'w')
        except Exception as e:
            traceback.print_exc()
    general_info = '/*\n'
    general_info += '   YARA Rule Set\n'
    general_info += '   Author: {0}\n'.format(args.a)
    general_info += '   Date: {0}\n'.format(get_timestamp_basic())
    general_info += '   Identifier: {0}\n'.format(identifier)
    general_info += '   Reference: {0}\n'.format(reference)
    if args.l != '':
        general_info += '   License: {0}\n'.format(args.l)
    general_info += '*/\n\n'
    fh.write(general_info)
    if args.globalrule:
        (condition, pe_module_necessary) = generate_general_condition(file_info)
        if condition != '':
            global_rule = '/* Global Rule -------------------------------------------------------------- */\n'
            global_rule += '/* Will be evaluated first, speeds up scanning process, remove at will */\n\n'
            global_rule += 'global private rule gen_characteristics {\n'
            global_rule += '   condition:\n'
            global_rule += '      {0}\n'.format(condition)
            global_rule += '}\n\n'
            if args.o:
                fh.write(global_rule)
    rules = ''
    printed_rules = {}
    opcodes_to_add = []
    rule_count = 0
    inverse_rule_count = 0
    super_rule_count = 0
    pe_module_necessary = False
    if not args.inverse:
        print('[+] Generating Simple Rules ...')
        print('[-] Applying intelligent filters to string findings ...')
        for filePath in file_strings:
            print('[-] Filtering string set for %s ...' % filePath)
            string_set = file_strings[filePath]
            file_strings[filePath] = []
            file_strings[filePath] = filter_string_set(string_set)
            if filePath not in file_opcodes:
                file_opcodes[filePath] = []
            else:
                print('[-] Filtering opcode set for %s ...' % filePath)
            opcode_set = file_opcodes[filePath]
            file_opcodes[filePath] = []
            file_opcodes[filePath] = filter_opcode_set(opcode_set)
        fh.write('/* Rule Set ----------------------------------------------------------------- */\n\n')
        for filePath in file_strings:
            if len(file_strings[filePath]) == 0:
                print('[W] Not enough high scoring strings to create a rule. (Try -z 0 to reduce the min score or --opcodes to include opcodes) FILE: %s' % filePath)
                continue
            elif len(file_strings[filePath]) == 0 and len(file_opcodes[filePath]) == 0:
                print('[W] Not enough high scoring strings and opcodes to create a rule. (Try -z 0 to reduce the min score) FILE: %s' % filePath)
                continue
            try:
                rule = ''
                (path, file) = os.path.split(filePath)
                fileBase = os.path.splitext(file)[0]
                cleanedName = fileBase
                if len(fileBase) < 8:
                    cleanedName = path.split('\\')[-1:][0] + '_' + cleanedName
                if re.search('^[0-9]', cleanedName):
                    cleanedName = 'sig_' + cleanedName
                cleanedName = re.sub('[^\\w]', '_', cleanedName)
                if cleanedName in printed_rules:
                    printed_rules[cleanedName] += 1
                    cleanedName = cleanedName + '_' + str(printed_rules[cleanedName])
                else:
                    printed_rules[cleanedName] = 1
                rule += 'rule %s {\n' % cleanedName
                rule += '   meta:\n'
                rule += '      description = ""%s - file %s""\n' % (prefix, file)
                rule += '      author = ""%s""\n' % args.a
                rule += '      reference = ""%s""\n' % reference
                rule += '      date = ""%s""\n' % get_timestamp_basic()
                rule += '      hash1 = ""%s""\n' % file_info[filePath]['hash']
                rule += '   strings:\n'
                (rule_strings, opcodes_included, string_rule_count, high_scoring_strings) = get_rule_strings(file_strings[filePath], file_opcodes[filePath])
                rule += rule_strings
                if args.strings:
                    strings = get_strings(file_strings[filePath])
                    write_strings(filePath, strings, args.e, args.score)
                conditions = []
                subconditions = []
                condition_pe = []
                condition_pe_part1 = []
                condition_pe_part2 = []
                if not args.noextras and file_info[filePath]['magic'] == 'MZ':
                    if file_info[filePath]['imphash'] not in good_imphashes_db and file_info[filePath]['imphash'] != '':
                        imphash = file_info[filePath]['imphash']
                        comment = ''
                        if imphash in KNOWN_IMPHASHES:
                            comment = ' /* {0} */'.format(KNOWN_IMPHASHES[imphash])
                        condition_pe_part1.append('pe.imphash() == ""{0}""{1}'.format(imphash, comment))
                        pe_module_necessary = True
                    if file_info[filePath]['exports']:
                        e_count = 0
                        for export in file_info[filePath]['exports']:
                            if export not in good_exports_db:
                                condition_pe_part2.append('pe.exports(""{0}"")'.format(export))
                                e_count += 1
                                pe_module_necessary = True
                            if e_count > 5:
                                break
                basic_conditions = []
                if not args.nofilesize:
                    basic_conditions.insert(0, get_file_range(file_info[filePath]['size']))
                if file_info[filePath]['magic'] != '':
                    uint_string = get_uint_string(file_info[filePath]['magic'])
                    basic_conditions.insert(0, uint_string)
                if len(basic_conditions):
                    conditions.append(' and '.join(basic_conditions))
                pe_conditions_add = False
                if condition_pe_part1 or condition_pe_part2:
                    if len(condition_pe_part1) == 1:
                        condition_pe.append(condition_pe_part1[0])
                    elif len(condition_pe_part1) > 1:
                        condition_pe.append('( %s )' % ' or '.join(condition_pe_part1))
                    if len(condition_pe_part2) == 1:
                        condition_pe.append(condition_pe_part2[0])
                    elif len(condition_pe_part2) > 1:
                        condition_pe.append('( %s )' % ' and '.join(condition_pe_part2))
                    pe_conditions_add = True
                    subconditions.append(' and '.join(condition_pe))
                cond_op = ''
                cond_hs = ''
                cond_ls = ''
                low_scoring_strings = string_rule_count - high_scoring_strings
                if high_scoring_strings > 0:
                    cond_hs = '1 of ($x*)'
                if low_scoring_strings > 0:
                    if low_scoring_strings > 10:
                        if high_scoring_strings > 0:
                            cond_ls = '4 of them'
                        else:
                            cond_ls = '8 of them'
                    else:
                        cond_ls = 'all of them'
                cond_combined = 'all of them'
                needs_brackets = False
                if low_scoring_strings > 0 and high_scoring_strings > 0:
                    if pe_conditions_add:
                        cond_combined = '{0} or {1}'.format(cond_hs, cond_ls)
                        needs_brackets = True
                    else:
                        cond_combined = '{0} and {1}'.format(cond_hs, cond_ls)
                elif low_scoring_strings > 0 and (not high_scoring_strings > 0):
                    cond_combined = '{0}'.format(cond_ls)
                elif not low_scoring_strings > 0 and high_scoring_strings > 0:
                    cond_combined = '{0}'.format(cond_hs)
                if opcodes_included:
                    cond_op = ' and all of ($op*)'
                if cond_op or needs_brackets:
                    subconditions.append('( {0}{1} )'.format(cond_combined, cond_op))
                else:
                    subconditions.append(cond_combined)
                if len(subconditions) == 1:
                    conditions.append(subconditions[0])
                elif len(subconditions) > 1:
                    conditions.append('( %s )' % ' or '.join(subconditions))
                condition_string = ' and\n      '.join(conditions)
                rule += '   condition:\n'
                rule += '      %s\n' % condition_string
                rule += '}\n\n'
                rules += rule
                rule_count += 1
            except Exception as e:
                traceback.print_exc()
    if not nosuper and (not args.inverse):
        rules += '/* Super Rules ------------------------------------------------------------- */\n\n'
        super_rule_names = []
        print('[+] Generating Super Rules ...')
        printed_combi = {}
        for super_rule in super_rules:
            try:
                rule = ''
                rule_name = ''
                file_list = []
                imphashes = Counter()
                for filePath in super_rule['files']:
                    (path, file) = os.path.split(filePath)
                    file_list.append(file)
                    fileBase = os.path.splitext(file)[0]
                    cleanedName = fileBase
                    rule_name += '_' + cleanedName
                    imphash = file_info[filePath]['imphash']
                    if imphash != '-' and imphash != '':
                        imphashes.update([imphash])
                if len(imphashes) == 1:
                    unique_imphash = imphashes.items()[0][0]
                    if unique_imphash in good_imphashes_db:
                        unique_imphash = ''
                rule_name = rule_name[:124]
                if rule_name not in super_rule_names:
                    rule_name = '%s_%s' % (rule_name, super_rule_count)
                super_rule_names.append(rule_name)
                file_listing = ', '.join(file_list)
                if re.search('^[0-9]', rule_name):
                    rule_name = 'sig_' + rule_name
                rule_name = re.sub('[^\\w]', '_', rule_name)
                if rule_name in printed_rules:
                    printed_combi[rule_name] += 1
                    rule_name = rule_name + '_' + str(printed_combi[rule_name])
                else:
                    printed_combi[rule_name] = 1
                rule += 'rule %s {\n' % rule_name
                rule += '   meta:\n'
                rule += '      description = ""%s - from files %s""\n' % (prefix, file_listing)
                rule += '      author = ""%s""\n' % args.a
                rule += '      reference = ""%s""\n' % reference
                rule += '      date = ""%s""\n' % get_timestamp_basic()
                for (i, filePath) in enumerate(super_rule['files']):
                    rule += '      hash%s = ""%s""\n' % (str(i + 1), file_info[filePath]['hash'])
                rule += '   strings:\n'
                if file_opcodes.get(filePath) is None:
                    tmp_file_opcodes = {}
                else:
                    tmp_file_opcodes = file_opcodes.get(filePath)
                (rule_strings, opcodes_included, string_rule_count, high_scoring_strings) = get_rule_strings(super_rule['strings'], tmp_file_opcodes)
                rule += rule_strings
                conditions = []
                file_info_super = {}
                for filePath in super_rule['files']:
                    file_info_super[filePath] = file_info[filePath]
                (condition_strings, pe_module_necessary_gen) = generate_general_condition(file_info_super)
                if pe_module_necessary_gen:
                    pe_module_necessary = True
                cond_op = ''
                cond_hs = ''
                cond_ls = ''
                low_scoring_strings = string_rule_count - high_scoring_strings
                if high_scoring_strings > 0:
                    cond_hs = '1 of ($x*)'
                if low_scoring_strings > 0:
                    if low_scoring_strings > 10:
                        if high_scoring_strings > 0:
                            cond_ls = '4 of them'
                        else:
                            cond_ls = '8 of them'
                    else:
                        cond_ls = 'all of them'
                cond_combined = 'all of them'
                if low_scoring_strings > 0 and high_scoring_strings > 0:
                    cond_combined = '{0} and {1}'.format(cond_hs, cond_ls)
                elif low_scoring_strings > 0 and (not high_scoring_strings > 0):
                    cond_combined = '{0}'.format(cond_ls)
                elif not low_scoring_strings > 0 and high_scoring_strings > 0:
                    cond_combined = '{0}'.format(cond_hs)
                if opcodes_included:
                    cond_op = ' and all of ($op*)'
                condition2 = '( {0} ){1}'.format(cond_combined, cond_op)
                conditions.append(' and '.join([condition_strings, condition2]))
                condition_pe = 'all of them'
                conditions.append(condition_pe)
                condition_string = '\n      ) or ( '.join(conditions)
                rule += '   condition:\n'
                rule += '      ( %s )\n' % condition_string
                rule += '}\n\n'
                rules += rule
                super_rule_count += 1
            except Exception as e:
                traceback.print_exc()
    try:
        if not args.noextras:
            if pe_module_necessary:
                fh.write('import ""pe""\n\n')
        if args.o:
            fh.write(rules)
    except Exception as e:
        traceback.print_exc()
    if args.inverse:
        print('[+] Generating inverse rules ...')
        inverse_rules = ''
        print('[+] Applying intelligent filters to string findings ...')
        for fileName in inverse_stats:
            print('[-] Filtering string set for %s ...' % fileName)
            string_set = inverse_stats[fileName]
            inverse_stats[fileName] = []
            inverse_stats[fileName] = filter_string_set(string_set)
            if fileName not in file_opcodes:
                file_opcodes[fileName] = {}
        fh.write('/* Inverse Rules ------------------------------------------------------------- */\n\n')
        for fileName in inverse_stats:
            try:
                rule = ''
                cleanedName = fileName.replace('.', '_')
                cleanedName += '_ANOMALY'
                if re.search('^[0-9]', cleanedName):
                    cleanedName = 'sig_' + cleanedName
                cleanedName = re.sub('[^\\w]', '_', cleanedName)
                if cleanedName in printed_rules:
                    printed_rules[cleanedName] += 1
                    cleanedName = cleanedName + '_' + str(printed_rules[cleanedName])
                else:
                    printed_rules[cleanedName] = 1
                rule += 'rule %s {\n' % cleanedName
                rule += '   meta:\n'
                rule += '      description = ""%s for anomaly detection - file %s""\n' % (prefix, fileName)
                rule += '      author = ""%s""\n' % args.a
                rule += '      reference = ""%s""\n' % reference
                rule += '      date = ""%s""\n' % get_timestamp_basic()
                for (i, hash) in enumerate(file_info[fileName]['hashes']):
                    rule += '      hash%s = ""%s""\n' % (str(i + 1), hash)
                rule += '   strings:\n'
                (rule_strings, opcodes_included, string_rule_count, high_scoring_strings) = get_rule_strings(inverse_stats[fileName], file_opcodes[fileName])
                rule += rule_strings
                folderNames = ''
                if not args.nodirname:
                    folderNames += 'and ( filepath matches /'
                    folderNames += '$/ or filepath matches /'.join(file_info[fileName]['folder_names'])
                    folderNames += '$/ )'
                condition = 'filename == ""%s"" %s and not ( all of them )' % (fileName, folderNames)
                rule += '   condition:\n'
                rule += '      %s\n' % condition
                rule += '}\n\n'
                inverse_rules += rule
            except Exception as e:
                traceback.print_exc()
        try:
            if args.o:
                fh.write(inverse_rules)
            inverse_rule_count += 1
        except Exception as e:
            traceback.print_exc()
    if args.o:
        try:
            fh.close()
        except Exception as e:
            traceback.print_exc()
    if args.debug:
        print(rules)
    return (rule_count, inverse_rule_count, super_rule_count)","for filePath in super_rule['files']:
    file_info_super[filePath] = file_info[filePath]",XXX,no_found,0,,,
pdfx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pdfx/pdfx/libs/xmp.py,https://github.com/metachris/pdfx/tree/master/pdfx/libs/xmp.py,XmpParser,_parse_value$71,"def _parse_value(self, el):
    """""" Extract the metadata value from an element. """"""
    if el.find(RDF_NS + 'Bag') is not None:
        value = []
        for li in el.findall(RDF_NS + 'Bag/' + RDF_NS + 'li'):
            value.append(li.text)
    elif el.find(RDF_NS + 'Seq') is not None:
        value = []
        for li in el.findall(RDF_NS + 'Seq/' + RDF_NS + 'li'):
            value.append(li.text)
    elif el.find(RDF_NS + 'Alt') is not None:
        value = {}
        for li in el.findall(RDF_NS + 'Alt/' + RDF_NS + 'li'):
            value[li.get(XML_NS + 'lang')] = li.text
    else:
        value = el.text
    return value","for li in el.findall(RDF_NS + 'Alt/' + RDF_NS + 'li'):
    value[li.get(XML_NS + 'lang')] = li.text",XXX,no_found,0,,,
EasyMocap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyMocap/easymocap/mytools/file_utils.py,https://github.com/zju3dv/EasyMocap/tree/master/easymocap/mytools/file_utils.py,,select_nf$205,"def select_nf(params_all, nf):
    output = {}
    for key in ['poses', 'Rh', 'Th']:
        output[key] = params_all[key][nf:nf + 1, :]
    if 'expression' in params_all.keys():
        output['expression'] = params_all['expression'][nf:nf + 1, :]
    if params_all['shapes'].shape[0] == 1:
        output['shapes'] = params_all['shapes']
    else:
        output['shapes'] = params_all['shapes'][nf:nf + 1, :]
    return output","for key in ['poses', 'Rh', 'Th']:
    output[key] = params_all[key][nf:nf + 1, :]",XXX,no_found,,,,
tqsdk-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tqsdk-python/tqsdk/backtest.py,https://github.com/shinnytech/tqsdk-python/tree/master/tqsdk/backtest.py,TqBacktest,_gc_data$638,"def _gc_data(self):
    need_rangeset = {}
    for (ins, dur) in self._serials:
        if dur == 0:
            continue
        symbol_list = ins.split(',')
        for s in symbol_list:
            need_rangeset.setdefault((s, dur), [])
        main_serial = _get_obj(self._data, ['klines', symbol_list[0], str(dur)])
        main_serial_rangeset = self._sended_to_api.get((symbol_list[0], dur), [])
        if not main_serial_rangeset:
            continue
        last_id = main_serial_rangeset[-1][-1] - 1
        assert last_id > -1
        need_rangeset[symbol_list[0], dur] = _rangeset_range_union(need_rangeset[symbol_list[0], dur], (last_id - 8963, last_id + 1))
        for symbol in symbol_list[1:]:
            symbol_need_rangeset = []
            symbol_binding = main_serial.get('binding', {}).get(symbol, {})
            if symbol_binding:
                for i in range(last_id - 8963, last_id + 1):
                    other_id = symbol_binding.get(str(i))
                    if other_id:
                        symbol_need_rangeset = _rangeset_range_union(symbol_need_rangeset, (other_id, other_id + 1))
            if symbol_need_rangeset:
                need_rangeset[symbol, dur] = _rangeset_union(need_rangeset[symbol, dur], symbol_need_rangeset)
    gc_rangeset = {}
    for (key, rs) in self._sended_to_api.items():
        gc_rangeset[key] = _rangeset_difference(rs, need_rangeset.get(key, []))
    for (key, rs) in gc_rangeset.items():
        self._sended_to_api[key] = _rangeset_difference(self._sended_to_api[key], rs)
    gc_klines_diff = {}
    for ((symbol, dur), rs) in gc_rangeset.items():
        gc_klines_diff.setdefault(symbol, {})
        gc_klines_diff[symbol][str(dur)] = {'data': {}}
        serial = _get_obj(self._data, ['klines', symbol, str(dur)])
        serial_binding = serial.get('binding', None)
        if serial_binding:
            gc_klines_diff[symbol][str(dur)]['binding'] = {s: {} for s in serial_binding.keys()}
        for (start_id, end_id) in rs:
            for i in range(start_id, end_id):
                gc_klines_diff[symbol][str(dur)]['data'][str(i)] = None
                if serial_binding:
                    for (s, s_binding) in serial_binding.items():
                        gc_klines_diff[symbol][str(dur)]['binding'][s][str(i)] = None
    return {'klines': gc_klines_diff}","for (key, rs) in self._sended_to_api.items():
    gc_rangeset[key] = _rangeset_difference(rs, need_rangeset.get(key, []))",XXX,no_found,0,,,
tqsdk-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tqsdk-python/tqsdk/backtest.py,https://github.com/shinnytech/tqsdk-python/tree/master/tqsdk/backtest.py,TqBacktest,_gc_data$638,"def _gc_data(self):
    need_rangeset = {}
    for (ins, dur) in self._serials:
        if dur == 0:
            continue
        symbol_list = ins.split(',')
        for s in symbol_list:
            need_rangeset.setdefault((s, dur), [])
        main_serial = _get_obj(self._data, ['klines', symbol_list[0], str(dur)])
        main_serial_rangeset = self._sended_to_api.get((symbol_list[0], dur), [])
        if not main_serial_rangeset:
            continue
        last_id = main_serial_rangeset[-1][-1] - 1
        assert last_id > -1
        need_rangeset[symbol_list[0], dur] = _rangeset_range_union(need_rangeset[symbol_list[0], dur], (last_id - 8963, last_id + 1))
        for symbol in symbol_list[1:]:
            symbol_need_rangeset = []
            symbol_binding = main_serial.get('binding', {}).get(symbol, {})
            if symbol_binding:
                for i in range(last_id - 8963, last_id + 1):
                    other_id = symbol_binding.get(str(i))
                    if other_id:
                        symbol_need_rangeset = _rangeset_range_union(symbol_need_rangeset, (other_id, other_id + 1))
            if symbol_need_rangeset:
                need_rangeset[symbol, dur] = _rangeset_union(need_rangeset[symbol, dur], symbol_need_rangeset)
    gc_rangeset = {}
    for (key, rs) in self._sended_to_api.items():
        gc_rangeset[key] = _rangeset_difference(rs, need_rangeset.get(key, []))
    for (key, rs) in gc_rangeset.items():
        self._sended_to_api[key] = _rangeset_difference(self._sended_to_api[key], rs)
    gc_klines_diff = {}
    for ((symbol, dur), rs) in gc_rangeset.items():
        gc_klines_diff.setdefault(symbol, {})
        gc_klines_diff[symbol][str(dur)] = {'data': {}}
        serial = _get_obj(self._data, ['klines', symbol, str(dur)])
        serial_binding = serial.get('binding', None)
        if serial_binding:
            gc_klines_diff[symbol][str(dur)]['binding'] = {s: {} for s in serial_binding.keys()}
        for (start_id, end_id) in rs:
            for i in range(start_id, end_id):
                gc_klines_diff[symbol][str(dur)]['data'][str(i)] = None
                if serial_binding:
                    for (s, s_binding) in serial_binding.items():
                        gc_klines_diff[symbol][str(dur)]['binding'][s][str(i)] = None
    return {'klines': gc_klines_diff}","for ((symbol, dur), rs) in gc_rangeset.items():
    gc_klines_diff.setdefault(symbol, {})
    gc_klines_diff[symbol][str(dur)] = {'data': {}}
    serial = _get_obj(self._data, ['klines', symbol, str(dur)])
    serial_binding = serial.get('binding', None)
    if serial_binding:
        gc_klines_diff[symbol][str(dur)]['binding'] = {s: {} for s in serial_binding.keys()}
    for (start_id, end_id) in rs:
        for i in range(start_id, end_id):
            gc_klines_diff[symbol][str(dur)]['data'][str(i)] = None
            if serial_binding:
                for (s, s_binding) in serial_binding.items():
                    gc_klines_diff[symbol][str(dur)]['binding'][s][str(i)] = None",XXX,no_found,0,,,
schema,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/schema/schema.py,https://github.com/keleshev/schema/tree/master//schema.py,Schema,_json_schema$484,"def _json_schema(schema, is_main_schema=True, description=None, allow_reference=True):
    Schema = self.__class__

    def _create_or_use_ref(return_dict):
        """"""If not already seen, return the provided part of the schema unchanged.
                If already seen, give an id to the already seen dict and return a reference to the previous part
                of the schema instead.
                """"""
        if not use_refs or is_main_schema:
            return return_schema
        hashed = hash(repr(sorted(return_dict.items())))
        if hashed not in seen:
            seen[hashed] = return_dict
            return return_dict
        else:
            id_str = '#' + str(hashed)
            seen[hashed]['$id'] = id_str
            return {'$ref': id_str}

    def _get_type_name(python_type):
        """"""Return the JSON schema name for a Python type""""""
        if python_type == str:
            return 'string'
        elif python_type == int:
            return 'integer'
        elif python_type == float:
            return 'number'
        elif python_type == bool:
            return 'boolean'
        elif python_type == list:
            return 'array'
        elif python_type == dict:
            return 'object'
        return 'string'

    def _to_json_type(value):
        """"""Attempt to convert a constant value (for ""const"" and ""default"") to a JSON serializable value""""""
        if value is None or type(value) in (str, int, float, bool, list, dict):
            return value
        if type(value) in (tuple, set, frozenset):
            return list(value)
        if isinstance(value, Literal):
            return value.schema
        return str(value)

    def _to_schema(s, ignore_extra_keys):
        if not isinstance(s, Schema):
            return Schema(s, ignore_extra_keys=ignore_extra_keys)
        return s
    s = schema.schema
    i = schema.ignore_extra_keys
    flavor = _priority(s)
    return_schema = {}
    return_description = description or schema.description
    if return_description:
        return_schema['description'] = return_description
    if allow_reference and schema.as_reference:
        if schema.name not in definitions_by_name:
            definitions_by_name[schema.name] = {}
            definitions_by_name[schema.name] = _json_schema(schema, is_main_schema=False, allow_reference=False)
        return_schema['$ref'] = '#/definitions/' + schema.name
    elif flavor == TYPE:
        return_schema['type'] = _get_type_name(s)
    elif flavor == ITERABLE:
        return_schema['type'] = 'array'
        if len(s) == 1:
            return_schema['items'] = _json_schema(_to_schema(s[0], i), is_main_schema=False)
        elif len(s) > 1:
            return_schema['items'] = _json_schema(Schema(Or(*s)), is_main_schema=False)
    elif isinstance(s, Or):
        if all((priority == COMPARABLE for priority in [_priority(value) for value in s.args])):
            or_values = [str(s) if isinstance(s, Literal) else s for s in s.args]
            if len(or_values) == 1:
                return_schema['const'] = _to_json_type(or_values[0])
                return return_schema
            return_schema['enum'] = or_values
        else:
            any_of_values = []
            for or_key in s.args:
                new_value = _json_schema(_to_schema(or_key, i), is_main_schema=False)
                if new_value != {} and new_value not in any_of_values:
                    any_of_values.append(new_value)
            if len(any_of_values) == 1:
                return_schema.update(any_of_values[0])
            else:
                return_schema['anyOf'] = any_of_values
    elif isinstance(s, And):
        all_of_values = []
        for and_key in s.args:
            new_value = _json_schema(_to_schema(and_key, i), is_main_schema=False)
            if new_value != {} and new_value not in all_of_values:
                all_of_values.append(new_value)
        if len(all_of_values) == 1:
            return_schema.update(all_of_values[0])
        else:
            return_schema['allOf'] = all_of_values
    elif flavor == COMPARABLE:
        return_schema['const'] = _to_json_type(s)
    elif flavor == VALIDATOR and type(s) == Regex:
        return_schema['type'] = 'string'
        return_schema['pattern'] = s.pattern_str
    else:
        if flavor != DICT:
            return return_schema
        required_keys = []
        expanded_schema = {}
        additional_properties = i
        for key in s:
            if isinstance(key, Hook):
                continue

            def _key_allows_additional_properties(key):
                """"""Check if a key is broad enough to allow additional properties""""""
                if isinstance(key, Optional):
                    return _key_allows_additional_properties(key.schema)
                return key == str or key == object

            def _get_key_description(key):
                """"""Get the description associated to a key (as specified in a Literal object). Return None if not a Literal""""""
                if isinstance(key, Optional):
                    return _get_key_description(key.schema)
                if isinstance(key, Literal):
                    return key.description
                return None

            def _get_key_name(key):
                """"""Get the name of a key (as specified in a Literal object). Return the key unchanged if not a Literal""""""
                if isinstance(key, Optional):
                    return _get_key_name(key.schema)
                if isinstance(key, Literal):
                    return key.schema
                return key
            additional_properties = additional_properties or _key_allows_additional_properties(key)
            sub_schema = _to_schema(s[key], ignore_extra_keys=i)
            key_name = _get_key_name(key)
            if isinstance(key_name, str):
                if not isinstance(key, Optional):
                    required_keys.append(key_name)
                expanded_schema[key_name] = _json_schema(sub_schema, is_main_schema=False, description=_get_key_description(key))
                if isinstance(key, Optional) and hasattr(key, 'default'):
                    expanded_schema[key_name]['default'] = _to_json_type(_invoke_with_optional_kwargs(key.default, **kwargs) if callable(key.default) else key.default)
            elif isinstance(key_name, Or):
                for or_key in key_name.args:
                    expanded_schema[_get_key_name(or_key)] = _json_schema(sub_schema, is_main_schema=False, description=_get_key_description(or_key))
        return_schema.update({'type': 'object', 'properties': expanded_schema, 'required': required_keys, 'additionalProperties': additional_properties})
    if is_main_schema:
        return_schema.update({'$id': schema_id, '$schema': 'http://json-schema.org/draft-07/schema#'})
        if self._name:
            return_schema['title'] = self._name
        if definitions_by_name:
            return_schema['definitions'] = {}
            for (definition_name, definition) in definitions_by_name.items():
                return_schema['definitions'][definition_name] = definition
    return _create_or_use_ref(return_schema)","for (definition_name, definition) in definitions_by_name.items():
    return_schema['definitions'][definition_name] = definition",XXX,no_found,0,,,
SegFormer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SegFormer/mmseg/datasets/custom.py,https://github.com/NVlabs/SegFormer/tree/master/mmseg/datasets/custom.py,CustomDataset,evaluate$306,"def evaluate(self, results, metric='mIoU', logger=None, efficient_test=False, **kwargs):
    """"""Evaluate the dataset.

        Args:
            results (list): Testing results of the dataset.
            metric (str | list[str]): Metrics to be evaluated. 'mIoU' and
                'mDice' are supported.
            logger (logging.Logger | None | str): Logger used for printing
                related information during evaluation. Default: None.

        Returns:
            dict[str, float]: Default metrics.
        """"""
    if isinstance(metric, str):
        metric = [metric]
    allowed_metrics = ['mIoU', 'mDice']
    if not set(metric).issubset(set(allowed_metrics)):
        raise KeyError('metric {} is not supported'.format(metric))
    eval_results = {}
    gt_seg_maps = self.get_gt_seg_maps(efficient_test)
    if self.CLASSES is None:
        num_classes = len(reduce(np.union1d, [np.unique(_) for _ in gt_seg_maps]))
    else:
        num_classes = len(self.CLASSES)
    ret_metrics = eval_metrics(results, gt_seg_maps, num_classes, self.ignore_index, metric, label_map=self.label_map, reduce_zero_label=self.reduce_zero_label)
    class_table_data = [['Class'] + [m[1:] for m in metric] + ['Acc']]
    if self.CLASSES is None:
        class_names = tuple(range(num_classes))
    else:
        class_names = self.CLASSES
    ret_metrics_round = [np.round(ret_metric * 100, 2) for ret_metric in ret_metrics]
    for i in range(num_classes):
        class_table_data.append([class_names[i]] + [m[i] for m in ret_metrics_round[2:]] + [ret_metrics_round[1][i]])
    summary_table_data = [['Scope'] + ['m' + head for head in class_table_data[0][1:]] + ['aAcc']]
    ret_metrics_mean = [np.round(np.nanmean(ret_metric) * 100, 2) for ret_metric in ret_metrics]
    summary_table_data.append(['global'] + ret_metrics_mean[2:] + [ret_metrics_mean[1]] + [ret_metrics_mean[0]])
    print_log('per class results:', logger)
    table = AsciiTable(class_table_data)
    print_log('\n' + table.table, logger=logger)
    print_log('Summary:', logger)
    table = AsciiTable(summary_table_data)
    print_log('\n' + table.table, logger=logger)
    for i in range(1, len(summary_table_data[0])):
        eval_results[summary_table_data[0][i]] = summary_table_data[1][i] / 100.0
    if mmcv.is_list_of(results, str):
        for file_name in results:
            os.remove(file_name)
    return eval_results","for i in range(1, len(summary_table_data[0])):
    eval_results[summary_table_data[0][i]] = summary_table_data[1][i] / 100.0",XXX,no_found,0,,,
RootTheBox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RootTheBox/libs/BotManager.py,https://github.com/moloch--/RootTheBox/tree/master/libs/BotManager.py,BotManager,count_all_teams$137,"def count_all_teams(self):
    from models.Team import Team
    teams = Team.all()
    botcount = {}
    for team in teams:
        botcount[team.uuid] = 0
    for bot in self.botdb.query(Bot).all():
        botcount[bot.team_uuid] += 1
    return botcount","for team in teams:
    botcount[team.uuid] = 0",XXX,no_found,,,,
python-devtools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-devtools/devtools/debug.py,https://github.com/samuelcolvin/python-devtools/tree/master/devtools/debug.py,Debug,_process_args$200,"def _process_args(self, ex: 'Any', args: 'Any', kwargs: 'Any') -> 'Generator[DebugArgument, None, None]':
    import ast
    func_ast = ex.node
    atok = ex.source.asttokens()
    for (arg, ast_arg) in zip(args, func_ast.args):
        if isinstance(ast_arg, ast.Name):
            yield self.output_class.arg_class(arg, name=ast_arg.id)
        else:
            name = ' '.join(map(str.strip, atok.get_text(ast_arg).splitlines()))
            yield self.output_class.arg_class(arg, name=name)
    kw_arg_names = {}
    for kw in func_ast.keywords:
        if isinstance(kw.value, ast.Name):
            kw_arg_names[kw.arg] = kw.value.id
    for (name, value) in kwargs.items():
        yield self.output_class.arg_class(value, name=name, variable=kw_arg_names.get(name))","for kw in func_ast.keywords:
    if isinstance(kw.value, ast.Name):
        kw_arg_names[kw.arg] = kw.value.id",XXX,no_found,,,,
virt-manager,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/virt-manager/virtManager/createvm.py,https://github.com/virt-manager/virt-manager/tree/master/virtManager/createvm.py,vmmCreateVM,_start_install$1948,"def _start_install(self, guest, installer):
    """"""
        Launch the async job to start the install
        """"""
    bootstrap_args = {}
    if guest.os.is_container() and self._get_config_oscontainer_bootstrap():
        bootstrap_arg_keys = {'src': self._get_config_oscontainer_source_url, 'dest': self.widget('install-oscontainer-fs').get_text, 'user': self._get_config_oscontainer_source_username, 'passwd': self._get_config_oscontainer_source_password, 'insecure': self._get_config_oscontainer_isecure, 'root_password': self._get_config_oscontainer_root_password}
        for (key, getter) in bootstrap_arg_keys.items():
            bootstrap_args[key] = getter()
    parentobj = self._customize_window or self
    progWin = vmmAsyncJob(self._do_async_install, [guest, installer, bootstrap_args], self._install_finished_cb, [guest, parentobj], _('Creating Virtual Machine'), _('The virtual machine is now being created. Allocation of disk storage and retrieval of the installation images may take a few minutes to complete.'), parentobj.topwin)
    progWin.run()","for (key, getter) in bootstrap_arg_keys.items():
    bootstrap_args[key] = getter()",XXX,no_found,,,,
mmcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmcv/mmcv/runner/hooks/logger/base.py,https://github.com/open-mmlab/mmcv/tree/master/mmcv/runner/hooks/logger/base.py,LoggerHook,get_lr_tags$92,"def get_lr_tags(self, runner):
    tags = {}
    lrs = runner.current_lr()
    if isinstance(lrs, dict):
        for (name, value) in lrs.items():
            tags[f'learning_rate/{name}'] = value[0]
    else:
        tags['learning_rate'] = lrs[0]
    return tags","for (name, value) in lrs.items():
    tags[f'learning_rate/{name}'] = value[0]",XXX,no_found,,,,
flax,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flax/examples/wmt/input_pipeline.py,https://github.com/google/flax/tree/master/examples/wmt/input_pipeline.py,,_pack_with_tf_ops$149,"def _pack_with_tf_ops(dataset: tf.data.Dataset, keys: List[str], key2length: Dict[str, int]) -> tf.data.Dataset:
    """"""Helper-function for packing a dataset which has already been batched.

  Helper for pack_dataset()  Uses tf.while_loop.

  Args:
    dataset: a dataset containing padded batches of examples.
    keys: a list of strings
    key2length: an dict from feature-key to integer

  Returns:
    a dataset.
  """"""
    empty_example = {}
    for k in keys:
        empty_example[k] = tf.zeros([0], dtype=tf.int32)
        empty_example[k + '_position'] = tf.zeros([0], dtype=tf.int32)
    keys_etc = empty_example.keys()

    def write_packed_example(partial, outputs):
        new_partial = empty_example.copy()
        new_outputs = {}
        for k in keys_etc:
            new_outputs[k] = outputs[k].write(outputs[k].size(), tf.pad(partial[k], [[0, key2length[k] - tf.size(partial[k])]]))
        return (new_partial, new_outputs)

    def map_fn(x):
        """"""Internal function to flat_map over.

    Consumes a batch of input examples and produces a variable number of output
    examples.
    Args:
      x: a single example

    Returns:
      a tf.data.Dataset
    """"""
        partial = empty_example.copy()
        i = tf.zeros([], dtype=tf.int32)
        dynamic_batch_size = tf.shape(x[keys[0]])[0]
        outputs = {}
        for k in keys:
            outputs[k] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])
            outputs[k + '_position'] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])

        def body_fn(i, partial, outputs):
            """"""Body function for while_loop.

      Args:
        i: integer scalar
        partial: dictionary of Tensor (partially-constructed example)
        outputs: dictionary of TensorArray

      Returns:
        A triple containing the new values of the inputs.
      """"""
            can_append = True
            one_example = {}
            for k in keys:
                val = tf.cast(x[k][i], tf.int32)
                val = val[:tf.reduce_sum(tf.cast(tf.not_equal(val, 0), tf.int32))]
                one_example[k] = val
            for k in keys:
                can_append = tf.logical_and(can_append, tf.less_equal(tf.size(partial[k]) + tf.size(one_example[k]), key2length[k]))

            def false_fn():
                return write_packed_example(partial, outputs)

            def true_fn():
                return (partial, outputs)
            (partial, outputs) = tf.cond(can_append, true_fn, false_fn)
            new_partial = {}
            for k in keys:
                new_seq = one_example[k][:key2length[k]]
                new_seq_len = tf.size(new_seq)
                new_partial[k] = tf.concat([partial[k], new_seq], 0)
                new_partial[k + '_position'] = tf.concat([partial[k + '_position'], tf.range(new_seq_len)], 0)
            partial = new_partial
            return (i + 1, partial, outputs)
        (i, partial, outputs) = tf.while_loop(cond=lambda *_: True, body=body_fn, loop_vars=(i, partial, outputs), shape_invariants=(tf.TensorShape([]), {k: tf.TensorShape([None]) for k in keys_etc}, {k: tf.TensorShape(None) for k in keys_etc}), maximum_iterations=dynamic_batch_size)
        (_, outputs) = write_packed_example(partial, outputs)
        packed = {k: outputs[k].stack() for k in keys_etc}
        for k in keys:
            packed[k + '_segmentation'] = tf.cumsum(tf.cast(tf.equal(packed[k + '_position'], 0), tf.int32), axis=1) * tf.cast(tf.not_equal(packed[k], 0), tf.int32)
        return packed
    dataset = dataset.map(map_fn, num_parallel_calls=AUTOTUNE)
    return dataset.unbatch()","for k in keys:
    empty_example[k] = tf.zeros([0], dtype=tf.int32)
    empty_example[k + '_position'] = tf.zeros([0], dtype=tf.int32)",XXX,no_found,,,,
flax,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flax/examples/wmt/input_pipeline.py,https://github.com/google/flax/tree/master/examples/wmt/input_pipeline.py,,_pack_with_tf_ops$149,"def _pack_with_tf_ops(dataset: tf.data.Dataset, keys: List[str], key2length: Dict[str, int]) -> tf.data.Dataset:
    """"""Helper-function for packing a dataset which has already been batched.

  Helper for pack_dataset()  Uses tf.while_loop.

  Args:
    dataset: a dataset containing padded batches of examples.
    keys: a list of strings
    key2length: an dict from feature-key to integer

  Returns:
    a dataset.
  """"""
    empty_example = {}
    for k in keys:
        empty_example[k] = tf.zeros([0], dtype=tf.int32)
        empty_example[k + '_position'] = tf.zeros([0], dtype=tf.int32)
    keys_etc = empty_example.keys()

    def write_packed_example(partial, outputs):
        new_partial = empty_example.copy()
        new_outputs = {}
        for k in keys_etc:
            new_outputs[k] = outputs[k].write(outputs[k].size(), tf.pad(partial[k], [[0, key2length[k] - tf.size(partial[k])]]))
        return (new_partial, new_outputs)

    def map_fn(x):
        """"""Internal function to flat_map over.

    Consumes a batch of input examples and produces a variable number of output
    examples.
    Args:
      x: a single example

    Returns:
      a tf.data.Dataset
    """"""
        partial = empty_example.copy()
        i = tf.zeros([], dtype=tf.int32)
        dynamic_batch_size = tf.shape(x[keys[0]])[0]
        outputs = {}
        for k in keys:
            outputs[k] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])
            outputs[k + '_position'] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])

        def body_fn(i, partial, outputs):
            """"""Body function for while_loop.

      Args:
        i: integer scalar
        partial: dictionary of Tensor (partially-constructed example)
        outputs: dictionary of TensorArray

      Returns:
        A triple containing the new values of the inputs.
      """"""
            can_append = True
            one_example = {}
            for k in keys:
                val = tf.cast(x[k][i], tf.int32)
                val = val[:tf.reduce_sum(tf.cast(tf.not_equal(val, 0), tf.int32))]
                one_example[k] = val
            for k in keys:
                can_append = tf.logical_and(can_append, tf.less_equal(tf.size(partial[k]) + tf.size(one_example[k]), key2length[k]))

            def false_fn():
                return write_packed_example(partial, outputs)

            def true_fn():
                return (partial, outputs)
            (partial, outputs) = tf.cond(can_append, true_fn, false_fn)
            new_partial = {}
            for k in keys:
                new_seq = one_example[k][:key2length[k]]
                new_seq_len = tf.size(new_seq)
                new_partial[k] = tf.concat([partial[k], new_seq], 0)
                new_partial[k + '_position'] = tf.concat([partial[k + '_position'], tf.range(new_seq_len)], 0)
            partial = new_partial
            return (i + 1, partial, outputs)
        (i, partial, outputs) = tf.while_loop(cond=lambda *_: True, body=body_fn, loop_vars=(i, partial, outputs), shape_invariants=(tf.TensorShape([]), {k: tf.TensorShape([None]) for k in keys_etc}, {k: tf.TensorShape(None) for k in keys_etc}), maximum_iterations=dynamic_batch_size)
        (_, outputs) = write_packed_example(partial, outputs)
        packed = {k: outputs[k].stack() for k in keys_etc}
        for k in keys:
            packed[k + '_segmentation'] = tf.cumsum(tf.cast(tf.equal(packed[k + '_position'], 0), tf.int32), axis=1) * tf.cast(tf.not_equal(packed[k], 0), tf.int32)
        return packed
    dataset = dataset.map(map_fn, num_parallel_calls=AUTOTUNE)
    return dataset.unbatch()","for k in keys:
    outputs[k] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])
    outputs[k + '_position'] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])",XXX,no_found,0,,,
flax,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flax/examples/wmt/input_pipeline.py,https://github.com/google/flax/tree/master/examples/wmt/input_pipeline.py,,_pack_with_tf_ops$149,"def _pack_with_tf_ops(dataset: tf.data.Dataset, keys: List[str], key2length: Dict[str, int]) -> tf.data.Dataset:
    """"""Helper-function for packing a dataset which has already been batched.

  Helper for pack_dataset()  Uses tf.while_loop.

  Args:
    dataset: a dataset containing padded batches of examples.
    keys: a list of strings
    key2length: an dict from feature-key to integer

  Returns:
    a dataset.
  """"""
    empty_example = {}
    for k in keys:
        empty_example[k] = tf.zeros([0], dtype=tf.int32)
        empty_example[k + '_position'] = tf.zeros([0], dtype=tf.int32)
    keys_etc = empty_example.keys()

    def write_packed_example(partial, outputs):
        new_partial = empty_example.copy()
        new_outputs = {}
        for k in keys_etc:
            new_outputs[k] = outputs[k].write(outputs[k].size(), tf.pad(partial[k], [[0, key2length[k] - tf.size(partial[k])]]))
        return (new_partial, new_outputs)

    def map_fn(x):
        """"""Internal function to flat_map over.

    Consumes a batch of input examples and produces a variable number of output
    examples.
    Args:
      x: a single example

    Returns:
      a tf.data.Dataset
    """"""
        partial = empty_example.copy()
        i = tf.zeros([], dtype=tf.int32)
        dynamic_batch_size = tf.shape(x[keys[0]])[0]
        outputs = {}
        for k in keys:
            outputs[k] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])
            outputs[k + '_position'] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])

        def body_fn(i, partial, outputs):
            """"""Body function for while_loop.

      Args:
        i: integer scalar
        partial: dictionary of Tensor (partially-constructed example)
        outputs: dictionary of TensorArray

      Returns:
        A triple containing the new values of the inputs.
      """"""
            can_append = True
            one_example = {}
            for k in keys:
                val = tf.cast(x[k][i], tf.int32)
                val = val[:tf.reduce_sum(tf.cast(tf.not_equal(val, 0), tf.int32))]
                one_example[k] = val
            for k in keys:
                can_append = tf.logical_and(can_append, tf.less_equal(tf.size(partial[k]) + tf.size(one_example[k]), key2length[k]))

            def false_fn():
                return write_packed_example(partial, outputs)

            def true_fn():
                return (partial, outputs)
            (partial, outputs) = tf.cond(can_append, true_fn, false_fn)
            new_partial = {}
            for k in keys:
                new_seq = one_example[k][:key2length[k]]
                new_seq_len = tf.size(new_seq)
                new_partial[k] = tf.concat([partial[k], new_seq], 0)
                new_partial[k + '_position'] = tf.concat([partial[k + '_position'], tf.range(new_seq_len)], 0)
            partial = new_partial
            return (i + 1, partial, outputs)
        (i, partial, outputs) = tf.while_loop(cond=lambda *_: True, body=body_fn, loop_vars=(i, partial, outputs), shape_invariants=(tf.TensorShape([]), {k: tf.TensorShape([None]) for k in keys_etc}, {k: tf.TensorShape(None) for k in keys_etc}), maximum_iterations=dynamic_batch_size)
        (_, outputs) = write_packed_example(partial, outputs)
        packed = {k: outputs[k].stack() for k in keys_etc}
        for k in keys:
            packed[k + '_segmentation'] = tf.cumsum(tf.cast(tf.equal(packed[k + '_position'], 0), tf.int32), axis=1) * tf.cast(tf.not_equal(packed[k], 0), tf.int32)
        return packed
    dataset = dataset.map(map_fn, num_parallel_calls=AUTOTUNE)
    return dataset.unbatch()","for k in keys:
    val = tf.cast(x[k][i], tf.int32)
    val = val[:tf.reduce_sum(tf.cast(tf.not_equal(val, 0), tf.int32))]
    one_example[k] = val",XXX,no_found,0,,,
flax,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flax/examples/wmt/input_pipeline.py,https://github.com/google/flax/tree/master/examples/wmt/input_pipeline.py,,_pack_with_tf_ops$149,"def _pack_with_tf_ops(dataset: tf.data.Dataset, keys: List[str], key2length: Dict[str, int]) -> tf.data.Dataset:
    """"""Helper-function for packing a dataset which has already been batched.

  Helper for pack_dataset()  Uses tf.while_loop.

  Args:
    dataset: a dataset containing padded batches of examples.
    keys: a list of strings
    key2length: an dict from feature-key to integer

  Returns:
    a dataset.
  """"""
    empty_example = {}
    for k in keys:
        empty_example[k] = tf.zeros([0], dtype=tf.int32)
        empty_example[k + '_position'] = tf.zeros([0], dtype=tf.int32)
    keys_etc = empty_example.keys()

    def write_packed_example(partial, outputs):
        new_partial = empty_example.copy()
        new_outputs = {}
        for k in keys_etc:
            new_outputs[k] = outputs[k].write(outputs[k].size(), tf.pad(partial[k], [[0, key2length[k] - tf.size(partial[k])]]))
        return (new_partial, new_outputs)

    def map_fn(x):
        """"""Internal function to flat_map over.

    Consumes a batch of input examples and produces a variable number of output
    examples.
    Args:
      x: a single example

    Returns:
      a tf.data.Dataset
    """"""
        partial = empty_example.copy()
        i = tf.zeros([], dtype=tf.int32)
        dynamic_batch_size = tf.shape(x[keys[0]])[0]
        outputs = {}
        for k in keys:
            outputs[k] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])
            outputs[k + '_position'] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])

        def body_fn(i, partial, outputs):
            """"""Body function for while_loop.

      Args:
        i: integer scalar
        partial: dictionary of Tensor (partially-constructed example)
        outputs: dictionary of TensorArray

      Returns:
        A triple containing the new values of the inputs.
      """"""
            can_append = True
            one_example = {}
            for k in keys:
                val = tf.cast(x[k][i], tf.int32)
                val = val[:tf.reduce_sum(tf.cast(tf.not_equal(val, 0), tf.int32))]
                one_example[k] = val
            for k in keys:
                can_append = tf.logical_and(can_append, tf.less_equal(tf.size(partial[k]) + tf.size(one_example[k]), key2length[k]))

            def false_fn():
                return write_packed_example(partial, outputs)

            def true_fn():
                return (partial, outputs)
            (partial, outputs) = tf.cond(can_append, true_fn, false_fn)
            new_partial = {}
            for k in keys:
                new_seq = one_example[k][:key2length[k]]
                new_seq_len = tf.size(new_seq)
                new_partial[k] = tf.concat([partial[k], new_seq], 0)
                new_partial[k + '_position'] = tf.concat([partial[k + '_position'], tf.range(new_seq_len)], 0)
            partial = new_partial
            return (i + 1, partial, outputs)
        (i, partial, outputs) = tf.while_loop(cond=lambda *_: True, body=body_fn, loop_vars=(i, partial, outputs), shape_invariants=(tf.TensorShape([]), {k: tf.TensorShape([None]) for k in keys_etc}, {k: tf.TensorShape(None) for k in keys_etc}), maximum_iterations=dynamic_batch_size)
        (_, outputs) = write_packed_example(partial, outputs)
        packed = {k: outputs[k].stack() for k in keys_etc}
        for k in keys:
            packed[k + '_segmentation'] = tf.cumsum(tf.cast(tf.equal(packed[k + '_position'], 0), tf.int32), axis=1) * tf.cast(tf.not_equal(packed[k], 0), tf.int32)
        return packed
    dataset = dataset.map(map_fn, num_parallel_calls=AUTOTUNE)
    return dataset.unbatch()","for k in keys:
    new_seq = one_example[k][:key2length[k]]
    new_seq_len = tf.size(new_seq)
    new_partial[k] = tf.concat([partial[k], new_seq], 0)
    new_partial[k + '_position'] = tf.concat([partial[k + '_position'], tf.range(new_seq_len)], 0)",XXX,no_found,0,,,
nnFormer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nnFormer/nnformer/evaluation/model_selection/rank_candidates_cascade.py,https://github.com/282857341/nnFormer/tree/master/nnformer/evaluation/model_selection/rank_candidates_cascade.py,,if_main_my$20,"if __name__ == '__main__':
    summary_files_dir = join(network_training_output_dir, 'summary_jsons_fold0_new')
    output_file = join(network_training_output_dir, 'summary_cascade.csv')
    folds = (0,)
    folds_str = ''
    for f in folds:
        folds_str += str(f)
    plans = 'nnFormerPlansv2.1'
    overwrite_plans = {'nnFormerTrainerCascadeFullRes': ['nnFormerPlans']}
    trainers = ['nnFormerTrainerCascadeFullRes', 'nnFormerTrainerV2CascadeFullRes_EducatedGuess', 'nnFormerTrainerV2CascadeFullRes_EducatedGuess2', 'nnFormerTrainerV2CascadeFullRes_EducatedGuess3', 'nnFormerTrainerV2CascadeFullRes_lowerLR', 'nnFormerTrainerV2CascadeFullRes', 'nnFormerTrainerV2CascadeFullRes_noConnComp', 'nnFormerTrainerV2CascadeFullRes_shorter_lowerLR', 'nnFormerTrainerV2CascadeFullRes_shorter', 'nnFormerTrainerV2CascadeFullRes_smallerBinStrel']
    datasets = {'Task003_Liver': ('3d_cascade_fullres',), 'Task006_Lung': ('3d_cascade_fullres',), 'Task007_Pancreas': ('3d_cascade_fullres',), 'Task008_HepaticVessel': ('3d_cascade_fullres',), 'Task009_Spleen': ('3d_cascade_fullres',), 'Task010_Colon': ('3d_cascade_fullres',), 'Task017_AbdominalOrganSegmentation': ('3d_cascade_fullres',), 'Task048_KiTS_clean': ('3d_cascade_fullres',), 'Task055_SegTHOR': ('3d_cascade_fullres',), 'Task056_VerSe': ('3d_cascade_fullres',)}
    expected_validation_folder = 'validation_raw'
    alternative_validation_folder = 'validation'
    alternative_alternative_validation_folder = 'validation_tiledTrue_doMirror_True'
    interested_in = 'mean'
    result_per_dataset = {}
    for d in datasets:
        result_per_dataset[d] = {}
        for c in datasets[d]:
            result_per_dataset[d][c] = []
    valid_trainers = []
    all_trainers = []
    with open(output_file, 'w') as f:
        f.write('trainer,')
        for t in datasets.keys():
            s = t[4:7]
            for c in datasets[t]:
                s1 = s + '_' + c[3]
                f.write('%s,' % s1)
        f.write('\n')
        for trainer in trainers:
            trainer_plans = [plans]
            if trainer in overwrite_plans.keys():
                trainer_plans = overwrite_plans[trainer]
            result_per_dataset_here = {}
            for d in datasets:
                result_per_dataset_here[d] = {}
            for p in trainer_plans:
                name = '%s__%s' % (trainer, p)
                all_present = True
                all_trainers.append(name)
                f.write('%s,' % name)
                for dataset in datasets.keys():
                    for configuration in datasets[dataset]:
                        summary_file = join(summary_files_dir, '%s__%s__%s__%s__%s__%s.json' % (dataset, configuration, trainer, p, expected_validation_folder, folds_str))
                        if not isfile(summary_file):
                            summary_file = join(summary_files_dir, '%s__%s__%s__%s__%s__%s.json' % (dataset, configuration, trainer, p, alternative_validation_folder, folds_str))
                            if not isfile(summary_file):
                                summary_file = join(summary_files_dir, '%s__%s__%s__%s__%s__%s.json' % (dataset, configuration, trainer, p, alternative_alternative_validation_folder, folds_str))
                                if not isfile(summary_file):
                                    all_present = False
                                    print(name, dataset, configuration, 'has missing summary file')
                        if isfile(summary_file):
                            result = load_json(summary_file)['results'][interested_in]['mean']['Dice']
                            result_per_dataset_here[dataset][configuration] = result
                            f.write('%02.4f,' % result)
                        else:
                            f.write('NA,')
                            result_per_dataset_here[dataset][configuration] = 0
                f.write('\n')
                if True:
                    valid_trainers.append(name)
                    for d in datasets:
                        for c in datasets[d]:
                            result_per_dataset[d][c].append(result_per_dataset_here[d][c])
    invalid_trainers = [i for i in all_trainers if i not in valid_trainers]
    num_valid = len(valid_trainers)
    num_datasets = len(datasets.keys())
    all_res = np.zeros((num_valid, num_datasets))
    for (j, d) in enumerate(datasets.keys()):
        ks = list(result_per_dataset[d].keys())
        tmp = result_per_dataset[d][ks[0]]
        for k in ks[1:]:
            for i in range(len(tmp)):
                tmp[i] = max(tmp[i], result_per_dataset[d][k][i])
        all_res[:, j] = tmp
    ranks_arr = np.zeros_like(all_res)
    for d in range(ranks_arr.shape[1]):
        temp = np.argsort(all_res[:, d])[::-1]
        ranks = np.empty_like(temp)
        ranks[temp] = np.arange(len(temp))
        ranks_arr[:, d] = ranks
    mn = np.mean(ranks_arr, 1)
    for i in np.argsort(mn):
        print(mn[i], valid_trainers[i])
    print()
    print(valid_trainers[np.argmin(mn)])","for d in datasets:
    result_per_dataset[d] = {}
    for c in datasets[d]:
        result_per_dataset[d][c] = []",XXX,no_found,0,,,
nnFormer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nnFormer/nnformer/evaluation/model_selection/rank_candidates_cascade.py,https://github.com/282857341/nnFormer/tree/master/nnformer/evaluation/model_selection/rank_candidates_cascade.py,,if_main_my$20,"if __name__ == '__main__':
    summary_files_dir = join(network_training_output_dir, 'summary_jsons_fold0_new')
    output_file = join(network_training_output_dir, 'summary_cascade.csv')
    folds = (0,)
    folds_str = ''
    for f in folds:
        folds_str += str(f)
    plans = 'nnFormerPlansv2.1'
    overwrite_plans = {'nnFormerTrainerCascadeFullRes': ['nnFormerPlans']}
    trainers = ['nnFormerTrainerCascadeFullRes', 'nnFormerTrainerV2CascadeFullRes_EducatedGuess', 'nnFormerTrainerV2CascadeFullRes_EducatedGuess2', 'nnFormerTrainerV2CascadeFullRes_EducatedGuess3', 'nnFormerTrainerV2CascadeFullRes_lowerLR', 'nnFormerTrainerV2CascadeFullRes', 'nnFormerTrainerV2CascadeFullRes_noConnComp', 'nnFormerTrainerV2CascadeFullRes_shorter_lowerLR', 'nnFormerTrainerV2CascadeFullRes_shorter', 'nnFormerTrainerV2CascadeFullRes_smallerBinStrel']
    datasets = {'Task003_Liver': ('3d_cascade_fullres',), 'Task006_Lung': ('3d_cascade_fullres',), 'Task007_Pancreas': ('3d_cascade_fullres',), 'Task008_HepaticVessel': ('3d_cascade_fullres',), 'Task009_Spleen': ('3d_cascade_fullres',), 'Task010_Colon': ('3d_cascade_fullres',), 'Task017_AbdominalOrganSegmentation': ('3d_cascade_fullres',), 'Task048_KiTS_clean': ('3d_cascade_fullres',), 'Task055_SegTHOR': ('3d_cascade_fullres',), 'Task056_VerSe': ('3d_cascade_fullres',)}
    expected_validation_folder = 'validation_raw'
    alternative_validation_folder = 'validation'
    alternative_alternative_validation_folder = 'validation_tiledTrue_doMirror_True'
    interested_in = 'mean'
    result_per_dataset = {}
    for d in datasets:
        result_per_dataset[d] = {}
        for c in datasets[d]:
            result_per_dataset[d][c] = []
    valid_trainers = []
    all_trainers = []
    with open(output_file, 'w') as f:
        f.write('trainer,')
        for t in datasets.keys():
            s = t[4:7]
            for c in datasets[t]:
                s1 = s + '_' + c[3]
                f.write('%s,' % s1)
        f.write('\n')
        for trainer in trainers:
            trainer_plans = [plans]
            if trainer in overwrite_plans.keys():
                trainer_plans = overwrite_plans[trainer]
            result_per_dataset_here = {}
            for d in datasets:
                result_per_dataset_here[d] = {}
            for p in trainer_plans:
                name = '%s__%s' % (trainer, p)
                all_present = True
                all_trainers.append(name)
                f.write('%s,' % name)
                for dataset in datasets.keys():
                    for configuration in datasets[dataset]:
                        summary_file = join(summary_files_dir, '%s__%s__%s__%s__%s__%s.json' % (dataset, configuration, trainer, p, expected_validation_folder, folds_str))
                        if not isfile(summary_file):
                            summary_file = join(summary_files_dir, '%s__%s__%s__%s__%s__%s.json' % (dataset, configuration, trainer, p, alternative_validation_folder, folds_str))
                            if not isfile(summary_file):
                                summary_file = join(summary_files_dir, '%s__%s__%s__%s__%s__%s.json' % (dataset, configuration, trainer, p, alternative_alternative_validation_folder, folds_str))
                                if not isfile(summary_file):
                                    all_present = False
                                    print(name, dataset, configuration, 'has missing summary file')
                        if isfile(summary_file):
                            result = load_json(summary_file)['results'][interested_in]['mean']['Dice']
                            result_per_dataset_here[dataset][configuration] = result
                            f.write('%02.4f,' % result)
                        else:
                            f.write('NA,')
                            result_per_dataset_here[dataset][configuration] = 0
                f.write('\n')
                if True:
                    valid_trainers.append(name)
                    for d in datasets:
                        for c in datasets[d]:
                            result_per_dataset[d][c].append(result_per_dataset_here[d][c])
    invalid_trainers = [i for i in all_trainers if i not in valid_trainers]
    num_valid = len(valid_trainers)
    num_datasets = len(datasets.keys())
    all_res = np.zeros((num_valid, num_datasets))
    for (j, d) in enumerate(datasets.keys()):
        ks = list(result_per_dataset[d].keys())
        tmp = result_per_dataset[d][ks[0]]
        for k in ks[1:]:
            for i in range(len(tmp)):
                tmp[i] = max(tmp[i], result_per_dataset[d][k][i])
        all_res[:, j] = tmp
    ranks_arr = np.zeros_like(all_res)
    for d in range(ranks_arr.shape[1]):
        temp = np.argsort(all_res[:, d])[::-1]
        ranks = np.empty_like(temp)
        ranks[temp] = np.arange(len(temp))
        ranks_arr[:, d] = ranks
    mn = np.mean(ranks_arr, 1)
    for i in np.argsort(mn):
        print(mn[i], valid_trainers[i])
    print()
    print(valid_trainers[np.argmin(mn)])","for c in datasets[d]:
    result_per_dataset[d][c] = []",XXX,no_found,0,,,
nnFormer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nnFormer/nnformer/evaluation/model_selection/rank_candidates_cascade.py,https://github.com/282857341/nnFormer/tree/master/nnformer/evaluation/model_selection/rank_candidates_cascade.py,,if_main_my$20,"if __name__ == '__main__':
    summary_files_dir = join(network_training_output_dir, 'summary_jsons_fold0_new')
    output_file = join(network_training_output_dir, 'summary_cascade.csv')
    folds = (0,)
    folds_str = ''
    for f in folds:
        folds_str += str(f)
    plans = 'nnFormerPlansv2.1'
    overwrite_plans = {'nnFormerTrainerCascadeFullRes': ['nnFormerPlans']}
    trainers = ['nnFormerTrainerCascadeFullRes', 'nnFormerTrainerV2CascadeFullRes_EducatedGuess', 'nnFormerTrainerV2CascadeFullRes_EducatedGuess2', 'nnFormerTrainerV2CascadeFullRes_EducatedGuess3', 'nnFormerTrainerV2CascadeFullRes_lowerLR', 'nnFormerTrainerV2CascadeFullRes', 'nnFormerTrainerV2CascadeFullRes_noConnComp', 'nnFormerTrainerV2CascadeFullRes_shorter_lowerLR', 'nnFormerTrainerV2CascadeFullRes_shorter', 'nnFormerTrainerV2CascadeFullRes_smallerBinStrel']
    datasets = {'Task003_Liver': ('3d_cascade_fullres',), 'Task006_Lung': ('3d_cascade_fullres',), 'Task007_Pancreas': ('3d_cascade_fullres',), 'Task008_HepaticVessel': ('3d_cascade_fullres',), 'Task009_Spleen': ('3d_cascade_fullres',), 'Task010_Colon': ('3d_cascade_fullres',), 'Task017_AbdominalOrganSegmentation': ('3d_cascade_fullres',), 'Task048_KiTS_clean': ('3d_cascade_fullres',), 'Task055_SegTHOR': ('3d_cascade_fullres',), 'Task056_VerSe': ('3d_cascade_fullres',)}
    expected_validation_folder = 'validation_raw'
    alternative_validation_folder = 'validation'
    alternative_alternative_validation_folder = 'validation_tiledTrue_doMirror_True'
    interested_in = 'mean'
    result_per_dataset = {}
    for d in datasets:
        result_per_dataset[d] = {}
        for c in datasets[d]:
            result_per_dataset[d][c] = []
    valid_trainers = []
    all_trainers = []
    with open(output_file, 'w') as f:
        f.write('trainer,')
        for t in datasets.keys():
            s = t[4:7]
            for c in datasets[t]:
                s1 = s + '_' + c[3]
                f.write('%s,' % s1)
        f.write('\n')
        for trainer in trainers:
            trainer_plans = [plans]
            if trainer in overwrite_plans.keys():
                trainer_plans = overwrite_plans[trainer]
            result_per_dataset_here = {}
            for d in datasets:
                result_per_dataset_here[d] = {}
            for p in trainer_plans:
                name = '%s__%s' % (trainer, p)
                all_present = True
                all_trainers.append(name)
                f.write('%s,' % name)
                for dataset in datasets.keys():
                    for configuration in datasets[dataset]:
                        summary_file = join(summary_files_dir, '%s__%s__%s__%s__%s__%s.json' % (dataset, configuration, trainer, p, expected_validation_folder, folds_str))
                        if not isfile(summary_file):
                            summary_file = join(summary_files_dir, '%s__%s__%s__%s__%s__%s.json' % (dataset, configuration, trainer, p, alternative_validation_folder, folds_str))
                            if not isfile(summary_file):
                                summary_file = join(summary_files_dir, '%s__%s__%s__%s__%s__%s.json' % (dataset, configuration, trainer, p, alternative_alternative_validation_folder, folds_str))
                                if not isfile(summary_file):
                                    all_present = False
                                    print(name, dataset, configuration, 'has missing summary file')
                        if isfile(summary_file):
                            result = load_json(summary_file)['results'][interested_in]['mean']['Dice']
                            result_per_dataset_here[dataset][configuration] = result
                            f.write('%02.4f,' % result)
                        else:
                            f.write('NA,')
                            result_per_dataset_here[dataset][configuration] = 0
                f.write('\n')
                if True:
                    valid_trainers.append(name)
                    for d in datasets:
                        for c in datasets[d]:
                            result_per_dataset[d][c].append(result_per_dataset_here[d][c])
    invalid_trainers = [i for i in all_trainers if i not in valid_trainers]
    num_valid = len(valid_trainers)
    num_datasets = len(datasets.keys())
    all_res = np.zeros((num_valid, num_datasets))
    for (j, d) in enumerate(datasets.keys()):
        ks = list(result_per_dataset[d].keys())
        tmp = result_per_dataset[d][ks[0]]
        for k in ks[1:]:
            for i in range(len(tmp)):
                tmp[i] = max(tmp[i], result_per_dataset[d][k][i])
        all_res[:, j] = tmp
    ranks_arr = np.zeros_like(all_res)
    for d in range(ranks_arr.shape[1]):
        temp = np.argsort(all_res[:, d])[::-1]
        ranks = np.empty_like(temp)
        ranks[temp] = np.arange(len(temp))
        ranks_arr[:, d] = ranks
    mn = np.mean(ranks_arr, 1)
    for i in np.argsort(mn):
        print(mn[i], valid_trainers[i])
    print()
    print(valid_trainers[np.argmin(mn)])","for d in datasets:
    result_per_dataset_here[d] = {}",XXX,no_found,0,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/containers/docker.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/containers/docker.py,DockerAPIInterface,_kwopt_to_params$399,"def _kwopt_to_params(map_spec, key, value):
    """"""For a given containers lib method parameter name and value, return the matching docker-py parameters with
        values set (including transformation with an optional map function).

        See :meth:`_create_docker_api_spec`.
        """"""
    params = {}
    if 'map' in map_spec:
        value = map_spec['map'](value)
    for param in DockerAPIInterface._kwopt_to_param_names(map_spec, key):
        params[param] = value
    return params","for param in DockerAPIInterface._kwopt_to_param_names(map_spec, key):
    params[param] = value",XXX,no_found,0,,,
DetectoRS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DetectoRS/mmdet/core/fp16/decorators.py,https://github.com/joe-siyuan-qiao/DetectoRS/tree/master/mmdet/core/fp16/decorators.py,,auto_fp16$9,"def auto_fp16(apply_to=None, out_fp32=False):
    """"""Decorator to enable fp16 training automatically.

    This decorator is useful when you write custom modules and want to support
    mixed precision training. If inputs arguments are fp32 tensors, they will
    be converted to fp16 automatically. Arguments other than fp32 tensors are
    ignored.

    Args:
        apply_to (Iterable, optional): The argument names to be converted.
            `None` indicates all arguments.
        out_fp32 (bool): Whether to convert the output back to fp32.

    :Example:

        class MyModule1(nn.Module)

            # Convert x and y to fp16
            @auto_fp16()
            def forward(self, x, y):
                pass

        class MyModule2(nn.Module):

            # convert pred to fp16
            @auto_fp16(apply_to=('pred', ))
            def do_something(self, pred, others):
                pass
    """"""

    def auto_fp16_wrapper(old_func):

        @functools.wraps(old_func)
        def new_func(*args, **kwargs):
            if not isinstance(args[0], torch.nn.Module):
                raise TypeError('@auto_fp16 can only be used to decorate the method of nn.Module')
            if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
                return old_func(*args, **kwargs)
            args_info = getfullargspec(old_func)
            args_to_cast = args_info.args if apply_to is None else apply_to
            new_args = []
            if args:
                arg_names = args_info.args[:len(args)]
                for (i, arg_name) in enumerate(arg_names):
                    if arg_name in args_to_cast:
                        new_args.append(cast_tensor_type(args[i], torch.float, torch.half))
                    else:
                        new_args.append(args[i])
            new_kwargs = {}
            if kwargs:
                for (arg_name, arg_value) in kwargs.items():
                    if arg_name in args_to_cast:
                        new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
                    else:
                        new_kwargs[arg_name] = arg_value
            output = old_func(*new_args, **new_kwargs)
            if out_fp32:
                output = cast_tensor_type(output, torch.half, torch.float)
            return output
        return new_func
    return auto_fp16_wrapper","for (arg_name, arg_value) in kwargs.items():
    if arg_name in args_to_cast:
        new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
    else:
        new_kwargs[arg_name] = arg_value",XXX,no_found,0,,,
code-catalog-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/code-catalog-python/catalog/suggested/graphs/floyd_warshall.py,https://github.com/jwasham/code-catalog-python/tree/master/catalog/suggested/graphs/floyd_warshall.py,,floyd_warshall$11,"def floyd_warshall(graph):
    nodes = graph.keys()
    ' distance[][] will be the output matrix that will finally\n        have the shortest distances between every pair of vertices '
    distance = {}
    for n in nodes:
        distance[n] = {}
        for k in nodes:
            distance[n][k] = graph[n][k]
    ' Add all vertices one by one to the set of intermediate\n     vertices.\n\n         ---> Before start of a iteration, we have shortest distances\n         between all pairs of vertices such that the shortest\n         distances consider only the vertices in set\n         {0, 1, 2, .. k-1} as intermediate vertices.\n\n          ----> After the end of a iteration, vertex no. k is\n         added to the set of intermediate vertices and the\n         set becomes {0, 1, 2, .. k}\n    '
    for k in nodes:
        for i in nodes:
            for j in nodes:
                distance[i][j] = min(distance[i][j], distance[i][k] + distance[k][j])
    return distance","for k in nodes:
    distance[n][k] = graph[n][k]",XXX,no_found,0,,,
dask,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dask/dask/array/core.py,https://github.com/dask/dask/tree/master/dask/array/core.py,,_concatenate2$355,"def _concatenate2(arrays, axes=None):
    """"""Recursively concatenate nested lists of arrays along axes

    Each entry in axes corresponds to each level of the nested list.  The
    length of axes should correspond to the level of nesting of arrays.
    If axes is an empty list or tuple, return arrays, or arrays[0] if
    arrays is a list.

    >>> x = np.array([[1, 2], [3, 4]])
    >>> _concatenate2([x, x], axes=[0])
    array([[1, 2],
           [3, 4],
           [1, 2],
           [3, 4]])

    >>> _concatenate2([x, x], axes=[1])
    array([[1, 2, 1, 2],
           [3, 4, 3, 4]])

    >>> _concatenate2([[x, x], [x, x]], axes=[0, 1])
    array([[1, 2, 1, 2],
           [3, 4, 3, 4],
           [1, 2, 1, 2],
           [3, 4, 3, 4]])

    Supports Iterators
    >>> _concatenate2(iter([x, x]), axes=[1])
    array([[1, 2, 1, 2],
           [3, 4, 3, 4]])

    Special Case
    >>> _concatenate2([x, x], axes=())
    array([[1, 2],
           [3, 4]])
    """"""
    if axes is None:
        axes = []
    if axes == ():
        if isinstance(arrays, list):
            return arrays[0]
        else:
            return arrays
    if isinstance(arrays, Iterator):
        arrays = list(arrays)
    if not isinstance(arrays, (list, tuple)):
        return arrays
    if len(axes) > 1:
        arrays = [_concatenate2(a, axes=axes[1:]) for a in arrays]
    concatenate = concatenate_lookup.dispatch(type(max(arrays, key=lambda x: getattr(x, '__array_priority__', 0))))
    if isinstance(arrays[0], dict):
        keys = list(arrays[0].keys())
        assert all((list(a.keys()) == keys for a in arrays))
        ret = dict()
        for k in keys:
            ret[k] = concatenate(list((a[k] for a in arrays)), axis=axes[0])
        return ret
    else:
        return concatenate(arrays, axis=axes[0])","for k in keys:
    ret[k] = concatenate(list((a[k] for a in arrays)), axis=axes[0])",XXX,no_found,0,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/boto_rds.py,https://github.com/saltstack/salt/tree/master/salt/modules/boto_rds.py,,describe_parameter_group$986,"def describe_parameter_group(name, Filters=None, MaxRecords=None, Marker=None, region=None, key=None, keyid=None, profile=None):
    """"""
    Returns a list of `DBParameterGroup` descriptions.
    CLI example to description of parameter group::

        salt myminion boto_rds.describe_parameter_group parametergroupname            region=us-east-1
    """"""
    res = __salt__['boto_rds.parameter_group_exists'](name, tags=None, region=region, key=key, keyid=keyid, profile=profile)
    if not res.get('exists'):
        return {'exists': bool(res)}
    try:
        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
        if not conn:
            return {'results': bool(conn)}
        kwargs = {}
        for key in ('Marker', 'Filters'):
            if locals()[key] is not None:
                kwargs[key] = str(locals()[key])
        if locals()['MaxRecords'] is not None:
            kwargs['MaxRecords'] = int(locals()['MaxRecords'])
        info = conn.describe_db_parameter_groups(DBParameterGroupName=name, **kwargs)
        if not info:
            return {'results': bool(info), 'message': 'Failed to get RDS description for group {}.'.format(name)}
        return {'results': bool(info), 'message': 'Got RDS descrition for group {}.'.format(name)}
    except ClientError as e:
        return {'error': __utils__['boto3.get_error'](e)}","for key in ('Marker', 'Filters'):
    if locals()[key] is not None:
        kwargs[key] = str(locals()[key])",XXX,no_found,,,,
second.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/second.pytorch/second/data/kitti_common.py,https://github.com/traveller59/second.pytorch/tree/master/second/data/kitti_common.py,,remove_low_score$351,"def remove_low_score(image_anno, thresh):
    img_filtered_annotations = {}
    relevant_annotation_indices = [i for (i, s) in enumerate(image_anno['score']) if s >= thresh]
    for key in image_anno.keys():
        img_filtered_annotations[key] = image_anno[key][relevant_annotation_indices]
    return img_filtered_annotations","for key in image_anno.keys():
    img_filtered_annotations[key] = image_anno[key][relevant_annotation_indices]",XXX,no_found,,,,
lightning-flash,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lightning-flash/flash/tabular/data.py,https://github.com/PyTorchLightning/lightning-flash/tree/master/flash/tabular/data.py,TabularDeserializer,example_input$213,"def example_input(self) -> str:
    parameters = self.parameters
    row = {}
    for cat_col in parameters['categorical_fields']:
        row[cat_col] = ['test']
    for num_col in parameters['numerical_fields']:
        row[num_col] = [0]
    return str(DataFrame.from_dict(row).to_csv())","for cat_col in parameters['categorical_fields']:
    row[cat_col] = ['test']",XXX,no_found,1,,,
lightning-flash,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lightning-flash/flash/tabular/data.py,https://github.com/PyTorchLightning/lightning-flash/tree/master/flash/tabular/data.py,TabularDeserializer,example_input$213,"def example_input(self) -> str:
    parameters = self.parameters
    row = {}
    for cat_col in parameters['categorical_fields']:
        row[cat_col] = ['test']
    for num_col in parameters['numerical_fields']:
        row[num_col] = [0]
    return str(DataFrame.from_dict(row).to_csv())","for num_col in parameters['numerical_fields']:
    row[num_col] = [0]",XXX,no_found,1,,,
exbert,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/exbert/server/transformers/src/transformers/data/metrics/squad_metrics.py,https://github.com/bhoov/exbert/tree/master/server/transformers/src/transformers/data/metrics/squad_metrics.py,,get_final_text$242,"def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):
    """"""Project the tokenized prediction back to the original text.""""""

    def _strip_spaces(text):
        ns_chars = []
        ns_to_s_map = collections.OrderedDict()
        for (i, c) in enumerate(text):
            if c == ' ':
                continue
            ns_to_s_map[len(ns_chars)] = i
            ns_chars.append(c)
        ns_text = ''.join(ns_chars)
        return (ns_text, ns_to_s_map)
    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)
    tok_text = ' '.join(tokenizer.tokenize(orig_text))
    start_position = tok_text.find(pred_text)
    if start_position == -1:
        if verbose_logging:
            logger.info(""Unable to find text: '%s' in '%s'"" % (pred_text, orig_text))
        return orig_text
    end_position = start_position + len(pred_text) - 1
    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)
    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)
    if len(orig_ns_text) != len(tok_ns_text):
        if verbose_logging:
            logger.info(""Length not equal after stripping spaces: '%s' vs '%s'"", orig_ns_text, tok_ns_text)
        return orig_text
    tok_s_to_ns_map = {}
    for (i, tok_index) in tok_ns_to_s_map.items():
        tok_s_to_ns_map[tok_index] = i
    orig_start_position = None
    if start_position in tok_s_to_ns_map:
        ns_start_position = tok_s_to_ns_map[start_position]
        if ns_start_position in orig_ns_to_s_map:
            orig_start_position = orig_ns_to_s_map[ns_start_position]
    if orig_start_position is None:
        if verbose_logging:
            logger.info(""Couldn't map start position"")
        return orig_text
    orig_end_position = None
    if end_position in tok_s_to_ns_map:
        ns_end_position = tok_s_to_ns_map[end_position]
        if ns_end_position in orig_ns_to_s_map:
            orig_end_position = orig_ns_to_s_map[ns_end_position]
    if orig_end_position is None:
        if verbose_logging:
            logger.info(""Couldn't map end position"")
        return orig_text
    output_text = orig_text[orig_start_position:orig_end_position + 1]
    return output_text","for (i, tok_index) in tok_ns_to_s_map.items():
    tok_s_to_ns_map[tok_index] = i",XXX,no_found,0,,,
sentry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/api/serializers/models/deploy.py,https://github.com/getsentry/sentry/tree/master/src/sentry/api/serializers/models/deploy.py,DeploySerializer,get_attrs$7,"def get_attrs(self, item_list, user, **kwargs):
    environments = {id: name for (id, name) in Environment.objects.filter(id__in=[d.environment_id for d in item_list]).values_list('id', 'name')}
    result = {}
    for item in item_list:
        result[item] = {'environment': environments.get(item.environment_id)}
    return result","for item in item_list:
    result[item] = {'environment': environments.get(item.environment_id)}",XXX,no_found,0,,,
platformio-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/platformio-core/platformio/platform/board.py,https://github.com/platformio/platformio-core/tree/master/platformio/platform/board.py,PlatformBoardConfig,get_debug_data$105,"def get_debug_data(self):
    if not self._manifest.get('debug', {}).get('tools'):
        return None
    tools = {}
    for (name, options) in self._manifest['debug']['tools'].items():
        tools[name] = {}
        for (key, value) in options.items():
            if key in ('default', 'onboard') and value:
                tools[name][key] = value
    return {'tools': tools}","for (key, value) in options.items():
    if key in ('default', 'onboard') and value:
        tools[name][key] = value",XXX,no_found,0,,,
celeb-detection-oss,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/celeb-detection-oss/model_training/utils.py,https://github.com/Giphy/celeb-detection-oss/tree/master/model_training/utils.py,,labels_by_name$57,"def labels_by_name(path):
    labels = {}
    with open(path) as f:
        file_reader = csv.reader(f)
        next(file_reader)
        rows = sorted(file_reader, key=lambda x: x[0])
        for (i, row) in enumerate(rows):
            labels[row[0]] = i
    return labels","for (i, row) in enumerate(rows):
    labels[row[0]] = i",XXX,no_found,,,,
spaCy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spaCy/spacy/lang/el/get_pos_from_wiktionary.py,https://github.com/explosion/spaCy/tree/master/spacy/lang/el/get_pos_from_wiktionary.py,,get_pos_from_wiktionary$1,"def get_pos_from_wiktionary():
    import re
    from gensim.corpora.wikicorpus import extract_pages
    regex = re.compile('==={{(\\w+)\\|el}}===')
    regex2 = re.compile('==={{(\\w+ \\w+)\\|el}}===')
    expected_parts = ['婵炴捇鎷曞樺磭鍞甸柟杈ㄥ介悥鐐烘媰', '闁挎粍妲掗幃妤銆掑ù', '闁藉嫭淇洪悢濠甸弮鍥у壄闁藉嫭淇洪悢鍡涘箞', '闁藉嫭淇洪悢濠甸弮鍥╁敥闁挎粏宕甸弲鍗炪掑ù', '闁硅鲸濯介悥娑㈡晭閽樺嬫緭濞村ジ鏁庨崐鐔哄數闁哥姴鍊块悺姗鏁', '濮掑嫬绻楅悥銉╂晭鐠囧弶婢撻柟 闁跨姴鐗愰惃鎶藉箞閻楀牐鍘濞', '缂備胶瀣鍞╅柤鍐插暢閻斾線骞']
    wiktionary_file_path = '/data/gsoc2018-spacy/spacy/lang/el/res/elwiktionary-latest-pages-articles.xml'
    proper_names_dict = {'闁硅鲸濯介悥娑㈡晭閽樺嬫緭濞村ジ鏁庨崐鐔哄數闁哥姴鍊块悺姗鏁': 'nouns', '闁藉嫭淇洪悢濠甸弮鍥у壄闁藉嫭淇洪悢鍡涘箞': 'adjectives', '缂備胶瀣鍞╅柤鍐插暢閻斾線骞': 'dets', '闁藉嫭淇洪悢濠甸弮鍥╁敥闁挎粏宕甸弲鍗炪掑ù': 'adverbs', '濮掑嫬绻楅悥銉╂晭鐠囧弶婢撻柟 闁跨姴鐗愰惃鎶藉箞閻楀牐鍘濞': 'proper_names', '婵炴捇鎷曞樺磭鍞甸柟杈ㄥ介悥鐐烘媰': 'participles', '闁挎粍妲掗幃妤銆掑ù': 'verbs'}
    expected_parts_dict = {}
    for expected_part in expected_parts:
        expected_parts_dict[expected_part] = []
    for (title, text, pageid) in extract_pages(wiktionary_file_path):
        if text.startswith('#REDIRECT'):
            continue
        title = title.lower()
        all_regex = regex.findall(text)
        all_regex.extend(regex2.findall(text))
        for a in all_regex:
            if a in expected_parts:
                expected_parts_dict[a].append(title)
    for i in expected_parts_dict:
        with open('_{0}.py'.format(proper_names_dict[i]), 'w') as f:
            f.write('from __future__ import unicode_literals\n')
            f.write('{} = set(""""""\n'.format(proper_names_dict[i].upper()))
            words = sorted(expected_parts_dict[i])
            line = ''
            to_write = []
            for word in words:
                if len(line + ' ' + word) > 79:
                    to_write.append(line)
                    line = ''
                else:
                    line = line + ' ' + word
            f.write('\n'.join(to_write))
            f.write('\n"""""".split())')","for expected_part in expected_parts:
    expected_parts_dict[expected_part] = []",XXX,no_found,,,,
kuma,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kuma/kuma/settings/common.py,https://github.com/mdn/kuma/tree/master/kuma/settings/common.py,,_get_locales$156,"def _get_locales():
    """"""
    Load LOCALES data from languages.json

    languages.json is from the product-details project:
    https://product-details.mozilla.org/1.0/languages.json
    """"""
    lang_path = BASE_DIR / 'settings' / 'languages.json'
    with open(lang_path) as lang_file:
        json_locales = json.load(lang_file)
    locales = {}
    _Language = namedtuple('Language', 'english native')
    for (locale, meta) in json_locales.items():
        locales[locale] = _Language(meta['English'], meta['native'])
    return locales","for (locale, meta) in json_locales.items():
    locales[locale] = _Language(meta['English'], meta['native'])",XXX,no_found,,,,
pbtk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pbtk/extractors/jar_extract.py,https://github.com/marin-m/pbtk/tree/master/extractors/jar_extract.py,,handle_jar$43,"def handle_jar(path):
    if path.endswith('.jar'):
        yield ('_progress', ('Decompressing JAR...', None))
    else:
        yield ('_progress', ('Converting DEX to JAR...', None))
    with JarWrapper(path) as jar:
        enums = {}
        pkg_to_codedinputstream = OrderedDict()
        pkg_to_codedoutputstream = {}
        map_entry_cls = []
        out_additional_cls = []
        pkg_to_j2me_protobuftype = OrderedDict()
        '\n        First iteration on classes: look for library classes signatures.\n        '
        for (i, cls) in enumerate(jar.classes):
            if i % 10 == 0:
                yield ('_progress', ('Scanning Java package contents...', i / len(jar.classes) * 0.5))
            pkg = cls[:cls.rfind('.')] if '.' in cls else ''
            binr = jar.read(cls)
            raw_cls = cls.replace('.', '/').encode('utf8')
            ""\n            Handle multiple cases:\n            1. CodedInputStream, before it was split out in multiple\n               subclasses (cc8ca5b - oct 2016)\n            2. CodedInputStream, after it was\n            3. CodedInputByteBufferNano\n            4. CodedInputStreamMicro\n            \n            The second case doesn't provide intelligible strings to\n            search for, so we'll use method signatures (the different\n            kinds that can be produced by Proguard) instead.\n            ""
            SIG_NANO = b'([BII)V'
            SIG_NANO_2 = b'([BI)V'
            SIG_DEF = b'([BIIZ)L%s;' % raw_cls
            SIG_DEF_2 = b'([BII)L%s;' % raw_cls
            SIG_CALL = b'([BIIZ)V'
            SIG_CALL_2 = b'([BII)V'
            SIG_CALL_3 = b'([BIIZL'
            has_constructor = SIG_DEF in binr or SIG_DEF_2 in binr
            calls_arraydecoder = SIG_CALL in binr or SIG_CALL_2 in binr or SIG_CALL_3 in binr
            is_legit_class = b'Beginning index' not in binr and b'Number too large' not in binr and (b'a byte array' not in binr)
            has_constructor_nano = SIG_NANO in binr or SIG_NANO_2 in binr
            has_relevant_string = b'message contained an invalid tag' in binr
            has_relevant_string_nano = b'is beyond current' in binr
            has_relevant_string_micro = b""when buffer wasn't empty"" in binr
            '\n            Try to match CodedOutputStream before CodedInputStream, as\n            it may have common points in signatures but always has a\n            recognizable string.\n            '
            has_out_constructor = b'([BII' in binr
            has_out_relevant_string = b'write as much data as' in binr
            has_out_relevant_string_old = b'UTF-8 not supported.' in binr
            has_out_relevant_string_nano = b'Unpaired surrogate at index ' in binr and b'wrap' in binr
            has_out_relevant_string_2 = b'Converting ill-formed UTF-16.' in binr and b'Pos:' not in binr
            is_legit_out_class = b'byte array' not in binr
            if has_out_constructor and ((has_out_relevant_string or has_out_relevant_string_old) and is_legit_out_class or has_out_relevant_string_nano or has_out_relevant_string_2):
                while pkg in pkg_to_codedoutputstream:
                    pkg += '_'
                pkg_to_codedoutputstream[pkg] = cls
            elif has_constructor and is_legit_class and (calls_arraydecoder or has_relevant_string) or (has_constructor_nano and (has_relevant_string_nano or has_relevant_string_micro)):
                while pkg in pkg_to_codedinputstream:
                    pkg += '_'
                pkg_to_codedinputstream[pkg] = cls
            elif b'Generated message class' in binr:
                out_additional_cls.append(cls)
            elif b'is not a primitive type' in binr:
                map_entry_cls.append(cls)
            elif b'Groups are not allowed in maps' in binr or b'a map entry message.' in binr:
                map_entry_cls.append(cls)
            elif b'Unexp.EOF' in binr:
                code = jar.decomp(cls, True).raw
                protobuftype_cls = search('public \\w+\\(([\\w.$]+) \\w+\\)', code).group(1)
                default_consts = {}
                for (prop, const) in findall('(\\w+) = new Boolean\\((\\w+)\\)', code):
                    default_consts[cls + '.' + prop] = const
                while pkg in pkg_to_j2me_protobuftype:
                    pkg += '_'
                pkg_to_j2me_protobuftype[pkg] = (protobuftype_cls, default_consts)
        for pkg in list(pkg_to_codedinputstream):
            if pkg not in pkg_to_codedoutputstream:
                del pkg_to_codedinputstream[pkg]
        '\n        Second iteration on classes: look for generated classes, that\n        contains method call signatures [1] for libraries we found, or\n        other extractible information.\n        \n        [1] https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.3\n        '
        gen_classes = OrderedDict()
        gen_classes_j2me = OrderedDict()
        had_metadata = set()
        for (i, cls) in enumerate(jar.classes):
            if i % 10 == 0:
                yield ('_progress', ('Scanning Java package contents...', i / len(jar.classes) * 0.5 + 0.5))
            binr = jar.read(cls)
            if b'.proto\x12' in binr or b'.protodevel\x12' in binr:
                code = jar.decomp(cls, True).raw
                code = sub('"",\\s+""', '', code, flags=MULTILINE)
                meta = search('""(\\\\n.+?\\.proto.+)""', code)
                if meta:
                    meta = meta.group(1).encode('latin1')
                    meta = meta.decode('unicode_escape').encode('latin1')
                    yield from walk_binary(meta)
                    had_metadata.add(cls)
            for impl in pkg_to_codedinputstream:
                if b'%s' % pkg_to_codedinputstream[impl].replace('.', '/').encode('ascii') in binr and b'(L%s;' % pkg_to_codedoutputstream[impl].replace('.', '/').encode('ascii') in binr and (cls not in (pkg_to_codedinputstream[impl], pkg_to_codedoutputstream[impl])):
                    gen_classes[cls] = (pkg_to_codedinputstream[impl], pkg_to_codedoutputstream[impl])
            for (impl, (protobuftype_cls, consts)) in pkg_to_j2me_protobuftype.items():
                if b'(IILjava/lang/Object;)L%s;' % protobuftype_cls.replace('.', '/').encode('ascii') in binr and cls != protobuftype_cls:
                    gen_classes_j2me[cls] = (protobuftype_cls, consts)
            if b'Ljava/lang/Enum<' in binr[:256]:
                enums[cls] = cls
                if '$' in cls:
                    enums[cls.replace('$', '.')] = cls
                    enums[cls.rsplit('.', 1)[0] + '.' + cls.rsplit('$', 1)[1]] = cls
        gen_classes_nodollar = OrderedDict(gen_classes)
        for (cls, pkg) in OrderedDict(gen_classes_nodollar).items():
            if '$' in cls:
                gen_classes_nodollar[cls.replace('$', '.')] = pkg
                gen_classes_nodollar[cls.rsplit('.', 1)[0] + '.' + cls.rsplit('$', 1)[1]] = pkg
        '\n        Once we know what classes we should look at, do the actual code\n        scraping and extraction work.\n        '
        msg_path_to_obj = OrderedDict()
        msg_to_referrers = defaultdict(list)
        for (i, (cls, (codedinputstream, codedoutputstream))) in enumerate(gen_classes.items()):
            yield ('_progress', ('Extracting %s...' % cls, i / len(gen_classes)))
            if cls.split('$')[0] not in had_metadata:
                extract_lite(jar, cls, enums, gen_classes_nodollar, codedinputstream, codedoutputstream, map_entry_cls, out_additional_cls, msg_path_to_obj, msg_to_referrers)
        for (i, (cls, (protobuftype_cls, consts))) in enumerate(gen_classes_j2me.items()):
            yield ('_progress', ('Extracting %s...' % cls, i / len(gen_classes_j2me)))
            extract_j2me(jar, cls, enums, gen_classes_j2me, protobuftype_cls, consts, msg_path_to_obj, msg_to_referrers)
        yield ('_progress', ('Dumping information to .protos...', None))
        yield from nest_and_print_to_files(msg_path_to_obj, msg_to_referrers)
        yield from jar.bonus_protos.items()","for (prop, const) in findall('(\\w+) = new Boolean\\((\\w+)\\)', code):
    default_consts[cls + '.' + prop] = const",XXX,no_found,0,,,
CLUEPretrainedModels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUEPretrainedModels/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_utils.py,https://github.com/CLUEbenchmark/CLUEPretrainedModels/tree/master/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_utils.py,PreTrainedTokenizer,_from_pretrained$286,"def _from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs):
    cache_dir = kwargs.pop('cache_dir', None)
    force_download = kwargs.pop('force_download', False)
    proxies = kwargs.pop('proxies', None)
    s3_models = list(cls.max_model_input_sizes.keys())
    vocab_files = {}
    init_configuration = {}
    if pretrained_model_name_or_path in s3_models:
        for (file_id, map_list) in cls.pretrained_vocab_files_map.items():
            vocab_files[file_id] = map_list[pretrained_model_name_or_path]
        if cls.pretrained_init_configuration and pretrained_model_name_or_path in cls.pretrained_init_configuration:
            init_configuration = cls.pretrained_init_configuration[pretrained_model_name_or_path]
    else:
        logger.info(""Model name '{}' not found in model shortcut name list ({}). Assuming '{}' is a path or url to a directory containing tokenizer files."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path))
        for (file_id, file_name) in cls.vocab_files_names.items():
            if os.path.isdir(pretrained_model_name_or_path):
                full_file_name = os.path.join(pretrained_model_name_or_path, file_name)
            else:
                full_file_name = pretrained_model_name_or_path
            if not os.path.exists(full_file_name):
                logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
                full_file_name = None
            vocab_files[file_id] = full_file_name
        additional_files_names = {'added_tokens_file': ADDED_TOKENS_FILE, 'special_tokens_map_file': SPECIAL_TOKENS_MAP_FILE, 'tokenizer_config_file': TOKENIZER_CONFIG_FILE}
        saved_directory = pretrained_model_name_or_path
        if os.path.exists(saved_directory) and (not os.path.isdir(saved_directory)):
            saved_directory = os.path.dirname(saved_directory)
        for (file_id, file_name) in additional_files_names.items():
            full_file_name = os.path.join(saved_directory, file_name)
            if not os.path.exists(full_file_name):
                logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
                full_file_name = None
            vocab_files[file_id] = full_file_name
        if all((full_file_name is None for full_file_name in vocab_files.values())):
            raise EnvironmentError(""Model name '{}' was not found in tokenizers model name list ({}). We assumed '{}' was a path or url to a directory containing vocabulary files named {} but couldn't find such vocabulary files at this path or url."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path, list(cls.vocab_files_names.values())))
    try:
        resolved_vocab_files = {}
        for (file_id, file_path) in vocab_files.items():
            if file_path is None:
                resolved_vocab_files[file_id] = None
            else:
                resolved_vocab_files[file_id] = cached_path(file_path, cache_dir=cache_dir, force_download=force_download, proxies=proxies)
    except EnvironmentError:
        if pretrained_model_name_or_path in s3_models:
            msg = ""Couldn't reach server at '{}' to download vocabulary files.""
        else:
            msg = ""Model name '{}' was not found in tokenizers model name list ({}). We assumed '{}' was a path or url to a directory containing vocabulary files named {}, but couldn't find such vocabulary files at this path or url."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path, list(cls.vocab_files_names.values()))
        raise EnvironmentError(msg)
    for (file_id, file_path) in vocab_files.items():
        if file_path == resolved_vocab_files[file_id]:
            logger.info('loading file {}'.format(file_path))
        else:
            logger.info('loading file {} from cache at {}'.format(file_path, resolved_vocab_files[file_id]))
    tokenizer_config_file = resolved_vocab_files.pop('tokenizer_config_file', None)
    if tokenizer_config_file is not None:
        init_kwargs = json.load(open(tokenizer_config_file, encoding='utf-8'))
        saved_init_inputs = init_kwargs.pop('init_inputs', ())
        if not init_inputs:
            init_inputs = saved_init_inputs
    else:
        init_kwargs = init_configuration
    init_kwargs.update(kwargs)
    if pretrained_model_name_or_path in cls.max_model_input_sizes:
        max_len = cls.max_model_input_sizes[pretrained_model_name_or_path]
        if max_len is not None and isinstance(max_len, (int, float)):
            init_kwargs['max_len'] = min(init_kwargs.get('max_len', int(1000000000000.0)), max_len)
    added_tokens_file = resolved_vocab_files.pop('added_tokens_file', None)
    special_tokens_map_file = resolved_vocab_files.pop('special_tokens_map_file', None)
    for (args_name, file_path) in resolved_vocab_files.items():
        if args_name not in init_kwargs:
            init_kwargs[args_name] = file_path
    if special_tokens_map_file is not None:
        special_tokens_map = json.load(open(special_tokens_map_file, encoding='utf-8'))
        for (key, value) in special_tokens_map.items():
            if key not in init_kwargs:
                init_kwargs[key] = value
    tokenizer = cls(*init_inputs, **init_kwargs)
    tokenizer.init_inputs = init_inputs
    tokenizer.init_kwargs = init_kwargs
    if added_tokens_file is not None:
        added_tok_encoder = json.load(open(added_tokens_file, encoding='utf-8'))
        added_tok_decoder = {v: k for (k, v) in added_tok_encoder.items()}
        tokenizer.added_tokens_encoder.update(added_tok_encoder)
        tokenizer.added_tokens_decoder.update(added_tok_decoder)
    return tokenizer","for (file_id, map_list) in cls.pretrained_vocab_files_map.items():
    vocab_files[file_id] = map_list[pretrained_model_name_or_path]",XXX,no_found,,,,
CLUEPretrainedModels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUEPretrainedModels/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_utils.py,https://github.com/CLUEbenchmark/CLUEPretrainedModels/tree/master/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_utils.py,PreTrainedTokenizer,_from_pretrained$286,"def _from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs):
    cache_dir = kwargs.pop('cache_dir', None)
    force_download = kwargs.pop('force_download', False)
    proxies = kwargs.pop('proxies', None)
    s3_models = list(cls.max_model_input_sizes.keys())
    vocab_files = {}
    init_configuration = {}
    if pretrained_model_name_or_path in s3_models:
        for (file_id, map_list) in cls.pretrained_vocab_files_map.items():
            vocab_files[file_id] = map_list[pretrained_model_name_or_path]
        if cls.pretrained_init_configuration and pretrained_model_name_or_path in cls.pretrained_init_configuration:
            init_configuration = cls.pretrained_init_configuration[pretrained_model_name_or_path]
    else:
        logger.info(""Model name '{}' not found in model shortcut name list ({}). Assuming '{}' is a path or url to a directory containing tokenizer files."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path))
        for (file_id, file_name) in cls.vocab_files_names.items():
            if os.path.isdir(pretrained_model_name_or_path):
                full_file_name = os.path.join(pretrained_model_name_or_path, file_name)
            else:
                full_file_name = pretrained_model_name_or_path
            if not os.path.exists(full_file_name):
                logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
                full_file_name = None
            vocab_files[file_id] = full_file_name
        additional_files_names = {'added_tokens_file': ADDED_TOKENS_FILE, 'special_tokens_map_file': SPECIAL_TOKENS_MAP_FILE, 'tokenizer_config_file': TOKENIZER_CONFIG_FILE}
        saved_directory = pretrained_model_name_or_path
        if os.path.exists(saved_directory) and (not os.path.isdir(saved_directory)):
            saved_directory = os.path.dirname(saved_directory)
        for (file_id, file_name) in additional_files_names.items():
            full_file_name = os.path.join(saved_directory, file_name)
            if not os.path.exists(full_file_name):
                logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
                full_file_name = None
            vocab_files[file_id] = full_file_name
        if all((full_file_name is None for full_file_name in vocab_files.values())):
            raise EnvironmentError(""Model name '{}' was not found in tokenizers model name list ({}). We assumed '{}' was a path or url to a directory containing vocabulary files named {} but couldn't find such vocabulary files at this path or url."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path, list(cls.vocab_files_names.values())))
    try:
        resolved_vocab_files = {}
        for (file_id, file_path) in vocab_files.items():
            if file_path is None:
                resolved_vocab_files[file_id] = None
            else:
                resolved_vocab_files[file_id] = cached_path(file_path, cache_dir=cache_dir, force_download=force_download, proxies=proxies)
    except EnvironmentError:
        if pretrained_model_name_or_path in s3_models:
            msg = ""Couldn't reach server at '{}' to download vocabulary files.""
        else:
            msg = ""Model name '{}' was not found in tokenizers model name list ({}). We assumed '{}' was a path or url to a directory containing vocabulary files named {}, but couldn't find such vocabulary files at this path or url."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path, list(cls.vocab_files_names.values()))
        raise EnvironmentError(msg)
    for (file_id, file_path) in vocab_files.items():
        if file_path == resolved_vocab_files[file_id]:
            logger.info('loading file {}'.format(file_path))
        else:
            logger.info('loading file {} from cache at {}'.format(file_path, resolved_vocab_files[file_id]))
    tokenizer_config_file = resolved_vocab_files.pop('tokenizer_config_file', None)
    if tokenizer_config_file is not None:
        init_kwargs = json.load(open(tokenizer_config_file, encoding='utf-8'))
        saved_init_inputs = init_kwargs.pop('init_inputs', ())
        if not init_inputs:
            init_inputs = saved_init_inputs
    else:
        init_kwargs = init_configuration
    init_kwargs.update(kwargs)
    if pretrained_model_name_or_path in cls.max_model_input_sizes:
        max_len = cls.max_model_input_sizes[pretrained_model_name_or_path]
        if max_len is not None and isinstance(max_len, (int, float)):
            init_kwargs['max_len'] = min(init_kwargs.get('max_len', int(1000000000000.0)), max_len)
    added_tokens_file = resolved_vocab_files.pop('added_tokens_file', None)
    special_tokens_map_file = resolved_vocab_files.pop('special_tokens_map_file', None)
    for (args_name, file_path) in resolved_vocab_files.items():
        if args_name not in init_kwargs:
            init_kwargs[args_name] = file_path
    if special_tokens_map_file is not None:
        special_tokens_map = json.load(open(special_tokens_map_file, encoding='utf-8'))
        for (key, value) in special_tokens_map.items():
            if key not in init_kwargs:
                init_kwargs[key] = value
    tokenizer = cls(*init_inputs, **init_kwargs)
    tokenizer.init_inputs = init_inputs
    tokenizer.init_kwargs = init_kwargs
    if added_tokens_file is not None:
        added_tok_encoder = json.load(open(added_tokens_file, encoding='utf-8'))
        added_tok_decoder = {v: k for (k, v) in added_tok_encoder.items()}
        tokenizer.added_tokens_encoder.update(added_tok_encoder)
        tokenizer.added_tokens_decoder.update(added_tok_decoder)
    return tokenizer","for (file_id, file_name) in cls.vocab_files_names.items():
    if os.path.isdir(pretrained_model_name_or_path):
        full_file_name = os.path.join(pretrained_model_name_or_path, file_name)
    else:
        full_file_name = pretrained_model_name_or_path
    if not os.path.exists(full_file_name):
        logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
        full_file_name = None
    vocab_files[file_id] = full_file_name",XXX,no_found,0,,,
CLUEPretrainedModels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUEPretrainedModels/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_utils.py,https://github.com/CLUEbenchmark/CLUEPretrainedModels/tree/master/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_utils.py,PreTrainedTokenizer,_from_pretrained$286,"def _from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs):
    cache_dir = kwargs.pop('cache_dir', None)
    force_download = kwargs.pop('force_download', False)
    proxies = kwargs.pop('proxies', None)
    s3_models = list(cls.max_model_input_sizes.keys())
    vocab_files = {}
    init_configuration = {}
    if pretrained_model_name_or_path in s3_models:
        for (file_id, map_list) in cls.pretrained_vocab_files_map.items():
            vocab_files[file_id] = map_list[pretrained_model_name_or_path]
        if cls.pretrained_init_configuration and pretrained_model_name_or_path in cls.pretrained_init_configuration:
            init_configuration = cls.pretrained_init_configuration[pretrained_model_name_or_path]
    else:
        logger.info(""Model name '{}' not found in model shortcut name list ({}). Assuming '{}' is a path or url to a directory containing tokenizer files."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path))
        for (file_id, file_name) in cls.vocab_files_names.items():
            if os.path.isdir(pretrained_model_name_or_path):
                full_file_name = os.path.join(pretrained_model_name_or_path, file_name)
            else:
                full_file_name = pretrained_model_name_or_path
            if not os.path.exists(full_file_name):
                logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
                full_file_name = None
            vocab_files[file_id] = full_file_name
        additional_files_names = {'added_tokens_file': ADDED_TOKENS_FILE, 'special_tokens_map_file': SPECIAL_TOKENS_MAP_FILE, 'tokenizer_config_file': TOKENIZER_CONFIG_FILE}
        saved_directory = pretrained_model_name_or_path
        if os.path.exists(saved_directory) and (not os.path.isdir(saved_directory)):
            saved_directory = os.path.dirname(saved_directory)
        for (file_id, file_name) in additional_files_names.items():
            full_file_name = os.path.join(saved_directory, file_name)
            if not os.path.exists(full_file_name):
                logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
                full_file_name = None
            vocab_files[file_id] = full_file_name
        if all((full_file_name is None for full_file_name in vocab_files.values())):
            raise EnvironmentError(""Model name '{}' was not found in tokenizers model name list ({}). We assumed '{}' was a path or url to a directory containing vocabulary files named {} but couldn't find such vocabulary files at this path or url."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path, list(cls.vocab_files_names.values())))
    try:
        resolved_vocab_files = {}
        for (file_id, file_path) in vocab_files.items():
            if file_path is None:
                resolved_vocab_files[file_id] = None
            else:
                resolved_vocab_files[file_id] = cached_path(file_path, cache_dir=cache_dir, force_download=force_download, proxies=proxies)
    except EnvironmentError:
        if pretrained_model_name_or_path in s3_models:
            msg = ""Couldn't reach server at '{}' to download vocabulary files.""
        else:
            msg = ""Model name '{}' was not found in tokenizers model name list ({}). We assumed '{}' was a path or url to a directory containing vocabulary files named {}, but couldn't find such vocabulary files at this path or url."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path, list(cls.vocab_files_names.values()))
        raise EnvironmentError(msg)
    for (file_id, file_path) in vocab_files.items():
        if file_path == resolved_vocab_files[file_id]:
            logger.info('loading file {}'.format(file_path))
        else:
            logger.info('loading file {} from cache at {}'.format(file_path, resolved_vocab_files[file_id]))
    tokenizer_config_file = resolved_vocab_files.pop('tokenizer_config_file', None)
    if tokenizer_config_file is not None:
        init_kwargs = json.load(open(tokenizer_config_file, encoding='utf-8'))
        saved_init_inputs = init_kwargs.pop('init_inputs', ())
        if not init_inputs:
            init_inputs = saved_init_inputs
    else:
        init_kwargs = init_configuration
    init_kwargs.update(kwargs)
    if pretrained_model_name_or_path in cls.max_model_input_sizes:
        max_len = cls.max_model_input_sizes[pretrained_model_name_or_path]
        if max_len is not None and isinstance(max_len, (int, float)):
            init_kwargs['max_len'] = min(init_kwargs.get('max_len', int(1000000000000.0)), max_len)
    added_tokens_file = resolved_vocab_files.pop('added_tokens_file', None)
    special_tokens_map_file = resolved_vocab_files.pop('special_tokens_map_file', None)
    for (args_name, file_path) in resolved_vocab_files.items():
        if args_name not in init_kwargs:
            init_kwargs[args_name] = file_path
    if special_tokens_map_file is not None:
        special_tokens_map = json.load(open(special_tokens_map_file, encoding='utf-8'))
        for (key, value) in special_tokens_map.items():
            if key not in init_kwargs:
                init_kwargs[key] = value
    tokenizer = cls(*init_inputs, **init_kwargs)
    tokenizer.init_inputs = init_inputs
    tokenizer.init_kwargs = init_kwargs
    if added_tokens_file is not None:
        added_tok_encoder = json.load(open(added_tokens_file, encoding='utf-8'))
        added_tok_decoder = {v: k for (k, v) in added_tok_encoder.items()}
        tokenizer.added_tokens_encoder.update(added_tok_encoder)
        tokenizer.added_tokens_decoder.update(added_tok_decoder)
    return tokenizer","for (file_id, file_name) in additional_files_names.items():
    full_file_name = os.path.join(saved_directory, file_name)
    if not os.path.exists(full_file_name):
        logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
        full_file_name = None
    vocab_files[file_id] = full_file_name",XXX,no_found,0,,,
CLUEPretrainedModels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CLUEPretrainedModels/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_utils.py,https://github.com/CLUEbenchmark/CLUEPretrainedModels/tree/master/baselines/models_pytorch/classifier_pytorch/transformers/tokenization_utils.py,PreTrainedTokenizer,_from_pretrained$286,"def _from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs):
    cache_dir = kwargs.pop('cache_dir', None)
    force_download = kwargs.pop('force_download', False)
    proxies = kwargs.pop('proxies', None)
    s3_models = list(cls.max_model_input_sizes.keys())
    vocab_files = {}
    init_configuration = {}
    if pretrained_model_name_or_path in s3_models:
        for (file_id, map_list) in cls.pretrained_vocab_files_map.items():
            vocab_files[file_id] = map_list[pretrained_model_name_or_path]
        if cls.pretrained_init_configuration and pretrained_model_name_or_path in cls.pretrained_init_configuration:
            init_configuration = cls.pretrained_init_configuration[pretrained_model_name_or_path]
    else:
        logger.info(""Model name '{}' not found in model shortcut name list ({}). Assuming '{}' is a path or url to a directory containing tokenizer files."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path))
        for (file_id, file_name) in cls.vocab_files_names.items():
            if os.path.isdir(pretrained_model_name_or_path):
                full_file_name = os.path.join(pretrained_model_name_or_path, file_name)
            else:
                full_file_name = pretrained_model_name_or_path
            if not os.path.exists(full_file_name):
                logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
                full_file_name = None
            vocab_files[file_id] = full_file_name
        additional_files_names = {'added_tokens_file': ADDED_TOKENS_FILE, 'special_tokens_map_file': SPECIAL_TOKENS_MAP_FILE, 'tokenizer_config_file': TOKENIZER_CONFIG_FILE}
        saved_directory = pretrained_model_name_or_path
        if os.path.exists(saved_directory) and (not os.path.isdir(saved_directory)):
            saved_directory = os.path.dirname(saved_directory)
        for (file_id, file_name) in additional_files_names.items():
            full_file_name = os.path.join(saved_directory, file_name)
            if not os.path.exists(full_file_name):
                logger.info(""Didn't find file {}. We won't load it."".format(full_file_name))
                full_file_name = None
            vocab_files[file_id] = full_file_name
        if all((full_file_name is None for full_file_name in vocab_files.values())):
            raise EnvironmentError(""Model name '{}' was not found in tokenizers model name list ({}). We assumed '{}' was a path or url to a directory containing vocabulary files named {} but couldn't find such vocabulary files at this path or url."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path, list(cls.vocab_files_names.values())))
    try:
        resolved_vocab_files = {}
        for (file_id, file_path) in vocab_files.items():
            if file_path is None:
                resolved_vocab_files[file_id] = None
            else:
                resolved_vocab_files[file_id] = cached_path(file_path, cache_dir=cache_dir, force_download=force_download, proxies=proxies)
    except EnvironmentError:
        if pretrained_model_name_or_path in s3_models:
            msg = ""Couldn't reach server at '{}' to download vocabulary files.""
        else:
            msg = ""Model name '{}' was not found in tokenizers model name list ({}). We assumed '{}' was a path or url to a directory containing vocabulary files named {}, but couldn't find such vocabulary files at this path or url."".format(pretrained_model_name_or_path, ', '.join(s3_models), pretrained_model_name_or_path, list(cls.vocab_files_names.values()))
        raise EnvironmentError(msg)
    for (file_id, file_path) in vocab_files.items():
        if file_path == resolved_vocab_files[file_id]:
            logger.info('loading file {}'.format(file_path))
        else:
            logger.info('loading file {} from cache at {}'.format(file_path, resolved_vocab_files[file_id]))
    tokenizer_config_file = resolved_vocab_files.pop('tokenizer_config_file', None)
    if tokenizer_config_file is not None:
        init_kwargs = json.load(open(tokenizer_config_file, encoding='utf-8'))
        saved_init_inputs = init_kwargs.pop('init_inputs', ())
        if not init_inputs:
            init_inputs = saved_init_inputs
    else:
        init_kwargs = init_configuration
    init_kwargs.update(kwargs)
    if pretrained_model_name_or_path in cls.max_model_input_sizes:
        max_len = cls.max_model_input_sizes[pretrained_model_name_or_path]
        if max_len is not None and isinstance(max_len, (int, float)):
            init_kwargs['max_len'] = min(init_kwargs.get('max_len', int(1000000000000.0)), max_len)
    added_tokens_file = resolved_vocab_files.pop('added_tokens_file', None)
    special_tokens_map_file = resolved_vocab_files.pop('special_tokens_map_file', None)
    for (args_name, file_path) in resolved_vocab_files.items():
        if args_name not in init_kwargs:
            init_kwargs[args_name] = file_path
    if special_tokens_map_file is not None:
        special_tokens_map = json.load(open(special_tokens_map_file, encoding='utf-8'))
        for (key, value) in special_tokens_map.items():
            if key not in init_kwargs:
                init_kwargs[key] = value
    tokenizer = cls(*init_inputs, **init_kwargs)
    tokenizer.init_inputs = init_inputs
    tokenizer.init_kwargs = init_kwargs
    if added_tokens_file is not None:
        added_tok_encoder = json.load(open(added_tokens_file, encoding='utf-8'))
        added_tok_decoder = {v: k for (k, v) in added_tok_encoder.items()}
        tokenizer.added_tokens_encoder.update(added_tok_encoder)
        tokenizer.added_tokens_decoder.update(added_tok_decoder)
    return tokenizer","for (file_id, file_path) in vocab_files.items():
    if file_path is None:
        resolved_vocab_files[file_id] = None
    else:
        resolved_vocab_files[file_id] = cached_path(file_path, cache_dir=cache_dir, force_download=force_download, proxies=proxies)",XXX,no_found,0,,,
random-network-distillation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/random-network-distillation/mpi_util.py,https://github.com/openai/random-network-distillation/tree/master//mpi_util.py,,dict_gather_mean$111,"def dict_gather_mean(comm, d):
    alldicts = comm.allgather(d)
    size = comm.Get_size()
    k2li = defaultdict(list)
    for d in alldicts:
        for (k, v) in d.items():
            k2li[k].append(v)
    k2mean = {}
    for (k, li) in k2li.items():
        k2mean[k] = np.mean(li, axis=0) if len(li) == size else np.nan
    return k2mean","for (k, li) in k2li.items():
    k2mean[k] = np.mean(li, axis=0) if len(li) == size else np.nan",XXX,no_found,,,,
causalnex,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/causalnex/causalnex/ebaybbn/bbn.py,https://github.com/quantumblacklabs/causalnex/tree/master/causalnex/ebaybbn/bbn.py,JoinTree,initialize_potentials$202,"def initialize_potentials(self, assignments, bbn, evidence={}):
    for node in self.nodes:
        tt = {}
        vals = []
        variables = node.variable_names
        variables.sort()
        for variable in variables:
            domain = bbn.domains.get(variable, [True, False])
            vals.append(list(product([variable], domain)))
        permutations = product(*vals)
        for permutation in permutations:
            tt[permutation] = 1
        node.potential_tt = tt
    for (clique, bbn_nodes) in assignments.items():
        tt = {}
        vals = []
        variables = list(clique.variable_names)
        variables.sort()
        for variable in variables:
            domain = bbn.domains.get(variable, [True, False])
            vals.append(list(product([variable], domain)))
        permutations = product(*vals)
        for permutation in permutations:
            argvals = dict(permutation)
            potential = 1
            for bbn_node in bbn_nodes:
                bbn_node.clique = clique
                arg_list = []
                for arg_name in get_args(bbn_node.func):
                    arg_list.append(argvals[arg_name])
                potential *= bbn_node.func(*arg_list)
            tt[permutation] = potential
        clique.potential_tt = tt
    if not evidence:
        return
    self.initial_likelihoods(assignments, bbn)
    for (clique, bbn_nodes) in assignments.items():
        for node in bbn_nodes:
            if node.variable_name in evidence:
                for (k, v) in list(clique.potential_tt.items()):
                    for (variable, value) in k:
                        if variable == node.variable_name:
                            if value != evidence[variable]:
                                clique.potential_tt[k] = 0","for permutation in permutations:
    tt[permutation] = 1",XXX,no_found,,,,
causalnex,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/causalnex/causalnex/ebaybbn/bbn.py,https://github.com/quantumblacklabs/causalnex/tree/master/causalnex/ebaybbn/bbn.py,JoinTree,initialize_potentials$202,"def initialize_potentials(self, assignments, bbn, evidence={}):
    for node in self.nodes:
        tt = {}
        vals = []
        variables = node.variable_names
        variables.sort()
        for variable in variables:
            domain = bbn.domains.get(variable, [True, False])
            vals.append(list(product([variable], domain)))
        permutations = product(*vals)
        for permutation in permutations:
            tt[permutation] = 1
        node.potential_tt = tt
    for (clique, bbn_nodes) in assignments.items():
        tt = {}
        vals = []
        variables = list(clique.variable_names)
        variables.sort()
        for variable in variables:
            domain = bbn.domains.get(variable, [True, False])
            vals.append(list(product([variable], domain)))
        permutations = product(*vals)
        for permutation in permutations:
            argvals = dict(permutation)
            potential = 1
            for bbn_node in bbn_nodes:
                bbn_node.clique = clique
                arg_list = []
                for arg_name in get_args(bbn_node.func):
                    arg_list.append(argvals[arg_name])
                potential *= bbn_node.func(*arg_list)
            tt[permutation] = potential
        clique.potential_tt = tt
    if not evidence:
        return
    self.initial_likelihoods(assignments, bbn)
    for (clique, bbn_nodes) in assignments.items():
        for node in bbn_nodes:
            if node.variable_name in evidence:
                for (k, v) in list(clique.potential_tt.items()):
                    for (variable, value) in k:
                        if variable == node.variable_name:
                            if value != evidence[variable]:
                                clique.potential_tt[k] = 0","for permutation in permutations:
    argvals = dict(permutation)
    potential = 1
    for bbn_node in bbn_nodes:
        bbn_node.clique = clique
        arg_list = []
        for arg_name in get_args(bbn_node.func):
            arg_list.append(argvals[arg_name])
        potential *= bbn_node.func(*arg_list)
    tt[permutation] = potential",XXX,no_found,0,,,
PatrickStar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PatrickStar/patrickstar/core/preprocess.py,https://github.com/Tencent/PatrickStar/tree/master/patrickstar/core/preprocess.py,,cast_forward$89,"def cast_forward(module, dtype):
    if not isinstance(dtype, torch.dtype):
        raise ValueError('dtype should be of torch.dtype.')
    old_forward = module.forward

    def forward(*args, **kwargs):
        casted_args = []
        for arg in args:
            if isinstance(arg, torch.Tensor) and torch.is_floating_point(arg):
                casted_args.append(arg.to(dtype))
            else:
                casted_args.append(arg)
        casted_kwargs = {}
        for (k, v) in kwargs.items():
            if isinstance(v, torch.Tensor) and torch.is_floating_point(v):
                casted_kwargs[k] = v.to(dtype)
            else:
                casted_kwargs[k] = v
        return old_forward(*casted_args, **casted_kwargs)
    module.forward = forward","for (k, v) in kwargs.items():
    if isinstance(v, torch.Tensor) and torch.is_floating_point(v):
        casted_kwargs[k] = v.to(dtype)
    else:
        casted_kwargs[k] = v",XXX,no_found,,,,
pyglossary,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglossary/pyglossary/plugin_lib/readmdict.py,https://github.com/ilius/pyglossary/tree/master/pyglossary/plugin_lib/readmdict.py,MDict,_read_header$308,"def _read_header(self):
    f = open(self._fname, 'rb')
    header_bytes_size = unpack('>I', f.read(4))[0]
    header_bytes = f.read(header_bytes_size)
    adler32 = unpack('<I', f.read(4))[0]
    assert adler32 == zlib.adler32(header_bytes) & 4294967295
    self._key_block_offset = f.tell()
    f.close()
    if header_bytes[-2:] == b'\x00\x00':
        header_text = header_bytes[:-2].decode('utf-16').encode('utf-8')
    else:
        header_text = header_bytes[:-1]
    header_tag = self._parse_header(header_text)
    if not self._encoding:
        encoding = header_tag.get(b'Encoding', b'utf-8')
        if sys.hexversion >= 50331648:
            encoding = encoding.decode('utf-8')
        if encoding in ['GBK', 'GB2312']:
            encoding = 'GB18030'
        self._encoding = encoding
    if b'Encrypted' not in header_tag or header_tag[b'Encrypted'] == b'No':
        self._encrypt = 0
    elif header_tag[b'Encrypted'] == b'Yes':
        self._encrypt = 1
    else:
        self._encrypt = int(header_tag[b'Encrypted'])
    self._stylesheet = {}
    if header_tag.get('StyleSheet'):
        lines = header_tag['StyleSheet'].splitlines()
        for i in range(0, len(lines), 3):
            self._stylesheet[lines[i]] = (lines[i + 1], lines[i + 2])
    self._version = float(header_tag[b'GeneratedByEngineVersion'])
    if self._version < 2.0:
        self._number_width = 4
        self._number_format = '>I'
    else:
        self._number_width = 8
        self._number_format = '>Q'
        if self._version >= 3:
            self._encoding = 'UTF-8'
    return header_tag","for i in range(0, len(lines), 3):
    self._stylesheet[lines[i]] = (lines[i + 1], lines[i + 2])",XXX,no_found,0,,,
dcc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dcc/dex2c/graph.py,https://github.com/amimo/dcc/tree/master/dex2c/graph.py,,construct$289,"def construct(start_block):
    bfs_blocks = bfs(start_block)
    graph = Graph()
    block_to_node = {}
    node_to_landing_pad = {}
    exception_to_landing_pad = {}
    for block in bfs_blocks:
        node = block_to_node.get(block)
        if node is None:
            node = IrBasicBlock(block)
            block_to_node[block] = node
        graph.add_node(node)
        if block.exception_analysis:
            if block.exception_analysis in exception_to_landing_pad:
                landing_pad = exception_to_landing_pad[block.exception_analysis]
            else:
                landing_pad = LandingPad(node)
                graph.add_landing_pad(landing_pad)
                exception_to_landing_pad[block.exception_analysis] = landing_pad
                for (_type, _, exception_target) in block.exception_analysis.exceptions:
                    catch_node = block_to_node.get(exception_target)
                    if catch_node is None:
                        catch_node = IrBasicBlock(exception_target)
                        block_to_node[exception_target] = catch_node
                    catch_node.set_catch_type(_type)
                    catch_node.in_catch = True
                    landing_pad.add_catch_handle(_type, catch_node)
            for (_type, _, exception_target) in block.exception_analysis.exceptions:
                catch_node = block_to_node.get(exception_target)
                assert catch_node is not None
                node.add_catch_successor(catch_node)
                graph.add_catch_edge(node, catch_node)
            node_to_landing_pad[node] = landing_pad
        for (_, _, child_block) in block.childs:
            child_node = block_to_node.get(child_block)
            if child_node is None:
                child_node = IrBasicBlock(child_block)
                block_to_node[child_block] = child_node
            graph.add_edge(node, child_node)
    graph.entry = block_to_node[start_block]
    graph.compute_rpo()
    offset_to_node = {}
    for node in graph.rpo:
        if node.start >= 0:
            offset_to_node[node.start] = node
    graph.node_to_landing_pad = node_to_landing_pad
    graph.offset_to_node = offset_to_node
    for node in graph.rpo:
        preds = [pred for pred in graph.all_preds(node) if pred.num < node.num]
        if preds and all((pred.in_catch for pred in preds)):
            node.in_catch = True
    return graph","for node in graph.rpo:
    if node.start >= 0:
        offset_to_node[node.start] = node",XXX,no_found,0,,,
server-tools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/server-tools/sql_export/wizard/wizard_file.py,https://github.com/OCA/server-tools/tree/master/sql_export/wizard/wizard_file.py,SqlFileWizard,export_sql$56,"def export_sql(self):
    self.ensure_one()
    sql_export = self.sql_export_id
    variable_dict = {}
    now_tz = fields.Datetime.context_timestamp(sql_export, datetime.now())
    date = now_tz.strftime(DEFAULT_SERVER_DATETIME_FORMAT)
    if sql_export.field_ids:
        for field in sql_export.field_ids:
            if field.ttype == 'many2one':
                variable_dict[field.name] = self[field.name].id
            elif field.ttype == 'many2many':
                variable_dict[field.name] = tuple(self[field.name].ids)
            else:
                variable_dict[field.name] = self[field.name]
    if '%(company_id)s' in sql_export.query:
        company_id = self.env.company.id
        variable_dict['company_id'] = company_id
    if '%(user_id)s' in sql_export.query:
        user_id = self.env.user.id
        variable_dict['user_id'] = user_id
    method_name = '%s_get_data_from_query' % sql_export.file_format
    data = getattr(sql_export, method_name)(variable_dict)
    extension = sql_export._get_file_extension()
    self.write({'binary_file': data, 'file_name': '%(name)s_%(date)s.%(extension)s' % {'name': sql_export.name, 'date': date, 'extension': extension}})
    return {'view_mode': 'form', 'res_model': 'sql.file.wizard', 'res_id': self.id, 'type': 'ir.actions.act_window', 'target': 'new', 'context': self.env.context, 'nodestroy': True}","for field in sql_export.field_ids:
    if field.ttype == 'many2one':
        variable_dict[field.name] = self[field.name].id
    elif field.ttype == 'many2many':
        variable_dict[field.name] = tuple(self[field.name].ids)
    else:
        variable_dict[field.name] = self[field.name]",XXX,no_found,,,,
orator,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/orator/orator/orm/builder.py,https://github.com/sdispater/orator/tree/master/orator/orm/builder.py,Builder,_nested_relations$518,"def _nested_relations(self, relation):
    """"""
        Get the deeply nested relations for a given top-level relation.

        :rtype: dict
        """"""
    nested = {}
    for (name, constraints) in self._eager_load.items():
        if self._is_nested(name, relation):
            nested[name[len(relation + '.'):]] = constraints
    return nested","for (name, constraints) in self._eager_load.items():
    if self._is_nested(name, relation):
        nested[name[len(relation + '.'):]] = constraints",XXX,no_found,,,,
pyroute2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyroute2/pyroute2.core/pr2modules/common.py,https://github.com/svinota/pyroute2/tree/master/pyroute2.core/pr2modules/common.py,,metaclass$603,"def metaclass(mc):

    def wrapped(cls):
        nvars = {}
        skip = ['__dict__', '__weakref__']
        slots = cls.__dict__.get('__slots__')
        if not isinstance(slots, (list, tuple)):
            slots = [slots]
        for k in slots:
            skip.append(k)
        for (k, v) in cls.__dict__.items():
            if k not in skip:
                nvars[k] = v
        return mc(cls.__name__, cls.__bases__, nvars)
    return wrapped","for (k, v) in cls.__dict__.items():
    if k not in skip:
        nvars[k] = v",XXX,no_found,0,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/cloud/clouds/msazure.py,https://github.com/saltstack/salt/tree/master/salt/cloud/clouds/msazure.py,,avail_locations$152,"def avail_locations(conn=None, call=None):
    """"""
    List available locations for Azure
    """"""
    if call == 'action':
        raise SaltCloudSystemExit('The avail_locations function must be called with -f or --function, or with the --list-locations option')
    if not conn:
        conn = get_conn()
    ret = {}
    locations = conn.list_locations()
    for location in locations:
        ret[location.name] = {'name': location.name, 'display_name': location.display_name, 'available_services': location.available_services}
    return ret","for location in locations:
    ret[location.name] = {'name': location.name, 'display_name': location.display_name, 'available_services': location.available_services}",XXX,no_found,,,,
edx-platform,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/lms/djangoapps/course_api/blocks/transformers/tests/test_video_urls.py,https://github.com/edx/edx-platform/tree/master/lms/djangoapps/course_api/blocks/transformers/tests/test_video_urls.py,TestVideoBlockURLTransformer,change_encoded_videos_presentation$34,"def change_encoded_videos_presentation(self, encoded_videos):
    """"""
        Relocate url data in new dictionary for pre & post transformation data comparison.
        """"""
    video_urls = {}
    for (video_format, video_data) in encoded_videos.items():
        video_urls[video_format] = video_data['url']
    return video_urls","for (video_format, video_data) in encoded_videos.items():
    video_urls[video_format] = video_data['url']",XXX,no_found,,,,
PGL,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/redis_graph.py,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/redis_graph.py,RedisGraph,subgraph$412,"def subgraph(self, nodes, eid, edges=None):
    """"""Generate subgraph with nodes and edge ids.

        This function will generate a :code:`pgl.graph.Subgraph` object and
        copy all corresponding node and edge features. Nodes and edges will
        be reindex from 0.

        WARNING: ALL NODES IN EID MUST BE INCLUDED BY NODES

        Args:
            nodes: Node ids which will be included in the subgraph.

            eid: Edge ids which will be included in the subgraph.

        Return:
            A :code:`pgl.graph.Subgraph` object.
        """"""
    reindex = {}
    for (ind, node) in enumerate(nodes):
        reindex[node] = ind
    if edges is None:
        edges = self.get_edges_by_id(eid)
    else:
        edges = np.array(edges, dtype='int64')
    sub_edges = graph_kernel.map_edges(np.arange(len(edges), dtype='int64'), edges, reindex)
    sub_edge_feat = {}
    for (key, _, _) in self.edge_feat_info():
        sub_edge_feat[key] = self.get_edge_feat_by_id(key, eid)
    sub_node_feat = {}
    for (key, _, _) in self.node_feat_info():
        sub_node_feat[key] = self.get_node_feat_by_id(key, nodes)
    subgraph = pgraph.SubGraph(num_nodes=len(nodes), edges=sub_edges, node_feat=sub_node_feat, edge_feat=sub_edge_feat, reindex=reindex)
    return subgraph","for (ind, node) in enumerate(nodes):
    reindex[node] = ind",XXX,no_found,0,,,
PGL,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/redis_graph.py,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/redis_graph.py,RedisGraph,subgraph$412,"def subgraph(self, nodes, eid, edges=None):
    """"""Generate subgraph with nodes and edge ids.

        This function will generate a :code:`pgl.graph.Subgraph` object and
        copy all corresponding node and edge features. Nodes and edges will
        be reindex from 0.

        WARNING: ALL NODES IN EID MUST BE INCLUDED BY NODES

        Args:
            nodes: Node ids which will be included in the subgraph.

            eid: Edge ids which will be included in the subgraph.

        Return:
            A :code:`pgl.graph.Subgraph` object.
        """"""
    reindex = {}
    for (ind, node) in enumerate(nodes):
        reindex[node] = ind
    if edges is None:
        edges = self.get_edges_by_id(eid)
    else:
        edges = np.array(edges, dtype='int64')
    sub_edges = graph_kernel.map_edges(np.arange(len(edges), dtype='int64'), edges, reindex)
    sub_edge_feat = {}
    for (key, _, _) in self.edge_feat_info():
        sub_edge_feat[key] = self.get_edge_feat_by_id(key, eid)
    sub_node_feat = {}
    for (key, _, _) in self.node_feat_info():
        sub_node_feat[key] = self.get_node_feat_by_id(key, nodes)
    subgraph = pgraph.SubGraph(num_nodes=len(nodes), edges=sub_edges, node_feat=sub_node_feat, edge_feat=sub_edge_feat, reindex=reindex)
    return subgraph","for (key, _, _) in self.edge_feat_info():
    sub_edge_feat[key] = self.get_edge_feat_by_id(key, eid)",XXX,no_found,1,,,
PGL,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGL/legacy/pgl/redis_graph.py,https://github.com/PaddlePaddle/PGL/tree/master/legacy/pgl/redis_graph.py,RedisGraph,subgraph$412,"def subgraph(self, nodes, eid, edges=None):
    """"""Generate subgraph with nodes and edge ids.

        This function will generate a :code:`pgl.graph.Subgraph` object and
        copy all corresponding node and edge features. Nodes and edges will
        be reindex from 0.

        WARNING: ALL NODES IN EID MUST BE INCLUDED BY NODES

        Args:
            nodes: Node ids which will be included in the subgraph.

            eid: Edge ids which will be included in the subgraph.

        Return:
            A :code:`pgl.graph.Subgraph` object.
        """"""
    reindex = {}
    for (ind, node) in enumerate(nodes):
        reindex[node] = ind
    if edges is None:
        edges = self.get_edges_by_id(eid)
    else:
        edges = np.array(edges, dtype='int64')
    sub_edges = graph_kernel.map_edges(np.arange(len(edges), dtype='int64'), edges, reindex)
    sub_edge_feat = {}
    for (key, _, _) in self.edge_feat_info():
        sub_edge_feat[key] = self.get_edge_feat_by_id(key, eid)
    sub_node_feat = {}
    for (key, _, _) in self.node_feat_info():
        sub_node_feat[key] = self.get_node_feat_by_id(key, nodes)
    subgraph = pgraph.SubGraph(num_nodes=len(nodes), edges=sub_edges, node_feat=sub_node_feat, edge_feat=sub_edge_feat, reindex=reindex)
    return subgraph","for (key, _, _) in self.node_feat_info():
    sub_node_feat[key] = self.get_node_feat_by_id(key, nodes)",XXX,no_found,1,,,
tapiriik,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tapiriik/tapiriik/web/views/ab.py,https://github.com/cpfair/tapiriik/tree/master/tapiriik/web/views/ab.py,,ab_experiment_context$43,"def ab_experiment_context(request):
    context = {}
    if request.user:
        for key in _experiments.keys():
            context['ab_%s_%s' % (key, ab_select_variant(key, request.user['_id']))] = True
    return context","for key in _experiments.keys():
    context['ab_%s_%s' % (key, ab_select_variant(key, request.user['_id']))] = True",XXX,no_found,,,,
Open3D-ML,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Open3D-ML/scripts/preprocess_scannet.py,https://github.com/isl-org/Open3D-ML/tree/master/scripts/preprocess_scannet.py,ScannetProcess,read_label_mapping$188,"def read_label_mapping(filename, label_from='raw_category', label_to='nyu40id'):
    assert os.path.isfile(filename)
    mapping = dict()
    with open(filename) as csvfile:
        reader = csv.DictReader(csvfile, delimiter='\t')
        for row in reader:
            mapping[row[label_from]] = int(row[label_to])
    if represents_int(list(mapping.keys())[0]):
        mapping = {int(k): v for (k, v) in mapping.items()}
    return mapping","for row in reader:
    mapping[row[label_from]] = int(row[label_to])",XXX,no_found,,,,
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/core/fp16/decorators.py,https://github.com/poodarchu/Det3D/tree/master/det3d/core/fp16/decorators.py,,force_fp32$81,"def force_fp32(apply_to=None, out_fp16=False):
    """"""Decorator to convert input arguments to fp32 in force.
    This decorator is useful when you write custom modules and want to support
    mixed precision training. If there are some inputs that must be processed
    in fp32 mode, then this decorator can handle it. If inputs arguments are
    fp16 tensors, they will be converted to fp32 automatically. Arguments other
    than fp16 tensors are ignored.
    Args:
        apply_to (Iterable, optional): The argument names to be converted.
            `None` indicates all arguments.
        out_fp16 (bool): Whether to convert the output back to fp16.
    :Example:
        class MyModule1(nn.Module)
            # Convert x and y to fp32
            @force_fp32()
            def loss(self, x, y):
                pass
        class MyModule2(nn.Module):
            # convert pred to fp32
            @force_fp32(apply_to=('pred', ))
            def post_process(self, pred, others):
                pass
    """"""

    def force_fp32_wrapper(old_func):

        @functools.wraps(old_func)
        def new_func(*args, **kwargs):
            if not isinstance(args[0], torch.nn.Module):
                raise TypeError('@force_fp32 can only be used to decorate the method of nn.Module')
            if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
                return old_func(*args, **kwargs)
            args_info = getfullargspec(old_func)
            args_to_cast = args_info.args if apply_to is None else apply_to
            new_args = []
            if args:
                arg_names = args_info.args[:len(args)]
                for (i, arg_name) in enumerate(arg_names):
                    if arg_name in args_to_cast:
                        new_args.append(cast_tensor_type(args[i], torch.half, torch.float))
                    else:
                        new_args.append(args[i])
            new_kwargs = dict()
            if kwargs:
                for (arg_name, arg_value) in kwargs.items():
                    if arg_name in args_to_cast:
                        new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.half, torch.float)
                    else:
                        new_kwargs[arg_name] = arg_value
            output = old_func(*new_args, **new_kwargs)
            if out_fp16:
                output = cast_tensor_type(output, torch.float, torch.half)
            return output
        return new_func
    return force_fp32_wrapper","for (arg_name, arg_value) in kwargs.items():
    if arg_name in args_to_cast:
        new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.half, torch.float)
    else:
        new_kwargs[arg_name] = arg_value",XXX,no_found,0,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/utils/network.py,https://github.com/saltstack/salt/tree/master/salt/utils/network.py,,_subnets$1302,"def _subnets(proto='inet', interfaces_=None):
    """"""
    Returns a list of subnets to which the host belongs
    """"""
    if interfaces_ is None:
        ifaces = interfaces()
    elif isinstance(interfaces_, list):
        ifaces = {}
        for (key, value) in interfaces().items():
            if key in interfaces_:
                ifaces[key] = value
    else:
        ifaces = {interfaces_: interfaces().get(interfaces_, {})}
    ret = set()
    if proto == 'inet':
        subnet = 'netmask'
        dflt_cidr = 32
    elif proto == 'inet6':
        subnet = 'prefixlen'
        dflt_cidr = 128
    else:
        log.error('Invalid proto %s calling subnets()', proto)
        return
    for ip_info in ifaces.values():
        addrs = ip_info.get(proto, [])
        addrs.extend([addr for addr in ip_info.get('secondary', []) if addr.get('type') == proto])
        for intf in addrs:
            if subnet in intf:
                intf = ipaddress.ip_interface('{}/{}'.format(intf['address'], intf[subnet]))
            else:
                intf = ipaddress.ip_interface('{}/{}'.format(intf['address'], dflt_cidr))
            if not intf.is_loopback:
                ret.add(intf.network)
    return [str(net) for net in sorted(ret)]","for (key, value) in interfaces().items():
    if key in interfaces_:
        ifaces[key] = value",XXX,no_found,,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,InterviewStatus,as_data$768,"def as_data(self, the_user_dict, encode=True):
    result = dict(language=self.question.language)
    debug = self.question.interview.debug
    if debug:
        output = dict(question='', help='')
    if 'progress' in the_user_dict['_internal']:
        result['progress'] = the_user_dict['_internal']['progress']
    if self.question.language in self.question.interview.default_validation_messages:
        result['validation_messages'] = copy.copy(self.question.interview.default_validation_messages[self.question.language])
    else:
        result['validation_messages'] = dict()
    if 'reload_after' in self.extras:
        result['reload'] = 1000 * int(self.extras['reload_after'])
    lang = docassemble.base.functions.get_language()
    if len(self.question.terms) or len(self.question.interview.terms):
        result['terms'] = dict()
        if 'terms' in self.extras:
            for (term, vals) in self.extras['terms'].items():
                result['terms'][term] = vals['definition']
        if lang in self.question.interview.terms and len(self.question.interview.terms[lang]):
            for (term, vals) in self.question.interview.terms[lang].items():
                result['terms'][term] = vals['definition']
        elif self.question.language in self.question.interview.terms and len(self.question.interview.terms[self.question.language]):
            for (term, vals) in self.question.interview.terms[self.question.language].items():
                result['terms'][term] = vals['definition']
    if len(self.question.autoterms) or len(self.question.interview.autoterms):
        result['autoterms'] = dict()
        if 'autoterms' in self.extras:
            for (term, vals) in self.extras['autoterms'].items():
                result['autoterms'][term] = vals['definition']
        if lang in self.question.interview.autoterms and len(self.question.interview.autoterms[lang]):
            for (term, vals) in self.question.interview.autoterms[lang].items():
                result['autoterms'][term] = vals['definition']
        elif self.question.language in self.question.interview.autoterms and len(self.question.interview.autoterms[self.question.language]):
            for (term, vals) in self.question.interview.autoterms[self.question.language].items():
                result['autoterms'][term] = vals['definition']
    if self.orig_sought is not None:
        result['event_list'] = [self.orig_sought]
    if 'action_buttons' in self.extras:
        result['additional_buttons'] = []
        for item in self.extras['action_buttons']:
            new_item = copy.deepcopy(item)
            new_item['label'] = docassemble.base.filter.markdown_to_html(item['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + new_item['label'] + '</p>'
    for param in ('questionText',):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if hasattr(self, 'breadcrumb') and self.breadcrumb is not None:
        output['breadcrumb label'] = self.breadcrumb
    output['breadcrumbs'] = docassemble.base.functions.get_action_stack()
    if hasattr(self, 'subquestionText') and self.subquestionText is not None:
        if self.question.question_type == 'fields':
            embedder = dummy_embed_input
        else:
            embedder = None
        result['subquestionText'] = docassemble.base.filter.markdown_to_html(self.subquestionText.rstrip(), status=self, verbatim=not encode, embedder=embedder)
        if debug:
            output['question'] += result['subquestionText']
    for param in ('continueLabel', 'helpLabel'):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + result[param] + '</p>'
    if 'menu_items' in self.extras and isinstance(self.extras['menu_items'], list):
        result['menu_items'] = self.extras['menu_items']
    for param in ('cssClass', 'tableCssClass', 'css', 'script'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = self.extras[param].rstrip()
    for param in ('back_button_label',):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
    for param in ('rightText', 'underText'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if 'continueLabel' not in result:
        if self.question.question_type == 'review':
            result['continueLabel'] = word('Resume')
        else:
            result['continueLabel'] = word('Continue')
        if debug:
            output['question'] += '<p>' + result['continueLabel'] + '</p>'
    if self.question.question_type == 'yesno':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
    elif self.question.question_type == 'noyes':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
    elif self.question.question_type == 'yesnomaybe':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    elif self.question.question_type == 'noyesmaybe':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    steps = the_user_dict['_internal']['steps'] - the_user_dict['_internal']['steps_offset']
    if self.can_go_back and steps > 1:
        result['allow_going_back'] = True
        result['backTitle'] = word('Go back to the previous question')
        back_button_val = self.extras.get('back_button', None)
        if back_button_val or (back_button_val is None and self.question.interview.question_back_button):
            result['questionBackButton'] = self.back
    else:
        result['allow_going_back'] = False
    if self.question.question_type == 'signature':
        result['signaturePhrases'] = {'clear': word('Clear'), 'noSignature': word('You must sign your name to continue.'), 'loading': word('Loading.  Please wait . . . ')}
    if 'questionMetadata' in self.extras:
        result['question_metadata'] = self.extras['questionMetadata']
    if 'segment' in self.extras:
        result['segment'] = self.extras['segment']
    if 'ga_id' in self.extras:
        result['ga_id'] = self.extras['ga_id']
    if hasattr(self.question, 'id'):
        result['id'] = self.question.id
    if hasattr(self, 'audiovideo') and self.audiovideo is not None:
        audio_result = docassemble.base.filter.get_audio_urls(self.audiovideo)
        video_result = docassemble.base.filter.get_video_urls(self.audiovideo)
        if len(audio_result) > 0:
            result['audio'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in audio_result]
        if len(video_result) > 0:
            result['video'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in video_result]
    if hasattr(self, 'helpText') and len(self.helpText) > 0:
        result['helpText'] = list()
        result['helpBackLabel'] = word('Back to question')
        for help_text in self.helpText:
            result['helpText'].append(self.convert_help(help_text, encode, debug))
        result['help'] = dict()
        if self.helpText[0]['label']:
            result['help']['label'] = docassemble.base.filter.markdown_to_html(self.helpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['label'] = self.question.help()
        result['help']['title'] = word('Help is available for this question')
        result['help']['specific'] = False if self.question.helptext is None else True
    if hasattr(self, 'interviewHelpText') and len(self.interviewHelpText) > 0:
        result['interviewHelpText'] = list()
        for help_text in self.interviewHelpText:
            result['interviewHelpText'].append(self.convert_help(help_text, encode, debug))
        if 'help' not in result:
            result['help'] = dict()
        if self.interviewHelpText[0]['label']:
            result['help']['interviewLabel'] = docassemble.base.filter.markdown_to_html(self.interviewHelpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['interviewLabel'] = self.question.help()
        result['help']['interviewTitle'] = word('Help is available')
        if not (hasattr(self, 'helpText') and len(self.helpText) > 0):
            result['help']['specific'] = False
    if 'questionText' not in result and self.question.question_type == 'signature':
        result['questionText'] = word('Sign Your Name')
        if debug:
            output['question'] += '<p>' + result['questionText'] + '</p>'
    result['questionType'] = self.question.question_type
    if hasattr(self.question, 'question_variety'):
        result['questionVariety'] = self.question.question_variety
    if self.question.is_mandatory or self.question.mandatory_code is not None:
        result['mandatory'] = True
    if hasattr(self.question, 'name'):
        result['_question_name'] = self.question.name
    result['_tracker'] = self.tracker
    if hasattr(self, 'datatypes'):
        result['_datatypes'] = safeid(json.dumps(self.datatypes))
    if hasattr(self, 'varnames'):
        result['_varnames'] = safeid(json.dumps(self.varnames))
    if len(self.question.fields) > 0:
        result['fields'] = list()
    if hasattr(self.question, 'review_saveas'):
        result['question_variable_name'] = self.question.review_saveas
    if hasattr(self.question, 'fields_saveas'):
        result['question_variable_name'] = self.question.fields_saveas
    if self.decorations is not None:
        width_value = get_config('decoration size', 2.0)
        width_units = get_config('decoration units', 'em')
        for decoration in self.decorations:
            if 'image' in decoration:
                result['decoration'] = {}
                the_image = self.question.interview.images.get(decoration['image'], None)
                if the_image is not None:
                    the_url = docassemble.base.functions.server.url_finder(str(the_image.package) + ':' + str(the_image.filename))
                    width = str(width_value) + str(width_units)
                    filename = docassemble.base.functions.server.file_finder(str(the_image.package) + ':' + str(the_image.filename))
                    if 'extension' in filename and filename['extension'] == 'svg' and ('width' in filename):
                        if filename['width'] and filename['height']:
                            height = str(width_value * (filename['height'] / filename['width'])) + str(width_units)
                    else:
                        height = 'auto'
                    if the_url is not None:
                        result['decoration']['url'] = the_url
                        result['decoration']['size'] = {'width': width, 'height': height}
                        if the_image.attribution is not None:
                            self.attributions.add(the_image.attribution)
                        break
                elif get_config('default icons', None) in ('material icons', 'font awesome'):
                    result['decoration']['name'] = decoration['image']
                    result['decoration']['size'] = str(width_value) + str(width_units)
                    break
    if len(self.attachments) > 0:
        result['attachments'] = list()
        if self.current_info['user']['is_authenticated'] and self.current_info['user']['email']:
            result['default_email'] = self.current_info['user']['email']
        for attachment in self.attachments:
            the_attachment = dict(url=dict(), number=dict(), filename_with_extension=dict())
            if 'orig_variable_name' in attachment and attachment['orig_variable_name']:
                the_attachment['variable_name'] = attachment['orig_variable_name']
            if 'name' in attachment:
                if attachment['name']:
                    the_attachment['name'] = docassemble.base.filter.markdown_to_html(attachment['name'], trim=True, status=self, verbatim=not encode)
                    if debug:
                        output['question'] += '<p>' + the_attachment['name'] + '</p>'
            if 'description' in attachment:
                if attachment['description']:
                    the_attachment['description'] = docassemble.base.filter.markdown_to_html(attachment['description'], status=self, verbatim=not encode)
                    if debug:
                        output['question'] += the_attachment['description']
            for key in ('valid_formats', 'filename', 'content', 'markdown', 'raw'):
                if key in attachment:
                    if attachment[key]:
                        the_attachment[key] = attachment[key]
            for the_format in attachment['file']:
                the_attachment['url'][the_format] = docassemble.base.functions.server.url_finder(attachment['file'][the_format], filename=attachment['filename'] + '.' + extension_of_doc_format[the_format])
                the_attachment['number'][the_format] = attachment['file'][the_format]
                the_attachment['filename_with_extension'][the_format] = attachment['filename'] + '.' + extension_of_doc_format[the_format]
            result['attachments'].append(the_attachment)
    if self.extras.get('list_collect', False) is not False:
        result['listCollect'] = {'deleteLabel': word('Delete'), 'addAnotherLabel': self.extras['list_collect_add_another_label'] if self.extras['list_collect_add_another_label'] else word('Add another'), 'deletedLabel': word('(Deleted)'), 'undeleteLabel': word('Undelete')}
    validation_rules_used = set()
    file_fields = list()
    for field in self.question.fields:
        the_field = dict()
        the_field['number'] = field.number
        if hasattr(field, 'saveas'):
            the_field['variable_name'] = from_safeid(field.saveas)
            if encode:
                the_field['variable_name_encoded'] = field.saveas
            the_field['validation_messages'] = dict()
            if self.question.question_type == 'multiple_choice' and self.question.question_variety in ['radio', 'dropdown', 'combobox']:
                if self.question.question_variety == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
            elif not (hasattr(field, 'datatype') and field.datatype in ['multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes']):
                if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                elif hasattr(field, 'inputtype') and field.inputtype == 'ajax':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one.'))
                elif hasattr(field, 'datatype') and (field.datatype == 'object_radio' or (hasattr(field, 'inputtype') and field.inputtype in ('yesnoradio', 'noyesradio', 'radio', 'dropdown'))):
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('required', self, word('This field is required.'))
            if hasattr(field, 'inputtype') and field.inputtype in ['yesno', 'noyes', 'yesnowide', 'noyeswide'] and hasattr(field, 'uncheckothers') and (field.uncheckothers is not False):
                the_field['validation_messages']['uncheckothers'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([strip_tags(self.labels[field.number])]))
            if hasattr(field, 'datatype') and field.datatype not in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                for key in ('minlength', 'maxlength'):
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
                        if key == 'minlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'maxlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        if hasattr(field, 'datatype'):
            if field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes') and (hasattr(field, 'nota') and self.extras['nota'][field.number] is not False or (hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)))):
                if field.datatype.endswith('checkboxes'):
                    d_type = 'checkbox'
                else:
                    d_type = 'multiselect'
                if hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)):
                    checkbox_messages = dict()
                    if 'minlength' in field.extras and 'minlength' in self.extras and ('maxlength' in field.extras) and ('maxlength' in self.extras) and (self.extras['minlength'][field.number] == self.extras['maxlength'][field.number]) and (self.extras['minlength'][field.number] > 0):
                        if 'nota' not in self.extras:
                            self.extras['nota'] = dict()
                        self.extras['nota'][field.number] = False
                        if d_type == 'checkbox':
                            checkbox_messages['checkexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                        else:
                            checkbox_messages['selectexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    else:
                        if 'minlength' in field.extras and 'minlength' in self.extras:
                            if d_type == 'checkbox':
                                if self.extras['minlength'][field.number] == 1:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select one.'))
                                else:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                                if int(float(self.extras['minlength'][field.number])) > 0:
                                    if 'nota' not in self.extras:
                                        self.extras['nota'] = dict()
                                    self.extras['nota'][field.number] = False
                            elif self.extras['minlength'][field.number] == 1:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select one.'))
                            else:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                        if 'maxlength' in field.extras and 'maxlength' in self.extras:
                            if d_type == 'checkbox':
                                checkbox_messages['checkatmost'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                            else:
                                checkbox_messages['maxlength'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    the_field['validation_messages'].update(checkbox_messages)
                if d_type == 'checkbox':
                    if hasattr(field, 'nota') and self.extras['nota'][field.number] is not False:
                        the_field['validation_messages']['checkatleast'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([self.extras['nota'][field.number]]))
            if field.datatype == 'date':
                the_field['validation_messages']['date'] = field.validation_message('date', self, word('You need to enter a valid date.'))
                if hasattr(field, 'extras') and 'min' in field.extras and ('min' in self.extras) and ('max' in field.extras) and ('max' in self.extras) and (field.number in self.extras['min']) and (field.number in self.extras['max']):
                    the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.extras['min'][field.number], format='medium'), docassemble.base.util.format_date(self.extras['max'][field.number], format='medium')))
                else:
                    was_defined = dict()
                    for key in ['min', 'max']:
                        if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                            was_defined[key] = True
                            if key == 'min':
                                the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                            elif key == 'max':
                                the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                    if len(was_defined) == 0 and 'default date min' in self.question.interview.options and ('default date max' in self.question.interview.options):
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium'), docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')))
                    elif 'max' not in was_defined and 'default date max' in self.question.interview.options:
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')]))
                    elif 'min' not in was_defined and 'default date min' in self.question.interview.options:
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium')]))
            if field.datatype == 'time':
                the_field['validation_messages']['time'] = field.validation_message('time', self, word('You need to enter a valid time.'))
            if field.datatype in ['datetime', 'datetime-local']:
                the_field['validation_messages']['datetime'] = field.validation_message('datetime', self, word('You need to enter a valid date and time.'))
            if field.datatype == 'email':
                the_field['validation_messages']['email'] = field.validation_message('email', self, word('You need to enter a complete e-mail address.'))
            if field.datatype in ['number', 'currency', 'float', 'integer']:
                the_field['validation_messages']['number'] = field.validation_message('number', self, word('You need to enter a number.'))
                if field.datatype == 'integer' and (not ('step' in self.extras and field.number in self.extras['step'])):
                    the_field['validation_messages']['step'] = field.validation_message('integer', self, word('Please enter a whole number.'))
                elif 'step' in self.extras and field.number in self.extras['step']:
                    the_field['validation_messages']['step'] = field.validation_message('step', self, word('Please enter a multiple of {0}.'))
                for key in ['min', 'max']:
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                        if key == 'min':
                            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'max':
                            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))
            if field.datatype in ['files', 'file', 'camera', 'user', 'environment', 'camcorder', 'microphone']:
                file_fields.append(field)
                the_field['validation_messages']['required'] = field.validation_message('file required', self, word('You must provide a file.'))
                if 'accept' in self.extras and field.number in self.extras['accept']:
                    the_field['validation_messages']['accept'] = field.validation_message('accept', self, word('Please upload a file with a valid file format.'))
                if get_config('maximum content length') is not None:
                    the_field['max'] = get_config('maximum content length')
                    the_field['validation_messages']['max'] = field.validation_message('maxuploadsize', self, word('Your file upload is larger than the server can accept. Please reduce the size of your file upload.'))
        for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
            if hasattr(field, param):
                the_field[param] = getattr(field, param)
        if hasattr(field, 'shuffle') and field.shuffle is not False:
            the_field['shuffle'] = True
        if hasattr(field, 'disableothers') and field.disableothers and hasattr(field, 'saveas'):
            the_field['disable_others'] = True
        if hasattr(field, 'uncheckothers') and field.uncheckothers is not False:
            the_field['uncheck_others'] = True
        for key in ('minlength', 'maxlength', 'min', 'max', 'step', 'scale', 'inline', 'inline width', 'rows', 'accept', 'currency symbol', 'field metadata', 'css class'):
            if key in self.extras and field.number in self.extras[key]:
                if key in ('minlength', 'maxlength', 'min', 'max', 'step'):
                    validation_rules_used.add(key)
                the_field[key] = self.extras[key][field.number]
        if hasattr(field, 'extras') and 'custom_parameters' in field.extras:
            for (parameter, parameter_value) in field.extras['custom_parameters'].items():
                the_field[parameter] = parameter_value
        for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
            if param_type in self.extras and field.number in self.extras[param_type]:
                for (parameter, parameter_value) in self.extras[param_type][field.number].items():
                    the_field[parameter] = parameter_value
        if hasattr(field, 'saveas') and field.saveas in self.embedded:
            the_field['embedded'] = True
        if hasattr(self, 'shuffle'):
            the_field['shuffle'] = self.shuffle
        if field.number in self.defaults:
            the_default = self.defaults[field.number]
            if isinstance(the_default, (str, int, bool, float)):
                the_field['default'] = the_default
        else:
            the_default = None
        if self.question.question_type == 'multiple_choice' or hasattr(field, 'choicetype') or (hasattr(field, 'datatype') and field.datatype in ('object', 'multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes', 'object_radio')):
            the_field['choices'] = self.get_choices_data(field, the_default, the_user_dict, encode=encode)
        if hasattr(field, 'nota'):
            the_field['none_of_the_above'] = docassemble.base.filter.markdown_to_html(self.extras['nota'][field.number], do_terms=False, status=self, verbatim=not encode)
        if field.number in self.extras['ok']:
            the_field['active'] = self.extras['ok'][field.number]
        else:
            the_field['active'] = True
        if field.number in self.extras['required']:
            the_field['required'] = self.extras['required'][field.number]
            if the_field['required']:
                validation_rules_used.add('required')
        if 'validation messages' in self.extras and field.number in self.extras['validation messages']:
            the_field['validation_messages'].update(self.extras['validation messages'][field.number])
        if 'permissions' in self.extras and field.number in self.extras['permissions']:
            the_field['permissions'] = self.extras['permissions'][field.number]
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('max_image_size' in self.extras) and self.extras['max_image_size']:
            the_field['max_image_size'] = self.extras['max_image_size']
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('image_type' in self.extras) and self.extras['image_type']:
            the_field['image_type'] = self.extras['image_type']
        if hasattr(field, 'extras'):
            if 'ml_group' in field.extras or 'ml_train' in field.extras:","for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
    if hasattr(field, param):
        the_field[param] = getattr(field, param)",XXX,no_found,0,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,InterviewStatus,as_data$768,"def as_data(self, the_user_dict, encode=True):
    result = dict(language=self.question.language)
    debug = self.question.interview.debug
    if debug:
        output = dict(question='', help='')
    if 'progress' in the_user_dict['_internal']:
        result['progress'] = the_user_dict['_internal']['progress']
    if self.question.language in self.question.interview.default_validation_messages:
        result['validation_messages'] = copy.copy(self.question.interview.default_validation_messages[self.question.language])
    else:
        result['validation_messages'] = dict()
    if 'reload_after' in self.extras:
        result['reload'] = 1000 * int(self.extras['reload_after'])
    lang = docassemble.base.functions.get_language()
    if len(self.question.terms) or len(self.question.interview.terms):
        result['terms'] = dict()
        if 'terms' in self.extras:
            for (term, vals) in self.extras['terms'].items():
                result['terms'][term] = vals['definition']
        if lang in self.question.interview.terms and len(self.question.interview.terms[lang]):
            for (term, vals) in self.question.interview.terms[lang].items():
                result['terms'][term] = vals['definition']
        elif self.question.language in self.question.interview.terms and len(self.question.interview.terms[self.question.language]):
            for (term, vals) in self.question.interview.terms[self.question.language].items():
                result['terms'][term] = vals['definition']
    if len(self.question.autoterms) or len(self.question.interview.autoterms):
        result['autoterms'] = dict()
        if 'autoterms' in self.extras:
            for (term, vals) in self.extras['autoterms'].items():
                result['autoterms'][term] = vals['definition']
        if lang in self.question.interview.autoterms and len(self.question.interview.autoterms[lang]):
            for (term, vals) in self.question.interview.autoterms[lang].items():
                result['autoterms'][term] = vals['definition']
        elif self.question.language in self.question.interview.autoterms and len(self.question.interview.autoterms[self.question.language]):
            for (term, vals) in self.question.interview.autoterms[self.question.language].items():
                result['autoterms'][term] = vals['definition']
    if self.orig_sought is not None:
        result['event_list'] = [self.orig_sought]
    if 'action_buttons' in self.extras:
        result['additional_buttons'] = []
        for item in self.extras['action_buttons']:
            new_item = copy.deepcopy(item)
            new_item['label'] = docassemble.base.filter.markdown_to_html(item['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + new_item['label'] + '</p>'
    for param in ('questionText',):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if hasattr(self, 'breadcrumb') and self.breadcrumb is not None:
        output['breadcrumb label'] = self.breadcrumb
    output['breadcrumbs'] = docassemble.base.functions.get_action_stack()
    if hasattr(self, 'subquestionText') and self.subquestionText is not None:
        if self.question.question_type == 'fields':
            embedder = dummy_embed_input
        else:
            embedder = None
        result['subquestionText'] = docassemble.base.filter.markdown_to_html(self.subquestionText.rstrip(), status=self, verbatim=not encode, embedder=embedder)
        if debug:
            output['question'] += result['subquestionText']
    for param in ('continueLabel', 'helpLabel'):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + result[param] + '</p>'
    if 'menu_items' in self.extras and isinstance(self.extras['menu_items'], list):
        result['menu_items'] = self.extras['menu_items']
    for param in ('cssClass', 'tableCssClass', 'css', 'script'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = self.extras[param].rstrip()
    for param in ('back_button_label',):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
    for param in ('rightText', 'underText'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if 'continueLabel' not in result:
        if self.question.question_type == 'review':
            result['continueLabel'] = word('Resume')
        else:
            result['continueLabel'] = word('Continue')
        if debug:
            output['question'] += '<p>' + result['continueLabel'] + '</p>'
    if self.question.question_type == 'yesno':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
    elif self.question.question_type == 'noyes':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
    elif self.question.question_type == 'yesnomaybe':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    elif self.question.question_type == 'noyesmaybe':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    steps = the_user_dict['_internal']['steps'] - the_user_dict['_internal']['steps_offset']
    if self.can_go_back and steps > 1:
        result['allow_going_back'] = True
        result['backTitle'] = word('Go back to the previous question')
        back_button_val = self.extras.get('back_button', None)
        if back_button_val or (back_button_val is None and self.question.interview.question_back_button):
            result['questionBackButton'] = self.back
    else:
        result['allow_going_back'] = False
    if self.question.question_type == 'signature':
        result['signaturePhrases'] = {'clear': word('Clear'), 'noSignature': word('You must sign your name to continue.'), 'loading': word('Loading.  Please wait . . . ')}
    if 'questionMetadata' in self.extras:
        result['question_metadata'] = self.extras['questionMetadata']
    if 'segment' in self.extras:
        result['segment'] = self.extras['segment']
    if 'ga_id' in self.extras:
        result['ga_id'] = self.extras['ga_id']
    if hasattr(self.question, 'id'):
        result['id'] = self.question.id
    if hasattr(self, 'audiovideo') and self.audiovideo is not None:
        audio_result = docassemble.base.filter.get_audio_urls(self.audiovideo)
        video_result = docassemble.base.filter.get_video_urls(self.audiovideo)
        if len(audio_result) > 0:
            result['audio'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in audio_result]
        if len(video_result) > 0:
            result['video'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in video_result]
    if hasattr(self, 'helpText') and len(self.helpText) > 0:
        result['helpText'] = list()
        result['helpBackLabel'] = word('Back to question')
        for help_text in self.helpText:
            result['helpText'].append(self.convert_help(help_text, encode, debug))
        result['help'] = dict()
        if self.helpText[0]['label']:
            result['help']['label'] = docassemble.base.filter.markdown_to_html(self.helpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['label'] = self.question.help()
        result['help']['title'] = word('Help is available for this question')
        result['help']['specific'] = False if self.question.helptext is None else True
    if hasattr(self, 'interviewHelpText') and len(self.interviewHelpText) > 0:
        result['interviewHelpText'] = list()
        for help_text in self.interviewHelpText:
            result['interviewHelpText'].append(self.convert_help(help_text, encode, debug))
        if 'help' not in result:
            result['help'] = dict()
        if self.interviewHelpText[0]['label']:
            result['help']['interviewLabel'] = docassemble.base.filter.markdown_to_html(self.interviewHelpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['interviewLabel'] = self.question.help()
        result['help']['interviewTitle'] = word('Help is available')
        if not (hasattr(self, 'helpText') and len(self.helpText) > 0):
            result['help']['specific'] = False
    if 'questionText' not in result and self.question.question_type == 'signature':
        result['questionText'] = word('Sign Your Name')
        if debug:
            output['question'] += '<p>' + result['questionText'] + '</p>'
    result['questionType'] = self.question.question_type
    if hasattr(self.question, 'question_variety'):
        result['questionVariety'] = self.question.question_variety
    if self.question.is_mandatory or self.question.mandatory_code is not None:
        result['mandatory'] = True
    if hasattr(self.question, 'name'):
        result['_question_name'] = self.question.name
    result['_tracker'] = self.tracker
    if hasattr(self, 'datatypes'):
        result['_datatypes'] = safeid(json.dumps(self.datatypes))
    if hasattr(self, 'varnames'):
        result['_varnames'] = safeid(json.dumps(self.varnames))
    if len(self.question.fields) > 0:
        result['fields'] = list()
    if hasattr(self.question, 'review_saveas'):
        result['question_variable_name'] = self.question.review_saveas
    if hasattr(self.question, 'fields_saveas'):
        result['question_variable_name'] = self.question.fields_saveas
    if self.decorations is not None:
        width_value = get_config('decoration size', 2.0)
        width_units = get_config('decoration units', 'em')
        for decoration in self.decorations:
            if 'image' in decoration:
                result['decoration'] = {}
                the_image = self.question.interview.images.get(decoration['image'], None)
                if the_image is not None:
                    the_url = docassemble.base.functions.server.url_finder(str(the_image.package) + ':' + str(the_image.filename))
                    width = str(width_value) + str(width_units)
                    filename = docassemble.base.functions.server.file_finder(str(the_image.package) + ':' + str(the_image.filename))
                    if 'extension' in filename and filename['extension'] == 'svg' and ('width' in filename):
                        if filename['width'] and filename['height']:
                            height = str(width_value * (filename['height'] / filename['width'])) + str(width_units)
                    else:
                        height = 'auto'
                    if the_url is not None:
                        result['decoration']['url'] = the_url
                        result['decoration']['size'] = {'width': width, 'height': height}
                        if the_image.attribution is not None:
                            self.attributions.add(the_image.attribution)
                        break
                elif get_config('default icons', None) in ('material icons', 'font awesome'):
                    result['decoration']['name'] = decoration['image']
                    result['decoration']['size'] = str(width_value) + str(width_units)
                    break
    if len(self.attachments) > 0:
        result['attachments'] = list()
        if self.current_info['user']['is_authenticated'] and self.current_info['user']['email']:
            result['default_email'] = self.current_info['user']['email']
        for attachment in self.attachments:
            the_attachment = dict(url=dict(), number=dict(), filename_with_extension=dict())
            if 'orig_variable_name' in attachment and attachment['orig_variable_name']:
                the_attachment['variable_name'] = attachment['orig_variable_name']
            if 'name' in attachment:
                if attachment['name']:
                    the_attachment['name'] = docassemble.base.filter.markdown_to_html(attachment['name'], trim=True, status=self, verbatim=not encode)
                    if debug:
                        output['question'] += '<p>' + the_attachment['name'] + '</p>'
            if 'description' in attachment:
                if attachment['description']:
                    the_attachment['description'] = docassemble.base.filter.markdown_to_html(attachment['description'], status=self, verbatim=not encode)
                    if debug:
                        output['question'] += the_attachment['description']
            for key in ('valid_formats', 'filename', 'content', 'markdown', 'raw'):
                if key in attachment:
                    if attachment[key]:
                        the_attachment[key] = attachment[key]
            for the_format in attachment['file']:
                the_attachment['url'][the_format] = docassemble.base.functions.server.url_finder(attachment['file'][the_format], filename=attachment['filename'] + '.' + extension_of_doc_format[the_format])
                the_attachment['number'][the_format] = attachment['file'][the_format]
                the_attachment['filename_with_extension'][the_format] = attachment['filename'] + '.' + extension_of_doc_format[the_format]
            result['attachments'].append(the_attachment)
    if self.extras.get('list_collect', False) is not False:
        result['listCollect'] = {'deleteLabel': word('Delete'), 'addAnotherLabel': self.extras['list_collect_add_another_label'] if self.extras['list_collect_add_another_label'] else word('Add another'), 'deletedLabel': word('(Deleted)'), 'undeleteLabel': word('Undelete')}
    validation_rules_used = set()
    file_fields = list()
    for field in self.question.fields:
        the_field = dict()
        the_field['number'] = field.number
        if hasattr(field, 'saveas'):
            the_field['variable_name'] = from_safeid(field.saveas)
            if encode:
                the_field['variable_name_encoded'] = field.saveas
            the_field['validation_messages'] = dict()
            if self.question.question_type == 'multiple_choice' and self.question.question_variety in ['radio', 'dropdown', 'combobox']:
                if self.question.question_variety == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
            elif not (hasattr(field, 'datatype') and field.datatype in ['multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes']):
                if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                elif hasattr(field, 'inputtype') and field.inputtype == 'ajax':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one.'))
                elif hasattr(field, 'datatype') and (field.datatype == 'object_radio' or (hasattr(field, 'inputtype') and field.inputtype in ('yesnoradio', 'noyesradio', 'radio', 'dropdown'))):
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('required', self, word('This field is required.'))
            if hasattr(field, 'inputtype') and field.inputtype in ['yesno', 'noyes', 'yesnowide', 'noyeswide'] and hasattr(field, 'uncheckothers') and (field.uncheckothers is not False):
                the_field['validation_messages']['uncheckothers'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([strip_tags(self.labels[field.number])]))
            if hasattr(field, 'datatype') and field.datatype not in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                for key in ('minlength', 'maxlength'):
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
                        if key == 'minlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'maxlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        if hasattr(field, 'datatype'):
            if field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes') and (hasattr(field, 'nota') and self.extras['nota'][field.number] is not False or (hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)))):
                if field.datatype.endswith('checkboxes'):
                    d_type = 'checkbox'
                else:
                    d_type = 'multiselect'
                if hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)):
                    checkbox_messages = dict()
                    if 'minlength' in field.extras and 'minlength' in self.extras and ('maxlength' in field.extras) and ('maxlength' in self.extras) and (self.extras['minlength'][field.number] == self.extras['maxlength'][field.number]) and (self.extras['minlength'][field.number] > 0):
                        if 'nota' not in self.extras:
                            self.extras['nota'] = dict()
                        self.extras['nota'][field.number] = False
                        if d_type == 'checkbox':
                            checkbox_messages['checkexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                        else:
                            checkbox_messages['selectexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    else:
                        if 'minlength' in field.extras and 'minlength' in self.extras:
                            if d_type == 'checkbox':
                                if self.extras['minlength'][field.number] == 1:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select one.'))
                                else:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                                if int(float(self.extras['minlength'][field.number])) > 0:
                                    if 'nota' not in self.extras:
                                        self.extras['nota'] = dict()
                                    self.extras['nota'][field.number] = False
                            elif self.extras['minlength'][field.number] == 1:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select one.'))
                            else:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                        if 'maxlength' in field.extras and 'maxlength' in self.extras:
                            if d_type == 'checkbox':
                                checkbox_messages['checkatmost'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                            else:
                                checkbox_messages['maxlength'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    the_field['validation_messages'].update(checkbox_messages)
                if d_type == 'checkbox':
                    if hasattr(field, 'nota') and self.extras['nota'][field.number] is not False:
                        the_field['validation_messages']['checkatleast'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([self.extras['nota'][field.number]]))
            if field.datatype == 'date':
                the_field['validation_messages']['date'] = field.validation_message('date', self, word('You need to enter a valid date.'))
                if hasattr(field, 'extras') and 'min' in field.extras and ('min' in self.extras) and ('max' in field.extras) and ('max' in self.extras) and (field.number in self.extras['min']) and (field.number in self.extras['max']):
                    the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.extras['min'][field.number], format='medium'), docassemble.base.util.format_date(self.extras['max'][field.number], format='medium')))
                else:
                    was_defined = dict()
                    for key in ['min', 'max']:
                        if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                            was_defined[key] = True
                            if key == 'min':
                                the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                            elif key == 'max':
                                the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                    if len(was_defined) == 0 and 'default date min' in self.question.interview.options and ('default date max' in self.question.interview.options):
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium'), docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')))
                    elif 'max' not in was_defined and 'default date max' in self.question.interview.options:
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')]))
                    elif 'min' not in was_defined and 'default date min' in self.question.interview.options:
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium')]))
            if field.datatype == 'time':
                the_field['validation_messages']['time'] = field.validation_message('time', self, word('You need to enter a valid time.'))
            if field.datatype in ['datetime', 'datetime-local']:
                the_field['validation_messages']['datetime'] = field.validation_message('datetime', self, word('You need to enter a valid date and time.'))
            if field.datatype == 'email':
                the_field['validation_messages']['email'] = field.validation_message('email', self, word('You need to enter a complete e-mail address.'))
            if field.datatype in ['number', 'currency', 'float', 'integer']:
                the_field['validation_messages']['number'] = field.validation_message('number', self, word('You need to enter a number.'))
                if field.datatype == 'integer' and (not ('step' in self.extras and field.number in self.extras['step'])):
                    the_field['validation_messages']['step'] = field.validation_message('integer', self, word('Please enter a whole number.'))
                elif 'step' in self.extras and field.number in self.extras['step']:
                    the_field['validation_messages']['step'] = field.validation_message('step', self, word('Please enter a multiple of {0}.'))
                for key in ['min', 'max']:
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                        if key == 'min':
                            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'max':
                            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))
            if field.datatype in ['files', 'file', 'camera', 'user', 'environment', 'camcorder', 'microphone']:
                file_fields.append(field)
                the_field['validation_messages']['required'] = field.validation_message('file required', self, word('You must provide a file.'))
                if 'accept' in self.extras and field.number in self.extras['accept']:
                    the_field['validation_messages']['accept'] = field.validation_message('accept', self, word('Please upload a file with a valid file format.'))
                if get_config('maximum content length') is not None:
                    the_field['max'] = get_config('maximum content length')
                    the_field['validation_messages']['max'] = field.validation_message('maxuploadsize', self, word('Your file upload is larger than the server can accept. Please reduce the size of your file upload.'))
        for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
            if hasattr(field, param):
                the_field[param] = getattr(field, param)
        if hasattr(field, 'shuffle') and field.shuffle is not False:
            the_field['shuffle'] = True
        if hasattr(field, 'disableothers') and field.disableothers and hasattr(field, 'saveas'):
            the_field['disable_others'] = True
        if hasattr(field, 'uncheckothers') and field.uncheckothers is not False:
            the_field['uncheck_others'] = True
        for key in ('minlength', 'maxlength', 'min', 'max', 'step', 'scale', 'inline', 'inline width', 'rows', 'accept', 'currency symbol', 'field metadata', 'css class'):
            if key in self.extras and field.number in self.extras[key]:
                if key in ('minlength', 'maxlength', 'min', 'max', 'step'):
                    validation_rules_used.add(key)
                the_field[key] = self.extras[key][field.number]
        if hasattr(field, 'extras') and 'custom_parameters' in field.extras:
            for (parameter, parameter_value) in field.extras['custom_parameters'].items():
                the_field[parameter] = parameter_value
        for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
            if param_type in self.extras and field.number in self.extras[param_type]:
                for (parameter, parameter_value) in self.extras[param_type][field.number].items():
                    the_field[parameter] = parameter_value
        if hasattr(field, 'saveas') and field.saveas in self.embedded:
            the_field['embedded'] = True
        if hasattr(self, 'shuffle'):
            the_field['shuffle'] = self.shuffle
        if field.number in self.defaults:
            the_default = self.defaults[field.number]
            if isinstance(the_default, (str, int, bool, float)):
                the_field['default'] = the_default
        else:
            the_default = None
        if self.question.question_type == 'multiple_choice' or hasattr(field, 'choicetype') or (hasattr(field, 'datatype') and field.datatype in ('object', 'multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes', 'object_radio')):
            the_field['choices'] = self.get_choices_data(field, the_default, the_user_dict, encode=encode)
        if hasattr(field, 'nota'):
            the_field['none_of_the_above'] = docassemble.base.filter.markdown_to_html(self.extras['nota'][field.number], do_terms=False, status=self, verbatim=not encode)
        if field.number in self.extras['ok']:
            the_field['active'] = self.extras['ok'][field.number]
        else:
            the_field['active'] = True
        if field.number in self.extras['required']:
            the_field['required'] = self.extras['required'][field.number]
            if the_field['required']:
                validation_rules_used.add('required')
        if 'validation messages' in self.extras and field.number in self.extras['validation messages']:
            the_field['validation_messages'].update(self.extras['validation messages'][field.number])
        if 'permissions' in self.extras and field.number in self.extras['permissions']:
            the_field['permissions'] = self.extras['permissions'][field.number]
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('max_image_size' in self.extras) and self.extras['max_image_size']:
            the_field['max_image_size'] = self.extras['max_image_size']
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('image_type' in self.extras) and self.extras['image_type']:
            the_field['image_type'] = self.extras['image_type']
        if hasattr(field, 'extras'):
            if 'ml_group' in field.extras or 'ml_train' in field.extras:","for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
    if param_type in self.extras and field.number in self.extras[param_type]:
        for (parameter, parameter_value) in self.extras[param_type][field.number].items():
            the_field[parameter] = parameter_value",XXX,no_found,0,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,InterviewStatus,as_data$768,"def as_data(self, the_user_dict, encode=True):
    result = dict(language=self.question.language)
    debug = self.question.interview.debug
    if debug:
        output = dict(question='', help='')
    if 'progress' in the_user_dict['_internal']:
        result['progress'] = the_user_dict['_internal']['progress']
    if self.question.language in self.question.interview.default_validation_messages:
        result['validation_messages'] = copy.copy(self.question.interview.default_validation_messages[self.question.language])
    else:
        result['validation_messages'] = dict()
    if 'reload_after' in self.extras:
        result['reload'] = 1000 * int(self.extras['reload_after'])
    lang = docassemble.base.functions.get_language()
    if len(self.question.terms) or len(self.question.interview.terms):
        result['terms'] = dict()
        if 'terms' in self.extras:
            for (term, vals) in self.extras['terms'].items():
                result['terms'][term] = vals['definition']
        if lang in self.question.interview.terms and len(self.question.interview.terms[lang]):
            for (term, vals) in self.question.interview.terms[lang].items():
                result['terms'][term] = vals['definition']
        elif self.question.language in self.question.interview.terms and len(self.question.interview.terms[self.question.language]):
            for (term, vals) in self.question.interview.terms[self.question.language].items():
                result['terms'][term] = vals['definition']
    if len(self.question.autoterms) or len(self.question.interview.autoterms):
        result['autoterms'] = dict()
        if 'autoterms' in self.extras:
            for (term, vals) in self.extras['autoterms'].items():
                result['autoterms'][term] = vals['definition']
        if lang in self.question.interview.autoterms and len(self.question.interview.autoterms[lang]):
            for (term, vals) in self.question.interview.autoterms[lang].items():
                result['autoterms'][term] = vals['definition']
        elif self.question.language in self.question.interview.autoterms and len(self.question.interview.autoterms[self.question.language]):
            for (term, vals) in self.question.interview.autoterms[self.question.language].items():
                result['autoterms'][term] = vals['definition']
    if self.orig_sought is not None:
        result['event_list'] = [self.orig_sought]
    if 'action_buttons' in self.extras:
        result['additional_buttons'] = []
        for item in self.extras['action_buttons']:
            new_item = copy.deepcopy(item)
            new_item['label'] = docassemble.base.filter.markdown_to_html(item['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + new_item['label'] + '</p>'
    for param in ('questionText',):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if hasattr(self, 'breadcrumb') and self.breadcrumb is not None:
        output['breadcrumb label'] = self.breadcrumb
    output['breadcrumbs'] = docassemble.base.functions.get_action_stack()
    if hasattr(self, 'subquestionText') and self.subquestionText is not None:
        if self.question.question_type == 'fields':
            embedder = dummy_embed_input
        else:
            embedder = None
        result['subquestionText'] = docassemble.base.filter.markdown_to_html(self.subquestionText.rstrip(), status=self, verbatim=not encode, embedder=embedder)
        if debug:
            output['question'] += result['subquestionText']
    for param in ('continueLabel', 'helpLabel'):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + result[param] + '</p>'
    if 'menu_items' in self.extras and isinstance(self.extras['menu_items'], list):
        result['menu_items'] = self.extras['menu_items']
    for param in ('cssClass', 'tableCssClass', 'css', 'script'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = self.extras[param].rstrip()
    for param in ('back_button_label',):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
    for param in ('rightText', 'underText'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if 'continueLabel' not in result:
        if self.question.question_type == 'review':
            result['continueLabel'] = word('Resume')
        else:
            result['continueLabel'] = word('Continue')
        if debug:
            output['question'] += '<p>' + result['continueLabel'] + '</p>'
    if self.question.question_type == 'yesno':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
    elif self.question.question_type == 'noyes':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
    elif self.question.question_type == 'yesnomaybe':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    elif self.question.question_type == 'noyesmaybe':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    steps = the_user_dict['_internal']['steps'] - the_user_dict['_internal']['steps_offset']
    if self.can_go_back and steps > 1:
        result['allow_going_back'] = True
        result['backTitle'] = word('Go back to the previous question')
        back_button_val = self.extras.get('back_button', None)
        if back_button_val or (back_button_val is None and self.question.interview.question_back_button):
            result['questionBackButton'] = self.back
    else:
        result['allow_going_back'] = False
    if self.question.question_type == 'signature':
        result['signaturePhrases'] = {'clear': word('Clear'), 'noSignature': word('You must sign your name to continue.'), 'loading': word('Loading.  Please wait . . . ')}
    if 'questionMetadata' in self.extras:
        result['question_metadata'] = self.extras['questionMetadata']
    if 'segment' in self.extras:
        result['segment'] = self.extras['segment']
    if 'ga_id' in self.extras:
        result['ga_id'] = self.extras['ga_id']
    if hasattr(self.question, 'id'):
        result['id'] = self.question.id
    if hasattr(self, 'audiovideo') and self.audiovideo is not None:
        audio_result = docassemble.base.filter.get_audio_urls(self.audiovideo)
        video_result = docassemble.base.filter.get_video_urls(self.audiovideo)
        if len(audio_result) > 0:
            result['audio'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in audio_result]
        if len(video_result) > 0:
            result['video'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in video_result]
    if hasattr(self, 'helpText') and len(self.helpText) > 0:
        result['helpText'] = list()
        result['helpBackLabel'] = word('Back to question')
        for help_text in self.helpText:
            result['helpText'].append(self.convert_help(help_text, encode, debug))
        result['help'] = dict()
        if self.helpText[0]['label']:
            result['help']['label'] = docassemble.base.filter.markdown_to_html(self.helpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['label'] = self.question.help()
        result['help']['title'] = word('Help is available for this question')
        result['help']['specific'] = False if self.question.helptext is None else True
    if hasattr(self, 'interviewHelpText') and len(self.interviewHelpText) > 0:
        result['interviewHelpText'] = list()
        for help_text in self.interviewHelpText:
            result['interviewHelpText'].append(self.convert_help(help_text, encode, debug))
        if 'help' not in result:
            result['help'] = dict()
        if self.interviewHelpText[0]['label']:
            result['help']['interviewLabel'] = docassemble.base.filter.markdown_to_html(self.interviewHelpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['interviewLabel'] = self.question.help()
        result['help']['interviewTitle'] = word('Help is available')
        if not (hasattr(self, 'helpText') and len(self.helpText) > 0):
            result['help']['specific'] = False
    if 'questionText' not in result and self.question.question_type == 'signature':
        result['questionText'] = word('Sign Your Name')
        if debug:
            output['question'] += '<p>' + result['questionText'] + '</p>'
    result['questionType'] = self.question.question_type
    if hasattr(self.question, 'question_variety'):
        result['questionVariety'] = self.question.question_variety
    if self.question.is_mandatory or self.question.mandatory_code is not None:
        result['mandatory'] = True
    if hasattr(self.question, 'name'):
        result['_question_name'] = self.question.name
    result['_tracker'] = self.tracker
    if hasattr(self, 'datatypes'):
        result['_datatypes'] = safeid(json.dumps(self.datatypes))
    if hasattr(self, 'varnames'):
        result['_varnames'] = safeid(json.dumps(self.varnames))
    if len(self.question.fields) > 0:
        result['fields'] = list()
    if hasattr(self.question, 'review_saveas'):
        result['question_variable_name'] = self.question.review_saveas
    if hasattr(self.question, 'fields_saveas'):
        result['question_variable_name'] = self.question.fields_saveas
    if self.decorations is not None:
        width_value = get_config('decoration size', 2.0)
        width_units = get_config('decoration units', 'em')
        for decoration in self.decorations:
            if 'image' in decoration:
                result['decoration'] = {}
                the_image = self.question.interview.images.get(decoration['image'], None)
                if the_image is not None:
                    the_url = docassemble.base.functions.server.url_finder(str(the_image.package) + ':' + str(the_image.filename))
                    width = str(width_value) + str(width_units)
                    filename = docassemble.base.functions.server.file_finder(str(the_image.package) + ':' + str(the_image.filename))
                    if 'extension' in filename and filename['extension'] == 'svg' and ('width' in filename):
                        if filename['width'] and filename['height']:
                            height = str(width_value * (filename['height'] / filename['width'])) + str(width_units)
                    else:
                        height = 'auto'
                    if the_url is not None:
                        result['decoration']['url'] = the_url
                        result['decoration']['size'] = {'width': width, 'height': height}
                        if the_image.attribution is not None:
                            self.attributions.add(the_image.attribution)
                        break
                elif get_config('default icons', None) in ('material icons', 'font awesome'):
                    result['decoration']['name'] = decoration['image']
                    result['decoration']['size'] = str(width_value) + str(width_units)
                    break
    if len(self.attachments) > 0:
        result['attachments'] = list()
        if self.current_info['user']['is_authenticated'] and self.current_info['user']['email']:
            result['default_email'] = self.current_info['user']['email']
        for attachment in self.attachments:
            the_attachment = dict(url=dict(), number=dict(), filename_with_extension=dict())
            if 'orig_variable_name' in attachment and attachment['orig_variable_name']:
                the_attachment['variable_name'] = attachment['orig_variable_name']
            if 'name' in attachment:
                if attachment['name']:
                    the_attachment['name'] = docassemble.base.filter.markdown_to_html(attachment['name'], trim=True, status=self, verbatim=not encode)
                    if debug:
                        output['question'] += '<p>' + the_attachment['name'] + '</p>'
            if 'description' in attachment:
                if attachment['description']:
                    the_attachment['description'] = docassemble.base.filter.markdown_to_html(attachment['description'], status=self, verbatim=not encode)
                    if debug:
                        output['question'] += the_attachment['description']
            for key in ('valid_formats', 'filename', 'content', 'markdown', 'raw'):
                if key in attachment:
                    if attachment[key]:
                        the_attachment[key] = attachment[key]
            for the_format in attachment['file']:
                the_attachment['url'][the_format] = docassemble.base.functions.server.url_finder(attachment['file'][the_format], filename=attachment['filename'] + '.' + extension_of_doc_format[the_format])
                the_attachment['number'][the_format] = attachment['file'][the_format]
                the_attachment['filename_with_extension'][the_format] = attachment['filename'] + '.' + extension_of_doc_format[the_format]
            result['attachments'].append(the_attachment)
    if self.extras.get('list_collect', False) is not False:
        result['listCollect'] = {'deleteLabel': word('Delete'), 'addAnotherLabel': self.extras['list_collect_add_another_label'] if self.extras['list_collect_add_another_label'] else word('Add another'), 'deletedLabel': word('(Deleted)'), 'undeleteLabel': word('Undelete')}
    validation_rules_used = set()
    file_fields = list()
    for field in self.question.fields:
        the_field = dict()
        the_field['number'] = field.number
        if hasattr(field, 'saveas'):
            the_field['variable_name'] = from_safeid(field.saveas)
            if encode:
                the_field['variable_name_encoded'] = field.saveas
            the_field['validation_messages'] = dict()
            if self.question.question_type == 'multiple_choice' and self.question.question_variety in ['radio', 'dropdown', 'combobox']:
                if self.question.question_variety == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
            elif not (hasattr(field, 'datatype') and field.datatype in ['multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes']):
                if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                elif hasattr(field, 'inputtype') and field.inputtype == 'ajax':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one.'))
                elif hasattr(field, 'datatype') and (field.datatype == 'object_radio' or (hasattr(field, 'inputtype') and field.inputtype in ('yesnoradio', 'noyesradio', 'radio', 'dropdown'))):
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('required', self, word('This field is required.'))
            if hasattr(field, 'inputtype') and field.inputtype in ['yesno', 'noyes', 'yesnowide', 'noyeswide'] and hasattr(field, 'uncheckothers') and (field.uncheckothers is not False):
                the_field['validation_messages']['uncheckothers'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([strip_tags(self.labels[field.number])]))
            if hasattr(field, 'datatype') and field.datatype not in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                for key in ('minlength', 'maxlength'):
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
                        if key == 'minlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'maxlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        if hasattr(field, 'datatype'):
            if field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes') and (hasattr(field, 'nota') and self.extras['nota'][field.number] is not False or (hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)))):
                if field.datatype.endswith('checkboxes'):
                    d_type = 'checkbox'
                else:
                    d_type = 'multiselect'
                if hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)):
                    checkbox_messages = dict()
                    if 'minlength' in field.extras and 'minlength' in self.extras and ('maxlength' in field.extras) and ('maxlength' in self.extras) and (self.extras['minlength'][field.number] == self.extras['maxlength'][field.number]) and (self.extras['minlength'][field.number] > 0):
                        if 'nota' not in self.extras:
                            self.extras['nota'] = dict()
                        self.extras['nota'][field.number] = False
                        if d_type == 'checkbox':
                            checkbox_messages['checkexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                        else:
                            checkbox_messages['selectexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    else:
                        if 'minlength' in field.extras and 'minlength' in self.extras:
                            if d_type == 'checkbox':
                                if self.extras['minlength'][field.number] == 1:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select one.'))
                                else:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                                if int(float(self.extras['minlength'][field.number])) > 0:
                                    if 'nota' not in self.extras:
                                        self.extras['nota'] = dict()
                                    self.extras['nota'][field.number] = False
                            elif self.extras['minlength'][field.number] == 1:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select one.'))
                            else:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                        if 'maxlength' in field.extras and 'maxlength' in self.extras:
                            if d_type == 'checkbox':
                                checkbox_messages['checkatmost'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                            else:
                                checkbox_messages['maxlength'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    the_field['validation_messages'].update(checkbox_messages)
                if d_type == 'checkbox':
                    if hasattr(field, 'nota') and self.extras['nota'][field.number] is not False:
                        the_field['validation_messages']['checkatleast'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([self.extras['nota'][field.number]]))
            if field.datatype == 'date':
                the_field['validation_messages']['date'] = field.validation_message('date', self, word('You need to enter a valid date.'))
                if hasattr(field, 'extras') and 'min' in field.extras and ('min' in self.extras) and ('max' in field.extras) and ('max' in self.extras) and (field.number in self.extras['min']) and (field.number in self.extras['max']):
                    the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.extras['min'][field.number], format='medium'), docassemble.base.util.format_date(self.extras['max'][field.number], format='medium')))
                else:
                    was_defined = dict()
                    for key in ['min', 'max']:
                        if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                            was_defined[key] = True
                            if key == 'min':
                                the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                            elif key == 'max':
                                the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                    if len(was_defined) == 0 and 'default date min' in self.question.interview.options and ('default date max' in self.question.interview.options):
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium'), docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')))
                    elif 'max' not in was_defined and 'default date max' in self.question.interview.options:
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')]))
                    elif 'min' not in was_defined and 'default date min' in self.question.interview.options:
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium')]))
            if field.datatype == 'time':
                the_field['validation_messages']['time'] = field.validation_message('time', self, word('You need to enter a valid time.'))
            if field.datatype in ['datetime', 'datetime-local']:
                the_field['validation_messages']['datetime'] = field.validation_message('datetime', self, word('You need to enter a valid date and time.'))
            if field.datatype == 'email':
                the_field['validation_messages']['email'] = field.validation_message('email', self, word('You need to enter a complete e-mail address.'))
            if field.datatype in ['number', 'currency', 'float', 'integer']:
                the_field['validation_messages']['number'] = field.validation_message('number', self, word('You need to enter a number.'))
                if field.datatype == 'integer' and (not ('step' in self.extras and field.number in self.extras['step'])):
                    the_field['validation_messages']['step'] = field.validation_message('integer', self, word('Please enter a whole number.'))
                elif 'step' in self.extras and field.number in self.extras['step']:
                    the_field['validation_messages']['step'] = field.validation_message('step', self, word('Please enter a multiple of {0}.'))
                for key in ['min', 'max']:
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                        if key == 'min':
                            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'max':
                            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))
            if field.datatype in ['files', 'file', 'camera', 'user', 'environment', 'camcorder', 'microphone']:
                file_fields.append(field)
                the_field['validation_messages']['required'] = field.validation_message('file required', self, word('You must provide a file.'))
                if 'accept' in self.extras and field.number in self.extras['accept']:
                    the_field['validation_messages']['accept'] = field.validation_message('accept', self, word('Please upload a file with a valid file format.'))
                if get_config('maximum content length') is not None:
                    the_field['max'] = get_config('maximum content length')
                    the_field['validation_messages']['max'] = field.validation_message('maxuploadsize', self, word('Your file upload is larger than the server can accept. Please reduce the size of your file upload.'))
        for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
            if hasattr(field, param):
                the_field[param] = getattr(field, param)
        if hasattr(field, 'shuffle') and field.shuffle is not False:
            the_field['shuffle'] = True
        if hasattr(field, 'disableothers') and field.disableothers and hasattr(field, 'saveas'):
            the_field['disable_others'] = True
        if hasattr(field, 'uncheckothers') and field.uncheckothers is not False:
            the_field['uncheck_others'] = True
        for key in ('minlength', 'maxlength', 'min', 'max', 'step', 'scale', 'inline', 'inline width', 'rows', 'accept', 'currency symbol', 'field metadata', 'css class'):
            if key in self.extras and field.number in self.extras[key]:
                if key in ('minlength', 'maxlength', 'min', 'max', 'step'):
                    validation_rules_used.add(key)
                the_field[key] = self.extras[key][field.number]
        if hasattr(field, 'extras') and 'custom_parameters' in field.extras:
            for (parameter, parameter_value) in field.extras['custom_parameters'].items():
                the_field[parameter] = parameter_value
        for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
            if param_type in self.extras and field.number in self.extras[param_type]:
                for (parameter, parameter_value) in self.extras[param_type][field.number].items():
                    the_field[parameter] = parameter_value
        if hasattr(field, 'saveas') and field.saveas in self.embedded:
            the_field['embedded'] = True
        if hasattr(self, 'shuffle'):
            the_field['shuffle'] = self.shuffle
        if field.number in self.defaults:
            the_default = self.defaults[field.number]
            if isinstance(the_default, (str, int, bool, float)):
                the_field['default'] = the_default
        else:
            the_default = None
        if self.question.question_type == 'multiple_choice' or hasattr(field, 'choicetype') or (hasattr(field, 'datatype') and field.datatype in ('object', 'multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes', 'object_radio')):
            the_field['choices'] = self.get_choices_data(field, the_default, the_user_dict, encode=encode)
        if hasattr(field, 'nota'):
            the_field['none_of_the_above'] = docassemble.base.filter.markdown_to_html(self.extras['nota'][field.number], do_terms=False, status=self, verbatim=not encode)
        if field.number in self.extras['ok']:
            the_field['active'] = self.extras['ok'][field.number]
        else:
            the_field['active'] = True
        if field.number in self.extras['required']:
            the_field['required'] = self.extras['required'][field.number]
            if the_field['required']:
                validation_rules_used.add('required')
        if 'validation messages' in self.extras and field.number in self.extras['validation messages']:
            the_field['validation_messages'].update(self.extras['validation messages'][field.number])
        if 'permissions' in self.extras and field.number in self.extras['permissions']:
            the_field['permissions'] = self.extras['permissions'][field.number]
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('max_image_size' in self.extras) and self.extras['max_image_size']:
            the_field['max_image_size'] = self.extras['max_image_size']
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('image_type' in self.extras) and self.extras['image_type']:
            the_field['image_type'] = self.extras['image_type']
        if hasattr(field, 'extras'):
            if 'ml_group' in field.extras or 'ml_train' in field.extras:","for question_type in ('question', 'help'):
    if question_type not in output:
        continue
    phrase = docassemble.base.functions.server.to_text(output[question_type])
    if not phrase or len(phrase) < 10:
        phrase = 'The sky is blue.'
    phrase = re.sub('[^A-Za-z 0-9\\.\\,\\?\\#\\!\\%\\&\\(\\)]', ' ', phrase)
    readability[question_type] = [('Flesch Reading Ease', textstat.flesch_reading_ease(phrase)), ('Flesch-Kincaid Grade Level', textstat.flesch_kincaid_grade(phrase)), ('Gunning FOG Scale', textstat.gunning_fog(phrase)), ('SMOG Index', textstat.smog_index(phrase)), ('Automated Readability Index', textstat.automated_readability_index(phrase)), ('Coleman-Liau Index', textstat.coleman_liau_index(phrase)), ('Linsear Write Formula', textstat.linsear_write_formula(phrase)), ('Dale-Chall Readability Score', textstat.dale_chall_readability_score(phrase)), ('Readability Consensus', textstat.text_standard(phrase))]",XXX,no_found,0,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,InterviewStatus,as_data$768,"def as_data(self, the_user_dict, encode=True):
    result = dict(language=self.question.language)
    debug = self.question.interview.debug
    if debug:
        output = dict(question='', help='')
    if 'progress' in the_user_dict['_internal']:
        result['progress'] = the_user_dict['_internal']['progress']
    if self.question.language in self.question.interview.default_validation_messages:
        result['validation_messages'] = copy.copy(self.question.interview.default_validation_messages[self.question.language])
    else:
        result['validation_messages'] = dict()
    if 'reload_after' in self.extras:
        result['reload'] = 1000 * int(self.extras['reload_after'])
    lang = docassemble.base.functions.get_language()
    if len(self.question.terms) or len(self.question.interview.terms):
        result['terms'] = dict()
        if 'terms' in self.extras:
            for (term, vals) in self.extras['terms'].items():
                result['terms'][term] = vals['definition']
        if lang in self.question.interview.terms and len(self.question.interview.terms[lang]):
            for (term, vals) in self.question.interview.terms[lang].items():
                result['terms'][term] = vals['definition']
        elif self.question.language in self.question.interview.terms and len(self.question.interview.terms[self.question.language]):
            for (term, vals) in self.question.interview.terms[self.question.language].items():
                result['terms'][term] = vals['definition']
    if len(self.question.autoterms) or len(self.question.interview.autoterms):
        result['autoterms'] = dict()
        if 'autoterms' in self.extras:
            for (term, vals) in self.extras['autoterms'].items():
                result['autoterms'][term] = vals['definition']
        if lang in self.question.interview.autoterms and len(self.question.interview.autoterms[lang]):
            for (term, vals) in self.question.interview.autoterms[lang].items():
                result['autoterms'][term] = vals['definition']
        elif self.question.language in self.question.interview.autoterms and len(self.question.interview.autoterms[self.question.language]):
            for (term, vals) in self.question.interview.autoterms[self.question.language].items():
                result['autoterms'][term] = vals['definition']
    if self.orig_sought is not None:
        result['event_list'] = [self.orig_sought]
    if 'action_buttons' in self.extras:
        result['additional_buttons'] = []
        for item in self.extras['action_buttons']:
            new_item = copy.deepcopy(item)
            new_item['label'] = docassemble.base.filter.markdown_to_html(item['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + new_item['label'] + '</p>'
    for param in ('questionText',):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if hasattr(self, 'breadcrumb') and self.breadcrumb is not None:
        output['breadcrumb label'] = self.breadcrumb
    output['breadcrumbs'] = docassemble.base.functions.get_action_stack()
    if hasattr(self, 'subquestionText') and self.subquestionText is not None:
        if self.question.question_type == 'fields':
            embedder = dummy_embed_input
        else:
            embedder = None
        result['subquestionText'] = docassemble.base.filter.markdown_to_html(self.subquestionText.rstrip(), status=self, verbatim=not encode, embedder=embedder)
        if debug:
            output['question'] += result['subquestionText']
    for param in ('continueLabel', 'helpLabel'):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + result[param] + '</p>'
    if 'menu_items' in self.extras and isinstance(self.extras['menu_items'], list):
        result['menu_items'] = self.extras['menu_items']
    for param in ('cssClass', 'tableCssClass', 'css', 'script'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = self.extras[param].rstrip()
    for param in ('back_button_label',):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
    for param in ('rightText', 'underText'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if 'continueLabel' not in result:
        if self.question.question_type == 'review':
            result['continueLabel'] = word('Resume')
        else:
            result['continueLabel'] = word('Continue')
        if debug:
            output['question'] += '<p>' + result['continueLabel'] + '</p>'
    if self.question.question_type == 'yesno':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
    elif self.question.question_type == 'noyes':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
    elif self.question.question_type == 'yesnomaybe':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    elif self.question.question_type == 'noyesmaybe':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    steps = the_user_dict['_internal']['steps'] - the_user_dict['_internal']['steps_offset']
    if self.can_go_back and steps > 1:
        result['allow_going_back'] = True
        result['backTitle'] = word('Go back to the previous question')
        back_button_val = self.extras.get('back_button', None)
        if back_button_val or (back_button_val is None and self.question.interview.question_back_button):
            result['questionBackButton'] = self.back
    else:
        result['allow_going_back'] = False
    if self.question.question_type == 'signature':
        result['signaturePhrases'] = {'clear': word('Clear'), 'noSignature': word('You must sign your name to continue.'), 'loading': word('Loading.  Please wait . . . ')}
    if 'questionMetadata' in self.extras:
        result['question_metadata'] = self.extras['questionMetadata']
    if 'segment' in self.extras:
        result['segment'] = self.extras['segment']
    if 'ga_id' in self.extras:
        result['ga_id'] = self.extras['ga_id']
    if hasattr(self.question, 'id'):
        result['id'] = self.question.id
    if hasattr(self, 'audiovideo') and self.audiovideo is not None:
        audio_result = docassemble.base.filter.get_audio_urls(self.audiovideo)
        video_result = docassemble.base.filter.get_video_urls(self.audiovideo)
        if len(audio_result) > 0:
            result['audio'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in audio_result]
        if len(video_result) > 0:
            result['video'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in video_result]
    if hasattr(self, 'helpText') and len(self.helpText) > 0:
        result['helpText'] = list()
        result['helpBackLabel'] = word('Back to question')
        for help_text in self.helpText:
            result['helpText'].append(self.convert_help(help_text, encode, debug))
        result['help'] = dict()
        if self.helpText[0]['label']:
            result['help']['label'] = docassemble.base.filter.markdown_to_html(self.helpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['label'] = self.question.help()
        result['help']['title'] = word('Help is available for this question')
        result['help']['specific'] = False if self.question.helptext is None else True
    if hasattr(self, 'interviewHelpText') and len(self.interviewHelpText) > 0:
        result['interviewHelpText'] = list()
        for help_text in self.interviewHelpText:
            result['interviewHelpText'].append(self.convert_help(help_text, encode, debug))
        if 'help' not in result:
            result['help'] = dict()
        if self.interviewHelpText[0]['label']:
            result['help']['interviewLabel'] = docassemble.base.filter.markdown_to_html(self.interviewHelpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['interviewLabel'] = self.question.help()
        result['help']['interviewTitle'] = word('Help is available')
        if not (hasattr(self, 'helpText') and len(self.helpText) > 0):
            result['help']['specific'] = False
    if 'questionText' not in result and self.question.question_type == 'signature':
        result['questionText'] = word('Sign Your Name')
        if debug:
            output['question'] += '<p>' + result['questionText'] + '</p>'
    result['questionType'] = self.question.question_type
    if hasattr(self.question, 'question_variety'):
        result['questionVariety'] = self.question.question_variety
    if self.question.is_mandatory or self.question.mandatory_code is not None:
        result['mandatory'] = True
    if hasattr(self.question, 'name'):
        result['_question_name'] = self.question.name
    result['_tracker'] = self.tracker
    if hasattr(self, 'datatypes'):
        result['_datatypes'] = safeid(json.dumps(self.datatypes))
    if hasattr(self, 'varnames'):
        result['_varnames'] = safeid(json.dumps(self.varnames))
    if len(self.question.fields) > 0:
        result['fields'] = list()
    if hasattr(self.question, 'review_saveas'):
        result['question_variable_name'] = self.question.review_saveas
    if hasattr(self.question, 'fields_saveas'):
        result['question_variable_name'] = self.question.fields_saveas
    if self.decorations is not None:
        width_value = get_config('decoration size', 2.0)
        width_units = get_config('decoration units', 'em')
        for decoration in self.decorations:
            if 'image' in decoration:
                result['decoration'] = {}
                the_image = self.question.interview.images.get(decoration['image'], None)
                if the_image is not None:
                    the_url = docassemble.base.functions.server.url_finder(str(the_image.package) + ':' + str(the_image.filename))
                    width = str(width_value) + str(width_units)
                    filename = docassemble.base.functions.server.file_finder(str(the_image.package) + ':' + str(the_image.filename))
                    if 'extension' in filename and filename['extension'] == 'svg' and ('width' in filename):
                        if filename['width'] and filename['height']:
                            height = str(width_value * (filename['height'] / filename['width'])) + str(width_units)
                    else:
                        height = 'auto'
                    if the_url is not None:
                        result['decoration']['url'] = the_url
                        result['decoration']['size'] = {'width': width, 'height': height}
                        if the_image.attribution is not None:
                            self.attributions.add(the_image.attribution)
                        break
                elif get_config('default icons', None) in ('material icons', 'font awesome'):
                    result['decoration']['name'] = decoration['image']
                    result['decoration']['size'] = str(width_value) + str(width_units)
                    break
    if len(self.attachments) > 0:
        result['attachments'] = list()
        if self.current_info['user']['is_authenticated'] and self.current_info['user']['email']:
            result['default_email'] = self.current_info['user']['email']
        for attachment in self.attachments:
            the_attachment = dict(url=dict(), number=dict(), filename_with_extension=dict())
            if 'orig_variable_name' in attachment and attachment['orig_variable_name']:
                the_attachment['variable_name'] = attachment['orig_variable_name']
            if 'name' in attachment:
                if attachment['name']:
                    the_attachment['name'] = docassemble.base.filter.markdown_to_html(attachment['name'], trim=True, status=self, verbatim=not encode)
                    if debug:
                        output['question'] += '<p>' + the_attachment['name'] + '</p>'
            if 'description' in attachment:
                if attachment['description']:
                    the_attachment['description'] = docassemble.base.filter.markdown_to_html(attachment['description'], status=self, verbatim=not encode)
                    if debug:
                        output['question'] += the_attachment['description']
            for key in ('valid_formats', 'filename', 'content', 'markdown', 'raw'):
                if key in attachment:
                    if attachment[key]:
                        the_attachment[key] = attachment[key]
            for the_format in attachment['file']:
                the_attachment['url'][the_format] = docassemble.base.functions.server.url_finder(attachment['file'][the_format], filename=attachment['filename'] + '.' + extension_of_doc_format[the_format])
                the_attachment['number'][the_format] = attachment['file'][the_format]
                the_attachment['filename_with_extension'][the_format] = attachment['filename'] + '.' + extension_of_doc_format[the_format]
            result['attachments'].append(the_attachment)
    if self.extras.get('list_collect', False) is not False:
        result['listCollect'] = {'deleteLabel': word('Delete'), 'addAnotherLabel': self.extras['list_collect_add_another_label'] if self.extras['list_collect_add_another_label'] else word('Add another'), 'deletedLabel': word('(Deleted)'), 'undeleteLabel': word('Undelete')}
    validation_rules_used = set()
    file_fields = list()
    for field in self.question.fields:
        the_field = dict()
        the_field['number'] = field.number
        if hasattr(field, 'saveas'):
            the_field['variable_name'] = from_safeid(field.saveas)
            if encode:
                the_field['variable_name_encoded'] = field.saveas
            the_field['validation_messages'] = dict()
            if self.question.question_type == 'multiple_choice' and self.question.question_variety in ['radio', 'dropdown', 'combobox']:
                if self.question.question_variety == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
            elif not (hasattr(field, 'datatype') and field.datatype in ['multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes']):
                if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                elif hasattr(field, 'inputtype') and field.inputtype == 'ajax':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one.'))
                elif hasattr(field, 'datatype') and (field.datatype == 'object_radio' or (hasattr(field, 'inputtype') and field.inputtype in ('yesnoradio', 'noyesradio', 'radio', 'dropdown'))):
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('required', self, word('This field is required.'))
            if hasattr(field, 'inputtype') and field.inputtype in ['yesno', 'noyes', 'yesnowide', 'noyeswide'] and hasattr(field, 'uncheckothers') and (field.uncheckothers is not False):
                the_field['validation_messages']['uncheckothers'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([strip_tags(self.labels[field.number])]))
            if hasattr(field, 'datatype') and field.datatype not in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                for key in ('minlength', 'maxlength'):
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
                        if key == 'minlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'maxlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        if hasattr(field, 'datatype'):
            if field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes') and (hasattr(field, 'nota') and self.extras['nota'][field.number] is not False or (hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)))):
                if field.datatype.endswith('checkboxes'):
                    d_type = 'checkbox'
                else:
                    d_type = 'multiselect'
                if hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)):
                    checkbox_messages = dict()
                    if 'minlength' in field.extras and 'minlength' in self.extras and ('maxlength' in field.extras) and ('maxlength' in self.extras) and (self.extras['minlength'][field.number] == self.extras['maxlength'][field.number]) and (self.extras['minlength'][field.number] > 0):
                        if 'nota' not in self.extras:
                            self.extras['nota'] = dict()
                        self.extras['nota'][field.number] = False
                        if d_type == 'checkbox':
                            checkbox_messages['checkexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                        else:
                            checkbox_messages['selectexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    else:
                        if 'minlength' in field.extras and 'minlength' in self.extras:
                            if d_type == 'checkbox':
                                if self.extras['minlength'][field.number] == 1:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select one.'))
                                else:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                                if int(float(self.extras['minlength'][field.number])) > 0:
                                    if 'nota' not in self.extras:
                                        self.extras['nota'] = dict()
                                    self.extras['nota'][field.number] = False
                            elif self.extras['minlength'][field.number] == 1:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select one.'))
                            else:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                        if 'maxlength' in field.extras and 'maxlength' in self.extras:
                            if d_type == 'checkbox':
                                checkbox_messages['checkatmost'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                            else:
                                checkbox_messages['maxlength'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    the_field['validation_messages'].update(checkbox_messages)
                if d_type == 'checkbox':
                    if hasattr(field, 'nota') and self.extras['nota'][field.number] is not False:
                        the_field['validation_messages']['checkatleast'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([self.extras['nota'][field.number]]))
            if field.datatype == 'date':
                the_field['validation_messages']['date'] = field.validation_message('date', self, word('You need to enter a valid date.'))
                if hasattr(field, 'extras') and 'min' in field.extras and ('min' in self.extras) and ('max' in field.extras) and ('max' in self.extras) and (field.number in self.extras['min']) and (field.number in self.extras['max']):
                    the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.extras['min'][field.number], format='medium'), docassemble.base.util.format_date(self.extras['max'][field.number], format='medium')))
                else:
                    was_defined = dict()
                    for key in ['min', 'max']:
                        if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                            was_defined[key] = True
                            if key == 'min':
                                the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                            elif key == 'max':
                                the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                    if len(was_defined) == 0 and 'default date min' in self.question.interview.options and ('default date max' in self.question.interview.options):
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium'), docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')))
                    elif 'max' not in was_defined and 'default date max' in self.question.interview.options:
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')]))
                    elif 'min' not in was_defined and 'default date min' in self.question.interview.options:
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium')]))
            if field.datatype == 'time':
                the_field['validation_messages']['time'] = field.validation_message('time', self, word('You need to enter a valid time.'))
            if field.datatype in ['datetime', 'datetime-local']:
                the_field['validation_messages']['datetime'] = field.validation_message('datetime', self, word('You need to enter a valid date and time.'))
            if field.datatype == 'email':
                the_field['validation_messages']['email'] = field.validation_message('email', self, word('You need to enter a complete e-mail address.'))
            if field.datatype in ['number', 'currency', 'float', 'integer']:
                the_field['validation_messages']['number'] = field.validation_message('number', self, word('You need to enter a number.'))
                if field.datatype == 'integer' and (not ('step' in self.extras and field.number in self.extras['step'])):
                    the_field['validation_messages']['step'] = field.validation_message('integer', self, word('Please enter a whole number.'))
                elif 'step' in self.extras and field.number in self.extras['step']:
                    the_field['validation_messages']['step'] = field.validation_message('step', self, word('Please enter a multiple of {0}.'))
                for key in ['min', 'max']:
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                        if key == 'min':
                            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'max':
                            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))
            if field.datatype in ['files', 'file', 'camera', 'user', 'environment', 'camcorder', 'microphone']:
                file_fields.append(field)
                the_field['validation_messages']['required'] = field.validation_message('file required', self, word('You must provide a file.'))
                if 'accept' in self.extras and field.number in self.extras['accept']:
                    the_field['validation_messages']['accept'] = field.validation_message('accept', self, word('Please upload a file with a valid file format.'))
                if get_config('maximum content length') is not None:
                    the_field['max'] = get_config('maximum content length')
                    the_field['validation_messages']['max'] = field.validation_message('maxuploadsize', self, word('Your file upload is larger than the server can accept. Please reduce the size of your file upload.'))
        for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
            if hasattr(field, param):
                the_field[param] = getattr(field, param)
        if hasattr(field, 'shuffle') and field.shuffle is not False:
            the_field['shuffle'] = True
        if hasattr(field, 'disableothers') and field.disableothers and hasattr(field, 'saveas'):
            the_field['disable_others'] = True
        if hasattr(field, 'uncheckothers') and field.uncheckothers is not False:
            the_field['uncheck_others'] = True
        for key in ('minlength', 'maxlength', 'min', 'max', 'step', 'scale', 'inline', 'inline width', 'rows', 'accept', 'currency symbol', 'field metadata', 'css class'):
            if key in self.extras and field.number in self.extras[key]:
                if key in ('minlength', 'maxlength', 'min', 'max', 'step'):
                    validation_rules_used.add(key)
                the_field[key] = self.extras[key][field.number]
        if hasattr(field, 'extras') and 'custom_parameters' in field.extras:
            for (parameter, parameter_value) in field.extras['custom_parameters'].items():
                the_field[parameter] = parameter_value
        for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
            if param_type in self.extras and field.number in self.extras[param_type]:
                for (parameter, parameter_value) in self.extras[param_type][field.number].items():
                    the_field[parameter] = parameter_value
        if hasattr(field, 'saveas') and field.saveas in self.embedded:
            the_field['embedded'] = True
        if hasattr(self, 'shuffle'):
            the_field['shuffle'] = self.shuffle
        if field.number in self.defaults:
            the_default = self.defaults[field.number]
            if isinstance(the_default, (str, int, bool, float)):
                the_field['default'] = the_default
        else:
            the_default = None
        if self.question.question_type == 'multiple_choice' or hasattr(field, 'choicetype') or (hasattr(field, 'datatype') and field.datatype in ('object', 'multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes', 'object_radio')):
            the_field['choices'] = self.get_choices_data(field, the_default, the_user_dict, encode=encode)
        if hasattr(field, 'nota'):
            the_field['none_of_the_above'] = docassemble.base.filter.markdown_to_html(self.extras['nota'][field.number], do_terms=False, status=self, verbatim=not encode)
        if field.number in self.extras['ok']:
            the_field['active'] = self.extras['ok'][field.number]
        else:
            the_field['active'] = True
        if field.number in self.extras['required']:
            the_field['required'] = self.extras['required'][field.number]
            if the_field['required']:
                validation_rules_used.add('required')
        if 'validation messages' in self.extras and field.number in self.extras['validation messages']:
            the_field['validation_messages'].update(self.extras['validation messages'][field.number])
        if 'permissions' in self.extras and field.number in self.extras['permissions']:
            the_field['permissions'] = self.extras['permissions'][field.number]
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('max_image_size' in self.extras) and self.extras['max_image_size']:
            the_field['max_image_size'] = self.extras['max_image_size']
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('image_type' in self.extras) and self.extras['image_type']:
            the_field['image_type'] = self.extras['image_type']
        if hasattr(field, 'extras'):
            if 'ml_group' in field.extras or 'ml_train' in field.extras:","for (term, vals) in self.extras['terms'].items():
    result['terms'][term] = vals['definition']",XXX,no_found,0,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,InterviewStatus,as_data$768,"def as_data(self, the_user_dict, encode=True):
    result = dict(language=self.question.language)
    debug = self.question.interview.debug
    if debug:
        output = dict(question='', help='')
    if 'progress' in the_user_dict['_internal']:
        result['progress'] = the_user_dict['_internal']['progress']
    if self.question.language in self.question.interview.default_validation_messages:
        result['validation_messages'] = copy.copy(self.question.interview.default_validation_messages[self.question.language])
    else:
        result['validation_messages'] = dict()
    if 'reload_after' in self.extras:
        result['reload'] = 1000 * int(self.extras['reload_after'])
    lang = docassemble.base.functions.get_language()
    if len(self.question.terms) or len(self.question.interview.terms):
        result['terms'] = dict()
        if 'terms' in self.extras:
            for (term, vals) in self.extras['terms'].items():
                result['terms'][term] = vals['definition']
        if lang in self.question.interview.terms and len(self.question.interview.terms[lang]):
            for (term, vals) in self.question.interview.terms[lang].items():
                result['terms'][term] = vals['definition']
        elif self.question.language in self.question.interview.terms and len(self.question.interview.terms[self.question.language]):
            for (term, vals) in self.question.interview.terms[self.question.language].items():
                result['terms'][term] = vals['definition']
    if len(self.question.autoterms) or len(self.question.interview.autoterms):
        result['autoterms'] = dict()
        if 'autoterms' in self.extras:
            for (term, vals) in self.extras['autoterms'].items():
                result['autoterms'][term] = vals['definition']
        if lang in self.question.interview.autoterms and len(self.question.interview.autoterms[lang]):
            for (term, vals) in self.question.interview.autoterms[lang].items():
                result['autoterms'][term] = vals['definition']
        elif self.question.language in self.question.interview.autoterms and len(self.question.interview.autoterms[self.question.language]):
            for (term, vals) in self.question.interview.autoterms[self.question.language].items():
                result['autoterms'][term] = vals['definition']
    if self.orig_sought is not None:
        result['event_list'] = [self.orig_sought]
    if 'action_buttons' in self.extras:
        result['additional_buttons'] = []
        for item in self.extras['action_buttons']:
            new_item = copy.deepcopy(item)
            new_item['label'] = docassemble.base.filter.markdown_to_html(item['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + new_item['label'] + '</p>'
    for param in ('questionText',):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if hasattr(self, 'breadcrumb') and self.breadcrumb is not None:
        output['breadcrumb label'] = self.breadcrumb
    output['breadcrumbs'] = docassemble.base.functions.get_action_stack()
    if hasattr(self, 'subquestionText') and self.subquestionText is not None:
        if self.question.question_type == 'fields':
            embedder = dummy_embed_input
        else:
            embedder = None
        result['subquestionText'] = docassemble.base.filter.markdown_to_html(self.subquestionText.rstrip(), status=self, verbatim=not encode, embedder=embedder)
        if debug:
            output['question'] += result['subquestionText']
    for param in ('continueLabel', 'helpLabel'):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + result[param] + '</p>'
    if 'menu_items' in self.extras and isinstance(self.extras['menu_items'], list):
        result['menu_items'] = self.extras['menu_items']
    for param in ('cssClass', 'tableCssClass', 'css', 'script'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = self.extras[param].rstrip()
    for param in ('back_button_label',):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
    for param in ('rightText', 'underText'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if 'continueLabel' not in result:
        if self.question.question_type == 'review':
            result['continueLabel'] = word('Resume')
        else:
            result['continueLabel'] = word('Continue')
        if debug:
            output['question'] += '<p>' + result['continueLabel'] + '</p>'
    if self.question.question_type == 'yesno':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
    elif self.question.question_type == 'noyes':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
    elif self.question.question_type == 'yesnomaybe':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    elif self.question.question_type == 'noyesmaybe':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    steps = the_user_dict['_internal']['steps'] - the_user_dict['_internal']['steps_offset']
    if self.can_go_back and steps > 1:
        result['allow_going_back'] = True
        result['backTitle'] = word('Go back to the previous question')
        back_button_val = self.extras.get('back_button', None)
        if back_button_val or (back_button_val is None and self.question.interview.question_back_button):
            result['questionBackButton'] = self.back
    else:
        result['allow_going_back'] = False
    if self.question.question_type == 'signature':
        result['signaturePhrases'] = {'clear': word('Clear'), 'noSignature': word('You must sign your name to continue.'), 'loading': word('Loading.  Please wait . . . ')}
    if 'questionMetadata' in self.extras:
        result['question_metadata'] = self.extras['questionMetadata']
    if 'segment' in self.extras:
        result['segment'] = self.extras['segment']
    if 'ga_id' in self.extras:
        result['ga_id'] = self.extras['ga_id']
    if hasattr(self.question, 'id'):
        result['id'] = self.question.id
    if hasattr(self, 'audiovideo') and self.audiovideo is not None:
        audio_result = docassemble.base.filter.get_audio_urls(self.audiovideo)
        video_result = docassemble.base.filter.get_video_urls(self.audiovideo)
        if len(audio_result) > 0:
            result['audio'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in audio_result]
        if len(video_result) > 0:
            result['video'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in video_result]
    if hasattr(self, 'helpText') and len(self.helpText) > 0:
        result['helpText'] = list()
        result['helpBackLabel'] = word('Back to question')
        for help_text in self.helpText:
            result['helpText'].append(self.convert_help(help_text, encode, debug))
        result['help'] = dict()
        if self.helpText[0]['label']:
            result['help']['label'] = docassemble.base.filter.markdown_to_html(self.helpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['label'] = self.question.help()
        result['help']['title'] = word('Help is available for this question')
        result['help']['specific'] = False if self.question.helptext is None else True
    if hasattr(self, 'interviewHelpText') and len(self.interviewHelpText) > 0:
        result['interviewHelpText'] = list()
        for help_text in self.interviewHelpText:
            result['interviewHelpText'].append(self.convert_help(help_text, encode, debug))
        if 'help' not in result:
            result['help'] = dict()
        if self.interviewHelpText[0]['label']:
            result['help']['interviewLabel'] = docassemble.base.filter.markdown_to_html(self.interviewHelpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['interviewLabel'] = self.question.help()
        result['help']['interviewTitle'] = word('Help is available')
        if not (hasattr(self, 'helpText') and len(self.helpText) > 0):
            result['help']['specific'] = False
    if 'questionText' not in result and self.question.question_type == 'signature':
        result['questionText'] = word('Sign Your Name')
        if debug:
            output['question'] += '<p>' + result['questionText'] + '</p>'
    result['questionType'] = self.question.question_type
    if hasattr(self.question, 'question_variety'):
        result['questionVariety'] = self.question.question_variety
    if self.question.is_mandatory or self.question.mandatory_code is not None:
        result['mandatory'] = True
    if hasattr(self.question, 'name'):
        result['_question_name'] = self.question.name
    result['_tracker'] = self.tracker
    if hasattr(self, 'datatypes'):
        result['_datatypes'] = safeid(json.dumps(self.datatypes))
    if hasattr(self, 'varnames'):
        result['_varnames'] = safeid(json.dumps(self.varnames))
    if len(self.question.fields) > 0:
        result['fields'] = list()
    if hasattr(self.question, 'review_saveas'):
        result['question_variable_name'] = self.question.review_saveas
    if hasattr(self.question, 'fields_saveas'):
        result['question_variable_name'] = self.question.fields_saveas
    if self.decorations is not None:
        width_value = get_config('decoration size', 2.0)
        width_units = get_config('decoration units', 'em')
        for decoration in self.decorations:
            if 'image' in decoration:
                result['decoration'] = {}
                the_image = self.question.interview.images.get(decoration['image'], None)
                if the_image is not None:
                    the_url = docassemble.base.functions.server.url_finder(str(the_image.package) + ':' + str(the_image.filename))
                    width = str(width_value) + str(width_units)
                    filename = docassemble.base.functions.server.file_finder(str(the_image.package) + ':' + str(the_image.filename))
                    if 'extension' in filename and filename['extension'] == 'svg' and ('width' in filename):
                        if filename['width'] and filename['height']:
                            height = str(width_value * (filename['height'] / filename['width'])) + str(width_units)
                    else:
                        height = 'auto'
                    if the_url is not None:
                        result['decoration']['url'] = the_url
                        result['decoration']['size'] = {'width': width, 'height': height}
                        if the_image.attribution is not None:
                            self.attributions.add(the_image.attribution)
                        break
                elif get_config('default icons', None) in ('material icons', 'font awesome'):
                    result['decoration']['name'] = decoration['image']
                    result['decoration']['size'] = str(width_value) + str(width_units)
                    break
    if len(self.attachments) > 0:
        result['attachments'] = list()
        if self.current_info['user']['is_authenticated'] and self.current_info['user']['email']:
            result['default_email'] = self.current_info['user']['email']
        for attachment in self.attachments:
            the_attachment = dict(url=dict(), number=dict(), filename_with_extension=dict())
            if 'orig_variable_name' in attachment and attachment['orig_variable_name']:
                the_attachment['variable_name'] = attachment['orig_variable_name']
            if 'name' in attachment:
                if attachment['name']:
                    the_attachment['name'] = docassemble.base.filter.markdown_to_html(attachment['name'], trim=True, status=self, verbatim=not encode)
                    if debug:
                        output['question'] += '<p>' + the_attachment['name'] + '</p>'
            if 'description' in attachment:
                if attachment['description']:
                    the_attachment['description'] = docassemble.base.filter.markdown_to_html(attachment['description'], status=self, verbatim=not encode)
                    if debug:
                        output['question'] += the_attachment['description']
            for key in ('valid_formats', 'filename', 'content', 'markdown', 'raw'):
                if key in attachment:
                    if attachment[key]:
                        the_attachment[key] = attachment[key]
            for the_format in attachment['file']:
                the_attachment['url'][the_format] = docassemble.base.functions.server.url_finder(attachment['file'][the_format], filename=attachment['filename'] + '.' + extension_of_doc_format[the_format])
                the_attachment['number'][the_format] = attachment['file'][the_format]
                the_attachment['filename_with_extension'][the_format] = attachment['filename'] + '.' + extension_of_doc_format[the_format]
            result['attachments'].append(the_attachment)
    if self.extras.get('list_collect', False) is not False:
        result['listCollect'] = {'deleteLabel': word('Delete'), 'addAnotherLabel': self.extras['list_collect_add_another_label'] if self.extras['list_collect_add_another_label'] else word('Add another'), 'deletedLabel': word('(Deleted)'), 'undeleteLabel': word('Undelete')}
    validation_rules_used = set()
    file_fields = list()
    for field in self.question.fields:
        the_field = dict()
        the_field['number'] = field.number
        if hasattr(field, 'saveas'):
            the_field['variable_name'] = from_safeid(field.saveas)
            if encode:
                the_field['variable_name_encoded'] = field.saveas
            the_field['validation_messages'] = dict()
            if self.question.question_type == 'multiple_choice' and self.question.question_variety in ['radio', 'dropdown', 'combobox']:
                if self.question.question_variety == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
            elif not (hasattr(field, 'datatype') and field.datatype in ['multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes']):
                if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                elif hasattr(field, 'inputtype') and field.inputtype == 'ajax':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one.'))
                elif hasattr(field, 'datatype') and (field.datatype == 'object_radio' or (hasattr(field, 'inputtype') and field.inputtype in ('yesnoradio', 'noyesradio', 'radio', 'dropdown'))):
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('required', self, word('This field is required.'))
            if hasattr(field, 'inputtype') and field.inputtype in ['yesno', 'noyes', 'yesnowide', 'noyeswide'] and hasattr(field, 'uncheckothers') and (field.uncheckothers is not False):
                the_field['validation_messages']['uncheckothers'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([strip_tags(self.labels[field.number])]))
            if hasattr(field, 'datatype') and field.datatype not in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                for key in ('minlength', 'maxlength'):
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
                        if key == 'minlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'maxlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        if hasattr(field, 'datatype'):
            if field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes') and (hasattr(field, 'nota') and self.extras['nota'][field.number] is not False or (hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)))):
                if field.datatype.endswith('checkboxes'):
                    d_type = 'checkbox'
                else:
                    d_type = 'multiselect'
                if hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)):
                    checkbox_messages = dict()
                    if 'minlength' in field.extras and 'minlength' in self.extras and ('maxlength' in field.extras) and ('maxlength' in self.extras) and (self.extras['minlength'][field.number] == self.extras['maxlength'][field.number]) and (self.extras['minlength'][field.number] > 0):
                        if 'nota' not in self.extras:
                            self.extras['nota'] = dict()
                        self.extras['nota'][field.number] = False
                        if d_type == 'checkbox':
                            checkbox_messages['checkexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                        else:
                            checkbox_messages['selectexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    else:
                        if 'minlength' in field.extras and 'minlength' in self.extras:
                            if d_type == 'checkbox':
                                if self.extras['minlength'][field.number] == 1:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select one.'))
                                else:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                                if int(float(self.extras['minlength'][field.number])) > 0:
                                    if 'nota' not in self.extras:
                                        self.extras['nota'] = dict()
                                    self.extras['nota'][field.number] = False
                            elif self.extras['minlength'][field.number] == 1:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select one.'))
                            else:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                        if 'maxlength' in field.extras and 'maxlength' in self.extras:
                            if d_type == 'checkbox':
                                checkbox_messages['checkatmost'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                            else:
                                checkbox_messages['maxlength'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    the_field['validation_messages'].update(checkbox_messages)
                if d_type == 'checkbox':
                    if hasattr(field, 'nota') and self.extras['nota'][field.number] is not False:
                        the_field['validation_messages']['checkatleast'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([self.extras['nota'][field.number]]))
            if field.datatype == 'date':
                the_field['validation_messages']['date'] = field.validation_message('date', self, word('You need to enter a valid date.'))
                if hasattr(field, 'extras') and 'min' in field.extras and ('min' in self.extras) and ('max' in field.extras) and ('max' in self.extras) and (field.number in self.extras['min']) and (field.number in self.extras['max']):
                    the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.extras['min'][field.number], format='medium'), docassemble.base.util.format_date(self.extras['max'][field.number], format='medium')))
                else:
                    was_defined = dict()
                    for key in ['min', 'max']:
                        if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                            was_defined[key] = True
                            if key == 'min':
                                the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                            elif key == 'max':
                                the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                    if len(was_defined) == 0 and 'default date min' in self.question.interview.options and ('default date max' in self.question.interview.options):
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium'), docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')))
                    elif 'max' not in was_defined and 'default date max' in self.question.interview.options:
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')]))
                    elif 'min' not in was_defined and 'default date min' in self.question.interview.options:
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium')]))
            if field.datatype == 'time':
                the_field['validation_messages']['time'] = field.validation_message('time', self, word('You need to enter a valid time.'))
            if field.datatype in ['datetime', 'datetime-local']:
                the_field['validation_messages']['datetime'] = field.validation_message('datetime', self, word('You need to enter a valid date and time.'))
            if field.datatype == 'email':
                the_field['validation_messages']['email'] = field.validation_message('email', self, word('You need to enter a complete e-mail address.'))
            if field.datatype in ['number', 'currency', 'float', 'integer']:
                the_field['validation_messages']['number'] = field.validation_message('number', self, word('You need to enter a number.'))
                if field.datatype == 'integer' and (not ('step' in self.extras and field.number in self.extras['step'])):
                    the_field['validation_messages']['step'] = field.validation_message('integer', self, word('Please enter a whole number.'))
                elif 'step' in self.extras and field.number in self.extras['step']:
                    the_field['validation_messages']['step'] = field.validation_message('step', self, word('Please enter a multiple of {0}.'))
                for key in ['min', 'max']:
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                        if key == 'min':
                            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'max':
                            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))
            if field.datatype in ['files', 'file', 'camera', 'user', 'environment', 'camcorder', 'microphone']:
                file_fields.append(field)
                the_field['validation_messages']['required'] = field.validation_message('file required', self, word('You must provide a file.'))
                if 'accept' in self.extras and field.number in self.extras['accept']:
                    the_field['validation_messages']['accept'] = field.validation_message('accept', self, word('Please upload a file with a valid file format.'))
                if get_config('maximum content length') is not None:
                    the_field['max'] = get_config('maximum content length')
                    the_field['validation_messages']['max'] = field.validation_message('maxuploadsize', self, word('Your file upload is larger than the server can accept. Please reduce the size of your file upload.'))
        for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
            if hasattr(field, param):
                the_field[param] = getattr(field, param)
        if hasattr(field, 'shuffle') and field.shuffle is not False:
            the_field['shuffle'] = True
        if hasattr(field, 'disableothers') and field.disableothers and hasattr(field, 'saveas'):
            the_field['disable_others'] = True
        if hasattr(field, 'uncheckothers') and field.uncheckothers is not False:
            the_field['uncheck_others'] = True
        for key in ('minlength', 'maxlength', 'min', 'max', 'step', 'scale', 'inline', 'inline width', 'rows', 'accept', 'currency symbol', 'field metadata', 'css class'):
            if key in self.extras and field.number in self.extras[key]:
                if key in ('minlength', 'maxlength', 'min', 'max', 'step'):
                    validation_rules_used.add(key)
                the_field[key] = self.extras[key][field.number]
        if hasattr(field, 'extras') and 'custom_parameters' in field.extras:
            for (parameter, parameter_value) in field.extras['custom_parameters'].items():
                the_field[parameter] = parameter_value
        for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
            if param_type in self.extras and field.number in self.extras[param_type]:
                for (parameter, parameter_value) in self.extras[param_type][field.number].items():
                    the_field[parameter] = parameter_value
        if hasattr(field, 'saveas') and field.saveas in self.embedded:
            the_field['embedded'] = True
        if hasattr(self, 'shuffle'):
            the_field['shuffle'] = self.shuffle
        if field.number in self.defaults:
            the_default = self.defaults[field.number]
            if isinstance(the_default, (str, int, bool, float)):
                the_field['default'] = the_default
        else:
            the_default = None
        if self.question.question_type == 'multiple_choice' or hasattr(field, 'choicetype') or (hasattr(field, 'datatype') and field.datatype in ('object', 'multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes', 'object_radio')):
            the_field['choices'] = self.get_choices_data(field, the_default, the_user_dict, encode=encode)
        if hasattr(field, 'nota'):
            the_field['none_of_the_above'] = docassemble.base.filter.markdown_to_html(self.extras['nota'][field.number], do_terms=False, status=self, verbatim=not encode)
        if field.number in self.extras['ok']:
            the_field['active'] = self.extras['ok'][field.number]
        else:
            the_field['active'] = True
        if field.number in self.extras['required']:
            the_field['required'] = self.extras['required'][field.number]
            if the_field['required']:
                validation_rules_used.add('required')
        if 'validation messages' in self.extras and field.number in self.extras['validation messages']:
            the_field['validation_messages'].update(self.extras['validation messages'][field.number])
        if 'permissions' in self.extras and field.number in self.extras['permissions']:
            the_field['permissions'] = self.extras['permissions'][field.number]
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('max_image_size' in self.extras) and self.extras['max_image_size']:
            the_field['max_image_size'] = self.extras['max_image_size']
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('image_type' in self.extras) and self.extras['image_type']:
            the_field['image_type'] = self.extras['image_type']
        if hasattr(field, 'extras'):
            if 'ml_group' in field.extras or 'ml_train' in field.extras:","for (term, vals) in self.question.interview.terms[lang].items():
    result['terms'][term] = vals['definition']",XXX,no_found,0,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,InterviewStatus,as_data$768,"def as_data(self, the_user_dict, encode=True):
    result = dict(language=self.question.language)
    debug = self.question.interview.debug
    if debug:
        output = dict(question='', help='')
    if 'progress' in the_user_dict['_internal']:
        result['progress'] = the_user_dict['_internal']['progress']
    if self.question.language in self.question.interview.default_validation_messages:
        result['validation_messages'] = copy.copy(self.question.interview.default_validation_messages[self.question.language])
    else:
        result['validation_messages'] = dict()
    if 'reload_after' in self.extras:
        result['reload'] = 1000 * int(self.extras['reload_after'])
    lang = docassemble.base.functions.get_language()
    if len(self.question.terms) or len(self.question.interview.terms):
        result['terms'] = dict()
        if 'terms' in self.extras:
            for (term, vals) in self.extras['terms'].items():
                result['terms'][term] = vals['definition']
        if lang in self.question.interview.terms and len(self.question.interview.terms[lang]):
            for (term, vals) in self.question.interview.terms[lang].items():
                result['terms'][term] = vals['definition']
        elif self.question.language in self.question.interview.terms and len(self.question.interview.terms[self.question.language]):
            for (term, vals) in self.question.interview.terms[self.question.language].items():
                result['terms'][term] = vals['definition']
    if len(self.question.autoterms) or len(self.question.interview.autoterms):
        result['autoterms'] = dict()
        if 'autoterms' in self.extras:
            for (term, vals) in self.extras['autoterms'].items():
                result['autoterms'][term] = vals['definition']
        if lang in self.question.interview.autoterms and len(self.question.interview.autoterms[lang]):
            for (term, vals) in self.question.interview.autoterms[lang].items():
                result['autoterms'][term] = vals['definition']
        elif self.question.language in self.question.interview.autoterms and len(self.question.interview.autoterms[self.question.language]):
            for (term, vals) in self.question.interview.autoterms[self.question.language].items():
                result['autoterms'][term] = vals['definition']
    if self.orig_sought is not None:
        result['event_list'] = [self.orig_sought]
    if 'action_buttons' in self.extras:
        result['additional_buttons'] = []
        for item in self.extras['action_buttons']:
            new_item = copy.deepcopy(item)
            new_item['label'] = docassemble.base.filter.markdown_to_html(item['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + new_item['label'] + '</p>'
    for param in ('questionText',):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if hasattr(self, 'breadcrumb') and self.breadcrumb is not None:
        output['breadcrumb label'] = self.breadcrumb
    output['breadcrumbs'] = docassemble.base.functions.get_action_stack()
    if hasattr(self, 'subquestionText') and self.subquestionText is not None:
        if self.question.question_type == 'fields':
            embedder = dummy_embed_input
        else:
            embedder = None
        result['subquestionText'] = docassemble.base.filter.markdown_to_html(self.subquestionText.rstrip(), status=self, verbatim=not encode, embedder=embedder)
        if debug:
            output['question'] += result['subquestionText']
    for param in ('continueLabel', 'helpLabel'):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + result[param] + '</p>'
    if 'menu_items' in self.extras and isinstance(self.extras['menu_items'], list):
        result['menu_items'] = self.extras['menu_items']
    for param in ('cssClass', 'tableCssClass', 'css', 'script'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = self.extras[param].rstrip()
    for param in ('back_button_label',):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
    for param in ('rightText', 'underText'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if 'continueLabel' not in result:
        if self.question.question_type == 'review':
            result['continueLabel'] = word('Resume')
        else:
            result['continueLabel'] = word('Continue')
        if debug:
            output['question'] += '<p>' + result['continueLabel'] + '</p>'
    if self.question.question_type == 'yesno':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
    elif self.question.question_type == 'noyes':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
    elif self.question.question_type == 'yesnomaybe':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    elif self.question.question_type == 'noyesmaybe':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    steps = the_user_dict['_internal']['steps'] - the_user_dict['_internal']['steps_offset']
    if self.can_go_back and steps > 1:
        result['allow_going_back'] = True
        result['backTitle'] = word('Go back to the previous question')
        back_button_val = self.extras.get('back_button', None)
        if back_button_val or (back_button_val is None and self.question.interview.question_back_button):
            result['questionBackButton'] = self.back
    else:
        result['allow_going_back'] = False
    if self.question.question_type == 'signature':
        result['signaturePhrases'] = {'clear': word('Clear'), 'noSignature': word('You must sign your name to continue.'), 'loading': word('Loading.  Please wait . . . ')}
    if 'questionMetadata' in self.extras:
        result['question_metadata'] = self.extras['questionMetadata']
    if 'segment' in self.extras:
        result['segment'] = self.extras['segment']
    if 'ga_id' in self.extras:
        result['ga_id'] = self.extras['ga_id']
    if hasattr(self.question, 'id'):
        result['id'] = self.question.id
    if hasattr(self, 'audiovideo') and self.audiovideo is not None:
        audio_result = docassemble.base.filter.get_audio_urls(self.audiovideo)
        video_result = docassemble.base.filter.get_video_urls(self.audiovideo)
        if len(audio_result) > 0:
            result['audio'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in audio_result]
        if len(video_result) > 0:
            result['video'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in video_result]
    if hasattr(self, 'helpText') and len(self.helpText) > 0:
        result['helpText'] = list()
        result['helpBackLabel'] = word('Back to question')
        for help_text in self.helpText:
            result['helpText'].append(self.convert_help(help_text, encode, debug))
        result['help'] = dict()
        if self.helpText[0]['label']:
            result['help']['label'] = docassemble.base.filter.markdown_to_html(self.helpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['label'] = self.question.help()
        result['help']['title'] = word('Help is available for this question')
        result['help']['specific'] = False if self.question.helptext is None else True
    if hasattr(self, 'interviewHelpText') and len(self.interviewHelpText) > 0:
        result['interviewHelpText'] = list()
        for help_text in self.interviewHelpText:
            result['interviewHelpText'].append(self.convert_help(help_text, encode, debug))
        if 'help' not in result:
            result['help'] = dict()
        if self.interviewHelpText[0]['label']:
            result['help']['interviewLabel'] = docassemble.base.filter.markdown_to_html(self.interviewHelpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['interviewLabel'] = self.question.help()
        result['help']['interviewTitle'] = word('Help is available')
        if not (hasattr(self, 'helpText') and len(self.helpText) > 0):
            result['help']['specific'] = False
    if 'questionText' not in result and self.question.question_type == 'signature':
        result['questionText'] = word('Sign Your Name')
        if debug:
            output['question'] += '<p>' + result['questionText'] + '</p>'
    result['questionType'] = self.question.question_type
    if hasattr(self.question, 'question_variety'):
        result['questionVariety'] = self.question.question_variety
    if self.question.is_mandatory or self.question.mandatory_code is not None:
        result['mandatory'] = True
    if hasattr(self.question, 'name'):
        result['_question_name'] = self.question.name
    result['_tracker'] = self.tracker
    if hasattr(self, 'datatypes'):
        result['_datatypes'] = safeid(json.dumps(self.datatypes))
    if hasattr(self, 'varnames'):
        result['_varnames'] = safeid(json.dumps(self.varnames))
    if len(self.question.fields) > 0:
        result['fields'] = list()
    if hasattr(self.question, 'review_saveas'):
        result['question_variable_name'] = self.question.review_saveas
    if hasattr(self.question, 'fields_saveas'):
        result['question_variable_name'] = self.question.fields_saveas
    if self.decorations is not None:
        width_value = get_config('decoration size', 2.0)
        width_units = get_config('decoration units', 'em')
        for decoration in self.decorations:
            if 'image' in decoration:
                result['decoration'] = {}
                the_image = self.question.interview.images.get(decoration['image'], None)
                if the_image is not None:
                    the_url = docassemble.base.functions.server.url_finder(str(the_image.package) + ':' + str(the_image.filename))
                    width = str(width_value) + str(width_units)
                    filename = docassemble.base.functions.server.file_finder(str(the_image.package) + ':' + str(the_image.filename))
                    if 'extension' in filename and filename['extension'] == 'svg' and ('width' in filename):
                        if filename['width'] and filename['height']:
                            height = str(width_value * (filename['height'] / filename['width'])) + str(width_units)
                    else:
                        height = 'auto'
                    if the_url is not None:
                        result['decoration']['url'] = the_url
                        result['decoration']['size'] = {'width': width, 'height': height}
                        if the_image.attribution is not None:
                            self.attributions.add(the_image.attribution)
                        break
                elif get_config('default icons', None) in ('material icons', 'font awesome'):
                    result['decoration']['name'] = decoration['image']
                    result['decoration']['size'] = str(width_value) + str(width_units)
                    break
    if len(self.attachments) > 0:
        result['attachments'] = list()
        if self.current_info['user']['is_authenticated'] and self.current_info['user']['email']:
            result['default_email'] = self.current_info['user']['email']
        for attachment in self.attachments:
            the_attachment = dict(url=dict(), number=dict(), filename_with_extension=dict())
            if 'orig_variable_name' in attachment and attachment['orig_variable_name']:
                the_attachment['variable_name'] = attachment['orig_variable_name']
            if 'name' in attachment:
                if attachment['name']:
                    the_attachment['name'] = docassemble.base.filter.markdown_to_html(attachment['name'], trim=True, status=self, verbatim=not encode)
                    if debug:
                        output['question'] += '<p>' + the_attachment['name'] + '</p>'
            if 'description' in attachment:
                if attachment['description']:
                    the_attachment['description'] = docassemble.base.filter.markdown_to_html(attachment['description'], status=self, verbatim=not encode)
                    if debug:
                        output['question'] += the_attachment['description']
            for key in ('valid_formats', 'filename', 'content', 'markdown', 'raw'):
                if key in attachment:
                    if attachment[key]:
                        the_attachment[key] = attachment[key]
            for the_format in attachment['file']:
                the_attachment['url'][the_format] = docassemble.base.functions.server.url_finder(attachment['file'][the_format], filename=attachment['filename'] + '.' + extension_of_doc_format[the_format])
                the_attachment['number'][the_format] = attachment['file'][the_format]
                the_attachment['filename_with_extension'][the_format] = attachment['filename'] + '.' + extension_of_doc_format[the_format]
            result['attachments'].append(the_attachment)
    if self.extras.get('list_collect', False) is not False:
        result['listCollect'] = {'deleteLabel': word('Delete'), 'addAnotherLabel': self.extras['list_collect_add_another_label'] if self.extras['list_collect_add_another_label'] else word('Add another'), 'deletedLabel': word('(Deleted)'), 'undeleteLabel': word('Undelete')}
    validation_rules_used = set()
    file_fields = list()
    for field in self.question.fields:
        the_field = dict()
        the_field['number'] = field.number
        if hasattr(field, 'saveas'):
            the_field['variable_name'] = from_safeid(field.saveas)
            if encode:
                the_field['variable_name_encoded'] = field.saveas
            the_field['validation_messages'] = dict()
            if self.question.question_type == 'multiple_choice' and self.question.question_variety in ['radio', 'dropdown', 'combobox']:
                if self.question.question_variety == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
            elif not (hasattr(field, 'datatype') and field.datatype in ['multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes']):
                if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                elif hasattr(field, 'inputtype') and field.inputtype == 'ajax':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one.'))
                elif hasattr(field, 'datatype') and (field.datatype == 'object_radio' or (hasattr(field, 'inputtype') and field.inputtype in ('yesnoradio', 'noyesradio', 'radio', 'dropdown'))):
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('required', self, word('This field is required.'))
            if hasattr(field, 'inputtype') and field.inputtype in ['yesno', 'noyes', 'yesnowide', 'noyeswide'] and hasattr(field, 'uncheckothers') and (field.uncheckothers is not False):
                the_field['validation_messages']['uncheckothers'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([strip_tags(self.labels[field.number])]))
            if hasattr(field, 'datatype') and field.datatype not in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                for key in ('minlength', 'maxlength'):
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
                        if key == 'minlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'maxlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        if hasattr(field, 'datatype'):
            if field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes') and (hasattr(field, 'nota') and self.extras['nota'][field.number] is not False or (hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)))):
                if field.datatype.endswith('checkboxes'):
                    d_type = 'checkbox'
                else:
                    d_type = 'multiselect'
                if hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)):
                    checkbox_messages = dict()
                    if 'minlength' in field.extras and 'minlength' in self.extras and ('maxlength' in field.extras) and ('maxlength' in self.extras) and (self.extras['minlength'][field.number] == self.extras['maxlength'][field.number]) and (self.extras['minlength'][field.number] > 0):
                        if 'nota' not in self.extras:
                            self.extras['nota'] = dict()
                        self.extras['nota'][field.number] = False
                        if d_type == 'checkbox':
                            checkbox_messages['checkexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                        else:
                            checkbox_messages['selectexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    else:
                        if 'minlength' in field.extras and 'minlength' in self.extras:
                            if d_type == 'checkbox':
                                if self.extras['minlength'][field.number] == 1:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select one.'))
                                else:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                                if int(float(self.extras['minlength'][field.number])) > 0:
                                    if 'nota' not in self.extras:
                                        self.extras['nota'] = dict()
                                    self.extras['nota'][field.number] = False
                            elif self.extras['minlength'][field.number] == 1:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select one.'))
                            else:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                        if 'maxlength' in field.extras and 'maxlength' in self.extras:
                            if d_type == 'checkbox':
                                checkbox_messages['checkatmost'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                            else:
                                checkbox_messages['maxlength'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    the_field['validation_messages'].update(checkbox_messages)
                if d_type == 'checkbox':
                    if hasattr(field, 'nota') and self.extras['nota'][field.number] is not False:
                        the_field['validation_messages']['checkatleast'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([self.extras['nota'][field.number]]))
            if field.datatype == 'date':
                the_field['validation_messages']['date'] = field.validation_message('date', self, word('You need to enter a valid date.'))
                if hasattr(field, 'extras') and 'min' in field.extras and ('min' in self.extras) and ('max' in field.extras) and ('max' in self.extras) and (field.number in self.extras['min']) and (field.number in self.extras['max']):
                    the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.extras['min'][field.number], format='medium'), docassemble.base.util.format_date(self.extras['max'][field.number], format='medium')))
                else:
                    was_defined = dict()
                    for key in ['min', 'max']:
                        if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                            was_defined[key] = True
                            if key == 'min':
                                the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                            elif key == 'max':
                                the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                    if len(was_defined) == 0 and 'default date min' in self.question.interview.options and ('default date max' in self.question.interview.options):
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium'), docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')))
                    elif 'max' not in was_defined and 'default date max' in self.question.interview.options:
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')]))
                    elif 'min' not in was_defined and 'default date min' in self.question.interview.options:
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium')]))
            if field.datatype == 'time':
                the_field['validation_messages']['time'] = field.validation_message('time', self, word('You need to enter a valid time.'))
            if field.datatype in ['datetime', 'datetime-local']:
                the_field['validation_messages']['datetime'] = field.validation_message('datetime', self, word('You need to enter a valid date and time.'))
            if field.datatype == 'email':
                the_field['validation_messages']['email'] = field.validation_message('email', self, word('You need to enter a complete e-mail address.'))
            if field.datatype in ['number', 'currency', 'float', 'integer']:
                the_field['validation_messages']['number'] = field.validation_message('number', self, word('You need to enter a number.'))
                if field.datatype == 'integer' and (not ('step' in self.extras and field.number in self.extras['step'])):
                    the_field['validation_messages']['step'] = field.validation_message('integer', self, word('Please enter a whole number.'))
                elif 'step' in self.extras and field.number in self.extras['step']:
                    the_field['validation_messages']['step'] = field.validation_message('step', self, word('Please enter a multiple of {0}.'))
                for key in ['min', 'max']:
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                        if key == 'min':
                            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'max':
                            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))
            if field.datatype in ['files', 'file', 'camera', 'user', 'environment', 'camcorder', 'microphone']:
                file_fields.append(field)
                the_field['validation_messages']['required'] = field.validation_message('file required', self, word('You must provide a file.'))
                if 'accept' in self.extras and field.number in self.extras['accept']:
                    the_field['validation_messages']['accept'] = field.validation_message('accept', self, word('Please upload a file with a valid file format.'))
                if get_config('maximum content length') is not None:
                    the_field['max'] = get_config('maximum content length')
                    the_field['validation_messages']['max'] = field.validation_message('maxuploadsize', self, word('Your file upload is larger than the server can accept. Please reduce the size of your file upload.'))
        for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
            if hasattr(field, param):
                the_field[param] = getattr(field, param)
        if hasattr(field, 'shuffle') and field.shuffle is not False:
            the_field['shuffle'] = True
        if hasattr(field, 'disableothers') and field.disableothers and hasattr(field, 'saveas'):
            the_field['disable_others'] = True
        if hasattr(field, 'uncheckothers') and field.uncheckothers is not False:
            the_field['uncheck_others'] = True
        for key in ('minlength', 'maxlength', 'min', 'max', 'step', 'scale', 'inline', 'inline width', 'rows', 'accept', 'currency symbol', 'field metadata', 'css class'):
            if key in self.extras and field.number in self.extras[key]:
                if key in ('minlength', 'maxlength', 'min', 'max', 'step'):
                    validation_rules_used.add(key)
                the_field[key] = self.extras[key][field.number]
        if hasattr(field, 'extras') and 'custom_parameters' in field.extras:
            for (parameter, parameter_value) in field.extras['custom_parameters'].items():
                the_field[parameter] = parameter_value
        for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
            if param_type in self.extras and field.number in self.extras[param_type]:
                for (parameter, parameter_value) in self.extras[param_type][field.number].items():
                    the_field[parameter] = parameter_value
        if hasattr(field, 'saveas') and field.saveas in self.embedded:
            the_field['embedded'] = True
        if hasattr(self, 'shuffle'):
            the_field['shuffle'] = self.shuffle
        if field.number in self.defaults:
            the_default = self.defaults[field.number]
            if isinstance(the_default, (str, int, bool, float)):
                the_field['default'] = the_default
        else:
            the_default = None
        if self.question.question_type == 'multiple_choice' or hasattr(field, 'choicetype') or (hasattr(field, 'datatype') and field.datatype in ('object', 'multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes', 'object_radio')):
            the_field['choices'] = self.get_choices_data(field, the_default, the_user_dict, encode=encode)
        if hasattr(field, 'nota'):
            the_field['none_of_the_above'] = docassemble.base.filter.markdown_to_html(self.extras['nota'][field.number], do_terms=False, status=self, verbatim=not encode)
        if field.number in self.extras['ok']:
            the_field['active'] = self.extras['ok'][field.number]
        else:
            the_field['active'] = True
        if field.number in self.extras['required']:
            the_field['required'] = self.extras['required'][field.number]
            if the_field['required']:
                validation_rules_used.add('required')
        if 'validation messages' in self.extras and field.number in self.extras['validation messages']:
            the_field['validation_messages'].update(self.extras['validation messages'][field.number])
        if 'permissions' in self.extras and field.number in self.extras['permissions']:
            the_field['permissions'] = self.extras['permissions'][field.number]
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('max_image_size' in self.extras) and self.extras['max_image_size']:
            the_field['max_image_size'] = self.extras['max_image_size']
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('image_type' in self.extras) and self.extras['image_type']:
            the_field['image_type'] = self.extras['image_type']
        if hasattr(field, 'extras'):
            if 'ml_group' in field.extras or 'ml_train' in field.extras:","for (term, vals) in self.extras['autoterms'].items():
    result['autoterms'][term] = vals['definition']",XXX,no_found,0,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,InterviewStatus,as_data$768,"def as_data(self, the_user_dict, encode=True):
    result = dict(language=self.question.language)
    debug = self.question.interview.debug
    if debug:
        output = dict(question='', help='')
    if 'progress' in the_user_dict['_internal']:
        result['progress'] = the_user_dict['_internal']['progress']
    if self.question.language in self.question.interview.default_validation_messages:
        result['validation_messages'] = copy.copy(self.question.interview.default_validation_messages[self.question.language])
    else:
        result['validation_messages'] = dict()
    if 'reload_after' in self.extras:
        result['reload'] = 1000 * int(self.extras['reload_after'])
    lang = docassemble.base.functions.get_language()
    if len(self.question.terms) or len(self.question.interview.terms):
        result['terms'] = dict()
        if 'terms' in self.extras:
            for (term, vals) in self.extras['terms'].items():
                result['terms'][term] = vals['definition']
        if lang in self.question.interview.terms and len(self.question.interview.terms[lang]):
            for (term, vals) in self.question.interview.terms[lang].items():
                result['terms'][term] = vals['definition']
        elif self.question.language in self.question.interview.terms and len(self.question.interview.terms[self.question.language]):
            for (term, vals) in self.question.interview.terms[self.question.language].items():
                result['terms'][term] = vals['definition']
    if len(self.question.autoterms) or len(self.question.interview.autoterms):
        result['autoterms'] = dict()
        if 'autoterms' in self.extras:
            for (term, vals) in self.extras['autoterms'].items():
                result['autoterms'][term] = vals['definition']
        if lang in self.question.interview.autoterms and len(self.question.interview.autoterms[lang]):
            for (term, vals) in self.question.interview.autoterms[lang].items():
                result['autoterms'][term] = vals['definition']
        elif self.question.language in self.question.interview.autoterms and len(self.question.interview.autoterms[self.question.language]):
            for (term, vals) in self.question.interview.autoterms[self.question.language].items():
                result['autoterms'][term] = vals['definition']
    if self.orig_sought is not None:
        result['event_list'] = [self.orig_sought]
    if 'action_buttons' in self.extras:
        result['additional_buttons'] = []
        for item in self.extras['action_buttons']:
            new_item = copy.deepcopy(item)
            new_item['label'] = docassemble.base.filter.markdown_to_html(item['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + new_item['label'] + '</p>'
    for param in ('questionText',):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if hasattr(self, 'breadcrumb') and self.breadcrumb is not None:
        output['breadcrumb label'] = self.breadcrumb
    output['breadcrumbs'] = docassemble.base.functions.get_action_stack()
    if hasattr(self, 'subquestionText') and self.subquestionText is not None:
        if self.question.question_type == 'fields':
            embedder = dummy_embed_input
        else:
            embedder = None
        result['subquestionText'] = docassemble.base.filter.markdown_to_html(self.subquestionText.rstrip(), status=self, verbatim=not encode, embedder=embedder)
        if debug:
            output['question'] += result['subquestionText']
    for param in ('continueLabel', 'helpLabel'):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + result[param] + '</p>'
    if 'menu_items' in self.extras and isinstance(self.extras['menu_items'], list):
        result['menu_items'] = self.extras['menu_items']
    for param in ('cssClass', 'tableCssClass', 'css', 'script'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = self.extras[param].rstrip()
    for param in ('back_button_label',):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
    for param in ('rightText', 'underText'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if 'continueLabel' not in result:
        if self.question.question_type == 'review':
            result['continueLabel'] = word('Resume')
        else:
            result['continueLabel'] = word('Continue')
        if debug:
            output['question'] += '<p>' + result['continueLabel'] + '</p>'
    if self.question.question_type == 'yesno':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
    elif self.question.question_type == 'noyes':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
    elif self.question.question_type == 'yesnomaybe':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    elif self.question.question_type == 'noyesmaybe':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    steps = the_user_dict['_internal']['steps'] - the_user_dict['_internal']['steps_offset']
    if self.can_go_back and steps > 1:
        result['allow_going_back'] = True
        result['backTitle'] = word('Go back to the previous question')
        back_button_val = self.extras.get('back_button', None)
        if back_button_val or (back_button_val is None and self.question.interview.question_back_button):
            result['questionBackButton'] = self.back
    else:
        result['allow_going_back'] = False
    if self.question.question_type == 'signature':
        result['signaturePhrases'] = {'clear': word('Clear'), 'noSignature': word('You must sign your name to continue.'), 'loading': word('Loading.  Please wait . . . ')}
    if 'questionMetadata' in self.extras:
        result['question_metadata'] = self.extras['questionMetadata']
    if 'segment' in self.extras:
        result['segment'] = self.extras['segment']
    if 'ga_id' in self.extras:
        result['ga_id'] = self.extras['ga_id']
    if hasattr(self.question, 'id'):
        result['id'] = self.question.id
    if hasattr(self, 'audiovideo') and self.audiovideo is not None:
        audio_result = docassemble.base.filter.get_audio_urls(self.audiovideo)
        video_result = docassemble.base.filter.get_video_urls(self.audiovideo)
        if len(audio_result) > 0:
            result['audio'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in audio_result]
        if len(video_result) > 0:
            result['video'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in video_result]
    if hasattr(self, 'helpText') and len(self.helpText) > 0:
        result['helpText'] = list()
        result['helpBackLabel'] = word('Back to question')
        for help_text in self.helpText:
            result['helpText'].append(self.convert_help(help_text, encode, debug))
        result['help'] = dict()
        if self.helpText[0]['label']:
            result['help']['label'] = docassemble.base.filter.markdown_to_html(self.helpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['label'] = self.question.help()
        result['help']['title'] = word('Help is available for this question')
        result['help']['specific'] = False if self.question.helptext is None else True
    if hasattr(self, 'interviewHelpText') and len(self.interviewHelpText) > 0:
        result['interviewHelpText'] = list()
        for help_text in self.interviewHelpText:
            result['interviewHelpText'].append(self.convert_help(help_text, encode, debug))
        if 'help' not in result:
            result['help'] = dict()
        if self.interviewHelpText[0]['label']:
            result['help']['interviewLabel'] = docassemble.base.filter.markdown_to_html(self.interviewHelpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['interviewLabel'] = self.question.help()
        result['help']['interviewTitle'] = word('Help is available')
        if not (hasattr(self, 'helpText') and len(self.helpText) > 0):
            result['help']['specific'] = False
    if 'questionText' not in result and self.question.question_type == 'signature':
        result['questionText'] = word('Sign Your Name')
        if debug:
            output['question'] += '<p>' + result['questionText'] + '</p>'
    result['questionType'] = self.question.question_type
    if hasattr(self.question, 'question_variety'):
        result['questionVariety'] = self.question.question_variety
    if self.question.is_mandatory or self.question.mandatory_code is not None:
        result['mandatory'] = True
    if hasattr(self.question, 'name'):
        result['_question_name'] = self.question.name
    result['_tracker'] = self.tracker
    if hasattr(self, 'datatypes'):
        result['_datatypes'] = safeid(json.dumps(self.datatypes))
    if hasattr(self, 'varnames'):
        result['_varnames'] = safeid(json.dumps(self.varnames))
    if len(self.question.fields) > 0:
        result['fields'] = list()
    if hasattr(self.question, 'review_saveas'):
        result['question_variable_name'] = self.question.review_saveas
    if hasattr(self.question, 'fields_saveas'):
        result['question_variable_name'] = self.question.fields_saveas
    if self.decorations is not None:
        width_value = get_config('decoration size', 2.0)
        width_units = get_config('decoration units', 'em')
        for decoration in self.decorations:
            if 'image' in decoration:
                result['decoration'] = {}
                the_image = self.question.interview.images.get(decoration['image'], None)
                if the_image is not None:
                    the_url = docassemble.base.functions.server.url_finder(str(the_image.package) + ':' + str(the_image.filename))
                    width = str(width_value) + str(width_units)
                    filename = docassemble.base.functions.server.file_finder(str(the_image.package) + ':' + str(the_image.filename))
                    if 'extension' in filename and filename['extension'] == 'svg' and ('width' in filename):
                        if filename['width'] and filename['height']:
                            height = str(width_value * (filename['height'] / filename['width'])) + str(width_units)
                    else:
                        height = 'auto'
                    if the_url is not None:
                        result['decoration']['url'] = the_url
                        result['decoration']['size'] = {'width': width, 'height': height}
                        if the_image.attribution is not None:
                            self.attributions.add(the_image.attribution)
                        break
                elif get_config('default icons', None) in ('material icons', 'font awesome'):
                    result['decoration']['name'] = decoration['image']
                    result['decoration']['size'] = str(width_value) + str(width_units)
                    break
    if len(self.attachments) > 0:
        result['attachments'] = list()
        if self.current_info['user']['is_authenticated'] and self.current_info['user']['email']:
            result['default_email'] = self.current_info['user']['email']
        for attachment in self.attachments:
            the_attachment = dict(url=dict(), number=dict(), filename_with_extension=dict())
            if 'orig_variable_name' in attachment and attachment['orig_variable_name']:
                the_attachment['variable_name'] = attachment['orig_variable_name']
            if 'name' in attachment:
                if attachment['name']:
                    the_attachment['name'] = docassemble.base.filter.markdown_to_html(attachment['name'], trim=True, status=self, verbatim=not encode)
                    if debug:
                        output['question'] += '<p>' + the_attachment['name'] + '</p>'
            if 'description' in attachment:
                if attachment['description']:
                    the_attachment['description'] = docassemble.base.filter.markdown_to_html(attachment['description'], status=self, verbatim=not encode)
                    if debug:
                        output['question'] += the_attachment['description']
            for key in ('valid_formats', 'filename', 'content', 'markdown', 'raw'):
                if key in attachment:
                    if attachment[key]:
                        the_attachment[key] = attachment[key]
            for the_format in attachment['file']:
                the_attachment['url'][the_format] = docassemble.base.functions.server.url_finder(attachment['file'][the_format], filename=attachment['filename'] + '.' + extension_of_doc_format[the_format])
                the_attachment['number'][the_format] = attachment['file'][the_format]
                the_attachment['filename_with_extension'][the_format] = attachment['filename'] + '.' + extension_of_doc_format[the_format]
            result['attachments'].append(the_attachment)
    if self.extras.get('list_collect', False) is not False:
        result['listCollect'] = {'deleteLabel': word('Delete'), 'addAnotherLabel': self.extras['list_collect_add_another_label'] if self.extras['list_collect_add_another_label'] else word('Add another'), 'deletedLabel': word('(Deleted)'), 'undeleteLabel': word('Undelete')}
    validation_rules_used = set()
    file_fields = list()
    for field in self.question.fields:
        the_field = dict()
        the_field['number'] = field.number
        if hasattr(field, 'saveas'):
            the_field['variable_name'] = from_safeid(field.saveas)
            if encode:
                the_field['variable_name_encoded'] = field.saveas
            the_field['validation_messages'] = dict()
            if self.question.question_type == 'multiple_choice' and self.question.question_variety in ['radio', 'dropdown', 'combobox']:
                if self.question.question_variety == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
            elif not (hasattr(field, 'datatype') and field.datatype in ['multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes']):
                if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                elif hasattr(field, 'inputtype') and field.inputtype == 'ajax':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one.'))
                elif hasattr(field, 'datatype') and (field.datatype == 'object_radio' or (hasattr(field, 'inputtype') and field.inputtype in ('yesnoradio', 'noyesradio', 'radio', 'dropdown'))):
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('required', self, word('This field is required.'))
            if hasattr(field, 'inputtype') and field.inputtype in ['yesno', 'noyes', 'yesnowide', 'noyeswide'] and hasattr(field, 'uncheckothers') and (field.uncheckothers is not False):
                the_field['validation_messages']['uncheckothers'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([strip_tags(self.labels[field.number])]))
            if hasattr(field, 'datatype') and field.datatype not in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                for key in ('minlength', 'maxlength'):
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
                        if key == 'minlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'maxlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        if hasattr(field, 'datatype'):
            if field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes') and (hasattr(field, 'nota') and self.extras['nota'][field.number] is not False or (hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)))):
                if field.datatype.endswith('checkboxes'):
                    d_type = 'checkbox'
                else:
                    d_type = 'multiselect'
                if hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)):
                    checkbox_messages = dict()
                    if 'minlength' in field.extras and 'minlength' in self.extras and ('maxlength' in field.extras) and ('maxlength' in self.extras) and (self.extras['minlength'][field.number] == self.extras['maxlength'][field.number]) and (self.extras['minlength'][field.number] > 0):
                        if 'nota' not in self.extras:
                            self.extras['nota'] = dict()
                        self.extras['nota'][field.number] = False
                        if d_type == 'checkbox':
                            checkbox_messages['checkexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                        else:
                            checkbox_messages['selectexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    else:
                        if 'minlength' in field.extras and 'minlength' in self.extras:
                            if d_type == 'checkbox':
                                if self.extras['minlength'][field.number] == 1:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select one.'))
                                else:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                                if int(float(self.extras['minlength'][field.number])) > 0:
                                    if 'nota' not in self.extras:
                                        self.extras['nota'] = dict()
                                    self.extras['nota'][field.number] = False
                            elif self.extras['minlength'][field.number] == 1:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select one.'))
                            else:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                        if 'maxlength' in field.extras and 'maxlength' in self.extras:
                            if d_type == 'checkbox':
                                checkbox_messages['checkatmost'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                            else:
                                checkbox_messages['maxlength'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    the_field['validation_messages'].update(checkbox_messages)
                if d_type == 'checkbox':
                    if hasattr(field, 'nota') and self.extras['nota'][field.number] is not False:
                        the_field['validation_messages']['checkatleast'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([self.extras['nota'][field.number]]))
            if field.datatype == 'date':
                the_field['validation_messages']['date'] = field.validation_message('date', self, word('You need to enter a valid date.'))
                if hasattr(field, 'extras') and 'min' in field.extras and ('min' in self.extras) and ('max' in field.extras) and ('max' in self.extras) and (field.number in self.extras['min']) and (field.number in self.extras['max']):
                    the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.extras['min'][field.number], format='medium'), docassemble.base.util.format_date(self.extras['max'][field.number], format='medium')))
                else:
                    was_defined = dict()
                    for key in ['min', 'max']:
                        if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                            was_defined[key] = True
                            if key == 'min':
                                the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                            elif key == 'max':
                                the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                    if len(was_defined) == 0 and 'default date min' in self.question.interview.options and ('default date max' in self.question.interview.options):
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium'), docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')))
                    elif 'max' not in was_defined and 'default date max' in self.question.interview.options:
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')]))
                    elif 'min' not in was_defined and 'default date min' in self.question.interview.options:
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium')]))
            if field.datatype == 'time':
                the_field['validation_messages']['time'] = field.validation_message('time', self, word('You need to enter a valid time.'))
            if field.datatype in ['datetime', 'datetime-local']:
                the_field['validation_messages']['datetime'] = field.validation_message('datetime', self, word('You need to enter a valid date and time.'))
            if field.datatype == 'email':
                the_field['validation_messages']['email'] = field.validation_message('email', self, word('You need to enter a complete e-mail address.'))
            if field.datatype in ['number', 'currency', 'float', 'integer']:
                the_field['validation_messages']['number'] = field.validation_message('number', self, word('You need to enter a number.'))
                if field.datatype == 'integer' and (not ('step' in self.extras and field.number in self.extras['step'])):
                    the_field['validation_messages']['step'] = field.validation_message('integer', self, word('Please enter a whole number.'))
                elif 'step' in self.extras and field.number in self.extras['step']:
                    the_field['validation_messages']['step'] = field.validation_message('step', self, word('Please enter a multiple of {0}.'))
                for key in ['min', 'max']:
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                        if key == 'min':
                            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'max':
                            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))
            if field.datatype in ['files', 'file', 'camera', 'user', 'environment', 'camcorder', 'microphone']:
                file_fields.append(field)
                the_field['validation_messages']['required'] = field.validation_message('file required', self, word('You must provide a file.'))
                if 'accept' in self.extras and field.number in self.extras['accept']:
                    the_field['validation_messages']['accept'] = field.validation_message('accept', self, word('Please upload a file with a valid file format.'))
                if get_config('maximum content length') is not None:
                    the_field['max'] = get_config('maximum content length')
                    the_field['validation_messages']['max'] = field.validation_message('maxuploadsize', self, word('Your file upload is larger than the server can accept. Please reduce the size of your file upload.'))
        for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
            if hasattr(field, param):
                the_field[param] = getattr(field, param)
        if hasattr(field, 'shuffle') and field.shuffle is not False:
            the_field['shuffle'] = True
        if hasattr(field, 'disableothers') and field.disableothers and hasattr(field, 'saveas'):
            the_field['disable_others'] = True
        if hasattr(field, 'uncheckothers') and field.uncheckothers is not False:
            the_field['uncheck_others'] = True
        for key in ('minlength', 'maxlength', 'min', 'max', 'step', 'scale', 'inline', 'inline width', 'rows', 'accept', 'currency symbol', 'field metadata', 'css class'):
            if key in self.extras and field.number in self.extras[key]:
                if key in ('minlength', 'maxlength', 'min', 'max', 'step'):
                    validation_rules_used.add(key)
                the_field[key] = self.extras[key][field.number]
        if hasattr(field, 'extras') and 'custom_parameters' in field.extras:
            for (parameter, parameter_value) in field.extras['custom_parameters'].items():
                the_field[parameter] = parameter_value
        for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
            if param_type in self.extras and field.number in self.extras[param_type]:
                for (parameter, parameter_value) in self.extras[param_type][field.number].items():
                    the_field[parameter] = parameter_value
        if hasattr(field, 'saveas') and field.saveas in self.embedded:
            the_field['embedded'] = True
        if hasattr(self, 'shuffle'):
            the_field['shuffle'] = self.shuffle
        if field.number in self.defaults:
            the_default = self.defaults[field.number]
            if isinstance(the_default, (str, int, bool, float)):
                the_field['default'] = the_default
        else:
            the_default = None
        if self.question.question_type == 'multiple_choice' or hasattr(field, 'choicetype') or (hasattr(field, 'datatype') and field.datatype in ('object', 'multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes', 'object_radio')):
            the_field['choices'] = self.get_choices_data(field, the_default, the_user_dict, encode=encode)
        if hasattr(field, 'nota'):
            the_field['none_of_the_above'] = docassemble.base.filter.markdown_to_html(self.extras['nota'][field.number], do_terms=False, status=self, verbatim=not encode)
        if field.number in self.extras['ok']:
            the_field['active'] = self.extras['ok'][field.number]
        else:
            the_field['active'] = True
        if field.number in self.extras['required']:
            the_field['required'] = self.extras['required'][field.number]
            if the_field['required']:
                validation_rules_used.add('required')
        if 'validation messages' in self.extras and field.number in self.extras['validation messages']:
            the_field['validation_messages'].update(self.extras['validation messages'][field.number])
        if 'permissions' in self.extras and field.number in self.extras['permissions']:
            the_field['permissions'] = self.extras['permissions'][field.number]
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('max_image_size' in self.extras) and self.extras['max_image_size']:
            the_field['max_image_size'] = self.extras['max_image_size']
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('image_type' in self.extras) and self.extras['image_type']:
            the_field['image_type'] = self.extras['image_type']
        if hasattr(field, 'extras'):
            if 'ml_group' in field.extras or 'ml_train' in field.extras:","for (term, vals) in self.question.interview.autoterms[lang].items():
    result['autoterms'][term] = vals['definition']",XXX,no_found,0,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,InterviewStatus,as_data$768,"def as_data(self, the_user_dict, encode=True):
    result = dict(language=self.question.language)
    debug = self.question.interview.debug
    if debug:
        output = dict(question='', help='')
    if 'progress' in the_user_dict['_internal']:
        result['progress'] = the_user_dict['_internal']['progress']
    if self.question.language in self.question.interview.default_validation_messages:
        result['validation_messages'] = copy.copy(self.question.interview.default_validation_messages[self.question.language])
    else:
        result['validation_messages'] = dict()
    if 'reload_after' in self.extras:
        result['reload'] = 1000 * int(self.extras['reload_after'])
    lang = docassemble.base.functions.get_language()
    if len(self.question.terms) or len(self.question.interview.terms):
        result['terms'] = dict()
        if 'terms' in self.extras:
            for (term, vals) in self.extras['terms'].items():
                result['terms'][term] = vals['definition']
        if lang in self.question.interview.terms and len(self.question.interview.terms[lang]):
            for (term, vals) in self.question.interview.terms[lang].items():
                result['terms'][term] = vals['definition']
        elif self.question.language in self.question.interview.terms and len(self.question.interview.terms[self.question.language]):
            for (term, vals) in self.question.interview.terms[self.question.language].items():
                result['terms'][term] = vals['definition']
    if len(self.question.autoterms) or len(self.question.interview.autoterms):
        result['autoterms'] = dict()
        if 'autoterms' in self.extras:
            for (term, vals) in self.extras['autoterms'].items():
                result['autoterms'][term] = vals['definition']
        if lang in self.question.interview.autoterms and len(self.question.interview.autoterms[lang]):
            for (term, vals) in self.question.interview.autoterms[lang].items():
                result['autoterms'][term] = vals['definition']
        elif self.question.language in self.question.interview.autoterms and len(self.question.interview.autoterms[self.question.language]):
            for (term, vals) in self.question.interview.autoterms[self.question.language].items():
                result['autoterms'][term] = vals['definition']
    if self.orig_sought is not None:
        result['event_list'] = [self.orig_sought]
    if 'action_buttons' in self.extras:
        result['additional_buttons'] = []
        for item in self.extras['action_buttons']:
            new_item = copy.deepcopy(item)
            new_item['label'] = docassemble.base.filter.markdown_to_html(item['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + new_item['label'] + '</p>'
    for param in ('questionText',):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if hasattr(self, 'breadcrumb') and self.breadcrumb is not None:
        output['breadcrumb label'] = self.breadcrumb
    output['breadcrumbs'] = docassemble.base.functions.get_action_stack()
    if hasattr(self, 'subquestionText') and self.subquestionText is not None:
        if self.question.question_type == 'fields':
            embedder = dummy_embed_input
        else:
            embedder = None
        result['subquestionText'] = docassemble.base.filter.markdown_to_html(self.subquestionText.rstrip(), status=self, verbatim=not encode, embedder=embedder)
        if debug:
            output['question'] += result['subquestionText']
    for param in ('continueLabel', 'helpLabel'):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + result[param] + '</p>'
    if 'menu_items' in self.extras and isinstance(self.extras['menu_items'], list):
        result['menu_items'] = self.extras['menu_items']
    for param in ('cssClass', 'tableCssClass', 'css', 'script'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = self.extras[param].rstrip()
    for param in ('back_button_label',):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
    for param in ('rightText', 'underText'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if 'continueLabel' not in result:
        if self.question.question_type == 'review':
            result['continueLabel'] = word('Resume')
        else:
            result['continueLabel'] = word('Continue')
        if debug:
            output['question'] += '<p>' + result['continueLabel'] + '</p>'
    if self.question.question_type == 'yesno':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
    elif self.question.question_type == 'noyes':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
    elif self.question.question_type == 'yesnomaybe':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    elif self.question.question_type == 'noyesmaybe':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    steps = the_user_dict['_internal']['steps'] - the_user_dict['_internal']['steps_offset']
    if self.can_go_back and steps > 1:
        result['allow_going_back'] = True
        result['backTitle'] = word('Go back to the previous question')
        back_button_val = self.extras.get('back_button', None)
        if back_button_val or (back_button_val is None and self.question.interview.question_back_button):
            result['questionBackButton'] = self.back
    else:
        result['allow_going_back'] = False
    if self.question.question_type == 'signature':
        result['signaturePhrases'] = {'clear': word('Clear'), 'noSignature': word('You must sign your name to continue.'), 'loading': word('Loading.  Please wait . . . ')}
    if 'questionMetadata' in self.extras:
        result['question_metadata'] = self.extras['questionMetadata']
    if 'segment' in self.extras:
        result['segment'] = self.extras['segment']
    if 'ga_id' in self.extras:
        result['ga_id'] = self.extras['ga_id']
    if hasattr(self.question, 'id'):
        result['id'] = self.question.id
    if hasattr(self, 'audiovideo') and self.audiovideo is not None:
        audio_result = docassemble.base.filter.get_audio_urls(self.audiovideo)
        video_result = docassemble.base.filter.get_video_urls(self.audiovideo)
        if len(audio_result) > 0:
            result['audio'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in audio_result]
        if len(video_result) > 0:
            result['video'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in video_result]
    if hasattr(self, 'helpText') and len(self.helpText) > 0:
        result['helpText'] = list()
        result['helpBackLabel'] = word('Back to question')
        for help_text in self.helpText:
            result['helpText'].append(self.convert_help(help_text, encode, debug))
        result['help'] = dict()
        if self.helpText[0]['label']:
            result['help']['label'] = docassemble.base.filter.markdown_to_html(self.helpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['label'] = self.question.help()
        result['help']['title'] = word('Help is available for this question')
        result['help']['specific'] = False if self.question.helptext is None else True
    if hasattr(self, 'interviewHelpText') and len(self.interviewHelpText) > 0:
        result['interviewHelpText'] = list()
        for help_text in self.interviewHelpText:
            result['interviewHelpText'].append(self.convert_help(help_text, encode, debug))
        if 'help' not in result:
            result['help'] = dict()
        if self.interviewHelpText[0]['label']:
            result['help']['interviewLabel'] = docassemble.base.filter.markdown_to_html(self.interviewHelpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['interviewLabel'] = self.question.help()
        result['help']['interviewTitle'] = word('Help is available')
        if not (hasattr(self, 'helpText') and len(self.helpText) > 0):
            result['help']['specific'] = False
    if 'questionText' not in result and self.question.question_type == 'signature':
        result['questionText'] = word('Sign Your Name')
        if debug:
            output['question'] += '<p>' + result['questionText'] + '</p>'
    result['questionType'] = self.question.question_type
    if hasattr(self.question, 'question_variety'):
        result['questionVariety'] = self.question.question_variety
    if self.question.is_mandatory or self.question.mandatory_code is not None:
        result['mandatory'] = True
    if hasattr(self.question, 'name'):
        result['_question_name'] = self.question.name
    result['_tracker'] = self.tracker
    if hasattr(self, 'datatypes'):
        result['_datatypes'] = safeid(json.dumps(self.datatypes))
    if hasattr(self, 'varnames'):
        result['_varnames'] = safeid(json.dumps(self.varnames))
    if len(self.question.fields) > 0:
        result['fields'] = list()
    if hasattr(self.question, 'review_saveas'):
        result['question_variable_name'] = self.question.review_saveas
    if hasattr(self.question, 'fields_saveas'):
        result['question_variable_name'] = self.question.fields_saveas
    if self.decorations is not None:
        width_value = get_config('decoration size', 2.0)
        width_units = get_config('decoration units', 'em')
        for decoration in self.decorations:
            if 'image' in decoration:
                result['decoration'] = {}
                the_image = self.question.interview.images.get(decoration['image'], None)
                if the_image is not None:
                    the_url = docassemble.base.functions.server.url_finder(str(the_image.package) + ':' + str(the_image.filename))
                    width = str(width_value) + str(width_units)
                    filename = docassemble.base.functions.server.file_finder(str(the_image.package) + ':' + str(the_image.filename))
                    if 'extension' in filename and filename['extension'] == 'svg' and ('width' in filename):
                        if filename['width'] and filename['height']:
                            height = str(width_value * (filename['height'] / filename['width'])) + str(width_units)
                    else:
                        height = 'auto'
                    if the_url is not None:
                        result['decoration']['url'] = the_url
                        result['decoration']['size'] = {'width': width, 'height': height}
                        if the_image.attribution is not None:
                            self.attributions.add(the_image.attribution)
                        break
                elif get_config('default icons', None) in ('material icons', 'font awesome'):
                    result['decoration']['name'] = decoration['image']
                    result['decoration']['size'] = str(width_value) + str(width_units)
                    break
    if len(self.attachments) > 0:
        result['attachments'] = list()
        if self.current_info['user']['is_authenticated'] and self.current_info['user']['email']:
            result['default_email'] = self.current_info['user']['email']
        for attachment in self.attachments:
            the_attachment = dict(url=dict(), number=dict(), filename_with_extension=dict())
            if 'orig_variable_name' in attachment and attachment['orig_variable_name']:
                the_attachment['variable_name'] = attachment['orig_variable_name']
            if 'name' in attachment:
                if attachment['name']:
                    the_attachment['name'] = docassemble.base.filter.markdown_to_html(attachment['name'], trim=True, status=self, verbatim=not encode)
                    if debug:
                        output['question'] += '<p>' + the_attachment['name'] + '</p>'
            if 'description' in attachment:
                if attachment['description']:
                    the_attachment['description'] = docassemble.base.filter.markdown_to_html(attachment['description'], status=self, verbatim=not encode)
                    if debug:
                        output['question'] += the_attachment['description']
            for key in ('valid_formats', 'filename', 'content', 'markdown', 'raw'):
                if key in attachment:
                    if attachment[key]:
                        the_attachment[key] = attachment[key]
            for the_format in attachment['file']:
                the_attachment['url'][the_format] = docassemble.base.functions.server.url_finder(attachment['file'][the_format], filename=attachment['filename'] + '.' + extension_of_doc_format[the_format])
                the_attachment['number'][the_format] = attachment['file'][the_format]
                the_attachment['filename_with_extension'][the_format] = attachment['filename'] + '.' + extension_of_doc_format[the_format]
            result['attachments'].append(the_attachment)
    if self.extras.get('list_collect', False) is not False:
        result['listCollect'] = {'deleteLabel': word('Delete'), 'addAnotherLabel': self.extras['list_collect_add_another_label'] if self.extras['list_collect_add_another_label'] else word('Add another'), 'deletedLabel': word('(Deleted)'), 'undeleteLabel': word('Undelete')}
    validation_rules_used = set()
    file_fields = list()
    for field in self.question.fields:
        the_field = dict()
        the_field['number'] = field.number
        if hasattr(field, 'saveas'):
            the_field['variable_name'] = from_safeid(field.saveas)
            if encode:
                the_field['variable_name_encoded'] = field.saveas
            the_field['validation_messages'] = dict()
            if self.question.question_type == 'multiple_choice' and self.question.question_variety in ['radio', 'dropdown', 'combobox']:
                if self.question.question_variety == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
            elif not (hasattr(field, 'datatype') and field.datatype in ['multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes']):
                if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                elif hasattr(field, 'inputtype') and field.inputtype == 'ajax':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one.'))
                elif hasattr(field, 'datatype') and (field.datatype == 'object_radio' or (hasattr(field, 'inputtype') and field.inputtype in ('yesnoradio', 'noyesradio', 'radio', 'dropdown'))):
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('required', self, word('This field is required.'))
            if hasattr(field, 'inputtype') and field.inputtype in ['yesno', 'noyes', 'yesnowide', 'noyeswide'] and hasattr(field, 'uncheckothers') and (field.uncheckothers is not False):
                the_field['validation_messages']['uncheckothers'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([strip_tags(self.labels[field.number])]))
            if hasattr(field, 'datatype') and field.datatype not in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                for key in ('minlength', 'maxlength'):
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
                        if key == 'minlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'maxlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        if hasattr(field, 'datatype'):
            if field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes') and (hasattr(field, 'nota') and self.extras['nota'][field.number] is not False or (hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)))):
                if field.datatype.endswith('checkboxes'):
                    d_type = 'checkbox'
                else:
                    d_type = 'multiselect'
                if hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)):
                    checkbox_messages = dict()
                    if 'minlength' in field.extras and 'minlength' in self.extras and ('maxlength' in field.extras) and ('maxlength' in self.extras) and (self.extras['minlength'][field.number] == self.extras['maxlength'][field.number]) and (self.extras['minlength'][field.number] > 0):
                        if 'nota' not in self.extras:
                            self.extras['nota'] = dict()
                        self.extras['nota'][field.number] = False
                        if d_type == 'checkbox':
                            checkbox_messages['checkexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                        else:
                            checkbox_messages['selectexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    else:
                        if 'minlength' in field.extras and 'minlength' in self.extras:
                            if d_type == 'checkbox':
                                if self.extras['minlength'][field.number] == 1:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select one.'))
                                else:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                                if int(float(self.extras['minlength'][field.number])) > 0:
                                    if 'nota' not in self.extras:
                                        self.extras['nota'] = dict()
                                    self.extras['nota'][field.number] = False
                            elif self.extras['minlength'][field.number] == 1:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select one.'))
                            else:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                        if 'maxlength' in field.extras and 'maxlength' in self.extras:
                            if d_type == 'checkbox':
                                checkbox_messages['checkatmost'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                            else:
                                checkbox_messages['maxlength'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    the_field['validation_messages'].update(checkbox_messages)
                if d_type == 'checkbox':
                    if hasattr(field, 'nota') and self.extras['nota'][field.number] is not False:
                        the_field['validation_messages']['checkatleast'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([self.extras['nota'][field.number]]))
            if field.datatype == 'date':
                the_field['validation_messages']['date'] = field.validation_message('date', self, word('You need to enter a valid date.'))
                if hasattr(field, 'extras') and 'min' in field.extras and ('min' in self.extras) and ('max' in field.extras) and ('max' in self.extras) and (field.number in self.extras['min']) and (field.number in self.extras['max']):
                    the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.extras['min'][field.number], format='medium'), docassemble.base.util.format_date(self.extras['max'][field.number], format='medium')))
                else:
                    was_defined = dict()
                    for key in ['min', 'max']:
                        if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                            was_defined[key] = True
                            if key == 'min':
                                the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                            elif key == 'max':
                                the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                    if len(was_defined) == 0 and 'default date min' in self.question.interview.options and ('default date max' in self.question.interview.options):
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium'), docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')))
                    elif 'max' not in was_defined and 'default date max' in self.question.interview.options:
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')]))
                    elif 'min' not in was_defined and 'default date min' in self.question.interview.options:
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium')]))
            if field.datatype == 'time':
                the_field['validation_messages']['time'] = field.validation_message('time', self, word('You need to enter a valid time.'))
            if field.datatype in ['datetime', 'datetime-local']:
                the_field['validation_messages']['datetime'] = field.validation_message('datetime', self, word('You need to enter a valid date and time.'))
            if field.datatype == 'email':
                the_field['validation_messages']['email'] = field.validation_message('email', self, word('You need to enter a complete e-mail address.'))
            if field.datatype in ['number', 'currency', 'float', 'integer']:
                the_field['validation_messages']['number'] = field.validation_message('number', self, word('You need to enter a number.'))
                if field.datatype == 'integer' and (not ('step' in self.extras and field.number in self.extras['step'])):
                    the_field['validation_messages']['step'] = field.validation_message('integer', self, word('Please enter a whole number.'))
                elif 'step' in self.extras and field.number in self.extras['step']:
                    the_field['validation_messages']['step'] = field.validation_message('step', self, word('Please enter a multiple of {0}.'))
                for key in ['min', 'max']:
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                        if key == 'min':
                            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'max':
                            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))
            if field.datatype in ['files', 'file', 'camera', 'user', 'environment', 'camcorder', 'microphone']:
                file_fields.append(field)
                the_field['validation_messages']['required'] = field.validation_message('file required', self, word('You must provide a file.'))
                if 'accept' in self.extras and field.number in self.extras['accept']:
                    the_field['validation_messages']['accept'] = field.validation_message('accept', self, word('Please upload a file with a valid file format.'))
                if get_config('maximum content length') is not None:
                    the_field['max'] = get_config('maximum content length')
                    the_field['validation_messages']['max'] = field.validation_message('maxuploadsize', self, word('Your file upload is larger than the server can accept. Please reduce the size of your file upload.'))
        for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
            if hasattr(field, param):
                the_field[param] = getattr(field, param)
        if hasattr(field, 'shuffle') and field.shuffle is not False:
            the_field['shuffle'] = True
        if hasattr(field, 'disableothers') and field.disableothers and hasattr(field, 'saveas'):
            the_field['disable_others'] = True
        if hasattr(field, 'uncheckothers') and field.uncheckothers is not False:
            the_field['uncheck_others'] = True
        for key in ('minlength', 'maxlength', 'min', 'max', 'step', 'scale', 'inline', 'inline width', 'rows', 'accept', 'currency symbol', 'field metadata', 'css class'):
            if key in self.extras and field.number in self.extras[key]:
                if key in ('minlength', 'maxlength', 'min', 'max', 'step'):
                    validation_rules_used.add(key)
                the_field[key] = self.extras[key][field.number]
        if hasattr(field, 'extras') and 'custom_parameters' in field.extras:
            for (parameter, parameter_value) in field.extras['custom_parameters'].items():
                the_field[parameter] = parameter_value
        for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
            if param_type in self.extras and field.number in self.extras[param_type]:
                for (parameter, parameter_value) in self.extras[param_type][field.number].items():
                    the_field[parameter] = parameter_value
        if hasattr(field, 'saveas') and field.saveas in self.embedded:
            the_field['embedded'] = True
        if hasattr(self, 'shuffle'):
            the_field['shuffle'] = self.shuffle
        if field.number in self.defaults:
            the_default = self.defaults[field.number]
            if isinstance(the_default, (str, int, bool, float)):
                the_field['default'] = the_default
        else:
            the_default = None
        if self.question.question_type == 'multiple_choice' or hasattr(field, 'choicetype') or (hasattr(field, 'datatype') and field.datatype in ('object', 'multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes', 'object_radio')):
            the_field['choices'] = self.get_choices_data(field, the_default, the_user_dict, encode=encode)
        if hasattr(field, 'nota'):
            the_field['none_of_the_above'] = docassemble.base.filter.markdown_to_html(self.extras['nota'][field.number], do_terms=False, status=self, verbatim=not encode)
        if field.number in self.extras['ok']:
            the_field['active'] = self.extras['ok'][field.number]
        else:
            the_field['active'] = True
        if field.number in self.extras['required']:
            the_field['required'] = self.extras['required'][field.number]
            if the_field['required']:
                validation_rules_used.add('required')
        if 'validation messages' in self.extras and field.number in self.extras['validation messages']:
            the_field['validation_messages'].update(self.extras['validation messages'][field.number])
        if 'permissions' in self.extras and field.number in self.extras['permissions']:
            the_field['permissions'] = self.extras['permissions'][field.number]
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('max_image_size' in self.extras) and self.extras['max_image_size']:
            the_field['max_image_size'] = self.extras['max_image_size']
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('image_type' in self.extras) and self.extras['image_type']:
            the_field['image_type'] = self.extras['image_type']
        if hasattr(field, 'extras'):
            if 'ml_group' in field.extras or 'ml_train' in field.extras:","for (parameter, parameter_value) in field.extras['custom_parameters'].items():
    the_field[parameter] = parameter_value",XXX,no_found,0,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,InterviewStatus,as_data$768,"def as_data(self, the_user_dict, encode=True):
    result = dict(language=self.question.language)
    debug = self.question.interview.debug
    if debug:
        output = dict(question='', help='')
    if 'progress' in the_user_dict['_internal']:
        result['progress'] = the_user_dict['_internal']['progress']
    if self.question.language in self.question.interview.default_validation_messages:
        result['validation_messages'] = copy.copy(self.question.interview.default_validation_messages[self.question.language])
    else:
        result['validation_messages'] = dict()
    if 'reload_after' in self.extras:
        result['reload'] = 1000 * int(self.extras['reload_after'])
    lang = docassemble.base.functions.get_language()
    if len(self.question.terms) or len(self.question.interview.terms):
        result['terms'] = dict()
        if 'terms' in self.extras:
            for (term, vals) in self.extras['terms'].items():
                result['terms'][term] = vals['definition']
        if lang in self.question.interview.terms and len(self.question.interview.terms[lang]):
            for (term, vals) in self.question.interview.terms[lang].items():
                result['terms'][term] = vals['definition']
        elif self.question.language in self.question.interview.terms and len(self.question.interview.terms[self.question.language]):
            for (term, vals) in self.question.interview.terms[self.question.language].items():
                result['terms'][term] = vals['definition']
    if len(self.question.autoterms) or len(self.question.interview.autoterms):
        result['autoterms'] = dict()
        if 'autoterms' in self.extras:
            for (term, vals) in self.extras['autoterms'].items():
                result['autoterms'][term] = vals['definition']
        if lang in self.question.interview.autoterms and len(self.question.interview.autoterms[lang]):
            for (term, vals) in self.question.interview.autoterms[lang].items():
                result['autoterms'][term] = vals['definition']
        elif self.question.language in self.question.interview.autoterms and len(self.question.interview.autoterms[self.question.language]):
            for (term, vals) in self.question.interview.autoterms[self.question.language].items():
                result['autoterms'][term] = vals['definition']
    if self.orig_sought is not None:
        result['event_list'] = [self.orig_sought]
    if 'action_buttons' in self.extras:
        result['additional_buttons'] = []
        for item in self.extras['action_buttons']:
            new_item = copy.deepcopy(item)
            new_item['label'] = docassemble.base.filter.markdown_to_html(item['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + new_item['label'] + '</p>'
    for param in ('questionText',):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if hasattr(self, 'breadcrumb') and self.breadcrumb is not None:
        output['breadcrumb label'] = self.breadcrumb
    output['breadcrumbs'] = docassemble.base.functions.get_action_stack()
    if hasattr(self, 'subquestionText') and self.subquestionText is not None:
        if self.question.question_type == 'fields':
            embedder = dummy_embed_input
        else:
            embedder = None
        result['subquestionText'] = docassemble.base.filter.markdown_to_html(self.subquestionText.rstrip(), status=self, verbatim=not encode, embedder=embedder)
        if debug:
            output['question'] += result['subquestionText']
    for param in ('continueLabel', 'helpLabel'):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + result[param] + '</p>'
    if 'menu_items' in self.extras and isinstance(self.extras['menu_items'], list):
        result['menu_items'] = self.extras['menu_items']
    for param in ('cssClass', 'tableCssClass', 'css', 'script'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = self.extras[param].rstrip()
    for param in ('back_button_label',):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
    for param in ('rightText', 'underText'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if 'continueLabel' not in result:
        if self.question.question_type == 'review':
            result['continueLabel'] = word('Resume')
        else:
            result['continueLabel'] = word('Continue')
        if debug:
            output['question'] += '<p>' + result['continueLabel'] + '</p>'
    if self.question.question_type == 'yesno':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
    elif self.question.question_type == 'noyes':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
    elif self.question.question_type == 'yesnomaybe':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    elif self.question.question_type == 'noyesmaybe':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    steps = the_user_dict['_internal']['steps'] - the_user_dict['_internal']['steps_offset']
    if self.can_go_back and steps > 1:
        result['allow_going_back'] = True
        result['backTitle'] = word('Go back to the previous question')
        back_button_val = self.extras.get('back_button', None)
        if back_button_val or (back_button_val is None and self.question.interview.question_back_button):
            result['questionBackButton'] = self.back
    else:
        result['allow_going_back'] = False
    if self.question.question_type == 'signature':
        result['signaturePhrases'] = {'clear': word('Clear'), 'noSignature': word('You must sign your name to continue.'), 'loading': word('Loading.  Please wait . . . ')}
    if 'questionMetadata' in self.extras:
        result['question_metadata'] = self.extras['questionMetadata']
    if 'segment' in self.extras:
        result['segment'] = self.extras['segment']
    if 'ga_id' in self.extras:
        result['ga_id'] = self.extras['ga_id']
    if hasattr(self.question, 'id'):
        result['id'] = self.question.id
    if hasattr(self, 'audiovideo') and self.audiovideo is not None:
        audio_result = docassemble.base.filter.get_audio_urls(self.audiovideo)
        video_result = docassemble.base.filter.get_video_urls(self.audiovideo)
        if len(audio_result) > 0:
            result['audio'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in audio_result]
        if len(video_result) > 0:
            result['video'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in video_result]
    if hasattr(self, 'helpText') and len(self.helpText) > 0:
        result['helpText'] = list()
        result['helpBackLabel'] = word('Back to question')
        for help_text in self.helpText:
            result['helpText'].append(self.convert_help(help_text, encode, debug))
        result['help'] = dict()
        if self.helpText[0]['label']:
            result['help']['label'] = docassemble.base.filter.markdown_to_html(self.helpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['label'] = self.question.help()
        result['help']['title'] = word('Help is available for this question')
        result['help']['specific'] = False if self.question.helptext is None else True
    if hasattr(self, 'interviewHelpText') and len(self.interviewHelpText) > 0:
        result['interviewHelpText'] = list()
        for help_text in self.interviewHelpText:
            result['interviewHelpText'].append(self.convert_help(help_text, encode, debug))
        if 'help' not in result:
            result['help'] = dict()
        if self.interviewHelpText[0]['label']:
            result['help']['interviewLabel'] = docassemble.base.filter.markdown_to_html(self.interviewHelpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['interviewLabel'] = self.question.help()
        result['help']['interviewTitle'] = word('Help is available')
        if not (hasattr(self, 'helpText') and len(self.helpText) > 0):
            result['help']['specific'] = False
    if 'questionText' not in result and self.question.question_type == 'signature':
        result['questionText'] = word('Sign Your Name')
        if debug:
            output['question'] += '<p>' + result['questionText'] + '</p>'
    result['questionType'] = self.question.question_type
    if hasattr(self.question, 'question_variety'):
        result['questionVariety'] = self.question.question_variety
    if self.question.is_mandatory or self.question.mandatory_code is not None:
        result['mandatory'] = True
    if hasattr(self.question, 'name'):
        result['_question_name'] = self.question.name
    result['_tracker'] = self.tracker
    if hasattr(self, 'datatypes'):
        result['_datatypes'] = safeid(json.dumps(self.datatypes))
    if hasattr(self, 'varnames'):
        result['_varnames'] = safeid(json.dumps(self.varnames))
    if len(self.question.fields) > 0:
        result['fields'] = list()
    if hasattr(self.question, 'review_saveas'):
        result['question_variable_name'] = self.question.review_saveas
    if hasattr(self.question, 'fields_saveas'):
        result['question_variable_name'] = self.question.fields_saveas
    if self.decorations is not None:
        width_value = get_config('decoration size', 2.0)
        width_units = get_config('decoration units', 'em')
        for decoration in self.decorations:
            if 'image' in decoration:
                result['decoration'] = {}
                the_image = self.question.interview.images.get(decoration['image'], None)
                if the_image is not None:
                    the_url = docassemble.base.functions.server.url_finder(str(the_image.package) + ':' + str(the_image.filename))
                    width = str(width_value) + str(width_units)
                    filename = docassemble.base.functions.server.file_finder(str(the_image.package) + ':' + str(the_image.filename))
                    if 'extension' in filename and filename['extension'] == 'svg' and ('width' in filename):
                        if filename['width'] and filename['height']:
                            height = str(width_value * (filename['height'] / filename['width'])) + str(width_units)
                    else:
                        height = 'auto'
                    if the_url is not None:
                        result['decoration']['url'] = the_url
                        result['decoration']['size'] = {'width': width, 'height': height}
                        if the_image.attribution is not None:
                            self.attributions.add(the_image.attribution)
                        break
                elif get_config('default icons', None) in ('material icons', 'font awesome'):
                    result['decoration']['name'] = decoration['image']
                    result['decoration']['size'] = str(width_value) + str(width_units)
                    break
    if len(self.attachments) > 0:
        result['attachments'] = list()
        if self.current_info['user']['is_authenticated'] and self.current_info['user']['email']:
            result['default_email'] = self.current_info['user']['email']
        for attachment in self.attachments:
            the_attachment = dict(url=dict(), number=dict(), filename_with_extension=dict())
            if 'orig_variable_name' in attachment and attachment['orig_variable_name']:
                the_attachment['variable_name'] = attachment['orig_variable_name']
            if 'name' in attachment:
                if attachment['name']:
                    the_attachment['name'] = docassemble.base.filter.markdown_to_html(attachment['name'], trim=True, status=self, verbatim=not encode)
                    if debug:
                        output['question'] += '<p>' + the_attachment['name'] + '</p>'
            if 'description' in attachment:
                if attachment['description']:
                    the_attachment['description'] = docassemble.base.filter.markdown_to_html(attachment['description'], status=self, verbatim=not encode)
                    if debug:
                        output['question'] += the_attachment['description']
            for key in ('valid_formats', 'filename', 'content', 'markdown', 'raw'):
                if key in attachment:
                    if attachment[key]:
                        the_attachment[key] = attachment[key]
            for the_format in attachment['file']:
                the_attachment['url'][the_format] = docassemble.base.functions.server.url_finder(attachment['file'][the_format], filename=attachment['filename'] + '.' + extension_of_doc_format[the_format])
                the_attachment['number'][the_format] = attachment['file'][the_format]
                the_attachment['filename_with_extension'][the_format] = attachment['filename'] + '.' + extension_of_doc_format[the_format]
            result['attachments'].append(the_attachment)
    if self.extras.get('list_collect', False) is not False:
        result['listCollect'] = {'deleteLabel': word('Delete'), 'addAnotherLabel': self.extras['list_collect_add_another_label'] if self.extras['list_collect_add_another_label'] else word('Add another'), 'deletedLabel': word('(Deleted)'), 'undeleteLabel': word('Undelete')}
    validation_rules_used = set()
    file_fields = list()
    for field in self.question.fields:
        the_field = dict()
        the_field['number'] = field.number
        if hasattr(field, 'saveas'):
            the_field['variable_name'] = from_safeid(field.saveas)
            if encode:
                the_field['variable_name_encoded'] = field.saveas
            the_field['validation_messages'] = dict()
            if self.question.question_type == 'multiple_choice' and self.question.question_variety in ['radio', 'dropdown', 'combobox']:
                if self.question.question_variety == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
            elif not (hasattr(field, 'datatype') and field.datatype in ['multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes']):
                if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                elif hasattr(field, 'inputtype') and field.inputtype == 'ajax':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one.'))
                elif hasattr(field, 'datatype') and (field.datatype == 'object_radio' or (hasattr(field, 'inputtype') and field.inputtype in ('yesnoradio', 'noyesradio', 'radio', 'dropdown'))):
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('required', self, word('This field is required.'))
            if hasattr(field, 'inputtype') and field.inputtype in ['yesno', 'noyes', 'yesnowide', 'noyeswide'] and hasattr(field, 'uncheckothers') and (field.uncheckothers is not False):
                the_field['validation_messages']['uncheckothers'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([strip_tags(self.labels[field.number])]))
            if hasattr(field, 'datatype') and field.datatype not in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                for key in ('minlength', 'maxlength'):
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
                        if key == 'minlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'maxlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        if hasattr(field, 'datatype'):
            if field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes') and (hasattr(field, 'nota') and self.extras['nota'][field.number] is not False or (hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)))):
                if field.datatype.endswith('checkboxes'):
                    d_type = 'checkbox'
                else:
                    d_type = 'multiselect'
                if hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)):
                    checkbox_messages = dict()
                    if 'minlength' in field.extras and 'minlength' in self.extras and ('maxlength' in field.extras) and ('maxlength' in self.extras) and (self.extras['minlength'][field.number] == self.extras['maxlength'][field.number]) and (self.extras['minlength'][field.number] > 0):
                        if 'nota' not in self.extras:
                            self.extras['nota'] = dict()
                        self.extras['nota'][field.number] = False
                        if d_type == 'checkbox':
                            checkbox_messages['checkexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                        else:
                            checkbox_messages['selectexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    else:
                        if 'minlength' in field.extras and 'minlength' in self.extras:
                            if d_type == 'checkbox':
                                if self.extras['minlength'][field.number] == 1:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select one.'))
                                else:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                                if int(float(self.extras['minlength'][field.number])) > 0:
                                    if 'nota' not in self.extras:
                                        self.extras['nota'] = dict()
                                    self.extras['nota'][field.number] = False
                            elif self.extras['minlength'][field.number] == 1:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select one.'))
                            else:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                        if 'maxlength' in field.extras and 'maxlength' in self.extras:
                            if d_type == 'checkbox':
                                checkbox_messages['checkatmost'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                            else:
                                checkbox_messages['maxlength'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    the_field['validation_messages'].update(checkbox_messages)
                if d_type == 'checkbox':
                    if hasattr(field, 'nota') and self.extras['nota'][field.number] is not False:
                        the_field['validation_messages']['checkatleast'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([self.extras['nota'][field.number]]))
            if field.datatype == 'date':
                the_field['validation_messages']['date'] = field.validation_message('date', self, word('You need to enter a valid date.'))
                if hasattr(field, 'extras') and 'min' in field.extras and ('min' in self.extras) and ('max' in field.extras) and ('max' in self.extras) and (field.number in self.extras['min']) and (field.number in self.extras['max']):
                    the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.extras['min'][field.number], format='medium'), docassemble.base.util.format_date(self.extras['max'][field.number], format='medium')))
                else:
                    was_defined = dict()
                    for key in ['min', 'max']:
                        if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                            was_defined[key] = True
                            if key == 'min':
                                the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                            elif key == 'max':
                                the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                    if len(was_defined) == 0 and 'default date min' in self.question.interview.options and ('default date max' in self.question.interview.options):
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium'), docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')))
                    elif 'max' not in was_defined and 'default date max' in self.question.interview.options:
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')]))
                    elif 'min' not in was_defined and 'default date min' in self.question.interview.options:
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium')]))
            if field.datatype == 'time':
                the_field['validation_messages']['time'] = field.validation_message('time', self, word('You need to enter a valid time.'))
            if field.datatype in ['datetime', 'datetime-local']:
                the_field['validation_messages']['datetime'] = field.validation_message('datetime', self, word('You need to enter a valid date and time.'))
            if field.datatype == 'email':
                the_field['validation_messages']['email'] = field.validation_message('email', self, word('You need to enter a complete e-mail address.'))
            if field.datatype in ['number', 'currency', 'float', 'integer']:
                the_field['validation_messages']['number'] = field.validation_message('number', self, word('You need to enter a number.'))
                if field.datatype == 'integer' and (not ('step' in self.extras and field.number in self.extras['step'])):
                    the_field['validation_messages']['step'] = field.validation_message('integer', self, word('Please enter a whole number.'))
                elif 'step' in self.extras and field.number in self.extras['step']:
                    the_field['validation_messages']['step'] = field.validation_message('step', self, word('Please enter a multiple of {0}.'))
                for key in ['min', 'max']:
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                        if key == 'min':
                            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'max':
                            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))
            if field.datatype in ['files', 'file', 'camera', 'user', 'environment', 'camcorder', 'microphone']:
                file_fields.append(field)
                the_field['validation_messages']['required'] = field.validation_message('file required', self, word('You must provide a file.'))
                if 'accept' in self.extras and field.number in self.extras['accept']:
                    the_field['validation_messages']['accept'] = field.validation_message('accept', self, word('Please upload a file with a valid file format.'))
                if get_config('maximum content length') is not None:
                    the_field['max'] = get_config('maximum content length')
                    the_field['validation_messages']['max'] = field.validation_message('maxuploadsize', self, word('Your file upload is larger than the server can accept. Please reduce the size of your file upload.'))
        for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
            if hasattr(field, param):
                the_field[param] = getattr(field, param)
        if hasattr(field, 'shuffle') and field.shuffle is not False:
            the_field['shuffle'] = True
        if hasattr(field, 'disableothers') and field.disableothers and hasattr(field, 'saveas'):
            the_field['disable_others'] = True
        if hasattr(field, 'uncheckothers') and field.uncheckothers is not False:
            the_field['uncheck_others'] = True
        for key in ('minlength', 'maxlength', 'min', 'max', 'step', 'scale', 'inline', 'inline width', 'rows', 'accept', 'currency symbol', 'field metadata', 'css class'):
            if key in self.extras and field.number in self.extras[key]:
                if key in ('minlength', 'maxlength', 'min', 'max', 'step'):
                    validation_rules_used.add(key)
                the_field[key] = self.extras[key][field.number]
        if hasattr(field, 'extras') and 'custom_parameters' in field.extras:
            for (parameter, parameter_value) in field.extras['custom_parameters'].items():
                the_field[parameter] = parameter_value
        for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
            if param_type in self.extras and field.number in self.extras[param_type]:
                for (parameter, parameter_value) in self.extras[param_type][field.number].items():
                    the_field[parameter] = parameter_value
        if hasattr(field, 'saveas') and field.saveas in self.embedded:
            the_field['embedded'] = True
        if hasattr(self, 'shuffle'):
            the_field['shuffle'] = self.shuffle
        if field.number in self.defaults:
            the_default = self.defaults[field.number]
            if isinstance(the_default, (str, int, bool, float)):
                the_field['default'] = the_default
        else:
            the_default = None
        if self.question.question_type == 'multiple_choice' or hasattr(field, 'choicetype') or (hasattr(field, 'datatype') and field.datatype in ('object', 'multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes', 'object_radio')):
            the_field['choices'] = self.get_choices_data(field, the_default, the_user_dict, encode=encode)
        if hasattr(field, 'nota'):
            the_field['none_of_the_above'] = docassemble.base.filter.markdown_to_html(self.extras['nota'][field.number], do_terms=False, status=self, verbatim=not encode)
        if field.number in self.extras['ok']:
            the_field['active'] = self.extras['ok'][field.number]
        else:
            the_field['active'] = True
        if field.number in self.extras['required']:
            the_field['required'] = self.extras['required'][field.number]
            if the_field['required']:
                validation_rules_used.add('required')
        if 'validation messages' in self.extras and field.number in self.extras['validation messages']:
            the_field['validation_messages'].update(self.extras['validation messages'][field.number])
        if 'permissions' in self.extras and field.number in self.extras['permissions']:
            the_field['permissions'] = self.extras['permissions'][field.number]
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('max_image_size' in self.extras) and self.extras['max_image_size']:
            the_field['max_image_size'] = self.extras['max_image_size']
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('image_type' in self.extras) and self.extras['image_type']:
            the_field['image_type'] = self.extras['image_type']
        if hasattr(field, 'extras'):
            if 'ml_group' in field.extras or 'ml_train' in field.extras:","for (term, vals) in self.question.interview.terms[self.question.language].items():
    result['terms'][term] = vals['definition']",XXX,no_found,0,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,InterviewStatus,as_data$768,"def as_data(self, the_user_dict, encode=True):
    result = dict(language=self.question.language)
    debug = self.question.interview.debug
    if debug:
        output = dict(question='', help='')
    if 'progress' in the_user_dict['_internal']:
        result['progress'] = the_user_dict['_internal']['progress']
    if self.question.language in self.question.interview.default_validation_messages:
        result['validation_messages'] = copy.copy(self.question.interview.default_validation_messages[self.question.language])
    else:
        result['validation_messages'] = dict()
    if 'reload_after' in self.extras:
        result['reload'] = 1000 * int(self.extras['reload_after'])
    lang = docassemble.base.functions.get_language()
    if len(self.question.terms) or len(self.question.interview.terms):
        result['terms'] = dict()
        if 'terms' in self.extras:
            for (term, vals) in self.extras['terms'].items():
                result['terms'][term] = vals['definition']
        if lang in self.question.interview.terms and len(self.question.interview.terms[lang]):
            for (term, vals) in self.question.interview.terms[lang].items():
                result['terms'][term] = vals['definition']
        elif self.question.language in self.question.interview.terms and len(self.question.interview.terms[self.question.language]):
            for (term, vals) in self.question.interview.terms[self.question.language].items():
                result['terms'][term] = vals['definition']
    if len(self.question.autoterms) or len(self.question.interview.autoterms):
        result['autoterms'] = dict()
        if 'autoterms' in self.extras:
            for (term, vals) in self.extras['autoterms'].items():
                result['autoterms'][term] = vals['definition']
        if lang in self.question.interview.autoterms and len(self.question.interview.autoterms[lang]):
            for (term, vals) in self.question.interview.autoterms[lang].items():
                result['autoterms'][term] = vals['definition']
        elif self.question.language in self.question.interview.autoterms and len(self.question.interview.autoterms[self.question.language]):
            for (term, vals) in self.question.interview.autoterms[self.question.language].items():
                result['autoterms'][term] = vals['definition']
    if self.orig_sought is not None:
        result['event_list'] = [self.orig_sought]
    if 'action_buttons' in self.extras:
        result['additional_buttons'] = []
        for item in self.extras['action_buttons']:
            new_item = copy.deepcopy(item)
            new_item['label'] = docassemble.base.filter.markdown_to_html(item['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + new_item['label'] + '</p>'
    for param in ('questionText',):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if hasattr(self, 'breadcrumb') and self.breadcrumb is not None:
        output['breadcrumb label'] = self.breadcrumb
    output['breadcrumbs'] = docassemble.base.functions.get_action_stack()
    if hasattr(self, 'subquestionText') and self.subquestionText is not None:
        if self.question.question_type == 'fields':
            embedder = dummy_embed_input
        else:
            embedder = None
        result['subquestionText'] = docassemble.base.filter.markdown_to_html(self.subquestionText.rstrip(), status=self, verbatim=not encode, embedder=embedder)
        if debug:
            output['question'] += result['subquestionText']
    for param in ('continueLabel', 'helpLabel'):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + result[param] + '</p>'
    if 'menu_items' in self.extras and isinstance(self.extras['menu_items'], list):
        result['menu_items'] = self.extras['menu_items']
    for param in ('cssClass', 'tableCssClass', 'css', 'script'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = self.extras[param].rstrip()
    for param in ('back_button_label',):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
    for param in ('rightText', 'underText'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if 'continueLabel' not in result:
        if self.question.question_type == 'review':
            result['continueLabel'] = word('Resume')
        else:
            result['continueLabel'] = word('Continue')
        if debug:
            output['question'] += '<p>' + result['continueLabel'] + '</p>'
    if self.question.question_type == 'yesno':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
    elif self.question.question_type == 'noyes':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
    elif self.question.question_type == 'yesnomaybe':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    elif self.question.question_type == 'noyesmaybe':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    steps = the_user_dict['_internal']['steps'] - the_user_dict['_internal']['steps_offset']
    if self.can_go_back and steps > 1:
        result['allow_going_back'] = True
        result['backTitle'] = word('Go back to the previous question')
        back_button_val = self.extras.get('back_button', None)
        if back_button_val or (back_button_val is None and self.question.interview.question_back_button):
            result['questionBackButton'] = self.back
    else:
        result['allow_going_back'] = False
    if self.question.question_type == 'signature':
        result['signaturePhrases'] = {'clear': word('Clear'), 'noSignature': word('You must sign your name to continue.'), 'loading': word('Loading.  Please wait . . . ')}
    if 'questionMetadata' in self.extras:
        result['question_metadata'] = self.extras['questionMetadata']
    if 'segment' in self.extras:
        result['segment'] = self.extras['segment']
    if 'ga_id' in self.extras:
        result['ga_id'] = self.extras['ga_id']
    if hasattr(self.question, 'id'):
        result['id'] = self.question.id
    if hasattr(self, 'audiovideo') and self.audiovideo is not None:
        audio_result = docassemble.base.filter.get_audio_urls(self.audiovideo)
        video_result = docassemble.base.filter.get_video_urls(self.audiovideo)
        if len(audio_result) > 0:
            result['audio'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in audio_result]
        if len(video_result) > 0:
            result['video'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in video_result]
    if hasattr(self, 'helpText') and len(self.helpText) > 0:
        result['helpText'] = list()
        result['helpBackLabel'] = word('Back to question')
        for help_text in self.helpText:
            result['helpText'].append(self.convert_help(help_text, encode, debug))
        result['help'] = dict()
        if self.helpText[0]['label']:
            result['help']['label'] = docassemble.base.filter.markdown_to_html(self.helpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['label'] = self.question.help()
        result['help']['title'] = word('Help is available for this question')
        result['help']['specific'] = False if self.question.helptext is None else True
    if hasattr(self, 'interviewHelpText') and len(self.interviewHelpText) > 0:
        result['interviewHelpText'] = list()
        for help_text in self.interviewHelpText:
            result['interviewHelpText'].append(self.convert_help(help_text, encode, debug))
        if 'help' not in result:
            result['help'] = dict()
        if self.interviewHelpText[0]['label']:
            result['help']['interviewLabel'] = docassemble.base.filter.markdown_to_html(self.interviewHelpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['interviewLabel'] = self.question.help()
        result['help']['interviewTitle'] = word('Help is available')
        if not (hasattr(self, 'helpText') and len(self.helpText) > 0):
            result['help']['specific'] = False
    if 'questionText' not in result and self.question.question_type == 'signature':
        result['questionText'] = word('Sign Your Name')
        if debug:
            output['question'] += '<p>' + result['questionText'] + '</p>'
    result['questionType'] = self.question.question_type
    if hasattr(self.question, 'question_variety'):
        result['questionVariety'] = self.question.question_variety
    if self.question.is_mandatory or self.question.mandatory_code is not None:
        result['mandatory'] = True
    if hasattr(self.question, 'name'):
        result['_question_name'] = self.question.name
    result['_tracker'] = self.tracker
    if hasattr(self, 'datatypes'):
        result['_datatypes'] = safeid(json.dumps(self.datatypes))
    if hasattr(self, 'varnames'):
        result['_varnames'] = safeid(json.dumps(self.varnames))
    if len(self.question.fields) > 0:
        result['fields'] = list()
    if hasattr(self.question, 'review_saveas'):
        result['question_variable_name'] = self.question.review_saveas
    if hasattr(self.question, 'fields_saveas'):
        result['question_variable_name'] = self.question.fields_saveas
    if self.decorations is not None:
        width_value = get_config('decoration size', 2.0)
        width_units = get_config('decoration units', 'em')
        for decoration in self.decorations:
            if 'image' in decoration:
                result['decoration'] = {}
                the_image = self.question.interview.images.get(decoration['image'], None)
                if the_image is not None:
                    the_url = docassemble.base.functions.server.url_finder(str(the_image.package) + ':' + str(the_image.filename))
                    width = str(width_value) + str(width_units)
                    filename = docassemble.base.functions.server.file_finder(str(the_image.package) + ':' + str(the_image.filename))
                    if 'extension' in filename and filename['extension'] == 'svg' and ('width' in filename):
                        if filename['width'] and filename['height']:
                            height = str(width_value * (filename['height'] / filename['width'])) + str(width_units)
                    else:
                        height = 'auto'
                    if the_url is not None:
                        result['decoration']['url'] = the_url
                        result['decoration']['size'] = {'width': width, 'height': height}
                        if the_image.attribution is not None:
                            self.attributions.add(the_image.attribution)
                        break
                elif get_config('default icons', None) in ('material icons', 'font awesome'):
                    result['decoration']['name'] = decoration['image']
                    result['decoration']['size'] = str(width_value) + str(width_units)
                    break
    if len(self.attachments) > 0:
        result['attachments'] = list()
        if self.current_info['user']['is_authenticated'] and self.current_info['user']['email']:
            result['default_email'] = self.current_info['user']['email']
        for attachment in self.attachments:
            the_attachment = dict(url=dict(), number=dict(), filename_with_extension=dict())
            if 'orig_variable_name' in attachment and attachment['orig_variable_name']:
                the_attachment['variable_name'] = attachment['orig_variable_name']
            if 'name' in attachment:
                if attachment['name']:
                    the_attachment['name'] = docassemble.base.filter.markdown_to_html(attachment['name'], trim=True, status=self, verbatim=not encode)
                    if debug:
                        output['question'] += '<p>' + the_attachment['name'] + '</p>'
            if 'description' in attachment:
                if attachment['description']:
                    the_attachment['description'] = docassemble.base.filter.markdown_to_html(attachment['description'], status=self, verbatim=not encode)
                    if debug:
                        output['question'] += the_attachment['description']
            for key in ('valid_formats', 'filename', 'content', 'markdown', 'raw'):
                if key in attachment:
                    if attachment[key]:
                        the_attachment[key] = attachment[key]
            for the_format in attachment['file']:
                the_attachment['url'][the_format] = docassemble.base.functions.server.url_finder(attachment['file'][the_format], filename=attachment['filename'] + '.' + extension_of_doc_format[the_format])
                the_attachment['number'][the_format] = attachment['file'][the_format]
                the_attachment['filename_with_extension'][the_format] = attachment['filename'] + '.' + extension_of_doc_format[the_format]
            result['attachments'].append(the_attachment)
    if self.extras.get('list_collect', False) is not False:
        result['listCollect'] = {'deleteLabel': word('Delete'), 'addAnotherLabel': self.extras['list_collect_add_another_label'] if self.extras['list_collect_add_another_label'] else word('Add another'), 'deletedLabel': word('(Deleted)'), 'undeleteLabel': word('Undelete')}
    validation_rules_used = set()
    file_fields = list()
    for field in self.question.fields:
        the_field = dict()
        the_field['number'] = field.number
        if hasattr(field, 'saveas'):
            the_field['variable_name'] = from_safeid(field.saveas)
            if encode:
                the_field['variable_name_encoded'] = field.saveas
            the_field['validation_messages'] = dict()
            if self.question.question_type == 'multiple_choice' and self.question.question_variety in ['radio', 'dropdown', 'combobox']:
                if self.question.question_variety == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
            elif not (hasattr(field, 'datatype') and field.datatype in ['multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes']):
                if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                elif hasattr(field, 'inputtype') and field.inputtype == 'ajax':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one.'))
                elif hasattr(field, 'datatype') and (field.datatype == 'object_radio' or (hasattr(field, 'inputtype') and field.inputtype in ('yesnoradio', 'noyesradio', 'radio', 'dropdown'))):
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('required', self, word('This field is required.'))
            if hasattr(field, 'inputtype') and field.inputtype in ['yesno', 'noyes', 'yesnowide', 'noyeswide'] and hasattr(field, 'uncheckothers') and (field.uncheckothers is not False):
                the_field['validation_messages']['uncheckothers'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([strip_tags(self.labels[field.number])]))
            if hasattr(field, 'datatype') and field.datatype not in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                for key in ('minlength', 'maxlength'):
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
                        if key == 'minlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'maxlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        if hasattr(field, 'datatype'):
            if field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes') and (hasattr(field, 'nota') and self.extras['nota'][field.number] is not False or (hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)))):
                if field.datatype.endswith('checkboxes'):
                    d_type = 'checkbox'
                else:
                    d_type = 'multiselect'
                if hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)):
                    checkbox_messages = dict()
                    if 'minlength' in field.extras and 'minlength' in self.extras and ('maxlength' in field.extras) and ('maxlength' in self.extras) and (self.extras['minlength'][field.number] == self.extras['maxlength'][field.number]) and (self.extras['minlength'][field.number] > 0):
                        if 'nota' not in self.extras:
                            self.extras['nota'] = dict()
                        self.extras['nota'][field.number] = False
                        if d_type == 'checkbox':
                            checkbox_messages['checkexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                        else:
                            checkbox_messages['selectexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    else:
                        if 'minlength' in field.extras and 'minlength' in self.extras:
                            if d_type == 'checkbox':
                                if self.extras['minlength'][field.number] == 1:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select one.'))
                                else:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                                if int(float(self.extras['minlength'][field.number])) > 0:
                                    if 'nota' not in self.extras:
                                        self.extras['nota'] = dict()
                                    self.extras['nota'][field.number] = False
                            elif self.extras['minlength'][field.number] == 1:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select one.'))
                            else:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                        if 'maxlength' in field.extras and 'maxlength' in self.extras:
                            if d_type == 'checkbox':
                                checkbox_messages['checkatmost'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                            else:
                                checkbox_messages['maxlength'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    the_field['validation_messages'].update(checkbox_messages)
                if d_type == 'checkbox':
                    if hasattr(field, 'nota') and self.extras['nota'][field.number] is not False:
                        the_field['validation_messages']['checkatleast'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([self.extras['nota'][field.number]]))
            if field.datatype == 'date':
                the_field['validation_messages']['date'] = field.validation_message('date', self, word('You need to enter a valid date.'))
                if hasattr(field, 'extras') and 'min' in field.extras and ('min' in self.extras) and ('max' in field.extras) and ('max' in self.extras) and (field.number in self.extras['min']) and (field.number in self.extras['max']):
                    the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.extras['min'][field.number], format='medium'), docassemble.base.util.format_date(self.extras['max'][field.number], format='medium')))
                else:
                    was_defined = dict()
                    for key in ['min', 'max']:
                        if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                            was_defined[key] = True
                            if key == 'min':
                                the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                            elif key == 'max':
                                the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                    if len(was_defined) == 0 and 'default date min' in self.question.interview.options and ('default date max' in self.question.interview.options):
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium'), docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')))
                    elif 'max' not in was_defined and 'default date max' in self.question.interview.options:
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')]))
                    elif 'min' not in was_defined and 'default date min' in self.question.interview.options:
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium')]))
            if field.datatype == 'time':
                the_field['validation_messages']['time'] = field.validation_message('time', self, word('You need to enter a valid time.'))
            if field.datatype in ['datetime', 'datetime-local']:
                the_field['validation_messages']['datetime'] = field.validation_message('datetime', self, word('You need to enter a valid date and time.'))
            if field.datatype == 'email':
                the_field['validation_messages']['email'] = field.validation_message('email', self, word('You need to enter a complete e-mail address.'))
            if field.datatype in ['number', 'currency', 'float', 'integer']:
                the_field['validation_messages']['number'] = field.validation_message('number', self, word('You need to enter a number.'))
                if field.datatype == 'integer' and (not ('step' in self.extras and field.number in self.extras['step'])):
                    the_field['validation_messages']['step'] = field.validation_message('integer', self, word('Please enter a whole number.'))
                elif 'step' in self.extras and field.number in self.extras['step']:
                    the_field['validation_messages']['step'] = field.validation_message('step', self, word('Please enter a multiple of {0}.'))
                for key in ['min', 'max']:
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                        if key == 'min':
                            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'max':
                            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))
            if field.datatype in ['files', 'file', 'camera', 'user', 'environment', 'camcorder', 'microphone']:
                file_fields.append(field)
                the_field['validation_messages']['required'] = field.validation_message('file required', self, word('You must provide a file.'))
                if 'accept' in self.extras and field.number in self.extras['accept']:
                    the_field['validation_messages']['accept'] = field.validation_message('accept', self, word('Please upload a file with a valid file format.'))
                if get_config('maximum content length') is not None:
                    the_field['max'] = get_config('maximum content length')
                    the_field['validation_messages']['max'] = field.validation_message('maxuploadsize', self, word('Your file upload is larger than the server can accept. Please reduce the size of your file upload.'))
        for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
            if hasattr(field, param):
                the_field[param] = getattr(field, param)
        if hasattr(field, 'shuffle') and field.shuffle is not False:
            the_field['shuffle'] = True
        if hasattr(field, 'disableothers') and field.disableothers and hasattr(field, 'saveas'):
            the_field['disable_others'] = True
        if hasattr(field, 'uncheckothers') and field.uncheckothers is not False:
            the_field['uncheck_others'] = True
        for key in ('minlength', 'maxlength', 'min', 'max', 'step', 'scale', 'inline', 'inline width', 'rows', 'accept', 'currency symbol', 'field metadata', 'css class'):
            if key in self.extras and field.number in self.extras[key]:
                if key in ('minlength', 'maxlength', 'min', 'max', 'step'):
                    validation_rules_used.add(key)
                the_field[key] = self.extras[key][field.number]
        if hasattr(field, 'extras') and 'custom_parameters' in field.extras:
            for (parameter, parameter_value) in field.extras['custom_parameters'].items():
                the_field[parameter] = parameter_value
        for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
            if param_type in self.extras and field.number in self.extras[param_type]:
                for (parameter, parameter_value) in self.extras[param_type][field.number].items():
                    the_field[parameter] = parameter_value
        if hasattr(field, 'saveas') and field.saveas in self.embedded:
            the_field['embedded'] = True
        if hasattr(self, 'shuffle'):
            the_field['shuffle'] = self.shuffle
        if field.number in self.defaults:
            the_default = self.defaults[field.number]
            if isinstance(the_default, (str, int, bool, float)):
                the_field['default'] = the_default
        else:
            the_default = None
        if self.question.question_type == 'multiple_choice' or hasattr(field, 'choicetype') or (hasattr(field, 'datatype') and field.datatype in ('object', 'multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes', 'object_radio')):
            the_field['choices'] = self.get_choices_data(field, the_default, the_user_dict, encode=encode)
        if hasattr(field, 'nota'):
            the_field['none_of_the_above'] = docassemble.base.filter.markdown_to_html(self.extras['nota'][field.number], do_terms=False, status=self, verbatim=not encode)
        if field.number in self.extras['ok']:
            the_field['active'] = self.extras['ok'][field.number]
        else:
            the_field['active'] = True
        if field.number in self.extras['required']:
            the_field['required'] = self.extras['required'][field.number]
            if the_field['required']:
                validation_rules_used.add('required')
        if 'validation messages' in self.extras and field.number in self.extras['validation messages']:
            the_field['validation_messages'].update(self.extras['validation messages'][field.number])
        if 'permissions' in self.extras and field.number in self.extras['permissions']:
            the_field['permissions'] = self.extras['permissions'][field.number]
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('max_image_size' in self.extras) and self.extras['max_image_size']:
            the_field['max_image_size'] = self.extras['max_image_size']
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('image_type' in self.extras) and self.extras['image_type']:
            the_field['image_type'] = self.extras['image_type']
        if hasattr(field, 'extras'):
            if 'ml_group' in field.extras or 'ml_train' in field.extras:","for (term, vals) in self.question.interview.autoterms[self.question.language].items():
    result['autoterms'][term] = vals['definition']",XXX,no_found,0,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,InterviewStatus,as_data$768,"def as_data(self, the_user_dict, encode=True):
    result = dict(language=self.question.language)
    debug = self.question.interview.debug
    if debug:
        output = dict(question='', help='')
    if 'progress' in the_user_dict['_internal']:
        result['progress'] = the_user_dict['_internal']['progress']
    if self.question.language in self.question.interview.default_validation_messages:
        result['validation_messages'] = copy.copy(self.question.interview.default_validation_messages[self.question.language])
    else:
        result['validation_messages'] = dict()
    if 'reload_after' in self.extras:
        result['reload'] = 1000 * int(self.extras['reload_after'])
    lang = docassemble.base.functions.get_language()
    if len(self.question.terms) or len(self.question.interview.terms):
        result['terms'] = dict()
        if 'terms' in self.extras:
            for (term, vals) in self.extras['terms'].items():
                result['terms'][term] = vals['definition']
        if lang in self.question.interview.terms and len(self.question.interview.terms[lang]):
            for (term, vals) in self.question.interview.terms[lang].items():
                result['terms'][term] = vals['definition']
        elif self.question.language in self.question.interview.terms and len(self.question.interview.terms[self.question.language]):
            for (term, vals) in self.question.interview.terms[self.question.language].items():
                result['terms'][term] = vals['definition']
    if len(self.question.autoterms) or len(self.question.interview.autoterms):
        result['autoterms'] = dict()
        if 'autoterms' in self.extras:
            for (term, vals) in self.extras['autoterms'].items():
                result['autoterms'][term] = vals['definition']
        if lang in self.question.interview.autoterms and len(self.question.interview.autoterms[lang]):
            for (term, vals) in self.question.interview.autoterms[lang].items():
                result['autoterms'][term] = vals['definition']
        elif self.question.language in self.question.interview.autoterms and len(self.question.interview.autoterms[self.question.language]):
            for (term, vals) in self.question.interview.autoterms[self.question.language].items():
                result['autoterms'][term] = vals['definition']
    if self.orig_sought is not None:
        result['event_list'] = [self.orig_sought]
    if 'action_buttons' in self.extras:
        result['additional_buttons'] = []
        for item in self.extras['action_buttons']:
            new_item = copy.deepcopy(item)
            new_item['label'] = docassemble.base.filter.markdown_to_html(item['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + new_item['label'] + '</p>'
    for param in ('questionText',):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if hasattr(self, 'breadcrumb') and self.breadcrumb is not None:
        output['breadcrumb label'] = self.breadcrumb
    output['breadcrumbs'] = docassemble.base.functions.get_action_stack()
    if hasattr(self, 'subquestionText') and self.subquestionText is not None:
        if self.question.question_type == 'fields':
            embedder = dummy_embed_input
        else:
            embedder = None
        result['subquestionText'] = docassemble.base.filter.markdown_to_html(self.subquestionText.rstrip(), status=self, verbatim=not encode, embedder=embedder)
        if debug:
            output['question'] += result['subquestionText']
    for param in ('continueLabel', 'helpLabel'):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + result[param] + '</p>'
    if 'menu_items' in self.extras and isinstance(self.extras['menu_items'], list):
        result['menu_items'] = self.extras['menu_items']
    for param in ('cssClass', 'tableCssClass', 'css', 'script'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = self.extras[param].rstrip()
    for param in ('back_button_label',):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
    for param in ('rightText', 'underText'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if 'continueLabel' not in result:
        if self.question.question_type == 'review':
            result['continueLabel'] = word('Resume')
        else:
            result['continueLabel'] = word('Continue')
        if debug:
            output['question'] += '<p>' + result['continueLabel'] + '</p>'
    if self.question.question_type == 'yesno':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
    elif self.question.question_type == 'noyes':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
    elif self.question.question_type == 'yesnomaybe':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    elif self.question.question_type == 'noyesmaybe':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    steps = the_user_dict['_internal']['steps'] - the_user_dict['_internal']['steps_offset']
    if self.can_go_back and steps > 1:
        result['allow_going_back'] = True
        result['backTitle'] = word('Go back to the previous question')
        back_button_val = self.extras.get('back_button', None)
        if back_button_val or (back_button_val is None and self.question.interview.question_back_button):
            result['questionBackButton'] = self.back
    else:
        result['allow_going_back'] = False
    if self.question.question_type == 'signature':
        result['signaturePhrases'] = {'clear': word('Clear'), 'noSignature': word('You must sign your name to continue.'), 'loading': word('Loading.  Please wait . . . ')}
    if 'questionMetadata' in self.extras:
        result['question_metadata'] = self.extras['questionMetadata']
    if 'segment' in self.extras:
        result['segment'] = self.extras['segment']
    if 'ga_id' in self.extras:
        result['ga_id'] = self.extras['ga_id']
    if hasattr(self.question, 'id'):
        result['id'] = self.question.id
    if hasattr(self, 'audiovideo') and self.audiovideo is not None:
        audio_result = docassemble.base.filter.get_audio_urls(self.audiovideo)
        video_result = docassemble.base.filter.get_video_urls(self.audiovideo)
        if len(audio_result) > 0:
            result['audio'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in audio_result]
        if len(video_result) > 0:
            result['video'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in video_result]
    if hasattr(self, 'helpText') and len(self.helpText) > 0:
        result['helpText'] = list()
        result['helpBackLabel'] = word('Back to question')
        for help_text in self.helpText:
            result['helpText'].append(self.convert_help(help_text, encode, debug))
        result['help'] = dict()
        if self.helpText[0]['label']:
            result['help']['label'] = docassemble.base.filter.markdown_to_html(self.helpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['label'] = self.question.help()
        result['help']['title'] = word('Help is available for this question')
        result['help']['specific'] = False if self.question.helptext is None else True
    if hasattr(self, 'interviewHelpText') and len(self.interviewHelpText) > 0:
        result['interviewHelpText'] = list()
        for help_text in self.interviewHelpText:
            result['interviewHelpText'].append(self.convert_help(help_text, encode, debug))
        if 'help' not in result:
            result['help'] = dict()
        if self.interviewHelpText[0]['label']:
            result['help']['interviewLabel'] = docassemble.base.filter.markdown_to_html(self.interviewHelpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['interviewLabel'] = self.question.help()
        result['help']['interviewTitle'] = word('Help is available')
        if not (hasattr(self, 'helpText') and len(self.helpText) > 0):
            result['help']['specific'] = False
    if 'questionText' not in result and self.question.question_type == 'signature':
        result['questionText'] = word('Sign Your Name')
        if debug:
            output['question'] += '<p>' + result['questionText'] + '</p>'
    result['questionType'] = self.question.question_type
    if hasattr(self.question, 'question_variety'):
        result['questionVariety'] = self.question.question_variety
    if self.question.is_mandatory or self.question.mandatory_code is not None:
        result['mandatory'] = True
    if hasattr(self.question, 'name'):
        result['_question_name'] = self.question.name
    result['_tracker'] = self.tracker
    if hasattr(self, 'datatypes'):
        result['_datatypes'] = safeid(json.dumps(self.datatypes))
    if hasattr(self, 'varnames'):
        result['_varnames'] = safeid(json.dumps(self.varnames))
    if len(self.question.fields) > 0:
        result['fields'] = list()
    if hasattr(self.question, 'review_saveas'):
        result['question_variable_name'] = self.question.review_saveas
    if hasattr(self.question, 'fields_saveas'):
        result['question_variable_name'] = self.question.fields_saveas
    if self.decorations is not None:
        width_value = get_config('decoration size', 2.0)
        width_units = get_config('decoration units', 'em')
        for decoration in self.decorations:
            if 'image' in decoration:
                result['decoration'] = {}
                the_image = self.question.interview.images.get(decoration['image'], None)
                if the_image is not None:
                    the_url = docassemble.base.functions.server.url_finder(str(the_image.package) + ':' + str(the_image.filename))
                    width = str(width_value) + str(width_units)
                    filename = docassemble.base.functions.server.file_finder(str(the_image.package) + ':' + str(the_image.filename))
                    if 'extension' in filename and filename['extension'] == 'svg' and ('width' in filename):
                        if filename['width'] and filename['height']:
                            height = str(width_value * (filename['height'] / filename['width'])) + str(width_units)
                    else:
                        height = 'auto'
                    if the_url is not None:
                        result['decoration']['url'] = the_url
                        result['decoration']['size'] = {'width': width, 'height': height}
                        if the_image.attribution is not None:
                            self.attributions.add(the_image.attribution)
                        break
                elif get_config('default icons', None) in ('material icons', 'font awesome'):
                    result['decoration']['name'] = decoration['image']
                    result['decoration']['size'] = str(width_value) + str(width_units)
                    break
    if len(self.attachments) > 0:
        result['attachments'] = list()
        if self.current_info['user']['is_authenticated'] and self.current_info['user']['email']:
            result['default_email'] = self.current_info['user']['email']
        for attachment in self.attachments:
            the_attachment = dict(url=dict(), number=dict(), filename_with_extension=dict())
            if 'orig_variable_name' in attachment and attachment['orig_variable_name']:
                the_attachment['variable_name'] = attachment['orig_variable_name']
            if 'name' in attachment:
                if attachment['name']:
                    the_attachment['name'] = docassemble.base.filter.markdown_to_html(attachment['name'], trim=True, status=self, verbatim=not encode)
                    if debug:
                        output['question'] += '<p>' + the_attachment['name'] + '</p>'
            if 'description' in attachment:
                if attachment['description']:
                    the_attachment['description'] = docassemble.base.filter.markdown_to_html(attachment['description'], status=self, verbatim=not encode)
                    if debug:
                        output['question'] += the_attachment['description']
            for key in ('valid_formats', 'filename', 'content', 'markdown', 'raw'):
                if key in attachment:
                    if attachment[key]:
                        the_attachment[key] = attachment[key]
            for the_format in attachment['file']:
                the_attachment['url'][the_format] = docassemble.base.functions.server.url_finder(attachment['file'][the_format], filename=attachment['filename'] + '.' + extension_of_doc_format[the_format])
                the_attachment['number'][the_format] = attachment['file'][the_format]
                the_attachment['filename_with_extension'][the_format] = attachment['filename'] + '.' + extension_of_doc_format[the_format]
            result['attachments'].append(the_attachment)
    if self.extras.get('list_collect', False) is not False:
        result['listCollect'] = {'deleteLabel': word('Delete'), 'addAnotherLabel': self.extras['list_collect_add_another_label'] if self.extras['list_collect_add_another_label'] else word('Add another'), 'deletedLabel': word('(Deleted)'), 'undeleteLabel': word('Undelete')}
    validation_rules_used = set()
    file_fields = list()
    for field in self.question.fields:
        the_field = dict()
        the_field['number'] = field.number
        if hasattr(field, 'saveas'):
            the_field['variable_name'] = from_safeid(field.saveas)
            if encode:
                the_field['variable_name_encoded'] = field.saveas
            the_field['validation_messages'] = dict()
            if self.question.question_type == 'multiple_choice' and self.question.question_variety in ['radio', 'dropdown', 'combobox']:
                if self.question.question_variety == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
            elif not (hasattr(field, 'datatype') and field.datatype in ['multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes']):
                if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                elif hasattr(field, 'inputtype') and field.inputtype == 'ajax':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one.'))
                elif hasattr(field, 'datatype') and (field.datatype == 'object_radio' or (hasattr(field, 'inputtype') and field.inputtype in ('yesnoradio', 'noyesradio', 'radio', 'dropdown'))):
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('required', self, word('This field is required.'))
            if hasattr(field, 'inputtype') and field.inputtype in ['yesno', 'noyes', 'yesnowide', 'noyeswide'] and hasattr(field, 'uncheckothers') and (field.uncheckothers is not False):
                the_field['validation_messages']['uncheckothers'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([strip_tags(self.labels[field.number])]))
            if hasattr(field, 'datatype') and field.datatype not in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                for key in ('minlength', 'maxlength'):
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
                        if key == 'minlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'maxlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        if hasattr(field, 'datatype'):
            if field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes') and (hasattr(field, 'nota') and self.extras['nota'][field.number] is not False or (hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)))):
                if field.datatype.endswith('checkboxes'):
                    d_type = 'checkbox'
                else:
                    d_type = 'multiselect'
                if hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)):
                    checkbox_messages = dict()
                    if 'minlength' in field.extras and 'minlength' in self.extras and ('maxlength' in field.extras) and ('maxlength' in self.extras) and (self.extras['minlength'][field.number] == self.extras['maxlength'][field.number]) and (self.extras['minlength'][field.number] > 0):
                        if 'nota' not in self.extras:
                            self.extras['nota'] = dict()
                        self.extras['nota'][field.number] = False
                        if d_type == 'checkbox':
                            checkbox_messages['checkexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                        else:
                            checkbox_messages['selectexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    else:
                        if 'minlength' in field.extras and 'minlength' in self.extras:
                            if d_type == 'checkbox':
                                if self.extras['minlength'][field.number] == 1:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select one.'))
                                else:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                                if int(float(self.extras['minlength'][field.number])) > 0:
                                    if 'nota' not in self.extras:
                                        self.extras['nota'] = dict()
                                    self.extras['nota'][field.number] = False
                            elif self.extras['minlength'][field.number] == 1:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select one.'))
                            else:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                        if 'maxlength' in field.extras and 'maxlength' in self.extras:
                            if d_type == 'checkbox':
                                checkbox_messages['checkatmost'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                            else:
                                checkbox_messages['maxlength'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    the_field['validation_messages'].update(checkbox_messages)
                if d_type == 'checkbox':
                    if hasattr(field, 'nota') and self.extras['nota'][field.number] is not False:
                        the_field['validation_messages']['checkatleast'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([self.extras['nota'][field.number]]))
            if field.datatype == 'date':
                the_field['validation_messages']['date'] = field.validation_message('date', self, word('You need to enter a valid date.'))
                if hasattr(field, 'extras') and 'min' in field.extras and ('min' in self.extras) and ('max' in field.extras) and ('max' in self.extras) and (field.number in self.extras['min']) and (field.number in self.extras['max']):
                    the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.extras['min'][field.number], format='medium'), docassemble.base.util.format_date(self.extras['max'][field.number], format='medium')))
                else:
                    was_defined = dict()
                    for key in ['min', 'max']:
                        if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                            was_defined[key] = True
                            if key == 'min':
                                the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                            elif key == 'max':
                                the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                    if len(was_defined) == 0 and 'default date min' in self.question.interview.options and ('default date max' in self.question.interview.options):
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium'), docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')))
                    elif 'max' not in was_defined and 'default date max' in self.question.interview.options:
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')]))
                    elif 'min' not in was_defined and 'default date min' in self.question.interview.options:
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium')]))
            if field.datatype == 'time':
                the_field['validation_messages']['time'] = field.validation_message('time', self, word('You need to enter a valid time.'))
            if field.datatype in ['datetime', 'datetime-local']:
                the_field['validation_messages']['datetime'] = field.validation_message('datetime', self, word('You need to enter a valid date and time.'))
            if field.datatype == 'email':
                the_field['validation_messages']['email'] = field.validation_message('email', self, word('You need to enter a complete e-mail address.'))
            if field.datatype in ['number', 'currency', 'float', 'integer']:
                the_field['validation_messages']['number'] = field.validation_message('number', self, word('You need to enter a number.'))
                if field.datatype == 'integer' and (not ('step' in self.extras and field.number in self.extras['step'])):
                    the_field['validation_messages']['step'] = field.validation_message('integer', self, word('Please enter a whole number.'))
                elif 'step' in self.extras and field.number in self.extras['step']:
                    the_field['validation_messages']['step'] = field.validation_message('step', self, word('Please enter a multiple of {0}.'))
                for key in ['min', 'max']:
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                        if key == 'min':
                            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'max':
                            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))
            if field.datatype in ['files', 'file', 'camera', 'user', 'environment', 'camcorder', 'microphone']:
                file_fields.append(field)
                the_field['validation_messages']['required'] = field.validation_message('file required', self, word('You must provide a file.'))
                if 'accept' in self.extras and field.number in self.extras['accept']:
                    the_field['validation_messages']['accept'] = field.validation_message('accept', self, word('Please upload a file with a valid file format.'))
                if get_config('maximum content length') is not None:
                    the_field['max'] = get_config('maximum content length')
                    the_field['validation_messages']['max'] = field.validation_message('maxuploadsize', self, word('Your file upload is larger than the server can accept. Please reduce the size of your file upload.'))
        for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
            if hasattr(field, param):
                the_field[param] = getattr(field, param)
        if hasattr(field, 'shuffle') and field.shuffle is not False:
            the_field['shuffle'] = True
        if hasattr(field, 'disableothers') and field.disableothers and hasattr(field, 'saveas'):
            the_field['disable_others'] = True
        if hasattr(field, 'uncheckothers') and field.uncheckothers is not False:
            the_field['uncheck_others'] = True
        for key in ('minlength', 'maxlength', 'min', 'max', 'step', 'scale', 'inline', 'inline width', 'rows', 'accept', 'currency symbol', 'field metadata', 'css class'):
            if key in self.extras and field.number in self.extras[key]:
                if key in ('minlength', 'maxlength', 'min', 'max', 'step'):
                    validation_rules_used.add(key)
                the_field[key] = self.extras[key][field.number]
        if hasattr(field, 'extras') and 'custom_parameters' in field.extras:
            for (parameter, parameter_value) in field.extras['custom_parameters'].items():
                the_field[parameter] = parameter_value
        for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
            if param_type in self.extras and field.number in self.extras[param_type]:
                for (parameter, parameter_value) in self.extras[param_type][field.number].items():
                    the_field[parameter] = parameter_value
        if hasattr(field, 'saveas') and field.saveas in self.embedded:
            the_field['embedded'] = True
        if hasattr(self, 'shuffle'):
            the_field['shuffle'] = self.shuffle
        if field.number in self.defaults:
            the_default = self.defaults[field.number]
            if isinstance(the_default, (str, int, bool, float)):
                the_field['default'] = the_default
        else:
            the_default = None
        if self.question.question_type == 'multiple_choice' or hasattr(field, 'choicetype') or (hasattr(field, 'datatype') and field.datatype in ('object', 'multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes', 'object_radio')):
            the_field['choices'] = self.get_choices_data(field, the_default, the_user_dict, encode=encode)
        if hasattr(field, 'nota'):
            the_field['none_of_the_above'] = docassemble.base.filter.markdown_to_html(self.extras['nota'][field.number], do_terms=False, status=self, verbatim=not encode)
        if field.number in self.extras['ok']:
            the_field['active'] = self.extras['ok'][field.number]
        else:
            the_field['active'] = True
        if field.number in self.extras['required']:
            the_field['required'] = self.extras['required'][field.number]
            if the_field['required']:
                validation_rules_used.add('required')
        if 'validation messages' in self.extras and field.number in self.extras['validation messages']:
            the_field['validation_messages'].update(self.extras['validation messages'][field.number])
        if 'permissions' in self.extras and field.number in self.extras['permissions']:
            the_field['permissions'] = self.extras['permissions'][field.number]
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('max_image_size' in self.extras) and self.extras['max_image_size']:
            the_field['max_image_size'] = self.extras['max_image_size']
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('image_type' in self.extras) and self.extras['image_type']:
            the_field['image_type'] = self.extras['image_type']
        if hasattr(field, 'extras'):
            if 'ml_group' in field.extras or 'ml_train' in field.extras:","for key in ('minlength', 'maxlength'):
    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
        if key == 'minlength':
            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        elif key == 'maxlength':
            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))",XXX,no_found,0,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,InterviewStatus,as_data$768,"def as_data(self, the_user_dict, encode=True):
    result = dict(language=self.question.language)
    debug = self.question.interview.debug
    if debug:
        output = dict(question='', help='')
    if 'progress' in the_user_dict['_internal']:
        result['progress'] = the_user_dict['_internal']['progress']
    if self.question.language in self.question.interview.default_validation_messages:
        result['validation_messages'] = copy.copy(self.question.interview.default_validation_messages[self.question.language])
    else:
        result['validation_messages'] = dict()
    if 'reload_after' in self.extras:
        result['reload'] = 1000 * int(self.extras['reload_after'])
    lang = docassemble.base.functions.get_language()
    if len(self.question.terms) or len(self.question.interview.terms):
        result['terms'] = dict()
        if 'terms' in self.extras:
            for (term, vals) in self.extras['terms'].items():
                result['terms'][term] = vals['definition']
        if lang in self.question.interview.terms and len(self.question.interview.terms[lang]):
            for (term, vals) in self.question.interview.terms[lang].items():
                result['terms'][term] = vals['definition']
        elif self.question.language in self.question.interview.terms and len(self.question.interview.terms[self.question.language]):
            for (term, vals) in self.question.interview.terms[self.question.language].items():
                result['terms'][term] = vals['definition']
    if len(self.question.autoterms) or len(self.question.interview.autoterms):
        result['autoterms'] = dict()
        if 'autoterms' in self.extras:
            for (term, vals) in self.extras['autoterms'].items():
                result['autoterms'][term] = vals['definition']
        if lang in self.question.interview.autoterms and len(self.question.interview.autoterms[lang]):
            for (term, vals) in self.question.interview.autoterms[lang].items():
                result['autoterms'][term] = vals['definition']
        elif self.question.language in self.question.interview.autoterms and len(self.question.interview.autoterms[self.question.language]):
            for (term, vals) in self.question.interview.autoterms[self.question.language].items():
                result['autoterms'][term] = vals['definition']
    if self.orig_sought is not None:
        result['event_list'] = [self.orig_sought]
    if 'action_buttons' in self.extras:
        result['additional_buttons'] = []
        for item in self.extras['action_buttons']:
            new_item = copy.deepcopy(item)
            new_item['label'] = docassemble.base.filter.markdown_to_html(item['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + new_item['label'] + '</p>'
    for param in ('questionText',):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if hasattr(self, 'breadcrumb') and self.breadcrumb is not None:
        output['breadcrumb label'] = self.breadcrumb
    output['breadcrumbs'] = docassemble.base.functions.get_action_stack()
    if hasattr(self, 'subquestionText') and self.subquestionText is not None:
        if self.question.question_type == 'fields':
            embedder = dummy_embed_input
        else:
            embedder = None
        result['subquestionText'] = docassemble.base.filter.markdown_to_html(self.subquestionText.rstrip(), status=self, verbatim=not encode, embedder=embedder)
        if debug:
            output['question'] += result['subquestionText']
    for param in ('continueLabel', 'helpLabel'):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + result[param] + '</p>'
    if 'menu_items' in self.extras and isinstance(self.extras['menu_items'], list):
        result['menu_items'] = self.extras['menu_items']
    for param in ('cssClass', 'tableCssClass', 'css', 'script'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = self.extras[param].rstrip()
    for param in ('back_button_label',):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
    for param in ('rightText', 'underText'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if 'continueLabel' not in result:
        if self.question.question_type == 'review':
            result['continueLabel'] = word('Resume')
        else:
            result['continueLabel'] = word('Continue')
        if debug:
            output['question'] += '<p>' + result['continueLabel'] + '</p>'
    if self.question.question_type == 'yesno':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
    elif self.question.question_type == 'noyes':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
    elif self.question.question_type == 'yesnomaybe':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    elif self.question.question_type == 'noyesmaybe':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    steps = the_user_dict['_internal']['steps'] - the_user_dict['_internal']['steps_offset']
    if self.can_go_back and steps > 1:
        result['allow_going_back'] = True
        result['backTitle'] = word('Go back to the previous question')
        back_button_val = self.extras.get('back_button', None)
        if back_button_val or (back_button_val is None and self.question.interview.question_back_button):
            result['questionBackButton'] = self.back
    else:
        result['allow_going_back'] = False
    if self.question.question_type == 'signature':
        result['signaturePhrases'] = {'clear': word('Clear'), 'noSignature': word('You must sign your name to continue.'), 'loading': word('Loading.  Please wait . . . ')}
    if 'questionMetadata' in self.extras:
        result['question_metadata'] = self.extras['questionMetadata']
    if 'segment' in self.extras:
        result['segment'] = self.extras['segment']
    if 'ga_id' in self.extras:
        result['ga_id'] = self.extras['ga_id']
    if hasattr(self.question, 'id'):
        result['id'] = self.question.id
    if hasattr(self, 'audiovideo') and self.audiovideo is not None:
        audio_result = docassemble.base.filter.get_audio_urls(self.audiovideo)
        video_result = docassemble.base.filter.get_video_urls(self.audiovideo)
        if len(audio_result) > 0:
            result['audio'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in audio_result]
        if len(video_result) > 0:
            result['video'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in video_result]
    if hasattr(self, 'helpText') and len(self.helpText) > 0:
        result['helpText'] = list()
        result['helpBackLabel'] = word('Back to question')
        for help_text in self.helpText:
            result['helpText'].append(self.convert_help(help_text, encode, debug))
        result['help'] = dict()
        if self.helpText[0]['label']:
            result['help']['label'] = docassemble.base.filter.markdown_to_html(self.helpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['label'] = self.question.help()
        result['help']['title'] = word('Help is available for this question')
        result['help']['specific'] = False if self.question.helptext is None else True
    if hasattr(self, 'interviewHelpText') and len(self.interviewHelpText) > 0:
        result['interviewHelpText'] = list()
        for help_text in self.interviewHelpText:
            result['interviewHelpText'].append(self.convert_help(help_text, encode, debug))
        if 'help' not in result:
            result['help'] = dict()
        if self.interviewHelpText[0]['label']:
            result['help']['interviewLabel'] = docassemble.base.filter.markdown_to_html(self.interviewHelpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['interviewLabel'] = self.question.help()
        result['help']['interviewTitle'] = word('Help is available')
        if not (hasattr(self, 'helpText') and len(self.helpText) > 0):
            result['help']['specific'] = False
    if 'questionText' not in result and self.question.question_type == 'signature':
        result['questionText'] = word('Sign Your Name')
        if debug:
            output['question'] += '<p>' + result['questionText'] + '</p>'
    result['questionType'] = self.question.question_type
    if hasattr(self.question, 'question_variety'):
        result['questionVariety'] = self.question.question_variety
    if self.question.is_mandatory or self.question.mandatory_code is not None:
        result['mandatory'] = True
    if hasattr(self.question, 'name'):
        result['_question_name'] = self.question.name
    result['_tracker'] = self.tracker
    if hasattr(self, 'datatypes'):
        result['_datatypes'] = safeid(json.dumps(self.datatypes))
    if hasattr(self, 'varnames'):
        result['_varnames'] = safeid(json.dumps(self.varnames))
    if len(self.question.fields) > 0:
        result['fields'] = list()
    if hasattr(self.question, 'review_saveas'):
        result['question_variable_name'] = self.question.review_saveas
    if hasattr(self.question, 'fields_saveas'):
        result['question_variable_name'] = self.question.fields_saveas
    if self.decorations is not None:
        width_value = get_config('decoration size', 2.0)
        width_units = get_config('decoration units', 'em')
        for decoration in self.decorations:
            if 'image' in decoration:
                result['decoration'] = {}
                the_image = self.question.interview.images.get(decoration['image'], None)
                if the_image is not None:
                    the_url = docassemble.base.functions.server.url_finder(str(the_image.package) + ':' + str(the_image.filename))
                    width = str(width_value) + str(width_units)
                    filename = docassemble.base.functions.server.file_finder(str(the_image.package) + ':' + str(the_image.filename))
                    if 'extension' in filename and filename['extension'] == 'svg' and ('width' in filename):
                        if filename['width'] and filename['height']:
                            height = str(width_value * (filename['height'] / filename['width'])) + str(width_units)
                    else:
                        height = 'auto'
                    if the_url is not None:
                        result['decoration']['url'] = the_url
                        result['decoration']['size'] = {'width': width, 'height': height}
                        if the_image.attribution is not None:
                            self.attributions.add(the_image.attribution)
                        break
                elif get_config('default icons', None) in ('material icons', 'font awesome'):
                    result['decoration']['name'] = decoration['image']
                    result['decoration']['size'] = str(width_value) + str(width_units)
                    break
    if len(self.attachments) > 0:
        result['attachments'] = list()
        if self.current_info['user']['is_authenticated'] and self.current_info['user']['email']:
            result['default_email'] = self.current_info['user']['email']
        for attachment in self.attachments:
            the_attachment = dict(url=dict(), number=dict(), filename_with_extension=dict())
            if 'orig_variable_name' in attachment and attachment['orig_variable_name']:
                the_attachment['variable_name'] = attachment['orig_variable_name']
            if 'name' in attachment:
                if attachment['name']:
                    the_attachment['name'] = docassemble.base.filter.markdown_to_html(attachment['name'], trim=True, status=self, verbatim=not encode)
                    if debug:
                        output['question'] += '<p>' + the_attachment['name'] + '</p>'
            if 'description' in attachment:
                if attachment['description']:
                    the_attachment['description'] = docassemble.base.filter.markdown_to_html(attachment['description'], status=self, verbatim=not encode)
                    if debug:
                        output['question'] += the_attachment['description']
            for key in ('valid_formats', 'filename', 'content', 'markdown', 'raw'):
                if key in attachment:
                    if attachment[key]:
                        the_attachment[key] = attachment[key]
            for the_format in attachment['file']:
                the_attachment['url'][the_format] = docassemble.base.functions.server.url_finder(attachment['file'][the_format], filename=attachment['filename'] + '.' + extension_of_doc_format[the_format])
                the_attachment['number'][the_format] = attachment['file'][the_format]
                the_attachment['filename_with_extension'][the_format] = attachment['filename'] + '.' + extension_of_doc_format[the_format]
            result['attachments'].append(the_attachment)
    if self.extras.get('list_collect', False) is not False:
        result['listCollect'] = {'deleteLabel': word('Delete'), 'addAnotherLabel': self.extras['list_collect_add_another_label'] if self.extras['list_collect_add_another_label'] else word('Add another'), 'deletedLabel': word('(Deleted)'), 'undeleteLabel': word('Undelete')}
    validation_rules_used = set()
    file_fields = list()
    for field in self.question.fields:
        the_field = dict()
        the_field['number'] = field.number
        if hasattr(field, 'saveas'):
            the_field['variable_name'] = from_safeid(field.saveas)
            if encode:
                the_field['variable_name_encoded'] = field.saveas
            the_field['validation_messages'] = dict()
            if self.question.question_type == 'multiple_choice' and self.question.question_variety in ['radio', 'dropdown', 'combobox']:
                if self.question.question_variety == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
            elif not (hasattr(field, 'datatype') and field.datatype in ['multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes']):
                if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                elif hasattr(field, 'inputtype') and field.inputtype == 'ajax':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one.'))
                elif hasattr(field, 'datatype') and (field.datatype == 'object_radio' or (hasattr(field, 'inputtype') and field.inputtype in ('yesnoradio', 'noyesradio', 'radio', 'dropdown'))):
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('required', self, word('This field is required.'))
            if hasattr(field, 'inputtype') and field.inputtype in ['yesno', 'noyes', 'yesnowide', 'noyeswide'] and hasattr(field, 'uncheckothers') and (field.uncheckothers is not False):
                the_field['validation_messages']['uncheckothers'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([strip_tags(self.labels[field.number])]))
            if hasattr(field, 'datatype') and field.datatype not in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                for key in ('minlength', 'maxlength'):
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
                        if key == 'minlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'maxlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        if hasattr(field, 'datatype'):
            if field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes') and (hasattr(field, 'nota') and self.extras['nota'][field.number] is not False or (hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)))):
                if field.datatype.endswith('checkboxes'):
                    d_type = 'checkbox'
                else:
                    d_type = 'multiselect'
                if hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)):
                    checkbox_messages = dict()
                    if 'minlength' in field.extras and 'minlength' in self.extras and ('maxlength' in field.extras) and ('maxlength' in self.extras) and (self.extras['minlength'][field.number] == self.extras['maxlength'][field.number]) and (self.extras['minlength'][field.number] > 0):
                        if 'nota' not in self.extras:
                            self.extras['nota'] = dict()
                        self.extras['nota'][field.number] = False
                        if d_type == 'checkbox':
                            checkbox_messages['checkexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                        else:
                            checkbox_messages['selectexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    else:
                        if 'minlength' in field.extras and 'minlength' in self.extras:
                            if d_type == 'checkbox':
                                if self.extras['minlength'][field.number] == 1:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select one.'))
                                else:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                                if int(float(self.extras['minlength'][field.number])) > 0:
                                    if 'nota' not in self.extras:
                                        self.extras['nota'] = dict()
                                    self.extras['nota'][field.number] = False
                            elif self.extras['minlength'][field.number] == 1:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select one.'))
                            else:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                        if 'maxlength' in field.extras and 'maxlength' in self.extras:
                            if d_type == 'checkbox':
                                checkbox_messages['checkatmost'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                            else:
                                checkbox_messages['maxlength'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    the_field['validation_messages'].update(checkbox_messages)
                if d_type == 'checkbox':
                    if hasattr(field, 'nota') and self.extras['nota'][field.number] is not False:
                        the_field['validation_messages']['checkatleast'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([self.extras['nota'][field.number]]))
            if field.datatype == 'date':
                the_field['validation_messages']['date'] = field.validation_message('date', self, word('You need to enter a valid date.'))
                if hasattr(field, 'extras') and 'min' in field.extras and ('min' in self.extras) and ('max' in field.extras) and ('max' in self.extras) and (field.number in self.extras['min']) and (field.number in self.extras['max']):
                    the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.extras['min'][field.number], format='medium'), docassemble.base.util.format_date(self.extras['max'][field.number], format='medium')))
                else:
                    was_defined = dict()
                    for key in ['min', 'max']:
                        if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                            was_defined[key] = True
                            if key == 'min':
                                the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                            elif key == 'max':
                                the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                    if len(was_defined) == 0 and 'default date min' in self.question.interview.options and ('default date max' in self.question.interview.options):
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium'), docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')))
                    elif 'max' not in was_defined and 'default date max' in self.question.interview.options:
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')]))
                    elif 'min' not in was_defined and 'default date min' in self.question.interview.options:
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium')]))
            if field.datatype == 'time':
                the_field['validation_messages']['time'] = field.validation_message('time', self, word('You need to enter a valid time.'))
            if field.datatype in ['datetime', 'datetime-local']:
                the_field['validation_messages']['datetime'] = field.validation_message('datetime', self, word('You need to enter a valid date and time.'))
            if field.datatype == 'email':
                the_field['validation_messages']['email'] = field.validation_message('email', self, word('You need to enter a complete e-mail address.'))
            if field.datatype in ['number', 'currency', 'float', 'integer']:
                the_field['validation_messages']['number'] = field.validation_message('number', self, word('You need to enter a number.'))
                if field.datatype == 'integer' and (not ('step' in self.extras and field.number in self.extras['step'])):
                    the_field['validation_messages']['step'] = field.validation_message('integer', self, word('Please enter a whole number.'))
                elif 'step' in self.extras and field.number in self.extras['step']:
                    the_field['validation_messages']['step'] = field.validation_message('step', self, word('Please enter a multiple of {0}.'))
                for key in ['min', 'max']:
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                        if key == 'min':
                            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'max':
                            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))
            if field.datatype in ['files', 'file', 'camera', 'user', 'environment', 'camcorder', 'microphone']:
                file_fields.append(field)
                the_field['validation_messages']['required'] = field.validation_message('file required', self, word('You must provide a file.'))
                if 'accept' in self.extras and field.number in self.extras['accept']:
                    the_field['validation_messages']['accept'] = field.validation_message('accept', self, word('Please upload a file with a valid file format.'))
                if get_config('maximum content length') is not None:
                    the_field['max'] = get_config('maximum content length')
                    the_field['validation_messages']['max'] = field.validation_message('maxuploadsize', self, word('Your file upload is larger than the server can accept. Please reduce the size of your file upload.'))
        for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
            if hasattr(field, param):
                the_field[param] = getattr(field, param)
        if hasattr(field, 'shuffle') and field.shuffle is not False:
            the_field['shuffle'] = True
        if hasattr(field, 'disableothers') and field.disableothers and hasattr(field, 'saveas'):
            the_field['disable_others'] = True
        if hasattr(field, 'uncheckothers') and field.uncheckothers is not False:
            the_field['uncheck_others'] = True
        for key in ('minlength', 'maxlength', 'min', 'max', 'step', 'scale', 'inline', 'inline width', 'rows', 'accept', 'currency symbol', 'field metadata', 'css class'):
            if key in self.extras and field.number in self.extras[key]:
                if key in ('minlength', 'maxlength', 'min', 'max', 'step'):
                    validation_rules_used.add(key)
                the_field[key] = self.extras[key][field.number]
        if hasattr(field, 'extras') and 'custom_parameters' in field.extras:
            for (parameter, parameter_value) in field.extras['custom_parameters'].items():
                the_field[parameter] = parameter_value
        for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
            if param_type in self.extras and field.number in self.extras[param_type]:
                for (parameter, parameter_value) in self.extras[param_type][field.number].items():
                    the_field[parameter] = parameter_value
        if hasattr(field, 'saveas') and field.saveas in self.embedded:
            the_field['embedded'] = True
        if hasattr(self, 'shuffle'):
            the_field['shuffle'] = self.shuffle
        if field.number in self.defaults:
            the_default = self.defaults[field.number]
            if isinstance(the_default, (str, int, bool, float)):
                the_field['default'] = the_default
        else:
            the_default = None
        if self.question.question_type == 'multiple_choice' or hasattr(field, 'choicetype') or (hasattr(field, 'datatype') and field.datatype in ('object', 'multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes', 'object_radio')):
            the_field['choices'] = self.get_choices_data(field, the_default, the_user_dict, encode=encode)
        if hasattr(field, 'nota'):
            the_field['none_of_the_above'] = docassemble.base.filter.markdown_to_html(self.extras['nota'][field.number], do_terms=False, status=self, verbatim=not encode)
        if field.number in self.extras['ok']:
            the_field['active'] = self.extras['ok'][field.number]
        else:
            the_field['active'] = True
        if field.number in self.extras['required']:
            the_field['required'] = self.extras['required'][field.number]
            if the_field['required']:
                validation_rules_used.add('required')
        if 'validation messages' in self.extras and field.number in self.extras['validation messages']:
            the_field['validation_messages'].update(self.extras['validation messages'][field.number])
        if 'permissions' in self.extras and field.number in self.extras['permissions']:
            the_field['permissions'] = self.extras['permissions'][field.number]
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('max_image_size' in self.extras) and self.extras['max_image_size']:
            the_field['max_image_size'] = self.extras['max_image_size']
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('image_type' in self.extras) and self.extras['image_type']:
            the_field['image_type'] = self.extras['image_type']
        if hasattr(field, 'extras'):
            if 'ml_group' in field.extras or 'ml_train' in field.extras:","for key in ['min', 'max']:
    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
        if key == 'min':
            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
        elif key == 'max':
            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))",XXX,no_found,0,,,
docassemble,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/docassemble/docassemble_base/docassemble/base/parse.py,https://github.com/jhpyle/docassemble/tree/master/docassemble_base/docassemble/base/parse.py,InterviewStatus,as_data$768,"def as_data(self, the_user_dict, encode=True):
    result = dict(language=self.question.language)
    debug = self.question.interview.debug
    if debug:
        output = dict(question='', help='')
    if 'progress' in the_user_dict['_internal']:
        result['progress'] = the_user_dict['_internal']['progress']
    if self.question.language in self.question.interview.default_validation_messages:
        result['validation_messages'] = copy.copy(self.question.interview.default_validation_messages[self.question.language])
    else:
        result['validation_messages'] = dict()
    if 'reload_after' in self.extras:
        result['reload'] = 1000 * int(self.extras['reload_after'])
    lang = docassemble.base.functions.get_language()
    if len(self.question.terms) or len(self.question.interview.terms):
        result['terms'] = dict()
        if 'terms' in self.extras:
            for (term, vals) in self.extras['terms'].items():
                result['terms'][term] = vals['definition']
        if lang in self.question.interview.terms and len(self.question.interview.terms[lang]):
            for (term, vals) in self.question.interview.terms[lang].items():
                result['terms'][term] = vals['definition']
        elif self.question.language in self.question.interview.terms and len(self.question.interview.terms[self.question.language]):
            for (term, vals) in self.question.interview.terms[self.question.language].items():
                result['terms'][term] = vals['definition']
    if len(self.question.autoterms) or len(self.question.interview.autoterms):
        result['autoterms'] = dict()
        if 'autoterms' in self.extras:
            for (term, vals) in self.extras['autoterms'].items():
                result['autoterms'][term] = vals['definition']
        if lang in self.question.interview.autoterms and len(self.question.interview.autoterms[lang]):
            for (term, vals) in self.question.interview.autoterms[lang].items():
                result['autoterms'][term] = vals['definition']
        elif self.question.language in self.question.interview.autoterms and len(self.question.interview.autoterms[self.question.language]):
            for (term, vals) in self.question.interview.autoterms[self.question.language].items():
                result['autoterms'][term] = vals['definition']
    if self.orig_sought is not None:
        result['event_list'] = [self.orig_sought]
    if 'action_buttons' in self.extras:
        result['additional_buttons'] = []
        for item in self.extras['action_buttons']:
            new_item = copy.deepcopy(item)
            new_item['label'] = docassemble.base.filter.markdown_to_html(item['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + new_item['label'] + '</p>'
    for param in ('questionText',):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if hasattr(self, 'breadcrumb') and self.breadcrumb is not None:
        output['breadcrumb label'] = self.breadcrumb
    output['breadcrumbs'] = docassemble.base.functions.get_action_stack()
    if hasattr(self, 'subquestionText') and self.subquestionText is not None:
        if self.question.question_type == 'fields':
            embedder = dummy_embed_input
        else:
            embedder = None
        result['subquestionText'] = docassemble.base.filter.markdown_to_html(self.subquestionText.rstrip(), status=self, verbatim=not encode, embedder=embedder)
        if debug:
            output['question'] += result['subquestionText']
    for param in ('continueLabel', 'helpLabel'):
        if hasattr(self, param) and getattr(self, param) is not None:
            result[param] = docassemble.base.filter.markdown_to_html(getattr(self, param).rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
            if debug:
                output['question'] += '<p>' + result[param] + '</p>'
    if 'menu_items' in self.extras and isinstance(self.extras['menu_items'], list):
        result['menu_items'] = self.extras['menu_items']
    for param in ('cssClass', 'tableCssClass', 'css', 'script'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = self.extras[param].rstrip()
    for param in ('back_button_label',):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), trim=True, do_terms=False, status=self, verbatim=not encode)
    for param in ('rightText', 'underText'):
        if param in self.extras and isinstance(self.extras[param], str):
            result[param] = docassemble.base.filter.markdown_to_html(self.extras[param].rstrip(), status=self, verbatim=not encode)
            if debug:
                output['question'] += result[param]
    if 'continueLabel' not in result:
        if self.question.question_type == 'review':
            result['continueLabel'] = word('Resume')
        else:
            result['continueLabel'] = word('Continue')
        if debug:
            output['question'] += '<p>' + result['continueLabel'] + '</p>'
    if self.question.question_type == 'yesno':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
    elif self.question.question_type == 'noyes':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
    elif self.question.question_type == 'yesnomaybe':
        result['yesLabel'] = self.question.yes()
        result['noLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    elif self.question.question_type == 'noyesmaybe':
        result['noLabel'] = self.question.yes()
        result['yesLabel'] = self.question.no()
        result['maybeLabel'] = self.question.maybe()
    steps = the_user_dict['_internal']['steps'] - the_user_dict['_internal']['steps_offset']
    if self.can_go_back and steps > 1:
        result['allow_going_back'] = True
        result['backTitle'] = word('Go back to the previous question')
        back_button_val = self.extras.get('back_button', None)
        if back_button_val or (back_button_val is None and self.question.interview.question_back_button):
            result['questionBackButton'] = self.back
    else:
        result['allow_going_back'] = False
    if self.question.question_type == 'signature':
        result['signaturePhrases'] = {'clear': word('Clear'), 'noSignature': word('You must sign your name to continue.'), 'loading': word('Loading.  Please wait . . . ')}
    if 'questionMetadata' in self.extras:
        result['question_metadata'] = self.extras['questionMetadata']
    if 'segment' in self.extras:
        result['segment'] = self.extras['segment']
    if 'ga_id' in self.extras:
        result['ga_id'] = self.extras['ga_id']
    if hasattr(self.question, 'id'):
        result['id'] = self.question.id
    if hasattr(self, 'audiovideo') and self.audiovideo is not None:
        audio_result = docassemble.base.filter.get_audio_urls(self.audiovideo)
        video_result = docassemble.base.filter.get_video_urls(self.audiovideo)
        if len(audio_result) > 0:
            result['audio'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in audio_result]
        if len(video_result) > 0:
            result['video'] = [dict(url=re.sub('.*""(http[^""]+)"".*', '\\1', x)) if isinstance(x, str) else dict(url=x[0], mime_type=x[1]) for x in video_result]
    if hasattr(self, 'helpText') and len(self.helpText) > 0:
        result['helpText'] = list()
        result['helpBackLabel'] = word('Back to question')
        for help_text in self.helpText:
            result['helpText'].append(self.convert_help(help_text, encode, debug))
        result['help'] = dict()
        if self.helpText[0]['label']:
            result['help']['label'] = docassemble.base.filter.markdown_to_html(self.helpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['label'] = self.question.help()
        result['help']['title'] = word('Help is available for this question')
        result['help']['specific'] = False if self.question.helptext is None else True
    if hasattr(self, 'interviewHelpText') and len(self.interviewHelpText) > 0:
        result['interviewHelpText'] = list()
        for help_text in self.interviewHelpText:
            result['interviewHelpText'].append(self.convert_help(help_text, encode, debug))
        if 'help' not in result:
            result['help'] = dict()
        if self.interviewHelpText[0]['label']:
            result['help']['interviewLabel'] = docassemble.base.filter.markdown_to_html(self.interviewHelpText[0]['label'], trim=True, do_terms=False, status=self, verbatim=not encode)
        else:
            result['help']['interviewLabel'] = self.question.help()
        result['help']['interviewTitle'] = word('Help is available')
        if not (hasattr(self, 'helpText') and len(self.helpText) > 0):
            result['help']['specific'] = False
    if 'questionText' not in result and self.question.question_type == 'signature':
        result['questionText'] = word('Sign Your Name')
        if debug:
            output['question'] += '<p>' + result['questionText'] + '</p>'
    result['questionType'] = self.question.question_type
    if hasattr(self.question, 'question_variety'):
        result['questionVariety'] = self.question.question_variety
    if self.question.is_mandatory or self.question.mandatory_code is not None:
        result['mandatory'] = True
    if hasattr(self.question, 'name'):
        result['_question_name'] = self.question.name
    result['_tracker'] = self.tracker
    if hasattr(self, 'datatypes'):
        result['_datatypes'] = safeid(json.dumps(self.datatypes))
    if hasattr(self, 'varnames'):
        result['_varnames'] = safeid(json.dumps(self.varnames))
    if len(self.question.fields) > 0:
        result['fields'] = list()
    if hasattr(self.question, 'review_saveas'):
        result['question_variable_name'] = self.question.review_saveas
    if hasattr(self.question, 'fields_saveas'):
        result['question_variable_name'] = self.question.fields_saveas
    if self.decorations is not None:
        width_value = get_config('decoration size', 2.0)
        width_units = get_config('decoration units', 'em')
        for decoration in self.decorations:
            if 'image' in decoration:
                result['decoration'] = {}
                the_image = self.question.interview.images.get(decoration['image'], None)
                if the_image is not None:
                    the_url = docassemble.base.functions.server.url_finder(str(the_image.package) + ':' + str(the_image.filename))
                    width = str(width_value) + str(width_units)
                    filename = docassemble.base.functions.server.file_finder(str(the_image.package) + ':' + str(the_image.filename))
                    if 'extension' in filename and filename['extension'] == 'svg' and ('width' in filename):
                        if filename['width'] and filename['height']:
                            height = str(width_value * (filename['height'] / filename['width'])) + str(width_units)
                    else:
                        height = 'auto'
                    if the_url is not None:
                        result['decoration']['url'] = the_url
                        result['decoration']['size'] = {'width': width, 'height': height}
                        if the_image.attribution is not None:
                            self.attributions.add(the_image.attribution)
                        break
                elif get_config('default icons', None) in ('material icons', 'font awesome'):
                    result['decoration']['name'] = decoration['image']
                    result['decoration']['size'] = str(width_value) + str(width_units)
                    break
    if len(self.attachments) > 0:
        result['attachments'] = list()
        if self.current_info['user']['is_authenticated'] and self.current_info['user']['email']:
            result['default_email'] = self.current_info['user']['email']
        for attachment in self.attachments:
            the_attachment = dict(url=dict(), number=dict(), filename_with_extension=dict())
            if 'orig_variable_name' in attachment and attachment['orig_variable_name']:
                the_attachment['variable_name'] = attachment['orig_variable_name']
            if 'name' in attachment:
                if attachment['name']:
                    the_attachment['name'] = docassemble.base.filter.markdown_to_html(attachment['name'], trim=True, status=self, verbatim=not encode)
                    if debug:
                        output['question'] += '<p>' + the_attachment['name'] + '</p>'
            if 'description' in attachment:
                if attachment['description']:
                    the_attachment['description'] = docassemble.base.filter.markdown_to_html(attachment['description'], status=self, verbatim=not encode)
                    if debug:
                        output['question'] += the_attachment['description']
            for key in ('valid_formats', 'filename', 'content', 'markdown', 'raw'):
                if key in attachment:
                    if attachment[key]:
                        the_attachment[key] = attachment[key]
            for the_format in attachment['file']:
                the_attachment['url'][the_format] = docassemble.base.functions.server.url_finder(attachment['file'][the_format], filename=attachment['filename'] + '.' + extension_of_doc_format[the_format])
                the_attachment['number'][the_format] = attachment['file'][the_format]
                the_attachment['filename_with_extension'][the_format] = attachment['filename'] + '.' + extension_of_doc_format[the_format]
            result['attachments'].append(the_attachment)
    if self.extras.get('list_collect', False) is not False:
        result['listCollect'] = {'deleteLabel': word('Delete'), 'addAnotherLabel': self.extras['list_collect_add_another_label'] if self.extras['list_collect_add_another_label'] else word('Add another'), 'deletedLabel': word('(Deleted)'), 'undeleteLabel': word('Undelete')}
    validation_rules_used = set()
    file_fields = list()
    for field in self.question.fields:
        the_field = dict()
        the_field['number'] = field.number
        if hasattr(field, 'saveas'):
            the_field['variable_name'] = from_safeid(field.saveas)
            if encode:
                the_field['variable_name_encoded'] = field.saveas
            the_field['validation_messages'] = dict()
            if self.question.question_type == 'multiple_choice' and self.question.question_variety in ['radio', 'dropdown', 'combobox']:
                if self.question.question_variety == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
            elif not (hasattr(field, 'datatype') and field.datatype in ['multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes']):
                if hasattr(field, 'inputtype') and field.inputtype == 'combobox':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one or type in a new value.'))
                elif hasattr(field, 'inputtype') and field.inputtype == 'ajax':
                    the_field['validation_messages']['required'] = field.validation_message('combobox required', self, word('You need to select one.'))
                elif hasattr(field, 'datatype') and (field.datatype == 'object_radio' or (hasattr(field, 'inputtype') and field.inputtype in ('yesnoradio', 'noyesradio', 'radio', 'dropdown'))):
                    the_field['validation_messages']['required'] = field.validation_message('multiple choice required', self, word('You need to select one.'))
                else:
                    the_field['validation_messages']['required'] = field.validation_message('required', self, word('This field is required.'))
            if hasattr(field, 'inputtype') and field.inputtype in ['yesno', 'noyes', 'yesnowide', 'noyeswide'] and hasattr(field, 'uncheckothers') and (field.uncheckothers is not False):
                the_field['validation_messages']['uncheckothers'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([strip_tags(self.labels[field.number])]))
            if hasattr(field, 'datatype') and field.datatype not in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes'):
                for key in ('minlength', 'maxlength'):
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras):
                        if key == 'minlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You must type at least %s characters.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'maxlength':
                            the_field['validation_messages'][key] = field.validation_message(key, self, word('You cannot type more than %s characters.'), parameters=tuple([self.extras[key][field.number]]))
        if hasattr(field, 'datatype'):
            if field.datatype in ('multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes') and (hasattr(field, 'nota') and self.extras['nota'][field.number] is not False or (hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)))):
                if field.datatype.endswith('checkboxes'):
                    d_type = 'checkbox'
                else:
                    d_type = 'multiselect'
                if hasattr(field, 'extras') and ('minlength' in field.extras and 'minlength' in self.extras or ('maxlength' in field.extras and 'maxlength' in self.extras)):
                    checkbox_messages = dict()
                    if 'minlength' in field.extras and 'minlength' in self.extras and ('maxlength' in field.extras) and ('maxlength' in self.extras) and (self.extras['minlength'][field.number] == self.extras['maxlength'][field.number]) and (self.extras['minlength'][field.number] > 0):
                        if 'nota' not in self.extras:
                            self.extras['nota'] = dict()
                        self.extras['nota'][field.number] = False
                        if d_type == 'checkbox':
                            checkbox_messages['checkexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                        else:
                            checkbox_messages['selectexactly'] = field.validation_message(d_type + ' minmaxlength', self, word('Please select exactly %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    else:
                        if 'minlength' in field.extras and 'minlength' in self.extras:
                            if d_type == 'checkbox':
                                if self.extras['minlength'][field.number] == 1:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select one.'))
                                else:
                                    checkbox_messages['checkatleast'] = field.validation_message('checkbox minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                                if int(float(self.extras['minlength'][field.number])) > 0:
                                    if 'nota' not in self.extras:
                                        self.extras['nota'] = dict()
                                    self.extras['nota'][field.number] = False
                            elif self.extras['minlength'][field.number] == 1:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select one.'))
                            else:
                                checkbox_messages['minlength'] = field.validation_message(d_type + ' minlength', self, word('Please select at least %s.'), parameters=tuple([self.extras['minlength'][field.number]]))
                        if 'maxlength' in field.extras and 'maxlength' in self.extras:
                            if d_type == 'checkbox':
                                checkbox_messages['checkatmost'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                            else:
                                checkbox_messages['maxlength'] = field.validation_message(d_type + ' maxlength', self, word('Please select no more than %s.'), parameters=tuple([self.extras['maxlength'][field.number]]))
                    the_field['validation_messages'].update(checkbox_messages)
                if d_type == 'checkbox':
                    if hasattr(field, 'nota') and self.extras['nota'][field.number] is not False:
                        the_field['validation_messages']['checkatleast'] = field.validation_message('checkboxes required', self, word('Check at least one option, or check 闂%s闂'), parameters=tuple([self.extras['nota'][field.number]]))
            if field.datatype == 'date':
                the_field['validation_messages']['date'] = field.validation_message('date', self, word('You need to enter a valid date.'))
                if hasattr(field, 'extras') and 'min' in field.extras and ('min' in self.extras) and ('max' in field.extras) and ('max' in self.extras) and (field.number in self.extras['min']) and (field.number in self.extras['max']):
                    the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.extras['min'][field.number], format='medium'), docassemble.base.util.format_date(self.extras['max'][field.number], format='medium')))
                else:
                    was_defined = dict()
                    for key in ['min', 'max']:
                        if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                            was_defined[key] = True
                            if key == 'min':
                                the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                            elif key == 'max':
                                the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
                    if len(was_defined) == 0 and 'default date min' in self.question.interview.options and ('default date max' in self.question.interview.options):
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['minmax'] = field.validation_message('date minmax', self, word('You need to enter a date between %s and %s.'), parameters=(docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium'), docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')))
                    elif 'max' not in was_defined and 'default date max' in self.question.interview.options:
                        the_field['max'] = docassemble.base.util.format_date(self.question.interview.options['default date max'], format='yyyy-MM-dd')
                        the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date max'], format='medium')]))
                    elif 'min' not in was_defined and 'default date min' in self.question.interview.options:
                        the_field['min'] = docassemble.base.util.format_date(self.question.interview.options['default date min'], format='yyyy-MM-dd')
                        the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.question.interview.options['default date min'], format='medium')]))
            if field.datatype == 'time':
                the_field['validation_messages']['time'] = field.validation_message('time', self, word('You need to enter a valid time.'))
            if field.datatype in ['datetime', 'datetime-local']:
                the_field['validation_messages']['datetime'] = field.validation_message('datetime', self, word('You need to enter a valid date and time.'))
            if field.datatype == 'email':
                the_field['validation_messages']['email'] = field.validation_message('email', self, word('You need to enter a complete e-mail address.'))
            if field.datatype in ['number', 'currency', 'float', 'integer']:
                the_field['validation_messages']['number'] = field.validation_message('number', self, word('You need to enter a number.'))
                if field.datatype == 'integer' and (not ('step' in self.extras and field.number in self.extras['step'])):
                    the_field['validation_messages']['step'] = field.validation_message('integer', self, word('Please enter a whole number.'))
                elif 'step' in self.extras and field.number in self.extras['step']:
                    the_field['validation_messages']['step'] = field.validation_message('step', self, word('Please enter a multiple of {0}.'))
                for key in ['min', 'max']:
                    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
                        if key == 'min':
                            the_field['validation_messages'][key] = field.validation_message('min', self, word('You need to enter a number that is at least %s.'), parameters=tuple([self.extras[key][field.number]]))
                        elif key == 'max':
                            the_field['validation_messages'][key] = field.validation_message('max', self, word('You need to enter a number that is at most %s.'), parameters=tuple([self.extras[key][field.number]]))
            if field.datatype in ['files', 'file', 'camera', 'user', 'environment', 'camcorder', 'microphone']:
                file_fields.append(field)
                the_field['validation_messages']['required'] = field.validation_message('file required', self, word('You must provide a file.'))
                if 'accept' in self.extras and field.number in self.extras['accept']:
                    the_field['validation_messages']['accept'] = field.validation_message('accept', self, word('Please upload a file with a valid file format.'))
                if get_config('maximum content length') is not None:
                    the_field['max'] = get_config('maximum content length')
                    the_field['validation_messages']['max'] = field.validation_message('maxuploadsize', self, word('Your file upload is larger than the server can accept. Please reduce the size of your file upload.'))
        for param in ('datatype', 'fieldtype', 'sign', 'inputtype', 'address_autocomplete', 'label_above_field'):
            if hasattr(field, param):
                the_field[param] = getattr(field, param)
        if hasattr(field, 'shuffle') and field.shuffle is not False:
            the_field['shuffle'] = True
        if hasattr(field, 'disableothers') and field.disableothers and hasattr(field, 'saveas'):
            the_field['disable_others'] = True
        if hasattr(field, 'uncheckothers') and field.uncheckothers is not False:
            the_field['uncheck_others'] = True
        for key in ('minlength', 'maxlength', 'min', 'max', 'step', 'scale', 'inline', 'inline width', 'rows', 'accept', 'currency symbol', 'field metadata', 'css class'):
            if key in self.extras and field.number in self.extras[key]:
                if key in ('minlength', 'maxlength', 'min', 'max', 'step'):
                    validation_rules_used.add(key)
                the_field[key] = self.extras[key][field.number]
        if hasattr(field, 'extras') and 'custom_parameters' in field.extras:
            for (parameter, parameter_value) in field.extras['custom_parameters'].items():
                the_field[parameter] = parameter_value
        for param_type in ('custom_parameters_code', 'custom_parameters_mako'):
            if param_type in self.extras and field.number in self.extras[param_type]:
                for (parameter, parameter_value) in self.extras[param_type][field.number].items():
                    the_field[parameter] = parameter_value
        if hasattr(field, 'saveas') and field.saveas in self.embedded:
            the_field['embedded'] = True
        if hasattr(self, 'shuffle'):
            the_field['shuffle'] = self.shuffle
        if field.number in self.defaults:
            the_default = self.defaults[field.number]
            if isinstance(the_default, (str, int, bool, float)):
                the_field['default'] = the_default
        else:
            the_default = None
        if self.question.question_type == 'multiple_choice' or hasattr(field, 'choicetype') or (hasattr(field, 'datatype') and field.datatype in ('object', 'multiselect', 'object_multiselect', 'checkboxes', 'object_checkboxes', 'object_radio')):
            the_field['choices'] = self.get_choices_data(field, the_default, the_user_dict, encode=encode)
        if hasattr(field, 'nota'):
            the_field['none_of_the_above'] = docassemble.base.filter.markdown_to_html(self.extras['nota'][field.number], do_terms=False, status=self, verbatim=not encode)
        if field.number in self.extras['ok']:
            the_field['active'] = self.extras['ok'][field.number]
        else:
            the_field['active'] = True
        if field.number in self.extras['required']:
            the_field['required'] = self.extras['required'][field.number]
            if the_field['required']:
                validation_rules_used.add('required')
        if 'validation messages' in self.extras and field.number in self.extras['validation messages']:
            the_field['validation_messages'].update(self.extras['validation messages'][field.number])
        if 'permissions' in self.extras and field.number in self.extras['permissions']:
            the_field['permissions'] = self.extras['permissions'][field.number]
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('max_image_size' in self.extras) and self.extras['max_image_size']:
            the_field['max_image_size'] = self.extras['max_image_size']
        if hasattr(field, 'datatype') and field.datatype in ('file', 'files', 'camera', 'user', 'environment') and ('image_type' in self.extras) and self.extras['image_type']:
            the_field['image_type'] = self.extras['image_type']
        if hasattr(field, 'extras'):
            if 'ml_group' in field.extras or 'ml_train' in field.extras:","for key in ['min', 'max']:
    if hasattr(field, 'extras') and key in field.extras and (key in self.extras) and (field.number in self.extras[key]):
        was_defined[key] = True
        if key == 'min':
            the_field['validation_messages']['min'] = field.validation_message('date min', self, word('You need to enter a date on or after %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))
        elif key == 'max':
            the_field['validation_messages']['max'] = field.validation_message('date max', self, word('You need to enter a date on or before %s.'), parameters=tuple([docassemble.base.util.format_date(self.extras[key][field.number], format='medium')]))",XXX,no_found,0,,,
not-youtube-dl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/naver.py,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/naver.py,NaverBaseIE,_extract_video_info$21,"def _extract_video_info(self, video_id, vid, key):
    video_data = self._download_json('http://play.rmcnmv.naver.com/vod/play/v2.0/' + vid, video_id, query={'key': key})
    meta = video_data['meta']
    title = meta['subject']
    formats = []
    get_list = lambda x: try_get(video_data, lambda y: y[x + 's']['list'], list) or []

    def extract_formats(streams, stream_type, query={}):
        for stream in streams:
            stream_url = stream.get('source')
            if not stream_url:
                continue
            stream_url = update_url_query(stream_url, query)
            encoding_option = stream.get('encodingOption', {})
            bitrate = stream.get('bitrate', {})
            formats.append({'format_id': '%s_%s' % (stream.get('type') or stream_type, dict_get(encoding_option, ('name', 'id'))), 'url': stream_url, 'width': int_or_none(encoding_option.get('width')), 'height': int_or_none(encoding_option.get('height')), 'vbr': int_or_none(bitrate.get('video')), 'abr': int_or_none(bitrate.get('audio')), 'filesize': int_or_none(stream.get('size')), 'protocol': 'm3u8_native' if stream_type == 'HLS' else None})
    extract_formats(get_list('video'), 'H264')
    for stream_set in video_data.get('streams', []):
        query = {}
        for param in stream_set.get('keys', []):
            query[param['name']] = param['value']
        stream_type = stream_set.get('type')
        videos = stream_set.get('videos')
        if videos:
            extract_formats(videos, stream_type, query)
        elif stream_type == 'HLS':
            stream_url = stream_set.get('source')
            if not stream_url:
                continue
            formats.extend(self._extract_m3u8_formats(update_url_query(stream_url, query), video_id, 'mp4', 'm3u8_native', m3u8_id=stream_type, fatal=False))
    self._sort_formats(formats)
    replace_ext = lambda x, y: re.sub(self._CAPTION_EXT_RE, '.' + y, x)

    def get_subs(caption_url):
        if re.search(self._CAPTION_EXT_RE, caption_url):
            return [{'url': replace_ext(caption_url, 'ttml')}, {'url': replace_ext(caption_url, 'vtt')}]
        else:
            return [{'url': caption_url}]
    automatic_captions = {}
    subtitles = {}
    for caption in get_list('caption'):
        caption_url = caption.get('source')
        if not caption_url:
            continue
        sub_dict = automatic_captions if caption.get('type') == 'auto' else subtitles
        sub_dict.setdefault(dict_get(caption, ('locale', 'language')), []).extend(get_subs(caption_url))
    user = meta.get('user', {})
    return {'id': video_id, 'title': title, 'formats': formats, 'subtitles': subtitles, 'automatic_captions': automatic_captions, 'thumbnail': try_get(meta, lambda x: x['cover']['source']), 'view_count': int_or_none(meta.get('count')), 'uploader_id': user.get('id'), 'uploader': user.get('name'), 'uploader_url': user.get('url')}","for param in stream_set.get('keys', []):
    query[param['name']] = param['value']",XXX,no_found,,,,
networkx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/networkx/networkx/algorithms/isomorphism/ismags.py,https://github.com/networkx/networkx/tree/master/networkx/algorithms/isomorphism/ismags.py,ISMAGS,_get_lookahead_candidates$508,"def _get_lookahead_candidates(self):
    """"""
        Returns a mapping of {subgraph node: collection of graph nodes} for
        which the graph nodes are feasible candidates for the subgraph node, as
        determined by looking ahead one edge.
        """"""
    g_counts = {}
    for gn in self.graph:
        g_counts[gn] = self._find_neighbor_color_count(self.graph, gn, self._gn_colors, self._ge_colors)
    candidates = defaultdict(set)
    for sgn in self.subgraph:
        sg_count = self._find_neighbor_color_count(self.subgraph, sgn, self._sgn_colors, self._sge_colors)
        new_sg_count = Counter()
        for ((sge_color, sgn_color), count) in sg_count.items():
            try:
                ge_color = self._edge_compatibility[sge_color]
                gn_color = self._node_compatibility[sgn_color]
            except KeyError:
                pass
            else:
                new_sg_count[ge_color, gn_color] = count
        for (gn, g_count) in g_counts.items():
            if all((new_sg_count[x] <= g_count[x] for x in new_sg_count)):
                candidates[sgn].add(gn)
    return candidates","for gn in self.graph:
    g_counts[gn] = self._find_neighbor_color_count(self.graph, gn, self._gn_colors, self._ge_colors)",XXX,no_found,0,,,
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/torchie/apis/train.py,https://github.com/poodarchu/Det3D/tree/master/det3d/torchie/apis/train.py,,example_convert_to_torch$24,"def example_convert_to_torch(example, dtype=torch.float32, device=None) -> dict:
    assert device is not None
    example_torch = {}
    float_names = ['voxels', 'bev_map']
    for (k, v) in example.items():
        if k in ['anchors', 'reg_targets', 'reg_weights']:
            res = []
            for (kk, vv) in v.items():
                res.append(torch.tensor(vv).to(device, non_blocking=True))
            example_torch[k] = res
        elif k in float_names:
            example_torch[k] = v.to(device, non_blocking=True)
        elif k in ['coordinates', 'num_points']:
            example_torch[k] = v.to(device, non_blocking=True)
        elif k == 'labels':
            res = []
            for (kk, vv) in v.items():
                res.append(torch.tensor(vv).to(device, non_blocking=True))
            example_torch[k] = res
        elif k == 'points':
            example_torch[k] = v.to(device, non_blocking=True)
        elif k in ['anchors_mask']:
            res = []
            for (kk, vv) in v.items():
                res.append(torch.tensor(vv).to(device, non_blocking=True))
            example_torch[k] = res
        elif k == 'calib':
            calib = {}
            for (k1, v1) in v.items():
                calib[k1] = torch.tensor(v1).to(device, non_blocking=True)
            example_torch[k] = calib
        elif k == 'num_voxels':
            example_torch[k] = v.to(device, non_blocking=True)
        else:
            example_torch[k] = v
    return example_torch","for (k, v) in example.items():
    if k in ['anchors', 'reg_targets', 'reg_weights']:
        res = []
        for (kk, vv) in v.items():
            res.append(torch.tensor(vv).to(device, non_blocking=True))
        example_torch[k] = res
    elif k in float_names:
        example_torch[k] = v.to(device, non_blocking=True)
    elif k in ['coordinates', 'num_points']:
        example_torch[k] = v.to(device, non_blocking=True)
    elif k == 'labels':
        res = []
        for (kk, vv) in v.items():
            res.append(torch.tensor(vv).to(device, non_blocking=True))
        example_torch[k] = res
    elif k == 'points':
        example_torch[k] = v.to(device, non_blocking=True)
    elif k in ['anchors_mask']:
        res = []
        for (kk, vv) in v.items():
            res.append(torch.tensor(vv).to(device, non_blocking=True))
        example_torch[k] = res
    elif k == 'calib':
        calib = {}
        for (k1, v1) in v.items():
            calib[k1] = torch.tensor(v1).to(device, non_blocking=True)
        example_torch[k] = calib
    elif k == 'num_voxels':
        example_torch[k] = v.to(device, non_blocking=True)
    else:
        example_torch[k] = v",XXX,no_found,,,,
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/torchie/apis/train.py,https://github.com/poodarchu/Det3D/tree/master/det3d/torchie/apis/train.py,,example_convert_to_torch$24,"def example_convert_to_torch(example, dtype=torch.float32, device=None) -> dict:
    assert device is not None
    example_torch = {}
    float_names = ['voxels', 'bev_map']
    for (k, v) in example.items():
        if k in ['anchors', 'reg_targets', 'reg_weights']:
            res = []
            for (kk, vv) in v.items():
                res.append(torch.tensor(vv).to(device, non_blocking=True))
            example_torch[k] = res
        elif k in float_names:
            example_torch[k] = v.to(device, non_blocking=True)
        elif k in ['coordinates', 'num_points']:
            example_torch[k] = v.to(device, non_blocking=True)
        elif k == 'labels':
            res = []
            for (kk, vv) in v.items():
                res.append(torch.tensor(vv).to(device, non_blocking=True))
            example_torch[k] = res
        elif k == 'points':
            example_torch[k] = v.to(device, non_blocking=True)
        elif k in ['anchors_mask']:
            res = []
            for (kk, vv) in v.items():
                res.append(torch.tensor(vv).to(device, non_blocking=True))
            example_torch[k] = res
        elif k == 'calib':
            calib = {}
            for (k1, v1) in v.items():
                calib[k1] = torch.tensor(v1).to(device, non_blocking=True)
            example_torch[k] = calib
        elif k == 'num_voxels':
            example_torch[k] = v.to(device, non_blocking=True)
        else:
            example_torch[k] = v
    return example_torch","for (k1, v1) in v.items():
    calib[k1] = torch.tensor(v1).to(device, non_blocking=True)",XXX,no_found,0,,,
BigGAN-PyTorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BigGAN-PyTorch/TFHub/converter.py,https://github.com/ajbrock/BigGAN-PyTorch/tree/master/TFHub/converter.py,,convert_from_v1$210,"def convert_from_v1(hub_dict, resolution=128):
    weightname_dict = {'weight_u': 'u0', 'weight_bar': 'weight', 'bias': 'bias'}
    convnum_dict = {'conv0': 'conv1', 'conv1': 'conv2', 'conv_sc': 'conv_sc'}
    attention_blocknum = {128: 3, 256: 4, 512: 3}[resolution]
    hub2me = {'linear.weight': 'shared.weight', 'G_linear.module.weight_bar': 'linear.weight', 'G_linear.module.bias': 'linear.bias', 'G_linear.module.weight_u': 'linear.u0', 'ScaledCrossReplicaBN.weight': 'output_layer.0.gain', 'ScaledCrossReplicaBN.bias': 'output_layer.0.bias', 'ScaledCrossReplicaBN.running_mean': 'output_layer.0.stored_mean', 'ScaledCrossReplicaBN.running_var': 'output_layer.0.stored_var', 'colorize.module.weight_bar': 'output_layer.2.weight', 'colorize.module.bias': 'output_layer.2.bias', 'colorize.module.weight_u': 'output_layer.2.u0', 'attention.gamma': 'blocks.%d.1.gamma' % attention_blocknum, 'attention.theta.module.weight_u': 'blocks.%d.1.theta.u0' % attention_blocknum, 'attention.theta.module.weight_bar': 'blocks.%d.1.theta.weight' % attention_blocknum, 'attention.phi.module.weight_u': 'blocks.%d.1.phi.u0' % attention_blocknum, 'attention.phi.module.weight_bar': 'blocks.%d.1.phi.weight' % attention_blocknum, 'attention.g.module.weight_u': 'blocks.%d.1.g.u0' % attention_blocknum, 'attention.g.module.weight_bar': 'blocks.%d.1.g.weight' % attention_blocknum, 'attention.o_conv.module.weight_u': 'blocks.%d.1.o.u0' % attention_blocknum, 'attention.o_conv.module.weight_bar': 'blocks.%d.1.o.weight' % attention_blocknum}
    for name in hub_dict.keys():
        if 'GBlock' in name:
            if 'HyperBN' not in name:
                out = parse.parse('GBlock.{:d}.{}.module.{}', name)
                (blocknum, convnum, weightname) = out
                if weightname not in weightname_dict:
                    continue
                out_name = 'blocks.%d.0.%s.%s' % (blocknum, convnum_dict[convnum], weightname_dict[weightname])
            else:
                BNnum = 2 if 'HyperBN_1' in name else 1
                if 'embed' in name:
                    out = parse.parse('GBlock.{:d}.{}.module.{}', name)
                    (blocknum, gamma_or_beta, weightname) = out
                    if weightname not in weightname_dict:
                        continue
                    out_name = 'blocks.%d.0.bn%d.%s.%s' % (blocknum, BNnum, 'gain' if 'gamma' in gamma_or_beta else 'bias', weightname_dict[weightname])
                else:
                    out = parse.parse('GBlock.{:d}.{}.bn.{}', name)
                    (blocknum, dummy, mean_or_var) = out
                    if 'num_batches_tracked' in mean_or_var:
                        continue
                    out_name = 'blocks.%d.0.bn%d.%s' % (blocknum, BNnum, 'stored_mean' if 'mean' in mean_or_var else 'stored_var')
            hub2me[name] = out_name
    me2hub = {hub2me[item]: item for item in hub2me}
    new_dict = {}
    dimz_dict = {128: 20, 256: 20, 512: 16}
    for item in me2hub:
        if ('bn' in item and 'weight' in item) and ('gain' in item or 'bias' in item) and ('output_layer' not in item):
            new_dict[item] = torch.cat([hub_dict[me2hub[item]][:, -128:], hub_dict[me2hub[item]][:, :dimz_dict[resolution]]], 1)
        elif item == 'linear.weight':
            new_dict[item] = hub_dict[me2hub[item]].contiguous().view(4, 4, 96 * 16, -1).permute(2, 0, 1, 3).contiguous().view(-1, dimz_dict[resolution])
        elif item == 'linear.bias':
            new_dict[item] = hub_dict[me2hub[item]].view(4, 4, 96 * 16).permute(2, 0, 1).contiguous().view(-1)
        elif item == 'linear.u0':
            new_dict[item] = hub_dict[me2hub[item]].view(4, 4, 96 * 16).permute(2, 0, 1).contiguous().view(1, -1)
        elif me2hub[item] == 'linear.weight':
            new_dict[item] = hub_dict[me2hub[item]].t()
        elif 'weight_u' in me2hub[item]:
            new_dict[item] = hub_dict[me2hub[item]].unsqueeze(0)
        else:
            new_dict[item] = hub_dict[me2hub[item]]
    return new_dict","for item in me2hub:
    if ('bn' in item and 'weight' in item) and ('gain' in item or 'bias' in item) and ('output_layer' not in item):
        new_dict[item] = torch.cat([hub_dict[me2hub[item]][:, -128:], hub_dict[me2hub[item]][:, :dimz_dict[resolution]]], 1)
    elif item == 'linear.weight':
        new_dict[item] = hub_dict[me2hub[item]].contiguous().view(4, 4, 96 * 16, -1).permute(2, 0, 1, 3).contiguous().view(-1, dimz_dict[resolution])
    elif item == 'linear.bias':
        new_dict[item] = hub_dict[me2hub[item]].view(4, 4, 96 * 16).permute(2, 0, 1).contiguous().view(-1)
    elif item == 'linear.u0':
        new_dict[item] = hub_dict[me2hub[item]].view(4, 4, 96 * 16).permute(2, 0, 1).contiguous().view(1, -1)
    elif me2hub[item] == 'linear.weight':
        new_dict[item] = hub_dict[me2hub[item]].t()
    elif 'weight_u' in me2hub[item]:
        new_dict[item] = hub_dict[me2hub[item]].unsqueeze(0)
    else:
        new_dict[item] = hub_dict[me2hub[item]]",XXX,no_found,,,,
qutip,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutip/qutip/qip/qasm.py,https://github.com/qutip/qutip/tree/master/qutip/qip/qasm.py,QasmProcessor,_custom_gate$320,"def _custom_gate(self, qc_temp, gate_call):
    """"""
        Recursively process a custom-defined gate with specified arguments
        to produce a dummy circuit with all the gates in the custom-defined
        gate.

        Parameters
        ----------

        qc_temp: :class:`.QubitCircuit`
            temporary circuit to process custom gate
        gate_call: list of str
            tokens corresponding to gate signature/call
        """"""
    (gate_name, args, regs) = gate_call
    gate = self.qasm_gates[gate_name]
    args_map = {}
    regs_map = {}
    for (i, arg) in enumerate(gate.gate_args):
        args_map[arg] = eval(str(args[i]))
    for (i, reg) in enumerate(gate.gate_regs):
        regs_map[reg] = regs[i]
    for call in gate.gates_inside:
        (name, com_args, com_regs) = call
        for (arg, real_arg) in args_map.items():
            com_args = [command.replace(arg.strip(), str(real_arg)) for command in com_args]
        for (reg, real_reg) in regs_map.items():
            com_regs = [command.replace(reg.strip(), str(real_reg)) for command in com_regs]
        com_args = [eval(arg) for arg in com_args]
        if name in self.predefined_gates:
            qc_temp.user_gates = _get_qiskit_gates()
            com_regs = [int(reg) for reg in com_regs]
            self._add_predefined_gates(qc_temp, name, com_regs, com_args)
        else:
            self._custom_gate(qc_temp, [name, com_args, com_regs])","for (i, arg) in enumerate(gate.gate_args):
    args_map[arg] = eval(str(args[i]))",XXX,no_found,1,,,
django-wiki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-wiki/src/wiki/forms.py,https://github.com/django-wiki/django-wiki/tree/master/src/wiki/forms.py,EditForm,__init__$244,"def __init__(self, request, current_revision, *args, **kwargs):
    self.request = request
    self.no_clean = kwargs.pop('no_clean', False)
    self.preview = kwargs.pop('preview', False)
    self.initial_revision = current_revision
    self.presumed_revision = None
    if current_revision:
        provided_content = True
        content = kwargs.pop('content', None)
        if content is None:
            provided_content = False
            content = current_revision.content
        initial = {'content': content, 'title': current_revision.title, 'current_revision': current_revision.id}
        initial.update(kwargs.get('initial', {}))
        data = None
        if len(args) > 0:
            data = args[0]
            args = args[1:]
        if data is None:
            data = kwargs.get('data', None)
        if data:
            self.presumed_revision = data.get('current_revision', None)
            if not str(self.presumed_revision) == str(self.initial_revision.id):
                newdata = {}
                for (k, v) in data.items():
                    newdata[k] = v
                newdata['current_revision'] = self.initial_revision.id
                if provided_content:
                    self.presumed_revision = self.initial_revision.id
                else:
                    newdata['content'] = simple_merge(content, data.get('content', ''))
                newdata['title'] = current_revision.title
                kwargs['data'] = newdata
            else:
                kwargs['data'] = data
        kwargs['initial'] = initial
    super().__init__(*args, **kwargs)","for (k, v) in data.items():
    newdata[k] = v",XXX,no_found,0,,,
gaphor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gaphor/gaphor/UML/profiles/stereotypepropertypages.py,https://github.com/gaphor/gaphor/tree/master/gaphor/UML/profiles/stereotypepropertypages.py,,refresh$76,"def refresh(subject, model):
    stereotypes = UML.model.get_stereotypes(subject)
    instances = subject.appliedStereotype

    def upsert(path, parent, row_data):
        try:
            new_row = model.get_iter(path)
        except ValueError:
            new_row = model.append(parent, row_data)
        else:
            row = model[path]
            row[:] = row_data
        return new_row
    slots = {}
    for applied in instances:
        for slot in applied.slot:
            slots[slot.definingFeature] = slot
    for (st_index, st) in enumerate(stereotypes):
        for applied in instances:
            if st in applied.classifier:
                break
        else:
            applied = None
        parent = upsert(f'{st_index}', None, (st.name, '', bool(applied), True, False, st, None, None))
        for (attr_index, attr) in enumerate((attr for attr in st.ownedAttribute if not attr.association)):
            slot = slots.get(attr)
            value = slot.value if slot else ''
            upsert(f'{st_index}:{attr_index}', parent, (attr.name, value, bool(applied), False, bool(applied), attr, applied, slot))","for applied in instances:
    for slot in applied.slot:
        slots[slot.definingFeature] = slot",XXX,no_found,,,,
zato,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zato/code/zato-common/src/zato/common/ext/configobj_.py,https://github.com/zatosource/zato/tree/master/code/zato-common/src/zato/common/ext/configobj_.py,Builder,build_Dict$190,"def build_Dict(self, o):
    d = {}
    i = iter(map(self.build, o.getChildren()))
    for el in i:
        d[el] = next(i)
    return d","for el in i:
    d[el] = next(i)",XXX,no_found,,,,
model-analysis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/model-analysis/tensorflow_model_analysis/api/verifier_lib.py,https://github.com/tensorflow/model-analysis/tree/master/tensorflow_model_analysis/api/verifier_lib.py,,Validate$26,"def Validate(extracts: beam.pvalue.PCollection, alternatives: Dict[str, beam.PTransform], validators: List[validator.Validator]) -> validator.Validation:
    """"""Performs validation of alternative evaluations.

  Args:
    extracts: PCollection of extracts.
    alternatives: Dict of PTransforms (Extracts -> Evaluation) whose output will
      be compared for validation purposes (e.g. 'baseline' vs 'candidate').
    validators: List of validators for validating the output from running the
      alternatives. The Validation outputs produced by the validators will be
      merged into a single output. If there are overlapping output keys, later
      outputs will replace earlier outputs sharing the same key.

  Returns:
    Validation dict.
  """"""
    evaluations = {}
    for key in alternatives:
        evaluations[key] = extracts | 'Evaluate(%s)' % key >> alternatives[key]
    validation = {}
    for v in validators:
        validation.update(evaluations | v.stage_name >> v.ptransform)
    return validation","for key in alternatives:
    evaluations[key] = extracts | 'Evaluate(%s)' % key >> alternatives[key]",XXX,no_found,0,,,
mmediting,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmediting/mmedit/models/restorers/tdan.py,https://github.com/open-mmlab/mmediting/tree/master/mmedit/models/restorers/tdan.py,TDAN,evaluate$71,"def evaluate(self, output, gt):
    """"""Evaluation function.

        Args:
            output (Tensor): Model output with shape (n, c, h, w).
            gt (Tensor): GT Tensor with shape (n, c, h, w).

        Returns:
            dict: Evaluation results.
        """"""
    crop_border = self.test_cfg.crop_border
    convert_to = self.test_cfg.get('convert_to', None)
    output = tensor2img(output)
    gt = tensor2img(gt)
    eval_result = dict()
    for metric in self.test_cfg.metrics:
        eval_result[metric] = self.allowed_metrics[metric](output, gt, crop_border, convert_to=convert_to)
    return eval_result","for metric in self.test_cfg.metrics:
    eval_result[metric] = self.allowed_metrics[metric](output, gt, crop_border, convert_to=convert_to)",XXX,no_found,0,,,
flax,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flax/examples/wmt/input_pipeline.py,https://github.com/google/flax/tree/master/examples/wmt/input_pipeline.py,,_pack_with_tf_ops$149,"def _pack_with_tf_ops(dataset: tf.data.Dataset, keys: List[str], key2length: Dict[str, int]) -> tf.data.Dataset:
    """"""Helper-function for packing a dataset which has already been batched.

  Helper for pack_dataset()  Uses tf.while_loop.

  Args:
    dataset: a dataset containing padded batches of examples.
    keys: a list of strings
    key2length: an dict from feature-key to integer

  Returns:
    a dataset.
  """"""
    empty_example = {}
    for k in keys:
        empty_example[k] = tf.zeros([0], dtype=tf.int32)
        empty_example[k + '_position'] = tf.zeros([0], dtype=tf.int32)
    keys_etc = empty_example.keys()

    def write_packed_example(partial, outputs):
        new_partial = empty_example.copy()
        new_outputs = {}
        for k in keys_etc:
            new_outputs[k] = outputs[k].write(outputs[k].size(), tf.pad(partial[k], [[0, key2length[k] - tf.size(partial[k])]]))
        return (new_partial, new_outputs)

    def map_fn(x):
        """"""Internal function to flat_map over.

    Consumes a batch of input examples and produces a variable number of output
    examples.
    Args:
      x: a single example

    Returns:
      a tf.data.Dataset
    """"""
        partial = empty_example.copy()
        i = tf.zeros([], dtype=tf.int32)
        dynamic_batch_size = tf.shape(x[keys[0]])[0]
        outputs = {}
        for k in keys:
            outputs[k] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])
            outputs[k + '_position'] = tf.TensorArray(tf.int32, size=0, dynamic_size=True, element_shape=[key2length[k]])

        def body_fn(i, partial, outputs):
            """"""Body function for while_loop.

      Args:
        i: integer scalar
        partial: dictionary of Tensor (partially-constructed example)
        outputs: dictionary of TensorArray

      Returns:
        A triple containing the new values of the inputs.
      """"""
            can_append = True
            one_example = {}
            for k in keys:
                val = tf.cast(x[k][i], tf.int32)
                val = val[:tf.reduce_sum(tf.cast(tf.not_equal(val, 0), tf.int32))]
                one_example[k] = val
            for k in keys:
                can_append = tf.logical_and(can_append, tf.less_equal(tf.size(partial[k]) + tf.size(one_example[k]), key2length[k]))

            def false_fn():
                return write_packed_example(partial, outputs)

            def true_fn():
                return (partial, outputs)
            (partial, outputs) = tf.cond(can_append, true_fn, false_fn)
            new_partial = {}
            for k in keys:
                new_seq = one_example[k][:key2length[k]]
                new_seq_len = tf.size(new_seq)
                new_partial[k] = tf.concat([partial[k], new_seq], 0)
                new_partial[k + '_position'] = tf.concat([partial[k + '_position'], tf.range(new_seq_len)], 0)
            partial = new_partial
            return (i + 1, partial, outputs)
        (i, partial, outputs) = tf.while_loop(cond=lambda *_: True, body=body_fn, loop_vars=(i, partial, outputs), shape_invariants=(tf.TensorShape([]), {k: tf.TensorShape([None]) for k in keys_etc}, {k: tf.TensorShape(None) for k in keys_etc}), maximum_iterations=dynamic_batch_size)
        (_, outputs) = write_packed_example(partial, outputs)
        packed = {k: outputs[k].stack() for k in keys_etc}
        for k in keys:
            packed[k + '_segmentation'] = tf.cumsum(tf.cast(tf.equal(packed[k + '_position'], 0), tf.int32), axis=1) * tf.cast(tf.not_equal(packed[k], 0), tf.int32)
        return packed
    dataset = dataset.map(map_fn, num_parallel_calls=AUTOTUNE)
    return dataset.unbatch()","for k in keys_etc:
    new_outputs[k] = outputs[k].write(outputs[k].size(), tf.pad(partial[k], [[0, key2length[k] - tf.size(partial[k])]]))",XXX,no_found,0,,,
ParlAI,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/tasks/light_genderation_bias/agents.py,https://github.com/facebookresearch/ParlAI/tree/master/parlai/tasks/light_genderation_bias/agents.py,,read_gender_tsv$93,"def read_gender_tsv(path, remove_verbs=True):
    """"""
    Load TSV of gendered word lists and return a dict.
    """"""
    gender_dct = {}
    with PathManager.open(path) as tsvfile:
        reader = list(csv.reader(tsvfile, delimiter='\t'))
        title_lst = reader[0]
        title_dict = {}
        for (idx, title) in enumerate(title_lst):
            title_dict[idx] = title
        for i in range(1, len(reader)):
            row = reader[i]
            word = row[0].lower()
            gender_dct[word] = {}
            for (j, category) in enumerate(row[1:]):
                gender_dct[word][title_dict[j + 1]] = category
    if remove_verbs:
        return {k: v for (k, v) in gender_dct.items() if v['syncategory'] != 'verb'}
    return gender_dct","for i in range(1, len(reader)):
    row = reader[i]
    word = row[0].lower()
    gender_dct[word] = {}
    for (j, category) in enumerate(row[1:]):
        gender_dct[word][title_dict[j + 1]] = category","gender_dct = {row[0].lower(): {title_dict[j + 1]: category for (j, category) in enumerate(row[1:])} for row in reader[1:]}","['gender_dct = {row[0].lower(): {title_dict[j + 1]: category for (j, category) in enumerate(row[1:])} for (i, row) in enumerate(reader) if i > 0}', 'Cannot refactor']",-1,,,
goatools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/goatools/goatools/grouper/aart_geneproducts_one.py,https://github.com/tanghaibao/goatools/tree/master/goatools/grouper/aart_geneproducts_one.py,_Init,get_go2chrs$184,"def get_go2chrs(sec2gos, sec2chr):
    """"""Dict: given a GO return a set of letters representing it's section membership(s).""""""
    go2chrs = {}
    for (goid, sections) in get_b2aset(sec2gos).items():
        go2chrs[goid] = set((sec2chr[s] for s in sections))
    return go2chrs","for (goid, sections) in get_b2aset(sec2gos).items():
    go2chrs[goid] = set((sec2chr[s] for s in sections))","go2chrs = {goid: {sec2chr[s] for s in sections} for (goid, sections) in get_b2aset(sec2gos).items()}","['go2chrs = {goid: set((sec2chr[s] for s in sections)) for (goid, sections) in get_b2aset(sec2gos).items()}']",-1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/cloud/clouds/parallels.py,https://github.com/saltstack/salt/tree/master/salt/cloud/clouds/parallels.py,,show_instance$468,"def show_instance(name, call=None):
    """"""
    Show the details from Parallels concerning an instance
    """"""
    if call != 'action':
        raise SaltCloudSystemExit('The show_instance action must be called with -a or --action.')
    items = query(action='ve', command=name)
    ret = {}
    for item in items:
        if 'text' in item.__dict__:
            ret[item.tag] = item.text
        else:
            ret[item.tag] = item.attrib
        if item._children:
            ret[item.tag] = {}
            children = item._children
            for child in children:
                ret[item.tag][child.tag] = child.attrib
    __utils__['cloud.cache_node'](ret, _get_active_provider_name(), __opts__)
    return ret","for item in items:
    if 'text' in item.__dict__:
        ret[item.tag] = item.text
    else:
        ret[item.tag] = item.attrib
    if item._children:
        ret[item.tag] = {}
        children = item._children
        for child in children:
            ret[item.tag][child.tag] = child.attrib","ret = {item.tag: item.text if 'text' in item.__dict__ else item.attrib for item in items}
for item in items:
    if item._children:
        ret[item.tag] = {child.tag: child.attrib for child in item._children}",['actually can be refactored'],-1,,,
horizon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/horizon/openstack_dashboard/test/helpers.py,https://github.com/openstack/horizon/tree/master/openstack_dashboard/test/helpers.py,,_apply_panel_mocks$143,"def _apply_panel_mocks(patchers=None):
    """"""Global mocks on panels that get called on all views.""""""
    if patchers is None:
        patchers = {}
    mocked_methods = settings.TEST_GLOBAL_MOCKS_ON_PANELS
    for (name, mock_config) in mocked_methods.items():
        method = mock_config['method']
        mock_params = {}
        for param in ['return_value', 'side_effect']:
            if param in mock_config:
                mock_params[param] = mock_config[param]
        patcher = mock.patch(method, **mock_params)
        patcher.start()
        patchers[name] = patcher
    return patchers","for (name, mock_config) in mocked_methods.items():
    method = mock_config['method']
    mock_params = {}
    for param in ['return_value', 'side_effect']:
        if param in mock_config:
            mock_params[param] = mock_config[param]
    patcher = mock.patch(method, **mock_params)
    patcher.start()
    patchers[name] = patcher","patchers = {name: mock.patch(mock_config['method'], **{param: mock_config[param] for param in ['return_value', 'side_effect'] if param in mock_config}).start() for (name, mock_config) in settings.TEST_GLOBAL_MOCKS_ON_PANELS.items()}",['actually can be refactored'],-1,,,
Jarvis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Jarvis/jarviscli/plugins/news.py,https://github.com/sukeesh/Jarvis/tree/master/jarviscli/plugins/news.py,News,remove_source$133,"def remove_source(self, jarvis):
    """"""
            removes a new source from the news channel of the user
        """"""
    sources = self.get_news_sources(jarvis)
    dic = {}
    for source in sources:
        dic[str(sources.index(source) + 1)] = source
    for index in sorted([int(x) for x in dic.keys()]):
        jarvis.say(str(index) + ' : ' + dic[str(index)])
    index_list = jarvis.input('Type the indexes of the sources you would like to remove from your channel separated by space: ')
    index_list = index_list.split(' ')
    if ' ' in index_list:
        index_list.remove(' ')
    if '' in index_list:
        index_list.remove('')
    for index in index_list:
        if str(index) in dic:
            source = dic[str(index)]
            sources.remove(source)
            jarvis.update_data('news-sources', sources)
            jarvis.say(source + ' has been successfully removed from your news channel!', Fore.GREEN)
        else:
            jarvis.say('Index not found!', Fore.RED)
    return self.get_news_sources(jarvis)","for source in sources:
    dic[str(sources.index(source) + 1)] = source","dic = {str(index + 1): source for (index, source) in enumerate(sources)}",['dic = {str(sources.index(source) + 1): source for source in sources}'],-1,,,
data-validation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-validation/tensorflow_data_validation/utils/display_util.py,https://github.com/tensorflow/data-validation/tree/master/tensorflow_data_validation/utils/display_util.py,,get_natural_language_statistics_dataframes$458,"def get_natural_language_statistics_dataframes(lhs_statistics: statistics_pb2.DatasetFeatureStatisticsList, rhs_statistics: Optional[statistics_pb2.DatasetFeatureStatisticsList]=None, lhs_name: Text='lhs_statistics', rhs_name: Text='rhs_statistics', allowlist_features: Optional[List[types.FeaturePath]]=None, denylist_features: Optional[List[types.FeaturePath]]=None) -> Optional[Dict[str, Dict[Union[int, str], Union[Dict[str, pd.DataFrame], pd.DataFrame]]]]:
    """"""Gets the `NaturalLanguageStatistics` as a dict of pandas.DataFrame.

  Each pd.DataFrame can be fed into a plot with little to no manipulation.

  For example, to plot the `token_length_histogram` in plot.ly:
  ```
  import pandas a pd
  import plotly
  import tensorflow_data_validation as tfdv
  from tensorflow_data_validation.utils import display_util as tfdv_display_util

  data = pd.DataFrame.from_dict({""col"": [1, 2, 3]})
  statistics = tfdv.generate_statistics_from_dataframe(data)

  df = tfdv_display_util.get_natural_language_statistics_dataframes(statistics)
  hist, bin_edges = np.histogram(df[ds_name][feature_name][
                      'token_length_histogram']['high_values'])
  fig = plotly.graph_objs.Figure(data=[
      plotly.graph_objs.Bar(x=bin_edges, y=hist, name='Histogram'),
  ])
  ```

  The resulting dict contains `token_length_histogram` and each token name as
  its keys. For each token, the data frame represents a list of stats as well
  as the token's positions histogram.

  Args:
    lhs_statistics: A DatasetFeatureStatisticsList protocol buffer.
    rhs_statistics: An optional DatasetFeatureStatisticsList protocol buffer to
      compare with lhs_statistics.
    lhs_name: Name of the lhs_statistics dataset.
    rhs_name: Name of the rhs_statistics dataset.
    allowlist_features: Set of features to be visualized.
    denylist_features: Set of features to ignore for visualization.

  Returns:
    A dict of pandas data frames. Returns None if natural language statistics
    does not exist in the statistics proto.
  """"""
    combined_statistics = _get_combined_statistics(lhs_statistics, rhs_statistics, lhs_name, rhs_name, allowlist_features, denylist_features)
    nlp_stats = _get_natural_language_statistics(combined_statistics)
    if not nlp_stats:
        return None
    result = {}
    for (ds_name, features_dict) in nlp_stats.items():
        result[ds_name] = {}
        for (feature_name, nlp_stat) in features_dict.items():
            result[ds_name][feature_name] = {'token_length_histogram': _get_histogram_dataframe(nlp_stat.token_length_histogram), 'token_statistics': _get_token_statistics(list(nlp_stat.token_statistics))}
    return result","for (ds_name, features_dict) in nlp_stats.items():
    result[ds_name] = {}
    for (feature_name, nlp_stat) in features_dict.items():
        result[ds_name][feature_name] = {'token_length_histogram': _get_histogram_dataframe(nlp_stat.token_length_histogram), 'token_statistics': _get_token_statistics(list(nlp_stat.token_statistics))}","result = {ds_name: {feature_name: {'token_length_histogram': _get_histogram_dataframe(nlp_stat.token_length_histogram), 'token_statistics': _get_token_statistics(list(nlp_stat.token_statistics)))} for ds_name, features_dict in nlp_stats.items() for feature_name, nlp_stat in features_dict.items()}","[""result = {ds_name: {feature_name: {'token_length_histogram': _get_histogram_dataframe(nlp_stat.token_length_histogram), 'token_statistics': _get_token_statistics(list(nlp_stat.token_statistics))} for (feature_name, nlp_stat) in features_dict.items()} for (ds_name, features_dict) in nlp_stats.items()}"", 'Cannot refactor']",-1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/glusterfs.py,https://github.com/saltstack/salt/tree/master/salt/modules/glusterfs.py,,info$400,"def info(name=None):
    """"""
    .. versionadded:: 2015.8.4

    Return gluster volume info.

    name
        Optional name to retrieve only information of one volume

    CLI Example:

    .. code-block:: bash

        salt '*' glusterfs.info
    """"""
    cmd = 'volume info'
    if name is not None:
        cmd += ' ' + name
    root = _gluster_xml(cmd)
    if not _gluster_ok(root):
        return None
    ret = {}
    for volume in _iter(root, 'volume'):
        name = volume.find('name').text
        ret[name] = _etree_to_dict(volume)
        bricks = {}
        for (i, brick) in enumerate(_iter(volume, 'brick'), start=1):
            brickkey = 'brick{}'.format(i)
            bricks[brickkey] = {'path': brick.text}
            for child in brick:
                if not child.tag == 'name':
                    bricks[brickkey].update({child.tag: child.text})
            for (k, v) in brick.items():
                bricks[brickkey][k] = v
        ret[name]['bricks'] = bricks
        options = {}
        for option in _iter(volume, 'option'):
            options[option.find('name').text] = option.find('value').text
        ret[name]['options'] = options
    return ret","for volume in _iter(root, 'volume'):
    name = volume.find('name').text
    ret[name] = _etree_to_dict(volume)
    bricks = {}
    for (i, brick) in enumerate(_iter(volume, 'brick'), start=1):
        brickkey = 'brick{}'.format(i)
        bricks[brickkey] = {'path': brick.text}
        for child in brick:
            if not child.tag == 'name':
                bricks[brickkey].update({child.tag: child.text})
        for (k, v) in brick.items():
            bricks[brickkey][k] = v
    ret[name]['bricks'] = bricks
    options = {}
    for option in _iter(volume, 'option'):
        options[option.find('name').text] = option.find('value').text
    ret[name]['options'] = options","ret = {volume.find('name').text: {**_etree_to_dict(volume), 'bricks': {f'brick{i}': {**{'path': brick.text}, **{child.tag: child.text for child in brick if not child.tag == 'name'}, **brick.items()} for (i, brick) in enumerate(_iter(volume, 'brick'), start=1)}, 'options': {option.find('name').text: option.find('value').text for option in _iter(volume, 'option')}} for volume in _iter(root, 'volume')}","[""ret = {volume.find('name').text: {'bricks': {'brick{}'.format(i): {'path': brick.text, **{child.tag: child.text for child in brick if not child.tag == 'name'}, **brick.items()} for (i, brick) in enumerate(_iter(volume, 'brick'), start=1)}, 'options': {option.find('name').text: option.find('value').text for option in _iter(volume, 'option')}, **_etree_to_dict(volume)} for volume in _iter(root, 'volume')}"", 'Cannot refactor']",-1,,,
3D-ResNets-PyTorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/3D-ResNets-PyTorch/datasets/activitynet.py,https://github.com/kenshohara/3D-ResNets-PyTorch/tree/master/datasets/activitynet.py,ActivityNet,__make_untrimmed_dataset$130,"def __make_untrimmed_dataset(self, root_path, annotation_path, subset, video_path_formatter):
    with annotation_path.open('r') as f:
        data = json.load(f)
    (video_ids, annotations, fps_values) = get_video_ids_annotations_and_fps(data, subset)
    class_to_idx = get_class_labels(data)
    idx_to_class = {}
    for (name, label) in class_to_idx.items():
        idx_to_class[label] = name
    dataset = []
    for i in range(len(video_ids)):
        if i % 1000 == 0:
            print('dataset loading [{}/{}]'.format(i, len(video_ids)))
        video_path = video_path_formatter(root_path, label, video_ids[i])
        if not video_path.exists():
            continue
        fps = fps_values[i]
        t_begin = 1
        t_end = get_n_frames(video_path) + 1
        frame_indices = list(range(t_begin, t_end))
        sample = {'video': video_path, 'segment': (frame_indices[0], frame_indices[-1] + 1), 'frame_indices': frame_indices, 'fps': fps, 'video_id': video_ids[i]}
        dataset.append(sample)
    return (dataset, idx_to_class)","for (name, label) in class_to_idx.items():
    idx_to_class[label] = name","idx_to_class = {label: name for (name, label) in class_to_class.items()}","['idx_to_class = {label: name for (name, label) in class_to_idx.items()}']",-1,,,
ViLT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ViLT/vilt/utils/write_vqa.py,https://github.com/dandelin/ViLT/tree/master/vilt/utils/write_vqa.py,,make_arrow$52,"def make_arrow(root, dataset_root):
    with open(f'{root}/v2_OpenEnded_mscoco_train2014_questions.json', 'r') as fp:
        questions_train2014 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_val2014_questions.json', 'r') as fp:
        questions_val2014 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_test2015_questions.json', 'r') as fp:
        questions_test2015 = json.load(fp)['questions']
    with open(f'{root}/v2_OpenEnded_mscoco_test-dev2015_questions.json', 'r') as fp:
        questions_test_dev2015 = json.load(fp)['questions']
    with open(f'{root}/v2_mscoco_train2014_annotations.json', 'r') as fp:
        annotations_train2014 = json.load(fp)['annotations']
    with open(f'{root}/v2_mscoco_val2014_annotations.json', 'r') as fp:
        annotations_val2014 = json.load(fp)['annotations']
    annotations = dict()
    for (split, questions) in zip(['train', 'val', 'test', 'test-dev'], [questions_train2014, questions_val2014, questions_test2015, questions_test_dev2015]):
        _annot = defaultdict(dict)
        for q in tqdm(questions):
            _annot[q['image_id']][q['question_id']] = [q['question']]
        annotations[split] = _annot
    all_major_answers = list()
    for (split, annots) in zip(['train', 'val'], [annotations_train2014, annotations_val2014]):
        _annot = annotations[split]
        for q in tqdm(annots):
            all_major_answers.append(q['multiple_choice_answer'])
    all_major_answers = [normalize_word(word) for word in tqdm(all_major_answers)]
    counter = {k: v for (k, v) in Counter(all_major_answers).items() if v >= 9}
    ans2label = {k: i for (i, k) in enumerate(counter.keys())}
    label2ans = list(counter.keys())
    for (split, annots) in zip(['train', 'val'], [annotations_train2014, annotations_val2014]):
        _annot = annotations[split]
        for q in tqdm(annots):
            answers = q['answers']
            answer_count = {}
            for answer in answers:
                answer_ = answer['answer']
                answer_count[answer_] = answer_count.get(answer_, 0) + 1
            labels = []
            scores = []
            for answer in answer_count:
                if answer not in ans2label:
                    continue
                labels.append(ans2label[answer])
                score = get_score(answer_count[answer])
                scores.append(score)
            _annot[q['image_id']][q['question_id']].append({'labels': labels, 'scores': scores})
    for split in ['train', 'val']:
        filtered_annot = dict()
        for (ik, iv) in annotations[split].items():
            new_q = dict()
            for (qk, qv) in iv.items():
                if len(qv[1]['labels']) != 0:
                    new_q[qk] = qv
            if len(new_q) != 0:
                filtered_annot[ik] = new_q
        annotations[split] = filtered_annot
    for split in ['train', 'val', 'test', 'test-dev']:
        annot = annotations[split]
        split_name = {'train': 'train2014', 'val': 'val2014', 'test': 'test2015', 'test-dev': 'test2015'}[split]
        paths = list(glob(f'{root}/{split_name}/*.jpg'))
        random.shuffle(paths)
        annot_paths = [path for path in paths if int(path.split('/')[-1].split('_')[-1][:-4]) in annot]
        if len(paths) == len(annot_paths):
            print('all images have caption annotations')
        else:
            print('not all images have caption annotations')
        print(len(paths), len(annot_paths), len(annot))
        bs = [path2rest(path, split, annotations, label2ans) for path in tqdm(annot_paths)]
        dataframe = pd.DataFrame(bs, columns=['image', 'questions', 'answers', 'answer_labels', 'answer_scores', 'image_id', 'question_id', 'split'])
        table = pa.Table.from_pandas(dataframe)
        os.makedirs(dataset_root, exist_ok=True)
        with pa.OSFile(f'{dataset_root}/vqav2_{split}.arrow', 'wb') as sink:
            with pa.RecordBatchFileWriter(sink, table.schema) as writer:
                writer.write_table(table)
    table = pa.ipc.RecordBatchFileReader(pa.memory_map(f'{dataset_root}/vqav2_val.arrow', 'r')).read_all()
    pdtable = table.to_pandas()
    df1 = pdtable[:-1000]
    df2 = pdtable[-1000:]
    df1 = pa.Table.from_pandas(df1)
    df2 = pa.Table.from_pandas(df2)
    with pa.OSFile(f'{dataset_root}/vqav2_trainable_val.arrow', 'wb') as sink:
        with pa.RecordBatchFileWriter(sink, df1.schema) as writer:
            writer.write_table(df1)
    with pa.OSFile(f'{dataset_root}/vqav2_rest_val.arrow', 'wb') as sink:
        with pa.RecordBatchFileWriter(sink, df2.schema) as writer:
            writer.write_table(df2)","for (split, questions) in zip(['train', 'val', 'test', 'test-dev'], [questions_train2014, questions_val2014, questions_test2015, questions_test_dev2015]):
    _annot = defaultdict(dict)
    for q in tqdm(questions):
        _annot[q['image_id']][q['question_id']] = [q['question']]
    annotations[split] = _annot","annotations = {split: {q['image_id']: {q['question_id']: [q['question']]} for q in tqdm(questions)} for (split, questions) in zip(['train', 'val', 'test', 'test-dev'], [questions_train2014, questions_val2014, questions_test2015, questions_test_dev2015])}","[""annotations = {split: {q['image_id']: {q['question_id']: [q['question']] for q in tqdm(questions)} for questions in [questions_train2014, questions_val2014, questions_test2015, questions_test_dev2015]} for (split, questions) in zip(['train', 'val', 'test', 'test-dev'], [questions_train2014, questions_val2014, questions_test2015, questions_test_dev2015])}"", 'Cannot refactor']",-1,,,
tensorpack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorpack/tensorpack/models/tflayer.py,https://github.com/tensorpack/tensorpack/tree/master/tensorpack/models/tflayer.py,,convert_to_tflayer_args$33,"def convert_to_tflayer_args(args_names, name_mapping):
    """"""
    After applying this decorator:
    1. data_format becomes tf.layers style
    2. nl becomes activation
    3. initializers are renamed
    4. positional args are transformed to corresponding kwargs, according to args_names
    5. kwargs are mapped to tf.layers names if needed, by name_mapping
    """"""

    def decorator(func):

        @functools.wraps(func)
        def decorated_func(inputs, *args, **kwargs):
            kwargs = map_common_tfargs(kwargs)
            posarg_dic = {}
            assert len(args) <= len(args_names), 'Please use kwargs instead of positional args to call this model, except for the following arguments: {}'.format(', '.join(args_names))
            for (pos_arg, name) in zip(args, args_names):
                posarg_dic[name] = pos_arg
            ret = {}
            for (name, arg) in six.iteritems(kwargs):
                newname = name_mapping.get(name, None)
                if newname is not None:
                    assert newname not in kwargs, 'Argument {} and {} conflicts!'.format(name, newname)
                else:
                    newname = name
                ret[newname] = arg
            ret.update(posarg_dic)
            return func(inputs, **ret)
        return decorated_func
    return decorator","for (name, arg) in six.iteritems(kwargs):
    newname = name_mapping.get(name, None)
    if newname is not None:
        assert newname not in kwargs, 'Argument {} and {} conflicts!'.format(name, newname)
    else:
        newname = name
    ret[newname] = arg","ret = {name_mapping.get(name, name): arg for (name, arg) in six.iteritems(kwargs)}
assert len(set(ret.keys()) & set(kwargs.keys())) == 0, 'Argument {} and {} conflicts!'.format(name, newname)",['actually can be refactored'],-1,,,
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/backup/custom.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/backup/custom.py,,assign_identity$207,"def assign_identity(client, resource_group_name, vault_name, system_assigned=None, user_assigned=None):
    vault_details = client.get(resource_group_name, vault_name)
    curr_identity_details = vault_details.identity
    curr_identity_type = 'none'
    identity_type = 'none'
    user_assigned_identity = None
    if curr_identity_details is not None:
        curr_identity_type = curr_identity_details.type.lower()
    if user_assigned is not None:
        userid = UserIdentity()
        user_assigned_identity = dict()
        for userMSI in user_assigned:
            user_assigned_identity[userMSI] = userid
        if system_assigned is not None or curr_identity_type in ['systemassigned', 'systemassigned, userassigned']:
            identity_type = 'systemassigned,userassigned'
        else:
            identity_type = 'userassigned'
    elif system_assigned is not None:
        if curr_identity_type in ['systemassigned, userassigned', 'userassigned']:
            identity_type = 'systemassigned,userassigned'
        else:
            identity_type = 'systemassigned'
    else:
        raise RequiredArgumentMissingError('\n            Invalid parameters, no operation specified.\n            ')
    identity_data = IdentityData(type=identity_type, user_assigned_identities=user_assigned_identity)
    vault = PatchVault(identity=identity_data)
    return client.begin_update(resource_group_name, vault_name, vault)","for userMSI in user_assigned:
    user_assigned_identity[userMSI] = userid",user_assigned_identity = {userMSI: UserIdentity() for userMSI in user_assigned},['user_assigned_identity = {userMSI: userid for userMSI in user_assigned}'],-1,,,
wifipumpkin3,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wifipumpkin3/wifipumpkin3/core/controllers/dhcpcontroller.py,https://github.com/P0cL4bs/wifipumpkin3/tree/master/wifipumpkin3/core/controllers/dhcpcontroller.py,DHCPController,__init__$30,"def __init__(self, parent):
    super(DHCPController, self).__init__()
    self.parent = parent
    self.parent.getDefault.addController(self)
    __dhcpmode = dhcp.DHCPSettings.instances[0].dhmode
    self.mode = {}
    for k in __dhcpmode:
        self.mode[k.ID] = k","for k in __dhcpmode:
    self.mode[k.ID] = k",self.mode = {k.ID: k for k in dhcp.DHCPSettings.instances[0].dhmode},['self.mode = {k.ID: k for k in __dhcpmode}'],-1,,,
zato,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zato/code/zato-common/src/zato/common/odb/api.py,https://github.com/zatosource/zato/tree/master/code/zato-common/src/zato/common/odb/api.py,PoolStore,__setitem__$423,"def __setitem__(self, name, config):
    """""" Stops a connection pool if it exists and replaces it with a new one
        using updated settings.
        """"""
    with self._lock:
        if name in self.wrappers:
            del self[name]
        config_no_sensitive = {}
        for key in config:
            if key != 'callback_func':
                config_no_sensitive[key] = config[key]
        config_no_sensitive['password'] = SECRET_SHADOW
        pool = self.sql_conn_class(name, config, config_no_sensitive)
        wrapper = SessionWrapper()
        wrapper.init_session(name, config, pool)
        self.wrappers[name] = wrapper","for key in config:
    if key != 'callback_func':
        config_no_sensitive[key] = config[key]","config_no_sensitive = {key: config[key] for key in config if key != 'callback_func'}
config_no_sensitive['password'] = SECRET_SHADOW",["config_no_sensitive = {key: config[key] for key in config if key != 'callback_func'}"],-1,,,
MultiQC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MultiQC/multiqc/modules/mirtrace/mirtrace.py,https://github.com/ewels/MultiQC/tree/master/multiqc/modules/mirtrace/mirtrace.py,MultiqcModule,parse_complexity$178,"def parse_complexity(self, f):
    header = []
    body = {}
    lines = f['f'].splitlines()
    for l in lines:
        s = l.split('\t')
        if len(header) == 0:
            if s[0] != 'DISTINCT_MIRNA_HAIRPINS_ACCUMULATED_COUNT':
                log.debug('No valid data {} for miRNA complexity'.format(f['fn']))
                return None
            header = s[1:]
        else:
            body[s[0]] = s[1:len(s)]
    for record in header[0:len(header)]:
        s_name = self.clean_s_name(record, f)
        parsed_data = {}
        idx = header[0:len(header)].index(record)
        for depth in body:
            parsed_data[depth] = int(body[depth][idx]) if body[depth][idx] else 0
        if s_name in self.complexity_data:
            log.debug('Duplicate sample name found! Overwriting: {}'.format(s_name))
        self.add_data_source(f, s_name)
        self.complexity_data[s_name] = parsed_data","for l in lines:
    s = l.split('\t')
    if len(header) == 0:
        if s[0] != 'DISTINCT_MIRNA_HAIRPINS_ACCUMULATED_COUNT':
            log.debug('No valid data {} for miRNA complexity'.format(f['fn']))
            return None
        header = s[1:]
    else:
        body[s[0]] = s[1:len(s)]",body = {s[0]: s[1:len(s)] for l in lines for s in [l.split('\t')] if len(header) != 0},['actually can be refactored'],-1,,,
freight,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freight/freight/api/serializer/app.py,https://github.com/getsentry/freight/tree/master/freight/api/serializer/app.py,AppSerializer,serialize$9,"def serialize(self, item, attrs):
    env_map = {}
    for (env, env_data) in list(item.environments.items()):
        env_map[env] = {'defaultRef': env_data.get('default_ref', 'master')}
    if not env_map:
        env_map['production'] = {'defaultRef': 'master'}
    if item.repository_id:
        repo = Repository.query.filter(Repository.id == item.repository_id).first().url
    else:
        repo = None
    return {'id': str(item.id), 'name': item.name, 'environments': env_map, 'repository': repo, 'changeLabels': item.change_labels, 'lockedReason': item.locked_reason}","for (env, env_data) in list(item.environments.items()):
    env_map[env] = {'defaultRef': env_data.get('default_ref', 'master')}","env_map = {env: {'defaultRef': env_data.get('default_ref', 'master')} for (env, env_data) in item.environments.items()}","[""env_map = {env: {'defaultRef': env_data.get('default_ref', 'master')} for (env, env_data) in list(item.environments.items())}""]",-1,,,
FACT_core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FACT_core/src/plugins/compare/file_coverage/code/file_coverage.py,https://github.com/fkie-cad/FACT_core/tree/master/src/plugins/compare/file_coverage/code/file_coverage.py,ComparePlugin,_get_files_in_more_than_one_but_not_in_all$68,"def _get_files_in_more_than_one_but_not_in_all(fo_list, result_dict):
    result = {}
    for current_element in fo_list:
        result[current_element.uid] = list(set.difference(set(current_element.list_of_all_included_files), result_dict['files_in_common']['all'], result_dict['exclusive_files'][current_element.uid]))
    return result","for current_element in fo_list:
    result[current_element.uid] = list(set.difference(set(current_element.list_of_all_included_files), result_dict['files_in_common']['all'], result_dict['exclusive_files'][current_element.uid]))","result = {current_element.uid: list(set(current_element.list_of_all_included_files).difference(result_dict['files_in_common']['all'], result_dict['exclusive_files'][current_element.uid])) for current_element in fo_list}","[""result = {current_element.uid: list(set.difference(set(current_element.list_of_all_included_files), result_dict['files_in_common']['all'], result_dict['exclusive_files'][current_element.uid])) for current_element in fo_list}""]",-1,,,
MultiQC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MultiQC/multiqc/modules/ngsderive/ngsderive.py,https://github.com/ewels/MultiQC/tree/master/multiqc/modules/ngsderive/ngsderive.py,MultiqcModule,add_junctions_data$355,"def add_junctions_data(self):
    self.write_data_file(self.junctions, 'ngsderive_junctions')
    data = {}
    for (sample, junctions_data) in self.junctions.items():
        data[sample] = {'total_junctions': junctions_data.get('total_junctions'), 'known_junctions': junctions_data.get('known_junctions'), 'partial_novel_junctions': junctions_data.get('partial_novel_junctions'), 'novel_junctions': junctions_data.get('complete_novel_junctions'), 'total_splice_events': junctions_data.get('total_splice_events'), 'known_spliced_reads': junctions_data.get('known_spliced_reads'), 'partial_novel_spliced_reads': junctions_data.get('partial_novel_spliced_reads'), 'novel_spliced_reads': junctions_data.get('complete_novel_spliced_reads')}
    headers = OrderedDict({'total_junctions': {'title': 'Total junctions', 'description': 'Total number of junctions found by ngsderive', 'hidden': True, 'format': '{:,d}'}, 'known_junctions': {'title': 'Known junctions', 'description': 'Number of annotated junctions found by ngsderive', 'format': '{:,d}'}, 'partial_novel_junctions': {'title': 'Partially novel junctions', 'description': 'Number of partially annotated junctions found by ngsderive', 'format': '{:,d}'}, 'novel_junctions': {'title': 'Novel junctions', 'description': 'Number of completely novel junctions found by ngsderive', 'format': '{:,d}'}, 'total_splice_events': {'title': 'Total splice events', 'description': 'Total number of spliced reads found by ngsderive', 'hidden': True, 'format': '{:,d}'}, 'known_spliced_reads': {'title': 'Annotated spliced reads', 'description': 'Number of annotated spliced reads found by ngsderive', 'hidden': True, 'format': '{:,d}'}, 'partial_novel_spliced_reads': {'title': 'Partially annotated spliced reads', 'description': 'Number of partially annotated spliced reads found by ngsderive', 'hidden': True, 'format': '{:,d}'}, 'novel_spliced_reads': {'title': 'Novel spliced reads', 'description': 'Number of completely un-annotated spliced reads found by ngsderive', 'hidden': True, 'format': '{:,d}'}})
    self.general_stats_addcols(data, headers)
    bardata = OrderedDict()
    sorted_junction_data = sorted(data.items(), key=lambda x: int(x[1].get('total_junctions')), reverse=True)
    for (k, v) in sorted_junction_data:
        bardata[k] = {'known_junctions': v['known_junctions'], 'partial_novel_junctions': v['partial_novel_junctions'], 'novel_junctions': v['novel_junctions'], 'known_spliced_reads': v['known_spliced_reads'], 'partial_novel_spliced_reads': v['partial_novel_spliced_reads'], 'novel_spliced_reads': v['novel_spliced_reads']}
    pconfig = {'id': 'ngsderive_junctions_plot', 'title': 'ngsderive: Junction Annotation', 'cpswitch_counts_label': 'Number', 'yDecimals': False, 'ylab': 'Number of junctions', 'data_labels': [{'name': 'Junctions', 'ylab': 'Number of junctions'}, {'name': 'Spliced Reads', 'ylab': 'Number of spliced reads'}]}
    cats = [OrderedDict(), OrderedDict()]
    cats[0]['known_junctions'] = {'name': 'Known junctions'}
    cats[0]['partial_novel_junctions'] = {'name': 'Partially novel junctions'}
    cats[0]['novel_junctions'] = {'name': 'Novel junctions'}
    cats[1]['known_spliced_reads'] = {'name': 'Annotated spliced reads'}
    cats[1]['partial_novel_spliced_reads'] = {'name': 'Partially annotated spliced reads'}
    cats[1]['novel_spliced_reads'] = {'name': 'Novel spliced reads'}
    self.add_section(name='Junction Annotations', anchor='ngsderive-junctions', description='Junction annotations provided by ngsderive. For more information, please see\n            [the documentation](https://stjudecloud.github.io/ngsderive/subcommands/junction_annotation/).', plot=bargraph.plot([bardata, bardata], cats, pconfig))","for (sample, junctions_data) in self.junctions.items():
    data[sample] = {'total_junctions': junctions_data.get('total_junctions'), 'known_junctions': junctions_data.get('known_junctions'), 'partial_novel_junctions': junctions_data.get('partial_novel_junctions'), 'novel_junctions': junctions_data.get('complete_novel_junctions'), 'total_splice_events': junctions_data.get('total_splice_events'), 'known_spliced_reads': junctions_data.get('known_spliced_reads'), 'partial_novel_spliced_reads': junctions_data.get('partial_novel_spliced_reads'), 'novel_spliced_reads': junctions_data.get('complete_novel_spliced_reads')}","data = {sample: {key: junctions_data.get(key) for key in ['total_junctions', 'known_junctions', 'partial_novel_junctions', 'complete_novel_junctions', 'total_splice_events', 'known_spliced_reads', 'partial_novel_spliced_reads', 'complete_novel_spliced_reads']} for (sample, junctions_data) in self.junctions.items()}","[""data = {sample: {'total_junctions': junctions_data.get('total_junctions'), 'known_junctions': junctions_data.get('known_junctions'), 'partial_novel_junctions': junctions_data.get('partial_novel_junctions'), 'novel_junctions': junctions_data.get('complete_novel_junctions'), 'total_splice_events': junctions_data.get('total_splice_events'), 'known_spliced_reads': junctions_data.get('known_spliced_reads'), 'partial_novel_spliced_reads': junctions_data.get('partial_novel_spliced_reads'), 'novel_spliced_reads': junctions_data.get('complete_novel_spliced_reads')} for (sample, junctions_data) in self.junctions.items()}""]",-1,,,
beets,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/beets/beetsplug/info.py,https://github.com/beetbox/beets/tree/master/beetsplug/info.py,,emitter$49,"def emitter(included_keys):
    if included_keys == '*':
        fields = tag_fields()
    else:
        fields = included_keys
    if 'images' in fields:
        fields.remove('images')
    mf = mediafile.MediaFile(syspath(path))
    tags = {}
    for field in fields:
        if field == 'art':
            tags[field] = mf.art is not None
        else:
            tags[field] = getattr(mf, field, None)
    item = Item.from_path(syspath(path))
    return (tags, item)","for field in fields:
    if field == 'art':
        tags[field] = mf.art is not None
    else:
        tags[field] = getattr(mf, field, None)","tags = {field: mf.art is not None if field == 'art' else getattr(mf, field, None) for field in fields if field != 'images'}","[""tags = {field: mf.art is not None if field == 'art' else getattr(mf, field, None) for field in fields}""]",-1,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/modjk.py,https://github.com/saltstack/salt/tree/master/salt/modules/modjk.py,,workers$181,"def workers(profile='default'):
    """"""
    Return a list of member workers and their status

    CLI Examples:

    .. code-block:: bash

        salt '*' modjk.workers
        salt '*' modjk.workers other-profile
    """"""
    config = get_running(profile)
    lbn = config['worker.list'].split(',')
    worker_list = []
    ret = {}
    for lb in lbn:
        try:
            worker_list.extend(config['worker.{}.balance_workers'.format(lb)].split(','))
        except KeyError:
            pass
    worker_list = list(set(worker_list))
    for worker in worker_list:
        ret[worker] = {'activation': config['worker.{}.activation'.format(worker)], 'state': config['worker.{}.state'.format(worker)]}
    return ret","for worker in worker_list:
    ret[worker] = {'activation': config['worker.{}.activation'.format(worker)], 'state': config['worker.{}.state'.format(worker)]}","ret = {worker: {'activation': config[f'worker.{worker}.activation'], 'state': config[f'worker.{worker}.state']} for worker in worker_list}","[""ret = {worker: {'activation': config['worker.{}.activation'.format(worker)], 'state': config['worker.{}.state'.format(worker)]} for worker in worker_list}""]",-1,,,
neural-structured-learning,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neural-structured-learning/research/kg_hyp_emb/utils/train.py,https://github.com/tensorflow/neural-structured-learning/tree/master/research/kg_hyp_emb/utils/train.py,,get_config_dict$27,"def get_config_dict():
    """"""Maps FLAGS to dictionnary in order to save it in json format.""""""
    config = {}
    for (_, arg_dict) in CONFIG.items():
        for (arg, _) in arg_dict.items():
            config[arg] = getattr(FLAGS, arg)
    return config","for (_, arg_dict) in CONFIG.items():
    for (arg, _) in arg_dict.items():
        config[arg] = getattr(FLAGS, arg)","config = {arg: getattr(FLAGS, arg) for arg_dict in CONFIG.values() for arg in arg_dict.keys()}","['config = {arg: getattr(FLAGS, arg) for (_, arg_dict) in CONFIG.items() for (arg, _) in arg_dict.items()}']",-1,,,
