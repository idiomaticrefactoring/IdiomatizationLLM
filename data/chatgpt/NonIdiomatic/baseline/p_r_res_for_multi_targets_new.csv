repo_name,file_path,file_html,class_name,me_name,me_code,old_code,new_code,gd_truth_code,acc
kivy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy/kivy/atlas.py,https://github.com/kivy/kivy/tree/master/kivy/atlas.py,Atlas,create$229,"def create(outname, filenames, size, padding=2, use_path=False):
    """"""This method can be used to create an atlas manually from a set of
        images.

        :Parameters:
            `outname`: str
                Basename to use for ``.atlas`` creation and ``-<idx>.png``
                associated images.
            `filenames`: list
                List of filenames to put in the atlas.
            `size`: int or list (width, height)
                Size of the atlas image. If the size is not large enough to
                fit all of the source images, more atlas images will created
                as required.
            `padding`: int, defaults to 2
                Padding to put around each image.

                Be careful. If you're using a padding < 2, you might have
                issues with the borders of the images. Because of the OpenGL
                linearization, it might use the pixels of the adjacent image.

                If you're using a padding >= 2, we'll automatically generate a
                ""border"" of 1px around your image. If you look at
                the result, don't be scared if the image inside is not
                exactly the same as yours :).

            `use_path`: bool, defaults to False
                If True, the relative path of the source png
                file names will be included in the atlas ids rather
                that just in the file names. Leading dots and slashes will be
                excluded and all other slashes in the path will be replaced
                with underscores. For example, if `use_path` is False
                (the default) and the file name is
                ``../data/tiles/green_grass.png``, the id will be
                ``green_grass``. If `use_path` is True, it will be
                ``data_tiles_green_grass``.

            .. versionchanged:: 1.8.0
                Parameter use_path added
        """"""
    try:
        from PIL import Image
    except ImportError:
        Logger.critical('Atlas: Imaging/PIL are missing')
        raise
    if isinstance(size, (tuple, list)):
        (size_w, size_h) = list(map(int, size))
    else:
        size_w = size_h = int(size)
    ims = list()
    for f in filenames:
        fp = open(f, 'rb')
        im = Image.open(fp)
        im.load()
        fp.close()
        ims.append((f, im))
    ims = sorted(ims, key=lambda im: im[1].size[0] * im[1].size[1], reverse=True)
    freeboxes = [(0, 0, 0, size_w, size_h)]
    numoutimages = 1
    fullboxes = []
    for imageinfo in ims:
        im = imageinfo[1]
        (imw, imh) = im.size
        imw += padding
        imh += padding
        if imw > size_w or imh > size_h:
            Logger.error('Atlas: image %s (%d by %d) is larger than the atlas size!' % (imageinfo[0], imw, imh))
            return
        inserted = False
        while not inserted:
            for (idx, fb) in enumerate(freeboxes):
                if fb[3] >= imw and fb[4] >= imh:
                    del freeboxes[idx]
                    if fb[3] > imw:
                        freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))
                    if fb[4] > imh:
                        freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))
                    freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])
                    fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo[0]))
                    inserted = True
                    break
            if not inserted:
                freeboxes.append((numoutimages, 0, 0, size_w, size_h))
                numoutimages += 1
    Logger.info('Atlas: create an {0}x{1} rgba image'.format(size_w, size_h))
    outimages = [Image.new('RGBA', (size_w, size_h)) for i in range(0, int(numoutimages))]
    for fb in fullboxes:
        (x, y) = (fb[2], fb[3])
        out = outimages[fb[1]]
        out.paste(fb[0], (fb[2], fb[3]))
        (w, h) = fb[0].size
        if padding > 1:
            out.paste(fb[0].crop((0, 0, w, 1)), (x, y - 1))
            out.paste(fb[0].crop((0, h - 1, w, h)), (x, y + h))
            out.paste(fb[0].crop((0, 0, 1, h)), (x - 1, y))
            out.paste(fb[0].crop((w - 1, 0, w, h)), (x + w, y))
    for (idx, outimage) in enumerate(outimages):
        outimage.save('%s-%d.png' % (outname, idx))
    meta = {}
    for fb in fullboxes:
        fn = '%s-%d.png' % (basename(outname), fb[1])
        if fn not in meta:
            d = meta[fn] = {}
        else:
            d = meta[fn]
        if use_path:
            uid = splitext(fb[6])[0]
            uid = uid.lstrip('./\\')
            uid = uid.replace('/', '_').replace('\\', '_')
        else:
            uid = splitext(basename(fb[6]))[0]
        (x, y, w, h) = fb[2:6]
        d[uid] = (x, size_h - y - h, w, h)
    outfn = '%s.atlas' % outname
    with open(outfn, 'w') as fd:
        json.dump(meta, fd)
    return (outfn, meta)","for imageinfo in ims:
    im = imageinfo[1]
    (imw, imh) = im.size
    imw += padding
    imh += padding
    if imw > size_w or imh > size_h:
        Logger.error('Atlas: image %s (%d by %d) is larger than the atlas size!' % (imageinfo[0], imw, imh))
        return
    inserted = False
    while not inserted:
        for (idx, fb) in enumerate(freeboxes):
            if fb[3] >= imw and fb[4] >= imh:
                del freeboxes[idx]
                if fb[3] > imw:
                    freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))
                if fb[4] > imh:
                    freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))
                freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])
                fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo[0]))
                inserted = True
                break
        if not inserted:
            freeboxes.append((numoutimages, 0, 0, size_w, size_h))
            numoutimages += 1","# do the actual atlasing by sticking the largest images we can
# have into the smallest valid free boxes
for imageinfo in ims:
    im = imageinfo[1]
    imw, imh = im.size
    imw += padding
    imh += padding
    if imw > size_w or imh > size_h:
        Logger.error(
            'Atlas: image %s (%d by %d) is larger than the atlas size!'
            % (imageinfo[0], imw, imh))
        return

    inserted = False
    while not inserted:
        for idx, fb in enumerate(freeboxes):
            # find the smallest free box that will contain this image
            if fb[3] >= imw and fb[4] >= imh:
                # we found a valid spot! Remove the current
                # freebox, and split the leftover space into (up to)
                # two new freeboxes","[""for imageinfo in ims:\n    (imageinfo_0, imageinfo_1, *_) = imageinfo\n    im = imageinfo_1\n    (imw, imh) = im.size\n    imw += padding\n    imh += padding\n    if imw > size_w or imh > size_h:\n        Logger.error('Atlas: image %s (%d by %d) is larger than the atlas size!' % (imageinfo_0, imw, imh))\n        return\n    inserted = False\n    while not inserted:\n        for (idx, fb) in enumerate(freeboxes):\n            if fb[3] >= imw and fb[4] >= imh:\n                del freeboxes[idx]\n                if fb[3] > imw:\n                    freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))\n                if fb[4] > imh:\n                    freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))\n                freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])\n                fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo_0))\n                inserted = True\n                break\n        if not inserted:\n            freeboxes.append((numoutimages, 0, 0, size_w, size_h))\n            numoutimages += 1"", ""for (imageinfo_0, imageinfo_1, *imageinfo_len) in ims:\n    im = \n    imageinfo_1\n    (imw, imh) = im.size\n    imw += padding\n    imh += padding\n    if imw > size_w or imh > size_h:\n        Logger.error('Atlas: image %s (%d by %d) is larger than the atlas size!' % (\n        imageinfo_0, imw, imh))\n        return\n    inserted = False\n    while not inserted:\n        for (idx, fb) in enumerate(freeboxes):\n            if fb[3] >= imw and fb[4] >= imh:\n                del freeboxes[idx]\n                if fb[3] > imw:\n                    freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))\n                if fb[4] > imh:\n                    freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))\n                freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])\n                fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, \n                imageinfo_0))\n                inserted = True\n                break\n        if not inserted:\n            freeboxes.append((numoutimages, 0, 0, size_w, size_h))\n            numoutimages += 1""]",-1
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/webapps/galaxy/controllers/tag.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/webapps/galaxy/controllers/tag.py,TagsController,_get_tag_autocomplete_values$137,"def _get_tag_autocomplete_values(self, trans, q, limit, timestamp, user=None, item=None, item_class=None):
    """"""
        Returns autocomplete data for tag values ordered from most frequently used to
        least frequently used.
        """"""
    tag_name_and_value = q.split(':')
    tag_name = tag_name_and_value[0]
    tag_value = tag_name_and_value[1]
    tag = self.get_tag_handler(trans).get_tag_by_name(tag_name)
    if tag is None:
        return ''
    if item is None and item_class is None:
        raise RuntimeError('Both item and item_class cannot be None')
    elif item is not None:
        item_class = item.__class__
    item_tag_assoc_class = self.get_tag_handler(trans).get_tag_assoc_class(item_class)
    cols_to_select = [item_tag_assoc_class.table.c.value, func.count('*')]
    from_obj = item_tag_assoc_class.table.join(item_class.table).join(trans.app.model.Tag.table)
    where_clause = and_(item_tag_assoc_class.table.c.user_id == user.id, trans.app.model.Tag.table.c.id == tag.id, item_tag_assoc_class.table.c.value.like(f'{tag_value}%'))
    order_by = [func.count('*').desc(), item_tag_assoc_class.table.c.value]
    group_by = item_tag_assoc_class.table.c.value
    query = select(columns=cols_to_select, from_obj=from_obj, whereclause=where_clause, group_by=group_by, order_by=order_by, limit=limit)
    result_set = trans.sa_session.execute(query)
    ac_data = f""#Header|Your Values for '{tag_name}'\n""
    tag_uname = self._get_usernames_for_tag(trans, trans.user, tag, item_class, item_tag_assoc_class)[0]
    for row in result_set:
        ac_data += f'{tag_uname}:{row[0]}|{row[0]}\n'
    return ac_data","for row in result_set:
    ac_data += f'{tag_uname}:{row[0]}|{row[0]}\n'",ac_data += '\n'.join([f'{tag_uname}:{row[0]}|{row[0]}' for row in result_set]) + '\n',"[""for row in result_set:\n    (row_0, *row_rrowmaining) = row\n    ac_data += f'{tag_uname}:{row_0}|{row_0}\\n'"", ""for (row_0, *row_len) in result_set:\n    ac_data += f'{tag_uname}:{row_0}|{row_0}\\n'""]",-1
abseil-py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/abseil-py/absl/testing/tests/parameterized_test.py,https://github.com/abseil/abseil-py/tree/master/absl/testing/tests/parameterized_test.py,,decorator$51,"def decorator(test_method):
    if isinstance(test_method, abc.Iterable):
        actual_tests = []
        for old_test in test_method.testcases:
            new_dict = old_test[1].copy()
            new_dict[key] = value
            test_suffix = '%s_%s_%s' % (old_test[0], key, value)
            actual_tests.append((test_suffix, new_dict))
        test_method.testcases = actual_tests
        return test_method
    else:
        test_suffix = '_%s_%s' % (key, value)
        tests_to_make = ((test_suffix, {key: value}),)
        return parameterized.named_parameters(*tests_to_make)(test_method)","for old_test in test_method.testcases:
    new_dict = old_test[1].copy()
    new_dict[key] = value
    test_suffix = '%s_%s_%s' % (old_test[0], key, value)
    actual_tests.append((test_suffix, new_dict))","actual_tests = []
for (old_test_suffix, old_test_dict) in test_method:
    new_dict = old_test_dict.copy()
    new_dict[key] = value
    test_suffix = f'{old_test_suffix}_{key}_{value}'
    actual_tests.append((test_suffix, new_dict))","[""for old_test in test_method.testcases:\n    (old_test_0, old_test_1, *_) = old_test\n    new_dict = old_test_1.copy()\n    new_dict[key] = value\n    test_suffix = '%s_%s_%s' % (old_test_0, key, value)\n    actual_tests.append((test_suffix, new_dict))"", ""for (old_test_0, old_test_1, *old_test_len) in test_method.testcases:\n    new_dict = \n    old_test_1.copy()\n    new_dict[key] = value\n    test_suffix = '%s_%s_%s' % (\n    old_test_0, key, value)\n    actual_tests.append((test_suffix, new_dict))""]",-1
folium,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/folium/folium/plugins/heat_map.py,https://github.com/python-visualization/folium/tree/master/folium/plugins/heat_map.py,HeatMap,_get_self_bounds$87,"def _get_self_bounds(self):
    """"""
        Computes the bounds of the object itself (not including it's children)
        in the form [[lat_min, lon_min], [lat_max, lon_max]].

        """"""
    bounds = [[None, None], [None, None]]
    for point in self.data:
        bounds = [[none_min(bounds[0][0], point[0]), none_min(bounds[0][1], point[1])], [none_max(bounds[1][0], point[0]), none_max(bounds[1][1], point[1])]]
    return bounds","for point in self.data:
    bounds = [[none_min(bounds[0][0], point[0]), none_min(bounds[0][1], point[1])], [none_max(bounds[1][0], point[0]), none_max(bounds[1][1], point[1])]]","bounds = [[None, None], [None, None]]
for point in self.data:
    bounds = [[point[0] if bounds[0][0] is None else min(bounds[0][0], point[0]), point[1] if bounds[0][1] is None else min(bounds[0][1], point[1])], [point[0] if bounds[1][0] is None else max(bounds[1][0], point[0]), point[1] if bounds[1][1] is None else max(bounds[1][1], point[1])]]","['for point in self.data:\n    (point_0, point_1, *_) = point\n    bounds = [[none_min(bounds[0][0], point_0), none_min(bounds[0][1], point_1)], [none_max(bounds[1][0], point_0), none_max(bounds[1][1], point_1)]]', 'for (point_0, point_1, *point_len) in self.data:\n    bounds = [[none_min(bounds[0][0], point_0), none_min(bounds[0][1], point_1)], [none_max(bounds[1][0], point_0), none_max(bounds[1][1], point_1)]]']",-1
plover,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plover/plover/orthography.py,https://github.com/openstenoproject/plover/tree/master/plover/orthography.py,,make_candidates_from_rules$9,"def make_candidates_from_rules(word, suffix, check=lambda x: True):
    candidates = []
    for r in system.ORTHOGRAPHY_RULES:
        m = r[0].match(word + ' ^ ' + suffix)
        if m:
            expanded = m.expand(r[1])
            if check(expanded):
                candidates.append(expanded)
    return candidates","for r in system.ORTHOGRAPHY_RULES:
    m = r[0].match(word + ' ^ ' + suffix)
    if m:
        expanded = m.expand(r[1])
        if check(expanded):
            candidates.append(expanded)",candidates = [m.expand(r[1]) for r in system.ORTHOGRAPHY_RULES if (m := r[0].match(word + ' ^ ' + suffix)) and check(m.expand(r[1]))],"[""for r in system.ORTHOGRAPHY_RULES:\n    (r_0, r_1, *_) = r\n    m = r_0.match(word + ' ^ ' + suffix)\n    if m:\n        expanded = m.expand(r_1)\n        if check(expanded):\n            candidates.append(expanded)"", ""for (r_0, r_1, *r_len) in system.ORTHOGRAPHY_RULES:\n    m = \n    r_0.match(word + ' ^ ' + suffix)\n    if m:\n        expanded = m.expand(\n        r_1)\n        if check(expanded):\n            candidates.append(expanded)""]",-1
OpsManage,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpsManage/service/mysql/mysql_base.py,https://github.com/welliamcao/OpsManage/tree/master/service/mysql/mysql_base.py,MySQLBase,get_global_status$87,"def get_global_status(self):
    dataList = []
    logs = self.execute_for_query(sql='show global variables;')
    for ds in logs[1]:
        data = {}
        data['value'] = ds[1]
        data['name'] = ds[0].capitalize()
        dataList.append(data)
    return dataList","for ds in logs[1]:
    data = {}
    data['value'] = ds[1]
    data['name'] = ds[0].capitalize()
    dataList.append(data)","dataList = [{'value': ds[1], 'name': ds[0].capitalize()} for ds in logs[1]]","[""for ds in logs[1]:\n    (ds_0, ds_1, *_) = ds\n    data = {}\n    data['value'] = ds_1\n    data['name'] = ds_0.capitalize()\n    dataList.append(data)"", ""for (ds_0, ds_1, *ds_len) in logs[1]:\n    data = {}\n    data['value'] = \n    ds_1\n    data['name'] = \n    ds_0.capitalize()\n    dataList.append(data)""]",-1
whatportis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/whatportis/tests/conftest.py,https://github.com/ncrocfer/whatportis/tree/master/tests/conftest.py,,create_ports$16,"def create_ports(tmpdir, monkeypatch):

    def _create_ports(ports):

        def get_db():
            tmp_db = tmpdir.join('db.json')
            db = TinyDB(str(tmp_db), storage=CachingMiddleware(JSONStorage))
            for port in ports:
                db.insert({'name': port[0], 'port': port[1], 'description': port[2], 'protocol': port[3]})
            return db
        return monkeypatch.setattr(whatportis.db, 'get_database', get_db)
    return _create_ports","for port in ports:
    db.insert({'name': port[0], 'port': port[1], 'description': port[2], 'protocol': port[3]})","db.insert([{'name': port[0], 'port': port[1], 'description': port[2], 'protocol': port[3]} for port in ports])","[""for port in ports:\n    (port_0, port_1, port_2, port_3, *_) = port\n    db.insert({'name': port_0, 'port': port_1, 'description': port_2, 'protocol': port_3})"", ""for (port_0, port_1, port_2, port_3, *port_len) in ports:\n    db.insert({'name': port_0, 'port': port_1, 'description': port_2, 'protocol': port_3})""]",-1
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_imperative_se_resnext.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_imperative_se_resnext.py,TestImperativeResneXt,reader_decorator$326,"def reader_decorator(self, reader):

    def _reader_imple():
        for item in reader():
            doc = np.array(item[0]).reshape(3, 224, 224)
            label = np.array(item[1]).astype('int64').reshape(1)
            yield (doc, label)
    return _reader_imple","for item in reader():
    doc = np.array(item[0]).reshape(3, 224, 224)
    label = np.array(item[1]).astype('int64').reshape(1)
    yield (doc, label)","def reader_decorator(self, reader):

    def _reader_imple():
        for (item_0, item_1) in reader():
            doc = np.array(item_0).reshape(3, 224, 224)
            label = np.array(item_1).astype('int64').reshape(1)
            yield (doc, label)
    return _reader_imple","[""for item in reader():\n    (item_0, item_1, *_) = item\n    doc = np.array(item_0).reshape(3, 224, 224)\n    label = np.array(item_1).astype('int64').reshape(1)\n    yield (doc, label)"", ""for (item_0, item_1, *item_len) in reader():\n    doc = np.array(item_0).reshape(3, 224, 224)\n    label = np.array(item_1).astype('int64').reshape(1)\n    yield (doc, label)""]",-1
3DDFA_V2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/3DDFA_V2/FaceBoxes/FaceBoxes.py,https://github.com/cleardusk/3DDFA_V2/tree/master/FaceBoxes/FaceBoxes.py,FaceBoxes,__call__$58,"def __call__(self, img_):
    img_raw = img_.copy()
    scale = 1
    if scale_flag:
        (h, w) = img_raw.shape[:2]
        if h > HEIGHT:
            scale = HEIGHT / h
        if w * scale > WIDTH:
            scale *= WIDTH / (w * scale)
        if scale == 1:
            img_raw_scale = img_raw
        else:
            h_s = int(scale * h)
            w_s = int(scale * w)
            img_raw_scale = cv2.resize(img_raw, dsize=(w_s, h_s))
        img = np.float32(img_raw_scale)
    else:
        img = np.float32(img_raw)
    _t = {'forward_pass': Timer(), 'misc': Timer()}
    (im_height, im_width, _) = img.shape
    scale_bbox = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])
    img -= (104, 117, 123)
    img = img.transpose(2, 0, 1)
    img = torch.from_numpy(img).unsqueeze(0)
    _t['forward_pass'].tic()
    (loc, conf) = self.net(img)
    _t['forward_pass'].toc()
    _t['misc'].tic()
    priorbox = PriorBox(image_size=(im_height, im_width))
    priors = priorbox.forward()
    prior_data = priors.data
    boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])
    if scale_flag:
        boxes = boxes * scale_bbox / scale / resize
    else:
        boxes = boxes * scale_bbox / resize
    boxes = boxes.cpu().numpy()
    scores = conf.squeeze(0).data.cpu().numpy()[:, 1]
    inds = np.where(scores > confidence_threshold)[0]
    boxes = boxes[inds]
    scores = scores[inds]
    order = scores.argsort()[::-1][:top_k]
    boxes = boxes[order]
    scores = scores[order]
    dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)
    keep = nms(dets, nms_threshold)
    dets = dets[keep, :]
    dets = dets[:keep_top_k, :]
    _t['misc'].toc()
    if self.timer_flag:
        print('Detection: {:d}/{:d} forward_pass_time: {:.4f}s misc: {:.4f}s'.format(1, 1, _t['forward_pass'].average_time, _t['misc'].average_time))
    det_bboxes = []
    for b in dets:
        if b[4] > vis_thres:
            (xmin, ymin, xmax, ymax, score) = (b[0], b[1], b[2], b[3], b[4])
            bbox = [xmin, ymin, xmax, ymax, score]
            det_bboxes.append(bbox)
    return det_bboxes","for b in dets:
    if b[4] > vis_thres:
        (xmin, ymin, xmax, ymax, score) = (b[0], b[1], b[2], b[3], b[4])
        bbox = [xmin, ymin, xmax, ymax, score]
        det_bboxes.append(bbox)","det_bboxes = [[b[0], b[1], b[2], b[3], b[4]] for b in dets if b[4] > vis_thres]","['for b in dets:\n    (b_0, b_1, b_2, b_3, b_4, *_) = b\n    if b_4 > vis_thres:\n        (xmin, ymin, xmax, ymax, score) = (b_0, b_1, b_2, b_3, b_4)\n        bbox = [xmin, ymin, xmax, ymax, score]\n        det_bboxes.append(bbox)', 'for (b_0, b_1, b_2, b_3, b_4, *b_len) in dets:\n    if \n    b_4 > vis_thres:\n        (xmin, ymin, xmax, ymax, score) = (\n        b_0, \n        b_1, \n        b_2, \n        b_3, \n        b_4)\n        bbox = [xmin, ymin, xmax, ymax, score]\n        det_bboxes.append(bbox)']",-1
frigate,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frigate/frigate/video.py,https://github.com/blakeblackshear/frigate/tree/master/frigate/video.py,,detect$404,"def detect(object_detector, frame, model_shape, region, objects_to_track, object_filters):
    tensor_input = create_tensor_input(frame, model_shape, region)
    detections = []
    region_detections = object_detector.detect(tensor_input)
    for d in region_detections:
        box = d[2]
        size = region[2] - region[0]
        x_min = int(box[1] * size + region[0])
        y_min = int(box[0] * size + region[1])
        x_max = int(box[3] * size + region[0])
        y_max = int(box[2] * size + region[1])
        det = (d[0], d[1], (x_min, y_min, x_max, y_max), (x_max - x_min) * (y_max - y_min), region)
        if filtered(det, objects_to_track, object_filters):
            continue
        detections.append(det)
    return detections","for d in region_detections:
    box = d[2]
    size = region[2] - region[0]
    x_min = int(box[1] * size + region[0])
    y_min = int(box[0] * size + region[1])
    x_max = int(box[3] * size + region[0])
    y_max = int(box[2] * size + region[1])
    det = (d[0], d[1], (x_min, y_min, x_max, y_max), (x_max - x_min) * (y_max - y_min), region)
    if filtered(det, objects_to_track, object_filters):
        continue
    detections.append(det)","detections = [(d[0], d[1], (int(d[2][1] * (region[2] - region[0]) + region[0]), int(d[2][0] * (region[2] - region[0]) + region[1]), int(d[2][3] * (region[2] - region[0]) + region[0]), int(d[2][2] * (region[2] - region[0]) + region[1])), int((d[2][3] - d[2][1]) * (d[2][2] - d[2][0])), region) for d in region_detections if not filtered((d[0], d[1], (int(d[2][1] * (region[2] - region[0]) + region[0]), int(d[2][0] * (region[2] - region[0]) + region[1]), int(d[2][3] * (region[2] - region[0]) + region[0]), int(d[2][2] * (region[2] - region[0]) + region[1])), int((d[2][3] - d[2][1]) * (d[2][2] - d[2][0])), region), objects_to_track, object_filters)]","['for d in region_detections:\n    (d_0, d_1, d_2, *_) = d\n    box = d_2\n    size = region[2] - region[0]\n    x_min = int(box[1] * size + region[0])\n    y_min = int(box[0] * size + region[1])\n    x_max = int(box[3] * size + region[0])\n    y_max = int(box[2] * size + region[1])\n    det = (d_0, d_1, (x_min, y_min, x_max, y_max), (x_max - x_min) * (y_max - y_min), region)\n    if filtered(det, objects_to_track, object_filters):\n        continue\n    detections.append(det)', 'for (d_0, d_1, d_2, *d_len) in region_detections:\n    box = \n    d_2\n    size = region[2] - region[0]\n    x_min = int(box[1] * size + region[0])\n    y_min = int(box[0] * size + region[1])\n    x_max = int(box[3] * size + region[0])\n    y_max = int(box[2] * size + region[1])\n    det = (\n    d_0, \n    d_1, (x_min, y_min, x_max, y_max), (x_max - x_min) * (y_max - y_min), region)\n    if filtered(det, objects_to_track, object_filters):\n        continue\n    detections.append(det)']",-1
Mycodo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/mycodo_flask/api/measurement.py,https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/api/measurement.py,MeasurementsPast,get$232,"def get(self, unique_id, unit, channel, past_seconds):
    """"""
        Return a list of measurements found within a duration from the past to the present
        """"""
    if not utils_general.user_has_permission('view_settings'):
        abort(403)
    if unit not in add_custom_units(Unit.query.all()):
        abort(422, custom='Unit ID not found')
    if channel < 0:
        abort(422, custom='channel must be >= 0')
    if past_seconds < 1:
        abort(422, custom='past_seconds must be >= 1')
    try:
        return_ = read_influxdb_list(unique_id, unit, channel, duration_sec=past_seconds)
        if return_ and len(return_) > 0:
            dict_return = {'measurements': []}
            for each_set in return_:
                dict_return['measurements'].append({'time': each_set[0], 'value': each_set[1]})
            return (dict_return, 200)
        else:
            return (return_, 200)
    except Exception:
        abort(500, message='An exception occurred', error=traceback.format_exc())","for each_set in return_:
    dict_return['measurements'].append({'time': each_set[0], 'value': each_set[1]})","dict_return = {'measurements': [{'time': each_set[0], 'value': each_set[1]} for each_set in return_]}","[""for each_set in return_:\n    (each_set_0, each_set_1, *_) = each_set\n    dict_return['measurements'].append({'time': each_set_0, 'value': each_set_1})"", ""for (each_set_0, each_set_1, *each_set_len) in return_:\n    dict_return['measurements'].append({'time': each_set_0, 'value': each_set_1})""]",-1
sympy_gamma,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy_gamma/app/views.py,https://github.com/sympy/sympy_gamma/tree/master/app/views.py,,random_example$132,"def random_example(request):
    examples = []
    for category in EXAMPLES:
        for subcategory in category[1]:
            for example in subcategory[1]:
                if isinstance(example, tuple):
                    examples.append(example[1])
                else:
                    examples.append(example)
    return redirect('input/?i=' + six.moves.urllib.parse.quote(random.choice(examples)))","for category in EXAMPLES:
    for subcategory in category[1]:
        for example in subcategory[1]:
            if isinstance(example, tuple):
                examples.append(example[1])
            else:
                examples.append(example)","examples = [example[1] if isinstance(example, tuple) else example for category in EXAMPLES for subcategory in category[1] for example in subcategory[1]]
return redirect('input/?i=' + six.moves.urllib.parse.quote(random.choice(examples)))","['for category in EXAMPLES:\n    (category_0, category_1, *category_rcategorymaining) = category\n    for subcategory in category_1:\n        for example in subcategory_1:\n            if isinstance(example, tuple):\n                examples.append(example[1])\n            else:\n                examples.append(example)', 'for (category_0, category_1, *category_len) in EXAMPLES:\n    for subcategory in \n    category_1:\n        for example in subcategory[1]:\n            if isinstance(example, tuple):\n                examples.append(example[1])\n            else:\n                examples.append(example)']",-1
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/setup/doctype/company/company.py,https://github.com/frappe/erpnext/tree/master/erpnext/setup/doctype/company/company.py,Company,validate_default_accounts$88,"def validate_default_accounts(self):
    accounts = [['Default Bank Account', 'default_bank_account'], ['Default Cash Account', 'default_cash_account'], ['Default Receivable Account', 'default_receivable_account'], ['Default Payable Account', 'default_payable_account'], ['Default Expense Account', 'default_expense_account'], ['Default Income Account', 'default_income_account'], ['Stock Received But Not Billed Account', 'stock_received_but_not_billed'], ['Stock Adjustment Account', 'stock_adjustment_account'], ['Expense Included In Valuation Account', 'expenses_included_in_valuation']]
    for account in accounts:
        if self.get(account[1]):
            for_company = frappe.db.get_value('Account', self.get(account[1]), 'company')
            if for_company != self.name:
                frappe.throw(_('Account {0} does not belong to company: {1}').format(self.get(account[1]), self.name))
            if get_account_currency(self.get(account[1])) != self.default_currency:
                error_message = _(""{0} currency must be same as company's default currency. Please select another account."").format(frappe.bold(account[0]))
                frappe.throw(error_message)","for account in accounts:
    if self.get(account[1]):
        for_company = frappe.db.get_value('Account', self.get(account[1]), 'company')
        if for_company != self.name:
            frappe.throw(_('Account {0} does not belong to company: {1}').format(self.get(account[1]), self.name))
        if get_account_currency(self.get(account[1])) != self.default_currency:
            error_message = _(""{0} currency must be same as company's default currency. Please select another account."").format(frappe.bold(account[0]))
            frappe.throw(error_message)","for (_, account_name) in accounts:
    account_value = self.get(account_name)
    if account_value:
        for_company = frappe.db.get_value('Account', account_value, 'company')
        if for_company != self.name:
            frappe.throw(_('Account {0} does not belong to company: {1}').format(account_value, self.name))
        if get_account_currency(account_value) != self.default_currency:
            error_message = _(""{0} currency must be same as company's default currency. Please select another account."").format(frappe.bold(_[0]))
            frappe.throw(error_message)","['for account in accounts:\n    (account_0, account_1, *_) = account\n    if self.get(account_1):\n        for_company = frappe.db.get_value(\'Account\', self.get(account_1), \'company\')\n        if for_company != self.name:\n            frappe.throw(_(\'Account {0} does not belong to company: {1}\').format(self.get(account_1), self.name))\n        if get_account_currency(self.get(account_1)) != self.default_currency:\n            error_message = _(""{0} currency must be same as company\'s default currency. Please select another account."").format(frappe.bold(account_0))\n            frappe.throw(error_message)', 'for (account_0, account_1, *account_len) in accounts:\n    if self.get(account_1):\n        for_company = frappe.db.get_value(\'Account\', self.get(account_1), \'company\')\n        if for_company != self.name:\n            frappe.throw(_(\'Account {0} does not belong to company: {1}\').format(self.get(account_1), self.name))\n        if get_account_currency(self.get(account_1)) != self.default_currency:\n            error_message = _(""{0} currency must be same as company\'s default currency. Please select another account."").format(frappe.bold(account_0))\n            frappe.throw(error_message)']",-1
pylearn2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pylearn2/pylearn2/scripts/tutorials/jobman_demo/utils.py,https://github.com/lisa-lab/pylearn2/tree/master/pylearn2/scripts/tutorials/jobman_demo/utils.py,,parse_results$44,"def parse_results(cwd):
    optimal_dd = None
    optimal_measure = numpy.inf
    for tup in tools.find_conf_files(cwd):
        dd = tup[1]
        if 'results.train_y_misclass' in dd:
            if dd['results.train_y_misclass'] < optimal_measure:
                optimal_measure = dd['results.train_y_misclass']
                optimal_dd = dd
    print('Optimal results.train_y_misclass:', str(optimal_measure))
    for (key, value) in optimal_dd.items():
        if 'hyper_parameters' in key:
            print(key + ': ' + str(value))","for tup in tools.find_conf_files(cwd):
    dd = tup[1]
    if 'results.train_y_misclass' in dd:
        if dd['results.train_y_misclass'] < optimal_measure:
            optimal_measure = dd['results.train_y_misclass']
            optimal_dd = dd","for (_, dd) in tools.find_conf_files(cwd):
    if 'results.train_y_misclass' in dd:
        if dd['results.train_y_misclass'] < optimal_measure:
            optimal_measure = dd['results.train_y_misclass']
            optimal_dd = dd","[""for tup in tools.find_conf_files(cwd):\n    (_, tup_1, *tup_rtupmaining) = tup\n    dd = tup_1\n    if 'results.train_y_misclass' in dd:\n        if dd['results.train_y_misclass'] < optimal_measure:\n            optimal_measure = dd['results.train_y_misclass']\n            optimal_dd = dd"", ""for (tup_0, tup_1, *tup_len) in tools.find_conf_files(cwd):\n    dd = \n    tup_1\n    if 'results.train_y_misclass' in dd:\n        if dd['results.train_y_misclass'] < optimal_measure:\n            optimal_measure = dd['results.train_y_misclass']\n            optimal_dd = dd""]",-1
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/dygraph_to_static/test_for_enumerate.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/dygraph_to_static/test_for_enumerate.py,,for_tuple_as_enumerate_iter$258,"def for_tuple_as_enumerate_iter(x_array):
    x = paddle.to_tensor(x_array)
    x_list = [x, x, x]
    a_result = paddle.zeros([5])
    for t in enumerate(x_list):
        a_result += t[1]
    return a_result","for t in enumerate(x_list):
    a_result += t[1]","for (_, t) in enumerate(x_list):
    a_result += t","['for t in enumerate(x_list):\n    (t_0, t_1, *t_rtmaining) = t\n    a_result += t_1', 'for (t_0, t_1, *t_len) in enumerate(x_list):\n    a_result += \n    t_1']",-1
build-webos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/build-webos/scripts/webos-iot-scripts/set_webosiot_layer.py,https://github.com/webosose/build-webos/tree/master/scripts/webos-iot-scripts/set_webosiot_layer.py,,make_webosiot_layers_as_rule$9,"def make_webosiot_layers_as_rule(path):
    sys.path.insert(0, path)
    if not os.path.isfile(os.path.join(path, 'weboslayers.py')):
        raise Exception('Error: Configuration file %s does not exist!' % os.path.join(path, 'weboslayers.py'))
    from webosiot_rule import webosiot_layer_rules
    from weboslayers import webos_layers
    maxpriority = webos_layers[-1][1]
    for rule in webosiot_layer_rules:
        action = rule[0]
        if action == 'remove':
            del webos_layers[layer_index(webos_layers, rule[1])]
        elif action == 'insert':
            webos_layers.insert(layer_index(webos_layers, rule[1]) + 1, rule[2])
        elif action == 'append':
            appendlayer = list(rule[1])
            appendlayer[1] = maxpriority + 1
            webos_layers.append(tuple(appendlayer))
            maxpriority += 1
    return webos_layers","for rule in webosiot_layer_rules:
    action = rule[0]
    if action == 'remove':
        del webos_layers[layer_index(webos_layers, rule[1])]
    elif action == 'insert':
        webos_layers.insert(layer_index(webos_layers, rule[1]) + 1, rule[2])
    elif action == 'append':
        appendlayer = list(rule[1])
        appendlayer[1] = maxpriority + 1
        webos_layers.append(tuple(appendlayer))
        maxpriority += 1","for (action, *rule) in webosiot_layer_rules:
    if action == 'remove':
        del webos_layers[layer_index(webos_layers, rule[0])]
    elif action == 'insert':
        webos_layers.insert(layer_index(webos_layers, rule[0]) + 1, rule[1])
    elif action == 'append':
        appendlayer = list(rule[0])
        appendlayer[1] = maxpriority + 1
        webos_layers.append(tuple(appendlayer))
        maxpriority += 1","[""for rule in webosiot_layer_rules:\n    (rule_0, rule_1, rule_2, *_) = rule\n    action = rule_0\n    if action == 'remove':\n        del webos_layers[layer_index(webos_layers, rule_1)]\n    elif action == 'insert':\n        webos_layers.insert(layer_index(webos_layers, rule_1) + 1, rule_2)\n    elif action == 'append':\n        appendlayer = list(rule_1)\n        appendlayer[1] = maxpriority + 1\n        webos_layers.append(tuple(appendlayer))\n        maxpriority += 1"", ""for (rule_0, rule_1, rule_2, *rule_len) in webosiot_layer_rules:\n    action = \n    rule_0\n    if action == 'remove':\n        del webos_layers[layer_index(webos_layers, \n        rule_1)]\n    elif action == 'insert':\n        webos_layers.insert(layer_index(webos_layers, \n        rule_1) + 1, \n        rule_2)\n    elif action == 'append':\n        appendlayer = list(\n        rule_1)\n        appendlayer[1] = maxpriority + 1\n        webos_layers.append(tuple(appendlayer))\n        maxpriority += 1""]",1
gif-for-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gif-for-cli/tests/test_display.py,https://github.com/google/gif-for-cli/tree/master/tests/test_display.py,TestDisplayTxtFrames,test_0_loops$58,"def test_0_loops(self):
    stdout = io.StringIO()
    txt_frames = self.txt_frames
    num_loops = 0
    error_after_num_loops = 5
    error_after_num_sleep_calls = error_after_num_loops * len(txt_frames)
    with patch('time.sleep') as mock_sleep:
        num_sleep_calls = 0

        def sleep_side_effect(s):
            nonlocal num_sleep_calls
            num_sleep_calls += 1
            if num_sleep_calls >= error_after_num_sleep_calls:
                raise KeyboardInterrupt()
            return
        mock_sleep.side_effect = sleep_side_effect
        display_txt_frames(txt_frames, stdout, num_loops, self.seconds_per_frame)
    self.assertEqual(mock_sleep.call_count, error_after_num_loops * len(txt_frames))
    for call in mock_sleep.call_args_list:
        self.assertEqual(call[0][0], self.seconds_per_frame)
    output_ending = '\n' + ANSI_RESET + '\n'
    output = stdout.getvalue()
    self.assertEqual(output[-len(output_ending):], output_ending)
    output = output[:-len(output_ending)]
    output = output.split('\n' + ANSI_CURSOR_UP * self.height)
    self.assertEqual(output, self.txt_frames * error_after_num_loops)","for call in mock_sleep.call_args_list:
    self.assertEqual(call[0][0], self.seconds_per_frame)","for (args, kwargs) in mock_sleep.call_args_list:
    self.assertEqual(args[0], self.seconds_per_frame)","['for ((call_0_0, *call_0_len), *call_len) in mock_sleep.call_args_list:\n    self.assertEqual(call_0_0, self.seconds_per_frame)']",-1
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/gluoncv/auto/estimators/yolo/yolo.py,https://github.com/dmlc/gluon-cv/tree/master/gluoncv/auto/estimators/yolo/yolo.py,YOLOv3Estimator,_evaluate$241,"def _evaluate(self, val_data):
    """"""Evaluate the current model on dataset.""""""
    if not isinstance(val_data, gluon.data.DataLoader):
        if hasattr(val_data, 'to_mxnet'):
            val_data = val_data.to_mxnet()
        val_batchify_fn = Tuple(Stack(), Pad(pad_val=-1))
        val_data = gluon.data.DataLoader(val_data.transform(YOLO3DefaultValTransform(self._cfg.yolo3.data_shape, self._cfg.yolo3.data_shape)), self._cfg.valid.batch_size, False, batchify_fn=val_batchify_fn, last_batch='keep', num_workers=self._cfg.num_workers)
    if self._cfg.valid.metric == 'voc07':
        eval_metric = VOC07MApMetric(iou_thresh=self._cfg.valid.iou_thresh, class_names=self.classes)
    elif self._cfg.valid.metric == 'voc':
        eval_metric = VOCMApMetric(iou_thresh=self._cfg.valid.iou_thresh, class_names=self.classes)
    else:
        raise ValueError(f'Invalid metric type: {self._cfg.valid.metric}')
    self.net.collect_params().reset_ctx(self.ctx)
    self.net.set_nms(nms_thresh=self._cfg.yolo3.nms_thresh, nms_topk=self._cfg.yolo3.nms_topk)
    mx.nd.waitall()
    self.net.hybridize()
    for batch in val_data:
        val_ctx = self.ctx
        if batch[0].shape[0] < len(val_ctx):
            val_ctx = val_ctx[:batch[0].shape[0]]
        data = gluon.utils.split_and_load(batch[0], ctx_list=val_ctx, batch_axis=0, even_split=False)
        label = gluon.utils.split_and_load(batch[1], ctx_list=val_ctx, batch_axis=0, even_split=False)
        det_bboxes = []
        det_ids = []
        det_scores = []
        gt_bboxes = []
        gt_ids = []
        gt_difficults = []
        for (x, y) in zip(data, label):
            (ids, scores, bboxes) = self.net(x)
            det_ids.append(ids)
            det_scores.append(scores)
            det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
            gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
            gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
            gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)
        eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)
    return eval_metric.get()","for batch in val_data:
    val_ctx = self.ctx
    if batch[0].shape[0] < len(val_ctx):
        val_ctx = val_ctx[:batch[0].shape[0]]
    data = gluon.utils.split_and_load(batch[0], ctx_list=val_ctx, batch_axis=0, even_split=False)
    label = gluon.utils.split_and_load(batch[1], ctx_list=val_ctx, batch_axis=0, even_split=False)
    det_bboxes = []
    det_ids = []
    det_scores = []
    gt_bboxes = []
    gt_ids = []
    gt_difficults = []
    for (x, y) in zip(data, label):
        (ids, scores, bboxes) = self.net(x)
        det_ids.append(ids)
        det_scores.append(scores)
        det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)
    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)","for (batch_0, batch_1, *_) in val_data:
    val_ctx = self.ctx
    if batch_0.shape[0] < len(val_ctx):
        val_ctx = val_ctx[:batch_0.shape[0]]
    data = gluon.utils.split_and_load(batch_0, ctx_list=val_ctx, batch_axis=0, even_split=False)
    label = gluon.utils.split_and_load(batch_1, ctx_list=val_ctx, batch_axis=0, even_split=False)
    det_bboxes = []
    det_ids = []
    det_scores = []
    gt_bboxes = []
    gt_ids = []
    gt_difficults = []
    for (x, y) in zip(data, label):
        (ids, scores, bboxes) = self.net(x)
        det_ids.append(ids)
        det_scores.append(scores)
        det_bboxes.append(bboxes.clip(0, batch_0.shape[2]))
        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)
    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)","['for batch in val_data:\n    (batch_0, batch_1, *_) = batch\n    val_ctx = self.ctx\n    if batch_0.shape[0] < len(val_ctx):\n        val_ctx = val_ctx[:batch_0.shape[0]]\n    data = gluon.utils.split_and_load(batch_0, ctx_list=val_ctx, batch_axis=0, even_split=False)\n    label = gluon.utils.split_and_load(batch_1, ctx_list=val_ctx, batch_axis=0, even_split=False)\n    det_bboxes = []\n    det_ids = []\n    det_scores = []\n    gt_bboxes = []\n    gt_ids = []\n    gt_difficults = []\n    for (x, y) in zip(data, label):\n        (ids, scores, bboxes) = self.net(x)\n        det_ids.append(ids)\n        det_scores.append(scores)\n        det_bboxes.append(bboxes.clip(0, batch_0.shape[2]))\n        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))\n        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))\n        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)\n    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)', 'for (batch_0, batch_1, *batch_len) in val_data:\n    val_ctx = self.ctx\n    if \n    batch_0.shape[0] < len(val_ctx):\n        val_ctx = val_ctx[:\n        batch_0.shape[0]]\n    data = gluon.utils.split_and_load(\n    batch_0, ctx_list=val_ctx, batch_axis=0, even_split=False)\n    label = gluon.utils.split_and_load(\n    batch_1, ctx_list=val_ctx, batch_axis=0, even_split=False)\n    det_bboxes = []\n    det_ids = []\n    det_scores = []\n    gt_bboxes = []\n    gt_ids = []\n    gt_difficults = []\n    for (x, y) in zip(data, label):\n        (ids, scores, bboxes) = self.net(x)\n        det_ids.append(ids)\n        det_scores.append(scores)\n        det_bboxes.append(bboxes.clip(0, \n        batch_0.shape[2]))\n        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))\n        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))\n        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)\n    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)']",1
checkov,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkov/tests/terraform/parser/test_parser_var_blocks.py,https://github.com/bridgecrewio/checkov/tree/master/tests/terraform/parser/test_parser_var_blocks.py,TestParserInternals,test_split_merge_args$10,"def test_split_merge_args(self):
    cases: List[Tuple[str, List[str]]] = [('local.one, local.two', ['local.one', 'local.two']), ('{Tag4 = ""four""}, {Tag5 = ""five""}', ['{Tag4 = ""four""}', '{Tag5 = ""five""}']), ('{a=""b""}, {a=[1,2], c=""z""}, {d=3}', ['{a=""b""}', '{a=[1,2], c=""z""}', '{d=3}']), ('local.common_tags, merge({Tag4 = ""four""}, {Tag5 = ""five""})', ['local.common_tags', 'merge({Tag4 = ""four""}, {Tag5 = ""five""})']), (', ', None), ('', None), (', leading_comma', ['leading_comma']), ('kinda_maybe_shouldnt_work_but_we_will_roll_with_it, ', ['kinda_maybe_shouldnt_work_but_we_will_roll_with_it']), ('local.one', ['local.one']), ('{""a"": ""}, evil""}', ['{""a"": ""}, evil""}']), (""{'a': '}, evil'}"", [""{'a': '}, evil'}""]), (""${merge({'a': '}, evil'})}"", [""${merge({'a': '}, evil'})}""]), (""local.common_tags,,{'Tag4': 'four'},,{'Tag2': 'Dev'},"", ['local.common_tags', ""{'Tag4': 'four'}"", ""{'Tag2': 'Dev'}""])]
    for case in cases:
        actual = split_merge_args(case[0])
        assert actual == case[1], f'Case ""{case[0]}"" failed. Expected: {case[1]}  Actual: {actual}'","for case in cases:
    actual = split_merge_args(case[0])
    assert actual == case[1], f'Case ""{case[0]}"" failed. Expected: {case[1]}  Actual: {actual}'","for (case_0, case_1) in cases:
    actual = split_merge_args(case_0)
    assert actual == case_1, f'Case ""{case_0}"" failed. Expected: {case_1}  Actual: {actual}'","['for case in cases:\n    (case_0, case_1, *_) = case\n    actual = split_merge_args(case_0)\n    assert actual == case_1, f\'Case ""{case_0}"" failed. Expected: {case_1}  Actual: {actual}\'', 'for (case_0, case_1, *case_len) in cases:\n    actual = split_merge_args(\n    case_0)\n    assert actual == \n    case_1, f\'Case ""{case_0}"" failed. Expected: {case_1}  Actual: {actual}\'']",-1
jesse,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jesse/jesse/store/state_candles.py,https://github.com/jesse-ai/jesse/tree/master/jesse/store/state_candles.py,CandlesState,time_loop_per_second$29,"def time_loop_per_second():
    if not self.are_all_initiated:
        return
    if jh.now() % 60000 != 1000:
        return
    for c in config['app']['considering_candles']:
        (exchange, symbol) = (c[0], c[1])
        current_candle = self.get_current_candle(exchange, symbol, '1m')
        if current_candle[0] <= 60000:
            continue
        if jh.now() > current_candle[0] + 60000:
            new_candle = self._generate_empty_candle_from_previous_candle(current_candle)
            self.add_candle(new_candle, exchange, symbol, '1m')","for c in config['app']['considering_candles']:
    (exchange, symbol) = (c[0], c[1])
    current_candle = self.get_current_candle(exchange, symbol, '1m')
    if current_candle[0] <= 60000:
        continue
    if jh.now() > current_candle[0] + 60000:
        new_candle = self._generate_empty_candle_from_previous_candle(current_candle)
        self.add_candle(new_candle, exchange, symbol, '1m')","for (exchange, symbol) in config['app']['considering_candles']:
    current_candle = self.get_current_candle(exchange, symbol, '1m')
    if current_candle[0] <= 60000:
        continue
    if jh.now() > current_candle[0] + 60000:
        new_candle = self._generate_empty_candle_from_previous_candle(current_candle)
        self.add_candle(new_candle, exchange, symbol, '1m')","[""for c in config['app']['considering_candles']:\n    (c_0, c_1, *_) = c\n    (exchange, symbol) = (c_0, c_1)\n    current_candle = self.get_current_candle(exchange, symbol, '1m')\n    if current_candle[0] <= 60000:\n        continue\n    if jh.now() > current_candle[0] + 60000:\n        new_candle = self._generate_empty_candle_from_previous_candle(current_candle)\n        self.add_candle(new_candle, exchange, symbol, '1m')"", ""for (c_0, c_1, *c_len) in config['app']['considering_candles']:\n    (exchange, symbol) = (c_0, c_1)\n    current_candle = self.get_current_candle(exchange, symbol, '1m')\n    if current_candle[0] <= 60000:\n        continue\n    if jh.now() > current_candle[0] + 60000:\n        new_candle = self._generate_empty_candle_from_previous_candle(current_candle)\n        self.add_candle(new_candle, exchange, symbol, '1m')""]",-1
freeipa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipatests/test_integration/test_epn.py,https://github.com/freeipa/freeipa/tree/master/ipatests/test_integration/test_epn.py,TestEPN,test_EPN_nbdays_1$559,"def test_EPN_nbdays_1(self, cleanupmail):
    """"""Test that for a given range, we find the users in that range""""""
    for date_range in [(0, 5), (7, 15), (1, 20)]:
        expected_user_list = ['user{i}'.format(i=i) for i in range(date_range[0], date_range[1])]
        (stdout_text_client, unused, _unused) = self._check_epn_output(self.clients[0], from_nbdays=date_range[0], to_nbdays=date_range[1])
        user_list = [user['uid'] for user in json.loads(stdout_text_client)]
        for user in expected_user_list:
            assert user in user_list
        for user in user_list:
            assert user in expected_user_list","for date_range in [(0, 5), (7, 15), (1, 20)]:
    expected_user_list = ['user{i}'.format(i=i) for i in range(date_range[0], date_range[1])]
    (stdout_text_client, unused, _unused) = self._check_epn_output(self.clients[0], from_nbdays=date_range[0], to_nbdays=date_range[1])
    user_list = [user['uid'] for user in json.loads(stdout_text_client)]
    for user in expected_user_list:
        assert user in user_list
    for user in user_list:
        assert user in expected_user_list","for (from_nbdays, to_nbdays) in [(0, 5), (7, 15), (1, 20)]:
    expected_user_list = ['user{i}'.format(i=i) for i in range(from_nbdays, to_nbdays)]
    (stdout_text_client, unused, _unused) = self._check_epn_output(self.clients[0], from_nbdays=from_nbdays, to_nbdays=to_nbdays)
    user_list = [user['uid'] for user in json.loads(stdout_text_client)]
    for user in expected_user_list:
        assert user in user_list
    for user in user_list:
        assert user in expected_user_list","[""for date_range in [(0, 5), (7, 15), (1, 20)]:\n    (date_range_0, date_range_1, *_) = date_range\n    expected_user_list = ['user{i}'.format(i=i) for i in range(date_range_0, date_range_1)]\n    (stdout_text_client, unused, _unused) = self._check_epn_output(self.clients[0], from_nbdays=date_range_0, to_nbdays=date_range_1)\n    user_list = [user['uid'] for user in json.loads(stdout_text_client)]\n    for user in expected_user_list:\n        assert user in user_list\n    for user in user_list:\n        assert user in expected_user_list"", ""for (date_range_0, date_range_1, *date_range_len) in [(0, 5), (7, 15), (1, 20)]:\n    expected_user_list = ['user{i}'.format(i=i) for i in range(date_range_0, date_range_1)]\n    (stdout_text_client, unused, _unused) = self._check_epn_output(self.clients[0], from_nbdays=date_range_0, to_nbdays=date_range_1)\n    user_list = [user['uid'] for user in json.loads(stdout_text_client)]\n    for user in expected_user_list:\n        assert user in user_list\n    for user in user_list:\n        assert user in expected_user_list""]",-1
SMARTS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMARTS/zoo/evaluation/metrics/evaluation_report.py,https://github.com/huawei-noah/SMARTS/tree/master/zoo/evaluation/metrics/evaluation_report.py,EvaluationReport,__init__$12,"def __init__(self, scenarios_list, agents_list, csv_file_result_path):
    self.scenarios_list = scenarios_list
    self.csv_file_result_path = csv_file_result_path
    self.group_agents_list = []
    for value in agents_list.items():
        for agent in value[1]:
            self.group_agents_list.append(value[0] + ':' + agent)
    self.agents_list = [agent.split(':')[-1] for agent in self.group_agents_list]
    self.group_list = [agent.split(':')[0] for agent in self.group_agents_list]","for value in agents_list.items():
    for agent in value[1]:
        self.group_agents_list.append(value[0] + ':' + agent)","for (group, agents) in agents_list.items():
    self.group_agents_list.extend([f'{group}:{agent}' for agent in agents])","[""for value in agents_list.items():\n    (value_0, value_1, *_) = value\n    for agent in value_1:\n        self.group_agents_list.append(value_0 + ':' + agent)"", ""for (value_0, value_1, *value_len) in agents_list.items():\n    for agent in \n    value_1:\n        self.group_agents_list.append(\n        value_0 + ':' + agent)""]",-1
CrackMapExec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CrackMapExec/cme/protocols/smb/db_navigator.py,https://github.com/byt3bl33d3r/CrackMapExec/tree/master/cme/protocols/smb/db_navigator.py,navigator,display_groups$29,"def display_groups(self, groups):
    data = [['GroupID', 'Domain', 'Name', 'Members']]
    for group in groups:
        groupID = group[0]
        domain = group[1]
        name = group[2]
        members = len(self.db.get_group_relations(groupID=groupID))
        data.append([groupID, domain, name, members])
    self.print_table(data, title='Groups')","for group in groups:
    groupID = group[0]
    domain = group[1]
    name = group[2]
    members = len(self.db.get_group_relations(groupID=groupID))
    data.append([groupID, domain, name, members])","for (groupID, domain, name, *_) in groups:
    members = len(self.db.get_group_relations(groupID=groupID))
    data.append([groupID, domain, name, members])","['for group in groups:\n    (group_0, group_1, group_2, *_) = group\n    groupID = group_0\n    domain = group_1\n    name = group_2\n    members = len(self.db.get_group_relations(groupID=groupID))\n    data.append([groupID, domain, name, members])', 'for (group_0, group_1, group_2, *group_len) in groups:\n    groupID = \n    group_0\n    domain = \n    group_1\n    name = \n    group_2\n    members = len(self.db.get_group_relations(groupID=groupID))\n    data.append([groupID, domain, name, members])']",1
Pyro4,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pyro4/tests/PyroTests/test_httpgateway.py,https://github.com/irmen/Pyro4/tree/master/tests/PyroTests/test_httpgateway.py,WSGITestBase,_start_response$72,"def _start_response(self, status, headers):
    """"""A callback passed into the application, to simulate a wsgi
        environment.

        @param status: The response status of the application (""200"", ""404"", etc)
        @param headers: Any headers to begin the response with.
        """"""
    assert not self.response_started
    self.response_started = True
    self.status = status
    self.headers = headers
    for header in headers:
        if header[0] == 'Set-Cookie':
            var = header[1].split(';', 1)
            if len(var) > 1 and var[1][0:9] == ' Max-Age=':
                if int(var[1][9:]) > 0:
                    self.cookies.append(var[0])
                else:
                    index = self.cookies.index(var[0])
                    self.cookies.pop(index)","for header in headers:
    if header[0] == 'Set-Cookie':
        var = header[1].split(';', 1)
        if len(var) > 1 and var[1][0:9] == ' Max-Age=':
            if int(var[1][9:]) > 0:
                self.cookies.append(var[0])
            else:
                index = self.cookies.index(var[0])
                self.cookies.pop(index)","for (header, value) in headers:
    if header == 'Set-Cookie':
        var = value.split(';', 1)
        if len(var) > 1 and var[1][0:9] == ' Max-Age=':
            if int(var[1][9:]) > 0:
                self.cookies.append(var[0])
            else:
                index = self.cookies.index(var[0])
                self.cookies.pop(index)","[""for header in headers:\n    (header_0, header_1, *_) = header\n    if header_0 == 'Set-Cookie':\n        var = header_1.split(';', 1)\n        if len(var) > 1 and var[1][0:9] == ' Max-Age=':\n            if int(var[1][9:]) > 0:\n                self.cookies.append(var[0])\n            else:\n                index = self.cookies.index(var[0])\n                self.cookies.pop(index)"", ""for (header_0, header_1, *header_len) in headers:\n    if \n    header_0 == 'Set-Cookie':\n        var = \n        header_1.split(';', 1)\n        if len(var) > 1 and var[1][0:9] == ' Max-Age=':\n            if int(var[1][9:]) > 0:\n                self.cookies.append(var[0])\n            else:\n                index = self.cookies.index(var[0])\n                self.cookies.pop(index)""]",-1
anki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anki/pylib/anki/stats.py,https://github.com/ankitects/anki/tree/master/pylib/anki/stats.py,CollectionStats,hourGraph$772,"def hourGraph(self) -> str:
    data = self._hourRet()
    if not data:
        return ''
    shifted = []
    counts = []
    mcount = 0
    trend: list[tuple[int, int]] = []
    peak = 0
    for d in data:
        hour = (d[0] - 4) % 24
        pct = d[1]
        if pct > peak:
            peak = pct
        shifted.append((hour, pct))
        counts.append((hour, d[2]))
        if d[2] > mcount:
            mcount = d[2]
    shifted.sort()
    counts.sort()
    if len(counts) < 4:
        return ''
    for d in shifted:
        hour = d[0]
        pct = d[1]
        if not trend:
            trend.append((hour, pct))
        else:
            prev = trend[-1][1]
            diff = pct - prev
            diff /= 3.0
            diff = round(diff, 1)
            trend.append((hour, prev + diff))
    txt = self._title('Hourly Breakdown', 'Review success rate for each hour of the day.')
    txt += self._graph(id='hour', data=[dict(data=shifted, color=colCum, label='% Correct'), dict(data=counts, color=colHour, label='Answers', yaxis=2, bars=dict(barWidth=0.2), stack=False)], conf=dict(xaxis=dict(ticks=[[0, '4AM'], [6, '10AM'], [12, '4PM'], [18, '10PM'], [23, '3AM']]), yaxes=[dict(max=peak), dict(position='right', max=mcount)]), ylabel='% Correct', ylabel2='Reviews')
    txt += 'Hours with less than 30 reviews are not shown.'
    return txt","for d in shifted:
    hour = d[0]
    pct = d[1]
    if not trend:
        trend.append((hour, pct))
    else:
        prev = trend[-1][1]
        diff = pct - prev
        diff /= 3.0
        diff = round(diff, 1)
        trend.append((hour, prev + diff))","for (hour, pct) in shifted:
    if not trend:
        trend.append((hour, pct))
    else:
        prev = trend[-1][1]
        diff = (pct - prev) / 3.0
        diff = round(diff, 1)
        trend.append((hour, prev + diff))","['for d in shifted:\n    (d_0, d_1, *_) = d\n    hour = d_0\n    pct = d_1\n    if not trend:\n        trend.append((hour, pct))\n    else:\n        prev = trend[-1][1]\n        diff = pct - prev\n        diff /= 3.0\n        diff = round(diff, 1)\n        trend.append((hour, prev + diff))', 'for (d_0, d_1, *d_len) in shifted:\n    hour = \n    d_0\n    pct = \n    d_1\n    if not trend:\n        trend.append((hour, pct))\n    else:\n        prev = trend[-1][1]\n        diff = pct - prev\n        diff /= 3.0\n        diff = round(diff, 1)\n        trend.append((hour, prev + diff))']",-1
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/extra/tunings.py,https://github.com/bspaans/python-mingus/tree/master/mingus/extra/tunings.py,StringTuning,find_chord_fingering$149,"def find_chord_fingering(self, notes, max_distance=4, maxfret=18, max_fingers=4, return_best_as_NoteContainer=False):
    """"""Return a list of fret lists that are considered possible fingerings.

        This function only looks at and matches on the note _names_ so it
        does more than find_fingering.

        Example:
        >>> t = get_tuning('guitar', 'standard', 6, 1)
        >>> t.find_chord_fingering(NoteContainer().from_chord('Am'))
        [[0, 0, 2, 2, 1, 0], [0, 3, 2, 2, 1, 0], ......]
        """"""

    def follow(string, next, name, prev=-1):
        """"""Follow the fret 'next' on 'string'; build result on the way.""""""
        if string >= len(self.tuning) - 1:
            return [[(next, name)]]
        result = []
        cur = res[string][next]
        if cur != []:
            for y in cur[1]:
                for sub in follow(string + 1, y[0], y[1]):
                    if prev < 0:
                        result.append([(next, name)] + sub)
                    elif sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
                        result.append([(next, name)] + sub)
        for s in follow(string + 1, maxfret + 1, None, next):
            result.append([(next, name)] + s)
        return [[(next, name)]] if result == [] else result

    def make_lookup_table():
        """"""Prepare the lookup table.

            table[string][fret] = (name, dest_frets)
            """"""
        res = [[[] for x in range(maxfret + 2)] for x in range(len(self.tuning) - 1)]
        for x in range(0, len(self.tuning) - 1):
            addedNone = -1
            next = fretdict[x + 1]
            for (fret, name) in fretdict[x]:
                for (f2, n2) in next:
                    if n2 != name and (f2 == 0 or abs(fret - f2) < max_distance):
                        if res[x][fret] != []:
                            res[x][fret][1].append((f2, n2))
                        else:
                            res[x][fret] = (name, [(f2, n2)])
                    if addedNone < x:
                        if res[x][maxfret + 1] != []:
                            res[x][maxfret + 1][1].append((f2, n2))
                        else:
                            res[x][maxfret + 1] = (None, [(f2, n2)])
                addedNone = x
        return res
    n = notes
    if notes != [] and isinstance(notes, list) and isinstance(notes[0], six.string_types):
        n = NoteContainer(notes)
    notenames = [x.name for x in n]
    if len(notenames) == 0 or len(notenames) > len(self.tuning):
        return []
    fretdict = []
    for x in range(0, len(self.tuning)):
        fretdict.append(self.find_note_names(notes, x, maxfret))
    res = make_lookup_table()
    result = []
    for (i, y) in enumerate(res[0]):
        if y != []:
            (yname, next) = (y[0], y[1])
            for (fret, name) in next:
                for s in follow(1, fret, name):
                    subresult = [(i, yname)] + s
                    (mi, ma, names) = (1000, -1000, [])
                    for (f, n) in subresult:
                        if n is not None:
                            if f != 0 and f <= mi:
                                mi = f
                            if f != 0 and f >= ma:
                                ma = f
                            names.append(n)
                    if abs(ma - mi) < max_distance:
                        covered = True
                        for n in notenames:
                            if n not in names:
                                covered = False
                        if covered and names != []:
                            result.append([y[0] if y[1] is not None else y[1] for y in subresult])
    s = sorted(result, key=lambda x: sum([t if t is not None else 1000 for (i, t) in enumerate(x)]))
    s = [a for a in s if fingers_needed(a) <= max_fingers]
    if not return_best_as_NoteContainer:
        return s
    else:
        rnotes = self.frets_to_NoteContainer(s[0])
        for (i, x) in enumerate(rnotes):
            if x.string < len(self.tuning) - 1:
                if res[x.string][x.fret] != []:
                    rnotes[i].name = res[x.string][x.fret][0]
        return rnotes","for (i, y) in enumerate(res[0]):
    if y != []:
        (yname, next) = (y[0], y[1])
        for (fret, name) in next:
            for s in follow(1, fret, name):
                subresult = [(i, yname)] + s
                (mi, ma, names) = (1000, -1000, [])
                for (f, n) in subresult:
                    if n is not None:
                        if f != 0 and f <= mi:
                            mi = f
                        if f != 0 and f >= ma:
                            ma = f
                        names.append(n)
                if abs(ma - mi) < max_distance:
                    covered = True
                    for n in notenames:
                        if n not in names:
                            covered = False
                    if covered and names != []:
                        result.append([y[0] if y[1] is not None else y[1] for y in subresult])","for (i, y) in enumerate(res[0]):
    if y:
        (yname, next) = (y[0], y[1])
        for (fret, name) in next:
            subresult = [(i, yname)] + follow(1, fret, name)
            (mi, ma, names) = (1000, -1000, [])
            for (f, n) in subresult:
                if n:
                    if f and f <= mi:
                        mi = f
                    if f and f >= ma:
                        ma = f
                    names.append(n)
            if abs(ma - mi) < max_distance and all((n in names for n in notenames)):
                result.append([y[0] if y[1] else y[1] for y in subresult])","['for (i, y) in enumerate(res[0]):\n    (y_0, y_1, *_) = y\n    if y != []:\n        (yname, next) = (y_0, y_1)\n        for (fret, name) in next:\n            for s in follow(1, fret, name):\n                subresult = [(i, yname)] + s\n                (mi, ma, names) = (1000, -1000, [])\n                for (f, n) in subresult:\n                    if n is not None:\n                        if f != 0 and f <= mi:\n                            mi = f\n                        if f != 0 and f >= ma:\n                            ma = f\n                        names.append(n)\n                if abs(ma - mi) < max_distance:\n                    covered = True\n                    for n in notenames:\n                        if n not in names:\n                            covered = False\n                    if covered and names != []:\n                        result.append([y_0 if y_1 is not None else y_1 for y in subresult])']",-1
dl-4-tsc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dl-4-tsc/classifiers/mcnn.py,https://github.com/hfawaz/dl-4-tsc/tree/master/classifiers/mcnn.py,Classifier_MCNN,split_input_for_model$410,"def split_input_for_model(self, x, input_shapes):
    res = []
    indx = 0
    for input_shape in input_shapes:
        res.append(x[:, indx:indx + input_shape[0], :])
        indx = indx + input_shape[0]
    return res","for input_shape in input_shapes:
    res.append(x[:, indx:indx + input_shape[0], :])
    indx = indx + input_shape[0]","for (indx, input_shape) in enumerate(input_shapes):
    res.append(x[:, indx:indx + input_shape[0], :])","['for input_shape in input_shapes:\n    (input_shape_0, *input_shape_rinput_shapemaining) = input_shape\n    res.append(x[:, indx:indx + input_shape_0, :])\n    indx = indx + input_shape_0', 'for (input_shape_0, *input_shape_len) in input_shapes:\n    res.append(x[:, indx:indx + \n    input_shape_0, :])\n    indx = indx + \n    input_shape_0']",-1
subDomainsBrute,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/subDomainsBrute/lib/scanner_py2.py,https://github.com/lijiejie/subDomainsBrute/tree/master/lib/scanner_py2.py,SubNameBrute,check_https_alt_names$90,"def check_https_alt_names(self, domain):
    try:
        x509 = reqs.OpenSSL.crypto.load_certificate(reqs.OpenSSL.crypto.FILETYPE_PEM, reqs.ssl.get_server_certificate((domain, 443)))
        for item in reqs.get_subj_alt_name(x509):
            if item[0].upper() == 'DNS':
                name = item[1].lower()
                if name.endswith(self.domain):
                    sub = name[:len(name) - len(self.domain) - 1]
                    sub = sub.replace('*', '')
                    sub = sub.strip('.')
                    if sub and sub not in self.found_subs and (sub not in self.normal_names_set) and (sub not in self.cert_subs):
                        self.cert_subs.add(sub)
                        self.queue.put((0, sub))
    except Exception as e:
        pass","for item in reqs.get_subj_alt_name(x509):
    if item[0].upper() == 'DNS':
        name = item[1].lower()
        if name.endswith(self.domain):
            sub = name[:len(name) - len(self.domain) - 1]
            sub = sub.replace('*', '')
            sub = sub.strip('.')
            if sub and sub not in self.found_subs and (sub not in self.normal_names_set) and (sub not in self.cert_subs):
                self.cert_subs.add(sub)
                self.queue.put((0, sub))","for (item_0, item_1) in reqs.get_subj_alt_name(x509):
    if item_0.upper() == 'DNS':
        name = item_1.lower()
        if name.endswith(self.domain):
            sub = name[:len(name) - len(self.domain) - 1]
            sub = sub.replace('*', '')
            sub = sub.strip('.')
            if sub and sub not in self.found_subs and (sub not in self.normal_names_set) and (sub not in self.cert_subs):
                self.cert_subs.add(sub)
                self.queue.put((0, sub))","[""for item in reqs.get_subj_alt_name(x509):\n    (item_0, item_1, *_) = item\n    if item_0.upper() == 'DNS':\n        name = item_1.lower()\n        if name.endswith(self.domain):\n            sub = name[:len(name) - len(self.domain) - 1]\n            sub = sub.replace('*', '')\n            sub = sub.strip('.')\n            if sub and sub not in self.found_subs and (sub not in self.normal_names_set) and (sub not in self.cert_subs):\n                self.cert_subs.add(sub)\n                self.queue.put((0, sub))"", ""for (item_0, item_1, *item_len) in reqs.get_subj_alt_name(x509):\n    if \n    item_0.upper() == 'DNS':\n        name = \n        item_1.lower()\n        if name.endswith(self.domain):\n            sub = name[:len(name) - len(self.domain) - 1]\n            sub = sub.replace('*', '')\n            sub = sub.strip('.')\n            if sub and sub not in self.found_subs and (sub not in self.normal_names_set) and (sub not in self.cert_subs):\n                self.cert_subs.add(sub)\n                self.queue.put((0, sub))""]",-1
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/scripts/datasets/hmdb51.py,https://github.com/dmlc/gluon-cv/tree/master/scripts/datasets/hmdb51.py,,build_split_list$292,"def build_split_list(split, frame_info, shuffle=False):

    def build_set_list(set_list):
        (rgb_list, flow_list) = (list(), list())
        for item in set_list:
            if item[0] not in frame_info:
                continue
            elif frame_info[item[0]][1] > 0:
                rgb_cnt = frame_info[item[0]][1]
                flow_cnt = frame_info[item[0]][2]
                rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
                flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
            else:
                rgb_list.append('{} {}\n'.format(item[0], item[1]))
                flow_list.append('{} {}\n'.format(item[0], item[1]))
        if shuffle:
            random.shuffle(rgb_list)
            random.shuffle(flow_list)
        return (rgb_list, flow_list)
    (train_rgb_list, train_flow_list) = build_set_list(split[0])
    (test_rgb_list, test_flow_list) = build_set_list(split[1])
    return ((train_rgb_list, test_rgb_list), (train_flow_list, test_flow_list))","for item in set_list:
    if item[0] not in frame_info:
        continue
    elif frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        flow_cnt = frame_info[item[0]][2]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
        flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
    else:
        rgb_list.append('{} {}\n'.format(item[0], item[1]))
        flow_list.append('{} {}\n'.format(item[0], item[1]))","for (item_0, item_1) in set_list:
    if item_0 not in frame_info:
        continue
    elif frame_info[item_0][1] > 0:
        rgb_cnt = frame_info[item_0][1]
        flow_cnt = frame_info[item_0][2]
        rgb_list.append('{} {} {}\n'.format(item_0, rgb_cnt, item_1))
        flow_list.append('{} {} {}\n'.format(item_0, flow_cnt, item_1))
    else:
        rgb_list.append('{} {}\n'.format(item_0, item_1))
        flow_list.append('{} {}\n'.format(item_0, item_1))","[""for item in set_list:\n    (item_0, item_1, *_) = item\n    if item_0 not in frame_info:\n        continue\n    elif frame_info[item_0][1] > 0:\n        rgb_cnt = frame_info[item_0][1]\n        flow_cnt = frame_info[item_0][2]\n        rgb_list.append('{} {} {}\\n'.format(item_0, rgb_cnt, item_1))\n        flow_list.append('{} {} {}\\n'.format(item_0, flow_cnt, item_1))\n    else:\n        rgb_list.append('{} {}\\n'.format(item_0, item_1))\n        flow_list.append('{} {}\\n'.format(item_0, item_1))"", ""for (item_0, item_1, *item_len) in set_list:\n    if \n    item_0 not in frame_info:\n        continue\n    elif frame_info[\n    item_0][1] > 0:\n        rgb_cnt = frame_info[\n        item_0][1]\n        flow_cnt = frame_info[\n        item_0][2]\n        rgb_list.append('{} {} {}\\n'.format(\n        item_0, rgb_cnt, \n        item_1))\n        flow_list.append('{} {} {}\\n'.format(\n        item_0, flow_cnt, \n        item_1))\n    else:\n        rgb_list.append('{} {}\\n'.format(\n        item_0, \n        item_1))\n        flow_list.append('{} {}\\n'.format(\n        item_0, \n        item_1))""]",-1
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/scripts/datasets/ucf101.py,https://github.com/dmlc/gluon-cv/tree/master/scripts/datasets/ucf101.py,,build_split_list$242,"def build_split_list(split, frame_info, shuffle=False):

    def build_set_list(set_list):
        (rgb_list, flow_list) = (list(), list())
        for item in set_list:
            if item[0] not in frame_info:
                continue
            elif frame_info[item[0]][1] > 0:
                rgb_cnt = frame_info[item[0]][1]
                flow_cnt = frame_info[item[0]][2]
                rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
                flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
            else:
                rgb_list.append('{} {}\n'.format(item[0], item[1]))
                flow_list.append('{} {}\n'.format(item[0], item[1]))
        if shuffle:
            random.shuffle(rgb_list)
            random.shuffle(flow_list)
        return (rgb_list, flow_list)
    (train_rgb_list, train_flow_list) = build_set_list(split[0])
    (test_rgb_list, test_flow_list) = build_set_list(split[1])
    return ((train_rgb_list, test_rgb_list), (train_flow_list, test_flow_list))","for item in set_list:
    if item[0] not in frame_info:
        continue
    elif frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        flow_cnt = frame_info[item[0]][2]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
        flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
    else:
        rgb_list.append('{} {}\n'.format(item[0], item[1]))
        flow_list.append('{} {}\n'.format(item[0], item[1]))","for (item_0, item_1) in set_list:
    if item_0 not in frame_info:
        continue
    elif frame_info[item_0][1] > 0:
        rgb_cnt = frame_info[item_0][1]
        flow_cnt = frame_info[item_0][2]
        rgb_list.append('{} {} {}\n'.format(item_0, rgb_cnt, item_1))
        flow_list.append('{} {} {}\n'.format(item_0, flow_cnt, item_1))
    else:
        rgb_list.append('{} {}\n'.format(item_0, item_1))
        flow_list.append('{} {}\n'.format(item_0, item_1))","[""for item in set_list:\n    (item_0, item_1, *_) = item\n    if item_0 not in frame_info:\n        continue\n    elif frame_info[item_0][1] > 0:\n        rgb_cnt = frame_info[item_0][1]\n        flow_cnt = frame_info[item_0][2]\n        rgb_list.append('{} {} {}\\n'.format(item_0, rgb_cnt, item_1))\n        flow_list.append('{} {} {}\\n'.format(item_0, flow_cnt, item_1))\n    else:\n        rgb_list.append('{} {}\\n'.format(item_0, item_1))\n        flow_list.append('{} {}\\n'.format(item_0, item_1))"", ""for (item_0, item_1, *item_len) in set_list:\n    if \n    item_0 not in frame_info:\n        continue\n    elif frame_info[\n    item_0][1] > 0:\n        rgb_cnt = frame_info[\n        item_0][1]\n        flow_cnt = frame_info[\n        item_0][2]\n        rgb_list.append('{} {} {}\\n'.format(\n        item_0, rgb_cnt, \n        item_1))\n        flow_list.append('{} {} {}\\n'.format(\n        item_0, flow_cnt, \n        item_1))\n    else:\n        rgb_list.append('{} {}\\n'.format(\n        item_0, \n        item_1))\n        flow_list.append('{} {}\\n'.format(\n        item_0, \n        item_1))""]",-1
PaddleVideo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleVideo/data/ucf101/build_ucf101_file_list.py,https://github.com/PaddlePaddle/PaddleVideo/tree/master/data/ucf101/build_ucf101_file_list.py,,build_set_list$49,"def build_set_list(set_list):
    rgb_list = list()
    for item in set_list:
        if item[0] not in frame_info:
            continue
        elif frame_info[item[0]][1] > 0:
            rgb_cnt = frame_info[item[0]][1]
            rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
        else:
            rgb_list.append('{} {}\n'.format(item[0], item[1]))
    if shuffle:
        random.shuffle(rgb_list)
    return rgb_list","for item in set_list:
    if item[0] not in frame_info:
        continue
    elif frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
    else:
        rgb_list.append('{} {}\n'.format(item[0], item[1]))","for (item_0, item_1) in set_list:
    if item_0 not in frame_info:
        continue
    elif frame_info[item_0][1] > 0:
        rgb_cnt = frame_info[item_0][1]
        rgb_list.append('{} {} {}\n'.format(item_0, rgb_cnt, item_1))
    else:
        rgb_list.append('{} {}\n'.format(item_0, item_1))","[""for item in set_list:\n    (item_0, item_1, *_) = item\n    if item_0 not in frame_info:\n        continue\n    elif frame_info[item_0][1] > 0:\n        rgb_cnt = frame_info[item_0][1]\n        rgb_list.append('{} {} {}\\n'.format(item_0, rgb_cnt, item_1))\n    else:\n        rgb_list.append('{} {}\\n'.format(item_0, item_1))"", ""for (item_0, item_1, *item_len) in set_list:\n    if \n    item_0 not in frame_info:\n        continue\n    elif frame_info[\n    item_0][1] > 0:\n        rgb_cnt = frame_info[\n        item_0][1]\n        rgb_list.append('{} {} {}\\n'.format(\n        item_0, rgb_cnt, \n        item_1))\n    else:\n        rgb_list.append('{} {}\\n'.format(\n        item_0, \n        item_1))""]",-1
two-stream-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/two-stream-pytorch/datasets/build_file_list.py,https://github.com/bryanyzhu/two-stream-pytorch/tree/master/datasets/build_file_list.py,,build_set_list$42,"def build_set_list(set_list):
    (rgb_list, flow_list) = (list(), list())
    for item in set_list:
        rgb_cnt = frame_info[0][item[0]]
        flow_cnt = frame_info[1][item[0]]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
        flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
    if shuffle:
        random.shuffle(rgb_list)
        random.shuffle(flow_list)
    return (rgb_list, flow_list)","for item in set_list:
    rgb_cnt = frame_info[0][item[0]]
    flow_cnt = frame_info[1][item[0]]
    rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
    flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))","for (item_0, item_1) in set_list:
    rgb_cnt = frame_info[0][item_0]
    flow_cnt = frame_info[1][item_0]
    rgb_list.append('{} {} {}\n'.format(item_0, rgb_cnt, item_1))
    flow_list.append('{} {} {}\n'.format(item_0, flow_cnt, item_1))","[""for item in set_list:\n    (item_0, item_1, *_) = item\n    rgb_cnt = frame_info[0][item_0]\n    flow_cnt = frame_info[1][item_0]\n    rgb_list.append('{} {} {}\\n'.format(item_0, rgb_cnt, item_1))\n    flow_list.append('{} {} {}\\n'.format(item_0, flow_cnt, item_1))"", ""for (item_0, item_1, *item_len) in set_list:\n    rgb_cnt = frame_info[0][item_0]\n    flow_cnt = frame_info[1][item_0]\n    rgb_list.append('{} {} {}\\n'.format(item_0, rgb_cnt, item_1))\n    flow_list.append('{} {} {}\\n'.format(item_0, flow_cnt, item_1))""]",-1
mmaction2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmaction2/tools/data/build_file_list.py,https://github.com/open-mmlab/mmaction2/tree/master/tools/data/build_file_list.py,,build_file_list$90,"def build_file_list(splits, frame_info, shuffle=False):
    """"""Build file list for a certain data split.

    Args:
        splits (tuple): Data split to generate file list.
        frame_info (dict): Dict mapping from frames to path. e.g.,
            'Skiing/v_Skiing_g18_c02': ('data/ucf101/rawframes/Skiing/v_Skiing_g18_c02', 0, 0).  # noqa: E501
        shuffle (bool): Whether to shuffle the file list.

    Returns:
        tuple: RGB file list for training and testing, together with
            Flow file list for training and testing.
    """"""

    def build_list(split):
        """"""Build RGB and Flow file list with a given split.

        Args:
            split (list): Split to be generate file list.

        Returns:
            tuple[list, list]: (rgb_list, flow_list), rgb_list is the
                generated file list for rgb, flow_list is the generated
                file list for flow.
        """"""
        (rgb_list, flow_list) = (list(), list())
        for item in split:
            if item[0] not in frame_info:
                continue
            if frame_info[item[0]][1] > 0:
                rgb_cnt = frame_info[item[0]][1]
                flow_cnt = frame_info[item[0]][2]
                if isinstance(item[1], int):
                    rgb_list.append(f'{item[0]} {rgb_cnt} {item[1]}\n')
                    flow_list.append(f'{item[0]} {flow_cnt} {item[1]}\n')
                elif isinstance(item[1], list):
                    rgb_list.append(f'{item[0]} {rgb_cnt} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
                    rgb_list.append(f'{item[0]} {flow_cnt} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
                else:
                    raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')
            elif isinstance(item[1], int):
                rgb_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
                flow_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
            elif isinstance(item[1], list):
                rgb_list.append(f'{frame_info[item[0]][0]} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
                flow_list.append(f'{frame_info[item[0]][0]} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
            else:
                raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')
        if shuffle:
            random.shuffle(rgb_list)
            random.shuffle(flow_list)
        return (rgb_list, flow_list)
    (train_rgb_list, train_flow_list) = build_list(splits[0])
    (test_rgb_list, test_flow_list) = build_list(splits[1])
    return ((train_rgb_list, test_rgb_list), (train_flow_list, test_flow_list))","for item in split:
    if item[0] not in frame_info:
        continue
    if frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        flow_cnt = frame_info[item[0]][2]
        if isinstance(item[1], int):
            rgb_list.append(f'{item[0]} {rgb_cnt} {item[1]}\n')
            flow_list.append(f'{item[0]} {flow_cnt} {item[1]}\n')
        elif isinstance(item[1], list):
            rgb_list.append(f'{item[0]} {rgb_cnt} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
            rgb_list.append(f'{item[0]} {flow_cnt} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
        else:
            raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')
    elif isinstance(item[1], int):
        rgb_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
        flow_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
    elif isinstance(item[1], list):
        rgb_list.append(f'{frame_info[item[0]][0]} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
        flow_list.append(f'{frame_info[item[0]][0]} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
    else:
        raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')","for (item_0, item_1) in split:
    if item_0 not in frame_info:
        continue
    if frame_info[item_0][1] > 0:
        rgb_cnt = frame_info[item_0][1]
        flow_cnt = frame_info[item_0][2]
        if isinstance(item_1, int):
            rgb_list.append(f'{item_0} {rgb_cnt} {item_1}\n')
            flow_list.append(f'{item_0} {flow_cnt} {item_1}\n')
        elif isinstance(item_1, list):
            rgb_list.append(f'{item_0} {rgb_cnt} ' + ' '.join([str(digit) for digit in item_1]) + '\n')
            rgb_list.append(f'{item_0} {flow_cnt} ' + ' '.join([str(digit) for digit in item_1]) + '\n')
        else:
            raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')
    elif isinstance(item_1, int):
        rgb_list.append(f'{frame_info[item_0][0]} {item_1}\n')
        flow_list.append(f'{frame_info[item_0][0]} {item_1}\n')
    elif isinstance(item_1, list):
        rgb_list.append(f'{frame_info[item_0][0]} ' + ' '.join([str(digit) for digit in item_1]) + '\n')
        flow_list.append(f'{frame_info[item_0][0]} ' + ' '.join([str(digit) for digit in item_1]) + '\n')
    else:
        raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')","[""for item in split:\n    (item_0, item_1, *_) = item\n    if item_0 not in frame_info:\n        continue\n    if frame_info[item_0][1] > 0:\n        rgb_cnt = frame_info[item_0][1]\n        flow_cnt = frame_info[item_0][2]\n        if isinstance(item_1, int):\n            rgb_list.append(f'{item_0} {rgb_cnt} {item_1}\\n')\n            flow_list.append(f'{item_0} {flow_cnt} {item_1}\\n')\n        elif isinstance(item_1, list):\n            rgb_list.append(f'{item_0} {rgb_cnt} ' + ' '.join([str(digit) for digit in item_1]) + '\\n')\n            rgb_list.append(f'{item_0} {flow_cnt} ' + ' '.join([str(digit) for digit in item_1]) + '\\n')\n        else:\n            raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')\n    elif isinstance(item_1, int):\n        rgb_list.append(f'{frame_info[item_0][0]} {item_1}\\n')\n        flow_list.append(f'{frame_info[item_0][0]} {item_1}\\n')\n    elif isinstance(item_1, list):\n        rgb_list.append(f'{frame_info[item_0][0]} ' + ' '.join([str(digit) for digit in item_1]) + '\\n')\n        flow_list.append(f'{frame_info[item_0][0]} ' + ' '.join([str(digit) for digit in item_1]) + '\\n')\n    else:\n        raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')"", 'for (item_0, item_1, *item_len) in split:\n    if \n    item_0 not in frame_info:\n        continue\n    if frame_info[\n    item_0][1] > 0:\n        rgb_cnt = frame_info[\n        item_0][1]\n        flow_cnt = frame_info[\n        item_0][2]\n        if isinstance(\n        item_1, int):\n            rgb_list.append(f\'{item_0} {rgb_cnt} {item_1}\\n\')\n            flow_list.append(f\'{item_0} {flow_cnt} {item_1}\\n\')\n        elif isinstance(\n        item_1, list):\n            rgb_list.append(f\'{item_0} {rgb_cnt} \' + \' \'.join([str(digit) for digit in \n            item_1]) + \'\\n\')\n            rgb_list.append(f\'{item_0} {flow_cnt} \' + \' \'.join([str(digit) for digit in \n            item_1]) + \'\\n\')\n        else:\n            raise ValueError(\'frame_info should be \' + \'[`video`(str), `label`(int)|`labels(list[int])`\')\n    elif isinstance(\n    item_1, int):\n        rgb_list.append(f""""""{frame_info[\nitem_0][0]} {item_1}\\n"""""")\n        flow_list.append(f""""""{frame_info[\nitem_0][0]} {item_1}\\n"""""")\n    elif isinstance(\n    item_1, list):\n        rgb_list.append(f""""""{frame_info[\nitem_0][0]} """""" + \' \'.join([str(digit) for digit in \n        item_1]) + \'\\n\')\n        flow_list.append(f""""""{frame_info[\nitem_0][0]} """""" + \' \'.join([str(digit) for digit in \n        item_1]) + \'\\n\')\n    else:\n        raise ValueError(\'frame_info should be \' + \'[`video`(str), `label`(int)|`labels(list[int])`\')']",-1
PaddleX,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex_restful/restful/utils.py,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex_restful/restful/utils.py,PrintableStructure,__str__$715,"def __str__(self):
    result = []
    for x in self._fields_:
        key = x[0]
        value = getattr(self, key)
        fmt = '%s'
        if key in self._fmt_:
            fmt = self._fmt_[key]
        elif '<default>' in self._fmt_:
            fmt = self._fmt_['<default>']
        result.append(('%s: ' + fmt) % (key, value))
    return self.__class__.__name__ + '(' + string.join(result, ', ') + ')'","for x in self._fields_:
    key = x[0]
    value = getattr(self, key)
    fmt = '%s'
    if key in self._fmt_:
        fmt = self._fmt_[key]
    elif '<default>' in self._fmt_:
        fmt = self._fmt_['<default>']
    result.append(('%s: ' + fmt) % (key, value))","for (key, value) in self._asdict().items():
    fmt = self._fmt_.get(key, '%s')
    result.append(f'{key}: {fmt % value}')","[""for x in self._fields_:\n    (x_0, *x_rxmaining) = x\n    key = x_0\n    value = getattr(self, key)\n    fmt = '%s'\n    if key in self._fmt_:\n        fmt = self._fmt_[key]\n    elif '<default>' in self._fmt_:\n        fmt = self._fmt_['<default>']\n    result.append(('%s: ' + fmt) % (key, value))"", ""for (x_0, *x_len) in self._fields_:\n    key = \n    x_0\n    value = getattr(self, key)\n    fmt = '%s'\n    if key in self._fmt_:\n        fmt = self._fmt_[key]\n    elif '<default>' in self._fmt_:\n        fmt = self._fmt_['<default>']\n    result.append(('%s: ' + fmt) % (key, value))""]",-1
tmuxomatic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tmuxomatic/windowgram/windowgram_test.py,https://github.com/oxidane/tmuxomatic/tree/master/windowgram/windowgram_test.py,SenseTestCase,runTest$123,"def runTest(self):
    try:
        methods = inspect.getmembers(self.__class__(), predicate=inspect.ismethod)
        methods = [method for method in methods if method[0].startswith('test_')]
        methods = [(method[1], inspect.getsourcelines(method[1])[1]) for method in methods]
        methods = sorted(methods, key=lambda tup: tup[1])
        for method in methods:
            method[0]()
    except AssertionError as e:
        raise e
    except Exception as e:
        error = 'An error occurred during testing: ' + repr(e)
        raise AssertionError(e)","for method in methods:
    method[0]()","for (method, _) in methods:
    method()","['for method in methods:\n    (method_0, *method_rmethodmaining) = method\n    method_0()', 'for (method_0, *method_len) in methods:\n    method_0()']",-1
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/database/database.py,https://github.com/frappe/frappe/tree/master/frappe/database/database.py,Database,commit$915,"def commit(self):
    """"""Commit current transaction. Calls SQL `COMMIT`.""""""
    for method in frappe.local.before_commit:
        frappe.call(method[0], *(method[1] or []), **method[2] or {})
    self.sql('commit')
    self.begin()
    frappe.local.rollback_observers = []
    self.flush_realtime_log()
    enqueue_jobs_after_commit()
    flush_local_link_count()","for method in frappe.local.before_commit:
    frappe.call(method[0], *(method[1] or []), **method[2] or {})","for (method, args, kwargs) in frappe.local.before_commit:
    frappe.call(method, *args, **kwargs)","['for method in frappe.local.before_commit:\n    (method_0, method_1, method_2, *_) = method\n    frappe.call(method_0, *(method_1 or []), **method_2 or {})', 'for (method_0, method_1, method_2, *method_len) in frappe.local.before_commit:\n    frappe.call(method_0, *(method_1 or []), **method_2 or {})']",-1
detectron2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/detectron2/tests/data/test_sampler.py,https://github.com/facebookresearch/detectron2/tree/master/tests/data/test_sampler.py,TestGroupedBatchSampler,test_groups$30,"def test_groups(self):
    sampler = SequentialSampler(list(range(100)))
    group_ids = [1, 0] * 50
    samples = GroupedBatchSampler(sampler, group_ids, 2)
    for mini_batch in samples:
        self.assertEqual((mini_batch[0] + mini_batch[1]) % 2, 0)","for mini_batch in samples:
    self.assertEqual((mini_batch[0] + mini_batch[1]) % 2, 0)","for (mini_batch_0, mini_batch_1) in samples:
    self.assertEqual((mini_batch_0 + mini_batch_1) % 2, 0)","['for mini_batch in samples:\n    (mini_batch_0, mini_batch_1, *_) = mini_batch\n    self.assertEqual((mini_batch_0 + mini_batch_1) % 2, 0)', 'for (mini_batch_0, mini_batch_1, *mini_batch_len) in samples:\n    self.assertEqual((mini_batch_0 + mini_batch_1) % 2, 0)']",-1
AlgorithmsByPython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsByPython/Target Offer/multiSparse.py,https://github.com/Jack-Lee-Hiter/AlgorithmsByPython/tree/master/Target Offer/multiSparse.py,,tripleToSparse$25,"def tripleToSparse(triple, m, n):
    outMatrix = zeros([m, n])
    for pointTuple in triple:
        mLocation = pointTuple[0]
        nLocation = pointTuple[1]
        value = pointTuple[2]
        outMatrix[mLocation][nLocation] = value
    return outMatrix","for pointTuple in triple:
    mLocation = pointTuple[0]
    nLocation = pointTuple[1]
    value = pointTuple[2]
    outMatrix[mLocation][nLocation] = value","for (mLocation, nLocation, value) in triple:
    outMatrix[mLocation][nLocation] = value","['for pointTuple in triple:\n    (pointTuple_0, pointTuple_1, pointTuple_2, *_) = pointTuple\n    mLocation = pointTuple_0\n    nLocation = pointTuple_1\n    value = pointTuple_2\n    outMatrix[mLocation][nLocation] = value', 'for (pointTuple_0, pointTuple_1, pointTuple_2, *pointTuple_len) in triple:\n    mLocation = \n    pointTuple_0\n    nLocation = \n    pointTuple_1\n    value = \n    pointTuple_2\n    outMatrix[mLocation][nLocation] = value']",-1
spot-sdk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spot-sdk/docs/python/fetch_tutorial/files/network_compute_server.py,https://github.com/boston-dynamics/spot-sdk/tree/master/docs/python/fetch_tutorial/files/network_compute_server.py,,main$256,"def main(argv):
    default_port = '50051'
    parser = argparse.ArgumentParser()
    parser.add_argument('-m', '--model', help=""[MODEL_DIR] [LABELS_FILE.pbtxt]: Path to a model's directory and path to its labels .pbtxt file"", action='append', nargs=2, required=True)
    parser.add_argument('-p', '--port', help=""Server's port number, default: "" + default_port, default=default_port)
    parser.add_argument('-d', '--no-debug', help='Disable writing debug images.', action='store_true')
    parser.add_argument('-n', '--name', help='Service name', default='fetch-server')
    bosdyn.client.util.add_base_arguments(parser)
    options = parser.parse_args(argv)
    print(options.model)
    for model in options.model:
        if not os.path.isdir(model[0]):
            print('Error: model directory (' + model[0] + ') not found or is not a directory.')
            sys.exit(1)
    register_with_robot(options)
    request_queue = queue.Queue()
    response_queue = queue.Queue()
    thread = threading.Thread(target=process_thread, args=[options, request_queue, response_queue])
    thread.start()
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    network_compute_bridge_service_pb2_grpc.add_NetworkComputeBridgeWorkerServicer_to_server(NetworkComputeBridgeWorkerServicer(request_queue, response_queue), server)
    server.add_insecure_port('[::]:' + options.port)
    server.start()
    print('Running...')
    thread.join()
    return True","for model in options.model:
    if not os.path.isdir(model[0]):
        print('Error: model directory (' + model[0] + ') not found or is not a directory.')
        sys.exit(1)","for (model_dir, labels_file) in options.model:
    if not os.path.isdir(model_dir):
        print(f'Error: model directory ({model_dir}) not found or is not a directory.')
        sys.exit(1)","[""for model in options.model:\n    (model_0, *model_rmodelmaining) = model\n    if not os.path.isdir(model_0):\n        print('Error: model directory (' + model_0 + ') not found or is not a directory.')\n        sys.exit(1)"", ""for (model_0, *model_len) in options.model:\n    if not os.path.isdir(model_0):\n        print('Error: model directory (' + model_0 + ') not found or is not a directory.')\n        sys.exit(1)""]",-1
Grokking-the-Coding-Interview-Patterns-for-Coding-Questions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Grokking-the-Coding-Interview-Patterns-for-Coding-Questions/16. Pattern Topological Sort (Graph)/Problem Challenge 2 - Minimum Height Trees (hard).py,https://github.com/cl2333/Grokking-the-Coding-Interview-Patterns-for-Coding-Questions/tree/master/16. Pattern Topological Sort (Graph)/Problem Challenge 2 - Minimum Height Trees (hard).py,,find_trees$67,"def find_trees(nodes, edges):
    if nodes <= 0:
        return []
    if nodes == 1:
        return [0]
    inDegree = {i: 0 for i in range(nodes)}
    graph = {i: [] for i in range(nodes)}
    for edge in edges:
        (n1, n2) = (edge[0], edge[1])
        graph[n1].append(n2)
        graph[n2].append(n1)
        inDegree[n1] += 1
        inDegree[n2] += 1
    leaves = deque()
    for key in inDegree:
        if inDegree[key] == 1:
            leaves.append(key)
    totalNodes = nodes
    while totalNodes > 2:
        leavesSize = len(leaves)
        totalNodes -= leavesSize
        for i in range(0, leavesSize):
            vertex = leaves.popleft()
            for child in graph[vertex]:
                inDegree[child] -= 1
                if inDegree[child] == 1:
                    leaves.append(child)
    return list(leaves)","for edge in edges:
    (n1, n2) = (edge[0], edge[1])
    graph[n1].append(n2)
    graph[n2].append(n1)
    inDegree[n1] += 1
    inDegree[n2] += 1","for (n1, n2) in edges:
    graph[n1].append(n2)
    graph[n2].append(n1)
    inDegree[n1] += 1
    inDegree[n2] += 1","['for edge in edges:\n    (edge_0, edge_1, *_) = edge\n    (n1, n2) = (edge_0, edge_1)\n    graph[n1].append(n2)\n    graph[n2].append(n1)\n    inDegree[n1] += 1\n    inDegree[n2] += 1', 'for (edge_0, edge_1, *edge_len) in edges:\n    (n1, n2) = (edge_0, edge_1)\n    graph[n1].append(n2)\n    graph[n2].append(n1)\n    inDegree[n1] += 1\n    inDegree[n2] += 1']",1
rotki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rotki/rotkehlchen/db/dbhandler.py,https://github.com/rotki/rotki/tree/master/rotkehlchen/db/dbhandler.py,DBHandler,get_tags$2597,"def get_tags(self, cursor: 'DBCursor') -> Dict[str, Tag]:
    tags_mapping: Dict[str, Tag] = {}
    cursor.execute('SELECT name, description, background_color, foreground_color FROM tags;')
    for result in cursor:
        name = result[0]
        description = result[1]
        if description is not None and (not isinstance(description, str)):
            self.msg_aggregator.add_warning(f'Tag {name} with invalid description found in the DB. Skipping tag')
            continue
        try:
            background_color = deserialize_hex_color_code(result[2])
            foreground_color = deserialize_hex_color_code(result[3])
        except DeserializationError as e:
            self.msg_aggregator.add_warning(f'Tag {name} with invalid color code found in the DB. {str(e)}. Skipping tag')
            continue
        tags_mapping[name] = Tag(name=name, description=description, background_color=background_color, foreground_color=foreground_color)
    return tags_mapping","for result in cursor:
    name = result[0]
    description = result[1]
    if description is not None and (not isinstance(description, str)):
        self.msg_aggregator.add_warning(f'Tag {name} with invalid description found in the DB. Skipping tag')
        continue
    try:
        background_color = deserialize_hex_color_code(result[2])
        foreground_color = deserialize_hex_color_code(result[3])
    except DeserializationError as e:
        self.msg_aggregator.add_warning(f'Tag {name} with invalid color code found in the DB. {str(e)}. Skipping tag')
        continue
    tags_mapping[name] = Tag(name=name, description=description, background_color=background_color, foreground_color=foreground_color)","for (name, description, bg_color, fg_color, *_) in cursor:
    if description is not None and (not isinstance(description, str)):
        self.msg_aggregator.add_warning(f'Tag {name} with invalid description found in the DB. Skipping tag')
        continue
    try:
        background_color = deserialize_hex_color_code(bg_color)
        foreground_color = deserialize_hex_color_code(fg_color)
    except DeserializationError as e:
        self.msg_aggregator.add_warning(f'Tag {name} with invalid color code found in the DB. {str(e)}. Skipping tag')
        continue
    tags_mapping[name] = Tag(name=name, description=description, background_color=background_color, foreground_color=foreground_color)","[""for result in cursor:\n    (result_0, result_1, result_2, result_3, *_) = result\n    name = result_0\n    description = result_1\n    if description is not None and (not isinstance(description, str)):\n        self.msg_aggregator.add_warning(f'Tag {name} with invalid description found in the DB. Skipping tag')\n        continue\n    try:\n        background_color = deserialize_hex_color_code(result_2)\n        foreground_color = deserialize_hex_color_code(result_3)\n    except DeserializationError as e:\n        self.msg_aggregator.add_warning(f'Tag {name} with invalid color code found in the DB. {str(e)}. Skipping tag')\n        continue\n    tags_mapping[name] = Tag(name=name, description=description, background_color=background_color, foreground_color=foreground_color)"", ""for (result_0, result_1, result_2, result_3, *result_len) in cursor:\n    name = \n    result_0\n    description = \n    result_1\n    if description is not None and (not isinstance(description, str)):\n        self.msg_aggregator.add_warning(f'Tag {name} with invalid description found in the DB. Skipping tag')\n        continue\n    try:\n        background_color = deserialize_hex_color_code(\n        result_2)\n        foreground_color = deserialize_hex_color_code(\n        result_3)\n    except DeserializationError as e:\n        self.msg_aggregator.add_warning(f'Tag {name} with invalid color code found in the DB. {str(e)}. Skipping tag')\n        continue\n    tags_mapping[name] = Tag(name=name, description=description, background_color=background_color, foreground_color=foreground_color)""]",1
FASPell,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FASPell/bert_modified/modeling.py,https://github.com/iqiyi/FASPell/tree/master/bert_modified/modeling.py,,get_assignment_map_from_checkpoint$318,"def get_assignment_map_from_checkpoint(tvars, init_checkpoint):
    """"""Compute the union of the current variables and checkpoint variables.""""""
    assignment_map = {}
    initialized_variable_names = {}
    name_to_variable = collections.OrderedDict()
    for var in tvars:
        name = var.name
        m = re.match('^(.*):\\d+$', name)
        if m is not None:
            name = m.group(1)
        name_to_variable[name] = var
    init_vars = tf.train.list_variables(init_checkpoint)
    assignment_map = collections.OrderedDict()
    for x in init_vars:
        (name, var) = (x[0], x[1])
        if name not in name_to_variable:
            continue
        assignment_map[name] = name
        initialized_variable_names[name] = 1
        initialized_variable_names[name + ':0'] = 1
    return (assignment_map, initialized_variable_names)","for x in init_vars:
    (name, var) = (x[0], x[1])
    if name not in name_to_variable:
        continue
    assignment_map[name] = name
    initialized_variable_names[name] = 1
    initialized_variable_names[name + ':0'] = 1","for (name, var) in init_vars:
    if name not in name_to_variable:
        continue
    assignment_map[name] = name
    initialized_variable_names[name] = 1
    initialized_variable_names[name + ':0'] = 1","[""for x in init_vars:\n    (x_0, x_1, *_) = x\n    (name, var) = (x_0, x_1)\n    if name not in name_to_variable:\n        continue\n    assignment_map[name] = name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + ':0'] = 1"", ""for (x_0, x_1, *x_len) in init_vars:\n    (name, var) = (x_0, x_1)\n    if name not in name_to_variable:\n        continue\n    assignment_map[name] = name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + ':0'] = 1""]",-1
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_fr.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_fr.py,Num2WordsENTest,test_number$168,"def test_number(self):
    for test in TEST_CASES_CARDINAL:
        self.assertEqual(num2words(test[0], lang='fr'), test[1])","for test in TEST_CASES_CARDINAL:
    self.assertEqual(num2words(test[0], lang='fr'), test[1])","for (num, expected_word) in TEST_CASES_CARDINAL:
    self.assertEqual(num2words(num, lang='fr'), expected_word)","[""for test in TEST_CASES_CARDINAL:\n    (test_0, test_1, *_) = test\n    self.assertEqual(num2words(test_0, lang='fr'), test_1)"", ""for (test_0, test_1, *test_len) in TEST_CASES_CARDINAL:\n    self.assertEqual(num2words(test_0, lang='fr'), test_1)""]",-1
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_fr.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_fr.py,Num2WordsENTest,test_ordinal$172,"def test_ordinal(self):
    for test in TEST_CASES_ORDINAL:
        self.assertEqual(num2words(test[0], lang='fr', ordinal=True), test[1])","for test in TEST_CASES_ORDINAL:
    self.assertEqual(num2words(test[0], lang='fr', ordinal=True), test[1])","for (num, expected_word) in TEST_CASES_ORDINAL:
    self.assertEqual(num2words(num, lang='fr', ordinal=True), expected_word)","[""for test in TEST_CASES_ORDINAL:\n    (test_0, test_1, *_) = test\n    self.assertEqual(num2words(test_0, lang='fr', ordinal=True), test_1)"", ""for (test_0, test_1, *test_len) in TEST_CASES_ORDINAL:\n    self.assertEqual(num2words(test_0, lang='fr', ordinal=True), test_1)""]",-1
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_fr_dz.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_fr_dz.py,Num2WordsPLTest,test_ordinal_num$60,"def test_ordinal_num(self):
    for test in test_fr.TEST_CASES_ORDINAL_NUM:
        self.assertEqual(num2words(test[0], lang='fr_DZ', to='ordinal_num'), test[1])","for test in test_fr.TEST_CASES_ORDINAL_NUM:
    self.assertEqual(num2words(test[0], lang='fr_DZ', to='ordinal_num'), test[1])","for (num, expected_word) in test_fr.TEST_CASES_ORDINAL_NUM:
    self.assertEqual(num2words(num, lang='fr_DZ', to='ordinal_num'), expected_word)","[""for test in test_fr.TEST_CASES_ORDINAL_NUM:\n    (test_0, test_1, *_) = test\n    self.assertEqual(num2words(test_0, lang='fr_DZ', to='ordinal_num'), test_1)"", ""for (test_0, test_1, *test_len) in test_fr.TEST_CASES_ORDINAL_NUM:\n    self.assertEqual(num2words(test_0, lang='fr_DZ', to='ordinal_num'), test_1)""]",-1
pyglossary,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglossary/pyglossary/text_utils.py,https://github.com/ilius/pyglossary/tree/master/pyglossary/text_utils.py,,replace$56,"def replace(st: str) -> str:
    for rpl in rplList:
        st = st.replace(rpl[0], rpl[1])
    return st","for rpl in rplList:
    st = st.replace(rpl[0], rpl[1])","for (old, new) in rplList:
    st = st.replace(old, new)","['for rpl in rplList:\n    (rpl_0, rpl_1, *_) = rpl\n    st = st.replace(rpl_0, rpl_1)', 'for (rpl_0, rpl_1, *rpl_len) in rplList:\n    st = st.replace(rpl_0, rpl_1)']",-1
integrations-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/datadog_checks_dev/tests/tooling/config_validator/test_validator.py,https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/tests/tooling/config_validator/test_validator.py,,test_validate_type$69,"def test_validate_type():
    test_cases = [(create('name', 'boolean'), True), (create('name', 'string'), True), (create('name', 'integer'), True), (create('name', 'double'), True), (create('name', 'float'), True), (create('name', 'object'), True), (create('name', 'list of anything_really'), True), (create('name', 'list of something'), True), (create('name', 'dictionary'), True), (create('name', 'custom object'), False)]
    for c in test_cases:
        errors = []
        c[0]._validate_type(errors)
        if c[1]:
            assert len(errors) == 0
        else:
            type_name = c[0].param_prop.type_name
            assert len(errors) == 1
            assert errors[0].error_str == 'Type {} is not accepted'.format(type_name)","for c in test_cases:
    errors = []
    c[0]._validate_type(errors)
    if c[1]:
        assert len(errors) == 0
    else:
        type_name = c[0].param_prop.type_name
        assert len(errors) == 1
        assert errors[0].error_str == 'Type {} is not accepted'.format(type_name)","for (param, expected_result) in test_cases:
    errors = []
    param._validate_type(errors)
    if expected_result:
        assert len(errors) == 0
    else:
        type_name = param.param_prop.type_name
        assert len(errors) == 1
        assert errors[0].error_str == f'Type {type_name} is not accepted'","[""for c in test_cases:\n    (c_0, c_1, *_) = c\n    errors = []\n    c_0._validate_type(errors)\n    if c_1:\n        assert len(errors) == 0\n    else:\n        type_name = c_0.param_prop.type_name\n        assert len(errors) == 1\n        assert errors[0].error_str == 'Type {} is not accepted'.format(type_name)"", ""for (c_0, c_1, *c_len) in test_cases:\n    errors = []\n    \n    c_0._validate_type(errors)\n    if \n    c_1:\n        assert len(errors) == 0\n    else:\n        type_name = \n        c_0.param_prop.type_name\n        assert len(errors) == 1\n        assert errors[0].error_str == 'Type {} is not accepted'.format(type_name)""]",-1
GitGutter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GitGutter/tests/test_json.py,https://github.com/jisaacks/GitGutter/tree/master/tests/test_json.py,CheckJsonFormat,get_line$111,"def get_line(self, pt):
    """"""Get the line from char index.""""""
    line = None
    for r in self.line_range:
        if pt >= r[0] and pt <= r[1]:
            line = r[2]
            break
    return line","for r in self.line_range:
    if pt >= r[0] and pt <= r[1]:
        line = r[2]
        break","for (r_0, r_1, line) in self.line_range:
    if pt >= r_0 and pt <= r_1:
        break
return line","['for r in self.line_range:\n    (r_0, r_1, r_2, *_) = r\n    if pt >= r_0 and pt <= r_1:\n        line = r_2\n        break', 'for (r_0, r_1, r_2, *r_len) in self.line_range:\n    if pt >= \n    r_0 and pt <= \n    r_1:\n        line = \n        r_2\n        break']",-1
BracketHighlighter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BracketHighlighter/tests/validate_json_format.py,https://github.com/facelessuser/BracketHighlighter/tree/master/tests/validate_json_format.py,CheckJsonFormat,get_line$106,"def get_line(self, pt):
    """"""Get the line from char index.""""""
    line = None
    for r in self.line_range:
        if pt >= r[0] and pt <= r[1]:
            line = r[2]
            break
    return line","for r in self.line_range:
    if pt >= r[0] and pt <= r[1]:
        line = r[2]
        break","for (r_0, r_1, r_2) in self.line_range:
    if pt >= r_0 and pt <= r_1:
        line = r_2
        break","['for r in self.line_range:\n    (r_0, r_1, r_2, *_) = r\n    if pt >= r_0 and pt <= r_1:\n        line = r_2\n        break', 'for (r_0, r_1, r_2, *r_len) in self.line_range:\n    if pt >= \n    r_0 and pt <= \n    r_1:\n        line = \n        r_2\n        break']",-1
Scout2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Scout2/AWSScout2/output/console.py,https://github.com/nccgroup/Scout2/tree/master/AWSScout2/output/console.py,,format_listall_output$16,"def format_listall_output(format_file, format_item_dir, format, rule, option_prefix=None, template=None, skip_options=False):
    """"""
    Prepare listall output template

    :param format_file:
    :param format_item_dir:
    :param format:
    :param config:
    :param option_prefix:
    :param template:
    :param skip_options:
    :return:
    """"""
    if format_file and os.path.isfile(format_file):
        if not template:
            with open(format_file, 'rt') as f:
                template = f.read()
        if not skip_options:
            re_option = re.compile('(%_OPTION_\\((.*?)\\)_NOITPO_)')
            optional_files = re_option.findall(template)
            for optional_file in optional_files:
                if optional_file[1].startswith(option_prefix + '-'):
                    with open(os.path.join(format_item_dir, optional_file[1].strip()), 'rt') as f:
                        template = template.replace(optional_file[0].strip(), f.read())
        re_file = re.compile('(_FILE_\\((.*?)\\)_ELIF_)')
        while True:
            requested_files = re_file.findall(template)
            available_files = os.listdir(format_item_dir) if format_item_dir else []
            for requested_file in requested_files:
                if requested_file[1].strip() in available_files:
                    with open(os.path.join(format_item_dir, requested_file[1].strip()), 'rt') as f:
                        template = template.replace(requested_file[0].strip(), f.read())
            re_line = re.compile('(_ITEM_\\((.*?)\\)_METI_)')
            re_key = re.compile('_KEY_\\(*(.*?)\\)', re.DOTALL | re.MULTILINE)
            lines = re_line.findall(template)
            for (i, line) in enumerate(lines):
                lines[i] = line + (re_key.findall(line[1]),)
            requested_files = re_file.findall(template)
            if len(requested_files) == 0:
                break
    elif format and format[0] == 'csv':
        keys = rule.keys
        line = ', '.join(('_KEY_(%s)' % k for k in keys))
        lines = [(line, line, keys)]
        template = line
    return (lines, template)","for requested_file in requested_files:
    if requested_file[1].strip() in available_files:
        with open(os.path.join(format_item_dir, requested_file[1].strip()), 'rt') as f:
            template = template.replace(requested_file[0].strip(), f.read())","for (requested_file_0, requested_file_1, *_) in requested_files:
    if requested_file_1.strip() in available_files:
        with open(os.path.join(format_item_dir, requested_file_1.strip()), 'rt') as f:
            template = template.replace(requested_file_0.strip(), f.read())","[""for requested_file in requested_files:\n    (requested_file_0, requested_file_1, *_) = requested_file\n    if requested_file_1.strip() in available_files:\n        with open(os.path.join(format_item_dir, requested_file_1.strip()), 'rt') as f:\n            template = template.replace(requested_file_0.strip(), f.read())"", ""for (requested_file_0, requested_file_1, *requested_file_len) in requested_files:\n    if \n    requested_file_1.strip() in available_files:\n        with open(os.path.join(format_item_dir, \n        requested_file_1.strip()), 'rt') as f:\n            template = template.replace(\n            requested_file_0.strip(), f.read())""]",1
few-shot-vid2vid,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/few-shot-vid2vid/data/image_folder.py,https://github.com/NVlabs/few-shot-vid2vid/tree/master/data/image_folder.py,,make_grouped_dataset$63,"def make_grouped_dataset(dir):
    images = []
    assert os.path.isdir(dir), '%s is not a valid directory' % dir
    fnames = sorted(os.walk(dir))
    for fname in sorted(fnames):
        paths = []
        root = fname[0]
        for f in sorted(fname[2]):
            if is_image_file(f):
                paths.append(os.path.join(root, f))
        if len(paths) > 0:
            images.append(paths)
    return images","for fname in sorted(fnames):
    paths = []
    root = fname[0]
    for f in sorted(fname[2]):
        if is_image_file(f):
            paths.append(os.path.join(root, f))
    if len(paths) > 0:
        images.append(paths)","for (root, _, files) in sorted(os.walk(dir)):
    paths = []
    for f in sorted(files):
        if is_image_file(f):
            paths.append(os.path.join(root, f))
    if len(paths) > 0:
        images.append(paths)","['for fname in sorted(fnames):\n    (fname_0, _, fname_2, *fname_rfnamemaining) = fname\n    paths = []\n    root = fname_0\n    for f in sorted(fname_2):\n        if is_image_file(f):\n            paths.append(os.path.join(root, f))\n    if len(paths) > 0:\n        images.append(paths)', 'for (fname_0, fname_1, fname_2, *fname_len) in sorted(fnames):\n    paths = []\n    root = \n    fname_0\n    for f in sorted(\n    fname_2):\n        if is_image_file(f):\n            paths.append(os.path.join(root, f))\n    if len(paths) > 0:\n        images.append(paths)']",1
PornHub-downloader-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PornHub-downloader-python/functions.py,https://github.com/mariosemes/PornHub-downloader-python/tree/master//functions.py,,dl_all_new_items$173,"def dl_all_new_items(conn):
    c = conn.cursor()
    try:
        c.execute(""SELECT * FROM ph_items WHERE new='1'"")
    except Error as e:
        print(e)
        sys.exit()
    rows = c.fetchall()
    for row in rows:
        if str(row[1]) == 'model':
            url_after = '/videos/upload'
        elif str(row[1]) == 'users':
            url_after = '/videos/public'
        elif str(row[1]) == 'channels':
            url_after = '/videos'
        else:
            url_after = ''
        print('-----------------------------')
        print(row[1])
        print(row[2])
        print('https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after)
        print('-----------------------------')
        outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
        ydl_opts = {'format': 'best', 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
        url = 'https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after
        with youtube_dl.YoutubeDL(ydl_opts) as ydl:
            ydl.download([url])
        try:
            c.execute(""UPDATE ph_items SET new='0', lastchecked=CURRENT_TIMESTAMP WHERE url_name=?"", (row[2],))
            conn.commit()
        except Error as e:
            print(e)
            sys.exit()","for row in rows:
    if str(row[1]) == 'model':
        url_after = '/videos/upload'
    elif str(row[1]) == 'users':
        url_after = '/videos/public'
    elif str(row[1]) == 'channels':
        url_after = '/videos'
    else:
        url_after = ''
    print('-----------------------------')
    print(row[1])
    print(row[2])
    print('https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after)
    print('-----------------------------')
    outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
    ydl_opts = {'format': 'best', 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
    url = 'https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after
    with youtube_dl.YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])
    try:
        c.execute(""UPDATE ph_items SET new='0', lastchecked=CURRENT_TIMESTAMP WHERE url_name=?"", (row[2],))
        conn.commit()
    except Error as e:
        print(e)
        sys.exit()","for (row_1, row_2, row_3, *_) in rows:
    if str(row_1) == 'model':
        url_after = '/videos/upload'
    elif str(row_1) == 'users':
        url_after = '/videos/public'
    elif str(row_1) == 'channels':
        url_after = '/videos'
    else:
        url_after = ''
    print('-----------------------------')
    print(row_1)
    print(row_2)
    print('https://www.pornhub.com/' + str(row_1) + '/' + str(row_2) + url_after)
    print('-----------------------------')
    outtmpl = get_dl_location('DownloadLocation') + '/' + str(row_1) + '/' + str(row_3) + '/%(title)s.%(ext)s'
    ydl_opts = {'format': 'best', 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
    url = 'https://www.pornhub.com/' + str(row_1) + '/' + str(row_2) + url_after
    with youtube_dl.YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])
    try:
        c.execute(""UPDATE ph_items SET new='0', lastchecked=CURRENT_TIMESTAMP WHERE url_name=?"", (row_2,))
        conn.commit()
    except Error as e:
        print(e)
        sys.exit()","['for row in rows:\n    (_, row_1, row_2, row_3, *_) = row\n    if str(row_1) == \'model\':\n        url_after = \'/videos/upload\'\n    elif str(row_1) == \'users\':\n        url_after = \'/videos/public\'\n    elif str(row_1) == \'channels\':\n        url_after = \'/videos\'\n    else:\n        url_after = \'\'\n    print(\'-----------------------------\')\n    print(row_1)\n    print(row_2)\n    print(\'https://www.pornhub.com/\' + str(row_1) + \'/\' + str(row_2) + url_after)\n    print(\'-----------------------------\')\n    outtmpl = get_dl_location(\'DownloadLocation\') + \'/\' + str(row_1) + \'/\' + str(row_3) + \'/%(title)s.%(ext)s\'\n    ydl_opts = {\'format\': \'best\', \'outtmpl\': outtmpl, \'nooverwrites\': True, \'no_warnings\': False, \'ignoreerrors\': True}\n    url = \'https://www.pornhub.com/\' + str(row_1) + \'/\' + str(row_2) + url_after\n    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n        ydl.download([url])\n    try:\n        c.execute(""UPDATE ph_items SET new=\'0\', lastchecked=CURRENT_TIMESTAMP WHERE url_name=?"", (row_2,))\n        conn.commit()\n    except Error as e:\n        print(e)\n        sys.exit()', 'for (row_0, row_1, row_2, row_3, *row_len) in rows:\n    if str(row_1) == \'model\':\n        url_after = \'/videos/upload\'\n    elif str(row_1) == \'users\':\n        url_after = \'/videos/public\'\n    elif str(row_1) == \'channels\':\n        url_after = \'/videos\'\n    else:\n        url_after = \'\'\n    print(\'-----------------------------\')\n    print(row_1)\n    print(row_2)\n    print(\'https://www.pornhub.com/\' + str(row_1) + \'/\' + str(row_2) + url_after)\n    print(\'-----------------------------\')\n    outtmpl = get_dl_location(\'DownloadLocation\') + \'/\' + str(row_1) + \'/\' + str(row_3) + \'/%(title)s.%(ext)s\'\n    ydl_opts = {\'format\': \'best\', \'outtmpl\': outtmpl, \'nooverwrites\': True, \'no_warnings\': False, \'ignoreerrors\': True}\n    url = \'https://www.pornhub.com/\' + str(row_1) + \'/\' + str(row_2) + url_after\n    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n        ydl.download([url])\n    try:\n        c.execute(""UPDATE ph_items SET new=\'0\', lastchecked=CURRENT_TIMESTAMP WHERE url_name=?"", (row_2,))\n        conn.commit()\n    except Error as e:\n        print(e)\n        sys.exit()']",1
jackdaw,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jackdaw/jackdaw/credentials/credentials.py,https://github.com/skelsec/jackdaw/tree/master/jackdaw/credentials/credentials.py,JackDawCredentials,get_uncracked_hashes$129,"def get_uncracked_hashes(self, hash_type, history):
    self.get_dbsession()
    try:
        if hash_type == 'NT':
            qry = self.dbsession.query(Credential.nt_hash).outerjoin(HashEntry, Credential.nt_hash == HashEntry.nt_hash).filter(Credential.nt_hash != None).distinct(Credential.nt_hash)
        else:
            qry = self.dbsession.query(Credential.lm_hash).outerjoin(HashEntry, Credential.lm_hash == HashEntry.lm_hash).filter(Credential.lm_hash != None).distinct(Credential.lm_hash)
        if history == False:
            qry = qry.filter(Credential.history_no == 0)
        for some_hash in qry.all():
            yield some_hash[0]
    except Exception as e:
        print(e)
    finally:
        self.dbsession.close()","for some_hash in qry.all():
    yield some_hash[0]","for (some_hash,) in qry.all():
    yield some_hash","['for some_hash in qry.all():\n    (some_hash_0, *some_hash_rsome_hashmaining) = some_hash\n    yield some_hash_0', 'for (some_hash_0, *some_hash_len) in qry.all():\n    yield\n    some_hash_0']",-1
NeoVintageous,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NeoVintageous/tests/functional/test__plugin_surround_ds.py,https://github.com/NeoVintageous/NeoVintageous/tree/master/tests/functional/test__plugin_surround_ds.py,TestSurround_ds,test_t_target_should_delete_tag$215,"def test_t_target_should_delete_tag(self):
    for t in tag_targets_data:
        self.eq('x {}a|b{} y'.format(t[0], t[1]), 'dst', 'x |ab y')","for t in tag_targets_data:
    self.eq('x {}a|b{} y'.format(t[0], t[1]), 'dst', 'x |ab y')","for (t_0, t_1) in tag_targets_data:
    self.eq(f'x {t_0}a|b{t_1} y', 'dst', 'x |ab y')","[""for t in tag_targets_data:\n    (t_0, t_1, *_) = t\n    self.eq('x {}a|b{} y'.format(t_0, t_1), 'dst', 'x |ab y')"", ""for (t_0, t_1, *t_len) in tag_targets_data:\n    self.eq('x {}a|b{} y'.format(t_0, t_1), 'dst', 'x |ab y')""]",-1
nlp_xiaojiang,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp_xiaojiang/ClassificationText/bert/keras_bert_classify_bi_lstm.py,https://github.com/yongzhuo/nlp_xiaojiang/tree/master/ClassificationText/bert/keras_bert_classify_bi_lstm.py,BertBiLstmModel,process_pair$99,"def process_pair(self, textss):
    input_ids = []
    input_masks = []
    input_type_ids = []
    for texts in textss:
        tokens_text = self.tokenizer.tokenize(texts[0])
        logger.info('Tokens1:', tokens_text)
        tokens_text2 = self.tokenizer.tokenize(texts[1])
        logger.info('Tokens2:', tokens_text2)
        (input_id, input_type_id) = self.tokenizer.encode(first=texts[0], second=texts[1], max_len=self.max_seq_len)
        input_mask = [0 if ids == 0 else 1 for ids in input_id]
        input_ids.append(input_id)
        input_type_ids.append(input_type_id)
        input_masks.append(input_mask)
    input_ids = np.array(input_ids)
    input_masks = np.array(input_masks)
    input_type_ids = np.array(input_type_ids)
    logger.info('process ok!')
    return (input_ids, input_masks, input_type_ids)","for texts in textss:
    tokens_text = self.tokenizer.tokenize(texts[0])
    logger.info('Tokens1:', tokens_text)
    tokens_text2 = self.tokenizer.tokenize(texts[1])
    logger.info('Tokens2:', tokens_text2)
    (input_id, input_type_id) = self.tokenizer.encode(first=texts[0], second=texts[1], max_len=self.max_seq_len)
    input_mask = [0 if ids == 0 else 1 for ids in input_id]
    input_ids.append(input_id)
    input_type_ids.append(input_type_id)
    input_masks.append(input_mask)","for (text1, text2) in textss:
    tokens_text = self.tokenizer.tokenize(text1)
    logger.info('Tokens1:', tokens_text)
    tokens_text2 = self.tokenizer.tokenize(text2)
    logger.info('Tokens2:', tokens_text2)
    (input_id, input_type_id) = self.tokenizer.encode(first=text1, second=text2, max_len=self.max_seq_len)
    input_mask = [0 if ids == 0 else 1 for ids in input_id]
    input_ids.append(input_id)
    input_type_ids.append(input_type_id)
    input_masks.append(input_mask)","[""for texts in textss:\n    (texts_0, texts_1, *_) = texts\n    tokens_text = self.tokenizer.tokenize(texts_0)\n    logger.info('Tokens1:', tokens_text)\n    tokens_text2 = self.tokenizer.tokenize(texts_1)\n    logger.info('Tokens2:', tokens_text2)\n    (input_id, input_type_id) = self.tokenizer.encode(first=texts_0, second=texts_1, max_len=self.max_seq_len)\n    input_mask = [0 if ids == 0 else 1 for ids in input_id]\n    input_ids.append(input_id)\n    input_type_ids.append(input_type_id)\n    input_masks.append(input_mask)"", ""for (texts_0, texts_1, *texts_len) in textss:\n    tokens_text = self.tokenizer.tokenize(texts_0)\n    logger.info('Tokens1:', tokens_text)\n    tokens_text2 = self.tokenizer.tokenize(texts_1)\n    logger.info('Tokens2:', tokens_text2)\n    (input_id, input_type_id) = self.tokenizer.encode(first=texts_0, second=texts_1, max_len=self.max_seq_len)\n    input_mask = [0 if ids == 0 else 1 for ids in input_id]\n    input_ids.append(input_id)\n    input_type_ids.append(input_type_id)\n    input_masks.append(input_mask)""]",-1
ASoulCnki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ASoulCnki/app/spider/reply/generate_refresh_like_spider.py,https://github.com/ASoulCnki/ASoulCnki/tree/master/app/spider/reply/generate_refresh_like_spider.py,,send_refresh_like_spider$6,"def send_refresh_like_spider(min_time):
    session = sqla['session']
    res = session.query(Reply.type_id, Reply.oid, Reply.dynamic_id, Reply.uid, min_time).filter(Reply.ctime > min_time).distinct(Reply.oid).all()
    for r in res:
        param = (r[0], r[1], r[2], r[3], r[4])
        tasks.refresh_like_num_task.apply_async(param, queue='reply_task_low_priority', routing_key='reply_low')","for r in res:
    param = (r[0], r[1], r[2], r[3], r[4])
    tasks.refresh_like_num_task.apply_async(param, queue='reply_task_low_priority', routing_key='reply_low')","for (type_id, oid, dynamic_id, uid, min_time) in res:
    param = (type_id, oid, dynamic_id, uid, min_time)
    tasks.refresh_like_num_task.apply_async(param, queue='reply_task_low_priority', routing_key='reply_low')","[""for r in res:\n    (r_0, r_1, r_2, r_3, r_4, *_) = r\n    param = (r_0, r_1, r_2, r_3, r_4)\n    tasks.refresh_like_num_task.apply_async(param, queue='reply_task_low_priority', routing_key='reply_low')"", ""for (r_0, r_1, r_2, r_3, r_4, *r_len) in res:\n    param = (r_0, r_1, r_2, r_3, r_4)\n    tasks.refresh_like_num_task.apply_async(param, queue='reply_task_low_priority', routing_key='reply_low')""]",-1
s3prl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/s3prl/s3prl/downstream/voxceleb2_amsoftmax_segment_eval/dataset.py,https://github.com/s3prl/s3prl/tree/master/s3prl/downstream/voxceleb2_amsoftmax_segment_eval/dataset.py,SpeakerVerifi_dev,segment_processing$162,"def segment_processing(self):
    wav_list = self.pair_dict['wav_table']
    utterance_id = 0
    segment_list = []
    print('processing test set to segments')
    for wav_info in tqdm.tqdm(wav_list):
        label_info = wav_info[0]
        pair_info = wav_info[1]
        (wav, _) = apply_effects_file(wav_info[2], EFFECTS)
        wav = wav.squeeze(0)
        index_end = len(wav) - self.segment_config['window']
        segment_num = index_end // self.segment_config['stride']
        if index_end < 0:
            segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, 0, len(wav), wav_info[2]])
        else:
            for index in range(0, index_end, self.segment_config['stride']):
                segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, index, index + self.segment_config['window'], wav_info[2]])
        utterance_id += 1
    return segment_list","for wav_info in tqdm.tqdm(wav_list):
    label_info = wav_info[0]
    pair_info = wav_info[1]
    (wav, _) = apply_effects_file(wav_info[2], EFFECTS)
    wav = wav.squeeze(0)
    index_end = len(wav) - self.segment_config['window']
    segment_num = index_end // self.segment_config['stride']
    if index_end < 0:
        segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, 0, len(wav), wav_info[2]])
    else:
        for index in range(0, index_end, self.segment_config['stride']):
            segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, index, index + self.segment_config['window'], wav_info[2]])
    utterance_id += 1","for (utterance_id, (label_info, pair_info, wav_file)) in enumerate(wav_list):
    (wav, _) = apply_effects_file(wav_file, EFFECTS)
    wav = wav.squeeze(0)
    index_end = len(wav) - self.segment_config['window']
    segment_num = index_end // self.segment_config['stride']
    if index_end < 0:
        segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, 0, len(wav), wav_file])
    else:
        for index in range(0, index_end, self.segment_config['stride']):
            segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, index, index + self.segment_config['window'], wav_file])
return segment_list","[""for wav_info in tqdm.tqdm(wav_list):\n    (wav_info_0, wav_info_1, wav_info_2, *_) = wav_info\n    label_info = wav_info_0\n    pair_info = wav_info_1\n    (wav, _) = apply_effects_file(wav_info_2, EFFECTS)\n    wav = wav.squeeze(0)\n    index_end = len(wav) - self.segment_config['window']\n    segment_num = index_end // self.segment_config['stride']\n    if index_end < 0:\n        segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, 0, len(wav), wav_info_2])\n    else:\n        for index in range(0, index_end, self.segment_config['stride']):\n            segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, index, index + self.segment_config['window'], wav_info_2])\n    utterance_id += 1"", ""for (wav_info_0, wav_info_1, wav_info_2, *wav_info_len) in tqdm.tqdm(wav_list):\n    label_info = \n    wav_info_0\n    pair_info = \n    wav_info_1\n    (wav, _) = apply_effects_file(\n    wav_info_2, EFFECTS)\n    wav = wav.squeeze(0)\n    index_end = len(wav) - self.segment_config['window']\n    segment_num = index_end // self.segment_config['stride']\n    if index_end < 0:\n        segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, 0, len(wav), \n        wav_info_2])\n    else:\n        for index in range(0, index_end, self.segment_config['stride']):\n            segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, index, index + self.segment_config['window'], \n            wav_info_2])\n    utterance_id += 1""]",-1
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_es.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_es.py,Num2WordsESTest,test_currency_omr$2686,"def test_currency_omr(self):
    for test in TEST_CASES_TO_CURRENCY_OMR:
        self.assertEqual(num2words(test[0], lang='es', to='currency', currency='OMR'), test[1])","for test in TEST_CASES_TO_CURRENCY_OMR:
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='OMR'), test[1])","for (value, expected_output) in TEST_CASES_TO_CURRENCY_OMR:
    self.assertEqual(num2words(value, lang='es', to='currency', currency='OMR'), expected_output)","[""for test in TEST_CASES_TO_CURRENCY_OMR:\n    (test_0, test_1, *_) = test\n    self.assertEqual(num2words(test_0, lang='es', to='currency', currency='OMR'), test_1)"", ""for (test_0, test_1, *test_len) in TEST_CASES_TO_CURRENCY_OMR:\n    self.assertEqual(num2words(test_0, lang='es', to='currency', currency='OMR'), test_1)""]",-1
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_es.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_es.py,Num2WordsESTest,test_currency_zar$2028,"def test_currency_zar(self):
    for test in TEST_CASES_TO_CURRENCY_ZAR:
        self.assertEqual(num2words(test[0], lang='es', to='currency', currency='ZAR'), test[1])","for test in TEST_CASES_TO_CURRENCY_ZAR:
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='ZAR'), test[1])","for (value, expected_output) in TEST_CASES_TO_CURRENCY_ZAR:
    self.assertEqual(num2words(value, lang='es', to='currency', currency='ZAR'), expected_output)","[""for test in TEST_CASES_TO_CURRENCY_ZAR:\n    (test_0, test_1, *_) = test\n    self.assertEqual(num2words(test_0, lang='es', to='currency', currency='ZAR'), test_1)"", ""for (test_0, test_1, *test_len) in TEST_CASES_TO_CURRENCY_ZAR:\n    self.assertEqual(num2words(test_0, lang='es', to='currency', currency='ZAR'), test_1)""]",-1
sublime_debugger,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sublime_debugger/debugger/interfaces/debugger_model.py,https://github.com/shuky19/sublime_debugger/tree/master/debugger/interfaces/debugger_model.py,DebuggerModel,referesh_data$123,"def referesh_data(self):
    for command in DebuggerModel.REFRESHABLE_COMMANDS:
        self.debugger.run_command(command)
    for watch_exp in self.data[DebuggerModel.DATA_WATCH]:
        self.debugger.run_result_command(DebuggerModel.COMMAND_GET_WATCH, watch_exp[0], watch_exp[0])","for watch_exp in self.data[DebuggerModel.DATA_WATCH]:
    self.debugger.run_result_command(DebuggerModel.COMMAND_GET_WATCH, watch_exp[0], watch_exp[0])","for (watch_exp_0, *_) in self.data[DebuggerModel.DATA_WATCH]:
    self.debugger.run_result_command(DebuggerModel.COMMAND_GET_WATCH, watch_exp_0, watch_exp_0)","['for watch_exp in self.data[DebuggerModel.DATA_WATCH]:\n    (watch_exp_0, *watch_exp_rwatch_expmaining) = watch_exp\n    self.debugger.run_result_command(DebuggerModel.COMMAND_GET_WATCH, watch_exp_0, watch_exp_0)', 'for (watch_exp_0, *watch_exp_len) in self.data[DebuggerModel.DATA_WATCH]:\n    self.debugger.run_result_command(DebuggerModel.COMMAND_GET_WATCH, watch_exp_0, watch_exp_0)']",1
cogdl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cogdl/cogdl/models/emb/gatne.py,https://github.com/THUDM/cogdl/tree/master/cogdl/models/emb/gatne.py,,get_G_from_edges$335,"def get_G_from_edges(edges):
    edge_dict = dict()
    for edge in edges:
        edge_key = str(edge[0]) + '_' + str(edge[1])
        if edge_key not in edge_dict:
            edge_dict[edge_key] = 1
        else:
            edge_dict[edge_key] += 1
    tmp_G = nx.Graph()
    for edge_key in edge_dict:
        weight = edge_dict[edge_key]
        x = int(edge_key.split('_')[0])
        y = int(edge_key.split('_')[1])
        tmp_G.add_edge(x, y)
        tmp_G[x][y]['weight'] = weight
    return tmp_G","for edge in edges:
    edge_key = str(edge[0]) + '_' + str(edge[1])
    if edge_key not in edge_dict:
        edge_dict[edge_key] = 1
    else:
        edge_dict[edge_key] += 1","for (x, y) in edges:
    edge_key = f'{x}_{y}'
    edge_dict[edge_key] = edge_dict.get(edge_key, 0) + 1","[""for edge in edges:\n    (edge_0, edge_1, *_) = edge\n    edge_key = str(edge_0) + '_' + str(edge_1)\n    if edge_key not in edge_dict:\n        edge_dict[edge_key] = 1\n    else:\n        edge_dict[edge_key] += 1"", ""for (edge_0, edge_1, *edge_len) in edges:\n    edge_key = str(edge_0) + '_' + str(edge_1)\n    if edge_key not in edge_dict:\n        edge_dict[edge_key] = 1\n    else:\n        edge_dict[edge_key] += 1""]",1
PaddleOCR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleOCR/PPOCRLabel/PPOCRLabel.py,https://github.com/PaddlePaddle/PaddleOCR/tree/master/PPOCRLabel/PPOCRLabel.py,MainWindow,editBox$904,"def editBox(self):
    if not self.canvas.editing():
        return
    item = self.currentBox()
    if not item:
        return
    text = self.labelDialog.popUp(item.text())
    imageSize = str(self.image.size())
    (width, height) = (self.image.width(), self.image.height())
    if text:
        try:
            text_list = eval(text)
        except:
            msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Please enter the correct format')
            msg_box.exec_()
            return
        if len(text_list) < 4:
            msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Please enter the coordinates of 4 points')
            msg_box.exec_()
            return
        for box in text_list:
            if box[0] > width or box[0] < 0 or box[1] > height or (box[1] < 0):
                msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Out of picture size')
                msg_box.exec_()
                return
        item.setText(text)
        self.setDirty()
        self.updateComboBox()","for box in text_list:
    if box[0] > width or box[0] < 0 or box[1] > height or (box[1] < 0):
        msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Out of picture size')
        msg_box.exec_()
        return","for (x, y, *_) in text_list:
    if x > width or x < 0 or y > height or (y < 0):
        msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Out of picture size')
        msg_box.exec_()
        return","[""for box in text_list:\n    (box_0, box_1, *_) = box\n    if box_0 > width or box_0 < 0 or box_1 > height or (box_1 < 0):\n        msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Out of picture size')\n        msg_box.exec_()\n        return"", ""for (box_0, box_1, *box_len) in text_list:\n    if \n    box_0 > width or \n    box_0 < 0 or \n    box_1 > height or (\n    box_1 < 0):\n        msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Out of picture size')\n        msg_box.exec_()\n        return""]",1
unilm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unilm/xtune/src/pequod/training/xtrainer.py,https://github.com/microsoft/unilm/tree/master/xtune/src/pequod/training/xtrainer.py,BaseTrainer,base_step$90,"def base_step(self, batches, is_qa=False, **kwargs):
    tot_loss = 0.0
    for step_batches in batches:
        batch = step_batches[0]
        batch_dict = self._parse_batch(batch)
        loss = self.model(**batch_dict)[0]
        self.backward_step(loss)
        tot_loss += loss.item()
    self.optim_step()
    return tot_loss / len(batches)","for step_batches in batches:
    batch = step_batches[0]
    batch_dict = self._parse_batch(batch)
    loss = self.model(**batch_dict)[0]
    self.backward_step(loss)
    tot_loss += loss.item()","for batch in batches:
    batch_dict = self._parse_batch(batch[0])
    loss = self.model(**batch_dict)[0]
    self.backward_step(loss)
    tot_loss += loss.item()","['for step_batches in batches:\n    (step_batches_0, *step_batches_rstep_batchesmaining) = step_batches\n    batch = step_batches_0\n    batch_dict = self._parse_batch(batch)\n    loss = self.model(**batch_dict)[0]\n    self.backward_step(loss)\n    tot_loss += loss.item()', 'for (step_batches_0, *step_batches_len) in batches:\n    batch = \n    step_batches_0\n    batch_dict = self._parse_batch(batch)\n    loss = self.model(**batch_dict)[0]\n    self.backward_step(loss)\n    tot_loss += loss.item()']",-1
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/obj/test_diskfile.py,https://github.com/openstack/swift/tree/master/test/unit/obj/test_diskfile.py,DiskFileMixin,_run_test$5727,"def _run_test():
    splice_called = [False]

    def fake_splice(fd_in, off_in, fd_out, off_out, len_, flags):
        if fd_out == devnull.fileno() and (not splice_called[0]):
            splice_called[0] = True
            err = errno.EWOULDBLOCK
            raise IOError(err, os.strerror(err))
        return splice(fd_in, off_in, fd_out, off_out, len_, flags)
    mock_splice.side_effect = fake_splice

    def fake_trampoline(fd, read=None, write=None, timeout=None, timeout_exc=timeout.Timeout, mark_as_closed=None):
        if write and fd == devnull.fileno():
            return
        else:
            hubs.trampoline(fd, read=read, write=write, timeout=timeout, timeout_exc=timeout_exc, mark_as_closed=mark_as_closed)
    mock_trampoline.side_effect = fake_trampoline
    reader.zero_copy_send(devnull.fileno())
    self.assertTrue(mock_close.called)
    mock_trampoline.assert_any_call(devnull.fileno(), write=True)
    for call in mock_splice.call_args_list:
        args = call[0]
        if args[2] == devnull.fileno():
            break
    else:
        self.fail('`splice` not called with expected arguments')","for call in mock_splice.call_args_list:
    args = call[0]
    if args[2] == devnull.fileno():
        break
else:
    self.fail('`splice` not called with expected arguments')","for call in mock_splice.call_args_list:
    args = call[0]
    if args[2] == devnull.fileno():
        break
else:
    self.fail('`splice` not called with expected arguments')","[""for call in mock_splice.call_args_list:\n    (call_0, *call_rcallmaining) = call\n    args = call_0\n    if args[2] == devnull.fileno():\n        break\nelse:\n    self.fail('`splice` not called with expected arguments')"", ""for (call_0, *call_len) in mock_splice.call_args_list:\n    args = \n    call_0\n    if args[2] == devnull.fileno():\n        break\nelse:\n    self.fail('`splice` not called with expected arguments')""]",-1
adminset,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adminset/cmdb/asset.py,https://github.com/guohongze/adminset/tree/master/cmdb/asset.py,,asset_import$160,"def asset_import(request):
    if request.method == 'POST':
        uf = request.FILES.get('asset_import')
        with open('/var/opt/adminset/data/asset.csv', 'wb+') as f:
            for chunk in uf.chunks(chunk_size=1024):
                f.write(chunk)
        try:
            filename = '/var/opt/adminset/data/asset.csv'
            with open(filename, 'rb') as f:
                title = next(csv.reader(f))
                for data in csv.reader(f):
                    data0 = str2gb2utf8(data[0])
                    if data0 == u'':
                        continue
                    try:
                        host = Host.objects.get(hostname=data0)
                    except Exception as msg:
                        host = Host()
                        host.hostname = data0
                    host.ip = data[1]
                    host.other_ip = str2gb2utf8(data[2])
                    if data[3]:
                        try:
                            idc_name = str2gb2utf8(data[3])
                            print('idc name is : {}'.format(idc_name))
                            print('idc name type: {}'.format(type(idc_name)))
                            item = Idc.objects.get(name=idc_name)
                            host.idc_id = item.id
                        except Exception as e:
                            print(e)
                            print('idc info import error')
                    host.asset_no = str2gb2utf8(data[4])
                    if data[5]:
                        asset_type = str2gb2utf8(data[5])
                        for (x, v) in ASSET_TYPE:
                            if v == asset_type:
                                ret = x
                        host.asset_type = ret
                    if data[6]:
                        status = str2gb2utf8(data[6])
                        for (x, v) in ASSET_STATUS:
                            if v == status:
                                ret = x
                        host.status = ret
                    host.os = str2gb2utf8(data[7])
                    host.vendor = str2gb2utf8(data[8])
                    host.cpu_model = str2gb2utf8(data[9])
                    host.cpu_num = str2gb2utf8(data[10])
                    host.memory = str2gb2utf8(data[11])
                    host.disk = data[12]
                    host.sn = str2gb2utf8(data[13])
                    host.position = str2gb2utf8(data[14])
                    host.memo = str2gb2utf8(data[15])
                    host.save()
            os.remove(filename)
            status = 1
        except Exception as e:
            print(e)
            print('import asset csv file error!')
            status = 2
    return render(request, 'cmdb/import.html', locals())","for data in csv.reader(f):
    data0 = str2gb2utf8(data[0])
    if data0 == u'':
        continue
    try:
        host = Host.objects.get(hostname=data0)
    except Exception as msg:
        host = Host()
        host.hostname = data0
    host.ip = data[1]
    host.other_ip = str2gb2utf8(data[2])
    if data[3]:
        try:
            idc_name = str2gb2utf8(data[3])
            print('idc name is : {}'.format(idc_name))
            print('idc name type: {}'.format(type(idc_name)))
            item = Idc.objects.get(name=idc_name)
            host.idc_id = item.id
        except Exception as e:
            print(e)
            print('idc info import error')
    host.asset_no = str2gb2utf8(data[4])
    if data[5]:
        asset_type = str2gb2utf8(data[5])
        for (x, v) in ASSET_TYPE:
            if v == asset_type:
                ret = x
        host.asset_type = ret
    if data[6]:
        status = str2gb2utf8(data[6])
        for (x, v) in ASSET_STATUS:
            if v == status:
                ret = x
        host.status = ret
    host.os = str2gb2utf8(data[7])
    host.vendor = str2gb2utf8(data[8])
    host.cpu_model = str2gb2utf8(data[9])
    host.cpu_num = str2gb2utf8(data[10])
    host.memory = str2gb2utf8(data[11])
    host.disk = data[12]
    host.sn = str2gb2utf8(data[13])
    host.position = str2gb2utf8(data[14])
    host.memo = str2gb2utf8(data[15])
    host.save()","for data in csv.reader(f):
    data0 = str2gb2utf8(data[0])
    if data0 == u'':
        continue
    try:
        host = Host.objects.get(hostname=data0)
    except Host.DoesNotExist:
        host = Host(hostname=data0)
    host.ip = data[1]
    host.other_ip = str2gb2utf8(data[2])
    if data[3]:
        try:
            idc_name = str2gb2utf8(data[3])
            print('idc name is : {}'.format(idc_name))
            print('idc name type: {}'.format(type(idc_name)))
            item = Idc.objects.get(name=idc_name)
            host.idc_id = item.id
        except Idc.DoesNotExist:
            print('idc info import error')
    host.asset_no = str2gb2utf8(data[4])
    if data[5]:
        asset_type = str2gb2utf8(data[5])
        for (x, v) in ASSET_TYPE:
            if v == asset_type:
                ret = x
        host.asset_type = ret
    if data[6]:
        status = str2gb2utf8(data[6])
        for (x, v) in ASSET_STATUS:
            if v == status:
                ret = x
        host.status = ret
    host.os = str2gb2utf8(data[7])
    host.vendor = str2gb2utf8(data[8])
    host.cpu_model = str2gb2utf8(data[9])
    host.cpu_num = str2gb2utf8(data[10])
    host.memory = str2gb2utf8(data[11])
    host.disk = data[12]
    host.sn = str2gb2utf8(data[13])
    host.position = str2gb2utf8(data[14])
    host.memo = str2gb2utf8(data[15])
    host.save()","[""for data in csv.reader(f):\n    (data_0, data_10, data_11, data_12, data_13, data_14, data_15, data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, *_) = data\n    data0 = str2gb2utf8(data_0)\n    if data0 == u'':\n        continue\n    try:\n        host = Host.objects.get(hostname=data0)\n    except Exception as msg:\n        host = Host()\n        host.hostname = data0\n    host.ip = data_1\n    host.other_ip = str2gb2utf8(data_2)\n    if data_3:\n        try:\n            idc_name = str2gb2utf8(data_3)\n            print('idc name is : {}'.format(idc_name))\n            print('idc name type: {}'.format(type(idc_name)))\n            item = Idc.objects.get(name=idc_name)\n            host.idc_id = item.id\n        except Exception as e:\n            print(e)\n            print('idc info import error')\n    host.asset_no = str2gb2utf8(data_4)\n    if data_5:\n        asset_type = str2gb2utf8(data_5)\n        for (x, v) in ASSET_TYPE:\n            if v == asset_type:\n                ret = x\n        host.asset_type = ret\n    if data_6:\n        status = str2gb2utf8(data_6)\n        for (x, v) in ASSET_STATUS:\n            if v == status:\n                ret = x\n        host.status = ret\n    host.os = str2gb2utf8(data_7)\n    host.vendor = str2gb2utf8(data_8)\n    host.cpu_model = str2gb2utf8(data_9)\n    host.cpu_num = str2gb2utf8(data_10)\n    host.memory = str2gb2utf8(data_11)\n    host.disk = data_12\n    host.sn = str2gb2utf8(data_13)\n    host.position = str2gb2utf8(data_14)\n    host.memo = str2gb2utf8(data_15)\n    host.save()"", ""for (data_0, data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10, data_11, data_12, data_13, data_14, data_15, *data_len) in csv.reader(f):\n    data0 = str2gb2utf8(\n    data_0)\n    if data0 == u'':\n        continue\n    try:\n        host = Host.objects.get(hostname=data0)\n    except Exception as msg:\n        host = Host()\n        host.hostname = data0\n    host.ip = \n    data_1\n    host.other_ip = str2gb2utf8(\n    data_2)\n    if \n    data_3:\n        try:\n            idc_name = str2gb2utf8(\n            data_3)\n            print('idc name is : {}'.format(idc_name))\n            print('idc name type: {}'.format(type(idc_name)))\n            item = Idc.objects.get(name=idc_name)\n            host.idc_id = item.id\n        except Exception as e:\n            print(e)\n            print('idc info import error')\n    host.asset_no = str2gb2utf8(\n    data_4)\n    if \n    data_5:\n        asset_type = str2gb2utf8(\n        data_5)\n        for (x, v) in ASSET_TYPE:\n            if v == asset_type:\n                ret = x\n        host.asset_type = ret\n    if \n    data_6:\n        status = str2gb2utf8(\n        data_6)\n        for (x, v) in ASSET_STATUS:\n            if v == status:\n                ret = x\n        host.status = ret\n    host.os = str2gb2utf8(\n    data_7)\n    host.vendor = str2gb2utf8(\n    data_8)\n    host.cpu_model = str2gb2utf8(\n    data_9)\n    host.cpu_num = str2gb2utf8(\n    data_10)\n    host.memory = str2gb2utf8(\n    data_11)\n    host.disk = \n    data_12\n    host.sn = str2gb2utf8(\n    data_13)\n    host.position = str2gb2utf8(\n    data_14)\n    host.memo = str2gb2utf8(\n    data_15)\n    host.save()""]",-1
dash,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dash/dash/testing/application_runners.py,https://github.com/plotly/dash/tree/master/dash/testing/application_runners.py,JuliaRunner,start$340,"def start(self, app, start_timeout=30, cwd=None):
    """"""Start the server with subprocess and julia.""""""
    if os.path.isfile(app) and os.path.exists(app):
        if not cwd:
            cwd = os.path.dirname(app)
            logger.info('JuliaRunner inferred cwd from app path: %s', cwd)
    else:
        self._tmp_app_path = os.path.join('/tmp' if not self.is_windows else os.getenv('TEMP'), uuid.uuid4().hex)
        try:
            os.mkdir(self.tmp_app_path)
        except OSError:
            logger.exception('cannot make temporary folder %s', self.tmp_app_path)
        path = os.path.join(self.tmp_app_path, 'app.jl')
        logger.info('JuliaRunner start => app is Julia code chunk')
        logger.info('make a temporary Julia file for execution => %s', path)
        logger.debug('content of the Dash.jl app')
        logger.debug('%s', app)
        with open(path, 'w') as fp:
            fp.write(app)
        app = path
        if not cwd:
            for entry in inspect.stack():
                if '/dash/testing/' not in entry[1].replace('\\', '/'):
                    cwd = os.path.dirname(os.path.realpath(entry[1]))
                    logger.warning('get cwd from inspect => %s', cwd)
                    break
        if cwd:
            logger.info('JuliaRunner inferred cwd from the Python call stack: %s', cwd)
            assets = [os.path.join(cwd, _) for _ in os.listdir(cwd) if not _.startswith('__') and os.path.isdir(os.path.join(cwd, _))]
            for asset in assets:
                target = os.path.join(self.tmp_app_path, os.path.basename(asset))
                if os.path.exists(target):
                    logger.debug('delete existing target %s', target)
                    shutil.rmtree(target)
                logger.debug('copying %s => %s', asset, self.tmp_app_path)
                shutil.copytree(asset, target)
                logger.debug('copied with %s', os.listdir(target))
        else:
            logger.warning('JuliaRunner found no cwd in the Python call stack. You may wish to specify an explicit working directory using something like: dashjl.run_server(app, cwd=os.path.dirname(__file__))')
    logger.info('Run Dash.jl app with julia => %s', app)
    args = shlex.split('julia {}'.format(os.path.realpath(app)), posix=not self.is_windows)
    logger.debug('start Dash.jl process with %s', args)
    try:
        self.proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=self.tmp_app_path if self.tmp_app_path else cwd)
        wait.until(lambda : self.accessible(self.url), timeout=start_timeout)
    except (OSError, ValueError):
        logger.exception('process server has encountered an error')
        self.started = False
        return
    self.started = True","for entry in inspect.stack():
    if '/dash/testing/' not in entry[1].replace('\\', '/'):
        cwd = os.path.dirname(os.path.realpath(entry[1]))
        logger.warning('get cwd from inspect => %s', cwd)
        break","for entry in inspect.stack():
    if '/dash/testing/' not in entry.filename.replace('\\', '/'):
        cwd = os.path.dirname(os.path.realpath(entry.filename))
        logger.warning('get cwd from inspect => %s', cwd)
        break","[""for entry in inspect.stack():\n    (entry_0, entry_1, *entry_rentrymaining) = entry\n    if '/dash/testing/' not in entry_1.replace('\\\\', '/'):\n        cwd = os.path.dirname(os.path.realpath(entry_1))\n        logger.warning('get cwd from inspect => %s', cwd)\n        break"", ""for (entry_0, entry_1, *entry_len) in inspect.stack():\n    if '/dash/testing/' not in \n    entry_1.replace('\\\\', '/'):\n        cwd = os.path.dirname(os.path.realpath(\n        entry_1))\n        logger.warning('get cwd from inspect => %s', cwd)\n        break""]",-1
clusterfuzz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/clusterfuzz/_internal/tests/appengine/libs/query/datastore_query_test.py,https://github.com/google/clusterfuzz/tree/master/src/clusterfuzz/_internal/tests/appengine/libs/query/datastore_query_test.py,QueryMockTest,test_third_page$253,"def test_third_page(self):
    """"""Test getting the third page with more total count.""""""
    query = datastore_query.Query(TestDatastoreModel)
    query.filter_in('tokens', ['a', 'b'])
    query.filter('boolean_value', True)
    query.order('datetime_value', is_desc=True)
    query.fetch_page(page=1, page_size=2, projection=['tokens'], more_limit=4)
    self.assertIsInstance(self.queries[0][-1].filters, ndb.AND)
    six.assertCountEqual(self, [('tokens', '=', 'a'), ('boolean_value', '=', True)], [f.__getnewargs__() for f in self.queries[0][-1].filters])
    self.assertIsInstance(self.queries[1][-1].filters, ndb.AND)
    six.assertCountEqual(self, [('tokens', '=', 'b'), ('boolean_value', '=', True)], [f.__getnewargs__() for f in self.queries[1][-1].filters])
    self.assertIsInstance(self.queries[2][-1].filters, ndb.OR)
    expected = []
    for item in [f.__getnewargs__() for f in self.queries[2][-1].filters]:
        expected.append((item[0], item[1], repr(item[2])))
    six.assertCountEqual(self, [('__key__', '=', ""<Key('TestDatastoreModel', 0), project=test-clusterfuzz>""), ('__key__', '=', ""<Key('TestDatastoreModel', 1), project=test-clusterfuzz>"")], expected)","for item in [f.__getnewargs__() for f in self.queries[2][-1].filters]:
    expected.append((item[0], item[1], repr(item[2])))","for f in self.queries[2][-1].filters:
    expected.append((f.property.name, f.operator, repr(f.value)))","['for item in [f.__getnewargs__() for f in self.queries[2][-1].filters]:\n    (item_0, item_1, item_2, *_) = item\n    expected.append((item_0, item_1, repr(item_2)))', 'for (item_0, item_1, item_2, *item_len) in [f.__getnewargs__() for f in self.queries[2][-1].filters]:\n    expected.append((item_0, item_1, repr(item_2)))']",-1
TSD,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TSD/mmdet/models/anchor_heads/fovea_head.py,https://github.com/Sense-X/TSD/tree/master/mmdet/models/anchor_heads/fovea_head.py,FoveaHead,get_points$181,"def get_points(self, featmap_sizes, dtype, device, flatten=False):
    points = []
    for featmap_size in featmap_sizes:
        x_range = torch.arange(featmap_size[1], dtype=dtype, device=device) + 0.5
        y_range = torch.arange(featmap_size[0], dtype=dtype, device=device) + 0.5
        (y, x) = torch.meshgrid(y_range, x_range)
        if flatten:
            points.append((y.flatten(), x.flatten()))
        else:
            points.append((y, x))
    return points","for featmap_size in featmap_sizes:
    x_range = torch.arange(featmap_size[1], dtype=dtype, device=device) + 0.5
    y_range = torch.arange(featmap_size[0], dtype=dtype, device=device) + 0.5
    (y, x) = torch.meshgrid(y_range, x_range)
    if flatten:
        points.append((y.flatten(), x.flatten()))
    else:
        points.append((y, x))","for featmap_size in featmap_sizes:
    x_range = torch.arange(featmap_size[1], dtype=dtype, device=device) + 0.5
    y_range = torch.arange(featmap_size[0], dtype=dtype, device=device) + 0.5
    (y, x) = torch.meshgrid(y_range, x_range)
    points.append((y.flatten(), x.flatten()) if flatten else (y, x))","['for featmap_size in featmap_sizes:\n    (featmap_size_0, featmap_size_1, *_) = featmap_size\n    x_range = torch.arange(featmap_size_1, dtype=dtype, device=device) + 0.5\n    y_range = torch.arange(featmap_size_0, dtype=dtype, device=device) + 0.5\n    (y, x) = torch.meshgrid(y_range, x_range)\n    if flatten:\n        points.append((y.flatten(), x.flatten()))\n    else:\n        points.append((y, x))', 'for (featmap_size_0, featmap_size_1, *featmap_size_len) in featmap_sizes:\n    x_range = torch.arange(featmap_size_1, dtype=dtype, device=device) + 0.5\n    y_range = torch.arange(featmap_size_0, dtype=dtype, device=device) + 0.5\n    (y, x) = torch.meshgrid(y_range, x_range)\n    if flatten:\n        points.append((y.flatten(), x.flatten()))\n    else:\n        points.append((y, x))']",-1
stellargraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stellargraph/tests/test_ensemble.py,https://github.com/stellargraph/stellargraph/tree/master/tests/test_ensemble.py,,test_evaluate_link_prediction$620,"def test_evaluate_link_prediction():
    tf.keras.backend.clear_session()
    edge_ids_test = np.array([[1, 2], [2, 3], [1, 3]])
    edge_labels_test = np.array([1, 1, 0])
    graph = example_graph_1(feature_size=4)
    gnn_models = [create_graphSAGE_model(graph, link_prediction=True), create_HinSAGE_model(graph, link_prediction=True)]
    for gnn_model in gnn_models:
        keras_model = gnn_model[1]
        generator = gnn_model[2]
        ens = Ensemble(keras_model, n_estimators=2, n_predictions=1)
        ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
        with pytest.raises(ValueError):
            ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
        with pytest.raises(ValueError):
            ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
        with pytest.raises(ValueError):
            ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
        (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
        assert len(test_metrics_mean) == len(test_metrics_std)
        assert len(test_metrics_mean.shape) == 1
        assert len(test_metrics_std.shape) == 1
        ens = BaggingEnsemble(keras_model, n_estimators=2, n_predictions=1)
        ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
        with pytest.raises(ValueError):
            ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
        with pytest.raises(ValueError):
            ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
        with pytest.raises(ValueError):
            ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
        (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
        assert len(test_metrics_mean) == len(test_metrics_std)
        assert len(test_metrics_mean.shape) == 1
        assert len(test_metrics_std.shape) == 1","for gnn_model in gnn_models:
    keras_model = gnn_model[1]
    generator = gnn_model[2]
    ens = Ensemble(keras_model, n_estimators=2, n_predictions=1)
    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
    assert len(test_metrics_mean) == len(test_metrics_std)
    assert len(test_metrics_mean.shape) == 1
    assert len(test_metrics_std.shape) == 1
    ens = BaggingEnsemble(keras_model, n_estimators=2, n_predictions=1)
    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
    assert len(test_metrics_mean) == len(test_metrics_std)
    assert len(test_metrics_mean.shape) == 1
    assert len(test_metrics_std.shape) == 1","for gnn_model in gnn_models:
    (keras_model, generator, ens) = (gnn_model[1], gnn_model[2], Ensemble(gnn_model[1], n_estimators=2, n_predictions=1))
    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
    assert len(test_metrics_mean) == len(test_metrics_std) == 1
    ens = BaggingEnsemble(keras_model, n_estimators=2, n_predictions=1)
    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
    assert len(test_metrics_mean) == len(test_metrics_std) == 1","[""for gnn_model in gnn_models:\n    (_, gnn_model_1, gnn_model_2, *gnn_model_rgnn_modelmaining) = gnn_model\n    keras_model = gnn_model_1\n    generator = gnn_model_2\n    ens = Ensemble(keras_model, n_estimators=2, n_predictions=1)\n    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)\n    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))\n    assert len(test_metrics_mean) == len(test_metrics_std)\n    assert len(test_metrics_mean.shape) == 1\n    assert len(test_metrics_std.shape) == 1\n    ens = BaggingEnsemble(keras_model, n_estimators=2, n_predictions=1)\n    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)\n    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))\n    assert len(test_metrics_mean) == len(test_metrics_std)\n    assert len(test_metrics_mean.shape) == 1\n    assert len(test_metrics_std.shape) == 1"", ""for (gnn_model_0, gnn_model_1, gnn_model_2, *gnn_model_len) in gnn_models:\n    keras_model = \n    gnn_model_1\n    generator = \n    gnn_model_2\n    ens = Ensemble(keras_model, n_estimators=2, n_predictions=1)\n    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)\n    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))\n    assert len(test_metrics_mean) == len(test_metrics_std)\n    assert len(test_metrics_mean.shape) == 1\n    assert len(test_metrics_std.shape) == 1\n    ens = BaggingEnsemble(keras_model, n_estimators=2, n_predictions=1)\n    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)\n    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))\n    assert len(test_metrics_mean) == len(test_metrics_std)\n    assert len(test_metrics_mean.shape) == 1\n    assert len(test_metrics_std.shape) == 1""]",1
fonttools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/cffLib/specializer.py,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/cffLib/specializer.py,_GeneralizerDecombinerCommandsMap,hhcurveto$200,"def hhcurveto(args):
    if len(args) < 4 or len(args) % 4 > 1:
        raise ValueError(args)
    if len(args) % 2 == 1:
        yield ('rrcurveto', [args[1], args[0], args[2], args[3], args[4], 0])
        args = args[5:]
    for args in _everyN(args, 4):
        yield ('rrcurveto', [args[0], 0, args[1], args[2], args[3], 0])","for args in _everyN(args, 4):
    yield ('rrcurveto', [args[0], 0, args[1], args[2], args[3], 0])","for i in range(0, len(args), 4):
    yield ('rrcurveto', [args[i], 0, args[i + 1], args[i + 2], args[i + 3], 0])","[""for args in _everyN(args, 4):\n    (args_0, args_1, args_2, args_3, *_) = args\n    yield ('rrcurveto', [args_0, 0, args_1, args_2, args_3, 0])"", ""for (args_0, args_1, args_2, args_3, *args_len) in _everyN(args, 4):\n    yield ('rrcurveto', [args_0, 0, args_1, args_2, args_3, 0])""]",-1
UER-py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/UER-py/uer/utils/mask.py,https://github.com/dbiir/UER-py/tree/master/uer/utils/mask.py,,mask_seq$5,"def mask_seq(src, tokenizer, whole_word_masking, span_masking, span_geo_prob, span_max_length):
    vocab = tokenizer.vocab
    PAD_ID = vocab.get(PAD_TOKEN)
    for i in range(len(src) - 1, -1, -1):
        if src[i] != PAD_ID:
            break
    src_no_pad = src[:i + 1]
    (tokens_index, src_no_pad) = create_index(src_no_pad, tokenizer, whole_word_masking, span_masking, span_geo_prob, span_max_length)
    if len(src_no_pad) < len(src):
        src = src_no_pad + (len(src) - len(src_no_pad)) * [PAD_ID]
    else:
        src = src_no_pad
    random.shuffle(tokens_index)
    num_to_predict = max(1, int(round(len(src_no_pad) * 0.15)))
    tgt_mlm = []
    for index_set in tokens_index:
        if len(tgt_mlm) >= num_to_predict:
            break
        if whole_word_masking:
            i = index_set[0]
            mask_len = index_set[1]
            if len(tgt_mlm) + mask_len > num_to_predict:
                continue
            for j in range(mask_len):
                token = src[i + j]
                tgt_mlm.append((i + j, token))
                prob = random.random()
                if prob < 0.8:
                    src[i + j] = vocab.get(MASK_TOKEN)
                elif prob < 0.9:
                    while True:
                        rdi = random.randint(1, len(vocab) - 1)
                        if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                            break
                    src[i + j] = rdi
        elif span_masking:
            i = index_set[0]
            span_len = index_set[1]
            if len(tgt_mlm) + span_len > num_to_predict:
                continue
            for j in range(span_len):
                token = src[i + j]
                tgt_mlm.append((i + j, token))
            prob = random.random()
            if prob < 0.8:
                for j in range(span_len):
                    src[i + j] = vocab.get(MASK_TOKEN)
            elif prob < 0.9:
                for j in range(span_len):
                    while True:
                        rdi = random.randint(1, len(vocab) - 1)
                        if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                            break
                    src[i + j] = rdi
        else:
            i = index_set[0]
            token = src[i]
            tgt_mlm.append((i, token))
            prob = random.random()
            if prob < 0.8:
                src[i] = vocab.get(MASK_TOKEN)
            elif prob < 0.9:
                while True:
                    rdi = random.randint(1, len(vocab) - 1)
                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                        break
                src[i] = rdi
    tgt_mlm = sorted(tgt_mlm, key=lambda x: x[0])
    return (src, tgt_mlm)","for index_set in tokens_index:
    if len(tgt_mlm) >= num_to_predict:
        break
    if whole_word_masking:
        i = index_set[0]
        mask_len = index_set[1]
        if len(tgt_mlm) + mask_len > num_to_predict:
            continue
        for j in range(mask_len):
            token = src[i + j]
            tgt_mlm.append((i + j, token))
            prob = random.random()
            if prob < 0.8:
                src[i + j] = vocab.get(MASK_TOKEN)
            elif prob < 0.9:
                while True:
                    rdi = random.randint(1, len(vocab) - 1)
                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                        break
                src[i + j] = rdi
    elif span_masking:
        i = index_set[0]
        span_len = index_set[1]
        if len(tgt_mlm) + span_len > num_to_predict:
            continue
        for j in range(span_len):
            token = src[i + j]
            tgt_mlm.append((i + j, token))
        prob = random.random()
        if prob < 0.8:
            for j in range(span_len):
                src[i + j] = vocab.get(MASK_TOKEN)
        elif prob < 0.9:
            for j in range(span_len):
                while True:
                    rdi = random.randint(1, len(vocab) - 1)
                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                        break
                src[i + j] = rdi
    else:
        i = index_set[0]
        token = src[i]
        tgt_mlm.append((i, token))
        prob = random.random()
        if prob < 0.8:
            src[i] = vocab.get(MASK_TOKEN)
        elif prob < 0.9:
            while True:
                rdi = random.randint(1, len(vocab) - 1)
                if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                    break
            src[i] = rdi","for index_set in tokens_index:
    if len(tgt_mlm) >= num_to_predict:
        break
    i = index_set[0]
    if whole_word_masking:
        mask_len = index_set[1]
        if len(tgt_mlm) + mask_len > num_to_predict:
            continue
        for j in range(mask_len):
            token = src[i + j]
            tgt_mlm.append((i + j, token))
            prob = random.random()
            if prob < 0.8:
                src[i + j] = vocab.get(MASK_TOKEN)
            elif prob < 0.9:
                while True:
                    rdi = random.randint(1, len(vocab) - 1)
                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                        break
                src[i + j] = rdi
    elif span_masking:
        span_len = index_set[1]
        if len(tgt_mlm) + span_len > num_to_predict:
            continue
        for j in range(span_len):
            token = src[i + j]
            tgt_mlm.append((i + j, token))
        prob = random.random()
        if prob < 0.8:
            for j in range(span_len):
                src[i + j] = vocab.get(MASK_TOKEN)
        elif prob < 0.9:
            for j in range(span_len):
                while True:
                    rdi = random.randint(1, len(vocab) - 1)
                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                        break
                src[i + j] = rdi
    else:
        token = src[i]
        tgt_mlm.append((i, token))
        prob = random.random()
        if prob < 0.8:
            src[i] = vocab.get(MASK_TOKEN)
        elif prob < 0.9:
            while True:
                rdi = random.randint(1, len(vocab) - 1)
                if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                    break
            src[i] = rdi","['for index_set in tokens_index:\n    (index_set_0, index_set_1, *_) = index_set\n    if len(tgt_mlm) >= num_to_predict:\n        break\n    if whole_word_masking:\n        i = index_set_0\n        mask_len = index_set_1\n        if len(tgt_mlm) + mask_len > num_to_predict:\n            continue\n        for j in range(mask_len):\n            token = src[i + j]\n            tgt_mlm.append((i + j, token))\n            prob = random.random()\n            if prob < 0.8:\n                src[i + j] = vocab.get(MASK_TOKEN)\n            elif prob < 0.9:\n                while True:\n                    rdi = random.randint(1, len(vocab) - 1)\n                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:\n                        break\n                src[i + j] = rdi\n    elif span_masking:\n        i = index_set_0\n        span_len = index_set_1\n        if len(tgt_mlm) + span_len > num_to_predict:\n            continue\n        for j in range(span_len):\n            token = src[i + j]\n            tgt_mlm.append((i + j, token))\n        prob = random.random()\n        if prob < 0.8:\n            for j in range(span_len):\n                src[i + j] = vocab.get(MASK_TOKEN)\n        elif prob < 0.9:\n            for j in range(span_len):\n                while True:\n                    rdi = random.randint(1, len(vocab) - 1)\n                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:\n                        break\n                src[i + j] = rdi\n    else:\n        i = index_set_0\n        token = src[i]\n        tgt_mlm.append((i, token))\n        prob = random.random()\n        if prob < 0.8:\n            src[i] = vocab.get(MASK_TOKEN)\n        elif prob < 0.9:\n            while True:\n                rdi = random.randint(1, len(vocab) - 1)\n                if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:\n                    break\n            src[i] = rdi', 'for (index_set_0, index_set_1, *index_set_len) in tokens_index:\n    if len(tgt_mlm) >= num_to_predict:\n        break\n    if whole_word_masking:\n        i = \n        index_set_0\n        mask_len = \n        index_set_1\n        if len(tgt_mlm) + mask_len > num_to_predict:\n            continue\n        for j in range(mask_len):\n            token = src[i + j]\n            tgt_mlm.append((i + j, token))\n            prob = random.random()\n            if prob < 0.8:\n                src[i + j] = vocab.get(MASK_TOKEN)\n            elif prob < 0.9:\n                while True:\n                    rdi = random.randint(1, len(vocab) - 1)\n                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:\n                        break\n                src[i + j] = rdi\n    elif span_masking:\n        i = \n        index_set_0\n        span_len = \n        index_set_1\n        if len(tgt_mlm) + span_len > num_to_predict:\n            continue\n        for j in range(span_len):\n            token = src[i + j]\n            tgt_mlm.append((i + j, token))\n        prob = random.random()\n        if prob < 0.8:\n            for j in range(span_len):\n                src[i + j] = vocab.get(MASK_TOKEN)\n        elif prob < 0.9:\n            for j in range(span_len):\n                while True:\n                    rdi = random.randint(1, len(vocab) - 1)\n                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:\n                        break\n                src[i + j] = rdi\n    else:\n        i = \n        index_set_0\n        token = src[i]\n        tgt_mlm.append((i, token))\n        prob = random.random()\n        if prob < 0.8:\n            src[i] = vocab.get(MASK_TOKEN)\n        elif prob < 0.9:\n            while True:\n                rdi = random.randint(1, len(vocab) - 1)\n                if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:\n                    break\n            src[i] = rdi']",-1
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/tools/offlinearize.py,https://github.com/nlplab/brat/tree/master/tools/offlinearize.py,,convert_coll$47,"def convert_coll(coll):
    if coll == '':
        ajax_coll = '/'
    else:
        ajax_coll = '/%s/' % coll
    coll_query_url = urljoin(base_url, 'ajax.cgi?action=getCollectionInformation&collection=%s' % ajax_coll)
    coll_dir = joinpath(datadir, coll)
    try:
        makedirs(coll_dir)
    except BaseException:
        pass
    print(ajax_coll)
    conn = urlopen(coll_query_url)
    jsonp = conn.read()
    conn.close
    with open(joinpath(coll_dir, 'collection.js'), 'w') as f:
        f.write('jsonp=')
        f.write(jsonp)
    coll_data = loads(jsonp)
    for item in coll_data['items']:
        if item[0] == 'd':
            doc = item[2]
            print('  %s' % doc)
            doc_query_url = urljoin(base_url, 'ajax.cgi?action=getDocument&collection=%s&document=%s' % (ajax_coll, doc))
            conn = urlopen(doc_query_url)
            jsonp = conn.read()
            conn.close
            with open(joinpath(coll_dir, '%s.data.js' % doc), 'w') as f:
                f.write('jsonp=')
                f.write(jsonp)
        elif item[0] == 'c' and item[2] != '..':
            convert_coll(item[2])","for item in coll_data['items']:
    if item[0] == 'd':
        doc = item[2]
        print('  %s' % doc)
        doc_query_url = urljoin(base_url, 'ajax.cgi?action=getDocument&collection=%s&document=%s' % (ajax_coll, doc))
        conn = urlopen(doc_query_url)
        jsonp = conn.read()
        conn.close
        with open(joinpath(coll_dir, '%s.data.js' % doc), 'w') as f:
            f.write('jsonp=')
            f.write(jsonp)
    elif item[0] == 'c' and item[2] != '..':
        convert_coll(item[2])","for item in coll_data['items']:
    if item[0] == 'd':
        doc = item[2]
        print('  %s' % doc)
        doc_query_url = urljoin(base_url, 'ajax.cgi?action=getDocument&collection=%s&document=%s' % (ajax_coll, doc))
        conn = urlopen(doc_query_url)
        jsonp = conn.read()
        conn.close
        with open(joinpath(coll_dir, '%s.data.js' % doc), 'w') as f:
            f.write('jsonp=')
            f.write(jsonp)
    elif item[0] == 'c' and item[2] != '..':
        convert_coll(item[2], base_url, datadir)","[""for item in coll_data['items']:\n    (item_0, _, item_2, *item_ritemmaining) = item\n    if item_0 == 'd':\n        doc = item_2\n        print('  %s' % doc)\n        doc_query_url = urljoin(base_url, 'ajax.cgi?action=getDocument&collection=%s&document=%s' % (ajax_coll, doc))\n        conn = urlopen(doc_query_url)\n        jsonp = conn.read()\n        conn.close\n        with open(joinpath(coll_dir, '%s.data.js' % doc), 'w') as f:\n            f.write('jsonp=')\n            f.write(jsonp)\n    elif item_0 == 'c' and item_2 != '..':\n        convert_coll(item_2)"", ""for (item_0, item_1, item_2, *item_len) in coll_data['items']:\n    if \n    item_0 == 'd':\n        doc = \n        item_2\n        print('  %s' % doc)\n        doc_query_url = urljoin(base_url, 'ajax.cgi?action=getDocument&collection=%s&document=%s' % (ajax_coll, doc))\n        conn = urlopen(doc_query_url)\n        jsonp = conn.read()\n        conn.close\n        with open(joinpath(coll_dir, '%s.data.js' % doc), 'w') as f:\n            f.write('jsonp=')\n            f.write(jsonp)\n    elif \n    item_0 == 'c' and \n    item_2 != '..':\n        convert_coll(\n        item_2)""]",-1
taurus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taurus/bzt/modules/aggregator.py,https://github.com/Blazemeter/taurus/tree/master/bzt/modules/aggregator.py,KPISet,items$351,"def items(self):
    for item in super(KPISet, self).items():
        yield (item[0], self.__getitem__(item[0]))","for item in super(KPISet, self).items():
    yield (item[0], self.__getitem__(item[0]))","for key in super(KPISet, self).keys():
    yield (key, self[key])","['for item in super(KPISet, self).items():\n    (item_0, *item_ritemmaining) = item\n    yield (item_0, self.__getitem__(item_0))', 'for (item_0, *item_len) in super(KPISet, self).items():\n    yield (item_0, self.__getitem__(item_0))']",-1
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/dataset/tests/flowers_test.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/dataset/tests/flowers_test.py,TestFlowers,check_reader$23,"def check_reader(self, reader):
    sum = 0
    label = 0
    size = 224 * 224 * 3
    for l in reader():
        self.assertEqual(l[0].size, size)
        if l[1] > label:
            label = l[1]
        sum += 1
    return (sum, label)","for l in reader():
    self.assertEqual(l[0].size, size)
    if l[1] > label:
        label = l[1]
    sum += 1","for l in reader():
    self.assertEqual(l[0].size, size)
    label = max(label, l[1])
    sum += 1","['for l in reader():\n    (l_0, l_1, *_) = l\n    self.assertEqual(l_0.size, size)\n    if l_1 > label:\n        label = l_1\n    sum += 1', 'for (l_0, l_1, *l_len) in reader():\n    self.assertEqual(\n    l_0.size, size)\n    if \n    l_1 > label:\n        label = \n        l_1\n    sum += 1']",-1
mifthtools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mifthtools/blender/addons/2.8/mira_tools/mi_curve_stretch.py,https://github.com/mifth/mifthtools/tree/master/blender/addons/2.8/mira_tools/mi_curve_stretch.py,MI_OT_CurveStretch,start_tool$82,"def start_tool(self, context):
    args = (self, context)
    cur_stretch_settings = context.scene.mi_cur_stretch_settings
    curve_settings = context.scene.mi_settings
    active_obj = context.active_object
    bm = bmesh.from_edit_mesh(active_obj.data)
    self.manipulator = context.space_data.show_gizmo
    context.space_data.show_gizmo = False
    for loop in self.loops:
        loop_verts = [active_obj.matrix_world @ bm.verts[i].co for i in loop[0]]
        loop_line = cur_main.pass_line(loop_verts, loop[1])
        new_curve = cur_main.create_curve_to_line(cur_stretch_settings.points_number, loop_line, self.all_curves, loop[1])
        if loop[1] is True:
            new_curve.closed = True
        self.all_curves.append(new_curve)
        self.active_curve = new_curve
        cur_main.generate_bezier_points(self.active_curve, self.active_curve.display_bezier, curve_settings.curve_resolution)
        self.original_verts_data.append(cur_main.pass_line([bm.verts[i].co.copy() for i in loop[0]], loop[1]))
        for curve in self.all_curves:
            update_curve_line(active_obj, curve, self.loops, self.all_curves, bm, curve_settings.spread_mode, self.original_verts_data[self.all_curves.index(curve)])
        if curve_settings.surface_snap is True:
            meshes_array = ut_base.get_obj_dup_meshes(curve_settings.snap_objects, curve_settings.convert_instances, context)
            if meshes_array:
                self.picked_meshes = meshes_array
    self.mi_deform_handle_3d = bpy.types.SpaceView3D.draw_handler_add(mi_curve_draw_3d, args, 'WINDOW', 'POST_VIEW')
    self.mi_deform_handle_2d = bpy.types.SpaceView3D.draw_handler_add(mi_curve_draw_2d, args, 'WINDOW', 'POST_PIXEL')
    self.gh_circle_select_handle = bpy.types.SpaceView3D.draw_handler_add(gh_circle_draw_2d, args, 'WINDOW', 'POST_PIXEL')
    bm.normal_update()
    bmesh.update_edit_mesh(active_obj.data)","for loop in self.loops:
    loop_verts = [active_obj.matrix_world @ bm.verts[i].co for i in loop[0]]
    loop_line = cur_main.pass_line(loop_verts, loop[1])
    new_curve = cur_main.create_curve_to_line(cur_stretch_settings.points_number, loop_line, self.all_curves, loop[1])
    if loop[1] is True:
        new_curve.closed = True
    self.all_curves.append(new_curve)
    self.active_curve = new_curve
    cur_main.generate_bezier_points(self.active_curve, self.active_curve.display_bezier, curve_settings.curve_resolution)
    self.original_verts_data.append(cur_main.pass_line([bm.verts[i].co.copy() for i in loop[0]], loop[1]))
    for curve in self.all_curves:
        update_curve_line(active_obj, curve, self.loops, self.all_curves, bm, curve_settings.spread_mode, self.original_verts_data[self.all_curves.index(curve)])
    if curve_settings.surface_snap is True:
        meshes_array = ut_base.get_obj_dup_meshes(curve_settings.snap_objects, curve_settings.convert_instances, context)
        if meshes_array:
            self.picked_meshes = meshes_array","for loop in self.loops:
    loop_verts = [active_obj.matrix_world @ bm.verts[i].co for i in loop[0]]
    loop_line = cur_main.pass_line(loop_verts, loop[1])
    new_curve = cur_main.create_curve_to_line(cur_stretch_settings.points_number, loop_line, self.all_curves, loop[1])
    if loop[1]:
        new_curve.closed = True
    self.all_curves.append(new_curve)
    self.active_curve = new_curve
    cur_main.generate_bezier_points(self.active_curve, self.active_curve.display_bezier, curve_settings.curve_resolution)
    self.original_verts_data.append(cur_main.pass_line([bm.verts[i].co.copy() for i in loop[0]], loop[1]))
    for curve in self.all_curves:
        update_curve_line(active_obj, curve, self.loops, self.all_curves, bm, curve_settings.spread_mode, self.original_verts_data[self.all_curves.index(curve)])
    if curve_settings.surface_snap:
        meshes_array = ut_base.get_obj_dup_meshes(curve_settings.snap_objects, curve_settings.convert_instances, context)
        if meshes_array:
            self.picked_meshes = meshes_array","['for loop in self.loops:\n    (loop_0, loop_1, *_) = loop\n    loop_verts = [active_obj.matrix_world @ bm.verts[i].co for i in loop_0]\n    loop_line = cur_main.pass_line(loop_verts, loop_1)\n    new_curve = cur_main.create_curve_to_line(cur_stretch_settings.points_number, loop_line, self.all_curves, loop_1)\n    if loop_1 is True:\n        new_curve.closed = True\n    self.all_curves.append(new_curve)\n    self.active_curve = new_curve\n    cur_main.generate_bezier_points(self.active_curve, self.active_curve.display_bezier, curve_settings.curve_resolution)\n    self.original_verts_data.append(cur_main.pass_line([bm.verts[i].co.copy() for i in loop_0], loop_1))\n    for curve in self.all_curves:\n        update_curve_line(active_obj, curve, self.loops, self.all_curves, bm, curve_settings.spread_mode, self.original_verts_data[self.all_curves.index(curve)])\n    if curve_settings.surface_snap is True:\n        meshes_array = ut_base.get_obj_dup_meshes(curve_settings.snap_objects, curve_settings.convert_instances, context)\n        if meshes_array:\n            self.picked_meshes = meshes_array', 'for (loop_0, loop_1, *loop_len) in self.loops:\n    loop_verts = [active_obj.matrix_world @ bm.verts[i].co for i in \n    loop_0]\n    loop_line = cur_main.pass_line(loop_verts, \n    loop_1)\n    new_curve = cur_main.create_curve_to_line(cur_stretch_settings.points_number, loop_line, self.all_curves, \n    loop_1)\n    if \n    loop_1 is True:\n        new_curve.closed = True\n    self.all_curves.append(new_curve)\n    self.active_curve = new_curve\n    cur_main.generate_bezier_points(self.active_curve, self.active_curve.display_bezier, curve_settings.curve_resolution)\n    self.original_verts_data.append(cur_main.pass_line([bm.verts[i].co.copy() for i in \n    loop_0], \n    loop_1))\n    for curve in self.all_curves:\n        update_curve_line(active_obj, curve, self.loops, self.all_curves, bm, curve_settings.spread_mode, self.original_verts_data[self.all_curves.index(curve)])\n    if curve_settings.surface_snap is True:\n        meshes_array = ut_base.get_obj_dup_meshes(curve_settings.snap_objects, curve_settings.convert_instances, context)\n        if meshes_array:\n            self.picked_meshes = meshes_array']",-1
deluge,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deluge/deluge/log.py,https://github.com/deluge-torrent/deluge/tree/master/deluge/log.py,_BackwardsCompatibleLOG,__getattribute__$318,"def __getattribute__(self, name):
    import warnings
    logger_name = 'deluge'
    stack = inspect.stack()
    stack.pop(0)
    module_stack = stack.pop(0)
    caller_module = inspect.getmodule(module_stack[0])
    caller_module_name = getattr(caller_module, '__name__', '')
    warnings.warn_explicit(DEPRECATION_WARNING, DeprecationWarning, module_stack[1], module_stack[2], caller_module_name)
    if caller_module:
        for member in stack:
            module = inspect.getmodule(member[0])
            if not module:
                continue
            if module.__name__ in ('deluge.plugins.pluginbase', 'deluge.plugins.init'):
                logger_name += '.plugin.%s' % caller_module_name
                caller_module.log = logging.getLogger(logger_name)
                break
    else:
        logging.getLogger(logger_name).warning(""Unable to monkey-patch the calling module's `log` attribute! You should really update and rebuild your plugins..."")
    return getattr(logging.getLogger(logger_name), name)","for member in stack:
    module = inspect.getmodule(member[0])
    if not module:
        continue
    if module.__name__ in ('deluge.plugins.pluginbase', 'deluge.plugins.init'):
        logger_name += '.plugin.%s' % caller_module_name
        caller_module.log = logging.getLogger(logger_name)
        break","for member in stack:
    module = inspect.getmodule(member[0])
    if module and module.__name__ in ('deluge.plugins.pluginbase', 'deluge.plugins.init'):
        logger_name += '.plugin.%s' % caller_module_name
        caller_module.log = logging.getLogger(logger_name)
        break","[""for member in stack:\n    (member_0, *member_rmembermaining) = member\n    module = inspect.getmodule(member_0)\n    if not module:\n        continue\n    if module.__name__ in ('deluge.plugins.pluginbase', 'deluge.plugins.init'):\n        logger_name += '.plugin.%s' % caller_module_name\n        caller_module.log = logging.getLogger(logger_name)\n        break"", ""for (member_0, *member_len) in stack:\n    module = inspect.getmodule(member_0)\n    if not module:\n        continue\n    if module.__name__ in ('deluge.plugins.pluginbase', 'deluge.plugins.init'):\n        logger_name += '.plugin.%s' % caller_module_name\n        caller_module.log = logging.getLogger(logger_name)\n        break""]",-1
strictyaml,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/strictyaml/strictyaml/ruamel/comments.py,https://github.com/crdoconnor/strictyaml/tree/master/strictyaml/ruamel/comments.py,CommentedMap,__getitem__$757,"def __getitem__(self, key):
    try:
        return ordereddict.__getitem__(self, key)
    except KeyError:
        for merged in getattr(self, merge_attrib, []):
            if key in merged[1]:
                return merged[1][key]
        raise","for merged in getattr(self, merge_attrib, []):
    if key in merged[1]:
        return merged[1][key]","for merged in getattr(self, merge_attrib, []):
    if key in merged[1]:
        return merged[1].get(key)","['for (merged_0, merged_1, *merged_len) in getattr(self, merge_attrib, []):\n    if key in \n    merged_1:\n        return \n        merged_1[key]']",-1
nfstream,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nfstream/nfstream/utils.py,https://github.com/nfstream/nfstream/tree/master/nfstream/utils.py,,update_performances$83,"def update_performances(performances, is_linux, flows_count):
    """""" Update performance report and check platform for consistency """"""
    drops = 0
    processed = 0
    ignored = 0
    load = []
    for meter in performances:
        if is_linux:
            drops += meter[0].value
            ignored += meter[2].value
        else:
            drops = max(meter[0].value, drops)
            ignored = max(meter[2].value, ignored)
        processed += meter[1].value
        load.append(meter[1].value)
    print(json.dumps({'flows_expired': flows_count.value, 'packets_processed': processed, 'packets_ignored': ignored, 'packets_dropped_filtered_by_kernel': drops, 'meters_packets_processing_balance': load}))","for meter in performances:
    if is_linux:
        drops += meter[0].value
        ignored += meter[2].value
    else:
        drops = max(meter[0].value, drops)
        ignored = max(meter[2].value, ignored)
    processed += meter[1].value
    load.append(meter[1].value)","for meter in performances:
    drops += meter[0].value if is_linux else max(meter[0].value, drops)
    ignored += meter[2].value if is_linux else max(meter[2].value, ignored)
    processed += meter[1].value
    load.append(meter[1].value)","['for meter in performances:\n    (meter_0, meter_1, meter_2, *_) = meter\n    if is_linux:\n        drops += meter_0.value\n        ignored += meter_2.value\n    else:\n        drops = max(meter_0.value, drops)\n        ignored = max(meter_2.value, ignored)\n    processed += meter_1.value\n    load.append(meter_1.value)', 'for (meter_0, meter_1, meter_2, *meter_len) in performances:\n    if is_linux:\n        drops += \n        meter_0.value\n        ignored += \n        meter_2.value\n    else:\n        drops = max(\n        meter_0.value, drops)\n        ignored = max(\n        meter_2.value, ignored)\n    processed += \n    meter_1.value\n    load.append(\n    meter_1.value)']",-1
dionaea,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dionaea/modules/python/dionaea/sip/extras.py,https://github.com/DinoTools/dionaea/tree/master/modules/python/dionaea/sip/extras.py,PCAP,open$329,"def open(self, msg_stack, **params):
    path = self.path.format(**params)
    today = datetime.datetime.now()
    path = today.strftime(path)
    filename = today.strftime(self.filename)
    filename = filename.format(**params)
    try:
        if not os.path.exists(path):
            os.makedirs(path)
    except:
        logger.info(""Can't create RTP-Dump dir: %s"", path)
    try:
        self._fp = open(os.path.join(path, filename), 'wb')
    except:
        logger.warning(""Can't create RTP-Dump file: %s"", os.path.join(path, filename))
    if self._fp is None:
        return False
    self._fp.write(b'\xd4\xc3\xb2\xa1')
    self._fp.write(b'\x02\x00\x04\x00')
    self._fp.write(b'\x00\x00\x00\x00')
    self._fp.write(b'\x00\x00\x00\x00')
    self._fp.write(b'\xff\xff\x00\x00')
    self._fp.write(b'\x01\x00\x00\x00')
    for msg in msg_stack:
        t = msg[1].time
        ts = int(t)
        tm = int((t - ts) * 1000000)
        src_port = 5060
        dst_port = 5060
        if msg[0] == 'in':
            src_ether = b'\x00\x00\x00\x00\x00\x02'
            src_host = b'\n\x00\x00\x02'
            dst_ether = b'\x00\x00\x00\x00\x00\x01'
            dst_host = b'\n\x00\x00\x01'
        else:
            src_ether = b'\x00\x00\x00\x00\x00\x01'
            src_host = b'\n\x00\x00\x01'
            dst_ether = b'\x00\x00\x00\x00\x00\x02'
            dst_host = b'\n\x00\x00\x02'
        self.write(ts=ts, tm=tm, src_ether=src_ether, src_host=src_host, src_port=src_port, dst_ether=dst_ether, dst_host=dst_host, dst_port=dst_port, data=msg[1].dumps())","for msg in msg_stack:
    t = msg[1].time
    ts = int(t)
    tm = int((t - ts) * 1000000)
    src_port = 5060
    dst_port = 5060
    if msg[0] == 'in':
        src_ether = b'\x00\x00\x00\x00\x00\x02'
        src_host = b'\n\x00\x00\x02'
        dst_ether = b'\x00\x00\x00\x00\x00\x01'
        dst_host = b'\n\x00\x00\x01'
    else:
        src_ether = b'\x00\x00\x00\x00\x00\x01'
        src_host = b'\n\x00\x00\x01'
        dst_ether = b'\x00\x00\x00\x00\x00\x02'
        dst_host = b'\n\x00\x00\x02'
    self.write(ts=ts, tm=tm, src_ether=src_ether, src_host=src_host, src_port=src_port, dst_ether=dst_ether, dst_host=dst_host, dst_port=dst_port, data=msg[1].dumps())","for msg in msg_stack:
    t = msg[1].time
    ts = int(t)
    tm = int((t - ts) * 1000000)
    src_port = 5060
    dst_port = 5060
    (src_ether, src_host, dst_ether, dst_host) = (b'\x00\x00\x00\x00\x00\x02', b'\n\x00\x00\x02', b'\x00\x00\x00\x00\x00\x01', b'\n\x00\x00\x01') if msg[0] == 'in' else (b'\x00\x00\x00\x00\x00\x01', b'\n\x00\x00\x01', b'\x00\x00\x00\x00\x00\x02', b'\n\x00\x00\x02')
    self.write(ts=ts, tm=tm, src_ether=src_ether, src_host=src_host, src_port=src_port, dst_ether=dst_ether, dst_host=dst_host, dst_port=dst_port, data=msg[1].dumps())","[""for msg in msg_stack:\n    (msg_0, msg_1, *_) = msg\n    t = msg_1.time\n    ts = int(t)\n    tm = int((t - ts) * 1000000)\n    src_port = 5060\n    dst_port = 5060\n    if msg_0 == 'in':\n        src_ether = b'\\x00\\x00\\x00\\x00\\x00\\x02'\n        src_host = b'\\n\\x00\\x00\\x02'\n        dst_ether = b'\\x00\\x00\\x00\\x00\\x00\\x01'\n        dst_host = b'\\n\\x00\\x00\\x01'\n    else:\n        src_ether = b'\\x00\\x00\\x00\\x00\\x00\\x01'\n        src_host = b'\\n\\x00\\x00\\x01'\n        dst_ether = b'\\x00\\x00\\x00\\x00\\x00\\x02'\n        dst_host = b'\\n\\x00\\x00\\x02'\n    self.write(ts=ts, tm=tm, src_ether=src_ether, src_host=src_host, src_port=src_port, dst_ether=dst_ether, dst_host=dst_host, dst_port=dst_port, data=msg_1.dumps())"", ""for (msg_0, msg_1, *msg_len) in msg_stack:\n    t = \n    msg_1.time\n    ts = int(t)\n    tm = int((t - ts) * 1000000)\n    src_port = 5060\n    dst_port = 5060\n    if \n    msg_0 == 'in':\n        src_ether = b'\\x00\\x00\\x00\\x00\\x00\\x02'\n        src_host = b'\\n\\x00\\x00\\x02'\n        dst_ether = b'\\x00\\x00\\x00\\x00\\x00\\x01'\n        dst_host = b'\\n\\x00\\x00\\x01'\n    else:\n        src_ether = b'\\x00\\x00\\x00\\x00\\x00\\x01'\n        src_host = b'\\n\\x00\\x00\\x01'\n        dst_ether = b'\\x00\\x00\\x00\\x00\\x00\\x02'\n        dst_host = b'\\n\\x00\\x00\\x02'\n    self.write(ts=ts, tm=tm, src_ether=src_ether, src_host=src_host, src_port=src_port, dst_ether=dst_ether, dst_host=dst_host, dst_port=dst_port, data=\n    msg_1.dumps())""]",-1
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/tool_shed/grids/util.py,https://github.com/ansible/galaxy/tree/master/lib/tool_shed/grids/util.py,,build_approved_select_field$9,"def build_approved_select_field(trans, name, selected_value=None, for_component=True):
    options = [('No', trans.model.ComponentReview.approved_states.NO), ('Yes', trans.model.ComponentReview.approved_states.YES)]
    if for_component:
        options.append(('Not applicable', trans.model.ComponentReview.approved_states.NA))
        if selected_value is None:
            selected_value = trans.model.ComponentReview.approved_states.NA
    select_field = SelectField(name=name)
    for option_tup in options:
        selected = selected_value and option_tup[1] == selected_value
        select_field.add_option(option_tup[0], option_tup[1], selected=selected)
    return select_field","for option_tup in options:
    selected = selected_value and option_tup[1] == selected_value
    select_field.add_option(option_tup[0], option_tup[1], selected=selected)","for option_tup in options:
    selected = selected_value == option_tup[1]
    select_field.add_option(option_tup[0], option_tup[1], selected=selected)","['for option_tup in options:\n    (option_tup_0, option_tup_1, *_) = option_tup\n    selected = selected_value and option_tup_1 == selected_value\n    select_field.add_option(option_tup_0, option_tup_1, selected=selected)', 'for (option_tup_0, option_tup_1, *option_tup_len) in options:\n    selected = selected_value and \n    option_tup_1 == selected_value\n    select_field.add_option(\n    option_tup_0, \n    option_tup_1, selected=selected)']",-1
GeneticAlgorithmsWithPython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GeneticAlgorithmsWithPython/es/ch16/pruebas.py,https://github.com/handcraftsman/GeneticAlgorithmsWithPython/tree/master/es/ch16/pruebas.py,,obtener_aptitud$26,"def obtener_aptitud(genes, reglas, entradas):
    circuito = nodos_a_circuito(genes)[0]
    etiquetasDeFuente = 'ABCD'
    reglasExitosas = 0
    for regla in reglas:
        entradas.clear()
        entradas.update(zip(etiquetasDeFuente, regla[0]))
        if circuito.obtener_salida() == regla[1]:
            reglasExitosas += 1
    return reglasExitosas","for regla in reglas:
    entradas.clear()
    entradas.update(zip(etiquetasDeFuente, regla[0]))
    if circuito.obtener_salida() == regla[1]:
        reglasExitosas += 1","for regla in reglas:
    entradas_temp = dict(zip(etiquetasDeFuente, regla[0]))
    if circuito.obtener_salida(entradas_temp) == regla[1]:
        reglasExitosas += 1","['for regla in reglas:\n    (regla_0, regla_1, *_) = regla\n    entradas.clear()\n    entradas.update(zip(etiquetasDeFuente, regla_0))\n    if circuito.obtener_salida() == regla_1:\n        reglasExitosas += 1', 'for (regla_0, regla_1, *regla_len) in reglas:\n    entradas.clear()\n    entradas.update(zip(etiquetasDeFuente, \n    regla_0))\n    if circuito.obtener_salida() == \n    regla_1:\n        reglasExitosas += 1']",-1
spiderfoot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spiderfoot/sfwebui.py,https://github.com/smicallef/spiderfoot/tree/master//sfwebui.py,SpiderFootWebUi,scanexportlogs$328,"def scanexportlogs(self: 'SpiderFootWebUi', id: str, dialect: str='excel') -> bytes:
    """"""Get scan log

        Args:
            id (str): scan ID
            dialect (str): CSV dialect (default: excel)

        Returns:
            bytes: scan logs in CSV format
        """"""
    dbh = SpiderFootDb(self.config)
    try:
        data = dbh.scanLogs(id, None, None, True)
    except Exception:
        return self.error('Scan ID not found.')
    if not data:
        return self.error('Scan ID not found.')
    fileobj = StringIO()
    parser = csv.writer(fileobj, dialect=dialect)
    parser.writerow(['Date', 'Component', 'Type', 'Event', 'Event ID'])
    for row in data:
        parser.writerow([time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(row[0] / 1000)), str(row[1]), str(row[2]), str(row[3]), row[4]])
    cherrypy.response.headers['Content-Disposition'] = f'attachment; filename=SpiderFoot-{id}.log.csv'
    cherrypy.response.headers['Content-Type'] = 'application/csv'
    cherrypy.response.headers['Pragma'] = 'no-cache'
    return fileobj.getvalue().encode('utf-8')","for row in data:
    parser.writerow([time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(row[0] / 1000)), str(row[1]), str(row[2]), str(row[3]), row[4]])","for row in data:
    row_0 = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(row[0] / 1000))
    (row_1, row_2, row_3, row_4) = map(str, row[1:])
    parser.writerow([row_0, row_1, row_2, row_3, row_4])","[""for row in data:\n    (row_0, row_1, row_2, row_3, row_4, *_) = row\n    parser.writerow([time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(row_0 / 1000)), str(row_1), str(row_2), str(row_3), row_4])"", ""for (row_0, row_1, row_2, row_3, row_4, *row_len) in data:\n    parser.writerow([time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(row_0 / 1000)), str(row_1), str(row_2), str(row_3), row_4])""]",1
openstates-scrapers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openstates-scrapers/scrapers/md/bills.py,https://github.com/openstates/openstates-scrapers/tree/master/scrapers/md/bills.py,MDBillScraper,scrape_bill_subjects$341,"def scrape_bill_subjects(self, bill, page):
    for row in page.xpath('//a[contains(@href, ""/Legislation/SubjectIndex/annotac"")]/text()'):
        bill.add_subject(row[0])","for row in page.xpath('//a[contains(@href, ""/Legislation/SubjectIndex/annotac"")]/text()'):
    bill.add_subject(row[0])","for row in page.xpath('//a[contains(@href, ""/Legislation/SubjectIndex/annotac"")]/text()'):
    bill.add_subject(row)","['for row in page.xpath(\'//a[contains(@href, ""/Legislation/SubjectIndex/annotac"")]/text()\'):\n    (row_0, *row_rrowmaining) = row\n    bill.add_subject(row_0)', 'for (row_0, *row_len) in page.xpath(\'//a[contains(@href, ""/Legislation/SubjectIndex/annotac"")]/text()\'):\n    bill.add_subject(row_0)']",-1
openstates-scrapers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openstates-scrapers/scrapers/va/csv_bills.py,https://github.com/openstates/openstates-scrapers/tree/master/scrapers/va/csv_bills.py,VaCSVBillScraper,load_amendments$88,"def load_amendments(self):
    resp = self.get(self._url_base + 'Amendments.csv').text
    reader = csv.reader(resp.splitlines(), delimiter=',')
    for row in reader:
        self._amendments[row[0].strip()].append({'bill_number': row[0].strip(), 'txt_docid': row[1].strip()})
    self.warning('Total Amendments Loaded: ' + str(len(self._amendments)))","for row in reader:
    self._amendments[row[0].strip()].append({'bill_number': row[0].strip(), 'txt_docid': row[1].strip()})","for row in reader:
    (bill_number, txt_docid) = map(str.strip, row)
    self._amendments[bill_number].append({'bill_number': bill_number, 'txt_docid': txt_docid})","[""for row in reader:\n    (row_0, row_1, *_) = row\n    self._amendments[row_0.strip()].append({'bill_number': row_0.strip(), 'txt_docid': row_1.strip()})"", ""for (row_0, row_1, *row_len) in reader:\n    self._amendments[row_0.strip()].append({'bill_number': row_0.strip(), 'txt_docid': row_1.strip()})""]",-1
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/objects/tvshows.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/objects/tvshows.py,TVShows,remove$519,"def remove(self, item_id, e_item):
    """""" Remove showid, fileid, pathid, jellyfin reference.
            There's no episodes left, delete show and any possible remaining seasons
        """"""
    obj = {'Id': item_id}
    try:
        obj['KodiId'] = e_item[0]
        obj['FileId'] = e_item[1]
        obj['ParentId'] = e_item[3]
        obj['Media'] = e_item[4]
    except TypeError:
        return
    if obj['Media'] == 'episode':
        temp_obj = dict(obj)
        self.remove_episode(obj['KodiId'], obj['FileId'], obj['Id'])
        season = self.jellyfin_db.get_full_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_season_obj))
        try:
            temp_obj['Id'] = season[0]
            temp_obj['ParentId'] = season[1]
        except TypeError:
            return
        if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
            self.remove_season(obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))
        temp_obj['Id'] = self.jellyfin_db.get_item_by_kodi_id(*values(temp_obj, QUEM.get_item_by_parent_tvshow_obj))
        if not self.get_total_episodes(*values(temp_obj, QU.get_total_episodes_obj)):
            for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
                self.remove_season(season[1], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))
            self.remove_tvshow(temp_obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))
    elif obj['Media'] == 'tvshow':
        obj['ParentId'] = obj['KodiId']
        for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):
            temp_obj = dict(obj)
            temp_obj['ParentId'] = season[1]
            for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
                self.remove_episode(episode[1], episode[2], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
        else:
            self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))
        self.remove_tvshow(obj['KodiId'], obj['Id'])
    elif obj['Media'] == 'season':
        for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
            self.remove_episode(episode[1], episode[2], obj['Id'])
        else:
            self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))
        self.remove_season(obj['KodiId'], obj['Id'])
        if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj)):
            self.remove_tvshow(obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_tvshow_obj))
    for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
        self.remove_episode(episode[2], episode[3], obj['Id'])
    else:
        self.jellyfin_db.remove_media_by_parent_id(obj['Id'])
    self.jellyfin_db.remove_item(*values(obj, QUEM.delete_item_obj))","for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):
    temp_obj = dict(obj)
    temp_obj['ParentId'] = season[1]
    for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
        self.remove_episode(episode[1], episode[2], obj['Id'])
    else:
        self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
else:
    self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))","for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):
    temp_obj = dict(obj)
    temp_obj['ParentId'] = season[1]
    for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
        self.remove_episode(episode[1], episode[2], obj['Id'])
    else:
        self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))","[""for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):\n    (season_0, season_1, *season_rseasonmaining) = season\n    temp_obj = dict(obj)\n    temp_obj['ParentId'] = season_1\n    for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):\n        self.remove_episode(episode[1], episode[2], obj['Id'])\n    else:\n        self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))\nelse:\n    self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))"", ""for (season_0, season_1, *season_len) in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):\n    temp_obj = dict(obj)\n    temp_obj['ParentId'] = \n    season_1\n    for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):\n        self.remove_episode(episode[1], episode[2], obj['Id'])\n    else:\n        self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))\nelse:\n    self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))""]",-1
capirca,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capirca/capirca/lib/policy.py,https://github.com/google/capirca/tree/master/capirca/lib/policy.py,Term,CheckPortIsContained$1379,"def CheckPortIsContained(self, superset, subset):
    """"""Check if the given list of ports is wholly contained.

    Args:
      superset: list of port tuples
      subset: list of port tuples

    Returns:
      bool: True if subset is contained in superset, false otherwise
    """"""
    if not superset:
        return True
    if not subset:
        return False
    for sub_port in subset:
        not_contains = True
        for sup_port in superset:
            if int(sub_port[0]) >= int(sup_port[0]) and int(sub_port[1]) <= int(sup_port[1]):
                not_contains = False
                break
        if not_contains:
            return False
    return True","for sub_port in subset:
    not_contains = True
    for sup_port in superset:
        if int(sub_port[0]) >= int(sup_port[0]) and int(sub_port[1]) <= int(sup_port[1]):
            not_contains = False
            break
    if not_contains:
        return False","for sub_port in subset:
    if all((int(sub_port[0]) >= int(sup_port[0]) and int(sub_port[1]) <= int(sup_port[1]) for sup_port in superset)):
        return False
return True","['for sub_port in subset:\n    (sub_port_0, sub_port_1, *_) = sub_port\n    not_contains = True\n    for sup_port in superset:\n        if int(sub_port_0) >= int(sup_port[0]) and int(sub_port_1) <= int(sup_port[1]):\n            not_contains = False\n            break\n    if not_contains:\n        return False', 'for (sub_port_0, sub_port_1, *sub_port_len) in subset:\n    not_contains = True\n    for sup_port in superset:\n        if int(sub_port_0) >= int(sup_port[0]) and int(sub_port_1) <= int(sup_port[1]):\n            not_contains = False\n            break\n    if not_contains:\n        return False']",-1
electricitymap-contrib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electricitymap-contrib/parsers/CL.py,https://github.com/tmrowco/electricitymap-contrib/tree/master/parsers/CL.py,,production_processor_live$45,"def production_processor_live(json_tot, json_ren):
    """"""
    Extracts generation data and timestamp into dictionary.
    Returns a list of dictionaries for all of the available ""live"" data, usually that day.
    """"""
    gen_total = json_tot['data'][0]['values']
    if json_ren['data'][1]['key'] == 'ENERGA SOLAR':
        rawgen_sol = json_ren['data'][1]['values']
    else:
        raise RuntimeError(f""Unexpected data label. Expected 'ENERGA SOLAR' and got {json_ren['data'][1]['key']}"")
    if json_ren['data'][0]['key'] == 'ENERGA ELICA':
        rawgen_wind = json_ren['data'][0]['values']
    else:
        raise RuntimeError(f""Unexpected data label. Expected 'ENERGA ELICA' and got {json_ren['data'][0]['key']}"")
    mapped_totals = []
    for total in gen_total:
        datapoint = {}
        dt = total[0]
        for pair in rawgen_sol:
            if pair[0] == dt:
                solar = pair[1]
                break
        for pair in rawgen_wind:
            if pair[0] == dt:
                wind = pair[1]
                break
        datapoint['datetime'] = arrow.get(dt / 1000, tzinfo='Chile/Continental').datetime
        datapoint['unknown'] = total[1] - wind - solar
        datapoint['wind'] = wind
        datapoint['solar'] = solar
        mapped_totals.append(datapoint)
    return mapped_totals","for total in gen_total:
    datapoint = {}
    dt = total[0]
    for pair in rawgen_sol:
        if pair[0] == dt:
            solar = pair[1]
            break
    for pair in rawgen_wind:
        if pair[0] == dt:
            wind = pair[1]
            break
    datapoint['datetime'] = arrow.get(dt / 1000, tzinfo='Chile/Continental').datetime
    datapoint['unknown'] = total[1] - wind - solar
    datapoint['wind'] = wind
    datapoint['solar'] = solar
    mapped_totals.append(datapoint)","for total in gen_total:
    datapoint = {}
    dt = total[0]
    solar = next((pair[1] for pair in rawgen_sol if pair[0] == dt))
    wind = next((pair[1] for pair in rawgen_wind if pair[0] == dt))
    datapoint['datetime'] = arrow.get(dt / 1000, tzinfo='Chile/Continental').datetime
    datapoint['unknown'] = total[1] - wind - solar
    datapoint['wind'] = wind
    datapoint['solar'] = solar
    mapped_totals.append(datapoint)","[""for total in gen_total:\n    (total_0, total_1, *_) = total\n    datapoint = {}\n    dt = total_0\n    for pair in rawgen_sol:\n        if pair[0] == dt:\n            solar = pair[1]\n            break\n    for pair in rawgen_wind:\n        if pair[0] == dt:\n            wind = pair[1]\n            break\n    datapoint['datetime'] = arrow.get(dt / 1000, tzinfo='Chile/Continental').datetime\n    datapoint['unknown'] = total_1 - wind - solar\n    datapoint['wind'] = wind\n    datapoint['solar'] = solar\n    mapped_totals.append(datapoint)"", ""for (total_0, total_1, *total_len) in gen_total:\n    datapoint = {}\n    dt = \n    total_0\n    for pair in rawgen_sol:\n        if pair[0] == dt:\n            solar = pair[1]\n            break\n    for pair in rawgen_wind:\n        if pair[0] == dt:\n            wind = pair[1]\n            break\n    datapoint['datetime'] = arrow.get(dt / 1000, tzinfo='Chile/Continental').datetime\n    datapoint['unknown'] = \n    total_1 - wind - solar\n    datapoint['wind'] = wind\n    datapoint['solar'] = solar\n    mapped_totals.append(datapoint)""]",-1
BlenderProc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BlenderProc/docs/source/ext/moduleoverview.py,https://github.com/DLR-RM/BlenderProc/tree/master/docs/source/ext/moduleoverview.py,,generate_tutorials_sidebar$63,"def generate_tutorials_sidebar(app, fromdocname, container):
    tutorials_dir = Path(__file__).absolute().parent.parent / 'docs' / 'tutorials'
    tutorials = [('Loading and manipulating objects', 'loader'), ('Configuring the camera', 'camera'), ('Rendering the scene', 'renderer'), ('Writing the results to file', 'writer'), ('How key frames work', 'key_frames'), ('Positioning objects via the physics simulator', 'physics')]
    container += nodes.caption('Tutorials', '', *[nodes.Text('Tutorials')])
    for tutorial in tutorials:
        toc = nodes.bullet_list()
        ref = nodes.reference('', '')
        ref['refuri'] = app.builder.get_relative_uri(fromdocname, 'docs/tutorials/' + tutorial[1])
        ref.append(nodes.Text(tutorial[0]))
        module_item = nodes.list_item('', addnodes.compact_paragraph('', '', ref), classes=['toctree-l1'])
        if fromdocname.startswith('docs/tutorials/' + tutorial[1]):
            module_item['classes'].append('current')
        toc += module_item
        container += toc","for tutorial in tutorials:
    toc = nodes.bullet_list()
    ref = nodes.reference('', '')
    ref['refuri'] = app.builder.get_relative_uri(fromdocname, 'docs/tutorials/' + tutorial[1])
    ref.append(nodes.Text(tutorial[0]))
    module_item = nodes.list_item('', addnodes.compact_paragraph('', '', ref), classes=['toctree-l1'])
    if fromdocname.startswith('docs/tutorials/' + tutorial[1]):
        module_item['classes'].append('current')
    toc += module_item
    container += toc","for tutorial in tutorials:
    toc = nodes.bullet_list()
    ref = nodes.reference('', '')
    ref['refuri'] = app.builder.get_relative_uri(fromdocname, f'docs/tutorials/{tutorial[1]}')
    ref.append(nodes.Text(tutorial[0]))
    module_item = nodes.list_item('', addnodes.compact_paragraph('', '', ref), classes=['toctree-l1'])
    if fromdocname.startswith(f'docs/tutorials/{tutorial[1]}'):
        module_item['classes'].append('current')
    toc += module_item
    container += toc","[""for tutorial in tutorials:\n    (tutorial_0, tutorial_1, *_) = tutorial\n    toc = nodes.bullet_list()\n    ref = nodes.reference('', '')\n    ref['refuri'] = app.builder.get_relative_uri(fromdocname, 'docs/tutorials/' + tutorial_1)\n    ref.append(nodes.Text(tutorial_0))\n    module_item = nodes.list_item('', addnodes.compact_paragraph('', '', ref), classes=['toctree-l1'])\n    if fromdocname.startswith('docs/tutorials/' + tutorial_1):\n        module_item['classes'].append('current')\n    toc += module_item\n    container += toc"", ""for (tutorial_0, tutorial_1, *tutorial_len) in tutorials:\n    toc = nodes.bullet_list()\n    ref = nodes.reference('', '')\n    ref['refuri'] = app.builder.get_relative_uri(fromdocname, 'docs/tutorials/' + tutorial_1)\n    ref.append(nodes.Text(tutorial_0))\n    module_item = nodes.list_item('', addnodes.compact_paragraph('', '', ref), classes=['toctree-l1'])\n    if fromdocname.startswith('docs/tutorials/' + tutorial_1):\n        module_item['classes'].append('current')\n    toc += module_item\n    container += toc""]",-1
pyOCD,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyOCD/pyocd/probe/stlink/detect/windows.py,https://github.com/pyocd/pyOCD/tree/master/pyocd/probe/stlink/detect/windows.py,,_get_cached_mounted_points$70,"def _get_cached_mounted_points():
    """"""! Get the volumes present on the system
    @return List of mount points and their associated target id
      Ex. [{ 'mount_point': 'D:', 'target_id_usb_id': 'xxxx'}, ...]
    """"""
    result = []
    try:
        mounted_devices_key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, 'SYSTEM\\MountedDevices')
        for v in _iter_vals(mounted_devices_key):
            if 'DosDevices' not in v[0]:
                continue
            volume_string = v[1].decode('utf-16le', 'ignore')
            if not _is_mbed_volume(volume_string):
                continue
            mount_point_match = re.match('.*\\\\(.:)$', v[0])
            if not mount_point_match:
                LOG.debug('Invalid disk pattern for entry %s, skipping', v[0])
                continue
            mount_point = mount_point_match.group(1)
            result.append({'mount_point': mount_point, 'volume_string': volume_string})
    except OSError:
        LOG.error('Failed to open ""MountedDevices"" in registry')
    return result","for v in _iter_vals(mounted_devices_key):
    if 'DosDevices' not in v[0]:
        continue
    volume_string = v[1].decode('utf-16le', 'ignore')
    if not _is_mbed_volume(volume_string):
        continue
    mount_point_match = re.match('.*\\\\(.:)$', v[0])
    if not mount_point_match:
        LOG.debug('Invalid disk pattern for entry %s, skipping', v[0])
        continue
    mount_point = mount_point_match.group(1)
    result.append({'mount_point': mount_point, 'volume_string': volume_string})","for v in _iter_vals(mounted_devices_key):
    if 'DosDevices' not in v[0]:
        continue
    volume_string = v[1].decode('utf-16le', 'ignore')
    if not _is_mbed_volume(volume_string):
        continue
    mount_point_match = re.match('.*\\\\(.:)$', v[0])
    if not mount_point_match:
        LOG.debug('Invalid disk pattern for entry %s, skipping', v[0])
        continue
    mount_point = mount_point_match.group(1)
    result.append({'mount_point': mount_point, 'volume_string': volume_string})","[""for v in _iter_vals(mounted_devices_key):\n    (v_0, v_1, *_) = v\n    if 'DosDevices' not in v_0:\n        continue\n    volume_string = v_1.decode('utf-16le', 'ignore')\n    if not _is_mbed_volume(volume_string):\n        continue\n    mount_point_match = re.match('.*\\\\\\\\(.:)$', v_0)\n    if not mount_point_match:\n        LOG.debug('Invalid disk pattern for entry %s, skipping', v_0)\n        continue\n    mount_point = mount_point_match.group(1)\n    result.append({'mount_point': mount_point, 'volume_string': volume_string})"", ""for (v_0, v_1, *v_len) in _iter_vals(mounted_devices_key):\n    if 'DosDevices' not in \n    v_0:\n        continue\n    volume_string = \n    v_1.decode('utf-16le', 'ignore')\n    if not _is_mbed_volume(volume_string):\n        continue\n    mount_point_match = re.match('.*\\\\\\\\(.:)$', \n    v_0)\n    if not mount_point_match:\n        LOG.debug('Invalid disk pattern for entry %s, skipping', \n        v_0)\n        continue\n    mount_point = mount_point_match.group(1)\n    result.append({'mount_point': mount_point, 'volume_string': volume_string})""]",-1
ros,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros/tools/rosunit/src/rosunit/baretest.py,https://github.com/ros/ros/tree/master/tools/rosunit/src/rosunit/baretest.py,,_format_errors$528,"def _format_errors(errors):
    formatted = []
    for e in errors:
        if '_testMethodName' in e[0].__dict__:
            formatted.append(e[0]._testMethodName)
        elif 'description' in e[0].__dict__:
            formatted.append('%s: %s\n' % (str(e[0].description), str(e[1])))
        else:
            formatted.append(str(e[0].__dict__))
    return formatted","for e in errors:
    if '_testMethodName' in e[0].__dict__:
        formatted.append(e[0]._testMethodName)
    elif 'description' in e[0].__dict__:
        formatted.append('%s: %s\n' % (str(e[0].description), str(e[1])))
    else:
        formatted.append(str(e[0].__dict__))","formatted = []
for (e_0, e_1) in errors:
    if '_testMethodName' in e_0.__dict__:
        formatted.append(e_0._testMethodName)
    elif 'description' in e_0.__dict__:
        formatted.append('%s: %s\n' % (str(e_0.description), str(e_1)))
    else:
        formatted.append(str(e_0.__dict__))","[""for e in errors:\n    (e_0, e_1, *_) = e\n    if '_testMethodName' in e_0.__dict__:\n        formatted.append(e_0._testMethodName)\n    elif 'description' in e_0.__dict__:\n        formatted.append('%s: %s\\n' % (str(e_0.description), str(e_1)))\n    else:\n        formatted.append(str(e_0.__dict__))"", ""for (e_0, e_1, *e_len) in errors:\n    if '_testMethodName' in \n    e_0.__dict__:\n        formatted.append(\n        e_0._testMethodName)\n    elif 'description' in \n    e_0.__dict__:\n        formatted.append('%s: %s\\n' % (str(\n        e_0.description), str(\n        e_1)))\n    else:\n        formatted.append(str(\n        e_0.__dict__))""]",-1
pororo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pororo/pororo/models/tts/utils/display.py,https://github.com/kakaobrain/pororo/tree/master/pororo/models/tts/utils/display.py,,simple_table$17,"def simple_table(item_tuples):
    border_pattern = '+---------------------------------------'
    whitespace = '                                            '
    (headings, cells) = ([], [])
    for item in item_tuples:
        (heading, cell) = (str(item[0]), str(item[1]))
        pad_head = True if len(heading) < len(cell) else False
        pad = abs(len(heading) - len(cell))
        pad = whitespace[:pad]
        pad_left = pad[:len(pad) // 2]
        pad_right = pad[len(pad) // 2:]
        if pad_head:
            heading = pad_left + heading + pad_right
        else:
            cell = pad_left + cell + pad_right
        headings += [heading]
        cells += [cell]
    (border, head, body) = ('', '', '')
    for i in range(len(item_tuples)):
        temp_head = f'| {headings[i]} '
        temp_body = f'| {cells[i]} '
        border += border_pattern[:len(temp_head)]
        head += temp_head
        body += temp_body
        if i == len(item_tuples) - 1:
            head += '|'
            body += '|'
            border += '+'
    print(border)
    print(head)
    print(border)
    print(body)
    print(border)
    print(' ')","for item in item_tuples:
    (heading, cell) = (str(item[0]), str(item[1]))
    pad_head = True if len(heading) < len(cell) else False
    pad = abs(len(heading) - len(cell))
    pad = whitespace[:pad]
    pad_left = pad[:len(pad) // 2]
    pad_right = pad[len(pad) // 2:]
    if pad_head:
        heading = pad_left + heading + pad_right
    else:
        cell = pad_left + cell + pad_right
    headings += [heading]
    cells += [cell]","headings = []
cells = []
for (heading, cell) in item_tuples:
    pad_head = len(heading) < len(cell)
    pad = abs(len(heading) - len(cell))
    pad = whitespace[:pad]
    pad_left = pad[:len(pad) // 2]
    pad_right = pad[len(pad) // 2:]
    if pad_head:
        heading = pad_left + heading + pad_right
    else:
        cell = pad_left + cell + pad_right
    headings.append(heading)
    cells.append(cell)","['for item in item_tuples:\n    (item_0, item_1, *_) = item\n    (heading, cell) = (str(item_0), str(item_1))\n    pad_head = True if len(heading) < len(cell) else False\n    pad = abs(len(heading) - len(cell))\n    pad = whitespace[:pad]\n    pad_left = pad[:len(pad) // 2]\n    pad_right = pad[len(pad) // 2:]\n    if pad_head:\n        heading = pad_left + heading + pad_right\n    else:\n        cell = pad_left + cell + pad_right\n    headings += [heading]\n    cells += [cell]', 'for (item_0, item_1, *item_len) in item_tuples:\n    (heading, cell) = (str(item_0), str(item_1))\n    pad_head = True if len(heading) < len(cell) else False\n    pad = abs(len(heading) - len(cell))\n    pad = whitespace[:pad]\n    pad_left = pad[:len(pad) // 2]\n    pad_right = pad[len(pad) // 2:]\n    if pad_head:\n        heading = pad_left + heading + pad_right\n    else:\n        cell = pad_left + cell + pad_right\n    headings += [heading]\n    cells += [cell]']",-1
pytext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytext/pytext/torchscript/seq2seq/export_model.py,https://github.com/facebookresearch/pytext/tree/master/pytext/torchscript/seq2seq/export_model.py,Seq2SeqJIT,forward$130,"def forward(self, src_tokens: List[str], dict_feat: Optional[Tuple[List[str], List[float], List[int]]]=None, contextual_token_embedding: Optional[List[float]]=None) -> List[Tuple[List[str], float, List[float]]]:
    word_ids = self.source_vocab.lookup_indices_1d(src_tokens)
    single_unk_token: Optional[str] = get_single_unk_token(src_tokens, word_ids, self.copy_unk_token, self.unk_idx)
    (words, dict_tensors, contextual_embedding_tensor, src_lengths) = self.prepare_generator_inputs(word_ids, dict_feat, contextual_token_embedding)
    hypos_etc = self.sequence_generator(words, dict_tensors, contextual_embedding_tensor, src_lengths)
    hypos_list: List[Tuple[List[str], float, List[float]]] = []
    filter_token_list: List[int] = []
    if self.filter_eos_bos:
        filter_token_list = [self.target_vocab.bos_idx, self.target_vocab.eos_idx]
    for seq in hypos_etc:
        hyopthesis = seq[0]
        stringified = self.target_vocab.lookup_words_1d(hyopthesis, filter_token_list=filter_token_list, possible_unk_token=single_unk_token)
        hypos_list.append((stringified, seq[1], seq[2]))
    return hypos_list","for seq in hypos_etc:
    hyopthesis = seq[0]
    stringified = self.target_vocab.lookup_words_1d(hyopthesis, filter_token_list=filter_token_list, possible_unk_token=single_unk_token)
    hypos_list.append((stringified, seq[1], seq[2]))","hypos_list = [(self.target_vocab.lookup_words_1d(seq[0], filter_token_list=filter_token_list, possible_unk_token=single_unk_token), seq[1], seq[2]) for seq in hypos_etc]","['for seq in hypos_etc:\n    (seq_0, seq_1, seq_2, *_) = seq\n    hyopthesis = seq_0\n    stringified = self.target_vocab.lookup_words_1d(hyopthesis, filter_token_list=filter_token_list, possible_unk_token=single_unk_token)\n    hypos_list.append((stringified, seq_1, seq_2))', 'for (seq_0, seq_1, seq_2, *seq_len) in hypos_etc:\n    hyopthesis = \n    seq_0\n    stringified = self.target_vocab.lookup_words_1d(hyopthesis, filter_token_list=filter_token_list, possible_unk_token=single_unk_token)\n    hypos_list.append((stringified, \n    seq_1, \n    seq_2))']",-1
xbmc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xbmc/plugin.video.amazon-test/resources/lib/amazontld.py,https://github.com/Sandmann79/xbmc/tree/master/plugin.video.amazon-test/resources/lib/amazontld.py,AmazonTLD,getcache$1065,"def getcache(uid):
    j = {}
    c = self._menuDb.cursor()
    for data in c.execute('select data from channels where uid = (?)', (uid,)).fetchall():
        j = json.loads(data[0])
    c.close()
    return j","for data in c.execute('select data from channels where uid = (?)', (uid,)).fetchall():
    j = json.loads(data[0])","j = {}
for (data,) in c.execute('select data from channels where uid = (?)', (uid,)):
    j = json.loads(data)","[""for data in c.execute('select data from channels where uid = (?)', (uid,)).fetchall():\n    (data_0, *data_rdatamaining) = data\n    j = json.loads(data_0)"", ""for (data_0, *data_len) in c.execute('select data from channels where uid = (?)', (uid,)).fetchall():\n    j = json.loads(data_0)""]",-1
Archery,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Archery/sql/engines/oracle.py,https://github.com/hhyo/Archery/tree/master/sql/engines/oracle.py,OracleEngine,get_kill_command$1429,"def get_kill_command(self, thread_ids):
    """"""sid+serial#kill""""""
    if [k for k in [[j for j in i if not isinstance(j, int)] for i in thread_ids] if k]:
        return None
    sql = ""select 'alter system kill session ' || '''' || s.sid || ',' || s.serial# || '''' || ' immediate' || ';'\n                 from v$process p, v$session s, v$sqlarea q\n                 where p.addr = s.paddr\n                 and s.sql_hash_value = q.hash_value\n                 and s.sid || ',' || s.serial# in ({});"".format(','.join((f""'{str(tid[0])},{str(tid[1])}'"" for tid in thread_ids)))
    all_kill_sql = self.query(sql=sql)
    kill_sql = ''
    for row in all_kill_sql.rows:
        kill_sql = kill_sql + row[0]
    return kill_sql","for row in all_kill_sql.rows:
    kill_sql = kill_sql + row[0]",kill_sql = ''.join((row[0] for row in all_kill_sql.rows)),"['for row in all_kill_sql.rows:\n    (row_0, *row_rrowmaining) = row\n    kill_sql = kill_sql + row_0', 'for (row_0, *row_len) in all_kill_sql.rows:\n    kill_sql = kill_sql + \n    row_0']",-1
nlpcda,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlpcda/nlpcda/tools/Ner.py,https://github.com/425776024/nlpcda/tree/master/nlpcda/tools/Ner.py,Ner,__data_augment_one$78,"def __data_augment_one(self, org_data):
    new_data = []
    for di in org_data:
        (t_tag, t_ner_sentence) = (di[0], di[1])
        if t_tag in self.data_augument_tag_list and t_tag in self.tag_map:
            rdm_select_ner = self.__get_random_ner(t_tag)
            new_data.append([t_tag, rdm_select_ner])
        else:
            new_data.append([t_tag, t_ner_sentence])
    return new_data","for di in org_data:
    (t_tag, t_ner_sentence) = (di[0], di[1])
    if t_tag in self.data_augument_tag_list and t_tag in self.tag_map:
        rdm_select_ner = self.__get_random_ner(t_tag)
        new_data.append([t_tag, rdm_select_ner])
    else:
        new_data.append([t_tag, t_ner_sentence])","new_data = [[t_tag, self.__get_random_ner(t_tag) if t_tag in self.data_augument_tag_list and t_tag in self.tag_map else t_ner_sentence] for (t_tag, t_ner_sentence) in org_data]","['for di in org_data:\n    (di_0, di_1, *_) = di\n    (t_tag, t_ner_sentence) = (di_0, di_1)\n    if t_tag in self.data_augument_tag_list and t_tag in self.tag_map:\n        rdm_select_ner = self.__get_random_ner(t_tag)\n        new_data.append([t_tag, rdm_select_ner])\n    else:\n        new_data.append([t_tag, t_ner_sentence])', 'for (di_0, di_1, *di_len) in org_data:\n    (t_tag, t_ner_sentence) = (di_0, di_1)\n    if t_tag in self.data_augument_tag_list and t_tag in self.tag_map:\n        rdm_select_ner = self.__get_random_ner(t_tag)\n        new_data.append([t_tag, rdm_select_ner])\n    else:\n        new_data.append([t_tag, t_ner_sentence])']",-1
geany-themes,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/geany-themes/scripts/autobump.py,https://github.com/codebrainz/geany-themes/tree/master/scripts/autobump.py,,write_log$26,"def write_log(log_file, entries):
    new_lines = []
    for ent in entries:
        new_lines.append('\t'.join((ent[1], ent[0])))
    open(log_file, 'w').write('\n'.join(new_lines) + '\n')","for ent in entries:
    new_lines.append('\t'.join((ent[1], ent[0])))","new_lines = ['\t'.join((ent[1], ent[0])) for ent in entries]","[""for ent in entries:\n    (ent_0, ent_1, *_) = ent\n    new_lines.append('\\t'.join((ent_1, ent_0)))"", ""for (ent_0, ent_1, *ent_len) in entries:\n    new_lines.append('\\t'.join((ent_1, ent_0)))""]",-1
sfepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sfepy/sfepy/terms/terms.py,https://github.com/sfepy/sfepy/tree/master/sfepy/terms/terms.py,Term,get_material_names$676,"def get_material_names(self):
    out = []
    for aux in self.names.material:
        if aux[0] is not None:
            out.append(aux[0])
    return out","for aux in self.names.material:
    if aux[0] is not None:
        out.append(aux[0])",out = [aux[0] for aux in self.names.material if aux[0] is not None],"['for aux in self.names.material:\n    (aux_0, *aux_rauxmaining) = aux\n    if aux_0 is not None:\n        out.append(aux_0)', 'for (aux_0, *aux_len) in self.names.material:\n    if \n    aux_0 is not None:\n        out.append(\n        aux_0)']",-1
VideoSuperResolution,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VideoSuperResolution/VSR/Backend/TF/Framework/Trainer.py,https://github.com/LoSealL/VideoSuperResolution/tree/master/VSR/Backend/TF/Framework/Trainer.py,,_ensemble_reduce_mean$47,"def _ensemble_reduce_mean(outputs):
    results = []
    for i in outputs:
        outputs_ensemble = [i[0], np.rot90(i[1], 3, axes=[-3, -2]), np.rot90(i[2], 2, axes=[-3, -2]), np.rot90(i[3], 1, axes=[-3, -2]), np.flip(i[4], axis=-2), np.flip(np.rot90(i[5], 3, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[6], 2, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[7], 1, axes=[-3, -2]), axis=-2)]
        results.append(np.concatenate(outputs_ensemble).mean(axis=0, keepdims=True))
    return results","for i in outputs:
    outputs_ensemble = [i[0], np.rot90(i[1], 3, axes=[-3, -2]), np.rot90(i[2], 2, axes=[-3, -2]), np.rot90(i[3], 1, axes=[-3, -2]), np.flip(i[4], axis=-2), np.flip(np.rot90(i[5], 3, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[6], 2, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[7], 1, axes=[-3, -2]), axis=-2)]
    results.append(np.concatenate(outputs_ensemble).mean(axis=0, keepdims=True))","outputs_ensemble = [[i[j], np.rot90(i[j + 1], 3, axes=[-3, -2]), np.rot90(i[j + 2], 2, axes=[-3, -2]), np.rot90(i[j + 3], 1, axes=[-3, -2]), np.flip(i[j + 4], axis=-2), np.flip(np.rot90(i[j + 5], 3, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[j + 6], 2, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[j + 7], 1, axes=[-3, -2]), axis=-2)] for j in range(0, len(i), 8)]
results = [np.concatenate(outputs).mean(axis=0, keepdims=True) for outputs in outputs_ensemble]","['for i in outputs:\n    (i_0, i_1, i_2, i_3, i_4, i_5, i_6, i_7, *_) = i\n    outputs_ensemble = [i_0, np.rot90(i_1, 3, axes=[-3, -2]), np.rot90(i_2, 2, axes=[-3, -2]), np.rot90(i_3, 1, axes=[-3, -2]), np.flip(i_4, axis=-2), np.flip(np.rot90(i_5, 3, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i_6, 2, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i_7, 1, axes=[-3, -2]), axis=-2)]\n    results.append(np.concatenate(outputs_ensemble).mean(axis=0, keepdims=True))', 'for (i_0, i_1, i_2, i_3, i_4, i_5, i_6, i_7, *i_len) in outputs:\n    outputs_ensemble = [i_0, np.rot90(i_1, 3, axes=[-3, -2]), np.rot90(i_2, 2, axes=[-3, -2]), np.rot90(i_3, 1, axes=[-3, -2]), np.flip(i_4, axis=-2), np.flip(np.rot90(i_5, 3, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i_6, 2, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i_7, 1, axes=[-3, -2]), axis=-2)]\n    results.append(np.concatenate(outputs_ensemble).mean(axis=0, keepdims=True))']",-1
grokking-the-object-oriented-design-interview,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/grokking-the-object-oriented-design-interview/example-codes/chess/king.py,https://github.com/tssovi/grokking-the-object-oriented-design-interview/tree/master/example-codes/chess/king.py,King,get_threatened_positions$20,"def get_threatened_positions(self, board):
    positions = []
    for increment in King.SPOT_INCREMENTS:
        positions.append(board.spot_search_threat(self._position, self._color, increment[0], increment[1]))
    positions = [x for x in positions if x is not None]
    return positions","for increment in King.SPOT_INCREMENTS:
    positions.append(board.spot_search_threat(self._position, self._color, increment[0], increment[1]))","positions = [board.spot_search_threat(self._position, self._color, increment[0], increment[1]) for increment in King.SPOT_INCREMENTS if board.spot_search_threat(self._position, self._color, increment[0], increment[1]) is not None]","['for increment in King.SPOT_INCREMENTS:\n    (increment_0, increment_1, *_) = increment\n    positions.append(board.spot_search_threat(self._position, self._color, increment_0, increment_1))', 'for (increment_0, increment_1, *increment_len) in King.SPOT_INCREMENTS:\n    positions.append(board.spot_search_threat(self._position, self._color, increment_0, increment_1))']",-1
mvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mvt/mvt/ios/modules/mixed/chrome_favicon.py,https://github.com/mvt-project/mvt/tree/master/mvt/ios/modules/mixed/chrome_favicon.py,ChromeFavicon,run$48,"def run(self):
    self._find_ios_database(backup_ids=CHROME_FAVICON_BACKUP_IDS, root_paths=CHROME_FAVICON_ROOT_PATHS)
    self.log.info('Found Chrome favicon cache database at path: %s', self.file_path)
    conn = sqlite3.connect(self.file_path)
    cur = conn.cursor()
    cur.execute('\n            SELECT\n                icon_mapping.page_url,\n                favicons.url,\n                favicon_bitmaps.last_updated,\n                favicon_bitmaps.last_requested\n            FROM icon_mapping\n            JOIN favicon_bitmaps ON icon_mapping.icon_id = favicon_bitmaps.icon_id\n            JOIN favicons ON icon_mapping.icon_id = favicons.id\n            ORDER BY icon_mapping.id;\n        ')
    records = []
    for row in cur:
        last_timestamp = int(row[2]) or int(row[3])
        records.append({'url': row[0], 'icon_url': row[1], 'timestamp': last_timestamp, 'isodate': convert_timestamp_to_iso(convert_chrometime_to_unix(last_timestamp))})
    cur.close()
    conn.close()
    self.log.info('Extracted a total of %d favicon records', len(records))
    self.results = sorted(records, key=lambda row: row['isodate'])","for row in cur:
    last_timestamp = int(row[2]) or int(row[3])
    records.append({'url': row[0], 'icon_url': row[1], 'timestamp': last_timestamp, 'isodate': convert_timestamp_to_iso(convert_chrometime_to_unix(last_timestamp))})","records = [{'url': row[0], 'icon_url': row[1], 'timestamp': int(row[2]) or int(row[3]), 'isodate': convert_timestamp_to_iso(convert_chrometime_to_unix(int(row[2]) or int(row[3])))} for row in cur]","[""for row in cur:\n    (row_0, row_1, row_2, row_3, *_) = row\n    last_timestamp = int(row_2) or int(row_3)\n    records.append({'url': row_0, 'icon_url': row_1, 'timestamp': last_timestamp, 'isodate': convert_timestamp_to_iso(convert_chrometime_to_unix(last_timestamp))})"", ""for (row_0, row_1, row_2, row_3, *row_len) in cur:\n    last_timestamp = int(row_2) or int(row_3)\n    records.append({'url': row_0, 'icon_url': row_1, 'timestamp': last_timestamp, 'isodate': convert_timestamp_to_iso(convert_chrometime_to_unix(last_timestamp))})""]",-1
dex,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dex/dex/dex.py,https://github.com/mongolab/dex/tree/master/dex/dex.py,Dex,_get_requested_databases$431,"def _get_requested_databases(self):
    """"""Returns a list of databases requested, not including ignored dbs""""""
    requested_databases = []
    if self._requested_namespaces is not None and self._requested_namespaces != []:
        for requested_namespace in self._requested_namespaces:
            if requested_namespace[0] is '*':
                return []
            elif requested_namespace[0] not in IGNORE_DBS:
                requested_databases.append(requested_namespace[0])
    return requested_databases","for requested_namespace in self._requested_namespaces:
    if requested_namespace[0] is '*':
        return []
    elif requested_namespace[0] not in IGNORE_DBS:
        requested_databases.append(requested_namespace[0])",requested_databases = [requested_namespace[0] for requested_namespace in self._requested_namespaces if requested_namespace[0] not in IGNORE_DBS and requested_namespace[0] != '*'],"[""for requested_namespace in self._requested_namespaces:\n    (requested_namespace_0, *requested_namespace_rrequested_namespacemaining) = requested_namespace\n    if requested_namespace_0 is '*':\n        return []\n    elif requested_namespace_0 not in IGNORE_DBS:\n        requested_databases.append(requested_namespace_0)"", ""for (requested_namespace_0, *requested_namespace_len) in self._requested_namespaces:\n    if \n    requested_namespace_0 is '*':\n        return []\n    elif \n    requested_namespace_0 not in IGNORE_DBS:\n        requested_databases.append(\n        requested_namespace_0)""]",-1
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/incubate/fleet/tests/ctr_dataset_reader.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/incubate/fleet/tests/ctr_dataset_reader.py,,load_lr_input_record$54,"def load_lr_input_record(sent):
    res = []
    for _ in [x.split(':') for x in sent.split()]:
        res.append(int(_[0]))
    return res","for _ in [x.split(':') for x in sent.split()]:
    res.append(int(_[0]))",res = [int(x.split(':')[0]) for x in sent.split()],"[""for _ in [x.split(':') for x in sent.split()]:\n    (__0, *__r_maining) = _\n    res.append(int(__0))"", ""for (__0, *__len) in [x.split(':') for x in sent.split()]:\n    res.append(int(__0))""]",-1
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/special_agents/agent_vsphere.py,https://github.com/tribe29/checkmk/tree/master/cmk/special_agents/agent_vsphere.py,,eval_snapshot_list$1689,"def eval_snapshot_list(info, _datastores):
    response = []
    snapshot_info = get_pattern('<name>(.*?)</name>.*?<id>(.*?)</id><createTime>(.*?)</createTime><state>(.*?)</state>', info)
    for entry in snapshot_info:
        try:
            creation_time = int(time.mktime(time.strptime(entry[2][:19], '%Y-%m-%dT%H:%M:%S')))
        except ValueError:
            creation_time = 0
        response.append('%s %s %s %s' % (entry[1], creation_time, entry[3], entry[0].replace('|', ' ')))
    return '|'.join(response)","for entry in snapshot_info:
    try:
        creation_time = int(time.mktime(time.strptime(entry[2][:19], '%Y-%m-%dT%H:%M:%S')))
    except ValueError:
        creation_time = 0
    response.append('%s %s %s %s' % (entry[1], creation_time, entry[3], entry[0].replace('|', ' ')))","response = [f""{entry[1]} {(int(time.mktime(time.strptime(entry[2][:19], '%Y-%m-%dT%H:%M:%S'))) if entry[2][:19] else 0)} {entry[3]} {entry[0].replace('|', ' ')}"" for entry in snapshot_info]","[""for (entry_0, entry_1, entry_2, entry_3, (*entry_2_0_1_19, *entry_2_0_1_len), *entry_len) in snapshot_info:\n    try:\n        creation_time = int(time.mktime(time.strptime(*entry_2_0_1_19, '%Y-%m-%dT%H:%M:%S')))\n    except ValueError:\n        creation_time = 0\n    response.append('%s %s %s %s' % (entry_1, creation_time, entry_3, entry_0.replace('|', ' ')))""]",-1
buku,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/buku/bukuserver/server.py,https://github.com/jarun/buku/tree/master/bukuserver/server.py,ApiBookmarkSearchView,get$532,"def get(self):
    arg_obj = request.args
    keywords = arg_obj.getlist('keywords')
    all_keywords = arg_obj.get('all_keywords')
    deep = arg_obj.get('deep')
    regex = arg_obj.get('regex')
    all_keywords = False if all_keywords is None else all_keywords
    deep = False if deep is None else deep
    regex = False if regex is None else regex
    all_keywords = all_keywords if isinstance(all_keywords, bool) else all_keywords.lower() == 'true'
    deep = deep if isinstance(deep, bool) else deep.lower() == 'true'
    regex = regex if isinstance(regex, bool) else regex.lower() == 'true'
    result = {'bookmarks': []}
    bukudb = getattr(flask.g, 'bukudb', get_bukudb())
    found_bookmarks = bukudb.searchdb(keywords, all_keywords, deep, regex)
    found_bookmarks = [] if found_bookmarks is None else found_bookmarks
    res = None
    if found_bookmarks is not None:
        for bookmark in found_bookmarks:
            result_bookmark = {'id': bookmark[0], 'url': bookmark[1], 'title': bookmark[2], 'tags': list(filter(lambda x: x, bookmark[3].split(','))), 'description': bookmark[4]}
            result['bookmarks'].append(result_bookmark)
    current_app.logger.debug('total bookmarks:{}'.format(len(result['bookmarks'])))
    res = jsonify(result)
    return res","for bookmark in found_bookmarks:
    result_bookmark = {'id': bookmark[0], 'url': bookmark[1], 'title': bookmark[2], 'tags': list(filter(lambda x: x, bookmark[3].split(','))), 'description': bookmark[4]}
    result['bookmarks'].append(result_bookmark)","result['bookmarks'] = [{'id': bookmark[0], 'url': bookmark[1], 'title': bookmark[2], 'tags': list(filter(lambda x: x, bookmark[3].split(','))), 'description': bookmark[4]} for bookmark in found_bookmarks]","[""for bookmark in found_bookmarks:\n    (bookmark_0, bookmark_1, bookmark_2, bookmark_3, bookmark_4, *_) = bookmark\n    result_bookmark = {'id': bookmark_0, 'url': bookmark_1, 'title': bookmark_2, 'tags': list(filter(lambda x: x, bookmark_3.split(','))), 'description': bookmark_4}\n    result['bookmarks'].append(result_bookmark)"", ""for (bookmark_0, bookmark_1, bookmark_2, bookmark_3, bookmark_4, *bookmark_len) in found_bookmarks:\n    result_bookmark = {'id': bookmark_0, 'url': bookmark_1, 'title': bookmark_2, 'tags': list(filter(lambda x: x, bookmark_3.split(','))), 'description': bookmark_4}\n    result['bookmarks'].append(result_bookmark)""]",-1
upvote_py2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/upvote_py2/upvote/monitoring/metrics.py,https://github.com/google/upvote_py2/tree/master/upvote/monitoring/metrics.py,Namespace,__init__$35,"def __init__(self, prefix, tuples):
    self.metrics = []
    for t in tuples:
        metric = Metric(prefix + t[0], t[1])
        setattr(self, t[0].upper(), metric)
        self.metrics.append(metric)
    setattr(self, 'ALL', self.metrics)","for t in tuples:
    metric = Metric(prefix + t[0], t[1])
    setattr(self, t[0].upper(), metric)
    self.metrics.append(metric)","self.metrics = [Metric(prefix + t[0], t[1]) for t in tuples]
for metric in self.metrics:
    setattr(self, metric.name.upper(), metric)
setattr(self, 'ALL', self.metrics)","['for t in tuples:\n    (t_0, t_1, *_) = t\n    metric = Metric(prefix + t_0, t_1)\n    setattr(self, t_0.upper(), metric)\n    self.metrics.append(metric)', 'for (t_0, t_1, *t_len) in tuples:\n    metric = Metric(prefix + t_0, t_1)\n    setattr(self, t_0.upper(), metric)\n    self.metrics.append(metric)']",-1
CMSmap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CMSmap/cmsmap/lib/jooscan.py,https://github.com/Dionach/CMSmap/tree/master/cmsmap/lib/jooscan.py,JooScan,JooFeed$140,"def JooFeed(self):
    requester.request(self.url + '/?format=feed', data=None)
    jooUsers = re.findall('<author>(.+?) \\((.+?)\\)</author>', requester.htmltext, re.IGNORECASE)
    if jooUsers:
        msg = 'Enumerating Joomla Usernames via ""Feed"" ...'
        report.message(msg)
        jooUsers = sorted(set(jooUsers))
        for user in jooUsers:
            self.usernames.append(user[1])
            msg = user[1] + ': ' + user[0]
            report.info(msg)","for user in jooUsers:
    self.usernames.append(user[1])
    msg = user[1] + ': ' + user[0]
    report.info(msg)","self.usernames = [user[1] for user in jooUsers]
for user in jooUsers:
    msg = user[1] + ': ' + user[0]
    report.info(msg)","[""for user in jooUsers:\n    (user_0, user_1, *_) = user\n    self.usernames.append(user_1)\n    msg = user_1 + ': ' + user_0\n    report.info(msg)"", ""for (user_0, user_1, *user_len) in jooUsers:\n    self.usernames.append(\n    user_1)\n    msg = \n    user_1 + ': ' + \n    user_0\n    report.info(msg)""]",-1
nltk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/sem/evaluate.py,https://github.com/nltk/nltk/tree/master/nltk/sem/evaluate.py,Assignment,_addvariant$355,"def _addvariant(self):
    """"""
        Create a more pretty-printable version of the assignment.
        """"""
    list_ = []
    for item in self.items():
        pair = (item[1], item[0])
        list_.append(pair)
    self.variant = list_
    return None","for item in self.items():
    pair = (item[1], item[0])
    list_.append(pair)","self.variant = [(value, key) for (key, value) in self.items()]","['for item in self.items():\n    (item_0, item_1, *_) = item\n    pair = (item_1, item_0)\n    list_.append(pair)', 'for (item_0, item_1, *item_len) in self.items():\n    pair = (item_1, item_0)\n    list_.append(pair)']",1
hfnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hfnet/colmap-helpers/features_from_npz.py,https://github.com/ethz-asl/hfnet/tree/master/colmap-helpers/features_from_npz.py,,export_features_from_npz$17,"def export_features_from_npz(filename, in_path, out_path):
    path_file = os.path.join(in_path, filename)
    frame1 = np.load(path_file)
    filename = os.path.splitext(os.path.basename(path_file))[0]
    out_path_and_name = os.path.join(out_path, filename) + '.jpg.txt'
    outfile = open(out_path_and_name, 'w+')
    SIFT_SIZE = 128
    kp1 = frame1['keypoints']
    outfile.write(str(kp1.shape[0]) + ' ' + str(SIFT_SIZE) + '\n')
    for keypoint in kp1:
        s = str(keypoint[0]) + ' ' + str(keypoint[1])
        s += ' 1 1 ' + '1 ' * SIFT_SIZE + '\n'
        outfile.write(s)
    outfile.close()","for keypoint in kp1:
    s = str(keypoint[0]) + ' ' + str(keypoint[1])
    s += ' 1 1 ' + '1 ' * SIFT_SIZE + '\n'
    outfile.write(s)","sift_values = '1 ' * SIFT_SIZE
for keypoint in kp1:
    s = f'{keypoint[0]} {keypoint[1]} 1 1 {sift_values}\n'
    outfile.write(s)","[""for keypoint in kp1:\n    (keypoint_0, keypoint_1, *_) = keypoint\n    s = str(keypoint_0) + ' ' + str(keypoint_1)\n    s += ' 1 1 ' + '1 ' * SIFT_SIZE + '\\n'\n    outfile.write(s)"", ""for (keypoint_0, keypoint_1, *keypoint_len) in kp1:\n    s = str(keypoint_0) + ' ' + str(keypoint_1)\n    s += ' 1 1 ' + '1 ' * SIFT_SIZE + '\\n'\n    outfile.write(s)""]",-1
ParlAI,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/core/torch_generator_agent.py,https://github.com/facebookresearch/ParlAI/tree/master/parlai/core/torch_generator_agent.py,TorchGeneratorAgent,eval_step$894,"def eval_step(self, batch):
    """"""
        Evaluate a single batch of examples.
        """"""
    if batch.text_vec is None and batch.image is None:
        return
    if batch.text_vec is not None:
        bsz = batch.text_vec.size(0)
    else:
        bsz = len(batch.image)
    self.model.eval()
    cand_scores = None
    token_losses = None
    text_token_info = None
    if batch.label_vec is not None:
        (loss, model_output) = self.compute_loss(batch, return_output=True)
        if self.show_token_details:
            token_losses = self._construct_label_token_losses(batch.label_vec, model_output)
    beam_preds_scores = None
    preds = None
    if self.skip_generation:
        warn_once('--skip-generation true produces limited metrics')
    else:
        maxlen = self.label_truncate or 256
        prefix_tokens = self.get_prefix_tokens(batch)
        (beam_preds_scores, beams) = self._generate(batch, self.beam_size, maxlen, prefix_tokens=prefix_tokens)
        (preds, _, _) = zip(*beam_preds_scores)
        self._add_generation_metrics(batch, preds)
        beam_texts: List[List[Tuple[str, float]]] = []
        beam_texts_token_info: List[List[List[Tuple]]] = []
        for beam in beams:
            beam_texts.append([])
            if self.show_token_details:
                beam_texts_token_info.append([])
            for (tokens, score, token_metadata) in beam.get_rescored_finished():
                try:
                    if self.show_token_details:
                        beam_texts_token_info[-1].append(self._construct_generated_token_details(tokens, token_metadata))
                    beam_texts[-1].append((self._v2t(tokens), score.item()))
                except KeyError:
                    logging.error('Decoding error: %s', tokens)
                    continue
    cand_choices = None
    cand_scores = None
    if self.rank_candidates:
        (cand_choices, cand_scores) = self.rank_eval_label_candidates(batch, bsz)
    text = [self._v2t(pred_data[0]) for pred_data in beam_preds_scores] if beam_preds_scores is not None else None
    if self.show_token_details and beam_preds_scores is not None:
        text_token_info = []
        for beam_text_token_info in beam_texts_token_info:
            text_token_info.append(beam_text_token_info[0])
    if text and self.compute_tokenized_bleu:
        self._compute_fairseq_bleu(batch, preds)
    retval = Output(text, cand_choices, token_losses=token_losses, cand_scores=cand_scores)
    if not self.skip_generation:
        retval.beam_texts = beam_texts
        retval.beam_texts_token_info = beam_texts_token_info
        retval.text_token_info = text_token_info
    return retval","for beam_text_token_info in beam_texts_token_info:
    text_token_info.append(beam_text_token_info[0])",text_token_info = [beam_text_token_info[0] for beam_text_token_info in beam_texts_token_info],"['for beam_text_token_info in beam_texts_token_info:\n    (beam_text_token_info_0, *beam_text_token_info_rbeam_text_token_infomaining) = beam_text_token_info\n    text_token_info.append(beam_text_token_info_0)', 'for (beam_text_token_info_0, *beam_text_token_info_len) in beam_texts_token_info:\n    text_token_info.append(beam_text_token_info_0)']",-1
stacker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stacker/stacker/tests/providers/aws/test_default.py,https://github.com/cloudtools/stacker/tree/master/stacker/tests/providers/aws/test_default.py,TestProviderInteractiveMode,test_select_update_method$945,"def test_select_update_method(self):
    for i in [[{'force_interactive': False, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:
        self.assertEquals(self.provider.select_update_method(**i[0]), i[1])","for i in [[{'force_interactive': False, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:
    self.assertEquals(self.provider.select_update_method(**i[0]), i[1])","update_methods = [{'force_interactive': False, 'force_change_set': False}, {'force_interactive': True, 'force_change_set': False}, {'force_interactive': False, 'force_change_set': True}, {'force_interactive': True, 'force_change_set': True}]
for update_method in update_methods:
    self.assertEquals(self.provider.select_update_method(**update_method), self.provider.interactive_update_stack)","[""for i in [[{'force_interactive': False, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:\n    (i_0, i_1, *_) = i\n    self.assertEquals(self.provider.select_update_method(**i_0), i_1)"", ""for (i_0, i_1, *i_len) in [[{'force_interactive': False, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:\n    self.assertEquals(self.provider.select_update_method(**i_0), i_1)""]",-1
stacker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stacker/stacker/tests/providers/aws/test_default.py,https://github.com/cloudtools/stacker/tree/master/stacker/tests/providers/aws/test_default.py,TestProviderDefaultMode,test_select_update_method$512,"def test_select_update_method(self):
    for i in [[{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': False}, self.provider.default_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.noninteractive_changeset_update], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:
        self.assertEquals(self.provider.select_update_method(**i[0]), i[1])","for i in [[{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': False}, self.provider.default_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.noninteractive_changeset_update], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:
    self.assertEquals(self.provider.select_update_method(**i[0]), i[1])","update_methods = [{'force_interactive': True, 'force_change_set': False}, {'force_interactive': False, 'force_change_set': False}, {'force_interactive': False, 'force_change_set': True}, {'force_interactive': True, 'force_change_set': True}]
for update_method in update_methods:
    expected_method = None
    if update_method['force_interactive']:
        expected_method = self.provider.interactive_update_stack
    elif update_method['force_change_set']:
        expected_method = self.provider.noninteractive_changeset_update
    else:
        expected_method = self.provider.default_update_stack
    self.assertEquals(self.provider.select_update_method(**update_method), expected_method)","[""for i in [[{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': False}, self.provider.default_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.noninteractive_changeset_update], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:\n    (i_0, i_1, *_) = i\n    self.assertEquals(self.provider.select_update_method(**i_0), i_1)"", ""for (i_0, i_1, *i_len) in [[{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': False}, self.provider.default_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.noninteractive_changeset_update], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:\n    self.assertEquals(self.provider.select_update_method(**i_0), i_1)""]",-1
mycroft-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mycroft-core/mycroft/tts/mimic2_tts.py,https://github.com/MycroftAI/mycroft-core/tree/master/mycroft/tts/mimic2_tts.py,Mimic2,viseme$189,"def viseme(self, phonemes):
    """"""Maps phonemes to appropriate viseme encoding

        Args:
            phonemes (list): list of tuples (phoneme, time_start)

        Returns:
            list: list of tuples (viseme_encoding, time_start)
        """"""
    visemes = []
    for pair in phonemes:
        if pair[0]:
            phone = pair[0].lower()
        else:
            phone = 'z'
        vis = VISIMES.get(phone)
        vis_dur = float(pair[1])
        visemes.append((vis, vis_dur))
    return visemes","for pair in phonemes:
    if pair[0]:
        phone = pair[0].lower()
    else:
        phone = 'z'
    vis = VISIMES.get(phone)
    vis_dur = float(pair[1])
    visemes.append((vis, vis_dur))","visemes = [(VISIMES.get(pair[0].lower(), 'z'), float(pair[1])) for pair in phonemes]","[""for pair in phonemes:\n    (pair_0, pair_1, *_) = pair\n    if pair_0:\n        phone = pair_0.lower()\n    else:\n        phone = 'z'\n    vis = VISIMES.get(phone)\n    vis_dur = float(pair_1)\n    visemes.append((vis, vis_dur))"", ""for (pair_0, pair_1, *pair_len) in phonemes:\n    if \n    pair_0:\n        phone = \n        pair_0.lower()\n    else:\n        phone = 'z'\n    vis = VISIMES.get(phone)\n    vis_dur = float(\n    pair_1)\n    visemes.append((vis, vis_dur))""]",-1
inception,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/inception/inception/interfaces/slotscreamer.py,https://github.com/carmaa/inception/tree/master/inception/interfaces/slotscreamer.py,SlotScreamer,readv$109,"def readv(self, req):
    for r in req:
        yield (r[0], self.read(r[0], r[1]))","for r in req:
    yield (r[0], self.read(r[0], r[1]))","yield from ((r[0], self.read(r[0], r[1])) for r in req)","['for r in req:\n    (r_0, r_1, *_) = r\n    yield (r_0, self.read(r_0, r_1))', 'for (r_0, r_1, *r_len) in req:\n    yield (r_0, self.read(r_0, r_1))']",-1
edx-platform,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/lms/djangoapps/grades/management/commands/compute_grades.py,https://github.com/edx/edx-platform/tree/master/lms/djangoapps/grades/management/commands/compute_grades.py,Command,_shuffled_task_kwargs$92,"def _shuffled_task_kwargs(self, options):
    """"""
        Iterate over all task keyword arguments in random order.

        Randomizing them will help even out the load on the task workers,
        though it will not entirely prevent the possibility of spikes.  It will
        also make the overall time to completion more predictable.
        """"""
    all_args = []
    estimate_first_attempted = options['estimate_first_attempted']
    for course_key in self._get_course_keys(options):
        for task_arg_tuple in tasks._course_task_args(course_key, **options):
            all_args.append(task_arg_tuple)
    all_args.sort(key=lambda x: hashlib.md5(f'{x!r}'.encode('utf-8')).digest())
    for args in all_args:
        yield {'course_key': args[0], 'offset': args[1], 'batch_size': args[2], 'estimate_first_attempted': estimate_first_attempted}","for args in all_args:
    yield {'course_key': args[0], 'offset': args[1], 'batch_size': args[2], 'estimate_first_attempted': estimate_first_attempted}","yield from ({'course_key': args[0], 'offset': args[1], 'batch_size': args[2], 'estimate_first_attempted': estimate_first_attempted} for args in all_args)","[""for args in all_args:\n    (args_0, args_1, args_2, *_) = args\n    yield {'course_key': args_0, 'offset': args_1, 'batch_size': args_2, 'estimate_first_attempted': estimate_first_attempted}"", ""for (args_0, args_1, args_2, *args_len) in all_args:\n    yield {'course_key': args_0, 'offset': args_1, 'batch_size': args_2, 'estimate_first_attempted': estimate_first_attempted}""]",-1
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/special_agents/agent_vsphere.py,https://github.com/tribe29/checkmk/tree/master/cmk/special_agents/agent_vsphere.py,,eval_snapshot_list$1689,"def eval_snapshot_list(info, _datastores):
    response = []
    snapshot_info = get_pattern('<name>(.*?)</name>.*?<id>(.*?)</id><createTime>(.*?)</createTime><state>(.*?)</state>', info)
    for entry in snapshot_info:
        try:
            creation_time = int(time.mktime(time.strptime(entry[2][:19], '%Y-%m-%dT%H:%M:%S')))
        except ValueError:
            creation_time = 0
        response.append('%s %s %s %s' % (entry[1], creation_time, entry[3], entry[0].replace('|', ' ')))
    return '|'.join(response)","for entry in snapshot_info:
    try:
        creation_time = int(time.mktime(time.strptime(entry[2][:19], '%Y-%m-%dT%H:%M:%S')))
    except ValueError:
        creation_time = 0
    response.append('%s %s %s %s' % (entry[1], creation_time, entry[3], entry[0].replace('|', ' ')))","[""for (entry_0, entry_1, entry_2, entry_3, (*entry_2_0_1_19, *entry_2_0_1_len), *entry_len) in snapshot_info:\n    try:\n        creation_time = int(time.mktime(time.strptime(*entry_2_0_1_19, '%Y-%m-%dT%H:%M:%S')))\n    except ValueError:\n        creation_time = 0\n    response.append('%s %s %s %s' % (entry_1, creation_time, entry_3, entry_0.replace('|', ' ')))""]",no_found,0
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/op/smooth_uv.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/op/smooth_uv.py,MUV_OT_SmoothUV,__smooth_wo_transmission$107,"def __smooth_wo_transmission(self, loop_seqs, uv_layer):
    loops = []
    for hseq in loop_seqs:
        loops.extend([hseq[0][0], hseq[0][1]])
    full_vlen = 0
    accm_vlens = [0.0]
    full_uvlen = 0
    accm_uvlens = [0.0]
    orig_uvs = [loop_seqs[0][0][0][uv_layer].uv.copy()]
    for (l1, l2) in zip(loops[:-1], loops[1:]):
        diff_v = l2.vert.co - l1.vert.co
        full_vlen = full_vlen + diff_v.length
        accm_vlens.append(full_vlen)
        diff_uv = l2[uv_layer].uv - l1[uv_layer].uv
        full_uvlen = full_uvlen + diff_uv.length
        accm_uvlens.append(full_uvlen)
        orig_uvs.append(l2[uv_layer].uv.copy())
    for (hidx, hseq) in enumerate(loop_seqs):
        pair = hseq[0]
        for (pidx, l) in enumerate(pair):
            if self.select:
                l[uv_layer].select = True
            if hidx == 0 and pidx == 0 or (hidx == len(loop_seqs) - 1 and pidx == len(pair) - 1):
                continue
            tgt_noinfl = full_uvlen * (hidx + pidx) / len(loop_seqs)
            tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
            target_length = tgt_noinfl * (1 - self.mesh_infl) + tgt_infl * self.mesh_infl
            for i in range(len(accm_uvlens[:-1])):
                if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                    tgt_seg_len = target_length - accm_uvlens[i]
                    seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                    uv1 = orig_uvs[i]
                    uv2 = orig_uvs[i + 1]
                    target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                    break
            else:
                self.report({'ERROR'}, 'Failed to get target UV')
                return {'CANCELLED'}
            l[uv_layer].uv = target_uv","for (hidx, hseq) in enumerate(loop_seqs):
    pair = hseq[0]
    for (pidx, l) in enumerate(pair):
        if self.select:
            l[uv_layer].select = True
        if hidx == 0 and pidx == 0 or (hidx == len(loop_seqs) - 1 and pidx == len(pair) - 1):
            continue
        tgt_noinfl = full_uvlen * (hidx + pidx) / len(loop_seqs)
        tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
        target_length = tgt_noinfl * (1 - self.mesh_infl) + tgt_infl * self.mesh_infl
        for i in range(len(accm_uvlens[:-1])):
            if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                tgt_seg_len = target_length - accm_uvlens[i]
                seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                uv1 = orig_uvs[i]
                uv2 = orig_uvs[i + 1]
                target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                break
        else:
            self.report({'ERROR'}, 'Failed to get target UV')
            return {'CANCELLED'}
        l[uv_layer].uv = target_uv","[""for (hidx, hseq) in enumerate(loop_seqs):\n    (hseq_0, *hseq_rhseqmaining) = hseq\n    pair = hseq_0\n    for (pidx, l) in enumerate(pair):\n        if self.select:\n            l[uv_layer].select = True\n        if hidx == 0 and pidx == 0 or (hidx == len(loop_seqs) - 1 and pidx == len(pair) - 1):\n            continue\n        tgt_noinfl = full_uvlen * (hidx + pidx) / len(loop_seqs)\n        tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen\n        target_length = tgt_noinfl * (1 - self.mesh_infl) + tgt_infl * self.mesh_infl\n        for i in range(len(accm_uvlens[:-1])):\n            if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:\n                tgt_seg_len = target_length - accm_uvlens[i]\n                seg_len = accm_uvlens[i + 1] - accm_uvlens[i]\n                uv1 = orig_uvs[i]\n                uv2 = orig_uvs[i + 1]\n                target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len\n                break\n        else:\n            self.report({'ERROR'}, 'Failed to get target UV')\n            return {'CANCELLED'}\n        l[uv_layer].uv = target_uv""]",no_found,0
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/time/tests/test_basic.py,https://github.com/astropy/astropy/tree/master/astropy/time/tests/test_basic.py,TestSubFormat,test_fits_scale$963,"def test_fits_scale(self):
    """"""Test that the previous FITS-string formatting can still be handled
        but with a DeprecationWarning.""""""
    for inputs in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
        with pytest.warns(AstropyDeprecationWarning):
            t = Time(inputs[0])
        assert t.scale == inputs[1]
        t2 = Time(inputs[0][:inputs[0].index('(')], format='isot', scale=inputs[1])
        assert t == t2
    with pytest.warns(AstropyDeprecationWarning):
        t = Time('1999-01-01T00:00:00.123456789(UTC)')
    t = t.tai
    assert t.isot == '1999-01-01T00:00:32.123'
    with pytest.warns(AstropyDeprecationWarning):
        t = Time('1999-01-01T00:00:32.123456789(TAI)')
    t = t.utc
    assert t.isot == '1999-01-01T00:00:00.123'
    with pytest.warns(AstropyDeprecationWarning):
        t = Time('1999-01-01T00:00:32.123456789(TAI)', scale='tai')
    assert t.scale == 'tai'
    with pytest.warns(AstropyDeprecationWarning):
        t = Time('1999-01-01T00:00:32.123456789(ET)', scale='tt')
    assert t.scale == 'tt'
    with pytest.raises(ValueError), pytest.warns(AstropyDeprecationWarning):
        t = Time('1999-01-01T00:00:32.123456789(TAI)', scale='utc')","for inputs in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):
    with pytest.warns(AstropyDeprecationWarning):
        t = Time(inputs[0])
    assert t.scale == inputs[1]
    t2 = Time(inputs[0][:inputs[0].index('(')], format='isot', scale=inputs[1])
    assert t == t2","[""for (inputs_0, inputs_1, *inputs_len) in (('2000-01-02(TAI)', 'tai'), ('1999-01-01T00:00:00.123(ET(NIST))', 'tt'), ('2014-12-12T01:00:44.1(UTC)', 'utc')):\n    with pytest.warns(AstropyDeprecationWarning):\n        t = Time(\n        inputs_0)\n    assert t.scale == \n    inputs_1\n    t2 = Time(\n    inputs_0[:\n    inputs_0.index('(')], format='isot', scale=\n    inputs_1)\n    assert t == t2""]",no_found,0
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
    dbg.createLogWindow()
    global currentArgs
    currentArgs = copy.copy(args)
    try:
        starttime = datetime.datetime.now()
        ptr_counter = 0
        commands = {}

        def getBanner():
            banners = {}
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |                         __               __                      |\n'
            bannertext += '    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n'
            bannertext += '    |  / ___/ __ \\/ ___/ _ \\/ / __ `/ __ \\   / __/ _ \\/ __ `/ __ `__ \\ |\n'
            bannertext += '    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n'
            bannertext += '    | \\___/\\____/_/   \\___/_/\\__,_/_/ /_/   \\__/\\___/\\__,_/_/ /_/ /_/  |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |     https://www.corelan.be | https://www.corelan-training.com    |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[0] = bannertext
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n'
            bannertext += ""    |       | '_ ` _ \\  / _ \\ | '_ \\  / _` |   | '_ \\ | | | |          |\n""
            bannertext += '    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n'
            bannertext += '    |       |_| |_| |_| \\___/ |_| |_| \\__,_|(_)| .__/  \\__, |          |\n'
            bannertext += '    |                                          |_|     |___/           |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[1] = bannertext
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |    _____ ___  ____  ____  ____ _                                 |\n'
            bannertext += '    |    / __ `__ \\/ __ \\/ __ \\/ __ `/  https://www.corelan.be         |\n'
            bannertext += '    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n'
            bannertext += '    |  /_/ /_/ /_/\\____/_/ /_/\\__,_/  #corelan (Freenode IRC)          |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[2] = bannertext
            bannertext = ''
            bannertext += '\n    .##.....##..#######..##....##....###........########..##....##\n'
            bannertext += '    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n'
            bannertext += '    .####.####.##.....##.####..##..##...##......##.....##...####..\n'
            bannertext += '    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n'
            bannertext += '    .##.....##.##.....##.##..####.#########.....##...........##...\n'
            bannertext += '    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n'
            bannertext += '    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n'
            banners[3] = bannertext
            bannerlist = []
            for i in range(0, len(banners)):
                bannerlist.append(i)
            random.shuffle(bannerlist)
            return banners[bannerlist[0]]

        def procHelp(args):
            dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__, str(arch)))
            dbg.log('     Plugin version : %s r%s' % (__VERSION__, __REV__))
            dbg.log('     Python version : %s' % getPythonVersion())
            if __DEBUGGERAPP__ == 'WinDBG':
                pykdversion = dbg.getPyKDVersionNr()
                dbg.log('     PyKD version %s' % pykdversion)
            dbg.log('     Written by Corelan - https://www.corelan.be')
            dbg.log('     Project page : https://github.com/corelan/mona')
            dbg.logLines(getBanner(), highlight=1)
            dbg.log('Global options :')
            dbg.log('----------------')
            dbg.log('You can use one or more of the following global options on any command that will perform')
            dbg.log('a search in one or more modules, returning a list of pointers :')
            dbg.log(' -n                     : Skip modules that start with a null byte. If this is too broad, use')
            dbg.log('                          option -cp nonull instead')
            dbg.log(' -o                     : Ignore OS modules')
            dbg.log(' -p <nr>                : Stop search after <nr> pointers.')
            dbg.log(' -m <module,module,...> : only query the given modules. Be sure what you are doing !')
            dbg.log('                          You can specify multiple modules (comma separated)')
            dbg.log('                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored')
            dbg.log('                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,')
            dbg.log('                          blah or *blah* = contains blah')
            dbg.log(' -cm <crit,crit,...>    : Apply some additional criteria to the modules to query.')
            dbg.log('                          You can use one or more of the following criteria :')
            dbg.log('                          aslr,safeseh,rebase,nx,os')
            dbg.log('                          You can enable or disable a certain criterium by setting it to true or false')
            dbg.log('                          Example :  -cm aslr=true,safeseh=false')
            dbg.log('                          Suppose you want to search for p/p/r in aslr enabled modules, you could call')
            dbg.log('                          !mona seh -cm aslr')
            dbg.log(' -cp <crit,crit,...>    : Apply some criteria to the pointers to return')
            dbg.log('                          Available options are :')
            dbg.log('                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev')
            dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
            dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
            dbg.log('                          You can use .. to indicate a range of bytes (in between 2 bad chars)')
            dbg.log(' -x <access>            : Specify desired access level of the returning pointers. If not specified,')
            dbg.log('                          only executable pointers will be returned.')
            dbg.log('                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *')
            if not args:
                args = []
            if len(args) > 1:
                thiscmd = args[1].lower().strip()
                if thiscmd in commands:
                    dbg.log('')
                    dbg.log(""Usage of command '%s' :"" % thiscmd)
                    dbg.log('%s' % ('-' * (22 + len(thiscmd))))
                    dbg.logLines(commands[thiscmd].usage)
                    dbg.log('')
                else:
                    aliasfound = False
                    for cmd in commands:
                        if commands[cmd].alias == thiscmd:
                            dbg.log('')
                            dbg.log(""Usage of command '%s' :"" % thiscmd)
                            dbg.log('%s' % ('-' * (22 + len(thiscmd))))
                            dbg.logLines(commands[cmd].usage)
                            dbg.log('')
                            aliasfound = True
                    if not aliasfound:
                        dbg.logLines('\nCommand %s does not exist. Run !mona to get a list of available commands\n' % thiscmd, highlight=1)
            else:
                dbg.logLines('\nUsage :')
                dbg.logLines('-------\n')
                dbg.log(' !mona <command> <parameter>')
                dbg.logLines('\nAvailable commands and parameters :\n')
                items = commands.items()
                items.sort(key=itemgetter(0))
                for item in items:
                    if commands[item[0]].usage != '':
                        aliastxt = ''
                        if commands[item[0]].alias != '':
                            aliastxt = ' / ' + commands[item[0]].alias
                        dbg.logLines('%s | %s' % (item[0] + aliastxt + ' ' * (20 - len(item[0] + aliastxt)), commands[item[0]].description))
                dbg.log('')
                dbg.log('Want more info about a given command ?  Run !mona help <command>', highlight=1)
                dbg.log('')
        commands['help'] = MnCommand('help', 'show help', '!mona help [command]', procHelp)

        def procConfig(args):
            showerror = False
            if not 'set' in args and (not 'get' in args) and (not 'add' in args):
                showerror = True
            if 'set' in args:
                if type(args['set']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['set'].split(' ')
                    if len(params) < 2:
                        showerror = True
            if 'add' in args:
                if type(args['add']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['add'].split(' ')
                    if len(params) < 2:
                        showerror = True
            if 'get' in args:
                if type(args['get']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['get'].split(' ')
                    if len(params) < 1:
                        showerror = True
            if showerror:
                dbg.log('Usage :')
                dbg.logLines(configUsage, highlight=1)
                return
            else:
                if 'get' in args:
                    dbg.log('Reading value from configuration file')
                    monaConfig = MnConfig()
                    thevalue = monaConfig.get(args['get'])
                    dbg.log('Parameter %s = %s' % (args['get'], thevalue))
                if 'set' in args:
                    dbg.log('Writing value to configuration file')
                    monaConfig = MnConfig()
                    value = args['set'].split(' ')
                    configparam = value[0].strip()
                    dbg.log('Old value of parameter %s = %s' % (configparam, monaConfig.get(configparam)))
                    configvalue = args['set'][0 + len(configparam):len(args['set'])]
                    monaConfig.set(configparam, configvalue)
                    dbg.log('New value of parameter %s = %s' % (configparam, configvalue))
                if 'add' in args:
                    dbg.log('Writing value to configuration file')
                    monaConfig = MnConfig()
                    value = args['add'].split(' ')
                    configparam = value[0].strip()
                    dbg.log('Old value of parameter %s = %s' % (configparam, monaConfig.get(configparam)))
                    configvalue = monaConfig.get(configparam).strip() + ',' + args['add'][0 + len(configparam):len(args['add'])].strip()
                    monaConfig.set(configparam, configvalue)
                    dbg.log('New value of parameter %s = %s' % (configparam, configvalue))

        def procFindJ(args):
            return procFindJMP(args)

        def procFindJMP(args):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            if inspect.stack()[1][3] == 'procFindJ':
                dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."", highlight=1)
            criteria = {}
            all_opcodes = {}
            global ptr_to_get
            ptr_to_get = -1
            distancestr = ''
            mindistance = 0
            maxdistance = 0
            showerror = False
            if 'r' in args:
                if type(args['r']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    thisreg = args['r'].upper().strip()
                    validregs = dbglib.Registers32BitsOrder
                    if not thisreg in validregs:
                        showerror = True
            else:
                showerror = True
            if 'distance' in args:
                if type(args['distance']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    distancestr = args['distance']
                    distanceparts = distancestr.split(',')
                    for parts in distanceparts:
                        valueparts = parts.split('=')
                        if len(valueparts) > 1:
                            if valueparts[0].lower() == 'min':
                                try:
                                    mindistance = int(valueparts[1])
                                except:
                                    mindistance = 0
                            if valueparts[0].lower() == 'max':
                                try:
                                    maxdistance = int(valueparts[1])
                                except:
                                    maxdistance = 0
            if maxdistance < mindistance:
                tmp = maxdistance
                maxdistance = mindistance
                mindistance = tmp
            criteria['mindistance'] = mindistance
            criteria['maxdistance'] = maxdistance
            if showerror:
                dbg.log('Usage :')
                dbg.logLines(jmpUsage, highlight=1)
                return
            else:
                (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
                all_opcodes = findJMP(modulecriteria, criteria, args['r'].lower().strip())
            logfile = MnLog('jmp.txt')
            thislog = logfile.reset()
            processResults(all_opcodes, logfile, thislog)

        def procFindSEH(args):
            modulecriteria = {}
            modulecriteria['safeseh'] = False
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            criteria = {}
            specialcases = {}
            all_opcodes = {}
            global ptr_to_get
            ptr_to_get = -1
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            if 'rop' in args:
                criteria['rop'] = True
            if 'all' in args:
                criteria['all'] = True
                specialcases['maponly'] = True
            else:
                criteria['all'] = False
                specialcases['maponly'] = False
            all_opcodes = findSEH(modulecriteria, criteria)
            logfile = MnLog('seh.txt')
            thislog = logfile.reset()
            processResults(all_opcodes, logfile, thislog, specialcases)

        def procShowMODULES(args):
            modulecriteria = {}
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            modulestosearch = getModulesToQuery(modulecriteria)
            showModuleTable('', modulestosearch)

        def procFindROPFUNC(args):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            ropfuncs = {}
            ropfuncoffsets = {}
            (ropfuncs, ropfuncoffsets) = findROPFUNC(modulecriteria, criteria)
            dbg.log('[+] Processing pointers to interesting rop functions')
            logfile = MnLog('ropfunc.txt')
            thislog = logfile.reset()
            processResults(ropfuncs, logfile, thislog)
            global silent
            silent = True
            dbg.log('[+] Processing offsets to pointers to interesting rop functions')
            logfile = MnLog('ropfunc_offset.txt')
            thislog = logfile.reset()
            processResults(ropfuncoffsets, logfile, thislog)

        def procStackPivots(args):
            procROP(args, 'stackpivot')

        def procROP(args, mode='all'):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            depth = 6
            maxoffset = 40
            thedistance = 8
            split = False
            fast = False
            sortedprint = False
            endingstr = ''
            endings = []
            technique = ''
            if 'depth' in args:
                if type(args['depth']).__name__.lower() != 'bool':
                    try:
                        depth = int(args['depth'])
                    except:
                        pass
            if 'offset' in args:
                if type(args['offset']).__name__.lower() != 'bool':
                    try:
                        maxoffset = int(args['offset'])
                    except:
                        pass
            if 'distance' in args:
                if type(args['distance']).__name__.lower() != 'bool':
                    try:
                        thedistance = args['distance']
                    except:
                        pass
            if 'split' in args:
                if type(args['split']).__name__.lower() == 'bool':
                    split = args['split']
            if 's' in args:
                if type(args['s']).__name__.lower() != 'bool':
                    technique = args['s'].replace(""'"", '').replace('""', '').strip().lower()
            if 'fast' in args:
                if type(args['fast']).__name__.lower() == 'bool':
                    fast = args['fast']
            if 'end' in args:
                if type(args['end']).__name__.lower() == 'str':
                    endingstr = args['end'].replace(""'"", '').replace('""', '').strip()
                    endings = endingstr.split('#')
            if 'f' in args:
                if args['f'] != '':
                    criteria['f'] = args['f']
            if 'sort' in args:
                sortedprint = True
            if 'rva' in args:
                criteria['rva'] = True
            if mode == 'stackpivot':
                fast = False
                endings = ''
                split = False
            else:
                mode = 'all'
            findROPGADGETS(modulecriteria, criteria, endings, maxoffset, depth, split, thedistance, fast, mode, sortedprint, technique)

        def procJseh(args):
            results = []
            showred = 0
            showall = False
            if 'all' in args:
                showall = True
            nrfound = 0
            dbg.log('-----------------------------------------------------------------------')
            dbg.log('Search for jmp/call dword[ebp/esp+nn] (and other) combinations started ')
            dbg.log('-----------------------------------------------------------------------')
            opcodej = ['T$\x08', 'd$\x08', 'T$\x14', 'T$\x14', 'T$\x1c', 'T$\x1c', 'T$,', 'T$,', 'T$D', 'T$D', 'T$P', 'T$P', 'U\x0c', 'e\x0c', 'U$', 'e$', 'U0', 'e0', 'U', 'e', 'U', 'e', 'U', 'e', '\x83\x08', '\x83\x08']
            fakeptrcriteria = {}
            fakeptrcriteria['accesslevel'] = '*'
            for opjc in opcodej:
                addys = []
                addys = searchInRange([[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
                results += addys
                for ptrtypes in addys:
                    for ad1 in addys[ptrtypes]:
                        ptr = MnPointer(ad1)
                        module = ptr.belongsTo()
                        if not module:
                            module = ''
                            page = dbg.getMemoryPageByAddress(ad1)
                            access = page.getAccess(human=True)
                            op = dbg.disasm(ad1)
                            opstring = op.getDisasm()
                            dbg.log('Found %s at 0x%08x - Access: (%s) - Outside of a loaded module' % (opstring, ad1, access), address=ad1, highlight=1)
                            nrfound += 1
                        elif showall:
                            page = dbg.getMemoryPageByAddress(ad1)
                            access = page.getAccess(human=True)
                            op = dbg.disasm(ad1)
                            opstring = op.getDisasm()
                            thismod = MnModule(module)
                            if not thismod.isSafeSEH:
                                extratext = '=== Safeseh : NO ==='
                                showred = 1
                            else:
                                extratext = 'Safeseh protected'
                                showred = 0
                            dbg.log('Found %s at 0x%08x (%s) - Access: (%s) - %s' % (opstring, ad1, module, access, extratext), address=ad1, highlight=showred)
                            nrfound += 1
            dbg.log('Search complete')
            if results:
                dbg.log('Found %d address(es)' % nrfound)
                return 'Found %d address(es) (Check the log Windows for details)' % nrfound
            else:
                dbg.log('No addresses found')
                return 'Sorry, no addresses found'

        def procJOP(args, mode='all'):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            depth = 6
            if 'depth' in args:
                if type(args['depth']).__name__.lower() != 'bool':
                    try:
                        depth = int(args['depth'])
                    except:
                        pass
            findJOPGADGETS(modulecriteria, criteria, depth)

        def procCreatePATTERN(args):
            size = 0
            pattern = ''
            if '?' in args and args['?'] != '':
                try:
                    if '0x' in args['?'].lower():
                        try:
                            size = int(args['?'], 16)
                        except:
                            size = 0
                    else:
                        size = int(args['?'])
                except:
                    size = 0
            if size == 0:
                dbg.log('Please enter a valid size', highlight=1)
            else:
                pattern = createPattern(size, args)
                dbg.log('Creating cyclic pattern of %d bytes' % size)
                dbg.log(pattern)
                global ignoremodules
                ignoremodules = True
                objpatternfile = MnLog('pattern.txt')
                patternfile = objpatternfile.reset()
                objpatternfile.write('\nPattern of ' + str(size) + ' bytes :\n', patternfile)
                objpatternfile.write('-' * (19 + len(str(size))), patternfile)
                objpatternfile.write('\nASCII:', patternfile)
                objpatternfile.write('\n' + pattern, patternfile)
                patternhex = ''
                for patternchar in pattern:
                    patternhex += str(hex(ord(patternchar))).replace('0x', '\\x')
                objpatternfile.write('\n\nHEX:\n', patternfile)
                objpatternfile.write(patternhex, patternfile)
                patternjs = str2js(pattern)
                objpatternfile.write('\n\nJAVASCRIPT (unescape() friendly):\n', patternfile)
                objpatternfile.write(patternjs, patternfile)
                if not silent:
                    dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"", highlight=1)
                    dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile, highlight=1)
                ignoremodules = False
            return

        def procOffsetPATTERN(args):
            egg = ''
            if '?' in args and args['?'] != '':
                try:
                    egg = args['?']
                except:
                    egg = ''
            if egg == '':
                dbg.log('Please enter a valid target', highlight=1)
            else:
                findOffsetInPattern(egg, -1, args)
            return

        def procFileCOMPARE(args):
            modulecriteria = {}
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            allfiles = []
            tomatch = ''
            checkstrict = True
            rangeval = 0
            fast = False
            if 'ptronly' in args or 'ptrsonly' in args:
                fast = True
            if 'f' in args:
                if args['f'] != '':
                    rawfilenames = args['f'].replace('""', '')
                    allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
                    dbg.log('[+] Number of files to be examined : %d ' % len(allfiles))
            if 'range' in args:
                if not type(args['range']).__name__.lower() == 'bool':
                    strrange = args['range'].lower()
                    if strrange.startswith('0x') and len(strrange) > 2:
                        rangeval = int(strrange, 16)
                    else:
                        try:
                            rangeval = int(args['range'])
                        except:
                            rangeval = 0
                    if rangeval > 0:
                        dbg.log('[+] Find overlap using pointer +/- range, value %d' % rangeval)
                        dbg.log('    Note : this will significantly slow down the comparison process !')
                else:
                    dbg.log('Please provide a numeric value ^(> 0) with option -range', highlight=1)
                    return
            else:
                if 'contains' in args:
                    if type(args['contains']).__name__.lower() == 'str':
                        tomatch = args['contains'].replace(""'"", '').replace('""', '')
                if 'nostrict' in args:
                    if type(args['nostrict']).__name__.lower() == 'bool':
                        checkstrict = not args['nostrict']
                        dbg.log('[+] Instructions must match in all files ? %s' % checkstrict)
            callfiles = allfiles
            allfiles = []
            for tfile in callfiles:
                if os.path.isdir(tfile):
                    for (root, dirs, files) in os.walk(tfile):
                        for dfile in files:
                            allfiles.append(os.path.join(root, dfile))
                else:
                    allfiles.append(tfile)
            if len(allfiles) > 1:
                findFILECOMPARISON(modulecriteria, criteria, allfiles, tomatch, checkstrict, rangeval, fast)
            else:
                dbg.log('Please specify at least 2 filenames to compare', highlight=1)

        def procFind(args):
            modulecriteria = {}
            criteria = {}
            pattern = ''
            base = 0
            offset = 0
            top = TOP_USERLAND
            consecutive = False
            ftype = ''
            level = 0
            offsetlevel = 0
            if not 'a' in args:
                args['a'] = '*'
            ptronly = False
            if 'ptronly' in args or 'ptrsonly' in args:
                ptronly = True
            if not 'x' in args:
                args['x'] = '*'
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            if criteria['accesslevel'] == '':
                return
            if not 's' in args:
                dbg.log('-s <search pattern (or filename)> is a mandatory argument', highlight=1)
                return
            pattern = args['s']
            if 'unicode' in args:
                criteria['unic'] = True
            if 'b' in args:
                try:
                    base = int(args['b'], 16)
                except:
                    dbg.log('invalid base address: %s' % args['b'], highlight=1)
                    return
            if 't' in args:
                try:
                    top = int(args['t'], 16)
                except:
                    dbg.log('invalid top address: %s' % args['t'], highlight=1)
                    return
            if 'offset' in args:
                if not args['offset'].__class__.__name__ == 'bool':
                    if '0x' in args['offset'].lower():
                        try:
                            offset = 0 - int(args['offset'], 16)
                        except:
                            dbg.log('invalid offset value', highlight=1)
                            return
                    else:
                        try:
                            offset = 0 - int(args['offset'])
                        except:
                            dbg.log('invalid offset value', highlight=1)
                            return
                else:
                    dbg.log('invalid offset value', highlight=1)
                    return
            if 'level' in args:
                try:
                    level = int(args['level'])
                except:
                    dbg.log('invalid level value', highlight=1)
                    return
            if 'offsetlevel' in args:
                try:
                    offsetlevel = int(args['offsetlevel'])
                except:
                    dbg.log('invalid offsetlevel value', highlight=1)
                    return
            if 'c' in args:
                dbg.log('    - Skipping consecutive pointers, showing size instead')
                consecutive = True
            if 'type' in args:
                if not args['type'] in ['bin', 'asc', 'ptr', 'instr', 'file']:
                    dbg.log('Invalid search type : %s' % args['type'], highlight=1)
                    return
                ftype = args['type']
                if ftype == 'file':
                    filename = args['s'].replace('""', '').replace(""'"", '')
                    if not os.path.isfile(filename):
                        dbg.log('Unable to find/read file %s' % filename, highlight=1)
                        return
            rangep2p = 0
            if 'p2p' in args or level > 0:
                dbg.log('    - Looking for pointers to pointers')
                criteria['p2p'] = True
                if 'r' in args:
                    try:
                        rangep2p = int(args['r'])
                    except:
                        pass
                    if rangep2p > 0:
                        dbg.log('    - Will search for close pointers (%d bytes backwards)' % rangep2p)
                if 'p2p' in args:
                    level = 1
            if level > 0:
                dbg.log('    - Recursive levels : %d' % level)
            allpointers = findPattern(modulecriteria, criteria, pattern, ftype, base, top, consecutive, rangep2p, level, offset, offsetlevel)
            logfile = MnLog('find.txt')
            thislog = logfile.reset()
            processResults(allpointers, logfile, thislog, {}, ptronly)
            return

        def procFindWild(args):
            modulecriteria = {}
            criteria = {}
            pattern = ''
            patterntype = ''
            base = 0
            top = TOP_USERLAND
            (modulecriteria, criteria) = args2criteria(","for (name, value) in zip(registers_to_fill, values_to_generate_all_255_values):
    padding = ''
    if value < 16:
        padding = '0'
    if 'h' in name:
        prefix += 'mov e%sx,0x4100%s%s00; ' % (name[0], padding, hex(value)[2:])
        prefix += 'add [ebp],ch; '
        additionalLength += 8
    if 'l' in name:
        prefix += 'mov e%sx,0x4100%s%s00; ' % (buf_sig, padding, hex(value)[2:])
        prefix += 'add %s,%sh; ' % (name, buf_sig)
        prefix += 'add [ebp],ch; '
        additionalLength += 10","[""for (name, value) in zip(registers_to_fill, values_to_generate_all_255_values):\n    (name_0, *name_rnamemaining) = name\n    padding = ''\n    if value < 16:\n        padding = '0'\n    if 'h' in name:\n        prefix += 'mov e%sx,0x4100%s%s00; ' % (name_0, padding, hex(value)[2:])\n        prefix += 'add [ebp],ch; '\n        additionalLength += 8\n    if 'l' in name:\n        prefix += 'mov e%sx,0x4100%s%s00; ' % (buf_sig, padding, hex(value)[2:])\n        prefix += 'add %s,%sh; ' % (name, buf_sig)\n        prefix += 'add [ebp],ch; '\n        additionalLength += 10""]",no_found,0
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):

    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(set((i.key_number for i in midi_obj.key_signature_changes)))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start) for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(max_pos)]
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j)) for (i, j) in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)
    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for (idx, inst) in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program, note.pitch + 128 if inst.is_drum else note.pitch, enc_dur(max(1, time_to_pos(note.end - note.start))), enc_vel(note.velocity), info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) * math.log2(x / tot) for x in start_distribution))
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(start_ppl)
    encoding.sort()
    (encoding, is_major) = normalize_to_c_major(encoding)
    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]
    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry = []
    chord_int = 2
    for (chord_idx, chord) in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(boundry) >= 2, f'segement must start and end in chords: {target_chords}'
    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i] if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch + encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch + encoding[i][3] % 12, *encoding[i][4:])
    lead_notes = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    lead_chords = infer_chords_for_sequence(lead_notes, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)
    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        (src_strs, tgt_strs) = ([], [])
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for (note_idx, note) in enumerate(notes):
                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                elif dec_dur(note[4]) >= pos_resolution:
                    pitch_type = note[3] % 12
                    if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
                        src_words.append('AUT')
                    else:
                        src_words.append('HALF')
                else:
                    src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return (src_strs, tgt_strs)
    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue
        if cur_len + last_len >= target_len:
            if cur_len + last_len <= max_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    (src_strs, tgt_strs) = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment
    if max_notes >= last_len >= min_notes:
        (src_strs, tgt_strs) = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs
    assert len(src_str_list) == len(tgt_str_list)
    return (src_str_list, tgt_str_list, get_hash(encoding))","for (note_idx, note) in enumerate(notes):
    cur_pos = bar_to_pos[note[0]] + note[1]
    chord_idx = 2 * note[0]
    if note[1] >= 2 * pos_resolution:
        chord_idx += 1
    cur_chord = lead_chords[chord_idx]
    src_words.append(f'Chord_{cur_chord}')
    if note_idx != len(notes) - 1:
        nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
        if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
            pitch_type = note[3] % 12
            if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                src_words.append('AUT')
            else:
                src_words.append('HALF')
        else:
            src_words.append('NOT')
    elif dec_dur(note[4]) >= pos_resolution:
        pitch_type = note[3] % 12
        if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
            src_words.append('AUT')
        else:
            src_words.append('HALF')
    else:
        src_words.append('NOT')
    beat_idx = note[1] // pos_resolution
    beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
    src_words.append(f'BEAT_{beat_idx}')
    tgt_words.append(f'Bar_{note[0] - min_bar}')
    tgt_words.append(f'Pos_{note[1]}')
    tgt_words.append(f'Pitch_{note[3]}')
    tgt_words.append(f'Dur_{note[4]}')","[""for (note_idx, note) in enumerate(notes):\n    (note_0, note_1, *_, note_3, note_4) = note\n    cur_pos = bar_to_pos[note_0] + note_1\n    chord_idx = 2 * note_0\n    if note_1 >= 2 * pos_resolution:\n        chord_idx += 1\n    cur_chord = lead_chords[chord_idx]\n    src_words.append(f'Chord_{cur_chord}')\n    if note_idx != len(notes) - 1:\n        nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]\n        if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note_4) >= pos_resolution:\n            pitch_type = note_3 % 12\n            if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):\n                src_words.append('AUT')\n            else:\n                src_words.append('HALF')\n        else:\n            src_words.append('NOT')\n    elif dec_dur(note_4) >= pos_resolution:\n        pitch_type = note_3 % 12\n        if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:\n            src_words.append('AUT')\n        else:\n            src_words.append('HALF')\n    else:\n        src_words.append('NOT')\n    beat_idx = note_1 // pos_resolution\n    beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)\n    src_words.append(f'BEAT_{beat_idx}')\n    tgt_words.append(f'Bar_{note_0 - min_bar}')\n    tgt_words.append(f'Pos_{note_1}')\n    tgt_words.append(f'Pitch_{note_3}')\n    tgt_words.append(f'Dur_{note_4}')""]",no_found,0
meld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meld/meld/matchers/merge.py,https://github.com/GNOME/meld/tree/master/meld/matchers/merge.py,AutoMergeDiffer,_auto_merge$32,"def _auto_merge(self, using, texts):
    for (out0, out1) in super()._auto_merge(using, texts):
        if self.auto_merge and out0[0] == 'conflict':
            (l0, h0, l1, h1, l2, h2) = (out0[3], out0[4], out0[1], out0[2], out1[3], out1[4])
            len0 = h0 - l0
            len1 = h1 - l1
            len2 = h2 - l2
            if (len0 > 0 and len2 > 0) and (len0 == len1 or len2 == len1 or len1 == 0):
                matcher = self._matcher(None, texts[0][l0:h0], texts[2][l2:h2])
                for chunk in matcher.get_opcodes():
                    s1 = l1
                    e1 = l1
                    if len0 == len1:
                        s1 += chunk[1]
                        e1 += chunk[2]
                    elif len2 == len1:
                        s1 += chunk[3]
                        e1 += chunk[4]
                    out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
                    out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
                    if chunk[0] == 'equal':
                        out0 = ('replace',) + out0_bounds
                        out1 = ('replace',) + out1_bounds
                        yield (out0, out1)
                    else:
                        out0 = ('conflict',) + out0_bounds
                        out1 = ('conflict',) + out1_bounds
                        yield (out0, out1)
                return
            else:
                chunktype = using[0][0][0]
                for chunkarr in using:
                    for chunk in chunkarr:
                        if chunk[0] != chunktype:
                            chunktype = None
                            break
                    if not chunktype:
                        break
                if chunktype == 'delete':
                    seq0 = seq1 = None
                    while 1:
                        if seq0 is None:
                            try:
                                seq0 = using[0].pop(0)
                                i0 = seq0[1]
                                end0 = seq0[4]
                            except IndexError:
                                break
                        if seq1 is None:
                            try:
                                seq1 = using[1].pop(0)
                                i1 = seq1[1]
                                end1 = seq1[4]
                            except IndexError:
                                break
                        highstart = max(i0, i1)
                        if i0 != i1:
                            out0 = ('conflict', i0 - highstart + i1, highstart, seq0[3] - highstart + i1, seq0[3])
                            out1 = ('conflict', i1 - highstart + i0, highstart, seq1[3] - highstart + i0, seq1[3])
                            yield (out0, out1)
                        lowend = min(seq0[2], seq1[2])
                        if highstart != lowend:
                            out0 = ('delete', highstart, lowend, seq0[3], seq0[4])
                            out1 = ('delete', highstart, lowend, seq1[3], seq1[4])
                            yield (out0, out1)
                        i0 = i1 = lowend
                        if lowend == seq0[2]:
                            seq0 = None
                        if lowend == seq1[2]:
                            seq1 = None
                    if seq0:
                        out0 = ('conflict', i0, seq0[2], seq0[3], seq0[4])
                        out1 = ('conflict', i0, seq0[2], end1, end1 + seq0[2] - i0)
                        yield (out0, out1)
                    elif seq1:
                        out0 = ('conflict', i1, seq1[2], end0, end0 + seq1[2] - i1)
                        out1 = ('conflict', i1, seq1[2], seq1[3], seq1[4])
                        yield (out0, out1)
                    return
        yield (out0, out1)","for (out0, out1) in super()._auto_merge(using, texts):
    if self.auto_merge and out0[0] == 'conflict':
        (l0, h0, l1, h1, l2, h2) = (out0[3], out0[4], out0[1], out0[2], out1[3], out1[4])
        len0 = h0 - l0
        len1 = h1 - l1
        len2 = h2 - l2
        if (len0 > 0 and len2 > 0) and (len0 == len1 or len2 == len1 or len1 == 0):
            matcher = self._matcher(None, texts[0][l0:h0], texts[2][l2:h2])
            for chunk in matcher.get_opcodes():
                s1 = l1
                e1 = l1
                if len0 == len1:
                    s1 += chunk[1]
                    e1 += chunk[2]
                elif len2 == len1:
                    s1 += chunk[3]
                    e1 += chunk[4]
                out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
                out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
                if chunk[0] == 'equal':
                    out0 = ('replace',) + out0_bounds
                    out1 = ('replace',) + out1_bounds
                    yield (out0, out1)
                else:
                    out0 = ('conflict',) + out0_bounds
                    out1 = ('conflict',) + out1_bounds
                    yield (out0, out1)
            return
        else:
            chunktype = using[0][0][0]
            for chunkarr in using:
                for chunk in chunkarr:
                    if chunk[0] != chunktype:
                        chunktype = None
                        break
                if not chunktype:
                    break
            if chunktype == 'delete':
                seq0 = seq1 = None
                while 1:
                    if seq0 is None:
                        try:
                            seq0 = using[0].pop(0)
                            i0 = seq0[1]
                            end0 = seq0[4]
                        except IndexError:
                            break
                    if seq1 is None:
                        try:
                            seq1 = using[1].pop(0)
                            i1 = seq1[1]
                            end1 = seq1[4]
                        except IndexError:
                            break
                    highstart = max(i0, i1)
                    if i0 != i1:
                        out0 = ('conflict', i0 - highstart + i1, highstart, seq0[3] - highstart + i1, seq0[3])
                        out1 = ('conflict', i1 - highstart + i0, highstart, seq1[3] - highstart + i0, seq1[3])
                        yield (out0, out1)
                    lowend = min(seq0[2], seq1[2])
                    if highstart != lowend:
                        out0 = ('delete', highstart, lowend, seq0[3], seq0[4])
                        out1 = ('delete', highstart, lowend, seq1[3], seq1[4])
                        yield (out0, out1)
                    i0 = i1 = lowend
                    if lowend == seq0[2]:
                        seq0 = None
                    if lowend == seq1[2]:
                        seq1 = None
                if seq0:
                    out0 = ('conflict', i0, seq0[2], seq0[3], seq0[4])
                    out1 = ('conflict', i0, seq0[2], end1, end1 + seq0[2] - i0)
                    yield (out0, out1)
                elif seq1:
                    out0 = ('conflict', i1, seq1[2], end0, end0 + seq1[2] - i1)
                    out1 = ('conflict', i1, seq1[2], seq1[3], seq1[4])
                    yield (out0, out1)
                return
    yield (out0, out1)","[""for (out0, out1) in super()._auto_merge(using, texts):\n    (_, _, _, out1_3, out1_4, *_) = out1\n    (out0_0, out0_1, out0_2, out0_3, out0_4, *_) = out0\n    if self.auto_merge and out0_0 == 'conflict':\n        (l0, h0, l1, h1, l2, h2) = (out0_3, out0_4, out0_1, out0_2, out1_3, out1_4)\n        len0 = h0 - l0\n        len1 = h1 - l1\n        len2 = h2 - l2\n        if (len0 > 0 and len2 > 0) and (len0 == len1 or len2 == len1 or len1 == 0):\n            matcher = self._matcher(None, texts[0][l0:h0], texts[2][l2:h2])\n            for chunk in matcher.get_opcodes():\n                s1 = l1\n                e1 = l1\n                if len0 == len1:\n                    s1 += chunk[1]\n                    e1 += chunk[2]\n                elif len2 == len1:\n                    s1 += chunk[3]\n                    e1 += chunk[4]\n                out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])\n                out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])\n                if chunk[0] == 'equal':\n                    out0 = ('replace',) + out0_bounds\n                    out1 = ('replace',) + out1_bounds\n                    yield (out0, out1)\n                else:\n                    out0 = ('conflict',) + out0_bounds\n                    out1 = ('conflict',) + out1_bounds\n                    yield (out0, out1)\n            return\n        else:\n            chunktype = using[0][0][0]\n            for chunkarr in using:\n                for chunk in chunkarr:\n                    if chunk[0] != chunktype:\n                        chunktype = None\n                        break\n                if not chunktype:\n                    break\n            if chunktype == 'delete':\n                seq0 = seq1 = None\n                while 1:\n                    if seq0 is None:\n                        try:\n                            seq0 = using[0].pop(0)\n                            i0 = seq0[1]\n                            end0 = seq0[4]\n                        except IndexError:\n                            break\n                    if seq1 is None:\n                        try:\n                            seq1 = using[1].pop(0)\n                            i1 = seq1[1]\n                            end1 = seq1[4]\n                        except IndexError:\n                            break\n                    highstart = max(i0, i1)\n                    if i0 != i1:\n                        out0 = ('conflict', i0 - highstart + i1, highstart, seq0[3] - highstart + i1, seq0[3])\n                        out1 = ('conflict', i1 - highstart + i0, highstart, seq1[3] - highstart + i0, seq1[3])\n                        yield (out0, out1)\n                    lowend = min(seq0[2], seq1[2])\n                    if highstart != lowend:\n                        out0 = ('delete', highstart, lowend, seq0[3], seq0[4])\n                        out1 = ('delete', highstart, lowend, seq1[3], seq1[4])\n                        yield (out0, out1)\n                    i0 = i1 = lowend\n                    if lowend == seq0[2]:\n                        seq0 = None\n                    if lowend == seq1[2]:\n                        seq1 = None\n                if seq0:\n                    out0 = ('conflict', i0, seq0[2], seq0[3], seq0[4])\n                    out1 = ('conflict', i0, seq0[2], end1, end1 + seq0[2] - i0)\n                    yield (out0, out1)\n                elif seq1:\n                    out0 = ('conflict', i1, seq1[2], end0, end0 + seq1[2] - i1)\n                    out1 = ('conflict', i1, seq1[2], seq1[3], seq1[4])\n                    yield (out0, out1)\n                return\n    yield (out0, out1)""]",no_found,0
spaCy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spaCy/spacy/tests/pipeline/test_tok2vec.py,https://github.com/explosion/spaCy/tree/master/spacy/tests/pipeline/test_tok2vec.py,,test_tok2vec_listener$143,"def test_tok2vec_listener():
    orig_config = Config().from_str(cfg_string)
    nlp = util.load_model_from_config(orig_config, auto_fill=True, validate=True)
    assert nlp.pipe_names == ['tok2vec', 'tagger']
    tagger = nlp.get_pipe('tagger')
    tok2vec = nlp.get_pipe('tok2vec')
    tagger_tok2vec = tagger.model.get_ref('tok2vec')
    assert isinstance(tok2vec, Tok2Vec)
    assert isinstance(tagger_tok2vec, Tok2VecListener)
    train_examples = []
    for t in TRAIN_DATA:
        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
        for tag in t[1]['tags']:
            tagger.add_label(tag)
    assert tok2vec.listeners == []
    optimizer = nlp.initialize(lambda : train_examples)
    assert tok2vec.listeners == [tagger_tok2vec]
    for i in range(5):
        losses = {}
        nlp.update(train_examples, sgd=optimizer, losses=losses)
    doc = nlp('Running the pipeline as a whole.')
    doc_tensor = tagger_tok2vec.predict([doc])[0]
    ops = get_current_ops()
    assert_array_equal(ops.to_numpy(doc.tensor), ops.to_numpy(doc_tensor))
    nlp.select_pipes(disable='tok2vec')
    assert nlp.pipe_names == ['tagger']
    nlp('Running the pipeline with the Tok2Vec component disabled.')","for t in TRAIN_DATA:
    train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
    for tag in t[1]['tags']:
        tagger.add_label(tag)","[""for (t_0, t_1, *t_len) in TRAIN_DATA:\n    train_examples.append(Example.from_dict(nlp.make_doc(\n    t_0), \n    t_1))\n    for tag in \n    t_1['tags']:\n        tagger.add_label(tag)""]",no_found,0
spacy-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spacy-transformers/spacy_transformers/tests/test_pipeline_component.py,https://github.com/explosion/spacy-transformers/tree/master/spacy_transformers/tests/test_pipeline_component.py,,test_transformer_pipeline_tagger_senter_listener$181,"def test_transformer_pipeline_tagger_senter_listener():
    """"""Test that a pipeline with just a transformer+tagger+senter runs and
    trains properly""""""
    orig_config = Config().from_str(cfg_string)
    nlp = util.load_model_from_config(orig_config, auto_fill=True, validate=True)
    assert nlp.pipe_names == ['transformer', 'tagger', 'senter']
    tagger = nlp.get_pipe('tagger')
    transformer = nlp.get_pipe('transformer')
    tagger_trf = tagger.model.get_ref('tok2vec').layers[0]
    assert isinstance(transformer, Transformer)
    assert isinstance(tagger_trf, TransformerListener)
    assert tagger_trf.upstream_name == 'custom_upstream'
    train_examples = []
    for t in TRAIN_DATA:
        train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
        for tag in t[1]['tags']:
            tagger.add_label(tag)
    assert transformer.listeners == []
    optimizer = nlp.initialize(lambda : train_examples)
    assert tagger_trf in transformer.listeners
    for i in range(2):
        losses = {}
        nlp.update(train_examples, sgd=optimizer, losses=losses)
    text = ""We're interested at underwater basket weaving.""
    doc = nlp(text)
    doc_tensor = tagger_trf.predict([doc])
    _assert_equal_tensors(doc._.trf_data.tensors, doc_tensor[0].tensors)
    with make_tempdir() as d:
        file_path = d / 'trained_nlp'
        nlp.to_disk(file_path)
        nlp2 = util.load_model_from_path(file_path)
        doc2 = nlp2(text)
        tagger2 = nlp2.get_pipe('tagger')
        tagger_trf2 = tagger2.model.get_ref('tok2vec').layers[0]
        doc_tensor2 = tagger_trf2.predict([doc2])
        _assert_equal_tensors(doc_tensor2[0].tensors, doc_tensor[0].tensors)
        file_path_2 = d / 'trained_nlp_2'
        nlp2.to_disk(file_path_2)
    nlp_bytes = nlp.to_bytes()
    nlp3 = util.load_model_from_config(orig_config, auto_fill=True, validate=True)
    nlp3.from_bytes(nlp_bytes)
    doc3 = nlp3(text)
    tagger3 = nlp3.get_pipe('tagger')
    tagger_trf3 = tagger3.model.get_ref('tok2vec').layers[0]
    doc_tensor3 = tagger_trf3.predict([doc3])
    _assert_equal_tensors(doc_tensor3[0].tensors, doc_tensor[0].tensors)","for t in TRAIN_DATA:
    train_examples.append(Example.from_dict(nlp.make_doc(t[0]), t[1]))
    for tag in t[1]['tags']:
        tagger.add_label(tag)","[""for (t_0, t_1, *t_len) in TRAIN_DATA:\n    train_examples.append(Example.from_dict(nlp.make_doc(\n    t_0), \n    t_1))\n    for tag in \n    t_1['tags']:\n        tagger.add_label(tag)""]",no_found,0
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/incubate/fleet/tests/ctr_dataset_reader.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/incubate/fleet/tests/ctr_dataset_reader.py,,load_lr_input_record$54,"def load_lr_input_record(sent):
    res = []
    for _ in [x.split(':') for x in sent.split()]:
        res.append(int(_[0]))
    return res","for _ in [x.split(':') for x in sent.split()]:
    res.append(int(_[0]))","[""for _ in [x.split(':') for x in sent.split()]:\n    (__0, *__r_maining) = _\n    res.append(int(__0))"", ""for (__0, *__len) in [x.split(':') for x in sent.split()]:\n    res.append(int(__0))""]",no_found,0
qutip,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutip/qutip/legacy/bloch_redfield.py,https://github.com/qutip/qutip/tree/master/qutip/legacy/bloch_redfield.py,,bloch_redfield_tensor$21,"def bloch_redfield_tensor(H, a_ops, spectra_cb=None, c_ops=[], use_secular=True, sec_cutoff=0.1):
    """"""
    Calculate the Bloch-Redfield tensor for a system given a set of operators
    and corresponding spectral functions that describes the system's coupling
    to its environment.

    .. note::

        This tensor generation requires a time-independent Hamiltonian.

    Parameters
    ----------

    H : :class:`qutip.qobj`
        System Hamiltonian.

    a_ops : list of :class:`qutip.qobj`
        List of system operators that couple to the environment.

    spectra_cb : list of callback functions
        List of callback functions that evaluate the noise power spectrum
        at a given frequency.

    c_ops : list of :class:`qutip.qobj`
        List of system collapse operators.

    use_secular : bool
        Flag (True of False) that indicates if the secular approximation should
        be used.
    
    sec_cutoff : float {0.1}
        Threshold for secular approximation.

    Returns
    -------

    R, kets: :class:`qutip.Qobj`, list of :class:`qutip.Qobj`

        R is the Bloch-Redfield tensor and kets is a list eigenstates of the
        Hamiltonian.

    """"""
    if not spectra_cb is None:
        warnings.warn('The use of spectra_cb is depreciated.', DeprecationWarning)
        _a_ops = []
        for (kk, a) in enumerate(a_ops):
            _a_ops.append([a, spectra_cb[kk]])
        a_ops = _a_ops
    if not isinstance(H, Qobj):
        raise TypeError('H must be an instance of Qobj')
    for a in a_ops:
        if not isinstance(a[0], Qobj) or not a[0].isherm:
            raise TypeError('Operators in a_ops must be Hermitian Qobj.')
    if c_ops is None:
        c_ops = []
    (evals, ekets) = H.eigenstates()
    N = len(evals)
    K = len(a_ops)
    if K == 0:
        Heb = qdiags(evals, 0, dims=H.dims)
        L = liouvillian(Heb, c_ops=[c_op.transform(ekets) for c_op in c_ops])
        return (L, ekets)
    A = np.array([a_ops[k][0].transform(ekets).full() for k in range(K)])
    Jw = np.zeros((K, N, N), dtype=complex)
    W = np.real(evals[:, np.newaxis] - evals[np.newaxis, :])
    for k in range(K):
        for n in range(N):
            for m in range(N):
                Jw[k, n, m] = a_ops[k][1](W[n, m])
    dw_min = np.abs(W[W.nonzero()]).min()
    Iabs = np.empty((N * N, 3), dtype=int)
    for (I, Iab) in enumerate(Iabs):
        Iab[0] = I
        Iab[1:] = vec2mat_index(N, I)
    Heb = qdiags(evals, 0, dims=H.dims)
    L = liouvillian(Heb, c_ops=[c_op.transform(ekets) for c_op in c_ops])
    rows = []
    cols = []
    data = []
    for (I, a, b) in Iabs:
        if use_secular:
            Jcds = Iabs[np.where(np.abs(W[a, b] - W[Iabs[:, 1], Iabs[:, 2]]) < dw_min * sec_cutoff)]
        else:
            Jcds = Iabs
        for (J, c, d) in Jcds:
            elem = 0 + 0j
            elem += 0.5 * np.sum(A[:, a, c] * A[:, d, b] * (Jw[:, c, a] + Jw[:, d, b]))
            if b == d:
                elem -= 0.5 * np.sum(A[:, a, :] * A[:, :, c] * Jw[:, c, :])
            if a == c:
                elem -= 0.5 * np.sum(A[:, d, :] * A[:, :, b] * Jw[:, d, :])
            if elem != 0:
                rows.append(I)
                cols.append(J)
                data.append(elem)
    R = arr_coo2fast(np.array(data, dtype=complex), np.array(rows, dtype=np.int32), np.array(cols, dtype=np.int32), N ** 2, N ** 2)
    L.data = L.data + R
    return (L, ekets)","for a in a_ops:
    if not isinstance(a[0], Qobj) or not a[0].isherm:
        raise TypeError('Operators in a_ops must be Hermitian Qobj.')","[""for a in a_ops:\n    (a_0, *a_ramaining) = a\n    if not isinstance(a_0, Qobj) or not a_0.isherm:\n        raise TypeError('Operators in a_ops must be Hermitian Qobj.')"", ""for (a_0, *a_len) in a_ops:\n    if not isinstance(\n    a_0, Qobj) or not \n    a_0.isherm:\n        raise TypeError('Operators in a_ops must be Hermitian Qobj.')""]",no_found,0
mssql-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mssql-cli/tests/test_sqlcompletion.py,https://github.com/dbcli/mssql-cli/tree/master/tests/test_sqlcompletion.py,SqlCompletionTests,test_distinct_and_order_by_suggestions_with_aliases$264,"def test_distinct_and_order_by_suggestions_with_aliases():
    test_args = [('SELECT DISTINCT FROM tbl x JOIN tbl1 y', 'SELECT DISTINCT', 'SELECT'), ('SELECT * FROM tbl x JOIN tbl1 y ORDER BY ', 'SELECT * FROM tbl x JOIN tbl1 y ORDER BY ', 'ORDER BY')]
    for arg in test_args:
        text = arg[0]
        text_before = arg[1]
        last_keyword = arg[2]
        suggestions = suggest_type(text, text_before)
        assert set(suggestions) == set([Column(table_refs=(TableReference(None, 'tbl', 'x', False), TableReference(None, 'tbl1', 'y', False)), local_tables=(), qualifiable=True), Function(schema=None), Keyword(last_keyword)])","for arg in test_args:
    text = arg[0]
    text_before = arg[1]
    last_keyword = arg[2]
    suggestions = suggest_type(text, text_before)
    assert set(suggestions) == set([Column(table_refs=(TableReference(None, 'tbl', 'x', False), TableReference(None, 'tbl1', 'y', False)), local_tables=(), qualifiable=True), Function(schema=None), Keyword(last_keyword)])","[""for arg in test_args:\n    (arg_0, arg_1, arg_2, *_) = arg\n    text = arg_0\n    text_before = arg_1\n    last_keyword = arg_2\n    suggestions = suggest_type(text, text_before)\n    assert set(suggestions) == set([Column(table_refs=(TableReference(None, 'tbl', 'x', False), TableReference(None, 'tbl1', 'y', False)), local_tables=(), qualifiable=True), Function(schema=None), Keyword(last_keyword)])"", ""for (arg_0, arg_1, arg_2, *arg_len) in test_args:\n    text = \n    arg_0\n    text_before = \n    arg_1\n    last_keyword = \n    arg_2\n    suggestions = suggest_type(text, text_before)\n    assert set(suggestions) == set([Column(table_refs=(TableReference(None, 'tbl', 'x', False), TableReference(None, 'tbl1', 'y', False)), local_tables=(), qualifiable=True), Function(schema=None), Keyword(last_keyword)])""]",no_found,0
fonttools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fonttools/Lib/fontTools/cffLib/specializer.py,https://github.com/fonttools/fonttools/tree/master/Lib/fontTools/cffLib/specializer.py,_GeneralizerDecombinerCommandsMap,hhcurveto$200,"def hhcurveto(args):
    if len(args) < 4 or len(args) % 4 > 1:
        raise ValueError(args)
    if len(args) % 2 == 1:
        yield ('rrcurveto', [args[1], args[0], args[2], args[3], args[4], 0])
        args = args[5:]
    for args in _everyN(args, 4):
        yield ('rrcurveto', [args[0], 0, args[1], args[2], args[3], 0])","for args in _everyN(args, 4):
    yield ('rrcurveto', [args[0], 0, args[1], args[2], args[3], 0])","[""for args in _everyN(args, 4):\n    (args_0, args_1, args_2, args_3, *_) = args\n    yield ('rrcurveto', [args_0, 0, args_1, args_2, args_3, 0])"", ""for (args_0, args_1, args_2, args_3, *args_len) in _everyN(args, 4):\n    yield ('rrcurveto', [args_0, 0, args_1, args_2, args_3, 0])""]",no_found,0
edx-platform,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/lms/djangoapps/grades/management/commands/compute_grades.py,https://github.com/edx/edx-platform/tree/master/lms/djangoapps/grades/management/commands/compute_grades.py,Command,_shuffled_task_kwargs$92,"def _shuffled_task_kwargs(self, options):
    """"""
        Iterate over all task keyword arguments in random order.

        Randomizing them will help even out the load on the task workers,
        though it will not entirely prevent the possibility of spikes.  It will
        also make the overall time to completion more predictable.
        """"""
    all_args = []
    estimate_first_attempted = options['estimate_first_attempted']
    for course_key in self._get_course_keys(options):
        for task_arg_tuple in tasks._course_task_args(course_key, **options):
            all_args.append(task_arg_tuple)
    all_args.sort(key=lambda x: hashlib.md5(f'{x!r}'.encode('utf-8')).digest())
    for args in all_args:
        yield {'course_key': args[0], 'offset': args[1], 'batch_size': args[2], 'estimate_first_attempted': estimate_first_attempted}","for args in all_args:
    yield {'course_key': args[0], 'offset': args[1], 'batch_size': args[2], 'estimate_first_attempted': estimate_first_attempted}","[""for args in all_args:\n    (args_0, args_1, args_2, *_) = args\n    yield {'course_key': args_0, 'offset': args_1, 'batch_size': args_2, 'estimate_first_attempted': estimate_first_attempted}"", ""for (args_0, args_1, args_2, *args_len) in all_args:\n    yield {'course_key': args_0, 'offset': args_1, 'batch_size': args_2, 'estimate_first_attempted': estimate_first_attempted}""]",no_found,0
LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/pedestrian_detection/data_provider_farm/reformat_caltech.py,https://github.com/YonghaoHe/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/tree/master/pedestrian_detection/data_provider_farm/reformat_caltech.py,,generate_data_list$8,"def generate_data_list():
    annotation_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_new_annotations/anno_test_1xnew'
    image_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_data/extracted_data'
    list_file_path = './data_folder/data_list_caltech_test.txt'
    if not os.path.exists(os.path.dirname(list_file_path)):
        os.makedirs(os.path.dirname(list_file_path))
    fout = open(list_file_path, 'w')
    counter = 0
    for (parent, dirnames, filenames) in os.walk(annotation_root):
        for filename in filenames:
            if not filename.endswith('.txt'):
                continue
            filename_splits = filename[:-4].split('_')
            set_name = filename_splits[0]
            seq_name = filename_splits[1]
            img_name = filename_splits[2]
            img_path = os.path.join(image_root, set_name, seq_name, 'images', img_name)
            if not os.path.exists(img_path):
                print('The corresponding image does not exist! [%s]' % img_path)
                continue
            line = img_path
            fin_anno = open(os.path.join(parent, filename), 'r')
            bbox_list = []
            for (i, anno) in enumerate(fin_anno):
                if i == 0:
                    continue
                anno = anno.strip('\n').split(' ')
                if anno[0] != 'person':
                    continue
                x = math.floor(float(anno[1]))
                y = math.floor(float(anno[2]))
                width = math.ceil(float(anno[3]))
                height = math.ceil(float(anno[4]))
                width_vis = math.ceil(float(anno[8]))
                height_vis = math.ceil(float(anno[9]))
                if width_vis * height_vis / (width * height) < 0.2:
                    continue
                bbox_list.append((x, y, width, height))
            if len(bbox_list) == 0:
                line += ',0,0'
                fout.write(line + '\n')
            else:
                bbox_line = ''
                for bbox in bbox_list:
                    bbox_line += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
                line += ',1,' + str(len(bbox_list)) + bbox_line
                fout.write(line + '\n')
            counter += 1
            print(counter)
    fout.close()","for bbox in bbox_list:
    bbox_line += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])","[""for bbox in bbox_list:\n    (bbox_0, bbox_1, bbox_2, bbox_3, *_) = bbox\n    bbox_line += ',' + str(bbox_0) + ',' + str(bbox_1) + ',' + str(bbox_2) + ',' + str(bbox_3)"", ""for (bbox_0, bbox_1, bbox_2, bbox_3, *bbox_len) in bbox_list:\n    bbox_line += ',' + str(bbox_0) + ',' + str(bbox_1) + ',' + str(bbox_2) + ',' + str(bbox_3)""]",no_found,0
LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/head_detection/data_provider_farm/reformat_brainwash.py,https://github.com/YonghaoHe/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/tree/master/head_detection/data_provider_farm/reformat_brainwash.py,,generate_data_list$12,"def generate_data_list():
    txt_file_path = '/media/heyonghao/HYH-4T-WD/public_dataset/head_detection/brainwash/brainwash/brainwash_test.idl'
    image_root = '/media/heyonghao/HYH-4T-WD/public_dataset/head_detection/brainwash/brainwash'
    list_file_path = './data_folder/data_list_brainwash_test.txt'
    if not os.path.exists(os.path.dirname(list_file_path)):
        os.makedirs(os.path.dirname(list_file_path))
    fin = open(txt_file_path, 'r')
    fout = open(list_file_path, 'w')
    counter = 0
    for line in fin:
        line = line.strip(';\n')
        im_path = re.findall('[""](.*?)[""]', line)[0]
        im_path = os.path.join(image_root, im_path)
        if not os.path.exists(im_path):
            print('im file does not exist : %s' % im_path)
            continue
        bbox_str_list = re.findall('[(](.*?)[)]', line)
        bbox_list = []
        for bbox_str in bbox_str_list:
            bbox_str = bbox_str.split(', ')
            xmin = int(float(bbox_str[0]))
            ymin = int(float(bbox_str[1]))
            xmax = int(float(bbox_str[2]))
            ymax = int(float(bbox_str[3]))
            bbox_list.append((xmin, ymin, xmax - xmin + 1, ymax - ymin + 1))
        if len(bbox_list) == 0:
            line_str = im_path + ',0,0'
            fout.write(line_str + '\n')
        else:
            line_str = im_path + ',1,' + str(len(bbox_list))
            for bbox in bbox_list:
                line_str += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
            fout.write(line_str + '\n')
        counter += 1
        print(counter)
    fout.close()
    fin.close()","for bbox in bbox_list:
    line_str += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])","[""for bbox in bbox_list:\n    (bbox_0, bbox_1, bbox_2, bbox_3, *_) = bbox\n    line_str += ',' + str(bbox_0) + ',' + str(bbox_1) + ',' + str(bbox_2) + ',' + str(bbox_3)"", ""for (bbox_0, bbox_1, bbox_2, bbox_3, *bbox_len) in bbox_list:\n    line_str += ',' + str(bbox_0) + ',' + str(bbox_1) + ',' + str(bbox_2) + ',' + str(bbox_3)""]",no_found,0
LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/head_detection/data_provider_farm/reformat_brainwash.py,https://github.com/YonghaoHe/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/tree/master/head_detection/data_provider_farm/reformat_brainwash.py,,generate_data_list$12,"def generate_data_list():
    txt_file_path = '/media/heyonghao/HYH-4T-WD/public_dataset/head_detection/brainwash/brainwash/brainwash_test.idl'
    image_root = '/media/heyonghao/HYH-4T-WD/public_dataset/head_detection/brainwash/brainwash'
    list_file_path = './data_folder/data_list_brainwash_test.txt'
    if not os.path.exists(os.path.dirname(list_file_path)):
        os.makedirs(os.path.dirname(list_file_path))
    fin = open(txt_file_path, 'r')
    fout = open(list_file_path, 'w')
    counter = 0
    for line in fin:
        line = line.strip(';\n')
        im_path = re.findall('[""](.*?)[""]', line)[0]
        im_path = os.path.join(image_root, im_path)
        if not os.path.exists(im_path):
            print('im file does not exist : %s' % im_path)
            continue
        bbox_str_list = re.findall('[(](.*?)[)]', line)
        bbox_list = []
        for bbox_str in bbox_str_list:
            bbox_str = bbox_str.split(', ')
            xmin = int(float(bbox_str[0]))
            ymin = int(float(bbox_str[1]))
            xmax = int(float(bbox_str[2]))
            ymax = int(float(bbox_str[3]))
            bbox_list.append((xmin, ymin, xmax - xmin + 1, ymax - ymin + 1))
        if len(bbox_list) == 0:
            line_str = im_path + ',0,0'
            fout.write(line_str + '\n')
        else:
            line_str = im_path + ',1,' + str(len(bbox_list))
            for bbox in bbox_list:
                line_str += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
            fout.write(line_str + '\n')
        counter += 1
        print(counter)
    fout.close()
    fin.close()","for bbox_str in bbox_str_list:
    bbox_str = bbox_str.split(', ')
    xmin = int(float(bbox_str[0]))
    ymin = int(float(bbox_str[1]))
    xmax = int(float(bbox_str[2]))
    ymax = int(float(bbox_str[3]))
    bbox_list.append((xmin, ymin, xmax - xmin + 1, ymax - ymin + 1))","[""for bbox_str in bbox_str_list:\n    (bbox_str_0, bbox_str_1, bbox_str_2, bbox_str_3, *_) = bbox_str\n    bbox_str = bbox_str.split(', ')\n    xmin = int(float(bbox_str_0))\n    ymin = int(float(bbox_str_1))\n    xmax = int(float(bbox_str_2))\n    ymax = int(float(bbox_str_3))\n    bbox_list.append((xmin, ymin, xmax - xmin + 1, ymax - ymin + 1))""]",no_found,
buku,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/buku/bukuserver/server.py,https://github.com/jarun/buku/tree/master/bukuserver/server.py,ApiBookmarkSearchView,get$532,"def get(self):
    arg_obj = request.args
    keywords = arg_obj.getlist('keywords')
    all_keywords = arg_obj.get('all_keywords')
    deep = arg_obj.get('deep')
    regex = arg_obj.get('regex')
    all_keywords = False if all_keywords is None else all_keywords
    deep = False if deep is None else deep
    regex = False if regex is None else regex
    all_keywords = all_keywords if isinstance(all_keywords, bool) else all_keywords.lower() == 'true'
    deep = deep if isinstance(deep, bool) else deep.lower() == 'true'
    regex = regex if isinstance(regex, bool) else regex.lower() == 'true'
    result = {'bookmarks': []}
    bukudb = getattr(flask.g, 'bukudb', get_bukudb())
    found_bookmarks = bukudb.searchdb(keywords, all_keywords, deep, regex)
    found_bookmarks = [] if found_bookmarks is None else found_bookmarks
    res = None
    if found_bookmarks is not None:
        for bookmark in found_bookmarks:
            result_bookmark = {'id': bookmark[0], 'url': bookmark[1], 'title': bookmark[2], 'tags': list(filter(lambda x: x, bookmark[3].split(','))), 'description': bookmark[4]}
            result['bookmarks'].append(result_bookmark)
    current_app.logger.debug('total bookmarks:{}'.format(len(result['bookmarks'])))
    res = jsonify(result)
    return res","for bookmark in found_bookmarks:
    result_bookmark = {'id': bookmark[0], 'url': bookmark[1], 'title': bookmark[2], 'tags': list(filter(lambda x: x, bookmark[3].split(','))), 'description': bookmark[4]}
    result['bookmarks'].append(result_bookmark)","[""for bookmark in found_bookmarks:\n    (bookmark_0, bookmark_1, bookmark_2, bookmark_3, bookmark_4, *_) = bookmark\n    result_bookmark = {'id': bookmark_0, 'url': bookmark_1, 'title': bookmark_2, 'tags': list(filter(lambda x: x, bookmark_3.split(','))), 'description': bookmark_4}\n    result['bookmarks'].append(result_bookmark)"", ""for (bookmark_0, bookmark_1, bookmark_2, bookmark_3, bookmark_4, *bookmark_len) in found_bookmarks:\n    result_bookmark = {'id': bookmark_0, 'url': bookmark_1, 'title': bookmark_2, 'tags': list(filter(lambda x: x, bookmark_3.split(','))), 'description': bookmark_4}\n    result['bookmarks'].append(result_bookmark)""]",no_found,0
yolov5-face,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yolov5-face/test_widerface.py,https://github.com/deepcam-cn/yolov5-face/tree/master//test_widerface.py,,if_main_my$113,"if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default='runs/train/exp5/weights/last.pt', help='model.pt path(s)')
    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf-thres', type=float, default=0.02, help='object confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.5, help='IOU threshold for NMS')
    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')
    parser.add_argument('--project', default='runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--save_folder', default='./widerface_evaluate/widerface_txt/', type=str, help='Dir to save txt results')
    parser.add_argument('--dataset_folder', default='../WiderFace/val/images/', type=str, help='dataset path')
    parser.add_argument('--folder_pict', default='/yolov5-face/data/widerface/val/wider_val.txt', type=str, help='folder_pict')
    opt = parser.parse_args()
    print(opt)
    pict_folder = {}
    with open(opt.folder_pict, 'r') as f:
        lines = f.readlines()
        for line in lines:
            line = line.strip().split('/')
            pict_folder[line[-1]] = line[-2]
    device = select_device(opt.device)
    model = attempt_load(opt.weights, map_location=device)
    with torch.no_grad():
        testset_folder = opt.dataset_folder
        for image_path in tqdm(glob.glob(os.path.join(testset_folder, '*'))):
            if image_path.endswith('.txt'):
                continue
            img0 = cv2.imread(image_path)
            if img0 is None:
                print(f'ignore : {image_path}')
                continue
            boxes = detect(model, img0)
            image_name = os.path.basename(image_path)
            txt_name = os.path.splitext(image_name)[0] + '.txt'
            save_name = os.path.join(opt.save_folder, pict_folder[image_name], txt_name)
            dirname = os.path.dirname(save_name)
            if not os.path.isdir(dirname):
                os.makedirs(dirname)
            with open(save_name, 'w') as fd:
                file_name = os.path.basename(save_name)[:-4] + '\n'
                bboxs_num = str(len(boxes)) + '\n'
                fd.write(file_name)
                fd.write(bboxs_num)
                for box in boxes:
                    fd.write('%d %d %d %d %.03f' % (box[0], box[1], box[2], box[3], box[4] if box[4] <= 1 else 1) + '\n')
        print('done.')","for box in boxes:
    fd.write('%d %d %d %d %.03f' % (box[0], box[1], box[2], box[3], box[4] if box[4] <= 1 else 1) + '\n')","[""for box in boxes:\n    (box_0, box_1, box_2, box_3, box_4, *_) = box\n    fd.write('%d %d %d %d %.03f' % (box_0, box_1, box_2, box_3, box_4 if box_4 <= 1 else 1) + '\\n')"", ""for (box_0, box_1, box_2, box_3, box_4, *box_len) in boxes:\n    fd.write('%d %d %d %d %.03f' % (box_0, box_1, box_2, box_3, box_4 if box_4 <= 1 else 1) + '\\n')""]",no_found,0
PaddleOCR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleOCR/PPOCRLabel/PPOCRLabel.py,https://github.com/PaddlePaddle/PaddleOCR/tree/master/PPOCRLabel/PPOCRLabel.py,MainWindow,editBox$904,"def editBox(self):
    if not self.canvas.editing():
        return
    item = self.currentBox()
    if not item:
        return
    text = self.labelDialog.popUp(item.text())
    imageSize = str(self.image.size())
    (width, height) = (self.image.width(), self.image.height())
    if text:
        try:
            text_list = eval(text)
        except:
            msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Please enter the correct format')
            msg_box.exec_()
            return
        if len(text_list) < 4:
            msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Please enter the coordinates of 4 points')
            msg_box.exec_()
            return
        for box in text_list:
            if box[0] > width or box[0] < 0 or box[1] > height or (box[1] < 0):
                msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Out of picture size')
                msg_box.exec_()
                return
        item.setText(text)
        self.setDirty()
        self.updateComboBox()","for box in text_list:
    if box[0] > width or box[0] < 0 or box[1] > height or (box[1] < 0):
        msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Out of picture size')
        msg_box.exec_()
        return","[""for box in text_list:\n    (box_0, box_1, *_) = box\n    if box_0 > width or box_0 < 0 or box_1 > height or (box_1 < 0):\n        msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Out of picture size')\n        msg_box.exec_()\n        return"", ""for (box_0, box_1, *box_len) in text_list:\n    if \n    box_0 > width or \n    box_0 < 0 or \n    box_1 > height or (\n    box_1 < 0):\n        msg_box = QMessageBox(QMessageBox.Warning, 'Warning', 'Out of picture size')\n        msg_box.exec_()\n        return""]",no_found,0
parliament,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/parliament/parliament/statement.py,https://github.com/duo-labs/parliament/tree/master/parliament/statement.py,Statement,_check_condition$496,"def _check_condition(self, operator, condition_block, expanded_actions):
    """"""
        operator is something like ""StringLike""
        condition_block is something like {""s3:prefix"":[""home/${aws:username}/*""]}
        """"""
    operator_type_requirement = None
    for documented_operator in OPERATORS:
        op = documented_operator.lower()
        if operator.lower() in [op, op + 'ifexists', 'forallvalues:' + op, 'foranyvalue:' + op, 'forallvalues:' + op + 'ifexists', 'foranyvalue:' + op + 'ifexists']:
            operator_type_requirement = OPERATORS[documented_operator]
            break
    if operator_type_requirement is None:
        self.add_finding('UNKNOWN_OPERATOR', detail=operator, location=condition_block)
    if operator_type_requirement == 'Bool':
        for c in condition_block:
            value = str(c[1].value).lower()
            if value != 'true' and value != 'false':
                self.add_finding('MISMATCHED_TYPE_OPERATION_TO_NULL', location=condition_block)
                return False
    for block in condition_block:
        key = block[0]
        values = []
        for v in make_list(block[1]):
            values.append(v.value)
        if operator.lower() == 'bool':
            if key.lower() == 'aws:MultiFactorAuthPresent'.lower() and 'false' in values:
                self.add_finding('BAD_PATTERN_FOR_MFA', detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.', location=condition_block)
        elif operator.lower() == 'null':
            if key.lower == 'aws:MultiFactorAuthPresent'.lower() and 'false' in values:
                self.add_finding('BAD_PATTERN_FOR_MFA', detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.', location=condition_block)
        if operator.lower() in ['null']:
            continue
        condition_type = get_global_key_type(key)
        if condition_type:
            if not is_value_in_correct_format_for_type(condition_type, values):
                self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)
        else:
            for action_struct in expanded_actions:
                privilege_info = get_privilege_info(action_struct['service'], action_struct['action'])
                match = None
                for resource_type in privilege_info['resource_types']:
                    for condition_key in resource_type['condition_keys']:
                        if is_condition_key_match(condition_key, key):
                            match = condition_key
                if match is None:
                    self.add_finding('UNKNOWN_CONDITION_FOR_ACTION', detail='Unknown condition {} for action {}:{}'.format(key, action_struct['service'], action_struct['action']), location=condition_block)
                    continue
                condition_type = None
                for condition in privilege_info['service_conditions']:
                    if condition['condition'] == match:
                        condition_type = condition['type']
                if condition_type is None:
                    raise Exception('Action condition not found in service definition for {}'.format(match))
                if not is_value_in_correct_format_for_type(condition_type, values):
                    self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)
            if condition_type is not None:
                documenation_condition_type = translate_documentation_types(condition_type)
                if operator_type_requirement != documenation_condition_type:
                    if operator_type_requirement == 'String' and documenation_condition_type == 'Arn':
                        self.add_finding('MISMATCHED_TYPE_BUT_USABLE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)
                    else:
                        self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)
    return","for c in condition_block:
    value = str(c[1].value).lower()
    if value != 'true' and value != 'false':
        self.add_finding('MISMATCHED_TYPE_OPERATION_TO_NULL', location=condition_block)
        return False","[""for c in condition_block:\n    (_, c_1, *c_rcmaining) = c\n    value = str(c_1.value).lower()\n    if value != 'true' and value != 'false':\n        self.add_finding('MISMATCHED_TYPE_OPERATION_TO_NULL', location=condition_block)\n        return False"", ""for (c_0, c_1, *c_len) in condition_block:\n    value = str(c_1.value).lower()\n    if value != 'true' and value != 'false':\n        self.add_finding('MISMATCHED_TYPE_OPERATION_TO_NULL', location=condition_block)\n        return False""]",no_found,0
jesse,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jesse/jesse/store/state_candles.py,https://github.com/jesse-ai/jesse/tree/master/jesse/store/state_candles.py,CandlesState,time_loop_per_second$29,"def time_loop_per_second():
    if not self.are_all_initiated:
        return
    if jh.now() % 60000 != 1000:
        return
    for c in config['app']['considering_candles']:
        (exchange, symbol) = (c[0], c[1])
        current_candle = self.get_current_candle(exchange, symbol, '1m')
        if current_candle[0] <= 60000:
            continue
        if jh.now() > current_candle[0] + 60000:
            new_candle = self._generate_empty_candle_from_previous_candle(current_candle)
            self.add_candle(new_candle, exchange, symbol, '1m')","for c in config['app']['considering_candles']:
    (exchange, symbol) = (c[0], c[1])
    current_candle = self.get_current_candle(exchange, symbol, '1m')
    if current_candle[0] <= 60000:
        continue
    if jh.now() > current_candle[0] + 60000:
        new_candle = self._generate_empty_candle_from_previous_candle(current_candle)
        self.add_candle(new_candle, exchange, symbol, '1m')","[""for c in config['app']['considering_candles']:\n    (c_0, c_1, *_) = c\n    (exchange, symbol) = (c_0, c_1)\n    current_candle = self.get_current_candle(exchange, symbol, '1m')\n    if current_candle[0] <= 60000:\n        continue\n    if jh.now() > current_candle[0] + 60000:\n        new_candle = self._generate_empty_candle_from_previous_candle(current_candle)\n        self.add_candle(new_candle, exchange, symbol, '1m')"", ""for (c_0, c_1, *c_len) in config['app']['considering_candles']:\n    (exchange, symbol) = (c_0, c_1)\n    current_candle = self.get_current_candle(exchange, symbol, '1m')\n    if current_candle[0] <= 60000:\n        continue\n    if jh.now() > current_candle[0] + 60000:\n        new_candle = self._generate_empty_candle_from_previous_candle(current_candle)\n        self.add_candle(new_candle, exchange, symbol, '1m')""]",no_found,0
integrations-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/datadog_checks_dev/tests/tooling/config_validator/test_validator.py,https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/tests/tooling/config_validator/test_validator.py,,test_validate_type$69,"def test_validate_type():
    test_cases = [(create('name', 'boolean'), True), (create('name', 'string'), True), (create('name', 'integer'), True), (create('name', 'double'), True), (create('name', 'float'), True), (create('name', 'object'), True), (create('name', 'list of anything_really'), True), (create('name', 'list of something'), True), (create('name', 'dictionary'), True), (create('name', 'custom object'), False)]
    for c in test_cases:
        errors = []
        c[0]._validate_type(errors)
        if c[1]:
            assert len(errors) == 0
        else:
            type_name = c[0].param_prop.type_name
            assert len(errors) == 1
            assert errors[0].error_str == 'Type {} is not accepted'.format(type_name)","for c in test_cases:
    errors = []
    c[0]._validate_type(errors)
    if c[1]:
        assert len(errors) == 0
    else:
        type_name = c[0].param_prop.type_name
        assert len(errors) == 1
        assert errors[0].error_str == 'Type {} is not accepted'.format(type_name)","[""for c in test_cases:\n    (c_0, c_1, *_) = c\n    errors = []\n    c_0._validate_type(errors)\n    if c_1:\n        assert len(errors) == 0\n    else:\n        type_name = c_0.param_prop.type_name\n        assert len(errors) == 1\n        assert errors[0].error_str == 'Type {} is not accepted'.format(type_name)"", ""for (c_0, c_1, *c_len) in test_cases:\n    errors = []\n    \n    c_0._validate_type(errors)\n    if \n    c_1:\n        assert len(errors) == 0\n    else:\n        type_name = \n        c_0.param_prop.type_name\n        assert len(errors) == 1\n        assert errors[0].error_str == 'Type {} is not accepted'.format(type_name)""]",no_found,0
magic-wormhole,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/magic-wormhole/src/wormhole/test/test_machines.py,https://github.com/magic-wormhole/magic-wormhole/tree/master/src/wormhole/test/test_machines.py,Rendezvous,sent_messages$1601,"def sent_messages(ws):
    for c in ws.mock_calls:
        self.assertEqual(c[0], 'sendMessage', ws.mock_calls)
        self.assertEqual(c[1][1], False, ws.mock_calls)
        yield bytes_to_dict(c[1][0])","for c in ws.mock_calls:
    self.assertEqual(c[0], 'sendMessage', ws.mock_calls)
    self.assertEqual(c[1][1], False, ws.mock_calls)
    yield bytes_to_dict(c[1][0])","[""for c in ws.mock_calls:\n    (c_0, (c_1_0, c_1_1, *c_1_rcmaining), *c_rcmaining) = c\n    self.assertEqual(c_0, 'sendMessage', ws.mock_calls)\n    self.assertEqual(c_1_1, False, ws.mock_calls)\n    yield bytes_to_dict(c_1_0)"", ""for (c_0, (c_1_0, c_1_1, *c_1_len), *c_len) in ws.mock_calls:\n    self.assertEqual(c_0, 'sendMessage', ws.mock_calls)\n    self.assertEqual(c_1_1, False, ws.mock_calls)\n    yield bytes_to_dict(c_1_0)""]",no_found,0
swift,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/swift/test/unit/obj/test_diskfile.py,https://github.com/openstack/swift/tree/master/test/unit/obj/test_diskfile.py,DiskFileMixin,_run_test$5727,"def _run_test():
    splice_called = [False]

    def fake_splice(fd_in, off_in, fd_out, off_out, len_, flags):
        if fd_out == devnull.fileno() and (not splice_called[0]):
            splice_called[0] = True
            err = errno.EWOULDBLOCK
            raise IOError(err, os.strerror(err))
        return splice(fd_in, off_in, fd_out, off_out, len_, flags)
    mock_splice.side_effect = fake_splice

    def fake_trampoline(fd, read=None, write=None, timeout=None, timeout_exc=timeout.Timeout, mark_as_closed=None):
        if write and fd == devnull.fileno():
            return
        else:
            hubs.trampoline(fd, read=read, write=write, timeout=timeout, timeout_exc=timeout_exc, mark_as_closed=mark_as_closed)
    mock_trampoline.side_effect = fake_trampoline
    reader.zero_copy_send(devnull.fileno())
    self.assertTrue(mock_close.called)
    mock_trampoline.assert_any_call(devnull.fileno(), write=True)
    for call in mock_splice.call_args_list:
        args = call[0]
        if args[2] == devnull.fileno():
            break
    else:
        self.fail('`splice` not called with expected arguments')","for call in mock_splice.call_args_list:
    args = call[0]
    if args[2] == devnull.fileno():
        break
else:
    self.fail('`splice` not called with expected arguments')","[""for call in mock_splice.call_args_list:\n    (call_0, *call_rcallmaining) = call\n    args = call_0\n    if args[2] == devnull.fileno():\n        break\nelse:\n    self.fail('`splice` not called with expected arguments')"", ""for (call_0, *call_len) in mock_splice.call_args_list:\n    args = \n    call_0\n    if args[2] == devnull.fileno():\n        break\nelse:\n    self.fail('`splice` not called with expected arguments')""]",no_found,0
kazoo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kazoo/kazoo/tests/test_utils.py,https://github.com/python-zk/kazoo/tree/master/kazoo/tests/test_utils.py,TestCreateTCPConnection,test_timeout_arg_eventlet$32,"def test_timeout_arg_eventlet(self):
    if not EVENTLET_HANDLER_AVAILABLE:
        pytest.skip('eventlet handler not available.')
    from kazoo.handlers import utils
    from kazoo.handlers.utils import create_tcp_connection, time
    with patch.object(socket, 'create_connection') as create_connection:
        with patch.object(utils, '_set_default_tcpsock_options'):
            with patch.object(time, 'time', side_effect=range(10)):
                create_tcp_connection(socket, ('127.0.0.1', 2181), timeout=1.5)
            for call_args in create_connection.call_args_list:
                timeout = call_args[0][1]
                assert timeout >= 0, 'socket timeout must be nonnegative'","for call_args in create_connection.call_args_list:
    timeout = call_args[0][1]
    assert timeout >= 0, 'socket timeout must be nonnegative'","[""for call_args in create_connection.call_args_list:\n    ((call_args_0_0, call_args_0_1, *call_args_0_rcall_argsmaining), *call_args_rcall_argsmaining) = call_args\n    timeout = call_args_0_1\n    assert timeout >= 0, 'socket timeout must be nonnegative'"", ""for ((call_args_0_0, call_args_0_1, *call_args_0_len), *call_args_len) in create_connection.call_args_list:\n    timeout = \n    call_args_0_1\n    assert timeout >= 0, 'socket timeout must be nonnegative'""]",no_found,0
PythonStdioGames,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonStdioGames/barebones/blackjack-barebones.py,https://github.com/asweigart/PythonStdioGames/tree/master/barebones/blackjack-barebones.py,,getHandValue$83,"def getHandValue(allCards):
    value = 0
    numberOfAces = 0
    for card in allCards:
        rank = card[0]
        if rank == 'A':
            numberOfAces += 1
        elif rank in ['K', 'Q', 'J']:
            value += 10
        elif rank in ['2', '3', '4', '5', '6', '7', '8', '9', '10']:
            value += int(rank)
    value += numberOfAces
    for i in range(numberOfAces):
        if value + 10 <= 21:
            value += 10
    return value","for card in allCards:
    rank = card[0]
    if rank == 'A':
        numberOfAces += 1
    elif rank in ['K', 'Q', 'J']:
        value += 10
    elif rank in ['2', '3', '4', '5', '6', '7', '8', '9', '10']:
        value += int(rank)","[""for card in allCards:\n    (card_0, *card_rcardmaining) = card\n    rank = card_0\n    if rank == 'A':\n        numberOfAces += 1\n    elif rank in ['K', 'Q', 'J']:\n        value += 10\n    elif rank in ['2', '3', '4', '5', '6', '7', '8', '9', '10']:\n        value += int(rank)"", ""for (card_0, *card_len) in allCards:\n    rank = \n    card_0\n    if rank == 'A':\n        numberOfAces += 1\n    elif rank in ['K', 'Q', 'J']:\n        value += 10\n    elif rank in ['2', '3', '4', '5', '6', '7', '8', '9', '10']:\n        value += int(rank)""]",no_found,0
meld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meld/meld/matchers/merge.py,https://github.com/GNOME/meld/tree/master/meld/matchers/merge.py,AutoMergeDiffer,_auto_merge$32,"def _auto_merge(self, using, texts):
    for (out0, out1) in super()._auto_merge(using, texts):
        if self.auto_merge and out0[0] == 'conflict':
            (l0, h0, l1, h1, l2, h2) = (out0[3], out0[4], out0[1], out0[2], out1[3], out1[4])
            len0 = h0 - l0
            len1 = h1 - l1
            len2 = h2 - l2
            if (len0 > 0 and len2 > 0) and (len0 == len1 or len2 == len1 or len1 == 0):
                matcher = self._matcher(None, texts[0][l0:h0], texts[2][l2:h2])
                for chunk in matcher.get_opcodes():
                    s1 = l1
                    e1 = l1
                    if len0 == len1:
                        s1 += chunk[1]
                        e1 += chunk[2]
                    elif len2 == len1:
                        s1 += chunk[3]
                        e1 += chunk[4]
                    out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
                    out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
                    if chunk[0] == 'equal':
                        out0 = ('replace',) + out0_bounds
                        out1 = ('replace',) + out1_bounds
                        yield (out0, out1)
                    else:
                        out0 = ('conflict',) + out0_bounds
                        out1 = ('conflict',) + out1_bounds
                        yield (out0, out1)
                return
            else:
                chunktype = using[0][0][0]
                for chunkarr in using:
                    for chunk in chunkarr:
                        if chunk[0] != chunktype:
                            chunktype = None
                            break
                    if not chunktype:
                        break
                if chunktype == 'delete':
                    seq0 = seq1 = None
                    while 1:
                        if seq0 is None:
                            try:
                                seq0 = using[0].pop(0)
                                i0 = seq0[1]
                                end0 = seq0[4]
                            except IndexError:
                                break
                        if seq1 is None:
                            try:
                                seq1 = using[1].pop(0)
                                i1 = seq1[1]
                                end1 = seq1[4]
                            except IndexError:
                                break
                        highstart = max(i0, i1)
                        if i0 != i1:
                            out0 = ('conflict', i0 - highstart + i1, highstart, seq0[3] - highstart + i1, seq0[3])
                            out1 = ('conflict', i1 - highstart + i0, highstart, seq1[3] - highstart + i0, seq1[3])
                            yield (out0, out1)
                        lowend = min(seq0[2], seq1[2])
                        if highstart != lowend:
                            out0 = ('delete', highstart, lowend, seq0[3], seq0[4])
                            out1 = ('delete', highstart, lowend, seq1[3], seq1[4])
                            yield (out0, out1)
                        i0 = i1 = lowend
                        if lowend == seq0[2]:
                            seq0 = None
                        if lowend == seq1[2]:
                            seq1 = None
                    if seq0:
                        out0 = ('conflict', i0, seq0[2], seq0[3], seq0[4])
                        out1 = ('conflict', i0, seq0[2], end1, end1 + seq0[2] - i0)
                        yield (out0, out1)
                    elif seq1:
                        out0 = ('conflict', i1, seq1[2], end0, end0 + seq1[2] - i1)
                        out1 = ('conflict', i1, seq1[2], seq1[3], seq1[4])
                        yield (out0, out1)
                    return
        yield (out0, out1)","for chunk in matcher.get_opcodes():
    s1 = l1
    e1 = l1
    if len0 == len1:
        s1 += chunk[1]
        e1 += chunk[2]
    elif len2 == len1:
        s1 += chunk[3]
        e1 += chunk[4]
    out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
    out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
    if chunk[0] == 'equal':
        out0 = ('replace',) + out0_bounds
        out1 = ('replace',) + out1_bounds
        yield (out0, out1)
    else:
        out0 = ('conflict',) + out0_bounds
        out1 = ('conflict',) + out1_bounds
        yield (out0, out1)","[""for chunk in matcher.get_opcodes():\n    (chunk_0, chunk_1, chunk_2, chunk_3, chunk_4, *_) = chunk\n    s1 = l1\n    e1 = l1\n    if len0 == len1:\n        s1 += chunk_1\n        e1 += chunk_2\n    elif len2 == len1:\n        s1 += chunk_3\n        e1 += chunk_4\n    out0_bounds = (s1, e1, l0 + chunk_1, l0 + chunk_2)\n    out1_bounds = (s1, e1, l2 + chunk_3, l2 + chunk_4)\n    if chunk_0 == 'equal':\n        out0 = ('replace',) + out0_bounds\n        out1 = ('replace',) + out1_bounds\n        yield (out0, out1)\n    else:\n        out0 = ('conflict',) + out0_bounds\n        out1 = ('conflict',) + out1_bounds\n        yield (out0, out1)"", ""for (chunk_0, chunk_1, chunk_2, chunk_3, chunk_4, *chunk_len) in matcher.get_opcodes():\n    s1 = l1\n    e1 = l1\n    if len0 == len1:\n        s1 += \n        chunk_1\n        e1 += \n        chunk_2\n    elif len2 == len1:\n        s1 += \n        chunk_3\n        e1 += \n        chunk_4\n    out0_bounds = (s1, e1, l0 + \n    chunk_1, l0 + \n    chunk_2)\n    out1_bounds = (s1, e1, l2 + \n    chunk_3, l2 + \n    chunk_4)\n    if \n    chunk_0 == 'equal':\n        out0 = ('replace',) + out0_bounds\n        out1 = ('replace',) + out1_bounds\n        yield (out0, out1)\n    else:\n        out0 = ('conflict',) + out0_bounds\n        out1 = ('conflict',) + out1_bounds\n        yield (out0, out1)""]",no_found,0
napalm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/napalm/napalm/junos/junos.py,https://github.com/napalm-automation/napalm/tree/master/napalm/junos/junos.py,JunOSDriver,get_snmp_information$1884,"def get_snmp_information(self):
    """"""Return the SNMP configuration.""""""
    snmp_information = {}
    snmp_config = junos_views.junos_snmp_config_table(self.device)
    snmp_config.get(options=self.junos_config_options)
    snmp_items = snmp_config.items()
    if not snmp_items:
        return snmp_information
    snmp_information = {str(ele[0]): ele[1] if ele[1] else '' for ele in snmp_items[0][1]}
    snmp_information['community'] = {}
    communities_table = snmp_information.pop('communities_table')
    if not communities_table:
        return snmp_information
    for community in communities_table.items():
        community_name = str(community[0])
        community_details = {'acl': ''}
        community_details.update({str(ele[0]): str(ele[1] if ele[0] != 'mode' else C.SNMP_AUTHORIZATION_MODE_MAP.get(ele[1])) for ele in community[1]})
        snmp_information['community'][community_name] = community_details
    return snmp_information","for community in communities_table.items():
    community_name = str(community[0])
    community_details = {'acl': ''}
    community_details.update({str(ele[0]): str(ele[1] if ele[0] != 'mode' else C.SNMP_AUTHORIZATION_MODE_MAP.get(ele[1])) for ele in community[1]})
    snmp_information['community'][community_name] = community_details","[""for community in communities_table.items():\n    (community_0, community_1, *_) = community\n    community_name = str(community_0)\n    community_details = {'acl': ''}\n    community_details.update({str(ele[0]): str(ele[1] if ele[0] != 'mode' else C.SNMP_AUTHORIZATION_MODE_MAP.get(ele[1])) for ele in community_1})\n    snmp_information['community'][community_name] = community_details"", ""for (community_0, community_1, *community_len) in communities_table.items():\n    community_name = str(community_0)\n    community_details = {'acl': ''}\n    community_details.update({str(ele[0]): str(ele[1] if ele[0] != 'mode' else C.SNMP_AUTHORIZATION_MODE_MAP.get(ele[1])) for ele in community_1})\n    snmp_information['community'][community_name] = community_details""]",no_found,0
sentry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/utils/snuba.py,https://github.com/getsentry/sentry/tree/master/src/sentry/utils/snuba.py,,get_query_params_to_update_for_projects$441,"def get_query_params_to_update_for_projects(query_params, with_org=False):
    """"""
    Get the project ID and query params that need to be updated for project
    based datasets, before we send the query to Snuba.
    """"""
    if 'project_id' in query_params.filter_keys:
        project_ids = list(set(query_params.filter_keys['project_id']))
    elif query_params.filter_keys:
        with timer('get_related_project_ids'):
            ids = [set(get_related_project_ids(k, query_params.filter_keys[k])) for k in query_params.filter_keys]
            project_ids = list(set.union(*ids))
    elif query_params.conditions:
        project_ids = []
        for cond in query_params.conditions:
            if cond[0] == 'project_id':
                project_ids = [cond[2]] if cond[1] == '=' else cond[2]
    else:
        project_ids = []
    if not project_ids:
        raise UnqualifiedQueryError('No project_id filter, or none could be inferred from other filters.')
    try:
        organization_id = Project.objects.get_from_cache(pk=project_ids[0]).organization_id
    except Project.DoesNotExist:
        project = Project.objects.filter(pk__in=project_ids).values('organization_id').first()
        if project is None:
            raise UnqualifiedQueryError('All project_ids from the filter no longer exist')
        organization_id = project.get('organization_id')
    params = {'project': project_ids}
    if with_org:
        params['organization'] = organization_id
    return (organization_id, params)","for cond in query_params.conditions:
    if cond[0] == 'project_id':
        project_ids = [cond[2]] if cond[1] == '=' else cond[2]","[""for cond in query_params.conditions:\n    (cond_0, cond_1, cond_2, *_) = cond\n    if cond_0 == 'project_id':\n        project_ids = [cond_2] if cond_1 == '=' else cond_2"", ""for (cond_0, cond_1, cond_2, *cond_len) in query_params.conditions:\n    if \n    cond_0 == 'project_id':\n        project_ids = [\n        cond_2] if \n        cond_1 == '=' else \n        cond_2""]",no_found,0
bCNC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/slicemesh.py,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/slicemesh.py,Tool,slice$211,"def slice(self, verts, faces, z, zout=None, axis='z'):
    tags = '[slice]'
    if axis == 'z':
        tags = f'[slice,minz:{float(z):f}]'
    block = Block(f'slice {axis}{float(z):f} {tags}')
    if axis == 'x':
        plane_orig = (z, 0, 0)
        plane_norm = (1, 0, 0)
    elif axis == 'y':
        plane_orig = (0, z, 0)
        plane_norm = (0, 1, 0)
    else:
        plane_orig = (0, 0, z)
        plane_norm = (0, 0, 1)
    contours = meshcut.cross_section(verts, faces, plane_orig, plane_norm)
    if zout is not None:
        for contour in contours:
            for segment in contour:
                segment[2] = zout
    for contour in contours:
        gtype = 0
        for segment in contour:
            block.append(f'g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}')
            gtype = 1
        block.append(f'g1 x{contour[0][0]:f} y{contour[0][1]:f} z{contour[0][2]:f}')
        block.append('( ---------- cut-here ---------- )')
    if block:
        del block[-1]
    if not block:
        block = None
    return block","for contour in contours:
    gtype = 0
    for segment in contour:
        block.append(f'g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}')
        gtype = 1
    block.append(f'g1 x{contour[0][0]:f} y{contour[0][1]:f} z{contour[0][2]:f}')
    block.append('( ---------- cut-here ---------- )')","[""for contour in contours:\n    ((contour_0_0, contour_0_1, contour_0_2, *contour_0_rcontourmaining), *contour_rcontourmaining) = contour\n    gtype = 0\n    for segment in contour:\n        block.append(f'g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}')\n        gtype = 1\n    block.append(f'g1 x{contour_0_0:f} y{contour_0_1:f} z{contour_0_2:f}')\n    block.append('( ---------- cut-here ---------- )')""]",no_found,0
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/doctype/sales_invoice/test_sales_invoice.py,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/doctype/sales_invoice/test_sales_invoice.py,TestSalesInvoice,test_rounding_adjustment_3$2029,"def test_rounding_adjustment_3(self):
    from erpnext.accounts.doctype.accounting_dimension.test_accounting_dimension import create_dimension, disable_dimension
    create_dimension()
    si = create_sales_invoice(do_not_save=True)
    si.items = []
    for d in [(1122, 2), (1122.01, 1), (1122.01, 1)]:
        si.append('items', {'item_code': '_Test Item', 'gst_hsn_code': '999800', 'warehouse': '_Test Warehouse - _TC', 'qty': d[1], 'rate': d[0], 'income_account': 'Sales - _TC', 'cost_center': '_Test Cost Center - _TC'})
    for tax_account in ['_Test Account VAT - _TC', '_Test Account Service Tax - _TC']:
        si.append('taxes', {'charge_type': 'On Net Total', 'account_head': tax_account, 'description': tax_account, 'rate': 6, 'cost_center': '_Test Cost Center - _TC', 'included_in_print_rate': 1})
    si.cost_center = '_Test Cost Center 2 - _TC'
    si.location = 'Block 1'
    si.save()
    si.submit()
    self.assertEqual(si.net_total, 4007.16)
    self.assertEqual(si.grand_total, 4488.02)
    self.assertEqual(si.total_taxes_and_charges, 480.86)
    self.assertEqual(si.rounding_adjustment, -0.02)
    expected_values = dict(((d[0], d) for d in [[si.debit_to, 4488.0, 0.0], ['_Test Account Service Tax - _TC', 0.0, 240.43], ['_Test Account VAT - _TC', 0.0, 240.43], ['Sales - _TC', 0.0, 4007.15], ['Round Off - _TC', 0.01, 0]]))
    gl_entries = frappe.db.sql(""select account, debit, credit\n\t\t\tfrom `tabGL Entry` where voucher_type='Sales Invoice' and voucher_no=%s\n\t\t\torder by account asc"", si.name, as_dict=1)
    debit_credit_diff = 0
    for gle in gl_entries:
        self.assertEqual(expected_values[gle.account][0], gle.account)
        self.assertEqual(expected_values[gle.account][1], gle.debit)
        self.assertEqual(expected_values[gle.account][2], gle.credit)
        debit_credit_diff += gle.debit - gle.credit
    self.assertEqual(debit_credit_diff, 0)
    round_off_gle = frappe.db.get_value('GL Entry', {'voucher_type': 'Sales Invoice', 'voucher_no': si.name, 'account': 'Round Off - _TC'}, ['cost_center', 'location'], as_dict=1)
    self.assertEqual(round_off_gle.cost_center, '_Test Cost Center 2 - _TC')
    self.assertEqual(round_off_gle.location, 'Block 1')
    disable_dimension()","for d in [(1122, 2), (1122.01, 1), (1122.01, 1)]:
    si.append('items', {'item_code': '_Test Item', 'gst_hsn_code': '999800', 'warehouse': '_Test Warehouse - _TC', 'qty': d[1], 'rate': d[0], 'income_account': 'Sales - _TC', 'cost_center': '_Test Cost Center - _TC'})","[""for d in [(1122, 2), (1122.01, 1), (1122.01, 1)]:\n    (d_0, d_1, *_) = d\n    si.append('items', {'item_code': '_Test Item', 'gst_hsn_code': '999800', 'warehouse': '_Test Warehouse - _TC', 'qty': d_1, 'rate': d_0, 'income_account': 'Sales - _TC', 'cost_center': '_Test Cost Center - _TC'})"", ""for (d_0, d_1, *d_len) in [(1122, 2), (1122.01, 1), (1122.01, 1)]:\n    si.append('items', {'item_code': '_Test Item', 'gst_hsn_code': '999800', 'warehouse': '_Test Warehouse - _TC', 'qty': d_1, 'rate': d_0, 'income_account': 'Sales - _TC', 'cost_center': '_Test Cost Center - _TC'})""]",no_found,0
xbmc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xbmc/plugin.video.amazon-test/resources/lib/amazontld.py,https://github.com/Sandmann79/xbmc/tree/master/plugin.video.amazon-test/resources/lib/amazontld.py,AmazonTLD,getcache$1065,"def getcache(uid):
    j = {}
    c = self._menuDb.cursor()
    for data in c.execute('select data from channels where uid = (?)', (uid,)).fetchall():
        j = json.loads(data[0])
    c.close()
    return j","for data in c.execute('select data from channels where uid = (?)', (uid,)).fetchall():
    j = json.loads(data[0])","[""for data in c.execute('select data from channels where uid = (?)', (uid,)).fetchall():\n    (data_0, *data_rdatamaining) = data\n    j = json.loads(data_0)"", ""for (data_0, *data_len) in c.execute('select data from channels where uid = (?)', (uid,)).fetchall():\n    j = json.loads(data_0)""]",no_found,0
adminset,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adminset/cmdb/asset.py,https://github.com/guohongze/adminset/tree/master/cmdb/asset.py,,asset_import$160,"def asset_import(request):
    if request.method == 'POST':
        uf = request.FILES.get('asset_import')
        with open('/var/opt/adminset/data/asset.csv', 'wb+') as f:
            for chunk in uf.chunks(chunk_size=1024):
                f.write(chunk)
        try:
            filename = '/var/opt/adminset/data/asset.csv'
            with open(filename, 'rb') as f:
                title = next(csv.reader(f))
                for data in csv.reader(f):
                    data0 = str2gb2utf8(data[0])
                    if data0 == u'':
                        continue
                    try:
                        host = Host.objects.get(hostname=data0)
                    except Exception as msg:
                        host = Host()
                        host.hostname = data0
                    host.ip = data[1]
                    host.other_ip = str2gb2utf8(data[2])
                    if data[3]:
                        try:
                            idc_name = str2gb2utf8(data[3])
                            print('idc name is : {}'.format(idc_name))
                            print('idc name type: {}'.format(type(idc_name)))
                            item = Idc.objects.get(name=idc_name)
                            host.idc_id = item.id
                        except Exception as e:
                            print(e)
                            print('idc info import error')
                    host.asset_no = str2gb2utf8(data[4])
                    if data[5]:
                        asset_type = str2gb2utf8(data[5])
                        for (x, v) in ASSET_TYPE:
                            if v == asset_type:
                                ret = x
                        host.asset_type = ret
                    if data[6]:
                        status = str2gb2utf8(data[6])
                        for (x, v) in ASSET_STATUS:
                            if v == status:
                                ret = x
                        host.status = ret
                    host.os = str2gb2utf8(data[7])
                    host.vendor = str2gb2utf8(data[8])
                    host.cpu_model = str2gb2utf8(data[9])
                    host.cpu_num = str2gb2utf8(data[10])
                    host.memory = str2gb2utf8(data[11])
                    host.disk = data[12]
                    host.sn = str2gb2utf8(data[13])
                    host.position = str2gb2utf8(data[14])
                    host.memo = str2gb2utf8(data[15])
                    host.save()
            os.remove(filename)
            status = 1
        except Exception as e:
            print(e)
            print('import asset csv file error!')
            status = 2
    return render(request, 'cmdb/import.html', locals())","for data in csv.reader(f):
    data0 = str2gb2utf8(data[0])
    if data0 == u'':
        continue
    try:
        host = Host.objects.get(hostname=data0)
    except Exception as msg:
        host = Host()
        host.hostname = data0
    host.ip = data[1]
    host.other_ip = str2gb2utf8(data[2])
    if data[3]:
        try:
            idc_name = str2gb2utf8(data[3])
            print('idc name is : {}'.format(idc_name))
            print('idc name type: {}'.format(type(idc_name)))
            item = Idc.objects.get(name=idc_name)
            host.idc_id = item.id
        except Exception as e:
            print(e)
            print('idc info import error')
    host.asset_no = str2gb2utf8(data[4])
    if data[5]:
        asset_type = str2gb2utf8(data[5])
        for (x, v) in ASSET_TYPE:
            if v == asset_type:
                ret = x
        host.asset_type = ret
    if data[6]:
        status = str2gb2utf8(data[6])
        for (x, v) in ASSET_STATUS:
            if v == status:
                ret = x
        host.status = ret
    host.os = str2gb2utf8(data[7])
    host.vendor = str2gb2utf8(data[8])
    host.cpu_model = str2gb2utf8(data[9])
    host.cpu_num = str2gb2utf8(data[10])
    host.memory = str2gb2utf8(data[11])
    host.disk = data[12]
    host.sn = str2gb2utf8(data[13])
    host.position = str2gb2utf8(data[14])
    host.memo = str2gb2utf8(data[15])
    host.save()","[""for data in csv.reader(f):\n    (data_0, data_10, data_11, data_12, data_13, data_14, data_15, data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, *_) = data\n    data0 = str2gb2utf8(data_0)\n    if data0 == u'':\n        continue\n    try:\n        host = Host.objects.get(hostname=data0)\n    except Exception as msg:\n        host = Host()\n        host.hostname = data0\n    host.ip = data_1\n    host.other_ip = str2gb2utf8(data_2)\n    if data_3:\n        try:\n            idc_name = str2gb2utf8(data_3)\n            print('idc name is : {}'.format(idc_name))\n            print('idc name type: {}'.format(type(idc_name)))\n            item = Idc.objects.get(name=idc_name)\n            host.idc_id = item.id\n        except Exception as e:\n            print(e)\n            print('idc info import error')\n    host.asset_no = str2gb2utf8(data_4)\n    if data_5:\n        asset_type = str2gb2utf8(data_5)\n        for (x, v) in ASSET_TYPE:\n            if v == asset_type:\n                ret = x\n        host.asset_type = ret\n    if data_6:\n        status = str2gb2utf8(data_6)\n        for (x, v) in ASSET_STATUS:\n            if v == status:\n                ret = x\n        host.status = ret\n    host.os = str2gb2utf8(data_7)\n    host.vendor = str2gb2utf8(data_8)\n    host.cpu_model = str2gb2utf8(data_9)\n    host.cpu_num = str2gb2utf8(data_10)\n    host.memory = str2gb2utf8(data_11)\n    host.disk = data_12\n    host.sn = str2gb2utf8(data_13)\n    host.position = str2gb2utf8(data_14)\n    host.memo = str2gb2utf8(data_15)\n    host.save()"", ""for (data_0, data_1, data_2, data_3, data_4, data_5, data_6, data_7, data_8, data_9, data_10, data_11, data_12, data_13, data_14, data_15, *data_len) in csv.reader(f):\n    data0 = str2gb2utf8(\n    data_0)\n    if data0 == u'':\n        continue\n    try:\n        host = Host.objects.get(hostname=data0)\n    except Exception as msg:\n        host = Host()\n        host.hostname = data0\n    host.ip = \n    data_1\n    host.other_ip = str2gb2utf8(\n    data_2)\n    if \n    data_3:\n        try:\n            idc_name = str2gb2utf8(\n            data_3)\n            print('idc name is : {}'.format(idc_name))\n            print('idc name type: {}'.format(type(idc_name)))\n            item = Idc.objects.get(name=idc_name)\n            host.idc_id = item.id\n        except Exception as e:\n            print(e)\n            print('idc info import error')\n    host.asset_no = str2gb2utf8(\n    data_4)\n    if \n    data_5:\n        asset_type = str2gb2utf8(\n        data_5)\n        for (x, v) in ASSET_TYPE:\n            if v == asset_type:\n                ret = x\n        host.asset_type = ret\n    if \n    data_6:\n        status = str2gb2utf8(\n        data_6)\n        for (x, v) in ASSET_STATUS:\n            if v == status:\n                ret = x\n        host.status = ret\n    host.os = str2gb2utf8(\n    data_7)\n    host.vendor = str2gb2utf8(\n    data_8)\n    host.cpu_model = str2gb2utf8(\n    data_9)\n    host.cpu_num = str2gb2utf8(\n    data_10)\n    host.memory = str2gb2utf8(\n    data_11)\n    host.disk = \n    data_12\n    host.sn = str2gb2utf8(\n    data_13)\n    host.position = str2gb2utf8(\n    data_14)\n    host.memo = str2gb2utf8(\n    data_15)\n    host.save()""]",no_found,0
Face_Pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Face_Pytorch/train_softmax.py,https://github.com/wujiyang/Face_Pytorch/tree/master//train_softmax.py,,train$38,"def train(args):
    multi_gpus = False
    if len(args.gpus.split(',')) > 1:
        multi_gpus = True
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpus
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    save_dir = os.path.join(args.save_dir, args.model_pre + args.backbone.upper() + '_' + datetime.now().strftime('%Y%m%d_%H%M%S'))
    if os.path.exists(save_dir):
        raise NameError('model dir exists!')
    os.makedirs(save_dir)
    logging = init_log(save_dir)
    _print = logging.info
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])
    trainset = CASIAWebFace(args.train_root, args.train_file_list, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=8, drop_last=False)
    lfwdataset = LFW(args.lfw_test_root, args.lfw_file_list, transform=transform)
    lfwloader = torch.utils.data.DataLoader(lfwdataset, batch_size=128, shuffle=False, num_workers=4, drop_last=False)
    agedbdataset = AgeDB30(args.agedb_test_root, args.agedb_file_list, transform=transform)
    agedbloader = torch.utils.data.DataLoader(agedbdataset, batch_size=128, shuffle=False, num_workers=4, drop_last=False)
    cfpfpdataset = CFP_FP(args.cfpfp_test_root, args.cfpfp_file_list, transform=transform)
    cfpfploader = torch.utils.data.DataLoader(cfpfpdataset, batch_size=128, shuffle=False, num_workers=4, drop_last=False)
    if args.backbone == 'MobileFace':
        net = MobileFaceNet()
    elif args.backbone == 'Res50':
        net = ResNet50()
    elif args.backbone == 'Res101':
        net = ResNet101()
    elif args.backbone == 'Res50_IR':
        net = SEResNet_IR(50, feature_dim=args.feature_dim, mode='ir')
    elif args.backbone == 'SERes50_IR':
        net = SEResNet_IR(50, feature_dim=args.feature_dim, mode='se_ir')
    elif args.backbone == 'SphereNet':
        net = SphereNet(num_layers=64, feature_dim=args.feature_dim)
    else:
        print(args.backbone, ' is not available!')
    if args.margin_type == 'ArcFace':
        margin = ArcMarginProduct(args.feature_dim, trainset.class_nums, s=args.scale_size)
    elif args.margin_type == 'CosFace':
        pass
    elif args.margin_type == 'SphereFace':
        pass
    elif args.margin_type == 'InnerProduct':
        margin = InnerProduct(args.feature_dim, trainset.class_nums)
    else:
        print(args.margin_type, 'is not available!')
    if args.resume:
        print('resume the model parameters from: ', args.net_path, args.margin_path)
        net.load_state_dict(torch.load(args.net_path)['net_state_dict'])
        margin.load_state_dict(torch.load(args.margin_path)['net_state_dict'])
    criterion_classi = torch.nn.CrossEntropyLoss().to(device)
    optimizer_classi = optim.SGD([{'params': net.parameters(), 'weight_decay': 0.0005}, {'params': margin.parameters(), 'weight_decay': 0.0005}], lr=0.1, momentum=0.9, nesterov=True)
    scheduler_classi = lr_scheduler.MultiStepLR(optimizer_classi, milestones=[20, 35, 45], gamma=0.1)
    if multi_gpus:
        net = DataParallel(net).to(device)
        margin = DataParallel(margin).to(device)
    else:
        net = net.to(device)
        margin = margin.to(device)
    best_lfw_acc = 0.0
    best_lfw_iters = 0
    best_agedb30_acc = 0.0
    best_agedb30_iters = 0
    best_cfp_fp_acc = 0.0
    best_cfp_fp_iters = 0
    total_iters = 0
    vis = Visualizer(env='softmax_train')
    for epoch in range(1, args.total_epoch + 1):
        scheduler_classi.step()
        _print('Train Epoch: {}/{} ...'.format(epoch, args.total_epoch))
        net.train()
        since = time.time()
        for data in trainloader:
            (img, label) = (data[0].to(device), data[1].to(device))
            feature = net(img)
            output = margin(feature)
            loss_classi = criterion_classi(output, label)
            total_loss = loss_classi
            optimizer_classi.zero_grad()
            total_loss.backward()
            optimizer_classi.step()
            total_iters += 1
            if total_iters % 100 == 0:
                (_, predict) = torch.max(output.data, 1)
                total = label.size(0)
                correct = (np.array(predict) == np.array(label.data)).sum()
                time_cur = (time.time() - since) / 100
                since = time.time()
                vis.plot_curves({'train loss': loss_classi.item()}, iters=total_iters, title='train loss', xlabel='iters', ylabel='train loss')
                vis.plot_curves({'train accuracy': correct / total}, iters=total_iters, title='train accuracy', xlabel='iters', ylabel='train accuracy')
                print('Iters: {:0>6d}/[{:0>2d}], loss_classi: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}'.format(total_iters, epoch, loss_classi.item(), correct / total, time_cur, scheduler_classi.get_lr()[0]))
            if total_iters % args.save_freq == 0:
                msg = 'Saving checkpoint: {}'.format(total_iters)
                _print(msg)
                if multi_gpus:
                    net_state_dict = net.module.state_dict()
                    margin_state_dict = margin.module.state_dict()
                else:
                    net_state_dict = net.state_dict()
                    margin_state_dict = margin.state_dict()
                if not os.path.exists(save_dir):
                    os.mkdir(save_dir)
                torch.save({'iters': total_iters, 'net_state_dict': net_state_dict}, os.path.join(save_dir, 'Iter_%06d_net.ckpt' % total_iters))
                torch.save({'iters': total_iters, 'net_state_dict': margin_state_dict}, os.path.join(save_dir, 'Iter_%06d_margin.ckpt' % total_iters))
            if total_iters % args.test_freq == 0:
                net.eval()
                getFeatureFromTorch('./result/cur_lfw_result.mat', net, device, lfwdataset, lfwloader)
                lfw_accs = evaluation_10_fold('./result/cur_lfw_result.mat')
                _print('LFW Ave Accuracy: {:.4f}'.format(np.mean(lfw_accs) * 100))
                if best_lfw_acc < np.mean(lfw_accs) * 100:
                    best_lfw_acc = np.mean(lfw_accs) * 100
                    best_lfw_iters = total_iters
                getFeatureFromTorch('./result/cur_agedb30_result.mat', net, device, agedbdataset, agedbloader)
                age_accs = evaluation_10_fold('./result/cur_agedb30_result.mat')
                _print('AgeDB-30 Ave Accuracy: {:.4f}'.format(np.mean(age_accs) * 100))
                if best_agedb30_acc < np.mean(age_accs) * 100:
                    best_agedb30_acc = np.mean(age_accs) * 100
                    best_agedb30_iters = total_iters
                getFeatureFromTorch('./result/cur_cfpfp_result.mat', net, device, cfpfpdataset, cfpfploader)
                cfp_accs = evaluation_10_fold('./result/cur_cfpfp_result.mat')
                _print('CFP-FP Ave Accuracy: {:.4f}'.format(np.mean(cfp_accs) * 100))
                if best_cfp_fp_acc < np.mean(cfp_accs) * 100:
                    best_cfp_fp_acc = np.mean(cfp_accs) * 100
                    best_cfp_fp_iters = total_iters
                _print('Current Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}'.format(best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))
                vis.plot_curves({'lfw': np.mean(lfw_accs), 'agedb-30': np.mean(age_accs), 'cfp-fp': np.mean(cfp_accs)}, iters=total_iters, title='test accuracy', xlabel='iters', ylabel='test accuracy')
                net.train()
    _print('Finally Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}'.format(best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))
    print('finishing training')","for data in trainloader:
    (img, label) = (data[0].to(device), data[1].to(device))
    feature = net(img)
    output = margin(feature)
    loss_classi = criterion_classi(output, label)
    total_loss = loss_classi
    optimizer_classi.zero_grad()
    total_loss.backward()
    optimizer_classi.step()
    total_iters += 1
    if total_iters % 100 == 0:
        (_, predict) = torch.max(output.data, 1)
        total = label.size(0)
        correct = (np.array(predict) == np.array(label.data)).sum()
        time_cur = (time.time() - since) / 100
        since = time.time()
        vis.plot_curves({'train loss': loss_classi.item()}, iters=total_iters, title='train loss', xlabel='iters', ylabel='train loss')
        vis.plot_curves({'train accuracy': correct / total}, iters=total_iters, title='train accuracy', xlabel='iters', ylabel='train accuracy')
        print('Iters: {:0>6d}/[{:0>2d}], loss_classi: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}'.format(total_iters, epoch, loss_classi.item(), correct / total, time_cur, scheduler_classi.get_lr()[0]))
    if total_iters % args.save_freq == 0:
        msg = 'Saving checkpoint: {}'.format(total_iters)
        _print(msg)
        if multi_gpus:
            net_state_dict = net.module.state_dict()
            margin_state_dict = margin.module.state_dict()
        else:
            net_state_dict = net.state_dict()
            margin_state_dict = margin.state_dict()
        if not os.path.exists(save_dir):
            os.mkdir(save_dir)
        torch.save({'iters': total_iters, 'net_state_dict': net_state_dict}, os.path.join(save_dir, 'Iter_%06d_net.ckpt' % total_iters))
        torch.save({'iters': total_iters, 'net_state_dict': margin_state_dict}, os.path.join(save_dir, 'Iter_%06d_margin.ckpt' % total_iters))
    if total_iters % args.test_freq == 0:
        net.eval()
        getFeatureFromTorch('./result/cur_lfw_result.mat', net, device, lfwdataset, lfwloader)
        lfw_accs = evaluation_10_fold('./result/cur_lfw_result.mat')
        _print('LFW Ave Accuracy: {:.4f}'.format(np.mean(lfw_accs) * 100))
        if best_lfw_acc < np.mean(lfw_accs) * 100:
            best_lfw_acc = np.mean(lfw_accs) * 100
            best_lfw_iters = total_iters
        getFeatureFromTorch('./result/cur_agedb30_result.mat', net, device, agedbdataset, agedbloader)
        age_accs = evaluation_10_fold('./result/cur_agedb30_result.mat')
        _print('AgeDB-30 Ave Accuracy: {:.4f}'.format(np.mean(age_accs) * 100))
        if best_agedb30_acc < np.mean(age_accs) * 100:
            best_agedb30_acc = np.mean(age_accs) * 100
            best_agedb30_iters = total_iters
        getFeatureFromTorch('./result/cur_cfpfp_result.mat', net, device, cfpfpdataset, cfpfploader)
        cfp_accs = evaluation_10_fold('./result/cur_cfpfp_result.mat')
        _print('CFP-FP Ave Accuracy: {:.4f}'.format(np.mean(cfp_accs) * 100))
        if best_cfp_fp_acc < np.mean(cfp_accs) * 100:
            best_cfp_fp_acc = np.mean(cfp_accs) * 100
            best_cfp_fp_iters = total_iters
        _print('Current Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}'.format(best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))
        vis.plot_curves({'lfw': np.mean(lfw_accs), 'agedb-30': np.mean(age_accs), 'cfp-fp': np.mean(cfp_accs)}, iters=total_iters, title='test accuracy', xlabel='iters', ylabel='test accuracy')
        net.train()","[""for data in trainloader:\n    (data_0, data_1, *_) = data\n    (img, label) = (data_0.to(device), data_1.to(device))\n    feature = net(img)\n    output = margin(feature)\n    loss_classi = criterion_classi(output, label)\n    total_loss = loss_classi\n    optimizer_classi.zero_grad()\n    total_loss.backward()\n    optimizer_classi.step()\n    total_iters += 1\n    if total_iters % 100 == 0:\n        (_, predict) = torch.max(output.data, 1)\n        total = label.size(0)\n        correct = (np.array(predict) == np.array(label.data)).sum()\n        time_cur = (time.time() - since) / 100\n        since = time.time()\n        vis.plot_curves({'train loss': loss_classi.item()}, iters=total_iters, title='train loss', xlabel='iters', ylabel='train loss')\n        vis.plot_curves({'train accuracy': correct / total}, iters=total_iters, title='train accuracy', xlabel='iters', ylabel='train accuracy')\n        print('Iters: {:0>6d}/[{:0>2d}], loss_classi: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}'.format(total_iters, epoch, loss_classi.item(), correct / total, time_cur, scheduler_classi.get_lr()[0]))\n    if total_iters % args.save_freq == 0:\n        msg = 'Saving checkpoint: {}'.format(total_iters)\n        _print(msg)\n        if multi_gpus:\n            net_state_dict = net.module.state_dict()\n            margin_state_dict = margin.module.state_dict()\n        else:\n            net_state_dict = net.state_dict()\n            margin_state_dict = margin.state_dict()\n        if not os.path.exists(save_dir):\n            os.mkdir(save_dir)\n        torch.save({'iters': total_iters, 'net_state_dict': net_state_dict}, os.path.join(save_dir, 'Iter_%06d_net.ckpt' % total_iters))\n        torch.save({'iters': total_iters, 'net_state_dict': margin_state_dict}, os.path.join(save_dir, 'Iter_%06d_margin.ckpt' % total_iters))\n    if total_iters % args.test_freq == 0:\n        net.eval()\n        getFeatureFromTorch('./result/cur_lfw_result.mat', net, device, lfwdataset, lfwloader)\n        lfw_accs = evaluation_10_fold('./result/cur_lfw_result.mat')\n        _print('LFW Ave Accuracy: {:.4f}'.format(np.mean(lfw_accs) * 100))\n        if best_lfw_acc < np.mean(lfw_accs) * 100:\n            best_lfw_acc = np.mean(lfw_accs) * 100\n            best_lfw_iters = total_iters\n        getFeatureFromTorch('./result/cur_agedb30_result.mat', net, device, agedbdataset, agedbloader)\n        age_accs = evaluation_10_fold('./result/cur_agedb30_result.mat')\n        _print('AgeDB-30 Ave Accuracy: {:.4f}'.format(np.mean(age_accs) * 100))\n        if best_agedb30_acc < np.mean(age_accs) * 100:\n            best_agedb30_acc = np.mean(age_accs) * 100\n            best_agedb30_iters = total_iters\n        getFeatureFromTorch('./result/cur_cfpfp_result.mat', net, device, cfpfpdataset, cfpfploader)\n        cfp_accs = evaluation_10_fold('./result/cur_cfpfp_result.mat')\n        _print('CFP-FP Ave Accuracy: {:.4f}'.format(np.mean(cfp_accs) * 100))\n        if best_cfp_fp_acc < np.mean(cfp_accs) * 100:\n            best_cfp_fp_acc = np.mean(cfp_accs) * 100\n            best_cfp_fp_iters = total_iters\n        _print('Current Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}'.format(best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))\n        vis.plot_curves({'lfw': np.mean(lfw_accs), 'agedb-30': np.mean(age_accs), 'cfp-fp': np.mean(cfp_accs)}, iters=total_iters, title='test accuracy', xlabel='iters', ylabel='test accuracy')\n        net.train()"", ""for (data_0, data_1, *data_len) in trainloader:\n    (img, label) = (data_0.to(device), data_1.to(device))\n    feature = net(img)\n    output = margin(feature)\n    loss_classi = criterion_classi(output, label)\n    total_loss = loss_classi\n    optimizer_classi.zero_grad()\n    total_loss.backward()\n    optimizer_classi.step()\n    total_iters += 1\n    if total_iters % 100 == 0:\n        (_, predict) = torch.max(output.data, 1)\n        total = label.size(0)\n        correct = (np.array(predict) == np.array(label.data)).sum()\n        time_cur = (time.time() - since) / 100\n        since = time.time()\n        vis.plot_curves({'train loss': loss_classi.item()}, iters=total_iters, title='train loss', xlabel='iters', ylabel='train loss')\n        vis.plot_curves({'train accuracy': correct / total}, iters=total_iters, title='train accuracy', xlabel='iters', ylabel='train accuracy')\n        print('Iters: {:0>6d}/[{:0>2d}], loss_classi: {:.4f}, train_accuracy: {:.4f}, time: {:.2f} s/iter, learning rate: {}'.format(total_iters, epoch, loss_classi.item(), correct / total, time_cur, scheduler_classi.get_lr()[0]))\n    if total_iters % args.save_freq == 0:\n        msg = 'Saving checkpoint: {}'.format(total_iters)\n        _print(msg)\n        if multi_gpus:\n            net_state_dict = net.module.state_dict()\n            margin_state_dict = margin.module.state_dict()\n        else:\n            net_state_dict = net.state_dict()\n            margin_state_dict = margin.state_dict()\n        if not os.path.exists(save_dir):\n            os.mkdir(save_dir)\n        torch.save({'iters': total_iters, 'net_state_dict': net_state_dict}, os.path.join(save_dir, 'Iter_%06d_net.ckpt' % total_iters))\n        torch.save({'iters': total_iters, 'net_state_dict': margin_state_dict}, os.path.join(save_dir, 'Iter_%06d_margin.ckpt' % total_iters))\n    if total_iters % args.test_freq == 0:\n        net.eval()\n        getFeatureFromTorch('./result/cur_lfw_result.mat', net, device, lfwdataset, lfwloader)\n        lfw_accs = evaluation_10_fold('./result/cur_lfw_result.mat')\n        _print('LFW Ave Accuracy: {:.4f}'.format(np.mean(lfw_accs) * 100))\n        if best_lfw_acc < np.mean(lfw_accs) * 100:\n            best_lfw_acc = np.mean(lfw_accs) * 100\n            best_lfw_iters = total_iters\n        getFeatureFromTorch('./result/cur_agedb30_result.mat', net, device, agedbdataset, agedbloader)\n        age_accs = evaluation_10_fold('./result/cur_agedb30_result.mat')\n        _print('AgeDB-30 Ave Accuracy: {:.4f}'.format(np.mean(age_accs) * 100))\n        if best_agedb30_acc < np.mean(age_accs) * 100:\n            best_agedb30_acc = np.mean(age_accs) * 100\n            best_agedb30_iters = total_iters\n        getFeatureFromTorch('./result/cur_cfpfp_result.mat', net, device, cfpfpdataset, cfpfploader)\n        cfp_accs = evaluation_10_fold('./result/cur_cfpfp_result.mat')\n        _print('CFP-FP Ave Accuracy: {:.4f}'.format(np.mean(cfp_accs) * 100))\n        if best_cfp_fp_acc < np.mean(cfp_accs) * 100:\n            best_cfp_fp_acc = np.mean(cfp_accs) * 100\n            best_cfp_fp_iters = total_iters\n        _print('Current Best Accuracy: LFW: {:.4f} in iters: {}, AgeDB-30: {:.4f} in iters: {} and CFP-FP: {:.4f} in iters: {}'.format(best_lfw_acc, best_lfw_iters, best_agedb30_acc, best_agedb30_iters, best_cfp_fp_acc, best_cfp_fp_iters))\n        vis.plot_curves({'lfw': np.mean(lfw_accs), 'agedb-30': np.mean(age_accs), 'cfp-fp': np.mean(cfp_accs)}, iters=total_iters, title='test accuracy', xlabel='iters', ylabel='test accuracy')\n        net.train()""]",no_found,0
congress-legislators,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/congress-legislators/scripts/icpsr_ids.py,https://github.com/unitedstates/congress-legislators/tree/master/scripts/icpsr_ids.py,,run$15,"def run():
    cache = utils.flags().get('cache', True)
    force = not cache
    only_bioguide = utils.flags().get('bioguide', None)
    congress = utils.flags().get('congress', None)
    data_files = []
    print('Loading %s...' % 'legislators-current.yaml')
    legislators = load_data('legislators-current.yaml')
    data_files.append((legislators, 'legislators-current.yaml'))
    print('Loading %s...' % 'legislators-historical.yaml')
    legislators = load_data('legislators-historical.yaml')
    data_files.append((legislators, 'legislators-historical.yaml'))
    if congress == None:
        raise Exception('the --congress flag is required')
    elif int(congress) < 10 and int(congress) > 0:
        url_senate = 'https://voteview.com/static/data/out/members/S00%s_members.csv' % congress
        url_house = 'https://voteview.com/static/data/out/members/H00%s_members.csv' % congress
    elif int(congress) < 100 and int(congress) >= 10:
        url_senate = 'https://voteview.com/static/data/out/members/S0%s_members.csv' % congress
        url_house = 'https://voteview.com/static/data/out/members/H0%s_members.csv' % congress
    elif int(congress) >= 100:
        url_senate = 'https://voteview.com/static/data/out/members/S%s_members.csv' % congress
        url_house = 'https://voteview.com/static/data/out/members/H%s_members.csv' % congress
    else:
        raise Exception('no data for congress ' + congress)
    senate_destination = 'icpsr/source/senate_rollcall%s.txt' % congress
    senate_data = utils.download(url_senate, senate_destination, force)
    house_destination = 'icpsr/source/house_rollcall%s.txt' % congress
    house_data = utils.download(url_house, house_destination, force)
    error_log = csv.writer(open('cache/errors/mismatch/mismatch_%s.csv' % congress, 'w'))
    error_log.writerow(['error_type', 'matches', 'icpsr_name', 'icpsr_state', 'is_territory', 'old_id', 'new_id'])
    read_files = [('sen', senate_data), ('rep', house_data)]
    print('Running for congress ' + congress)
    for (read_file_chamber, read_file_content) in read_files:
        for data_file in data_files:
            for legislator in data_file[0]:
                num_matches = 0
                write_id = ''
                bioguide = legislator['id'].get('bioguide', None)
                if only_bioguide and bioguide != only_bioguide:
                    continue
                chamber = legislator['terms'][len(legislator['terms']) - 1]['type']
                if chamber != read_file_chamber:
                    continue
                latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms']) - 1]['start'])))
                if chamber == 'sen':
                    congresses = [latest_congress, latest_congress + 1, latest_congress + 2]
                else:
                    congresses = [latest_congress]
                if int(congress) not in congresses:
                    continue
                last_name = legislator['name']['last'].upper()
                state = utils.states[legislator['terms'][len(legislator['terms']) - 1]['state']].upper()[:7].strip()
                content_as_file = StringIO(read_file_content)
                content_parsed = csv.reader(content_as_file, delimiter=',')
                for icpsr_member in content_parsed:
                    if bioguide == icpsr_member[10]:
                        num_matches += 1
                        write_id = int(icpsr_member[2])
                if 'icpsr' in legislator['id']:
                    if write_id == legislator['id']['icpsr'] or write_id == '':
                        continue
                    elif write_id != legislator['id']['icpsr'] and write_id != '':
                        error_log.writerow(['Incorrect_ID', 'NA', last_name[:8], state, 'NA', legislator['id']['icpsr'], write_id])
                        print('ID updated for %s' % last_name)
                if num_matches == 1:
                    legislator['id']['icpsr'] = int(write_id)
                elif state == 'GUAM' or state == 'PUERTO' or state == 'VIRGIN' or (state == 'DISTRIC') or (state == 'AMERICA') or (state == 'NORTHER') or (state == 'PHILIPP'):
                    print('error: non 1 match')
                    error_log.writerow(['Non_1_match_number', str(num_matches), last_name[:8], state, 'Y', 'NA', 'NA'])
                else:
                    print(str(num_matches) + ' matches found for ' + last_name[:8] + ', ' + state + ' in congress ' + str(congress))
                    error_log.writerow(['Non_1_match_number', str(num_matches), last_name, state, 'N', 'NA', 'NA'])
            save_data(data_file[0], data_file[1])","for data_file in data_files:
    for legislator in data_file[0]:
        num_matches = 0
        write_id = ''
        bioguide = legislator['id'].get('bioguide', None)
        if only_bioguide and bioguide != only_bioguide:
            continue
        chamber = legislator['terms'][len(legislator['terms']) - 1]['type']
        if chamber != read_file_chamber:
            continue
        latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms']) - 1]['start'])))
        if chamber == 'sen':
            congresses = [latest_congress, latest_congress + 1, latest_congress + 2]
        else:
            congresses = [latest_congress]
        if int(congress) not in congresses:
            continue
        last_name = legislator['name']['last'].upper()
        state = utils.states[legislator['terms'][len(legislator['terms']) - 1]['state']].upper()[:7].strip()
        content_as_file = StringIO(read_file_content)
        content_parsed = csv.reader(content_as_file, delimiter=',')
        for icpsr_member in content_parsed:
            if bioguide == icpsr_member[10]:
                num_matches += 1
                write_id = int(icpsr_member[2])
        if 'icpsr' in legislator['id']:
            if write_id == legislator['id']['icpsr'] or write_id == '':
                continue
            elif write_id != legislator['id']['icpsr'] and write_id != '':
                error_log.writerow(['Incorrect_ID', 'NA', last_name[:8], state, 'NA', legislator['id']['icpsr'], write_id])
                print('ID updated for %s' % last_name)
        if num_matches == 1:
            legislator['id']['icpsr'] = int(write_id)
        elif state == 'GUAM' or state == 'PUERTO' or state == 'VIRGIN' or (state == 'DISTRIC') or (state == 'AMERICA') or (state == 'NORTHER') or (state == 'PHILIPP'):
            print('error: non 1 match')
            error_log.writerow(['Non_1_match_number', str(num_matches), last_name[:8], state, 'Y', 'NA', 'NA'])
        else:
            print(str(num_matches) + ' matches found for ' + last_name[:8] + ', ' + state + ' in congress ' + str(congress))
            error_log.writerow(['Non_1_match_number', str(num_matches), last_name, state, 'N', 'NA', 'NA'])
    save_data(data_file[0], data_file[1])","[""for data_file in data_files:\n    (data_file_0, data_file_1, *_) = data_file\n    for legislator in data_file_0:\n        num_matches = 0\n        write_id = ''\n        bioguide = legislator['id'].get('bioguide', None)\n        if only_bioguide and bioguide != only_bioguide:\n            continue\n        chamber = legislator['terms'][len(legislator['terms']) - 1]['type']\n        if chamber != read_file_chamber:\n            continue\n        latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms']) - 1]['start'])))\n        if chamber == 'sen':\n            congresses = [latest_congress, latest_congress + 1, latest_congress + 2]\n        else:\n            congresses = [latest_congress]\n        if int(congress) not in congresses:\n            continue\n        last_name = legislator['name']['last'].upper()\n        state = utils.states[legislator['terms'][len(legislator['terms']) - 1]['state']].upper()[:7].strip()\n        content_as_file = StringIO(read_file_content)\n        content_parsed = csv.reader(content_as_file, delimiter=',')\n        for icpsr_member in content_parsed:\n            if bioguide == icpsr_member[10]:\n                num_matches += 1\n                write_id = int(icpsr_member[2])\n        if 'icpsr' in legislator['id']:\n            if write_id == legislator['id']['icpsr'] or write_id == '':\n                continue\n            elif write_id != legislator['id']['icpsr'] and write_id != '':\n                error_log.writerow(['Incorrect_ID', 'NA', last_name[:8], state, 'NA', legislator['id']['icpsr'], write_id])\n                print('ID updated for %s' % last_name)\n        if num_matches == 1:\n            legislator['id']['icpsr'] = int(write_id)\n        elif state == 'GUAM' or state == 'PUERTO' or state == 'VIRGIN' or (state == 'DISTRIC') or (state == 'AMERICA') or (state == 'NORTHER') or (state == 'PHILIPP'):\n            print('error: non 1 match')\n            error_log.writerow(['Non_1_match_number', str(num_matches), last_name[:8], state, 'Y', 'NA', 'NA'])\n        else:\n            print(str(num_matches) + ' matches found for ' + last_name[:8] + ', ' + state + ' in congress ' + str(congress))\n            error_log.writerow(['Non_1_match_number', str(num_matches), last_name, state, 'N', 'NA', 'NA'])\n    save_data(data_file_0, data_file_1)"", ""for (data_file_0, data_file_1, *data_file_len) in data_files:\n    for legislator in \n    data_file_0:\n        num_matches = 0\n        write_id = ''\n        bioguide = legislator['id'].get('bioguide', None)\n        if only_bioguide and bioguide != only_bioguide:\n            continue\n        chamber = legislator['terms'][len(legislator['terms']) - 1]['type']\n        if chamber != read_file_chamber:\n            continue\n        latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms']) - 1]['start'])))\n        if chamber == 'sen':\n            congresses = [latest_congress, latest_congress + 1, latest_congress + 2]\n        else:\n            congresses = [latest_congress]\n        if int(congress) not in congresses:\n            continue\n        last_name = legislator['name']['last'].upper()\n        state = utils.states[legislator['terms'][len(legislator['terms']) - 1]['state']].upper()[:7].strip()\n        content_as_file = StringIO(read_file_content)\n        content_parsed = csv.reader(content_as_file, delimiter=',')\n        for icpsr_member in content_parsed:\n            if bioguide == icpsr_member[10]:\n                num_matches += 1\n                write_id = int(icpsr_member[2])\n        if 'icpsr' in legislator['id']:\n            if write_id == legislator['id']['icpsr'] or write_id == '':\n                continue\n            elif write_id != legislator['id']['icpsr'] and write_id != '':\n                error_log.writerow(['Incorrect_ID', 'NA', last_name[:8], state, 'NA', legislator['id']['icpsr'], write_id])\n                print('ID updated for %s' % last_name)\n        if num_matches == 1:\n            legislator['id']['icpsr'] = int(write_id)\n        elif state == 'GUAM' or state == 'PUERTO' or state == 'VIRGIN' or (state == 'DISTRIC') or (state == 'AMERICA') or (state == 'NORTHER') or (state == 'PHILIPP'):\n            print('error: non 1 match')\n            error_log.writerow(['Non_1_match_number', str(num_matches), last_name[:8], state, 'Y', 'NA', 'NA'])\n        else:\n            print(str(num_matches) + ' matches found for ' + last_name[:8] + ', ' + state + ' in congress ' + str(congress))\n            error_log.writerow(['Non_1_match_number', str(num_matches), last_name, state, 'N', 'NA', 'NA'])\n    save_data(\n    data_file_0, \n    data_file_1)""]",no_found,0
freeipa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipatests/test_integration/test_epn.py,https://github.com/freeipa/freeipa/tree/master/ipatests/test_integration/test_epn.py,TestEPN,test_EPN_nbdays_1$559,"def test_EPN_nbdays_1(self, cleanupmail):
    """"""Test that for a given range, we find the users in that range""""""
    for date_range in [(0, 5), (7, 15), (1, 20)]:
        expected_user_list = ['user{i}'.format(i=i) for i in range(date_range[0], date_range[1])]
        (stdout_text_client, unused, _unused) = self._check_epn_output(self.clients[0], from_nbdays=date_range[0], to_nbdays=date_range[1])
        user_list = [user['uid'] for user in json.loads(stdout_text_client)]
        for user in expected_user_list:
            assert user in user_list
        for user in user_list:
            assert user in expected_user_list","for date_range in [(0, 5), (7, 15), (1, 20)]:
    expected_user_list = ['user{i}'.format(i=i) for i in range(date_range[0], date_range[1])]
    (stdout_text_client, unused, _unused) = self._check_epn_output(self.clients[0], from_nbdays=date_range[0], to_nbdays=date_range[1])
    user_list = [user['uid'] for user in json.loads(stdout_text_client)]
    for user in expected_user_list:
        assert user in user_list
    for user in user_list:
        assert user in expected_user_list","[""for date_range in [(0, 5), (7, 15), (1, 20)]:\n    (date_range_0, date_range_1, *_) = date_range\n    expected_user_list = ['user{i}'.format(i=i) for i in range(date_range_0, date_range_1)]\n    (stdout_text_client, unused, _unused) = self._check_epn_output(self.clients[0], from_nbdays=date_range_0, to_nbdays=date_range_1)\n    user_list = [user['uid'] for user in json.loads(stdout_text_client)]\n    for user in expected_user_list:\n        assert user in user_list\n    for user in user_list:\n        assert user in expected_user_list"", ""for (date_range_0, date_range_1, *date_range_len) in [(0, 5), (7, 15), (1, 20)]:\n    expected_user_list = ['user{i}'.format(i=i) for i in range(date_range_0, date_range_1)]\n    (stdout_text_client, unused, _unused) = self._check_epn_output(self.clients[0], from_nbdays=date_range_0, to_nbdays=date_range_1)\n    user_list = [user['uid'] for user in json.loads(stdout_text_client)]\n    for user in expected_user_list:\n        assert user in user_list\n    for user in user_list:\n        assert user in expected_user_list""]",no_found,0
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/packages/app/galaxy/queue_worker.py,https://github.com/ansible/galaxy/tree/master/packages/app/galaxy/queue_worker.py,,__job_rule_module_names$279,"def __job_rule_module_names(app):
    rules_module_names = {'galaxy.jobs.rules'}
    if app.job_config.dynamic_params is not None:
        module_name = app.job_config.dynamic_params.get('rules_module')
        if module_name:
            rules_module_names.add(module_name)
    for dest_tuple in app.job_config.destinations.values():
        module_name = dest_tuple[0].params.get('rules_module')
        if module_name:
            rules_module_names.add(module_name)
    return rules_module_names","for dest_tuple in app.job_config.destinations.values():
    module_name = dest_tuple[0].params.get('rules_module')
    if module_name:
        rules_module_names.add(module_name)","[""for dest_tuple in app.job_config.destinations.values():\n    (dest_tuple_0, *dest_tuple_rdest_tuplemaining) = dest_tuple\n    module_name = dest_tuple_0.params.get('rules_module')\n    if module_name:\n        rules_module_names.add(module_name)"", ""for (dest_tuple_0, *dest_tuple_len) in app.job_config.destinations.values():\n    module_name = \n    dest_tuple_0.params.get('rules_module')\n    if module_name:\n        rules_module_names.add(module_name)""]",no_found,0
core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/homeassistant/components/nzbget/coordinator.py,https://github.com/home-assistant/core/tree/master/homeassistant/components/nzbget/coordinator.py,NZBGetDataUpdateCoordinator,_check_completed_downloads$51,"def _check_completed_downloads(self, history):
    """"""Check history for newly completed downloads.""""""
    actual_completed_downloads = {(x['Name'], x['Category'], x['Status']) for x in history}
    if self._completed_downloads_init:
        tmp_completed_downloads = list(actual_completed_downloads.difference(self._completed_downloads))
        for download in tmp_completed_downloads:
            self.hass.bus.fire('nzbget_download_complete', {'name': download[0], 'category': download[1], 'status': download[2]})
    self._completed_downloads = actual_completed_downloads
    self._completed_downloads_init = True","for download in tmp_completed_downloads:
    self.hass.bus.fire('nzbget_download_complete', {'name': download[0], 'category': download[1], 'status': download[2]})","[""for download in tmp_completed_downloads:\n    (download_0, download_1, download_2, *_) = download\n    self.hass.bus.fire('nzbget_download_complete', {'name': download_0, 'category': download_1, 'status': download_2})"", ""for (download_0, download_1, download_2, *download_len) in tmp_completed_downloads:\n    self.hass.bus.fire('nzbget_download_complete', {'name': download_0, 'category': download_1, 'status': download_2})""]",no_found,
OpsManage,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpsManage/service/mysql/mysql_base.py,https://github.com/welliamcao/OpsManage/tree/master/service/mysql/mysql_base.py,MySQLBase,get_global_status$87,"def get_global_status(self):
    dataList = []
    logs = self.execute_for_query(sql='show global variables;')
    for ds in logs[1]:
        data = {}
        data['value'] = ds[1]
        data['name'] = ds[0].capitalize()
        dataList.append(data)
    return dataList","for ds in logs[1]:
    data = {}
    data['value'] = ds[1]
    data['name'] = ds[0].capitalize()
    dataList.append(data)","[""for ds in logs[1]:\n    (ds_0, ds_1, *_) = ds\n    data = {}\n    data['value'] = ds_1\n    data['name'] = ds_0.capitalize()\n    dataList.append(data)"", ""for (ds_0, ds_1, *ds_len) in logs[1]:\n    data = {}\n    data['value'] = \n    ds_1\n    data['name'] = \n    ds_0.capitalize()\n    dataList.append(data)""]",no_found,0
scanpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scanpy/scanpy/external/exporting.py,https://github.com/theislab/scanpy/tree/master/scanpy/external/exporting.py,,_write_edges$313,"def _write_edges(filename, edges):
    with open(filename, 'w') as f:
        for e in edges:
            f.write('%i;%i\n' % (e[0], e[1]))","for e in edges:
    f.write('%i;%i\n' % (e[0], e[1]))","[""for e in edges:\n    (e_0, e_1, *_) = e\n    f.write('%i;%i\\n' % (e_0, e_1))"", ""for (e_0, e_1, *e_len) in edges:\n    f.write('%i;%i\\n' % (e_0, e_1))""]",no_found,0
few-shot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/few-shot/tests/test_utils.py,https://github.com/oscarknagg/few-shot/tree/master/tests/test_utils.py,TestAutogradGraphRetrieval,test_retrieval$89,"def test_retrieval(self):
    """"""Create a simple autograd graph and check that the output is what is expected""""""
    x = torch.ones(2, 2, requires_grad=True)
    y = x + 2
    z = y * y
    out = z.mean()
    (nodes, edges) = autograd_graph(out)
    expected_nodes = ['MeanBackward1', 'MulBackward0', 'AddBackward0', 'AccumulateGrad']
    self.assertEqual(set(expected_nodes), set((n.__class__.__name__ for n in nodes)), 'autograd_graph() must return all nodes in the autograd graph.')
    expected_edges = [('MulBackward0', 'MeanBackward1'), ('AddBackward0', 'MulBackward0'), ('AccumulateGrad', 'AddBackward0')]
    for e in edges:
        self.assertIn((e[0].__class__.__name__, e[1].__class__.__name__), expected_edges, 'autograd_graph() must return all edges in the autograd graph.')
    num_y_squared_edges = 0
    for e in edges:
        if e[0].__class__.__name__ == 'AddBackward0' and e[1].__class__.__name__ == 'MulBackward0':
            num_y_squared_edges += 1
    self.assertEqual(num_y_squared_edges, 2, 'autograd_graph() must return multiple edges between nodes if they exist.')","for e in edges:
    if e[0].__class__.__name__ == 'AddBackward0' and e[1].__class__.__name__ == 'MulBackward0':
        num_y_squared_edges += 1","[""for e in edges:\n    (e_0, e_1, *_) = e\n    if e_0.__class__.__name__ == 'AddBackward0' and e_1.__class__.__name__ == 'MulBackward0':\n        num_y_squared_edges += 1"", ""for (e_0, e_1, *e_len) in edges:\n    if \n    e_0.__class__.__name__ == 'AddBackward0' and \n    e_1.__class__.__name__ == 'MulBackward0':\n        num_y_squared_edges += 1""]",no_found,0
few-shot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/few-shot/tests/test_utils.py,https://github.com/oscarknagg/few-shot/tree/master/tests/test_utils.py,TestAutogradGraphRetrieval,test_retrieval$89,"def test_retrieval(self):
    """"""Create a simple autograd graph and check that the output is what is expected""""""
    x = torch.ones(2, 2, requires_grad=True)
    y = x + 2
    z = y * y
    out = z.mean()
    (nodes, edges) = autograd_graph(out)
    expected_nodes = ['MeanBackward1', 'MulBackward0', 'AddBackward0', 'AccumulateGrad']
    self.assertEqual(set(expected_nodes), set((n.__class__.__name__ for n in nodes)), 'autograd_graph() must return all nodes in the autograd graph.')
    expected_edges = [('MulBackward0', 'MeanBackward1'), ('AddBackward0', 'MulBackward0'), ('AccumulateGrad', 'AddBackward0')]
    for e in edges:
        self.assertIn((e[0].__class__.__name__, e[1].__class__.__name__), expected_edges, 'autograd_graph() must return all edges in the autograd graph.')
    num_y_squared_edges = 0
    for e in edges:
        if e[0].__class__.__name__ == 'AddBackward0' and e[1].__class__.__name__ == 'MulBackward0':
            num_y_squared_edges += 1
    self.assertEqual(num_y_squared_edges, 2, 'autograd_graph() must return multiple edges between nodes if they exist.')","for e in edges:
    self.assertIn((e[0].__class__.__name__, e[1].__class__.__name__), expected_edges, 'autograd_graph() must return all edges in the autograd graph.')","[""for e in edges:\n    (e_0, e_1, *_) = e\n    self.assertIn((e_0.__class__.__name__, e_1.__class__.__name__), expected_edges, 'autograd_graph() must return all edges in the autograd graph.')"", ""for (e_0, e_1, *e_len) in edges:\n    self.assertIn((e_0.__class__.__name__, e_1.__class__.__name__), expected_edges, 'autograd_graph() must return all edges in the autograd graph.')""]",no_found,0
ros,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros/tools/rosunit/src/rosunit/baretest.py,https://github.com/ros/ros/tree/master/tools/rosunit/src/rosunit/baretest.py,,_format_errors$528,"def _format_errors(errors):
    formatted = []
    for e in errors:
        if '_testMethodName' in e[0].__dict__:
            formatted.append(e[0]._testMethodName)
        elif 'description' in e[0].__dict__:
            formatted.append('%s: %s\n' % (str(e[0].description), str(e[1])))
        else:
            formatted.append(str(e[0].__dict__))
    return formatted","for e in errors:
    if '_testMethodName' in e[0].__dict__:
        formatted.append(e[0]._testMethodName)
    elif 'description' in e[0].__dict__:
        formatted.append('%s: %s\n' % (str(e[0].description), str(e[1])))
    else:
        formatted.append(str(e[0].__dict__))","[""for e in errors:\n    (e_0, e_1, *_) = e\n    if '_testMethodName' in e_0.__dict__:\n        formatted.append(e_0._testMethodName)\n    elif 'description' in e_0.__dict__:\n        formatted.append('%s: %s\\n' % (str(e_0.description), str(e_1)))\n    else:\n        formatted.append(str(e_0.__dict__))"", ""for (e_0, e_1, *e_len) in errors:\n    if '_testMethodName' in \n    e_0.__dict__:\n        formatted.append(\n        e_0._testMethodName)\n    elif 'description' in \n    e_0.__dict__:\n        formatted.append('%s: %s\\n' % (str(\n        e_0.description), str(\n        e_1)))\n    else:\n        formatted.append(str(\n        e_0.__dict__))""]",no_found,0
networkx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/networkx/networkx/generators/line.py,https://github.com/networkx/networkx/tree/master/networkx/generators/line.py,,_odd_triangle$340,"def _odd_triangle(G, T):
    """"""Test whether T is an odd triangle in G

    Parameters
    ----------
    G : NetworkX Graph
    T : 3-tuple of vertices forming triangle in G

    Returns
    -------
    True is T is an odd triangle
    False otherwise

    Raises
    ------
    NetworkXError
        T is not a triangle in G

    Notes
    -----
    An odd triangle is one in which there exists another vertex in G which is
    adjacent to either exactly one or exactly all three of the vertices in the
    triangle.

    """"""
    for u in T:
        if u not in G.nodes():
            raise nx.NetworkXError(f'Vertex {u} not in graph')
    for e in list(combinations(T, 2)):
        if e[0] not in G[e[1]]:
            raise nx.NetworkXError(f'Edge ({e[0]}, {e[1]}) not in graph')
    T_neighbors = defaultdict(int)
    for t in T:
        for v in G[t]:
            if v not in T:
                T_neighbors[v] += 1
    for v in T_neighbors:
        if T_neighbors[v] in [1, 3]:
            return True
    return False","for e in list(combinations(T, 2)):
    if e[0] not in G[e[1]]:
        raise nx.NetworkXError(f'Edge ({e[0]}, {e[1]}) not in graph')","[""for e in list(combinations(T, 2)):\n    (e_0, e_1, *_) = e\n    if e_0 not in G[e_1]:\n        raise nx.NetworkXError(f'Edge ({e_0}, {e_1}) not in graph')"", ""for (e_0, e_1, *e_len) in list(combinations(T, 2)):\n    if \n    e_0 not in G[\n    e_1]:\n        raise nx.NetworkXError(f'Edge ({e_0}, {e_1}) not in graph')""]",no_found,0
Mycodo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/mycodo_flask/api/measurement.py,https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/api/measurement.py,MeasurementsPast,get$232,"def get(self, unique_id, unit, channel, past_seconds):
    """"""
        Return a list of measurements found within a duration from the past to the present
        """"""
    if not utils_general.user_has_permission('view_settings'):
        abort(403)
    if unit not in add_custom_units(Unit.query.all()):
        abort(422, custom='Unit ID not found')
    if channel < 0:
        abort(422, custom='channel must be >= 0')
    if past_seconds < 1:
        abort(422, custom='past_seconds must be >= 1')
    try:
        return_ = read_influxdb_list(unique_id, unit, channel, duration_sec=past_seconds)
        if return_ and len(return_) > 0:
            dict_return = {'measurements': []}
            for each_set in return_:
                dict_return['measurements'].append({'time': each_set[0], 'value': each_set[1]})
            return (dict_return, 200)
        else:
            return (return_, 200)
    except Exception:
        abort(500, message='An exception occurred', error=traceback.format_exc())","for each_set in return_:
    dict_return['measurements'].append({'time': each_set[0], 'value': each_set[1]})","[""for each_set in return_:\n    (each_set_0, each_set_1, *_) = each_set\n    dict_return['measurements'].append({'time': each_set_0, 'value': each_set_1})"", ""for (each_set_0, each_set_1, *each_set_len) in return_:\n    dict_return['measurements'].append({'time': each_set_0, 'value': each_set_1})""]",no_found,0
cogdl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cogdl/cogdl/models/emb/gatne.py,https://github.com/THUDM/cogdl/tree/master/cogdl/models/emb/gatne.py,,get_G_from_edges$335,"def get_G_from_edges(edges):
    edge_dict = dict()
    for edge in edges:
        edge_key = str(edge[0]) + '_' + str(edge[1])
        if edge_key not in edge_dict:
            edge_dict[edge_key] = 1
        else:
            edge_dict[edge_key] += 1
    tmp_G = nx.Graph()
    for edge_key in edge_dict:
        weight = edge_dict[edge_key]
        x = int(edge_key.split('_')[0])
        y = int(edge_key.split('_')[1])
        tmp_G.add_edge(x, y)
        tmp_G[x][y]['weight'] = weight
    return tmp_G","for edge in edges:
    edge_key = str(edge[0]) + '_' + str(edge[1])
    if edge_key not in edge_dict:
        edge_dict[edge_key] = 1
    else:
        edge_dict[edge_key] += 1","[""for edge in edges:\n    (edge_0, edge_1, *_) = edge\n    edge_key = str(edge_0) + '_' + str(edge_1)\n    if edge_key not in edge_dict:\n        edge_dict[edge_key] = 1\n    else:\n        edge_dict[edge_key] += 1"", ""for (edge_0, edge_1, *edge_len) in edges:\n    edge_key = str(edge_0) + '_' + str(edge_1)\n    if edge_key not in edge_dict:\n        edge_dict[edge_key] = 1\n    else:\n        edge_dict[edge_key] += 1""]",no_found,0
peregrine,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/peregrine/peregrinearb/tests/test_bellmannx.py,https://github.com/wardbradt/peregrine/tree/master/peregrinearb/tests/test_bellmannx.py,,build_graph_from_edge_list$51,"def build_graph_from_edge_list(edges, fee):
    graph = nx.DiGraph()
    for edge in edges:
        sell = edge[4] == 'SELL'
        graph.add_edge(edge[0], edge[1], weight=-math.log(edge[2] * (1 - fee)), depth=-math.log(edge[3]), trade_type=edge[4], fee=fee, no_fee_rate=edge[2] if sell else 1 / edge[2], market_name='{}/{}'.format(edge[0], edge[1]) if sell else '{}/{}'.format(edge[1], edge[0]))
    return graph","for edge in edges:
    sell = edge[4] == 'SELL'
    graph.add_edge(edge[0], edge[1], weight=-math.log(edge[2] * (1 - fee)), depth=-math.log(edge[3]), trade_type=edge[4], fee=fee, no_fee_rate=edge[2] if sell else 1 / edge[2], market_name='{}/{}'.format(edge[0], edge[1]) if sell else '{}/{}'.format(edge[1], edge[0]))","[""for edge in edges:\n    (edge_0, edge_1, edge_2, edge_3, edge_4, *_) = edge\n    sell = edge_4 == 'SELL'\n    graph.add_edge(edge_0, edge_1, weight=-math.log(edge_2 * (1 - fee)), depth=-math.log(edge_3), trade_type=edge_4, fee=fee, no_fee_rate=edge_2 if sell else 1 / edge_2, market_name='{}/{}'.format(edge_0, edge_1) if sell else '{}/{}'.format(edge_1, edge_0))"", ""for (edge_0, edge_1, edge_2, edge_3, edge_4, *edge_len) in edges:\n    sell = \n    edge_4 == 'SELL'\n    graph.add_edge(\n    edge_0, \n    edge_1, weight=-math.log(\n    edge_2 * (1 - fee)), depth=-math.log(\n    edge_3), trade_type=\n    edge_4, fee=fee, no_fee_rate=\n    edge_2 if sell else 1 / \n    edge_2, market_name='{}/{}'.format(\n    edge_0, \n    edge_1) if sell else '{}/{}'.format(\n    edge_1, \n    edge_0))""]",no_found,0
transitions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transitions/transitions/extensions/diagrams_pygraphviz.py,https://github.com/pytransitions/transitions/tree/master/transitions/extensions/diagrams_pygraphviz.py,Graph,get_graph$55,"def get_graph(self, title=None, roi_state=None):
    if title:
        self.fsm_graph.graph_attr['label'] = title
    if roi_state:
        filtered = _copy_agraph(self.fsm_graph)
        kept_nodes = set()
        active_state = roi_state.name if hasattr(roi_state, 'name') else roi_state
        if not filtered.has_node(roi_state):
            active_state += '_anchor'
        kept_nodes.add(active_state)
        for edge in filtered.edges():
            if active_state not in edge:
                filtered.delete_edge(edge)
        for edge in filtered.in_edges(active_state):
            if edge.attr['color'] == self.fsm_graph.style_attributes['edge']['previous']['color']:
                kept_nodes.add(edge[0])
            else:
                filtered.delete_edge(edge)
        for edge in filtered.out_edges_iter(active_state):
            kept_nodes.add(edge[1])
        for node in filtered.nodes():
            if node not in kept_nodes:
                filtered.delete_node(node)
        return filtered
    return self.fsm_graph","for edge in filtered.in_edges(active_state):
    if edge.attr['color'] == self.fsm_graph.style_attributes['edge']['previous']['color']:
        kept_nodes.add(edge[0])
    else:
        filtered.delete_edge(edge)","[""for edge in filtered.in_edges(active_state):\n    (edge_0, *edge_redgemaining) = edge\n    if edge.attr['color'] == self.fsm_graph.style_attributes['edge']['previous']['color']:\n        kept_nodes.add(edge_0)\n    else:\n        filtered.delete_edge(edge)""]",no_found,0
pytextrank,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytextrank/deprecated/oldsrc.py,https://github.com/DerwenAI/pytextrank/tree/master/deprecated/oldsrc.py,TextRank,write_dot$455,"def write_dot(self, path='graph.dot'):
    """"""
        output the lemma graph in Dot file format
        """"""
    keys = list(self.seen_lemma.keys())
    dot = graphviz.Digraph()
    for node_id in self.lemma_graph.nodes():
        text = keys[node_id][0].lower()
        rank = self.ranks[node_id]
        label = '{} ({:.4f})'.format(text, rank)
        dot.node(str(node_id), label)
    for edge in self.lemma_graph.edges():
        dot.edge(str(edge[0]), str(edge[1]), constraint='false')
    with open(path, 'w') as f:
        f.write(dot.source)","for edge in self.lemma_graph.edges():
    dot.edge(str(edge[0]), str(edge[1]), constraint='false')","[""for edge in self.lemma_graph.edges():\n    (edge_0, edge_1, *_) = edge\n    dot.edge(str(edge_0), str(edge_1), constraint='false')"", ""for (edge_0, edge_1, *edge_len) in self.lemma_graph.edges():\n    dot.edge(str(edge_0), str(edge_1), constraint='false')""]",no_found,0
geany-themes,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/geany-themes/scripts/autobump.py,https://github.com/codebrainz/geany-themes/tree/master/scripts/autobump.py,,write_log$26,"def write_log(log_file, entries):
    new_lines = []
    for ent in entries:
        new_lines.append('\t'.join((ent[1], ent[0])))
    open(log_file, 'w').write('\n'.join(new_lines) + '\n')","for ent in entries:
    new_lines.append('\t'.join((ent[1], ent[0])))","[""for ent in entries:\n    (ent_0, ent_1, *_) = ent\n    new_lines.append('\\t'.join((ent_1, ent_0)))"", ""for (ent_0, ent_1, *ent_len) in entries:\n    new_lines.append('\\t'.join((ent_1, ent_0)))""]",no_found,0
sphinx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sphinx/sphinx/writers/latex.py,https://github.com/sphinx-doc/sphinx/tree/master/sphinx/writers/latex.py,LaTeXTranslator,generate_indices$474,"def generate_indices(self) -> str:

    def generate(content: List[Tuple[str, List[IndexEntry]]], collapsed: bool) -> None:
        ret.append('\\begin{sphinxtheindex}' + CR)
        ret.append('\\let\\bigletter\\sphinxstyleindexlettergroup' + CR)
        for (i, (letter, entries)) in enumerate(content):
            if i > 0:
                ret.append('\\indexspace' + CR)
            ret.append('\\bigletter{%s}' % self.escape(letter) + CR)
            for entry in entries:
                if not entry[3]:
                    continue
                ret.append('\\item\\relax\\sphinxstyleindexentry{%s}' % self.encode(entry[0]))
                if entry[4]:
                    ret.append('\\sphinxstyleindexextra{%s}' % self.encode(entry[4]))
                ret.append('\\sphinxstyleindexpageref{%s:%s}' % (entry[2], self.idescape(entry[3])) + CR)
        ret.append('\\end{sphinxtheindex}' + CR)
    ret = []
    indices_config = self.config.latex_domain_indices
    if indices_config:
        for domain in self.builder.env.domains.values():
            for indexcls in domain.indices:
                indexname = '%s-%s' % (domain.name, indexcls.name)
                if isinstance(indices_config, list):
                    if indexname not in indices_config:
                        continue
                (content, collapsed) = indexcls(domain).generate(self.builder.docnames)
                if not content:
                    continue
                ret.append('\\renewcommand{\\indexname}{%s}' % indexcls.localname + CR)
                generate(content, collapsed)
    return ''.join(ret)","for entry in entries:
    if not entry[3]:
        continue
    ret.append('\\item\\relax\\sphinxstyleindexentry{%s}' % self.encode(entry[0]))
    if entry[4]:
        ret.append('\\sphinxstyleindexextra{%s}' % self.encode(entry[4]))
    ret.append('\\sphinxstyleindexpageref{%s:%s}' % (entry[2], self.idescape(entry[3])) + CR)","[""for entry in entries:\n    (entry_0, _, entry_2, entry_3, entry_4, *_) = entry\n    if not entry_3:\n        continue\n    ret.append('\\\\item\\\\relax\\\\sphinxstyleindexentry{%s}' % self.encode(entry_0))\n    if entry_4:\n        ret.append('\\\\sphinxstyleindexextra{%s}' % self.encode(entry_4))\n    ret.append('\\\\sphinxstyleindexpageref{%s:%s}' % (entry_2, self.idescape(entry_3)) + CR)"", ""for (entry_0, entry_1, entry_2, entry_3, entry_4, *entry_len) in entries:\n    if not \n    entry_3:\n        continue\n    ret.append('\\\\item\\\\relax\\\\sphinxstyleindexentry{%s}' % self.encode(\n    entry_0))\n    if \n    entry_4:\n        ret.append('\\\\sphinxstyleindexextra{%s}' % self.encode(\n        entry_4))\n    ret.append('\\\\sphinxstyleindexpageref{%s:%s}' % (\n    entry_2, self.idescape(\n    entry_3)) + CR)""]",no_found,0
dash,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dash/dash/testing/application_runners.py,https://github.com/plotly/dash/tree/master/dash/testing/application_runners.py,JuliaRunner,start$340,"def start(self, app, start_timeout=30, cwd=None):
    """"""Start the server with subprocess and julia.""""""
    if os.path.isfile(app) and os.path.exists(app):
        if not cwd:
            cwd = os.path.dirname(app)
            logger.info('JuliaRunner inferred cwd from app path: %s', cwd)
    else:
        self._tmp_app_path = os.path.join('/tmp' if not self.is_windows else os.getenv('TEMP'), uuid.uuid4().hex)
        try:
            os.mkdir(self.tmp_app_path)
        except OSError:
            logger.exception('cannot make temporary folder %s', self.tmp_app_path)
        path = os.path.join(self.tmp_app_path, 'app.jl')
        logger.info('JuliaRunner start => app is Julia code chunk')
        logger.info('make a temporary Julia file for execution => %s', path)
        logger.debug('content of the Dash.jl app')
        logger.debug('%s', app)
        with open(path, 'w') as fp:
            fp.write(app)
        app = path
        if not cwd:
            for entry in inspect.stack():
                if '/dash/testing/' not in entry[1].replace('\\', '/'):
                    cwd = os.path.dirname(os.path.realpath(entry[1]))
                    logger.warning('get cwd from inspect => %s', cwd)
                    break
        if cwd:
            logger.info('JuliaRunner inferred cwd from the Python call stack: %s', cwd)
            assets = [os.path.join(cwd, _) for _ in os.listdir(cwd) if not _.startswith('__') and os.path.isdir(os.path.join(cwd, _))]
            for asset in assets:
                target = os.path.join(self.tmp_app_path, os.path.basename(asset))
                if os.path.exists(target):
                    logger.debug('delete existing target %s', target)
                    shutil.rmtree(target)
                logger.debug('copying %s => %s', asset, self.tmp_app_path)
                shutil.copytree(asset, target)
                logger.debug('copied with %s', os.listdir(target))
        else:
            logger.warning('JuliaRunner found no cwd in the Python call stack. You may wish to specify an explicit working directory using something like: dashjl.run_server(app, cwd=os.path.dirname(__file__))')
    logger.info('Run Dash.jl app with julia => %s', app)
    args = shlex.split('julia {}'.format(os.path.realpath(app)), posix=not self.is_windows)
    logger.debug('start Dash.jl process with %s', args)
    try:
        self.proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=self.tmp_app_path if self.tmp_app_path else cwd)
        wait.until(lambda : self.accessible(self.url), timeout=start_timeout)
    except (OSError, ValueError):
        logger.exception('process server has encountered an error')
        self.started = False
        return
    self.started = True","for entry in inspect.stack():
    if '/dash/testing/' not in entry[1].replace('\\', '/'):
        cwd = os.path.dirname(os.path.realpath(entry[1]))
        logger.warning('get cwd from inspect => %s', cwd)
        break","[""for entry in inspect.stack():\n    (entry_0, entry_1, *entry_rentrymaining) = entry\n    if '/dash/testing/' not in entry_1.replace('\\\\', '/'):\n        cwd = os.path.dirname(os.path.realpath(entry_1))\n        logger.warning('get cwd from inspect => %s', cwd)\n        break"", ""for (entry_0, entry_1, *entry_len) in inspect.stack():\n    if '/dash/testing/' not in \n    entry_1.replace('\\\\', '/'):\n        cwd = os.path.dirname(os.path.realpath(\n        entry_1))\n        logger.warning('get cwd from inspect => %s', cwd)\n        break""]",no_found,0
rotki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rotki/rotkehlchen/db/dbhandler.py,https://github.com/rotki/rotki/tree/master/rotkehlchen/db/dbhandler.py,DBHandler,get_manually_tracked_balances$1356,"def get_manually_tracked_balances(self, cursor: 'DBCursor', balance_type: Optional[BalanceType]=BalanceType.ASSET) -> List[ManuallyTrackedBalance]:
    """"""Returns the manually tracked balances from the DB""""""
    query_balance_type = ''
    if balance_type is not None:
        query_balance_type = f'WHERE A.category=""{balance_type.serialize_for_db()}""'
    query = cursor.execute(f'SELECT A.asset, A.label, A.amount, A.location, group_concat(B.tag_name,"",""), A.category, A.id FROM manually_tracked_balances as A LEFT OUTER JOIN tag_mappings as B on B.object_reference = A.id {query_balance_type} GROUP BY label;')
    data = []
    for entry in query:
        tags = deserialize_tags_from_db(entry[4])
        try:
            balance_type = BalanceType.deserialize_from_db(entry[5])
            data.append(ManuallyTrackedBalance(id=entry[6], asset=Asset(entry[0]).check_existence(), label=entry[1], amount=FVal(entry[2]), location=Location.deserialize_from_db(entry[3]), tags=tags, balance_type=balance_type))
        except (DeserializationError, UnknownAsset, UnsupportedAsset, ValueError) as e:
            self.msg_aggregator.add_warning(f'Unexpected data in a ManuallyTrackedBalance entry in the DB: {str(e)}')
    return data","for entry in query:
    tags = deserialize_tags_from_db(entry[4])
    try:
        balance_type = BalanceType.deserialize_from_db(entry[5])
        data.append(ManuallyTrackedBalance(id=entry[6], asset=Asset(entry[0]).check_existence(), label=entry[1], amount=FVal(entry[2]), location=Location.deserialize_from_db(entry[3]), tags=tags, balance_type=balance_type))
    except (DeserializationError, UnknownAsset, UnsupportedAsset, ValueError) as e:
        self.msg_aggregator.add_warning(f'Unexpected data in a ManuallyTrackedBalance entry in the DB: {str(e)}')","[""for entry in query:\n    (entry_0, entry_1, entry_2, entry_3, entry_4, entry_5, entry_6, *_) = entry\n    tags = deserialize_tags_from_db(entry_4)\n    try:\n        balance_type = BalanceType.deserialize_from_db(entry_5)\n        data.append(ManuallyTrackedBalance(id=entry_6, asset=Asset(entry_0).check_existence(), label=entry_1, amount=FVal(entry_2), location=Location.deserialize_from_db(entry_3), tags=tags, balance_type=balance_type))\n    except (DeserializationError, UnknownAsset, UnsupportedAsset, ValueError) as e:\n        self.msg_aggregator.add_warning(f'Unexpected data in a ManuallyTrackedBalance entry in the DB: {str(e)}')"", ""for (entry_0, entry_1, entry_2, entry_3, entry_4, entry_5, entry_6, *entry_len) in query:\n    tags = deserialize_tags_from_db(entry_4)\n    try:\n        balance_type = BalanceType.deserialize_from_db(entry_5)\n        data.append(ManuallyTrackedBalance(id=entry_6, asset=Asset(entry_0).check_existence(), label=entry_1, amount=FVal(entry_2), location=Location.deserialize_from_db(entry_3), tags=tags, balance_type=balance_type))\n    except (DeserializationError, UnknownAsset, UnsupportedAsset, ValueError) as e:\n        self.msg_aggregator.add_warning(f'Unexpected data in a ManuallyTrackedBalance entry in the DB: {str(e)}')""]",no_found,0
aws-parallelcluster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-parallelcluster/awsbatch-cli/src/awsbatch/awsbsub.py,https://github.com/aws/aws-parallelcluster/tree/master/awsbatch-cli/src/awsbatch/awsbsub.py,AWSBsubCommand,run$433,"def run(self, job_definition, job_name, job_queue, command, nodes=None, vcpus=None, memory=None, array_size=None, retry_attempts=1, timeout=None, dependencies=None, env=None):
    """"""Submit the job.""""""
    try:
        array_properties = {}
        if array_size:
            array_properties.update(size=array_size)
        retry_strategy = {'attempts': retry_attempts}
        depends_on = dependencies if dependencies else []
        container_overrides = {'command': command}
        if vcpus:
            container_overrides.update(vcpus=vcpus)
        if memory:
            container_overrides.update(memory=memory)
        environment = []
        for env_var in env:
            environment.append({'name': env_var[0], 'value': env_var[1]})
        container_overrides.update(environment=environment)
        submission_args = {'jobName': job_name, 'jobQueue': job_queue, 'dependsOn': depends_on, 'retryStrategy': retry_strategy}
        if nodes:
            submission_args.update({'jobDefinition': job_definition})
            target_nodes = '0:'
            node_overrides = {'numNodes': nodes, 'nodePropertyOverrides': [{'targetNodes': target_nodes, 'containerOverrides': container_overrides}]}
            submission_args.update({'nodeOverrides': node_overrides})
            if timeout:
                submission_args.update({'timeout': {'attemptDurationSeconds': timeout}})
        else:
            submission_args.update({'jobDefinition': job_definition})
            submission_args.update({'containerOverrides': container_overrides})
            submission_args.update({'arrayProperties': array_properties})
            if timeout:
                submission_args.update({'timeout': {'attemptDurationSeconds': timeout}})
        self.log.debug('Job submission args: %s', submission_args)
        response = self.batch_client.submit_job(**submission_args)
        print('Job %s (%s) has been submitted.' % (response['jobId'], response['jobName']))
    except Exception as e:
        fail('Error submitting job to AWS Batch. Failed with exception: %s' % e)","for env_var in env:
    environment.append({'name': env_var[0], 'value': env_var[1]})","[""for env_var in env:\n    (env_var_0, env_var_1, *_) = env_var\n    environment.append({'name': env_var_0, 'value': env_var_1})"", ""for (env_var_0, env_var_1, *env_var_len) in env:\n    environment.append({'name': env_var_0, 'value': env_var_1})""]",no_found,0
joinmarket-clientserver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/joinmarket-clientserver/scripts/snicker/snicker-recovery.py,https://github.com/JoinMarket-Org/joinmarket-clientserver/tree/master/scripts/snicker/snicker-recovery.py,,main$74,"def main():
    parser = OptionParser(usage='usage: %prog [options] walletname', description=description)
    parser.add_option('-m', '--mixdepth', action='store', type='int', dest='mixdepth', default=0, help='mixdepth to source coins from')
    parser.add_option('-a', '--amtmixdepths', action='store', type='int', dest='amtmixdepths', help='number of mixdepths in wallet, default 5', default=5)
    parser.add_option('-g', '--gap-limit', type='int', action='store', dest='gaplimit', help='gap limit for wallet, default=6', default=6)
    add_base_options(parser)
    (options, args) = parser.parse_args()
    load_program_config(config_path=options.datadir)
    check_regtest()
    if len(args) != 1:
        log.error('Invalid arguments, see --help')
        sys.exit(EXIT_ARGERROR)
    wallet_name = args[0]
    wallet_path = get_wallet_path(wallet_name, None)
    max_mix_depth = max([options.mixdepth, options.amtmixdepths - 1])
    wallet = open_test_wallet_maybe(wallet_path, wallet_name, max_mix_depth, wallet_password_stdin=options.wallet_password_stdin, gap_limit=options.gaplimit)
    wallet_service = WalletService(wallet)
    if not options.recoversync:
        jmprint('Recovery sync was not set, but using it anyway.')
    while not wallet_service.synced:
        wallet_service.sync_wallet(fast=False)
    seed_transactions = wallet_service.get_all_transactions()
    if len(seed_transactions) == 0:
        jmprint('No transactions were found for this wallet. Did you rescan?')
        return False
    new_txs = []
    current_block_heights = set()
    for tx in seed_transactions:
        if btc.is_snicker_tx(tx):
            jmprint('Found a snicker tx: {}'.format(bintohex(tx.GetTxid()[::-1])))
            equal_outs = btc.get_equal_outs(tx)
            if not equal_outs:
                continue
            if all([wallet_service.is_known_script(x.scriptPubKey) == False for x in [a[1] for a in equal_outs]]):
                my_indices = get_pubs_and_indices_of_inputs(tx, wallet_service, ours=True)
                for (mypub, mi) in my_indices:
                    for eo in equal_outs:
                        for (other_pub, i) in get_pubs_and_indices_of_inputs(tx, wallet_service, ours=False):
                            for (our_pub, j) in get_pubs_and_indices_of_ancestor_inputs(tx.vin[mi], wallet_service, ours=True):
                                our_spk = wallet_service.pubkey_to_script(our_pub)
                                our_priv = wallet_service.get_key_from_addr(wallet_service.script_to_addr(our_spk))
                                tweak_bytes = btc.ecdh(our_priv[:-1], other_pub)
                                tweaked_pub = btc.snicker_pubkey_tweak(our_pub, tweak_bytes)
                                tweaked_spk = wallet_service.pubkey_to_script(tweaked_pub)
                                if tweaked_spk == eo[1].scriptPubKey:
                                    address_found = str(btc.CCoinAddress.from_scriptPubKey(btc.CScript(tweaked_spk)))
                                    jmprint('Found a new SNICKER output belonging to us.')
                                    jmprint('Output address {} in the following transaction:'.format(address_found))
                                    jmprint(btc.human_readable_transaction(tx))
                                    jmprint('Importing the address into the joinmarket wallet...')
                                    (success, msg) = wallet_service.check_tweak_matches_and_import(wallet_service.script_to_addr(our_spk), tweak_bytes, tweaked_pub, wallet_service.mixdepth)
                                    if not success:
                                        jmprint('Failed to import SNICKER key: {}'.format(msg), 'error')
                                        return False
                                    else:
                                        jmprint('... success.')
                                    current_block_heights.add(wallet_service.get_transaction_block_height(tx))
                                    new_txs.append(tx)
    if len(new_txs) == 0:
        return True
    seed_transactions.extend(new_txs)
    earliest_new_blockheight = min(current_block_heights)
    jmprint('New SNICKER addresses were imported to the Core wallet; do rescanblockchain again, starting from block {}, before restarting this script.'.format(earliest_new_blockheight))
    return False","for eo in equal_outs:
    for (other_pub, i) in get_pubs_and_indices_of_inputs(tx, wallet_service, ours=False):
        for (our_pub, j) in get_pubs_and_indices_of_ancestor_inputs(tx.vin[mi], wallet_service, ours=True):
            our_spk = wallet_service.pubkey_to_script(our_pub)
            our_priv = wallet_service.get_key_from_addr(wallet_service.script_to_addr(our_spk))
            tweak_bytes = btc.ecdh(our_priv[:-1], other_pub)
            tweaked_pub = btc.snicker_pubkey_tweak(our_pub, tweak_bytes)
            tweaked_spk = wallet_service.pubkey_to_script(tweaked_pub)
            if tweaked_spk == eo[1].scriptPubKey:
                address_found = str(btc.CCoinAddress.from_scriptPubKey(btc.CScript(tweaked_spk)))
                jmprint('Found a new SNICKER output belonging to us.')
                jmprint('Output address {} in the following transaction:'.format(address_found))
                jmprint(btc.human_readable_transaction(tx))
                jmprint('Importing the address into the joinmarket wallet...')
                (success, msg) = wallet_service.check_tweak_matches_and_import(wallet_service.script_to_addr(our_spk), tweak_bytes, tweaked_pub, wallet_service.mixdepth)
                if not success:
                    jmprint('Failed to import SNICKER key: {}'.format(msg), 'error')
                    return False
                else:
                    jmprint('... success.')
                current_block_heights.add(wallet_service.get_transaction_block_height(tx))
                new_txs.append(tx)","[""for eo in equal_outs:\n    (_, eo_1, *eo_reomaining) = eo\n    for (other_pub, i) in get_pubs_and_indices_of_inputs(tx, wallet_service, ours=False):\n        for (our_pub, j) in get_pubs_and_indices_of_ancestor_inputs(tx.vin[mi], wallet_service, ours=True):\n            our_spk = wallet_service.pubkey_to_script(our_pub)\n            our_priv = wallet_service.get_key_from_addr(wallet_service.script_to_addr(our_spk))\n            tweak_bytes = btc.ecdh(our_priv[:-1], other_pub)\n            tweaked_pub = btc.snicker_pubkey_tweak(our_pub, tweak_bytes)\n            tweaked_spk = wallet_service.pubkey_to_script(tweaked_pub)\n            if tweaked_spk == eo_1.scriptPubKey:\n                address_found = str(btc.CCoinAddress.from_scriptPubKey(btc.CScript(tweaked_spk)))\n                jmprint('Found a new SNICKER output belonging to us.')\n                jmprint('Output address {} in the following transaction:'.format(address_found))\n                jmprint(btc.human_readable_transaction(tx))\n                jmprint('Importing the address into the joinmarket wallet...')\n                (success, msg) = wallet_service.check_tweak_matches_and_import(wallet_service.script_to_addr(our_spk), tweak_bytes, tweaked_pub, wallet_service.mixdepth)\n                if not success:\n                    jmprint('Failed to import SNICKER key: {}'.format(msg), 'error')\n                    return False\n                else:\n                    jmprint('... success.')\n                current_block_heights.add(wallet_service.get_transaction_block_height(tx))\n                new_txs.append(tx)"", ""for (eo_0, eo_1, *eo_len) in equal_outs:\n    for (other_pub, i) in get_pubs_and_indices_of_inputs(tx, wallet_service, ours=False):\n        for (our_pub, j) in get_pubs_and_indices_of_ancestor_inputs(tx.vin[mi], wallet_service, ours=True):\n            our_spk = wallet_service.pubkey_to_script(our_pub)\n            our_priv = wallet_service.get_key_from_addr(wallet_service.script_to_addr(our_spk))\n            tweak_bytes = btc.ecdh(our_priv[:-1], other_pub)\n            tweaked_pub = btc.snicker_pubkey_tweak(our_pub, tweak_bytes)\n            tweaked_spk = wallet_service.pubkey_to_script(tweaked_pub)\n            if tweaked_spk == \n            eo_1.scriptPubKey:\n                address_found = str(btc.CCoinAddress.from_scriptPubKey(btc.CScript(tweaked_spk)))\n                jmprint('Found a new SNICKER output belonging to us.')\n                jmprint('Output address {} in the following transaction:'.format(address_found))\n                jmprint(btc.human_readable_transaction(tx))\n                jmprint('Importing the address into the joinmarket wallet...')\n                (success, msg) = wallet_service.check_tweak_matches_and_import(wallet_service.script_to_addr(our_spk), tweak_bytes, tweaked_pub, wallet_service.mixdepth)\n                if not success:\n                    jmprint('Failed to import SNICKER key: {}'.format(msg), 'error')\n                    return False\n                else:\n                    jmprint('... success.')\n                current_block_heights.add(wallet_service.get_transaction_block_height(tx))\n                new_txs.append(tx)""]",no_found,0
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/objects/tvshows.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/objects/tvshows.py,TVShows,remove$519,"def remove(self, item_id, e_item):
    """""" Remove showid, fileid, pathid, jellyfin reference.
            There's no episodes left, delete show and any possible remaining seasons
        """"""
    obj = {'Id': item_id}
    try:
        obj['KodiId'] = e_item[0]
        obj['FileId'] = e_item[1]
        obj['ParentId'] = e_item[3]
        obj['Media'] = e_item[4]
    except TypeError:
        return
    if obj['Media'] == 'episode':
        temp_obj = dict(obj)
        self.remove_episode(obj['KodiId'], obj['FileId'], obj['Id'])
        season = self.jellyfin_db.get_full_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_season_obj))
        try:
            temp_obj['Id'] = season[0]
            temp_obj['ParentId'] = season[1]
        except TypeError:
            return
        if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
            self.remove_season(obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))
        temp_obj['Id'] = self.jellyfin_db.get_item_by_kodi_id(*values(temp_obj, QUEM.get_item_by_parent_tvshow_obj))
        if not self.get_total_episodes(*values(temp_obj, QU.get_total_episodes_obj)):
            for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
                self.remove_season(season[1], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))
            self.remove_tvshow(temp_obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))
    elif obj['Media'] == 'tvshow':
        obj['ParentId'] = obj['KodiId']
        for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):
            temp_obj = dict(obj)
            temp_obj['ParentId'] = season[1]
            for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
                self.remove_episode(episode[1], episode[2], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
        else:
            self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))
        self.remove_tvshow(obj['KodiId'], obj['Id'])
    elif obj['Media'] == 'season':
        for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
            self.remove_episode(episode[1], episode[2], obj['Id'])
        else:
            self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))
        self.remove_season(obj['KodiId'], obj['Id'])
        if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj)):
            self.remove_tvshow(obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_tvshow_obj))
    for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
        self.remove_episode(episode[2], episode[3], obj['Id'])
    else:
        self.jellyfin_db.remove_media_by_parent_id(obj['Id'])
    self.jellyfin_db.remove_item(*values(obj, QUEM.delete_item_obj))","for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
    self.remove_episode(episode[1], episode[2], obj['Id'])
else:
    self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))","[""for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):\n    (_, episode_1, episode_2, *episode_repisodemaining) = episode\n    self.remove_episode(episode_1, episode_2, obj['Id'])\nelse:\n    self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))"", ""for (episode_0, episode_1, episode_2, *episode_len) in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):\n    self.remove_episode(episode_1, episode_2, obj['Id'])\nelse:\n    self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))""]",no_found,0
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/objects/tvshows.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/objects/tvshows.py,TVShows,remove$519,"def remove(self, item_id, e_item):
    """""" Remove showid, fileid, pathid, jellyfin reference.
            There's no episodes left, delete show and any possible remaining seasons
        """"""
    obj = {'Id': item_id}
    try:
        obj['KodiId'] = e_item[0]
        obj['FileId'] = e_item[1]
        obj['ParentId'] = e_item[3]
        obj['Media'] = e_item[4]
    except TypeError:
        return
    if obj['Media'] == 'episode':
        temp_obj = dict(obj)
        self.remove_episode(obj['KodiId'], obj['FileId'], obj['Id'])
        season = self.jellyfin_db.get_full_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_season_obj))
        try:
            temp_obj['Id'] = season[0]
            temp_obj['ParentId'] = season[1]
        except TypeError:
            return
        if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
            self.remove_season(obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))
        temp_obj['Id'] = self.jellyfin_db.get_item_by_kodi_id(*values(temp_obj, QUEM.get_item_by_parent_tvshow_obj))
        if not self.get_total_episodes(*values(temp_obj, QU.get_total_episodes_obj)):
            for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
                self.remove_season(season[1], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))
            self.remove_tvshow(temp_obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))
    elif obj['Media'] == 'tvshow':
        obj['ParentId'] = obj['KodiId']
        for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):
            temp_obj = dict(obj)
            temp_obj['ParentId'] = season[1]
            for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
                self.remove_episode(episode[1], episode[2], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
        else:
            self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))
        self.remove_tvshow(obj['KodiId'], obj['Id'])
    elif obj['Media'] == 'season':
        for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
            self.remove_episode(episode[1], episode[2], obj['Id'])
        else:
            self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))
        self.remove_season(obj['KodiId'], obj['Id'])
        if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj)):
            self.remove_tvshow(obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_tvshow_obj))
    for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
        self.remove_episode(episode[2], episode[3], obj['Id'])
    else:
        self.jellyfin_db.remove_media_by_parent_id(obj['Id'])
    self.jellyfin_db.remove_item(*values(obj, QUEM.delete_item_obj))","for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
    self.remove_episode(episode[1], episode[2], obj['Id'])
else:
    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))","[""for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):\n    (_, episode_1, episode_2, *episode_repisodemaining) = episode\n    self.remove_episode(episode_1, episode_2, obj['Id'])\nelse:\n    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))"", ""for (episode_0, episode_1, episode_2, *episode_len) in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):\n    self.remove_episode(episode_1, episode_2, obj['Id'])\nelse:\n    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))""]",no_found,0
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/objects/tvshows.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/objects/tvshows.py,TVShows,remove$519,"def remove(self, item_id, e_item):
    """""" Remove showid, fileid, pathid, jellyfin reference.
            There's no episodes left, delete show and any possible remaining seasons
        """"""
    obj = {'Id': item_id}
    try:
        obj['KodiId'] = e_item[0]
        obj['FileId'] = e_item[1]
        obj['ParentId'] = e_item[3]
        obj['Media'] = e_item[4]
    except TypeError:
        return
    if obj['Media'] == 'episode':
        temp_obj = dict(obj)
        self.remove_episode(obj['KodiId'], obj['FileId'], obj['Id'])
        season = self.jellyfin_db.get_full_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_season_obj))
        try:
            temp_obj['Id'] = season[0]
            temp_obj['ParentId'] = season[1]
        except TypeError:
            return
        if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
            self.remove_season(obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))
        temp_obj['Id'] = self.jellyfin_db.get_item_by_kodi_id(*values(temp_obj, QUEM.get_item_by_parent_tvshow_obj))
        if not self.get_total_episodes(*values(temp_obj, QU.get_total_episodes_obj)):
            for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
                self.remove_season(season[1], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))
            self.remove_tvshow(temp_obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))
    elif obj['Media'] == 'tvshow':
        obj['ParentId'] = obj['KodiId']
        for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):
            temp_obj = dict(obj)
            temp_obj['ParentId'] = season[1]
            for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
                self.remove_episode(episode[1], episode[2], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
        else:
            self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))
        self.remove_tvshow(obj['KodiId'], obj['Id'])
    elif obj['Media'] == 'season':
        for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
            self.remove_episode(episode[1], episode[2], obj['Id'])
        else:
            self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))
        self.remove_season(obj['KodiId'], obj['Id'])
        if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj)):
            self.remove_tvshow(obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_tvshow_obj))
    for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
        self.remove_episode(episode[2], episode[3], obj['Id'])
    else:
        self.jellyfin_db.remove_media_by_parent_id(obj['Id'])
    self.jellyfin_db.remove_item(*values(obj, QUEM.delete_item_obj))","for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
    self.remove_episode(episode[2], episode[3], obj['Id'])
else:
    self.jellyfin_db.remove_media_by_parent_id(obj['Id'])","[""for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):\n    (_, _, episode_2, episode_3, *_) = episode\n    self.remove_episode(episode_2, episode_3, obj['Id'])\nelse:\n    self.jellyfin_db.remove_media_by_parent_id(obj['Id'])""]",no_found,0
freeipa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/topology.py,https://github.com/freeipa/freeipa/tree/master/ipaserver/topology.py,,_format_topology_errors$116,"def _format_topology_errors(topo_errors):
    msg_lines = []
    for error in topo_errors:
        msg_lines.append(_('Topology does not allow server %(server)s to replicate with servers:') % {'server': error[0]})
        for srv in error[2]:
            msg_lines.append('    %s' % srv)
    return '\n'.join(msg_lines)","for error in topo_errors:
    msg_lines.append(_('Topology does not allow server %(server)s to replicate with servers:') % {'server': error[0]})
    for srv in error[2]:
        msg_lines.append('    %s' % srv)","[""for error in topo_errors:\n    (error_0, _, error_2, *error_rerrormaining) = error\n    msg_lines.append(_('Topology does not allow server %(server)s to replicate with servers:') % {'server': error_0})\n    for srv in error_2:\n        msg_lines.append('    %s' % srv)"", ""for (error_0, error_1, error_2, *error_len) in topo_errors:\n    msg_lines.append(_('Topology does not allow server %(server)s to replicate with servers:') % {'server': \n    error_0})\n    for srv in \n    error_2:\n        msg_lines.append('    %s' % srv)""]",no_found,0
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/tools/verify_headers.py,https://github.com/Qiskit/qiskit-terra/tree/master/tools/verify_headers.py,,_main$81,"def _main():
    default_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'qiskit')
    parser = argparse.ArgumentParser(description='Check file headers.')
    parser.add_argument('paths', type=str, nargs='*', default=[default_path], help='Paths to scan by default uses ../qiskit from the script')
    args = parser.parse_args()
    files = discover_files(args.paths)
    with multiprocessing.Pool() as pool:
        res = pool.map(validate_header, files)
    failed_files = [x for x in res if x[1] is False]
    if len(failed_files) > 0:
        for failed_file in failed_files:
            sys.stderr.write('%s failed header check because:\n' % failed_file[0])
            sys.stderr.write('%s\n\n' % failed_file[2])
        sys.exit(1)
    sys.exit(0)","for failed_file in failed_files:
    sys.stderr.write('%s failed header check because:\n' % failed_file[0])
    sys.stderr.write('%s\n\n' % failed_file[2])","[""for failed_file in failed_files:\n    (failed_file_0, _, failed_file_2, *failed_file_rfailed_filemaining) = failed_file\n    sys.stderr.write('%s failed header check because:\\n' % failed_file_0)\n    sys.stderr.write('%s\\n\\n' % failed_file_2)"", ""for (failed_file_0, failed_file_1, failed_file_2, *failed_file_len) in failed_files:\n    sys.stderr.write('%s failed header check because:\\n' % failed_file_0)\n    sys.stderr.write('%s\\n\\n' % failed_file_2)""]",no_found,0
sphinx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sphinx/sphinx/ext/autodoc/typehints.py,https://github.com/sphinx-doc/sphinx/tree/master/sphinx/ext/autodoc/typehints.py,,modify_field_list$90,"def modify_field_list(node: nodes.field_list, annotations: Dict[str, str], suppress_rtype: bool=False) -> None:
    arguments: Dict[str, Dict[str, bool]] = {}
    fields = cast(Iterable[nodes.field], node)
    for field in fields:
        field_name = field[0].astext()
        parts = re.split(' +', field_name)
        if parts[0] == 'param':
            if len(parts) == 2:
                arg = arguments.setdefault(parts[1], {})
                arg['param'] = True
            elif len(parts) > 2:
                name = ' '.join(parts[2:])
                arg = arguments.setdefault(name, {})
                arg['param'] = True
                arg['type'] = True
        elif parts[0] == 'type':
            name = ' '.join(parts[1:])
            arg = arguments.setdefault(name, {})
            arg['type'] = True
        elif parts[0] == 'rtype':
            arguments['return'] = {'type': True}
    for (name, annotation) in annotations.items():
        if name == 'return':
            continue
        if '*' + name in arguments:
            name = '*' + name
            arguments.get(name)
        elif '**' + name in arguments:
            name = '**' + name
            arguments.get(name)
        else:
            arg = arguments.get(name, {})
        if not arg.get('type'):
            field = nodes.field()
            field += nodes.field_name('', 'type ' + name)
            field += nodes.field_body('', nodes.paragraph('', annotation))
            node += field
        if not arg.get('param'):
            field = nodes.field()
            field += nodes.field_name('', 'param ' + name)
            field += nodes.field_body('', nodes.paragraph('', ''))
            node += field
    if 'return' in annotations and 'return' not in arguments:
        annotation = annotations['return']
        if annotation == 'None' and suppress_rtype:
            return
        field = nodes.field()
        field += nodes.field_name('', 'rtype')
        field += nodes.field_body('', nodes.paragraph('', annotation))
        node += field","for field in fields:
    field_name = field[0].astext()
    parts = re.split(' +', field_name)
    if parts[0] == 'param':
        if len(parts) == 2:
            arg = arguments.setdefault(parts[1], {})
            arg['param'] = True
        elif len(parts) > 2:
            name = ' '.join(parts[2:])
            arg = arguments.setdefault(name, {})
            arg['param'] = True
            arg['type'] = True
    elif parts[0] == 'type':
        name = ' '.join(parts[1:])
        arg = arguments.setdefault(name, {})
        arg['type'] = True
    elif parts[0] == 'rtype':
        arguments['return'] = {'type': True}","[""for field in fields:\n    (field_0, *field_rfieldmaining) = field\n    field_name = field_0.astext()\n    parts = re.split(' +', field_name)\n    if parts[0] == 'param':\n        if len(parts) == 2:\n            arg = arguments.setdefault(parts[1], {})\n            arg['param'] = True\n        elif len(parts) > 2:\n            name = ' '.join(parts[2:])\n            arg = arguments.setdefault(name, {})\n            arg['param'] = True\n            arg['type'] = True\n    elif parts[0] == 'type':\n        name = ' '.join(parts[1:])\n        arg = arguments.setdefault(name, {})\n        arg['type'] = True\n    elif parts[0] == 'rtype':\n        arguments['return'] = {'type': True}"", ""for (field_0, *field_len) in fields:\n    field_name = \n    field_0.astext()\n    parts = re.split(' +', field_name)\n    if parts[0] == 'param':\n        if len(parts) == 2:\n            arg = arguments.setdefault(parts[1], {})\n            arg['param'] = True\n        elif len(parts) > 2:\n            name = ' '.join(parts[2:])\n            arg = arguments.setdefault(name, {})\n            arg['param'] = True\n            arg['type'] = True\n    elif parts[0] == 'type':\n        name = ' '.join(parts[1:])\n        arg = arguments.setdefault(name, {})\n        arg['type'] = True\n    elif parts[0] == 'rtype':\n        arguments['return'] = {'type': True}""]",no_found,0
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(sound_dir, annotation_dir, target_dir, mode, speaker_info, new_data_dir, speaker_details, text_format):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)
    segments = open(os.path.join(target_dir, 'segments'), 'w', encoding='utf-8')
    wavscp = open(os.path.join(target_dir, 'wav.scp'), 'w', encoding='utf-8')
    utt2spk = open(os.path.join(target_dir, 'utt2spk'), 'w', encoding='utf-8')
    spk2utt = open(os.path.join(target_dir, 'spk2utt'), 'w', encoding='utf-8')
    text = open(os.path.join(target_dir, 'text'), 'w', encoding='utf-8')
    name2spk = open(os.path.join(target_dir, 'name2spk'), 'w', encoding='utf-8')
    remix_script = open(os.path.join(target_dir, 'remix_script.sh'), 'w', encoding='utf-8')
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}
    if mode == 'trs':
        if not os.path.exists(os.path.join(target_dir, 'temp')):
            os.mkdir(os.path.join(target_dir, 'temp'))
        audio_set = set()
        for (root, dirs, files) in os.walk(sound_dir):
            for file in files:
                if file[-4:] == '.wav':
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for (root, dirs, files) in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == '.trs':
                    XMLRefine(os.path.join(root, file), os.path.join(target_dir, 'temp', file))
                    annotation_files[file] = os.path.join(target_dir, 'temp', file)
        for afile in annotation_files.keys():
            if afile == 'error':
                continue
            try:
                (audio_name, speakers, segment_info) = XMLProcessing(annotation_files[afile])
            except Exception:
                print('error process %s' % annotation_files[afile])
            audio_name = audio_name.replace(' ', '')
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if '%s.wav' % audio_name not in sound_files.keys():
                print('no audio found for annotation: %s' % afile)
                continue
            print('%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |' % (audio_name, sound_files['%s.wav' % audio_name]), file=wavscp)
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker]['name']] = name2spk_prep.get(speakers[speaker]['name'], spk_id)
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker]['name']]
                if name2spk_prep[speakers[speaker]['name']] == spk_id:
                    print('%s %s' % (speakers[speaker]['name'], PackZero(spk_id)), file=name2spk)
                    spk_id += 1
            for segment in segment_info:
                if segment[0] == 'None':
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = '%s_%s_%s' % (PackZero(spk), audio_name, PackZero(segment_number))
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print('warning segment %s in %s' % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue
                print('%s %s %s %s' % (segment_id, audio_name, segment[2], segment[3]), file=segments)
                print('%s %s' % (segment_id, PackZero(spk)), file=utt2spk)
                print('%s %s' % (segment_id, segment[1]), file=text)
                spk2utt_prep[spk] = spk2utt_prep.get(spk, '') + ' %s' % segment_id
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print('%s %s' % (spk, spk2utt_prep[spk]), file=spk2utt)
            print('successfully processing %s' % afile)
        shutil.rmtree(os.path.join(target_dir, 'temp'))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for (root, dirs, files) in os.walk(sound_dir):
            for file in files:
                if file[-4:] == '.wav':
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)
        for (root, dirs, files) in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == '.eaf':
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == 'error':
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            (left_channel_segments, right_channel_segments) = segment_info
            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print('sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1' % (sound_files[afile], os.path.join(new_data_dir, afile)), file=remix_script)
            print('%s-L %s-L.wav' % (afile, os.path.join(new_data_dir, afile)), file=wavscp)
            segment_number = 0
            for segment in left_channel_segments:
                segment_id = '%s_%s-L_%s' % (spk_info[0], afile, PackZero(segment_number))
                if float(segment[1]) > max_length:
                    continue
                print('%s %s-L %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
                print('%s %s' % (segment_id, spk_info[0]), file=utt2spk)
                print('%s %s' % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(spk_info[0], '') + ' %s' % segment_id
                segment_number += 1
            if len(right_channel_segments) > 0:
                print('sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2' % (sound_files[afile], os.path.join(new_data_dir, afile)), file=remix_script)
                print('%s-R %s-R.wav' % (afile, os.path.join(new_data_dir, afile)), file=wavscp)
                for segment in right_channel_segments:
                    segment_id = '%s_%s-R_%s' % (spk_info[1], afile, PackZero(segment_number))
                    if float(segment[1]) > max_length:
                        continue
                    print('%s %s-R %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
                    print('%s %s' % (segment_id, spk_info[1]), file=utt2spk)
                    print('%s %s' % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(spk_info[1], '') + ' %s' % segment_id
                    segment_number += 1
            print('successfully processing %s' % afile)
        for spk in spk2utt_prep.keys():
            print('%s %s' % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for file in files:
    if file[-4:] == '.wav':
        sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)","[""for file in files:\n    (*file_rfilemaining, file_nfileg_4, file_nfileg_3, file_nfileg_2, file_nfileg_1) = file\n    if file[-4:] == '.wav':\n        sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)""]",no_found,0
nicotine-plus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nicotine-plus/pynicotine/gtkgui/userbrowse.py,https://github.com/nicotine-plus/nicotine-plus/tree/master/pynicotine/gtkgui/userbrowse.py,UserBrowse,download_directory$557,"def download_directory(self, folder, prefix='', recurse=False):
    if folder is None:
        return
    self.frame.np.transfers.requested_folders[self.user][folder] = prefix
    destination = self.frame.np.transfers.get_folder_destination(self.user, folder)
    files = self.shares.get(folder)
    if files:
        if config.sections['transfers']['reverseorder']:
            files.sort(key=lambda x: x[1], reverse=True)
        for file_data in files:
            virtualpath = '\\'.join([folder, file_data[1]])
            size = file_data[2]
            (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, file_data[4])
            self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)
    if not recurse:
        return
    for (subdir, subf) in self.shares.items():
        if folder in subdir and folder != subdir:
            self.download_directory(subdir, prefix=os.path.join(destination, ''))","for file_data in files:
    virtualpath = '\\'.join([folder, file_data[1]])
    size = file_data[2]
    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, file_data[4])
    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)","[""for file_data in files:\n    (_, file_data_1, file_data_2, _, file_data_4, *_) = file_data\n    virtualpath = '\\\\'.join([folder, file_data_1])\n    size = file_data_2\n    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, file_data_4)\n    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)"", ""for (file_data_0, file_data_1, file_data_2, file_data_3, file_data_4, *file_data_len) in files:\n    virtualpath = '\\\\'.join([folder, \n    file_data_1])\n    size = \n    file_data_2\n    (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, \n    file_data_4)\n    self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)""]",no_found,0
LinOTP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LinOTP/linotp/tests/unit/lib/test_get_client.py,https://github.com/LinOTP/LinOTP/tree/master/linotp/tests/unit/lib/test_get_client.py,TestGetClientCase,test_get_client_from_request_by_forwarded$200,"def test_get_client_from_request_by_forwarded(self):
    """"""
         according to the spec the old expression is the same as the
         new one:

             X-Forwarded-For: 192.0.2.43, 2001:db8:cafe::17
        becomes:
            Forwarded: for=192.0.2.43, for=""[2001:db8:cafe::17]

        """"""
    global LinConfig
    forward_test_strings = [('for=192.0.2.43,for=""[2001:db8:cafe::17]"",for=unknown', '192.0.2.43'), ('for=""_gazonk""', '_gazonk'), ('for=""_gazonk:800""', '_gazonk'), ('For=""[2001:db8:cafe::17]:4711""', '2001:db8:cafe::17'), ('for=192.0.2.60;proto=http;by=203.0.113.43', '192.0.2.60'), ('for=192.0.2.43, for=198.51.100.17', '192.0.2.43')]
    LinConfig = {'client.FORWARDED': 'true', 'client.FORWARDED_PROXY': '121.121.121.121, 123.234.123.234'}
    environ = {'REMOTE_ADDR': '123.234.123.234'}
    for forward_test_string in forward_test_strings:
        environ['Forwarded'] = forward_test_string[0]
        request = Request(environ)
        client = _get_client_from_request(request)
        assert client == forward_test_string[1], client","for forward_test_string in forward_test_strings:
    environ['Forwarded'] = forward_test_string[0]
    request = Request(environ)
    client = _get_client_from_request(request)
    assert client == forward_test_string[1], client","[""for forward_test_string in forward_test_strings:\n    (forward_test_string_0, forward_test_string_1, *_) = forward_test_string\n    environ['Forwarded'] = forward_test_string_0\n    request = Request(environ)\n    client = _get_client_from_request(request)\n    assert client == forward_test_string_1, client"", ""for (forward_test_string_0, forward_test_string_1, *forward_test_string_len) in forward_test_strings:\n    environ['Forwarded'] = \n    forward_test_string_0\n    request = Request(environ)\n    client = _get_client_from_request(request)\n    assert client == \n    forward_test_string_1, client""]",no_found,0
osxphotos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/osxphotos/osxphotos/cli.py,https://github.com/RhetTbull/osxphotos/tree/master/osxphotos/cli.py,,export$1069,"def export(ctx, cli_obj, db, photos_library, keyword, person, album, folder, uuid, name, uuid_from_file, title, no_title, description, no_description, uti, ignore_case, edited, external_edit, favorite, not_favorite, hidden, not_hidden, shared, not_shared, from_date, to_date, from_time, to_time, verbose, missing, update, ignore_signature, only_new, dry_run, export_as_hardlink, touch_file, overwrite, retry, export_by_date, skip_edited, skip_original_if_edited, skip_bursts, skip_live, skip_raw, person_keyword, album_keyword, keyword_template, replace_keywords, description_template, finder_tag_template, finder_tag_keywords, xattr_template, current_name, convert_to_jpeg, jpeg_quality, sidecar, sidecar_drop_ext, only_photos, only_movies, burst, not_burst, live, not_live, download_missing, dest, exiftool, exiftool_path, exiftool_option, exiftool_merge_keywords, exiftool_merge_persons, ignore_date_modified, portrait, not_portrait, screenshot, not_screenshot, slow_mo, not_slow_mo, time_lapse, not_time_lapse, hdr, not_hdr, selfie, not_selfie, panorama, not_panorama, has_raw, directory, filename_template, jpeg_ext, strip, edited_suffix, original_suffix, place, no_place, location, no_location, has_comment, no_comment, has_likes, no_likes, label, deleted, deleted_only, use_photos_export, use_photokit, report, cleanup, add_exported_to_album, add_skipped_to_album, add_missing_to_album, exportdb, load_config, save_config, is_reference, beta, in_album, not_in_album, min_size, max_size, regex, selected, query_eval, query_function, duplicate, post_command, post_function, preview, preview_suffix, preview_if_missing):
    """"""Export photos from the Photos database.
    Export path DEST is required.
    Optionally, query the Photos database using 1 or more search options;
    if more than one option is provided, they are treated as ""AND""
    (e.g. search for photos matching all options).
    If no query options are provided, all photos will be exported.
    By default, all versions of all photos will be exported including edited
    versions, live photo movies, burst photos, and associated raw images.
    See --skip-edited, --skip-live, --skip-bursts, and --skip-raw options
    to modify this behavior.
    """"""
    cfg = ConfigOptions('export', locals(), ignore=['ctx', 'cli_obj', 'dest', 'load_config', 'save_config'])
    global VERBOSE
    VERBOSE = bool(verbose)
    if load_config:
        try:
            cfg.load_from_file(load_config)
        except ConfigOptionsLoadError as e:
            click.echo(click.style(f'Error parsing {load_config} config file: {e.message}', fg=CLI_COLOR_ERROR), err=True)
            raise click.Abort()
        db = cfg.db
        photos_library = cfg.photos_library
        keyword = cfg.keyword
        person = cfg.person
        album = cfg.album
        folder = cfg.folder
        name = cfg.name
        uuid = cfg.uuid
        uuid_from_file = cfg.uuid_from_file
        title = cfg.title
        no_title = cfg.no_title
        description = cfg.description
        no_description = cfg.no_description
        uti = cfg.uti
        ignore_case = cfg.ignore_case
        edited = cfg.edited
        external_edit = cfg.external_edit
        favorite = cfg.favorite
        not_favorite = cfg.not_favorite
        hidden = cfg.hidden
        not_hidden = cfg.not_hidden
        shared = cfg.shared
        not_shared = cfg.not_shared
        from_date = cfg.from_date
        to_date = cfg.to_date
        from_time = cfg.from_time
        to_time = cfg.to_time
        verbose = cfg.verbose
        missing = cfg.missing
        update = cfg.update
        ignore_signature = cfg.ignore_signature
        dry_run = cfg.dry_run
        export_as_hardlink = cfg.export_as_hardlink
        touch_file = cfg.touch_file
        overwrite = cfg.overwrite
        retry = cfg.retry
        export_by_date = cfg.export_by_date
        skip_edited = cfg.skip_edited
        skip_original_if_edited = cfg.skip_original_if_edited
        skip_bursts = cfg.skip_bursts
        skip_live = cfg.skip_live
        skip_raw = cfg.skip_raw
        person_keyword = cfg.person_keyword
        album_keyword = cfg.album_keyword
        keyword_template = cfg.keyword_template
        replace_keywords = cfg.replace_keywords
        description_template = cfg.description_template
        finder_tag_template = cfg.finder_tag_template
        finder_tag_keywords = cfg.finder_tag_keywords
        xattr_template = cfg.xattr_template
        current_name = cfg.current_name
        convert_to_jpeg = cfg.convert_to_jpeg
        jpeg_quality = cfg.jpeg_quality
        sidecar = cfg.sidecar
        sidecar_drop_ext = cfg.sidecar_drop_ext
        only_photos = cfg.only_photos
        only_movies = cfg.only_movies
        burst = cfg.burst
        not_burst = cfg.not_burst
        live = cfg.live
        not_live = cfg.not_live
        download_missing = cfg.download_missing
        exiftool = cfg.exiftool
        exiftool_path = cfg.exiftool_path
        exiftool_option = cfg.exiftool_option
        exiftool_merge_keywords = cfg.exiftool_merge_keywords
        exiftool_merge_persons = cfg.exiftool_merge_persons
        ignore_date_modified = cfg.ignore_date_modified
        portrait = cfg.portrait
        not_portrait = cfg.not_portrait
        screenshot = cfg.screenshot
        not_screenshot = cfg.not_screenshot
        slow_mo = cfg.slow_mo
        not_slow_mo = cfg.not_slow_mo
        time_lapse = cfg.time_lapse
        not_time_lapse = cfg.not_time_lapse
        hdr = cfg.hdr
        not_hdr = cfg.not_hdr
        selfie = cfg.selfie
        not_selfie = cfg.not_selfie
        panorama = cfg.panorama
        not_panorama = cfg.not_panorama
        has_raw = cfg.has_raw
        directory = cfg.directory
        filename_template = cfg.filename_template
        jpeg_ext = cfg.jpeg_ext
        strip = cfg.strip
        edited_suffix = cfg.edited_suffix
        original_suffix = cfg.original_suffix
        place = cfg.place
        no_place = cfg.no_place
        location = cfg.location
        no_location = cfg.no_location
        has_comment = cfg.has_comment
        no_comment = cfg.no_comment
        has_likes = cfg.has_likes
        no_likes = cfg.no_likes
        label = cfg.label
        deleted = cfg.deleted
        deleted_only = cfg.deleted_only
        use_photos_export = cfg.use_photos_export
        use_photokit = cfg.use_photokit
        report = cfg.report
        cleanup = cfg.cleanup
        add_exported_to_album = cfg.add_exported_to_album
        add_skipped_to_album = cfg.add_skipped_to_album
        add_missing_to_album = cfg.add_missing_to_album
        exportdb = cfg.exportdb
        beta = cfg.beta
        only_new = cfg.only_new
        in_album = cfg.in_album
        not_in_album = cfg.not_in_album
        min_size = cfg.min_size
        max_size = cfg.max_size
        regex = cfg.regex
        selected = cfg.selected
        query_eval = cfg.query_eval
        query_function = cfg.query_function
        duplicate = cfg.duplicate
        post_command = cfg.post_command
        post_function = cfg.post_function
        preview = cfg.preview
        preview_suffix = cfg.preview_suffix
        preview_if_missing = cfg.preview_if_missing
        VERBOSE = bool(verbose)
        verbose_(f'Loaded options from file {load_config}')
    verbose_(f'osxphotos version {__version__}')
    exclusive_options = [('favorite', 'not_favorite'), ('hidden', 'not_hidden'), ('title', 'no_title'), ('description', 'no_description'), ('only_photos', 'only_movies'), ('burst', 'not_burst'), ('live', 'not_live'), ('portrait', 'not_portrait'), ('screenshot', 'not_screenshot'), ('slow_mo', 'not_slow_mo'), ('time_lapse', 'not_time_lapse'), ('hdr', 'not_hdr'), ('selfie', 'not_selfie'), ('panorama', 'not_panorama'), ('export_by_date', 'directory'), ('export_as_hardlink', 'exiftool'), ('place', 'no_place'), ('deleted', 'deleted_only'), ('skip_edited', 'skip_original_if_edited'), ('export_as_hardlink', 'convert_to_jpeg'), ('export_as_hardlink', 'download_missing'), ('shared', 'not_shared'), ('has_comment', 'no_comment'), ('has_likes', 'no_likes'), ('in_album', 'not_in_album'), ('location', 'no_location')]
    dependent_options = [('missing', ('download_missing', 'use_photos_export')), ('jpeg_quality', 'convert_to_jpeg'), ('ignore_signature', 'update'), ('only_new', 'update'), ('exiftool_option', 'exiftool'), ('exiftool_merge_keywords', ('exiftool', 'sidecar')), ('exiftool_merge_persons', ('exiftool', 'sidecar'))]
    try:
        cfg.validate(exclusive=exclusive_options, dependent=dependent_options, cli=True)
    except ConfigOptionsInvalidError as e:
        click.echo(click.style(f'Incompatible export options: {e.message}', fg=CLI_COLOR_ERROR), err=True)
        raise click.Abort()
    if all((x in [s.lower() for s in sidecar] for x in ['json', 'exiftool'])):
        click.echo(click.style('Cannot use --sidecar json with --sidecar exiftool due to name collisions', fg=CLI_COLOR_ERROR), err=True)
        raise click.Abort()
    if xattr_template:
        for (attr, _) in xattr_template:
            if attr not in EXTENDED_ATTRIBUTE_NAMES:
                click.echo(click.style(f""Invalid attribute '{attr}' for --xattr-template; valid values are {', '.join(EXTENDED_ATTRIBUTE_NAMES_QUOTED)}"", fg=CLI_COLOR_ERROR), err=True)
                raise click.Abort()
    if save_config:
        verbose_(f'Saving options to file {save_config}')
        cfg.write_to_file(save_config)
    jpeg_quality = DEFAULT_JPEG_QUALITY if jpeg_quality is None else jpeg_quality
    edited_suffix = DEFAULT_EDITED_SUFFIX if edited_suffix is None else edited_suffix
    original_suffix = DEFAULT_ORIGINAL_SUFFIX if original_suffix is None else original_suffix
    preview_suffix = DEFAULT_PREVIEW_SUFFIX if preview_suffix is None else preview_suffix
    retry = 0 if not retry else retry
    if not os.path.isdir(dest):
        click.echo(click.style(f'DEST {dest} must be valid path', fg=CLI_COLOR_ERROR), err=True)
        raise click.Abort()
    dest = str(pathlib.Path(dest).resolve())
    if report and os.path.isdir(report):
        click.echo(click.style(f'report is a directory, must be file name', fg=CLI_COLOR_ERROR), err=True)
        raise click.Abort()
    (export_edited, export_bursts, export_live, export_raw) = [not x for x in [skip_edited, skip_bursts, skip_live, skip_raw]]
    if any([exiftool, exiftool_merge_keywords, exiftool_merge_persons]) and (not exiftool_path):
        try:
            exiftool_path = get_exiftool_path()
        except FileNotFoundError:
            click.echo(click.style('Could not find exiftool. Please download and install from https://exiftool.org/', fg=CLI_COLOR_ERROR), err=True)
            ctx.exit(2)
    if any([exiftool, exiftool_merge_keywords, exiftool_merge_persons]):
        verbose_(f'exiftool path: {exiftool_path}')
    photos = movies = True
    if only_movies:
        photos = False
    if only_photos:
        movies = False
    if uuid_from_file:
        uuid_list = list(uuid)
        uuid_list.extend(load_uuid_from_file(uuid_from_file))
        uuid = tuple(uuid_list)
    cli_db = cli_obj.db if cli_obj is not None else None
    db = get_photos_db(*photos_library, db, cli_db)
    if db is None:
        click.echo(cli.commands['export'].get_help(ctx), err=True)
        click.echo('\n\nLocated the following Photos library databases: ', err=True)
        _list_libraries()
        return
    if exportdb and exportdb != OSXPHOTOS_EXPORT_DB:
        if '/' in exportdb:
            click.echo(click.style(f'Error: --exportdb must be specified as filename not path; ' + f""export database will saved in export directory '{dest}'."", fg=CLI_COLOR_ERROR))
            raise click.Abort()
        elif pathlib.Path(pathlib.Path(dest) / OSXPHOTOS_EXPORT_DB).exists():
            click.echo(click.style(f""Warning: export database is '{exportdb}' but found '{OSXPHOTOS_EXPORT_DB}' in {dest}; using '{exportdb}'"", fg=CLI_COLOR_WARNING))
    export_db_path = os.path.join(dest, exportdb or OSXPHOTOS_EXPORT_DB)
    other_db_files = find_files_in_branch(dest, OSXPHOTOS_EXPORT_DB)
    if other_db_files:
        click.echo(click.style('WARNING: found other export database files in this destination directory branch.  ' + 'This likely means you are attempting to export files into a directory ' + 'that is either the parent or a child directory of a previous export. ' + 'Proceeding may cause your exported files to be overwritten.', fg=CLI_COLOR_WARNING), err=True)
        click.echo(f'You are exporting to {dest}, found {OSXPHOTOS_EXPORT_DB} files in:')
        for other_db in other_db_files:
            click.echo(f'{other_db}')
        click.confirm('Do you want to continue?', abort=True)
    if dry_run:
        export_db = ExportDBInMemory(export_db_path)
        fileutil = FileUtilNoOp
    else:
        export_db = ExportDB(export_db_path)
        fileutil = FileUtil
    if verbose_:
        if export_db.was_created:
            verbose_(f'Created export database {export_db_path}')
        else:
            verbose_(f'Using export database {export_db_path}')
        upgraded = export_db.was_upgraded
        if upgraded:
            verbose_(f'Upgraded export database {export_db_path} from version {upgraded[0]} to {upgraded[1]}')
    photosdb = osxphotos.PhotosDB(dbfile=db, verbose=verbose_, exiftool=exiftool_path)
    photosdb._beta = beta
    query_options = QueryOptions(keyword=keyword, person=person, album=album, folder=folder, uuid=uuid, title=title, no_title=no_title, description=description, no_description=no_description, ignore_case=ignore_case, edited=edited, external_edit=external_edit, favorite=favorite, not_favorite=not_favorite, hidden=hidden, not_hidden=not_hidden, missing=missing, not_missing=None, shared=shared, not_shared=not_shared, photos=photos, movies=movies, uti=uti, burst=burst, not_burst=not_burst, live=live, not_live=not_live, cloudasset=False, not_cloudasset=False, incloud=False, not_incloud=False, from_date=from_date, to_date=to_date, from_time=from_time, to_time=to_time, portrait=portrait, not_portrait=not_portrait, screenshot=screenshot, not_screenshot=not_screenshot, slow_mo=slow_mo, not_slow_mo=not_slow_mo, time_lapse=time_lapse, not_time_lapse=not_time_lapse, hdr=hdr, not_hdr=not_hdr, selfie=selfie, not_selfie=not_selfie, panorama=panorama, not_panorama=not_panorama, has_raw=has_raw, place=place, no_place=no_place, location=location, no_location=no_location, label=label, deleted=deleted, deleted_only=deleted_only, has_comment=has_comment, no_comment=no_comment, has_likes=has_likes, no_likes=no_likes, is_reference=is_reference, in_album=in_album, not_in_album=not_in_album, burst_photos=export_bursts, missing_bursts=download_missing and use_photokit or not download_missing, name=name, min_size=min_size, max_size=max_size, regex=regex, selected=selected, query_eval=query_eval, function=query_function, duplicate=duplicate)
    try:
        photos = photosdb.query(query_options)
    except ValueError as e:
        if 'Invalid query_eval CRITERIA:' in str(e):
            msg = str(e).split(':')[1]
            raise click.BadOptionUsage('query_eval', f'Invalid query-eval CRITERIA: {msg}')
        else:
            raise ValueError(e)
    if photos and only_new:
        previous_uuids = {uuid: 1 for uuid in export_db.get_previous_uuids()}
        photos = [p for p in photos if p.uuid not in previous_uuids]
    results = ExportResults()
    if photos:
        num_photos = len(photos)
        photo_str = 'photos' if num_photos > 1 else 'photo'
        click.echo(f'Exporting {num_photos} {photo_str} to {dest}...')
        start_time = time.perf_counter()
        original_name = not current_name
        album_export = PhotosAlbum(add_exported_to_album, verbose=verbose_) if add_exported_to_album else None
        album_skipped = PhotosAlbum(add_skipped_to_album, verbose=verbose_) if add_skipped_to_album else None
        album_missing = PhotosAlbum(add_missing_to_album, verbose=verbose_) if add_missing_to_album else None
        fp = open(os.devnull, 'w') if verbose else None
        with click.progressbar(photos, file=fp) as bar:
            for p in bar:
                export_results = export_photo(photo=p, dest=dest, verbose=verbose, export_by_date=export_by_date, sidecar=sidecar, sidecar_drop_ext=sidecar_drop_ext, update=update, ignore_signature=ignore_signature, export_as_hardlink=export_as_hardlink, overwrite=overwrite, export_edited=export_edited, skip_original_if_edited=skip_original_if_edited, original_name=original_name, export_live=export_live, download_missing=download_missing, exiftool=exiftool, exiftool_merge_keywords=exiftool_merge_keywords, exiftool_merge_persons=exiftool_merge_persons, directory=directory, filename_template=filename_template, export_raw=export_raw, album_keyword=album_keyword, person_keyword=person_keyword, keyword_template=keyword_template, description_template=description_template, export_db=export_db, fileutil=fileutil, dry_run=dry_run, touch_file=touch_file, edited_suffix=edited_suffix, original_suffix=original_suffix, use_photos_export=use_photos_export, convert_to_jpeg=convert_to_jpeg, jpeg_quality=jpeg_quality, ignore_date_modified=ignore_date_modified, use_photokit=use_photokit, exiftool_option=exiftool_option, strip=strip, jpeg_ext=jpeg_ext, replace_keywords=replace_keywords, retry=retry, export_dir=dest, export_preview=preview, preview_suffix=preview_suffix, preview_if_missing=preview_if_missing)
                if post_function:
                    for function in post_function:
                        verbose_(f'Calling post-function {function[1]}')
                        if not dry_run:
                            try:
                                function[0](p, export_results, verbose_)
                            except Exception as e:
                                click.secho(f'Error running post-function {function[1]}: {e}', fg=CLI_COLOR_ERROR, err=True)
                run_post_command(photo=p, post_command=post_command, export_results=export_results, export_dir=dest, dry_run=dry_run, exiftool_path=exiftool_path, export_db=export_db)
                if album_export and export_results.exported:
                    try:
                        album_export.add(p)
                        export_results.exported_album = [(filename, album_export.name) for filename in export_results.exported]
                    except Exception as e:
                        click.secho(f'Error adding photo {p.original_filename} ({p.uuid}) to album {album_export.name}: {e}', fg=CLI_COLOR_ERROR, err=True)
                if album_skipped and export_results.skipped:
                    try:
                        album_skipped.add(p)
                        export_results.skipped_album = [(filename, album_skipped.name) for filename in export_results.skipped]
                    except Exception as e:
                        click.secho(f'Error adding photo {p.original_filename} ({p.uuid}) to album {album_skipped.name}: {e}', fg=CLI_COLOR_ERROR, err=True)
                if album_missing and export_results.missing:
                    try:
                        album_missing.add(p)
                        export_results.missing_album = [(filename, album_missing.name) for filename in export_results.missing]
                    except Exception as e:
                        click.secho(f'Error adding photo {p.original_filename} ({p.uuid}) to album {album_missing.name}: {e}', fg=CLI_COLOR_ERROR, err=True)
                results += export_results
                photo_files = set(export_results.exported + export_results.new + export_results.updated + export_results.exif_updated + export_results.converted_to_jpeg + export_results.skipped)
                if finder_tag_keywords or finder_tag_template:
                    (tags_written, tags_skipped) = write_finder_tags(p, photo_files, keywords=finder_tag_keywords, keyword_template=keyword_template, album_keyword=album_keyword, person_keyword=person_keyword, exiftool_merge_keywords=exiftool_merge_keywords, finder_tag_template=finder_tag_template, strip=strip, export_dir=dest, export_db=export_db)
                    results.xattr_written.extend(tags_written)
                    results.xattr_skipped.extend(tags_skipped)
                if xattr_template:
                    (xattr_written, xattr_skipped) = write_extended_attributes(p, photo_files, xattr_template, strip=strip, export_dir=dest, export_db=export_db)
                    results.xattr_written.extend(xattr_written)
                    results.xattr_skipped.extend(xattr_skipped)
        if fp is not None:
            fp.close()
        photo_str_total = 'photos' if len(photos) != 1 else 'photo'
        if update:
            summary = f'Processed: {len(photos)} {photo_str_total}, exported: {len(results.new)}, updated: {len(results.updated)}, skipped: {len(results.skipped)}, updated EXIF data: {len(results.exif_updated)}, '
        else:
            summary = f'Processed: {len(photos)} {photo_str_total}, exported: {len(results.exported)}, '
        summary += f'missing: {len(results.missing)}, '
        summary += f'error: {len(results.error)}'
        if touch_file:
            summary += f', touched date: {len(results.touched)}'
        click.echo(summary)
        stop_time = time.perf_counter()
        click.echo(f'Elapsed time: {stop_time - start_time:.3f} seconds')
    else:
        click.echo('Did not find any photos to export')
    if cleanup:
        all_files = results.exported + results.skipped + results.exif_updated + results.touched + results.converted_to_jpeg + results.sidecar_json_written + results.sidecar_json_skipped + results.sidecar_exiftool_written + results.sidecar_exiftool_skipped + results.sidecar_xmp_written + results.sidecar_xmp_skipped + results.missing + [r[0] for r in results.error] + [str(pathlib.Path(export_db_path).resolve())]
        click.echo(f'Cleaning up {dest}')
        (cleaned_files, cleaned_dirs) = cleanup_files(dest, all_files, fileutil)
        file_str = 'files' if len(cleaned_files) != 1 else 'file'
        dir_str = 'directories' if len(cleaned_dirs) != 1 else 'directory'
        click.echo(f'Deleted: {len(cleaned_files)} {file_str}, {len(cleaned_dirs)} {dir_str}')
        results.deleted_files = cleaned_files
        results.deleted_directories = cleaned_dirs
    if report:
        verbose_(f'Writing export report to {report}')
        write_export_report(report, results)
    export_db.close()","for function in post_function:
    verbose_(f'Calling post-function {function[1]}')
    if not dry_run:
        try:
            function[0](p, export_results, verbose_)
        except Exception as e:
            click.secho(f'Error running post-function {function[1]}: {e}', fg=CLI_COLOR_ERROR, err=True)","[""for function in post_function:\n    (function_0, function_1, *_) = function\n    verbose_(f'Calling post-function {function_1}')\n    if not dry_run:\n        try:\n            function_0(p, export_results, verbose_)\n        except Exception as e:\n            click.secho(f'Error running post-function {function_1}: {e}', fg=CLI_COLOR_ERROR, err=True)"", ""for (function_0, function_1, *function_len) in post_function:\n    verbose_(f'Calling post-function {function_1}')\n    if not dry_run:\n        try:\n            function_0(p, export_results, verbose_)\n        except Exception as e:\n            click.secho(f'Error running post-function {function_1}: {e}', fg=CLI_COLOR_ERROR, err=True)""]",no_found,0
stellargraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stellargraph/tests/test_ensemble.py,https://github.com/stellargraph/stellargraph/tree/master/tests/test_ensemble.py,,test_evaluate_link_prediction$620,"def test_evaluate_link_prediction():
    tf.keras.backend.clear_session()
    edge_ids_test = np.array([[1, 2], [2, 3], [1, 3]])
    edge_labels_test = np.array([1, 1, 0])
    graph = example_graph_1(feature_size=4)
    gnn_models = [create_graphSAGE_model(graph, link_prediction=True), create_HinSAGE_model(graph, link_prediction=True)]
    for gnn_model in gnn_models:
        keras_model = gnn_model[1]
        generator = gnn_model[2]
        ens = Ensemble(keras_model, n_estimators=2, n_predictions=1)
        ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
        with pytest.raises(ValueError):
            ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
        with pytest.raises(ValueError):
            ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
        with pytest.raises(ValueError):
            ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
        (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
        assert len(test_metrics_mean) == len(test_metrics_std)
        assert len(test_metrics_mean.shape) == 1
        assert len(test_metrics_std.shape) == 1
        ens = BaggingEnsemble(keras_model, n_estimators=2, n_predictions=1)
        ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
        with pytest.raises(ValueError):
            ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
        with pytest.raises(ValueError):
            ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
        with pytest.raises(ValueError):
            ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
        (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
        assert len(test_metrics_mean) == len(test_metrics_std)
        assert len(test_metrics_mean.shape) == 1
        assert len(test_metrics_std.shape) == 1","for gnn_model in gnn_models:
    keras_model = gnn_model[1]
    generator = gnn_model[2]
    ens = Ensemble(keras_model, n_estimators=2, n_predictions=1)
    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
    assert len(test_metrics_mean) == len(test_metrics_std)
    assert len(test_metrics_mean.shape) == 1
    assert len(test_metrics_std.shape) == 1
    ens = BaggingEnsemble(keras_model, n_estimators=2, n_predictions=1)
    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)
    with pytest.raises(ValueError):
        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)
    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))
    assert len(test_metrics_mean) == len(test_metrics_std)
    assert len(test_metrics_mean.shape) == 1
    assert len(test_metrics_std.shape) == 1","[""for gnn_model in gnn_models:\n    (_, gnn_model_1, gnn_model_2, *gnn_model_rgnn_modelmaining) = gnn_model\n    keras_model = gnn_model_1\n    generator = gnn_model_2\n    ens = Ensemble(keras_model, n_estimators=2, n_predictions=1)\n    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)\n    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))\n    assert len(test_metrics_mean) == len(test_metrics_std)\n    assert len(test_metrics_mean.shape) == 1\n    assert len(test_metrics_std.shape) == 1\n    ens = BaggingEnsemble(keras_model, n_estimators=2, n_predictions=1)\n    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)\n    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))\n    assert len(test_metrics_mean) == len(test_metrics_std)\n    assert len(test_metrics_mean.shape) == 1\n    assert len(test_metrics_std.shape) == 1"", ""for (gnn_model_0, gnn_model_1, gnn_model_2, *gnn_model_len) in gnn_models:\n    keras_model = \n    gnn_model_1\n    generator = \n    gnn_model_2\n    ens = Ensemble(keras_model, n_estimators=2, n_predictions=1)\n    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)\n    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))\n    assert len(test_metrics_mean) == len(test_metrics_std)\n    assert len(test_metrics_mean.shape) == 1\n    assert len(test_metrics_std.shape) == 1\n    ens = BaggingEnsemble(keras_model, n_estimators=2, n_predictions=1)\n    ens.compile(optimizer=Adam(), loss=binary_crossentropy, weighted_metrics=['acc'])\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_ids_test, test_targets=edge_labels_test)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator, test_data=edge_labels_test, test_targets=None)\n    with pytest.raises(ValueError):\n        ens.evaluate(generator=generator.flow(edge_ids_test, edge_labels_test), test_data=edge_ids_test, test_targets=edge_labels_test)\n    (test_metrics_mean, test_metrics_std) = ens.evaluate(generator.flow(edge_ids_test, edge_labels_test))\n    assert len(test_metrics_mean) == len(test_metrics_std)\n    assert len(test_metrics_mean.shape) == 1\n    assert len(test_metrics_std.shape) == 1""]",no_found,0
Pyro4,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Pyro4/tests/PyroTests/test_httpgateway.py,https://github.com/irmen/Pyro4/tree/master/tests/PyroTests/test_httpgateway.py,WSGITestBase,_start_response$72,"def _start_response(self, status, headers):
    """"""A callback passed into the application, to simulate a wsgi
        environment.

        @param status: The response status of the application (""200"", ""404"", etc)
        @param headers: Any headers to begin the response with.
        """"""
    assert not self.response_started
    self.response_started = True
    self.status = status
    self.headers = headers
    for header in headers:
        if header[0] == 'Set-Cookie':
            var = header[1].split(';', 1)
            if len(var) > 1 and var[1][0:9] == ' Max-Age=':
                if int(var[1][9:]) > 0:
                    self.cookies.append(var[0])
                else:
                    index = self.cookies.index(var[0])
                    self.cookies.pop(index)","for header in headers:
    if header[0] == 'Set-Cookie':
        var = header[1].split(';', 1)
        if len(var) > 1 and var[1][0:9] == ' Max-Age=':
            if int(var[1][9:]) > 0:
                self.cookies.append(var[0])
            else:
                index = self.cookies.index(var[0])
                self.cookies.pop(index)","[""for header in headers:\n    (header_0, header_1, *_) = header\n    if header_0 == 'Set-Cookie':\n        var = header_1.split(';', 1)\n        if len(var) > 1 and var[1][0:9] == ' Max-Age=':\n            if int(var[1][9:]) > 0:\n                self.cookies.append(var[0])\n            else:\n                index = self.cookies.index(var[0])\n                self.cookies.pop(index)"", ""for (header_0, header_1, *header_len) in headers:\n    if \n    header_0 == 'Set-Cookie':\n        var = \n        header_1.split(';', 1)\n        if len(var) > 1 and var[1][0:9] == ' Max-Age=':\n            if int(var[1][9:]) > 0:\n                self.cookies.append(var[0])\n            else:\n                index = self.cookies.index(var[0])\n                self.cookies.pop(index)""]",no_found,0
stacker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stacker/stacker/tests/providers/aws/test_default.py,https://github.com/cloudtools/stacker/tree/master/stacker/tests/providers/aws/test_default.py,TestProviderInteractiveMode,test_select_update_method$945,"def test_select_update_method(self):
    for i in [[{'force_interactive': False, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:
        self.assertEquals(self.provider.select_update_method(**i[0]), i[1])","for i in [[{'force_interactive': False, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:
    self.assertEquals(self.provider.select_update_method(**i[0]), i[1])","[""for i in [[{'force_interactive': False, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:\n    (i_0, i_1, *_) = i\n    self.assertEquals(self.provider.select_update_method(**i_0), i_1)"", ""for (i_0, i_1, *i_len) in [[{'force_interactive': False, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.interactive_update_stack], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:\n    self.assertEquals(self.provider.select_update_method(**i_0), i_1)""]",no_found,0
stacker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stacker/stacker/tests/providers/aws/test_default.py,https://github.com/cloudtools/stacker/tree/master/stacker/tests/providers/aws/test_default.py,TestProviderDefaultMode,test_select_update_method$512,"def test_select_update_method(self):
    for i in [[{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': False}, self.provider.default_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.noninteractive_changeset_update], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:
        self.assertEquals(self.provider.select_update_method(**i[0]), i[1])","for i in [[{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': False}, self.provider.default_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.noninteractive_changeset_update], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:
    self.assertEquals(self.provider.select_update_method(**i[0]), i[1])","[""for i in [[{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': False}, self.provider.default_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.noninteractive_changeset_update], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:\n    (i_0, i_1, *_) = i\n    self.assertEquals(self.provider.select_update_method(**i_0), i_1)"", ""for (i_0, i_1, *i_len) in [[{'force_interactive': True, 'force_change_set': False}, self.provider.interactive_update_stack], [{'force_interactive': False, 'force_change_set': False}, self.provider.default_update_stack], [{'force_interactive': False, 'force_change_set': True}, self.provider.noninteractive_changeset_update], [{'force_interactive': True, 'force_change_set': True}, self.provider.interactive_update_stack]]:\n    self.assertEquals(self.provider.select_update_method(**i_0), i_1)""]",no_found,0
BBTz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BBTz/gctexposer.py,https://github.com/m4ll0k/BBTz/tree/master//gctexposer.py,,GCTExposer$39,"def GCTExposer(domain, more):
    (domains, next_) = contentParser(getContent(domain).content)
    for i in domains:
        if args.moreinfo:
            print('{_} -> {__}'.format(_=i[1], __=i[2]))
        else:
            print(i[1])
    range_ = next_[-1]
    for i in range(range_ - 1):
        (domains, next_) = contentParser(getNextContent(next_[1]))
        next_ = next_
        for ii in domains:
            if args.moreinfo:
                print('{_} -> {__}'.format(_=ii[1], __=ii[2]))
            else:
                print(ii[1])","for i in domains:
    if args.moreinfo:
        print('{_} -> {__}'.format(_=i[1], __=i[2]))
    else:
        print(i[1])","[""for i in domains:\n    (_, i_1, i_2, *i_rimaining) = i\n    if args.moreinfo:\n        print('{_} -> {__}'.format(_=i_1, __=i_2))\n    else:\n        print(i_1)"", ""for (i_0, i_1, i_2, *i_len) in domains:\n    if args.moreinfo:\n        print('{_} -> {__}'.format(_=i_1, __=i_2))\n    else:\n        print(i_1)""]",no_found,0
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/thread_modules.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/thread_modules.py,FindPosterThread,run$393,"def run(self):
    name = self.name
    url = self.url
    direct_url = self.direct_url
    fanart = os.path.join(TMPDIR, name + '-fanart.jpg')
    thumb = os.path.join(TMPDIR, name + '.jpg')
    fan_text = os.path.join(TMPDIR, name + '-fanart.txt')
    post_text = os.path.join(TMPDIR, name + '-poster.txt')
    logger.info(fanart)
    logger.info(thumb)
    final_link = ''
    m = []
    if site == 'Music':
        final = ''
        if self.copy_fanart and self.copy_poster and self.copy_summary:
            if not direct_url and (not url):
                nam = ui.metaengine.name_adjust(name)
                url = 'http://www.last.fm/search?q=' + nam
                logger.info(url)
            wiki = ''
            content = ccurl(url)
            soup = BeautifulSoup(content, 'lxml')
            link = soup.findAll('div', {'class': 'row clearfix'})
            name3 = ''
            for i in link:
                j = i.findAll('a')
                for k in j:
                    try:
                        url = k['href']
                        if '?q=' not in url:
                            logger.info(url)
                            break
                    except Exception as err:
                        print(err, '--108--')
            logger.info(url)
            if url.startswith('http'):
                url = url
            else:
                url = 'http://www.last.fm' + url
            logger.info(url)
            img_url = url + '/+images'
            wiki_url = url + '/+wiki'
            logger.info(wiki_url)
            content = ccurl(wiki_url)
            soup = BeautifulSoup(content, 'lxml')
            link = soup.find('div', {'class': 'wiki-content'})
            if link:
                wiki = link.text
                self.summary_signal.emit(name, wiki, 'summary')
            content = ccurl(img_url)
            soup = BeautifulSoup(content, 'lxml')
            link = soup.findAll('ul', {'class': 'image-list'})
            img = []
            for i in link:
                j = i.findAll('img')
                for k in j:
                    l = k['src']
                    u1 = l.rsplit('/', 2)[0]
                    u2 = l.split('/')[-1]
                    u = u1 + '/770x0/' + u2
                    img.append(u)
            img = list(set(img))
            logger.info(len(img))
            thumb = os.path.join(TMPDIR, name + '.jpg')
            if img:
                url = img[0]
                try:
                    ccurl(url, curl_opt='-o', out_file=thumb)
                except Exception as err:
                    print(err, '--151--')
        elif (self.copy_poster or self.copy_fanart) and url and direct_url:
            if 'last.fm' in url:
                logger.info('--artist-link---{0}'.format(url))
                content = ccurl(url)
                soup = BeautifulSoup(content, 'lxml')
                link = soup.findAll('img')
                url1Code = url.split('/')[-1]
                found = None
                for i in link:
                    if 'src' in str(i):
                        j = i['src']
                        k = j.split('/')[-1]
                        if url1Code == k:
                            found = j
                            break
                logger.info(str(found))
                if found:
                    u1 = found.rsplit('/', 2)[0]
                    u2 = found.split('/')[-1]
                    final = u1 + '/770x0/' + u2
                    logger.info(final)
            elif ('.jpg' in url or '.png' in url) and url.startswith('http'):
                final = url
            else:
                final = ''
            try:
                if final.startswith('http'):
                    ccurl(final, curl_opt='-o', out_file=thumb)
            except Exception as e:
                print(e)
    else:
        nam = ui.metaengine.name_adjust(name)
        src_site = 'tvdb'
        epn_arr = []
        post_val = ''
        fan_val = ''
        logger.debug('\nvideo_dir={0}\n'.format(self.video_dir))
        if site.lower() == 'video' and self.video_dir:
            video_db = os.path.join(ui.home_folder, 'VideoDB', 'Video.db')
            if os.path.exists(video_db):
                epn_arr_tmp = ui.media_data.get_video_db(video_db, 'Directory', self.video_dir)
                for i in epn_arr_tmp:
                    epn_name = i[0] + '\t' + i[1]
                    logger.debug(epn_name)
                    epn_arr.append(epn_name)
        elif self.video_dir:
            new_name_with_info = self.video_dir.strip()
            extra_info = ''
            if '\t' in new_name_with_info:
                name_title = new_name_with_info.split('\t')[0]
                extra_info = new_name_with_info.split('\t')[1]
            else:
                name_title = new_name_with_info
            if site.lower() == 'subbedanime' or (site.lower() == 'dubbedanime' and siteName):
                hist_site = os.path.join(ui.home_folder, 'History', site, siteName, name_title)
            else:
                hist_site = os.path.join(ui.home_folder, 'History', site, name_title)
            hist_epn = os.path.join(hist_site, 'Ep.txt')
            logger.info(hist_epn)
            if os.path.exists(hist_epn):
                lines = open_files(hist_epn, True)
                for i in lines:
                    i = i.strip()
                    j = i.split('\t')
                    if len(j) == 1:
                        epn_arr.append(i + '\t' + i + '\t' + name)
                    elif len(j) >= 2:
                        epn_arr.append(i + '\t' + name)
        if ui.series_info_dict.get(name) and (not epn_arr):
            logger.debug('getting values from cache')
            dict_val = ui.series_info_dict.get(name)
            post_arr = dict_val.get('poster')
            fan_arr = dict_val.get('fanart')
            fan_index = dict_val.get('f')
            post_index = dict_val.get('p')
            if fan_index < len(fan_arr):
                fan_val = fan_arr[fan_index]
                fan_index = (fan_index + 1) % len(fan_arr)
                dict_val.update({'f': fan_index})
            if post_index < len(post_arr):
                post_val = post_arr[post_index]
                post_index = (post_index + 1) % len(post_arr)
                dict_val.update({'p': post_index})
            ui.series_info_dict.update({name: dict_val})
            if isinstance(self.use_search, bool):
                src_site = 'tvdb'
            else:
                src_site = self.use_search
        else:
            (m, final_link, src_site) = self.init_search(nam, url, direct_url, thumb, fanart, src_site)
        if m and src_site in ['tvdb', 'tvdb+g', 'tvdb+ddg'] or post_val or fan_val:
            if post_val or fan_val:
                if post_val:
                    url = 'http://thetvdb.com/' + post_val
                    ccurl(url + '#' + '-o' + '#' + thumb)
                if fan_val:
                    url = 'http://thetvdb.com/' + fan_val
                    ccurl(url + '#' + '-o' + '#' + fanart)
            else:
                if not final_link:
                    n = re.sub('amp;', '', m[0])
                    elist = re.sub('tab=series', 'tab=seasonall', n)
                    url = 'http://thetvdb.com' + n
                    logger.info(url)
                    elist_url = 'http://thetvdb.com' + elist
                else:
                    url = final_link
                (post_arr, fan_arr) = self.parse_tvdb(name, url)
                if post_arr:
                    url = 'http://thetvdb.com/' + post_arr[0]
                    ccurl(url + '#' + '-o' + '#' + thumb)
                    logger.info(post_arr)
                if fan_arr:
                    fan_arr = [i for i in fan_arr if 'vignette' not in i]
                    if fan_arr:
                        url = 'http://thetvdb.com/' + fan_arr[0]
                        ccurl(url + '#' + '-o' + '#' + fanart)
                    logger.debug(fan_arr)
                fan_arr.sort()
                post_arr.sort()
                ui.series_info_dict.update({name: {'fanart': fan_arr.copy(), 'poster': post_arr.copy(), 'f': 0, 'p': 0}})
                elist_url = re.sub('tab=series', 'tab=seasonall', final_link)
                if epn_arr:
                    ui.metaengine.getTvdbEpnInfo(elist_url, epn_arr=epn_arr.copy(), site=site, name=name, thread=self, video_dir=self.video_dir)
                    image_dict = self.image_dict_list.copy()
                    dest_dir = self.dest_dir
                    self.imagesignal.emit(image_dict, dest_dir, site)
        elif m and src_site in ['tmdb', 'tmdb+g', 'tmdb+ddg']:
            self.parse_tmdb(name, final_link, thumb, fanart)","for i in epn_arr_tmp:
    epn_name = i[0] + '\t' + i[1]
    logger.debug(epn_name)
    epn_arr.append(epn_name)","[""for i in epn_arr_tmp:\n    (i_0, i_1, *_) = i\n    epn_name = i_0 + '\\t' + i_1\n    logger.debug(epn_name)\n    epn_arr.append(epn_name)"", ""for (i_0, i_1, *i_len) in epn_arr_tmp:\n    epn_name = \n    i_0 + '\\t' + \n    i_1\n    logger.debug(epn_name)\n    epn_arr.append(epn_name)""]",no_found,0
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/meta_engine.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/meta_engine.py,MetaEngine,get_epn_arr_list$195,"def get_epn_arr_list(self, site, name, video_dir):
    epn_arr = []
    if site.lower() == 'video' and video_dir:
        video_db = os.path.join(ui.home_folder, 'VideoDB', 'Video.db')
        if os.path.exists(video_db):
            epn_arr_tmp = ui.media_data.get_video_db(video_db, 'Directory', video_dir)
            for i in epn_arr_tmp:
                epn_name = i[0] + '\t' + i[1]
                logger.debug(epn_name)
                epn_arr.append(epn_name)
    elif video_dir:
        new_name_with_info = video_dir.strip()
        extra_info = ''
        if '\t' in new_name_with_info:
            name_title = new_name_with_info.split('\t')[0]
            extra_info = new_name_with_info.split('\t')[1]
        else:
            name_title = new_name_with_info
        if site.lower() == 'subbedanime' or site.lower() == 'dubbedanime':
            siteName = ui.get_parameters_value(s='siteName')['siteName']
            hist_site = os.path.join(ui.home_folder, 'History', site, siteName, name_title)
        else:
            hist_site = os.path.join(ui.home_folder, 'History', site, name_title)
        hist_epn = os.path.join(hist_site, 'Ep.txt')
        logger.info(hist_epn)
        if os.path.exists(hist_epn):
            lines = open_files(hist_epn, True)
            for i in lines:
                i = i.strip()
                j = i.split('\t')
                if len(j) == 1:
                    epn_arr.append(i + '\t' + i + '\t' + name)
                elif len(j) >= 2:
                    epn_arr.append(i + '\t' + name)
    return epn_arr","for i in epn_arr_tmp:
    epn_name = i[0] + '\t' + i[1]
    logger.debug(epn_name)
    epn_arr.append(epn_name)","[""for i in epn_arr_tmp:\n    (i_0, i_1, *_) = i\n    epn_name = i_0 + '\\t' + i_1\n    logger.debug(epn_name)\n    epn_arr.append(epn_name)"", ""for (i_0, i_1, *i_len) in epn_arr_tmp:\n    epn_name = \n    i_0 + '\\t' + \n    i_1\n    logger.debug(epn_name)\n    epn_arr.append(epn_name)""]",no_found,0
PersonRelationKnowledgeGraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PersonRelationKnowledgeGraph/collect_person_rel.py,https://github.com/liuhuanyong/PersonRelationKnowledgeGraph/tree/master//collect_person_rel.py,PersonSpider,modify_data$131,"def modify_data(self):
    f_rel = open('rel_data.txt', 'w+')
    f_reltype = open('rel_type.txt', 'w+')
    f_person = open('person2id.txt', 'w+')
    person_dict = {}
    rel_dict = {}
    rel_list = set()
    rel_types = []
    for item in self.conn['person_rel']['data2'].find():
        nodes = item['nodes']
        for node in nodes:
            id = node['id']
            name = node['name']
            person_dict[id] = name
    for item in self.conn['person_rel']['data2'].find():
        links = item['links']
        for link in links:
            from_person = person_dict.get(link['from'], '')
            to_person = person_dict.get(link['to'], '')
            if not from_person or not to_person:
                continue
            rel_name = link['name']
            rel_type = link['type']
            rel_dict[rel_name] = rel_type
            data = [from_person, to_person, rel_name, str(rel_type)]
            rel_list.add('###'.join(data))
    rels_num = len(rel_list)
    persons_num = len(person_dict.keys())
    for rel in rel_list:
        if len(rel.split('###')) != 4:
            continue
        rel_name = rel.split('###')[2]
        rel_types.append(rel_name)
    for (id, name) in person_dict.items():
        f_person.write(str(id) + '\t' + name + '\n')
    reltype_dict = Counter(rel_types).most_common()
    sum = 0.0
    for i in reltype_dict:
        rel_name = i[0]
        rel_freq = i[1]
        rel_percent = rel_freq / rels_num
        sum += rel_percent
        f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')
    f_rel.write('\n'.join(list(rel_list)))
    f_person.close()
    f_rel.close()
    f_reltype.close()
    print('rels_num', rels_num)
    print('persons_num', persons_num)
    return","for i in reltype_dict:
    rel_name = i[0]
    rel_freq = i[1]
    rel_percent = rel_freq / rels_num
    sum += rel_percent
    f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')","[""for i in reltype_dict:\n    (i_0, i_1, *_) = i\n    rel_name = i_0\n    rel_freq = i_1\n    rel_percent = rel_freq / rels_num\n    sum += rel_percent\n    f_reltype.write(rel_name + '\\t' + str(rel_freq) + '\\t' + str(rel_percent) + '\\t' + str(sum) + '\\n')"", ""for (i_0, i_1, *i_len) in reltype_dict:\n    rel_name = \n    i_0\n    rel_freq = \n    i_1\n    rel_percent = rel_freq / rels_num\n    sum += rel_percent\n    f_reltype.write(rel_name + '\\t' + str(rel_freq) + '\\t' + str(rel_percent) + '\\t' + str(sum) + '\\n')""]",no_found,0
quay,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quay/test/test_ldap.py,https://github.com/quay/quay/tree/master/test/test_ldap.py,TestLDAP,test_iterate_group_members$556,"def test_iterate_group_members(self):
    with mock_ldap() as ldap:
        (it, err) = ldap.iterate_group_members({'group_dn': 'cn=AwesomeFolk'}, disable_pagination=True)
        self.assertIsNone(err)
        results = list(it)
        self.assertEqual(4, len(results))
        first = results[0][0]
        second = results[1][0]
        third = results[2][0]
        fourth = results[3][0]
        assert all((x in [u[0].id for u in results] for x in ['testy', 'someuser', 'somesuperuser', 'somerestricteduser']))
        for i in results:
            u = i[0]
            if u.id == 'testy':
                self.assertEqual('testy', u.id)
                self.assertEqual('testy', u.username)
                self.assertEqual('bar@baz.com', u.email)
            if u.id == 'someuser':
                self.assertEqual('someuser', u.id)
                self.assertEqual('someuser', u.username)
                self.assertEqual('foo@bar.com', u.email)
            if u.id == 'somesuperuser':
                self.assertEqual('somesuperuser', u.id)
                self.assertEqual('somesuperuser', u.username)
                self.assertEqual('superfoo@bar.com', u.email)
            if u.id == 'somerestricteduser':
                self.assertEqual('somerestricteduser', u.id)
                self.assertEqual('somerestricteduser', u.username)
                self.assertEqual('restrictedfoo@bar.com', u.email)","for i in results:
    u = i[0]
    if u.id == 'testy':
        self.assertEqual('testy', u.id)
        self.assertEqual('testy', u.username)
        self.assertEqual('bar@baz.com', u.email)
    if u.id == 'someuser':
        self.assertEqual('someuser', u.id)
        self.assertEqual('someuser', u.username)
        self.assertEqual('foo@bar.com', u.email)
    if u.id == 'somesuperuser':
        self.assertEqual('somesuperuser', u.id)
        self.assertEqual('somesuperuser', u.username)
        self.assertEqual('superfoo@bar.com', u.email)
    if u.id == 'somerestricteduser':
        self.assertEqual('somerestricteduser', u.id)
        self.assertEqual('somerestricteduser', u.username)
        self.assertEqual('restrictedfoo@bar.com', u.email)","[""for i in results:\n    (i_0, *i_rimaining) = i\n    u = i_0\n    if u.id == 'testy':\n        self.assertEqual('testy', u.id)\n        self.assertEqual('testy', u.username)\n        self.assertEqual('bar@baz.com', u.email)\n    if u.id == 'someuser':\n        self.assertEqual('someuser', u.id)\n        self.assertEqual('someuser', u.username)\n        self.assertEqual('foo@bar.com', u.email)\n    if u.id == 'somesuperuser':\n        self.assertEqual('somesuperuser', u.id)\n        self.assertEqual('somesuperuser', u.username)\n        self.assertEqual('superfoo@bar.com', u.email)\n    if u.id == 'somerestricteduser':\n        self.assertEqual('somerestricteduser', u.id)\n        self.assertEqual('somerestricteduser', u.username)\n        self.assertEqual('restrictedfoo@bar.com', u.email)"", ""for (i_0, *i_len) in results:\n    u = \n    i_0\n    if u.id == 'testy':\n        self.assertEqual('testy', u.id)\n        self.assertEqual('testy', u.username)\n        self.assertEqual('bar@baz.com', u.email)\n    if u.id == 'someuser':\n        self.assertEqual('someuser', u.id)\n        self.assertEqual('someuser', u.username)\n        self.assertEqual('foo@bar.com', u.email)\n    if u.id == 'somesuperuser':\n        self.assertEqual('somesuperuser', u.id)\n        self.assertEqual('somesuperuser', u.username)\n        self.assertEqual('superfoo@bar.com', u.email)\n    if u.id == 'somerestricteduser':\n        self.assertEqual('somerestricteduser', u.id)\n        self.assertEqual('somerestricteduser', u.username)\n        self.assertEqual('restrictedfoo@bar.com', u.email)""]",no_found,0
you-get,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/you-get/src/you_get/extractors/mgtv.py,https://github.com/soimort/you-get/tree/master/src/you_get/extractors/mgtv.py,MGTV,prepare$93,"def prepare(self, **kwargs):
    if self.url:
        self.vid = self.get_vid_from_url(self.url)
    content_info = get_content(self.info_endpoint.format(video_id=self.vid))
    log.d(content_info)
    content_info = loads(content_info)
    self.title = content_info['data']['info']['videoName']
    content_player = get_content(self.player_endpoint.format(did=self.did, video_id=self.vid, tk2=self.tk2()))
    log.d(content_player)
    content_player = loads(content_player)
    pm2 = content_player['data']['atc']['pm2']
    content_source = get_content(self.source_endpoint.format(video_id=self.vid, tk2=self.tk2(), pm2=pm2))
    log.d(content_source)
    content_source = loads(content_source)
    domain = content_source['data']['stream_domain'][0]
    stream_available = {}
    for i in content_source['data']['stream']:
        stream_available[i['name']] = i['url']
    for s in self.stream_types:
        if s['video_profile'] in stream_available.keys():
            quality_id = self.id_dic[s['video_profile']]
            url = stream_available[s['video_profile']]
            if url is None or url == '':
                continue
            url = domain + re.sub('(\\&arange\\=\\d+)', '', url)
            (m3u8_url, m3u8_size, segment_list_this) = self.get_mgtv_real_url(url)
            stream_fileid_list = []
            for i in segment_list_this:
                stream_fileid_list.append(os.path.basename(i).split('.')[0])
            pieces = []
            for i in zip(stream_fileid_list, segment_list_this):
                pieces.append({'fileid': i[0], 'segs': i[1]})
                self.streams[quality_id] = {'container': s['container'], 'video_profile': s['video_profile'], 'size': m3u8_size, 'pieces': pieces, 'm3u8_url': m3u8_url}
            if not kwargs['info_only']:
                self.streams[quality_id]['src'] = segment_list_this","for i in zip(stream_fileid_list, segment_list_this):
    pieces.append({'fileid': i[0], 'segs': i[1]})
    self.streams[quality_id] = {'container': s['container'], 'video_profile': s['video_profile'], 'size': m3u8_size, 'pieces': pieces, 'm3u8_url': m3u8_url}","[""for i in zip(stream_fileid_list, segment_list_this):\n    (i_0, i_1, *_) = i\n    pieces.append({'fileid': i_0, 'segs': i_1})\n    self.streams[quality_id] = {'container': s['container'], 'video_profile': s['video_profile'], 'size': m3u8_size, 'pieces': pieces, 'm3u8_url': m3u8_url}"", ""for (i_0, i_1, *i_len) in zip(stream_fileid_list, segment_list_this):\n    pieces.append({'fileid': i_0, 'segs': i_1})\n    self.streams[quality_id] = {'container': s['container'], 'video_profile': s['video_profile'], 'size': m3u8_size, 'pieces': pieces, 'm3u8_url': m3u8_url}""]",no_found,0
BBTz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BBTz/gctexposer.py,https://github.com/m4ll0k/BBTz/tree/master//gctexposer.py,,GCTExposer$39,"def GCTExposer(domain, more):
    (domains, next_) = contentParser(getContent(domain).content)
    for i in domains:
        if args.moreinfo:
            print('{_} -> {__}'.format(_=i[1], __=i[2]))
        else:
            print(i[1])
    range_ = next_[-1]
    for i in range(range_ - 1):
        (domains, next_) = contentParser(getNextContent(next_[1]))
        next_ = next_
        for ii in domains:
            if args.moreinfo:
                print('{_} -> {__}'.format(_=ii[1], __=ii[2]))
            else:
                print(ii[1])","for ii in domains:
    if args.moreinfo:
        print('{_} -> {__}'.format(_=ii[1], __=ii[2]))
    else:
        print(ii[1])","[""for ii in domains:\n    (_, ii_1, ii_2, *ii_riimaining) = ii\n    if args.moreinfo:\n        print('{_} -> {__}'.format(_=ii_1, __=ii_2))\n    else:\n        print(ii_1)"", ""for (ii_0, ii_1, ii_2, *ii_len) in domains:\n    if args.moreinfo:\n        print('{_} -> {__}'.format(_=ii_1, __=ii_2))\n    else:\n        print(ii_1)""]",no_found,0
kivy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy/kivy/atlas.py,https://github.com/kivy/kivy/tree/master/kivy/atlas.py,Atlas,create$229,"def create(outname, filenames, size, padding=2, use_path=False):
    """"""This method can be used to create an atlas manually from a set of
        images.

        :Parameters:
            `outname`: str
                Basename to use for ``.atlas`` creation and ``-<idx>.png``
                associated images.
            `filenames`: list
                List of filenames to put in the atlas.
            `size`: int or list (width, height)
                Size of the atlas image. If the size is not large enough to
                fit all of the source images, more atlas images will created
                as required.
            `padding`: int, defaults to 2
                Padding to put around each image.

                Be careful. If you're using a padding < 2, you might have
                issues with the borders of the images. Because of the OpenGL
                linearization, it might use the pixels of the adjacent image.

                If you're using a padding >= 2, we'll automatically generate a
                ""border"" of 1px around your image. If you look at
                the result, don't be scared if the image inside is not
                exactly the same as yours :).

            `use_path`: bool, defaults to False
                If True, the relative path of the source png
                file names will be included in the atlas ids rather
                that just in the file names. Leading dots and slashes will be
                excluded and all other slashes in the path will be replaced
                with underscores. For example, if `use_path` is False
                (the default) and the file name is
                ``../data/tiles/green_grass.png``, the id will be
                ``green_grass``. If `use_path` is True, it will be
                ``data_tiles_green_grass``.

            .. versionchanged:: 1.8.0
                Parameter use_path added
        """"""
    try:
        from PIL import Image
    except ImportError:
        Logger.critical('Atlas: Imaging/PIL are missing')
        raise
    if isinstance(size, (tuple, list)):
        (size_w, size_h) = list(map(int, size))
    else:
        size_w = size_h = int(size)
    ims = list()
    for f in filenames:
        fp = open(f, 'rb')
        im = Image.open(fp)
        im.load()
        fp.close()
        ims.append((f, im))
    ims = sorted(ims, key=lambda im: im[1].size[0] * im[1].size[1], reverse=True)
    freeboxes = [(0, 0, 0, size_w, size_h)]
    numoutimages = 1
    fullboxes = []
    for imageinfo in ims:
        im = imageinfo[1]
        (imw, imh) = im.size
        imw += padding
        imh += padding
        if imw > size_w or imh > size_h:
            Logger.error('Atlas: image %s (%d by %d) is larger than the atlas size!' % (imageinfo[0], imw, imh))
            return
        inserted = False
        while not inserted:
            for (idx, fb) in enumerate(freeboxes):
                if fb[3] >= imw and fb[4] >= imh:
                    del freeboxes[idx]
                    if fb[3] > imw:
                        freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))
                    if fb[4] > imh:
                        freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))
                    freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])
                    fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo[0]))
                    inserted = True
                    break
            if not inserted:
                freeboxes.append((numoutimages, 0, 0, size_w, size_h))
                numoutimages += 1
    Logger.info('Atlas: create an {0}x{1} rgba image'.format(size_w, size_h))
    outimages = [Image.new('RGBA', (size_w, size_h)) for i in range(0, int(numoutimages))]
    for fb in fullboxes:
        (x, y) = (fb[2], fb[3])
        out = outimages[fb[1]]
        out.paste(fb[0], (fb[2], fb[3]))
        (w, h) = fb[0].size
        if padding > 1:
            out.paste(fb[0].crop((0, 0, w, 1)), (x, y - 1))
            out.paste(fb[0].crop((0, h - 1, w, h)), (x, y + h))
            out.paste(fb[0].crop((0, 0, 1, h)), (x - 1, y))
            out.paste(fb[0].crop((w - 1, 0, w, h)), (x + w, y))
    for (idx, outimage) in enumerate(outimages):
        outimage.save('%s-%d.png' % (outname, idx))
    meta = {}
    for fb in fullboxes:
        fn = '%s-%d.png' % (basename(outname), fb[1])
        if fn not in meta:
            d = meta[fn] = {}
        else:
            d = meta[fn]
        if use_path:
            uid = splitext(fb[6])[0]
            uid = uid.lstrip('./\\')
            uid = uid.replace('/', '_').replace('\\', '_')
        else:
            uid = splitext(basename(fb[6]))[0]
        (x, y, w, h) = fb[2:6]
        d[uid] = (x, size_h - y - h, w, h)
    outfn = '%s.atlas' % outname
    with open(outfn, 'w') as fd:
        json.dump(meta, fd)
    return (outfn, meta)","for imageinfo in ims:
    im = imageinfo[1]
    (imw, imh) = im.size
    imw += padding
    imh += padding
    if imw > size_w or imh > size_h:
        Logger.error('Atlas: image %s (%d by %d) is larger than the atlas size!' % (imageinfo[0], imw, imh))
        return
    inserted = False
    while not inserted:
        for (idx, fb) in enumerate(freeboxes):
            if fb[3] >= imw and fb[4] >= imh:
                del freeboxes[idx]
                if fb[3] > imw:
                    freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))
                if fb[4] > imh:
                    freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))
                freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])
                fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo[0]))
                inserted = True
                break
        if not inserted:
            freeboxes.append((numoutimages, 0, 0, size_w, size_h))
            numoutimages += 1","[""for imageinfo in ims:\n    (imageinfo_0, imageinfo_1, *_) = imageinfo\n    im = imageinfo_1\n    (imw, imh) = im.size\n    imw += padding\n    imh += padding\n    if imw > size_w or imh > size_h:\n        Logger.error('Atlas: image %s (%d by %d) is larger than the atlas size!' % (imageinfo_0, imw, imh))\n        return\n    inserted = False\n    while not inserted:\n        for (idx, fb) in enumerate(freeboxes):\n            if fb[3] >= imw and fb[4] >= imh:\n                del freeboxes[idx]\n                if fb[3] > imw:\n                    freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))\n                if fb[4] > imh:\n                    freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))\n                freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])\n                fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo_0))\n                inserted = True\n                break\n        if not inserted:\n            freeboxes.append((numoutimages, 0, 0, size_w, size_h))\n            numoutimages += 1"", ""for (imageinfo_0, imageinfo_1, *imageinfo_len) in ims:\n    im = \n    imageinfo_1\n    (imw, imh) = im.size\n    imw += padding\n    imh += padding\n    if imw > size_w or imh > size_h:\n        Logger.error('Atlas: image %s (%d by %d) is larger than the atlas size!' % (\n        imageinfo_0, imw, imh))\n        return\n    inserted = False\n    while not inserted:\n        for (idx, fb) in enumerate(freeboxes):\n            if fb[3] >= imw and fb[4] >= imh:\n                del freeboxes[idx]\n                if fb[3] > imw:\n                    freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))\n                if fb[4] > imh:\n                    freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))\n                freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])\n                fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, \n                imageinfo_0))\n                inserted = True\n                break\n        if not inserted:\n            freeboxes.append((numoutimages, 0, 0, size_w, size_h))\n            numoutimages += 1""]",no_found,0
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/tools/offlinearize.py,https://github.com/nlplab/brat/tree/master/tools/offlinearize.py,,convert_coll$47,"def convert_coll(coll):
    if coll == '':
        ajax_coll = '/'
    else:
        ajax_coll = '/%s/' % coll
    coll_query_url = urljoin(base_url, 'ajax.cgi?action=getCollectionInformation&collection=%s' % ajax_coll)
    coll_dir = joinpath(datadir, coll)
    try:
        makedirs(coll_dir)
    except BaseException:
        pass
    print(ajax_coll)
    conn = urlopen(coll_query_url)
    jsonp = conn.read()
    conn.close
    with open(joinpath(coll_dir, 'collection.js'), 'w') as f:
        f.write('jsonp=')
        f.write(jsonp)
    coll_data = loads(jsonp)
    for item in coll_data['items']:
        if item[0] == 'd':
            doc = item[2]
            print('  %s' % doc)
            doc_query_url = urljoin(base_url, 'ajax.cgi?action=getDocument&collection=%s&document=%s' % (ajax_coll, doc))
            conn = urlopen(doc_query_url)
            jsonp = conn.read()
            conn.close
            with open(joinpath(coll_dir, '%s.data.js' % doc), 'w') as f:
                f.write('jsonp=')
                f.write(jsonp)
        elif item[0] == 'c' and item[2] != '..':
            convert_coll(item[2])","for item in coll_data['items']:
    if item[0] == 'd':
        doc = item[2]
        print('  %s' % doc)
        doc_query_url = urljoin(base_url, 'ajax.cgi?action=getDocument&collection=%s&document=%s' % (ajax_coll, doc))
        conn = urlopen(doc_query_url)
        jsonp = conn.read()
        conn.close
        with open(joinpath(coll_dir, '%s.data.js' % doc), 'w') as f:
            f.write('jsonp=')
            f.write(jsonp)
    elif item[0] == 'c' and item[2] != '..':
        convert_coll(item[2])","[""for item in coll_data['items']:\n    (item_0, _, item_2, *item_ritemmaining) = item\n    if item_0 == 'd':\n        doc = item_2\n        print('  %s' % doc)\n        doc_query_url = urljoin(base_url, 'ajax.cgi?action=getDocument&collection=%s&document=%s' % (ajax_coll, doc))\n        conn = urlopen(doc_query_url)\n        jsonp = conn.read()\n        conn.close\n        with open(joinpath(coll_dir, '%s.data.js' % doc), 'w') as f:\n            f.write('jsonp=')\n            f.write(jsonp)\n    elif item_0 == 'c' and item_2 != '..':\n        convert_coll(item_2)"", ""for (item_0, item_1, item_2, *item_len) in coll_data['items']:\n    if \n    item_0 == 'd':\n        doc = \n        item_2\n        print('  %s' % doc)\n        doc_query_url = urljoin(base_url, 'ajax.cgi?action=getDocument&collection=%s&document=%s' % (ajax_coll, doc))\n        conn = urlopen(doc_query_url)\n        jsonp = conn.read()\n        conn.close\n        with open(joinpath(coll_dir, '%s.data.js' % doc), 'w') as f:\n            f.write('jsonp=')\n            f.write(jsonp)\n    elif \n    item_0 == 'c' and \n    item_2 != '..':\n        convert_coll(\n        item_2)""]",no_found,0
fuzzbench,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fuzzbench/common/gcloud.py,https://github.com/google/fuzzbench/tree/master/common/gcloud.py,,create_instance_template$139,"def create_instance_template(template_name, docker_image, env, project, zone):
    """"""Returns a ProcessResult from running the command to create an instance
    template.""""""
    command = ['gcloud', 'compute', '--project', project, 'instance-templates', 'create-with-container', template_name, '--no-address', '--image-family=cos-stable', '--image-project=cos-cloud', f'--region={zone}', '--scopes=cloud-platform', f'--machine-type={MEASURER_WORKER_MACHINE_TYPE}', f'--boot-disk-size={MEASURER_WORKER_BOOT_DISK_SIZE}', '--preemptible', '--container-image', docker_image]
    for item in env.items():
        command.extend(['--container-env', f'{item[0]}={item[1]}'])
    new_process.execute(command)
    return posixpath.join('https://www.googleapis.com/compute/v1/projects/', project, 'global', 'instanceTemplates', template_name)","for item in env.items():
    command.extend(['--container-env', f'{item[0]}={item[1]}'])","[""for item in env.items():\n    (item_0, item_1, *_) = item\n    command.extend(['--container-env', f'{item_0}={item_1}'])"", ""for (item_0, item_1, *item_len) in env.items():\n    command.extend(['--container-env', f'{item_0}={item_1}'])""]",no_found,0
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
    dbg.createLogWindow()
    global currentArgs
    currentArgs = copy.copy(args)
    try:
        starttime = datetime.datetime.now()
        ptr_counter = 0
        commands = {}

        def getBanner():
            banners = {}
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |                         __               __                      |\n'
            bannertext += '    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n'
            bannertext += '    |  / ___/ __ \\/ ___/ _ \\/ / __ `/ __ \\   / __/ _ \\/ __ `/ __ `__ \\ |\n'
            bannertext += '    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n'
            bannertext += '    | \\___/\\____/_/   \\___/_/\\__,_/_/ /_/   \\__/\\___/\\__,_/_/ /_/ /_/  |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |     https://www.corelan.be | https://www.corelan-training.com    |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[0] = bannertext
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n'
            bannertext += ""    |       | '_ ` _ \\  / _ \\ | '_ \\  / _` |   | '_ \\ | | | |          |\n""
            bannertext += '    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n'
            bannertext += '    |       |_| |_| |_| \\___/ |_| |_| \\__,_|(_)| .__/  \\__, |          |\n'
            bannertext += '    |                                          |_|     |___/           |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[1] = bannertext
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |    _____ ___  ____  ____  ____ _                                 |\n'
            bannertext += '    |    / __ `__ \\/ __ \\/ __ \\/ __ `/  https://www.corelan.be         |\n'
            bannertext += '    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n'
            bannertext += '    |  /_/ /_/ /_/\\____/_/ /_/\\__,_/  #corelan (Freenode IRC)          |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[2] = bannertext
            bannertext = ''
            bannertext += '\n    .##.....##..#######..##....##....###........########..##....##\n'
            bannertext += '    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n'
            bannertext += '    .####.####.##.....##.####..##..##...##......##.....##...####..\n'
            bannertext += '    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n'
            bannertext += '    .##.....##.##.....##.##..####.#########.....##...........##...\n'
            bannertext += '    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n'
            bannertext += '    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n'
            banners[3] = bannertext
            bannerlist = []
            for i in range(0, len(banners)):
                bannerlist.append(i)
            random.shuffle(bannerlist)
            return banners[bannerlist[0]]

        def procHelp(args):
            dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__, str(arch)))
            dbg.log('     Plugin version : %s r%s' % (__VERSION__, __REV__))
            dbg.log('     Python version : %s' % getPythonVersion())
            if __DEBUGGERAPP__ == 'WinDBG':
                pykdversion = dbg.getPyKDVersionNr()
                dbg.log('     PyKD version %s' % pykdversion)
            dbg.log('     Written by Corelan - https://www.corelan.be')
            dbg.log('     Project page : https://github.com/corelan/mona')
            dbg.logLines(getBanner(), highlight=1)
            dbg.log('Global options :')
            dbg.log('----------------')
            dbg.log('You can use one or more of the following global options on any command that will perform')
            dbg.log('a search in one or more modules, returning a list of pointers :')
            dbg.log(' -n                     : Skip modules that start with a null byte. If this is too broad, use')
            dbg.log('                          option -cp nonull instead')
            dbg.log(' -o                     : Ignore OS modules')
            dbg.log(' -p <nr>                : Stop search after <nr> pointers.')
            dbg.log(' -m <module,module,...> : only query the given modules. Be sure what you are doing !')
            dbg.log('                          You can specify multiple modules (comma separated)')
            dbg.log('                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored')
            dbg.log('                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,')
            dbg.log('                          blah or *blah* = contains blah')
            dbg.log(' -cm <crit,crit,...>    : Apply some additional criteria to the modules to query.')
            dbg.log('                          You can use one or more of the following criteria :')
            dbg.log('                          aslr,safeseh,rebase,nx,os')
            dbg.log('                          You can enable or disable a certain criterium by setting it to true or false')
            dbg.log('                          Example :  -cm aslr=true,safeseh=false')
            dbg.log('                          Suppose you want to search for p/p/r in aslr enabled modules, you could call')
            dbg.log('                          !mona seh -cm aslr')
            dbg.log(' -cp <crit,crit,...>    : Apply some criteria to the pointers to return')
            dbg.log('                          Available options are :')
            dbg.log('                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev')
            dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
            dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
            dbg.log('                          You can use .. to indicate a range of bytes (in between 2 bad chars)')
            dbg.log(' -x <access>            : Specify desired access level of the returning pointers. If not specified,')
            dbg.log('                          only executable pointers will be returned.')
            dbg.log('                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *')
            if not args:
                args = []
            if len(args) > 1:
                thiscmd = args[1].lower().strip()
                if thiscmd in commands:
                    dbg.log('')
                    dbg.log(""Usage of command '%s' :"" % thiscmd)
                    dbg.log('%s' % ('-' * (22 + len(thiscmd))))
                    dbg.logLines(commands[thiscmd].usage)
                    dbg.log('')
                else:
                    aliasfound = False
                    for cmd in commands:
                        if commands[cmd].alias == thiscmd:
                            dbg.log('')
                            dbg.log(""Usage of command '%s' :"" % thiscmd)
                            dbg.log('%s' % ('-' * (22 + len(thiscmd))))
                            dbg.logLines(commands[cmd].usage)
                            dbg.log('')
                            aliasfound = True
                    if not aliasfound:
                        dbg.logLines('\nCommand %s does not exist. Run !mona to get a list of available commands\n' % thiscmd, highlight=1)
            else:
                dbg.logLines('\nUsage :')
                dbg.logLines('-------\n')
                dbg.log(' !mona <command> <parameter>')
                dbg.logLines('\nAvailable commands and parameters :\n')
                items = commands.items()
                items.sort(key=itemgetter(0))
                for item in items:
                    if commands[item[0]].usage != '':
                        aliastxt = ''
                        if commands[item[0]].alias != '':
                            aliastxt = ' / ' + commands[item[0]].alias
                        dbg.logLines('%s | %s' % (item[0] + aliastxt + ' ' * (20 - len(item[0] + aliastxt)), commands[item[0]].description))
                dbg.log('')
                dbg.log('Want more info about a given command ?  Run !mona help <command>', highlight=1)
                dbg.log('')
        commands['help'] = MnCommand('help', 'show help', '!mona help [command]', procHelp)

        def procConfig(args):
            showerror = False
            if not 'set' in args and (not 'get' in args) and (not 'add' in args):
                showerror = True
            if 'set' in args:
                if type(args['set']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['set'].split(' ')
                    if len(params) < 2:
                        showerror = True
            if 'add' in args:
                if type(args['add']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['add'].split(' ')
                    if len(params) < 2:
                        showerror = True
            if 'get' in args:
                if type(args['get']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['get'].split(' ')
                    if len(params) < 1:
                        showerror = True
            if showerror:
                dbg.log('Usage :')
                dbg.logLines(configUsage, highlight=1)
                return
            else:
                if 'get' in args:
                    dbg.log('Reading value from configuration file')
                    monaConfig = MnConfig()
                    thevalue = monaConfig.get(args['get'])
                    dbg.log('Parameter %s = %s' % (args['get'], thevalue))
                if 'set' in args:
                    dbg.log('Writing value to configuration file')
                    monaConfig = MnConfig()
                    value = args['set'].split(' ')
                    configparam = value[0].strip()
                    dbg.log('Old value of parameter %s = %s' % (configparam, monaConfig.get(configparam)))
                    configvalue = args['set'][0 + len(configparam):len(args['set'])]
                    monaConfig.set(configparam, configvalue)
                    dbg.log('New value of parameter %s = %s' % (configparam, configvalue))
                if 'add' in args:
                    dbg.log('Writing value to configuration file')
                    monaConfig = MnConfig()
                    value = args['add'].split(' ')
                    configparam = value[0].strip()
                    dbg.log('Old value of parameter %s = %s' % (configparam, monaConfig.get(configparam)))
                    configvalue = monaConfig.get(configparam).strip() + ',' + args['add'][0 + len(configparam):len(args['add'])].strip()
                    monaConfig.set(configparam, configvalue)
                    dbg.log('New value of parameter %s = %s' % (configparam, configvalue))

        def procFindJ(args):
            return procFindJMP(args)

        def procFindJMP(args):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            if inspect.stack()[1][3] == 'procFindJ':
                dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."", highlight=1)
            criteria = {}
            all_opcodes = {}
            global ptr_to_get
            ptr_to_get = -1
            distancestr = ''
            mindistance = 0
            maxdistance = 0
            showerror = False
            if 'r' in args:
                if type(args['r']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    thisreg = args['r'].upper().strip()
                    validregs = dbglib.Registers32BitsOrder
                    if not thisreg in validregs:
                        showerror = True
            else:
                showerror = True
            if 'distance' in args:
                if type(args['distance']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    distancestr = args['distance']
                    distanceparts = distancestr.split(',')
                    for parts in distanceparts:
                        valueparts = parts.split('=')
                        if len(valueparts) > 1:
                            if valueparts[0].lower() == 'min':
                                try:
                                    mindistance = int(valueparts[1])
                                except:
                                    mindistance = 0
                            if valueparts[0].lower() == 'max':
                                try:
                                    maxdistance = int(valueparts[1])
                                except:
                                    maxdistance = 0
            if maxdistance < mindistance:
                tmp = maxdistance
                maxdistance = mindistance
                mindistance = tmp
            criteria['mindistance'] = mindistance
            criteria['maxdistance'] = maxdistance
            if showerror:
                dbg.log('Usage :')
                dbg.logLines(jmpUsage, highlight=1)
                return
            else:
                (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
                all_opcodes = findJMP(modulecriteria, criteria, args['r'].lower().strip())
            logfile = MnLog('jmp.txt')
            thislog = logfile.reset()
            processResults(all_opcodes, logfile, thislog)

        def procFindSEH(args):
            modulecriteria = {}
            modulecriteria['safeseh'] = False
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            criteria = {}
            specialcases = {}
            all_opcodes = {}
            global ptr_to_get
            ptr_to_get = -1
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            if 'rop' in args:
                criteria['rop'] = True
            if 'all' in args:
                criteria['all'] = True
                specialcases['maponly'] = True
            else:
                criteria['all'] = False
                specialcases['maponly'] = False
            all_opcodes = findSEH(modulecriteria, criteria)
            logfile = MnLog('seh.txt')
            thislog = logfile.reset()
            processResults(all_opcodes, logfile, thislog, specialcases)

        def procShowMODULES(args):
            modulecriteria = {}
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            modulestosearch = getModulesToQuery(modulecriteria)
            showModuleTable('', modulestosearch)

        def procFindROPFUNC(args):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            ropfuncs = {}
            ropfuncoffsets = {}
            (ropfuncs, ropfuncoffsets) = findROPFUNC(modulecriteria, criteria)
            dbg.log('[+] Processing pointers to interesting rop functions')
            logfile = MnLog('ropfunc.txt')
            thislog = logfile.reset()
            processResults(ropfuncs, logfile, thislog)
            global silent
            silent = True
            dbg.log('[+] Processing offsets to pointers to interesting rop functions')
            logfile = MnLog('ropfunc_offset.txt')
            thislog = logfile.reset()
            processResults(ropfuncoffsets, logfile, thislog)

        def procStackPivots(args):
            procROP(args, 'stackpivot')

        def procROP(args, mode='all'):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            depth = 6
            maxoffset = 40
            thedistance = 8
            split = False
            fast = False
            sortedprint = False
            endingstr = ''
            endings = []
            technique = ''
            if 'depth' in args:
                if type(args['depth']).__name__.lower() != 'bool':
                    try:
                        depth = int(args['depth'])
                    except:
                        pass
            if 'offset' in args:
                if type(args['offset']).__name__.lower() != 'bool':
                    try:
                        maxoffset = int(args['offset'])
                    except:
                        pass
            if 'distance' in args:
                if type(args['distance']).__name__.lower() != 'bool':
                    try:
                        thedistance = args['distance']
                    except:
                        pass
            if 'split' in args:
                if type(args['split']).__name__.lower() == 'bool':
                    split = args['split']
            if 's' in args:
                if type(args['s']).__name__.lower() != 'bool':
                    technique = args['s'].replace(""'"", '').replace('""', '').strip().lower()
            if 'fast' in args:
                if type(args['fast']).__name__.lower() == 'bool':
                    fast = args['fast']
            if 'end' in args:
                if type(args['end']).__name__.lower() == 'str':
                    endingstr = args['end'].replace(""'"", '').replace('""', '').strip()
                    endings = endingstr.split('#')
            if 'f' in args:
                if args['f'] != '':
                    criteria['f'] = args['f']
            if 'sort' in args:
                sortedprint = True
            if 'rva' in args:
                criteria['rva'] = True
            if mode == 'stackpivot':
                fast = False
                endings = ''
                split = False
            else:
                mode = 'all'
            findROPGADGETS(modulecriteria, criteria, endings, maxoffset, depth, split, thedistance, fast, mode, sortedprint, technique)

        def procJseh(args):
            results = []
            showred = 0
            showall = False
            if 'all' in args:
                showall = True
            nrfound = 0
            dbg.log('-----------------------------------------------------------------------')
            dbg.log('Search for jmp/call dword[ebp/esp+nn] (and other) combinations started ')
            dbg.log('-----------------------------------------------------------------------')
            opcodej = ['T$\x08', 'd$\x08', 'T$\x14', 'T$\x14', 'T$\x1c', 'T$\x1c', 'T$,', 'T$,', 'T$D', 'T$D', 'T$P', 'T$P', 'U\x0c', 'e\x0c', 'U$', 'e$', 'U0', 'e0', 'U', 'e', 'U', 'e', 'U', 'e', '\x83\x08', '\x83\x08']
            fakeptrcriteria = {}
            fakeptrcriteria['accesslevel'] = '*'
            for opjc in opcodej:
                addys = []
                addys = searchInRange([[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
                results += addys
                for ptrtypes in addys:
                    for ad1 in addys[ptrtypes]:
                        ptr = MnPointer(ad1)
                        module = ptr.belongsTo()
                        if not module:
                            module = ''
                            page = dbg.getMemoryPageByAddress(ad1)
                            access = page.getAccess(human=True)
                            op = dbg.disasm(ad1)
                            opstring = op.getDisasm()
                            dbg.log('Found %s at 0x%08x - Access: (%s) - Outside of a loaded module' % (opstring, ad1, access), address=ad1, highlight=1)
                            nrfound += 1
                        elif showall:
                            page = dbg.getMemoryPageByAddress(ad1)
                            access = page.getAccess(human=True)
                            op = dbg.disasm(ad1)
                            opstring = op.getDisasm()
                            thismod = MnModule(module)
                            if not thismod.isSafeSEH:
                                extratext = '=== Safeseh : NO ==='
                                showred = 1
                            else:
                                extratext = 'Safeseh protected'
                                showred = 0
                            dbg.log('Found %s at 0x%08x (%s) - Access: (%s) - %s' % (opstring, ad1, module, access, extratext), address=ad1, highlight=showred)
                            nrfound += 1
            dbg.log('Search complete')
            if results:
                dbg.log('Found %d address(es)' % nrfound)
                return 'Found %d address(es) (Check the log Windows for details)' % nrfound
            else:
                dbg.log('No addresses found')
                return 'Sorry, no addresses found'

        def procJOP(args, mode='all'):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            depth = 6
            if 'depth' in args:
                if type(args['depth']).__name__.lower() != 'bool':
                    try:
                        depth = int(args['depth'])
                    except:
                        pass
            findJOPGADGETS(modulecriteria, criteria, depth)

        def procCreatePATTERN(args):
            size = 0
            pattern = ''
            if '?' in args and args['?'] != '':
                try:
                    if '0x' in args['?'].lower():
                        try:
                            size = int(args['?'], 16)
                        except:
                            size = 0
                    else:
                        size = int(args['?'])
                except:
                    size = 0
            if size == 0:
                dbg.log('Please enter a valid size', highlight=1)
            else:
                pattern = createPattern(size, args)
                dbg.log('Creating cyclic pattern of %d bytes' % size)
                dbg.log(pattern)
                global ignoremodules
                ignoremodules = True
                objpatternfile = MnLog('pattern.txt')
                patternfile = objpatternfile.reset()
                objpatternfile.write('\nPattern of ' + str(size) + ' bytes :\n', patternfile)
                objpatternfile.write('-' * (19 + len(str(size))), patternfile)
                objpatternfile.write('\nASCII:', patternfile)
                objpatternfile.write('\n' + pattern, patternfile)
                patternhex = ''
                for patternchar in pattern:
                    patternhex += str(hex(ord(patternchar))).replace('0x', '\\x')
                objpatternfile.write('\n\nHEX:\n', patternfile)
                objpatternfile.write(patternhex, patternfile)
                patternjs = str2js(pattern)
                objpatternfile.write('\n\nJAVASCRIPT (unescape() friendly):\n', patternfile)
                objpatternfile.write(patternjs, patternfile)
                if not silent:
                    dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"", highlight=1)
                    dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile, highlight=1)
                ignoremodules = False
            return

        def procOffsetPATTERN(args):
            egg = ''
            if '?' in args and args['?'] != '':
                try:
                    egg = args['?']
                except:
                    egg = ''
            if egg == '':
                dbg.log('Please enter a valid target', highlight=1)
            else:
                findOffsetInPattern(egg, -1, args)
            return

        def procFileCOMPARE(args):
            modulecriteria = {}
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            allfiles = []
            tomatch = ''
            checkstrict = True
            rangeval = 0
            fast = False
            if 'ptronly' in args or 'ptrsonly' in args:
                fast = True
            if 'f' in args:
                if args['f'] != '':
                    rawfilenames = args['f'].replace('""', '')
                    allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
                    dbg.log('[+] Number of files to be examined : %d ' % len(allfiles))
            if 'range' in args:
                if not type(args['range']).__name__.lower() == 'bool':
                    strrange = args['range'].lower()
                    if strrange.startswith('0x') and len(strrange) > 2:
                        rangeval = int(strrange, 16)
                    else:
                        try:
                            rangeval = int(args['range'])
                        except:
                            rangeval = 0
                    if rangeval > 0:
                        dbg.log('[+] Find overlap using pointer +/- range, value %d' % rangeval)
                        dbg.log('    Note : this will significantly slow down the comparison process !')
                else:
                    dbg.log('Please provide a numeric value ^(> 0) with option -range', highlight=1)
                    return
            else:
                if 'contains' in args:
                    if type(args['contains']).__name__.lower() == 'str':
                        tomatch = args['contains'].replace(""'"", '').replace('""', '')
                if 'nostrict' in args:
                    if type(args['nostrict']).__name__.lower() == 'bool':
                        checkstrict = not args['nostrict']
                        dbg.log('[+] Instructions must match in all files ? %s' % checkstrict)
            callfiles = allfiles
            allfiles = []
            for tfile in callfiles:
                if os.path.isdir(tfile):
                    for (root, dirs, files) in os.walk(tfile):
                        for dfile in files:
                            allfiles.append(os.path.join(root, dfile))
                else:
                    allfiles.append(tfile)
            if len(allfiles) > 1:
                findFILECOMPARISON(modulecriteria, criteria, allfiles, tomatch, checkstrict, rangeval, fast)
            else:
                dbg.log('Please specify at least 2 filenames to compare', highlight=1)

        def procFind(args):
            modulecriteria = {}
            criteria = {}
            pattern = ''
            base = 0
            offset = 0
            top = TOP_USERLAND
            consecutive = False
            ftype = ''
            level = 0
            offsetlevel = 0
            if not 'a' in args:
                args['a'] = '*'
            ptronly = False
            if 'ptronly' in args or 'ptrsonly' in args:
                ptronly = True
            if not 'x' in args:
                args['x'] = '*'
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            if criteria['accesslevel'] == '':
                return
            if not 's' in args:
                dbg.log('-s <search pattern (or filename)> is a mandatory argument', highlight=1)
                return
            pattern = args['s']
            if 'unicode' in args:
                criteria['unic'] = True
            if 'b' in args:
                try:
                    base = int(args['b'], 16)
                except:
                    dbg.log('invalid base address: %s' % args['b'], highlight=1)
                    return
            if 't' in args:
                try:
                    top = int(args['t'], 16)
                except:
                    dbg.log('invalid top address: %s' % args['t'], highlight=1)
                    return
            if 'offset' in args:
                if not args['offset'].__class__.__name__ == 'bool':
                    if '0x' in args['offset'].lower():
                        try:
                            offset = 0 - int(args['offset'], 16)
                        except:
                            dbg.log('invalid offset value', highlight=1)
                            return
                    else:
                        try:
                            offset = 0 - int(args['offset'])
                        except:
                            dbg.log('invalid offset value', highlight=1)
                            return
                else:
                    dbg.log('invalid offset value', highlight=1)
                    return
            if 'level' in args:
                try:
                    level = int(args['level'])
                except:
                    dbg.log('invalid level value', highlight=1)
                    return
            if 'offsetlevel' in args:
                try:
                    offsetlevel = int(args['offsetlevel'])
                except:
                    dbg.log('invalid offsetlevel value', highlight=1)
                    return
            if 'c' in args:
                dbg.log('    - Skipping consecutive pointers, showing size instead')
                consecutive = True
            if 'type' in args:
                if not args['type'] in ['bin', 'asc', 'ptr', 'instr', 'file']:
                    dbg.log('Invalid search type : %s' % args['type'], highlight=1)
                    return
                ftype = args['type']
                if ftype == 'file':
                    filename = args['s'].replace('""', '').replace(""'"", '')
                    if not os.path.isfile(filename):
                        dbg.log('Unable to find/read file %s' % filename, highlight=1)
                        return
            rangep2p = 0
            if 'p2p' in args or level > 0:
                dbg.log('    - Looking for pointers to pointers')
                criteria['p2p'] = True
                if 'r' in args:
                    try:
                        rangep2p = int(args['r'])
                    except:
                        pass
                    if rangep2p > 0:
                        dbg.log('    - Will search for close pointers (%d bytes backwards)' % rangep2p)
                if 'p2p' in args:
                    level = 1
            if level > 0:
                dbg.log('    - Recursive levels : %d' % level)
            allpointers = findPattern(modulecriteria, criteria, pattern, ftype, base, top, consecutive, rangep2p, level, offset, offsetlevel)
            logfile = MnLog('find.txt')
            thislog = logfile.reset()
            processResults(allpointers, logfile, thislog, {}, ptronly)
            return

        def procFindWild(args):
            modulecriteria = {}
            criteria = {}
            pattern = ''
            patterntype = ''
            base = 0
            top = TOP_USERLAND
            (modulecriteria, criteria) = args2criteria(","for item in items:
    if commands[item[0]].usage != '':
        aliastxt = ''
        if commands[item[0]].alias != '':
            aliastxt = ' / ' + commands[item[0]].alias
        dbg.logLines('%s | %s' % (item[0] + aliastxt + ' ' * (20 - len(item[0] + aliastxt)), commands[item[0]].description))","[""for item in items:\n    (item_0, *item_ritemmaining) = item\n    if commands[item_0].usage != '':\n        aliastxt = ''\n        if commands[item_0].alias != '':\n            aliastxt = ' / ' + commands[item_0].alias\n        dbg.logLines('%s | %s' % (item_0 + aliastxt + ' ' * (20 - len(item_0 + aliastxt)), commands[item_0].description))"", ""for (item_0, *item_len) in items:\n    if commands[item_0].usage != '':\n        aliastxt = ''\n        if commands[item_0].alias != '':\n            aliastxt = ' / ' + commands[item_0].alias\n        dbg.logLines('%s | %s' % (item_0 + aliastxt + ' ' * (20 - len(item_0 + aliastxt)), commands[item_0].description))""]",no_found,0
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/full_sync.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/full_sync.py,FullSync,remove_library$524,"def remove_library(self, library_id, dialog):
    """""" Remove library by their id from the Kodi database.
        """"""
    direct_path = self.library.direct_path
    with Database('jellyfin') as jellyfindb:
        db = jellyfin_db.JellyfinDatabase(jellyfindb.cursor)
        library = db.get_view(library_id.replace('Mixed:', ''))
        items = db.get_item_by_media_folder(library_id.replace('Mixed:', ''))
        media = 'music' if library.media_type == 'music' else 'video'
        if media == 'music':
            settings('MusicRescan.bool', False)
        if items:
            with self.library.music_database_lock if media == 'music' else self.library.database_lock:
                with Database(media) as kodidb:
                    count = 0
                    if library.media_type == 'mixed':
                        movies = [x for x in items if x[1] == 'Movie']
                        tvshows = [x for x in items if x[1] == 'Series']
                        obj = Movies(self.server, jellyfindb, kodidb, direct_path, library).remove
                        for item in movies:
                            obj(item[0])
                            dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
                            count += 1
                        obj = TVShows(self.server, jellyfindb, kodidb, direct_path, library).remove
                        for item in tvshows:
                            obj(item[0])
                            dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
                            count += 1
                    else:
                        default_args = (self.server, jellyfindb, kodidb, direct_path)
                        for item in items:
                            if item[1] in ('Series', 'Season', 'Episode'):
                                TVShows(*default_args).remove(item[0])
                            elif item[1] in ('Movie', 'BoxSet'):
                                Movies(*default_args).remove(item[0])
                            elif item[1] in ('MusicAlbum', 'MusicArtist', 'AlbumArtist', 'Audio'):
                                Music(*default_args).remove(item[0])
                            elif item[1] == 'MusicVideo':
                                MusicVideos(*default_args).remove(item[0])
                            dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library[0]))
                            count += 1
    self.sync = get_sync()
    if library_id in self.sync['Whitelist']:
        self.sync['Whitelist'].remove(library_id)
    elif 'Mixed:%s' % library_id in self.sync['Whitelist']:
        self.sync['Whitelist'].remove('Mixed:%s' % library_id)
    save_sync(self.sync)","for item in items:
    if item[1] in ('Series', 'Season', 'Episode'):
        TVShows(*default_args).remove(item[0])
    elif item[1] in ('Movie', 'BoxSet'):
        Movies(*default_args).remove(item[0])
    elif item[1] in ('MusicAlbum', 'MusicArtist', 'AlbumArtist', 'Audio'):
        Music(*default_args).remove(item[0])
    elif item[1] == 'MusicVideo':
        MusicVideos(*default_args).remove(item[0])
    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library[0]))
    count += 1","[""for item in items:\n    (item_0, item_1, *_) = item\n    if item_1 in ('Series', 'Season', 'Episode'):\n        TVShows(*default_args).remove(item_0)\n    elif item_1 in ('Movie', 'BoxSet'):\n        Movies(*default_args).remove(item_0)\n    elif item_1 in ('MusicAlbum', 'MusicArtist', 'AlbumArtist', 'Audio'):\n        Music(*default_args).remove(item_0)\n    elif item_1 == 'MusicVideo':\n        MusicVideos(*default_args).remove(item_0)\n    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library[0]))\n    count += 1"", ""for (item_0, item_1, *item_len) in items:\n    if \n    item_1 in ('Series', 'Season', 'Episode'):\n        TVShows(*default_args).remove(\n        item_0)\n    elif \n    item_1 in ('Movie', 'BoxSet'):\n        Movies(*default_args).remove(\n        item_0)\n    elif \n    item_1 in ('MusicAlbum', 'MusicArtist', 'AlbumArtist', 'Audio'):\n        Music(*default_args).remove(\n        item_0)\n    elif \n    item_1 == 'MusicVideo':\n        MusicVideos(*default_args).remove(\n        item_0)\n    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library[0]))\n    count += 1""]",no_found,0
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/full_sync.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/full_sync.py,FullSync,remove_library$524,"def remove_library(self, library_id, dialog):
    """""" Remove library by their id from the Kodi database.
        """"""
    direct_path = self.library.direct_path
    with Database('jellyfin') as jellyfindb:
        db = jellyfin_db.JellyfinDatabase(jellyfindb.cursor)
        library = db.get_view(library_id.replace('Mixed:', ''))
        items = db.get_item_by_media_folder(library_id.replace('Mixed:', ''))
        media = 'music' if library.media_type == 'music' else 'video'
        if media == 'music':
            settings('MusicRescan.bool', False)
        if items:
            with self.library.music_database_lock if media == 'music' else self.library.database_lock:
                with Database(media) as kodidb:
                    count = 0
                    if library.media_type == 'mixed':
                        movies = [x for x in items if x[1] == 'Movie']
                        tvshows = [x for x in items if x[1] == 'Series']
                        obj = Movies(self.server, jellyfindb, kodidb, direct_path, library).remove
                        for item in movies:
                            obj(item[0])
                            dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
                            count += 1
                        obj = TVShows(self.server, jellyfindb, kodidb, direct_path, library).remove
                        for item in tvshows:
                            obj(item[0])
                            dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
                            count += 1
                    else:
                        default_args = (self.server, jellyfindb, kodidb, direct_path)
                        for item in items:
                            if item[1] in ('Series', 'Season', 'Episode'):
                                TVShows(*default_args).remove(item[0])
                            elif item[1] in ('Movie', 'BoxSet'):
                                Movies(*default_args).remove(item[0])
                            elif item[1] in ('MusicAlbum', 'MusicArtist', 'AlbumArtist', 'Audio'):
                                Music(*default_args).remove(item[0])
                            elif item[1] == 'MusicVideo':
                                MusicVideos(*default_args).remove(item[0])
                            dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library[0]))
                            count += 1
    self.sync = get_sync()
    if library_id in self.sync['Whitelist']:
        self.sync['Whitelist'].remove(library_id)
    elif 'Mixed:%s' % library_id in self.sync['Whitelist']:
        self.sync['Whitelist'].remove('Mixed:%s' % library_id)
    save_sync(self.sync)","for item in movies:
    obj(item[0])
    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
    count += 1","[""for item in movies:\n    (item_0, *item_ritemmaining) = item\n    obj(item_0)\n    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))\n    count += 1"", ""for (item_0, *item_len) in movies:\n    obj(item_0)\n    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))\n    count += 1""]",no_found,0
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_imperative_se_resnext.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_imperative_se_resnext.py,TestImperativeResneXt,reader_decorator$326,"def reader_decorator(self, reader):

    def _reader_imple():
        for item in reader():
            doc = np.array(item[0]).reshape(3, 224, 224)
            label = np.array(item[1]).astype('int64').reshape(1)
            yield (doc, label)
    return _reader_imple","for item in reader():
    doc = np.array(item[0]).reshape(3, 224, 224)
    label = np.array(item[1]).astype('int64').reshape(1)
    yield (doc, label)","[""for item in reader():\n    (item_0, item_1, *_) = item\n    doc = np.array(item_0).reshape(3, 224, 224)\n    label = np.array(item_1).astype('int64').reshape(1)\n    yield (doc, label)"", ""for (item_0, item_1, *item_len) in reader():\n    doc = np.array(item_0).reshape(3, 224, 224)\n    label = np.array(item_1).astype('int64').reshape(1)\n    yield (doc, label)""]",no_found,0
subDomainsBrute,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/subDomainsBrute/lib/scanner_py2.py,https://github.com/lijiejie/subDomainsBrute/tree/master/lib/scanner_py2.py,SubNameBrute,check_https_alt_names$90,"def check_https_alt_names(self, domain):
    try:
        x509 = reqs.OpenSSL.crypto.load_certificate(reqs.OpenSSL.crypto.FILETYPE_PEM, reqs.ssl.get_server_certificate((domain, 443)))
        for item in reqs.get_subj_alt_name(x509):
            if item[0].upper() == 'DNS':
                name = item[1].lower()
                if name.endswith(self.domain):
                    sub = name[:len(name) - len(self.domain) - 1]
                    sub = sub.replace('*', '')
                    sub = sub.strip('.')
                    if sub and sub not in self.found_subs and (sub not in self.normal_names_set) and (sub not in self.cert_subs):
                        self.cert_subs.add(sub)
                        self.queue.put((0, sub))
    except Exception as e:
        pass","for item in reqs.get_subj_alt_name(x509):
    if item[0].upper() == 'DNS':
        name = item[1].lower()
        if name.endswith(self.domain):
            sub = name[:len(name) - len(self.domain) - 1]
            sub = sub.replace('*', '')
            sub = sub.strip('.')
            if sub and sub not in self.found_subs and (sub not in self.normal_names_set) and (sub not in self.cert_subs):
                self.cert_subs.add(sub)
                self.queue.put((0, sub))","[""for item in reqs.get_subj_alt_name(x509):\n    (item_0, item_1, *_) = item\n    if item_0.upper() == 'DNS':\n        name = item_1.lower()\n        if name.endswith(self.domain):\n            sub = name[:len(name) - len(self.domain) - 1]\n            sub = sub.replace('*', '')\n            sub = sub.strip('.')\n            if sub and sub not in self.found_subs and (sub not in self.normal_names_set) and (sub not in self.cert_subs):\n                self.cert_subs.add(sub)\n                self.queue.put((0, sub))"", ""for (item_0, item_1, *item_len) in reqs.get_subj_alt_name(x509):\n    if \n    item_0.upper() == 'DNS':\n        name = \n        item_1.lower()\n        if name.endswith(self.domain):\n            sub = name[:len(name) - len(self.domain) - 1]\n            sub = sub.replace('*', '')\n            sub = sub.strip('.')\n            if sub and sub not in self.found_subs and (sub not in self.normal_names_set) and (sub not in self.cert_subs):\n                self.cert_subs.add(sub)\n                self.queue.put((0, sub))""]",no_found,0
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/scripts/datasets/hmdb51.py,https://github.com/dmlc/gluon-cv/tree/master/scripts/datasets/hmdb51.py,,build_split_list$292,"def build_split_list(split, frame_info, shuffle=False):

    def build_set_list(set_list):
        (rgb_list, flow_list) = (list(), list())
        for item in set_list:
            if item[0] not in frame_info:
                continue
            elif frame_info[item[0]][1] > 0:
                rgb_cnt = frame_info[item[0]][1]
                flow_cnt = frame_info[item[0]][2]
                rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
                flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
            else:
                rgb_list.append('{} {}\n'.format(item[0], item[1]))
                flow_list.append('{} {}\n'.format(item[0], item[1]))
        if shuffle:
            random.shuffle(rgb_list)
            random.shuffle(flow_list)
        return (rgb_list, flow_list)
    (train_rgb_list, train_flow_list) = build_set_list(split[0])
    (test_rgb_list, test_flow_list) = build_set_list(split[1])
    return ((train_rgb_list, test_rgb_list), (train_flow_list, test_flow_list))","for item in set_list:
    if item[0] not in frame_info:
        continue
    elif frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        flow_cnt = frame_info[item[0]][2]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
        flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
    else:
        rgb_list.append('{} {}\n'.format(item[0], item[1]))
        flow_list.append('{} {}\n'.format(item[0], item[1]))","[""for item in set_list:\n    (item_0, item_1, *_) = item\n    if item_0 not in frame_info:\n        continue\n    elif frame_info[item_0][1] > 0:\n        rgb_cnt = frame_info[item_0][1]\n        flow_cnt = frame_info[item_0][2]\n        rgb_list.append('{} {} {}\\n'.format(item_0, rgb_cnt, item_1))\n        flow_list.append('{} {} {}\\n'.format(item_0, flow_cnt, item_1))\n    else:\n        rgb_list.append('{} {}\\n'.format(item_0, item_1))\n        flow_list.append('{} {}\\n'.format(item_0, item_1))"", ""for (item_0, item_1, *item_len) in set_list:\n    if \n    item_0 not in frame_info:\n        continue\n    elif frame_info[\n    item_0][1] > 0:\n        rgb_cnt = frame_info[\n        item_0][1]\n        flow_cnt = frame_info[\n        item_0][2]\n        rgb_list.append('{} {} {}\\n'.format(\n        item_0, rgb_cnt, \n        item_1))\n        flow_list.append('{} {} {}\\n'.format(\n        item_0, flow_cnt, \n        item_1))\n    else:\n        rgb_list.append('{} {}\\n'.format(\n        item_0, \n        item_1))\n        flow_list.append('{} {}\\n'.format(\n        item_0, \n        item_1))""]",no_found,0
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/scripts/datasets/ucf101.py,https://github.com/dmlc/gluon-cv/tree/master/scripts/datasets/ucf101.py,,build_split_list$242,"def build_split_list(split, frame_info, shuffle=False):

    def build_set_list(set_list):
        (rgb_list, flow_list) = (list(), list())
        for item in set_list:
            if item[0] not in frame_info:
                continue
            elif frame_info[item[0]][1] > 0:
                rgb_cnt = frame_info[item[0]][1]
                flow_cnt = frame_info[item[0]][2]
                rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
                flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
            else:
                rgb_list.append('{} {}\n'.format(item[0], item[1]))
                flow_list.append('{} {}\n'.format(item[0], item[1]))
        if shuffle:
            random.shuffle(rgb_list)
            random.shuffle(flow_list)
        return (rgb_list, flow_list)
    (train_rgb_list, train_flow_list) = build_set_list(split[0])
    (test_rgb_list, test_flow_list) = build_set_list(split[1])
    return ((train_rgb_list, test_rgb_list), (train_flow_list, test_flow_list))","for item in set_list:
    if item[0] not in frame_info:
        continue
    elif frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        flow_cnt = frame_info[item[0]][2]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
        flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
    else:
        rgb_list.append('{} {}\n'.format(item[0], item[1]))
        flow_list.append('{} {}\n'.format(item[0], item[1]))","[""for item in set_list:\n    (item_0, item_1, *_) = item\n    if item_0 not in frame_info:\n        continue\n    elif frame_info[item_0][1] > 0:\n        rgb_cnt = frame_info[item_0][1]\n        flow_cnt = frame_info[item_0][2]\n        rgb_list.append('{} {} {}\\n'.format(item_0, rgb_cnt, item_1))\n        flow_list.append('{} {} {}\\n'.format(item_0, flow_cnt, item_1))\n    else:\n        rgb_list.append('{} {}\\n'.format(item_0, item_1))\n        flow_list.append('{} {}\\n'.format(item_0, item_1))"", ""for (item_0, item_1, *item_len) in set_list:\n    if \n    item_0 not in frame_info:\n        continue\n    elif frame_info[\n    item_0][1] > 0:\n        rgb_cnt = frame_info[\n        item_0][1]\n        flow_cnt = frame_info[\n        item_0][2]\n        rgb_list.append('{} {} {}\\n'.format(\n        item_0, rgb_cnt, \n        item_1))\n        flow_list.append('{} {} {}\\n'.format(\n        item_0, flow_cnt, \n        item_1))\n    else:\n        rgb_list.append('{} {}\\n'.format(\n        item_0, \n        item_1))\n        flow_list.append('{} {}\\n'.format(\n        item_0, \n        item_1))""]",no_found,0
PaddleVideo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleVideo/data/ucf101/build_ucf101_file_list.py,https://github.com/PaddlePaddle/PaddleVideo/tree/master/data/ucf101/build_ucf101_file_list.py,,build_set_list$49,"def build_set_list(set_list):
    rgb_list = list()
    for item in set_list:
        if item[0] not in frame_info:
            continue
        elif frame_info[item[0]][1] > 0:
            rgb_cnt = frame_info[item[0]][1]
            rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
        else:
            rgb_list.append('{} {}\n'.format(item[0], item[1]))
    if shuffle:
        random.shuffle(rgb_list)
    return rgb_list","for item in set_list:
    if item[0] not in frame_info:
        continue
    elif frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
    else:
        rgb_list.append('{} {}\n'.format(item[0], item[1]))","[""for item in set_list:\n    (item_0, item_1, *_) = item\n    if item_0 not in frame_info:\n        continue\n    elif frame_info[item_0][1] > 0:\n        rgb_cnt = frame_info[item_0][1]\n        rgb_list.append('{} {} {}\\n'.format(item_0, rgb_cnt, item_1))\n    else:\n        rgb_list.append('{} {}\\n'.format(item_0, item_1))"", ""for (item_0, item_1, *item_len) in set_list:\n    if \n    item_0 not in frame_info:\n        continue\n    elif frame_info[\n    item_0][1] > 0:\n        rgb_cnt = frame_info[\n        item_0][1]\n        rgb_list.append('{} {} {}\\n'.format(\n        item_0, rgb_cnt, \n        item_1))\n    else:\n        rgb_list.append('{} {}\\n'.format(\n        item_0, \n        item_1))""]",no_found,0
two-stream-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/two-stream-pytorch/datasets/build_file_list.py,https://github.com/bryanyzhu/two-stream-pytorch/tree/master/datasets/build_file_list.py,,build_set_list$42,"def build_set_list(set_list):
    (rgb_list, flow_list) = (list(), list())
    for item in set_list:
        rgb_cnt = frame_info[0][item[0]]
        flow_cnt = frame_info[1][item[0]]
        rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
        flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))
    if shuffle:
        random.shuffle(rgb_list)
        random.shuffle(flow_list)
    return (rgb_list, flow_list)","for item in set_list:
    rgb_cnt = frame_info[0][item[0]]
    flow_cnt = frame_info[1][item[0]]
    rgb_list.append('{} {} {}\n'.format(item[0], rgb_cnt, item[1]))
    flow_list.append('{} {} {}\n'.format(item[0], flow_cnt, item[1]))","[""for item in set_list:\n    (item_0, item_1, *_) = item\n    rgb_cnt = frame_info[0][item_0]\n    flow_cnt = frame_info[1][item_0]\n    rgb_list.append('{} {} {}\\n'.format(item_0, rgb_cnt, item_1))\n    flow_list.append('{} {} {}\\n'.format(item_0, flow_cnt, item_1))"", ""for (item_0, item_1, *item_len) in set_list:\n    rgb_cnt = frame_info[0][item_0]\n    flow_cnt = frame_info[1][item_0]\n    rgb_list.append('{} {} {}\\n'.format(item_0, rgb_cnt, item_1))\n    flow_list.append('{} {} {}\\n'.format(item_0, flow_cnt, item_1))""]",no_found,0
MultiQC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MultiQC/multiqc/modules/bbmap/plot_bhist.py,https://github.com/ewels/MultiQC/tree/master/multiqc/modules/bbmap/plot_bhist.py,,plot_bhist$6,"def plot_bhist(samples, file_type, **plot_args):
    """"""Create line graph plot of histogram data for BBMap 'bhist' output.

    The 'samples' parameter could be from the bbmap mod_data dictionary:
    samples = bbmap.MultiqcModule.mod_data[file_type]
    """"""
    all_x = set()
    for item in sorted(chain(*[samples[sample]['data'].items() for sample in samples])):
        all_x.add(item[0])
    columns_to_plot = {'GC': {1: 'C', 2: 'G'}, 'AT': {0: 'A', 3: 'T'}, 'N': {4: 'N'}}
    nucleotide_data = []
    for column_type in columns_to_plot:
        nucleotide_data.append({sample + '.' + column_name: {x: samples[sample]['data'][x][column] * 100 if x in samples[sample]['data'] else 0 for x in all_x} for sample in samples for (column, column_name) in columns_to_plot[column_type].items()})
    plot_params = {'id': 'bbmap-' + file_type + '_plot', 'title': 'BBTools: ' + plot_args['plot_title'], 'xlab': 'Read position', 'ylab': 'Percentage of G+C bases', 'ymin': 0, 'ymax': 100, 'data_labels': [{'name': 'Percentage of G+C bases'}, {'name': 'Percentage of A+T bases'}, {'name': 'Percentage of N bases'}]}
    plot_params.update(plot_args['plot_params'])
    plot = linegraph.plot(nucleotide_data, plot_params)
    return plot","for item in sorted(chain(*[samples[sample]['data'].items() for sample in samples])):
    all_x.add(item[0])","[""for item in sorted(chain(*[samples[sample]['data'].items() for sample in samples])):\n    (item_0, *item_ritemmaining) = item\n    all_x.add(item_0)"", ""for (item_0, *item_len) in sorted(chain(*[samples[sample]['data'].items() for sample in samples])):\n    all_x.add(item_0)""]",no_found,0
hydra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydra/hydra/_internal/hydra.py,https://github.com/facebookresearch/hydra/tree/master/hydra/_internal/hydra.py,Hydra,_print_plugins_profiling_info$449,"def _print_plugins_profiling_info(self, top_n: int) -> None:
    assert log is not None
    stats = Plugins.instance().get_stats()
    if stats is None:
        return
    items = list(stats.modules_import_time.items())
    filtered = filter(lambda x: x[1] > 0.0005, items)
    sorted_items = sorted(filtered, key=lambda x: x[1], reverse=True)
    top_n = max(len(sorted_items), top_n)
    box: List[List[str]] = [['Module', 'Sec']]
    for item in sorted_items[0:top_n]:
        box.append([item[0], f'{item[1]:.3f}'])
    padding = get_column_widths(box)
    log.debug('')
    self._log_header(header='Profiling information', filler='*')
    self._log_header(header=f'Total plugins scan time : {stats.total_time:.3f} seconds', filler='-')
    header = f'| {box[0][0].ljust(padding[0])} | {box[0][1].ljust(padding[1])} |'
    self._log_header(header=header, filler='-')
    del box[0]
    for row in box:
        a = row[0].ljust(padding[0])
        b = row[1].ljust(padding[1])
        log.debug(f'| {a} | {b} |')
    self._log_footer(header=header, filler='-')","for item in sorted_items[0:top_n]:
    box.append([item[0], f'{item[1]:.3f}'])","[""for item in sorted_items[0:top_n]:\n    (item_0, item_1, *_) = item\n    box.append([item_0, f'{item_1:.3f}'])"", ""for (item_0, item_1, *item_len) in sorted_items[0:top_n]:\n    box.append([item_0, f'{item_1:.3f}'])""]",no_found,0
mmaction2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmaction2/tools/data/build_file_list.py,https://github.com/open-mmlab/mmaction2/tree/master/tools/data/build_file_list.py,,build_file_list$90,"def build_file_list(splits, frame_info, shuffle=False):
    """"""Build file list for a certain data split.

    Args:
        splits (tuple): Data split to generate file list.
        frame_info (dict): Dict mapping from frames to path. e.g.,
            'Skiing/v_Skiing_g18_c02': ('data/ucf101/rawframes/Skiing/v_Skiing_g18_c02', 0, 0).  # noqa: E501
        shuffle (bool): Whether to shuffle the file list.

    Returns:
        tuple: RGB file list for training and testing, together with
            Flow file list for training and testing.
    """"""

    def build_list(split):
        """"""Build RGB and Flow file list with a given split.

        Args:
            split (list): Split to be generate file list.

        Returns:
            tuple[list, list]: (rgb_list, flow_list), rgb_list is the
                generated file list for rgb, flow_list is the generated
                file list for flow.
        """"""
        (rgb_list, flow_list) = (list(), list())
        for item in split:
            if item[0] not in frame_info:
                continue
            if frame_info[item[0]][1] > 0:
                rgb_cnt = frame_info[item[0]][1]
                flow_cnt = frame_info[item[0]][2]
                if isinstance(item[1], int):
                    rgb_list.append(f'{item[0]} {rgb_cnt} {item[1]}\n')
                    flow_list.append(f'{item[0]} {flow_cnt} {item[1]}\n')
                elif isinstance(item[1], list):
                    rgb_list.append(f'{item[0]} {rgb_cnt} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
                    rgb_list.append(f'{item[0]} {flow_cnt} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
                else:
                    raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')
            elif isinstance(item[1], int):
                rgb_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
                flow_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
            elif isinstance(item[1], list):
                rgb_list.append(f'{frame_info[item[0]][0]} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
                flow_list.append(f'{frame_info[item[0]][0]} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
            else:
                raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')
        if shuffle:
            random.shuffle(rgb_list)
            random.shuffle(flow_list)
        return (rgb_list, flow_list)
    (train_rgb_list, train_flow_list) = build_list(splits[0])
    (test_rgb_list, test_flow_list) = build_list(splits[1])
    return ((train_rgb_list, test_rgb_list), (train_flow_list, test_flow_list))","for item in split:
    if item[0] not in frame_info:
        continue
    if frame_info[item[0]][1] > 0:
        rgb_cnt = frame_info[item[0]][1]
        flow_cnt = frame_info[item[0]][2]
        if isinstance(item[1], int):
            rgb_list.append(f'{item[0]} {rgb_cnt} {item[1]}\n')
            flow_list.append(f'{item[0]} {flow_cnt} {item[1]}\n')
        elif isinstance(item[1], list):
            rgb_list.append(f'{item[0]} {rgb_cnt} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
            rgb_list.append(f'{item[0]} {flow_cnt} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
        else:
            raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')
    elif isinstance(item[1], int):
        rgb_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
        flow_list.append(f'{frame_info[item[0]][0]} {item[1]}\n')
    elif isinstance(item[1], list):
        rgb_list.append(f'{frame_info[item[0]][0]} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
        flow_list.append(f'{frame_info[item[0]][0]} ' + ' '.join([str(digit) for digit in item[1]]) + '\n')
    else:
        raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')","[""for item in split:\n    (item_0, item_1, *_) = item\n    if item_0 not in frame_info:\n        continue\n    if frame_info[item_0][1] > 0:\n        rgb_cnt = frame_info[item_0][1]\n        flow_cnt = frame_info[item_0][2]\n        if isinstance(item_1, int):\n            rgb_list.append(f'{item_0} {rgb_cnt} {item_1}\\n')\n            flow_list.append(f'{item_0} {flow_cnt} {item_1}\\n')\n        elif isinstance(item_1, list):\n            rgb_list.append(f'{item_0} {rgb_cnt} ' + ' '.join([str(digit) for digit in item_1]) + '\\n')\n            rgb_list.append(f'{item_0} {flow_cnt} ' + ' '.join([str(digit) for digit in item_1]) + '\\n')\n        else:\n            raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')\n    elif isinstance(item_1, int):\n        rgb_list.append(f'{frame_info[item_0][0]} {item_1}\\n')\n        flow_list.append(f'{frame_info[item_0][0]} {item_1}\\n')\n    elif isinstance(item_1, list):\n        rgb_list.append(f'{frame_info[item_0][0]} ' + ' '.join([str(digit) for digit in item_1]) + '\\n')\n        flow_list.append(f'{frame_info[item_0][0]} ' + ' '.join([str(digit) for digit in item_1]) + '\\n')\n    else:\n        raise ValueError('frame_info should be ' + '[`video`(str), `label`(int)|`labels(list[int])`')"", 'for (item_0, item_1, *item_len) in split:\n    if \n    item_0 not in frame_info:\n        continue\n    if frame_info[\n    item_0][1] > 0:\n        rgb_cnt = frame_info[\n        item_0][1]\n        flow_cnt = frame_info[\n        item_0][2]\n        if isinstance(\n        item_1, int):\n            rgb_list.append(f\'{item_0} {rgb_cnt} {item_1}\\n\')\n            flow_list.append(f\'{item_0} {flow_cnt} {item_1}\\n\')\n        elif isinstance(\n        item_1, list):\n            rgb_list.append(f\'{item_0} {rgb_cnt} \' + \' \'.join([str(digit) for digit in \n            item_1]) + \'\\n\')\n            rgb_list.append(f\'{item_0} {flow_cnt} \' + \' \'.join([str(digit) for digit in \n            item_1]) + \'\\n\')\n        else:\n            raise ValueError(\'frame_info should be \' + \'[`video`(str), `label`(int)|`labels(list[int])`\')\n    elif isinstance(\n    item_1, int):\n        rgb_list.append(f""""""{frame_info[\nitem_0][0]} {item_1}\\n"""""")\n        flow_list.append(f""""""{frame_info[\nitem_0][0]} {item_1}\\n"""""")\n    elif isinstance(\n    item_1, list):\n        rgb_list.append(f""""""{frame_info[\nitem_0][0]} """""" + \' \'.join([str(digit) for digit in \n        item_1]) + \'\\n\')\n        flow_list.append(f""""""{frame_info[\nitem_0][0]} """""" + \' \'.join([str(digit) for digit in \n        item_1]) + \'\\n\')\n    else:\n        raise ValueError(\'frame_info should be \' + \'[`video`(str), `label`(int)|`labels(list[int])`\')']",no_found,0
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/full_sync.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/full_sync.py,FullSync,remove_library$524,"def remove_library(self, library_id, dialog):
    """""" Remove library by their id from the Kodi database.
        """"""
    direct_path = self.library.direct_path
    with Database('jellyfin') as jellyfindb:
        db = jellyfin_db.JellyfinDatabase(jellyfindb.cursor)
        library = db.get_view(library_id.replace('Mixed:', ''))
        items = db.get_item_by_media_folder(library_id.replace('Mixed:', ''))
        media = 'music' if library.media_type == 'music' else 'video'
        if media == 'music':
            settings('MusicRescan.bool', False)
        if items:
            with self.library.music_database_lock if media == 'music' else self.library.database_lock:
                with Database(media) as kodidb:
                    count = 0
                    if library.media_type == 'mixed':
                        movies = [x for x in items if x[1] == 'Movie']
                        tvshows = [x for x in items if x[1] == 'Series']
                        obj = Movies(self.server, jellyfindb, kodidb, direct_path, library).remove
                        for item in movies:
                            obj(item[0])
                            dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
                            count += 1
                        obj = TVShows(self.server, jellyfindb, kodidb, direct_path, library).remove
                        for item in tvshows:
                            obj(item[0])
                            dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
                            count += 1
                    else:
                        default_args = (self.server, jellyfindb, kodidb, direct_path)
                        for item in items:
                            if item[1] in ('Series', 'Season', 'Episode'):
                                TVShows(*default_args).remove(item[0])
                            elif item[1] in ('Movie', 'BoxSet'):
                                Movies(*default_args).remove(item[0])
                            elif item[1] in ('MusicAlbum', 'MusicArtist', 'AlbumArtist', 'Audio'):
                                Music(*default_args).remove(item[0])
                            elif item[1] == 'MusicVideo':
                                MusicVideos(*default_args).remove(item[0])
                            dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library[0]))
                            count += 1
    self.sync = get_sync()
    if library_id in self.sync['Whitelist']:
        self.sync['Whitelist'].remove(library_id)
    elif 'Mixed:%s' % library_id in self.sync['Whitelist']:
        self.sync['Whitelist'].remove('Mixed:%s' % library_id)
    save_sync(self.sync)","for item in tvshows:
    obj(item[0])
    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
    count += 1","[""for item in tvshows:\n    (item_0, *item_ritemmaining) = item\n    obj(item_0)\n    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))\n    count += 1"", ""for (item_0, *item_len) in tvshows:\n    obj(item_0)\n    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))\n    count += 1""]",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for js_info in js_infos:
    js_name = js_info[1]
    js_path = js_info[2]
    para2 = para1.insert_paragraph_before('')
    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para2.add_run(js_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para2.add_run(vuln_info[8])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    self.creat_num = self.creat_num + 1
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)","[""for js_info in js_infos:\n    (_, js_info_1, js_info_2, *js_info_rjs_infomaining) = js_info\n    js_name = js_info_1\n    js_path = js_info_2\n    para2 = para1.insert_paragraph_before('')\n    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\\n')\n    run.font.name = 'Arial'\n    run.font.size = Pt(16)\n    run.font.bold = True\n    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para2.add_run(js_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para2.add_run('\\n' + Utils().getMyWord('{r_js_des}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para2.add_run(vuln_info[8])\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run6 = para2.add_run('\\n' + Utils().getMyWord('{r_js_detial}'))\n    run6.font.name = 'Arial'\n    run6.font.size = Pt(10)\n    run6.font.bold = True\n    self.creat_num = self.creat_num + 1\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)"", ""for (js_info_0, js_info_1, js_info_2, *js_info_len) in js_infos:\n    js_name = \n    js_info_1\n    js_path = \n    js_info_2\n    para2 = para1.insert_paragraph_before('')\n    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\\n')\n    run.font.name = 'Arial'\n    run.font.size = Pt(16)\n    run.font.bold = True\n    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para2.add_run(js_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para2.add_run('\\n' + Utils().getMyWord('{r_js_des}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para2.add_run(vuln_info[8])\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run6 = para2.add_run('\\n' + Utils().getMyWord('{r_js_detial}'))\n    run6.font.name = 'Arial'\n    run6.font.size = Pt(10)\n    run6.font.bold = True\n    self.creat_num = self.creat_num + 1\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)""]",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for js_path in js_paths:
    run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para2.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para2.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num = self.creat_num + 1
    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)","[""for js_path in js_paths:\n    (js_path_0, *js_path_rjs_pathmaining) = js_path\n    run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para2.add_run(api_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para2.add_run('\\n' + Utils().getMyWord('{r_api_js}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para2.add_run(js_path_0)\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5 = para2.add_run('\\n' + Utils().getMyWord('{r_api_res}'))\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5.font.bold = True\n    self.creat_num = self.creat_num + 1\n    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)"", ""for (js_path_0, *js_path_len) in js_paths:\n    run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para2.add_run(api_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para2.add_run('\\n' + Utils().getMyWord('{r_api_js}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para2.add_run(js_path_0)\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5 = para2.add_run('\\n' + Utils().getMyWord('{r_api_res}'))\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5.font.bold = True\n    self.creat_num = self.creat_num + 1\n    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)""]",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for js_path in js_paths:
    run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para2.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para2.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num = self.creat_num + 1
    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)","[""for js_path in js_paths:\n    (js_path_0, *js_path_rjs_pathmaining) = js_path\n    run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para2.add_run(api_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para2.add_run('\\n' + Utils().getMyWord('{r_api_js}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para2.add_run(js_path_0)\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5 = para2.add_run('\\n' + Utils().getMyWord('{r_api_res}'))\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5.font.bold = True\n    self.creat_num = self.creat_num + 1\n    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)"", ""for (js_path_0, *js_path_len) in js_paths:\n    run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para2.add_run(api_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para2.add_run('\\n' + Utils().getMyWord('{r_api_js}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para2.add_run(js_path_0)\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5 = para2.add_run('\\n' + Utils().getMyWord('{r_api_res}'))\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5.font.bold = True\n    self.creat_num = self.creat_num + 1\n    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)""]",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for js_path in js_paths:
    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para3.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para3.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num1 = self.creat_num1 + 1
    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    run6 = para2.add_run(Utils().getMyWord('{request_info}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)","[""for js_path in js_paths:\n    (js_path_0, *js_path_rjs_pathmaining) = js_path\n    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para3.add_run(api_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para3.add_run('\\n' + Utils().getMyWord('{r_api_js}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para3.add_run(js_path_0)\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5 = para3.add_run('\\n' + Utils().getMyWord('{r_api_res}'))\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5.font.bold = True\n    self.creat_num1 = self.creat_num1 + 1\n    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)\n    run6 = para2.add_run(Utils().getMyWord('{request_info}'))\n    run6.font.name = 'Arial'\n    run6.font.size = Pt(10)\n    run6.font.bold = True\n    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)"", ""for (js_path_0, *js_path_len) in js_paths:\n    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para3.add_run(api_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para3.add_run('\\n' + Utils().getMyWord('{r_api_js}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para3.add_run(js_path_0)\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5 = para3.add_run('\\n' + Utils().getMyWord('{r_api_res}'))\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5.font.bold = True\n    self.creat_num1 = self.creat_num1 + 1\n    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)\n    run6 = para2.add_run(Utils().getMyWord('{request_info}'))\n    run6.font.name = 'Arial'\n    run6.font.size = Pt(10)\n    run6.font.bold = True\n    vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)""]",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for js_path in js_paths:
    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para3.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para3.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num1 = self.creat_num1 + 1
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","[""for js_path in js_paths:\n    (js_path_0, *js_path_rjs_pathmaining) = js_path\n    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para3.add_run(api_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para3.add_run('\\n' + Utils().getMyWord('{r_api_js}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para3.add_run(js_path_0)\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5 = para3.add_run('\\n' + Utils().getMyWord('{request_info}'))\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5.font.bold = True\n    self.creat_num1 = self.creat_num1 + 1\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)\n    run6 = para2.add_run('\\n' + Utils().getMyWord('{r_api_res}'))\n    run6.font.name = 'Arial'\n    run6.font.size = Pt(10)\n    run6.font.bold = True\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)"", ""for (js_path_0, *js_path_len) in js_paths:\n    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para3.add_run(api_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para3.add_run('\\n' + Utils().getMyWord('{r_api_js}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para3.add_run(js_path_0)\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5 = para3.add_run('\\n' + Utils().getMyWord('{request_info}'))\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5.font.bold = True\n    self.creat_num1 = self.creat_num1 + 1\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)\n    run6 = para2.add_run('\\n' + Utils().getMyWord('{r_api_res}'))\n    run6.font.name = 'Arial'\n    run6.font.size = Pt(10)\n    run6.font.bold = True\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)""]",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for js_path in js_paths:
    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para3.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para3.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num1 = self.creat_num1 + 1
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","[""for js_path in js_paths:\n    (js_path_0, *js_path_rjs_pathmaining) = js_path\n    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para3.add_run(api_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para3.add_run('\\n' + Utils().getMyWord('{r_api_js}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para3.add_run(js_path_0)\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5 = para3.add_run('\\n' + Utils().getMyWord('{request_info}'))\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5.font.bold = True\n    self.creat_num1 = self.creat_num1 + 1\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)\n    run6 = para2.add_run('\\n' + Utils().getMyWord('{r_api_res}'))\n    run6.font.name = 'Arial'\n    run6.font.size = Pt(10)\n    run6.font.bold = True\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)"", ""for (js_path_0, *js_path_len) in js_paths:\n    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para3.add_run(api_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para3.add_run('\\n' + Utils().getMyWord('{r_api_js}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para3.add_run(js_path_0)\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5 = para3.add_run('\\n' + Utils().getMyWord('{request_info}'))\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5.font.bold = True\n    self.creat_num1 = self.creat_num1 + 1\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)\n    run6 = para2.add_run('\\n' + Utils().getMyWord('{r_api_res}'))\n    run6.font.name = 'Arial'\n    run6.font.size = Pt(10)\n    run6.font.bold = True\n    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)""]",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for js_path in js_paths:
    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
    run2.font.name = 'Arial'
    run2.font.size = Pt(10)
    run2.font.bold = True
    run3 = para3.add_run(api_path)
    run3.font.name = 'Arial'
    run3.font.size = Pt(10)
    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
    run4.font.name = 'Arial'
    run4.font.size = Pt(10)
    run4.font.bold = True
    run5 = para3.add_run(js_path[0])
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
    run5.font.name = 'Arial'
    run5.font.size = Pt(10)
    run5.font.bold = True
    self.creat_num1 = self.creat_num1 + 1
    info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
    info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
    Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
    run6.font.name = 'Arial'
    run6.font.size = Pt(10)
    run6.font.bold = True
    Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)","[""for js_path in js_paths:\n    (js_path_0, *js_path_rjs_pathmaining) = js_path\n    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para3.add_run(api_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para3.add_run('\\n' + Utils().getMyWord('{r_api_js}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para3.add_run(js_path_0)\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5 = para3.add_run('\\n' + Utils().getMyWord('{request_info}'))\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5.font.bold = True\n    self.creat_num1 = self.creat_num1 + 1\n    info1 = '1: ' + vuln_info[5].split('')[0] + '\\n\\n' + '2: ' + vuln_info[5].split('')[1]\n    info2 = '1: ' + vuln_info[6].split('')[0] + '\\n\\n' + '2: ' + vuln_info[6].split('')[1]\n    Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)\n    run6 = para2.add_run('\\n' + Utils().getMyWord('{r_api_res}'))\n    run6.font.name = 'Arial'\n    run6.font.size = Pt(10)\n    run6.font.bold = True\n    Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)"", ""for (js_path_0, *js_path_len) in js_paths:\n    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))\n    run2.font.name = 'Arial'\n    run2.font.size = Pt(10)\n    run2.font.bold = True\n    run3 = para3.add_run(api_path)\n    run3.font.name = 'Arial'\n    run3.font.size = Pt(10)\n    run4 = para3.add_run('\\n' + Utils().getMyWord('{r_api_js}'))\n    run4.font.name = 'Arial'\n    run4.font.size = Pt(10)\n    run4.font.bold = True\n    run5 = para3.add_run(js_path_0)\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5 = para3.add_run('\\n' + Utils().getMyWord('{request_info}'))\n    run5.font.name = 'Arial'\n    run5.font.size = Pt(10)\n    run5.font.bold = True\n    self.creat_num1 = self.creat_num1 + 1\n    info1 = '1: ' + vuln_info[5].split('')[0] + '\\n\\n' + '2: ' + vuln_info[5].split('')[1]\n    info2 = '1: ' + vuln_info[6].split('')[0] + '\\n\\n' + '2: ' + vuln_info[6].split('')[1]\n    Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)\n    run6 = para2.add_run('\\n' + Utils().getMyWord('{r_api_res}'))\n    run6.font.name = 'Arial'\n    run6.font.size = Pt(10)\n    run6.font.bold = True\n    Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)""]",no_found,0
redis-py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/redis-py/tests/test_search.py,https://github.com/redis/redis-py/tree/master/tests/test_search.py,,test_drop_index$308,"def test_drop_index(client):
    """"""
    Ensure the index gets dropped by data remains by default
    """"""
    for x in range(20):
        for keep_docs in [[True, {}], [False, {'name': 'haveit'}]]:
            idx = 'HaveIt'
            index = getClient(client)
            index.hset('index:haveit', mapping={'name': 'haveit'})
            idef = IndexDefinition(prefix=['index:'])
            index.ft(idx).create_index((TextField('name'),), definition=idef)
            waitForIndex(index, idx)
            index.ft(idx).dropindex(delete_documents=keep_docs[0])
            i = index.hgetall('index:haveit')
            assert i == keep_docs[1]","for keep_docs in [[True, {}], [False, {'name': 'haveit'}]]:
    idx = 'HaveIt'
    index = getClient(client)
    index.hset('index:haveit', mapping={'name': 'haveit'})
    idef = IndexDefinition(prefix=['index:'])
    index.ft(idx).create_index((TextField('name'),), definition=idef)
    waitForIndex(index, idx)
    index.ft(idx).dropindex(delete_documents=keep_docs[0])
    i = index.hgetall('index:haveit')
    assert i == keep_docs[1]","[""for keep_docs in [[True, {}], [False, {'name': 'haveit'}]]:\n    (keep_docs_0, keep_docs_1, *_) = keep_docs\n    idx = 'HaveIt'\n    index = getClient(client)\n    index.hset('index:haveit', mapping={'name': 'haveit'})\n    idef = IndexDefinition(prefix=['index:'])\n    index.ft(idx).create_index((TextField('name'),), definition=idef)\n    waitForIndex(index, idx)\n    index.ft(idx).dropindex(delete_documents=keep_docs_0)\n    i = index.hgetall('index:haveit')\n    assert i == keep_docs_1"", ""for (keep_docs_0, keep_docs_1, *keep_docs_len) in [[True, {}], [False, {'name': 'haveit'}]]:\n    idx = 'HaveIt'\n    index = getClient(client)\n    index.hset('index:haveit', mapping={'name': 'haveit'})\n    idef = IndexDefinition(prefix=['index:'])\n    index.ft(idx).create_index((TextField('name'),), definition=idef)\n    waitForIndex(index, idx)\n    index.ft(idx).dropindex(delete_documents=\n    keep_docs_0)\n    i = index.hgetall('index:haveit')\n    assert i == \n    keep_docs_1""]",no_found,0
hfnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hfnet/colmap-helpers/features_from_npz.py,https://github.com/ethz-asl/hfnet/tree/master/colmap-helpers/features_from_npz.py,,export_features_from_npz$17,"def export_features_from_npz(filename, in_path, out_path):
    path_file = os.path.join(in_path, filename)
    frame1 = np.load(path_file)
    filename = os.path.splitext(os.path.basename(path_file))[0]
    out_path_and_name = os.path.join(out_path, filename) + '.jpg.txt'
    outfile = open(out_path_and_name, 'w+')
    SIFT_SIZE = 128
    kp1 = frame1['keypoints']
    outfile.write(str(kp1.shape[0]) + ' ' + str(SIFT_SIZE) + '\n')
    for keypoint in kp1:
        s = str(keypoint[0]) + ' ' + str(keypoint[1])
        s += ' 1 1 ' + '1 ' * SIFT_SIZE + '\n'
        outfile.write(s)
    outfile.close()","for keypoint in kp1:
    s = str(keypoint[0]) + ' ' + str(keypoint[1])
    s += ' 1 1 ' + '1 ' * SIFT_SIZE + '\n'
    outfile.write(s)","[""for keypoint in kp1:\n    (keypoint_0, keypoint_1, *_) = keypoint\n    s = str(keypoint_0) + ' ' + str(keypoint_1)\n    s += ' 1 1 ' + '1 ' * SIFT_SIZE + '\\n'\n    outfile.write(s)"", ""for (keypoint_0, keypoint_1, *keypoint_len) in kp1:\n    s = str(keypoint_0) + ' ' + str(keypoint_1)\n    s += ' 1 1 ' + '1 ' * SIFT_SIZE + '\\n'\n    outfile.write(s)""]",no_found,0
wttr.in,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wttr.in/lib/wttr_srv.py,https://github.com/chubin/wttr.in/tree/master/lib/wttr_srv.py,,_parse_language_header$79,"def _parse_language_header(header):
    """"""
    >>> _parse_language_header(""en-US,en;q=0.9"")
    >>> _parse_language_header(""en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7"")
    >>> _parse_language_header(""xx, fr-CA;q=0.8, da-DK;q=0.9"")
    'da'
    """"""

    def _parse_accept_language(accept_language):
        languages = accept_language.split(',')
        locale_q_pairs = []
        for language in languages:
            try:
                if language.split(';')[0] == language:
                    locale_q_pairs.append((language.strip(), 1))
                else:
                    locale = language.split(';')[0].strip()
                    weight = float(language.split(';')[1].split('=')[1])
                    locale_q_pairs.append((locale, weight))
            except (IndexError, ValueError):
                pass
        return locale_q_pairs

    def _find_supported_language(accepted_languages):

        def supported_langs():
            """"""Yields all pairs in the Accept-Language header
            supported in SUPPORTED_LANGS or None if 'en' is the preferred""""""
            for lang_tuple in accepted_languages:
                lang = lang_tuple[0]
                if '-' in lang:
                    lang = lang.split('-', 1)[0]
                if lang in SUPPORTED_LANGS:
                    yield (lang, lang_tuple[1])
                elif lang == 'en':
                    yield (None, lang_tuple[1])
        try:
            return max(supported_langs(), key=lambda lang_tuple: lang_tuple[1])[0]
        except ValueError:
            return None
    return _find_supported_language(_parse_accept_language(header))","for lang_tuple in accepted_languages:
    lang = lang_tuple[0]
    if '-' in lang:
        lang = lang.split('-', 1)[0]
    if lang in SUPPORTED_LANGS:
        yield (lang, lang_tuple[1])
    elif lang == 'en':
        yield (None, lang_tuple[1])","[""for lang_tuple in accepted_languages:\n    (lang_tuple_0, lang_tuple_1, *_) = lang_tuple\n    lang = lang_tuple_0\n    if '-' in lang:\n        lang = lang.split('-', 1)[0]\n    if lang in SUPPORTED_LANGS:\n        yield (lang, lang_tuple_1)\n    elif lang == 'en':\n        yield (None, lang_tuple_1)"", ""for (lang_tuple_0, lang_tuple_1, *lang_tuple_len) in accepted_languages:\n    lang = \n    lang_tuple_0\n    if '-' in lang:\n        lang = lang.split('-', 1)[0]\n    if lang in SUPPORTED_LANGS:\n        yield (lang, \n        lang_tuple_1)\n    elif lang == 'en':\n        yield (None, \n        lang_tuple_1)""]",no_found,0
StyleGAN-nada,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/StyleGAN-nada/convert_weight.py,https://github.com/rinongal/StyleGAN-nada/tree/master//convert_weight.py,,if_main_my$199,"if __name__ == '__main__':
    device = 'cuda'
    parser = argparse.ArgumentParser(description='Tensorflow to pytorch model checkpoint converter')
    parser.add_argument('--repo', type=str, required=True, help='path to the offical StyleGAN2 repository with dnnlib/ folder')
    parser.add_argument('--gen', action='store_true', help='convert the generator weights')
    parser.add_argument('--disc', action='store_true', help='convert the discriminator weights')
    parser.add_argument('--channel_multiplier', type=int, default=2, help='channel multiplier factor. config-f = 2, else = 1')
    parser.add_argument('path', metavar='PATH', help='path to the tensorflow weights')
    args = parser.parse_args()
    sys.path.append(args.repo)
    import dnnlib
    from dnnlib import tflib
    tflib.init_tf()
    with open(args.path, 'rb') as f:
        (generator, discriminator, g_ema) = pickle.load(f)
    size = g_ema.output_shape[2]
    n_mlp = 0
    mapping_layers_names = g_ema.__getstate__()['components']['mapping'].list_layers()
    for layer in mapping_layers_names:
        if layer[0].startswith('Dense'):
            n_mlp += 1
    g = Generator(size, 512, n_mlp, channel_multiplier=args.channel_multiplier)
    state_dict = g.state_dict()
    state_dict = fill_statedict(state_dict, g_ema.vars, size, n_mlp)
    g.load_state_dict(state_dict)
    latent_avg = torch.from_numpy(g_ema.vars['dlatent_avg'].value().eval())
    ckpt = {'g_ema': state_dict, 'latent_avg': latent_avg}
    if args.gen:
        g_train = Generator(size, 512, n_mlp, channel_multiplier=args.channel_multiplier)
        g_train_state = g_train.state_dict()
        g_train_state = fill_statedict(g_train_state, generator.vars, size, n_mlp)
        ckpt['g'] = g_train_state
    if args.disc:
        disc = Discriminator(size, channel_multiplier=args.channel_multiplier)
        d_state = disc.state_dict()
        d_state = discriminator_fill_statedict(d_state, discriminator.vars, size)
        ckpt['d'] = d_state
    name = os.path.splitext(args.path)[0]
    torch.save(ckpt, name + '.pt')
    batch_size = {256: 16, 512: 9, 1024: 4}
    n_sample = batch_size.get(size, 25)
    g = g.to(device)
    z = np.random.RandomState(0).randn(n_sample, 512).astype('float32')
    with torch.no_grad():
        (img_pt, _) = g([torch.from_numpy(z).to(device)], truncation=0.5, truncation_latent=latent_avg.to(device), randomize_noise=False)
    Gs_kwargs = dnnlib.EasyDict()
    Gs_kwargs.randomize_noise = False
    img_tf = g_ema.run(z, None, **Gs_kwargs)
    img_tf = torch.from_numpy(img_tf).to(device)
    img_diff = ((img_pt + 1) / 2).clamp(0.0, 1.0) - ((img_tf.to(device) + 1) / 2).clamp(0.0, 1.0)
    img_concat = torch.cat((img_tf, img_pt, img_diff), dim=0)
    print(img_diff.abs().max())
    utils.save_image(img_concat, name + '.png', nrow=n_sample, normalize=True, range=(-1, 1))","for layer in mapping_layers_names:
    if layer[0].startswith('Dense'):
        n_mlp += 1","[""for layer in mapping_layers_names:\n    (layer_0, *layer_rlayermaining) = layer\n    if layer_0.startswith('Dense'):\n        n_mlp += 1"", ""for (layer_0, *layer_len) in mapping_layers_names:\n    if \n    layer_0.startswith('Dense'):\n        n_mlp += 1""]",no_found,0
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/printing/pretty/pretty.py,https://github.com/sympy/sympy/tree/master/sympy/printing/pretty/pretty.py,PrettyPrinter,_print_Integral$426,"def _print_Integral(self, integral):
    f = integral.function
    prettyF = self._print(f)
    if f.is_Add:
        prettyF = prettyForm(*prettyF.parens())
    arg = prettyF
    for x in integral.limits:
        prettyArg = self._print(x[0])
        if prettyArg.width() > 1:
            prettyArg = prettyForm(*prettyArg.parens())
        arg = prettyForm(*arg.right(' d', prettyArg))
    firstterm = True
    s = None
    for lim in integral.limits:
        h = arg.height()
        H = h + 2
        ascii_mode = not self._use_unicode
        if ascii_mode:
            H += 2
        vint = vobj('int', H)
        pform = prettyForm(vint)
        pform.baseline = arg.baseline + (H - h) // 2
        if len(lim) > 1:
            if len(lim) == 2:
                prettyA = prettyForm('')
                prettyB = self._print(lim[1])
            if len(lim) == 3:
                prettyA = self._print(lim[1])
                prettyB = self._print(lim[2])
            if ascii_mode:
                spc = max(1, 3 - prettyB.width())
                prettyB = prettyForm(*prettyB.left(' ' * spc))
                spc = max(1, 4 - prettyA.width())
                prettyA = prettyForm(*prettyA.right(' ' * spc))
            pform = prettyForm(*pform.above(prettyB))
            pform = prettyForm(*pform.below(prettyA))
        if not ascii_mode:
            pform = prettyForm(*pform.right(' '))
        if firstterm:
            s = pform
            firstterm = False
        else:
            s = prettyForm(*s.left(pform))
    pform = prettyForm(*arg.left(s))
    pform.binding = prettyForm.MUL
    return pform","for lim in integral.limits:
    h = arg.height()
    H = h + 2
    ascii_mode = not self._use_unicode
    if ascii_mode:
        H += 2
    vint = vobj('int', H)
    pform = prettyForm(vint)
    pform.baseline = arg.baseline + (H - h) // 2
    if len(lim) > 1:
        if len(lim) == 2:
            prettyA = prettyForm('')
            prettyB = self._print(lim[1])
        if len(lim) == 3:
            prettyA = self._print(lim[1])
            prettyB = self._print(lim[2])
        if ascii_mode:
            spc = max(1, 3 - prettyB.width())
            prettyB = prettyForm(*prettyB.left(' ' * spc))
            spc = max(1, 4 - prettyA.width())
            prettyA = prettyForm(*prettyA.right(' ' * spc))
        pform = prettyForm(*pform.above(prettyB))
        pform = prettyForm(*pform.below(prettyA))
    if not ascii_mode:
        pform = prettyForm(*pform.right(' '))
    if firstterm:
        s = pform
        firstterm = False
    else:
        s = prettyForm(*s.left(pform))","[""for lim in integral.limits:\n    (_, lim_1, lim_2, *lim_rlimmaining) = lim\n    h = arg.height()\n    H = h + 2\n    ascii_mode = not self._use_unicode\n    if ascii_mode:\n        H += 2\n    vint = vobj('int', H)\n    pform = prettyForm(vint)\n    pform.baseline = arg.baseline + (H - h) // 2\n    if len(lim) > 1:\n        if len(lim) == 2:\n            prettyA = prettyForm('')\n            prettyB = self._print(lim_1)\n        if len(lim) == 3:\n            prettyA = self._print(lim_1)\n            prettyB = self._print(lim_2)\n        if ascii_mode:\n            spc = max(1, 3 - prettyB.width())\n            prettyB = prettyForm(*prettyB.left(' ' * spc))\n            spc = max(1, 4 - prettyA.width())\n            prettyA = prettyForm(*prettyA.right(' ' * spc))\n        pform = prettyForm(*pform.above(prettyB))\n        pform = prettyForm(*pform.below(prettyA))\n    if not ascii_mode:\n        pform = prettyForm(*pform.right(' '))\n    if firstterm:\n        s = pform\n        firstterm = False\n    else:\n        s = prettyForm(*s.left(pform))""]",no_found,0
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/base/check_legacy_includes/fsc_sc2.py,https://github.com/tribe29/checkmk/tree/master/cmk/base/check_legacy_includes/fsc_sc2.py,,inventory_fsc_sc2_temp$318,"def inventory_fsc_sc2_temp(info):
    for line in info:
        if line[1] != '2':
            yield (line[0], {})","for line in info:
    if line[1] != '2':
        yield (line[0], {})","[""for line in info:\n    (line_0, line_1, *_) = line\n    if line_1 != '2':\n        yield (line_0, {})"", ""for (line_0, line_1, *line_len) in info:\n    if \n    line_1 != '2':\n        yield (\n        line_0, {})""]",no_found,0
yolov5-face,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yolov5-face/test_widerface.py,https://github.com/deepcam-cn/yolov5-face/tree/master//test_widerface.py,,if_main_my$113,"if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default='runs/train/exp5/weights/last.pt', help='model.pt path(s)')
    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf-thres', type=float, default=0.02, help='object confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.5, help='IOU threshold for NMS')
    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')
    parser.add_argument('--project', default='runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--save_folder', default='./widerface_evaluate/widerface_txt/', type=str, help='Dir to save txt results')
    parser.add_argument('--dataset_folder', default='../WiderFace/val/images/', type=str, help='dataset path')
    parser.add_argument('--folder_pict', default='/yolov5-face/data/widerface/val/wider_val.txt', type=str, help='folder_pict')
    opt = parser.parse_args()
    print(opt)
    pict_folder = {}
    with open(opt.folder_pict, 'r') as f:
        lines = f.readlines()
        for line in lines:
            line = line.strip().split('/')
            pict_folder[line[-1]] = line[-2]
    device = select_device(opt.device)
    model = attempt_load(opt.weights, map_location=device)
    with torch.no_grad():
        testset_folder = opt.dataset_folder
        for image_path in tqdm(glob.glob(os.path.join(testset_folder, '*'))):
            if image_path.endswith('.txt'):
                continue
            img0 = cv2.imread(image_path)
            if img0 is None:
                print(f'ignore : {image_path}')
                continue
            boxes = detect(model, img0)
            image_name = os.path.basename(image_path)
            txt_name = os.path.splitext(image_name)[0] + '.txt'
            save_name = os.path.join(opt.save_folder, pict_folder[image_name], txt_name)
            dirname = os.path.dirname(save_name)
            if not os.path.isdir(dirname):
                os.makedirs(dirname)
            with open(save_name, 'w') as fd:
                file_name = os.path.basename(save_name)[:-4] + '\n'
                bboxs_num = str(len(boxes)) + '\n'
                fd.write(file_name)
                fd.write(bboxs_num)
                for box in boxes:
                    fd.write('%d %d %d %d %.03f' % (box[0], box[1], box[2], box[3], box[4] if box[4] <= 1 else 1) + '\n')
        print('done.')","for line in lines:
    line = line.strip().split('/')
    pict_folder[line[-1]] = line[-2]","[""for line in lines:\n    (*line_rlinemaining, line_nlineg2) = line\n    line = line.strip().split('/')\n    pict_folder[line[-1]] = line_nlineg2""]",no_found,0
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/base/plugins/agent_based/jenkins_jobs.py,https://github.com/tribe29/checkmk/tree/master/cmk/base/plugins/agent_based/jenkins_jobs.py,,parse_jenkins_jobs$59,"def parse_jenkins_jobs(string_table) -> Section:
    parsed: Dict[str, JenkinsJobInfo] = {}
    for line in string_table:
        jenkins_data = json.loads(line[0])
        parsed.update(_handle_job_type(jenkins_data, {}, ''))
    return parsed","for line in string_table:
    jenkins_data = json.loads(line[0])
    parsed.update(_handle_job_type(jenkins_data, {}, ''))","[""for line in string_table:\n    (line_0, *line_rlinemaining) = line\n    jenkins_data = json.loads(line_0)\n    parsed.update(_handle_job_type(jenkins_data, {}, ''))"", ""for (line_0, *line_len) in string_table:\n    jenkins_data = json.loads(line_0)\n    parsed.update(_handle_job_type(jenkins_data, {}, ''))""]",no_found,0
WireViz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WireViz/src/wireviz/Harness.py,https://github.com/formatc1702/WireViz/tree/master/src/wireviz/Harness.py,Harness,create_graph$94,"def create_graph(self) -> Graph:
    dot = Graph()
    dot.body.append(f'// Graph generated by {APP_NAME} {__version__}\n')
    dot.body.append(f'// {APP_URL}\n')
    dot.attr('graph', rankdir='LR', ranksep='2', bgcolor=wv_colors.translate_color(self.options.bgcolor, 'HEX'), nodesep='0.33', fontname=self.options.fontname)
    dot.attr('node', shape='none', width='0', height='0', margin='0', style='filled', fillcolor=wv_colors.translate_color(self.options.bgcolor_node, 'HEX'), fontname=self.options.fontname)
    dot.attr('edge', style='bold', fontname=self.options.fontname)
    for (_, cable) in self.cables.items():
        for connection_color in cable.connections:
            if connection_color.from_port is not None:
                self.connectors[connection_color.from_name].ports_right = True
            if connection_color.to_port is not None:
                self.connectors[connection_color.to_name].ports_left = True
    for connector in self.connectors.values():
        if not (connector.ports_left or connector.ports_right):
            connector.ports_left = True
        html = []
        rows = [[f'{html_bgcolor(connector.bgcolor_title)}{remove_links(connector.name)}' if connector.show_name else None], [pn_info_string(HEADER_PN, None, remove_links(connector.pn)), html_line_breaks(pn_info_string(HEADER_MPN, connector.manufacturer, connector.mpn)), html_line_breaks(pn_info_string(HEADER_SPN, connector.supplier, connector.spn))], [html_line_breaks(connector.type), html_line_breaks(connector.subtype), f'{connector.pincount}-pin' if connector.show_pincount else None, translate_color(connector.color, self.options.color_mode) if connector.color else None, html_colorbar(connector.color)], '<!-- connector table -->' if connector.style != 'simple' else None, [html_image(connector.image)], [html_caption(connector.image)]]
        rows.extend(get_additional_component_table(self, connector))
        rows.append([html_line_breaks(connector.notes)])
        html.extend(nested_html_table(rows, html_bgcolor_attr(connector.bgcolor)))
        if connector.style != 'simple':
            pinhtml = []
            pinhtml.append('<table border=""0"" cellspacing=""0"" cellpadding=""3"" cellborder=""1"">')
            for (pinindex, (pinname, pinlabel, pincolor)) in enumerate(zip_longest(connector.pins, connector.pinlabels, connector.pincolors)):
                if connector.hide_disconnected_pins and (not connector.visible_pins.get(pinname, False)):
                    continue
                pinhtml.append('   <tr>')
                if connector.ports_left:
                    pinhtml.append(f'    <td port=""p{pinindex + 1}l"">{pinname}</td>')
                if pinlabel:
                    pinhtml.append(f'    <td>{pinlabel}</td>')
                if connector.pincolors:
                    if pincolor in wv_colors._color_hex.keys():
                        pinhtml.append(f'    <td sides=""tbl"">{translate_color(pincolor, self.options.color_mode)}</td>')
                        pinhtml.append('    <td sides=""tbr"">')
                        pinhtml.append('     <table border=""0"" cellborder=""1""><tr>')
                        pinhtml.append(f'''      <td bgcolor=""{wv_colors.translate_color(pincolor, 'HEX')}"" width=""8"" height=""8"" fixedsize=""true""></td>''')
                        pinhtml.append('     </tr></table>')
                        pinhtml.append('    </td>')
                    else:
                        pinhtml.append('    <td colspan=""2""></td>')
                if connector.ports_right:
                    pinhtml.append(f'    <td port=""p{pinindex + 1}r"">{pinname}</td>')
                pinhtml.append('   </tr>')
            pinhtml.append('  </table>')
            html = [row.replace('<!-- connector table -->', '\n'.join(pinhtml)) for row in html]
        html = '\n'.join(html)
        dot.node(connector.name, label=f'<\n{html}\n>', shape='box', style='filled', fillcolor=translate_color(self.options.bgcolor_connector, 'HEX'))
        if len(connector.loops) > 0:
            dot.attr('edge', color='#000000:#ffffff:#000000')
            if connector.ports_left:
                loop_side = 'l'
                loop_dir = 'w'
            elif connector.ports_right:
                loop_side = 'r'
                loop_dir = 'e'
            else:
                raise Exception('No side for loops')
            for loop in connector.loops:
                dot.edge(f'{connector.name}:p{loop[0]}{loop_side}:{loop_dir}', f'{connector.name}:p{loop[1]}{loop_side}:{loop_dir}')
    pad = any((len(colorstr) > 2 for cable in self.cables.values() for colorstr in cable.colors))
    for cable in self.cables.values():
        html = []
        awg_fmt = ''
        if cable.show_equiv:
            if cable.gauge_unit == 'mm':
                awg_fmt = f' ({awg_equiv(cable.gauge)} AWG)'
            elif cable.gauge_unit.upper() == 'AWG':
                awg_fmt = f' ({mm2_equiv(cable.gauge)} mm)'
        rows = [[f'{html_bgcolor(cable.bgcolor_title)}{remove_links(cable.name)}' if cable.show_name else None], [pn_info_string(HEADER_PN, None, remove_links(cable.pn)) if not isinstance(cable.pn, list) else None, html_line_breaks(pn_info_string(HEADER_MPN, cable.manufacturer if not isinstance(cable.manufacturer, list) else None, cable.mpn if not isinstance(cable.mpn, list) else None)), html_line_breaks(pn_info_string(HEADER_SPN, cable.supplier if not isinstance(cable.supplier, list) else None, cable.spn if not isinstance(cable.spn, list) else None))], [html_line_breaks(cable.type), f'{cable.wirecount}x' if cable.show_wirecount else None, f'{cable.gauge} {cable.gauge_unit}{awg_fmt}' if cable.gauge else None, '+ S' if cable.shield else None, f'{cable.length} {cable.length_unit}' if cable.length > 0 else None, translate_color(cable.color, self.options.color_mode) if cable.color else None, html_colorbar(cable.color)], '<!-- wire table -->', [html_image(cable.image)], [html_caption(cable.image)]]
        rows.extend(get_additional_component_table(self, cable))
        rows.append([html_line_breaks(cable.notes)])
        html.extend(nested_html_table(rows, html_bgcolor_attr(cable.bgcolor)))
        wirehtml = []
        wirehtml.append('<table border=""0"" cellspacing=""0"" cellborder=""0"">')
        wirehtml.append('   <tr><td>&nbsp;</td></tr>')
        for (i, (connection_color, wirelabel)) in enumerate(zip_longest(cable.colors, cable.wirelabels), 1):
            wirehtml.append('   <tr>')
            wirehtml.append(f'    <td><!-- {i}_in --></td>')
            wirehtml.append(f'    <td>')
            wireinfo = []
            if cable.show_wirenumbers:
                wireinfo.append(str(i))
            colorstr = wv_colors.translate_color(connection_color, self.options.color_mode)
            if colorstr:
                wireinfo.append(colorstr)
            if cable.wirelabels:
                wireinfo.append(wirelabel if wirelabel is not None else '')
            wirehtml.append(f""     {':'.join(wireinfo)}"")
            wirehtml.append(f'    </td>')
            wirehtml.append(f'    <td><!-- {i}_out --></td>')
            wirehtml.append('   </tr>')
            bgcolors = ['#000000'] + get_color_hex(connection_color, pad=pad) + ['#000000']
            wirehtml.append(f'   <tr>')
            wirehtml.append(f'    <td colspan=""3"" border=""0"" cellspacing=""0"" cellpadding=""0"" port=""w{i}"" height=""{2 * len(bgcolors)}"">')
            wirehtml.append('     <table cellspacing=""0"" cellborder=""0"" border=""0"">')
            for (j, bgcolor) in enumerate(bgcolors[::-1]):
                wirehtml.append(f'''      <tr><td colspan=""3"" cellpadding=""0"" height=""2"" bgcolor=""{(bgcolor if bgcolor != '' else wv_colors.default_color)}"" border=""0""></td></tr>''')
            wirehtml.append('     </table>')
            wirehtml.append('    </td>')
            wirehtml.append('   </tr>')
            if cable.category == 'bundle':
                wireidentification = []
                if isinstance(cable.pn, list):
                    wireidentification.append(pn_info_string(HEADER_PN, None, remove_links(cable.pn[i - 1])))
                manufacturer_info = pn_info_string(HEADER_MPN, cable.manufacturer[i - 1] if isinstance(cable.manufacturer, list) else None, cable.mpn[i - 1] if isinstance(cable.mpn, list) else None)
                supplier_info = pn_info_string(HEADER_SPN, cable.supplier[i - 1] if isinstance(cable.supplier, list) else None, cable.spn[i - 1] if isinstance(cable.spn, list) else None)
                if manufacturer_info:
                    wireidentification.append(html_line_breaks(manufacturer_info))
                if supplier_info:
                    wireidentification.append(html_line_breaks(supplier_info))
                if len(wireidentification) > 0:
                    wirehtml.append('   <tr><td colspan=""3"">')
                    wirehtml.append('    <table border=""0"" cellspacing=""0"" cellborder=""0""><tr>')
                    for attrib in wireidentification:
                        wirehtml.append(f'     <td>{attrib}</td>')
                    wirehtml.append('    </tr></table>')
                    wirehtml.append('   </td></tr>')
        if cable.shield:
            wirehtml.append('   <tr><td>&nbsp;</td></tr>')
            wirehtml.append('   <tr>')
            wirehtml.append('    <td><!-- s_in --></td>')
            wirehtml.append('    <td>Shield</td>')
            wirehtml.append('    <td><!-- s_out --></td>')
            wirehtml.append('   </tr>')
            if isinstance(cable.shield, str):
                shield_color_hex = wv_colors.get_color_hex(cable.shield)[0]
                attributes = f'height=""6"" bgcolor=""{shield_color_hex}"" border=""2"" sides=""tb""'
            else:
                attributes = f'height=""2"" bgcolor=""#000000"" border=""0""'
            wirehtml.append(f'   <tr><td colspan=""3"" cellpadding=""0"" {attributes} port=""ws""></td></tr>')
        wirehtml.append('   <tr><td>&nbsp;</td></tr>')
        wirehtml.append('  </table>')
        html = [row.replace('<!-- wire table -->', '\n'.join(wirehtml)) for row in html]
        for connection in cable.connections:
            if isinstance(connection.via_port, int):
                dot.attr('edge', color=':'.join(['#000000'] + wv_colors.get_color_hex(cable.colors[connection.via_port - 1], pad=pad) + ['#000000']))
            else:
                dot.attr('edge', color=':'.join(['#000000', shield_color_hex, '#000000']) if isinstance(cable.shield, str) else '#000000')
            if connection.from_port is not None:
                from_connector = self.connectors[connection.from_name]
                from_port = f':p{connection.from_port + 1}r' if from_connector.style != 'simple' else ''
                code_left_1 = f'{connection.from_name}{from_port}:e'
                code_left_2 = f'{cable.name}:w{connection.via_port}:w'
                dot.edge(code_left_1, code_left_2)
                if from_connector.show_name:
                    from_info = [str(connection.from_name), str(self.connectors[connection.from_name].pins[connection.from_port])]
                    if from_connector.pinlabels:
                        pinlabel = from_connector.pinlabels[connection.from_port]
                        if pinlabel != '':
                            from_info.append(pinlabel)
                    from_string = ':'.join(from_info)
                else:
                    from_string = ''
                html = [row.replace(f'<!-- {connection.via_port}_in -->', from_string) for row in html]
            if connection.to_port is not None:
                to_connector = self.connectors[connection.to_name]
                code_right_1 = f'{cable.name}:w{connection.via_port}:e'
                to_port = f':p{connection.to_port + 1}l' if self.connectors[connection.to_name].style != 'simple' else ''
                code_right_2 = f'{connection.to_name}{to_port}:w'
                dot.edge(code_right_1, code_right_2)
                if to_connector.show_name:
                    to_info = [str(connection.to_name), str(self.connectors[connection.to_name].pins[connection.to_port])]
                    if to_connector.pinlabels:
                        pinlabel = to_connector.pinlabels[connection.to_port]
                        if pinlabel != '':
                            to_info.append(pinlabel)
                    to_string = ':'.join(to_info)
                else:
                    to_string = ''
                html = [row.replace(f'<!-- {connection.via_port}_out -->', to_string) for row in html]
        (style, bgcolor) = ('filled,dashed', self.options.bgcolor_bundle) if cable.category == 'bundle' else ('filled', self.options.bgcolor_cable)
        html = '\n'.join(html)
        dot.node(cable.name, label=f'<\n{html}\n>', shape='box', style=style, fillcolor=translate_color(bgcolor, 'HEX'))

    def typecheck(name: str, value: Any, expect: type) -> None:
        if not isinstance(value, expect):
            raise Exception(f'Unexpected value type of {name}: Expected {expect}, got {type(value)}\n{value}')
    if self.tweak.override is not None:
        typecheck('tweak.override', self.tweak.override, dict)
        for (k, d) in self.tweak.override.items():
            typecheck(f'tweak.override.{k} key', k, str)
            typecheck(f'tweak.override.{k} value', d, dict)
            for (a, v) in d.items():
                typecheck(f'tweak.override.{k}.{a} key', a, str)
                typecheck(f'tweak.override.{k}.{a} value', v, (str, type(None)))
        for (i, entry) in enumerate(dot.body):
            if isinstance(entry, str):
                match = re.match('^\\t*("")?((?(1)[^""]|[^ ""])+)(?(1)"") \\[.*\\]$', entry, re.S)
                keyword = match and match[2]
                if keyword in self.tweak.override.keys():
                    for (attr, value) in self.tweak.override[keyword].items():
                        if value is None:
                            (entry, n_subs) = re.subn(f'( +)?{attr}=(""[^""]*""|[^] ]*)(?(1)| *)', '', entry)
                            if n_subs < 1:
                                print(f'Harness.create_graph() warning: {attr} not found in {keyword}!')
                            elif n_subs > 1:
                                print(f'Harness.create_graph() warning: {attr} removed {n_subs} times in {keyword}!')
                            continue
                        if len(value) == 0 or ' ' in value:
                            value = value.replace('""', '\\""')
                            value = f'""{value}""'
                        (entry, n_subs) = re.subn(f'{attr}=(""[^""]*""|[^] ]*)', f'{attr}={value}', entry)
                        if n_subs < 1:
                            entry = re.sub('\\]$', f' {attr}={value}]', entry)
                        elif n_subs > 1:
                            print(f'Harness.create_graph() warning: {attr} overridden {n_subs} times in {keyword}!')
                    dot.body[i] = entry
    if self.tweak.append is not None:
        if isinstance(self.tweak.append, list):
            for (i, element) in enumerate(self.tweak.append, 1):
                typecheck(f'tweak.append[{i}]', element, str)
            dot.body.extend(self.tweak.append)
        else:
            typecheck('tweak.append', self.tweak.append, str)
            dot.body.append(self.tweak.append)
    return dot","for loop in connector.loops:
    dot.edge(f'{connector.name}:p{loop[0]}{loop_side}:{loop_dir}', f'{connector.name}:p{loop[1]}{loop_side}:{loop_dir}')","[""for loop in connector.loops:\n    (loop_0, loop_1, *_) = loop\n    dot.edge(f'{connector.name}:p{loop_0}{loop_side}:{loop_dir}', f'{connector.name}:p{loop_1}{loop_side}:{loop_dir}')"", ""for (loop_0, loop_1, *loop_len) in connector.loops:\n    dot.edge(f'{connector.name}:p{loop_0}{loop_side}:{loop_dir}', f'{connector.name}:p{loop_1}{loop_side}:{loop_dir}')""]",no_found,0
deluge,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deluge/deluge/log.py,https://github.com/deluge-torrent/deluge/tree/master/deluge/log.py,_BackwardsCompatibleLOG,__getattribute__$318,"def __getattribute__(self, name):
    import warnings
    logger_name = 'deluge'
    stack = inspect.stack()
    stack.pop(0)
    module_stack = stack.pop(0)
    caller_module = inspect.getmodule(module_stack[0])
    caller_module_name = getattr(caller_module, '__name__', '')
    warnings.warn_explicit(DEPRECATION_WARNING, DeprecationWarning, module_stack[1], module_stack[2], caller_module_name)
    if caller_module:
        for member in stack:
            module = inspect.getmodule(member[0])
            if not module:
                continue
            if module.__name__ in ('deluge.plugins.pluginbase', 'deluge.plugins.init'):
                logger_name += '.plugin.%s' % caller_module_name
                caller_module.log = logging.getLogger(logger_name)
                break
    else:
        logging.getLogger(logger_name).warning(""Unable to monkey-patch the calling module's `log` attribute! You should really update and rebuild your plugins..."")
    return getattr(logging.getLogger(logger_name), name)","for member in stack:
    module = inspect.getmodule(member[0])
    if not module:
        continue
    if module.__name__ in ('deluge.plugins.pluginbase', 'deluge.plugins.init'):
        logger_name += '.plugin.%s' % caller_module_name
        caller_module.log = logging.getLogger(logger_name)
        break","[""for member in stack:\n    (member_0, *member_rmembermaining) = member\n    module = inspect.getmodule(member_0)\n    if not module:\n        continue\n    if module.__name__ in ('deluge.plugins.pluginbase', 'deluge.plugins.init'):\n        logger_name += '.plugin.%s' % caller_module_name\n        caller_module.log = logging.getLogger(logger_name)\n        break"", ""for (member_0, *member_len) in stack:\n    module = inspect.getmodule(member_0)\n    if not module:\n        continue\n    if module.__name__ in ('deluge.plugins.pluginbase', 'deluge.plugins.init'):\n        logger_name += '.plugin.%s' % caller_module_name\n        caller_module.log = logging.getLogger(logger_name)\n        break""]",no_found,0
spot-sdk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spot-sdk/docs/python/fetch_tutorial/files/network_compute_server.py,https://github.com/boston-dynamics/spot-sdk/tree/master/docs/python/fetch_tutorial/files/network_compute_server.py,,main$256,"def main(argv):
    default_port = '50051'
    parser = argparse.ArgumentParser()
    parser.add_argument('-m', '--model', help=""[MODEL_DIR] [LABELS_FILE.pbtxt]: Path to a model's directory and path to its labels .pbtxt file"", action='append', nargs=2, required=True)
    parser.add_argument('-p', '--port', help=""Server's port number, default: "" + default_port, default=default_port)
    parser.add_argument('-d', '--no-debug', help='Disable writing debug images.', action='store_true')
    parser.add_argument('-n', '--name', help='Service name', default='fetch-server')
    bosdyn.client.util.add_base_arguments(parser)
    options = parser.parse_args(argv)
    print(options.model)
    for model in options.model:
        if not os.path.isdir(model[0]):
            print('Error: model directory (' + model[0] + ') not found or is not a directory.')
            sys.exit(1)
    register_with_robot(options)
    request_queue = queue.Queue()
    response_queue = queue.Queue()
    thread = threading.Thread(target=process_thread, args=[options, request_queue, response_queue])
    thread.start()
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    network_compute_bridge_service_pb2_grpc.add_NetworkComputeBridgeWorkerServicer_to_server(NetworkComputeBridgeWorkerServicer(request_queue, response_queue), server)
    server.add_insecure_port('[::]:' + options.port)
    server.start()
    print('Running...')
    thread.join()
    return True","for model in options.model:
    if not os.path.isdir(model[0]):
        print('Error: model directory (' + model[0] + ') not found or is not a directory.')
        sys.exit(1)","[""for model in options.model:\n    (model_0, *model_rmodelmaining) = model\n    if not os.path.isdir(model_0):\n        print('Error: model directory (' + model_0 + ') not found or is not a directory.')\n        sys.exit(1)"", ""for (model_0, *model_len) in options.model:\n    if not os.path.isdir(model_0):\n        print('Error: model directory (' + model_0 + ') not found or is not a directory.')\n        sys.exit(1)""]",no_found,0
dionaea,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dionaea/modules/python/dionaea/sip/extras.py,https://github.com/DinoTools/dionaea/tree/master/modules/python/dionaea/sip/extras.py,PCAP,open$329,"def open(self, msg_stack, **params):
    path = self.path.format(**params)
    today = datetime.datetime.now()
    path = today.strftime(path)
    filename = today.strftime(self.filename)
    filename = filename.format(**params)
    try:
        if not os.path.exists(path):
            os.makedirs(path)
    except:
        logger.info(""Can't create RTP-Dump dir: %s"", path)
    try:
        self._fp = open(os.path.join(path, filename), 'wb')
    except:
        logger.warning(""Can't create RTP-Dump file: %s"", os.path.join(path, filename))
    if self._fp is None:
        return False
    self._fp.write(b'\xd4\xc3\xb2\xa1')
    self._fp.write(b'\x02\x00\x04\x00')
    self._fp.write(b'\x00\x00\x00\x00')
    self._fp.write(b'\x00\x00\x00\x00')
    self._fp.write(b'\xff\xff\x00\x00')
    self._fp.write(b'\x01\x00\x00\x00')
    for msg in msg_stack:
        t = msg[1].time
        ts = int(t)
        tm = int((t - ts) * 1000000)
        src_port = 5060
        dst_port = 5060
        if msg[0] == 'in':
            src_ether = b'\x00\x00\x00\x00\x00\x02'
            src_host = b'\n\x00\x00\x02'
            dst_ether = b'\x00\x00\x00\x00\x00\x01'
            dst_host = b'\n\x00\x00\x01'
        else:
            src_ether = b'\x00\x00\x00\x00\x00\x01'
            src_host = b'\n\x00\x00\x01'
            dst_ether = b'\x00\x00\x00\x00\x00\x02'
            dst_host = b'\n\x00\x00\x02'
        self.write(ts=ts, tm=tm, src_ether=src_ether, src_host=src_host, src_port=src_port, dst_ether=dst_ether, dst_host=dst_host, dst_port=dst_port, data=msg[1].dumps())","for msg in msg_stack:
    t = msg[1].time
    ts = int(t)
    tm = int((t - ts) * 1000000)
    src_port = 5060
    dst_port = 5060
    if msg[0] == 'in':
        src_ether = b'\x00\x00\x00\x00\x00\x02'
        src_host = b'\n\x00\x00\x02'
        dst_ether = b'\x00\x00\x00\x00\x00\x01'
        dst_host = b'\n\x00\x00\x01'
    else:
        src_ether = b'\x00\x00\x00\x00\x00\x01'
        src_host = b'\n\x00\x00\x01'
        dst_ether = b'\x00\x00\x00\x00\x00\x02'
        dst_host = b'\n\x00\x00\x02'
    self.write(ts=ts, tm=tm, src_ether=src_ether, src_host=src_host, src_port=src_port, dst_ether=dst_ether, dst_host=dst_host, dst_port=dst_port, data=msg[1].dumps())","[""for msg in msg_stack:\n    (msg_0, msg_1, *_) = msg\n    t = msg_1.time\n    ts = int(t)\n    tm = int((t - ts) * 1000000)\n    src_port = 5060\n    dst_port = 5060\n    if msg_0 == 'in':\n        src_ether = b'\\x00\\x00\\x00\\x00\\x00\\x02'\n        src_host = b'\\n\\x00\\x00\\x02'\n        dst_ether = b'\\x00\\x00\\x00\\x00\\x00\\x01'\n        dst_host = b'\\n\\x00\\x00\\x01'\n    else:\n        src_ether = b'\\x00\\x00\\x00\\x00\\x00\\x01'\n        src_host = b'\\n\\x00\\x00\\x01'\n        dst_ether = b'\\x00\\x00\\x00\\x00\\x00\\x02'\n        dst_host = b'\\n\\x00\\x00\\x02'\n    self.write(ts=ts, tm=tm, src_ether=src_ether, src_host=src_host, src_port=src_port, dst_ether=dst_ether, dst_host=dst_host, dst_port=dst_port, data=msg_1.dumps())"", ""for (msg_0, msg_1, *msg_len) in msg_stack:\n    t = \n    msg_1.time\n    ts = int(t)\n    tm = int((t - ts) * 1000000)\n    src_port = 5060\n    dst_port = 5060\n    if \n    msg_0 == 'in':\n        src_ether = b'\\x00\\x00\\x00\\x00\\x00\\x02'\n        src_host = b'\\n\\x00\\x00\\x02'\n        dst_ether = b'\\x00\\x00\\x00\\x00\\x00\\x01'\n        dst_host = b'\\n\\x00\\x00\\x01'\n    else:\n        src_ether = b'\\x00\\x00\\x00\\x00\\x00\\x01'\n        src_host = b'\\n\\x00\\x00\\x01'\n        dst_ether = b'\\x00\\x00\\x00\\x00\\x00\\x02'\n        dst_host = b'\\n\\x00\\x00\\x02'\n    self.write(ts=ts, tm=tm, src_ether=src_ether, src_host=src_host, src_port=src_port, dst_ether=dst_ether, dst_host=dst_host, dst_port=dst_port, data=\n    msg_1.dumps())""]",no_found,0
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
    dbg.createLogWindow()
    global currentArgs
    currentArgs = copy.copy(args)
    try:
        starttime = datetime.datetime.now()
        ptr_counter = 0
        commands = {}

        def getBanner():
            banners = {}
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |                         __               __                      |\n'
            bannertext += '    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n'
            bannertext += '    |  / ___/ __ \\/ ___/ _ \\/ / __ `/ __ \\   / __/ _ \\/ __ `/ __ `__ \\ |\n'
            bannertext += '    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n'
            bannertext += '    | \\___/\\____/_/   \\___/_/\\__,_/_/ /_/   \\__/\\___/\\__,_/_/ /_/ /_/  |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |     https://www.corelan.be | https://www.corelan-training.com    |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[0] = bannertext
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n'
            bannertext += ""    |       | '_ ` _ \\  / _ \\ | '_ \\  / _` |   | '_ \\ | | | |          |\n""
            bannertext += '    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n'
            bannertext += '    |       |_| |_| |_| \\___/ |_| |_| \\__,_|(_)| .__/  \\__, |          |\n'
            bannertext += '    |                                          |_|     |___/           |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[1] = bannertext
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |    _____ ___  ____  ____  ____ _                                 |\n'
            bannertext += '    |    / __ `__ \\/ __ \\/ __ \\/ __ `/  https://www.corelan.be         |\n'
            bannertext += '    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n'
            bannertext += '    |  /_/ /_/ /_/\\____/_/ /_/\\__,_/  #corelan (Freenode IRC)          |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[2] = bannertext
            bannertext = ''
            bannertext += '\n    .##.....##..#######..##....##....###........########..##....##\n'
            bannertext += '    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n'
            bannertext += '    .####.####.##.....##.####..##..##...##......##.....##...####..\n'
            bannertext += '    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n'
            bannertext += '    .##.....##.##.....##.##..####.#########.....##...........##...\n'
            bannertext += '    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n'
            bannertext += '    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n'
            banners[3] = bannertext
            bannerlist = []
            for i in range(0, len(banners)):
                bannerlist.append(i)
            random.shuffle(bannerlist)
            return banners[bannerlist[0]]

        def procHelp(args):
            dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__, str(arch)))
            dbg.log('     Plugin version : %s r%s' % (__VERSION__, __REV__))
            dbg.log('     Python version : %s' % getPythonVersion())
            if __DEBUGGERAPP__ == 'WinDBG':
                pykdversion = dbg.getPyKDVersionNr()
                dbg.log('     PyKD version %s' % pykdversion)
            dbg.log('     Written by Corelan - https://www.corelan.be')
            dbg.log('     Project page : https://github.com/corelan/mona')
            dbg.logLines(getBanner(), highlight=1)
            dbg.log('Global options :')
            dbg.log('----------------')
            dbg.log('You can use one or more of the following global options on any command that will perform')
            dbg.log('a search in one or more modules, returning a list of pointers :')
            dbg.log(' -n                     : Skip modules that start with a null byte. If this is too broad, use')
            dbg.log('                          option -cp nonull instead')
            dbg.log(' -o                     : Ignore OS modules')
            dbg.log(' -p <nr>                : Stop search after <nr> pointers.')
            dbg.log(' -m <module,module,...> : only query the given modules. Be sure what you are doing !')
            dbg.log('                          You can specify multiple modules (comma separated)')
            dbg.log('                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored')
            dbg.log('                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,')
            dbg.log('                          blah or *blah* = contains blah')
            dbg.log(' -cm <crit,crit,...>    : Apply some additional criteria to the modules to query.')
            dbg.log('                          You can use one or more of the following criteria :')
            dbg.log('                          aslr,safeseh,rebase,nx,os')
            dbg.log('                          You can enable or disable a certain criterium by setting it to true or false')
            dbg.log('                          Example :  -cm aslr=true,safeseh=false')
            dbg.log('                          Suppose you want to search for p/p/r in aslr enabled modules, you could call')
            dbg.log('                          !mona seh -cm aslr')
            dbg.log(' -cp <crit,crit,...>    : Apply some criteria to the pointers to return')
            dbg.log('                          Available options are :')
            dbg.log('                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev')
            dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
            dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
            dbg.log('                          You can use .. to indicate a range of bytes (in between 2 bad chars)')
            dbg.log(' -x <access>            : Specify desired access level of the returning pointers. If not specified,')
            dbg.log('                          only executable pointers will be returned.')
            dbg.log('                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *')
            if not args:
                args = []
            if len(args) > 1:
                thiscmd = args[1].lower().strip()
                if thiscmd in commands:
                    dbg.log('')
                    dbg.log(""Usage of command '%s' :"" % thiscmd)
                    dbg.log('%s' % ('-' * (22 + len(thiscmd))))
                    dbg.logLines(commands[thiscmd].usage)
                    dbg.log('')
                else:
                    aliasfound = False
                    for cmd in commands:
                        if commands[cmd].alias == thiscmd:
                            dbg.log('')
                            dbg.log(""Usage of command '%s' :"" % thiscmd)
                            dbg.log('%s' % ('-' * (22 + len(thiscmd))))
                            dbg.logLines(commands[cmd].usage)
                            dbg.log('')
                            aliasfound = True
                    if not aliasfound:
                        dbg.logLines('\nCommand %s does not exist. Run !mona to get a list of available commands\n' % thiscmd, highlight=1)
            else:
                dbg.logLines('\nUsage :')
                dbg.logLines('-------\n')
                dbg.log(' !mona <command> <parameter>')
                dbg.logLines('\nAvailable commands and parameters :\n')
                items = commands.items()
                items.sort(key=itemgetter(0))
                for item in items:
                    if commands[item[0]].usage != '':
                        aliastxt = ''
                        if commands[item[0]].alias != '':
                            aliastxt = ' / ' + commands[item[0]].alias
                        dbg.logLines('%s | %s' % (item[0] + aliastxt + ' ' * (20 - len(item[0] + aliastxt)), commands[item[0]].description))
                dbg.log('')
                dbg.log('Want more info about a given command ?  Run !mona help <command>', highlight=1)
                dbg.log('')
        commands['help'] = MnCommand('help', 'show help', '!mona help [command]', procHelp)

        def procConfig(args):
            showerror = False
            if not 'set' in args and (not 'get' in args) and (not 'add' in args):
                showerror = True
            if 'set' in args:
                if type(args['set']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['set'].split(' ')
                    if len(params) < 2:
                        showerror = True
            if 'add' in args:
                if type(args['add']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['add'].split(' ')
                    if len(params) < 2:
                        showerror = True
            if 'get' in args:
                if type(args['get']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['get'].split(' ')
                    if len(params) < 1:
                        showerror = True
            if showerror:
                dbg.log('Usage :')
                dbg.logLines(configUsage, highlight=1)
                return
            else:
                if 'get' in args:
                    dbg.log('Reading value from configuration file')
                    monaConfig = MnConfig()
                    thevalue = monaConfig.get(args['get'])
                    dbg.log('Parameter %s = %s' % (args['get'], thevalue))
                if 'set' in args:
                    dbg.log('Writing value to configuration file')
                    monaConfig = MnConfig()
                    value = args['set'].split(' ')
                    configparam = value[0].strip()
                    dbg.log('Old value of parameter %s = %s' % (configparam, monaConfig.get(configparam)))
                    configvalue = args['set'][0 + len(configparam):len(args['set'])]
                    monaConfig.set(configparam, configvalue)
                    dbg.log('New value of parameter %s = %s' % (configparam, configvalue))
                if 'add' in args:
                    dbg.log('Writing value to configuration file')
                    monaConfig = MnConfig()
                    value = args['add'].split(' ')
                    configparam = value[0].strip()
                    dbg.log('Old value of parameter %s = %s' % (configparam, monaConfig.get(configparam)))
                    configvalue = monaConfig.get(configparam).strip() + ',' + args['add'][0 + len(configparam):len(args['add'])].strip()
                    monaConfig.set(configparam, configvalue)
                    dbg.log('New value of parameter %s = %s' % (configparam, configvalue))

        def procFindJ(args):
            return procFindJMP(args)

        def procFindJMP(args):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            if inspect.stack()[1][3] == 'procFindJ':
                dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."", highlight=1)
            criteria = {}
            all_opcodes = {}
            global ptr_to_get
            ptr_to_get = -1
            distancestr = ''
            mindistance = 0
            maxdistance = 0
            showerror = False
            if 'r' in args:
                if type(args['r']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    thisreg = args['r'].upper().strip()
                    validregs = dbglib.Registers32BitsOrder
                    if not thisreg in validregs:
                        showerror = True
            else:
                showerror = True
            if 'distance' in args:
                if type(args['distance']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    distancestr = args['distance']
                    distanceparts = distancestr.split(',')
                    for parts in distanceparts:
                        valueparts = parts.split('=')
                        if len(valueparts) > 1:
                            if valueparts[0].lower() == 'min':
                                try:
                                    mindistance = int(valueparts[1])
                                except:
                                    mindistance = 0
                            if valueparts[0].lower() == 'max':
                                try:
                                    maxdistance = int(valueparts[1])
                                except:
                                    maxdistance = 0
            if maxdistance < mindistance:
                tmp = maxdistance
                maxdistance = mindistance
                mindistance = tmp
            criteria['mindistance'] = mindistance
            criteria['maxdistance'] = maxdistance
            if showerror:
                dbg.log('Usage :')
                dbg.logLines(jmpUsage, highlight=1)
                return
            else:
                (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
                all_opcodes = findJMP(modulecriteria, criteria, args['r'].lower().strip())
            logfile = MnLog('jmp.txt')
            thislog = logfile.reset()
            processResults(all_opcodes, logfile, thislog)

        def procFindSEH(args):
            modulecriteria = {}
            modulecriteria['safeseh'] = False
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            criteria = {}
            specialcases = {}
            all_opcodes = {}
            global ptr_to_get
            ptr_to_get = -1
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            if 'rop' in args:
                criteria['rop'] = True
            if 'all' in args:
                criteria['all'] = True
                specialcases['maponly'] = True
            else:
                criteria['all'] = False
                specialcases['maponly'] = False
            all_opcodes = findSEH(modulecriteria, criteria)
            logfile = MnLog('seh.txt')
            thislog = logfile.reset()
            processResults(all_opcodes, logfile, thislog, specialcases)

        def procShowMODULES(args):
            modulecriteria = {}
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            modulestosearch = getModulesToQuery(modulecriteria)
            showModuleTable('', modulestosearch)

        def procFindROPFUNC(args):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            ropfuncs = {}
            ropfuncoffsets = {}
            (ropfuncs, ropfuncoffsets) = findROPFUNC(modulecriteria, criteria)
            dbg.log('[+] Processing pointers to interesting rop functions')
            logfile = MnLog('ropfunc.txt')
            thislog = logfile.reset()
            processResults(ropfuncs, logfile, thislog)
            global silent
            silent = True
            dbg.log('[+] Processing offsets to pointers to interesting rop functions')
            logfile = MnLog('ropfunc_offset.txt')
            thislog = logfile.reset()
            processResults(ropfuncoffsets, logfile, thislog)

        def procStackPivots(args):
            procROP(args, 'stackpivot')

        def procROP(args, mode='all'):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            depth = 6
            maxoffset = 40
            thedistance = 8
            split = False
            fast = False
            sortedprint = False
            endingstr = ''
            endings = []
            technique = ''
            if 'depth' in args:
                if type(args['depth']).__name__.lower() != 'bool':
                    try:
                        depth = int(args['depth'])
                    except:
                        pass
            if 'offset' in args:
                if type(args['offset']).__name__.lower() != 'bool':
                    try:
                        maxoffset = int(args['offset'])
                    except:
                        pass
            if 'distance' in args:
                if type(args['distance']).__name__.lower() != 'bool':
                    try:
                        thedistance = args['distance']
                    except:
                        pass
            if 'split' in args:
                if type(args['split']).__name__.lower() == 'bool':
                    split = args['split']
            if 's' in args:
                if type(args['s']).__name__.lower() != 'bool':
                    technique = args['s'].replace(""'"", '').replace('""', '').strip().lower()
            if 'fast' in args:
                if type(args['fast']).__name__.lower() == 'bool':
                    fast = args['fast']
            if 'end' in args:
                if type(args['end']).__name__.lower() == 'str':
                    endingstr = args['end'].replace(""'"", '').replace('""', '').strip()
                    endings = endingstr.split('#')
            if 'f' in args:
                if args['f'] != '':
                    criteria['f'] = args['f']
            if 'sort' in args:
                sortedprint = True
            if 'rva' in args:
                criteria['rva'] = True
            if mode == 'stackpivot':
                fast = False
                endings = ''
                split = False
            else:
                mode = 'all'
            findROPGADGETS(modulecriteria, criteria, endings, maxoffset, depth, split, thedistance, fast, mode, sortedprint, technique)

        def procJseh(args):
            results = []
            showred = 0
            showall = False
            if 'all' in args:
                showall = True
            nrfound = 0
            dbg.log('-----------------------------------------------------------------------')
            dbg.log('Search for jmp/call dword[ebp/esp+nn] (and other) combinations started ')
            dbg.log('-----------------------------------------------------------------------')
            opcodej = ['T$\x08', 'd$\x08', 'T$\x14', 'T$\x14', 'T$\x1c', 'T$\x1c', 'T$,', 'T$,', 'T$D', 'T$D', 'T$P', 'T$P', 'U\x0c', 'e\x0c', 'U$', 'e$', 'U0', 'e0', 'U', 'e', 'U', 'e', 'U', 'e', '\x83\x08', '\x83\x08']
            fakeptrcriteria = {}
            fakeptrcriteria['accesslevel'] = '*'
            for opjc in opcodej:
                addys = []
                addys = searchInRange([[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
                results += addys
                for ptrtypes in addys:
                    for ad1 in addys[ptrtypes]:
                        ptr = MnPointer(ad1)
                        module = ptr.belongsTo()
                        if not module:
                            module = ''
                            page = dbg.getMemoryPageByAddress(ad1)
                            access = page.getAccess(human=True)
                            op = dbg.disasm(ad1)
                            opstring = op.getDisasm()
                            dbg.log('Found %s at 0x%08x - Access: (%s) - Outside of a loaded module' % (opstring, ad1, access), address=ad1, highlight=1)
                            nrfound += 1
                        elif showall:
                            page = dbg.getMemoryPageByAddress(ad1)
                            access = page.getAccess(human=True)
                            op = dbg.disasm(ad1)
                            opstring = op.getDisasm()
                            thismod = MnModule(module)
                            if not thismod.isSafeSEH:
                                extratext = '=== Safeseh : NO ==='
                                showred = 1
                            else:
                                extratext = 'Safeseh protected'
                                showred = 0
                            dbg.log('Found %s at 0x%08x (%s) - Access: (%s) - %s' % (opstring, ad1, module, access, extratext), address=ad1, highlight=showred)
                            nrfound += 1
            dbg.log('Search complete')
            if results:
                dbg.log('Found %d address(es)' % nrfound)
                return 'Found %d address(es) (Check the log Windows for details)' % nrfound
            else:
                dbg.log('No addresses found')
                return 'Sorry, no addresses found'

        def procJOP(args, mode='all'):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            depth = 6
            if 'depth' in args:
                if type(args['depth']).__name__.lower() != 'bool':
                    try:
                        depth = int(args['depth'])
                    except:
                        pass
            findJOPGADGETS(modulecriteria, criteria, depth)

        def procCreatePATTERN(args):
            size = 0
            pattern = ''
            if '?' in args and args['?'] != '':
                try:
                    if '0x' in args['?'].lower():
                        try:
                            size = int(args['?'], 16)
                        except:
                            size = 0
                    else:
                        size = int(args['?'])
                except:
                    size = 0
            if size == 0:
                dbg.log('Please enter a valid size', highlight=1)
            else:
                pattern = createPattern(size, args)
                dbg.log('Creating cyclic pattern of %d bytes' % size)
                dbg.log(pattern)
                global ignoremodules
                ignoremodules = True
                objpatternfile = MnLog('pattern.txt')
                patternfile = objpatternfile.reset()
                objpatternfile.write('\nPattern of ' + str(size) + ' bytes :\n', patternfile)
                objpatternfile.write('-' * (19 + len(str(size))), patternfile)
                objpatternfile.write('\nASCII:', patternfile)
                objpatternfile.write('\n' + pattern, patternfile)
                patternhex = ''
                for patternchar in pattern:
                    patternhex += str(hex(ord(patternchar))).replace('0x', '\\x')
                objpatternfile.write('\n\nHEX:\n', patternfile)
                objpatternfile.write(patternhex, patternfile)
                patternjs = str2js(pattern)
                objpatternfile.write('\n\nJAVASCRIPT (unescape() friendly):\n', patternfile)
                objpatternfile.write(patternjs, patternfile)
                if not silent:
                    dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"", highlight=1)
                    dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile, highlight=1)
                ignoremodules = False
            return

        def procOffsetPATTERN(args):
            egg = ''
            if '?' in args and args['?'] != '':
                try:
                    egg = args['?']
                except:
                    egg = ''
            if egg == '':
                dbg.log('Please enter a valid target', highlight=1)
            else:
                findOffsetInPattern(egg, -1, args)
            return

        def procFileCOMPARE(args):
            modulecriteria = {}
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            allfiles = []
            tomatch = ''
            checkstrict = True
            rangeval = 0
            fast = False
            if 'ptronly' in args or 'ptrsonly' in args:
                fast = True
            if 'f' in args:
                if args['f'] != '':
                    rawfilenames = args['f'].replace('""', '')
                    allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
                    dbg.log('[+] Number of files to be examined : %d ' % len(allfiles))
            if 'range' in args:
                if not type(args['range']).__name__.lower() == 'bool':
                    strrange = args['range'].lower()
                    if strrange.startswith('0x') and len(strrange) > 2:
                        rangeval = int(strrange, 16)
                    else:
                        try:
                            rangeval = int(args['range'])
                        except:
                            rangeval = 0
                    if rangeval > 0:
                        dbg.log('[+] Find overlap using pointer +/- range, value %d' % rangeval)
                        dbg.log('    Note : this will significantly slow down the comparison process !')
                else:
                    dbg.log('Please provide a numeric value ^(> 0) with option -range', highlight=1)
                    return
            else:
                if 'contains' in args:
                    if type(args['contains']).__name__.lower() == 'str':
                        tomatch = args['contains'].replace(""'"", '').replace('""', '')
                if 'nostrict' in args:
                    if type(args['nostrict']).__name__.lower() == 'bool':
                        checkstrict = not args['nostrict']
                        dbg.log('[+] Instructions must match in all files ? %s' % checkstrict)
            callfiles = allfiles
            allfiles = []
            for tfile in callfiles:
                if os.path.isdir(tfile):
                    for (root, dirs, files) in os.walk(tfile):
                        for dfile in files:
                            allfiles.append(os.path.join(root, dfile))
                else:
                    allfiles.append(tfile)
            if len(allfiles) > 1:
                findFILECOMPARISON(modulecriteria, criteria, allfiles, tomatch, checkstrict, rangeval, fast)
            else:
                dbg.log('Please specify at least 2 filenames to compare', highlight=1)

        def procFind(args):
            modulecriteria = {}
            criteria = {}
            pattern = ''
            base = 0
            offset = 0
            top = TOP_USERLAND
            consecutive = False
            ftype = ''
            level = 0
            offsetlevel = 0
            if not 'a' in args:
                args['a'] = '*'
            ptronly = False
            if 'ptronly' in args or 'ptrsonly' in args:
                ptronly = True
            if not 'x' in args:
                args['x'] = '*'
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            if criteria['accesslevel'] == '':
                return
            if not 's' in args:
                dbg.log('-s <search pattern (or filename)> is a mandatory argument', highlight=1)
                return
            pattern = args['s']
            if 'unicode' in args:
                criteria['unic'] = True
            if 'b' in args:
                try:
                    base = int(args['b'], 16)
                except:
                    dbg.log('invalid base address: %s' % args['b'], highlight=1)
                    return
            if 't' in args:
                try:
                    top = int(args['t'], 16)
                except:
                    dbg.log('invalid top address: %s' % args['t'], highlight=1)
                    return
            if 'offset' in args:
                if not args['offset'].__class__.__name__ == 'bool':
                    if '0x' in args['offset'].lower():
                        try:
                            offset = 0 - int(args['offset'], 16)
                        except:
                            dbg.log('invalid offset value', highlight=1)
                            return
                    else:
                        try:
                            offset = 0 - int(args['offset'])
                        except:
                            dbg.log('invalid offset value', highlight=1)
                            return
                else:
                    dbg.log('invalid offset value', highlight=1)
                    return
            if 'level' in args:
                try:
                    level = int(args['level'])
                except:
                    dbg.log('invalid level value', highlight=1)
                    return
            if 'offsetlevel' in args:
                try:
                    offsetlevel = int(args['offsetlevel'])
                except:
                    dbg.log('invalid offsetlevel value', highlight=1)
                    return
            if 'c' in args:
                dbg.log('    - Skipping consecutive pointers, showing size instead')
                consecutive = True
            if 'type' in args:
                if not args['type'] in ['bin', 'asc', 'ptr', 'instr', 'file']:
                    dbg.log('Invalid search type : %s' % args['type'], highlight=1)
                    return
                ftype = args['type']
                if ftype == 'file':
                    filename = args['s'].replace('""', '').replace(""'"", '')
                    if not os.path.isfile(filename):
                        dbg.log('Unable to find/read file %s' % filename, highlight=1)
                        return
            rangep2p = 0
            if 'p2p' in args or level > 0:
                dbg.log('    - Looking for pointers to pointers')
                criteria['p2p'] = True
                if 'r' in args:
                    try:
                        rangep2p = int(args['r'])
                    except:
                        pass
                    if rangep2p > 0:
                        dbg.log('    - Will search for close pointers (%d bytes backwards)' % rangep2p)
                if 'p2p' in args:
                    level = 1
            if level > 0:
                dbg.log('    - Recursive levels : %d' % level)
            allpointers = findPattern(modulecriteria, criteria, pattern, ftype, base, top, consecutive, rangep2p, level, offset, offsetlevel)
            logfile = MnLog('find.txt')
            thislog = logfile.reset()
            processResults(allpointers, logfile, thislog, {}, ptronly)
            return

        def procFindWild(args):
            modulecriteria = {}
            criteria = {}
            pattern = ''
            patterntype = ''
            base = 0
            top = TOP_USERLAND
            (modulecriteria, criteria) = args2criteria(","for new in registers_to_fill[::2]:
    n = new[0]
    registers['e%sx' % n] = calculateNewXregister(registers['e%sx' % n], new_values_dict['%sh' % n], new_values_dict['%sl' % n])","[""for new in registers_to_fill[::2]:\n    (new_0, *new_rnewmaining) = new\n    n = new_0\n    registers['e%sx' % n] = calculateNewXregister(registers['e%sx' % n], new_values_dict['%sh' % n], new_values_dict['%sl' % n])"", ""for (new_0, *new_len) in registers_to_fill[::2]:\n    n = \n    new_0\n    registers['e%sx' % n] = calculateNewXregister(registers['e%sx' % n], new_values_dict['%sh' % n], new_values_dict['%sl' % n])""]",no_found,0
capirca,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capirca/capirca/lib/cisco.py,https://github.com/google/capirca/tree/master/capirca/lib/cisco.py,TermStandard,__str__$106,"def __str__(self):
    if self.term.platform:
        if self.platform not in self.term.platform:
            return ''
    if self.term.platform_exclude:
        if self.platform in self.term.platform_exclude:
            return ''
    ret_str = []
    if self.term.verbatim:
        for next_verbatim in self.term.verbatim:
            if next_verbatim[0] == self.platform:
                ret_str.append(str(next_verbatim[1]))
            return '\n'.join(ret_str)
    v4_addresses = [x for x in self.term.address if not isinstance(x, nacaddr.IPv6)]
    if self.filter_name.isdigit():
        if self.verbose:
            ret_str.append('access-list %s remark %s' % (self.filter_name, self.term.name))
            comments = aclgenerator.WrapWords(self.term.comment, _COMMENT_MAX_WIDTH)
            for comment in comments:
                ret_str.append('access-list %s remark %s' % (self.filter_name, comment))
        action = _ACTION_TABLE.get(str(self.term.action[0]))
        if v4_addresses:
            for addr in v4_addresses:
                if addr.prefixlen == 32:
                    ret_str.append('access-list %s %s %s%s%s' % (self.filter_name, action, addr.network_address, self.logstring, self.dscpstring))
                else:
                    ret_str.append('access-list %s %s %s %s%s%s' % (self.filter_name, action, addr.network_address, addr.hostmask, self.logstring, self.dscpstring))
        else:
            ret_str.append('access-list %s %s %s%s%s' % (self.filter_name, action, 'any', self.logstring, self.dscpstring))
    else:
        if self.verbose:
            ret_str.append(' remark ' + self.term.name)
            comments = aclgenerator.WrapWords(self.term.comment, _COMMENT_MAX_WIDTH)
            if comments and comments[0]:
                for comment in comments:
                    ret_str.append(' remark ' + str(comment))
        action = _ACTION_TABLE.get(str(self.term.action[0]))
        if v4_addresses:
            for addr in v4_addresses:
                if addr.prefixlen == 32:
                    ret_str.append(' %s host %s%s%s' % (action, addr.network_address, self.logstring, self.dscpstring))
                elif self.platform == 'arista':
                    ret_str.append(' %s %s/%s%s%s' % (action, addr.network_address, addr.prefixlen, self.logstring, self.dscpstring))
                else:
                    ret_str.append(' %s %s %s%s%s' % (action, addr.network_address, addr.hostmask, self.logstring, self.dscpstring))
        else:
            ret_str.append(' %s %s%s%s' % (action, 'any', self.logstring, self.dscpstring))
    return '\n'.join(ret_str)","for next_verbatim in self.term.verbatim:
    if next_verbatim[0] == self.platform:
        ret_str.append(str(next_verbatim[1]))
    return '\n'.join(ret_str)","[""for next_verbatim in self.term.verbatim:\n    (next_verbatim_0, next_verbatim_1, *_) = next_verbatim\n    if next_verbatim_0 == self.platform:\n        ret_str.append(str(next_verbatim_1))\n    return '\\n'.join(ret_str)"", ""for (next_verbatim_0, next_verbatim_1, *next_verbatim_len) in self.term.verbatim:\n    if \n    next_verbatim_0 == self.platform:\n        ret_str.append(str(\n        next_verbatim_1))\n    return '\\n'.join(ret_str)""]",no_found,0
microk8s,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/microk8s/scripts/wrappers/status.py,https://github.com/ubuntu/microk8s/tree/master/scripts/wrappers/status.py,,print_pretty$33,"def print_pretty(isReady, enabled_addons, disabled_addons):
    console_formatter = '{:>3} {:<20} # ({}) {}'
    if isReady:
        print('microk8s is running')
        if not is_ha_enabled():
            print('high-availability: no')
        else:
            info = get_dqlite_info()
            if ha_cluster_formed(info):
                print('high-availability: yes')
            else:
                print('high-availability: no')
            masters = 'none'
            standby = 'none'
            for node in info:
                if node[1] == 'voter':
                    if masters == 'none':
                        masters = '{}'.format(node[0])
                    else:
                        masters = '{} {}'.format(masters, node[0])
                if node[1] == 'standby':
                    if standby == 'none':
                        standby = '{}'.format(node[0])
                    else:
                        standby = '{} {}'.format(standby, node[0])
            print('{:>2}{} {}'.format('', 'datastore master nodes:', masters))
            print('{:>2}{} {}'.format('', 'datastore standby nodes:', standby))
        print('addons:')
        if enabled_addons and len(enabled_addons) > 0:
            print('{:>2}{}'.format('', 'enabled:'))
            for enabled in enabled_addons:
                print(console_formatter.format('', enabled['name'], enabled['repository'], enabled['description']))
        if disabled_addons and len(disabled_addons) > 0:
            print('{:>2}{}'.format('', 'disabled:'))
            for disabled in disabled_addons:
                print(console_formatter.format('', disabled['name'], disabled['repository'], disabled['description']))
    else:
        print('microk8s is not running. Use microk8s inspect for a deeper inspection.')","for node in info:
    if node[1] == 'voter':
        if masters == 'none':
            masters = '{}'.format(node[0])
        else:
            masters = '{} {}'.format(masters, node[0])
    if node[1] == 'standby':
        if standby == 'none':
            standby = '{}'.format(node[0])
        else:
            standby = '{} {}'.format(standby, node[0])","[""for node in info:\n    (node_0, node_1, *_) = node\n    if node_1 == 'voter':\n        if masters == 'none':\n            masters = '{}'.format(node_0)\n        else:\n            masters = '{} {}'.format(masters, node_0)\n    if node_1 == 'standby':\n        if standby == 'none':\n            standby = '{}'.format(node_0)\n        else:\n            standby = '{} {}'.format(standby, node_0)"", ""for (node_0, node_1, *node_len) in info:\n    if \n    node_1 == 'voter':\n        if masters == 'none':\n            masters = '{}'.format(\n            node_0)\n        else:\n            masters = '{} {}'.format(masters, \n            node_0)\n    if \n    node_1 == 'standby':\n        if standby == 'none':\n            standby = '{}'.format(\n            node_0)\n        else:\n            standby = '{} {}'.format(standby, \n            node_0)""]",no_found,0
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):

    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(set((i.key_number for i in midi_obj.key_signature_changes)))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start) for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(max_pos)]
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j)) for (i, j) in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)
    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for (idx, inst) in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program, note.pitch + 128 if inst.is_drum else note.pitch, enc_dur(max(1, time_to_pos(note.end - note.start))), enc_vel(note.velocity), info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) * math.log2(x / tot) for x in start_distribution))
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(start_ppl)
    encoding.sort()
    (encoding, is_major) = normalize_to_c_major(encoding)
    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]
    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry = []
    chord_int = 2
    for (chord_idx, chord) in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(boundry) >= 2, f'segement must start and end in chords: {target_chords}'
    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i] if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch + encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch + encoding[i][3] % 12, *encoding[i][4:])
    lead_notes = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    lead_chords = infer_chords_for_sequence(lead_notes, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)
    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        (src_strs, tgt_strs) = ([], [])
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for (note_idx, note) in enumerate(notes):
                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                elif dec_dur(note[4]) >= pos_resolution:
                    pitch_type = note[3] % 12
                    if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
                        src_words.append('AUT')
                    else:
                        src_words.append('HALF')
                else:
                    src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return (src_strs, tgt_strs)
    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue
        if cur_len + last_len >= target_len:
            if cur_len + last_len <= max_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    (src_strs, tgt_strs) = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment
    if max_notes >= last_len >= min_notes:
        (src_strs, tgt_strs) = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs
    assert len(src_str_list) == len(tgt_str_list)
    return (src_str_list, tgt_str_list, get_hash(encoding))","for note in encoding:
    max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
    if 0 <= note[3] < 128:
        lead_notes.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))","[""for note in encoding:\n    (_, note_1, _, note_3, note_4, note_5, *_) = note\n    max_pos = max(max_pos, bar_to_pos[note[0]] + note_1 + dec_dur(note_4))\n    if 0 <= note_3 < 128:\n        lead_notes.append(Item(name='On', start=bar_to_pos[note[0]] + note_1, end=bar_to_pos[note[0]] + note_1 + dec_dur(note_4), vel=dec_vel(note_5), pitch=note_3, track=0))"", ""for (note_0, note_1, note_2, note_3, note_4, note_5, *note_len) in encoding:\n    max_pos = max(max_pos, bar_to_pos[\n    note_0] + \n    note_1 + dec_dur(\n    note_4))\n    if 0 <= \n    note_3 < 128:\n        lead_notes.append(Item(name='On', start=bar_to_pos[\n        note_0] + \n        note_1, end=bar_to_pos[\n        note_0] + \n        note_1 + dec_dur(\n        note_4), vel=dec_vel(\n        note_5), pitch=\n        note_3, track=0))""]",no_found,0
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):

    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(set((i.key_number for i in midi_obj.key_signature_changes)))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start) for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(max_pos)]
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j)) for (i, j) in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)
    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for (idx, inst) in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program, note.pitch + 128 if inst.is_drum else note.pitch, enc_dur(max(1, time_to_pos(note.end - note.start))), enc_vel(note.velocity), info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) * math.log2(x / tot) for x in start_distribution))
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(start_ppl)
    encoding.sort()
    (encoding, is_major) = normalize_to_c_major(encoding)
    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]
    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry = []
    chord_int = 2
    for (chord_idx, chord) in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(boundry) >= 2, f'segement must start and end in chords: {target_chords}'
    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i] if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch + encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch + encoding[i][3] % 12, *encoding[i][4:])
    lead_notes = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    lead_chords = infer_chords_for_sequence(lead_notes, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)
    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        (src_strs, tgt_strs) = ([], [])
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for (note_idx, note) in enumerate(notes):
                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                elif dec_dur(note[4]) >= pos_resolution:
                    pitch_type = note[3] % 12
                    if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
                        src_words.append('AUT')
                    else:
                        src_words.append('HALF')
                else:
                    src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return (src_strs, tgt_strs)
    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue
        if cur_len + last_len >= target_len:
            if cur_len + last_len <= max_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    (src_strs, tgt_strs) = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment
    if max_notes >= last_len >= min_notes:
        (src_strs, tgt_strs) = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs
    assert len(src_str_list) == len(tgt_str_list)
    return (src_str_list, tgt_str_list, get_hash(encoding))","for note in encoding:
    max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
    if 0 <= note[3] < 128:
        note_items.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))","[""for note in encoding:\n    (_, note_1, _, note_3, note_4, note_5, *_) = note\n    max_pos = max(max_pos, bar_to_pos[note[0]] + note_1 + dec_dur(note_4))\n    if 0 <= note_3 < 128:\n        note_items.append(Item(name='On', start=bar_to_pos[note[0]] + note_1, end=bar_to_pos[note[0]] + note_1 + dec_dur(note_4), vel=dec_vel(note_5), pitch=note_3, track=0))"", ""for (note_0, note_1, note_2, note_3, note_4, note_5, *note_len) in encoding:\n    max_pos = max(max_pos, bar_to_pos[\n    note_0] + \n    note_1 + dec_dur(\n    note_4))\n    if 0 <= \n    note_3 < 128:\n        note_items.append(Item(name='On', start=bar_to_pos[\n        note_0] + \n        note_1, end=bar_to_pos[\n        note_0] + \n        note_1 + dec_dur(\n        note_4), vel=dec_vel(\n        note_5), pitch=\n        note_3, track=0))""]",no_found,0
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):

    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(set((i.key_number for i in midi_obj.key_signature_changes)))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start) for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(max_pos)]
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j)) for (i, j) in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)
    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for (idx, inst) in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program, note.pitch + 128 if inst.is_drum else note.pitch, enc_dur(max(1, time_to_pos(note.end - note.start))), enc_vel(note.velocity), info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) * math.log2(x / tot) for x in start_distribution))
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(start_ppl)
    encoding.sort()
    (encoding, is_major) = normalize_to_c_major(encoding)
    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]
    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry = []
    chord_int = 2
    for (chord_idx, chord) in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(boundry) >= 2, f'segement must start and end in chords: {target_chords}'
    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i] if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch + encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch + encoding[i][3] % 12, *encoding[i][4:])
    lead_notes = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    lead_chords = infer_chords_for_sequence(lead_notes, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)
    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        (src_strs, tgt_strs) = ([], [])
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for (note_idx, note) in enumerate(notes):
                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                elif dec_dur(note[4]) >= pos_resolution:
                    pitch_type = note[3] % 12
                    if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
                        src_words.append('AUT')
                    else:
                        src_words.append('HALF')
                else:
                    src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return (src_strs, tgt_strs)
    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue
        if cur_len + last_len >= target_len:
            if cur_len + last_len <= max_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    (src_strs, tgt_strs) = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment
    if max_notes >= last_len >= min_notes:
        (src_strs, tgt_strs) = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs
    assert len(src_str_list) == len(tgt_str_list)
    return (src_str_list, tgt_str_list, get_hash(encoding))","for notes in notes_list:
    if len(notes) < min_notes or len(notes) > max_notes:
        continue
    src_words = []
    if is_major:
        src_words.append('MAJ')
    else:
        src_words.append('MIN')
    tgt_words = []
    first_note = notes[0]
    min_bar = first_note[0]
    for (note_idx, note) in enumerate(notes):
        cur_pos = bar_to_pos[note[0]] + note[1]
        chord_idx = 2 * note[0]
        if note[1] >= 2 * pos_resolution:
            chord_idx += 1
        cur_chord = lead_chords[chord_idx]
        src_words.append(f'Chord_{cur_chord}')
        if note_idx != len(notes) - 1:
            nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
            if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                pitch_type = note[3] % 12
                if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                    src_words.append('AUT')
                else:
                    src_words.append('HALF')
            else:
                src_words.append('NOT')
        elif dec_dur(note[4]) >= pos_resolution:
            pitch_type = note[3] % 12
            if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
                src_words.append('AUT')
            else:
                src_words.append('HALF')
        else:
            src_words.append('NOT')
        beat_idx = note[1] // pos_resolution
        beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
        src_words.append(f'BEAT_{beat_idx}')
        tgt_words.append(f'Bar_{note[0] - min_bar}')
        tgt_words.append(f'Pos_{note[1]}')
        tgt_words.append(f'Pitch_{note[3]}')
        tgt_words.append(f'Dur_{note[4]}')
    src_strs.append(' '.join(src_words))
    tgt_strs.append(' '.join(tgt_words))","[""for notes in notes_list:\n    (notes_0, *notes_rnotesmaining) = notes\n    if len(notes) < min_notes or len(notes) > max_notes:\n        continue\n    src_words = []\n    if is_major:\n        src_words.append('MAJ')\n    else:\n        src_words.append('MIN')\n    tgt_words = []\n    first_note = notes_0\n    min_bar = first_note[0]\n    for (note_idx, note) in enumerate(notes):\n        cur_pos = bar_to_pos[note[0]] + note[1]\n        chord_idx = 2 * note[0]\n        if note[1] >= 2 * pos_resolution:\n            chord_idx += 1\n        cur_chord = lead_chords[chord_idx]\n        src_words.append(f'Chord_{cur_chord}')\n        if note_idx != len(notes) - 1:\n            nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]\n            if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:\n                pitch_type = note[3] % 12\n                if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):\n                    src_words.append('AUT')\n                else:\n                    src_words.append('HALF')\n            else:\n                src_words.append('NOT')\n        elif dec_dur(note[4]) >= pos_resolution:\n            pitch_type = note[3] % 12\n            if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:\n                src_words.append('AUT')\n            else:\n                src_words.append('HALF')\n        else:\n            src_words.append('NOT')\n        beat_idx = note[1] // pos_resolution\n        beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)\n        src_words.append(f'BEAT_{beat_idx}')\n        tgt_words.append(f'Bar_{note[0] - min_bar}')\n        tgt_words.append(f'Pos_{note[1]}')\n        tgt_words.append(f'Pitch_{note[3]}')\n        tgt_words.append(f'Dur_{note[4]}')\n    src_strs.append(' '.join(src_words))\n    tgt_strs.append(' '.join(tgt_words))""]",no_found,0
PythonRobotics,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonRobotics/ArmNavigation/arm_obstacle_navigation/arm_obstacle_navigation_2.py,https://github.com/AtsushiSakai/PythonRobotics/tree/master/ArmNavigation/arm_obstacle_navigation/arm_obstacle_navigation_2.py,NLinkArm,plot_arm$278,"def plot_arm(self, myplt, obstacles=[]):
    myplt.cla()
    for obstacle in obstacles:
        circle = myplt.Circle((obstacle[0], obstacle[1]), radius=0.5 * obstacle[2], fc='k')
        myplt.gca().add_patch(circle)
    for i in range(self.n_links + 1):
        if i is not self.n_links:
            myplt.plot([self.points[i][0], self.points[i + 1][0]], [self.points[i][1], self.points[i + 1][1]], 'r-')
        myplt.plot(self.points[i][0], self.points[i][1], 'k.')
    myplt.xlim([-self.lim, self.lim])
    myplt.ylim([-self.lim, self.lim])
    myplt.draw()","for obstacle in obstacles:
    circle = myplt.Circle((obstacle[0], obstacle[1]), radius=0.5 * obstacle[2], fc='k')
    myplt.gca().add_patch(circle)","[""for obstacle in obstacles:\n    (obstacle_0, obstacle_1, obstacle_2, *_) = obstacle\n    circle = myplt.Circle((obstacle_0, obstacle_1), radius=0.5 * obstacle_2, fc='k')\n    myplt.gca().add_patch(circle)"", ""for (obstacle_0, obstacle_1, obstacle_2, *obstacle_len) in obstacles:\n    circle = myplt.Circle((obstacle_0, obstacle_1), radius=0.5 * obstacle_2, fc='k')\n    myplt.gca().add_patch(circle)""]",no_found,0
abseil-py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/abseil-py/absl/testing/tests/parameterized_test.py,https://github.com/abseil/abseil-py/tree/master/absl/testing/tests/parameterized_test.py,,decorator$51,"def decorator(test_method):
    if isinstance(test_method, abc.Iterable):
        actual_tests = []
        for old_test in test_method.testcases:
            new_dict = old_test[1].copy()
            new_dict[key] = value
            test_suffix = '%s_%s_%s' % (old_test[0], key, value)
            actual_tests.append((test_suffix, new_dict))
        test_method.testcases = actual_tests
        return test_method
    else:
        test_suffix = '_%s_%s' % (key, value)
        tests_to_make = ((test_suffix, {key: value}),)
        return parameterized.named_parameters(*tests_to_make)(test_method)","for old_test in test_method.testcases:
    new_dict = old_test[1].copy()
    new_dict[key] = value
    test_suffix = '%s_%s_%s' % (old_test[0], key, value)
    actual_tests.append((test_suffix, new_dict))","[""for old_test in test_method.testcases:\n    (old_test_0, old_test_1, *_) = old_test\n    new_dict = old_test_1.copy()\n    new_dict[key] = value\n    test_suffix = '%s_%s_%s' % (old_test_0, key, value)\n    actual_tests.append((test_suffix, new_dict))"", ""for (old_test_0, old_test_1, *old_test_len) in test_method.testcases:\n    new_dict = \n    old_test_1.copy()\n    new_dict[key] = value\n    test_suffix = '%s_%s_%s' % (\n    old_test_0, key, value)\n    actual_tests.append((test_suffix, new_dict))""]",no_found,0
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/tests/python/magic_uv_test/common.py,https://github.com/nutti/Magic-UV/tree/master/tests/python/magic_uv_test/common.py,TestBase,tearDownClass$264,"def tearDownClass(cls):
    try:
        check_addon_disabled(cls.package_name)
        for op in cls.idname:
            if op[0] == 'OPERATOR':
                assert not operator_exists(op[1]), 'Operator {} exists'.format(op[1])
            elif op[0] == 'MENU':
                assert not menu_exists(op[1]), 'Menu %s exists'.format(op[1])
    except AssertionError as e:
        print(e)
        sys.exit(1)","for op in cls.idname:
    if op[0] == 'OPERATOR':
        assert not operator_exists(op[1]), 'Operator {} exists'.format(op[1])
    elif op[0] == 'MENU':
        assert not menu_exists(op[1]), 'Menu %s exists'.format(op[1])","[""for op in cls.idname:\n    (op_0, op_1, *_) = op\n    if op_0 == 'OPERATOR':\n        assert not operator_exists(op_1), 'Operator {} exists'.format(op_1)\n    elif op_0 == 'MENU':\n        assert not menu_exists(op_1), 'Menu %s exists'.format(op_1)"", ""for (op_0, op_1, *op_len) in cls.idname:\n    if \n    op_0 == 'OPERATOR':\n        assert not operator_exists(\n        op_1), 'Operator {} exists'.format(\n        op_1)\n    elif \n    op_0 == 'MENU':\n        assert not menu_exists(\n        op_1), 'Menu %s exists'.format(\n        op_1)""]",no_found,0
kivy-designer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy-designer/designer/components/playground.py,https://github.com/kivy/kivy-designer/tree/master/designer/components/playground.py,Playground,get_playground_drag_element$847,"def get_playground_drag_element(self, instance, widget_name, touch, default_args, extra_args, *args):
    """"""This function will return the desired playground element
           for widget_name.
           :param extra_args: extra args used to display the dragging widget
           :param default_args: default widget args
           :param touch: instance of the current touch
           :param instance: if from toolbox, ToolboxButton instance.
                    None otherwise
           :param widget_name: name of the widget that will be dragged
        """"""
    widget = self.get_widget(widget_name, **default_args)
    widget._KD_KV_STR = self.generate_kv_from_args(widget_name, default_args)
    values = default_args.copy()
    values.update(extra_args)
    child = self.get_widget(widget_name, **values)
    custom = False
    for op in widgets_common:
        if op[0] == widget_name:
            if op[1] == 'custom':
                custom = True
            break
    container = PlaygroundDragElement(playground=self, child=child, widget=widget)
    if not custom:
        container.fit_child()
    touch.grab(container)
    touch_pos = [touch.x, touch.y]
    if instance:
        touch_pos = instance.to_window(*touch.pos)
    container.center_x = touch_pos[0]
    container.y = touch_pos[1] + 20
    return container","for op in widgets_common:
    if op[0] == widget_name:
        if op[1] == 'custom':
            custom = True
        break","[""for op in widgets_common:\n    (op_0, op_1, *_) = op\n    if op_0 == widget_name:\n        if op_1 == 'custom':\n            custom = True\n        break"", ""for (op_0, op_1, *op_len) in widgets_common:\n    if \n    op_0 == widget_name:\n        if \n        op_1 == 'custom':\n            custom = True\n        break""]",no_found,0
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/visualization/data_providers/genome.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/visualization/data_providers/genome.py,BamDataProvider,convert_cigar$1053,"def convert_cigar(read, start_field, cigar_field, seq_field):
    """"""
            Convert read cigar from pysam format to string format.
            """"""
    cigar_ops = 'MIDNSHP=X'
    read_cigar = ''
    for op_tuple in read[cigar_field]:
        read_cigar += '%i%s' % (op_tuple[1], cigar_ops[op_tuple[0]])
    read[cigar_field] = read_cigar","for op_tuple in read[cigar_field]:
    read_cigar += '%i%s' % (op_tuple[1], cigar_ops[op_tuple[0]])","[""for op_tuple in read[cigar_field]:\n    (op_tuple_0, op_tuple_1, *op_tuple_rop_tuplemaining) = op_tuple\n    read_cigar += '%i%s' % (op_tuple_1, cigar_ops[op_tuple[0]])"", ""for (op_tuple_0, op_tuple_1, *op_tuple_len) in read[cigar_field]:\n    read_cigar += '%i%s' % (op_tuple_1, cigar_ops[op_tuple_0])""]",no_found,0
Scout2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Scout2/AWSScout2/output/console.py,https://github.com/nccgroup/Scout2/tree/master/AWSScout2/output/console.py,,format_listall_output$16,"def format_listall_output(format_file, format_item_dir, format, rule, option_prefix=None, template=None, skip_options=False):
    """"""
    Prepare listall output template

    :param format_file:
    :param format_item_dir:
    :param format:
    :param config:
    :param option_prefix:
    :param template:
    :param skip_options:
    :return:
    """"""
    if format_file and os.path.isfile(format_file):
        if not template:
            with open(format_file, 'rt') as f:
                template = f.read()
        if not skip_options:
            re_option = re.compile('(%_OPTION_\\((.*?)\\)_NOITPO_)')
            optional_files = re_option.findall(template)
            for optional_file in optional_files:
                if optional_file[1].startswith(option_prefix + '-'):
                    with open(os.path.join(format_item_dir, optional_file[1].strip()), 'rt') as f:
                        template = template.replace(optional_file[0].strip(), f.read())
        re_file = re.compile('(_FILE_\\((.*?)\\)_ELIF_)')
        while True:
            requested_files = re_file.findall(template)
            available_files = os.listdir(format_item_dir) if format_item_dir else []
            for requested_file in requested_files:
                if requested_file[1].strip() in available_files:
                    with open(os.path.join(format_item_dir, requested_file[1].strip()), 'rt') as f:
                        template = template.replace(requested_file[0].strip(), f.read())
            re_line = re.compile('(_ITEM_\\((.*?)\\)_METI_)')
            re_key = re.compile('_KEY_\\(*(.*?)\\)', re.DOTALL | re.MULTILINE)
            lines = re_line.findall(template)
            for (i, line) in enumerate(lines):
                lines[i] = line + (re_key.findall(line[1]),)
            requested_files = re_file.findall(template)
            if len(requested_files) == 0:
                break
    elif format and format[0] == 'csv':
        keys = rule.keys
        line = ', '.join(('_KEY_(%s)' % k for k in keys))
        lines = [(line, line, keys)]
        template = line
    return (lines, template)","for optional_file in optional_files:
    if optional_file[1].startswith(option_prefix + '-'):
        with open(os.path.join(format_item_dir, optional_file[1].strip()), 'rt') as f:
            template = template.replace(optional_file[0].strip(), f.read())","[""for optional_file in optional_files:\n    (optional_file_0, optional_file_1, *_) = optional_file\n    if optional_file_1.startswith(option_prefix + '-'):\n        with open(os.path.join(format_item_dir, optional_file_1.strip()), 'rt') as f:\n            template = template.replace(optional_file_0.strip(), f.read())"", ""for (optional_file_0, optional_file_1, *optional_file_len) in optional_files:\n    if \n    optional_file_1.startswith(option_prefix + '-'):\n        with open(os.path.join(format_item_dir, \n        optional_file_1.strip()), 'rt') as f:\n            template = template.replace(\n            optional_file_0.strip(), f.read())""]",no_found,0
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/flow/utils/aimsun/generate.py,https://github.com/flow-project/flow/tree/master/flow/utils/aimsun/generate.py,,generate_net$36,"def generate_net(nodes, edges, connections, inflows, veh_types, traffic_lights):
    """"""Generate a network in the Aimsun template.

    Parameters
    ----------
    nodes : list of dict
        all available nodes
    edges : list of dict
        all available edges
    connections : list of dict
        all available connections
    inflows : flow.core.params.InFlows
        the flow inflow object
    veh_types : list of dict
        list of vehicle types and their corresponding properties
    traffic_lights : flow.core.params.TrafficLightParams
        traffic light specific parameters
    """"""
    inflows = inflows.get()
    lane_width = 3.6
    type_section = model.getType('GKSection')
    type_node = model.getType('GKNode')
    type_turn = model.getType('GKTurning')
    type_traffic_state = model.getType('GKTrafficState')
    type_vehicle = model.getType('GKVehicle')
    type_demand = model.getType('GKTrafficDemand')
    for edge in edges:
        points = GKPoints()
        if 'shape' in edge:
            for p in edge['shape']:
                new_point = GKPoint()
                new_point.set(p[0], p[1], 0)
                points.append(new_point)
            cmd = model.createNewCmd(model.getType('GKSection'))
            cmd.setPoints(edge['numLanes'], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge['id'])
            edge_aimsun = model.getCatalog().findByName(edge['id'], type_section)
            edge_aimsun.setSpeed(edge['speed'] * 3.6)
        else:
            (first_node, last_node) = get_edge_nodes(edge, nodes)
            theta = get_edge_angle(first_node, last_node)
            first_node_offset = [0, 0]
            last_node_offset = [0, 0]
            if 'radius' in first_node:
                first_node_offset[0] = first_node['radius'] * np.cos(theta * np.pi / 180)
                first_node_offset[1] = first_node['radius'] * np.sin(theta * np.pi / 180)
            if 'radius' in last_node:
                last_node_offset[0] = -last_node['radius'] * np.cos(theta * np.pi / 180)
                last_node_offset[1] = -last_node['radius'] * np.sin(theta * np.pi / 180)
            edges_shared_node = [edg for edg in edges if first_node['id'] == edg['to'] or last_node['id'] == edg['from']]
            for new_edge in edges_shared_node:
                (new_first_node, new_last_node) = get_edge_nodes(new_edge, nodes)
                new_theta = get_edge_angle(new_first_node, new_last_node)
                if new_theta == theta - 180 or new_theta == theta + 180:
                    first_node_offset[0] += lane_width * 0.5 * np.sin(theta * np.pi / 180)
                    first_node_offset[1] -= lane_width * 0.5 * np.cos(theta * np.pi / 180)
                    last_node_offset[0] += lane_width * 0.5 * np.sin(theta * np.pi / 180)
                    last_node_offset[1] -= lane_width * 0.5 * np.cos(theta * np.pi / 180)
                    break
            new_point = GKPoint()
            new_point.set(first_node['x'] + first_node_offset[0], first_node['y'] + first_node_offset[1], 0)
            points.append(new_point)
            new_point = GKPoint()
            new_point.set(last_node['x'] + last_node_offset[0], last_node['y'] + last_node_offset[1], 0)
            points.append(new_point)
            cmd = model.createNewCmd(type_section)
            cmd.setPoints(edge['numLanes'], lane_width, points)
            model.getCommander().addCommand(cmd)
            section = cmd.createdObject()
            section.setName(edge['id'])
            edge_aimsun = model.getCatalog().findByName(edge['id'], type_section)
            edge_aimsun.setSpeed(edge['speed'] * 3.6)
    for node in nodes:
        node_pos = GKPoint()
        node_pos.set(node['x'], node['y'], 0)
        cmd = model.createNewCmd(type_node)
        cmd.setPosition(node_pos)
        model.getCommander().addCommand(cmd)
        new_node = cmd.createdObject()
        new_node.setName(node['id'])
        from_edges = [edge['id'] for edge in edges if edge['from'] == node['id']]
        to_edges = [edge['id'] for edge in edges if edge['to'] == node['id']]
        if len(to_edges) > 1 and len(from_edges) > 1 and (connections[node['id']] is not None):
            for connection in connections[node['id']]:
                cmd = model.createNewCmd(type_turn)
                from_section = model.getCatalog().findByName(connection['from'], type_section, True)
                to_section = model.getCatalog().findByName(connection['to'], type_section, True)
                cmd.setTurning(from_section, to_section)
                model.getCommander().addCommand(cmd)
                turn = cmd.createdObject()
                turn_name = '{}_to_{}'.format(connection['from'], connection['to'])
                turn.setName(turn_name)
                existing_node = turn.getNode()
                if existing_node is not None:
                    existing_node.removeTurning(turn)
                new_node.addTurning(turn, False, True)
        else:
            for i in range(len(from_edges)):
                for j in range(len(to_edges)):
                    cmd = model.createNewCmd(type_turn)
                    to_section = model.getCatalog().findByName(from_edges[i], type_section, True)
                    from_section = model.getCatalog().findByName(to_edges[j], type_section, True)
                    cmd.setTurning(from_section, to_section)
                    model.getCommander().addCommand(cmd)
                    turn = cmd.createdObject()
                    turn_name = '{}_to_{}'.format(from_edges[i], to_edges[j])
                    turn.setName(turn_name)
                    existing_node = turn.getNode()
                    if existing_node is not None:
                        existing_node.removeTurning(turn)
                    new_node.addTurning(turn, False, True)
    control_plan = model.getCatalog().findByName('Control Plan', model.getType('GKControlPlan'))
    tls_properties = traffic_lights.get_properties()
    junctions = get_junctions(nodes)
    for node in junctions:
        phases = tls_properties[node['id']]['phases']
        print(phases)
        create_node_meters(model, control_plan, node['id'], phases)
    vehicles = model.getCatalog().getObjectsByType(type_vehicle)
    if vehicles is not None:
        for vehicle in vehicles.itervalues():
            name = vehicle.getName()
            if name == 'Car':
                for veh_type in veh_types:
                    cmd = GKObjectDuplicateCmd()
                    cmd.init(vehicle)
                    model.getCommander().addCommand(cmd)
                    new_veh = cmd.createdObject()
                    new_veh.setName(veh_type['veh_id'])
    for veh_type in veh_types:
        new_state = create_state(model, veh_type['veh_id'])
        veh_type = model.getCatalog().findByName(veh_type['veh_id'], model.getType('GKVehicle'))
        new_state.setVehicle(veh_type)
    for inflow in inflows:
        traffic_state_aimsun = model.getCatalog().findByName(inflow['vtype'], type_traffic_state)
        edge_aimsun = model.getCatalog().findByName(inflow['edge'], type_section)
        traffic_state_aimsun.setEntranceFlow(edge_aimsun, None, inflow['vehsPerHour'])
    demand = model.getCatalog().findByName('Traffic Demand 864', type_demand)
    demand.removeSchedule()
    for veh_type in veh_types:
        state_car = model.getCatalog().findByName(veh_type['veh_id'], type_traffic_state)
        if demand is not None and demand.isA('GKTrafficDemand'):
            if state_car is not None and state_car.isA('GKTrafficState'):
                set_demand_item(model, demand, state_car)
            model.getCommander().addCommand(None)
        else:
            create_traffic_demand(model, veh_type['veh_id'])
    view = gui.getActiveViewWindow().getView()
    if view is not None:
        view.wholeWorld()
    set_vehicles_color(model)
    network_name = data['network_name']
    scenario = model.getCatalog().findByName(network_name, model.getType('GKScenario'))
    scenario_data = scenario.getInputData()
    scenario_data.addExtension(os.path.join(config.PROJECT_PATH, 'flow/utils/aimsun/run.py'), True)
    gui.save(model, 'flow.ang', GGui.GGuiSaveType.eSaveAs)","for p in edge['shape']:
    new_point = GKPoint()
    new_point.set(p[0], p[1], 0)
    points.append(new_point)","[""for p in edge['shape']:\n    (p_0, p_1, *_) = p\n    new_point = GKPoint()\n    new_point.set(p_0, p_1, 0)\n    points.append(new_point)"", ""for (p_0, p_1, *p_len) in edge['shape']:\n    new_point = GKPoint()\n    new_point.set(p_0, p_1, 0)\n    points.append(new_point)""]",no_found,0
sktime,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sktime/sktime/benchmarking/evaluation.py,https://github.com/alan-turing-institute/sktime/tree/master/sktime/benchmarking/evaluation.py,Evaluator,wilcoxon_test$322,"def wilcoxon_test(self, metric_name=None):
    """"""Wilcoxon signed-rank test.

        http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test
        `Wilcoxon signed-rank test
        <https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test>`_.
        Tests whether two  related paired samples come from the same
        distribution. In particular, it tests whether the distribution of the
        differences x-y is symmetric about zero
        """"""
    self._check_is_evaluated()
    metric_name = self._validate_metric_name(metric_name)
    metrics_per_estimator_dataset = self._get_metrics_per_estimator_dataset(metric_name)
    wilcoxon_df = pd.DataFrame()
    prod = itertools.combinations(metrics_per_estimator_dataset.keys(), 2)
    for p in prod:
        estim_1 = p[0]
        estim_2 = p[1]
        (w, p_val) = stats.wilcoxon(metrics_per_estimator_dataset[p[0]], metrics_per_estimator_dataset[p[1]])
        w_test = {'estimator_1': estim_1, 'estimator_2': estim_2, 'statistic': w, 'p_val': p_val}
        wilcoxon_df = wilcoxon_df.append(w_test, ignore_index=True)
    return wilcoxon_df","for p in prod:
    estim_1 = p[0]
    estim_2 = p[1]
    (w, p_val) = stats.wilcoxon(metrics_per_estimator_dataset[p[0]], metrics_per_estimator_dataset[p[1]])
    w_test = {'estimator_1': estim_1, 'estimator_2': estim_2, 'statistic': w, 'p_val': p_val}
    wilcoxon_df = wilcoxon_df.append(w_test, ignore_index=True)","[""for p in prod:\n    (p_0, p_1, *_) = p\n    estim_1 = p_0\n    estim_2 = p_1\n    (w, p_val) = stats.wilcoxon(metrics_per_estimator_dataset[p_0], metrics_per_estimator_dataset[p_1])\n    w_test = {'estimator_1': estim_1, 'estimator_2': estim_2, 'statistic': w, 'p_val': p_val}\n    wilcoxon_df = wilcoxon_df.append(w_test, ignore_index=True)"", ""for (p_0, p_1, *p_len) in prod:\n    estim_1 = \n    p_0\n    estim_2 = \n    p_1\n    (w, p_val) = stats.wilcoxon(metrics_per_estimator_dataset[\n    p_0], metrics_per_estimator_dataset[\n    p_1])\n    w_test = {'estimator_1': estim_1, 'estimator_2': estim_2, 'statistic': w, 'p_val': p_val}\n    wilcoxon_df = wilcoxon_df.append(w_test, ignore_index=True)""]",no_found,0
OpenFermion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenFermion/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,,taper_off_qubits$404,"def taper_off_qubits(operator, stabilizers, manual_input=False, fixed_positions=None, output_tapered_positions=False):
    """"""
    Remove qubits from given operator.

    Qubits are removed by eliminating an equivalent number of
    stabilizer conditions. Which qubits that are can either be determined
    automatically or their positions can be set manually.

    Qubits can be disregarded from the Hamiltonian when the effect of all its
    terms on them is rendered trivial. This algorithm employs a stabilizers
    like $\\pm X \\otimes p$ to fix the action of every Pauli
    string on the first qubit to $Z$ or the identity. A string
    $X \\otimes h$ would for instance be multiplied with the stabilizer
    to obtain $1 \\otimes (\\pm h\\cdot p)$ while a string
    $Z \\otimes h^\\prime$ would pass without correction. The first
    qubit can subsequently be removed as it must be in the computational basis
    in Hamiltonian eigenstates.
    For stabilizers acting as $Y$ ($Z$) on selected qubits,
    the algorithm would fix the action of every Hamiltonian string to
    $Z$ ($X$). Updating also the list of remaining stabilizer
    generators, the algorithm is run iteratively.

    Args:
        operator (QubitOperator): Operator of which qubits will be removed.
        stabilizers (QubitOperator): Stabilizer generators for the tapering.
                                     Can also be passed as a list of
                                     QubitOperator.
        manual_input (Boolean): Option to pass the list of fixed qubits
                                positions manually. Set to False by default.
        fixed_positions (list): (optional) List of fixed qubit positions.
                                Passing a list is only effective if
                                manual_input is True.
        output_tapered_positions (Boolean): Option to output the positions of
                                            qubits that have been removed.
    Returns:
        skimmed_operator (QubitOperator): Operator with fewer qubits.
        removed_positions (list): (optional) List of removed qubit positions.
                                  For the qubits to be gone in the qubit count,
                                  the remaining qubits have been moved up to
                                  those indices.
    """"""
    if isinstance(stabilizers, (list, tuple, numpy.ndarray)):
        n_qbits_stabs = 0
        for ent in stabilizers:
            if op_utils.count_qubits(ent) > n_qbits_stabs:
                n_qbits_stabs = op_utils.count_qubits(ent)
    else:
        n_qbits_stabs = op_utils.count_qubits(stabilizers)
    n_qbits = max(op_utils.count_qubits(operator), n_qbits_stabs)
    (ham_to_update, qbts_to_rm) = reduce_number_of_terms(operator, stabilizers, maintain_length=False, manual_input=manual_input, fixed_positions=fixed_positions, output_fixed_positions=True)
    qbit_order = list(numpy.arange(n_qbits - len(qbts_to_rm), dtype=int))
    removed_positions = qbts_to_rm
    qbts_to_rm.sort()
    for x in qbts_to_rm:
        qbit_order.insert(x, 'remove')
    skimmed_operator = QubitOperator()
    for (term, coef) in ham_to_update.terms.items():
        if term == ():
            skimmed_operator += QubitOperator('', coef)
            continue
        tap_tpls = []
        for p in term:
            if qbit_order[p[0]] != 'remove':
                tap_tpls.append((qbit_order[p[0]].item(), p[1]))
        skimmed_operator += QubitOperator(tuple(tap_tpls), coef)
    if output_tapered_positions:
        return (skimmed_operator, removed_positions)
    else:
        return skimmed_operator","for p in term:
    if qbit_order[p[0]] != 'remove':
        tap_tpls.append((qbit_order[p[0]].item(), p[1]))","[""for p in term:\n    (p_0, p_1, *p_rpmaining) = p\n    if qbit_order[p[0]] != 'remove':\n        tap_tpls.append((qbit_order[p[0]].item(), p_1))"", ""for (p_0, p_1, *p_len) in term:\n    if qbit_order[p_0] != 'remove':\n        tap_tpls.append((qbit_order[p_0].item(), p_1))""]",no_found,0
CANalyzat0r,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CANalyzat0r/src/ManagerTab.py,https://github.com/schutzwerk/CANalyzat0r/tree/master/src/ManagerTab.py,ManagerTab,saveToFile$1058,"def saveToFile(self):
    """"""
        Save the packets in the GUI table to a file in SocketCAN format.
        """"""
    socketCANPackets = []
    packetsToSave = self.rawData
    if len(packetsToSave) == 0:
        return
    for packet in packetsToSave:
        socketCANPacket = SocketCANPacket(packet[3], Globals.CANData.ifaceName if Globals.CANData is not None else 'can0', packet[0], packet[1])
        socketCANPackets.append(socketCANPacket)
        self.logger.debug(Strings.snifferTabElementSocketCANConvertOK)
    filePath = Toolbox.Toolbox.getSaveFileName(Strings.saveDialogTitle)
    if filePath:
        CANData.writeCANFile(filePath, socketCANPackets)
        self.logger.info(Strings.dataWritten + ' ' + str(len(socketCANPackets)))
    else:
        self.logger.info(Strings.dataNotWritten)","for packet in packetsToSave:
    socketCANPacket = SocketCANPacket(packet[3], Globals.CANData.ifaceName if Globals.CANData is not None else 'can0', packet[0], packet[1])
    socketCANPackets.append(socketCANPacket)
    self.logger.debug(Strings.snifferTabElementSocketCANConvertOK)","[""for packet in packetsToSave:\n    (packet_0, packet_1, _, packet_3, *packet_rpacketmaining) = packet\n    socketCANPacket = SocketCANPacket(packet_3, Globals.CANData.ifaceName if Globals.CANData is not None else 'can0', packet_0, packet_1)\n    socketCANPackets.append(socketCANPacket)\n    self.logger.debug(Strings.snifferTabElementSocketCANConvertOK)"", ""for (packet_0, packet_1, packet_2, packet_3, *packet_len) in packetsToSave:\n    socketCANPacket = SocketCANPacket(packet_3, Globals.CANData.ifaceName if Globals.CANData is not None else 'can0', packet_0, packet_1)\n    socketCANPackets.append(socketCANPacket)\n    self.logger.debug(Strings.snifferTabElementSocketCANConvertOK)""]",no_found,0
rope,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rope/rope/refactor/change_signature.py,https://github.com/python-rope/rope/tree/master/rope/refactor/change_signature.py,ArgumentAdder,change_definition_info$255,"def change_definition_info(self, definition_info):
    for pair in definition_info.args_with_defaults:
        if pair[0] == self.name:
            raise rope.base.exceptions.RefactoringError('Adding duplicate parameter: <%s>.' % self.name)
    definition_info.args_with_defaults.insert(self.index, (self.name, self.default))","for pair in definition_info.args_with_defaults:
    if pair[0] == self.name:
        raise rope.base.exceptions.RefactoringError('Adding duplicate parameter: <%s>.' % self.name)","[""for pair in definition_info.args_with_defaults:\n    (pair_0, *pair_rpairmaining) = pair\n    if pair_0 == self.name:\n        raise rope.base.exceptions.RefactoringError('Adding duplicate parameter: <%s>.' % self.name)"", ""for (pair_0, *pair_len) in definition_info.args_with_defaults:\n    if \n    pair_0 == self.name:\n        raise rope.base.exceptions.RefactoringError('Adding duplicate parameter: <%s>.' % self.name)""]",no_found,0
mycroft-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mycroft-core/mycroft/tts/mimic2_tts.py,https://github.com/MycroftAI/mycroft-core/tree/master/mycroft/tts/mimic2_tts.py,Mimic2,viseme$189,"def viseme(self, phonemes):
    """"""Maps phonemes to appropriate viseme encoding

        Args:
            phonemes (list): list of tuples (phoneme, time_start)

        Returns:
            list: list of tuples (viseme_encoding, time_start)
        """"""
    visemes = []
    for pair in phonemes:
        if pair[0]:
            phone = pair[0].lower()
        else:
            phone = 'z'
        vis = VISIMES.get(phone)
        vis_dur = float(pair[1])
        visemes.append((vis, vis_dur))
    return visemes","for pair in phonemes:
    if pair[0]:
        phone = pair[0].lower()
    else:
        phone = 'z'
    vis = VISIMES.get(phone)
    vis_dur = float(pair[1])
    visemes.append((vis, vis_dur))","[""for pair in phonemes:\n    (pair_0, pair_1, *_) = pair\n    if pair_0:\n        phone = pair_0.lower()\n    else:\n        phone = 'z'\n    vis = VISIMES.get(phone)\n    vis_dur = float(pair_1)\n    visemes.append((vis, vis_dur))"", ""for (pair_0, pair_1, *pair_len) in phonemes:\n    if \n    pair_0:\n        phone = \n        pair_0.lower()\n    else:\n        phone = 'z'\n    vis = VISIMES.get(phone)\n    vis_dur = float(\n    pair_1)\n    visemes.append((vis, vis_dur))""]",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/FuzzParam.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/FuzzParam.py,FuzzerParam,result_method_1$355,"def result_method_1(self, str):
    result_key_list = []
    result_value_list = []
    regxs_1 = 'method\\:.*?\\,url\\:.*?\\,data\\:({.*?})'
    if re.findall(regxs_1, str, re.S):
        result_json = re.findall(regxs_1, str, re.S)[0].replace(' ', '').replace('\n', '').replace('{', '').replace('\t', '')
        regx_key = '(.*?)\\:.*?\\,|(.*?)\\:.*?\\}'
        result_keys = re.findall(regx_key, result_json, re.S)
        for result_key in result_keys:
            if result_key[0] != '':
                result_key_list.append(result_key[0])
            if result_key[1] != '':
                result_key_list.append(result_key[1])
        regx_value = '\\:(.*?)\\,|\\:(.*?)\\}'
        result_values = re.findall(regx_value, result_json, re.S)
        for para in result_values:
            if para[0] != '':
                result_value_list.append(para[0])
            if para[1] != '':
                result_value_list.append(para[1])
    result_list = [result_key_list, result_value_list]
    return result_list","for para in result_values:
    if para[0] != '':
        result_value_list.append(para[0])
    if para[1] != '':
        result_value_list.append(para[1])","[""for para in result_values:\n    (para_0, para_1, *_) = para\n    if para_0 != '':\n        result_value_list.append(para_0)\n    if para_1 != '':\n        result_value_list.append(para_1)"", ""for (para_0, para_1, *para_len) in result_values:\n    if \n    para_0 != '':\n        result_value_list.append(\n        para_0)\n    if \n    para_1 != '':\n        result_value_list.append(\n        para_1)""]",no_found,0
unrpyc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unrpyc/decompiler/util.py,https://github.com/CensoredUsername/unrpyc/tree/master/decompiler/util.py,,reconstruct_paraminfo$177,"def reconstruct_paraminfo(paraminfo):
    if paraminfo is None:
        return ''
    rv = ['(']
    sep = First('', ', ')
    positional = [i for i in paraminfo.parameters if i[0] in paraminfo.positional]
    nameonly = [i for i in paraminfo.parameters if i not in positional]
    for parameter in positional:
        rv.append(sep())
        rv.append(parameter[0])
        if parameter[1] is not None:
            rv.append('=%s' % parameter[1])
    if paraminfo.extrapos:
        rv.append(sep())
        rv.append('*%s' % paraminfo.extrapos)
    if nameonly:
        if not paraminfo.extrapos:
            rv.append(sep())
            rv.append('*')
        for parameter in nameonly:
            rv.append(sep())
            rv.append(parameter[0])
            if parameter[1] is not None:
                rv.append('=%s' % parameter[1])
    if paraminfo.extrakw:
        rv.append(sep())
        rv.append('**%s' % paraminfo.extrakw)
    rv.append(')')
    return ''.join(rv)","for parameter in nameonly:
    rv.append(sep())
    rv.append(parameter[0])
    if parameter[1] is not None:
        rv.append('=%s' % parameter[1])","[""for parameter in nameonly:\n    (parameter_0, parameter_1, *_) = parameter\n    rv.append(sep())\n    rv.append(parameter_0)\n    if parameter_1 is not None:\n        rv.append('=%s' % parameter_1)"", ""for (parameter_0, parameter_1, *parameter_len) in nameonly:\n    rv.append(sep())\n    rv.append(\n    parameter_0)\n    if \n    parameter_1 is not None:\n        rv.append('=%s' % \n        parameter_1)""]",no_found,0
unrpyc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unrpyc/decompiler/util.py,https://github.com/CensoredUsername/unrpyc/tree/master/decompiler/util.py,,reconstruct_paraminfo$177,"def reconstruct_paraminfo(paraminfo):
    if paraminfo is None:
        return ''
    rv = ['(']
    sep = First('', ', ')
    positional = [i for i in paraminfo.parameters if i[0] in paraminfo.positional]
    nameonly = [i for i in paraminfo.parameters if i not in positional]
    for parameter in positional:
        rv.append(sep())
        rv.append(parameter[0])
        if parameter[1] is not None:
            rv.append('=%s' % parameter[1])
    if paraminfo.extrapos:
        rv.append(sep())
        rv.append('*%s' % paraminfo.extrapos)
    if nameonly:
        if not paraminfo.extrapos:
            rv.append(sep())
            rv.append('*')
        for parameter in nameonly:
            rv.append(sep())
            rv.append(parameter[0])
            if parameter[1] is not None:
                rv.append('=%s' % parameter[1])
    if paraminfo.extrakw:
        rv.append(sep())
        rv.append('**%s' % paraminfo.extrakw)
    rv.append(')')
    return ''.join(rv)","for parameter in positional:
    rv.append(sep())
    rv.append(parameter[0])
    if parameter[1] is not None:
        rv.append('=%s' % parameter[1])","[""for parameter in positional:\n    (parameter_0, parameter_1, *_) = parameter\n    rv.append(sep())\n    rv.append(parameter_0)\n    if parameter_1 is not None:\n        rv.append('=%s' % parameter_1)"", ""for (parameter_0, parameter_1, *parameter_len) in positional:\n    rv.append(sep())\n    rv.append(\n    parameter_0)\n    if \n    parameter_1 is not None:\n        rv.append('=%s' % \n        parameter_1)""]",no_found,0
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/azurearm_network.py,https://github.com/saltstack/salt/tree/master/salt/modules/azurearm_network.py,,security_rule_create_or_update$263,"def security_rule_create_or_update(name, access, direction, priority, protocol, security_group, resource_group, source_address_prefix=None, destination_address_prefix=None, source_port_range=None, destination_port_range=None, source_address_prefixes=None, destination_address_prefixes=None, source_port_ranges=None, destination_port_ranges=None, **kwargs):
    """"""
    .. versionadded:: 2019.2.0

    Create or update a security rule within a specified network security group.

    :param name: The name of the security rule to create.

    :param access:
        'allow' or 'deny'

    :param direction:
        'inbound' or 'outbound'

    :param priority:
        Integer between 100 and 4096 used for ordering rule application.

    :param protocol:
        'tcp', 'udp', or '*'

    :param destination_address_prefix:
        The CIDR or destination IP range. Asterix '*' can also be used to match all destination IPs.
        Default tags such as 'VirtualNetwork', 'AzureLoadBalancer' and 'Internet' can also be used.
        If this is an ingress rule, specifies where network traffic originates from.

    :param destination_port_range:
        The destination port or range. Integer or range between 0 and 65535. Asterix '*'
        can also be used to match all ports.

    :param source_address_prefix:
        The CIDR or source IP range. Asterix '*' can also be used to match all source IPs.
        Default tags such as 'VirtualNetwork', 'AzureLoadBalancer' and 'Internet' can also be used.
        If this is an ingress rule, specifies where network traffic originates from.

    :param source_port_range:
        The source port or range. Integer or range between 0 and 65535. Asterix '*'
        can also be used to match all ports.

    :param destination_address_prefixes:
        A list of destination_address_prefix values. This parameter overrides destination_address_prefix
        and will cause any value entered there to be ignored.

    :param destination_port_ranges:
        A list of destination_port_range values. This parameter overrides destination_port_range
        and will cause any value entered there to be ignored.

    :param source_address_prefixes:
        A list of source_address_prefix values. This parameter overrides source_address_prefix
        and will cause any value entered there to be ignored.

    :param source_port_ranges:
        A list of source_port_range values. This parameter overrides source_port_range
        and will cause any value entered there to be ignored.

    :param security_group: The network security group containing the
        security rule.

    :param resource_group: The resource group name assigned to the
        network security group.

    CLI Example:

    .. code-block:: bash

        salt-call azurearm_network.security_rule_create_or_update testrule1 allow outbound 101 tcp testnsg testgroup                   source_address_prefix='*' destination_address_prefix=internet source_port_range='*'                   destination_port_range='1-1024'

    """"""
    exclusive_params = [('source_port_ranges', 'source_port_range'), ('source_address_prefixes', 'source_address_prefix'), ('destination_port_ranges', 'destination_port_range'), ('destination_address_prefixes', 'destination_address_prefix')]
    for params in exclusive_params:
        if not eval(params[0]) and (not eval(params[1])):
            log.error('Either the %s or %s parameter must be provided!', params[0], params[1])
            return False
        if eval(params[0]):
            exec('{} = None'.format(params[1]))
    netconn = __utils__['azurearm.get_client']('network', **kwargs)
    try:
        rulemodel = __utils__['azurearm.create_object_model']('network', 'SecurityRule', name=name, access=access, direction=direction, priority=priority, protocol=protocol, source_port_ranges=source_port_ranges, source_port_range=source_port_range, source_address_prefixes=source_address_prefixes, source_address_prefix=source_address_prefix, destination_port_ranges=destination_port_ranges, destination_port_range=destination_port_range, destination_address_prefixes=destination_address_prefixes, destination_address_prefix=destination_address_prefix, **kwargs)
    except TypeError as exc:
        result = {'error': 'The object model could not be built. ({})'.format(str(exc))}
        return result
    try:
        secrule = netconn.security_rules.create_or_update(resource_group_name=resource_group, network_security_group_name=security_group, security_rule_name=name, security_rule_parameters=rulemodel)
        secrule.wait()
        secrule_result = secrule.result()
        result = secrule_result.as_dict()
    except CloudError as exc:
        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)
        result = {'error': str(exc)}
    except SerializationError as exc:
        result = {'error': 'The object model could not be parsed. ({})'.format(str(exc))}
    return result","for params in exclusive_params:
    if not eval(params[0]) and (not eval(params[1])):
        log.error('Either the %s or %s parameter must be provided!', params[0], params[1])
        return False
    if eval(params[0]):
        exec('{} = None'.format(params[1]))","[""for params in exclusive_params:\n    (params_0, params_1, *_) = params\n    if not eval(params_0) and (not eval(params_1)):\n        log.error('Either the %s or %s parameter must be provided!', params_0, params_1)\n        return False\n    if eval(params_0):\n        exec('{} = None'.format(params_1))"", ""for (params_0, params_1, *params_len) in exclusive_params:\n    if not eval(params_0) and (not eval(params_1)):\n        log.error('Either the %s or %s parameter must be provided!', params_0, params_1)\n        return False\n    if eval(params_0):\n        exec('{} = None'.format(params_1))""]",no_found,0
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/test/python/circuit/library/test_evolution_gate.py,https://github.com/Qiskit/qiskit-terra/tree/master/test/python/circuit/library/test_evolution_gate.py,TestEvolutionGate,test_qdrift_manual$135,"def test_qdrift_manual(self, op, time, reps, sampled_ops):
    """"""Test the evolution circuit of Suzuki Trotter against a manually constructed circuit.""""""
    qdrift = QDrift(reps=reps)
    evo_gate = PauliEvolutionGate(op, time, synthesis=qdrift)
    evo_gate.definition.decompose()
    expected = QuantumCircuit(1)
    for pauli in sampled_ops:
        if pauli[0].to_label() == 'X':
            expected.rx(2 * pauli[1], 0)
        elif pauli[0].to_label() == 'Y':
            expected.ry(2 * pauli[1], 0)
    self.assertTrue(Operator(evo_gate.definition).equiv(expected))","for pauli in sampled_ops:
    if pauli[0].to_label() == 'X':
        expected.rx(2 * pauli[1], 0)
    elif pauli[0].to_label() == 'Y':
        expected.ry(2 * pauli[1], 0)","[""for pauli in sampled_ops:\n    (pauli_0, pauli_1, *_) = pauli\n    if pauli_0.to_label() == 'X':\n        expected.rx(2 * pauli_1, 0)\n    elif pauli_0.to_label() == 'Y':\n        expected.ry(2 * pauli_1, 0)"", ""for (pauli_0, pauli_1, *pauli_len) in sampled_ops:\n    if \n    pauli_0.to_label() == 'X':\n        expected.rx(2 * \n        pauli_1, 0)\n    elif \n    pauli_0.to_label() == 'Y':\n        expected.ry(2 * \n        pauli_1, 0)""]",no_found,0
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""""""""
    outline = Outline()
    playlist = Playlist()
    counter = Counter()
    video_list = []
    pdf_list = []
    rich_text_list = []
    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190', 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean', 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id, 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr', data=post_data).text.encode('utf_8').decode('unicode_escape')
    chapters = re.findall('homeworks=\\w+;.+id=(\\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)
        lessons = re.findall('chapterId=' + chapter[0] + '.+contentType=1.+id=(\\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)
            videos = re.findall('contentId=(\\d+).+contentType=(1).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()
            pdfs = re.findall('contentId=(\\d+).+contentType=(3).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()
            rich_text = re.findall('contentId=(\\d+).+contentType=(4).+id=(\\d+).+jsonContent=(.+);.+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')
                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()
    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","for pdf in pdfs:
    counter.add(2)
    outline.write(pdf[3], counter, 2, sign='*')
    if CONFIG['doc']:
        pdf_list.append(Document(counter, pdf[3], pdf))","[""for pdf in pdfs:\n    (_, _, _, pdf_3, *_) = pdf\n    counter.add(2)\n    outline.write(pdf_3, counter, 2, sign='*')\n    if CONFIG['doc']:\n        pdf_list.append(Document(counter, pdf_3, pdf))""]",no_found,0
natlas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/natlas/natlas-server/app/api/routes.py,https://github.com/MJL85/natlas/tree/master/natlas-server/app/api/routes.py,,submit$94,"def submit():
    status_code = None
    response_body = None
    data = request.get_json()
    newhost = {}
    newhost = json.loads(data)
    newhost['ctime'] = dt.now(tz.utc)
    if newhost['scan_reason'] == 'requested':
        mark_scan_completed(newhost['ip'], newhost['scan_id'])
    try:
        nmap = NmapParser.parse(newhost.get('xml_data', None))
        if nmap.hosts_total != 1:
            status_code = 400
            response_body = json.dumps({'status': status_code, 'message': 'XML had too many hosts in it', 'retry': False})
        elif len(nmap.hosts) == 1 and (not current_app.ScopeManager.is_acceptable_target(nmap.hosts[0].address)):
            status_code = 400
            response_body = json.dumps({'status': status_code, 'message': 'Out of scope: ' + nmap.hosts[0].address, 'retry': False})
        elif not newhost['is_up'] or (newhost['is_up'] and newhost['port_count'] == 0):
            current_app.elastic.new_result(newhost)
            status_code = 200
            response_body = json.dumps({'status': status_code, 'message': 'Received: ' + newhost['ip']})
    except NmapParserException:
        status_code = 400
        response_body = json.dumps({'status': status_code, 'message': 'Invalid nmap xml data provided', 'retry': False})
    if status_code and response_body:
        response = Response(response=response_body, status=status_code, content_type=json_content)
        return response
    if newhost['scan_start'] and newhost['scan_stop']:
        elapsed = dateutil.parser.parse(newhost['scan_stop']) - dateutil.parser.parse(newhost['scan_start'])
        newhost['elapsed'] = elapsed.seconds
    newhost['ip'] = nmap.hosts[0].address
    if len(nmap.hosts[0].hostnames) > 0:
        newhost['hostname'] = nmap.hosts[0].hostnames[0]
    tmpports = []
    newhost['ports'] = []
    for port in nmap.hosts[0].get_open_ports():
        tmpports.append(str(port[0]))
        srv = nmap.hosts[0].get_service(port[0], port[1])
        portinfo = srv.get_dict()
        portinfo['service'] = srv.service_dict
        portinfo['scripts'] = []
        for script in srv.scripts_results:
            scriptsave = {'id': script['id'], 'output': script['output']}
            portinfo['scripts'].append(scriptsave)
            if script['id'] == 'ssl-cert':
                portinfo['ssl'] = parse_ssl_data(script)
        newhost['ports'].append(portinfo)
    newhost['port_str'] = ', '.join(tmpports)
    if 'screenshots' in newhost and newhost['screenshots']:
        (newhost['screenshots'], newhost['num_screenshots']) = process_screenshots(newhost['screenshots'])
    if len(newhost['ports']) == 0:
        status_code = 200
        response_body = json.dumps({'status': status_code, 'message': f""Expected open ports but didn't find any for {newhost['ip']}""})
    elif len(newhost['ports']) > 500:
        status_code = 200
        response_body = json.dumps({'status': status_code, 'message': 'More than 500 ports found, throwing data out'})
    else:
        status_code = 200
        current_app.elastic.new_result(newhost)
        response_body = json.dumps({'status': status_code, 'message': f""Received {len(newhost['ports'])} ports for {newhost['ip']}""})
    response = Response(response=response_body, status=status_code, content_type=json_content)
    return response","for port in nmap.hosts[0].get_open_ports():
    tmpports.append(str(port[0]))
    srv = nmap.hosts[0].get_service(port[0], port[1])
    portinfo = srv.get_dict()
    portinfo['service'] = srv.service_dict
    portinfo['scripts'] = []
    for script in srv.scripts_results:
        scriptsave = {'id': script['id'], 'output': script['output']}
        portinfo['scripts'].append(scriptsave)
        if script['id'] == 'ssl-cert':
            portinfo['ssl'] = parse_ssl_data(script)
    newhost['ports'].append(portinfo)","[""for port in nmap.hosts[0].get_open_ports():\n    (port_0, port_1, *_) = port\n    tmpports.append(str(port_0))\n    srv = nmap.hosts[0].get_service(port_0, port_1)\n    portinfo = srv.get_dict()\n    portinfo['service'] = srv.service_dict\n    portinfo['scripts'] = []\n    for script in srv.scripts_results:\n        scriptsave = {'id': script['id'], 'output': script['output']}\n        portinfo['scripts'].append(scriptsave)\n        if script['id'] == 'ssl-cert':\n            portinfo['ssl'] = parse_ssl_data(script)\n    newhost['ports'].append(portinfo)"", ""for (port_0, port_1, *port_len) in nmap.hosts[0].get_open_ports():\n    tmpports.append(str(port_0))\n    srv = nmap.hosts[0].get_service(port_0, port_1)\n    portinfo = srv.get_dict()\n    portinfo['service'] = srv.service_dict\n    portinfo['scripts'] = []\n    for script in srv.scripts_results:\n        scriptsave = {'id': script['id'], 'output': script['output']}\n        portinfo['scripts'].append(scriptsave)\n        if script['id'] == 'ssl-cert':\n            portinfo['ssl'] = parse_ssl_data(script)\n    newhost['ports'].append(portinfo)""]",no_found,0
whatportis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/whatportis/tests/conftest.py,https://github.com/ncrocfer/whatportis/tree/master/tests/conftest.py,,create_ports$16,"def create_ports(tmpdir, monkeypatch):

    def _create_ports(ports):

        def get_db():
            tmp_db = tmpdir.join('db.json')
            db = TinyDB(str(tmp_db), storage=CachingMiddleware(JSONStorage))
            for port in ports:
                db.insert({'name': port[0], 'port': port[1], 'description': port[2], 'protocol': port[3]})
            return db
        return monkeypatch.setattr(whatportis.db, 'get_database', get_db)
    return _create_ports","for port in ports:
    db.insert({'name': port[0], 'port': port[1], 'description': port[2], 'protocol': port[3]})","[""for port in ports:\n    (port_0, port_1, port_2, port_3, *_) = port\n    db.insert({'name': port_0, 'port': port_1, 'description': port_2, 'protocol': port_3})"", ""for (port_0, port_1, port_2, port_3, *port_len) in ports:\n    db.insert({'name': port_0, 'port': port_1, 'description': port_2, 'protocol': port_3})""]",no_found,0
ASoulCnki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ASoulCnki/app/spider/reply/generate_refresh_like_spider.py,https://github.com/ASoulCnki/ASoulCnki/tree/master/app/spider/reply/generate_refresh_like_spider.py,,send_refresh_like_spider$6,"def send_refresh_like_spider(min_time):
    session = sqla['session']
    res = session.query(Reply.type_id, Reply.oid, Reply.dynamic_id, Reply.uid, min_time).filter(Reply.ctime > min_time).distinct(Reply.oid).all()
    for r in res:
        param = (r[0], r[1], r[2], r[3], r[4])
        tasks.refresh_like_num_task.apply_async(param, queue='reply_task_low_priority', routing_key='reply_low')","for r in res:
    param = (r[0], r[1], r[2], r[3], r[4])
    tasks.refresh_like_num_task.apply_async(param, queue='reply_task_low_priority', routing_key='reply_low')","[""for r in res:\n    (r_0, r_1, r_2, r_3, r_4, *_) = r\n    param = (r_0, r_1, r_2, r_3, r_4)\n    tasks.refresh_like_num_task.apply_async(param, queue='reply_task_low_priority', routing_key='reply_low')"", ""for (r_0, r_1, r_2, r_3, r_4, *r_len) in res:\n    param = (r_0, r_1, r_2, r_3, r_4)\n    tasks.refresh_like_num_task.apply_async(param, queue='reply_task_low_priority', routing_key='reply_low')""]",no_found,0
flask-profiler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flask-profiler/flask_profiler/storage/sqlite.py,https://github.com/muatik/flask-profiler/tree/master/flask_profiler/storage/sqlite.py,Sqlite,getSummary$281,"def getSummary(self, kwds={}):
    filters = Sqlite.getFilters(kwds)
    conditions = 'WHERE 1=1 and '
    if filters['startedAt']:
        conditions = conditions + 'startedAt>={0} AND '.format(filters['startedAt'])
    if filters['endedAt']:
        conditions = conditions + 'endedAt<={0} AND '.format(filters['endedAt'])
    if filters['elapsed']:
        conditions = conditions + 'elapsed>={0} AND'.format(filters['elapsed'])
    conditions = conditions.rstrip(' AND')
    with self.lock:
        sql = 'SELECT\n                    method, name,\n                    count(id) as count,\n                    min(elapsed) as minElapsed,\n                    max(elapsed) as maxElapsed,\n                    avg(elapsed) as avgElapsed\n                FROM ""{table_name}"" {conditions}\n                group by method, name\n                order by {sort_field} {sort_direction}\n                '.format(table_name=self.table_name, conditions=conditions, sort_field=filters['sort'][0], sort_direction=filters['sort'][1])
        self.cursor.execute(sql)
        rows = self.cursor.fetchall()
    result = []
    for r in rows:
        result.append({'method': r[0], 'name': r[1], 'count': r[2], 'minElapsed': r[3], 'maxElapsed': r[4], 'avgElapsed': r[5]})
    return result","for r in rows:
    result.append({'method': r[0], 'name': r[1], 'count': r[2], 'minElapsed': r[3], 'maxElapsed': r[4], 'avgElapsed': r[5]})","[""for r in rows:\n    (r_0, r_1, r_2, r_3, r_4, r_5, *_) = r\n    result.append({'method': r_0, 'name': r_1, 'count': r_2, 'minElapsed': r_3, 'maxElapsed': r_4, 'avgElapsed': r_5})"", ""for (r_0, r_1, r_2, r_3, r_4, r_5, *r_len) in rows:\n    result.append({'method': r_0, 'name': r_1, 'count': r_2, 'minElapsed': r_3, 'maxElapsed': r_4, 'avgElapsed': r_5})""]",no_found,0
plover,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plover/plover/orthography.py,https://github.com/openstenoproject/plover/tree/master/plover/orthography.py,,make_candidates_from_rules$9,"def make_candidates_from_rules(word, suffix, check=lambda x: True):
    candidates = []
    for r in system.ORTHOGRAPHY_RULES:
        m = r[0].match(word + ' ^ ' + suffix)
        if m:
            expanded = m.expand(r[1])
            if check(expanded):
                candidates.append(expanded)
    return candidates","for r in system.ORTHOGRAPHY_RULES:
    m = r[0].match(word + ' ^ ' + suffix)
    if m:
        expanded = m.expand(r[1])
        if check(expanded):
            candidates.append(expanded)","[""for r in system.ORTHOGRAPHY_RULES:\n    (r_0, r_1, *_) = r\n    m = r_0.match(word + ' ^ ' + suffix)\n    if m:\n        expanded = m.expand(r_1)\n        if check(expanded):\n            candidates.append(expanded)"", ""for (r_0, r_1, *r_len) in system.ORTHOGRAPHY_RULES:\n    m = \n    r_0.match(word + ' ^ ' + suffix)\n    if m:\n        expanded = m.expand(\n        r_1)\n        if check(expanded):\n            candidates.append(expanded)""]",no_found,0
dnsrecon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dnsrecon/tests/test_dnshelper.py,https://github.com/darkoperator/dnsrecon/tree/master/tests/test_dnshelper.py,Test_Lib_dnshelper,test_get_srv$71,"def test_get_srv(self):
    helper = DnsHelper('nsztm1.digi.ninja')
    records = helper.get_srv('_sip._tcp.zonetransfer.me')
    for record in records:
        assert record[0] == 'SRV'","for record in records:
    assert record[0] == 'SRV'","[""for record in records:\n    (record_0, *record_rrecordmaining) = record\n    assert record_0 == 'SRV'"", ""for (record_0, *record_len) in records:\n    assert \n    record_0 == 'SRV'""]",no_found,0
Scout2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Scout2/AWSScout2/output/console.py,https://github.com/nccgroup/Scout2/tree/master/AWSScout2/output/console.py,,format_listall_output$16,"def format_listall_output(format_file, format_item_dir, format, rule, option_prefix=None, template=None, skip_options=False):
    """"""
    Prepare listall output template

    :param format_file:
    :param format_item_dir:
    :param format:
    :param config:
    :param option_prefix:
    :param template:
    :param skip_options:
    :return:
    """"""
    if format_file and os.path.isfile(format_file):
        if not template:
            with open(format_file, 'rt') as f:
                template = f.read()
        if not skip_options:
            re_option = re.compile('(%_OPTION_\\((.*?)\\)_NOITPO_)')
            optional_files = re_option.findall(template)
            for optional_file in optional_files:
                if optional_file[1].startswith(option_prefix + '-'):
                    with open(os.path.join(format_item_dir, optional_file[1].strip()), 'rt') as f:
                        template = template.replace(optional_file[0].strip(), f.read())
        re_file = re.compile('(_FILE_\\((.*?)\\)_ELIF_)')
        while True:
            requested_files = re_file.findall(template)
            available_files = os.listdir(format_item_dir) if format_item_dir else []
            for requested_file in requested_files:
                if requested_file[1].strip() in available_files:
                    with open(os.path.join(format_item_dir, requested_file[1].strip()), 'rt') as f:
                        template = template.replace(requested_file[0].strip(), f.read())
            re_line = re.compile('(_ITEM_\\((.*?)\\)_METI_)')
            re_key = re.compile('_KEY_\\(*(.*?)\\)', re.DOTALL | re.MULTILINE)
            lines = re_line.findall(template)
            for (i, line) in enumerate(lines):
                lines[i] = line + (re_key.findall(line[1]),)
            requested_files = re_file.findall(template)
            if len(requested_files) == 0:
                break
    elif format and format[0] == 'csv':
        keys = rule.keys
        line = ', '.join(('_KEY_(%s)' % k for k in keys))
        lines = [(line, line, keys)]
        template = line
    return (lines, template)","for requested_file in requested_files:
    if requested_file[1].strip() in available_files:
        with open(os.path.join(format_item_dir, requested_file[1].strip()), 'rt') as f:
            template = template.replace(requested_file[0].strip(), f.read())","[""for requested_file in requested_files:\n    (requested_file_0, requested_file_1, *_) = requested_file\n    if requested_file_1.strip() in available_files:\n        with open(os.path.join(format_item_dir, requested_file_1.strip()), 'rt') as f:\n            template = template.replace(requested_file_0.strip(), f.read())"", ""for (requested_file_0, requested_file_1, *requested_file_len) in requested_files:\n    if \n    requested_file_1.strip() in available_files:\n        with open(os.path.join(format_item_dir, \n        requested_file_1.strip()), 'rt') as f:\n            template = template.replace(\n            requested_file_0.strip(), f.read())""]",no_found,0
dex,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dex/dex/dex.py,https://github.com/mongolab/dex/tree/master/dex/dex.py,Dex,_get_requested_databases$431,"def _get_requested_databases(self):
    """"""Returns a list of databases requested, not including ignored dbs""""""
    requested_databases = []
    if self._requested_namespaces is not None and self._requested_namespaces != []:
        for requested_namespace in self._requested_namespaces:
            if requested_namespace[0] is '*':
                return []
            elif requested_namespace[0] not in IGNORE_DBS:
                requested_databases.append(requested_namespace[0])
    return requested_databases","for requested_namespace in self._requested_namespaces:
    if requested_namespace[0] is '*':
        return []
    elif requested_namespace[0] not in IGNORE_DBS:
        requested_databases.append(requested_namespace[0])","[""for requested_namespace in self._requested_namespaces:\n    (requested_namespace_0, *requested_namespace_rrequested_namespacemaining) = requested_namespace\n    if requested_namespace_0 is '*':\n        return []\n    elif requested_namespace_0 not in IGNORE_DBS:\n        requested_databases.append(requested_namespace_0)"", ""for (requested_namespace_0, *requested_namespace_len) in self._requested_namespaces:\n    if \n    requested_namespace_0 is '*':\n        return []\n    elif \n    requested_namespace_0 not in IGNORE_DBS:\n        requested_databases.append(\n        requested_namespace_0)""]",no_found,0
rotki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rotki/rotkehlchen/db/dbhandler.py,https://github.com/rotki/rotki/tree/master/rotkehlchen/db/dbhandler.py,DBHandler,get_tags$2597,"def get_tags(self, cursor: 'DBCursor') -> Dict[str, Tag]:
    tags_mapping: Dict[str, Tag] = {}
    cursor.execute('SELECT name, description, background_color, foreground_color FROM tags;')
    for result in cursor:
        name = result[0]
        description = result[1]
        if description is not None and (not isinstance(description, str)):
            self.msg_aggregator.add_warning(f'Tag {name} with invalid description found in the DB. Skipping tag')
            continue
        try:
            background_color = deserialize_hex_color_code(result[2])
            foreground_color = deserialize_hex_color_code(result[3])
        except DeserializationError as e:
            self.msg_aggregator.add_warning(f'Tag {name} with invalid color code found in the DB. {str(e)}. Skipping tag')
            continue
        tags_mapping[name] = Tag(name=name, description=description, background_color=background_color, foreground_color=foreground_color)
    return tags_mapping","for result in cursor:
    name = result[0]
    description = result[1]
    if description is not None and (not isinstance(description, str)):
        self.msg_aggregator.add_warning(f'Tag {name} with invalid description found in the DB. Skipping tag')
        continue
    try:
        background_color = deserialize_hex_color_code(result[2])
        foreground_color = deserialize_hex_color_code(result[3])
    except DeserializationError as e:
        self.msg_aggregator.add_warning(f'Tag {name} with invalid color code found in the DB. {str(e)}. Skipping tag')
        continue
    tags_mapping[name] = Tag(name=name, description=description, background_color=background_color, foreground_color=foreground_color)","[""for result in cursor:\n    (result_0, result_1, result_2, result_3, *_) = result\n    name = result_0\n    description = result_1\n    if description is not None and (not isinstance(description, str)):\n        self.msg_aggregator.add_warning(f'Tag {name} with invalid description found in the DB. Skipping tag')\n        continue\n    try:\n        background_color = deserialize_hex_color_code(result_2)\n        foreground_color = deserialize_hex_color_code(result_3)\n    except DeserializationError as e:\n        self.msg_aggregator.add_warning(f'Tag {name} with invalid color code found in the DB. {str(e)}. Skipping tag')\n        continue\n    tags_mapping[name] = Tag(name=name, description=description, background_color=background_color, foreground_color=foreground_color)"", ""for (result_0, result_1, result_2, result_3, *result_len) in cursor:\n    name = \n    result_0\n    description = \n    result_1\n    if description is not None and (not isinstance(description, str)):\n        self.msg_aggregator.add_warning(f'Tag {name} with invalid description found in the DB. Skipping tag')\n        continue\n    try:\n        background_color = deserialize_hex_color_code(\n        result_2)\n        foreground_color = deserialize_hex_color_code(\n        result_3)\n    except DeserializationError as e:\n        self.msg_aggregator.add_warning(f'Tag {name} with invalid color code found in the DB. {str(e)}. Skipping tag')\n        continue\n    tags_mapping[name] = Tag(name=name, description=description, background_color=background_color, foreground_color=foreground_color)""]",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/FuzzParam.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/FuzzParam.py,FuzzerParam,result_method_1$355,"def result_method_1(self, str):
    result_key_list = []
    result_value_list = []
    regxs_1 = 'method\\:.*?\\,url\\:.*?\\,data\\:({.*?})'
    if re.findall(regxs_1, str, re.S):
        result_json = re.findall(regxs_1, str, re.S)[0].replace(' ', '').replace('\n', '').replace('{', '').replace('\t', '')
        regx_key = '(.*?)\\:.*?\\,|(.*?)\\:.*?\\}'
        result_keys = re.findall(regx_key, result_json, re.S)
        for result_key in result_keys:
            if result_key[0] != '':
                result_key_list.append(result_key[0])
            if result_key[1] != '':
                result_key_list.append(result_key[1])
        regx_value = '\\:(.*?)\\,|\\:(.*?)\\}'
        result_values = re.findall(regx_value, result_json, re.S)
        for para in result_values:
            if para[0] != '':
                result_value_list.append(para[0])
            if para[1] != '':
                result_value_list.append(para[1])
    result_list = [result_key_list, result_value_list]
    return result_list","for result_key in result_keys:
    if result_key[0] != '':
        result_key_list.append(result_key[0])
    if result_key[1] != '':
        result_key_list.append(result_key[1])","[""for result_key in result_keys:\n    (result_key_0, result_key_1, *_) = result_key\n    if result_key_0 != '':\n        result_key_list.append(result_key_0)\n    if result_key_1 != '':\n        result_key_list.append(result_key_1)"", ""for (result_key_0, result_key_1, *result_key_len) in result_keys:\n    if \n    result_key_0 != '':\n        result_key_list.append(\n        result_key_0)\n    if \n    result_key_1 != '':\n        result_key_list.append(\n        result_key_1)""]",no_found,0
hydra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydra/hydra/_internal/hydra.py,https://github.com/facebookresearch/hydra/tree/master/hydra/_internal/hydra.py,Hydra,_print_plugins_profiling_info$449,"def _print_plugins_profiling_info(self, top_n: int) -> None:
    assert log is not None
    stats = Plugins.instance().get_stats()
    if stats is None:
        return
    items = list(stats.modules_import_time.items())
    filtered = filter(lambda x: x[1] > 0.0005, items)
    sorted_items = sorted(filtered, key=lambda x: x[1], reverse=True)
    top_n = max(len(sorted_items), top_n)
    box: List[List[str]] = [['Module', 'Sec']]
    for item in sorted_items[0:top_n]:
        box.append([item[0], f'{item[1]:.3f}'])
    padding = get_column_widths(box)
    log.debug('')
    self._log_header(header='Profiling information', filler='*')
    self._log_header(header=f'Total plugins scan time : {stats.total_time:.3f} seconds', filler='-')
    header = f'| {box[0][0].ljust(padding[0])} | {box[0][1].ljust(padding[1])} |'
    self._log_header(header=header, filler='-')
    del box[0]
    for row in box:
        a = row[0].ljust(padding[0])
        b = row[1].ljust(padding[1])
        log.debug(f'| {a} | {b} |')
    self._log_footer(header=header, filler='-')","for row in box:
    a = row[0].ljust(padding[0])
    b = row[1].ljust(padding[1])
    log.debug(f'| {a} | {b} |')","[""for row in box:\n    (row_0, row_1, *_) = row\n    a = row_0.ljust(padding[0])\n    b = row_1.ljust(padding[1])\n    log.debug(f'| {a} | {b} |')"", ""for (row_0, row_1, *row_len) in box:\n    a = \n    row_0.ljust(padding[0])\n    b = \n    row_1.ljust(padding[1])\n    log.debug(f'| {a} | {b} |')""]",no_found,0
mvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mvt/mvt/ios/modules/mixed/chrome_favicon.py,https://github.com/mvt-project/mvt/tree/master/mvt/ios/modules/mixed/chrome_favicon.py,ChromeFavicon,run$48,"def run(self):
    self._find_ios_database(backup_ids=CHROME_FAVICON_BACKUP_IDS, root_paths=CHROME_FAVICON_ROOT_PATHS)
    self.log.info('Found Chrome favicon cache database at path: %s', self.file_path)
    conn = sqlite3.connect(self.file_path)
    cur = conn.cursor()
    cur.execute('\n            SELECT\n                icon_mapping.page_url,\n                favicons.url,\n                favicon_bitmaps.last_updated,\n                favicon_bitmaps.last_requested\n            FROM icon_mapping\n            JOIN favicon_bitmaps ON icon_mapping.icon_id = favicon_bitmaps.icon_id\n            JOIN favicons ON icon_mapping.icon_id = favicons.id\n            ORDER BY icon_mapping.id;\n        ')
    records = []
    for row in cur:
        last_timestamp = int(row[2]) or int(row[3])
        records.append({'url': row[0], 'icon_url': row[1], 'timestamp': last_timestamp, 'isodate': convert_timestamp_to_iso(convert_chrometime_to_unix(last_timestamp))})
    cur.close()
    conn.close()
    self.log.info('Extracted a total of %d favicon records', len(records))
    self.results = sorted(records, key=lambda row: row['isodate'])","for row in cur:
    last_timestamp = int(row[2]) or int(row[3])
    records.append({'url': row[0], 'icon_url': row[1], 'timestamp': last_timestamp, 'isodate': convert_timestamp_to_iso(convert_chrometime_to_unix(last_timestamp))})","[""for row in cur:\n    (row_0, row_1, row_2, row_3, *_) = row\n    last_timestamp = int(row_2) or int(row_3)\n    records.append({'url': row_0, 'icon_url': row_1, 'timestamp': last_timestamp, 'isodate': convert_timestamp_to_iso(convert_chrometime_to_unix(last_timestamp))})"", ""for (row_0, row_1, row_2, row_3, *row_len) in cur:\n    last_timestamp = int(row_2) or int(row_3)\n    records.append({'url': row_0, 'icon_url': row_1, 'timestamp': last_timestamp, 'isodate': convert_timestamp_to_iso(convert_chrometime_to_unix(last_timestamp))})""]",no_found,0
mysql-connector-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mysql-connector-python/examples/dates.py,https://github.com/mysql/mysql-connector-python/tree/master/examples/dates.py,,main$56,"def main(config):
    output = []
    db = mysql.connector.Connect(**config)
    cursor = db.cursor()
    tbl = 'myconnpy_dates'
    cursor.execute('SET sql_mode = """"')
    stmt_drop = 'DROP TABLE IF EXISTS {0}'.format(tbl)
    cursor.execute(stmt_drop)
    stmt_create = 'CREATE TABLE {0} (   `id` tinyint(4) NOT NULL AUTO_INCREMENT,   `c1` date DEFAULT NULL,   `c2` datetime NOT NULL,   `c3` time DEFAULT NULL,   `changed` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP     ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`))'.format(tbl)
    cursor.execute(stmt_create)
    stmt_insert = 'INSERT INTO {0} (c1,c2,c3) VALUES (%s,%s,%s)'.format(tbl)
    for data in DATA:
        try:
            cursor.execute(stmt_insert, data)
        except (mysql.connector.errors.Error, TypeError) as exc:
            output.append('Failed inserting {0}\nError: {1}\n'.format(data, exc))
            cursor.execute(stmt_drop)
            raise
    stmt_select = 'SELECT * FROM {0} ORDER BY id'.format(tbl)
    cursor.execute(stmt_select)
    for row in cursor.fetchall():
        output.append('%3s | %10s | %19s | %8s |' % (row[0], row[1], row[2], row[3]))
    cursor.execute(stmt_drop)
    cursor.close()
    db.close()
    return output","for row in cursor.fetchall():
    output.append('%3s | %10s | %19s | %8s |' % (row[0], row[1], row[2], row[3]))","[""for row in cursor.fetchall():\n    (row_0, row_1, row_2, row_3, *_) = row\n    output.append('%3s | %10s | %19s | %8s |' % (row_0, row_1, row_2, row_3))"", ""for (row_0, row_1, row_2, row_3, *row_len) in cursor.fetchall():\n    output.append('%3s | %10s | %19s | %8s |' % (row_0, row_1, row_2, row_3))""]",no_found,0
spiderfoot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spiderfoot/sfwebui.py,https://github.com/smicallef/spiderfoot/tree/master//sfwebui.py,SpiderFootWebUi,scanexportlogs$328,"def scanexportlogs(self: 'SpiderFootWebUi', id: str, dialect: str='excel') -> bytes:
    """"""Get scan log

        Args:
            id (str): scan ID
            dialect (str): CSV dialect (default: excel)

        Returns:
            bytes: scan logs in CSV format
        """"""
    dbh = SpiderFootDb(self.config)
    try:
        data = dbh.scanLogs(id, None, None, True)
    except Exception:
        return self.error('Scan ID not found.')
    if not data:
        return self.error('Scan ID not found.')
    fileobj = StringIO()
    parser = csv.writer(fileobj, dialect=dialect)
    parser.writerow(['Date', 'Component', 'Type', 'Event', 'Event ID'])
    for row in data:
        parser.writerow([time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(row[0] / 1000)), str(row[1]), str(row[2]), str(row[3]), row[4]])
    cherrypy.response.headers['Content-Disposition'] = f'attachment; filename=SpiderFoot-{id}.log.csv'
    cherrypy.response.headers['Content-Type'] = 'application/csv'
    cherrypy.response.headers['Pragma'] = 'no-cache'
    return fileobj.getvalue().encode('utf-8')","for row in data:
    parser.writerow([time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(row[0] / 1000)), str(row[1]), str(row[2]), str(row[3]), row[4]])","[""for row in data:\n    (row_0, row_1, row_2, row_3, row_4, *_) = row\n    parser.writerow([time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(row_0 / 1000)), str(row_1), str(row_2), str(row_3), row_4])"", ""for (row_0, row_1, row_2, row_3, row_4, *row_len) in data:\n    parser.writerow([time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(row_0 / 1000)), str(row_1), str(row_2), str(row_3), row_4])""]",no_found,0
openstates-scrapers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openstates-scrapers/scrapers/va/csv_bills.py,https://github.com/openstates/openstates-scrapers/tree/master/scrapers/va/csv_bills.py,VaCSVBillScraper,load_amendments$88,"def load_amendments(self):
    resp = self.get(self._url_base + 'Amendments.csv').text
    reader = csv.reader(resp.splitlines(), delimiter=',')
    for row in reader:
        self._amendments[row[0].strip()].append({'bill_number': row[0].strip(), 'txt_docid': row[1].strip()})
    self.warning('Total Amendments Loaded: ' + str(len(self._amendments)))","for row in reader:
    self._amendments[row[0].strip()].append({'bill_number': row[0].strip(), 'txt_docid': row[1].strip()})","[""for row in reader:\n    (row_0, row_1, *_) = row\n    self._amendments[row_0.strip()].append({'bill_number': row_0.strip(), 'txt_docid': row_1.strip()})"", ""for (row_0, row_1, *row_len) in reader:\n    self._amendments[row_0.strip()].append({'bill_number': row_0.strip(), 'txt_docid': row_1.strip()})""]",no_found,0
cloudtracker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloudtracker/cloudtracker/datasources/athena.py,https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,Athena,get_performed_users$377,"def get_performed_users(self):
    """"""
        Returns the users that performed actions within the search filters
        """"""
    query = 'select distinct userIdentity.userName from {table_name} where {search_filter}'.format(table_name=self.table_name, search_filter=self.search_filter)
    response = self.query_athena(query)
    user_names = {}
    for row in response:
        user_name = row[0]
        if user_name == 'HIDDEN_DUE_TO_SECURITY_REASONS':
            continue
        user_names[user_name] = True
    return user_names","for row in response:
    user_name = row[0]
    if user_name == 'HIDDEN_DUE_TO_SECURITY_REASONS':
        continue
    user_names[user_name] = True","[""for row in response:\n    (row_0, *row_rrowmaining) = row\n    user_name = row_0\n    if user_name == 'HIDDEN_DUE_TO_SECURITY_REASONS':\n        continue\n    user_names[user_name] = True"", ""for (row_0, *row_len) in response:\n    user_name = \n    row_0\n    if user_name == 'HIDDEN_DUE_TO_SECURITY_REASONS':\n        continue\n    user_names[user_name] = True""]",no_found,0
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/webapps/galaxy/controllers/tag.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/webapps/galaxy/controllers/tag.py,TagsController,_get_tag_autocomplete_values$137,"def _get_tag_autocomplete_values(self, trans, q, limit, timestamp, user=None, item=None, item_class=None):
    """"""
        Returns autocomplete data for tag values ordered from most frequently used to
        least frequently used.
        """"""
    tag_name_and_value = q.split(':')
    tag_name = tag_name_and_value[0]
    tag_value = tag_name_and_value[1]
    tag = self.get_tag_handler(trans).get_tag_by_name(tag_name)
    if tag is None:
        return ''
    if item is None and item_class is None:
        raise RuntimeError('Both item and item_class cannot be None')
    elif item is not None:
        item_class = item.__class__
    item_tag_assoc_class = self.get_tag_handler(trans).get_tag_assoc_class(item_class)
    cols_to_select = [item_tag_assoc_class.table.c.value, func.count('*')]
    from_obj = item_tag_assoc_class.table.join(item_class.table).join(trans.app.model.Tag.table)
    where_clause = and_(item_tag_assoc_class.table.c.user_id == user.id, trans.app.model.Tag.table.c.id == tag.id, item_tag_assoc_class.table.c.value.like(f'{tag_value}%'))
    order_by = [func.count('*').desc(), item_tag_assoc_class.table.c.value]
    group_by = item_tag_assoc_class.table.c.value
    query = select(columns=cols_to_select, from_obj=from_obj, whereclause=where_clause, group_by=group_by, order_by=order_by, limit=limit)
    result_set = trans.sa_session.execute(query)
    ac_data = f""#Header|Your Values for '{tag_name}'\n""
    tag_uname = self._get_usernames_for_tag(trans, trans.user, tag, item_class, item_tag_assoc_class)[0]
    for row in result_set:
        ac_data += f'{tag_uname}:{row[0]}|{row[0]}\n'
    return ac_data","for row in result_set:
    ac_data += f'{tag_uname}:{row[0]}|{row[0]}\n'","[""for row in result_set:\n    (row_0, *row_rrowmaining) = row\n    ac_data += f'{tag_uname}:{row_0}|{row_0}\\n'"", ""for (row_0, *row_len) in result_set:\n    ac_data += f'{tag_uname}:{row_0}|{row_0}\\n'""]",no_found,0
PornHub-downloader-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PornHub-downloader-python/functions.py,https://github.com/mariosemes/PornHub-downloader-python/tree/master//functions.py,,dl_all_items$121,"def dl_all_items(conn):
    c = conn.cursor()
    try:
        c.execute('SELECT * FROM ph_items')
    except Error as e:
        print(e)
        sys.exit()
    rows = c.fetchall()
    for row in rows:
        if row[1] == 'model':
            url_after = '/videos/upload'
        elif row[1] == 'users':
            url_after = '/videos/public'
        elif row[1] == 'channels':
            url_after = '/videos'
        else:
            url_after = ''
        print('-----------------------------')
        print(row[1])
        print(row[2])
        print('https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after)
        print('-----------------------------')
        outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
        ydl_opts_start = {'format': 'best', 'playliststart:': 1, 'playlistend': 4, 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
        url = 'https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2] + url_after)
        with youtube_dl.YoutubeDL(ydl_opts_start) as ydl:
            ydl.download([url])
        try:
            c.execute('UPDATE ph_items SET lastchecked=CURRENT_TIMESTAMP WHERE url_name = ?', (row[2],))
            conn.commit()
        except Error as e:
            print(e)
            sys.exit()","for row in rows:
    if row[1] == 'model':
        url_after = '/videos/upload'
    elif row[1] == 'users':
        url_after = '/videos/public'
    elif row[1] == 'channels':
        url_after = '/videos'
    else:
        url_after = ''
    print('-----------------------------')
    print(row[1])
    print(row[2])
    print('https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after)
    print('-----------------------------')
    outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
    ydl_opts_start = {'format': 'best', 'playliststart:': 1, 'playlistend': 4, 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
    url = 'https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2] + url_after)
    with youtube_dl.YoutubeDL(ydl_opts_start) as ydl:
        ydl.download([url])
    try:
        c.execute('UPDATE ph_items SET lastchecked=CURRENT_TIMESTAMP WHERE url_name = ?', (row[2],))
        conn.commit()
    except Error as e:
        print(e)
        sys.exit()","[""for row in rows:\n    (_, row_1, row_2, row_3, *_) = row\n    if row_1 == 'model':\n        url_after = '/videos/upload'\n    elif row_1 == 'users':\n        url_after = '/videos/public'\n    elif row_1 == 'channels':\n        url_after = '/videos'\n    else:\n        url_after = ''\n    print('-----------------------------')\n    print(row_1)\n    print(row_2)\n    print('https://www.pornhub.com/' + str(row_1) + '/' + str(row_2) + url_after)\n    print('-----------------------------')\n    outtmpl = get_dl_location('DownloadLocation') + '/' + str(row_1) + '/' + str(row_3) + '/%(title)s.%(ext)s'\n    ydl_opts_start = {'format': 'best', 'playliststart:': 1, 'playlistend': 4, 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}\n    url = 'https://www.pornhub.com/' + str(row_1) + '/' + str(row_2 + url_after)\n    with youtube_dl.YoutubeDL(ydl_opts_start) as ydl:\n        ydl.download([url])\n    try:\n        c.execute('UPDATE ph_items SET lastchecked=CURRENT_TIMESTAMP WHERE url_name = ?', (row_2,))\n        conn.commit()\n    except Error as e:\n        print(e)\n        sys.exit()"", ""for (row_0, row_1, row_2, row_3, *row_len) in rows:\n    if \n    row_1 == 'model':\n        url_after = '/videos/upload'\n    elif \n    row_1 == 'users':\n        url_after = '/videos/public'\n    elif \n    row_1 == 'channels':\n        url_after = '/videos'\n    else:\n        url_after = ''\n    print('-----------------------------')\n    print(\n    row_1)\n    print(\n    row_2)\n    print('https://www.pornhub.com/' + str(\n    row_1) + '/' + str(\n    row_2) + url_after)\n    print('-----------------------------')\n    outtmpl = get_dl_location('DownloadLocation') + '/' + str(\n    row_1) + '/' + str(\n    row_3) + '/%(title)s.%(ext)s'\n    ydl_opts_start = {'format': 'best', 'playliststart:': 1, 'playlistend': 4, 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}\n    url = 'https://www.pornhub.com/' + str(\n    row_1) + '/' + str(\n    row_2 + url_after)\n    with youtube_dl.YoutubeDL(ydl_opts_start) as ydl:\n        ydl.download([url])\n    try:\n        c.execute('UPDATE ph_items SET lastchecked=CURRENT_TIMESTAMP WHERE url_name = ?', (\n        row_2,))\n        conn.commit()\n    except Error as e:\n        print(e)\n        sys.exit()""]",no_found,0
toil,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/toil/src/toil/fileStores/cachingFileStore.py,https://github.com/DataBiosphere/toil/tree/master/src/toil/fileStores/cachingFileStore.py,CachingFileStore,getFileReaderCount$541,"def getFileReaderCount(self, fileID):
    """"""
        Return the number of current outstanding reads of the given file.

        Counts mutable references too.
        """"""
    for row in self.cur.execute('SELECT COUNT(*) FROM refs WHERE file_id = ?', (fileID,)):
        return row[0]
    return 0","for row in self.cur.execute('SELECT COUNT(*) FROM refs WHERE file_id = ?', (fileID,)):
    return row[0]","[""for row in self.cur.execute('SELECT COUNT(*) FROM refs WHERE file_id = ?', (fileID,)):\n    (row_0, *row_rrowmaining) = row\n    return row_0"", ""for (row_0, *row_len) in self.cur.execute('SELECT COUNT(*) FROM refs WHERE file_id = ?', (fileID,)):\n    return\n    row_0""]",no_found,0
build-webos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/build-webos/scripts/webos-iot-scripts/set_webosiot_layer.py,https://github.com/webosose/build-webos/tree/master/scripts/webos-iot-scripts/set_webosiot_layer.py,,make_webosiot_layers_as_rule$9,"def make_webosiot_layers_as_rule(path):
    sys.path.insert(0, path)
    if not os.path.isfile(os.path.join(path, 'weboslayers.py')):
        raise Exception('Error: Configuration file %s does not exist!' % os.path.join(path, 'weboslayers.py'))
    from webosiot_rule import webosiot_layer_rules
    from weboslayers import webos_layers
    maxpriority = webos_layers[-1][1]
    for rule in webosiot_layer_rules:
        action = rule[0]
        if action == 'remove':
            del webos_layers[layer_index(webos_layers, rule[1])]
        elif action == 'insert':
            webos_layers.insert(layer_index(webos_layers, rule[1]) + 1, rule[2])
        elif action == 'append':
            appendlayer = list(rule[1])
            appendlayer[1] = maxpriority + 1
            webos_layers.append(tuple(appendlayer))
            maxpriority += 1
    return webos_layers","for rule in webosiot_layer_rules:
    action = rule[0]
    if action == 'remove':
        del webos_layers[layer_index(webos_layers, rule[1])]
    elif action == 'insert':
        webos_layers.insert(layer_index(webos_layers, rule[1]) + 1, rule[2])
    elif action == 'append':
        appendlayer = list(rule[1])
        appendlayer[1] = maxpriority + 1
        webos_layers.append(tuple(appendlayer))
        maxpriority += 1","[""for rule in webosiot_layer_rules:\n    (rule_0, rule_1, rule_2, *_) = rule\n    action = rule_0\n    if action == 'remove':\n        del webos_layers[layer_index(webos_layers, rule_1)]\n    elif action == 'insert':\n        webos_layers.insert(layer_index(webos_layers, rule_1) + 1, rule_2)\n    elif action == 'append':\n        appendlayer = list(rule_1)\n        appendlayer[1] = maxpriority + 1\n        webos_layers.append(tuple(appendlayer))\n        maxpriority += 1"", ""for (rule_0, rule_1, rule_2, *rule_len) in webosiot_layer_rules:\n    action = \n    rule_0\n    if action == 'remove':\n        del webos_layers[layer_index(webos_layers, \n        rule_1)]\n    elif action == 'insert':\n        webos_layers.insert(layer_index(webos_layers, \n        rule_1) + 1, \n        rule_2)\n    elif action == 'append':\n        appendlayer = list(\n        rule_1)\n        appendlayer[1] = maxpriority + 1\n        webos_layers.append(tuple(appendlayer))\n        maxpriority += 1""]",no_found,0
pygmsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/src/pygmsh/occ/geometry.py,https://github.com/nschloe/pygmsh/tree/master/src/pygmsh/occ/geometry.py,Geometry,boolean_difference$197,"def boolean_difference(self, d0, d1, delete_first: bool=True, delete_other: bool=True):
    """"""Boolean difference, see
        https://gmsh.info/doc/texinfo/gmsh.html#Boolean-operations input_entity
        and tool_entity are called object and tool in gmsh documentation.
        """"""
    d0 = d0 if isinstance(d0, list) else [d0]
    d1 = d1 if isinstance(d1, list) else [d1]
    (dim_tags, _) = gmsh.model.occ.cut([d.dim_tag for d in d0], [d.dim_tag for d in d1], removeObject=delete_first, removeTool=delete_other)
    all_entities = []
    if delete_first:
        all_entities += d0
    if delete_other:
        all_entities += d1
    for s in self._SIZE_QUEUE:
        if s[0] in all_entities:
            warnings.warn(f'Specified mesh size for {s[0]} discarded in Boolean difference operation.')
    self._SIZE_QUEUE = [s for s in self._SIZE_QUEUE if s[0] not in all_entities]
    return [Dummy(*dim_tag) for dim_tag in dim_tags]","for s in self._SIZE_QUEUE:
    if s[0] in all_entities:
        warnings.warn(f'Specified mesh size for {s[0]} discarded in Boolean difference operation.')","[""for s in self._SIZE_QUEUE:\n    (s_0, *s_rsmaining) = s\n    if s_0 in all_entities:\n        warnings.warn(f'Specified mesh size for {s_0} discarded in Boolean difference operation.')"", ""for (s_0, *s_len) in self._SIZE_QUEUE:\n    if \n    s_0 in all_entities:\n        warnings.warn(f'Specified mesh size for {s_0} discarded in Boolean difference operation.')""]",no_found,0
dcos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dcos/packages/dcos-integration-test/extra/test_metrics.py,https://github.com/dcos/dcos/tree/master/packages/dcos-integration-test/extra/test_metrics.py,,_check_calico_metrics$212,"def _check_calico_metrics() -> None:
    response = get_metrics_prom(dcos_api_session, node)
    for family in text_string_to_metric_families(response.text):
        for sample in family.samples:
            if sample[0].startswith('felix') and sample[1].get('dcos_component_name') == 'DC/OS Calico':
                return
    raise Exception('Expected DC/OS Calico felix* metric on agent nodes not found')","for sample in family.samples:
    if sample[0].startswith('felix') and sample[1].get('dcos_component_name') == 'DC/OS Calico':
        return","[""for sample in family.samples:\n    (sample_0, sample_1, *_) = sample\n    if sample_0.startswith('felix') and sample_1.get('dcos_component_name') == 'DC/OS Calico':\n        return"", ""for (sample_0, sample_1, *sample_len) in family.samples:\n    if \n    sample_0.startswith('felix') and \n    sample_1.get('dcos_component_name') == 'DC/OS Calico':\n        return""]",no_found,0
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/objects/tvshows.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/objects/tvshows.py,TVShows,remove$519,"def remove(self, item_id, e_item):
    """""" Remove showid, fileid, pathid, jellyfin reference.
            There's no episodes left, delete show and any possible remaining seasons
        """"""
    obj = {'Id': item_id}
    try:
        obj['KodiId'] = e_item[0]
        obj['FileId'] = e_item[1]
        obj['ParentId'] = e_item[3]
        obj['Media'] = e_item[4]
    except TypeError:
        return
    if obj['Media'] == 'episode':
        temp_obj = dict(obj)
        self.remove_episode(obj['KodiId'], obj['FileId'], obj['Id'])
        season = self.jellyfin_db.get_full_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_season_obj))
        try:
            temp_obj['Id'] = season[0]
            temp_obj['ParentId'] = season[1]
        except TypeError:
            return
        if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
            self.remove_season(obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))
        temp_obj['Id'] = self.jellyfin_db.get_item_by_kodi_id(*values(temp_obj, QUEM.get_item_by_parent_tvshow_obj))
        if not self.get_total_episodes(*values(temp_obj, QU.get_total_episodes_obj)):
            for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
                self.remove_season(season[1], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))
            self.remove_tvshow(temp_obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))
    elif obj['Media'] == 'tvshow':
        obj['ParentId'] = obj['KodiId']
        for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):
            temp_obj = dict(obj)
            temp_obj['ParentId'] = season[1]
            for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
                self.remove_episode(episode[1], episode[2], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
        else:
            self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))
        self.remove_tvshow(obj['KodiId'], obj['Id'])
    elif obj['Media'] == 'season':
        for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
            self.remove_episode(episode[1], episode[2], obj['Id'])
        else:
            self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))
        self.remove_season(obj['KodiId'], obj['Id'])
        if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj)):
            self.remove_tvshow(obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_tvshow_obj))
    for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
        self.remove_episode(episode[2], episode[3], obj['Id'])
    else:
        self.jellyfin_db.remove_media_by_parent_id(obj['Id'])
    self.jellyfin_db.remove_item(*values(obj, QUEM.delete_item_obj))","for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):
    temp_obj = dict(obj)
    temp_obj['ParentId'] = season[1]
    for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
        self.remove_episode(episode[1], episode[2], obj['Id'])
    else:
        self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
else:
    self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))","[""for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):\n    (season_0, season_1, *season_rseasonmaining) = season\n    temp_obj = dict(obj)\n    temp_obj['ParentId'] = season_1\n    for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):\n        self.remove_episode(episode[1], episode[2], obj['Id'])\n    else:\n        self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))\nelse:\n    self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))"", ""for (season_0, season_1, *season_len) in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):\n    temp_obj = dict(obj)\n    temp_obj['ParentId'] = \n    season_1\n    for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):\n        self.remove_episode(episode[1], episode[2], obj['Id'])\n    else:\n        self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))\nelse:\n    self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))""]",no_found,0
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/objects/tvshows.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/objects/tvshows.py,TVShows,remove$519,"def remove(self, item_id, e_item):
    """""" Remove showid, fileid, pathid, jellyfin reference.
            There's no episodes left, delete show and any possible remaining seasons
        """"""
    obj = {'Id': item_id}
    try:
        obj['KodiId'] = e_item[0]
        obj['FileId'] = e_item[1]
        obj['ParentId'] = e_item[3]
        obj['Media'] = e_item[4]
    except TypeError:
        return
    if obj['Media'] == 'episode':
        temp_obj = dict(obj)
        self.remove_episode(obj['KodiId'], obj['FileId'], obj['Id'])
        season = self.jellyfin_db.get_full_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_season_obj))
        try:
            temp_obj['Id'] = season[0]
            temp_obj['ParentId'] = season[1]
        except TypeError:
            return
        if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
            self.remove_season(obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))
        temp_obj['Id'] = self.jellyfin_db.get_item_by_kodi_id(*values(temp_obj, QUEM.get_item_by_parent_tvshow_obj))
        if not self.get_total_episodes(*values(temp_obj, QU.get_total_episodes_obj)):
            for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
                self.remove_season(season[1], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))
            self.remove_tvshow(temp_obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item(*values(temp_obj, QUEM.delete_item_obj))
    elif obj['Media'] == 'tvshow':
        obj['ParentId'] = obj['KodiId']
        for season in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_season_obj)):
            temp_obj = dict(obj)
            temp_obj['ParentId'] = season[1]
            for episode in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_episode_obj)):
                self.remove_episode(episode[1], episode[2], obj['Id'])
            else:
                self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_episode_obj))
        else:
            self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj))
        self.remove_tvshow(obj['KodiId'], obj['Id'])
    elif obj['Media'] == 'season':
        for episode in self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.get_item_by_parent_episode_obj)):
            self.remove_episode(episode[1], episode[2], obj['Id'])
        else:
            self.jellyfin_db.remove_items_by_parent_id(*values(obj, QUEM.delete_item_by_parent_episode_obj))
        self.remove_season(obj['KodiId'], obj['Id'])
        if not self.jellyfin_db.get_item_by_parent_id(*values(obj, QUEM.delete_item_by_parent_season_obj)):
            self.remove_tvshow(obj['ParentId'], obj['Id'])
            self.jellyfin_db.remove_item_by_kodi_id(*values(obj, QUEM.delete_item_by_parent_tvshow_obj))
    for episode in self.jellyfin_db.get_media_by_parent_id(obj['Id']):
        self.remove_episode(episode[2], episode[3], obj['Id'])
    else:
        self.jellyfin_db.remove_media_by_parent_id(obj['Id'])
    self.jellyfin_db.remove_item(*values(obj, QUEM.delete_item_obj))","for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):
    self.remove_season(season[1], obj['Id'])
else:
    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))","[""for season in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):\n    (season_0, season_1, *season_rseasonmaining) = season\n    self.remove_season(season_1, obj['Id'])\nelse:\n    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))"", ""for (season_0, season_1, *season_len) in self.jellyfin_db.get_item_by_parent_id(*values(temp_obj, QUEM.get_item_by_parent_season_obj)):\n    self.remove_season(season_1, obj['Id'])\nelse:\n    self.jellyfin_db.remove_items_by_parent_id(*values(temp_obj, QUEM.delete_item_by_parent_season_obj))""]",no_found,0
bCNC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/plugins/slicemesh.py,https://github.com/vlachoudis/bCNC/tree/master/bCNC/plugins/slicemesh.py,Tool,slice$211,"def slice(self, verts, faces, z, zout=None, axis='z'):
    tags = '[slice]'
    if axis == 'z':
        tags = f'[slice,minz:{float(z):f}]'
    block = Block(f'slice {axis}{float(z):f} {tags}')
    if axis == 'x':
        plane_orig = (z, 0, 0)
        plane_norm = (1, 0, 0)
    elif axis == 'y':
        plane_orig = (0, z, 0)
        plane_norm = (0, 1, 0)
    else:
        plane_orig = (0, 0, z)
        plane_norm = (0, 0, 1)
    contours = meshcut.cross_section(verts, faces, plane_orig, plane_norm)
    if zout is not None:
        for contour in contours:
            for segment in contour:
                segment[2] = zout
    for contour in contours:
        gtype = 0
        for segment in contour:
            block.append(f'g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}')
            gtype = 1
        block.append(f'g1 x{contour[0][0]:f} y{contour[0][1]:f} z{contour[0][2]:f}')
        block.append('( ---------- cut-here ---------- )')
    if block:
        del block[-1]
    if not block:
        block = None
    return block","for segment in contour:
    block.append(f'g{gtype} x{segment[0]:f} y{segment[1]:f} z{segment[2]:f}')
    gtype = 1","[""for segment in contour:\n    (segment_0, segment_1, segment_2, *_) = segment\n    block.append(f'g{gtype} x{segment_0:f} y{segment_1:f} z{segment_2:f}')\n    gtype = 1"", ""for (segment_0, segment_1, segment_2, *segment_len) in contour:\n    block.append(f'g{gtype} x{segment_0:f} y{segment_1:f} z{segment_2:f}')\n    gtype = 1""]",no_found,0
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(sound_dir, annotation_dir, target_dir, mode, speaker_info, new_data_dir, speaker_details, text_format):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)
    segments = open(os.path.join(target_dir, 'segments'), 'w', encoding='utf-8')
    wavscp = open(os.path.join(target_dir, 'wav.scp'), 'w', encoding='utf-8')
    utt2spk = open(os.path.join(target_dir, 'utt2spk'), 'w', encoding='utf-8')
    spk2utt = open(os.path.join(target_dir, 'spk2utt'), 'w', encoding='utf-8')
    text = open(os.path.join(target_dir, 'text'), 'w', encoding='utf-8')
    name2spk = open(os.path.join(target_dir, 'name2spk'), 'w', encoding='utf-8')
    remix_script = open(os.path.join(target_dir, 'remix_script.sh'), 'w', encoding='utf-8')
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}
    if mode == 'trs':
        if not os.path.exists(os.path.join(target_dir, 'temp')):
            os.mkdir(os.path.join(target_dir, 'temp'))
        audio_set = set()
        for (root, dirs, files) in os.walk(sound_dir):
            for file in files:
                if file[-4:] == '.wav':
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for (root, dirs, files) in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == '.trs':
                    XMLRefine(os.path.join(root, file), os.path.join(target_dir, 'temp', file))
                    annotation_files[file] = os.path.join(target_dir, 'temp', file)
        for afile in annotation_files.keys():
            if afile == 'error':
                continue
            try:
                (audio_name, speakers, segment_info) = XMLProcessing(annotation_files[afile])
            except Exception:
                print('error process %s' % annotation_files[afile])
            audio_name = audio_name.replace(' ', '')
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if '%s.wav' % audio_name not in sound_files.keys():
                print('no audio found for annotation: %s' % afile)
                continue
            print('%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |' % (audio_name, sound_files['%s.wav' % audio_name]), file=wavscp)
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker]['name']] = name2spk_prep.get(speakers[speaker]['name'], spk_id)
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker]['name']]
                if name2spk_prep[speakers[speaker]['name']] == spk_id:
                    print('%s %s' % (speakers[speaker]['name'], PackZero(spk_id)), file=name2spk)
                    spk_id += 1
            for segment in segment_info:
                if segment[0] == 'None':
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = '%s_%s_%s' % (PackZero(spk), audio_name, PackZero(segment_number))
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print('warning segment %s in %s' % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue
                print('%s %s %s %s' % (segment_id, audio_name, segment[2], segment[3]), file=segments)
                print('%s %s' % (segment_id, PackZero(spk)), file=utt2spk)
                print('%s %s' % (segment_id, segment[1]), file=text)
                spk2utt_prep[spk] = spk2utt_prep.get(spk, '') + ' %s' % segment_id
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print('%s %s' % (spk, spk2utt_prep[spk]), file=spk2utt)
            print('successfully processing %s' % afile)
        shutil.rmtree(os.path.join(target_dir, 'temp'))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for (root, dirs, files) in os.walk(sound_dir):
            for file in files:
                if file[-4:] == '.wav':
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)
        for (root, dirs, files) in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == '.eaf':
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == 'error':
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            (left_channel_segments, right_channel_segments) = segment_info
            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print('sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1' % (sound_files[afile], os.path.join(new_data_dir, afile)), file=remix_script)
            print('%s-L %s-L.wav' % (afile, os.path.join(new_data_dir, afile)), file=wavscp)
            segment_number = 0
            for segment in left_channel_segments:
                segment_id = '%s_%s-L_%s' % (spk_info[0], afile, PackZero(segment_number))
                if float(segment[1]) > max_length:
                    continue
                print('%s %s-L %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
                print('%s %s' % (segment_id, spk_info[0]), file=utt2spk)
                print('%s %s' % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(spk_info[0], '') + ' %s' % segment_id
                segment_number += 1
            if len(right_channel_segments) > 0:
                print('sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2' % (sound_files[afile], os.path.join(new_data_dir, afile)), file=remix_script)
                print('%s-R %s-R.wav' % (afile, os.path.join(new_data_dir, afile)), file=wavscp)
                for segment in right_channel_segments:
                    segment_id = '%s_%s-R_%s' % (spk_info[1], afile, PackZero(segment_number))
                    if float(segment[1]) > max_length:
                        continue
                    print('%s %s-R %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
                    print('%s %s' % (segment_id, spk_info[1]), file=utt2spk)
                    print('%s %s' % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(spk_info[1], '') + ' %s' % segment_id
                    segment_number += 1
            print('successfully processing %s' % afile)
        for spk in spk2utt_prep.keys():
            print('%s %s' % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for segment in left_channel_segments:
    segment_id = '%s_%s-L_%s' % (spk_info[0], afile, PackZero(segment_number))
    if float(segment[1]) > max_length:
        continue
    print('%s %s-L %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
    print('%s %s' % (segment_id, spk_info[0]), file=utt2spk)
    print('%s %s' % (segment_id, segment[2]), file=text)
    spk2utt_prep[spk_info[0]] = spk2utt_prep.get(spk_info[0], '') + ' %s' % segment_id
    segment_number += 1","[""for segment in left_channel_segments:\n    (segment_0, segment_1, segment_2, *_) = segment\n    segment_id = '%s_%s-L_%s' % (spk_info[0], afile, PackZero(segment_number))\n    if float(segment_1) > max_length:\n        continue\n    print('%s %s-L %s %s' % (segment_id, afile, segment_0, segment_1), file=segments)\n    print('%s %s' % (segment_id, spk_info[0]), file=utt2spk)\n    print('%s %s' % (segment_id, segment_2), file=text)\n    spk2utt_prep[spk_info[0]] = spk2utt_prep.get(spk_info[0], '') + ' %s' % segment_id\n    segment_number += 1"", ""for (segment_0, segment_1, segment_2, *segment_len) in left_channel_segments:\n    segment_id = '%s_%s-L_%s' % (spk_info[0], afile, PackZero(segment_number))\n    if float(segment_1) > max_length:\n        continue\n    print('%s %s-L %s %s' % (segment_id, afile, segment_0, segment_1), file=segments)\n    print('%s %s' % (segment_id, spk_info[0]), file=utt2spk)\n    print('%s %s' % (segment_id, segment_2), file=text)\n    spk2utt_prep[spk_info[0]] = spk2utt_prep.get(spk_info[0], '') + ' %s' % segment_id\n    segment_number += 1""]",no_found,0
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(sound_dir, annotation_dir, target_dir, mode, speaker_info, new_data_dir, speaker_details, text_format):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)
    segments = open(os.path.join(target_dir, 'segments'), 'w', encoding='utf-8')
    wavscp = open(os.path.join(target_dir, 'wav.scp'), 'w', encoding='utf-8')
    utt2spk = open(os.path.join(target_dir, 'utt2spk'), 'w', encoding='utf-8')
    spk2utt = open(os.path.join(target_dir, 'spk2utt'), 'w', encoding='utf-8')
    text = open(os.path.join(target_dir, 'text'), 'w', encoding='utf-8')
    name2spk = open(os.path.join(target_dir, 'name2spk'), 'w', encoding='utf-8')
    remix_script = open(os.path.join(target_dir, 'remix_script.sh'), 'w', encoding='utf-8')
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}
    if mode == 'trs':
        if not os.path.exists(os.path.join(target_dir, 'temp')):
            os.mkdir(os.path.join(target_dir, 'temp'))
        audio_set = set()
        for (root, dirs, files) in os.walk(sound_dir):
            for file in files:
                if file[-4:] == '.wav':
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for (root, dirs, files) in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == '.trs':
                    XMLRefine(os.path.join(root, file), os.path.join(target_dir, 'temp', file))
                    annotation_files[file] = os.path.join(target_dir, 'temp', file)
        for afile in annotation_files.keys():
            if afile == 'error':
                continue
            try:
                (audio_name, speakers, segment_info) = XMLProcessing(annotation_files[afile])
            except Exception:
                print('error process %s' % annotation_files[afile])
            audio_name = audio_name.replace(' ', '')
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if '%s.wav' % audio_name not in sound_files.keys():
                print('no audio found for annotation: %s' % afile)
                continue
            print('%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |' % (audio_name, sound_files['%s.wav' % audio_name]), file=wavscp)
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker]['name']] = name2spk_prep.get(speakers[speaker]['name'], spk_id)
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker]['name']]
                if name2spk_prep[speakers[speaker]['name']] == spk_id:
                    print('%s %s' % (speakers[speaker]['name'], PackZero(spk_id)), file=name2spk)
                    spk_id += 1
            for segment in segment_info:
                if segment[0] == 'None':
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = '%s_%s_%s' % (PackZero(spk), audio_name, PackZero(segment_number))
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print('warning segment %s in %s' % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue
                print('%s %s %s %s' % (segment_id, audio_name, segment[2], segment[3]), file=segments)
                print('%s %s' % (segment_id, PackZero(spk)), file=utt2spk)
                print('%s %s' % (segment_id, segment[1]), file=text)
                spk2utt_prep[spk] = spk2utt_prep.get(spk, '') + ' %s' % segment_id
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print('%s %s' % (spk, spk2utt_prep[spk]), file=spk2utt)
            print('successfully processing %s' % afile)
        shutil.rmtree(os.path.join(target_dir, 'temp'))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for (root, dirs, files) in os.walk(sound_dir):
            for file in files:
                if file[-4:] == '.wav':
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)
        for (root, dirs, files) in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == '.eaf':
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == 'error':
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            (left_channel_segments, right_channel_segments) = segment_info
            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print('sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1' % (sound_files[afile], os.path.join(new_data_dir, afile)), file=remix_script)
            print('%s-L %s-L.wav' % (afile, os.path.join(new_data_dir, afile)), file=wavscp)
            segment_number = 0
            for segment in left_channel_segments:
                segment_id = '%s_%s-L_%s' % (spk_info[0], afile, PackZero(segment_number))
                if float(segment[1]) > max_length:
                    continue
                print('%s %s-L %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
                print('%s %s' % (segment_id, spk_info[0]), file=utt2spk)
                print('%s %s' % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(spk_info[0], '') + ' %s' % segment_id
                segment_number += 1
            if len(right_channel_segments) > 0:
                print('sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2' % (sound_files[afile], os.path.join(new_data_dir, afile)), file=remix_script)
                print('%s-R %s-R.wav' % (afile, os.path.join(new_data_dir, afile)), file=wavscp)
                for segment in right_channel_segments:
                    segment_id = '%s_%s-R_%s' % (spk_info[1], afile, PackZero(segment_number))
                    if float(segment[1]) > max_length:
                        continue
                    print('%s %s-R %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
                    print('%s %s' % (segment_id, spk_info[1]), file=utt2spk)
                    print('%s %s' % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(spk_info[1], '') + ' %s' % segment_id
                    segment_number += 1
            print('successfully processing %s' % afile)
        for spk in spk2utt_prep.keys():
            print('%s %s' % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for segment in right_channel_segments:
    segment_id = '%s_%s-R_%s' % (spk_info[1], afile, PackZero(segment_number))
    if float(segment[1]) > max_length:
        continue
    print('%s %s-R %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
    print('%s %s' % (segment_id, spk_info[1]), file=utt2spk)
    print('%s %s' % (segment_id, segment[2]), file=text)
    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(spk_info[1], '') + ' %s' % segment_id
    segment_number += 1","[""for segment in right_channel_segments:\n    (segment_0, segment_1, segment_2, *_) = segment\n    segment_id = '%s_%s-R_%s' % (spk_info[1], afile, PackZero(segment_number))\n    if float(segment_1) > max_length:\n        continue\n    print('%s %s-R %s %s' % (segment_id, afile, segment_0, segment_1), file=segments)\n    print('%s %s' % (segment_id, spk_info[1]), file=utt2spk)\n    print('%s %s' % (segment_id, segment_2), file=text)\n    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(spk_info[1], '') + ' %s' % segment_id\n    segment_number += 1"", ""for (segment_0, segment_1, segment_2, *segment_len) in right_channel_segments:\n    segment_id = '%s_%s-R_%s' % (spk_info[1], afile, PackZero(segment_number))\n    if float(segment_1) > max_length:\n        continue\n    print('%s %s-R %s %s' % (segment_id, afile, segment_0, segment_1), file=segments)\n    print('%s %s' % (segment_id, spk_info[1]), file=utt2spk)\n    print('%s %s' % (segment_id, segment_2), file=text)\n    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(spk_info[1], '') + ' %s' % segment_id\n    segment_number += 1""]",no_found,0
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(sound_dir, annotation_dir, target_dir, mode, speaker_info, new_data_dir, speaker_details, text_format):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)
    segments = open(os.path.join(target_dir, 'segments'), 'w', encoding='utf-8')
    wavscp = open(os.path.join(target_dir, 'wav.scp'), 'w', encoding='utf-8')
    utt2spk = open(os.path.join(target_dir, 'utt2spk'), 'w', encoding='utf-8')
    spk2utt = open(os.path.join(target_dir, 'spk2utt'), 'w', encoding='utf-8')
    text = open(os.path.join(target_dir, 'text'), 'w', encoding='utf-8')
    name2spk = open(os.path.join(target_dir, 'name2spk'), 'w', encoding='utf-8')
    remix_script = open(os.path.join(target_dir, 'remix_script.sh'), 'w', encoding='utf-8')
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}
    if mode == 'trs':
        if not os.path.exists(os.path.join(target_dir, 'temp')):
            os.mkdir(os.path.join(target_dir, 'temp'))
        audio_set = set()
        for (root, dirs, files) in os.walk(sound_dir):
            for file in files:
                if file[-4:] == '.wav':
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for (root, dirs, files) in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == '.trs':
                    XMLRefine(os.path.join(root, file), os.path.join(target_dir, 'temp', file))
                    annotation_files[file] = os.path.join(target_dir, 'temp', file)
        for afile in annotation_files.keys():
            if afile == 'error':
                continue
            try:
                (audio_name, speakers, segment_info) = XMLProcessing(annotation_files[afile])
            except Exception:
                print('error process %s' % annotation_files[afile])
            audio_name = audio_name.replace(' ', '')
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if '%s.wav' % audio_name not in sound_files.keys():
                print('no audio found for annotation: %s' % afile)
                continue
            print('%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |' % (audio_name, sound_files['%s.wav' % audio_name]), file=wavscp)
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker]['name']] = name2spk_prep.get(speakers[speaker]['name'], spk_id)
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker]['name']]
                if name2spk_prep[speakers[speaker]['name']] == spk_id:
                    print('%s %s' % (speakers[speaker]['name'], PackZero(spk_id)), file=name2spk)
                    spk_id += 1
            for segment in segment_info:
                if segment[0] == 'None':
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = '%s_%s_%s' % (PackZero(spk), audio_name, PackZero(segment_number))
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print('warning segment %s in %s' % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue
                print('%s %s %s %s' % (segment_id, audio_name, segment[2], segment[3]), file=segments)
                print('%s %s' % (segment_id, PackZero(spk)), file=utt2spk)
                print('%s %s' % (segment_id, segment[1]), file=text)
                spk2utt_prep[spk] = spk2utt_prep.get(spk, '') + ' %s' % segment_id
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print('%s %s' % (spk, spk2utt_prep[spk]), file=spk2utt)
            print('successfully processing %s' % afile)
        shutil.rmtree(os.path.join(target_dir, 'temp'))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for (root, dirs, files) in os.walk(sound_dir):
            for file in files:
                if file[-4:] == '.wav':
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)
        for (root, dirs, files) in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == '.eaf':
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == 'error':
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            (left_channel_segments, right_channel_segments) = segment_info
            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print('sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1' % (sound_files[afile], os.path.join(new_data_dir, afile)), file=remix_script)
            print('%s-L %s-L.wav' % (afile, os.path.join(new_data_dir, afile)), file=wavscp)
            segment_number = 0
            for segment in left_channel_segments:
                segment_id = '%s_%s-L_%s' % (spk_info[0], afile, PackZero(segment_number))
                if float(segment[1]) > max_length:
                    continue
                print('%s %s-L %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
                print('%s %s' % (segment_id, spk_info[0]), file=utt2spk)
                print('%s %s' % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(spk_info[0], '') + ' %s' % segment_id
                segment_number += 1
            if len(right_channel_segments) > 0:
                print('sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2' % (sound_files[afile], os.path.join(new_data_dir, afile)), file=remix_script)
                print('%s-R %s-R.wav' % (afile, os.path.join(new_data_dir, afile)), file=wavscp)
                for segment in right_channel_segments:
                    segment_id = '%s_%s-R_%s' % (spk_info[1], afile, PackZero(segment_number))
                    if float(segment[1]) > max_length:
                        continue
                    print('%s %s-R %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
                    print('%s %s' % (segment_id, spk_info[1]), file=utt2spk)
                    print('%s %s' % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(spk_info[1], '') + ' %s' % segment_id
                    segment_number += 1
            print('successfully processing %s' % afile)
        for spk in spk2utt_prep.keys():
            print('%s %s' % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for segment in segment_info:
    if segment[0] == 'None':
        spk = spk_id
        spk_id += 1
    else:
        spk = temp_speaker_id[segment[0]]
    segment_id = '%s_%s_%s' % (PackZero(spk), audio_name, PackZero(segment_number))
    skip = False
    for seg in segment:
        if len(seg) < 1:
            print('warning segment %s in %s' % (segment_id, audio_name))
            skip = True
    if skip:
        continue
    print('%s %s %s %s' % (segment_id, audio_name, segment[2], segment[3]), file=segments)
    print('%s %s' % (segment_id, PackZero(spk)), file=utt2spk)
    print('%s %s' % (segment_id, segment[1]), file=text)
    spk2utt_prep[spk] = spk2utt_prep.get(spk, '') + ' %s' % segment_id
    segment_number += 1","[""for segment in segment_info:\n    (segment_0, segment_1, segment_2, segment_3, *_) = segment\n    if segment_0 == 'None':\n        spk = spk_id\n        spk_id += 1\n    else:\n        spk = temp_speaker_id[segment_0]\n    segment_id = '%s_%s_%s' % (PackZero(spk), audio_name, PackZero(segment_number))\n    skip = False\n    for seg in segment:\n        if len(seg) < 1:\n            print('warning segment %s in %s' % (segment_id, audio_name))\n            skip = True\n    if skip:\n        continue\n    print('%s %s %s %s' % (segment_id, audio_name, segment_2, segment_3), file=segments)\n    print('%s %s' % (segment_id, PackZero(spk)), file=utt2spk)\n    print('%s %s' % (segment_id, segment_1), file=text)\n    spk2utt_prep[spk] = spk2utt_prep.get(spk, '') + ' %s' % segment_id\n    segment_number += 1""]",no_found,0
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
    dbg.createLogWindow()
    global currentArgs
    currentArgs = copy.copy(args)
    try:
        starttime = datetime.datetime.now()
        ptr_counter = 0
        commands = {}

        def getBanner():
            banners = {}
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |                         __               __                      |\n'
            bannertext += '    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n'
            bannertext += '    |  / ___/ __ \\/ ___/ _ \\/ / __ `/ __ \\   / __/ _ \\/ __ `/ __ `__ \\ |\n'
            bannertext += '    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n'
            bannertext += '    | \\___/\\____/_/   \\___/_/\\__,_/_/ /_/   \\__/\\___/\\__,_/_/ /_/ /_/  |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |     https://www.corelan.be | https://www.corelan-training.com    |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[0] = bannertext
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n'
            bannertext += ""    |       | '_ ` _ \\  / _ \\ | '_ \\  / _` |   | '_ \\ | | | |          |\n""
            bannertext += '    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n'
            bannertext += '    |       |_| |_| |_| \\___/ |_| |_| \\__,_|(_)| .__/  \\__, |          |\n'
            bannertext += '    |                                          |_|     |___/           |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[1] = bannertext
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |    _____ ___  ____  ____  ____ _                                 |\n'
            bannertext += '    |    / __ `__ \\/ __ \\/ __ \\/ __ `/  https://www.corelan.be         |\n'
            bannertext += '    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n'
            bannertext += '    |  /_/ /_/ /_/\\____/_/ /_/\\__,_/  #corelan (Freenode IRC)          |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[2] = bannertext
            bannertext = ''
            bannertext += '\n    .##.....##..#######..##....##....###........########..##....##\n'
            bannertext += '    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n'
            bannertext += '    .####.####.##.....##.####..##..##...##......##.....##...####..\n'
            bannertext += '    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n'
            bannertext += '    .##.....##.##.....##.##..####.#########.....##...........##...\n'
            bannertext += '    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n'
            bannertext += '    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n'
            banners[3] = bannertext
            bannerlist = []
            for i in range(0, len(banners)):
                bannerlist.append(i)
            random.shuffle(bannerlist)
            return banners[bannerlist[0]]

        def procHelp(args):
            dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__, str(arch)))
            dbg.log('     Plugin version : %s r%s' % (__VERSION__, __REV__))
            dbg.log('     Python version : %s' % getPythonVersion())
            if __DEBUGGERAPP__ == 'WinDBG':
                pykdversion = dbg.getPyKDVersionNr()
                dbg.log('     PyKD version %s' % pykdversion)
            dbg.log('     Written by Corelan - https://www.corelan.be')
            dbg.log('     Project page : https://github.com/corelan/mona')
            dbg.logLines(getBanner(), highlight=1)
            dbg.log('Global options :')
            dbg.log('----------------')
            dbg.log('You can use one or more of the following global options on any command that will perform')
            dbg.log('a search in one or more modules, returning a list of pointers :')
            dbg.log(' -n                     : Skip modules that start with a null byte. If this is too broad, use')
            dbg.log('                          option -cp nonull instead')
            dbg.log(' -o                     : Ignore OS modules')
            dbg.log(' -p <nr>                : Stop search after <nr> pointers.')
            dbg.log(' -m <module,module,...> : only query the given modules. Be sure what you are doing !')
            dbg.log('                          You can specify multiple modules (comma separated)')
            dbg.log('                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored')
            dbg.log('                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,')
            dbg.log('                          blah or *blah* = contains blah')
            dbg.log(' -cm <crit,crit,...>    : Apply some additional criteria to the modules to query.')
            dbg.log('                          You can use one or more of the following criteria :')
            dbg.log('                          aslr,safeseh,rebase,nx,os')
            dbg.log('                          You can enable or disable a certain criterium by setting it to true or false')
            dbg.log('                          Example :  -cm aslr=true,safeseh=false')
            dbg.log('                          Suppose you want to search for p/p/r in aslr enabled modules, you could call')
            dbg.log('                          !mona seh -cm aslr')
            dbg.log(' -cp <crit,crit,...>    : Apply some criteria to the pointers to return')
            dbg.log('                          Available options are :')
            dbg.log('                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev')
            dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
            dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
            dbg.log('                          You can use .. to indicate a range of bytes (in between 2 bad chars)')
            dbg.log(' -x <access>            : Specify desired access level of the returning pointers. If not specified,')
            dbg.log('                          only executable pointers will be returned.')
            dbg.log('                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *')
            if not args:
                args = []
            if len(args) > 1:
                thiscmd = args[1].lower().strip()
                if thiscmd in commands:
                    dbg.log('')
                    dbg.log(""Usage of command '%s' :"" % thiscmd)
                    dbg.log('%s' % ('-' * (22 + len(thiscmd))))
                    dbg.logLines(commands[thiscmd].usage)
                    dbg.log('')
                else:
                    aliasfound = False
                    for cmd in commands:
                        if commands[cmd].alias == thiscmd:
                            dbg.log('')
                            dbg.log(""Usage of command '%s' :"" % thiscmd)
                            dbg.log('%s' % ('-' * (22 + len(thiscmd))))
                            dbg.logLines(commands[cmd].usage)
                            dbg.log('')
                            aliasfound = True
                    if not aliasfound:
                        dbg.logLines('\nCommand %s does not exist. Run !mona to get a list of available commands\n' % thiscmd, highlight=1)
            else:
                dbg.logLines('\nUsage :')
                dbg.logLines('-------\n')
                dbg.log(' !mona <command> <parameter>')
                dbg.logLines('\nAvailable commands and parameters :\n')
                items = commands.items()
                items.sort(key=itemgetter(0))
                for item in items:
                    if commands[item[0]].usage != '':
                        aliastxt = ''
                        if commands[item[0]].alias != '':
                            aliastxt = ' / ' + commands[item[0]].alias
                        dbg.logLines('%s | %s' % (item[0] + aliastxt + ' ' * (20 - len(item[0] + aliastxt)), commands[item[0]].description))
                dbg.log('')
                dbg.log('Want more info about a given command ?  Run !mona help <command>', highlight=1)
                dbg.log('')
        commands['help'] = MnCommand('help', 'show help', '!mona help [command]', procHelp)

        def procConfig(args):
            showerror = False
            if not 'set' in args and (not 'get' in args) and (not 'add' in args):
                showerror = True
            if 'set' in args:
                if type(args['set']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['set'].split(' ')
                    if len(params) < 2:
                        showerror = True
            if 'add' in args:
                if type(args['add']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['add'].split(' ')
                    if len(params) < 2:
                        showerror = True
            if 'get' in args:
                if type(args['get']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['get'].split(' ')
                    if len(params) < 1:
                        showerror = True
            if showerror:
                dbg.log('Usage :')
                dbg.logLines(configUsage, highlight=1)
                return
            else:
                if 'get' in args:
                    dbg.log('Reading value from configuration file')
                    monaConfig = MnConfig()
                    thevalue = monaConfig.get(args['get'])
                    dbg.log('Parameter %s = %s' % (args['get'], thevalue))
                if 'set' in args:
                    dbg.log('Writing value to configuration file')
                    monaConfig = MnConfig()
                    value = args['set'].split(' ')
                    configparam = value[0].strip()
                    dbg.log('Old value of parameter %s = %s' % (configparam, monaConfig.get(configparam)))
                    configvalue = args['set'][0 + len(configparam):len(args['set'])]
                    monaConfig.set(configparam, configvalue)
                    dbg.log('New value of parameter %s = %s' % (configparam, configvalue))
                if 'add' in args:
                    dbg.log('Writing value to configuration file')
                    monaConfig = MnConfig()
                    value = args['add'].split(' ')
                    configparam = value[0].strip()
                    dbg.log('Old value of parameter %s = %s' % (configparam, monaConfig.get(configparam)))
                    configvalue = monaConfig.get(configparam).strip() + ',' + args['add'][0 + len(configparam):len(args['add'])].strip()
                    monaConfig.set(configparam, configvalue)
                    dbg.log('New value of parameter %s = %s' % (configparam, configvalue))

        def procFindJ(args):
            return procFindJMP(args)

        def procFindJMP(args):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            if inspect.stack()[1][3] == 'procFindJ':
                dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."", highlight=1)
            criteria = {}
            all_opcodes = {}
            global ptr_to_get
            ptr_to_get = -1
            distancestr = ''
            mindistance = 0
            maxdistance = 0
            showerror = False
            if 'r' in args:
                if type(args['r']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    thisreg = args['r'].upper().strip()
                    validregs = dbglib.Registers32BitsOrder
                    if not thisreg in validregs:
                        showerror = True
            else:
                showerror = True
            if 'distance' in args:
                if type(args['distance']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    distancestr = args['distance']
                    distanceparts = distancestr.split(',')
                    for parts in distanceparts:
                        valueparts = parts.split('=')
                        if len(valueparts) > 1:
                            if valueparts[0].lower() == 'min':
                                try:
                                    mindistance = int(valueparts[1])
                                except:
                                    mindistance = 0
                            if valueparts[0].lower() == 'max':
                                try:
                                    maxdistance = int(valueparts[1])
                                except:
                                    maxdistance = 0
            if maxdistance < mindistance:
                tmp = maxdistance
                maxdistance = mindistance
                mindistance = tmp
            criteria['mindistance'] = mindistance
            criteria['maxdistance'] = maxdistance
            if showerror:
                dbg.log('Usage :')
                dbg.logLines(jmpUsage, highlight=1)
                return
            else:
                (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
                all_opcodes = findJMP(modulecriteria, criteria, args['r'].lower().strip())
            logfile = MnLog('jmp.txt')
            thislog = logfile.reset()
            processResults(all_opcodes, logfile, thislog)

        def procFindSEH(args):
            modulecriteria = {}
            modulecriteria['safeseh'] = False
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            criteria = {}
            specialcases = {}
            all_opcodes = {}
            global ptr_to_get
            ptr_to_get = -1
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            if 'rop' in args:
                criteria['rop'] = True
            if 'all' in args:
                criteria['all'] = True
                specialcases['maponly'] = True
            else:
                criteria['all'] = False
                specialcases['maponly'] = False
            all_opcodes = findSEH(modulecriteria, criteria)
            logfile = MnLog('seh.txt')
            thislog = logfile.reset()
            processResults(all_opcodes, logfile, thislog, specialcases)

        def procShowMODULES(args):
            modulecriteria = {}
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            modulestosearch = getModulesToQuery(modulecriteria)
            showModuleTable('', modulestosearch)

        def procFindROPFUNC(args):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            ropfuncs = {}
            ropfuncoffsets = {}
            (ropfuncs, ropfuncoffsets) = findROPFUNC(modulecriteria, criteria)
            dbg.log('[+] Processing pointers to interesting rop functions')
            logfile = MnLog('ropfunc.txt')
            thislog = logfile.reset()
            processResults(ropfuncs, logfile, thislog)
            global silent
            silent = True
            dbg.log('[+] Processing offsets to pointers to interesting rop functions')
            logfile = MnLog('ropfunc_offset.txt')
            thislog = logfile.reset()
            processResults(ropfuncoffsets, logfile, thislog)

        def procStackPivots(args):
            procROP(args, 'stackpivot')

        def procROP(args, mode='all'):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            depth = 6
            maxoffset = 40
            thedistance = 8
            split = False
            fast = False
            sortedprint = False
            endingstr = ''
            endings = []
            technique = ''
            if 'depth' in args:
                if type(args['depth']).__name__.lower() != 'bool':
                    try:
                        depth = int(args['depth'])
                    except:
                        pass
            if 'offset' in args:
                if type(args['offset']).__name__.lower() != 'bool':
                    try:
                        maxoffset = int(args['offset'])
                    except:
                        pass
            if 'distance' in args:
                if type(args['distance']).__name__.lower() != 'bool':
                    try:
                        thedistance = args['distance']
                    except:
                        pass
            if 'split' in args:
                if type(args['split']).__name__.lower() == 'bool':
                    split = args['split']
            if 's' in args:
                if type(args['s']).__name__.lower() != 'bool':
                    technique = args['s'].replace(""'"", '').replace('""', '').strip().lower()
            if 'fast' in args:
                if type(args['fast']).__name__.lower() == 'bool':
                    fast = args['fast']
            if 'end' in args:
                if type(args['end']).__name__.lower() == 'str':
                    endingstr = args['end'].replace(""'"", '').replace('""', '').strip()
                    endings = endingstr.split('#')
            if 'f' in args:
                if args['f'] != '':
                    criteria['f'] = args['f']
            if 'sort' in args:
                sortedprint = True
            if 'rva' in args:
                criteria['rva'] = True
            if mode == 'stackpivot':
                fast = False
                endings = ''
                split = False
            else:
                mode = 'all'
            findROPGADGETS(modulecriteria, criteria, endings, maxoffset, depth, split, thedistance, fast, mode, sortedprint, technique)

        def procJseh(args):
            results = []
            showred = 0
            showall = False
            if 'all' in args:
                showall = True
            nrfound = 0
            dbg.log('-----------------------------------------------------------------------')
            dbg.log('Search for jmp/call dword[ebp/esp+nn] (and other) combinations started ')
            dbg.log('-----------------------------------------------------------------------')
            opcodej = ['T$\x08', 'd$\x08', 'T$\x14', 'T$\x14', 'T$\x1c', 'T$\x1c', 'T$,', 'T$,', 'T$D', 'T$D', 'T$P', 'T$P', 'U\x0c', 'e\x0c', 'U$', 'e$', 'U0', 'e0', 'U', 'e', 'U', 'e', 'U', 'e', '\x83\x08', '\x83\x08']
            fakeptrcriteria = {}
            fakeptrcriteria['accesslevel'] = '*'
            for opjc in opcodej:
                addys = []
                addys = searchInRange([[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
                results += addys
                for ptrtypes in addys:
                    for ad1 in addys[ptrtypes]:
                        ptr = MnPointer(ad1)
                        module = ptr.belongsTo()
                        if not module:
                            module = ''
                            page = dbg.getMemoryPageByAddress(ad1)
                            access = page.getAccess(human=True)
                            op = dbg.disasm(ad1)
                            opstring = op.getDisasm()
                            dbg.log('Found %s at 0x%08x - Access: (%s) - Outside of a loaded module' % (opstring, ad1, access), address=ad1, highlight=1)
                            nrfound += 1
                        elif showall:
                            page = dbg.getMemoryPageByAddress(ad1)
                            access = page.getAccess(human=True)
                            op = dbg.disasm(ad1)
                            opstring = op.getDisasm()
                            thismod = MnModule(module)
                            if not thismod.isSafeSEH:
                                extratext = '=== Safeseh : NO ==='
                                showred = 1
                            else:
                                extratext = 'Safeseh protected'
                                showred = 0
                            dbg.log('Found %s at 0x%08x (%s) - Access: (%s) - %s' % (opstring, ad1, module, access, extratext), address=ad1, highlight=showred)
                            nrfound += 1
            dbg.log('Search complete')
            if results:
                dbg.log('Found %d address(es)' % nrfound)
                return 'Found %d address(es) (Check the log Windows for details)' % nrfound
            else:
                dbg.log('No addresses found')
                return 'Sorry, no addresses found'

        def procJOP(args, mode='all'):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            depth = 6
            if 'depth' in args:
                if type(args['depth']).__name__.lower() != 'bool':
                    try:
                        depth = int(args['depth'])
                    except:
                        pass
            findJOPGADGETS(modulecriteria, criteria, depth)

        def procCreatePATTERN(args):
            size = 0
            pattern = ''
            if '?' in args and args['?'] != '':
                try:
                    if '0x' in args['?'].lower():
                        try:
                            size = int(args['?'], 16)
                        except:
                            size = 0
                    else:
                        size = int(args['?'])
                except:
                    size = 0
            if size == 0:
                dbg.log('Please enter a valid size', highlight=1)
            else:
                pattern = createPattern(size, args)
                dbg.log('Creating cyclic pattern of %d bytes' % size)
                dbg.log(pattern)
                global ignoremodules
                ignoremodules = True
                objpatternfile = MnLog('pattern.txt')
                patternfile = objpatternfile.reset()
                objpatternfile.write('\nPattern of ' + str(size) + ' bytes :\n', patternfile)
                objpatternfile.write('-' * (19 + len(str(size))), patternfile)
                objpatternfile.write('\nASCII:', patternfile)
                objpatternfile.write('\n' + pattern, patternfile)
                patternhex = ''
                for patternchar in pattern:
                    patternhex += str(hex(ord(patternchar))).replace('0x', '\\x')
                objpatternfile.write('\n\nHEX:\n', patternfile)
                objpatternfile.write(patternhex, patternfile)
                patternjs = str2js(pattern)
                objpatternfile.write('\n\nJAVASCRIPT (unescape() friendly):\n', patternfile)
                objpatternfile.write(patternjs, patternfile)
                if not silent:
                    dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"", highlight=1)
                    dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile, highlight=1)
                ignoremodules = False
            return

        def procOffsetPATTERN(args):
            egg = ''
            if '?' in args and args['?'] != '':
                try:
                    egg = args['?']
                except:
                    egg = ''
            if egg == '':
                dbg.log('Please enter a valid target', highlight=1)
            else:
                findOffsetInPattern(egg, -1, args)
            return

        def procFileCOMPARE(args):
            modulecriteria = {}
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            allfiles = []
            tomatch = ''
            checkstrict = True
            rangeval = 0
            fast = False
            if 'ptronly' in args or 'ptrsonly' in args:
                fast = True
            if 'f' in args:
                if args['f'] != '':
                    rawfilenames = args['f'].replace('""', '')
                    allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
                    dbg.log('[+] Number of files to be examined : %d ' % len(allfiles))
            if 'range' in args:
                if not type(args['range']).__name__.lower() == 'bool':
                    strrange = args['range'].lower()
                    if strrange.startswith('0x') and len(strrange) > 2:
                        rangeval = int(strrange, 16)
                    else:
                        try:
                            rangeval = int(args['range'])
                        except:
                            rangeval = 0
                    if rangeval > 0:
                        dbg.log('[+] Find overlap using pointer +/- range, value %d' % rangeval)
                        dbg.log('    Note : this will significantly slow down the comparison process !')
                else:
                    dbg.log('Please provide a numeric value ^(> 0) with option -range', highlight=1)
                    return
            else:
                if 'contains' in args:
                    if type(args['contains']).__name__.lower() == 'str':
                        tomatch = args['contains'].replace(""'"", '').replace('""', '')
                if 'nostrict' in args:
                    if type(args['nostrict']).__name__.lower() == 'bool':
                        checkstrict = not args['nostrict']
                        dbg.log('[+] Instructions must match in all files ? %s' % checkstrict)
            callfiles = allfiles
            allfiles = []
            for tfile in callfiles:
                if os.path.isdir(tfile):
                    for (root, dirs, files) in os.walk(tfile):
                        for dfile in files:
                            allfiles.append(os.path.join(root, dfile))
                else:
                    allfiles.append(tfile)
            if len(allfiles) > 1:
                findFILECOMPARISON(modulecriteria, criteria, allfiles, tomatch, checkstrict, rangeval, fast)
            else:
                dbg.log('Please specify at least 2 filenames to compare', highlight=1)

        def procFind(args):
            modulecriteria = {}
            criteria = {}
            pattern = ''
            base = 0
            offset = 0
            top = TOP_USERLAND
            consecutive = False
            ftype = ''
            level = 0
            offsetlevel = 0
            if not 'a' in args:
                args['a'] = '*'
            ptronly = False
            if 'ptronly' in args or 'ptrsonly' in args:
                ptronly = True
            if not 'x' in args:
                args['x'] = '*'
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            if criteria['accesslevel'] == '':
                return
            if not 's' in args:
                dbg.log('-s <search pattern (or filename)> is a mandatory argument', highlight=1)
                return
            pattern = args['s']
            if 'unicode' in args:
                criteria['unic'] = True
            if 'b' in args:
                try:
                    base = int(args['b'], 16)
                except:
                    dbg.log('invalid base address: %s' % args['b'], highlight=1)
                    return
            if 't' in args:
                try:
                    top = int(args['t'], 16)
                except:
                    dbg.log('invalid top address: %s' % args['t'], highlight=1)
                    return
            if 'offset' in args:
                if not args['offset'].__class__.__name__ == 'bool':
                    if '0x' in args['offset'].lower():
                        try:
                            offset = 0 - int(args['offset'], 16)
                        except:
                            dbg.log('invalid offset value', highlight=1)
                            return
                    else:
                        try:
                            offset = 0 - int(args['offset'])
                        except:
                            dbg.log('invalid offset value', highlight=1)
                            return
                else:
                    dbg.log('invalid offset value', highlight=1)
                    return
            if 'level' in args:
                try:
                    level = int(args['level'])
                except:
                    dbg.log('invalid level value', highlight=1)
                    return
            if 'offsetlevel' in args:
                try:
                    offsetlevel = int(args['offsetlevel'])
                except:
                    dbg.log('invalid offsetlevel value', highlight=1)
                    return
            if 'c' in args:
                dbg.log('    - Skipping consecutive pointers, showing size instead')
                consecutive = True
            if 'type' in args:
                if not args['type'] in ['bin', 'asc', 'ptr', 'instr', 'file']:
                    dbg.log('Invalid search type : %s' % args['type'], highlight=1)
                    return
                ftype = args['type']
                if ftype == 'file':
                    filename = args['s'].replace('""', '').replace(""'"", '')
                    if not os.path.isfile(filename):
                        dbg.log('Unable to find/read file %s' % filename, highlight=1)
                        return
            rangep2p = 0
            if 'p2p' in args or level > 0:
                dbg.log('    - Looking for pointers to pointers')
                criteria['p2p'] = True
                if 'r' in args:
                    try:
                        rangep2p = int(args['r'])
                    except:
                        pass
                    if rangep2p > 0:
                        dbg.log('    - Will search for close pointers (%d bytes backwards)' % rangep2p)
                if 'p2p' in args:
                    level = 1
            if level > 0:
                dbg.log('    - Recursive levels : %d' % level)
            allpointers = findPattern(modulecriteria, criteria, pattern, ftype, base, top, consecutive, rangep2p, level, offset, offsetlevel)
            logfile = MnLog('find.txt')
            thislog = logfile.reset()
            processResults(allpointers, logfile, thislog, {}, ptronly)
            return

        def procFindWild(args):
            modulecriteria = {}
            criteria = {}
            pattern = ''
            patterntype = ''
            base = 0
            top = TOP_USERLAND
            (modulecriteria, criteria) = args2criteria(","for sehrecord in sehchain:
    address = sehrecord[0]
    sehandler = sehrecord[1]
    nseh = ''
    try:
        nsehvalue = struct.unpack('<L', dbg.readMemory(address, 4))[0]
        nseh = '0x%08x' % nsehvalue
    except:
        nseh = '0x????????'
    bpsuccess = True
    try:
        if __DEBUGGERAPP__ == 'WinDBG':
            bpsuccess = dbg.setBreakpoint(sehandler)
        else:
            dbg.setBreakpoint(sehandler)
            bpsuccess = True
    except:
        bpsuccess = False
    bptext = ''
    if not bpsuccess:
        bptext = 'BP failed'
    else:
        bptext = 'BP set'
    ptr = MnPointer(sehandler)
    funcinfo = ptr.getPtrFunction()
    dbg.log('0x%08x  %s  0x%08x %s <- %s' % (address, nseh, sehandler, funcinfo, bptext))","[""for sehrecord in sehchain:\n    (sehrecord_0, sehrecord_1, *_) = sehrecord\n    address = sehrecord_0\n    sehandler = sehrecord_1\n    nseh = ''\n    try:\n        nsehvalue = struct.unpack('<L', dbg.readMemory(address, 4))[0]\n        nseh = '0x%08x' % nsehvalue\n    except:\n        nseh = '0x????????'\n    bpsuccess = True\n    try:\n        if __DEBUGGERAPP__ == 'WinDBG':\n            bpsuccess = dbg.setBreakpoint(sehandler)\n        else:\n            dbg.setBreakpoint(sehandler)\n            bpsuccess = True\n    except:\n        bpsuccess = False\n    bptext = ''\n    if not bpsuccess:\n        bptext = 'BP failed'\n    else:\n        bptext = 'BP set'\n    ptr = MnPointer(sehandler)\n    funcinfo = ptr.getPtrFunction()\n    dbg.log('0x%08x  %s  0x%08x %s <- %s' % (address, nseh, sehandler, funcinfo, bptext))"", ""for (sehrecord_0, sehrecord_1, *sehrecord_len) in sehchain:\n    address = \n    sehrecord_0\n    sehandler = \n    sehrecord_1\n    nseh = ''\n    try:\n        nsehvalue = struct.unpack('<L', dbg.readMemory(address, 4))[0]\n        nseh = '0x%08x' % nsehvalue\n    except:\n        nseh = '0x????????'\n    bpsuccess = True\n    try:\n        if __DEBUGGERAPP__ == 'WinDBG':\n            bpsuccess = dbg.setBreakpoint(sehandler)\n        else:\n            dbg.setBreakpoint(sehandler)\n            bpsuccess = True\n    except:\n        bpsuccess = False\n    bptext = ''\n    if not bpsuccess:\n        bptext = 'BP failed'\n    else:\n        bptext = 'BP set'\n    ptr = MnPointer(sehandler)\n    funcinfo = ptr.getPtrFunction()\n    dbg.log('0x%08x  %s  0x%08x %s <- %s' % (address, nseh, sehandler, funcinfo, bptext))""]",no_found,0
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
    dbg.createLogWindow()
    global currentArgs
    currentArgs = copy.copy(args)
    try:
        starttime = datetime.datetime.now()
        ptr_counter = 0
        commands = {}

        def getBanner():
            banners = {}
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |                         __               __                      |\n'
            bannertext += '    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n'
            bannertext += '    |  / ___/ __ \\/ ___/ _ \\/ / __ `/ __ \\   / __/ _ \\/ __ `/ __ `__ \\ |\n'
            bannertext += '    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n'
            bannertext += '    | \\___/\\____/_/   \\___/_/\\__,_/_/ /_/   \\__/\\___/\\__,_/_/ /_/ /_/  |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |     https://www.corelan.be | https://www.corelan-training.com    |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[0] = bannertext
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n'
            bannertext += ""    |       | '_ ` _ \\  / _ \\ | '_ \\  / _` |   | '_ \\ | | | |          |\n""
            bannertext += '    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n'
            bannertext += '    |       |_| |_| |_| \\___/ |_| |_| \\__,_|(_)| .__/  \\__, |          |\n'
            bannertext += '    |                                          |_|     |___/           |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[1] = bannertext
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |    _____ ___  ____  ____  ____ _                                 |\n'
            bannertext += '    |    / __ `__ \\/ __ \\/ __ \\/ __ `/  https://www.corelan.be         |\n'
            bannertext += '    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n'
            bannertext += '    |  /_/ /_/ /_/\\____/_/ /_/\\__,_/  #corelan (Freenode IRC)          |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[2] = bannertext
            bannertext = ''
            bannertext += '\n    .##.....##..#######..##....##....###........########..##....##\n'
            bannertext += '    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n'
            bannertext += '    .####.####.##.....##.####..##..##...##......##.....##...####..\n'
            bannertext += '    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n'
            bannertext += '    .##.....##.##.....##.##..####.#########.....##...........##...\n'
            bannertext += '    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n'
            bannertext += '    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n'
            banners[3] = bannertext
            bannerlist = []
            for i in range(0, len(banners)):
                bannerlist.append(i)
            random.shuffle(bannerlist)
            return banners[bannerlist[0]]

        def procHelp(args):
            dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__, str(arch)))
            dbg.log('     Plugin version : %s r%s' % (__VERSION__, __REV__))
            dbg.log('     Python version : %s' % getPythonVersion())
            if __DEBUGGERAPP__ == 'WinDBG':
                pykdversion = dbg.getPyKDVersionNr()
                dbg.log('     PyKD version %s' % pykdversion)
            dbg.log('     Written by Corelan - https://www.corelan.be')
            dbg.log('     Project page : https://github.com/corelan/mona')
            dbg.logLines(getBanner(), highlight=1)
            dbg.log('Global options :')
            dbg.log('----------------')
            dbg.log('You can use one or more of the following global options on any command that will perform')
            dbg.log('a search in one or more modules, returning a list of pointers :')
            dbg.log(' -n                     : Skip modules that start with a null byte. If this is too broad, use')
            dbg.log('                          option -cp nonull instead')
            dbg.log(' -o                     : Ignore OS modules')
            dbg.log(' -p <nr>                : Stop search after <nr> pointers.')
            dbg.log(' -m <module,module,...> : only query the given modules. Be sure what you are doing !')
            dbg.log('                          You can specify multiple modules (comma separated)')
            dbg.log('                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored')
            dbg.log('                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,')
            dbg.log('                          blah or *blah* = contains blah')
            dbg.log(' -cm <crit,crit,...>    : Apply some additional criteria to the modules to query.')
            dbg.log('                          You can use one or more of the following criteria :')
            dbg.log('                          aslr,safeseh,rebase,nx,os')
            dbg.log('                          You can enable or disable a certain criterium by setting it to true or false')
            dbg.log('                          Example :  -cm aslr=true,safeseh=false')
            dbg.log('                          Suppose you want to search for p/p/r in aslr enabled modules, you could call')
            dbg.log('                          !mona seh -cm aslr')
            dbg.log(' -cp <crit,crit,...>    : Apply some criteria to the pointers to return')
            dbg.log('                          Available options are :')
            dbg.log('                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev')
            dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
            dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
            dbg.log('                          You can use .. to indicate a range of bytes (in between 2 bad chars)')
            dbg.log(' -x <access>            : Specify desired access level of the returning pointers. If not specified,')
            dbg.log('                          only executable pointers will be returned.')
            dbg.log('                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *')
            if not args:
                args = []
            if len(args) > 1:
                thiscmd = args[1].lower().strip()
                if thiscmd in commands:
                    dbg.log('')
                    dbg.log(""Usage of command '%s' :"" % thiscmd)
                    dbg.log('%s' % ('-' * (22 + len(thiscmd))))
                    dbg.logLines(commands[thiscmd].usage)
                    dbg.log('')
                else:
                    aliasfound = False
                    for cmd in commands:
                        if commands[cmd].alias == thiscmd:
                            dbg.log('')
                            dbg.log(""Usage of command '%s' :"" % thiscmd)
                            dbg.log('%s' % ('-' * (22 + len(thiscmd))))
                            dbg.logLines(commands[cmd].usage)
                            dbg.log('')
                            aliasfound = True
                    if not aliasfound:
                        dbg.logLines('\nCommand %s does not exist. Run !mona to get a list of available commands\n' % thiscmd, highlight=1)
            else:
                dbg.logLines('\nUsage :')
                dbg.logLines('-------\n')
                dbg.log(' !mona <command> <parameter>')
                dbg.logLines('\nAvailable commands and parameters :\n')
                items = commands.items()
                items.sort(key=itemgetter(0))
                for item in items:
                    if commands[item[0]].usage != '':
                        aliastxt = ''
                        if commands[item[0]].alias != '':
                            aliastxt = ' / ' + commands[item[0]].alias
                        dbg.logLines('%s | %s' % (item[0] + aliastxt + ' ' * (20 - len(item[0] + aliastxt)), commands[item[0]].description))
                dbg.log('')
                dbg.log('Want more info about a given command ?  Run !mona help <command>', highlight=1)
                dbg.log('')
        commands['help'] = MnCommand('help', 'show help', '!mona help [command]', procHelp)

        def procConfig(args):
            showerror = False
            if not 'set' in args and (not 'get' in args) and (not 'add' in args):
                showerror = True
            if 'set' in args:
                if type(args['set']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['set'].split(' ')
                    if len(params) < 2:
                        showerror = True
            if 'add' in args:
                if type(args['add']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['add'].split(' ')
                    if len(params) < 2:
                        showerror = True
            if 'get' in args:
                if type(args['get']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['get'].split(' ')
                    if len(params) < 1:
                        showerror = True
            if showerror:
                dbg.log('Usage :')
                dbg.logLines(configUsage, highlight=1)
                return
            else:
                if 'get' in args:
                    dbg.log('Reading value from configuration file')
                    monaConfig = MnConfig()
                    thevalue = monaConfig.get(args['get'])
                    dbg.log('Parameter %s = %s' % (args['get'], thevalue))
                if 'set' in args:
                    dbg.log('Writing value to configuration file')
                    monaConfig = MnConfig()
                    value = args['set'].split(' ')
                    configparam = value[0].strip()
                    dbg.log('Old value of parameter %s = %s' % (configparam, monaConfig.get(configparam)))
                    configvalue = args['set'][0 + len(configparam):len(args['set'])]
                    monaConfig.set(configparam, configvalue)
                    dbg.log('New value of parameter %s = %s' % (configparam, configvalue))
                if 'add' in args:
                    dbg.log('Writing value to configuration file')
                    monaConfig = MnConfig()
                    value = args['add'].split(' ')
                    configparam = value[0].strip()
                    dbg.log('Old value of parameter %s = %s' % (configparam, monaConfig.get(configparam)))
                    configvalue = monaConfig.get(configparam).strip() + ',' + args['add'][0 + len(configparam):len(args['add'])].strip()
                    monaConfig.set(configparam, configvalue)
                    dbg.log('New value of parameter %s = %s' % (configparam, configvalue))

        def procFindJ(args):
            return procFindJMP(args)

        def procFindJMP(args):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            if inspect.stack()[1][3] == 'procFindJ':
                dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."", highlight=1)
            criteria = {}
            all_opcodes = {}
            global ptr_to_get
            ptr_to_get = -1
            distancestr = ''
            mindistance = 0
            maxdistance = 0
            showerror = False
            if 'r' in args:
                if type(args['r']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    thisreg = args['r'].upper().strip()
                    validregs = dbglib.Registers32BitsOrder
                    if not thisreg in validregs:
                        showerror = True
            else:
                showerror = True
            if 'distance' in args:
                if type(args['distance']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    distancestr = args['distance']
                    distanceparts = distancestr.split(',')
                    for parts in distanceparts:
                        valueparts = parts.split('=')
                        if len(valueparts) > 1:
                            if valueparts[0].lower() == 'min':
                                try:
                                    mindistance = int(valueparts[1])
                                except:
                                    mindistance = 0
                            if valueparts[0].lower() == 'max':
                                try:
                                    maxdistance = int(valueparts[1])
                                except:
                                    maxdistance = 0
            if maxdistance < mindistance:
                tmp = maxdistance
                maxdistance = mindistance
                mindistance = tmp
            criteria['mindistance'] = mindistance
            criteria['maxdistance'] = maxdistance
            if showerror:
                dbg.log('Usage :')
                dbg.logLines(jmpUsage, highlight=1)
                return
            else:
                (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
                all_opcodes = findJMP(modulecriteria, criteria, args['r'].lower().strip())
            logfile = MnLog('jmp.txt')
            thislog = logfile.reset()
            processResults(all_opcodes, logfile, thislog)

        def procFindSEH(args):
            modulecriteria = {}
            modulecriteria['safeseh'] = False
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            criteria = {}
            specialcases = {}
            all_opcodes = {}
            global ptr_to_get
            ptr_to_get = -1
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            if 'rop' in args:
                criteria['rop'] = True
            if 'all' in args:
                criteria['all'] = True
                specialcases['maponly'] = True
            else:
                criteria['all'] = False
                specialcases['maponly'] = False
            all_opcodes = findSEH(modulecriteria, criteria)
            logfile = MnLog('seh.txt')
            thislog = logfile.reset()
            processResults(all_opcodes, logfile, thislog, specialcases)

        def procShowMODULES(args):
            modulecriteria = {}
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            modulestosearch = getModulesToQuery(modulecriteria)
            showModuleTable('', modulestosearch)

        def procFindROPFUNC(args):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            ropfuncs = {}
            ropfuncoffsets = {}
            (ropfuncs, ropfuncoffsets) = findROPFUNC(modulecriteria, criteria)
            dbg.log('[+] Processing pointers to interesting rop functions')
            logfile = MnLog('ropfunc.txt')
            thislog = logfile.reset()
            processResults(ropfuncs, logfile, thislog)
            global silent
            silent = True
            dbg.log('[+] Processing offsets to pointers to interesting rop functions')
            logfile = MnLog('ropfunc_offset.txt')
            thislog = logfile.reset()
            processResults(ropfuncoffsets, logfile, thislog)

        def procStackPivots(args):
            procROP(args, 'stackpivot')

        def procROP(args, mode='all'):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            depth = 6
            maxoffset = 40
            thedistance = 8
            split = False
            fast = False
            sortedprint = False
            endingstr = ''
            endings = []
            technique = ''
            if 'depth' in args:
                if type(args['depth']).__name__.lower() != 'bool':
                    try:
                        depth = int(args['depth'])
                    except:
                        pass
            if 'offset' in args:
                if type(args['offset']).__name__.lower() != 'bool':
                    try:
                        maxoffset = int(args['offset'])
                    except:
                        pass
            if 'distance' in args:
                if type(args['distance']).__name__.lower() != 'bool':
                    try:
                        thedistance = args['distance']
                    except:
                        pass
            if 'split' in args:
                if type(args['split']).__name__.lower() == 'bool':
                    split = args['split']
            if 's' in args:
                if type(args['s']).__name__.lower() != 'bool':
                    technique = args['s'].replace(""'"", '').replace('""', '').strip().lower()
            if 'fast' in args:
                if type(args['fast']).__name__.lower() == 'bool':
                    fast = args['fast']
            if 'end' in args:
                if type(args['end']).__name__.lower() == 'str':
                    endingstr = args['end'].replace(""'"", '').replace('""', '').strip()
                    endings = endingstr.split('#')
            if 'f' in args:
                if args['f'] != '':
                    criteria['f'] = args['f']
            if 'sort' in args:
                sortedprint = True
            if 'rva' in args:
                criteria['rva'] = True
            if mode == 'stackpivot':
                fast = False
                endings = ''
                split = False
            else:
                mode = 'all'
            findROPGADGETS(modulecriteria, criteria, endings, maxoffset, depth, split, thedistance, fast, mode, sortedprint, technique)

        def procJseh(args):
            results = []
            showred = 0
            showall = False
            if 'all' in args:
                showall = True
            nrfound = 0
            dbg.log('-----------------------------------------------------------------------')
            dbg.log('Search for jmp/call dword[ebp/esp+nn] (and other) combinations started ')
            dbg.log('-----------------------------------------------------------------------')
            opcodej = ['T$\x08', 'd$\x08', 'T$\x14', 'T$\x14', 'T$\x1c', 'T$\x1c', 'T$,', 'T$,', 'T$D', 'T$D', 'T$P', 'T$P', 'U\x0c', 'e\x0c', 'U$', 'e$', 'U0', 'e0', 'U', 'e', 'U', 'e', 'U', 'e', '\x83\x08', '\x83\x08']
            fakeptrcriteria = {}
            fakeptrcriteria['accesslevel'] = '*'
            for opjc in opcodej:
                addys = []
                addys = searchInRange([[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
                results += addys
                for ptrtypes in addys:
                    for ad1 in addys[ptrtypes]:
                        ptr = MnPointer(ad1)
                        module = ptr.belongsTo()
                        if not module:
                            module = ''
                            page = dbg.getMemoryPageByAddress(ad1)
                            access = page.getAccess(human=True)
                            op = dbg.disasm(ad1)
                            opstring = op.getDisasm()
                            dbg.log('Found %s at 0x%08x - Access: (%s) - Outside of a loaded module' % (opstring, ad1, access), address=ad1, highlight=1)
                            nrfound += 1
                        elif showall:
                            page = dbg.getMemoryPageByAddress(ad1)
                            access = page.getAccess(human=True)
                            op = dbg.disasm(ad1)
                            opstring = op.getDisasm()
                            thismod = MnModule(module)
                            if not thismod.isSafeSEH:
                                extratext = '=== Safeseh : NO ==='
                                showred = 1
                            else:
                                extratext = 'Safeseh protected'
                                showred = 0
                            dbg.log('Found %s at 0x%08x (%s) - Access: (%s) - %s' % (opstring, ad1, module, access, extratext), address=ad1, highlight=showred)
                            nrfound += 1
            dbg.log('Search complete')
            if results:
                dbg.log('Found %d address(es)' % nrfound)
                return 'Found %d address(es) (Check the log Windows for details)' % nrfound
            else:
                dbg.log('No addresses found')
                return 'Sorry, no addresses found'

        def procJOP(args, mode='all'):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            depth = 6
            if 'depth' in args:
                if type(args['depth']).__name__.lower() != 'bool':
                    try:
                        depth = int(args['depth'])
                    except:
                        pass
            findJOPGADGETS(modulecriteria, criteria, depth)

        def procCreatePATTERN(args):
            size = 0
            pattern = ''
            if '?' in args and args['?'] != '':
                try:
                    if '0x' in args['?'].lower():
                        try:
                            size = int(args['?'], 16)
                        except:
                            size = 0
                    else:
                        size = int(args['?'])
                except:
                    size = 0
            if size == 0:
                dbg.log('Please enter a valid size', highlight=1)
            else:
                pattern = createPattern(size, args)
                dbg.log('Creating cyclic pattern of %d bytes' % size)
                dbg.log(pattern)
                global ignoremodules
                ignoremodules = True
                objpatternfile = MnLog('pattern.txt')
                patternfile = objpatternfile.reset()
                objpatternfile.write('\nPattern of ' + str(size) + ' bytes :\n', patternfile)
                objpatternfile.write('-' * (19 + len(str(size))), patternfile)
                objpatternfile.write('\nASCII:', patternfile)
                objpatternfile.write('\n' + pattern, patternfile)
                patternhex = ''
                for patternchar in pattern:
                    patternhex += str(hex(ord(patternchar))).replace('0x', '\\x')
                objpatternfile.write('\n\nHEX:\n', patternfile)
                objpatternfile.write(patternhex, patternfile)
                patternjs = str2js(pattern)
                objpatternfile.write('\n\nJAVASCRIPT (unescape() friendly):\n', patternfile)
                objpatternfile.write(patternjs, patternfile)
                if not silent:
                    dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"", highlight=1)
                    dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile, highlight=1)
                ignoremodules = False
            return

        def procOffsetPATTERN(args):
            egg = ''
            if '?' in args and args['?'] != '':
                try:
                    egg = args['?']
                except:
                    egg = ''
            if egg == '':
                dbg.log('Please enter a valid target', highlight=1)
            else:
                findOffsetInPattern(egg, -1, args)
            return

        def procFileCOMPARE(args):
            modulecriteria = {}
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            allfiles = []
            tomatch = ''
            checkstrict = True
            rangeval = 0
            fast = False
            if 'ptronly' in args or 'ptrsonly' in args:
                fast = True
            if 'f' in args:
                if args['f'] != '':
                    rawfilenames = args['f'].replace('""', '')
                    allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
                    dbg.log('[+] Number of files to be examined : %d ' % len(allfiles))
            if 'range' in args:
                if not type(args['range']).__name__.lower() == 'bool':
                    strrange = args['range'].lower()
                    if strrange.startswith('0x') and len(strrange) > 2:
                        rangeval = int(strrange, 16)
                    else:
                        try:
                            rangeval = int(args['range'])
                        except:
                            rangeval = 0
                    if rangeval > 0:
                        dbg.log('[+] Find overlap using pointer +/- range, value %d' % rangeval)
                        dbg.log('    Note : this will significantly slow down the comparison process !')
                else:
                    dbg.log('Please provide a numeric value ^(> 0) with option -range', highlight=1)
                    return
            else:
                if 'contains' in args:
                    if type(args['contains']).__name__.lower() == 'str':
                        tomatch = args['contains'].replace(""'"", '').replace('""', '')
                if 'nostrict' in args:
                    if type(args['nostrict']).__name__.lower() == 'bool':
                        checkstrict = not args['nostrict']
                        dbg.log('[+] Instructions must match in all files ? %s' % checkstrict)
            callfiles = allfiles
            allfiles = []
            for tfile in callfiles:
                if os.path.isdir(tfile):
                    for (root, dirs, files) in os.walk(tfile):
                        for dfile in files:
                            allfiles.append(os.path.join(root, dfile))
                else:
                    allfiles.append(tfile)
            if len(allfiles) > 1:
                findFILECOMPARISON(modulecriteria, criteria, allfiles, tomatch, checkstrict, rangeval, fast)
            else:
                dbg.log('Please specify at least 2 filenames to compare', highlight=1)

        def procFind(args):
            modulecriteria = {}
            criteria = {}
            pattern = ''
            base = 0
            offset = 0
            top = TOP_USERLAND
            consecutive = False
            ftype = ''
            level = 0
            offsetlevel = 0
            if not 'a' in args:
                args['a'] = '*'
            ptronly = False
            if 'ptronly' in args or 'ptrsonly' in args:
                ptronly = True
            if not 'x' in args:
                args['x'] = '*'
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            if criteria['accesslevel'] == '':
                return
            if not 's' in args:
                dbg.log('-s <search pattern (or filename)> is a mandatory argument', highlight=1)
                return
            pattern = args['s']
            if 'unicode' in args:
                criteria['unic'] = True
            if 'b' in args:
                try:
                    base = int(args['b'], 16)
                except:
                    dbg.log('invalid base address: %s' % args['b'], highlight=1)
                    return
            if 't' in args:
                try:
                    top = int(args['t'], 16)
                except:
                    dbg.log('invalid top address: %s' % args['t'], highlight=1)
                    return
            if 'offset' in args:
                if not args['offset'].__class__.__name__ == 'bool':
                    if '0x' in args['offset'].lower():
                        try:
                            offset = 0 - int(args['offset'], 16)
                        except:
                            dbg.log('invalid offset value', highlight=1)
                            return
                    else:
                        try:
                            offset = 0 - int(args['offset'])
                        except:
                            dbg.log('invalid offset value', highlight=1)
                            return
                else:
                    dbg.log('invalid offset value', highlight=1)
                    return
            if 'level' in args:
                try:
                    level = int(args['level'])
                except:
                    dbg.log('invalid level value', highlight=1)
                    return
            if 'offsetlevel' in args:
                try:
                    offsetlevel = int(args['offsetlevel'])
                except:
                    dbg.log('invalid offsetlevel value', highlight=1)
                    return
            if 'c' in args:
                dbg.log('    - Skipping consecutive pointers, showing size instead')
                consecutive = True
            if 'type' in args:
                if not args['type'] in ['bin', 'asc', 'ptr', 'instr', 'file']:
                    dbg.log('Invalid search type : %s' % args['type'], highlight=1)
                    return
                ftype = args['type']
                if ftype == 'file':
                    filename = args['s'].replace('""', '').replace(""'"", '')
                    if not os.path.isfile(filename):
                        dbg.log('Unable to find/read file %s' % filename, highlight=1)
                        return
            rangep2p = 0
            if 'p2p' in args or level > 0:
                dbg.log('    - Looking for pointers to pointers')
                criteria['p2p'] = True
                if 'r' in args:
                    try:
                        rangep2p = int(args['r'])
                    except:
                        pass
                    if rangep2p > 0:
                        dbg.log('    - Will search for close pointers (%d bytes backwards)' % rangep2p)
                if 'p2p' in args:
                    level = 1
            if level > 0:
                dbg.log('    - Recursive levels : %d' % level)
            allpointers = findPattern(modulecriteria, criteria, pattern, ftype, base, top, consecutive, rangep2p, level, offset, offsetlevel)
            logfile = MnLog('find.txt')
            thislog = logfile.reset()
            processResults(allpointers, logfile, thislog, {}, ptronly)
            return

        def procFindWild(args):
            modulecriteria = {}
            criteria = {}
            pattern = ''
            patterntype = ''
            base = 0
            top = TOP_USERLAND
            (modulecriteria, criteria) = args2criteria(","for sehrecord in sehchain:
    recaddress = sehrecord[0]
    sehandler = sehrecord[1]
    nseh = ''
    try:
        nsehvalue = struct.unpack('<L', dbg.readMemory(recaddress, 4))[0]
        nseh = '0x%08x' % nsehvalue
    except:
        nseh = 0
        sehandler = 0
    overwritedata = checkSEHOverwrite(recaddress, nseh, sehandler)
    overwritemark = ''
    funcinfo = ''
    if sehandler > 0:
        ptr = MnPointer(sehandler)
        funcinfo = ptr.getPtrFunction()
    else:
        funcinfo = ' (corrupted record)'
        if str(nseh).startswith('0x'):
            nseh = '0x%08x' % int(nseh, 16)
        else:
            nseh = '0x%08x' % int(nseh)
    if len(overwritedata) > 0:
        handlersoverwritten[recaddress] = overwritedata
        smashoffset = int(overwritedata[1])
        typeinfo = ''
        if overwritedata[0] == 'unicode':
            smashoffset += 2
            typeinfo = ' [unicode]'
        overwritemark = ' (record smashed at offset %d%s)' % (smashoffset, typeinfo)
    dbg.log('0x%08x  %s  0x%08x %s%s' % (recaddress, nseh, sehandler, funcinfo, overwritemark), recaddress)","[""for sehrecord in sehchain:\n    (sehrecord_0, sehrecord_1, *_) = sehrecord\n    recaddress = sehrecord_0\n    sehandler = sehrecord_1\n    nseh = ''\n    try:\n        nsehvalue = struct.unpack('<L', dbg.readMemory(recaddress, 4))[0]\n        nseh = '0x%08x' % nsehvalue\n    except:\n        nseh = 0\n        sehandler = 0\n    overwritedata = checkSEHOverwrite(recaddress, nseh, sehandler)\n    overwritemark = ''\n    funcinfo = ''\n    if sehandler > 0:\n        ptr = MnPointer(sehandler)\n        funcinfo = ptr.getPtrFunction()\n    else:\n        funcinfo = ' (corrupted record)'\n        if str(nseh).startswith('0x'):\n            nseh = '0x%08x' % int(nseh, 16)\n        else:\n            nseh = '0x%08x' % int(nseh)\n    if len(overwritedata) > 0:\n        handlersoverwritten[recaddress] = overwritedata\n        smashoffset = int(overwritedata[1])\n        typeinfo = ''\n        if overwritedata[0] == 'unicode':\n            smashoffset += 2\n            typeinfo = ' [unicode]'\n        overwritemark = ' (record smashed at offset %d%s)' % (smashoffset, typeinfo)\n    dbg.log('0x%08x  %s  0x%08x %s%s' % (recaddress, nseh, sehandler, funcinfo, overwritemark), recaddress)"", ""for (sehrecord_0, sehrecord_1, *sehrecord_len) in sehchain:\n    recaddress = \n    sehrecord_0\n    sehandler = \n    sehrecord_1\n    nseh = ''\n    try:\n        nsehvalue = struct.unpack('<L', dbg.readMemory(recaddress, 4))[0]\n        nseh = '0x%08x' % nsehvalue\n    except:\n        nseh = 0\n        sehandler = 0\n    overwritedata = checkSEHOverwrite(recaddress, nseh, sehandler)\n    overwritemark = ''\n    funcinfo = ''\n    if sehandler > 0:\n        ptr = MnPointer(sehandler)\n        funcinfo = ptr.getPtrFunction()\n    else:\n        funcinfo = ' (corrupted record)'\n        if str(nseh).startswith('0x'):\n            nseh = '0x%08x' % int(nseh, 16)\n        else:\n            nseh = '0x%08x' % int(nseh)\n    if len(overwritedata) > 0:\n        handlersoverwritten[recaddress] = overwritedata\n        smashoffset = int(overwritedata[1])\n        typeinfo = ''\n        if overwritedata[0] == 'unicode':\n            smashoffset += 2\n            typeinfo = ' [unicode]'\n        overwritemark = ' (record smashed at offset %d%s)' % (smashoffset, typeinfo)\n    dbg.log('0x%08x  %s  0x%08x %s%s' % (recaddress, nseh, sehandler, funcinfo, overwritemark), recaddress)""]",no_found,0
EasyOCR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyOCR/trainer/utils.py,https://github.com/JaidedAI/EasyOCR/tree/master/trainer/utils.py,,word_segmentation$176,"def word_segmentation(mat, separator_idx={'th': [1, 2], 'en': [3, 4]}, separator_idx_list=[1, 2, 3, 4]):
    result = []
    sep_list = []
    start_idx = 0
    for sep_idx in separator_idx_list:
        if sep_idx % 2 == 0:
            mode = 'first'
        else:
            mode = 'last'
        a = consecutive(np.argwhere(mat == sep_idx).flatten(), mode)
        new_sep = [[item, sep_idx] for item in a]
        sep_list += new_sep
    sep_list = sorted(sep_list, key=lambda x: x[0])
    for sep in sep_list:
        for lang in separator_idx.keys():
            if sep[1] == separator_idx[lang][0]:
                sep_lang = lang
                sep_start_idx = sep[0]
            elif sep[1] == separator_idx[lang][1]:
                if sep_lang == lang:
                    new_sep_pair = [lang, [sep_start_idx + 1, sep[0] - 1]]
                    if sep_start_idx > start_idx:
                        result.append(['', [start_idx, sep_start_idx - 1]])
                    start_idx = sep[0] + 1
                    result.append(new_sep_pair)
                else:
                    sep_lang = ''
    if start_idx <= len(mat) - 1:
        result.append(['', [start_idx, len(mat) - 1]])
    return result","for sep in sep_list:
    for lang in separator_idx.keys():
        if sep[1] == separator_idx[lang][0]:
            sep_lang = lang
            sep_start_idx = sep[0]
        elif sep[1] == separator_idx[lang][1]:
            if sep_lang == lang:
                new_sep_pair = [lang, [sep_start_idx + 1, sep[0] - 1]]
                if sep_start_idx > start_idx:
                    result.append(['', [start_idx, sep_start_idx - 1]])
                start_idx = sep[0] + 1
                result.append(new_sep_pair)
            else:
                sep_lang = ''","[""for sep in sep_list:\n    (sep_0, sep_1, *_) = sep\n    for lang in separator_idx.keys():\n        if sep_1 == separator_idx[lang][0]:\n            sep_lang = lang\n            sep_start_idx = sep_0\n        elif sep_1 == separator_idx[lang][1]:\n            if sep_lang == lang:\n                new_sep_pair = [lang, [sep_start_idx + 1, sep_0 - 1]]\n                if sep_start_idx > start_idx:\n                    result.append(['', [start_idx, sep_start_idx - 1]])\n                start_idx = sep_0 + 1\n                result.append(new_sep_pair)\n            else:\n                sep_lang = ''"", ""for (sep_0, sep_1, *sep_len) in sep_list:\n    for lang in separator_idx.keys():\n        if \n        sep_1 == separator_idx[lang][0]:\n            sep_lang = lang\n            sep_start_idx = \n            sep_0\n        elif \n        sep_1 == separator_idx[lang][1]:\n            if sep_lang == lang:\n                new_sep_pair = [lang, [sep_start_idx + 1, \n                sep_0 - 1]]\n                if sep_start_idx > start_idx:\n                    result.append(['', [start_idx, sep_start_idx - 1]])\n                start_idx = \n                sep_0 + 1\n                result.append(new_sep_pair)\n            else:\n                sep_lang = ''""]",no_found,0
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/google/cloud/forseti/scanner/scanners/groups_settings_scanner.py,https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/scanner/scanners/groups_settings_scanner.py,GroupsSettingsScanner,_retrieve$134,"def _retrieve(self):
    """"""Runs the data collection.

        Returns:
            tupl: 2 lists of GroupsSettings objects, 1 only for settings that
            have iam policies and 1 with all groups settings.
        Raises:
            ValueError: if resources have an unexpected type.
        """"""
    all_groups_settings = []
    iam_groups_settings = []
    model_manager = self.service_config.model_manager
    (scoped_session, data_access) = model_manager.get(self.model_name)
    with scoped_session as session:
        for settings in data_access.scanner_fetch_groups_settings(session, True):
            email = settings[0].split('group/')[1]
            iam_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))
        for settings in data_access.scanner_fetch_groups_settings(session, False):
            email = settings[0].split('group/')[1]
            all_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))
    return (all_groups_settings, iam_groups_settings)","for settings in data_access.scanner_fetch_groups_settings(session, False):
    email = settings[0].split('group/')[1]
    all_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))","[""for settings in data_access.scanner_fetch_groups_settings(session, False):\n    (_, settings_1, *settings_rsettingsmaining) = settings\n    email = settings[0].split('group/')[1]\n    all_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings_1))"", ""for (settings_0, settings_1, *settings_len) in data_access.scanner_fetch_groups_settings(session, False):\n    email = \n    settings_0.split('group/')[1]\n    all_groups_settings.append(groups_settings.GroupsSettings.from_json(email, \n    settings_1))""]",no_found,0
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/google/cloud/forseti/scanner/scanners/groups_settings_scanner.py,https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/scanner/scanners/groups_settings_scanner.py,GroupsSettingsScanner,_retrieve$134,"def _retrieve(self):
    """"""Runs the data collection.

        Returns:
            tupl: 2 lists of GroupsSettings objects, 1 only for settings that
            have iam policies and 1 with all groups settings.
        Raises:
            ValueError: if resources have an unexpected type.
        """"""
    all_groups_settings = []
    iam_groups_settings = []
    model_manager = self.service_config.model_manager
    (scoped_session, data_access) = model_manager.get(self.model_name)
    with scoped_session as session:
        for settings in data_access.scanner_fetch_groups_settings(session, True):
            email = settings[0].split('group/')[1]
            iam_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))
        for settings in data_access.scanner_fetch_groups_settings(session, False):
            email = settings[0].split('group/')[1]
            all_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))
    return (all_groups_settings, iam_groups_settings)","for settings in data_access.scanner_fetch_groups_settings(session, True):
    email = settings[0].split('group/')[1]
    iam_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))","[""for settings in data_access.scanner_fetch_groups_settings(session, True):\n    (settings_0, settings_1, *settings_rsettingsmaining) = settings\n    email = settings[0].split('group/')[1]\n    iam_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings_1))"", ""for (settings_0, settings_1, *settings_len) in data_access.scanner_fetch_groups_settings(session, True):\n    email = \n    settings_0.split('group/')[1]\n    iam_groups_settings.append(groups_settings.GroupsSettings.from_json(email, \n    settings_1))""]",no_found,0
anime-downloader,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anime-downloader/anime_downloader/sites/init.py,https://github.com/anime-dl/anime-downloader/tree/master/anime_downloader/sites/init.py,,get_anime_class$53,"def get_anime_class(url):
    """"""
    Get anime class corresposing to url or name.
    See :py:data:`anime_downloader.sites.ALL_ANIME_SITES` to get the possible anime sites.

    Parameters
    ----------
    url: string
        URL of the anime.

    Returns
    -------
    :py:class:`anime_downloader.sites.anime.Anime`
        Concrete implementation of :py:class:`anime_downloader.sites.anime.Anime`
    """"""
    for site in ALL_ANIME_SITES:
        if site[1] in url:
            try:
                module = import_module('anime_downloader.sites.{}'.format(site[0]))
            except ImportError:
                raise
            return getattr(module, site[2])","for site in ALL_ANIME_SITES:
    if site[1] in url:
        try:
            module = import_module('anime_downloader.sites.{}'.format(site[0]))
        except ImportError:
            raise
        return getattr(module, site[2])","[""for site in ALL_ANIME_SITES:\n    (site_0, site_1, site_2, *_) = site\n    if site_1 in url:\n        try:\n            module = import_module('anime_downloader.sites.{}'.format(site_0))\n        except ImportError:\n            raise\n        return getattr(module, site_2)"", ""for (site_0, site_1, site_2, *site_len) in ALL_ANIME_SITES:\n    if \n    site_1 in url:\n        try:\n            module = import_module('anime_downloader.sites.{}'.format(\n            site_0))\n        except ImportError:\n            raise\n        return getattr(module, \n        site_2)""]",no_found,0
MarkdownEditing,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MarkdownEditing/plugins/lint.py,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/lint.py,MdeMarkdownLintCommand,run$76,"def run(self, edit):
    mddef = globals()['mddef']
    text = self.view.substr(sublime.Region(0, self.view.size()))
    st = self.view.settings().get('mde.lint', {})
    uselist = []
    disablelist = st['disable']
    for cl in mddef.__subclasses__():
        if cl.__name__ not in disablelist:
            uselist.append(cl)
    result = []
    for mddef in uselist:
        r = self.test(mddef(st[mddef.__name__] if mddef.__name__ in st else None, self.view), text)
        result.extend(r)
    window = self.view.window() or sublime.active_window()
    if len(result) > 0:
        sublime.status_message('MarkdownLint: %d error(s) found' % len(result))
        result = sorted(result, key=lambda t: t[0])
        outputtxt = ''
        for t in result:
            (row, col) = self.view.rowcol(t[0])
            outputtxt += 'line %d: %s, %s\n' % (row + 1, t[1], t[2])
        output = window.create_output_panel('mde')
        output.run_command('insert', {'characters': outputtxt})
        window.run_command('show_panel', {'panel': 'output.mde'})
    else:
        sublime.status_message('MarkdownLint: no errors found')
        window.destroy_output_panel('mde')","for t in result:
    (row, col) = self.view.rowcol(t[0])
    outputtxt += 'line %d: %s, %s\n' % (row + 1, t[1], t[2])","[""for t in result:\n    (t_0, t_1, t_2, *_) = t\n    (row, col) = self.view.rowcol(t_0)\n    outputtxt += 'line %d: %s, %s\\n' % (row + 1, t_1, t_2)"", ""for (t_0, t_1, t_2, *t_len) in result:\n    (row, col) = self.view.rowcol(t_0)\n    outputtxt += 'line %d: %s, %s\\n' % (row + 1, t_1, t_2)""]",no_found,0
NeoVintageous,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NeoVintageous/tests/functional/test__plugin_surround_ds.py,https://github.com/NeoVintageous/NeoVintageous/tree/master/tests/functional/test__plugin_surround_ds.py,TestSurround_ds,test_t_target_should_delete_tag$215,"def test_t_target_should_delete_tag(self):
    for t in tag_targets_data:
        self.eq('x {}a|b{} y'.format(t[0], t[1]), 'dst', 'x |ab y')","for t in tag_targets_data:
    self.eq('x {}a|b{} y'.format(t[0], t[1]), 'dst', 'x |ab y')","[""for t in tag_targets_data:\n    (t_0, t_1, *_) = t\n    self.eq('x {}a|b{} y'.format(t_0, t_1), 'dst', 'x |ab y')"", ""for (t_0, t_1, *t_len) in tag_targets_data:\n    self.eq('x {}a|b{} y'.format(t_0, t_1), 'dst', 'x |ab y')""]",no_found,0
thonny,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/thonny/thonny/plugins/find_replace.py,https://github.com/thonny/thonny/tree/master/thonny/plugins/find_replace.py,FindDialog,_remove_all_tags$358,"def _remove_all_tags(self):
    for tag in self.passive_found_tags:
        self.codeview.text.tag_remove('found', tag[0], tag[1])
    if self.active_found_tag is not None:
        self.codeview.text.tag_remove('current_found', self.active_found_tag[0], self.active_found_tag[1])
    self.active_found_tag = None
    self.replace_and_find_button.config(state='disabled')
    self.replace_button.config(state='disabled')","for tag in self.passive_found_tags:
    self.codeview.text.tag_remove('found', tag[0], tag[1])","[""for tag in self.passive_found_tags:\n    (tag_0, tag_1, *_) = tag\n    self.codeview.text.tag_remove('found', tag_0, tag_1)"", ""for (tag_0, tag_1, *tag_len) in self.passive_found_tags:\n    self.codeview.text.tag_remove('found', tag_0, tag_1)""]",no_found,0
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_fr.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_fr.py,Num2WordsENTest,test_number$168,"def test_number(self):
    for test in TEST_CASES_CARDINAL:
        self.assertEqual(num2words(test[0], lang='fr'), test[1])","for test in TEST_CASES_CARDINAL:
    self.assertEqual(num2words(test[0], lang='fr'), test[1])","[""for test in TEST_CASES_CARDINAL:\n    (test_0, test_1, *_) = test\n    self.assertEqual(num2words(test_0, lang='fr'), test_1)"", ""for (test_0, test_1, *test_len) in TEST_CASES_CARDINAL:\n    self.assertEqual(num2words(test_0, lang='fr'), test_1)""]",no_found,0
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_fr.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_fr.py,Num2WordsENTest,test_ordinal$172,"def test_ordinal(self):
    for test in TEST_CASES_ORDINAL:
        self.assertEqual(num2words(test[0], lang='fr', ordinal=True), test[1])","for test in TEST_CASES_ORDINAL:
    self.assertEqual(num2words(test[0], lang='fr', ordinal=True), test[1])","[""for test in TEST_CASES_ORDINAL:\n    (test_0, test_1, *_) = test\n    self.assertEqual(num2words(test_0, lang='fr', ordinal=True), test_1)"", ""for (test_0, test_1, *test_len) in TEST_CASES_ORDINAL:\n    self.assertEqual(num2words(test_0, lang='fr', ordinal=True), test_1)""]",no_found,0
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_fr_ch.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_fr_ch.py,Num2WordsENTest,test_currency_frf$121,"def test_currency_frf(self):
    for test in TEST_CASES_TO_CURRENCY_FRF:
        self.assertEqual(num2words(test[0], lang='fr_CH', to='currency', currency='FRF'), test[1])","for test in TEST_CASES_TO_CURRENCY_FRF:
    self.assertEqual(num2words(test[0], lang='fr_CH', to='currency', currency='FRF'), test[1])","[""for test in TEST_CASES_TO_CURRENCY_FRF:\n    (test_0, test_1, *_) = test\n    self.assertEqual(num2words(test_0, lang='fr_CH', to='currency', currency='FRF'), test_1)"", ""for (test_0, test_1, *test_len) in TEST_CASES_TO_CURRENCY_FRF:\n    self.assertEqual(num2words(test_0, lang='fr_CH', to='currency', currency='FRF'), test_1)""]",no_found,0
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_es.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_es.py,Num2WordsESTest,test_currency_khr$2476,"def test_currency_khr(self):
    for test in TEST_CASES_TO_CURRENCY_KHR:
        self.assertEqual(num2words(test[0], lang='es', to='currency', currency='KHR'), test[1])","for test in TEST_CASES_TO_CURRENCY_KHR:
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='KHR'), test[1])","[""for test in TEST_CASES_TO_CURRENCY_KHR:\n    (test_0, test_1, *_) = test\n    self.assertEqual(num2words(test_0, lang='es', to='currency', currency='KHR'), test_1)"", ""for (test_0, test_1, *test_len) in TEST_CASES_TO_CURRENCY_KHR:\n    self.assertEqual(num2words(test_0, lang='es', to='currency', currency='KHR'), test_1)""]",no_found,0
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_es.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_es.py,Num2WordsESTest,test_currency_omr$2686,"def test_currency_omr(self):
    for test in TEST_CASES_TO_CURRENCY_OMR:
        self.assertEqual(num2words(test[0], lang='es', to='currency', currency='OMR'), test[1])","for test in TEST_CASES_TO_CURRENCY_OMR:
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='OMR'), test[1])","[""for test in TEST_CASES_TO_CURRENCY_OMR:\n    (test_0, test_1, *_) = test\n    self.assertEqual(num2words(test_0, lang='es', to='currency', currency='OMR'), test_1)"", ""for (test_0, test_1, *test_len) in TEST_CASES_TO_CURRENCY_OMR:\n    self.assertEqual(num2words(test_0, lang='es', to='currency', currency='OMR'), test_1)""]",no_found,0
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_es.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_es.py,Num2WordsESTest,test_currency_zar$2028,"def test_currency_zar(self):
    for test in TEST_CASES_TO_CURRENCY_ZAR:
        self.assertEqual(num2words(test[0], lang='es', to='currency', currency='ZAR'), test[1])","for test in TEST_CASES_TO_CURRENCY_ZAR:
    self.assertEqual(num2words(test[0], lang='es', to='currency', currency='ZAR'), test[1])","[""for test in TEST_CASES_TO_CURRENCY_ZAR:\n    (test_0, test_1, *_) = test\n    self.assertEqual(num2words(test_0, lang='es', to='currency', currency='ZAR'), test_1)"", ""for (test_0, test_1, *test_len) in TEST_CASES_TO_CURRENCY_ZAR:\n    self.assertEqual(num2words(test_0, lang='es', to='currency', currency='ZAR'), test_1)""]",no_found,0
num2words,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/num2words/tests/test_fr_dz.py,https://github.com/savoirfairelinux/num2words/tree/master/tests/test_fr_dz.py,Num2WordsPLTest,test_ordinal_num$60,"def test_ordinal_num(self):
    for test in test_fr.TEST_CASES_ORDINAL_NUM:
        self.assertEqual(num2words(test[0], lang='fr_DZ', to='ordinal_num'), test[1])","for test in test_fr.TEST_CASES_ORDINAL_NUM:
    self.assertEqual(num2words(test[0], lang='fr_DZ', to='ordinal_num'), test[1])","[""for test in test_fr.TEST_CASES_ORDINAL_NUM:\n    (test_0, test_1, *_) = test\n    self.assertEqual(num2words(test_0, lang='fr_DZ', to='ordinal_num'), test_1)"", ""for (test_0, test_1, *test_len) in test_fr.TEST_CASES_ORDINAL_NUM:\n    self.assertEqual(num2words(test_0, lang='fr_DZ', to='ordinal_num'), test_1)""]",no_found,0
ros_comm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros_comm/test/test_rosmaster/test/testSlave.py,https://github.com/ros/ros_comm/tree/master/test/test_rosmaster/test/testSlave.py,SlaveTestCase,_sink_StartNodes$243,"def _sink_StartNodes(self, tests):
    """"""
        Test subroutine to startup all the nodes specified in the tests
        """"""
    master = self.master
    sourceUris = {}
    sinks = {}
    (pkg, node) = testNode
    for test in tests:
        (sourceName, sinkName) = test[0]
        sourcePort = apiSuccess(master.addNode('', '', sourceName, pkg, node, TEST_MACHINE, 0))
        sinkPort = apiSuccess(master.addNode('', '', sinkName, pkg, node, TEST_MACHINE, 0))
        sourceUri = 'http://%s:%s/' % (testNodeAddr[0], sourcePort)
        sinkUri = 'http://%s:%s/' % (testNodeAddr[0], sinkPort)
        sourceUris[sourceName] = sourceUri
        sinks[sinkName] = ServerProxy(sinkUri)
    return (sourceUris, sinks)","for test in tests:
    (sourceName, sinkName) = test[0]
    sourcePort = apiSuccess(master.addNode('', '', sourceName, pkg, node, TEST_MACHINE, 0))
    sinkPort = apiSuccess(master.addNode('', '', sinkName, pkg, node, TEST_MACHINE, 0))
    sourceUri = 'http://%s:%s/' % (testNodeAddr[0], sourcePort)
    sinkUri = 'http://%s:%s/' % (testNodeAddr[0], sinkPort)
    sourceUris[sourceName] = sourceUri
    sinks[sinkName] = ServerProxy(sinkUri)","[""for test in tests:\n    (test_0, *test_rtestmaining) = test\n    (sourceName, sinkName) = test_0\n    sourcePort = apiSuccess(master.addNode('', '', sourceName, pkg, node, TEST_MACHINE, 0))\n    sinkPort = apiSuccess(master.addNode('', '', sinkName, pkg, node, TEST_MACHINE, 0))\n    sourceUri = 'http://%s:%s/' % (testNodeAddr[0], sourcePort)\n    sinkUri = 'http://%s:%s/' % (testNodeAddr[0], sinkPort)\n    sourceUris[sourceName] = sourceUri\n    sinks[sinkName] = ServerProxy(sinkUri)"", ""for (test_0, *test_len) in tests:\n    (sourceName, sinkName) = \n    test_0\n    sourcePort = apiSuccess(master.addNode('', '', sourceName, pkg, node, TEST_MACHINE, 0))\n    sinkPort = apiSuccess(master.addNode('', '', sinkName, pkg, node, TEST_MACHINE, 0))\n    sourceUri = 'http://%s:%s/' % (testNodeAddr[0], sourcePort)\n    sinkUri = 'http://%s:%s/' % (testNodeAddr[0], sinkPort)\n    sourceUris[sourceName] = sourceUri\n    sinks[sinkName] = ServerProxy(sinkUri)""]",no_found,
raven-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/raven-python/tests/handlers/logging/tests.py,https://github.com/getsentry/raven-python/tree/master/tests/handlers/logging/tests.py,LoggingIntegrationTest,test_can_record$70,"def test_can_record(self):
    tests = [('raven', False), ('raven.foo', False), ('sentry.errors', False), ('sentry.errors.foo', False), ('raven_utils', True)]
    for test in tests:
        record = self.make_record('Test', name=test[0])
        self.assertEqual(self.handler.can_record(record), test[1])","for test in tests:
    record = self.make_record('Test', name=test[0])
    self.assertEqual(self.handler.can_record(record), test[1])","[""for test in tests:\n    (test_0, test_1, *_) = test\n    record = self.make_record('Test', name=test_0)\n    self.assertEqual(self.handler.can_record(record), test_1)"", ""for (test_0, test_1, *test_len) in tests:\n    record = self.make_record('Test', name=test_0)\n    self.assertEqual(self.handler.can_record(record), test_1)""]",no_found,0
nefarious,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nefarious/src/nefarious/tests/test_import_movie.py,https://github.com/lardbit/nefarious/tree/master/src/nefarious/tests/test_import_movie.py,MovieImportTest,test_movie$25,"def test_movie(self):
    nefarious_settings = NefariousSettings()
    nefarious_settings.tmdb_configuration = {'images': {'secure_base_url': 'https://image.tmdb.org/t/p/'}}
    tmdb_client = get_tmdb_client(nefarious_settings)
    user = User.objects.create_superuser('test', 'test@test.com', 'test')
    importer = MovieImporter(nefarious_settings=nefarious_settings, root_path='/test-download', tmdb_client=tmdb_client, user=user)
    for test_result in self.movie_tests:
        test_path = os.path.join('/movie', test_result[0])
        import_result = importer.ingest_path(test_path)
        if test_result[1] is False or import_result is False:
            self.assertEqual(test_result[1], import_result, '{} != {}'.format(test_result[1], import_result))
        else:
            watch_movie = WatchMovie(name=test_result[1])
            self.assertTrue(watch_movie.name == import_result.name, '{} != {}'.format(watch_movie.name, import_result.name))","for test_result in self.movie_tests:
    test_path = os.path.join('/movie', test_result[0])
    import_result = importer.ingest_path(test_path)
    if test_result[1] is False or import_result is False:
        self.assertEqual(test_result[1], import_result, '{} != {}'.format(test_result[1], import_result))
    else:
        watch_movie = WatchMovie(name=test_result[1])
        self.assertTrue(watch_movie.name == import_result.name, '{} != {}'.format(watch_movie.name, import_result.name))","[""for test_result in self.movie_tests:\n    (test_result_0, test_result_1, *_) = test_result\n    test_path = os.path.join('/movie', test_result_0)\n    import_result = importer.ingest_path(test_path)\n    if test_result_1 is False or import_result is False:\n        self.assertEqual(test_result_1, import_result, '{} != {}'.format(test_result_1, import_result))\n    else:\n        watch_movie = WatchMovie(name=test_result_1)\n        self.assertTrue(watch_movie.name == import_result.name, '{} != {}'.format(watch_movie.name, import_result.name))"", ""for (test_result_0, test_result_1, *test_result_len) in self.movie_tests:\n    test_path = os.path.join('/movie', \n    test_result_0)\n    import_result = importer.ingest_path(test_path)\n    if \n    test_result_1 is False or import_result is False:\n        self.assertEqual(\n        test_result_1, import_result, '{} != {}'.format(\n        test_result_1, import_result))\n    else:\n        watch_movie = WatchMovie(name=\n        test_result_1)\n        self.assertTrue(watch_movie.name == import_result.name, '{} != {}'.format(watch_movie.name, import_result.name))""]",no_found,0
azure-devops-cli-extension,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-devops-cli-extension/scripts/findEmptyHelpTexts.py,https://github.com/Azure/azure-devops-cli-extension/tree/master/scripts/findEmptyHelpTexts.py,,print_missing_help_files$63,"def print_missing_help_files(help_files):
    missing_help_text = False
    missing_help_list = []
    for help_file in help_files:
        if help_file.short_summary == '' and help_file.command != '':
            is_command = isinstance(help_file, CliCommandHelpFile)
            command_type = 'command' if is_command else 'group'
            missing_help_list.append((command_type, help_file.command))
            missing_help_text = True
    if not missing_help_text:
        print('No missing help texts found.')
    else:
        print('\n\nNo help texts were found for below command(s):')
        for text in missing_help_list:
            print('{} : {}'.format(text[0], text[1]))
        raise Exception('Please update the help text(s).')","for text in missing_help_list:
    print('{} : {}'.format(text[0], text[1]))","[""for text in missing_help_list:\n    (text_0, text_1, *text_rtextmaining) = text\n    print('{} : {}'.format(text_0, text_1))"", ""for (text_0, text_1, *text_len) in missing_help_list:\n    print('{} : {}'.format(text_0, text_1))""]",no_found,0
nlp_xiaojiang,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp_xiaojiang/ClassificationText/bert/keras_bert_classify_bi_lstm.py,https://github.com/yongzhuo/nlp_xiaojiang/tree/master/ClassificationText/bert/keras_bert_classify_bi_lstm.py,BertBiLstmModel,process_pair$99,"def process_pair(self, textss):
    input_ids = []
    input_masks = []
    input_type_ids = []
    for texts in textss:
        tokens_text = self.tokenizer.tokenize(texts[0])
        logger.info('Tokens1:', tokens_text)
        tokens_text2 = self.tokenizer.tokenize(texts[1])
        logger.info('Tokens2:', tokens_text2)
        (input_id, input_type_id) = self.tokenizer.encode(first=texts[0], second=texts[1], max_len=self.max_seq_len)
        input_mask = [0 if ids == 0 else 1 for ids in input_id]
        input_ids.append(input_id)
        input_type_ids.append(input_type_id)
        input_masks.append(input_mask)
    input_ids = np.array(input_ids)
    input_masks = np.array(input_masks)
    input_type_ids = np.array(input_type_ids)
    logger.info('process ok!')
    return (input_ids, input_masks, input_type_ids)","for texts in textss:
    tokens_text = self.tokenizer.tokenize(texts[0])
    logger.info('Tokens1:', tokens_text)
    tokens_text2 = self.tokenizer.tokenize(texts[1])
    logger.info('Tokens2:', tokens_text2)
    (input_id, input_type_id) = self.tokenizer.encode(first=texts[0], second=texts[1], max_len=self.max_seq_len)
    input_mask = [0 if ids == 0 else 1 for ids in input_id]
    input_ids.append(input_id)
    input_type_ids.append(input_type_id)
    input_masks.append(input_mask)","[""for texts in textss:\n    (texts_0, texts_1, *_) = texts\n    tokens_text = self.tokenizer.tokenize(texts_0)\n    logger.info('Tokens1:', tokens_text)\n    tokens_text2 = self.tokenizer.tokenize(texts_1)\n    logger.info('Tokens2:', tokens_text2)\n    (input_id, input_type_id) = self.tokenizer.encode(first=texts_0, second=texts_1, max_len=self.max_seq_len)\n    input_mask = [0 if ids == 0 else 1 for ids in input_id]\n    input_ids.append(input_id)\n    input_type_ids.append(input_type_id)\n    input_masks.append(input_mask)"", ""for (texts_0, texts_1, *texts_len) in textss:\n    tokens_text = self.tokenizer.tokenize(texts_0)\n    logger.info('Tokens1:', tokens_text)\n    tokens_text2 = self.tokenizer.tokenize(texts_1)\n    logger.info('Tokens2:', tokens_text2)\n    (input_id, input_type_id) = self.tokenizer.encode(first=texts_0, second=texts_1, max_len=self.max_seq_len)\n    input_mask = [0 if ids == 0 else 1 for ids in input_id]\n    input_ids.append(input_id)\n    input_type_ids.append(input_type_id)\n    input_masks.append(input_mask)""]",no_found,0
supervisor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/supervisor/supervisor/xmlrpc.py,https://github.com/home-assistant/supervisor/tree/master/supervisor/xmlrpc.py,SystemNamespaceRPCInterface,methodSignature$197,"def methodSignature(self, name):
    """""" Return an array describing the method signature in the
        form [rtype, ptype, ptype...] where rtype is the return data type
        of the method, and ptypes are the parameter data types that the
        method accepts in method argument order.

        @param string name  The name of the method.
        @return array result  The result.
        """"""
    methods = self._listMethods()
    for method in methods:
        if method == name:
            rtype = None
            ptypes = []
            parsed = gettags(methods[method])
            for thing in parsed:
                if thing[1] == 'return':
                    rtype = thing[2]
                elif thing[1] == 'param':
                    ptypes.append(thing[2])
            if rtype is None:
                raise RPCError(Faults.SIGNATURE_UNSUPPORTED)
            return [rtype] + ptypes
    raise RPCError(Faults.SIGNATURE_UNSUPPORTED)","for thing in parsed:
    if thing[1] == 'return':
        rtype = thing[2]
    elif thing[1] == 'param':
        ptypes.append(thing[2])","[""for thing in parsed:\n    (_, thing_1, thing_2, *thing_rthingmaining) = thing\n    if thing_1 == 'return':\n        rtype = thing_2\n    elif thing_1 == 'param':\n        ptypes.append(thing_2)"", ""for (thing_0, thing_1, thing_2, *thing_len) in parsed:\n    if \n    thing_1 == 'return':\n        rtype = \n        thing_2\n    elif \n    thing_1 == 'param':\n        ptypes.append(\n        thing_2)""]",no_found,0
flow-forecast,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow-forecast/flood_forecast/trainer.py,https://github.com/AIStream-Peelout/flow-forecast/tree/master/flood_forecast/trainer.py,,handle_model_evaluation1$16,"def handle_model_evaluation1(trained_model, params: Dict, model_type: str) -> None:
    """"""Utility function to help handle model evaluation. Primarily used at the moment for forcast

    :param trained_model: A PyTorchForecast model that has already been trained. 
    :type trained_model: PyTorchForecast
    :param params: A dictionary of the trained model parameters.
    :type params: Dict
    :param model_type: The type of model. Almost always PyTorch in practice.
    :type model_type: str
    """"""
    test_acc = evaluate_model(trained_model, model_type, params['dataset_params']['target_col'], params['metrics'], params['inference_params'], {})
    wandb.run.summary['test_accuracy'] = test_acc[0]
    df_train_and_test = test_acc[1]
    forecast_start_idx = test_acc[2]
    df_prediction_samples = test_acc[3]
    mae = (df_train_and_test.loc[forecast_start_idx:, 'preds'] - df_train_and_test.loc[forecast_start_idx:, params['dataset_params']['target_col'][0]]).abs()
    inverse_mae = 1 / mae
    i = 0
    for df in df_prediction_samples:
        pred_std = df.std(axis=1)
        average_prediction_sharpe = (inverse_mae / pred_std).mean()
        wandb.log({'average_prediction_sharpe' + str(i): average_prediction_sharpe})
        i += 1
    df_train_and_test.to_csv('temp_preds.csv')
    if 'probabilistic' in params['inference_params']:
        test_plot = plot_df_test_with_probabilistic_confidence_interval(df_train_and_test, forecast_start_idx, params)
    elif len(df_prediction_samples) > 0:
        for thing in zip(df_prediction_samples, params['dataset_params']['target_col']):
            thing[0].to_csv(thing[1] + '.csv')
            test_plot = plot_df_test_with_confidence_interval(df_train_and_test, thing[0], forecast_start_idx, params, targ_col=thing[1], ci=95, alpha=0.25)
            wandb.log({'test_plot_' + thing[1]: test_plot})
    else:
        pd.options.plotting.backend = 'plotly'
        t = params['dataset_params']['target_col'][0]
        test_plot = df_train_and_test[[t, 'preds']].plot()
        wandb.log({'test_plot_' + t: test_plot})
    print('Now plotting final plots')
    test_plot_all = go.Figure()
    for relevant_col in params['dataset_params']['relevant_cols']:
        test_plot_all.add_trace(go.Scatter(x=df_train_and_test.index, y=df_train_and_test[relevant_col], name=relevant_col))
    wandb.log({'test_plot_all': test_plot_all})","for thing in zip(df_prediction_samples, params['dataset_params']['target_col']):
    thing[0].to_csv(thing[1] + '.csv')
    test_plot = plot_df_test_with_confidence_interval(df_train_and_test, thing[0], forecast_start_idx, params, targ_col=thing[1], ci=95, alpha=0.25)
    wandb.log({'test_plot_' + thing[1]: test_plot})","[""for thing in zip(df_prediction_samples, params['dataset_params']['target_col']):\n    (thing_0, thing_1, *_) = thing\n    thing_0.to_csv(thing_1 + '.csv')\n    test_plot = plot_df_test_with_confidence_interval(df_train_and_test, thing_0, forecast_start_idx, params, targ_col=thing_1, ci=95, alpha=0.25)\n    wandb.log({'test_plot_' + thing_1: test_plot})"", ""for (thing_0, thing_1, *thing_len) in zip(df_prediction_samples, params['dataset_params']['target_col']):\n    thing_0.to_csv(thing_1 + '.csv')\n    test_plot = plot_df_test_with_confidence_interval(df_train_and_test, thing_0, forecast_start_idx, params, targ_col=thing_1, ci=95, alpha=0.25)\n    wandb.log({'test_plot_' + thing_1: test_plot})""]",no_found,0
jieba,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jieba/test/jieba_test.py,https://github.com/fxsjy/jieba/tree/master/test/jieba_test.py,JiebaTestCase,testTokenize_NOHMM$185,"def testTokenize_NOHMM(self):
    for content in test_contents:
        result = jieba.tokenize(content, HMM=False)
        assert isinstance(result, types.GeneratorType), 'Test Tokenize Generator error'
        result = list(result)
        assert isinstance(result, list), 'Test Tokenize error on content: %s' % content
        for tk in result:
            print('word %s\t\t start: %d \t\t end:%d' % (tk[0], tk[1], tk[2]), file=sys.stderr)
    print('testTokenize_NOHMM', file=sys.stderr)","for tk in result:
    print('word %s\t\t start: %d \t\t end:%d' % (tk[0], tk[1], tk[2]), file=sys.stderr)","[""for tk in result:\n    (tk_0, tk_1, tk_2, *_) = tk\n    print('word %s\\t\\t start: %d \\t\\t end:%d' % (tk_0, tk_1, tk_2), file=sys.stderr)"", ""for (tk_0, tk_1, tk_2, *tk_len) in result:\n    print('word %s\\t\\t start: %d \\t\\t end:%d' % (tk_0, tk_1, tk_2), file=sys.stderr)""]",no_found,0
electricitymap-contrib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electricitymap-contrib/parsers/CL.py,https://github.com/tmrowco/electricitymap-contrib/tree/master/parsers/CL.py,,production_processor_live$45,"def production_processor_live(json_tot, json_ren):
    """"""
    Extracts generation data and timestamp into dictionary.
    Returns a list of dictionaries for all of the available ""live"" data, usually that day.
    """"""
    gen_total = json_tot['data'][0]['values']
    if json_ren['data'][1]['key'] == 'ENERGA SOLAR':
        rawgen_sol = json_ren['data'][1]['values']
    else:
        raise RuntimeError(f""Unexpected data label. Expected 'ENERGA SOLAR' and got {json_ren['data'][1]['key']}"")
    if json_ren['data'][0]['key'] == 'ENERGA ELICA':
        rawgen_wind = json_ren['data'][0]['values']
    else:
        raise RuntimeError(f""Unexpected data label. Expected 'ENERGA ELICA' and got {json_ren['data'][0]['key']}"")
    mapped_totals = []
    for total in gen_total:
        datapoint = {}
        dt = total[0]
        for pair in rawgen_sol:
            if pair[0] == dt:
                solar = pair[1]
                break
        for pair in rawgen_wind:
            if pair[0] == dt:
                wind = pair[1]
                break
        datapoint['datetime'] = arrow.get(dt / 1000, tzinfo='Chile/Continental').datetime
        datapoint['unknown'] = total[1] - wind - solar
        datapoint['wind'] = wind
        datapoint['solar'] = solar
        mapped_totals.append(datapoint)
    return mapped_totals","for total in gen_total:
    datapoint = {}
    dt = total[0]
    for pair in rawgen_sol:
        if pair[0] == dt:
            solar = pair[1]
            break
    for pair in rawgen_wind:
        if pair[0] == dt:
            wind = pair[1]
            break
    datapoint['datetime'] = arrow.get(dt / 1000, tzinfo='Chile/Continental').datetime
    datapoint['unknown'] = total[1] - wind - solar
    datapoint['wind'] = wind
    datapoint['solar'] = solar
    mapped_totals.append(datapoint)","[""for total in gen_total:\n    (total_0, total_1, *_) = total\n    datapoint = {}\n    dt = total_0\n    for pair in rawgen_sol:\n        if pair[0] == dt:\n            solar = pair[1]\n            break\n    for pair in rawgen_wind:\n        if pair[0] == dt:\n            wind = pair[1]\n            break\n    datapoint['datetime'] = arrow.get(dt / 1000, tzinfo='Chile/Continental').datetime\n    datapoint['unknown'] = total_1 - wind - solar\n    datapoint['wind'] = wind\n    datapoint['solar'] = solar\n    mapped_totals.append(datapoint)"", ""for (total_0, total_1, *total_len) in gen_total:\n    datapoint = {}\n    dt = \n    total_0\n    for pair in rawgen_sol:\n        if pair[0] == dt:\n            solar = pair[1]\n            break\n    for pair in rawgen_wind:\n        if pair[0] == dt:\n            wind = pair[1]\n            break\n    datapoint['datetime'] = arrow.get(dt / 1000, tzinfo='Chile/Continental').datetime\n    datapoint['unknown'] = \n    total_1 - wind - solar\n    datapoint['wind'] = wind\n    datapoint['solar'] = solar\n    mapped_totals.append(datapoint)""]",no_found,0
pylearn2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pylearn2/pylearn2/scripts/tutorials/jobman_demo/utils.py,https://github.com/lisa-lab/pylearn2/tree/master/pylearn2/scripts/tutorials/jobman_demo/utils.py,,parse_results$44,"def parse_results(cwd):
    optimal_dd = None
    optimal_measure = numpy.inf
    for tup in tools.find_conf_files(cwd):
        dd = tup[1]
        if 'results.train_y_misclass' in dd:
            if dd['results.train_y_misclass'] < optimal_measure:
                optimal_measure = dd['results.train_y_misclass']
                optimal_dd = dd
    print('Optimal results.train_y_misclass:', str(optimal_measure))
    for (key, value) in optimal_dd.items():
        if 'hyper_parameters' in key:
            print(key + ': ' + str(value))","for tup in tools.find_conf_files(cwd):
    dd = tup[1]
    if 'results.train_y_misclass' in dd:
        if dd['results.train_y_misclass'] < optimal_measure:
            optimal_measure = dd['results.train_y_misclass']
            optimal_dd = dd","[""for tup in tools.find_conf_files(cwd):\n    (_, tup_1, *tup_rtupmaining) = tup\n    dd = tup_1\n    if 'results.train_y_misclass' in dd:\n        if dd['results.train_y_misclass'] < optimal_measure:\n            optimal_measure = dd['results.train_y_misclass']\n            optimal_dd = dd"", ""for (tup_0, tup_1, *tup_len) in tools.find_conf_files(cwd):\n    dd = \n    tup_1\n    if 'results.train_y_misclass' in dd:\n        if dd['results.train_y_misclass'] < optimal_measure:\n            optimal_measure = dd['results.train_y_misclass']\n            optimal_dd = dd""]",no_found,0
BlenderProc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BlenderProc/docs/source/ext/moduleoverview.py,https://github.com/DLR-RM/BlenderProc/tree/master/docs/source/ext/moduleoverview.py,,generate_tutorials_sidebar$63,"def generate_tutorials_sidebar(app, fromdocname, container):
    tutorials_dir = Path(__file__).absolute().parent.parent / 'docs' / 'tutorials'
    tutorials = [('Loading and manipulating objects', 'loader'), ('Configuring the camera', 'camera'), ('Rendering the scene', 'renderer'), ('Writing the results to file', 'writer'), ('How key frames work', 'key_frames'), ('Positioning objects via the physics simulator', 'physics')]
    container += nodes.caption('Tutorials', '', *[nodes.Text('Tutorials')])
    for tutorial in tutorials:
        toc = nodes.bullet_list()
        ref = nodes.reference('', '')
        ref['refuri'] = app.builder.get_relative_uri(fromdocname, 'docs/tutorials/' + tutorial[1])
        ref.append(nodes.Text(tutorial[0]))
        module_item = nodes.list_item('', addnodes.compact_paragraph('', '', ref), classes=['toctree-l1'])
        if fromdocname.startswith('docs/tutorials/' + tutorial[1]):
            module_item['classes'].append('current')
        toc += module_item
        container += toc","for tutorial in tutorials:
    toc = nodes.bullet_list()
    ref = nodes.reference('', '')
    ref['refuri'] = app.builder.get_relative_uri(fromdocname, 'docs/tutorials/' + tutorial[1])
    ref.append(nodes.Text(tutorial[0]))
    module_item = nodes.list_item('', addnodes.compact_paragraph('', '', ref), classes=['toctree-l1'])
    if fromdocname.startswith('docs/tutorials/' + tutorial[1]):
        module_item['classes'].append('current')
    toc += module_item
    container += toc","[""for tutorial in tutorials:\n    (tutorial_0, tutorial_1, *_) = tutorial\n    toc = nodes.bullet_list()\n    ref = nodes.reference('', '')\n    ref['refuri'] = app.builder.get_relative_uri(fromdocname, 'docs/tutorials/' + tutorial_1)\n    ref.append(nodes.Text(tutorial_0))\n    module_item = nodes.list_item('', addnodes.compact_paragraph('', '', ref), classes=['toctree-l1'])\n    if fromdocname.startswith('docs/tutorials/' + tutorial_1):\n        module_item['classes'].append('current')\n    toc += module_item\n    container += toc"", ""for (tutorial_0, tutorial_1, *tutorial_len) in tutorials:\n    toc = nodes.bullet_list()\n    ref = nodes.reference('', '')\n    ref['refuri'] = app.builder.get_relative_uri(fromdocname, 'docs/tutorials/' + tutorial_1)\n    ref.append(nodes.Text(tutorial_0))\n    module_item = nodes.list_item('', addnodes.compact_paragraph('', '', ref), classes=['toctree-l1'])\n    if fromdocname.startswith('docs/tutorials/' + tutorial_1):\n        module_item['classes'].append('current')\n    toc += module_item\n    container += toc""]",no_found,0
lbry-sdk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lbry-sdk/lbry/error/generate.py,https://github.com/lbryio/lbry-sdk/tree/master/lbry/error/generate.py,,analyze$127,"def analyze():
    errors = {e.class_name: [] for e in get_errors() if e.is_leaf}
    here = Path(__file__).absolute().parents[0]
    module = here.parent
    for file_path in module.glob('**/*.py'):
        if here in file_path.parents:
            continue
        with open(file_path) as src_file:
            src = src_file.read()
            for error in errors.keys():
                found = src.count(error)
                if found > 0:
                    errors[error].append((file_path, found))
    print('Unused Errors:\n')
    for (error, used) in errors.items():
        if used:
            print(f' - {error}')
            for use in used:
                print(f'   {use[0].relative_to(module.parent)} {use[1]}')
            print('')
    print('')
    print('Unused Errors:')
    for (error, used) in errors.items():
        if not used:
            print(f' - {error}')","for use in used:
    print(f'   {use[0].relative_to(module.parent)} {use[1]}')","[""for use in used:\n    (use_0, use_1, *_) = use\n    print(f'   {use_0.relative_to(module.parent)} {use_1}')"", ""for (use_0, use_1, *use_len) in used:\n    print(f'   {use_0.relative_to(module.parent)} {use_1}')""]",no_found,0
CMSmap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CMSmap/cmsmap/lib/jooscan.py,https://github.com/Dionach/CMSmap/tree/master/cmsmap/lib/jooscan.py,JooScan,JooFeed$140,"def JooFeed(self):
    requester.request(self.url + '/?format=feed', data=None)
    jooUsers = re.findall('<author>(.+?) \\((.+?)\\)</author>', requester.htmltext, re.IGNORECASE)
    if jooUsers:
        msg = 'Enumerating Joomla Usernames via ""Feed"" ...'
        report.message(msg)
        jooUsers = sorted(set(jooUsers))
        for user in jooUsers:
            self.usernames.append(user[1])
            msg = user[1] + ': ' + user[0]
            report.info(msg)","for user in jooUsers:
    self.usernames.append(user[1])
    msg = user[1] + ': ' + user[0]
    report.info(msg)","[""for user in jooUsers:\n    (user_0, user_1, *_) = user\n    self.usernames.append(user_1)\n    msg = user_1 + ': ' + user_0\n    report.info(msg)"", ""for (user_0, user_1, *user_len) in jooUsers:\n    self.usernames.append(\n    user_1)\n    msg = \n    user_1 + ': ' + \n    user_0\n    report.info(msg)""]",no_found,0
pyOCD,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyOCD/pyocd/probe/stlink/detect/windows.py,https://github.com/pyocd/pyOCD/tree/master/pyocd/probe/stlink/detect/windows.py,,_get_cached_mounted_points$70,"def _get_cached_mounted_points():
    """"""! Get the volumes present on the system
    @return List of mount points and their associated target id
      Ex. [{ 'mount_point': 'D:', 'target_id_usb_id': 'xxxx'}, ...]
    """"""
    result = []
    try:
        mounted_devices_key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, 'SYSTEM\\MountedDevices')
        for v in _iter_vals(mounted_devices_key):
            if 'DosDevices' not in v[0]:
                continue
            volume_string = v[1].decode('utf-16le', 'ignore')
            if not _is_mbed_volume(volume_string):
                continue
            mount_point_match = re.match('.*\\\\(.:)$', v[0])
            if not mount_point_match:
                LOG.debug('Invalid disk pattern for entry %s, skipping', v[0])
                continue
            mount_point = mount_point_match.group(1)
            result.append({'mount_point': mount_point, 'volume_string': volume_string})
    except OSError:
        LOG.error('Failed to open ""MountedDevices"" in registry')
    return result","for v in _iter_vals(mounted_devices_key):
    if 'DosDevices' not in v[0]:
        continue
    volume_string = v[1].decode('utf-16le', 'ignore')
    if not _is_mbed_volume(volume_string):
        continue
    mount_point_match = re.match('.*\\\\(.:)$', v[0])
    if not mount_point_match:
        LOG.debug('Invalid disk pattern for entry %s, skipping', v[0])
        continue
    mount_point = mount_point_match.group(1)
    result.append({'mount_point': mount_point, 'volume_string': volume_string})","[""for v in _iter_vals(mounted_devices_key):\n    (v_0, v_1, *_) = v\n    if 'DosDevices' not in v_0:\n        continue\n    volume_string = v_1.decode('utf-16le', 'ignore')\n    if not _is_mbed_volume(volume_string):\n        continue\n    mount_point_match = re.match('.*\\\\\\\\(.:)$', v_0)\n    if not mount_point_match:\n        LOG.debug('Invalid disk pattern for entry %s, skipping', v_0)\n        continue\n    mount_point = mount_point_match.group(1)\n    result.append({'mount_point': mount_point, 'volume_string': volume_string})"", ""for (v_0, v_1, *v_len) in _iter_vals(mounted_devices_key):\n    if 'DosDevices' not in \n    v_0:\n        continue\n    volume_string = \n    v_1.decode('utf-16le', 'ignore')\n    if not _is_mbed_volume(volume_string):\n        continue\n    mount_point_match = re.match('.*\\\\\\\\(.:)$', \n    v_0)\n    if not mount_point_match:\n        LOG.debug('Invalid disk pattern for entry %s, skipping', \n        v_0)\n        continue\n    mount_point = mount_point_match.group(1)\n    result.append({'mount_point': mount_point, 'volume_string': volume_string})""]",no_found,0
SMARTS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMARTS/zoo/evaluation/metrics/evaluation_report.py,https://github.com/huawei-noah/SMARTS/tree/master/zoo/evaluation/metrics/evaluation_report.py,EvaluationReport,__init__$12,"def __init__(self, scenarios_list, agents_list, csv_file_result_path):
    self.scenarios_list = scenarios_list
    self.csv_file_result_path = csv_file_result_path
    self.group_agents_list = []
    for value in agents_list.items():
        for agent in value[1]:
            self.group_agents_list.append(value[0] + ':' + agent)
    self.agents_list = [agent.split(':')[-1] for agent in self.group_agents_list]
    self.group_list = [agent.split(':')[0] for agent in self.group_agents_list]","for value in agents_list.items():
    for agent in value[1]:
        self.group_agents_list.append(value[0] + ':' + agent)","[""for value in agents_list.items():\n    (value_0, value_1, *_) = value\n    for agent in value_1:\n        self.group_agents_list.append(value_0 + ':' + agent)"", ""for (value_0, value_1, *value_len) in agents_list.items():\n    for agent in \n    value_1:\n        self.group_agents_list.append(\n        value_0 + ':' + agent)""]",no_found,0
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""""""""
    outline = Outline()
    playlist = Playlist()
    counter = Counter()
    video_list = []
    pdf_list = []
    rich_text_list = []
    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190', 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean', 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id, 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr', data=post_data).text.encode('utf_8').decode('unicode_escape')
    chapters = re.findall('homeworks=\\w+;.+id=(\\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)
        lessons = re.findall('chapterId=' + chapter[0] + '.+contentType=1.+id=(\\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)
            videos = re.findall('contentId=(\\d+).+contentType=(1).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()
            pdfs = re.findall('contentId=(\\d+).+contentType=(3).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()
            rich_text = re.findall('contentId=(\\d+).+contentType=(4).+id=(\\d+).+jsonContent=(.+);.+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')
                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()
    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","for video in videos:
    counter.add(2)
    outline.write(video[3], counter, 2, sign='#')
    video_list.append(Video(counter, video[3], video))","[""for video in videos:\n    (_, _, _, video_3, *_) = video\n    counter.add(2)\n    outline.write(video_3, counter, 2, sign='#')\n    video_list.append(Video(counter, video_3, video))""]",no_found,0
Vxscan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Vxscan/analyzer.py,https://github.com/al0ne/Vxscan/tree/master//analyzer.py,,gener$97,"def gener():
    f = open('output.log', 'a', encoding='utf-8')
    webinfo = Sqldb(dbname).query('select domain,ipaddr,title,server,apps,waf,os from webinfo')
    for i in webinfo:
        (domain, ipaddr, title, server, apps, waf, os) = i
        print('\n' + '*' * 40 + ' ' + domain + ' ' + '*' * 40)
        f.write('\n' + '*' * 40 + ' ' + domain + ' ' + '*' * 40 + '\n')
        print('{}|{}|{}|{}|{}'.format(domain, ipaddr, title, server, waf))
        f.write('{}|{}|{}|{}|{}'.format(domain, ipaddr, title, server, waf) + '\n')
        print('' + str(apps))
        f.write('' + str(apps) + '\n')
        print('' + str(os))
        f.write('' + str(os) + '\n')
        ports = Sqldb(dbname).query(f""select ipaddr,service,port from ports where ipaddr = '{domain}'"")
        for port in ports:
            (domain, server, port) = port
            print(domain, server, port)
            f.write('{}\t{}\t{}'.format(domain, server, port) + '\n')
        urls = Sqldb(dbname).query(f""select title,url,contype,rsp_len,rsp_code from urls where domain = '{domain}'"")
        for url in urls:
            (title, url, contype, rsp_len, rsp_code) = url
            print('{}\t{}\t{}\t{}t{}'.format(title, url, contype, rsp_len, rsp_code))
            f.write('{}\t{}\t{}\t{}t{}'.format(title, url, contype, rsp_len, rsp_code) + '\n')
        vulns = Sqldb(dbname).query(f""select vuln from vuln where domain = '{ipaddr}'"")
        for vuln in vulns:
            print(vuln[0])
            f.write(vuln[0] + '\n')","for vuln in vulns:
    print(vuln[0])
    f.write(vuln[0] + '\n')","[""for vuln in vulns:\n    (vuln_0, *vuln_rvulnmaining) = vuln\n    print(vuln_0)\n    f.write(vuln_0 + '\\n')"", ""for (vuln_0, *vuln_len) in vulns:\n    print(vuln_0)\n    f.write(vuln_0 + '\\n')""]",no_found,0
s3prl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/s3prl/s3prl/downstream/voxceleb2_amsoftmax_segment_eval/dataset.py,https://github.com/s3prl/s3prl/tree/master/s3prl/downstream/voxceleb2_amsoftmax_segment_eval/dataset.py,SpeakerVerifi_dev,segment_processing$162,"def segment_processing(self):
    wav_list = self.pair_dict['wav_table']
    utterance_id = 0
    segment_list = []
    print('processing test set to segments')
    for wav_info in tqdm.tqdm(wav_list):
        label_info = wav_info[0]
        pair_info = wav_info[1]
        (wav, _) = apply_effects_file(wav_info[2], EFFECTS)
        wav = wav.squeeze(0)
        index_end = len(wav) - self.segment_config['window']
        segment_num = index_end // self.segment_config['stride']
        if index_end < 0:
            segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, 0, len(wav), wav_info[2]])
        else:
            for index in range(0, index_end, self.segment_config['stride']):
                segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, index, index + self.segment_config['window'], wav_info[2]])
        utterance_id += 1
    return segment_list","for wav_info in tqdm.tqdm(wav_list):
    label_info = wav_info[0]
    pair_info = wav_info[1]
    (wav, _) = apply_effects_file(wav_info[2], EFFECTS)
    wav = wav.squeeze(0)
    index_end = len(wav) - self.segment_config['window']
    segment_num = index_end // self.segment_config['stride']
    if index_end < 0:
        segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, 0, len(wav), wav_info[2]])
    else:
        for index in range(0, index_end, self.segment_config['stride']):
            segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, index, index + self.segment_config['window'], wav_info[2]])
    utterance_id += 1","[""for wav_info in tqdm.tqdm(wav_list):\n    (wav_info_0, wav_info_1, wav_info_2, *_) = wav_info\n    label_info = wav_info_0\n    pair_info = wav_info_1\n    (wav, _) = apply_effects_file(wav_info_2, EFFECTS)\n    wav = wav.squeeze(0)\n    index_end = len(wav) - self.segment_config['window']\n    segment_num = index_end // self.segment_config['stride']\n    if index_end < 0:\n        segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, 0, len(wav), wav_info_2])\n    else:\n        for index in range(0, index_end, self.segment_config['stride']):\n            segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, index, index + self.segment_config['window'], wav_info_2])\n    utterance_id += 1"", ""for (wav_info_0, wav_info_1, wav_info_2, *wav_info_len) in tqdm.tqdm(wav_list):\n    label_info = \n    wav_info_0\n    pair_info = \n    wav_info_1\n    (wav, _) = apply_effects_file(\n    wav_info_2, EFFECTS)\n    wav = wav.squeeze(0)\n    index_end = len(wav) - self.segment_config['window']\n    segment_num = index_end // self.segment_config['stride']\n    if index_end < 0:\n        segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, 0, len(wav), \n        wav_info_2])\n    else:\n        for index in range(0, index_end, self.segment_config['stride']):\n            segment_list.append([int(label_info), pair_info, str(utterance_id), segment_num, index, index + self.segment_config['window'], \n            wav_info_2])\n    utterance_id += 1""]",no_found,0
mona,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mona/mona.py,https://github.com/corelan/mona/tree/master//mona.py,,main$11682,"def main(args):
    dbg.createLogWindow()
    global currentArgs
    currentArgs = copy.copy(args)
    try:
        starttime = datetime.datetime.now()
        ptr_counter = 0
        commands = {}

        def getBanner():
            banners = {}
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |                         __               __                      |\n'
            bannertext += '    |   _________  ________  / /___ _____     / /____  ____ _____ ___  |\n'
            bannertext += '    |  / ___/ __ \\/ ___/ _ \\/ / __ `/ __ \\   / __/ _ \\/ __ `/ __ `__ \\ |\n'
            bannertext += '    | / /__/ /_/ / /  /  __/ / /_/ / / / /  / /_/  __/ /_/ / / / / / / |\n'
            bannertext += '    | \\___/\\____/_/   \\___/_/\\__,_/_/ /_/   \\__/\\___/\\__,_/_/ /_/ /_/  |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |     https://www.corelan.be | https://www.corelan-training.com    |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[0] = bannertext
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |        _ __ ___    ___   _ __    __ _     _ __   _   _           |\n'
            bannertext += ""    |       | '_ ` _ \\  / _ \\ | '_ \\  / _` |   | '_ \\ | | | |          |\n""
            bannertext += '    |       | | | | | || (_) || | | || (_| | _ | |_) || |_| |          |\n'
            bannertext += '    |       |_| |_| |_| \\___/ |_| |_| \\__,_|(_)| .__/  \\__, |          |\n'
            bannertext += '    |                                          |_|     |___/           |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[1] = bannertext
            bannertext = ''
            bannertext += '    |------------------------------------------------------------------|\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |    _____ ___  ____  ____  ____ _                                 |\n'
            bannertext += '    |    / __ `__ \\/ __ \\/ __ \\/ __ `/  https://www.corelan.be         |\n'
            bannertext += '    |   / / / / / / /_/ / / / / /_/ /  https://www.corelan-training.com|\n'
            bannertext += '    |  /_/ /_/ /_/\\____/_/ /_/\\__,_/  #corelan (Freenode IRC)          |\n'
            bannertext += '    |                                                                  |\n'
            bannertext += '    |------------------------------------------------------------------|\n'
            banners[2] = bannertext
            bannertext = ''
            bannertext += '\n    .##.....##..#######..##....##....###........########..##....##\n'
            bannertext += '    .###...###.##.....##.###...##...##.##.......##.....##..##..##.\n'
            bannertext += '    .####.####.##.....##.####..##..##...##......##.....##...####..\n'
            bannertext += '    .##.###.##.##.....##.##.##.##.##.....##.....########.....##...\n'
            bannertext += '    .##.....##.##.....##.##..####.#########.....##...........##...\n'
            bannertext += '    .##.....##.##.....##.##...###.##.....##.###.##...........##...\n'
            bannertext += '    .##.....##..#######..##....##.##.....##.###.##...........##...\n\n'
            banners[3] = bannertext
            bannerlist = []
            for i in range(0, len(banners)):
                bannerlist.append(i)
            random.shuffle(bannerlist)
            return banners[bannerlist[0]]

        def procHelp(args):
            dbg.log(""     'mona' - Exploit Development Swiss Army Knife - %s (%sbit)"" % (__DEBUGGERAPP__, str(arch)))
            dbg.log('     Plugin version : %s r%s' % (__VERSION__, __REV__))
            dbg.log('     Python version : %s' % getPythonVersion())
            if __DEBUGGERAPP__ == 'WinDBG':
                pykdversion = dbg.getPyKDVersionNr()
                dbg.log('     PyKD version %s' % pykdversion)
            dbg.log('     Written by Corelan - https://www.corelan.be')
            dbg.log('     Project page : https://github.com/corelan/mona')
            dbg.logLines(getBanner(), highlight=1)
            dbg.log('Global options :')
            dbg.log('----------------')
            dbg.log('You can use one or more of the following global options on any command that will perform')
            dbg.log('a search in one or more modules, returning a list of pointers :')
            dbg.log(' -n                     : Skip modules that start with a null byte. If this is too broad, use')
            dbg.log('                          option -cp nonull instead')
            dbg.log(' -o                     : Ignore OS modules')
            dbg.log(' -p <nr>                : Stop search after <nr> pointers.')
            dbg.log(' -m <module,module,...> : only query the given modules. Be sure what you are doing !')
            dbg.log('                          You can specify multiple modules (comma separated)')
            dbg.log('                          Tip : you can use -m *  to include all modules. All other module criteria will be ignored')
            dbg.log('                          Other wildcards : *blah.dll = ends with blah.dll, blah* = starts with blah,')
            dbg.log('                          blah or *blah* = contains blah')
            dbg.log(' -cm <crit,crit,...>    : Apply some additional criteria to the modules to query.')
            dbg.log('                          You can use one or more of the following criteria :')
            dbg.log('                          aslr,safeseh,rebase,nx,os')
            dbg.log('                          You can enable or disable a certain criterium by setting it to true or false')
            dbg.log('                          Example :  -cm aslr=true,safeseh=false')
            dbg.log('                          Suppose you want to search for p/p/r in aslr enabled modules, you could call')
            dbg.log('                          !mona seh -cm aslr')
            dbg.log(' -cp <crit,crit,...>    : Apply some criteria to the pointers to return')
            dbg.log('                          Available options are :')
            dbg.log('                          unicode,ascii,asciiprint,upper,lower,uppernum,lowernum,numeric,alphanum,nonull,startswithnull,unicoderev')
            dbg.log(""                          Note : Multiple criteria will be evaluated using 'AND', except if you are looking for unicode + one crit"")
            dbg.log("" -cpb '\\x00\\x01'        : Provide list with bad chars, applies to pointers"")
            dbg.log('                          You can use .. to indicate a range of bytes (in between 2 bad chars)')
            dbg.log(' -x <access>            : Specify desired access level of the returning pointers. If not specified,')
            dbg.log('                          only executable pointers will be returned.')
            dbg.log('                          Access levels can be one of the following values : R,W,X,RW,RX,WX,RWX or *')
            if not args:
                args = []
            if len(args) > 1:
                thiscmd = args[1].lower().strip()
                if thiscmd in commands:
                    dbg.log('')
                    dbg.log(""Usage of command '%s' :"" % thiscmd)
                    dbg.log('%s' % ('-' * (22 + len(thiscmd))))
                    dbg.logLines(commands[thiscmd].usage)
                    dbg.log('')
                else:
                    aliasfound = False
                    for cmd in commands:
                        if commands[cmd].alias == thiscmd:
                            dbg.log('')
                            dbg.log(""Usage of command '%s' :"" % thiscmd)
                            dbg.log('%s' % ('-' * (22 + len(thiscmd))))
                            dbg.logLines(commands[cmd].usage)
                            dbg.log('')
                            aliasfound = True
                    if not aliasfound:
                        dbg.logLines('\nCommand %s does not exist. Run !mona to get a list of available commands\n' % thiscmd, highlight=1)
            else:
                dbg.logLines('\nUsage :')
                dbg.logLines('-------\n')
                dbg.log(' !mona <command> <parameter>')
                dbg.logLines('\nAvailable commands and parameters :\n')
                items = commands.items()
                items.sort(key=itemgetter(0))
                for item in items:
                    if commands[item[0]].usage != '':
                        aliastxt = ''
                        if commands[item[0]].alias != '':
                            aliastxt = ' / ' + commands[item[0]].alias
                        dbg.logLines('%s | %s' % (item[0] + aliastxt + ' ' * (20 - len(item[0] + aliastxt)), commands[item[0]].description))
                dbg.log('')
                dbg.log('Want more info about a given command ?  Run !mona help <command>', highlight=1)
                dbg.log('')
        commands['help'] = MnCommand('help', 'show help', '!mona help [command]', procHelp)

        def procConfig(args):
            showerror = False
            if not 'set' in args and (not 'get' in args) and (not 'add' in args):
                showerror = True
            if 'set' in args:
                if type(args['set']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['set'].split(' ')
                    if len(params) < 2:
                        showerror = True
            if 'add' in args:
                if type(args['add']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['add'].split(' ')
                    if len(params) < 2:
                        showerror = True
            if 'get' in args:
                if type(args['get']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    params = args['get'].split(' ')
                    if len(params) < 1:
                        showerror = True
            if showerror:
                dbg.log('Usage :')
                dbg.logLines(configUsage, highlight=1)
                return
            else:
                if 'get' in args:
                    dbg.log('Reading value from configuration file')
                    monaConfig = MnConfig()
                    thevalue = monaConfig.get(args['get'])
                    dbg.log('Parameter %s = %s' % (args['get'], thevalue))
                if 'set' in args:
                    dbg.log('Writing value to configuration file')
                    monaConfig = MnConfig()
                    value = args['set'].split(' ')
                    configparam = value[0].strip()
                    dbg.log('Old value of parameter %s = %s' % (configparam, monaConfig.get(configparam)))
                    configvalue = args['set'][0 + len(configparam):len(args['set'])]
                    monaConfig.set(configparam, configvalue)
                    dbg.log('New value of parameter %s = %s' % (configparam, configvalue))
                if 'add' in args:
                    dbg.log('Writing value to configuration file')
                    monaConfig = MnConfig()
                    value = args['add'].split(' ')
                    configparam = value[0].strip()
                    dbg.log('Old value of parameter %s = %s' % (configparam, monaConfig.get(configparam)))
                    configvalue = monaConfig.get(configparam).strip() + ',' + args['add'][0 + len(configparam):len(args['add'])].strip()
                    monaConfig.set(configparam, configvalue)
                    dbg.log('New value of parameter %s = %s' % (configparam, configvalue))

        def procFindJ(args):
            return procFindJMP(args)

        def procFindJMP(args):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            if inspect.stack()[1][3] == 'procFindJ':
                dbg.log("" ** Note : command 'j' has been replaced with 'jmp'. Now launching 'jmp' instead..."", highlight=1)
            criteria = {}
            all_opcodes = {}
            global ptr_to_get
            ptr_to_get = -1
            distancestr = ''
            mindistance = 0
            maxdistance = 0
            showerror = False
            if 'r' in args:
                if type(args['r']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    thisreg = args['r'].upper().strip()
                    validregs = dbglib.Registers32BitsOrder
                    if not thisreg in validregs:
                        showerror = True
            else:
                showerror = True
            if 'distance' in args:
                if type(args['distance']).__name__.lower() == 'bool':
                    showerror = True
                else:
                    distancestr = args['distance']
                    distanceparts = distancestr.split(',')
                    for parts in distanceparts:
                        valueparts = parts.split('=')
                        if len(valueparts) > 1:
                            if valueparts[0].lower() == 'min':
                                try:
                                    mindistance = int(valueparts[1])
                                except:
                                    mindistance = 0
                            if valueparts[0].lower() == 'max':
                                try:
                                    maxdistance = int(valueparts[1])
                                except:
                                    maxdistance = 0
            if maxdistance < mindistance:
                tmp = maxdistance
                maxdistance = mindistance
                mindistance = tmp
            criteria['mindistance'] = mindistance
            criteria['maxdistance'] = maxdistance
            if showerror:
                dbg.log('Usage :')
                dbg.logLines(jmpUsage, highlight=1)
                return
            else:
                (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
                all_opcodes = findJMP(modulecriteria, criteria, args['r'].lower().strip())
            logfile = MnLog('jmp.txt')
            thislog = logfile.reset()
            processResults(all_opcodes, logfile, thislog)

        def procFindSEH(args):
            modulecriteria = {}
            modulecriteria['safeseh'] = False
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            criteria = {}
            specialcases = {}
            all_opcodes = {}
            global ptr_to_get
            ptr_to_get = -1
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            if 'rop' in args:
                criteria['rop'] = True
            if 'all' in args:
                criteria['all'] = True
                specialcases['maponly'] = True
            else:
                criteria['all'] = False
                specialcases['maponly'] = False
            all_opcodes = findSEH(modulecriteria, criteria)
            logfile = MnLog('seh.txt')
            thislog = logfile.reset()
            processResults(all_opcodes, logfile, thislog, specialcases)

        def procShowMODULES(args):
            modulecriteria = {}
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            modulestosearch = getModulesToQuery(modulecriteria)
            showModuleTable('', modulestosearch)

        def procFindROPFUNC(args):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            ropfuncs = {}
            ropfuncoffsets = {}
            (ropfuncs, ropfuncoffsets) = findROPFUNC(modulecriteria, criteria)
            dbg.log('[+] Processing pointers to interesting rop functions')
            logfile = MnLog('ropfunc.txt')
            thislog = logfile.reset()
            processResults(ropfuncs, logfile, thislog)
            global silent
            silent = True
            dbg.log('[+] Processing offsets to pointers to interesting rop functions')
            logfile = MnLog('ropfunc_offset.txt')
            thislog = logfile.reset()
            processResults(ropfuncoffsets, logfile, thislog)

        def procStackPivots(args):
            procROP(args, 'stackpivot')

        def procROP(args, mode='all'):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            depth = 6
            maxoffset = 40
            thedistance = 8
            split = False
            fast = False
            sortedprint = False
            endingstr = ''
            endings = []
            technique = ''
            if 'depth' in args:
                if type(args['depth']).__name__.lower() != 'bool':
                    try:
                        depth = int(args['depth'])
                    except:
                        pass
            if 'offset' in args:
                if type(args['offset']).__name__.lower() != 'bool':
                    try:
                        maxoffset = int(args['offset'])
                    except:
                        pass
            if 'distance' in args:
                if type(args['distance']).__name__.lower() != 'bool':
                    try:
                        thedistance = args['distance']
                    except:
                        pass
            if 'split' in args:
                if type(args['split']).__name__.lower() == 'bool':
                    split = args['split']
            if 's' in args:
                if type(args['s']).__name__.lower() != 'bool':
                    technique = args['s'].replace(""'"", '').replace('""', '').strip().lower()
            if 'fast' in args:
                if type(args['fast']).__name__.lower() == 'bool':
                    fast = args['fast']
            if 'end' in args:
                if type(args['end']).__name__.lower() == 'str':
                    endingstr = args['end'].replace(""'"", '').replace('""', '').strip()
                    endings = endingstr.split('#')
            if 'f' in args:
                if args['f'] != '':
                    criteria['f'] = args['f']
            if 'sort' in args:
                sortedprint = True
            if 'rva' in args:
                criteria['rva'] = True
            if mode == 'stackpivot':
                fast = False
                endings = ''
                split = False
            else:
                mode = 'all'
            findROPGADGETS(modulecriteria, criteria, endings, maxoffset, depth, split, thedistance, fast, mode, sortedprint, technique)

        def procJseh(args):
            results = []
            showred = 0
            showall = False
            if 'all' in args:
                showall = True
            nrfound = 0
            dbg.log('-----------------------------------------------------------------------')
            dbg.log('Search for jmp/call dword[ebp/esp+nn] (and other) combinations started ')
            dbg.log('-----------------------------------------------------------------------')
            opcodej = ['T$\x08', 'd$\x08', 'T$\x14', 'T$\x14', 'T$\x1c', 'T$\x1c', 'T$,', 'T$,', 'T$D', 'T$D', 'T$P', 'T$P', 'U\x0c', 'e\x0c', 'U$', 'e$', 'U0', 'e0', 'U', 'e', 'U', 'e', 'U', 'e', '\x83\x08', '\x83\x08']
            fakeptrcriteria = {}
            fakeptrcriteria['accesslevel'] = '*'
            for opjc in opcodej:
                addys = []
                addys = searchInRange([[opjc, opjc]], 0, TOP_USERLAND, fakeptrcriteria)
                results += addys
                for ptrtypes in addys:
                    for ad1 in addys[ptrtypes]:
                        ptr = MnPointer(ad1)
                        module = ptr.belongsTo()
                        if not module:
                            module = ''
                            page = dbg.getMemoryPageByAddress(ad1)
                            access = page.getAccess(human=True)
                            op = dbg.disasm(ad1)
                            opstring = op.getDisasm()
                            dbg.log('Found %s at 0x%08x - Access: (%s) - Outside of a loaded module' % (opstring, ad1, access), address=ad1, highlight=1)
                            nrfound += 1
                        elif showall:
                            page = dbg.getMemoryPageByAddress(ad1)
                            access = page.getAccess(human=True)
                            op = dbg.disasm(ad1)
                            opstring = op.getDisasm()
                            thismod = MnModule(module)
                            if not thismod.isSafeSEH:
                                extratext = '=== Safeseh : NO ==='
                                showred = 1
                            else:
                                extratext = 'Safeseh protected'
                                showred = 0
                            dbg.log('Found %s at 0x%08x (%s) - Access: (%s) - %s' % (opstring, ad1, module, access, extratext), address=ad1, highlight=showred)
                            nrfound += 1
            dbg.log('Search complete')
            if results:
                dbg.log('Found %d address(es)' % nrfound)
                return 'Found %d address(es) (Check the log Windows for details)' % nrfound
            else:
                dbg.log('No addresses found')
                return 'Sorry, no addresses found'

        def procJOP(args, mode='all'):
            modulecriteria = {}
            modulecriteria['aslr'] = False
            modulecriteria['rebase'] = False
            modulecriteria['os'] = False
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            depth = 6
            if 'depth' in args:
                if type(args['depth']).__name__.lower() != 'bool':
                    try:
                        depth = int(args['depth'])
                    except:
                        pass
            findJOPGADGETS(modulecriteria, criteria, depth)

        def procCreatePATTERN(args):
            size = 0
            pattern = ''
            if '?' in args and args['?'] != '':
                try:
                    if '0x' in args['?'].lower():
                        try:
                            size = int(args['?'], 16)
                        except:
                            size = 0
                    else:
                        size = int(args['?'])
                except:
                    size = 0
            if size == 0:
                dbg.log('Please enter a valid size', highlight=1)
            else:
                pattern = createPattern(size, args)
                dbg.log('Creating cyclic pattern of %d bytes' % size)
                dbg.log(pattern)
                global ignoremodules
                ignoremodules = True
                objpatternfile = MnLog('pattern.txt')
                patternfile = objpatternfile.reset()
                objpatternfile.write('\nPattern of ' + str(size) + ' bytes :\n', patternfile)
                objpatternfile.write('-' * (19 + len(str(size))), patternfile)
                objpatternfile.write('\nASCII:', patternfile)
                objpatternfile.write('\n' + pattern, patternfile)
                patternhex = ''
                for patternchar in pattern:
                    patternhex += str(hex(ord(patternchar))).replace('0x', '\\x')
                objpatternfile.write('\n\nHEX:\n', patternfile)
                objpatternfile.write(patternhex, patternfile)
                patternjs = str2js(pattern)
                objpatternfile.write('\n\nJAVASCRIPT (unescape() friendly):\n', patternfile)
                objpatternfile.write(patternjs, patternfile)
                if not silent:
                    dbg.log(""Note: don't copy this pattern from the log window, it might be truncated !"", highlight=1)
                    dbg.log(""It's better to open %s and copy the pattern from the file"" % patternfile, highlight=1)
                ignoremodules = False
            return

        def procOffsetPATTERN(args):
            egg = ''
            if '?' in args and args['?'] != '':
                try:
                    egg = args['?']
                except:
                    egg = ''
            if egg == '':
                dbg.log('Please enter a valid target', highlight=1)
            else:
                findOffsetInPattern(egg, -1, args)
            return

        def procFileCOMPARE(args):
            modulecriteria = {}
            criteria = {}
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            allfiles = []
            tomatch = ''
            checkstrict = True
            rangeval = 0
            fast = False
            if 'ptronly' in args or 'ptrsonly' in args:
                fast = True
            if 'f' in args:
                if args['f'] != '':
                    rawfilenames = args['f'].replace('""', '')
                    allfiles = [getAbsolutePath(f) for f in rawfilenames.split(',')]
                    dbg.log('[+] Number of files to be examined : %d ' % len(allfiles))
            if 'range' in args:
                if not type(args['range']).__name__.lower() == 'bool':
                    strrange = args['range'].lower()
                    if strrange.startswith('0x') and len(strrange) > 2:
                        rangeval = int(strrange, 16)
                    else:
                        try:
                            rangeval = int(args['range'])
                        except:
                            rangeval = 0
                    if rangeval > 0:
                        dbg.log('[+] Find overlap using pointer +/- range, value %d' % rangeval)
                        dbg.log('    Note : this will significantly slow down the comparison process !')
                else:
                    dbg.log('Please provide a numeric value ^(> 0) with option -range', highlight=1)
                    return
            else:
                if 'contains' in args:
                    if type(args['contains']).__name__.lower() == 'str':
                        tomatch = args['contains'].replace(""'"", '').replace('""', '')
                if 'nostrict' in args:
                    if type(args['nostrict']).__name__.lower() == 'bool':
                        checkstrict = not args['nostrict']
                        dbg.log('[+] Instructions must match in all files ? %s' % checkstrict)
            callfiles = allfiles
            allfiles = []
            for tfile in callfiles:
                if os.path.isdir(tfile):
                    for (root, dirs, files) in os.walk(tfile):
                        for dfile in files:
                            allfiles.append(os.path.join(root, dfile))
                else:
                    allfiles.append(tfile)
            if len(allfiles) > 1:
                findFILECOMPARISON(modulecriteria, criteria, allfiles, tomatch, checkstrict, rangeval, fast)
            else:
                dbg.log('Please specify at least 2 filenames to compare', highlight=1)

        def procFind(args):
            modulecriteria = {}
            criteria = {}
            pattern = ''
            base = 0
            offset = 0
            top = TOP_USERLAND
            consecutive = False
            ftype = ''
            level = 0
            offsetlevel = 0
            if not 'a' in args:
                args['a'] = '*'
            ptronly = False
            if 'ptronly' in args or 'ptrsonly' in args:
                ptronly = True
            if not 'x' in args:
                args['x'] = '*'
            (modulecriteria, criteria) = args2criteria(args, modulecriteria, criteria)
            if criteria['accesslevel'] == '':
                return
            if not 's' in args:
                dbg.log('-s <search pattern (or filename)> is a mandatory argument', highlight=1)
                return
            pattern = args['s']
            if 'unicode' in args:
                criteria['unic'] = True
            if 'b' in args:
                try:
                    base = int(args['b'], 16)
                except:
                    dbg.log('invalid base address: %s' % args['b'], highlight=1)
                    return
            if 't' in args:
                try:
                    top = int(args['t'], 16)
                except:
                    dbg.log('invalid top address: %s' % args['t'], highlight=1)
                    return
            if 'offset' in args:
                if not args['offset'].__class__.__name__ == 'bool':
                    if '0x' in args['offset'].lower():
                        try:
                            offset = 0 - int(args['offset'], 16)
                        except:
                            dbg.log('invalid offset value', highlight=1)
                            return
                    else:
                        try:
                            offset = 0 - int(args['offset'])
                        except:
                            dbg.log('invalid offset value', highlight=1)
                            return
                else:
                    dbg.log('invalid offset value', highlight=1)
                    return
            if 'level' in args:
                try:
                    level = int(args['level'])
                except:
                    dbg.log('invalid level value', highlight=1)
                    return
            if 'offsetlevel' in args:
                try:
                    offsetlevel = int(args['offsetlevel'])
                except:
                    dbg.log('invalid offsetlevel value', highlight=1)
                    return
            if 'c' in args:
                dbg.log('    - Skipping consecutive pointers, showing size instead')
                consecutive = True
            if 'type' in args:
                if not args['type'] in ['bin', 'asc', 'ptr', 'instr', 'file']:
                    dbg.log('Invalid search type : %s' % args['type'], highlight=1)
                    return
                ftype = args['type']
                if ftype == 'file':
                    filename = args['s'].replace('""', '').replace(""'"", '')
                    if not os.path.isfile(filename):
                        dbg.log('Unable to find/read file %s' % filename, highlight=1)
                        return
            rangep2p = 0
            if 'p2p' in args or level > 0:
                dbg.log('    - Looking for pointers to pointers')
                criteria['p2p'] = True
                if 'r' in args:
                    try:
                        rangep2p = int(args['r'])
                    except:
                        pass
                    if rangep2p > 0:
                        dbg.log('    - Will search for close pointers (%d bytes backwards)' % rangep2p)
                if 'p2p' in args:
                    level = 1
            if level > 0:
                dbg.log('    - Recursive levels : %d' % level)
            allpointers = findPattern(modulecriteria, criteria, pattern, ftype, base, top, consecutive, rangep2p, level, offset, offsetlevel)
            logfile = MnLog('find.txt')
            thislog = logfile.reset()
            processResults(allpointers, logfile, thislog, {}, ptronly)
            return

        def procFindWild(args):
            modulecriteria = {}
            criteria = {}
            pattern = ''
            patterntype = ''
            base = 0
            top = TOP_USERLAND
            (modulecriteria, criteria) = args2criteria(","for word in arguments:
    if word[0] == '-':
        word = word.lstrip('-')
        opts[word] = True
        last = word
    elif last != '':
        if str(opts[last]) == 'True':
            opts[last] = word
        else:
            opts[last] = opts[last] + ' ' + word","[""for word in arguments:\n    (word_0, *word_rwordmaining) = word\n    if word_0 == '-':\n        word = word.lstrip('-')\n        opts[word] = True\n        last = word\n    elif last != '':\n        if str(opts[last]) == 'True':\n            opts[last] = word\n        else:\n            opts[last] = opts[last] + ' ' + word""]",no_found,0
SSDTTime,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SSDTTime/SSDTTime.py,https://github.com/corpnewt/SSDTTime/tree/master//SSDTTime.py,SSDT,fake_ec$134,"def fake_ec(self, laptop=False):
    rename = False
    if not self.ensure_dsdt():
        return
    self.u.head('Fake EC')
    print('')
    print('Locating PNP0C09 (EC) devices...')
    ec_list = self.d.get_device_paths_with_hid('PNP0C09')
    ec_to_patch = []
    patches = []
    lpc_name = None
    if len(ec_list):
        lpc_name = '.'.join(ec_list[0][0].split('.')[:-1])
        print(' - Got {}'.format(len(ec_list)))
        print(' - Validating...')
        for x in ec_list:
            device = x[0]
            print(' --> {}'.format(device))
            if device.split('.')[-1] == 'EC':
                if laptop:
                    print(' ----> Named EC device located - no fake needed.')
                    print('')
                    self.u.grab('Press [enter] to return to main menu...')
                    return
                print(' ----> EC called EC. Renaming')
                device = '.'.join(device.split('.')[:-1] + ['EC0'])
                rename = True
            scope = '\n'.join(self.d.get_scope(x[1], strip_comments=True))
            if all((y in scope for y in ['_HID', '_CRS', '_GPE'])):
                print(' ----> Valid EC Device')
                sta = self.d.get_method_paths(device + '._STA')
                if len(sta):
                    print(' ----> Contains _STA method. Skipping')
                    continue
                if not laptop:
                    ec_to_patch.append(device)
            else:
                print(' ----> NOT Valid EC Device')
    else:
        print(' - None found - only needs a Fake EC device')
    print('Locating LPC(B)/SBRG...')
    if lpc_name == None:
        for x in ('LPCB', 'LPC0', 'LPC', 'SBRG', 'PX40'):
            try:
                lpc_name = self.d.get_device_paths(x)[0][0]
                break
            except:
                pass
    if not lpc_name:
        print(' - Could not locate LPC(B)! Aborting!')
        print('')
        self.u.grab('Press [enter] to return to main menu...')
        return
    print(' - Found {}'.format(lpc_name))
    comment = 'SSDT-EC'
    if rename == True:
        patches.append({'Comment': 'EC to EC0', 'Find': '45435f5f', 'Replace': '4543305f'})
        comment += ' (Needs EC to EC0 rename)'
    oc = {'Comment': comment, 'Enabled': True, 'Path': 'SSDT-EC.aml'}
    self.make_plist(oc, 'SSDT-EC.aml', patches)
    print('Creating SSDT-EC...')
    ssdt = '\nDefinitionBlock ("""", ""SSDT"", 2, ""CORP "", ""SsdtEC"", 0x00001000)\n{\n    External ([[LPCName]], DeviceObj)\n'.replace('[[LPCName]]', lpc_name)
    for x in ec_to_patch:
        ssdt += '    External ({}, DeviceObj)\n'.format(x)
    for x in ec_to_patch:
        ssdt += '\n    Scope ([[ECName]])\n    {\n        Method (_STA, 0, NotSerialized)  // _STA: Status\n        {\n            If (_OSI (""Darwin""))\n            {\n                Return (0)\n            }\n            Else\n            {\n                Return (0x0F)\n            }\n        }\n    }\n'.replace('[[LPCName]]', lpc_name).replace('[[ECName]]', x)
    ssdt += '\n    Scope ([[LPCName]])\n    {\n        Device (EC)\n        {\n            Name (_HID, ""ACID0001"")  // _HID: Hardware ID\n            Method (_STA, 0, NotSerialized)  // _STA: Status\n            {\n                If (_OSI (""Darwin""))\n                {\n                    Return (0x0F)\n                }\n                Else\n                {\n                    Return (Zero)\n                }\n            }\n        }\n    }\n}'.replace('[[LPCName]]', lpc_name)
    self.write_ssdt('SSDT-EC', ssdt)
    print('')
    print('Done.')
    print('')
    self.u.grab('Press [enter] to return...')","for x in ec_list:
    device = x[0]
    print(' --> {}'.format(device))
    if device.split('.')[-1] == 'EC':
        if laptop:
            print(' ----> Named EC device located - no fake needed.')
            print('')
            self.u.grab('Press [enter] to return to main menu...')
            return
        print(' ----> EC called EC. Renaming')
        device = '.'.join(device.split('.')[:-1] + ['EC0'])
        rename = True
    scope = '\n'.join(self.d.get_scope(x[1], strip_comments=True))
    if all((y in scope for y in ['_HID', '_CRS', '_GPE'])):
        print(' ----> Valid EC Device')
        sta = self.d.get_method_paths(device + '._STA')
        if len(sta):
            print(' ----> Contains _STA method. Skipping')
            continue
        if not laptop:
            ec_to_patch.append(device)
    else:
        print(' ----> NOT Valid EC Device')","[""for x in ec_list:\n    (x_0, x_1, *_) = x\n    device = x_0\n    print(' --> {}'.format(device))\n    if device.split('.')[-1] == 'EC':\n        if laptop:\n            print(' ----> Named EC device located - no fake needed.')\n            print('')\n            self.u.grab('Press [enter] to return to main menu...')\n            return\n        print(' ----> EC called EC. Renaming')\n        device = '.'.join(device.split('.')[:-1] + ['EC0'])\n        rename = True\n    scope = '\\n'.join(self.d.get_scope(x_1, strip_comments=True))\n    if all((y in scope for y in ['_HID', '_CRS', '_GPE'])):\n        print(' ----> Valid EC Device')\n        sta = self.d.get_method_paths(device + '._STA')\n        if len(sta):\n            print(' ----> Contains _STA method. Skipping')\n            continue\n        if not laptop:\n            ec_to_patch.append(device)\n    else:\n        print(' ----> NOT Valid EC Device')"", ""for (x_0, x_1, *x_len) in ec_list:\n    device = \n    x_0\n    print(' --> {}'.format(device))\n    if device.split('.')[-1] == 'EC':\n        if laptop:\n            print(' ----> Named EC device located - no fake needed.')\n            print('')\n            self.u.grab('Press [enter] to return to main menu...')\n            return\n        print(' ----> EC called EC. Renaming')\n        device = '.'.join(device.split('.')[:-1] + ['EC0'])\n        rename = True\n    scope = '\\n'.join(self.d.get_scope(\n    x_1, strip_comments=True))\n    if all((y in scope for y in ['_HID', '_CRS', '_GPE'])):\n        print(' ----> Valid EC Device')\n        sta = self.d.get_method_paths(device + '._STA')\n        if len(sta):\n            print(' ----> Contains _STA method. Skipping')\n            continue\n        if not laptop:\n            ec_to_patch.append(device)\n    else:\n        print(' ----> NOT Valid EC Device')""]",no_found,0
FASPell,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FASPell/bert_modified/modeling.py,https://github.com/iqiyi/FASPell/tree/master/bert_modified/modeling.py,,get_assignment_map_from_checkpoint$318,"def get_assignment_map_from_checkpoint(tvars, init_checkpoint):
    """"""Compute the union of the current variables and checkpoint variables.""""""
    assignment_map = {}
    initialized_variable_names = {}
    name_to_variable = collections.OrderedDict()
    for var in tvars:
        name = var.name
        m = re.match('^(.*):\\d+$', name)
        if m is not None:
            name = m.group(1)
        name_to_variable[name] = var
    init_vars = tf.train.list_variables(init_checkpoint)
    assignment_map = collections.OrderedDict()
    for x in init_vars:
        (name, var) = (x[0], x[1])
        if name not in name_to_variable:
            continue
        assignment_map[name] = name
        initialized_variable_names[name] = 1
        initialized_variable_names[name + ':0'] = 1
    return (assignment_map, initialized_variable_names)","for x in init_vars:
    (name, var) = (x[0], x[1])
    if name not in name_to_variable:
        continue
    assignment_map[name] = name
    initialized_variable_names[name] = 1
    initialized_variable_names[name + ':0'] = 1","[""for x in init_vars:\n    (x_0, x_1, *_) = x\n    (name, var) = (x_0, x_1)\n    if name not in name_to_variable:\n        continue\n    assignment_map[name] = name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + ':0'] = 1"", ""for (x_0, x_1, *x_len) in init_vars:\n    (name, var) = (x_0, x_1)\n    if name not in name_to_variable:\n        continue\n    assignment_map[name] = name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + ':0'] = 1""]",no_found,0
bert,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bert/modeling.py,https://github.com/google-research/bert/tree/master//modeling.py,,get_assignment_map_from_checkpoint$317,"def get_assignment_map_from_checkpoint(tvars, init_checkpoint):
    """"""Compute the union of the current variables and checkpoint variables.""""""
    assignment_map = {}
    initialized_variable_names = {}
    name_to_variable = collections.OrderedDict()
    for var in tvars:
        name = var.name
        m = re.match('^(.*):\\d+$', name)
        if m is not None:
            name = m.group(1)
        name_to_variable[name] = var
    init_vars = tf.train.list_variables(init_checkpoint)
    assignment_map = collections.OrderedDict()
    for x in init_vars:
        (name, var) = (x[0], x[1])
        if name not in name_to_variable:
            continue
        assignment_map[name] = name
        initialized_variable_names[name] = 1
        initialized_variable_names[name + ':0'] = 1
    return (assignment_map, initialized_variable_names)","for x in init_vars:
    (name, var) = (x[0], x[1])
    if name not in name_to_variable:
        continue
    assignment_map[name] = name
    initialized_variable_names[name] = 1
    initialized_variable_names[name + ':0'] = 1","[""for x in init_vars:\n    (x_0, x_1, *_) = x\n    (name, var) = (x_0, x_1)\n    if name not in name_to_variable:\n        continue\n    assignment_map[name] = name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + ':0'] = 1"", ""for (x_0, x_1, *x_len) in init_vars:\n    (name, var) = (x_0, x_1)\n    if name not in name_to_variable:\n        continue\n    assignment_map[name] = name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + ':0'] = 1""]",no_found,0
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/printing/pretty/pretty.py,https://github.com/sympy/sympy/tree/master/sympy/printing/pretty/pretty.py,PrettyPrinter,_print_Integral$426,"def _print_Integral(self, integral):
    f = integral.function
    prettyF = self._print(f)
    if f.is_Add:
        prettyF = prettyForm(*prettyF.parens())
    arg = prettyF
    for x in integral.limits:
        prettyArg = self._print(x[0])
        if prettyArg.width() > 1:
            prettyArg = prettyForm(*prettyArg.parens())
        arg = prettyForm(*arg.right(' d', prettyArg))
    firstterm = True
    s = None
    for lim in integral.limits:
        h = arg.height()
        H = h + 2
        ascii_mode = not self._use_unicode
        if ascii_mode:
            H += 2
        vint = vobj('int', H)
        pform = prettyForm(vint)
        pform.baseline = arg.baseline + (H - h) // 2
        if len(lim) > 1:
            if len(lim) == 2:
                prettyA = prettyForm('')
                prettyB = self._print(lim[1])
            if len(lim) == 3:
                prettyA = self._print(lim[1])
                prettyB = self._print(lim[2])
            if ascii_mode:
                spc = max(1, 3 - prettyB.width())
                prettyB = prettyForm(*prettyB.left(' ' * spc))
                spc = max(1, 4 - prettyA.width())
                prettyA = prettyForm(*prettyA.right(' ' * spc))
            pform = prettyForm(*pform.above(prettyB))
            pform = prettyForm(*pform.below(prettyA))
        if not ascii_mode:
            pform = prettyForm(*pform.right(' '))
        if firstterm:
            s = pform
            firstterm = False
        else:
            s = prettyForm(*s.left(pform))
    pform = prettyForm(*arg.left(s))
    pform.binding = prettyForm.MUL
    return pform","for x in integral.limits:
    prettyArg = self._print(x[0])
    if prettyArg.width() > 1:
        prettyArg = prettyForm(*prettyArg.parens())
    arg = prettyForm(*arg.right(' d', prettyArg))","[""for x in integral.limits:\n    (x_0, *x_rxmaining) = x\n    prettyArg = self._print(x_0)\n    if prettyArg.width() > 1:\n        prettyArg = prettyForm(*prettyArg.parens())\n    arg = prettyForm(*arg.right(' d', prettyArg))"", ""for (x_0, *x_len) in integral.limits:\n    prettyArg = self._print(x_0)\n    if prettyArg.width() > 1:\n        prettyArg = prettyForm(*prettyArg.parens())\n    arg = prettyForm(*arg.right(' d', prettyArg))""]",no_found,
flow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow/tests/fast_tests/test_traffic_lights.py,https://github.com/flow-project/flow/tree/master/tests/fast_tests/test_traffic_lights.py,TestPOEnv,compare_ordering$167,"def compare_ordering(ordering):
    for x in ordering:
        if not (x[0].startswith('bot') and x[1].startswith('right') and x[2].startswith('top') and x[3].startswith('left')):
            return False
    return True","for x in ordering:
    if not (x[0].startswith('bot') and x[1].startswith('right') and x[2].startswith('top') and x[3].startswith('left')):
        return False","[""for x in ordering:\n    (x_0, x_1, x_2, x_3, *_) = x\n    if not (x_0.startswith('bot') and x_1.startswith('right') and x_2.startswith('top') and x_3.startswith('left')):\n        return False"", ""for (x_0, x_1, x_2, x_3, *x_len) in ordering:\n    if not (x_0.startswith('bot') and x_1.startswith('right') and x_2.startswith('top') and x_3.startswith('left')):\n        return False""]",no_found,0
PaddleX,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex_restful/restful/utils.py,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex_restful/restful/utils.py,PrintableStructure,__str__$715,"def __str__(self):
    result = []
    for x in self._fields_:
        key = x[0]
        value = getattr(self, key)
        fmt = '%s'
        if key in self._fmt_:
            fmt = self._fmt_[key]
        elif '<default>' in self._fmt_:
            fmt = self._fmt_['<default>']
        result.append(('%s: ' + fmt) % (key, value))
    return self.__class__.__name__ + '(' + string.join(result, ', ') + ')'","for x in self._fields_:
    key = x[0]
    value = getattr(self, key)
    fmt = '%s'
    if key in self._fmt_:
        fmt = self._fmt_[key]
    elif '<default>' in self._fmt_:
        fmt = self._fmt_['<default>']
    result.append(('%s: ' + fmt) % (key, value))","[""for x in self._fields_:\n    (x_0, *x_rxmaining) = x\n    key = x_0\n    value = getattr(self, key)\n    fmt = '%s'\n    if key in self._fmt_:\n        fmt = self._fmt_[key]\n    elif '<default>' in self._fmt_:\n        fmt = self._fmt_['<default>']\n    result.append(('%s: ' + fmt) % (key, value))"", ""for (x_0, *x_len) in self._fields_:\n    key = \n    x_0\n    value = getattr(self, key)\n    fmt = '%s'\n    if key in self._fmt_:\n        fmt = self._fmt_[key]\n    elif '<default>' in self._fmt_:\n        fmt = self._fmt_['<default>']\n    result.append(('%s: ' + fmt) % (key, value))""]",no_found,0
PathPlanning,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PathPlanning/Search_based_Planning/Search_2D/D_star.py,https://github.com/zhm-real/PathPlanning/tree/master/Search_based_Planning/Search_2D/D_star.py,DStar,plot_visited$284,"def plot_visited(self, visited):
    color = ['gainsboro', 'lightgray', 'silver', 'darkgray', 'bisque', 'navajowhite', 'moccasin', 'wheat', 'powderblue', 'skyblue', 'lightskyblue', 'cornflowerblue']
    if self.count >= len(color) - 1:
        self.count = 0
    for x in visited:
        plt.plot(x[0], x[1], marker='s', color=color[self.count])","for x in visited:
    plt.plot(x[0], x[1], marker='s', color=color[self.count])","[""for x in visited:\n    (x_0, x_1, *_) = x\n    plt.plot(x_0, x_1, marker='s', color=color[self.count])"", ""for (x_0, x_1, *x_len) in visited:\n    plt.plot(x_0, x_1, marker='s', color=color[self.count])""]",no_found,0
chartpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chartpy/chartpy/engine.py,https://github.com/cuemacro/chartpy/tree/master/chartpy/engine.py,EnginePlotly,plot_chart$1458,"def plot_chart(self, data_frame, style, chart_type):
    if isinstance(data_frame, Figure):
        return self.publish_plot(data_frame, style)
    mode = 'lines'
    if style is None:
        style = Style()
    marker_size = 1
    x = ''
    y = ''
    z = ''
    scale = 1
    try:
        if style.plotly_plot_mode == 'offline_html' and style.scale_factor > 0:
            scale = float(2.0 / 3.0)
    except:
        pass
    if style.plotly_webgl:
        plotly.graph_objs.Scatter = plotly.graph_objs.Scattergl
    cm = ColorMaster()
    data_frame_list = self.split_data_frame_to_list(data_frame, style)
    fig_list = []
    cols = []
    try:
        for data_frame in data_frame_list:
            cols.append(data_frame.columns)
        cols = list(numpy.array(cols).flat)
        color_list = cm.create_color_list(style, [], cols=cols)
        color_spec = []
        if color_list == [None] * len(color_list):
            color_spec = [None] * len(color_list)
            for i in range(0, len(color_list)):
                if color_spec[i] is None:
                    color_spec[i] = self.get_color_list(i)
                try:
                    color_spec[i] = matplotlib.colors.rgb2hex(color_spec[i])
                except:
                    pass
        else:
            for color in color_list:
                color = 'rgba' + str(color)
                color_spec.append(color)
    except Exception as e:
        pass
    start = 0
    title_list = style.title
    if not isinstance(title_list, list):
        title_list = [style.title] * len(data_frame_list)
    for i in range(0, len(data_frame_list)):
        data_frame = data_frame_list[i]
        title = title_list[i]
        if isinstance(data_frame, Figure):
            fig = data_frame
        else:
            if style.drop_na:
                data_frame = data_frame.dropna()
            if isinstance(chart_type, list):
                chart_type_ord = chart_type[i]
            else:
                chart_type_ord = chart_type
            end = start + len(data_frame.columns)
            color_spec1 = color_spec[start:start + end]
            start = end
            if chart_type_ord == 'choropleth':
                for col in data_frame.columns:
                    try:
                        data_frame[col] = data_frame[col].astype(str)
                    except:
                        pass
                if style.color != []:
                    color = style.color
                else:
                    color = [[0.0, 'rgb(242,240,247)'], [0.2, 'rgb(218,218,235)'], [0.4, 'rgb(188,189,220)'], [0.6, 'rgb(158,154,200)'], [0.8, 'rgb(117,107,177)'], [1.0, 'rgb(84,39,143)']]
                text = ''
                if 'text' in data_frame.columns:
                    text = data_frame['Text']
                data = [dict(type='choropleth', colorscale=color, autocolorscale=False, locations=data_frame['Code'], z=data_frame[style.plotly_choropleth_field].astype(float), locationmode=style.plotly_location_mode, text=text, marker=dict(line=dict(color='rgb(255,255,255)', width=1)), colorbar=dict(title=style.units))]
                layout = dict(title=title, geo=dict(scope=style.plotly_scope, projection=dict(type=style.plotly_projection), showlakes=True, lakecolor='rgb(255, 255, 255)'))
                fig = dict(data=data, layout=layout)
            elif style.plotly_helper == 'cufflinks':
                if chart_type_ord == 'surface':
                    fig = data_frame.iplot(kind=chart_type, title=title, xTitle=style.x_title, yTitle=style.y_title, zTitle=style.z_title, x=x, y=y, z=z, mode=mode, size=marker_size, sharing=style.plotly_sharing, theme=style.plotly_theme, bestfit=style.line_of_best_fit, legend=style.display_legend, colorscale=style.color, dimensions=(style.width * abs(style.scale_factor) * scale, style.height * abs(style.scale_factor) * scale), asFigure=True)
                    if style.x_axis_range is not None:
                        fig.update_layout(scene=dict(xaxis=dict(range=style.x_axis_range)))
                    if style.y_axis_range is not None:
                        fig.update_layout(scene=dict(xaxis=dict(range=style.y_axis_range)))
                    if style.z_axis_range is not None:
                        fig.update_layout(scene=dict(xaxis=dict(range=style.z_axis_range)))
                elif chart_type_ord == 'heatmap':
                    fig = data_frame.iplot(kind=chart_type, title=title, xTitle=style.x_title, yTitle=style.y_title, x=x, y=y, mode=mode, size=marker_size, sharing=style.plotly_sharing, theme=style.plotly_theme, bestfit=style.line_of_best_fit, legend=style.display_legend, colorscale=style.color, dimensions=(style.width * abs(style.scale_factor) * scale, style.height * abs(style.scale_factor) * scale), asFigure=True)
                elif chart_type_ord == 'annotated-heatmap':
                    if style.color == []:
                        color = None
                    else:
                        color = style.color
                    fig = px.imshow(data_frame, text_auto=True, title=style.title, color_continuous_scale=color, width=style.width * abs(style.scale_factor) * scale, height=style.height * abs(style.scale_factor) * scale)
                else:
                    full_line = style.connect_line_gaps
                    if chart_type_ord == 'line':
                        full_line = True
                        mode = 'lines'
                    elif chart_type_ord in ['dash', 'dashdot', 'dot']:
                        chart_type_ord = 'scatter'
                    elif chart_type_ord == 'line+markers':
                        full_line = True
                        chart_type_ord = 'line'
                        mode = 'lines+markers'
                        marker_size = 5
                    elif chart_type_ord == 'scatter':
                        mode = 'markers'
                        marker_size = 5
                    elif chart_type_ord == 'bubble':
                        chart_type_ord = 'scatter'
                        mode = 'markers'
                    if style.plotly_theme is None:
                        plotly_theme = 'pearl'
                    else:
                        plotly_theme = style.plotly_theme
                    m = 0
                    y_axis_2_series = [x for x in style.y_axis_2_series if x in data_frame.columns]
                    vspan = None
                    if style.x_shade_dates is not None:
                        vspan = {'x0': data_frame.index[0].strftime('%Y-%m-%d'), 'x1': data_frame.index[-1].strftime('%Y-%m-%d'), 'color': 'rgba(30,30,30,0.3)', 'fill': True, 'opacity': 0.4}
                    while m < 10:
                        if True:
                            if vspan is None:
                                fig = data_frame.iplot(kind=chart_type_ord, title=title, xTitle=style.x_title, yTitle=style.y_title, x=x, y=y, z=z, subplots=False, sharing=style.plotly_sharing, mode=mode, secondary_y=y_axis_2_series, size=marker_size, theme=plotly_theme, colorscale='dflt', bestfit=style.line_of_best_fit, legend=style.display_legend, width=style.linewidth, color=color_spec1, dimensions=(style.width * abs(style.scale_factor) * scale, style.height * abs(style.scale_factor) * scale), asFigure=True)
                            else:
                                fig = data_frame.iplot(kind=chart_type_ord, title=title, xTitle=style.x_title, yTitle=style.y_title, x=x, y=y, z=z, subplots=False, sharing=style.plotly_sharing, mode=mode, secondary_y=y_axis_2_series, size=marker_size, theme=plotly_theme, colorscale='dflt', bestfit=style.line_of_best_fit, legend=style.display_legend, width=style.linewidth, color=color_spec1, dimensions=(style.width * abs(style.scale_factor) * scale, style.height * abs(style.scale_factor) * scale), vspan=vspan, asFigure=True)
                            m = 10
                            break
                            print('Will attempt to re-render: ' + str(e))
                            import time
                            time.sleep(0.3)
                        m = m + 1
                    if full_line:
                        for z in range(0, len(fig['data'])):
                            fig['data'][z].connectgaps = style.connect_line_gaps
                            for k in range(0, len(fig['data'])):
                                if full_line:
                                    fig['data'][k].connectgaps = style.connect_line_gaps
                    if style.line_shape != None:
                        if isinstance(style.line_shape, str):
                            line_shape = [style.line_shape] * len(fig['data'])
                        else:
                            line_shape = style.line_shape
                        for k in range(0, len(fig['data'])):
                            fig['data'][k].line.shape = line_shape[k]
                    if style.stackgroup is not None:
                        if isinstance(style.stackgroup, list):
                            stackgroup = style.stackgroup
                        else:
                            stackgroup = ['A'] * len(fig['data'])
                        for k in range(0, len(fig['data'])):
                            fig['data'][k].stackgroup = stackgroup[k]
            elif style.plotly_helper == 'plotly_express':
                pass
        if style.title is not None:
            try:
                fig.update_layout(title=style.title)
            except:
                pass
        if style.y_2_title is not None:
            if style.y_2_title != '':
                try:
                    fig['layout'].update(yaxis2=dict(title=style.y_2_title))
                except:
                    pass
        if style.x_axis_range is not None:
            try:
                fig['layout'].update(xaxis=dict(range=style.x_axis_range, autorange=False))
            except:
                pass
        if style.y_axis_range is not None:
            try:
                fig['layout'].update(yaxis=dict(range=style.y_axis_range, autorange=False))
            except:
                pass
        if style.y_axis_2_range is not None:
            try:
                fig['layout'].update(yaxis2=dict(range=style.y_axis_2_range, autorange=False))
            except:
                pass
        if style.z_axis_range is not None:
            try:
                fig['layout'].update(zaxis=dict(range=style.z_axis_range, autorange=False))
            except:
                pass
        if style.font_family is not None:
            try:
                fig.update_layout(font_family=style.font_family)
            except:
                pass
        if style.x_axis_type is not None:
            try:
                fig.update_xaxes(type=style.x_axis_type)
            except:
                pass
        if style.y_axis_type is not None:
            try:
                fig.update_yaxes(type=style.y_axis_type)
            except:
                pass
        if style.x_dtick is not None:
            try:
                fig.update_layout(xaxis=dict(tickmode='linear', dtick=style.x_dtick))
            except:
                pass
        if style.y_dtick is not None:
            try:
                fig.update_layout(yaxis=dict(tickmode='linear', dtick=style.y_dtick))
            except:
                pass
        fig = self._multi_shade(fig, style)
        if style.legend_x_anchor is not None:
            try:
                fig.update_layout(legend=dict(xanchor=style.legend_x_anchor))
            except:
                pass
        if style.legend_y_anchor is not None:
            try:
                fig.update_layout(legend=dict(yanchor=style.legend_y_anchor))
            except:
                pass
        if style.legend_x_pos is not None:
            try:
                fig.update_layout(legend=dict(x=style.legend_x_pos))
            except:
                pass
        if style.legend_y_pos is not None:
            try:
                fig.update_layout(legend=dict(y=style.legend_y_pos))
            except:
                pass
        if style.legend_bgcolor is not None:
            try:
                fig.update_layout(legend=dict(bgcolor=style.legend_bgcolor))
            except:
                pass
        if style.legend_orientation is not None:
            try:
                fig.update_layout(legend=dict(orientation=style.legend_orientation))
            except:
                pass
        if style.barmode is not None:
            try:
                fig.update_layout(barmode=style.barmode)
            except:
                pass
        fig_list.append(fig)
    if len(fig_list) > 1 and style.animate_figure == False and (style.subplots == True):
        from plotly.subplots import make_subplots
        if style.subplot_titles:
            fig = make_subplots(rows=len(fig_list), cols=1, subplot_titles=style.subplot_titles)
        else:
            fig = make_subplots(rows=len(fig_list), cols=1)
        for (i, f) in enumerate(fig_list):
            f = f.data[0]
            f.update(legendgroup=i)
            fig.add_trace(f, row=i + 1, col=1)
        if not isinstance(style.title, list):
            fig['layout'].update(title=style.title)
        fig.update_layout(height=style.height * abs(style.scale_factor), width=style.width * abs(style.scale_factor), showlegend=style.display_legend)
    elif style.animate_figure:
        fig = fig_list[0]
        fig['layout']['updatemenus'] = [{'buttons': [{'args': [None, {'frame': {'duration': style.animate_frame_ms, 'redraw': True}, 'fromcurrent': True, 'transition': {'duration': style.animate_frame_ms, 'easing': 'quadratic-in-out'}}], 'label': 'Play', 'method': 'animate'}, {'args': [[None], {'frame': {'duration': 0, 'redraw': True}, 'mode': 'immediate', 'transition': {'duration': 0}}], 'label': 'Pause', 'method': 'animate'}], 'direction': 'left', 'pad': {'r': 10, 't': 87}, 'showactive': False, 'type': 'buttons', 'x': 0.1, 'xanchor': 'right', 'y': 0, 'yanchor': 'top'}]
        if style.animate_titles is not None:
            animate_titles = style.animate_titles
        else:
            animate_titles = list(range(0, len(fig_list)))
        frames = []
        for (fig_temp, title_temp) in zip(fig_list, animate_titles):
            frames.append(go.Frame(data=fig_temp['data'], name=str(title_temp), layout=go.Layout(title=str(title_temp))))
        fig.update(frames=frames)
        sliders_dict = {'active': 0, 'yanchor': 'top', 'xanchor': 'left', 'currentvalue': {'visible': True, 'xanchor': 'right'}, 'transition': {'duration': style.animate_frame_ms, 'easing': 'cubic-in-out'}, 'pad': {'b': 10, 't': 50}, 'len': 0.9, 'x': 0.1, 'y': 0, 'steps': []}
        for i in range(0, len(fig_list)):
            slider_step = {'args': [[animate_titles[i]], {'frame': {'duration': style.animate_frame_ms, 'redraw': True}, 'mode': 'immediate', 'transition': {'duration': style.animate_frame_ms}}], 'label': str(animate_titles[i]), 'method': 'animate'}
            sliders_dict['steps'].append(slider_step)
        fig['layout']['sliders'] = [sliders_dict]
    else:
        fig = fig_list[0]
    fig.update(dict(layout=dict(legend=dict(x=0.05, y=1))))
    if style.thin_margin:
        fig.update(dict(layout=dict(margin=go.layout.Margin(l=20, r=20, b=40, t=40, pad=0))))
    fig.update(dict(layout=dict(paper_bgcolor='rgba(0,0,0,0)')))
    fig.update(dict(layout=dict(plot_bgcolor='rgba(0,0,0,0)')))
    if not style.x_axis_showgrid:
        fig.update(dict(layout=dict(xaxis=dict(showgrid=style.x_axis_showgrid))))
    if not style.y_axis_showgrid:
        fig.update(dict(layout=dict(yaxis=dict(showgrid=style.y_axis_showgrid))))
    if not style.y_axis_2_showgrid:
        fig.update(dict(layout=dict(yaxis2=dict(showgrid=style.y_axis_2_showgrid))))
    if style.subplots == False and isinstance(chart_type, list):
        for j in range(0, len(fig['data'])):
            mode = None
            dash = None
            line_shape = None
            if chart_type[j] == 'line':
                mode = 'lines'
            elif chart_type[j] == 'line+markers':
                mode = 'lines+markers'
            elif chart_type[j] == 'scatter':
                mode = 'markers'
            elif chart_type[j] in ['dash', 'dashdot', 'dot']:
                dash = chart_type[j]
                mode = 'lines'
            elif chart_type[j] in ['hv', 'vh', 'vhv', 'spline', 'linear']:
                line_shape = chart_type[j]
                mode = 'lines'
            elif chart_type[j] == 'bubble':
                mode = 'markers'
                bubble_series = style.bubble_series[cols[j]]
                bubble_series = bubble_series.fillna(0)
                scale = float(bubble_series.max())
                fig['data'][j].marker.size = (style.bubble_size_scalar * (bubble_series.values / scale)).tolist()
            if mode is not None:
                fig['data'][j].mode = mode
            if dash is not None:
                fig['data'][j].line.dash = dash
            if line_shape is not None:
                fig['data'][j].line.shape = line_shape
    if style.candlestick_series is not None and (not style.plotly_webgl):
        if isinstance(style.candlestick_series, Figure):
            fig_candle = style.candlestick_series
        else:
            fig_candle = create_candlestick(style.candlestick_series['open'], style.candlestick_series['high'], style.candlestick_series['low'], style.candlestick_series['close'], dates=style.candlestick_series['close'].index)
        if style.candlestick_increasing_color is not None:
            fig_candle['data'][0].fillcolor = cm.get_color_code(style.candlestick_increasing_color)
            fig_candle['data'][0].line.color = cm.get_color_code(style.candlestick_increasing_line_color)
        if style.candlestick_decreasing_color is not None:
            fig_candle['data'][1].fillcolor = cm.get_color_code(style.candlestick_decreasing_color)
            fig_candle['data'][1].line.color = cm.get_color_code(style.candlestick_decreasing_line_color)
        try:
            fig.data.append(fig_candle.data[0])
            fig.data.append(fig_candle.data[1])
        except:
            fig.add_trace(fig_candle.data[0])
            fig.add_trace(fig_candle.data[1])
    if style.overlay_fig is not None:
        for d in style.overlay_fig.data:
            fig.add_trace(d)
    x_y_line_list = []
    for x_y_line in style.x_y_line:
        start = x_y_line[0]
        finish = x_y_line[1]
        x_y_line_list.append({'type': 'line', 'x0': start[0], 'y0': start[1], 'x1': finish[0], 'y1': finish[1], 'line': {'color': 'black', 'width': 0.5, 'dash': 'dot'}})
    if len(x_y_line_list) > 0:
        fig.layout.shapes = x_y_line_list
    return self.publish_plot(fig, style)","for x_y_line in style.x_y_line:
    start = x_y_line[0]
    finish = x_y_line[1]
    x_y_line_list.append({'type': 'line', 'x0': start[0], 'y0': start[1], 'x1': finish[0], 'y1': finish[1], 'line': {'color': 'black', 'width': 0.5, 'dash': 'dot'}})","[""for x_y_line in style.x_y_line:\n    (x_y_line_0, x_y_line_1, *_) = x_y_line\n    start = x_y_line_0\n    finish = x_y_line_1\n    x_y_line_list.append({'type': 'line', 'x0': start[0], 'y0': start[1], 'x1': finish[0], 'y1': finish[1], 'line': {'color': 'black', 'width': 0.5, 'dash': 'dot'}})"", ""for (x_y_line_0, x_y_line_1, *x_y_line_len) in style.x_y_line:\n    start = \n    x_y_line_0\n    finish = \n    x_y_line_1\n    x_y_line_list.append({'type': 'line', 'x0': start[0], 'y0': start[1], 'x1': finish[0], 'y1': finish[1], 'line': {'color': 'black', 'width': 0.5, 'dash': 'dot'}})""]",no_found,0
OnlyFans,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OnlyFans/modules/onlyfans.py,https://github.com/DIGITALCRIMINAL/OnlyFans/tree/master/modules/onlyfans.py,,scrape_choice$166,"def scrape_choice(authed: create_auth, subscription):
    user_id = subscription.id
    post_count = subscription.postsCount
    archived_count = subscription.archivedPostsCount
    message = 'Scrape: 0 = All | 1 = Images | 2 = Videos | 3 = Audios | 4 = Texts'
    media_types = [[['', 'All'], ['', 'Images'], ['', 'Videos'], ['', 'Audios'], ['', 'Texts']], message]
    choice_list = main_helper.choose_option(media_types, auto_media_choice)
    user_api = OnlyFans.endpoint_links(user_id).users
    message_api = OnlyFans.endpoint_links(user_id).message_api
    stories_api = OnlyFans.endpoint_links(user_id).stories_api
    list_highlights = OnlyFans.endpoint_links(user_id).list_highlights
    post_api = OnlyFans.endpoint_links(user_id).post_api
    archived_api = OnlyFans.endpoint_links(user_id).archived_posts
    only_links = False
    mandatory = [download_directory, only_links]
    y = ['photo', 'video', 'stream', 'gif', 'audio', 'text']
    u_array = ['You have chosen to scrape {}', [user_api, media_types, *mandatory, post_count], 'Profile']
    s_array = ['You have chosen to scrape {}', [stories_api, media_types, *mandatory, post_count], 'Stories']
    h_array = ['You have chosen to scrape {}', [list_highlights, media_types, *mandatory, post_count], 'Highlights']
    p_array = ['You have chosen to scrape {}', [post_api, media_types, *mandatory, post_count], 'Posts']
    m_array = ['You have chosen to scrape {}', [message_api, media_types, *mandatory, post_count], 'Messages']
    a_array = ['You have chosen to scrape {}', [archived_api, media_types, *mandatory, archived_count], 'Archived']
    array = [u_array, s_array, p_array, a_array, m_array]
    new_array = []
    valid_input = True
    for xxx in array:
        if xxx[2] == 'Mass Messages':
            if not subscription.is_me():
                continue
        new_item = dict()
        new_item['api_message'] = xxx[0]
        new_item['api_array'] = {}
        new_item['api_array']['api_link'] = xxx[1][0]
        new_item['api_array']['media_types'] = xxx[1][1]
        new_item['api_array']['directory'] = xxx[1][2]
        new_item['api_array']['only_links'] = xxx[1][3]
        new_item['api_array']['post_count'] = xxx[1][4]
        formatted = format_media_types()
        final_format = []
        for choice in choice_list:
            choice = choice[1]
            final_format.extend([result for result in formatted if result[0] == choice])
        new_item['api_array']['media_types'] = final_format
        new_item['api_type'] = xxx[2]
        if valid_input:
            new_array.append(new_item)
    return new_array","for xxx in array:
    if xxx[2] == 'Mass Messages':
        if not subscription.is_me():
            continue
    new_item = dict()
    new_item['api_message'] = xxx[0]
    new_item['api_array'] = {}
    new_item['api_array']['api_link'] = xxx[1][0]
    new_item['api_array']['media_types'] = xxx[1][1]
    new_item['api_array']['directory'] = xxx[1][2]
    new_item['api_array']['only_links'] = xxx[1][3]
    new_item['api_array']['post_count'] = xxx[1][4]
    formatted = format_media_types()
    final_format = []
    for choice in choice_list:
        choice = choice[1]
        final_format.extend([result for result in formatted if result[0] == choice])
    new_item['api_array']['media_types'] = final_format
    new_item['api_type'] = xxx[2]
    if valid_input:
        new_array.append(new_item)","[""for xxx in array:\n    (xxx_0, (xxx_1_0, xxx_1_1, xxx_1_2, xxx_1_3, xxx_1_4), xxx_2, *xxx_rxxxmaining) = xxx\n    if xxx_2 == 'Mass Messages':\n        if not subscription.is_me():\n            continue\n    new_item = dict()\n    new_item['api_message'] = xxx_0\n    new_item['api_array'] = {}\n    new_item['api_array']['api_link'] = xxx_1_0\n    new_item['api_array']['media_types'] = xxx_1_1\n    new_item['api_array']['directory'] = xxx_1_2\n    new_item['api_array']['only_links'] = xxx_1_3\n    new_item['api_array']['post_count'] = xxx_1_4\n    formatted = format_media_types()\n    final_format = []\n    for choice in choice_list:\n        choice = choice[1]\n        final_format.extend([result for result in formatted if result[0] == choice])\n    new_item['api_array']['media_types'] = final_format\n    new_item['api_type'] = xxx_2\n    if valid_input:\n        new_array.append(new_item)""]",no_found,
gif-for-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gif-for-cli/tests/test_display.py,https://github.com/google/gif-for-cli/tree/master/tests/test_display.py,TestDisplayTxtFrames,test_0_loops$58,"def test_0_loops(self):
    stdout = io.StringIO()
    txt_frames = self.txt_frames
    num_loops = 0
    error_after_num_loops = 5
    error_after_num_sleep_calls = error_after_num_loops * len(txt_frames)
    with patch('time.sleep') as mock_sleep:
        num_sleep_calls = 0

        def sleep_side_effect(s):
            nonlocal num_sleep_calls
            num_sleep_calls += 1
            if num_sleep_calls >= error_after_num_sleep_calls:
                raise KeyboardInterrupt()
            return
        mock_sleep.side_effect = sleep_side_effect
        display_txt_frames(txt_frames, stdout, num_loops, self.seconds_per_frame)
    self.assertEqual(mock_sleep.call_count, error_after_num_loops * len(txt_frames))
    for call in mock_sleep.call_args_list:
        self.assertEqual(call[0][0], self.seconds_per_frame)
    output_ending = '\n' + ANSI_RESET + '\n'
    output = stdout.getvalue()
    self.assertEqual(output[-len(output_ending):], output_ending)
    output = output[:-len(output_ending)]
    output = output.split('\n' + ANSI_CURSOR_UP * self.height)
    self.assertEqual(output, self.txt_frames * error_after_num_loops)","for call in mock_sleep.call_args_list:
    self.assertEqual(call[0][0], self.seconds_per_frame)","['for ((call_0_0, *call_0_len), *call_len) in mock_sleep.call_args_list:\n    self.assertEqual(call_0_0, self.seconds_per_frame)']",no_found,0
st2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/st2/contrib/runners/action_chain_runner/tests/unit/test_actionchain.py,https://github.com/StackStorm/st2/tree/master/contrib/runners/action_chain_runner/tests/unit/test_actionchain.py,TestActionChainRunner,test_chain_runner_dependent_param_temp$559,"def test_chain_runner_dependent_param_temp(self, request):
    chain_runner = acr.get_runner()
    chain_runner.entry_point = CHAIN_DEP_INPUT
    chain_runner.action = ACTION_1
    action_ref = ResourceReference.to_string_reference(name=ACTION_1.name, pack=ACTION_1.pack)
    chain_runner.liveaction = LiveActionDB(action=action_ref)
    chain_runner.pre_run()
    chain_runner.run({'s1': 1, 's2': 2, 's3': 3, 's4': 4})
    self.assertNotEqual(chain_runner.chain_holder.actionchain, None)
    expected_values = [{'p1': '1'}, {'p1': '1'}, {'p2': '1', 'p3': '1', 'p1': '1'}]
    for call_args in request.call_args_list:
        self.assertIn(call_args[0][0].parameters, expected_values)
        expected_values.remove(call_args[0][0].parameters)
    self.assertEqual(len(expected_values), 0, 'Not all expected values received.')","for call_args in request.call_args_list:
    self.assertIn(call_args[0][0].parameters, expected_values)
    expected_values.remove(call_args[0][0].parameters)","['for ((call_args_0_0, *call_args_0_len), *call_args_len) in request.call_args_list:\n    self.assertIn(call_args_0_0.parameters, expected_values)\n    expected_values.remove(call_args_0_0.parameters)']",no_found,0
edx-platform,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/common/djangoapps/util/testing.py,https://github.com/edx/edx-platform/tree/master/common/djangoapps/util/testing.py,EventTestMixin,assert_event_emission_count$99,"def assert_event_emission_count(self, event_name, expected_count):
    """"""
        Verify that the event with the given name was emitted
        a specific number of times.
        """"""
    actual_count = 0
    for call_args in self.mock_tracker.emit.call_args_list:
        if call_args[0][0] == event_name:
            actual_count += 1
    assert actual_count == expected_count","for call_args in self.mock_tracker.emit.call_args_list:
    if call_args[0][0] == event_name:
        actual_count += 1","['for ((call_args_0_0, *call_args_0_len), *call_args_len) in self.mock_tracker.emit.call_args_list:\n    if \n    call_args_0_0 == event_name:\n        actual_count += 1']",no_found,0
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/op/smooth_uv.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/op/smooth_uv.py,MUV_OT_SmoothUV,__smooth_wo_transmission$107,"def __smooth_wo_transmission(self, loop_seqs, uv_layer):
    loops = []
    for hseq in loop_seqs:
        loops.extend([hseq[0][0], hseq[0][1]])
    full_vlen = 0
    accm_vlens = [0.0]
    full_uvlen = 0
    accm_uvlens = [0.0]
    orig_uvs = [loop_seqs[0][0][0][uv_layer].uv.copy()]
    for (l1, l2) in zip(loops[:-1], loops[1:]):
        diff_v = l2.vert.co - l1.vert.co
        full_vlen = full_vlen + diff_v.length
        accm_vlens.append(full_vlen)
        diff_uv = l2[uv_layer].uv - l1[uv_layer].uv
        full_uvlen = full_uvlen + diff_uv.length
        accm_uvlens.append(full_uvlen)
        orig_uvs.append(l2[uv_layer].uv.copy())
    for (hidx, hseq) in enumerate(loop_seqs):
        pair = hseq[0]
        for (pidx, l) in enumerate(pair):
            if self.select:
                l[uv_layer].select = True
            if hidx == 0 and pidx == 0 or (hidx == len(loop_seqs) - 1 and pidx == len(pair) - 1):
                continue
            tgt_noinfl = full_uvlen * (hidx + pidx) / len(loop_seqs)
            tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
            target_length = tgt_noinfl * (1 - self.mesh_infl) + tgt_infl * self.mesh_infl
            for i in range(len(accm_uvlens[:-1])):
                if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                    tgt_seg_len = target_length - accm_uvlens[i]
                    seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                    uv1 = orig_uvs[i]
                    uv2 = orig_uvs[i + 1]
                    target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                    break
            else:
                self.report({'ERROR'}, 'Failed to get target UV')
                return {'CANCELLED'}
            l[uv_layer].uv = target_uv","for hseq in loop_seqs:
    loops.extend([hseq[0][0], hseq[0][1]])","['for ((hseq_0_0, hseq_0_1, *hseq_0_len), *hseq_len) in loop_seqs:\n    loops.extend([hseq_0_0, hseq_0_1])']",no_found,0
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2020/med_test.py,https://github.com/3b1b/videos/tree/master/_2020/med_test.py,SamplePopulationBreastCancer,construct$820,"def construct(self):
    title = TexText('Sample of ', '$1{,}000$', ' women', font_size=72)
    title.add(Underline(title, color=GREY_B))
    title.to_edge(UP, buff=MED_SMALL_BUFF)
    self.add(title)
    woman = WomanIcon()
    globals()['woman'] = woman
    population = VGroup(*[woman.copy() for x in range(1000)])
    population.arrange_in_grid(25, 40, buff=LARGE_BUFF, fill_rows_first=False)
    population.set_height(6)
    population.next_to(title, DOWN)
    counter = Integer(1000, edge_to_fix=UL)
    counter.replace(title[1])
    counter.set_value(0)
    title[1].set_opacity(0)
    self.play(ShowIncreasingSubsets(population), ChangeDecimalToValue(counter, 1000), run_time=5)
    self.remove(counter)
    title[1].set_opacity(1)
    self.wait()
    rects = VGroup(Rectangle(), Rectangle())
    rects.set_height(6)
    rects[0].set_width(4, stretch=True)
    rects[1].set_width(8, stretch=True)
    rects[0].set_stroke(YELLOW, 3)
    rects[1].set_stroke(GREY, 3)
    rects.arrange(RIGHT)
    rects.center().to_edge(DOWN, buff=MED_SMALL_BUFF)
    positive_cases = population[:10]
    negative_cases = population[10:]
    positive_cases.generate_target()
    positive_cases.target.move_to(rects[0])
    positive_cases.target.set_color(YELLOW)
    negative_cases.generate_target()
    negative_cases.target.set_height(rects[1].get_height() * 0.8)
    negative_cases.target.move_to(rects[1])
    positive_words = TexText('1\\% ', 'Have breast cancer', font_size=36)
    positive_words.set_color(YELLOW)
    positive_words.next_to(rects[0], UP, SMALL_BUFF)
    negative_words = TexText('99\\% ', 'Do not have cancer', font_size=36)
    negative_words.set_color(GREY_B)
    negative_words.next_to(rects[1], UP, SMALL_BUFF)
    self.play(MoveToTarget(positive_cases), MoveToTarget(negative_cases), Write(positive_words, run_time=1), Write(negative_words, run_time=1), FadeIn(rects))
    self.wait()
    scan_lines = VGroup(*(Line(FRAME_HEIGHT * DOWN / 2, icon.get_center(), stroke_width=1, stroke_color=interpolate_color(BLUE, GREEN, random.random())) for icon in population))
    self.play(LaggedStartMap(ShowCreationThenFadeOut, scan_lines, lag_ratio=1 / len(scan_lines), run_time=3))
    self.wait()
    tpr_words = TexText('9 True positives', font_size=36)
    fnr_words = TexText('1 False negative', font_size=36)
    tnr_words = TexText('901 True negatives', font_size=36)
    fpr_words = TexText('89 False positives', font_size=36)
    tpr_words.set_color(GREEN_B)
    fnr_words.set_color(RED_D)
    tnr_words.set_color(RED_B)
    fpr_words.set_color(GREEN_D)
    tp_cases = positive_cases[:9]
    fn_cases = positive_cases[9:]
    tpr_words.next_to(tp_cases, UP)
    fnr_words.next_to(fn_cases, DOWN)
    signs = VGroup()
    for woman in tp_cases:
        sign = Tex('+')
        sign.set_color(GREEN_B)
        sign.match_height(woman)
        sign.next_to(woman, RIGHT, SMALL_BUFF)
        woman.sign = sign
        signs.add(sign)
    for woman in fn_cases:
        sign = Tex('-')
        sign.set_color(RED)
        sign.match_width(signs[0])
        sign.next_to(woman, RIGHT, SMALL_BUFF)
        woman.sign = sign
        signs.add(sign)
    boxes = VGroup()
    for (n, woman) in enumerate(positive_cases):
        box = SurroundingRectangle(woman, buff=0)
        box.set_stroke(width=2)
        if woman in tp_cases:
            box.set_color(GREEN)
        else:
            box.set_color(RED)
        woman.box = box
        boxes.add(box)
    self.play(FadeIn(tpr_words, shift=0.2 * UP), ShowIncreasingSubsets(signs[:9]), ShowIncreasingSubsets(boxes[:9]))
    self.wait()
    self.play(FadeIn(fnr_words, shift=0.2 * DOWN), Write(signs[9:]), ShowCreation(boxes[9:]))
    self.wait()
    negative_cases.sort(lambda p: -p[1])
    num_fp = int(len(negative_cases) * 0.09)
    fp_cases = negative_cases[:num_fp]
    tn_cases = negative_cases[num_fp:]
    new_boxes = VGroup()
    for (n, woman) in enumerate(negative_cases):
        box = SurroundingRectangle(woman, buff=0)
        box.set_stroke(width=2)
        if woman in fp_cases:
            box.set_color(GREEN)
        else:
            box.set_color(RED)
        woman.box = box
        new_boxes.add(box)
    fpr_words.next_to(fp_cases, UP, buff=SMALL_BUFF)
    tnr_words.next_to(tn_cases, DOWN, buff=0.2)
    self.play(FadeIn(fpr_words, shift=0.2 * UP), ShowIncreasingSubsets(new_boxes[:num_fp]))
    self.wait()
    self.play(FadeIn(tnr_words, shift=0.2 * DOWN), ShowIncreasingSubsets(new_boxes[num_fp:]))
    self.wait()
    self.remove(boxes, new_boxes, population)
    for woman in population:
        woman.add(woman.box)
    self.add(population)
    for (cases, nr, rect) in zip([tp_cases, fp_cases], [3, 7], rects):
        cases.save_state()
        cases.generate_target()
        for case in cases.target:
            case[-1].set_stroke(width=3)
            case[-1].scale(1.1)
        cases.target.arrange_in_grid(n_rows=nr, buff=0.5 * cases[0].get_width())
        cases.target.scale(0.5 / cases.target[0].get_height())
        cases.target.move_to(rect)
    fp_cases.target.shift(0.4 * DOWN)
    positive_words.save_state()
    negative_words.save_state()
    tpr_words.save_state()
    fpr_words.save_state()
    self.play(MoveToTarget(tp_cases), MoveToTarget(fp_cases), tpr_words.next_to, tp_cases.target, UP, fpr_words.next_to, fp_cases.target, UP, FadeOut(signs), positive_words[0].set_opacity, 0, negative_words[0].set_opacity, 0, positive_words[1].match_x, rects[0], negative_words[1].match_x, rects[1], LaggedStart(FadeOut(fn_cases, shift=DOWN), FadeOut(fnr_words, shift=DOWN), FadeOut(tn_cases, shift=DOWN), FadeOut(tnr_words, shift=DOWN)))
    self.wait()
    self.play(ShowCreationThenFadeOut(SurroundingRectangle(tpr_words[0][:1], stroke_width=2, stroke_color=WHITE, buff=0.05)), LaggedStartMap(Indicate, tp_cases, color=YELLOW, lag_ratio=0.3, run_time=1))
    self.wait()
    self.play(ShowCreationThenFadeOut(SurroundingRectangle(fpr_words[0][:2], stroke_width=2, stroke_color=WHITE, buff=0.05)), LaggedStartMap(Indicate, fp_cases, color=GREEN_A, lag_ratio=0.05, run_time=3))
    self.wait()
    equation = Tex('P(', '\\text{Have cancer }', '|', '\\text{ positive test})', '\\approx', '\\frac{9}{9 + 89}', '\\approx \\frac{1}{11}')
    equation.set_color_by_tex('cancer', YELLOW)
    equation.set_color_by_tex('positive', GREEN)
    equation.to_edge(UP, buff=SMALL_BUFF)
    self.play(FadeIn(equation[:-1], shift=UP), FadeOut(title, shift=UP))
    self.wait()
    self.play(Write(equation[-1]))
    self.wait()
    frame = self.camera.frame
    frame.save_state()
    ppv_words = TexText('Positive\\\\', 'Predictive\\\\', 'Value\\\\', alignment='')
    ppv_words.next_to(equation, RIGHT, LARGE_BUFF, DOWN)
    for word in ppv_words:
        word[0].set_color(BLUE)
    ppv_rhs = Tex('={\\text{TP} \\over \\text{TP} + \\text{FP}}', tex_to_color_map={'\\text{TP}': GREEN_B, '\\text{FP}': GREEN_C})
    ppv_rhs.next_to(ppv_words, RIGHT)
    ppv_rhs.shift(1.5 * LEFT)
    self.play(frame.scale, 1.1, {'about_edge': DL})
    self.play(ShowIncreasingSubsets(ppv_words))
    self.wait()
    self.play(equation.shift, 1.5 * LEFT + 0.5 * UP, ppv_words.shift, 1.5 * LEFT, FadeIn(ppv_rhs, lag_ratio=0.1), frame.scale, 1.1, {'about_edge': DL})
    self.wait()
    self.play(frame.restore, frame.shift, 0.5 * DOWN, LaggedStartMap(FadeOut, VGroup(equation, ppv_words, ppv_rhs)), LaggedStartMap(Restore, VGroup(tpr_words, tp_cases, fpr_words, fp_cases)), run_time=3)
    self.play(LaggedStartMap(FadeIn, VGroup(fnr_words, fn_cases, tnr_words, tn_cases)))
    self.wait()
    fade_rects = VGroup(*(BackgroundRectangle(VGroup(rect, words), fill_opacity=0.9, fill_color=BLACK, buff=SMALL_BUFF) for (rect, words) in zip(rects, [positive_words, negative_words])))
    sens_eq = Tex('\\text{Sensitivity}', '= {9 \\over 10}', '= 90\\%')
    sens_eq.next_to(rects[0], LEFT, MED_LARGE_BUFF, aligned_edge=UP)
    sens_eq.shift(DOWN)
    fnr_eq = Tex('\\text{False Negative Rate}', '= 10\\%')
    fnr_eq.set_color(RED)
    fnr_eq.scale(0.9)
    equiv = Tex('\\Leftrightarrow')
    equiv.scale(1.5)
    equiv.rotate(90 * DEGREES)
    equiv.next_to(sens_eq, DOWN, MED_LARGE_BUFF)
    fnr_eq.next_to(equiv, DOWN, MED_LARGE_BUFF)
    self.play(frame.shift, 5 * LEFT, FadeIn(fade_rects[1]), Write(sens_eq[0]))
    self.wait()
    self.play(TransformFromCopy(tpr_words[0][0], sens_eq[1][1]), Write(sens_eq[1][0]), Write(sens_eq[1][2:]))
    self.play(Write(sens_eq[2]))
    self.wait()
    self.play(FadeIn(equiv, shift=0.5 * DOWN), FadeIn(fnr_eq, shift=1.0 * DOWN))
    self.wait()
    fade_rects[0].stretch(5, 0, about_edge=RIGHT)
    self.play(ApplyMethod(frame.shift, 10 * RIGHT, run_time=4), FadeIn(fade_rects[0], run_time=2), FadeOut(fade_rects[1], run_time=2))
    spec_eq = Tex('\\text{Specificity}', '= {901 \\over 990}', '\\approx 91\\%')
    spec_eq.next_to(rects[1], RIGHT, MED_LARGE_BUFF, aligned_edge=DOWN)
    spec_eq.shift(UP)
    fpr_eq = Tex('\\text{False Positive Rate}', '= 9\\%')
    fpr_eq.set_color(GREEN)
    fpr_eq.scale(0.9)
    equiv2 = Tex('\\Leftrightarrow')
    equiv2.scale(1.5)
    equiv2.rotate(90 * DEGREES)
    equiv2.next_to(spec_eq, UP, MED_LARGE_BUFF)
    fpr_eq.next_to(equiv2, UP, MED_LARGE_BUFF)
    self.play(Write(spec_eq[0]))
    self.wait()
    self.play(Write(spec_eq[1][0]), TransformFromCopy(tnr_words[0][:3], spec_eq[1][1:4], run_time=2, path_arc=30 * DEGREES), Write(spec_eq[1][4:]))
    self.wait()
    self.play(Write(spec_eq[2]))
    self.wait()
    self.play(FadeIn(equiv2, shift=0.5 * UP), FadeIn(fpr_eq, shift=1.0 * UP))
    self.wait()
    eqs = [sens_eq, spec_eq]
    for (eq, word) in zip(eqs, [positive_words, negative_words]):
        eq.generate_target()
        eq.target[1].set_opacity(0)
        (eq.target[2].move_to(eq.target[1], LEFT),)
        eq.target.next_to(word, UP, buff=0.3)
    self.play(FadeOut(fade_rects[0]), frame.shift, 5 * LEFT, frame.scale, 1.1, {'about_edge': DOWN}, MoveToTarget(sens_eq), MoveToTarget(spec_eq), *map(FadeOut, (fnr_eq, fpr_eq, equiv, equiv2)), run_time=2)
    self.wait()
    self.play(VGroup(fn_cases, fnr_words, fp_cases, fpr_words).set_opacity, 0.2, rate_func=there_and_back_with_pause, run_time=3)","for (cases, nr, rect) in zip([tp_cases, fp_cases], [3, 7], rects):
    cases.save_state()
    cases.generate_target()
    for case in cases.target:
        case[-1].set_stroke(width=3)
        case[-1].scale(1.1)
    cases.target.arrange_in_grid(n_rows=nr, buff=0.5 * cases[0].get_width())
    cases.target.scale(0.5 / cases.target[0].get_height())
    cases.target.move_to(rect)","['for (cases, nr, rect) in zip([tp_cases, fp_cases], [3, 7], rects):\n    (cases_0, *cases_rcasesmaining) = cases\n    cases.save_state()\n    cases.generate_target()\n    for case in cases.target:\n        case[-1].set_stroke(width=3)\n        case[-1].scale(1.1)\n    cases.target.arrange_in_grid(n_rows=nr, buff=0.5 * cases_0.get_width())\n    cases.target.scale(0.5 / cases.target[0].get_height())\n    cases.target.move_to(rect)']",no_found,0
frigate,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frigate/frigate/edgetpu.py,https://github.com/blakeblackshear/frigate/tree/master/frigate/edgetpu.py,RemoteObjectDetector,detect$260,"def detect(self, tensor_input, threshold=0.4):
    detections = []
    self.np_shm[:] = tensor_input[:]
    self.event.clear()
    self.detection_queue.put(self.name)
    result = self.event.wait(timeout=10.0)
    if result is None:
        return detections
    for d in self.out_np_shm:
        if d[1] < threshold:
            break
        detections.append((self.labels[int(d[0])], float(d[1]), (d[2], d[3], d[4], d[5])))
    self.fps.update()
    return detections","for d in self.out_np_shm:
    if d[1] < threshold:
        break
    detections.append((self.labels[int(d[0])], float(d[1]), (d[2], d[3], d[4], d[5])))","['for (d_0, d_1, d_2, d_3, d_4, d_5, *d_len) in self.out_np_shm:\n    if \n    d_1 < threshold:\n        break\n    detections.append((self.labels[int(\n    d_0)], float(\n    d_1), (\n    d_2, \n    d_3, \n    d_4, \n    d_5)))']",no_found,0
Scout2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Scout2/AWSScout2/output/console.py,https://github.com/nccgroup/Scout2/tree/master/AWSScout2/output/console.py,,format_listall_output$16,"def format_listall_output(format_file, format_item_dir, format, rule, option_prefix=None, template=None, skip_options=False):
    """"""
    Prepare listall output template

    :param format_file:
    :param format_item_dir:
    :param format:
    :param config:
    :param option_prefix:
    :param template:
    :param skip_options:
    :return:
    """"""
    if format_file and os.path.isfile(format_file):
        if not template:
            with open(format_file, 'rt') as f:
                template = f.read()
        if not skip_options:
            re_option = re.compile('(%_OPTION_\\((.*?)\\)_NOITPO_)')
            optional_files = re_option.findall(template)
            for optional_file in optional_files:
                if optional_file[1].startswith(option_prefix + '-'):
                    with open(os.path.join(format_item_dir, optional_file[1].strip()), 'rt') as f:
                        template = template.replace(optional_file[0].strip(), f.read())
        re_file = re.compile('(_FILE_\\((.*?)\\)_ELIF_)')
        while True:
            requested_files = re_file.findall(template)
            available_files = os.listdir(format_item_dir) if format_item_dir else []
            for requested_file in requested_files:
                if requested_file[1].strip() in available_files:
                    with open(os.path.join(format_item_dir, requested_file[1].strip()), 'rt') as f:
                        template = template.replace(requested_file[0].strip(), f.read())
            re_line = re.compile('(_ITEM_\\((.*?)\\)_METI_)')
            re_key = re.compile('_KEY_\\(*(.*?)\\)', re.DOTALL | re.MULTILINE)
            lines = re_line.findall(template)
            for (i, line) in enumerate(lines):
                lines[i] = line + (re_key.findall(line[1]),)
            requested_files = re_file.findall(template)
            if len(requested_files) == 0:
                break
    elif format and format[0] == 'csv':
        keys = rule.keys
        line = ', '.join(('_KEY_(%s)' % k for k in keys))
        lines = [(line, line, keys)]
        template = line
    return (lines, template)","for (i, line) in enumerate(lines):
    lines[i] = line + (re_key.findall(line[1]),)","['for (i, line) in enumerate(lines):\n    (_, line_1, *line_rlinemaining) = line\n    lines[i] = line + (re_key.findall(line_1),)']",no_found,0
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/extra/tunings.py,https://github.com/bspaans/python-mingus/tree/master/mingus/extra/tunings.py,StringTuning,find_chord_fingering$149,"def find_chord_fingering(self, notes, max_distance=4, maxfret=18, max_fingers=4, return_best_as_NoteContainer=False):
    """"""Return a list of fret lists that are considered possible fingerings.

        This function only looks at and matches on the note _names_ so it
        does more than find_fingering.

        Example:
        >>> t = get_tuning('guitar', 'standard', 6, 1)
        >>> t.find_chord_fingering(NoteContainer().from_chord('Am'))
        [[0, 0, 2, 2, 1, 0], [0, 3, 2, 2, 1, 0], ......]
        """"""

    def follow(string, next, name, prev=-1):
        """"""Follow the fret 'next' on 'string'; build result on the way.""""""
        if string >= len(self.tuning) - 1:
            return [[(next, name)]]
        result = []
        cur = res[string][next]
        if cur != []:
            for y in cur[1]:
                for sub in follow(string + 1, y[0], y[1]):
                    if prev < 0:
                        result.append([(next, name)] + sub)
                    elif sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
                        result.append([(next, name)] + sub)
        for s in follow(string + 1, maxfret + 1, None, next):
            result.append([(next, name)] + s)
        return [[(next, name)]] if result == [] else result

    def make_lookup_table():
        """"""Prepare the lookup table.

            table[string][fret] = (name, dest_frets)
            """"""
        res = [[[] for x in range(maxfret + 2)] for x in range(len(self.tuning) - 1)]
        for x in range(0, len(self.tuning) - 1):
            addedNone = -1
            next = fretdict[x + 1]
            for (fret, name) in fretdict[x]:
                for (f2, n2) in next:
                    if n2 != name and (f2 == 0 or abs(fret - f2) < max_distance):
                        if res[x][fret] != []:
                            res[x][fret][1].append((f2, n2))
                        else:
                            res[x][fret] = (name, [(f2, n2)])
                    if addedNone < x:
                        if res[x][maxfret + 1] != []:
                            res[x][maxfret + 1][1].append((f2, n2))
                        else:
                            res[x][maxfret + 1] = (None, [(f2, n2)])
                addedNone = x
        return res
    n = notes
    if notes != [] and isinstance(notes, list) and isinstance(notes[0], six.string_types):
        n = NoteContainer(notes)
    notenames = [x.name for x in n]
    if len(notenames) == 0 or len(notenames) > len(self.tuning):
        return []
    fretdict = []
    for x in range(0, len(self.tuning)):
        fretdict.append(self.find_note_names(notes, x, maxfret))
    res = make_lookup_table()
    result = []
    for (i, y) in enumerate(res[0]):
        if y != []:
            (yname, next) = (y[0], y[1])
            for (fret, name) in next:
                for s in follow(1, fret, name):
                    subresult = [(i, yname)] + s
                    (mi, ma, names) = (1000, -1000, [])
                    for (f, n) in subresult:
                        if n is not None:
                            if f != 0 and f <= mi:
                                mi = f
                            if f != 0 and f >= ma:
                                ma = f
                            names.append(n)
                    if abs(ma - mi) < max_distance:
                        covered = True
                        for n in notenames:
                            if n not in names:
                                covered = False
                        if covered and names != []:
                            result.append([y[0] if y[1] is not None else y[1] for y in subresult])
    s = sorted(result, key=lambda x: sum([t if t is not None else 1000 for (i, t) in enumerate(x)]))
    s = [a for a in s if fingers_needed(a) <= max_fingers]
    if not return_best_as_NoteContainer:
        return s
    else:
        rnotes = self.frets_to_NoteContainer(s[0])
        for (i, x) in enumerate(rnotes):
            if x.string < len(self.tuning) - 1:
                if res[x.string][x.fret] != []:
                    rnotes[i].name = res[x.string][x.fret][0]
        return rnotes","for (i, y) in enumerate(res[0]):
    if y != []:
        (yname, next) = (y[0], y[1])
        for (fret, name) in next:
            for s in follow(1, fret, name):
                subresult = [(i, yname)] + s
                (mi, ma, names) = (1000, -1000, [])
                for (f, n) in subresult:
                    if n is not None:
                        if f != 0 and f <= mi:
                            mi = f
                        if f != 0 and f >= ma:
                            ma = f
                        names.append(n)
                if abs(ma - mi) < max_distance:
                    covered = True
                    for n in notenames:
                        if n not in names:
                            covered = False
                    if covered and names != []:
                        result.append([y[0] if y[1] is not None else y[1] for y in subresult])","['for (i, y) in enumerate(res[0]):\n    (y_0, y_1, *_) = y\n    if y != []:\n        (yname, next) = (y_0, y_1)\n        for (fret, name) in next:\n            for s in follow(1, fret, name):\n                subresult = [(i, yname)] + s\n                (mi, ma, names) = (1000, -1000, [])\n                for (f, n) in subresult:\n                    if n is not None:\n                        if f != 0 and f <= mi:\n                            mi = f\n                        if f != 0 and f >= ma:\n                            ma = f\n                        names.append(n)\n                if abs(ma - mi) < max_distance:\n                    covered = True\n                    for n in notenames:\n                        if n not in names:\n                            covered = False\n                    if covered and names != []:\n                        result.append([y_0 if y_1 is not None else y_1 for y in subresult])']",no_found,0
joinmarket-clientserver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/joinmarket-clientserver/jmbitcoin/jmbitcoin/secp256k1_transaction.py,https://github.com/JoinMarket-Org/joinmarket-clientserver/tree/master/jmbitcoin/jmbitcoin/secp256k1_transaction.py,,mktx$315,"def mktx(ins, outs, version=1, locktime=0):
    """""" Given a list of input tuples (txid(bytes), n(int)),
    and a list of outputs which are dicts with
    keys ""address"" (value should be *str* not CCoinAddress) (
    or alternately ""script"" (for nonstandard outputs, value
    should be CScript)),
    ""value"" (value should be integer satoshis), outputs a
    CMutableTransaction object.
    Tx version and locktime are optionally set, for non-default
    locktimes, inputs are given nSequence as per below comment.
    """"""
    vin = []
    vout = []
    if locktime != 0:
        sequence = 4294967295 - 1
    else:
        sequence = 4294967295
    for i in ins:
        outpoint = CMutableOutPoint(i[0][::-1], i[1])
        inp = CMutableTxIn(prevout=outpoint, nSequence=sequence)
        vin.append(inp)
    for o in outs:
        if 'script' in o:
            sPK = o['script']
        else:
            sPK = CCoinAddress(o['address']).to_scriptPubKey()
        out = CMutableTxOut(o['value'], sPK)
        vout.append(out)
    return CMutableTransaction(vin, vout, nLockTime=locktime, nVersion=version)","for i in ins:
    outpoint = CMutableOutPoint(i[0][::-1], i[1])
    inp = CMutableTxIn(prevout=outpoint, nSequence=sequence)
    vin.append(inp)","['for (i_0, i_1, (*i_0_0_-1_len,), *i_len) in ins:\n    outpoint = CMutableOutPoint(\n    *i_0_0_-1_len, \n    i_1)\n    inp = CMutableTxIn(prevout=outpoint, nSequence=sequence)\n    vin.append(inp)']",no_found,0
kivy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy/kivy/atlas.py,https://github.com/kivy/kivy/tree/master/kivy/atlas.py,Atlas,create$229,"def create(outname, filenames, size, padding=2, use_path=False):
    """"""This method can be used to create an atlas manually from a set of
        images.

        :Parameters:
            `outname`: str
                Basename to use for ``.atlas`` creation and ``-<idx>.png``
                associated images.
            `filenames`: list
                List of filenames to put in the atlas.
            `size`: int or list (width, height)
                Size of the atlas image. If the size is not large enough to
                fit all of the source images, more atlas images will created
                as required.
            `padding`: int, defaults to 2
                Padding to put around each image.

                Be careful. If you're using a padding < 2, you might have
                issues with the borders of the images. Because of the OpenGL
                linearization, it might use the pixels of the adjacent image.

                If you're using a padding >= 2, we'll automatically generate a
                ""border"" of 1px around your image. If you look at
                the result, don't be scared if the image inside is not
                exactly the same as yours :).

            `use_path`: bool, defaults to False
                If True, the relative path of the source png
                file names will be included in the atlas ids rather
                that just in the file names. Leading dots and slashes will be
                excluded and all other slashes in the path will be replaced
                with underscores. For example, if `use_path` is False
                (the default) and the file name is
                ``../data/tiles/green_grass.png``, the id will be
                ``green_grass``. If `use_path` is True, it will be
                ``data_tiles_green_grass``.

            .. versionchanged:: 1.8.0
                Parameter use_path added
        """"""
    try:
        from PIL import Image
    except ImportError:
        Logger.critical('Atlas: Imaging/PIL are missing')
        raise
    if isinstance(size, (tuple, list)):
        (size_w, size_h) = list(map(int, size))
    else:
        size_w = size_h = int(size)
    ims = list()
    for f in filenames:
        fp = open(f, 'rb')
        im = Image.open(fp)
        im.load()
        fp.close()
        ims.append((f, im))
    ims = sorted(ims, key=lambda im: im[1].size[0] * im[1].size[1], reverse=True)
    freeboxes = [(0, 0, 0, size_w, size_h)]
    numoutimages = 1
    fullboxes = []
    for imageinfo in ims:
        im = imageinfo[1]
        (imw, imh) = im.size
        imw += padding
        imh += padding
        if imw > size_w or imh > size_h:
            Logger.error('Atlas: image %s (%d by %d) is larger than the atlas size!' % (imageinfo[0], imw, imh))
            return
        inserted = False
        while not inserted:
            for (idx, fb) in enumerate(freeboxes):
                if fb[3] >= imw and fb[4] >= imh:
                    del freeboxes[idx]
                    if fb[3] > imw:
                        freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))
                    if fb[4] > imh:
                        freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))
                    freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])
                    fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo[0]))
                    inserted = True
                    break
            if not inserted:
                freeboxes.append((numoutimages, 0, 0, size_w, size_h))
                numoutimages += 1
    Logger.info('Atlas: create an {0}x{1} rgba image'.format(size_w, size_h))
    outimages = [Image.new('RGBA', (size_w, size_h)) for i in range(0, int(numoutimages))]
    for fb in fullboxes:
        (x, y) = (fb[2], fb[3])
        out = outimages[fb[1]]
        out.paste(fb[0], (fb[2], fb[3]))
        (w, h) = fb[0].size
        if padding > 1:
            out.paste(fb[0].crop((0, 0, w, 1)), (x, y - 1))
            out.paste(fb[0].crop((0, h - 1, w, h)), (x, y + h))
            out.paste(fb[0].crop((0, 0, 1, h)), (x - 1, y))
            out.paste(fb[0].crop((w - 1, 0, w, h)), (x + w, y))
    for (idx, outimage) in enumerate(outimages):
        outimage.save('%s-%d.png' % (outname, idx))
    meta = {}
    for fb in fullboxes:
        fn = '%s-%d.png' % (basename(outname), fb[1])
        if fn not in meta:
            d = meta[fn] = {}
        else:
            d = meta[fn]
        if use_path:
            uid = splitext(fb[6])[0]
            uid = uid.lstrip('./\\')
            uid = uid.replace('/', '_').replace('\\', '_')
        else:
            uid = splitext(basename(fb[6]))[0]
        (x, y, w, h) = fb[2:6]
        d[uid] = (x, size_h - y - h, w, h)
    outfn = '%s.atlas' % outname
    with open(outfn, 'w') as fd:
        json.dump(meta, fd)
    return (outfn, meta)","for (idx, fb) in enumerate(freeboxes):
    if fb[3] >= imw and fb[4] >= imh:
        del freeboxes[idx]
        if fb[3] > imw:
            freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))
        if fb[4] > imh:
            freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))
        freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])
        fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo[0]))
        inserted = True
        break","['for (idx, fb) in enumerate(freeboxes):\n    (fb_0, fb_1, fb_2, fb_3, fb_4, *_) = fb\n    if fb_3 >= imw and fb_4 >= imh:\n        del freeboxes[idx]\n        if fb_3 > imw:\n            freeboxes.append((fb_0, fb_1 + imw, fb_2, fb_3 - imw, imh))\n        if fb_4 > imh:\n            freeboxes.append((fb_0, fb_1, fb_2 + imh, fb_3, fb_4 - imh))\n        freeboxes = sorted(freeboxes, key=lambda fb: fb_3 * fb_4)\n        fullboxes.append((im, fb_0, fb_1 + padding, fb_2 + padding, imw - padding, imh - padding, imageinfo[0]))\n        inserted = True\n        break']",no_found,0
PaddleVideo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleVideo/paddlevideo/utils/multigrid/multigrid.py,https://github.com/PaddlePaddle/PaddleVideo/tree/master/paddlevideo/utils/multigrid/multigrid.py,MultigridSchedule,get_long_cycle_schedule$110,"def get_long_cycle_schedule(self, cfg):
    """"""
        Based on multigrid hyperparameters, define the schedule of a long cycle.
        Args:
            cfg (configs): configs that contains training and multigrid specific
                hyperparameters.
        Returns:
            schedule (list): Specifies a list long cycle base shapes and their
                corresponding training epochs.
        """"""
    steps = cfg.OPTIMIZER.learning_rate.steps
    default_size = float(cfg.PIPELINE.train.decode_sampler.num_frames * cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size'] ** 2)
    default_iters = steps[-1]
    avg_bs = []
    all_shapes = []
    for item in cfg.MULTIGRID.long_cycle_factors:
        (t_factor, s_factor) = item['value']
        base_t = int(round(cfg.PIPELINE.train.decode_sampler.num_frames * t_factor))
        base_s = int(round(cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size'] * s_factor))
        if cfg.MULTIGRID.SHORT_CYCLE:
            shapes = [[base_t, cfg.MULTIGRID.default_crop_size * cfg.MULTIGRID.short_cycle_factors[0]], [base_t, cfg.MULTIGRID.default_crop_size * cfg.MULTIGRID.short_cycle_factors[1]], [base_t, base_s]]
        else:
            shapes = [[base_t, base_s]]
        shapes = [[int(round(default_size / (s[0] * s[1] * s[1]))), s[0], s[1]] for s in shapes]
        avg_bs.append(np.mean([s[0] for s in shapes]))
        all_shapes.append(shapes)
    total_iters = 0
    schedule = []
    for step_index in range(len(steps) - 1):
        step_epochs = steps[step_index + 1] - steps[step_index]
        for (long_cycle_index, shapes) in enumerate(all_shapes):
            cur_epochs = step_epochs * avg_bs[long_cycle_index] / sum(avg_bs)
            cur_iters = cur_epochs / avg_bs[long_cycle_index]
            total_iters += cur_iters
            schedule.append((step_index, shapes[-1], cur_epochs))
    iter_saving = default_iters / total_iters
    final_step_epochs = cfg.OPTIMIZER.learning_rate.max_epoch - steps[-1]
    ft_epochs = final_step_epochs / iter_saving * avg_bs[-1]
    schedule.append((step_index + 1, all_shapes[-1][-1], ft_epochs))
    x = cfg.OPTIMIZER.learning_rate.max_epoch * cfg.MULTIGRID.epoch_factor / sum((s[-1] for s in schedule))
    final_schedule = []
    total_epochs = 0
    for s in schedule:
        epochs = s[2] * x
        total_epochs += epochs
        final_schedule.append((s[0], s[1], int(round(total_epochs))))
    print_schedule(final_schedule)
    return final_schedule","for (long_cycle_index, shapes) in enumerate(all_shapes):
    cur_epochs = step_epochs * avg_bs[long_cycle_index] / sum(avg_bs)
    cur_iters = cur_epochs / avg_bs[long_cycle_index]
    total_iters += cur_iters
    schedule.append((step_index, shapes[-1], cur_epochs))","['for (long_cycle_index, shapes) in enumerate(all_shapes):\n    (*shapes_rshapesmaining, shapes_last) = shapes\n    cur_epochs = step_epochs * avg_bs[long_cycle_index] / sum(avg_bs)\n    cur_iters = cur_epochs / avg_bs[long_cycle_index]\n    total_iters += cur_iters\n    schedule.append((step_index, shapes_last, cur_epochs))']",no_found,0
strictyaml,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/strictyaml/strictyaml/ruamel/comments.py,https://github.com/crdoconnor/strictyaml/tree/master/strictyaml/ruamel/comments.py,CommentedMap,__getitem__$757,"def __getitem__(self, key):
    try:
        return ordereddict.__getitem__(self, key)
    except KeyError:
        for merged in getattr(self, merge_attrib, []):
            if key in merged[1]:
                return merged[1][key]
        raise","for merged in getattr(self, merge_attrib, []):
    if key in merged[1]:
        return merged[1][key]","['for (merged_0, merged_1, *merged_len) in getattr(self, merge_attrib, []):\n    if key in \n    merged_1:\n        return \n        merged_1[key]']",no_found,0
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/tools/fix_annotations.py,https://github.com/nlplab/brat/tree/master/tools/fix_annotations.py,,correct_annotations$28,"def correct_annotations(orig_fn, ann_fn, change_fn):
    with annotation.TextAnnotations(ann_fn) as anns:
        orig_text = anns.get_document_text()
        with annotation.open_textfile(change_fn, 'r') as f:
            changed_text = f.read()
        diffs = diff_match_patch().diff_main(orig_text, changed_text)
        orig_offset = 0
        offsets = []
        for diff in diffs:
            kind = diff[0]
            text = diff[1]
            size = len(text)
            delta = size * kind
            offsets.append((orig_offset, delta))
            if kind != 1:
                orig_offset += size
        offsets = offsets[::-1]
        tbs = list(anns.get_textbounds())
        indices = []
        for (tbi, tb) in enumerate(tbs):
            for (spani, span) in enumerate(tb.spans):
                indices.append((span[0], tbi, spani, 0))
                indices.append((span[1], tbi, spani, 1))
        indices.sort(reverse=True)
        for (orig_offset, delta) in offsets:
            for index in indices:
                if index[0] < orig_offset:
                    break
                frag = list(tbs[index[1]].spans[index[2]])
                frag[index[3]] += delta
                tbs[index[1]].spans[index[2]] = tuple(frag)
        for tb in tbs:
            if isinstance(tb, annotation.TextBoundAnnotationWithText):
                tb.text = annotation.DISCONT_SEP.join((changed_text[start:end] for (start, end) in tb.spans))
    copy(change_fn, orig_fn)","for (spani, span) in enumerate(tb.spans):
    indices.append((span[0], tbi, spani, 0))
    indices.append((span[1], tbi, spani, 1))","['for (spani, span) in enumerate(tb.spans):\n    (span_0, span_1, *span_rspanmaining) = span\n    indices.append((span_0, tbi, spani, 0))\n    indices.append((span_1, tbi, spani, 1))']",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/CreatHtml.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/CreatHtml.py,CreatHtml,vuln_list$27,"def vuln_list(self):
    global creat_vuln_num
    creat_vuln_num = 1
    tr_whole_api_list = ''
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    for vuln_info in vuln_infos:
        if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
            sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
            cursor.execute(sql)
            api_infos = cursor.fetchall()
            for api_info in api_infos:
                tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
                if api_info[5] == 1:
                    vuln_url_type = Utils().getMyWord('{r_get}')
                else:
                    vuln_url_type = Utils().getMyWord('{r_post}')
                tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                try:
                    api_length_info = len(api_info[4])
                except:
                    api_length_info = 0
                for js_path in js_paths:
                    tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                    tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                    tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                    tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                    tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
                    tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                    self.creat_api_num = self.creat_api_num + 1
                    creat_vuln_num = creat_vuln_num + 1
                    tr_whole_api_list = tr_whole_api_list + tr_api_list
        elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
            sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
            cursor.execute(sql)
            api_infos = cursor.fetchall()
            for api_info in api_infos:
                tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
                if api_info[5] == 1:
                    vuln_url_type = Utils().getMyWord('{r_get}')
                else:
                    vuln_url_type = Utils().getMyWord('{r_post}')
                tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                try:
                    api_length_info = len(api_info[4])
                except:
                    api_length_info = 0
                for js_path in js_paths:
                    tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                    tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                    tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                    tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                    tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '' + Utils().getMyWord('{r_vuln_maybe}'))
                    tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                    self.creat_api_num = self.creat_api_num + 1
                    creat_vuln_num = creat_vuln_num + 1
                    tr_whole_api_list = tr_whole_api_list + tr_api_list
    return tr_whole_api_list","for vuln_info in vuln_infos:
    if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list
    elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '' + Utils().getMyWord('{r_vuln_maybe}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list","['for (vuln_info_0, vuln_info_1, vuln_info_2, vuln_info_3, vuln_info_4, *vuln_info_len) in vuln_infos:\n    if \n    vuln_info_3 == \'unAuth\' and \n    vuln_info_4 == 1:\n        sql = ""select * from api_tree where id=\'%s\'"" % \n        vuln_info_1\n        cursor.execute(sql)\n        api_infos = cursor.fetchall()\n        for api_info in api_infos:\n            tr_api_list = \'\\n                    <tr>\\n                      <td>{vuln_num}</td>\\n                      <td>{vuln_api_name}</td>\\n                      <td>{vuln_url}</td>\\n                      <td>{vuln_url_type}</td>\\n                      <td>{vuln_risk}</td>\\n                      <td>{vuln_length}</td>\\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\\n                    </tr>\'\n            if api_info[5] == 1:\n                vuln_url_type = Utils().getMyWord(\'{r_get}\')\n            else:\n                vuln_url_type = Utils().getMyWord(\'{r_post}\')\n            tr_api_list = tr_api_list.replace(\'{vuln_num}\', str(self.creat_api_num))\n            sql = ""select path from js_file where id=\'%s\'"" % \n            vuln_info_2\n            cursor.execute(sql)\n            js_paths = cursor.fetchall()\n            try:\n                api_length_info = len(api_info[4])\n            except:\n                api_length_info = 0\n            for js_path in js_paths:\n                tr_api_list = tr_api_list.replace(\'{vuln_api_name}\', api_info[2])\n                tr_api_list = tr_api_list.replace(\'{vuln_url}\', api_info[1])\n                tr_api_list = tr_api_list.replace(\'{vuln_url_type}\', vuln_url_type)\n                tr_api_list = tr_api_list.replace(\'{vuln_length}\', str(api_length_info))\n                tr_api_list = tr_api_list.replace(\'{vuln_risk}\', Utils().getMyWord(\'{r_l_m}\'))\n                tr_api_list = tr_api_list.replace(\'{vuln_id}\', \'vuln_\' + str(creat_vuln_num))\n                self.creat_api_num = self.creat_api_num + 1\n                creat_vuln_num = creat_vuln_num + 1\n                tr_whole_api_list = tr_whole_api_list + tr_api_list\n    elif \n    vuln_info_3 == \'unAuth\' and \n    vuln_info_4 == 2:\n        sql = ""select * from api_tree where id=\'%s\'"" % \n        vuln_info_1\n        cursor.execute(sql)\n        api_infos = cursor.fetchall()\n        for api_info in api_infos:\n            tr_api_list = \'\\n                    <tr>\\n                      <td>{vuln_num}</td>\\n                      <td>{vuln_api_name}</td>\\n                      <td>{vuln_url}</td>\\n                      <td>{vuln_url_type}</td>\\n                      <td>{vuln_risk}</td>\\n                      <td>{vuln_length}</td>\\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\\n                    </tr>\'\n            if api_info[5] == 1:\n                vuln_url_type = Utils().getMyWord(\'{r_get}\')\n            else:\n                vuln_url_type = Utils().getMyWord(\'{r_post}\')\n            tr_api_list = tr_api_list.replace(\'{vuln_num}\', str(self.creat_api_num))\n            sql = ""select path from js_file where id=\'%s\'"" % \n            vuln_info_2\n            cursor.execute(sql)\n            js_paths = cursor.fetchall()\n            try:\n                api_length_info = len(api_info[4])\n            except:\n                api_length_info = 0\n            for js_path in js_paths:\n                tr_api_list = tr_api_list.replace(\'{vuln_api_name}\', api_info[2])\n                tr_api_list = tr_api_list.replace(\'{vuln_url}\', api_info[1])\n                tr_api_list = tr_api_list.replace(\'{vuln_url_type}\', vuln_url_type)\n                tr_api_list = tr_api_list.replace(\'{vuln_length}\', str(api_length_info))\n                tr_api_list = tr_api_list.replace(\'{vuln_risk}\', Utils().getMyWord(\'{r_l_l}\') + \'' + Utils().getMyWord(\'{r_vuln_maybe}\'))\n                tr_api_list = tr_api_list.replace(\'{vuln_id}\', \'vuln_\' + str(creat_vuln_num))\n                self.creat_api_num = self.creat_api_num + 1\n                creat_vuln_num = creat_vuln_num + 1\n                tr_whole_api_list = tr_whole_api_list + tr_api_list']",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for vuln_info in vuln_infos:
    if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = para1.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para2.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para2.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num = self.creat_num + 1
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = para1.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para2.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para2.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num = self.creat_num + 1
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
    elif vuln_info[3] == 'INFO':
        sql = ""select * from js_file where id='%s'"" % vuln_info[2]
        cursor.execute(sql)
        js_infos = cursor.fetchall()
        for js_info in js_infos:
            js_name = js_info[1]
            js_path = js_info[2]
            para2 = para1.insert_paragraph_before('')
            run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
            run2.font.name = 'Arial'
            run2.font.size = Pt(10)
            run2.font.bold = True
            run3 = para2.add_run(js_path)
            run3.font.name = 'Arial'
            run3.font.size = Pt(10)
            run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
            run4.font.name = 'Arial'
            run4.font.size = Pt(10)
            run4.font.bold = True
            run5 = para2.add_run(vuln_info[8])
            run5.font.name = 'Arial'
            run5.font.size = Pt(10)
            run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
            run6.font.name = 'Arial'
            run6.font.size = Pt(10)
            run6.font.bold = True
            self.creat_num = self.creat_num + 1
            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
    elif vuln_info[3] == 'CORS':
        sql = ""select vaule from info where name='%s'"" % 'host'
        cursor.execute(sql)
        infos = cursor.fetchall()
        for info in infos:
            api_path = info[0]
        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
        para3 = para2.insert_paragraph_before('')
        run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
        run5.font.name = 'Arial'
        run5.font.size = Pt(16)
        run5.font.bold = True
        run6 = para3.add_run(':')
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        run7 = para3.add_run(api_path)
        run7.font.name = 'Arial'
        run7.font.size = Pt(10)
        run8 = para3.add_run('\n' + '{response_head}')
        run8.font.name = 'Arial'
        run8.font.size = Pt(10)
        run8.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
        run9 = para2.add_run('\n' + '{request_head}')
        run9.font.name = 'Arial'
        run9.font.size = Pt(10)
        run9.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","['for (vuln_info_0, vuln_info_1, vuln_info_2, vuln_info_3, vuln_info_4, vuln_info_5, vuln_info_6, vuln_info_7, vuln_info_8, *vuln_info_len) in vuln_infos:\n    if \n    vuln_info_3 == \'unAuth\' and \n    vuln_info_4 == 1:\n        sql = ""select * from api_tree where id=\'%s\'"" % \n        vuln_info_1\n        cursor.execute(sql)\n        api_infos = cursor.fetchall()\n        for api_info in api_infos:\n            para2 = para1.insert_paragraph_before(\'\')\n            UserLogin = api_info[2]\n            api_path = api_info[1]\n            run = para2.add_run(\'2.\' + str(self.creat_num) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_unauth}\') + \'\\n\')\n            run.font.name = \'Arial\'\n            run.font.size = Pt(16)\n            run.font.bold = True\n            sql = ""select path from js_file where id=\'%s\'"" % \n            vuln_info_2\n            cursor.execute(sql)\n            js_paths = cursor.fetchall()\n            for js_path in js_paths:\n                run2 = para2.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n                run2.font.name = \'Arial\'\n                run2.font.size = Pt(10)\n                run2.font.bold = True\n                run3 = para2.add_run(api_path)\n                run3.font.name = \'Arial\'\n                run3.font.size = Pt(10)\n                run4 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n                run4.font.name = \'Arial\'\n                run4.font.size = Pt(10)\n                run4.font.bold = True\n                run5 = para2.add_run(js_path[0])\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5.font.bold = True\n                self.creat_num = self.creat_num + 1\n                vuln_info_js_unicode = json.dumps(json.loads(\n                vuln_info_6), sort_keys=True, indent=4, ensure_ascii=False)\n                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)\n    elif \n    vuln_info_3 == \'unAuth\' and \n    vuln_info_4 == 2:\n        sql = ""select * from api_tree where id=\'%s\'"" % \n        vuln_info_1\n        cursor.execute(sql)\n        api_infos = cursor.fetchall()\n        for api_info in api_infos:\n            para2 = para1.insert_paragraph_before(\'\')\n            UserLogin = api_info[2]\n            api_path = api_info[1]\n            run = para2.add_run(\'2.\' + str(self.creat_num) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_unauth_maybe}\') + \'\\n\')\n            run.font.name = \'Arial\'\n            run.font.size = Pt(16)\n            run.font.bold = True\n            sql = ""select path from js_file where id=\'%s\'"" % \n            vuln_info_2\n            cursor.execute(sql)\n            js_paths = cursor.fetchall()\n            for js_path in js_paths:\n                run2 = para2.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n                run2.font.name = \'Arial\'\n                run2.font.size = Pt(10)\n                run2.font.bold = True\n                run3 = para2.add_run(api_path)\n                run3.font.name = \'Arial\'\n                run3.font.size = Pt(10)\n                run4 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n                run4.font.name = \'Arial\'\n                run4.font.size = Pt(10)\n                run4.font.bold = True\n                run5 = para2.add_run(js_path[0])\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5.font.bold = True\n                self.creat_num = self.creat_num + 1\n                vuln_info_js_unicode = json.dumps(json.loads(\n                vuln_info_6), sort_keys=True, indent=4, ensure_ascii=False)\n                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)\n    elif \n    vuln_info_3 == \'INFO\':\n        sql = ""select * from js_file where id=\'%s\'"" % \n        vuln_info_2\n        cursor.execute(sql)\n        js_infos = cursor.fetchall()\n        for js_info in js_infos:\n            js_name = js_info[1]\n            js_path = js_info[2]\n            para2 = para1.insert_paragraph_before(\'\')\n            run = para2.add_run(\'2.\' + str(self.creat_num) + \' \' + str(js_name) + Utils().getMyWord(\'{r_vuln_info}\') + \'\\n\')\n            run.font.name = \'Arial\'\n            run.font.size = Pt(16)\n            run.font.bold = True\n            run2 = para2.add_run(Utils().getMyWord(\'{r_js_path}\'))\n            run2.font.name = \'Arial\'\n            run2.font.size = Pt(10)\n            run2.font.bold = True\n            run3 = para2.add_run(js_path)\n            run3.font.name = \'Arial\'\n            run3.font.size = Pt(10)\n            run4 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_js_des}\'))\n            run4.font.name = \'Arial\'\n            run4.font.size = Pt(10)\n            run4.font.bold = True\n            run5 = para2.add_run(\n            vuln_info_8)\n            run5.font.name = \'Arial\'\n            run5.font.size = Pt(10)\n            run6 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_js_detial}\'))\n            run6.font.name = \'Arial\'\n            run6.font.size = Pt(10)\n            run6.font.bold = True\n            self.creat_num = self.creat_num + 1\n            Creat_vuln_detail(self.projectTag).creat_table(document, \n            vuln_info_7, para2)\n    elif \n    vuln_info_3 == \'CORS\':\n        sql = ""select vaule from info where name=\'%s\'"" % \'host\'\n        cursor.execute(sql)\n        infos = cursor.fetchall()\n        for info in infos:\n            api_path = info[0]\n        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n        para3 = para2.insert_paragraph_before(\'\')\n        run5 = para3.add_run(\'2.\' + str(self.creat_num) + \' \' + str(api_path) + Utils().getMyWord(\'{r_vuln_CORS}\') + \'\\n\')\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(16)\n        run5.font.bold = True\n        run6 = para3.add_run(\':\')\n        run6.font.name = \'Arial\'\n        run6.font.size = Pt(10)\n        run6.font.bold = True\n        run7 = para3.add_run(api_path)\n        run7.font.name = \'Arial\'\n        run7.font.size = Pt(10)\n        run8 = para3.add_run(\'\\n\' + \'{response_head}\')\n        run8.font.name = \'Arial\'\n        run8.font.size = Pt(10)\n        run8.font.bold = True\n        Creat_vuln_detail(self.projectTag).creat_table(document, \n        vuln_info_7, para2)\n        run9 = para2.add_run(\'\\n\' + \'{request_head}\')\n        run9.font.name = \'Arial\'\n        run9.font.size = Pt(10)\n        run9.font.bold = True\n        Creat_vuln_detail(self.projectTag).creat_table(document, \n        vuln_info_5, para3)']",no_found,0
Vulmap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Vulmap/Vulmap-Linux/vulmap-linux.py,https://github.com/vulmon/Vulmap/tree/master/Vulmap-Linux/vulmap-linux.py,,ReadFromFile$207,"def ReadFromFile(InventoryOutFile):
    count = 0
    print('Reading software inventory from ' + InventoryOutFile)
    with open(InventoryOutFile) as json_file:
        products = json.load(json_file)
    for a in products:
        if count == 0:
            queryData = '['
        queryData += '{'
        queryData += '""product"": ""' + a[0] + '"",'
        queryData += '""version"": ""' + a[1] + '"",'
        queryData += '""arc"": ""' + a[2] + '""'
        queryData += '},'
        count += 1
        if count == 100:
            count = 0
            outResults(queryData)
    outResults(queryData)","for a in products:
    if count == 0:
        queryData = '['
    queryData += '{'
    queryData += '""product"": ""' + a[0] + '"",'
    queryData += '""version"": ""' + a[1] + '"",'
    queryData += '""arc"": ""' + a[2] + '""'
    queryData += '},'
    count += 1
    if count == 100:
        count = 0
        outResults(queryData)","['for a in products:\n    (a_0, a_1, a_2, *_) = a\n    if count == 0:\n        queryData = \'[\'\n    queryData += \'{\'\n    queryData += \'""product"": ""\' + a_0 + \'"",\'\n    queryData += \'""version"": ""\' + a_1 + \'"",\'\n    queryData += \'""arc"": ""\' + a_2 + \'""\'\n    queryData += \'},\'\n    count += 1\n    if count == 100:\n        count = 0\n        outResults(queryData)', 'for (a_0, a_1, a_2, *a_len) in products:\n    if count == 0:\n        queryData = \'[\'\n    queryData += \'{\'\n    queryData += \'""product"": ""\' + \n    a_0 + \'"",\'\n    queryData += \'""version"": ""\' + \n    a_1 + \'"",\'\n    queryData += \'""arc"": ""\' + \n    a_2 + \'""\'\n    queryData += \'},\'\n    count += 1\n    if count == 100:\n        count = 0\n        outResults(queryData)']",no_found,0
HanLP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HanLP/hanlp/components/srl/span_rank/srl_eval_utils.py,https://github.com/hankcs/HanLP/tree/master/hanlp/components/srl/span_rank/srl_eval_utils.py,,compute_srl_f1$162,"def compute_srl_f1(sentences, gold_srl, predictions, gold_path=None) -> SRLScores:
    assert len(gold_srl) == len(predictions)
    total_gold = 0
    total_predicted = 0
    total_matched = 0
    total_unlabeled_matched = 0
    num_sents = 0
    label_confusions = Counter()
    for (gold, prediction) in zip(gold_srl, predictions):
        gold_rels = 0
        pred_rels = 0
        matched = 0
        for (pred_id, gold_args) in gold.items():
            filtered_gold_args = [a for a in gold_args if a[2] not in ['V', 'C-V']]
            total_gold += len(filtered_gold_args)
            gold_rels += len(filtered_gold_args)
            if pred_id not in prediction:
                continue
            for a0 in filtered_gold_args:
                for a1 in prediction[pred_id]:
                    if a0[0] == a1[0] and a0[1] == a1[1]:
                        total_unlabeled_matched += 1
                        label_confusions.update([(a0[2], a1[2])])
                        if a0[2] == a1[2]:
                            total_matched += 1
                            matched += 1
        for (pred_id, args) in prediction.items():
            filtered_args = [a for a in args if a[2] not in ['V']]
            total_predicted += len(filtered_args)
            pred_rels += len(filtered_args)
        if gold_rels == matched and pred_rels == matched:
            num_sents += 1
    (precision, recall, f1) = _calc_f1(total_gold, total_predicted, total_matched)
    (unlabeled_precision, unlabeled_recall, unlabeled_f1) = _calc_f1(total_gold, total_predicted, total_unlabeled_matched)
    if not gold_path:
        gold_path = tempfile.NamedTemporaryFile().name
        print_to_conll(sentences, gold_srl, gold_path, None)
        gold_predicates = None
    else:
        gold_predicates = read_gold_predicates(gold_path)
    temp_output = tempfile.NamedTemporaryFile().name
    print_to_conll(sentences, predictions, temp_output, gold_predicates)
    (conll_precision, conll_recall, conll_f1) = official_conll_05_evaluate(temp_output, gold_path)
    return SRLScores(unlabeled_precision, unlabeled_recall, unlabeled_f1, precision, recall, f1, conll_precision, conll_recall, conll_f1, label_confusions, num_sents)","for a0 in filtered_gold_args:
    for a1 in prediction[pred_id]:
        if a0[0] == a1[0] and a0[1] == a1[1]:
            total_unlabeled_matched += 1
            label_confusions.update([(a0[2], a1[2])])
            if a0[2] == a1[2]:
                total_matched += 1
                matched += 1","['for a0 in filtered_gold_args:\n    (a0_0, a0_1, a0_2, *_) = a0\n    for a1 in prediction[pred_id]:\n        if a0_0 == a1[0] and a0_1 == a1[1]:\n            total_unlabeled_matched += 1\n            label_confusions.update([(a0_2, a1[2])])\n            if a0_2 == a1[2]:\n                total_matched += 1\n                matched += 1', 'for (a0_0, a0_1, a0_2, *a0_len) in filtered_gold_args:\n    for a1 in prediction[pred_id]:\n        if \n        a0_0 == a1[0] and \n        a0_1 == a1[1]:\n            total_unlabeled_matched += 1\n            label_confusions.update([(\n            a0_2, a1[2])])\n            if \n            a0_2 == a1[2]:\n                total_matched += 1\n                matched += 1']",no_found,0
HanLP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HanLP/hanlp/components/srl/span_rank/srl_eval_utils.py,https://github.com/hankcs/HanLP/tree/master/hanlp/components/srl/span_rank/srl_eval_utils.py,,compute_srl_f1$162,"def compute_srl_f1(sentences, gold_srl, predictions, gold_path=None) -> SRLScores:
    assert len(gold_srl) == len(predictions)
    total_gold = 0
    total_predicted = 0
    total_matched = 0
    total_unlabeled_matched = 0
    num_sents = 0
    label_confusions = Counter()
    for (gold, prediction) in zip(gold_srl, predictions):
        gold_rels = 0
        pred_rels = 0
        matched = 0
        for (pred_id, gold_args) in gold.items():
            filtered_gold_args = [a for a in gold_args if a[2] not in ['V', 'C-V']]
            total_gold += len(filtered_gold_args)
            gold_rels += len(filtered_gold_args)
            if pred_id not in prediction:
                continue
            for a0 in filtered_gold_args:
                for a1 in prediction[pred_id]:
                    if a0[0] == a1[0] and a0[1] == a1[1]:
                        total_unlabeled_matched += 1
                        label_confusions.update([(a0[2], a1[2])])
                        if a0[2] == a1[2]:
                            total_matched += 1
                            matched += 1
        for (pred_id, args) in prediction.items():
            filtered_args = [a for a in args if a[2] not in ['V']]
            total_predicted += len(filtered_args)
            pred_rels += len(filtered_args)
        if gold_rels == matched and pred_rels == matched:
            num_sents += 1
    (precision, recall, f1) = _calc_f1(total_gold, total_predicted, total_matched)
    (unlabeled_precision, unlabeled_recall, unlabeled_f1) = _calc_f1(total_gold, total_predicted, total_unlabeled_matched)
    if not gold_path:
        gold_path = tempfile.NamedTemporaryFile().name
        print_to_conll(sentences, gold_srl, gold_path, None)
        gold_predicates = None
    else:
        gold_predicates = read_gold_predicates(gold_path)
    temp_output = tempfile.NamedTemporaryFile().name
    print_to_conll(sentences, predictions, temp_output, gold_predicates)
    (conll_precision, conll_recall, conll_f1) = official_conll_05_evaluate(temp_output, gold_path)
    return SRLScores(unlabeled_precision, unlabeled_recall, unlabeled_f1, precision, recall, f1, conll_precision, conll_recall, conll_f1, label_confusions, num_sents)","for a1 in prediction[pred_id]:
    if a0[0] == a1[0] and a0[1] == a1[1]:
        total_unlabeled_matched += 1
        label_confusions.update([(a0[2], a1[2])])
        if a0[2] == a1[2]:
            total_matched += 1
            matched += 1","['for a1 in prediction[pred_id]:\n    (a1_0, a1_1, a1_2, *_) = a1\n    if a0[0] == a1_0 and a0[1] == a1_1:\n        total_unlabeled_matched += 1\n        label_confusions.update([(a0[2], a1_2)])\n        if a0[2] == a1_2:\n            total_matched += 1\n            matched += 1', 'for (a1_0, a1_1, a1_2, *a1_len) in prediction[pred_id]:\n    if a0[0] == \n    a1_0 and a0[1] == \n    a1_1:\n        total_unlabeled_matched += 1\n        label_confusions.update([(a0[2], \n        a1_2)])\n        if a0[2] == \n        a1_2:\n            total_matched += 1\n            matched += 1']",no_found,0
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/setup/doctype/company/company.py,https://github.com/frappe/erpnext/tree/master/erpnext/setup/doctype/company/company.py,Company,validate_default_accounts$88,"def validate_default_accounts(self):
    accounts = [['Default Bank Account', 'default_bank_account'], ['Default Cash Account', 'default_cash_account'], ['Default Receivable Account', 'default_receivable_account'], ['Default Payable Account', 'default_payable_account'], ['Default Expense Account', 'default_expense_account'], ['Default Income Account', 'default_income_account'], ['Stock Received But Not Billed Account', 'stock_received_but_not_billed'], ['Stock Adjustment Account', 'stock_adjustment_account'], ['Expense Included In Valuation Account', 'expenses_included_in_valuation']]
    for account in accounts:
        if self.get(account[1]):
            for_company = frappe.db.get_value('Account', self.get(account[1]), 'company')
            if for_company != self.name:
                frappe.throw(_('Account {0} does not belong to company: {1}').format(self.get(account[1]), self.name))
            if get_account_currency(self.get(account[1])) != self.default_currency:
                error_message = _(""{0} currency must be same as company's default currency. Please select another account."").format(frappe.bold(account[0]))
                frappe.throw(error_message)","for account in accounts:
    if self.get(account[1]):
        for_company = frappe.db.get_value('Account', self.get(account[1]), 'company')
        if for_company != self.name:
            frappe.throw(_('Account {0} does not belong to company: {1}').format(self.get(account[1]), self.name))
        if get_account_currency(self.get(account[1])) != self.default_currency:
            error_message = _(""{0} currency must be same as company's default currency. Please select another account."").format(frappe.bold(account[0]))
            frappe.throw(error_message)","['for account in accounts:\n    (account_0, account_1, *_) = account\n    if self.get(account_1):\n        for_company = frappe.db.get_value(\'Account\', self.get(account_1), \'company\')\n        if for_company != self.name:\n            frappe.throw(_(\'Account {0} does not belong to company: {1}\').format(self.get(account_1), self.name))\n        if get_account_currency(self.get(account_1)) != self.default_currency:\n            error_message = _(""{0} currency must be same as company\'s default currency. Please select another account."").format(frappe.bold(account_0))\n            frappe.throw(error_message)', 'for (account_0, account_1, *account_len) in accounts:\n    if self.get(account_1):\n        for_company = frappe.db.get_value(\'Account\', self.get(account_1), \'company\')\n        if for_company != self.name:\n            frappe.throw(_(\'Account {0} does not belong to company: {1}\').format(self.get(account_1), self.name))\n        if get_account_currency(self.get(account_1)) != self.default_currency:\n            error_message = _(""{0} currency must be same as company\'s default currency. Please select another account."").format(frappe.bold(account_0))\n            frappe.throw(error_message)']",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for api_info in api_infos:
    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
    para3 = para2.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para3.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para3.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num1 = self.creat_num1 + 1
        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)","['for api_info in api_infos:\n    (_, api_info_1, api_info_2, *api_info_rapi_infomaining) = api_info\n    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n    para3 = para2.insert_paragraph_before(\'\')\n    UserLogin = api_info_2\n    api_path = api_info_1\n    run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_bac}\') + \'\\n\')\n    run.font.name = \'Arial\'\n    run.font.size = Pt(16)\n    run.font.bold = True\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    for js_path in js_paths:\n        run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n        run2.font.name = \'Arial\'\n        run2.font.size = Pt(10)\n        run2.font.bold = True\n        run3 = para3.add_run(api_path)\n        run3.font.name = \'Arial\'\n        run3.font.size = Pt(10)\n        run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n        run4.font.name = \'Arial\'\n        run4.font.size = Pt(10)\n        run4.font.bold = True\n        run5 = para3.add_run(js_path[0])\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{request_info}\'))\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5.font.bold = True\n        self.creat_num1 = self.creat_num1 + 1\n        info1 = \'1: \' + vuln_info[5].split(\'\')[0] + \'\\n\\n\' + \'2: \' + vuln_info[5].split(\'\')[1]\n        info2 = \'1: \' + vuln_info[6].split(\'\')[0] + \'\\n\\n\' + \'2: \' + vuln_info[6].split(\'\')[1]\n        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)\n        run6 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n        run6.font.name = \'Arial\'\n        run6.font.size = Pt(10)\n        run6.font.bold = True\n        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)', 'for (api_info_0, api_info_1, api_info_2, *api_info_len) in api_infos:\n    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n    para3 = para2.insert_paragraph_before(\'\')\n    UserLogin = \n    api_info_2\n    api_path = \n    api_info_1\n    run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_bac}\') + \'\\n\')\n    run.font.name = \'Arial\'\n    run.font.size = Pt(16)\n    run.font.bold = True\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    for js_path in js_paths:\n        run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n        run2.font.name = \'Arial\'\n        run2.font.size = Pt(10)\n        run2.font.bold = True\n        run3 = para3.add_run(api_path)\n        run3.font.name = \'Arial\'\n        run3.font.size = Pt(10)\n        run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n        run4.font.name = \'Arial\'\n        run4.font.size = Pt(10)\n        run4.font.bold = True\n        run5 = para3.add_run(js_path[0])\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{request_info}\'))\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5.font.bold = True\n        self.creat_num1 = self.creat_num1 + 1\n        info1 = \'1: \' + vuln_info[5].split(\'\')[0] + \'\\n\\n\' + \'2: \' + vuln_info[5].split(\'\')[1]\n        info2 = \'1: \' + vuln_info[6].split(\'\')[0] + \'\\n\\n\' + \'2: \' + vuln_info[6].split(\'\')[1]\n        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)\n        run6 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n        run6.font.name = \'Arial\'\n        run6.font.size = Pt(10)\n        run6.font.bold = True\n        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)']",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for api_info in api_infos:
    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
    para3 = para2.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para3.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para3.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num1 = self.creat_num1 + 1
        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)","['for api_info in api_infos:\n    (_, api_info_1, api_info_2, *api_info_rapi_infomaining) = api_info\n    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n    para3 = para2.insert_paragraph_before(\'\')\n    UserLogin = api_info_2\n    api_path = api_info_1\n    run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_passWord}\') + \'\\n\')\n    run.font.name = \'Arial\'\n    run.font.size = Pt(16)\n    run.font.bold = True\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    for js_path in js_paths:\n        run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n        run2.font.name = \'Arial\'\n        run2.font.size = Pt(10)\n        run2.font.bold = True\n        run3 = para3.add_run(api_path)\n        run3.font.name = \'Arial\'\n        run3.font.size = Pt(10)\n        run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n        run4.font.name = \'Arial\'\n        run4.font.size = Pt(10)\n        run4.font.bold = True\n        run5 = para3.add_run(js_path[0])\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5.font.bold = True\n        self.creat_num1 = self.creat_num1 + 1\n        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)\n        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)\n        run6 = para2.add_run(Utils().getMyWord(\'{request_info}\'))\n        run6.font.name = \'Arial\'\n        run6.font.size = Pt(10)\n        run6.font.bold = True\n        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)\n        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)', 'for (api_info_0, api_info_1, api_info_2, *api_info_len) in api_infos:\n    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n    para3 = para2.insert_paragraph_before(\'\')\n    UserLogin = \n    api_info_2\n    api_path = \n    api_info_1\n    run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_passWord}\') + \'\\n\')\n    run.font.name = \'Arial\'\n    run.font.size = Pt(16)\n    run.font.bold = True\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    for js_path in js_paths:\n        run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n        run2.font.name = \'Arial\'\n        run2.font.size = Pt(10)\n        run2.font.bold = True\n        run3 = para3.add_run(api_path)\n        run3.font.name = \'Arial\'\n        run3.font.size = Pt(10)\n        run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n        run4.font.name = \'Arial\'\n        run4.font.size = Pt(10)\n        run4.font.bold = True\n        run5 = para3.add_run(js_path[0])\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5.font.bold = True\n        self.creat_num1 = self.creat_num1 + 1\n        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)\n        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)\n        run6 = para2.add_run(Utils().getMyWord(\'{request_info}\'))\n        run6.font.name = \'Arial\'\n        run6.font.size = Pt(10)\n        run6.font.bold = True\n        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)\n        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)']",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for api_info in api_infos:
    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
    para3 = para2.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para3.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para3.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num1 = self.creat_num1 + 1
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run6.font.name = 'Arial'
        run6.font.size = Pt(10)
        run6.font.bold = True
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","['for api_info in api_infos:\n    (_, api_info_1, api_info_2, *api_info_rapi_infomaining) = api_info\n    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n    para3 = para2.insert_paragraph_before(\'\')\n    UserLogin = api_info_2\n    api_path = api_info_1\n    run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_upload}\') + \'\\n\')\n    run.font.name = \'Arial\'\n    run.font.size = Pt(16)\n    run.font.bold = True\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    for js_path in js_paths:\n        run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n        run2.font.name = \'Arial\'\n        run2.font.size = Pt(10)\n        run2.font.bold = True\n        run3 = para3.add_run(api_path)\n        run3.font.name = \'Arial\'\n        run3.font.size = Pt(10)\n        run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n        run4.font.name = \'Arial\'\n        run4.font.size = Pt(10)\n        run4.font.bold = True\n        run5 = para3.add_run(js_path[0])\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{request_info}\'))\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5.font.bold = True\n        self.creat_num1 = self.creat_num1 + 1\n        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)\n        run6 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n        run6.font.name = \'Arial\'\n        run6.font.size = Pt(10)\n        run6.font.bold = True\n        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)', 'for (api_info_0, api_info_1, api_info_2, *api_info_len) in api_infos:\n    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n    para3 = para2.insert_paragraph_before(\'\')\n    UserLogin = \n    api_info_2\n    api_path = \n    api_info_1\n    run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_upload}\') + \'\\n\')\n    run.font.name = \'Arial\'\n    run.font.size = Pt(16)\n    run.font.bold = True\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    for js_path in js_paths:\n        run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n        run2.font.name = \'Arial\'\n        run2.font.size = Pt(10)\n        run2.font.bold = True\n        run3 = para3.add_run(api_path)\n        run3.font.name = \'Arial\'\n        run3.font.size = Pt(10)\n        run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n        run4.font.name = \'Arial\'\n        run4.font.size = Pt(10)\n        run4.font.bold = True\n        run5 = para3.add_run(js_path[0])\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{request_info}\'))\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5.font.bold = True\n        self.creat_num1 = self.creat_num1 + 1\n        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)\n        run6 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n        run6.font.name = \'Arial\'\n        run6.font.size = Pt(10)\n        run6.font.bold = True\n        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)']",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for api_info in api_infos:
    para2 = para1.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para2.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para2.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num = self.creat_num + 1
        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)","['for api_info in api_infos:\n    (_, api_info_1, api_info_2, *api_info_rapi_infomaining) = api_info\n    para2 = para1.insert_paragraph_before(\'\')\n    UserLogin = api_info_2\n    api_path = api_info_1\n    run = para2.add_run(\'2.\' + str(self.creat_num) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_unauth_maybe}\') + \'\\n\')\n    run.font.name = \'Arial\'\n    run.font.size = Pt(16)\n    run.font.bold = True\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    for js_path in js_paths:\n        run2 = para2.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n        run2.font.name = \'Arial\'\n        run2.font.size = Pt(10)\n        run2.font.bold = True\n        run3 = para2.add_run(api_path)\n        run3.font.name = \'Arial\'\n        run3.font.size = Pt(10)\n        run4 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n        run4.font.name = \'Arial\'\n        run4.font.size = Pt(10)\n        run4.font.bold = True\n        run5 = para2.add_run(js_path[0])\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5.font.bold = True\n        self.creat_num = self.creat_num + 1\n        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)\n        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)', 'for (api_info_0, api_info_1, api_info_2, *api_info_len) in api_infos:\n    para2 = para1.insert_paragraph_before(\'\')\n    UserLogin = \n    api_info_2\n    api_path = \n    api_info_1\n    run = para2.add_run(\'2.\' + str(self.creat_num) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_unauth_maybe}\') + \'\\n\')\n    run.font.name = \'Arial\'\n    run.font.size = Pt(16)\n    run.font.bold = True\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    for js_path in js_paths:\n        run2 = para2.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n        run2.font.name = \'Arial\'\n        run2.font.size = Pt(10)\n        run2.font.bold = True\n        run3 = para2.add_run(api_path)\n        run3.font.name = \'Arial\'\n        run3.font.size = Pt(10)\n        run4 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n        run4.font.name = \'Arial\'\n        run4.font.size = Pt(10)\n        run4.font.bold = True\n        run5 = para2.add_run(js_path[0])\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5.font.bold = True\n        self.creat_num = self.creat_num + 1\n        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)\n        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)']",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for api_info in api_infos:
    para2 = para1.insert_paragraph_before('')
    UserLogin = api_info[2]
    api_path = api_info[1]
    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
    run.font.name = 'Arial'
    run.font.size = Pt(16)
    run.font.bold = True
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    for js_path in js_paths:
        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
        run2.font.name = 'Arial'
        run2.font.size = Pt(10)
        run2.font.bold = True
        run3 = para2.add_run(api_path)
        run3.font.name = 'Arial'
        run3.font.size = Pt(10)
        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
        run4.font.name = 'Arial'
        run4.font.size = Pt(10)
        run4.font.bold = True
        run5 = para2.add_run(js_path[0])
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
        run5.font.name = 'Arial'
        run5.font.size = Pt(10)
        run5.font.bold = True
        self.creat_num = self.creat_num + 1
        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)","['for api_info in api_infos:\n    (_, api_info_1, api_info_2, *api_info_rapi_infomaining) = api_info\n    para2 = para1.insert_paragraph_before(\'\')\n    UserLogin = api_info_2\n    api_path = api_info_1\n    run = para2.add_run(\'2.\' + str(self.creat_num) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_unauth}\') + \'\\n\')\n    run.font.name = \'Arial\'\n    run.font.size = Pt(16)\n    run.font.bold = True\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    for js_path in js_paths:\n        run2 = para2.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n        run2.font.name = \'Arial\'\n        run2.font.size = Pt(10)\n        run2.font.bold = True\n        run3 = para2.add_run(api_path)\n        run3.font.name = \'Arial\'\n        run3.font.size = Pt(10)\n        run4 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n        run4.font.name = \'Arial\'\n        run4.font.size = Pt(10)\n        run4.font.bold = True\n        run5 = para2.add_run(js_path[0])\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5.font.bold = True\n        self.creat_num = self.creat_num + 1\n        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)\n        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)', 'for (api_info_0, api_info_1, api_info_2, *api_info_len) in api_infos:\n    para2 = para1.insert_paragraph_before(\'\')\n    UserLogin = \n    api_info_2\n    api_path = \n    api_info_1\n    run = para2.add_run(\'2.\' + str(self.creat_num) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_unauth}\') + \'\\n\')\n    run.font.name = \'Arial\'\n    run.font.size = Pt(16)\n    run.font.bold = True\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    for js_path in js_paths:\n        run2 = para2.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n        run2.font.name = \'Arial\'\n        run2.font.size = Pt(10)\n        run2.font.bold = True\n        run3 = para2.add_run(api_path)\n        run3.font.name = \'Arial\'\n        run3.font.size = Pt(10)\n        run4 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n        run4.font.name = \'Arial\'\n        run4.font.size = Pt(10)\n        run4.font.bold = True\n        run5 = para2.add_run(js_path[0])\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n        run5.font.name = \'Arial\'\n        run5.font.size = Pt(10)\n        run5.font.bold = True\n        self.creat_num = self.creat_num + 1\n        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)\n        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)']",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/CreatHtml.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/CreatHtml.py,CreatHtml,vuln_list$27,"def vuln_list(self):
    global creat_vuln_num
    creat_vuln_num = 1
    tr_whole_api_list = ''
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    for vuln_info in vuln_infos:
        if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
            sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
            cursor.execute(sql)
            api_infos = cursor.fetchall()
            for api_info in api_infos:
                tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
                if api_info[5] == 1:
                    vuln_url_type = Utils().getMyWord('{r_get}')
                else:
                    vuln_url_type = Utils().getMyWord('{r_post}')
                tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                try:
                    api_length_info = len(api_info[4])
                except:
                    api_length_info = 0
                for js_path in js_paths:
                    tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                    tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                    tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                    tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                    tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
                    tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                    self.creat_api_num = self.creat_api_num + 1
                    creat_vuln_num = creat_vuln_num + 1
                    tr_whole_api_list = tr_whole_api_list + tr_api_list
        elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
            sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
            cursor.execute(sql)
            api_infos = cursor.fetchall()
            for api_info in api_infos:
                tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
                if api_info[5] == 1:
                    vuln_url_type = Utils().getMyWord('{r_get}')
                else:
                    vuln_url_type = Utils().getMyWord('{r_post}')
                tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                try:
                    api_length_info = len(api_info[4])
                except:
                    api_length_info = 0
                for js_path in js_paths:
                    tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                    tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                    tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                    tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                    tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '' + Utils().getMyWord('{r_vuln_maybe}'))
                    tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                    self.creat_api_num = self.creat_api_num + 1
                    creat_vuln_num = creat_vuln_num + 1
                    tr_whole_api_list = tr_whole_api_list + tr_api_list
    return tr_whole_api_list","for api_info in api_infos:
    tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
    if api_info[5] == 1:
        vuln_url_type = Utils().getMyWord('{r_get}')
    else:
        vuln_url_type = Utils().getMyWord('{r_post}')
    tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    try:
        api_length_info = len(api_info[4])
    except:
        api_length_info = 0
    for js_path in js_paths:
        tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
        tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
        tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
        tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
        tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '' + Utils().getMyWord('{r_vuln_maybe}'))
        tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
        self.creat_api_num = self.creat_api_num + 1
        creat_vuln_num = creat_vuln_num + 1
        tr_whole_api_list = tr_whole_api_list + tr_api_list","['for api_info in api_infos:\n    (_, api_info_1, api_info_2, _, api_info_4, api_info_5, *_) = api_info\n    tr_api_list = \'\\n                    <tr>\\n                      <td>{vuln_num}</td>\\n                      <td>{vuln_api_name}</td>\\n                      <td>{vuln_url}</td>\\n                      <td>{vuln_url_type}</td>\\n                      <td>{vuln_risk}</td>\\n                      <td>{vuln_length}</td>\\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\\n                    </tr>\'\n    if api_info_5 == 1:\n        vuln_url_type = Utils().getMyWord(\'{r_get}\')\n    else:\n        vuln_url_type = Utils().getMyWord(\'{r_post}\')\n    tr_api_list = tr_api_list.replace(\'{vuln_num}\', str(self.creat_api_num))\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    try:\n        api_length_info = len(api_info_4)\n    except:\n        api_length_info = 0\n    for js_path in js_paths:\n        tr_api_list = tr_api_list.replace(\'{vuln_api_name}\', api_info_2)\n        tr_api_list = tr_api_list.replace(\'{vuln_url}\', api_info_1)\n        tr_api_list = tr_api_list.replace(\'{vuln_url_type}\', vuln_url_type)\n        tr_api_list = tr_api_list.replace(\'{vuln_length}\', str(api_length_info))\n        tr_api_list = tr_api_list.replace(\'{vuln_risk}\', Utils().getMyWord(\'{r_l_l}\') + \'' + Utils().getMyWord(\'{r_vuln_maybe}\'))\n        tr_api_list = tr_api_list.replace(\'{vuln_id}\', \'vuln_\' + str(creat_vuln_num))\n        self.creat_api_num = self.creat_api_num + 1\n        creat_vuln_num = creat_vuln_num + 1\n        tr_whole_api_list = tr_whole_api_list + tr_api_list', 'for (api_info_0, api_info_1, api_info_2, api_info_3, api_info_4, api_info_5, *api_info_len) in api_infos:\n    tr_api_list = \'\\n                    <tr>\\n                      <td>{vuln_num}</td>\\n                      <td>{vuln_api_name}</td>\\n                      <td>{vuln_url}</td>\\n                      <td>{vuln_url_type}</td>\\n                      <td>{vuln_risk}</td>\\n                      <td>{vuln_length}</td>\\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\\n                    </tr>\'\n    if \n    api_info_5 == 1:\n        vuln_url_type = Utils().getMyWord(\'{r_get}\')\n    else:\n        vuln_url_type = Utils().getMyWord(\'{r_post}\')\n    tr_api_list = tr_api_list.replace(\'{vuln_num}\', str(self.creat_api_num))\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    try:\n        api_length_info = len(\n        api_info_4)\n    except:\n        api_length_info = 0\n    for js_path in js_paths:\n        tr_api_list = tr_api_list.replace(\'{vuln_api_name}\', \n        api_info_2)\n        tr_api_list = tr_api_list.replace(\'{vuln_url}\', \n        api_info_1)\n        tr_api_list = tr_api_list.replace(\'{vuln_url_type}\', vuln_url_type)\n        tr_api_list = tr_api_list.replace(\'{vuln_length}\', str(api_length_info))\n        tr_api_list = tr_api_list.replace(\'{vuln_risk}\', Utils().getMyWord(\'{r_l_l}\') + \'' + Utils().getMyWord(\'{r_vuln_maybe}\'))\n        tr_api_list = tr_api_list.replace(\'{vuln_id}\', \'vuln_\' + str(creat_vuln_num))\n        self.creat_api_num = self.creat_api_num + 1\n        creat_vuln_num = creat_vuln_num + 1\n        tr_whole_api_list = tr_whole_api_list + tr_api_list']",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/CreatHtml.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/CreatHtml.py,CreatHtml,vuln_list$27,"def vuln_list(self):
    global creat_vuln_num
    creat_vuln_num = 1
    tr_whole_api_list = ''
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    for vuln_info in vuln_infos:
        if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
            sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
            cursor.execute(sql)
            api_infos = cursor.fetchall()
            for api_info in api_infos:
                tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
                if api_info[5] == 1:
                    vuln_url_type = Utils().getMyWord('{r_get}')
                else:
                    vuln_url_type = Utils().getMyWord('{r_post}')
                tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                try:
                    api_length_info = len(api_info[4])
                except:
                    api_length_info = 0
                for js_path in js_paths:
                    tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                    tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                    tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                    tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                    tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
                    tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                    self.creat_api_num = self.creat_api_num + 1
                    creat_vuln_num = creat_vuln_num + 1
                    tr_whole_api_list = tr_whole_api_list + tr_api_list
        elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
            sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
            cursor.execute(sql)
            api_infos = cursor.fetchall()
            for api_info in api_infos:
                tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
                if api_info[5] == 1:
                    vuln_url_type = Utils().getMyWord('{r_get}')
                else:
                    vuln_url_type = Utils().getMyWord('{r_post}')
                tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                try:
                    api_length_info = len(api_info[4])
                except:
                    api_length_info = 0
                for js_path in js_paths:
                    tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                    tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                    tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                    tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                    tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '' + Utils().getMyWord('{r_vuln_maybe}'))
                    tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                    self.creat_api_num = self.creat_api_num + 1
                    creat_vuln_num = creat_vuln_num + 1
                    tr_whole_api_list = tr_whole_api_list + tr_api_list
    return tr_whole_api_list","for api_info in api_infos:
    tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
    if api_info[5] == 1:
        vuln_url_type = Utils().getMyWord('{r_get}')
    else:
        vuln_url_type = Utils().getMyWord('{r_post}')
    tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
    cursor.execute(sql)
    js_paths = cursor.fetchall()
    try:
        api_length_info = len(api_info[4])
    except:
        api_length_info = 0
    for js_path in js_paths:
        tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
        tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
        tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
        tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
        tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
        tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
        self.creat_api_num = self.creat_api_num + 1
        creat_vuln_num = creat_vuln_num + 1
        tr_whole_api_list = tr_whole_api_list + tr_api_list","['for api_info in api_infos:\n    (_, api_info_1, api_info_2, _, api_info_4, api_info_5, *_) = api_info\n    tr_api_list = \'\\n                    <tr>\\n                      <td>{vuln_num}</td>\\n                      <td>{vuln_api_name}</td>\\n                      <td>{vuln_url}</td>\\n                      <td>{vuln_url_type}</td>\\n                      <td>{vuln_risk}</td>\\n                      <td>{vuln_length}</td>\\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\\n                    </tr>\'\n    if api_info_5 == 1:\n        vuln_url_type = Utils().getMyWord(\'{r_get}\')\n    else:\n        vuln_url_type = Utils().getMyWord(\'{r_post}\')\n    tr_api_list = tr_api_list.replace(\'{vuln_num}\', str(self.creat_api_num))\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    try:\n        api_length_info = len(api_info_4)\n    except:\n        api_length_info = 0\n    for js_path in js_paths:\n        tr_api_list = tr_api_list.replace(\'{vuln_api_name}\', api_info_2)\n        tr_api_list = tr_api_list.replace(\'{vuln_url}\', api_info_1)\n        tr_api_list = tr_api_list.replace(\'{vuln_url_type}\', vuln_url_type)\n        tr_api_list = tr_api_list.replace(\'{vuln_length}\', str(api_length_info))\n        tr_api_list = tr_api_list.replace(\'{vuln_risk}\', Utils().getMyWord(\'{r_l_m}\'))\n        tr_api_list = tr_api_list.replace(\'{vuln_id}\', \'vuln_\' + str(creat_vuln_num))\n        self.creat_api_num = self.creat_api_num + 1\n        creat_vuln_num = creat_vuln_num + 1\n        tr_whole_api_list = tr_whole_api_list + tr_api_list', 'for (api_info_0, api_info_1, api_info_2, api_info_3, api_info_4, api_info_5, *api_info_len) in api_infos:\n    tr_api_list = \'\\n                    <tr>\\n                      <td>{vuln_num}</td>\\n                      <td>{vuln_api_name}</td>\\n                      <td>{vuln_url}</td>\\n                      <td>{vuln_url_type}</td>\\n                      <td>{vuln_risk}</td>\\n                      <td>{vuln_length}</td>\\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\\n                    </tr>\'\n    if \n    api_info_5 == 1:\n        vuln_url_type = Utils().getMyWord(\'{r_get}\')\n    else:\n        vuln_url_type = Utils().getMyWord(\'{r_post}\')\n    tr_api_list = tr_api_list.replace(\'{vuln_num}\', str(self.creat_api_num))\n    sql = ""select path from js_file where id=\'%s\'"" % vuln_info[2]\n    cursor.execute(sql)\n    js_paths = cursor.fetchall()\n    try:\n        api_length_info = len(\n        api_info_4)\n    except:\n        api_length_info = 0\n    for js_path in js_paths:\n        tr_api_list = tr_api_list.replace(\'{vuln_api_name}\', \n        api_info_2)\n        tr_api_list = tr_api_list.replace(\'{vuln_url}\', \n        api_info_1)\n        tr_api_list = tr_api_list.replace(\'{vuln_url_type}\', vuln_url_type)\n        tr_api_list = tr_api_list.replace(\'{vuln_length}\', str(api_length_info))\n        tr_api_list = tr_api_list.replace(\'{vuln_risk}\', Utils().getMyWord(\'{r_l_m}\'))\n        tr_api_list = tr_api_list.replace(\'{vuln_id}\', \'vuln_\' + str(creat_vuln_num))\n        self.creat_api_num = self.creat_api_num + 1\n        creat_vuln_num = creat_vuln_num + 1\n        tr_whole_api_list = tr_whole_api_list + tr_api_list']",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/CreatHtml.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/CreatHtml.py,CreatHtml,api_detail_html$1072,"def api_detail_html(self):
    api_id = 1
    whole_html = ''
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from api_tree where success = 1 or success = 2'
    cursor.execute(sql)
    api_infos = cursor.fetchall()
    for api_info in api_infos:
        api_info_html = '<div id=""api_{api_id}"" class=""modal fade"" role=""dialog"">\n            \t<div class=""modal-dialog"">\n            \t\t<div class=""modal-content"">\n            \t\t\t<div class=""modal-header"">\n            \t\t\t\t<h6 class=""modal-title"">%s...</h4>\n            \t\t\t</div>\n            \t\t\t<div class=""modal-body"">\n            \t\t\t\t<div class=""box-body no-padding"">\n            \t\t\t\t\t<p>\n            \t\t\t\t\t\t%sapi_name}<br><br>\n                        %sapi_type}<br><br>\n                        %s <a href=""{api_path}"">{api_path}</a><br><br>\n                        %s {api_js}<br><br>\n                        %s <code>{api_res}</code><br>\n            \t\t\t\t\t</p>\n            \t\t\t\t</div>\n            \t\t\t</div>\n            \t\t</div>\n            \t</div>\n            </div>' % (Utils().getMyWord('{api_detail}'), Utils().getMyWord('{api_name}'), Utils().getMyWord('{r_type}'), Utils().getMyWord('{r_api_addr}'), Utils().getMyWord('{r_api_js}'), Utils().getMyWord('{r_api_res}'))
        sql = ""select path from js_file where id='%s'"" % api_info[6]
        cursor.execute(sql)
        js_path = cursor.fetchone()
        if api_info[5] == 2:
            api_type = Utils().getMyWord('{r_post}')
        else:
            api_type = Utils().getMyWord('{r_get}')
        if api_info[4] == None:
            api_res = ''
        else:
            api_res = api_info[4]
        api_info_html = api_info_html.replace('{api_id}', str(api_id))
        api_info_html = api_info_html.replace('{api_type}', str(api_type))
        api_info_html = api_info_html.replace('{api_name}', api_info[2])
        api_info_html = api_info_html.replace('{api_path}', api_info[1])
        api_info_html = api_info_html.replace('{api_js}', js_path[0])
        api_info_html = api_info_html.replace('{api_res}', api_res)
        api_id = api_id + 1
        whole_html = whole_html + api_info_html
    return whole_html","for api_info in api_infos:
    api_info_html = '<div id=""api_{api_id}"" class=""modal fade"" role=""dialog"">\n            \t<div class=""modal-dialog"">\n            \t\t<div class=""modal-content"">\n            \t\t\t<div class=""modal-header"">\n            \t\t\t\t<h6 class=""modal-title"">%s...</h4>\n            \t\t\t</div>\n            \t\t\t<div class=""modal-body"">\n            \t\t\t\t<div class=""box-body no-padding"">\n            \t\t\t\t\t<p>\n            \t\t\t\t\t\t%sapi_name}<br><br>\n                        %sapi_type}<br><br>\n                        %s <a href=""{api_path}"">{api_path}</a><br><br>\n                        %s {api_js}<br><br>\n                        %s <code>{api_res}</code><br>\n            \t\t\t\t\t</p>\n            \t\t\t\t</div>\n            \t\t\t</div>\n            \t\t</div>\n            \t</div>\n            </div>' % (Utils().getMyWord('{api_detail}'), Utils().getMyWord('{api_name}'), Utils().getMyWord('{r_type}'), Utils().getMyWord('{r_api_addr}'), Utils().getMyWord('{r_api_js}'), Utils().getMyWord('{r_api_res}'))
    sql = ""select path from js_file where id='%s'"" % api_info[6]
    cursor.execute(sql)
    js_path = cursor.fetchone()
    if api_info[5] == 2:
        api_type = Utils().getMyWord('{r_post}')
    else:
        api_type = Utils().getMyWord('{r_get}')
    if api_info[4] == None:
        api_res = ''
    else:
        api_res = api_info[4]
    api_info_html = api_info_html.replace('{api_id}', str(api_id))
    api_info_html = api_info_html.replace('{api_type}', str(api_type))
    api_info_html = api_info_html.replace('{api_name}', api_info[2])
    api_info_html = api_info_html.replace('{api_path}', api_info[1])
    api_info_html = api_info_html.replace('{api_js}', js_path[0])
    api_info_html = api_info_html.replace('{api_res}', api_res)
    api_id = api_id + 1
    whole_html = whole_html + api_info_html","['for api_info in api_infos:\n    (_, api_info_1, api_info_2, _, api_info_4, api_info_5, api_info_6, *_) = api_info\n    api_info_html = \'<div id=""api_{api_id}"" class=""modal fade"" role=""dialog"">\\n            \\t<div class=""modal-dialog"">\\n            \\t\\t<div class=""modal-content"">\\n            \\t\\t\\t<div class=""modal-header"">\\n            \\t\\t\\t\\t<h6 class=""modal-title"">%s...</h4>\\n            \\t\\t\\t</div>\\n            \\t\\t\\t<div class=""modal-body"">\\n            \\t\\t\\t\\t<div class=""box-body no-padding"">\\n            \\t\\t\\t\\t\\t<p>\\n            \\t\\t\\t\\t\\t\\t%sapi_name}<br><br>\\n                        %sapi_type}<br><br>\\n                        %s <a href=""{api_path}"">{api_path}</a><br><br>\\n                        %s {api_js}<br><br>\\n                        %s <code>{api_res}</code><br>\\n            \\t\\t\\t\\t\\t</p>\\n            \\t\\t\\t\\t</div>\\n            \\t\\t\\t</div>\\n            \\t\\t</div>\\n            \\t</div>\\n            </div>\' % (Utils().getMyWord(\'{api_detail}\'), Utils().getMyWord(\'{api_name}\'), Utils().getMyWord(\'{r_type}\'), Utils().getMyWord(\'{r_api_addr}\'), Utils().getMyWord(\'{r_api_js}\'), Utils().getMyWord(\'{r_api_res}\'))\n    sql = ""select path from js_file where id=\'%s\'"" % api_info_6\n    cursor.execute(sql)\n    js_path = cursor.fetchone()\n    if api_info_5 == 2:\n        api_type = Utils().getMyWord(\'{r_post}\')\n    else:\n        api_type = Utils().getMyWord(\'{r_get}\')\n    if api_info_4 == None:\n        api_res = \'\'\n    else:\n        api_res = api_info_4\n    api_info_html = api_info_html.replace(\'{api_id}\', str(api_id))\n    api_info_html = api_info_html.replace(\'{api_type}\', str(api_type))\n    api_info_html = api_info_html.replace(\'{api_name}\', api_info_2)\n    api_info_html = api_info_html.replace(\'{api_path}\', api_info_1)\n    api_info_html = api_info_html.replace(\'{api_js}\', js_path[0])\n    api_info_html = api_info_html.replace(\'{api_res}\', api_res)\n    api_id = api_id + 1\n    whole_html = whole_html + api_info_html', 'for (api_info_0, api_info_1, api_info_2, api_info_3, api_info_4, api_info_5, api_info_6, *api_info_len) in api_infos:\n    api_info_html = \'<div id=""api_{api_id}"" class=""modal fade"" role=""dialog"">\\n            \\t<div class=""modal-dialog"">\\n            \\t\\t<div class=""modal-content"">\\n            \\t\\t\\t<div class=""modal-header"">\\n            \\t\\t\\t\\t<h6 class=""modal-title"">%s...</h4>\\n            \\t\\t\\t</div>\\n            \\t\\t\\t<div class=""modal-body"">\\n            \\t\\t\\t\\t<div class=""box-body no-padding"">\\n            \\t\\t\\t\\t\\t<p>\\n            \\t\\t\\t\\t\\t\\t%sapi_name}<br><br>\\n                        %sapi_type}<br><br>\\n                        %s <a href=""{api_path}"">{api_path}</a><br><br>\\n                        %s {api_js}<br><br>\\n                        %s <code>{api_res}</code><br>\\n            \\t\\t\\t\\t\\t</p>\\n            \\t\\t\\t\\t</div>\\n            \\t\\t\\t</div>\\n            \\t\\t</div>\\n            \\t</div>\\n            </div>\' % (Utils().getMyWord(\'{api_detail}\'), Utils().getMyWord(\'{api_name}\'), Utils().getMyWord(\'{r_type}\'), Utils().getMyWord(\'{r_api_addr}\'), Utils().getMyWord(\'{r_api_js}\'), Utils().getMyWord(\'{r_api_res}\'))\n    sql = ""select path from js_file where id=\'%s\'"" % \n    api_info_6\n    cursor.execute(sql)\n    js_path = cursor.fetchone()\n    if \n    api_info_5 == 2:\n        api_type = Utils().getMyWord(\'{r_post}\')\n    else:\n        api_type = Utils().getMyWord(\'{r_get}\')\n    if \n    api_info_4 == None:\n        api_res = \'\'\n    else:\n        api_res = \n        api_info_4\n    api_info_html = api_info_html.replace(\'{api_id}\', str(api_id))\n    api_info_html = api_info_html.replace(\'{api_type}\', str(api_type))\n    api_info_html = api_info_html.replace(\'{api_name}\', \n    api_info_2)\n    api_info_html = api_info_html.replace(\'{api_path}\', \n    api_info_1)\n    api_info_html = api_info_html.replace(\'{api_js}\', js_path[0])\n    api_info_html = api_info_html.replace(\'{api_res}\', api_res)\n    api_id = api_id + 1\n    whole_html = whole_html + api_info_html']",no_found,0
mimicry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mimicry/tests/modules/test_resblocks.py,https://github.com/kwotsin/mimicry/tree/master/tests/modules/test_resblocks.py,TestResBlocks,test_GBlock$12,"def test_GBlock(self):
    num_classes_list = [0, 10]
    spectral_norm_list = [True, False]
    in_channels = 3
    out_channels = 8
    args_comb = product(num_classes_list, spectral_norm_list)
    for args in args_comb:
        num_classes = args[0]
        spectral_norm = args[1]
        if num_classes > 0:
            y = torch.ones((4,), dtype=torch.int64)
        else:
            y = None
        gen_block_up = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=True, num_classes=num_classes, spectral_norm=spectral_norm)
        gen_block = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)
        gen_block_no_sc = resblocks.GBlock(in_channels=in_channels, out_channels=in_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)
        assert gen_block_up(self.images, y).shape == (4, 8, 32, 32)
        assert gen_block(self.images, y).shape == (4, 8, 16, 16)
        assert gen_block_no_sc(self.images, y).shape == (4, 3, 16, 16)","for args in args_comb:
    num_classes = args[0]
    spectral_norm = args[1]
    if num_classes > 0:
        y = torch.ones((4,), dtype=torch.int64)
    else:
        y = None
    gen_block_up = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=True, num_classes=num_classes, spectral_norm=spectral_norm)
    gen_block = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)
    gen_block_no_sc = resblocks.GBlock(in_channels=in_channels, out_channels=in_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)
    assert gen_block_up(self.images, y).shape == (4, 8, 32, 32)
    assert gen_block(self.images, y).shape == (4, 8, 16, 16)
    assert gen_block_no_sc(self.images, y).shape == (4, 3, 16, 16)","['for args in args_comb:\n    (args_0, args_1, *_) = args\n    num_classes = args_0\n    spectral_norm = args_1\n    if num_classes > 0:\n        y = torch.ones((4,), dtype=torch.int64)\n    else:\n        y = None\n    gen_block_up = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=True, num_classes=num_classes, spectral_norm=spectral_norm)\n    gen_block = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)\n    gen_block_no_sc = resblocks.GBlock(in_channels=in_channels, out_channels=in_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)\n    assert gen_block_up(self.images, y).shape == (4, 8, 32, 32)\n    assert gen_block(self.images, y).shape == (4, 8, 16, 16)\n    assert gen_block_no_sc(self.images, y).shape == (4, 3, 16, 16)', 'for (args_0, args_1, *args_len) in args_comb:\n    num_classes = \n    args_0\n    spectral_norm = \n    args_1\n    if num_classes > 0:\n        y = torch.ones((4,), dtype=torch.int64)\n    else:\n        y = None\n    gen_block_up = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=True, num_classes=num_classes, spectral_norm=spectral_norm)\n    gen_block = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)\n    gen_block_no_sc = resblocks.GBlock(in_channels=in_channels, out_channels=in_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)\n    assert gen_block_up(self.images, y).shape == (4, 8, 32, 32)\n    assert gen_block(self.images, y).shape == (4, 8, 16, 16)\n    assert gen_block_no_sc(self.images, y).shape == (4, 3, 16, 16)']",no_found,0
sfepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sfepy/sfepy/terms/terms.py,https://github.com/sfepy/sfepy/tree/master/sfepy/terms/terms.py,Term,get_material_names$676,"def get_material_names(self):
    out = []
    for aux in self.names.material:
        if aux[0] is not None:
            out.append(aux[0])
    return out","for aux in self.names.material:
    if aux[0] is not None:
        out.append(aux[0])","['for aux in self.names.material:\n    (aux_0, *aux_rauxmaining) = aux\n    if aux_0 is not None:\n        out.append(aux_0)', 'for (aux_0, *aux_len) in self.names.material:\n    if \n    aux_0 is not None:\n        out.append(\n        aux_0)']",no_found,0
3DDFA_V2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/3DDFA_V2/FaceBoxes/FaceBoxes.py,https://github.com/cleardusk/3DDFA_V2/tree/master/FaceBoxes/FaceBoxes.py,FaceBoxes,__call__$58,"def __call__(self, img_):
    img_raw = img_.copy()
    scale = 1
    if scale_flag:
        (h, w) = img_raw.shape[:2]
        if h > HEIGHT:
            scale = HEIGHT / h
        if w * scale > WIDTH:
            scale *= WIDTH / (w * scale)
        if scale == 1:
            img_raw_scale = img_raw
        else:
            h_s = int(scale * h)
            w_s = int(scale * w)
            img_raw_scale = cv2.resize(img_raw, dsize=(w_s, h_s))
        img = np.float32(img_raw_scale)
    else:
        img = np.float32(img_raw)
    _t = {'forward_pass': Timer(), 'misc': Timer()}
    (im_height, im_width, _) = img.shape
    scale_bbox = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])
    img -= (104, 117, 123)
    img = img.transpose(2, 0, 1)
    img = torch.from_numpy(img).unsqueeze(0)
    _t['forward_pass'].tic()
    (loc, conf) = self.net(img)
    _t['forward_pass'].toc()
    _t['misc'].tic()
    priorbox = PriorBox(image_size=(im_height, im_width))
    priors = priorbox.forward()
    prior_data = priors.data
    boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])
    if scale_flag:
        boxes = boxes * scale_bbox / scale / resize
    else:
        boxes = boxes * scale_bbox / resize
    boxes = boxes.cpu().numpy()
    scores = conf.squeeze(0).data.cpu().numpy()[:, 1]
    inds = np.where(scores > confidence_threshold)[0]
    boxes = boxes[inds]
    scores = scores[inds]
    order = scores.argsort()[::-1][:top_k]
    boxes = boxes[order]
    scores = scores[order]
    dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)
    keep = nms(dets, nms_threshold)
    dets = dets[keep, :]
    dets = dets[:keep_top_k, :]
    _t['misc'].toc()
    if self.timer_flag:
        print('Detection: {:d}/{:d} forward_pass_time: {:.4f}s misc: {:.4f}s'.format(1, 1, _t['forward_pass'].average_time, _t['misc'].average_time))
    det_bboxes = []
    for b in dets:
        if b[4] > vis_thres:
            (xmin, ymin, xmax, ymax, score) = (b[0], b[1], b[2], b[3], b[4])
            bbox = [xmin, ymin, xmax, ymax, score]
            det_bboxes.append(bbox)
    return det_bboxes","for b in dets:
    if b[4] > vis_thres:
        (xmin, ymin, xmax, ymax, score) = (b[0], b[1], b[2], b[3], b[4])
        bbox = [xmin, ymin, xmax, ymax, score]
        det_bboxes.append(bbox)","['for b in dets:\n    (b_0, b_1, b_2, b_3, b_4, *_) = b\n    if b_4 > vis_thres:\n        (xmin, ymin, xmax, ymax, score) = (b_0, b_1, b_2, b_3, b_4)\n        bbox = [xmin, ymin, xmax, ymax, score]\n        det_bboxes.append(bbox)', 'for (b_0, b_1, b_2, b_3, b_4, *b_len) in dets:\n    if \n    b_4 > vis_thres:\n        (xmin, ymin, xmax, ymax, score) = (\n        b_0, \n        b_1, \n        b_2, \n        b_3, \n        b_4)\n        bbox = [xmin, ymin, xmax, ymax, score]\n        det_bboxes.append(bbox)']",no_found,0
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/scripts/detection/ssd/train_ssd.py,https://github.com/dmlc/gluon-cv/tree/master/scripts/detection/ssd/train_ssd.py,,validate$223,"def validate(net, val_data, ctx, eval_metric):
    """"""Test on validation dataset.""""""
    eval_metric.reset()
    net.set_nms(nms_thresh=0.45, nms_topk=400)
    net.hybridize(static_alloc=True, static_shape=True)
    for batch in val_data:
        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)
        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)
        det_bboxes = []
        det_ids = []
        det_scores = []
        gt_bboxes = []
        gt_ids = []
        gt_difficults = []
        for (x, y) in zip(data, label):
            (ids, scores, bboxes) = net(x)
            det_ids.append(ids)
            det_scores.append(scores)
            det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
            gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
            gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
            gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)
        eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)
    return eval_metric.get()","for batch in val_data:
    data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)
    label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)
    det_bboxes = []
    det_ids = []
    det_scores = []
    gt_bboxes = []
    gt_ids = []
    gt_difficults = []
    for (x, y) in zip(data, label):
        (ids, scores, bboxes) = net(x)
        det_ids.append(ids)
        det_scores.append(scores)
        det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)
    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)","['for batch in val_data:\n    (batch_0, batch_1, *_) = batch\n    data = gluon.utils.split_and_load(batch_0, ctx_list=ctx, batch_axis=0, even_split=False)\n    label = gluon.utils.split_and_load(batch_1, ctx_list=ctx, batch_axis=0, even_split=False)\n    det_bboxes = []\n    det_ids = []\n    det_scores = []\n    gt_bboxes = []\n    gt_ids = []\n    gt_difficults = []\n    for (x, y) in zip(data, label):\n        (ids, scores, bboxes) = net(x)\n        det_ids.append(ids)\n        det_scores.append(scores)\n        det_bboxes.append(bboxes.clip(0, batch_0.shape[2]))\n        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))\n        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))\n        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)\n    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)', 'for (batch_0, batch_1, *batch_len) in val_data:\n    data = gluon.utils.split_and_load(batch_0, ctx_list=ctx, batch_axis=0, even_split=False)\n    label = gluon.utils.split_and_load(batch_1, ctx_list=ctx, batch_axis=0, even_split=False)\n    det_bboxes = []\n    det_ids = []\n    det_scores = []\n    gt_bboxes = []\n    gt_ids = []\n    gt_difficults = []\n    for (x, y) in zip(data, label):\n        (ids, scores, bboxes) = net(x)\n        det_ids.append(ids)\n        det_scores.append(scores)\n        det_bboxes.append(bboxes.clip(0, batch_0.shape[2]))\n        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))\n        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))\n        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)\n    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)']",no_found,0
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/gluoncv/auto/estimators/yolo/yolo.py,https://github.com/dmlc/gluon-cv/tree/master/gluoncv/auto/estimators/yolo/yolo.py,YOLOv3Estimator,_evaluate$241,"def _evaluate(self, val_data):
    """"""Evaluate the current model on dataset.""""""
    if not isinstance(val_data, gluon.data.DataLoader):
        if hasattr(val_data, 'to_mxnet'):
            val_data = val_data.to_mxnet()
        val_batchify_fn = Tuple(Stack(), Pad(pad_val=-1))
        val_data = gluon.data.DataLoader(val_data.transform(YOLO3DefaultValTransform(self._cfg.yolo3.data_shape, self._cfg.yolo3.data_shape)), self._cfg.valid.batch_size, False, batchify_fn=val_batchify_fn, last_batch='keep', num_workers=self._cfg.num_workers)
    if self._cfg.valid.metric == 'voc07':
        eval_metric = VOC07MApMetric(iou_thresh=self._cfg.valid.iou_thresh, class_names=self.classes)
    elif self._cfg.valid.metric == 'voc':
        eval_metric = VOCMApMetric(iou_thresh=self._cfg.valid.iou_thresh, class_names=self.classes)
    else:
        raise ValueError(f'Invalid metric type: {self._cfg.valid.metric}')
    self.net.collect_params().reset_ctx(self.ctx)
    self.net.set_nms(nms_thresh=self._cfg.yolo3.nms_thresh, nms_topk=self._cfg.yolo3.nms_topk)
    mx.nd.waitall()
    self.net.hybridize()
    for batch in val_data:
        val_ctx = self.ctx
        if batch[0].shape[0] < len(val_ctx):
            val_ctx = val_ctx[:batch[0].shape[0]]
        data = gluon.utils.split_and_load(batch[0], ctx_list=val_ctx, batch_axis=0, even_split=False)
        label = gluon.utils.split_and_load(batch[1], ctx_list=val_ctx, batch_axis=0, even_split=False)
        det_bboxes = []
        det_ids = []
        det_scores = []
        gt_bboxes = []
        gt_ids = []
        gt_difficults = []
        for (x, y) in zip(data, label):
            (ids, scores, bboxes) = self.net(x)
            det_ids.append(ids)
            det_scores.append(scores)
            det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
            gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
            gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
            gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)
        eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)
    return eval_metric.get()","for batch in val_data:
    val_ctx = self.ctx
    if batch[0].shape[0] < len(val_ctx):
        val_ctx = val_ctx[:batch[0].shape[0]]
    data = gluon.utils.split_and_load(batch[0], ctx_list=val_ctx, batch_axis=0, even_split=False)
    label = gluon.utils.split_and_load(batch[1], ctx_list=val_ctx, batch_axis=0, even_split=False)
    det_bboxes = []
    det_ids = []
    det_scores = []
    gt_bboxes = []
    gt_ids = []
    gt_difficults = []
    for (x, y) in zip(data, label):
        (ids, scores, bboxes) = self.net(x)
        det_ids.append(ids)
        det_scores.append(scores)
        det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)
    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)","['for batch in val_data:\n    (batch_0, batch_1, *_) = batch\n    val_ctx = self.ctx\n    if batch_0.shape[0] < len(val_ctx):\n        val_ctx = val_ctx[:batch_0.shape[0]]\n    data = gluon.utils.split_and_load(batch_0, ctx_list=val_ctx, batch_axis=0, even_split=False)\n    label = gluon.utils.split_and_load(batch_1, ctx_list=val_ctx, batch_axis=0, even_split=False)\n    det_bboxes = []\n    det_ids = []\n    det_scores = []\n    gt_bboxes = []\n    gt_ids = []\n    gt_difficults = []\n    for (x, y) in zip(data, label):\n        (ids, scores, bboxes) = self.net(x)\n        det_ids.append(ids)\n        det_scores.append(scores)\n        det_bboxes.append(bboxes.clip(0, batch_0.shape[2]))\n        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))\n        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))\n        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)\n    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)', 'for (batch_0, batch_1, *batch_len) in val_data:\n    val_ctx = self.ctx\n    if \n    batch_0.shape[0] < len(val_ctx):\n        val_ctx = val_ctx[:\n        batch_0.shape[0]]\n    data = gluon.utils.split_and_load(\n    batch_0, ctx_list=val_ctx, batch_axis=0, even_split=False)\n    label = gluon.utils.split_and_load(\n    batch_1, ctx_list=val_ctx, batch_axis=0, even_split=False)\n    det_bboxes = []\n    det_ids = []\n    det_scores = []\n    gt_bboxes = []\n    gt_ids = []\n    gt_difficults = []\n    for (x, y) in zip(data, label):\n        (ids, scores, bboxes) = self.net(x)\n        det_ids.append(ids)\n        det_scores.append(scores)\n        det_bboxes.append(bboxes.clip(0, \n        batch_0.shape[2]))\n        gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))\n        gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))\n        gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)\n    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)']",no_found,0
ShuiZe_0x727,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ShuiZe_0x727/Plugins/infoGather/subdomain/Spider/Baidu/baidu.py,https://github.com/0x727/ShuiZe_0x727/tree/master/Plugins/infoGather/subdomain/Spider/Baidu/baidu.py,BaiduSpider,get_subdomain$41,"def get_subdomain(self, each_wd, i):
    for page in range(1, self.PAGES + 1):
        wd = 'site:{} {}'.format(self.domain, each_wd)
        print('[{}] -> [page: {}]'.format(wd, page))
        wd = quote(wd)
        bd_link_titles = self.keyword(wd=wd, page=page)
        if bd_link_titles:
            for bd_link_title in bd_link_titles:
                (title, link) = (bd_link_title[0], bd_link_title[1])
                subdomain = self.location(each_wd, link, title)
                self.bdSubdomains.append(urlparse(subdomain).netloc)","for bd_link_title in bd_link_titles:
    (title, link) = (bd_link_title[0], bd_link_title[1])
    subdomain = self.location(each_wd, link, title)
    self.bdSubdomains.append(urlparse(subdomain).netloc)","['for bd_link_title in bd_link_titles:\n    (bd_link_title_0, bd_link_title_1, *_) = bd_link_title\n    (title, link) = (bd_link_title_0, bd_link_title_1)\n    subdomain = self.location(each_wd, link, title)\n    self.bdSubdomains.append(urlparse(subdomain).netloc)', 'for (bd_link_title_0, bd_link_title_1, *bd_link_title_len) in bd_link_titles:\n    (title, link) = (bd_link_title_0, bd_link_title_1)\n    subdomain = self.location(each_wd, link, title)\n    self.bdSubdomains.append(urlparse(subdomain).netloc)']",no_found,0
ParlAI,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ParlAI/parlai/core/torch_generator_agent.py,https://github.com/facebookresearch/ParlAI/tree/master/parlai/core/torch_generator_agent.py,TorchGeneratorAgent,eval_step$894,"def eval_step(self, batch):
    """"""
        Evaluate a single batch of examples.
        """"""
    if batch.text_vec is None and batch.image is None:
        return
    if batch.text_vec is not None:
        bsz = batch.text_vec.size(0)
    else:
        bsz = len(batch.image)
    self.model.eval()
    cand_scores = None
    token_losses = None
    text_token_info = None
    if batch.label_vec is not None:
        (loss, model_output) = self.compute_loss(batch, return_output=True)
        if self.show_token_details:
            token_losses = self._construct_label_token_losses(batch.label_vec, model_output)
    beam_preds_scores = None
    preds = None
    if self.skip_generation:
        warn_once('--skip-generation true produces limited metrics')
    else:
        maxlen = self.label_truncate or 256
        prefix_tokens = self.get_prefix_tokens(batch)
        (beam_preds_scores, beams) = self._generate(batch, self.beam_size, maxlen, prefix_tokens=prefix_tokens)
        (preds, _, _) = zip(*beam_preds_scores)
        self._add_generation_metrics(batch, preds)
        beam_texts: List[List[Tuple[str, float]]] = []
        beam_texts_token_info: List[List[List[Tuple]]] = []
        for beam in beams:
            beam_texts.append([])
            if self.show_token_details:
                beam_texts_token_info.append([])
            for (tokens, score, token_metadata) in beam.get_rescored_finished():
                try:
                    if self.show_token_details:
                        beam_texts_token_info[-1].append(self._construct_generated_token_details(tokens, token_metadata))
                    beam_texts[-1].append((self._v2t(tokens), score.item()))
                except KeyError:
                    logging.error('Decoding error: %s', tokens)
                    continue
    cand_choices = None
    cand_scores = None
    if self.rank_candidates:
        (cand_choices, cand_scores) = self.rank_eval_label_candidates(batch, bsz)
    text = [self._v2t(pred_data[0]) for pred_data in beam_preds_scores] if beam_preds_scores is not None else None
    if self.show_token_details and beam_preds_scores is not None:
        text_token_info = []
        for beam_text_token_info in beam_texts_token_info:
            text_token_info.append(beam_text_token_info[0])
    if text and self.compute_tokenized_bleu:
        self._compute_fairseq_bleu(batch, preds)
    retval = Output(text, cand_choices, token_losses=token_losses, cand_scores=cand_scores)
    if not self.skip_generation:
        retval.beam_texts = beam_texts
        retval.beam_texts_token_info = beam_texts_token_info
        retval.text_token_info = text_token_info
    return retval","for beam_text_token_info in beam_texts_token_info:
    text_token_info.append(beam_text_token_info[0])","['for beam_text_token_info in beam_texts_token_info:\n    (beam_text_token_info_0, *beam_text_token_info_rbeam_text_token_infomaining) = beam_text_token_info\n    text_token_info.append(beam_text_token_info_0)', 'for (beam_text_token_info_0, *beam_text_token_info_len) in beam_texts_token_info:\n    text_token_info.append(beam_text_token_info_0)']",no_found,0
parliament,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/parliament/parliament/statement.py,https://github.com/duo-labs/parliament/tree/master/parliament/statement.py,Statement,_check_condition$496,"def _check_condition(self, operator, condition_block, expanded_actions):
    """"""
        operator is something like ""StringLike""
        condition_block is something like {""s3:prefix"":[""home/${aws:username}/*""]}
        """"""
    operator_type_requirement = None
    for documented_operator in OPERATORS:
        op = documented_operator.lower()
        if operator.lower() in [op, op + 'ifexists', 'forallvalues:' + op, 'foranyvalue:' + op, 'forallvalues:' + op + 'ifexists', 'foranyvalue:' + op + 'ifexists']:
            operator_type_requirement = OPERATORS[documented_operator]
            break
    if operator_type_requirement is None:
        self.add_finding('UNKNOWN_OPERATOR', detail=operator, location=condition_block)
    if operator_type_requirement == 'Bool':
        for c in condition_block:
            value = str(c[1].value).lower()
            if value != 'true' and value != 'false':
                self.add_finding('MISMATCHED_TYPE_OPERATION_TO_NULL', location=condition_block)
                return False
    for block in condition_block:
        key = block[0]
        values = []
        for v in make_list(block[1]):
            values.append(v.value)
        if operator.lower() == 'bool':
            if key.lower() == 'aws:MultiFactorAuthPresent'.lower() and 'false' in values:
                self.add_finding('BAD_PATTERN_FOR_MFA', detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.', location=condition_block)
        elif operator.lower() == 'null':
            if key.lower == 'aws:MultiFactorAuthPresent'.lower() and 'false' in values:
                self.add_finding('BAD_PATTERN_FOR_MFA', detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.', location=condition_block)
        if operator.lower() in ['null']:
            continue
        condition_type = get_global_key_type(key)
        if condition_type:
            if not is_value_in_correct_format_for_type(condition_type, values):
                self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)
        else:
            for action_struct in expanded_actions:
                privilege_info = get_privilege_info(action_struct['service'], action_struct['action'])
                match = None
                for resource_type in privilege_info['resource_types']:
                    for condition_key in resource_type['condition_keys']:
                        if is_condition_key_match(condition_key, key):
                            match = condition_key
                if match is None:
                    self.add_finding('UNKNOWN_CONDITION_FOR_ACTION', detail='Unknown condition {} for action {}:{}'.format(key, action_struct['service'], action_struct['action']), location=condition_block)
                    continue
                condition_type = None
                for condition in privilege_info['service_conditions']:
                    if condition['condition'] == match:
                        condition_type = condition['type']
                if condition_type is None:
                    raise Exception('Action condition not found in service definition for {}'.format(match))
                if not is_value_in_correct_format_for_type(condition_type, values):
                    self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)
            if condition_type is not None:
                documenation_condition_type = translate_documentation_types(condition_type)
                if operator_type_requirement != documenation_condition_type:
                    if operator_type_requirement == 'String' and documenation_condition_type == 'Arn':
                        self.add_finding('MISMATCHED_TYPE_BUT_USABLE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)
                    else:
                        self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)
    return","for block in condition_block:
    key = block[0]
    values = []
    for v in make_list(block[1]):
        values.append(v.value)
    if operator.lower() == 'bool':
        if key.lower() == 'aws:MultiFactorAuthPresent'.lower() and 'false' in values:
            self.add_finding('BAD_PATTERN_FOR_MFA', detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.', location=condition_block)
    elif operator.lower() == 'null':
        if key.lower == 'aws:MultiFactorAuthPresent'.lower() and 'false' in values:
            self.add_finding('BAD_PATTERN_FOR_MFA', detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.', location=condition_block)
    if operator.lower() in ['null']:
        continue
    condition_type = get_global_key_type(key)
    if condition_type:
        if not is_value_in_correct_format_for_type(condition_type, values):
            self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)
    else:
        for action_struct in expanded_actions:
            privilege_info = get_privilege_info(action_struct['service'], action_struct['action'])
            match = None
            for resource_type in privilege_info['resource_types']:
                for condition_key in resource_type['condition_keys']:
                    if is_condition_key_match(condition_key, key):
                        match = condition_key
            if match is None:
                self.add_finding('UNKNOWN_CONDITION_FOR_ACTION', detail='Unknown condition {} for action {}:{}'.format(key, action_struct['service'], action_struct['action']), location=condition_block)
                continue
            condition_type = None
            for condition in privilege_info['service_conditions']:
                if condition['condition'] == match:
                    condition_type = condition['type']
            if condition_type is None:
                raise Exception('Action condition not found in service definition for {}'.format(match))
            if not is_value_in_correct_format_for_type(condition_type, values):
                self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)
        if condition_type is not None:
            documenation_condition_type = translate_documentation_types(condition_type)
            if operator_type_requirement != documenation_condition_type:
                if operator_type_requirement == 'String' and documenation_condition_type == 'Arn':
                    self.add_finding('MISMATCHED_TYPE_BUT_USABLE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)
                else:
                    self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)","['for block in condition_block:\n    (block_0, block_1, *_) = block\n    key = block_0\n    values = []\n    for v in make_list(block_1):\n        values.append(v.value)\n    if operator.lower() == \'bool\':\n        if key.lower() == \'aws:MultiFactorAuthPresent\'.lower() and \'false\' in values:\n            self.add_finding(\'BAD_PATTERN_FOR_MFA\', detail=\'The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.\', location=condition_block)\n    elif operator.lower() == \'null\':\n        if key.lower == \'aws:MultiFactorAuthPresent\'.lower() and \'false\' in values:\n            self.add_finding(\'BAD_PATTERN_FOR_MFA\', detail=\'The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.\', location=condition_block)\n    if operator.lower() in [\'null\']:\n        continue\n    condition_type = get_global_key_type(key)\n    if condition_type:\n        if not is_value_in_correct_format_for_type(condition_type, values):\n            self.add_finding(\'MISMATCHED_TYPE\', detail=\'Type mismatch: {} requires a value of type {} but given {}\'.format(key, condition_type, values), location=condition_block)\n    else:\n        for action_struct in expanded_actions:\n            privilege_info = get_privilege_info(action_struct[\'service\'], action_struct[\'action\'])\n            match = None\n            for resource_type in privilege_info[\'resource_types\']:\n                for condition_key in resource_type[\'condition_keys\']:\n                    if is_condition_key_match(condition_key, key):\n                        match = condition_key\n            if match is None:\n                self.add_finding(\'UNKNOWN_CONDITION_FOR_ACTION\', detail=\'Unknown condition {} for action {}:{}\'.format(key, action_struct[\'service\'], action_struct[\'action\']), location=condition_block)\n                continue\n            condition_type = None\n            for condition in privilege_info[\'service_conditions\']:\n                if condition[\'condition\'] == match:\n                    condition_type = condition[\'type\']\n            if condition_type is None:\n                raise Exception(\'Action condition not found in service definition for {}\'.format(match))\n            if not is_value_in_correct_format_for_type(condition_type, values):\n                self.add_finding(\'MISMATCHED_TYPE\', detail=\'Type mismatch: {} requires a value of type {} but given {}\'.format(key, condition_type, values), location=condition_block)\n        if condition_type is not None:\n            documenation_condition_type = translate_documentation_types(condition_type)\n            if operator_type_requirement != documenation_condition_type:\n                if operator_type_requirement == \'String\' and documenation_condition_type == \'Arn\':\n                    self.add_finding(\'MISMATCHED_TYPE_BUT_USABLE\', detail=\'Type mismatch: {} requires a value of type {} but given {}\'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)\n                else:\n                    self.add_finding(\'MISMATCHED_TYPE\', detail=\'Type mismatch: {} requires a value of type {} but given {}\'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)', 'for (block_0, block_1, *block_len) in condition_block:\n    key = \n    block_0\n    values = []\n    for v in make_list(\n    block_1):\n        values.append(v.value)\n    if operator.lower() == \'bool\':\n        if key.lower() == \'aws:MultiFactorAuthPresent\'.lower() and \'false\' in values:\n            self.add_finding(\'BAD_PATTERN_FOR_MFA\', detail=\'The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.\', location=condition_block)\n    elif operator.lower() == \'null\':\n        if key.lower == \'aws:MultiFactorAuthPresent\'.lower() and \'false\' in values:\n            self.add_finding(\'BAD_PATTERN_FOR_MFA\', detail=\'The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.\', location=condition_block)\n    if operator.lower() in [\'null\']:\n        continue\n    condition_type = get_global_key_type(key)\n    if condition_type:\n        if not is_value_in_correct_format_for_type(condition_type, values):\n            self.add_finding(\'MISMATCHED_TYPE\', detail=\'Type mismatch: {} requires a value of type {} but given {}\'.format(key, condition_type, values), location=condition_block)\n    else:\n        for action_struct in expanded_actions:\n            privilege_info = get_privilege_info(action_struct[\'service\'], action_struct[\'action\'])\n            match = None\n            for resource_type in privilege_info[\'resource_types\']:\n                for condition_key in resource_type[\'condition_keys\']:\n                    if is_condition_key_match(condition_key, key):\n                        match = condition_key\n            if match is None:\n                self.add_finding(\'UNKNOWN_CONDITION_FOR_ACTION\', detail=\'Unknown condition {} for action {}:{}\'.format(key, action_struct[\'service\'], action_struct[\'action\']), location=condition_block)\n                continue\n            condition_type = None\n            for condition in privilege_info[\'service_conditions\']:\n                if condition[\'condition\'] == match:\n                    condition_type = condition[\'type\']\n            if condition_type is None:\n                raise Exception(\'Action condition not found in service definition for {}\'.format(match))\n            if not is_value_in_correct_format_for_type(condition_type, values):\n                self.add_finding(\'MISMATCHED_TYPE\', detail=\'Type mismatch: {} requires a value of type {} but given {}\'.format(key, condition_type, values), location=condition_block)\n        if condition_type is not None:\n            documenation_condition_type = translate_documentation_types(condition_type)\n            if operator_type_requirement != documenation_condition_type:\n                if operator_type_requirement == \'String\' and documenation_condition_type == \'Arn\':\n                    self.add_finding(\'MISMATCHED_TYPE_BUT_USABLE\', detail=\'Type mismatch: {} requires a value of type {} but given {}\'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)\n                else:\n                    self.add_finding(\'MISMATCHED_TYPE\', detail=\'Type mismatch: {} requires a value of type {} but given {}\'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)']",no_found,0
PythonStdioGames,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonStdioGames/src/gamesbyexample/pygame_games/memorypuzzle.py,https://github.com/asweigart/PythonStdioGames/tree/master/src/gamesbyexample/pygame_games/memorypuzzle.py,,drawBoxCovers$209,"def drawBoxCovers(board, boxes, coverage):
    for box in boxes:
        (left, top) = leftTopCoordsOfBox(box[0], box[1])
        pygame.draw.rect(DISPLAYSURF, BGCOLOR, (left, top, BOXSIZE, BOXSIZE))
        (shape, color) = getShapeAndColor(board, box[0], box[1])
        drawIcon(shape, color, box[0], box[1])
        if coverage > 0:
            pygame.draw.rect(DISPLAYSURF, BOXCOLOR, (left, top, coverage, BOXSIZE))
    pygame.display.update()
    FPSCLOCK.tick(FPS)","for box in boxes:
    (left, top) = leftTopCoordsOfBox(box[0], box[1])
    pygame.draw.rect(DISPLAYSURF, BGCOLOR, (left, top, BOXSIZE, BOXSIZE))
    (shape, color) = getShapeAndColor(board, box[0], box[1])
    drawIcon(shape, color, box[0], box[1])
    if coverage > 0:
        pygame.draw.rect(DISPLAYSURF, BOXCOLOR, (left, top, coverage, BOXSIZE))","['for box in boxes:\n    (box_0, box_1, *_) = box\n    (left, top) = leftTopCoordsOfBox(box_0, box_1)\n    pygame.draw.rect(DISPLAYSURF, BGCOLOR, (left, top, BOXSIZE, BOXSIZE))\n    (shape, color) = getShapeAndColor(board, box_0, box_1)\n    drawIcon(shape, color, box_0, box_1)\n    if coverage > 0:\n        pygame.draw.rect(DISPLAYSURF, BOXCOLOR, (left, top, coverage, BOXSIZE))', 'for (box_0, box_1, *box_len) in boxes:\n    (left, top) = leftTopCoordsOfBox(box_0, box_1)\n    pygame.draw.rect(DISPLAYSURF, BGCOLOR, (left, top, BOXSIZE, BOXSIZE))\n    (shape, color) = getShapeAndColor(board, box_0, box_1)\n    drawIcon(shape, color, box_0, box_1)\n    if coverage > 0:\n        pygame.draw.rect(DISPLAYSURF, BOXCOLOR, (left, top, coverage, BOXSIZE))']",no_found,0
centerNet-deep-sort,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/centerNet-deep-sort/CenterNet/src/tools/calc_coco_overlap.py,https://github.com/kimyoon-young/centerNet-deep-sort/tree/master/CenterNet/src/tools/calc_coco_overlap.py,,count_agnostic$117,"def count_agnostic(split):
    coco = COCO.COCO(ANN_PATH + ANN_FILES[split])
    images = coco.getImgIds()
    cnt = 0
    for img_id in images:
        ann_ids = coco.getAnnIds(imgIds=[img_id])
        anns = coco.loadAnns(ids=ann_ids)
        centers = []
        for ann in anns:
            bbox = ann['bbox']
            center = ((bbox[0] + bbox[2] / 2) // 4, (bbox[1] + bbox[3] / 2) // 4)
            for c in centers:
                if center[0] == c[0] and center[1] == c[1]:
                    cnt += 1
            centers.append(center)
    print('find {} collisions!'.format(cnt))","for c in centers:
    if center[0] == c[0] and center[1] == c[1]:
        cnt += 1","['for c in centers:\n    (c_0, c_1, *_) = c\n    if center[0] == c_0 and center[1] == c_1:\n        cnt += 1', 'for (c_0, c_1, *c_len) in centers:\n    if center[0] == \n    c_0 and center[1] == \n    c_1:\n        cnt += 1']",no_found,0
veusz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/veusz/veusz/windows/mainwindow.py,https://github.com/veusz/veusz/tree/master/veusz/windows/mainwindow.py,MainWindow,definePlugins$378,"def definePlugins(self, pluginlist, actions, menuname):
    """"""Create menu items and actions for plugins.

        pluginlist: list of plugin classes
        actions: dict of actions to add new actions to
        menuname: string giving prefix for new menu entries (inside actions)
        """"""

    def getLoadDialog(pluginkls):

        def _loadPlugin():
            from ..dialogs.plugin import handlePlugin
            handlePlugin(self, self.document, pluginkls)
        return _loadPlugin
    menu = []
    for pluginkls in pluginlist:
        actname = menuname + '.' + '.'.join(pluginkls.menu)
        text = pluginkls.menu[-1]
        if pluginkls.has_parameters:
            text += '...'
        actions[actname] = utils.makeAction(self, pluginkls.description_short, text, getLoadDialog(pluginkls))
        menulook = menu
        namebuild = [menuname]
        for cmpt in pluginkls.menu[:-1]:
            namebuild.append(cmpt)
            name = '.'.join(namebuild)
            for c in menulook:
                if c[0] == name:
                    menulook = c[2]
                    break
            else:
                menulook.append([name, cmpt, []])
                menulook = menulook[-1][2]
        menulook.append(actname)
    return menu","for c in menulook:
    if c[0] == name:
        menulook = c[2]
        break
else:
    menulook.append([name, cmpt, []])
    menulook = menulook[-1][2]","['for c in menulook:\n    (c_0, _, c_2, *c_rcmaining) = c\n    if c_0 == name:\n        menulook = c_2\n        break\nelse:\n    menulook.append([name, cmpt, []])\n    menulook = menulook[-1][2]', 'for (c_0, c_1, c_2, *c_len) in menulook:\n    if \n    c_0 == name:\n        menulook = \n        c_2\n        break\nelse:\n    menulook.append([name, cmpt, []])\n    menulook = menulook[-1][2]']",no_found,0
pygame_tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygame_tutorials/examples/pathfinding/mob_graph_test_navmesh.py,https://github.com/kidscancode/pygame_tutorials/tree/master/examples/pathfinding/mob_graph_test_navmesh.py,WeightedMesh,draw$121,"def draw(self):
    for node in self.edges.keys():
        x = int(node[0] * TILESIZE + TILESIZE / 2)
        y = int(node[1] * TILESIZE + TILESIZE / 2)
        pg.draw.circle(screen, CYAN, (x, y), 10)
        for c in self.edges[node]:
            cx = c[0] * TILESIZE + TILESIZE / 2
            cy = c[1] * TILESIZE + TILESIZE / 2
            pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)","for c in self.edges[node]:
    cx = c[0] * TILESIZE + TILESIZE / 2
    cy = c[1] * TILESIZE + TILESIZE / 2
    pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)","['for c in self.edges[node]:\n    (c_0, c_1, *_) = c\n    cx = c_0 * TILESIZE + TILESIZE / 2\n    cy = c_1 * TILESIZE + TILESIZE / 2\n    pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)', 'for (c_0, c_1, *c_len) in self.edges[node]:\n    cx = \n    c_0 * TILESIZE + TILESIZE / 2\n    cy = \n    c_1 * TILESIZE + TILESIZE / 2\n    pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)']",no_found,0
integrations-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/datadog_checks_dev/tests/tooling/config_validator/test_config_block.py,https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/tests/tooling/config_validator/test_config_block.py,,test_is_comment$216,"def test_is_comment():
    dir = os.path.dirname(__file__)
    test_file = open(os.path.join(dir, 'test_config_block_5.yaml'), 'r')
    config_lines = test_file.read().split('\n')
    test_file.close()
    test_cases = [(0, 0, False), (0, 2, False), (6, 0, True), (6, 2, True), (12, 0, True), (12, 2, True)]
    test_cases_with_errors = [(12, 0, True, 'Comment block incorrectly indented')]
    for c in test_cases:
        status = _is_comment(c[0], config_lines, c[1], [])
        assert status == c[2]
    for c in test_cases_with_errors:
        errors = []
        status = _is_comment(c[0], config_lines, c[1], errors)
        assert status == c[2]
        assert len(errors) == 1
        assert errors[0].error_str == c[3]","for c in test_cases:
    status = _is_comment(c[0], config_lines, c[1], [])
    assert status == c[2]","['for c in test_cases:\n    (c_0, c_1, c_2, *_) = c\n    status = _is_comment(c_0, config_lines, c_1, [])\n    assert status == c_2', 'for (c_0, c_1, c_2, *c_len) in test_cases:\n    status = _is_comment(\n    c_0, config_lines, \n    c_1, [])\n    assert status == \n    c_2']",no_found,0
integrations-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/datadog_checks_dev/tests/tooling/config_validator/test_config_block.py,https://github.com/DataDog/integrations-core/tree/master/datadog_checks_dev/tests/tooling/config_validator/test_config_block.py,,test_is_comment$216,"def test_is_comment():
    dir = os.path.dirname(__file__)
    test_file = open(os.path.join(dir, 'test_config_block_5.yaml'), 'r')
    config_lines = test_file.read().split('\n')
    test_file.close()
    test_cases = [(0, 0, False), (0, 2, False), (6, 0, True), (6, 2, True), (12, 0, True), (12, 2, True)]
    test_cases_with_errors = [(12, 0, True, 'Comment block incorrectly indented')]
    for c in test_cases:
        status = _is_comment(c[0], config_lines, c[1], [])
        assert status == c[2]
    for c in test_cases_with_errors:
        errors = []
        status = _is_comment(c[0], config_lines, c[1], errors)
        assert status == c[2]
        assert len(errors) == 1
        assert errors[0].error_str == c[3]","for c in test_cases_with_errors:
    errors = []
    status = _is_comment(c[0], config_lines, c[1], errors)
    assert status == c[2]
    assert len(errors) == 1
    assert errors[0].error_str == c[3]","['for c in test_cases_with_errors:\n    (c_0, c_1, c_2, c_3, *_) = c\n    errors = []\n    status = _is_comment(c_0, config_lines, c_1, errors)\n    assert status == c_2\n    assert len(errors) == 1\n    assert errors[0].error_str == c_3', 'for (c_0, c_1, c_2, c_3, *c_len) in test_cases_with_errors:\n    errors = []\n    status = _is_comment(\n    c_0, config_lines, \n    c_1, errors)\n    assert status == \n    c_2\n    assert len(errors) == 1\n    assert errors[0].error_str == \n    c_3']",no_found,0
opentelemetry-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/opentelemetry-python/propagator/opentelemetry-propagator-jaeger/tests/test_jaeger_propagator.py,https://github.com/open-telemetry/opentelemetry-python/tree/master/propagator/opentelemetry-propagator-jaeger/tests/test_jaeger_propagator.py,TestJaegerPropagator,test_fields$181,"def test_fields(self):
    tracer = trace.TracerProvider().get_tracer('sdk_tracer_provider')
    mock_setter = Mock()
    with tracer.start_as_current_span('parent'):
        with tracer.start_as_current_span('child'):
            FORMAT.inject({}, setter=mock_setter)
    inject_fields = set()
    for call in mock_setter.mock_calls:
        inject_fields.add(call[1][1])
    self.assertEqual(FORMAT.fields, inject_fields)","for call in mock_setter.mock_calls:
    inject_fields.add(call[1][1])","['for call in mock_setter.mock_calls:\n    (_, call_1, *call_rcallmaining) = call\n    inject_fields.add(call_1[1])', 'for (call_0, (call_1_0, call_1_1, *call_1_len), *call_len) in mock_setter.mock_calls:\n    inject_fields.add(call_1_1)']",no_found,0
integrations-core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/integrations-core/vsphere/tests/legacy/test_vsphere.py,https://github.com/DataDog/integrations-core/tree/master/vsphere/tests/legacy/test_vsphere.py,,test__process_mor_objects_queue$379,"def test__process_mor_objects_queue(vsphere, instance):
    vsphere.log = MagicMock()
    vsphere._process_mor_objects_queue_async = MagicMock()
    vsphere._process_mor_objects_queue(instance)
    vsphere.log.debug.assert_called_once_with('Objects queue is not initialized yet for instance %s, skipping processing', vsphere._instance_key(instance))
    vsphere.batch_morlist_size = 1
    i_key = vsphere._instance_key(instance)
    with mock.patch('datadog_checks.vsphere.legacy.vsphere_legacy.vmodl'):
        vsphere._cache_morlist_raw(instance)
        assert sum((vsphere.mor_objects_queue.size(i_key, res_type) for res_type in RESOURCE_TYPE_METRICS)) == 11
        vsphere._process_mor_objects_queue(instance)
        assert sum((vsphere.mor_objects_queue.size(i_key, res_type) for res_type in RESOURCE_TYPE_METRICS)) == 0
        assert vsphere._process_mor_objects_queue_async.call_count == 0
        for call_args in vsphere._process_mor_objects_queue_async.call_args_list:
            assert len(call_args[0][1]) == 1
        instance['collect_realtime_only'] = False
        vsphere._cache_morlist_raw(instance)
        assert sum((vsphere.mor_objects_queue.size(i_key, res_type) for res_type in RESOURCE_TYPE_METRICS)) == 11
        vsphere._process_mor_objects_queue(instance)
        assert sum((vsphere.mor_objects_queue.size(i_key, res_type) for res_type in RESOURCE_TYPE_METRICS)) == 0
        assert vsphere._process_mor_objects_queue_async.call_count == 5","for call_args in vsphere._process_mor_objects_queue_async.call_args_list:
    assert len(call_args[0][1]) == 1","['for call_args in vsphere._process_mor_objects_queue_async.call_args_list:\n    ((call_args_0_0, call_args_0_1, *call_args_0_rcall_argsmaining), *call_args_rcall_argsmaining) = call_args\n    assert len(call_args_0_1) == 1', 'for ((call_args_0_0, call_args_0_1, *call_args_0_len), *call_args_len) in vsphere._process_mor_objects_queue_async.call_args_list:\n    assert len(call_args_0_1) == 1']",no_found,0
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2020/med_test.py,https://github.com/3b1b/videos/tree/master/_2020/med_test.py,SamplePopulationBreastCancer,construct$820,"def construct(self):
    title = TexText('Sample of ', '$1{,}000$', ' women', font_size=72)
    title.add(Underline(title, color=GREY_B))
    title.to_edge(UP, buff=MED_SMALL_BUFF)
    self.add(title)
    woman = WomanIcon()
    globals()['woman'] = woman
    population = VGroup(*[woman.copy() for x in range(1000)])
    population.arrange_in_grid(25, 40, buff=LARGE_BUFF, fill_rows_first=False)
    population.set_height(6)
    population.next_to(title, DOWN)
    counter = Integer(1000, edge_to_fix=UL)
    counter.replace(title[1])
    counter.set_value(0)
    title[1].set_opacity(0)
    self.play(ShowIncreasingSubsets(population), ChangeDecimalToValue(counter, 1000), run_time=5)
    self.remove(counter)
    title[1].set_opacity(1)
    self.wait()
    rects = VGroup(Rectangle(), Rectangle())
    rects.set_height(6)
    rects[0].set_width(4, stretch=True)
    rects[1].set_width(8, stretch=True)
    rects[0].set_stroke(YELLOW, 3)
    rects[1].set_stroke(GREY, 3)
    rects.arrange(RIGHT)
    rects.center().to_edge(DOWN, buff=MED_SMALL_BUFF)
    positive_cases = population[:10]
    negative_cases = population[10:]
    positive_cases.generate_target()
    positive_cases.target.move_to(rects[0])
    positive_cases.target.set_color(YELLOW)
    negative_cases.generate_target()
    negative_cases.target.set_height(rects[1].get_height() * 0.8)
    negative_cases.target.move_to(rects[1])
    positive_words = TexText('1\\% ', 'Have breast cancer', font_size=36)
    positive_words.set_color(YELLOW)
    positive_words.next_to(rects[0], UP, SMALL_BUFF)
    negative_words = TexText('99\\% ', 'Do not have cancer', font_size=36)
    negative_words.set_color(GREY_B)
    negative_words.next_to(rects[1], UP, SMALL_BUFF)
    self.play(MoveToTarget(positive_cases), MoveToTarget(negative_cases), Write(positive_words, run_time=1), Write(negative_words, run_time=1), FadeIn(rects))
    self.wait()
    scan_lines = VGroup(*(Line(FRAME_HEIGHT * DOWN / 2, icon.get_center(), stroke_width=1, stroke_color=interpolate_color(BLUE, GREEN, random.random())) for icon in population))
    self.play(LaggedStartMap(ShowCreationThenFadeOut, scan_lines, lag_ratio=1 / len(scan_lines), run_time=3))
    self.wait()
    tpr_words = TexText('9 True positives', font_size=36)
    fnr_words = TexText('1 False negative', font_size=36)
    tnr_words = TexText('901 True negatives', font_size=36)
    fpr_words = TexText('89 False positives', font_size=36)
    tpr_words.set_color(GREEN_B)
    fnr_words.set_color(RED_D)
    tnr_words.set_color(RED_B)
    fpr_words.set_color(GREEN_D)
    tp_cases = positive_cases[:9]
    fn_cases = positive_cases[9:]
    tpr_words.next_to(tp_cases, UP)
    fnr_words.next_to(fn_cases, DOWN)
    signs = VGroup()
    for woman in tp_cases:
        sign = Tex('+')
        sign.set_color(GREEN_B)
        sign.match_height(woman)
        sign.next_to(woman, RIGHT, SMALL_BUFF)
        woman.sign = sign
        signs.add(sign)
    for woman in fn_cases:
        sign = Tex('-')
        sign.set_color(RED)
        sign.match_width(signs[0])
        sign.next_to(woman, RIGHT, SMALL_BUFF)
        woman.sign = sign
        signs.add(sign)
    boxes = VGroup()
    for (n, woman) in enumerate(positive_cases):
        box = SurroundingRectangle(woman, buff=0)
        box.set_stroke(width=2)
        if woman in tp_cases:
            box.set_color(GREEN)
        else:
            box.set_color(RED)
        woman.box = box
        boxes.add(box)
    self.play(FadeIn(tpr_words, shift=0.2 * UP), ShowIncreasingSubsets(signs[:9]), ShowIncreasingSubsets(boxes[:9]))
    self.wait()
    self.play(FadeIn(fnr_words, shift=0.2 * DOWN), Write(signs[9:]), ShowCreation(boxes[9:]))
    self.wait()
    negative_cases.sort(lambda p: -p[1])
    num_fp = int(len(negative_cases) * 0.09)
    fp_cases = negative_cases[:num_fp]
    tn_cases = negative_cases[num_fp:]
    new_boxes = VGroup()
    for (n, woman) in enumerate(negative_cases):
        box = SurroundingRectangle(woman, buff=0)
        box.set_stroke(width=2)
        if woman in fp_cases:
            box.set_color(GREEN)
        else:
            box.set_color(RED)
        woman.box = box
        new_boxes.add(box)
    fpr_words.next_to(fp_cases, UP, buff=SMALL_BUFF)
    tnr_words.next_to(tn_cases, DOWN, buff=0.2)
    self.play(FadeIn(fpr_words, shift=0.2 * UP), ShowIncreasingSubsets(new_boxes[:num_fp]))
    self.wait()
    self.play(FadeIn(tnr_words, shift=0.2 * DOWN), ShowIncreasingSubsets(new_boxes[num_fp:]))
    self.wait()
    self.remove(boxes, new_boxes, population)
    for woman in population:
        woman.add(woman.box)
    self.add(population)
    for (cases, nr, rect) in zip([tp_cases, fp_cases], [3, 7], rects):
        cases.save_state()
        cases.generate_target()
        for case in cases.target:
            case[-1].set_stroke(width=3)
            case[-1].scale(1.1)
        cases.target.arrange_in_grid(n_rows=nr, buff=0.5 * cases[0].get_width())
        cases.target.scale(0.5 / cases.target[0].get_height())
        cases.target.move_to(rect)
    fp_cases.target.shift(0.4 * DOWN)
    positive_words.save_state()
    negative_words.save_state()
    tpr_words.save_state()
    fpr_words.save_state()
    self.play(MoveToTarget(tp_cases), MoveToTarget(fp_cases), tpr_words.next_to, tp_cases.target, UP, fpr_words.next_to, fp_cases.target, UP, FadeOut(signs), positive_words[0].set_opacity, 0, negative_words[0].set_opacity, 0, positive_words[1].match_x, rects[0], negative_words[1].match_x, rects[1], LaggedStart(FadeOut(fn_cases, shift=DOWN), FadeOut(fnr_words, shift=DOWN), FadeOut(tn_cases, shift=DOWN), FadeOut(tnr_words, shift=DOWN)))
    self.wait()
    self.play(ShowCreationThenFadeOut(SurroundingRectangle(tpr_words[0][:1], stroke_width=2, stroke_color=WHITE, buff=0.05)), LaggedStartMap(Indicate, tp_cases, color=YELLOW, lag_ratio=0.3, run_time=1))
    self.wait()
    self.play(ShowCreationThenFadeOut(SurroundingRectangle(fpr_words[0][:2], stroke_width=2, stroke_color=WHITE, buff=0.05)), LaggedStartMap(Indicate, fp_cases, color=GREEN_A, lag_ratio=0.05, run_time=3))
    self.wait()
    equation = Tex('P(', '\\text{Have cancer }', '|', '\\text{ positive test})', '\\approx', '\\frac{9}{9 + 89}', '\\approx \\frac{1}{11}')
    equation.set_color_by_tex('cancer', YELLOW)
    equation.set_color_by_tex('positive', GREEN)
    equation.to_edge(UP, buff=SMALL_BUFF)
    self.play(FadeIn(equation[:-1], shift=UP), FadeOut(title, shift=UP))
    self.wait()
    self.play(Write(equation[-1]))
    self.wait()
    frame = self.camera.frame
    frame.save_state()
    ppv_words = TexText('Positive\\\\', 'Predictive\\\\', 'Value\\\\', alignment='')
    ppv_words.next_to(equation, RIGHT, LARGE_BUFF, DOWN)
    for word in ppv_words:
        word[0].set_color(BLUE)
    ppv_rhs = Tex('={\\text{TP} \\over \\text{TP} + \\text{FP}}', tex_to_color_map={'\\text{TP}': GREEN_B, '\\text{FP}': GREEN_C})
    ppv_rhs.next_to(ppv_words, RIGHT)
    ppv_rhs.shift(1.5 * LEFT)
    self.play(frame.scale, 1.1, {'about_edge': DL})
    self.play(ShowIncreasingSubsets(ppv_words))
    self.wait()
    self.play(equation.shift, 1.5 * LEFT + 0.5 * UP, ppv_words.shift, 1.5 * LEFT, FadeIn(ppv_rhs, lag_ratio=0.1), frame.scale, 1.1, {'about_edge': DL})
    self.wait()
    self.play(frame.restore, frame.shift, 0.5 * DOWN, LaggedStartMap(FadeOut, VGroup(equation, ppv_words, ppv_rhs)), LaggedStartMap(Restore, VGroup(tpr_words, tp_cases, fpr_words, fp_cases)), run_time=3)
    self.play(LaggedStartMap(FadeIn, VGroup(fnr_words, fn_cases, tnr_words, tn_cases)))
    self.wait()
    fade_rects = VGroup(*(BackgroundRectangle(VGroup(rect, words), fill_opacity=0.9, fill_color=BLACK, buff=SMALL_BUFF) for (rect, words) in zip(rects, [positive_words, negative_words])))
    sens_eq = Tex('\\text{Sensitivity}', '= {9 \\over 10}', '= 90\\%')
    sens_eq.next_to(rects[0], LEFT, MED_LARGE_BUFF, aligned_edge=UP)
    sens_eq.shift(DOWN)
    fnr_eq = Tex('\\text{False Negative Rate}', '= 10\\%')
    fnr_eq.set_color(RED)
    fnr_eq.scale(0.9)
    equiv = Tex('\\Leftrightarrow')
    equiv.scale(1.5)
    equiv.rotate(90 * DEGREES)
    equiv.next_to(sens_eq, DOWN, MED_LARGE_BUFF)
    fnr_eq.next_to(equiv, DOWN, MED_LARGE_BUFF)
    self.play(frame.shift, 5 * LEFT, FadeIn(fade_rects[1]), Write(sens_eq[0]))
    self.wait()
    self.play(TransformFromCopy(tpr_words[0][0], sens_eq[1][1]), Write(sens_eq[1][0]), Write(sens_eq[1][2:]))
    self.play(Write(sens_eq[2]))
    self.wait()
    self.play(FadeIn(equiv, shift=0.5 * DOWN), FadeIn(fnr_eq, shift=1.0 * DOWN))
    self.wait()
    fade_rects[0].stretch(5, 0, about_edge=RIGHT)
    self.play(ApplyMethod(frame.shift, 10 * RIGHT, run_time=4), FadeIn(fade_rects[0], run_time=2), FadeOut(fade_rects[1], run_time=2))
    spec_eq = Tex('\\text{Specificity}', '= {901 \\over 990}', '\\approx 91\\%')
    spec_eq.next_to(rects[1], RIGHT, MED_LARGE_BUFF, aligned_edge=DOWN)
    spec_eq.shift(UP)
    fpr_eq = Tex('\\text{False Positive Rate}', '= 9\\%')
    fpr_eq.set_color(GREEN)
    fpr_eq.scale(0.9)
    equiv2 = Tex('\\Leftrightarrow')
    equiv2.scale(1.5)
    equiv2.rotate(90 * DEGREES)
    equiv2.next_to(spec_eq, UP, MED_LARGE_BUFF)
    fpr_eq.next_to(equiv2, UP, MED_LARGE_BUFF)
    self.play(Write(spec_eq[0]))
    self.wait()
    self.play(Write(spec_eq[1][0]), TransformFromCopy(tnr_words[0][:3], spec_eq[1][1:4], run_time=2, path_arc=30 * DEGREES), Write(spec_eq[1][4:]))
    self.wait()
    self.play(Write(spec_eq[2]))
    self.wait()
    self.play(FadeIn(equiv2, shift=0.5 * UP), FadeIn(fpr_eq, shift=1.0 * UP))
    self.wait()
    eqs = [sens_eq, spec_eq]
    for (eq, word) in zip(eqs, [positive_words, negative_words]):
        eq.generate_target()
        eq.target[1].set_opacity(0)
        (eq.target[2].move_to(eq.target[1], LEFT),)
        eq.target.next_to(word, UP, buff=0.3)
    self.play(FadeOut(fade_rects[0]), frame.shift, 5 * LEFT, frame.scale, 1.1, {'about_edge': DOWN}, MoveToTarget(sens_eq), MoveToTarget(spec_eq), *map(FadeOut, (fnr_eq, fpr_eq, equiv, equiv2)), run_time=2)
    self.wait()
    self.play(VGroup(fn_cases, fnr_words, fp_cases, fpr_words).set_opacity, 0.2, rate_func=there_and_back_with_pause, run_time=3)","for case in cases.target:
    case[-1].set_stroke(width=3)
    case[-1].scale(1.1)","['for case in cases.target:\n    (*case_rcasemaining, case_last) = case\n    case_last.set_stroke(width=3)\n    case_last.scale(1.1)']",no_found,0
checkov,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkov/tests/terraform/parser/test_parser_var_blocks.py,https://github.com/bridgecrewio/checkov/tree/master/tests/terraform/parser/test_parser_var_blocks.py,TestParserInternals,test_split_merge_args$10,"def test_split_merge_args(self):
    cases: List[Tuple[str, List[str]]] = [('local.one, local.two', ['local.one', 'local.two']), ('{Tag4 = ""four""}, {Tag5 = ""five""}', ['{Tag4 = ""four""}', '{Tag5 = ""five""}']), ('{a=""b""}, {a=[1,2], c=""z""}, {d=3}', ['{a=""b""}', '{a=[1,2], c=""z""}', '{d=3}']), ('local.common_tags, merge({Tag4 = ""four""}, {Tag5 = ""five""})', ['local.common_tags', 'merge({Tag4 = ""four""}, {Tag5 = ""five""})']), (', ', None), ('', None), (', leading_comma', ['leading_comma']), ('kinda_maybe_shouldnt_work_but_we_will_roll_with_it, ', ['kinda_maybe_shouldnt_work_but_we_will_roll_with_it']), ('local.one', ['local.one']), ('{""a"": ""}, evil""}', ['{""a"": ""}, evil""}']), (""{'a': '}, evil'}"", [""{'a': '}, evil'}""]), (""${merge({'a': '}, evil'})}"", [""${merge({'a': '}, evil'})}""]), (""local.common_tags,,{'Tag4': 'four'},,{'Tag2': 'Dev'},"", ['local.common_tags', ""{'Tag4': 'four'}"", ""{'Tag2': 'Dev'}""])]
    for case in cases:
        actual = split_merge_args(case[0])
        assert actual == case[1], f'Case ""{case[0]}"" failed. Expected: {case[1]}  Actual: {actual}'","for case in cases:
    actual = split_merge_args(case[0])
    assert actual == case[1], f'Case ""{case[0]}"" failed. Expected: {case[1]}  Actual: {actual}'","['for case in cases:\n    (case_0, case_1, *_) = case\n    actual = split_merge_args(case_0)\n    assert actual == case_1, f\'Case ""{case_0}"" failed. Expected: {case_1}  Actual: {actual}\'', 'for (case_0, case_1, *case_len) in cases:\n    actual = split_merge_args(\n    case_0)\n    assert actual == \n    case_1, f\'Case ""{case_0}"" failed. Expected: {case_1}  Actual: {actual}\'']",no_found,0
mmcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmcv/tests/test_ops/test_focal_loss.py,https://github.com/open-mmlab/mmcv/tree/master/tests/test_ops/test_focal_loss.py,Testfocalloss,_test_grad_softmax$80,"def _test_grad_softmax(self, dtype=torch.float):
    if not torch.cuda.is_available():
        return
    from mmcv.ops import SoftmaxFocalLoss
    alpha = 0.25
    gamma = 2.0
    for case in inputs:
        np_x = np.array(case[0])
        np_y = np.array(case[1])
        x = torch.from_numpy(np_x).cuda().type(dtype)
        x.requires_grad_()
        y = torch.from_numpy(np_y).cuda().long()
        floss = SoftmaxFocalLoss(gamma, alpha)
        if _USING_PARROTS:
            pass
        else:
            gradcheck(floss, (x, y), eps=0.01, atol=0.01)","for case in inputs:
    np_x = np.array(case[0])
    np_y = np.array(case[1])
    x = torch.from_numpy(np_x).cuda().type(dtype)
    x.requires_grad_()
    y = torch.from_numpy(np_y).cuda().long()
    floss = SoftmaxFocalLoss(gamma, alpha)
    if _USING_PARROTS:
        pass
    else:
        gradcheck(floss, (x, y), eps=0.01, atol=0.01)","['for case in inputs:\n    (case_0, case_1, *_) = case\n    np_x = np.array(case_0)\n    np_y = np.array(case_1)\n    x = torch.from_numpy(np_x).cuda().type(dtype)\n    x.requires_grad_()\n    y = torch.from_numpy(np_y).cuda().long()\n    floss = SoftmaxFocalLoss(gamma, alpha)\n    if _USING_PARROTS:\n        pass\n    else:\n        gradcheck(floss, (x, y), eps=0.01, atol=0.01)', 'for (case_0, case_1, *case_len) in inputs:\n    np_x = np.array(case_0)\n    np_y = np.array(case_1)\n    x = torch.from_numpy(np_x).cuda().type(dtype)\n    x.requires_grad_()\n    y = torch.from_numpy(np_y).cuda().long()\n    floss = SoftmaxFocalLoss(gamma, alpha)\n    if _USING_PARROTS:\n        pass\n    else:\n        gradcheck(floss, (x, y), eps=0.01, atol=0.01)']",no_found,0
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/units/module_utils/common/validation/test_check_type_dict.py,https://github.com/ansible/ansible/tree/master/test/units/module_utils/common/validation/test_check_type_dict.py,,test_check_type_dict$13,"def test_check_type_dict():
    test_cases = (({'k1': 'v1'}, {'k1': 'v1'}), ('k1=v1,k2=v2', {'k1': 'v1', 'k2': 'v2'}), ('k1=v1, k2=v2', {'k1': 'v1', 'k2': 'v2'}), ('k1=v1,     k2=v2,  k3=v3', {'k1': 'v1', 'k2': 'v2', 'k3': 'v3'}), ('{""key"": ""value"", ""list"": [""one"", ""two""]}', {'key': 'value', 'list': ['one', 'two']}))
    for case in test_cases:
        assert case[1] == check_type_dict(case[0])","for case in test_cases:
    assert case[1] == check_type_dict(case[0])","['for case in test_cases:\n    (case_0, case_1, *_) = case\n    assert case_1 == check_type_dict(case_0)', 'for (case_0, case_1, *case_len) in test_cases:\n    assert \n    case_1 == check_type_dict(\n    case_0)']",no_found,0
sympy_gamma,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy_gamma/app/views.py,https://github.com/sympy/sympy_gamma/tree/master/app/views.py,,random_example$132,"def random_example(request):
    examples = []
    for category in EXAMPLES:
        for subcategory in category[1]:
            for example in subcategory[1]:
                if isinstance(example, tuple):
                    examples.append(example[1])
                else:
                    examples.append(example)
    return redirect('input/?i=' + six.moves.urllib.parse.quote(random.choice(examples)))","for category in EXAMPLES:
    for subcategory in category[1]:
        for example in subcategory[1]:
            if isinstance(example, tuple):
                examples.append(example[1])
            else:
                examples.append(example)","['for category in EXAMPLES:\n    (category_0, category_1, *category_rcategorymaining) = category\n    for subcategory in category_1:\n        for example in subcategory_1:\n            if isinstance(example, tuple):\n                examples.append(example[1])\n            else:\n                examples.append(example)', 'for (category_0, category_1, *category_len) in EXAMPLES:\n    for subcategory in \n    category_1:\n        for example in subcategory[1]:\n            if isinstance(example, tuple):\n                examples.append(example[1])\n            else:\n                examples.append(example)']",no_found,0
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""""""""
    outline = Outline()
    playlist = Playlist()
    counter = Counter()
    video_list = []
    pdf_list = []
    rich_text_list = []
    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190', 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean', 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id, 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr', data=post_data).text.encode('utf_8').decode('unicode_escape')
    chapters = re.findall('homeworks=\\w+;.+id=(\\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)
        lessons = re.findall('chapterId=' + chapter[0] + '.+contentType=1.+id=(\\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)
            videos = re.findall('contentId=(\\d+).+contentType=(1).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()
            pdfs = re.findall('contentId=(\\d+).+contentType=(3).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()
            rich_text = re.findall('contentId=(\\d+).+contentType=(4).+id=(\\d+).+jsonContent=(.+);.+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')
                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()
    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","for chapter in chapters:
    counter.add(0)
    outline.write(chapter[1], counter, 0)
    lessons = re.findall('chapterId=' + chapter[0] + '.+contentType=1.+id=(\\d+).+name=""(.+)"".+test', res)
    for lesson in lessons:
        counter.add(1)
        outline.write(lesson[1], counter, 1)
        videos = re.findall('contentId=(\\d+).+contentType=(1).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
        for video in videos:
            counter.add(2)
            outline.write(video[3], counter, 2, sign='#')
            video_list.append(Video(counter, video[3], video))
        counter.reset()
        pdfs = re.findall('contentId=(\\d+).+contentType=(3).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
        for pdf in pdfs:
            counter.add(2)
            outline.write(pdf[3], counter, 2, sign='*')
            if CONFIG['doc']:
                pdf_list.append(Document(counter, pdf[3], pdf))
        counter.reset()
        rich_text = re.findall('contentId=(\\d+).+contentType=(4).+id=(\\d+).+jsonContent=(.+);.+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
        for text in rich_text:
            counter.add(2)
            outline.write(text[4], counter, 2, sign='+')
            if CONFIG['text']:
                rich_text_list.append(RichText(counter, text[4], text))
            if CONFIG['file']:
                if text[3] != 'null' and text[3] != '""""':
                    params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                    file_name = Resource.file_to_save(params['fileName'])
                    outline.write(file_name, counter, 2, sign='!')
                    WORK_DIR.change('Files')
                    res_print(params['fileName'])
                    file_name = '%s %s' % (counter, file_name)
                    CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
        counter.reset()","['for chapter in chapters:\n    (chapter_0, chapter_1, *_) = chapter\n    counter.add(0)\n    outline.write(chapter_1, counter, 0)\n    lessons = re.findall(\'chapterId=\' + chapter_0 + \'.+contentType=1.+id=(\\\\d+).+name=""(.+)"".+test\', res)\n    for lesson in lessons:\n        counter.add(1)\n        outline.write(lesson[1], counter, 1)\n        videos = re.findall(\'contentId=(\\\\d+).+contentType=(1).+id=(\\\\d+).+lessonId=\' + lesson[0] + \'.+name=""(.+)""\', res)\n        for video in videos:\n            counter.add(2)\n            outline.write(video[3], counter, 2, sign=\'#\')\n            video_list.append(Video(counter, video[3], video))\n        counter.reset()\n        pdfs = re.findall(\'contentId=(\\\\d+).+contentType=(3).+id=(\\\\d+).+lessonId=\' + lesson[0] + \'.+name=""(.+)""\', res)\n        for pdf in pdfs:\n            counter.add(2)\n            outline.write(pdf[3], counter, 2, sign=\'*\')\n            if CONFIG[\'doc\']:\n                pdf_list.append(Document(counter, pdf[3], pdf))\n        counter.reset()\n        rich_text = re.findall(\'contentId=(\\\\d+).+contentType=(4).+id=(\\\\d+).+jsonContent=(.+);.+lessonId=\' + lesson[0] + \'.+name=""(.+)""\', res)\n        for text in rich_text:\n            counter.add(2)\n            outline.write(text[4], counter, 2, sign=\'+\')\n            if CONFIG[\'text\']:\n                rich_text_list.append(RichText(counter, text[4], text))\n            if CONFIG[\'file\']:\n                if text[3] != \'null\' and text[3] != \'""""\':\n                    params = {\'nosKey\': re.search(\'nosKey"":""(.+?)""\', text[3]).group(1), \'fileName\': re.search(\'""fileName"":""(.+?)""\', text[3]).group(1)}\n                    file_name = Resource.file_to_save(params[\'fileName\'])\n                    outline.write(file_name, counter, 2, sign=\'!\')\n                    WORK_DIR.change(\'Files\')\n                    res_print(params[\'fileName\'])\n                    file_name = \'%s %s\' % (counter, file_name)\n                    CANDY.download_bin(\'https://www.icourse163.org/course/attachment.htm\', WORK_DIR.file(file_name), params=params, cookies={\'STUDY_SESS\': None})\n        counter.reset()', 'for (chapter_0, chapter_1, *chapter_len) in chapters:\n    counter.add(0)\n    outline.write(chapter_1, counter, 0)\n    lessons = re.findall(\'chapterId=\' + chapter_0 + \'.+contentType=1.+id=(\\\\d+).+name=""(.+)"".+test\', res)\n    for lesson in lessons:\n        counter.add(1)\n        outline.write(lesson[1], counter, 1)\n        videos = re.findall(\'contentId=(\\\\d+).+contentType=(1).+id=(\\\\d+).+lessonId=\' + lesson[0] + \'.+name=""(.+)""\', res)\n        for video in videos:\n            counter.add(2)\n            outline.write(video[3], counter, 2, sign=\'#\')\n            video_list.append(Video(counter, video[3], video))\n        counter.reset()\n        pdfs = re.findall(\'contentId=(\\\\d+).+contentType=(3).+id=(\\\\d+).+lessonId=\' + lesson[0] + \'.+name=""(.+)""\', res)\n        for pdf in pdfs:\n            counter.add(2)\n            outline.write(pdf[3], counter, 2, sign=\'*\')\n            if CONFIG[\'doc\']:\n                pdf_list.append(Document(counter, pdf[3], pdf))\n        counter.reset()\n        rich_text = re.findall(\'contentId=(\\\\d+).+contentType=(4).+id=(\\\\d+).+jsonContent=(.+);.+lessonId=\' + lesson[0] + \'.+name=""(.+)""\', res)\n        for text in rich_text:\n            counter.add(2)\n            outline.write(text[4], counter, 2, sign=\'+\')\n            if CONFIG[\'text\']:\n                rich_text_list.append(RichText(counter, text[4], text))\n            if CONFIG[\'file\']:\n                if text[3] != \'null\' and text[3] != \'""""\':\n                    params = {\'nosKey\': re.search(\'nosKey"":""(.+?)""\', text[3]).group(1), \'fileName\': re.search(\'""fileName"":""(.+?)""\', text[3]).group(1)}\n                    file_name = Resource.file_to_save(params[\'fileName\'])\n                    outline.write(file_name, counter, 2, sign=\'!\')\n                    WORK_DIR.change(\'Files\')\n                    res_print(params[\'fileName\'])\n                    file_name = \'%s %s\' % (counter, file_name)\n                    CANDY.download_bin(\'https://www.icourse163.org/course/attachment.htm\', WORK_DIR.file(file_name), params=params, cookies={\'STUDY_SESS\': None})\n        counter.reset()']",no_found,0
quodlibet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quodlibet/quodlibet/ext/songsmenu/duplicates.py,https://github.com/quodlibet/quodlibet/tree/master/quodlibet/ext/songsmenu/duplicates.py,DuplicateSongsView,get_selected_songs$36,"def get_selected_songs(self):
    selection = self.get_selection()
    if selection is None:
        return []
    (model, rows) = selection.get_selected_rows()
    if not rows:
        return []
    selected = []
    for row in rows:
        row = model[row]
        if row.parent is None:
            for child in row.iterchildren():
                selected.append(child[0])
        else:
            selected.append(row[0])
    return selected","for child in row.iterchildren():
    selected.append(child[0])","['for child in row.iterchildren():\n    (child_0, *child_rchildmaining) = child\n    selected.append(child_0)', 'for (child_0, *child_len) in row.iterchildren():\n    selected.append(child_0)']",no_found,0
OnlyFans,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OnlyFans/modules/onlyfans.py,https://github.com/DIGITALCRIMINAL/OnlyFans/tree/master/modules/onlyfans.py,,scrape_choice$166,"def scrape_choice(authed: create_auth, subscription):
    user_id = subscription.id
    post_count = subscription.postsCount
    archived_count = subscription.archivedPostsCount
    message = 'Scrape: 0 = All | 1 = Images | 2 = Videos | 3 = Audios | 4 = Texts'
    media_types = [[['', 'All'], ['', 'Images'], ['', 'Videos'], ['', 'Audios'], ['', 'Texts']], message]
    choice_list = main_helper.choose_option(media_types, auto_media_choice)
    user_api = OnlyFans.endpoint_links(user_id).users
    message_api = OnlyFans.endpoint_links(user_id).message_api
    stories_api = OnlyFans.endpoint_links(user_id).stories_api
    list_highlights = OnlyFans.endpoint_links(user_id).list_highlights
    post_api = OnlyFans.endpoint_links(user_id).post_api
    archived_api = OnlyFans.endpoint_links(user_id).archived_posts
    only_links = False
    mandatory = [download_directory, only_links]
    y = ['photo', 'video', 'stream', 'gif', 'audio', 'text']
    u_array = ['You have chosen to scrape {}', [user_api, media_types, *mandatory, post_count], 'Profile']
    s_array = ['You have chosen to scrape {}', [stories_api, media_types, *mandatory, post_count], 'Stories']
    h_array = ['You have chosen to scrape {}', [list_highlights, media_types, *mandatory, post_count], 'Highlights']
    p_array = ['You have chosen to scrape {}', [post_api, media_types, *mandatory, post_count], 'Posts']
    m_array = ['You have chosen to scrape {}', [message_api, media_types, *mandatory, post_count], 'Messages']
    a_array = ['You have chosen to scrape {}', [archived_api, media_types, *mandatory, archived_count], 'Archived']
    array = [u_array, s_array, p_array, a_array, m_array]
    new_array = []
    valid_input = True
    for xxx in array:
        if xxx[2] == 'Mass Messages':
            if not subscription.is_me():
                continue
        new_item = dict()
        new_item['api_message'] = xxx[0]
        new_item['api_array'] = {}
        new_item['api_array']['api_link'] = xxx[1][0]
        new_item['api_array']['media_types'] = xxx[1][1]
        new_item['api_array']['directory'] = xxx[1][2]
        new_item['api_array']['only_links'] = xxx[1][3]
        new_item['api_array']['post_count'] = xxx[1][4]
        formatted = format_media_types()
        final_format = []
        for choice in choice_list:
            choice = choice[1]
            final_format.extend([result for result in formatted if result[0] == choice])
        new_item['api_array']['media_types'] = final_format
        new_item['api_type'] = xxx[2]
        if valid_input:
            new_array.append(new_item)
    return new_array","for choice in choice_list:
    choice = choice[1]
    final_format.extend([result for result in formatted if result[0] == choice])","['for choice in choice_list:\n    (choice_0, choice_1, *choice_rchoicemaining) = choice\n    choice = choice_1\n    final_format.extend([result for result in formatted if result[0] == choice])']",no_found,0
meld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meld/meld/matchers/merge.py,https://github.com/GNOME/meld/tree/master/meld/matchers/merge.py,AutoMergeDiffer,_auto_merge$32,"def _auto_merge(self, using, texts):
    for (out0, out1) in super()._auto_merge(using, texts):
        if self.auto_merge and out0[0] == 'conflict':
            (l0, h0, l1, h1, l2, h2) = (out0[3], out0[4], out0[1], out0[2], out1[3], out1[4])
            len0 = h0 - l0
            len1 = h1 - l1
            len2 = h2 - l2
            if (len0 > 0 and len2 > 0) and (len0 == len1 or len2 == len1 or len1 == 0):
                matcher = self._matcher(None, texts[0][l0:h0], texts[2][l2:h2])
                for chunk in matcher.get_opcodes():
                    s1 = l1
                    e1 = l1
                    if len0 == len1:
                        s1 += chunk[1]
                        e1 += chunk[2]
                    elif len2 == len1:
                        s1 += chunk[3]
                        e1 += chunk[4]
                    out0_bounds = (s1, e1, l0 + chunk[1], l0 + chunk[2])
                    out1_bounds = (s1, e1, l2 + chunk[3], l2 + chunk[4])
                    if chunk[0] == 'equal':
                        out0 = ('replace',) + out0_bounds
                        out1 = ('replace',) + out1_bounds
                        yield (out0, out1)
                    else:
                        out0 = ('conflict',) + out0_bounds
                        out1 = ('conflict',) + out1_bounds
                        yield (out0, out1)
                return
            else:
                chunktype = using[0][0][0]
                for chunkarr in using:
                    for chunk in chunkarr:
                        if chunk[0] != chunktype:
                            chunktype = None
                            break
                    if not chunktype:
                        break
                if chunktype == 'delete':
                    seq0 = seq1 = None
                    while 1:
                        if seq0 is None:
                            try:
                                seq0 = using[0].pop(0)
                                i0 = seq0[1]
                                end0 = seq0[4]
                            except IndexError:
                                break
                        if seq1 is None:
                            try:
                                seq1 = using[1].pop(0)
                                i1 = seq1[1]
                                end1 = seq1[4]
                            except IndexError:
                                break
                        highstart = max(i0, i1)
                        if i0 != i1:
                            out0 = ('conflict', i0 - highstart + i1, highstart, seq0[3] - highstart + i1, seq0[3])
                            out1 = ('conflict', i1 - highstart + i0, highstart, seq1[3] - highstart + i0, seq1[3])
                            yield (out0, out1)
                        lowend = min(seq0[2], seq1[2])
                        if highstart != lowend:
                            out0 = ('delete', highstart, lowend, seq0[3], seq0[4])
                            out1 = ('delete', highstart, lowend, seq1[3], seq1[4])
                            yield (out0, out1)
                        i0 = i1 = lowend
                        if lowend == seq0[2]:
                            seq0 = None
                        if lowend == seq1[2]:
                            seq1 = None
                    if seq0:
                        out0 = ('conflict', i0, seq0[2], seq0[3], seq0[4])
                        out1 = ('conflict', i0, seq0[2], end1, end1 + seq0[2] - i0)
                        yield (out0, out1)
                    elif seq1:
                        out0 = ('conflict', i1, seq1[2], end0, end0 + seq1[2] - i1)
                        out1 = ('conflict', i1, seq1[2], seq1[3], seq1[4])
                        yield (out0, out1)
                    return
        yield (out0, out1)","for chunk in chunkarr:
    if chunk[0] != chunktype:
        chunktype = None
        break","['for chunk in chunkarr:\n    (chunk_0, *chunk_rchunkmaining) = chunk\n    if chunk_0 != chunktype:\n        chunktype = None\n        break', 'for (chunk_0, *chunk_len) in chunkarr:\n    if \n    chunk_0 != chunktype:\n        chunktype = None\n        break']",no_found,0
rtfm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rtfm/rtfm.py,https://github.com/leostat/rtfm/tree/master//rtfm.py,,dbInsertCmd$386,"def dbInsertCmd(conn, cmds):
    cur = conn.cursor()
    cur.execute('SELECT max(cmdid) from tblcommand')
    max_id = cur.fetchall()
    if options.debug:
        for cmd in cmds:
            debug(""I: INSERT INTO tblcommand VALUES (NULL, '"" + str(cmd[0]) + ""', '"" + str(cmd[1]) + ""', '"" + str(cmd[2]) + ""', "" + ""date('now'))"")
    cur.executemany('INSERT INTO tblcommand VALUES (NULL, ?, ?, ?, date(""now""));', cmds)
    conn.commit()
    ok('Added Rows : ' + str(cur.rowcount))
    cur.execute('SELECT max(cmdid) FROM tblcommand')
    new_max_id = cur.fetchall()
    ok('New Top ID : ' + str(new_max_id[0][0]) + "" | Number of CMD's Added : "" + str(new_max_id[0][0] - max_id[0][0]))","for cmd in cmds:
    debug(""I: INSERT INTO tblcommand VALUES (NULL, '"" + str(cmd[0]) + ""', '"" + str(cmd[1]) + ""', '"" + str(cmd[2]) + ""', "" + ""date('now'))"")","['for cmd in cmds:\n    (cmd_0, cmd_1, cmd_2, *_) = cmd\n    debug(""I: INSERT INTO tblcommand VALUES (NULL, \'"" + str(cmd_0) + ""\', \'"" + str(cmd_1) + ""\', \'"" + str(cmd_2) + ""\', "" + ""date(\'now\'))"")', 'for (cmd_0, cmd_1, cmd_2, *cmd_len) in cmds:\n    debug(""I: INSERT INTO tblcommand VALUES (NULL, \'"" + str(cmd_0) + ""\', \'"" + str(cmd_1) + ""\', \'"" + str(cmd_2) + ""\', "" + ""date(\'now\'))"")']",no_found,0
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/mysql.py,https://github.com/saltstack/salt/tree/master/salt/modules/mysql.py,,__do_query_into_hash$2673,"def __do_query_into_hash(conn, sql_str):
    """"""
    Perform the query that is passed to it (sql_str).

    Returns:
       results in a dict.

    """"""
    mod = sys._getframe().f_code.co_name
    log.debug('%s<--(%s)', mod, sql_str)
    rtn_results = []
    try:
        cursor = conn.cursor()
    except MySQLdb.MySQLError:
        log.error(""%s: Can't get cursor for SQL->%s"", mod, sql_str)
        cursor.close()
        log.debug('%s-->', mod)
        return rtn_results
    try:
        _execute(cursor, sql_str)
    except MySQLdb.MySQLError:
        log.error('%s: try to execute : SQL->%s', mod, sql_str)
        cursor.close()
        log.debug('%s-->', mod)
        return rtn_results
    qrs = cursor.fetchall()
    for row_data in qrs:
        col_cnt = 0
        row = {}
        for col_data in cursor.description:
            col_name = col_data[0]
            row[col_name] = row_data[col_cnt]
            col_cnt += 1
        rtn_results.append(row)
    cursor.close()
    log.debug('%s-->', mod)
    return rtn_results","for col_data in cursor.description:
    col_name = col_data[0]
    row[col_name] = row_data[col_cnt]
    col_cnt += 1","['for col_data in cursor.description:\n    (col_data_0, *col_data_rcol_datamaining) = col_data\n    col_name = col_data_0\n    row[col_name] = row_data[col_cnt]\n    col_cnt += 1', 'for (col_data_0, *col_data_len) in cursor.description:\n    col_name = \n    col_data_0\n    row[col_name] = row_data[col_cnt]\n    col_cnt += 1']",no_found,0
geojson,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/geojson/tests/test_utils.py,https://github.com/jazzband/geojson/tree/master/tests/test_utils.py,,check_polygon_bbox$24,"def check_polygon_bbox(polygon, bbox):
    (min_lon, min_lat, max_lon, max_lat) = bbox
    eps = 0.001
    for linear_ring in polygon['coordinates']:
        for coordinate in linear_ring:
            if not (min_lon - eps <= coordinate[0] <= max_lon + eps and min_lat - eps <= coordinate[1] <= max_lat + eps):
                return False
    return True","for coordinate in linear_ring:
    if not (min_lon - eps <= coordinate[0] <= max_lon + eps and min_lat - eps <= coordinate[1] <= max_lat + eps):
        return False","['for coordinate in linear_ring:\n    (coordinate_0, coordinate_1, *_) = coordinate\n    if not (min_lon - eps <= coordinate_0 <= max_lon + eps and min_lat - eps <= coordinate_1 <= max_lat + eps):\n        return False', 'for (coordinate_0, coordinate_1, *coordinate_len) in linear_ring:\n    if not (min_lon - eps <= coordinate_0 <= max_lon + eps and min_lat - eps <= coordinate_1 <= max_lat + eps):\n        return False']",no_found,0
anki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anki/pylib/anki/stats.py,https://github.com/ankitects/anki/tree/master/pylib/anki/stats.py,CollectionStats,hourGraph$772,"def hourGraph(self) -> str:
    data = self._hourRet()
    if not data:
        return ''
    shifted = []
    counts = []
    mcount = 0
    trend: list[tuple[int, int]] = []
    peak = 0
    for d in data:
        hour = (d[0] - 4) % 24
        pct = d[1]
        if pct > peak:
            peak = pct
        shifted.append((hour, pct))
        counts.append((hour, d[2]))
        if d[2] > mcount:
            mcount = d[2]
    shifted.sort()
    counts.sort()
    if len(counts) < 4:
        return ''
    for d in shifted:
        hour = d[0]
        pct = d[1]
        if not trend:
            trend.append((hour, pct))
        else:
            prev = trend[-1][1]
            diff = pct - prev
            diff /= 3.0
            diff = round(diff, 1)
            trend.append((hour, prev + diff))
    txt = self._title('Hourly Breakdown', 'Review success rate for each hour of the day.')
    txt += self._graph(id='hour', data=[dict(data=shifted, color=colCum, label='% Correct'), dict(data=counts, color=colHour, label='Answers', yaxis=2, bars=dict(barWidth=0.2), stack=False)], conf=dict(xaxis=dict(ticks=[[0, '4AM'], [6, '10AM'], [12, '4PM'], [18, '10PM'], [23, '3AM']]), yaxes=[dict(max=peak), dict(position='right', max=mcount)]), ylabel='% Correct', ylabel2='Reviews')
    txt += 'Hours with less than 30 reviews are not shown.'
    return txt","for d in data:
    hour = (d[0] - 4) % 24
    pct = d[1]
    if pct > peak:
        peak = pct
    shifted.append((hour, pct))
    counts.append((hour, d[2]))
    if d[2] > mcount:
        mcount = d[2]","['for d in data:\n    (d_0, d_1, d_2, *_) = d\n    hour = (d_0 - 4) % 24\n    pct = d_1\n    if pct > peak:\n        peak = pct\n    shifted.append((hour, pct))\n    counts.append((hour, d_2))\n    if d_2 > mcount:\n        mcount = d_2', 'for (d_0, d_1, d_2, *d_len) in data:\n    hour = (\n    d_0 - 4) % 24\n    pct = \n    d_1\n    if pct > peak:\n        peak = pct\n    shifted.append((hour, pct))\n    counts.append((hour, \n    d_2))\n    if \n    d_2 > mcount:\n        mcount = \n        d_2']",no_found,0
frigate,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frigate/frigate/video.py,https://github.com/blakeblackshear/frigate/tree/master/frigate/video.py,,detect$404,"def detect(object_detector, frame, model_shape, region, objects_to_track, object_filters):
    tensor_input = create_tensor_input(frame, model_shape, region)
    detections = []
    region_detections = object_detector.detect(tensor_input)
    for d in region_detections:
        box = d[2]
        size = region[2] - region[0]
        x_min = int(box[1] * size + region[0])
        y_min = int(box[0] * size + region[1])
        x_max = int(box[3] * size + region[0])
        y_max = int(box[2] * size + region[1])
        det = (d[0], d[1], (x_min, y_min, x_max, y_max), (x_max - x_min) * (y_max - y_min), region)
        if filtered(det, objects_to_track, object_filters):
            continue
        detections.append(det)
    return detections","for d in region_detections:
    box = d[2]
    size = region[2] - region[0]
    x_min = int(box[1] * size + region[0])
    y_min = int(box[0] * size + region[1])
    x_max = int(box[3] * size + region[0])
    y_max = int(box[2] * size + region[1])
    det = (d[0], d[1], (x_min, y_min, x_max, y_max), (x_max - x_min) * (y_max - y_min), region)
    if filtered(det, objects_to_track, object_filters):
        continue
    detections.append(det)","['for d in region_detections:\n    (d_0, d_1, d_2, *_) = d\n    box = d_2\n    size = region[2] - region[0]\n    x_min = int(box[1] * size + region[0])\n    y_min = int(box[0] * size + region[1])\n    x_max = int(box[3] * size + region[0])\n    y_max = int(box[2] * size + region[1])\n    det = (d_0, d_1, (x_min, y_min, x_max, y_max), (x_max - x_min) * (y_max - y_min), region)\n    if filtered(det, objects_to_track, object_filters):\n        continue\n    detections.append(det)', 'for (d_0, d_1, d_2, *d_len) in region_detections:\n    box = \n    d_2\n    size = region[2] - region[0]\n    x_min = int(box[1] * size + region[0])\n    y_min = int(box[0] * size + region[1])\n    x_max = int(box[3] * size + region[0])\n    y_max = int(box[2] * size + region[1])\n    det = (\n    d_0, \n    d_1, (x_min, y_min, x_max, y_max), (x_max - x_min) * (y_max - y_min), region)\n    if filtered(det, objects_to_track, object_filters):\n        continue\n    detections.append(det)']",no_found,0
anki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anki/pylib/anki/stats.py,https://github.com/ankitects/anki/tree/master/pylib/anki/stats.py,CollectionStats,hourGraph$772,"def hourGraph(self) -> str:
    data = self._hourRet()
    if not data:
        return ''
    shifted = []
    counts = []
    mcount = 0
    trend: list[tuple[int, int]] = []
    peak = 0
    for d in data:
        hour = (d[0] - 4) % 24
        pct = d[1]
        if pct > peak:
            peak = pct
        shifted.append((hour, pct))
        counts.append((hour, d[2]))
        if d[2] > mcount:
            mcount = d[2]
    shifted.sort()
    counts.sort()
    if len(counts) < 4:
        return ''
    for d in shifted:
        hour = d[0]
        pct = d[1]
        if not trend:
            trend.append((hour, pct))
        else:
            prev = trend[-1][1]
            diff = pct - prev
            diff /= 3.0
            diff = round(diff, 1)
            trend.append((hour, prev + diff))
    txt = self._title('Hourly Breakdown', 'Review success rate for each hour of the day.')
    txt += self._graph(id='hour', data=[dict(data=shifted, color=colCum, label='% Correct'), dict(data=counts, color=colHour, label='Answers', yaxis=2, bars=dict(barWidth=0.2), stack=False)], conf=dict(xaxis=dict(ticks=[[0, '4AM'], [6, '10AM'], [12, '4PM'], [18, '10PM'], [23, '3AM']]), yaxes=[dict(max=peak), dict(position='right', max=mcount)]), ylabel='% Correct', ylabel2='Reviews')
    txt += 'Hours with less than 30 reviews are not shown.'
    return txt","for d in shifted:
    hour = d[0]
    pct = d[1]
    if not trend:
        trend.append((hour, pct))
    else:
        prev = trend[-1][1]
        diff = pct - prev
        diff /= 3.0
        diff = round(diff, 1)
        trend.append((hour, prev + diff))","['for d in shifted:\n    (d_0, d_1, *_) = d\n    hour = d_0\n    pct = d_1\n    if not trend:\n        trend.append((hour, pct))\n    else:\n        prev = trend[-1][1]\n        diff = pct - prev\n        diff /= 3.0\n        diff = round(diff, 1)\n        trend.append((hour, prev + diff))', 'for (d_0, d_1, *d_len) in shifted:\n    hour = \n    d_0\n    pct = \n    d_1\n    if not trend:\n        trend.append((hour, pct))\n    else:\n        prev = trend[-1][1]\n        diff = pct - prev\n        diff /= 3.0\n        diff = round(diff, 1)\n        trend.append((hour, prev + diff))']",no_found,0
checkmk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/checkmk/cmk/base/plugins/agent_based/esx_vsphere_counters.py,https://github.com/tribe29/checkmk/tree/master/cmk/base/plugins/agent_based/esx_vsphere_counters.py,,_max_latency$252,"def _max_latency(latencies: SubSectionCounter) -> Optional[int]:
    all_latencies: List[int] = []
    for data in latencies.values():
        (multivalues, _unit) = data[0]
        all_latencies.extend(map(int, multivalues))
    return max(all_latencies) if all_latencies else None","for data in latencies.values():
    (multivalues, _unit) = data[0]
    all_latencies.extend(map(int, multivalues))","['for data in latencies.values():\n    (data_0, *data_rdatamaining) = data\n    (multivalues, _unit) = data_0\n    all_latencies.extend(map(int, multivalues))', 'for (data_0, *data_len) in latencies.values():\n    (multivalues, _unit) = \n    data_0\n    all_latencies.extend(map(int, multivalues))']",no_found,0
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/bluez_bluetooth.py,https://github.com/saltstack/salt/tree/master/salt/modules/bluez_bluetooth.py,,scan$167,"def scan():
    """"""
    Scan for bluetooth devices in the area

    CLI Example:

    .. code-block:: bash

        salt '*' bluetooth.scan
    """"""
    ret = []
    devices = bluetooth.discover_devices(lookup_names=True)
    for device in devices:
        ret.append({device[0]: device[1]})
    return ret","for device in devices:
    ret.append({device[0]: device[1]})","['for device in devices:\n    (device_0, device_1, *_) = device\n    ret.append({device_0: device_1})', 'for (device_0, device_1, *device_len) in devices:\n    ret.append({device_0: device_1})']",no_found,0
nlpcda,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlpcda/nlpcda/tools/Ner.py,https://github.com/425776024/nlpcda/tree/master/nlpcda/tools/Ner.py,Ner,__data_augment_one$78,"def __data_augment_one(self, org_data):
    new_data = []
    for di in org_data:
        (t_tag, t_ner_sentence) = (di[0], di[1])
        if t_tag in self.data_augument_tag_list and t_tag in self.tag_map:
            rdm_select_ner = self.__get_random_ner(t_tag)
            new_data.append([t_tag, rdm_select_ner])
        else:
            new_data.append([t_tag, t_ner_sentence])
    return new_data","for di in org_data:
    (t_tag, t_ner_sentence) = (di[0], di[1])
    if t_tag in self.data_augument_tag_list and t_tag in self.tag_map:
        rdm_select_ner = self.__get_random_ner(t_tag)
        new_data.append([t_tag, rdm_select_ner])
    else:
        new_data.append([t_tag, t_ner_sentence])","['for di in org_data:\n    (di_0, di_1, *_) = di\n    (t_tag, t_ner_sentence) = (di_0, di_1)\n    if t_tag in self.data_augument_tag_list and t_tag in self.tag_map:\n        rdm_select_ner = self.__get_random_ner(t_tag)\n        new_data.append([t_tag, rdm_select_ner])\n    else:\n        new_data.append([t_tag, t_ner_sentence])', 'for (di_0, di_1, *di_len) in org_data:\n    (t_tag, t_ner_sentence) = (di_0, di_1)\n    if t_tag in self.data_augument_tag_list and t_tag in self.tag_map:\n        rdm_select_ner = self.__get_random_ner(t_tag)\n        new_data.append([t_tag, rdm_select_ner])\n    else:\n        new_data.append([t_tag, t_ner_sentence])']",no_found,0
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/tools/fix_annotations.py,https://github.com/nlplab/brat/tree/master/tools/fix_annotations.py,,correct_annotations$28,"def correct_annotations(orig_fn, ann_fn, change_fn):
    with annotation.TextAnnotations(ann_fn) as anns:
        orig_text = anns.get_document_text()
        with annotation.open_textfile(change_fn, 'r') as f:
            changed_text = f.read()
        diffs = diff_match_patch().diff_main(orig_text, changed_text)
        orig_offset = 0
        offsets = []
        for diff in diffs:
            kind = diff[0]
            text = diff[1]
            size = len(text)
            delta = size * kind
            offsets.append((orig_offset, delta))
            if kind != 1:
                orig_offset += size
        offsets = offsets[::-1]
        tbs = list(anns.get_textbounds())
        indices = []
        for (tbi, tb) in enumerate(tbs):
            for (spani, span) in enumerate(tb.spans):
                indices.append((span[0], tbi, spani, 0))
                indices.append((span[1], tbi, spani, 1))
        indices.sort(reverse=True)
        for (orig_offset, delta) in offsets:
            for index in indices:
                if index[0] < orig_offset:
                    break
                frag = list(tbs[index[1]].spans[index[2]])
                frag[index[3]] += delta
                tbs[index[1]].spans[index[2]] = tuple(frag)
        for tb in tbs:
            if isinstance(tb, annotation.TextBoundAnnotationWithText):
                tb.text = annotation.DISCONT_SEP.join((changed_text[start:end] for (start, end) in tb.spans))
    copy(change_fn, orig_fn)","for diff in diffs:
    kind = diff[0]
    text = diff[1]
    size = len(text)
    delta = size * kind
    offsets.append((orig_offset, delta))
    if kind != 1:
        orig_offset += size","['for diff in diffs:\n    (diff_0, diff_1, *_) = diff\n    kind = diff_0\n    text = diff_1\n    size = len(text)\n    delta = size * kind\n    offsets.append((orig_offset, delta))\n    if kind != 1:\n        orig_offset += size', 'for (diff_0, diff_1, *diff_len) in diffs:\n    kind = \n    diff_0\n    text = \n    diff_1\n    size = len(text)\n    delta = size * kind\n    offsets.append((orig_offset, delta))\n    if kind != 1:\n        orig_offset += size']",no_found,0
PyGaze,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyGaze/pygaze/_eyetracker/libtobiilegacy.py,https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/libtobiilegacy.py,TobiiController,flushData$1922,"def flushData(self):
    """"""
        
        arguments
        None
        
        keyword arguments
        None
        
        returns
        None
        """"""
    if self.datafile == None:
        print('WARNING! libtobii.TobiiController.flushData: data file is not set.')
        return
    if len(self.gazeData) == 0:
        print('WARNING! libtobii.TobiiController.flushData: no data to write to file.')
        return
    self.datafile.write('\t'.join(['TimeStamp', 'GazePointXLeft', 'GazePointYLeft', 'ValidityLeft', 'GazePointXRight', 'GazePointYRight', 'ValidityRight', 'GazePointX', 'GazePointY', 'Event']) + '\n')
    timeStampStart = self.gazeData[0].Timestamp
    for g in self.gazeData:
        self.datafile.write('{}\t{}\t{}\t{}\t{}\t{}\t{}'.format(round((g.Timestamp - timeStampStart) / 1000.0, ndigits=1), round(g.LeftGazePoint2D.x * self.disp.dispsize[0] if g.LeftValidity != 4 else -1.0, ndigits=4), round(g.LeftGazePoint2D.y * self.disp.dispsize[1] if g.LeftValidity != 4 else -1.0, ndigits=4), g.LeftValidity, round(g.RightGazePoint2D.x * self.disp.dispsize[0] if g.RightValidity != 4 else -1.0, ndigits=4), round(g.RightGazePoint2D.y * self.disp.dispsize[1] if g.RightValidity != 4 else -1.0, ndigits=4), g.RightValidity))
        if g.LeftValidity == 4 and g.RightValidity == 4:
            ave = (-1.0, -1.0)
        elif g.LeftValidity == 4:
            ave = (g.RightGazePoint2D.x, g.RightGazePoint2D.y)
        elif g.RightValidity == 4:
            ave = (g.LeftGazePoint2D.x, g.LeftGazePoint2D.y)
        else:
            ave = ((g.LeftGazePoint2D.x + g.RightGazePoint2D.x) / 2.0, (g.LeftGazePoint2D.y + g.RightGazePoint2D.y) / 2.0)
        self.datafile.write('\t{}\t{}\t'.format(round(ave[0], ndigits=4), round(ave[1], ndigits=4)))
        self.datafile.write('\n')
    formatstr = '{}' + '\t' * 9 + '{}\n'
    for e in self.eventData:
        self.datafile.write(formatstr.format(round((e[0] - timeStampStart) / 1000.0, ndigits=4), e[1]))
    self.datafile.flush()
    os.fsync(self.datafile.fileno())","for e in self.eventData:
    self.datafile.write(formatstr.format(round((e[0] - timeStampStart) / 1000.0, ndigits=4), e[1]))","['for e in self.eventData:\n    (e_0, e_1, *_) = e\n    self.datafile.write(formatstr.format(round((e_0 - timeStampStart) / 1000.0, ndigits=4), e_1))', 'for (e_0, e_1, *e_len) in self.eventData:\n    self.datafile.write(formatstr.format(round((e_0 - timeStampStart) / 1000.0, ndigits=4), e_1))']",no_found,0
Mycodo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/mycodo_flask/utils/utils_general.py,https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/utils/utils_general.py,,generate_form_input_list$1635,"def generate_form_input_list(dict_inputs):
    list_tuples_sorted = sorted(dict_inputs.items(), key=lambda x: (x[1]['input_manufacturer'], x[1]['input_name']))
    list_inputs_sorted = []
    for each_input in list_tuples_sorted:
        list_inputs_sorted.append(each_input[0])
    return list_inputs_sorted","for each_input in list_tuples_sorted:
    list_inputs_sorted.append(each_input[0])","['for each_input in list_tuples_sorted:\n    (each_input_0, *each_input_reach_inputmaining) = each_input\n    list_inputs_sorted.append(each_input_0)', 'for (each_input_0, *each_input_len) in list_tuples_sorted:\n    list_inputs_sorted.append(each_input_0)']",no_found,0
Grokking-the-Coding-Interview-Patterns-for-Coding-Questions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Grokking-the-Coding-Interview-Patterns-for-Coding-Questions/16. Pattern Topological Sort (Graph)/Problem Challenge 2 - Minimum Height Trees (hard).py,https://github.com/cl2333/Grokking-the-Coding-Interview-Patterns-for-Coding-Questions/tree/master/16. Pattern Topological Sort (Graph)/Problem Challenge 2 - Minimum Height Trees (hard).py,,find_trees$67,"def find_trees(nodes, edges):
    if nodes <= 0:
        return []
    if nodes == 1:
        return [0]
    inDegree = {i: 0 for i in range(nodes)}
    graph = {i: [] for i in range(nodes)}
    for edge in edges:
        (n1, n2) = (edge[0], edge[1])
        graph[n1].append(n2)
        graph[n2].append(n1)
        inDegree[n1] += 1
        inDegree[n2] += 1
    leaves = deque()
    for key in inDegree:
        if inDegree[key] == 1:
            leaves.append(key)
    totalNodes = nodes
    while totalNodes > 2:
        leavesSize = len(leaves)
        totalNodes -= leavesSize
        for i in range(0, leavesSize):
            vertex = leaves.popleft()
            for child in graph[vertex]:
                inDegree[child] -= 1
                if inDegree[child] == 1:
                    leaves.append(child)
    return list(leaves)","for edge in edges:
    (n1, n2) = (edge[0], edge[1])
    graph[n1].append(n2)
    graph[n2].append(n1)
    inDegree[n1] += 1
    inDegree[n2] += 1","['for edge in edges:\n    (edge_0, edge_1, *_) = edge\n    (n1, n2) = (edge_0, edge_1)\n    graph[n1].append(n2)\n    graph[n2].append(n1)\n    inDegree[n1] += 1\n    inDegree[n2] += 1', 'for (edge_0, edge_1, *edge_len) in edges:\n    (n1, n2) = (edge_0, edge_1)\n    graph[n1].append(n2)\n    graph[n2].append(n1)\n    inDegree[n1] += 1\n    inDegree[n2] += 1']",no_found,0
transitions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/transitions/transitions/extensions/diagrams_pygraphviz.py,https://github.com/pytransitions/transitions/tree/master/transitions/extensions/diagrams_pygraphviz.py,Graph,get_graph$55,"def get_graph(self, title=None, roi_state=None):
    if title:
        self.fsm_graph.graph_attr['label'] = title
    if roi_state:
        filtered = _copy_agraph(self.fsm_graph)
        kept_nodes = set()
        active_state = roi_state.name if hasattr(roi_state, 'name') else roi_state
        if not filtered.has_node(roi_state):
            active_state += '_anchor'
        kept_nodes.add(active_state)
        for edge in filtered.edges():
            if active_state not in edge:
                filtered.delete_edge(edge)
        for edge in filtered.in_edges(active_state):
            if edge.attr['color'] == self.fsm_graph.style_attributes['edge']['previous']['color']:
                kept_nodes.add(edge[0])
            else:
                filtered.delete_edge(edge)
        for edge in filtered.out_edges_iter(active_state):
            kept_nodes.add(edge[1])
        for node in filtered.nodes():
            if node not in kept_nodes:
                filtered.delete_node(node)
        return filtered
    return self.fsm_graph","for edge in filtered.out_edges_iter(active_state):
    kept_nodes.add(edge[1])","['for edge in filtered.out_edges_iter(active_state):\n    (edge_0, edge_1, *edge_redgemaining) = edge\n    kept_nodes.add(edge_1)', 'for (edge_0, edge_1, *edge_len) in filtered.out_edges_iter(active_state):\n    kept_nodes.add(edge_1)']",no_found,0
coa_tools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/operators/edit_mesh.py,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/operators/edit_mesh.py,,average_edge_cuts$91,"def average_edge_cuts(bm, obj, cuts=1):
    (edges_len_average, shortest_edge) = get_average_edge_length(bm, obj)
    subdivide_edges = []
    for edge in bm.edges:
        cut_count = int(edge.calc_length() / shortest_edge) * cuts
        if cut_count < 0:
            cut_count = 0
        if not edge.is_boundary:
            subdivide_edges.append([edge, cut_count])
    for edge in subdivide_edges:
        bmesh.ops.subdivide_edges(bm, edges=[edge[0]], cuts=edge[1])
        bmesh.update_edit_mesh(obj.data)","for edge in subdivide_edges:
    bmesh.ops.subdivide_edges(bm, edges=[edge[0]], cuts=edge[1])
    bmesh.update_edit_mesh(obj.data)","['for edge in subdivide_edges:\n    (edge_0, edge_1, *_) = edge\n    bmesh.ops.subdivide_edges(bm, edges=[edge_0], cuts=edge_1)\n    bmesh.update_edit_mesh(obj.data)', 'for (edge_0, edge_1, *edge_len) in subdivide_edges:\n    bmesh.ops.subdivide_edges(bm, edges=[edge_0], cuts=edge_1)\n    bmesh.update_edit_mesh(obj.data)']",no_found,0
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/qiskit/dagcircuit/dagdependency.py,https://github.com/Qiskit/qiskit-terra/tree/master/qiskit/dagcircuit/dagdependency.py,DAGDependency,copy$567,"def copy(self):
    """"""
        Function to copy a DAGDependency object.
        Returns:
            DAGDependency: a copy of a DAGDependency object.
        """"""
    dag = DAGDependency()
    dag.name = self.name
    dag.cregs = self.cregs.copy()
    dag.qregs = self.qregs.copy()
    for node in self.get_nodes():
        dag._multi_graph.add_node(node.copy())
    for edges in self.get_all_edges():
        dag._multi_graph.add_edge(edges[0], edges[1], edges[2])
    return dag","for edges in self.get_all_edges():
    dag._multi_graph.add_edge(edges[0], edges[1], edges[2])","['for edges in self.get_all_edges():\n    (edges_0, edges_1, edges_2, *_) = edges\n    dag._multi_graph.add_edge(edges_0, edges_1, edges_2)', 'for (edges_0, edges_1, edges_2, *edges_len) in self.get_all_edges():\n    dag._multi_graph.add_edge(edges_0, edges_1, edges_2)']",no_found,0
pydicom,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pydicom/pydicom/tests/test_util.py,https://github.com/pydicom/pydicom/tree/master/pydicom/tests/test_util.py,TestLeanRead,test_implicit_little$388,"def test_implicit_little(self):
    p = get_testdata_file('MR_small_implicit.dcm')
    ds = dcmread(p)
    assert ds.file_meta.TransferSyntaxUID == ImplicitVRLittleEndian
    with dicomfile(p) as ds:
        assert ds.preamble is not None
        for elem in ds:
            if elem[0] == (32736, 16):
                assert elem[2] == 8192","for elem in ds:
    if elem[0] == (32736, 16):
        assert elem[2] == 8192","['for elem in ds:\n    (elem_0, _, elem_2, *elem_relemmaining) = elem\n    if elem_0 == (32736, 16):\n        assert elem_2 == 8192', 'for (elem_0, elem_1, elem_2, *elem_len) in ds:\n    if \n    elem_0 == (32736, 16):\n        assert \n        elem_2 == 8192']",no_found,0
python-ternary,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-ternary/examples/scatter_colorbar.py,https://github.com/marcharper/python-ternary/tree/master/examples/scatter_colorbar.py,,_energy_to_enthalpy$37,"def _energy_to_enthalpy(energy):
    """"""Converts energy to enthalpy.
    
    This function take the energies stored in the energy array and
    converts them to formation enthalpy.

    Parameters
    ---------
    energy : list of lists of floats
    
    Returns
    -------
    enthalpy : list of lists containing the enthalpies.
    """"""
    pureA = [energy[0][0], energy[0][1]]
    pureB = [energy[1][0], energy[1][1]]
    pureC = [energy[2][0], energy[2][1]]
    enthalpy = []
    for en in energy:
        c = en[2]
        conc = [float(i) / sum(c) for i in c]
        CE = _en_to_enth(en[0], conc, pureA[0], pureB[0], pureC[0])
        VASP = _en_to_enth(en[1], conc, pureA[1], pureB[1], pureC[1])
        enthalpy.append([CE, VASP, c])
    return enthalpy","for en in energy:
    c = en[2]
    conc = [float(i) / sum(c) for i in c]
    CE = _en_to_enth(en[0], conc, pureA[0], pureB[0], pureC[0])
    VASP = _en_to_enth(en[1], conc, pureA[1], pureB[1], pureC[1])
    enthalpy.append([CE, VASP, c])","['for en in energy:\n    (en_0, en_1, en_2, *_) = en\n    c = en_2\n    conc = [float(i) / sum(c) for i in c]\n    CE = _en_to_enth(en_0, conc, pureA[0], pureB[0], pureC[0])\n    VASP = _en_to_enth(en_1, conc, pureA[1], pureB[1], pureC[1])\n    enthalpy.append([CE, VASP, c])', 'for (en_0, en_1, en_2, *en_len) in energy:\n    c = \n    en_2\n    conc = [float(i) / sum(c) for i in c]\n    CE = _en_to_enth(\n    en_0, conc, pureA[0], pureB[0], pureC[0])\n    VASP = _en_to_enth(\n    en_1, conc, pureA[1], pureB[1], pureC[1])\n    enthalpy.append([CE, VASP, c])']",no_found,0
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):

    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(set((i.key_number for i in midi_obj.key_signature_changes)))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start) for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(max_pos)]
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j)) for (i, j) in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)
    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for (idx, inst) in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program, note.pitch + 128 if inst.is_drum else note.pitch, enc_dur(max(1, time_to_pos(note.end - note.start))), enc_vel(note.velocity), info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) * math.log2(x / tot) for x in start_distribution))
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(start_ppl)
    encoding.sort()
    (encoding, is_major) = normalize_to_c_major(encoding)
    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]
    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry = []
    chord_int = 2
    for (chord_idx, chord) in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(boundry) >= 2, f'segement must start and end in chords: {target_chords}'
    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i] if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch + encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch + encoding[i][3] % 12, *encoding[i][4:])
    lead_notes = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    lead_chords = infer_chords_for_sequence(lead_notes, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)
    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        (src_strs, tgt_strs) = ([], [])
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for (note_idx, note) in enumerate(notes):
                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                elif dec_dur(note[4]) >= pos_resolution:
                    pitch_type = note[3] % 12
                    if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
                        src_words.append('AUT')
                    else:
                        src_words.append('HALF')
                else:
                    src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return (src_strs, tgt_strs)
    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue
        if cur_len + last_len >= target_len:
            if cur_len + last_len <= max_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    (src_strs, tgt_strs) = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment
    if max_notes >= last_len >= min_notes:
        (src_strs, tgt_strs) = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs
    assert len(src_str_list) == len(tgt_str_list)
    return (src_str_list, tgt_str_list, get_hash(encoding))","for enc in encoding:
    if boundry_idx >= len(boundry):
        break
    cur_pos = bar_to_pos[enc[0]] + enc[1]
    while cur_pos >= boundry[boundry_idx]:
        boundry_idx += 1
        if boundry_idx >= len(boundry):
            break
        segments.append([])
    if len(segments):
        segments[-1].append(enc)","['for enc in encoding:\n    (_, enc_1, *enc_rencmaining) = enc\n    if boundry_idx >= len(boundry):\n        break\n    cur_pos = bar_to_pos[enc[0]] + enc_1\n    while cur_pos >= boundry[boundry_idx]:\n        boundry_idx += 1\n        if boundry_idx >= len(boundry):\n            break\n        segments.append([])\n    if len(segments):\n        segments[-1].append(enc)']",no_found,0
pynguin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pynguin/pynguin/testcase/statement.py,https://github.com/se2p/pynguin/tree/master/pynguin/testcase/statement.py,DictStatement,get_variable_references$681,"def get_variable_references(self) -> Set[vr.VariableReference]:
    references = set()
    references.add(self.ret_val)
    for entry in self._elements:
        references.add(entry[0])
        references.add(entry[1])
    return references","for entry in self._elements:
    references.add(entry[0])
    references.add(entry[1])","['for entry in self._elements:\n    (entry_0, entry_1, *_) = entry\n    references.add(entry_0)\n    references.add(entry_1)', 'for (entry_0, entry_1, *entry_len) in self._elements:\n    references.add(entry_0)\n    references.add(entry_1)']",no_found,0
djongo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/djongo/tests/django_tests/tests/v22/tests/template_tests/test_custom.py,https://github.com/nesdis/djongo/tree/master/tests/django_tests/tests/v22/tests/template_tests/test_custom.py,InclusionTagTests,test_inclusion_tags$159,"def test_inclusion_tags(self):
    c = Context({'value': 42})
    templates = [('{% load inclusion %}{% inclusion_no_params %}', 'inclusion_no_params - Expected result\n'), ('{% load inclusion %}{% inclusion_one_param 37 %}', 'inclusion_one_param - Expected result: 37\n'), ('{% load inclusion %}{% inclusion_explicit_no_context 37 %}', 'inclusion_explicit_no_context - Expected result: 37\n'), ('{% load inclusion %}{% inclusion_no_params_with_context %}', 'inclusion_no_params_with_context - Expected result (context value: 42)\n'), ('{% load inclusion %}{% inclusion_params_and_context 37 %}', 'inclusion_params_and_context - Expected result (context value: 42): 37\n'), ('{% load inclusion %}{% inclusion_two_params 37 42 %}', 'inclusion_two_params - Expected result: 37, 42\n'), ('{% load inclusion %}{% inclusion_one_default 37 %}', 'inclusion_one_default - Expected result: 37, hi\n'), ('{% load inclusion %}{% inclusion_one_default 37 two=""hello"" %}', 'inclusion_one_default - Expected result: 37, hello\n'), ('{% load inclusion %}{% inclusion_one_default one=99 two=""hello"" %}', 'inclusion_one_default - Expected result: 99, hello\n'), ('{% load inclusion %}{% inclusion_one_default 37 42 %}', 'inclusion_one_default - Expected result: 37, 42\n'), ('{% load inclusion %}{% inclusion_unlimited_args 37 %}', 'inclusion_unlimited_args - Expected result: 37, hi\n'), ('{% load inclusion %}{% inclusion_unlimited_args 37 42 56 89 %}', 'inclusion_unlimited_args - Expected result: 37, 42, 56, 89\n'), ('{% load inclusion %}{% inclusion_only_unlimited_args %}', 'inclusion_only_unlimited_args - Expected result: \n'), ('{% load inclusion %}{% inclusion_only_unlimited_args 37 42 56 89 %}', 'inclusion_only_unlimited_args - Expected result: 37, 42, 56, 89\n'), ('{% load inclusion %}{% inclusion_unlimited_args_kwargs 37 40|add:2 56 eggs=""scrambled"" four=1|add:3 %}', 'inclusion_unlimited_args_kwargs - Expected result: 37, 42, 56 / eggs=scrambled, four=4\n')]
    for entry in templates:
        t = self.engine.from_string(entry[0])
        self.assertEqual(t.render(c), entry[1])","for entry in templates:
    t = self.engine.from_string(entry[0])
    self.assertEqual(t.render(c), entry[1])","['for entry in templates:\n    (entry_0, entry_1, *_) = entry\n    t = self.engine.from_string(entry_0)\n    self.assertEqual(t.render(c), entry_1)', 'for (entry_0, entry_1, *entry_len) in templates:\n    t = self.engine.from_string(entry_0)\n    self.assertEqual(t.render(c), entry_1)']",no_found,0
ottertune,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ottertune/server/analysis/simulation.py,https://github.com/cmu-db/ottertune/tree/master/server/analysis/simulation.py,,gpr_new$280,"def gpr_new(env, config, n_loops=100):
    model_name = 'BasicGP'
    model_opt_frequency = 0
    model_kwargs = {}
    model_kwargs['model_learning_rate'] = 0.001
    model_kwargs['model_maxiter'] = 5000
    opt_kwargs = {}
    opt_kwargs['learning_rate'] = 0.01
    opt_kwargs['maxiter'] = 500
    results = []
    x_axis = []
    memory = ReplayMemory()
    num_samples = config['num_samples']
    num_collections = config['num_collections']
    X_min = np.zeros(env.knob_dim)
    X_max = np.ones(env.knob_dim)
    X_bounds = [X_min, X_max]
    opt_kwargs['bounds'] = X_bounds
    for _ in range(num_collections):
        action = np.random.rand(env.knob_dim)
        (reward, _) = env.simulate(action)
        memory.push(action, reward)
    for i in range(n_loops):
        X_samples = np.random.rand(num_samples, env.knob_dim)
        if i >= 5:
            (actions, rewards) = memory.get_all()
            tuples = tuple(zip(actions, rewards))
            top10 = heapq.nlargest(10, tuples, key=lambda e: e[1])
            for entry in top10:
                X_samples = np.vstack((X_samples, np.array(entry[0]) * 0.97 + 0.01))
        (actions, rewards) = memory.get_all()
        ucb_beta = config['beta']
        opt_kwargs['ucb_beta'] = ucb.get_ucb_beta(ucb_beta, scale=config['scale'], t=i + 1.0, ndim=env.knob_dim)
        if model_opt_frequency > 0:
            optimize_hyperparams = i % model_opt_frequency == 0
            if not optimize_hyperparams:
                model_kwargs['hyperparameters'] = hyperparameters
        else:
            optimize_hyperparams = False
            model_kwargs['hyperparameters'] = None
        model_kwargs['optimize_hyperparameters'] = optimize_hyperparams
        (X_new, ypred, _, hyperparameters) = run_optimize(np.array(actions), -np.array(rewards), X_samples, model_name, opt_kwargs, model_kwargs)
        sort_index = np.argsort(ypred.squeeze())
        X_new = X_new[sort_index]
        ypred = ypred[sort_index].squeeze()
        action = X_new[0]
        (reward, _) = env.simulate(action)
        memory.push(action, reward)
        LOG.info('loop: %d reward: %f', i, reward[0])
        results.append(reward)
        x_axis.append(i + 1)
    return (np.array(results), np.array(x_axis))","for entry in top10:
    X_samples = np.vstack((X_samples, np.array(entry[0]) * 0.97 + 0.01))","['for entry in top10:\n    (entry_0, *entry_rentrymaining) = entry\n    X_samples = np.vstack((X_samples, np.array(entry_0) * 0.97 + 0.01))', 'for (entry_0, *entry_len) in top10:\n    X_samples = np.vstack((X_samples, np.array(entry_0) * 0.97 + 0.01))']",no_found,0
FIR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FIR/fir_email/views.py,https://github.com/certsocietegenerale/FIR/tree/master/fir_email/views.py,,smime_configuration$11,"def smime_configuration(request):
    form = SMIMECertificateForm(request.POST, user=request.user)
    if form.is_valid():
        form.save()
    else:
        for error in form.errors.items():
            messages.error(request, error[1])
    return redirect('user:profile')","for error in form.errors.items():
    messages.error(request, error[1])","['for error in form.errors.items():\n    (_, error_1, *error_rerrormaining) = error\n    messages.error(request, error_1)', 'for (error_0, error_1, *error_len) in form.errors.items():\n    messages.error(request, error_1)']",no_found,0
sympy_gamma,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy_gamma/app/views.py,https://github.com/sympy/sympy_gamma/tree/master/app/views.py,,random_example$132,"def random_example(request):
    examples = []
    for category in EXAMPLES:
        for subcategory in category[1]:
            for example in subcategory[1]:
                if isinstance(example, tuple):
                    examples.append(example[1])
                else:
                    examples.append(example)
    return redirect('input/?i=' + six.moves.urllib.parse.quote(random.choice(examples)))","for example in subcategory[1]:
    if isinstance(example, tuple):
        examples.append(example[1])
    else:
        examples.append(example)","['for example in subcategory[1]:\n    (_, example_1, *example_rexamplemaining) = example\n    if isinstance(example, tuple):\n        examples.append(example_1)\n    else:\n        examples.append(example)']",no_found,0
pithos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pithos/pithos/pithos.py,https://github.com/pithos/pithos/tree/master/pithos/pithos.py,PithosWindow,station_added$1002,"def station_added(self, station, user_data):
    (music_type, description) = user_data
    for existing_station in self.stations_model:
        if existing_station[0].id == station.id:
            self.station_already_exists(existing_station[0], description, music_type, self)
            return
    self.pandora.stations.append(station)
    self.stations_model.insert_with_valuesv(0, (0, 1, 2), (station, station.name, 0))
    self.emit('station-added', station)
    self.station_changed(station)","for existing_station in self.stations_model:
    if existing_station[0].id == station.id:
        self.station_already_exists(existing_station[0], description, music_type, self)
        return","['for existing_station in self.stations_model:\n    (existing_station_0, *existing_station_rexisting_stationmaining) = existing_station\n    if existing_station_0.id == station.id:\n        self.station_already_exists(existing_station_0, description, music_type, self)\n        return', 'for (existing_station_0, *existing_station_len) in self.stations_model:\n    if \n    existing_station_0.id == station.id:\n        self.station_already_exists(\n        existing_station_0, description, music_type, self)\n        return']",no_found,0
kivy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kivy/kivy/atlas.py,https://github.com/kivy/kivy/tree/master/kivy/atlas.py,Atlas,create$229,"def create(outname, filenames, size, padding=2, use_path=False):
    """"""This method can be used to create an atlas manually from a set of
        images.

        :Parameters:
            `outname`: str
                Basename to use for ``.atlas`` creation and ``-<idx>.png``
                associated images.
            `filenames`: list
                List of filenames to put in the atlas.
            `size`: int or list (width, height)
                Size of the atlas image. If the size is not large enough to
                fit all of the source images, more atlas images will created
                as required.
            `padding`: int, defaults to 2
                Padding to put around each image.

                Be careful. If you're using a padding < 2, you might have
                issues with the borders of the images. Because of the OpenGL
                linearization, it might use the pixels of the adjacent image.

                If you're using a padding >= 2, we'll automatically generate a
                ""border"" of 1px around your image. If you look at
                the result, don't be scared if the image inside is not
                exactly the same as yours :).

            `use_path`: bool, defaults to False
                If True, the relative path of the source png
                file names will be included in the atlas ids rather
                that just in the file names. Leading dots and slashes will be
                excluded and all other slashes in the path will be replaced
                with underscores. For example, if `use_path` is False
                (the default) and the file name is
                ``../data/tiles/green_grass.png``, the id will be
                ``green_grass``. If `use_path` is True, it will be
                ``data_tiles_green_grass``.

            .. versionchanged:: 1.8.0
                Parameter use_path added
        """"""
    try:
        from PIL import Image
    except ImportError:
        Logger.critical('Atlas: Imaging/PIL are missing')
        raise
    if isinstance(size, (tuple, list)):
        (size_w, size_h) = list(map(int, size))
    else:
        size_w = size_h = int(size)
    ims = list()
    for f in filenames:
        fp = open(f, 'rb')
        im = Image.open(fp)
        im.load()
        fp.close()
        ims.append((f, im))
    ims = sorted(ims, key=lambda im: im[1].size[0] * im[1].size[1], reverse=True)
    freeboxes = [(0, 0, 0, size_w, size_h)]
    numoutimages = 1
    fullboxes = []
    for imageinfo in ims:
        im = imageinfo[1]
        (imw, imh) = im.size
        imw += padding
        imh += padding
        if imw > size_w or imh > size_h:
            Logger.error('Atlas: image %s (%d by %d) is larger than the atlas size!' % (imageinfo[0], imw, imh))
            return
        inserted = False
        while not inserted:
            for (idx, fb) in enumerate(freeboxes):
                if fb[3] >= imw and fb[4] >= imh:
                    del freeboxes[idx]
                    if fb[3] > imw:
                        freeboxes.append((fb[0], fb[1] + imw, fb[2], fb[3] - imw, imh))
                    if fb[4] > imh:
                        freeboxes.append((fb[0], fb[1], fb[2] + imh, fb[3], fb[4] - imh))
                    freeboxes = sorted(freeboxes, key=lambda fb: fb[3] * fb[4])
                    fullboxes.append((im, fb[0], fb[1] + padding, fb[2] + padding, imw - padding, imh - padding, imageinfo[0]))
                    inserted = True
                    break
            if not inserted:
                freeboxes.append((numoutimages, 0, 0, size_w, size_h))
                numoutimages += 1
    Logger.info('Atlas: create an {0}x{1} rgba image'.format(size_w, size_h))
    outimages = [Image.new('RGBA', (size_w, size_h)) for i in range(0, int(numoutimages))]
    for fb in fullboxes:
        (x, y) = (fb[2], fb[3])
        out = outimages[fb[1]]
        out.paste(fb[0], (fb[2], fb[3]))
        (w, h) = fb[0].size
        if padding > 1:
            out.paste(fb[0].crop((0, 0, w, 1)), (x, y - 1))
            out.paste(fb[0].crop((0, h - 1, w, h)), (x, y + h))
            out.paste(fb[0].crop((0, 0, 1, h)), (x - 1, y))
            out.paste(fb[0].crop((w - 1, 0, w, h)), (x + w, y))
    for (idx, outimage) in enumerate(outimages):
        outimage.save('%s-%d.png' % (outname, idx))
    meta = {}
    for fb in fullboxes:
        fn = '%s-%d.png' % (basename(outname), fb[1])
        if fn not in meta:
            d = meta[fn] = {}
        else:
            d = meta[fn]
        if use_path:
            uid = splitext(fb[6])[0]
            uid = uid.lstrip('./\\')
            uid = uid.replace('/', '_').replace('\\', '_')
        else:
            uid = splitext(basename(fb[6]))[0]
        (x, y, w, h) = fb[2:6]
        d[uid] = (x, size_h - y - h, w, h)
    outfn = '%s.atlas' % outname
    with open(outfn, 'w') as fd:
        json.dump(meta, fd)
    return (outfn, meta)","for fb in fullboxes:
    (x, y) = (fb[2], fb[3])
    out = outimages[fb[1]]
    out.paste(fb[0], (fb[2], fb[3]))
    (w, h) = fb[0].size
    if padding > 1:
        out.paste(fb[0].crop((0, 0, w, 1)), (x, y - 1))
        out.paste(fb[0].crop((0, h - 1, w, h)), (x, y + h))
        out.paste(fb[0].crop((0, 0, 1, h)), (x - 1, y))
        out.paste(fb[0].crop((w - 1, 0, w, h)), (x + w, y))","['for fb in fullboxes:\n    (fb_0, _, fb_2, fb_3, *_) = fb\n    (x, y) = (fb_2, fb_3)\n    out = outimages[fb[1]]\n    out.paste(fb_0, (fb_2, fb_3))\n    (w, h) = fb_0.size\n    if padding > 1:\n        out.paste(fb_0.crop((0, 0, w, 1)), (x, y - 1))\n        out.paste(fb_0.crop((0, h - 1, w, h)), (x, y + h))\n        out.paste(fb_0.crop((0, 0, 1, h)), (x - 1, y))\n        out.paste(fb_0.crop((w - 1, 0, w, h)), (x + w, y))', 'for (fb_0, fb_1, fb_2, fb_3, *fb_len) in fullboxes:\n    (x, y) = (\n    fb_2, \n    fb_3)\n    out = outimages[\n    fb_1]\n    out.paste(\n    fb_0, (\n    fb_2, \n    fb_3))\n    (w, h) = \n    fb_0.size\n    if padding > 1:\n        out.paste(\n        fb_0.crop((0, 0, w, 1)), (x, y - 1))\n        out.paste(\n        fb_0.crop((0, h - 1, w, h)), (x, y + h))\n        out.paste(\n        fb_0.crop((0, 0, 1, h)), (x - 1, y))\n        out.paste(\n        fb_0.crop((w - 1, 0, w, h)), (x + w, y))']",no_found,0
TSD,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TSD/mmdet/models/anchor_heads/fovea_head.py,https://github.com/Sense-X/TSD/tree/master/mmdet/models/anchor_heads/fovea_head.py,FoveaHead,get_points$181,"def get_points(self, featmap_sizes, dtype, device, flatten=False):
    points = []
    for featmap_size in featmap_sizes:
        x_range = torch.arange(featmap_size[1], dtype=dtype, device=device) + 0.5
        y_range = torch.arange(featmap_size[0], dtype=dtype, device=device) + 0.5
        (y, x) = torch.meshgrid(y_range, x_range)
        if flatten:
            points.append((y.flatten(), x.flatten()))
        else:
            points.append((y, x))
    return points","for featmap_size in featmap_sizes:
    x_range = torch.arange(featmap_size[1], dtype=dtype, device=device) + 0.5
    y_range = torch.arange(featmap_size[0], dtype=dtype, device=device) + 0.5
    (y, x) = torch.meshgrid(y_range, x_range)
    if flatten:
        points.append((y.flatten(), x.flatten()))
    else:
        points.append((y, x))","['for featmap_size in featmap_sizes:\n    (featmap_size_0, featmap_size_1, *_) = featmap_size\n    x_range = torch.arange(featmap_size_1, dtype=dtype, device=device) + 0.5\n    y_range = torch.arange(featmap_size_0, dtype=dtype, device=device) + 0.5\n    (y, x) = torch.meshgrid(y_range, x_range)\n    if flatten:\n        points.append((y.flatten(), x.flatten()))\n    else:\n        points.append((y, x))', 'for (featmap_size_0, featmap_size_1, *featmap_size_len) in featmap_sizes:\n    x_range = torch.arange(featmap_size_1, dtype=dtype, device=device) + 0.5\n    y_range = torch.arange(featmap_size_0, dtype=dtype, device=device) + 0.5\n    (y, x) = torch.meshgrid(y_range, x_range)\n    if flatten:\n        points.append((y.flatten(), x.flatten()))\n    else:\n        points.append((y, x))']",no_found,0
virt-manager,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/virt-manager/virtinst/install/installer.py,https://github.com/virt-manager/virt-manager/tree/master/virtinst/install/installer.py,Installer,_prepare_cloudinit$371,"def _prepare_cloudinit(self, guest, meter):
    scratchdir = InstallerTreeMedia.make_scratchdir(guest)
    filepairs = cloudinit.create_files(scratchdir, self._cloudinit_data)
    for filepair in filepairs:
        self._tmpfiles.append(filepair[0])
    iso = perform_cdrom_injections(filepairs, scratchdir, cloudinit=True)
    self._tmpfiles.append(iso)
    iso = self._upload_media(guest, meter, [iso])[0]
    self._add_unattended_install_cdrom_device(guest, iso)","for filepair in filepairs:
    self._tmpfiles.append(filepair[0])","['for filepair in filepairs:\n    (filepair_0, *filepair_rfilepairmaining) = filepair\n    self._tmpfiles.append(filepair_0)', 'for (filepair_0, *filepair_len) in filepairs:\n    self._tmpfiles.append(filepair_0)']",no_found,0
few-shot-vid2vid,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/few-shot-vid2vid/data/image_folder.py,https://github.com/NVlabs/few-shot-vid2vid/tree/master/data/image_folder.py,,make_grouped_dataset$63,"def make_grouped_dataset(dir):
    images = []
    assert os.path.isdir(dir), '%s is not a valid directory' % dir
    fnames = sorted(os.walk(dir))
    for fname in sorted(fnames):
        paths = []
        root = fname[0]
        for f in sorted(fname[2]):
            if is_image_file(f):
                paths.append(os.path.join(root, f))
        if len(paths) > 0:
            images.append(paths)
    return images","for fname in sorted(fnames):
    paths = []
    root = fname[0]
    for f in sorted(fname[2]):
        if is_image_file(f):
            paths.append(os.path.join(root, f))
    if len(paths) > 0:
        images.append(paths)","['for fname in sorted(fnames):\n    (fname_0, _, fname_2, *fname_rfnamemaining) = fname\n    paths = []\n    root = fname_0\n    for f in sorted(fname_2):\n        if is_image_file(f):\n            paths.append(os.path.join(root, f))\n    if len(paths) > 0:\n        images.append(paths)', 'for (fname_0, fname_1, fname_2, *fname_len) in sorted(fnames):\n    paths = []\n    root = \n    fname_0\n    for f in sorted(\n    fname_2):\n        if is_image_file(f):\n            paths.append(os.path.join(root, f))\n    if len(paths) > 0:\n        images.append(paths)']",no_found,0
cocoNLP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cocoNLP/dist/cocoNLP-0.0.9/cocoNLP/config/phrase/rake.py,https://github.com/fighting41love/cocoNLP/tree/master/dist/cocoNLP-0.0.9/cocoNLP/config/phrase/rake.py,Rake,_get_phrase_list_from_words$218,"def _get_phrase_list_from_words(self, word_list, min_len, max_len):
    """"""Method to create contender phrases from the list of words that form
        a sentence by dropping stopwords and punctuations and grouping the left
        words into phrases. Only phrases in the given length range (both limits
        inclusive) would be considered to build co-occurrence matrix. Ex:

        Sentence: Red apples, are good in flavour.
        List of words: ['red', 'apples', "","", 'are', 'good', 'in', 'flavour']
        List after dropping punctuations and stopwords.
        List of words: ['red', 'apples', *, *, good, *, 'flavour']
        List of phrases: [('red', 'apples'), ('good',), ('flavour',)]

        List of phrases with a correct length:
        For the range [1, 2]: [('red', 'apples'), ('good',), ('flavour',)]
        For the range [1, 1]: [('good',), ('flavour',)]
        For the range [2, 2]: [('red', 'apples')]

        :param word_list: List of words which form a sentence when joined in
                          the same order.
        :return: List of contender phrases that are formed after dropping
                 stopwords and punctuations.
        """"""
    groups = groupby(word_list, lambda x: x not in self.to_ignore)
    phrases = []
    for group in groups:
        tmp = tuple(group[1])
        len_g1 = len(list(tmp))
        if group[0] and len_g1 >= min_len and (len_g1 <= max_len):
            phrases.append(tuple(tmp))
    return list(filter(lambda x: self.min_length <= len(x) <= self.max_length, phrases))","for group in groups:
    tmp = tuple(group[1])
    len_g1 = len(list(tmp))
    if group[0] and len_g1 >= min_len and (len_g1 <= max_len):
        phrases.append(tuple(tmp))","['for group in groups:\n    (group_0, group_1, *_) = group\n    tmp = tuple(group_1)\n    len_g1 = len(list(tmp))\n    if group_0 and len_g1 >= min_len and (len_g1 <= max_len):\n        phrases.append(tuple(tmp))', 'for (group_0, group_1, *group_len) in groups:\n    tmp = tuple(\n    group_1)\n    len_g1 = len(list(tmp))\n    if \n    group_0 and len_g1 >= min_len and (len_g1 <= max_len):\n        phrases.append(tuple(tmp))']",no_found,0
CrackMapExec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CrackMapExec/cme/protocols/smb/db_navigator.py,https://github.com/byt3bl33d3r/CrackMapExec/tree/master/cme/protocols/smb/db_navigator.py,navigator,display_groups$29,"def display_groups(self, groups):
    data = [['GroupID', 'Domain', 'Name', 'Members']]
    for group in groups:
        groupID = group[0]
        domain = group[1]
        name = group[2]
        members = len(self.db.get_group_relations(groupID=groupID))
        data.append([groupID, domain, name, members])
    self.print_table(data, title='Groups')","for group in groups:
    groupID = group[0]
    domain = group[1]
    name = group[2]
    members = len(self.db.get_group_relations(groupID=groupID))
    data.append([groupID, domain, name, members])","['for group in groups:\n    (group_0, group_1, group_2, *_) = group\n    groupID = group_0\n    domain = group_1\n    name = group_2\n    members = len(self.db.get_group_relations(groupID=groupID))\n    data.append([groupID, domain, name, members])', 'for (group_0, group_1, group_2, *group_len) in groups:\n    groupID = \n    group_0\n    domain = \n    group_1\n    name = \n    group_2\n    members = len(self.db.get_group_relations(groupID=groupID))\n    data.append([groupID, domain, name, members])']",no_found,0
kickthemout,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kickthemout/kickthemout.py,https://github.com/k4m4/kickthemout/tree/master//kickthemout.py,,kickalloff$552,"def kickalloff():
    os.system('clear||cls')
    print('\n{}kickALLOff{} selected...{}\n'.format(RED, GREEN, END))
    global stopAnimation
    stopAnimation = False
    t = threading.Thread(target=scanningAnimation, args=('Hang on...',))
    t.daemon = True
    t.start()
    try:
        scanNetwork()
    except KeyboardInterrupt:
        shutdown()
    stopAnimation = True
    print('Target(s): ')
    for i in range(len(onlineIPs)):
        mac = ''
        for host in hostsList:
            if host[0] == onlineIPs[i]:
                mac = host[1]
        try:
            hostname = utils.socket.gethostbyaddr(onlineIPs[i])[0]
        except:
            hostname = 'N/A'
        vendor = resolveMac(mac)
        print('  [{}{}{}] {}{}{}\t{}{}\t{} ({}{}{}){}'.format(YELLOW, str(i), WHITE, RED, str(onlineIPs[i]), BLUE, mac, GREEN, vendor, YELLOW, hostname, GREEN, END))
    if options.packets is not None:
        print('\n{}Spoofing started... {}( {} pkts/min )'.format(GREEN, END, str(options.packets)))
    else:
        print('\n{}Spoofing started... {}'.format(GREEN, END))
    try:
        reScan = 0
        while True:
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    spoof.sendPacket(defaultInterfaceMac, defaultGatewayIP, host[0], host[1])
            reScan += 1
            if reScan == 4:
                reScan = 0
                scanNetwork()
            if options.packets is not None:
                time.sleep(60 / float(options.packets))
            else:
                time.sleep(10)
    except KeyboardInterrupt:
        print('\n{}Re-arping{} targets...{}'.format(RED, GREEN, END))
        reArp = 1
        while reArp != 10:
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    try:
                        spoof.sendPacket(defaultGatewayMac, defaultGatewayIP, host[0], host[1])
                    except KeyboardInterrupt:
                        pass
                    except:
                        runDebug()
            reArp += 1
            time.sleep(0.2)
        print('{}Re-arped{} targets successfully.{}'.format(RED, GREEN, END))","for host in hostsList:
    if host[0] != defaultGatewayIP:
        spoof.sendPacket(defaultInterfaceMac, defaultGatewayIP, host[0], host[1])","['for host in hostsList:\n    (host_0, host_1, *_) = host\n    if host_0 != defaultGatewayIP:\n        spoof.sendPacket(defaultInterfaceMac, defaultGatewayIP, host_0, host_1)', 'for (host_0, host_1, *host_len) in hostsList:\n    if \n    host_0 != defaultGatewayIP:\n        spoof.sendPacket(defaultInterfaceMac, defaultGatewayIP, \n        host_0, \n        host_1)']",no_found,0
kickthemout,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kickthemout/kickthemout.py,https://github.com/k4m4/kickthemout/tree/master//kickthemout.py,,kickalloff$552,"def kickalloff():
    os.system('clear||cls')
    print('\n{}kickALLOff{} selected...{}\n'.format(RED, GREEN, END))
    global stopAnimation
    stopAnimation = False
    t = threading.Thread(target=scanningAnimation, args=('Hang on...',))
    t.daemon = True
    t.start()
    try:
        scanNetwork()
    except KeyboardInterrupt:
        shutdown()
    stopAnimation = True
    print('Target(s): ')
    for i in range(len(onlineIPs)):
        mac = ''
        for host in hostsList:
            if host[0] == onlineIPs[i]:
                mac = host[1]
        try:
            hostname = utils.socket.gethostbyaddr(onlineIPs[i])[0]
        except:
            hostname = 'N/A'
        vendor = resolveMac(mac)
        print('  [{}{}{}] {}{}{}\t{}{}\t{} ({}{}{}){}'.format(YELLOW, str(i), WHITE, RED, str(onlineIPs[i]), BLUE, mac, GREEN, vendor, YELLOW, hostname, GREEN, END))
    if options.packets is not None:
        print('\n{}Spoofing started... {}( {} pkts/min )'.format(GREEN, END, str(options.packets)))
    else:
        print('\n{}Spoofing started... {}'.format(GREEN, END))
    try:
        reScan = 0
        while True:
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    spoof.sendPacket(defaultInterfaceMac, defaultGatewayIP, host[0], host[1])
            reScan += 1
            if reScan == 4:
                reScan = 0
                scanNetwork()
            if options.packets is not None:
                time.sleep(60 / float(options.packets))
            else:
                time.sleep(10)
    except KeyboardInterrupt:
        print('\n{}Re-arping{} targets...{}'.format(RED, GREEN, END))
        reArp = 1
        while reArp != 10:
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    try:
                        spoof.sendPacket(defaultGatewayMac, defaultGatewayIP, host[0], host[1])
                    except KeyboardInterrupt:
                        pass
                    except:
                        runDebug()
            reArp += 1
            time.sleep(0.2)
        print('{}Re-arped{} targets successfully.{}'.format(RED, GREEN, END))","for host in hostsList:
    if host[0] != defaultGatewayIP:
        try:
            spoof.sendPacket(defaultGatewayMac, defaultGatewayIP, host[0], host[1])
        except KeyboardInterrupt:
            pass
        except:
            runDebug()","['for host in hostsList:\n    (host_0, host_1, *_) = host\n    if host_0 != defaultGatewayIP:\n        try:\n            spoof.sendPacket(defaultGatewayMac, defaultGatewayIP, host_0, host_1)\n        except KeyboardInterrupt:\n            pass\n        except:\n            runDebug()', 'for (host_0, host_1, *host_len) in hostsList:\n    if \n    host_0 != defaultGatewayIP:\n        try:\n            spoof.sendPacket(defaultGatewayMac, defaultGatewayIP, \n            host_0, \n            host_1)\n        except KeyboardInterrupt:\n            pass\n        except:\n            runDebug()']",no_found,0
kickthemout,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kickthemout/kickthemout.py,https://github.com/k4m4/kickthemout/tree/master//kickthemout.py,,kickalloff$552,"def kickalloff():
    os.system('clear||cls')
    print('\n{}kickALLOff{} selected...{}\n'.format(RED, GREEN, END))
    global stopAnimation
    stopAnimation = False
    t = threading.Thread(target=scanningAnimation, args=('Hang on...',))
    t.daemon = True
    t.start()
    try:
        scanNetwork()
    except KeyboardInterrupt:
        shutdown()
    stopAnimation = True
    print('Target(s): ')
    for i in range(len(onlineIPs)):
        mac = ''
        for host in hostsList:
            if host[0] == onlineIPs[i]:
                mac = host[1]
        try:
            hostname = utils.socket.gethostbyaddr(onlineIPs[i])[0]
        except:
            hostname = 'N/A'
        vendor = resolveMac(mac)
        print('  [{}{}{}] {}{}{}\t{}{}\t{} ({}{}{}){}'.format(YELLOW, str(i), WHITE, RED, str(onlineIPs[i]), BLUE, mac, GREEN, vendor, YELLOW, hostname, GREEN, END))
    if options.packets is not None:
        print('\n{}Spoofing started... {}( {} pkts/min )'.format(GREEN, END, str(options.packets)))
    else:
        print('\n{}Spoofing started... {}'.format(GREEN, END))
    try:
        reScan = 0
        while True:
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    spoof.sendPacket(defaultInterfaceMac, defaultGatewayIP, host[0], host[1])
            reScan += 1
            if reScan == 4:
                reScan = 0
                scanNetwork()
            if options.packets is not None:
                time.sleep(60 / float(options.packets))
            else:
                time.sleep(10)
    except KeyboardInterrupt:
        print('\n{}Re-arping{} targets...{}'.format(RED, GREEN, END))
        reArp = 1
        while reArp != 10:
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    try:
                        spoof.sendPacket(defaultGatewayMac, defaultGatewayIP, host[0], host[1])
                    except KeyboardInterrupt:
                        pass
                    except:
                        runDebug()
            reArp += 1
            time.sleep(0.2)
        print('{}Re-arped{} targets successfully.{}'.format(RED, GREEN, END))","for host in hostsList:
    if host[0] == onlineIPs[i]:
        mac = host[1]","['for host in hostsList:\n    (host_0, host_1, *_) = host\n    if host_0 == onlineIPs[i]:\n        mac = host_1', 'for (host_0, host_1, *host_len) in hostsList:\n    if \n    host_0 == onlineIPs[i]:\n        mac = \n        host_1']",no_found,0
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/backup/custom_wl.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/backup/custom_wl.py,,restore_azure_wl$615,"def restore_azure_wl(cmd, client, resource_group_name, vault_name, recovery_config, rehydration_duration=15, rehydration_priority=None, use_secondary_region=None):
    recovery_config_object = cust_help.get_or_read_json(recovery_config)
    restore_mode = recovery_config_object['restore_mode']
    container_uri = recovery_config_object['container_uri']
    item_uri = recovery_config_object['item_uri']
    recovery_point_id = recovery_config_object['recovery_point_id']
    log_point_in_time = recovery_config_object['log_point_in_time']
    item_type = recovery_config_object['item_type']
    workload_type = recovery_config_object['workload_type']
    source_resource_id = recovery_config_object['source_resource_id']
    database_name = recovery_config_object['database_name']
    container_id = recovery_config_object['container_id']
    alternate_directory_paths = recovery_config_object['alternate_directory_paths']
    recovery_mode = recovery_config_object['recovery_mode']
    filepath = recovery_config_object['filepath']
    item = common.show_item(cmd, backup_protected_items_cf(cmd.cli_ctx), resource_group_name, vault_name, container_uri, item_uri, 'AzureWorkload')
    cust_help.validate_item(item)
    validate_wl_restore(item, item_type, restore_mode, recovery_mode)
    trigger_restore_properties = _get_restore_request_instance(item_type, log_point_in_time, None)
    if log_point_in_time is None:
        recovery_point = common.show_recovery_point(cmd, recovery_points_cf(cmd.cli_ctx), resource_group_name, vault_name, container_uri, item_uri, recovery_point_id, workload_type, 'AzureWorkload', use_secondary_region)
        if recovery_point is None:
            raise InvalidArgumentValueError('\n            Specified recovery point not found. Please check the recovery config file\n            or try removing --use-secondary-region if provided')
        common.fetch_tier_for_rp(recovery_point)
        if recovery_point.tier_type is not None and recovery_point.tier_type == 'VaultArchive':
            if rehydration_priority is None:
                raise InvalidArgumentValueError('The selected recovery point is in archive tier, provide additional\n                parameters of rehydration duration and rehydration priority.')
            trigger_restore_properties = _get_restore_request_instance(item_type, log_point_in_time, rehydration_priority)
            rehyd_duration = 'P' + str(rehydration_duration) + 'D'
            rehydration_info = RecoveryPointRehydrationInfo(rehydration_retention_duration=rehyd_duration, rehydration_priority=rehydration_priority)
            trigger_restore_properties.recovery_point_rehydration_info = rehydration_info
    trigger_restore_properties.recovery_type = restore_mode
    if container_id is not None:
        target_container_name = cust_help.get_protection_container_uri_from_id(container_id)
        target_resource_group = cust_help.get_resource_group_from_id(container_id)
        target_vault_name = cust_help.get_vault_from_arm_id(container_id)
        target_container = common.show_container(cmd, backup_protection_containers_cf(cmd.cli_ctx), target_container_name, target_resource_group, target_vault_name, 'AzureWorkload')
        setattr(trigger_restore_properties, 'target_virtual_machine_id', target_container.properties.source_resource_id)
    if restore_mode == 'AlternateLocation':
        if recovery_mode != 'FileRecovery':
            setattr(trigger_restore_properties, 'source_resource_id', source_resource_id)
            setattr(trigger_restore_properties, 'target_info', TargetRestoreInfo(overwrite_option='Overwrite', database_name=database_name, container_id=container_id))
            if 'sql' in item_type.lower():
                directory_map = []
                for i in alternate_directory_paths:
                    directory_map.append(SQLDataDirectoryMapping(mapping_type=i[0], source_path=i[1], source_logical_name=i[2], target_path=i[3]))
                setattr(trigger_restore_properties, 'alternate_directory_paths', directory_map)
        else:
            target_info = TargetRestoreInfo(overwrite_option='Overwrite', container_id=container_id, target_directory_for_file_restore=filepath)
            setattr(trigger_restore_properties, 'target_info', target_info)
            trigger_restore_properties.recovery_mode = recovery_mode
    if log_point_in_time is not None:
        log_point_in_time = datetime_type(log_point_in_time)
        time_range_list = _get_log_time_range(cmd, resource_group_name, vault_name, item, use_secondary_region)
        validate_log_point_in_time(log_point_in_time, time_range_list)
        setattr(trigger_restore_properties, 'point_in_time', log_point_in_time)
    if 'sql' in item_type.lower():
        setattr(trigger_restore_properties, 'should_use_alternate_target_location', True)
        setattr(trigger_restore_properties, 'is_non_recoverable', False)
    trigger_restore_request = RestoreRequestResource(properties=trigger_restore_properties)
    if use_secondary_region:
        if rehydration_priority is not None:
            raise MutuallyExclusiveArgumentError(""Archive restore isn't supported for secondary region."")
        vault = vaults_cf(cmd.cli_ctx).get(resource_group_name, vault_name)
        vault_location = vault.location
        azure_region = custom.secondary_region_map[vault_location]
        aad_client = aad_properties_cf(cmd.cli_ctx)
        filter_string = cust_help.get_filter_string({'backupManagementType': 'AzureWorkload'})
        aad_result = aad_client.get(azure_region, filter_string)
        rp_client = recovery_points_passive_cf(cmd.cli_ctx)
        crr_access_token = rp_client.get_access_token(vault_name, resource_group_name, fabric_name, container_uri, item_uri, recovery_point_id, aad_result).properties
        crr_client = cross_region_restore_cf(cmd.cli_ctx)
        trigger_restore_properties.region = azure_region
        trigger_crr_request = CrossRegionRestoreRequest(cross_region_restore_access_details=crr_access_token, restore_request=trigger_restore_properties)
        result = crr_client.begin_trigger(azure_region, trigger_crr_request, cls=cust_help.get_pipeline_response, polling=False).result()
        return cust_help.track_backup_crr_job(cmd.cli_ctx, result, azure_region, vault.id)
    result = client.begin_trigger(vault_name, resource_group_name, fabric_name, container_uri, item_uri, recovery_point_id, trigger_restore_request, cls=cust_help.get_pipeline_response, polling=False).result()
    return cust_help.track_backup_job(cmd.cli_ctx, result, vault_name, resource_group_name)","for i in alternate_directory_paths:
    directory_map.append(SQLDataDirectoryMapping(mapping_type=i[0], source_path=i[1], source_logical_name=i[2], target_path=i[3]))","['for i in alternate_directory_paths:\n    (i_0, i_1, i_2, i_3, *_) = i\n    directory_map.append(SQLDataDirectoryMapping(mapping_type=i_0, source_path=i_1, source_logical_name=i_2, target_path=i_3))', 'for (i_0, i_1, i_2, i_3, *i_len) in alternate_directory_paths:\n    directory_map.append(SQLDataDirectoryMapping(mapping_type=i_0, source_path=i_1, source_logical_name=i_2, target_path=i_3))']",no_found,0
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):

    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(set((i.key_number for i in midi_obj.key_signature_changes)))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start) for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(max_pos)]
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j)) for (i, j) in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)
    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for (idx, inst) in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program, note.pitch + 128 if inst.is_drum else note.pitch, enc_dur(max(1, time_to_pos(note.end - note.start))), enc_vel(note.velocity), info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) * math.log2(x / tot) for x in start_distribution))
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(start_ppl)
    encoding.sort()
    (encoding, is_major) = normalize_to_c_major(encoding)
    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]
    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry = []
    chord_int = 2
    for (chord_idx, chord) in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(boundry) >= 2, f'segement must start and end in chords: {target_chords}'
    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i] if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch + encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch + encoding[i][3] % 12, *encoding[i][4:])
    lead_notes = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    lead_chords = infer_chords_for_sequence(lead_notes, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)
    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        (src_strs, tgt_strs) = ([], [])
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for (note_idx, note) in enumerate(notes):
                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                elif dec_dur(note[4]) >= pos_resolution:
                    pitch_type = note[3] % 12
                    if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
                        src_words.append('AUT')
                    else:
                        src_words.append('HALF')
                else:
                    src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return (src_strs, tgt_strs)
    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue
        if cur_len + last_len >= target_len:
            if cur_len + last_len <= max_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    (src_strs, tgt_strs) = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment
    if max_notes >= last_len >= min_notes:
        (src_strs, tgt_strs) = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs
    assert len(src_str_list) == len(tgt_str_list)
    return (src_str_list, tgt_str_list, get_hash(encoding))","for i in encoding:
    if i[2] < 128:
        pitch_sum[i[-1]] += i[3]
        note_cnt[i[-1]] += 1","['for i in encoding:\n    (_, _, i_2, i_3, *_) = i\n    if i_2 < 128:\n        pitch_sum[i[-1]] += i_3\n        note_cnt[i[-1]] += 1']",no_found,0
WechatSogou,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WechatSogou/wechatsogou/tools.py,https://github.com/chyroc/WechatSogou/tree/master/wechatsogou/tools.py,,_replace_str_html$73,"def _replace_str_html(s):
    """"""html&quot;

    Args:
        s: 

    Returns:
        s: 
    """"""
    html_str_list = [('&#39;', ""'""), ('&quot;', '""'), ('&amp;', '&'), ('&yen;', ''), ('amp;', ''), ('&lt;', '<'), ('&gt;', '>'), ('&nbsp;', ' '), ('\\', '')]
    for i in html_str_list:
        s = s.replace(i[0], i[1])
    return s","for i in html_str_list:
    s = s.replace(i[0], i[1])","['for i in html_str_list:\n    (i_0, i_1, *_) = i\n    s = s.replace(i_0, i_1)', 'for (i_0, i_1, *i_len) in html_str_list:\n    s = s.replace(i_0, i_1)']",no_found,0
GWSL-Source,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GWSL-Source/manager.py,https://github.com/Opticos/GWSL-Source/tree/master//manager.py,,get_version$605,"def get_version(machine):
    try:
        machines = os.popen('wsl.exe -l -v').read()
        machines = re.sub('[^a-z A-Z0-9./\\n-]', '', machines).splitlines()
        machines2 = []
        wsl_1 = True
        for i in machines:
            b = ''.join(i).split()
            if 'VERSION' in b:
                wsl_1 = False
            if 'NAME' not in b and b != [] and (b != None):
                machines2.append(b)
        if wsl_1 == True:
            print('assuming wsl 1')
            return 1
        for i in machines2:
            if i[0] == machine:
                return int(i[2])
        return 1
    except:
        return 1","for i in machines2:
    if i[0] == machine:
        return int(i[2])","['for i in machines2:\n    (i_0, _, i_2, *i_rimaining) = i\n    if i_0 == machine:\n        return int(i_2)', 'for (i_0, i_1, i_2, *i_len) in machines2:\n    if \n    i_0 == machine:\n        return int(\n        i_2)']",no_found,0
sparkup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sparkup/sparkup.py,https://github.com/rstacruz/sparkup/tree/master//sparkup.py,Parser,_textmatify$404,"def _textmatify(self, output):
    """"""Returns a version of the output with TextMate placeholders in it.
        """"""
    matches = re.findall('(></)|("""")|(\\n\\s+)\\n|(.|\\s)', output)
    output = ''
    n = 1
    for i in matches:
        if i[0]:
            output += '>$%i</' % n
            n += 1
        elif i[1]:
            output += '""$%i""' % n
            n += 1
        elif i[2]:
            output += i[2] + '$%i\n' % n
            n += 1
        elif i[3]:
            output += i[3]
    output += '$0'
    return output","for i in matches:
    if i[0]:
        output += '>$%i</' % n
        n += 1
    elif i[1]:
        output += '""$%i""' % n
        n += 1
    elif i[2]:
        output += i[2] + '$%i\n' % n
        n += 1
    elif i[3]:
        output += i[3]","['for i in matches:\n    (i_0, i_1, i_2, i_3, *_) = i\n    if i_0:\n        output += \'>$%i</\' % n\n        n += 1\n    elif i_1:\n        output += \'""$%i""\' % n\n        n += 1\n    elif i_2:\n        output += i_2 + \'$%i\\n\' % n\n        n += 1\n    elif i_3:\n        output += i_3', 'for (i_0, i_1, i_2, i_3, *i_len) in matches:\n    if \n    i_0:\n        output += \'>$%i</\' % n\n        n += 1\n    elif \n    i_1:\n        output += \'""$%i""\' % n\n        n += 1\n    elif \n    i_2:\n        output += \n        i_2 + \'$%i\\n\' % n\n        n += 1\n    elif \n    i_3:\n        output += \n        i_3']",no_found,0
VideoSuperResolution,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VideoSuperResolution/VSR/Backend/TF/Framework/Trainer.py,https://github.com/LoSealL/VideoSuperResolution/tree/master/VSR/Backend/TF/Framework/Trainer.py,,_ensemble_reduce_mean$47,"def _ensemble_reduce_mean(outputs):
    results = []
    for i in outputs:
        outputs_ensemble = [i[0], np.rot90(i[1], 3, axes=[-3, -2]), np.rot90(i[2], 2, axes=[-3, -2]), np.rot90(i[3], 1, axes=[-3, -2]), np.flip(i[4], axis=-2), np.flip(np.rot90(i[5], 3, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[6], 2, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[7], 1, axes=[-3, -2]), axis=-2)]
        results.append(np.concatenate(outputs_ensemble).mean(axis=0, keepdims=True))
    return results","for i in outputs:
    outputs_ensemble = [i[0], np.rot90(i[1], 3, axes=[-3, -2]), np.rot90(i[2], 2, axes=[-3, -2]), np.rot90(i[3], 1, axes=[-3, -2]), np.flip(i[4], axis=-2), np.flip(np.rot90(i[5], 3, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[6], 2, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i[7], 1, axes=[-3, -2]), axis=-2)]
    results.append(np.concatenate(outputs_ensemble).mean(axis=0, keepdims=True))","['for i in outputs:\n    (i_0, i_1, i_2, i_3, i_4, i_5, i_6, i_7, *_) = i\n    outputs_ensemble = [i_0, np.rot90(i_1, 3, axes=[-3, -2]), np.rot90(i_2, 2, axes=[-3, -2]), np.rot90(i_3, 1, axes=[-3, -2]), np.flip(i_4, axis=-2), np.flip(np.rot90(i_5, 3, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i_6, 2, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i_7, 1, axes=[-3, -2]), axis=-2)]\n    results.append(np.concatenate(outputs_ensemble).mean(axis=0, keepdims=True))', 'for (i_0, i_1, i_2, i_3, i_4, i_5, i_6, i_7, *i_len) in outputs:\n    outputs_ensemble = [i_0, np.rot90(i_1, 3, axes=[-3, -2]), np.rot90(i_2, 2, axes=[-3, -2]), np.rot90(i_3, 1, axes=[-3, -2]), np.flip(i_4, axis=-2), np.flip(np.rot90(i_5, 3, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i_6, 2, axes=[-3, -2]), axis=-2), np.flip(np.rot90(i_7, 1, axes=[-3, -2]), axis=-2)]\n    results.append(np.concatenate(outputs_ensemble).mean(axis=0, keepdims=True))']",no_found,0
cats-blender-plugin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cats-blender-plugin/extern_tools/google_trans_new/google_trans_new.py,https://github.com/absolute-quantum/cats-blender-plugin/tree/master/extern_tools/google_trans_new/google_trans_new.py,google_translator,translate$111,"def translate(self, text, lang_tgt='auto', lang_src='auto', pronounce=False):
    print('\n\nDEBUG: Translating', text)
    try:
        lang = LANGUAGES[lang_src]
    except:
        lang_src = 'auto'
    try:
        lang = LANGUAGES[lang_tgt]
    except:
        lang_src = 'auto'
    text = str(text)
    if len(text) >= 5000:
        return 'Warning: Can only detect less than 5000 characters'
    if len(text) == 0:
        return ''
    headers = {'Referer': 'http://translate.google.{}/'.format(self.url_suffix), 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36', 'Content-Type': 'application/x-www-form-urlencoded;charset=utf-8'}
    freq = self._package_rpc(text, lang_src, lang_tgt)
    response = requests.Request(method='POST', url=self.url, data=freq, headers=headers)
    try:
        if self.proxies == None or type(self.proxies) != dict:
            self.proxies = {}
        with requests.Session() as s:
            s.proxies = self.proxies
            r = s.send(request=response.prepare(), verify=False, timeout=self.timeout)
        for line in r.iter_lines(chunk_size=1024):
            decoded_line = line.decode('utf-8')
            if 'MkEWBc' in decoded_line:
                try:
                    response = decoded_line + ']'
                    response = json.loads(response)
                    print('DEBUG1', response)
                    response = list(response)
                    response = json.loads(response[0][2])
                    response_ = list(response)
                    response = response_[1][0]
                    print('DEBUG2', response)
                    if len(response) == 1:
                        if len(response[0]) > 5:
                            sentences = response[0][5]
                            print('DEBUG3', sentences)
                        else:
                            sentences = response[0][0]
                            print('DEBUG4', sentences)
                            if pronounce == False:
                                return sentences
                            elif pronounce == True:
                                return [sentences, None, None]
                        translate_text = ''
                        translations = sentences[0]
                        if not translations:
                            return text
                        translate_text = translations[0]
                        if pronounce == False:
                            return translate_text
                        elif pronounce == True:
                            pronounce_src = response_[0][0]
                            pronounce_tgt = response_[1][0][0][1]
                            return [translate_text, pronounce_src, pronounce_tgt]
                    elif len(response) == 2:
                        sentences = []
                        for i in response:
                            sentences.append(i[0])
                        if pronounce == False:
                            return sentences
                        elif pronounce == True:
                            pronounce_src = response_[0][0]
                            pronounce_tgt = response_[1][0][0][1]
                            return [sentences, pronounce_src, pronounce_tgt]
                except Exception as e:
                    raise e
        r.raise_for_status()
    except requests.exceptions.ConnectTimeout as e:
        raise e
    except requests.exceptions.HTTPError as e:
        raise google_new_transError(tts=self, response=r)
    except requests.exceptions.RequestException as e:
        raise google_new_transError(tts=self)","for i in response:
    sentences.append(i[0])","['for i in response:\n    (i_0, *i_rimaining) = i\n    sentences.append(i_0)', 'for (i_0, *i_len) in response:\n    sentences.append(i_0)']",no_found,0
grokking-the-object-oriented-design-interview,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/grokking-the-object-oriented-design-interview/example-codes/chess/king.py,https://github.com/tssovi/grokking-the-object-oriented-design-interview/tree/master/example-codes/chess/king.py,King,get_threatened_positions$20,"def get_threatened_positions(self, board):
    positions = []
    for increment in King.SPOT_INCREMENTS:
        positions.append(board.spot_search_threat(self._position, self._color, increment[0], increment[1]))
    positions = [x for x in positions if x is not None]
    return positions","for increment in King.SPOT_INCREMENTS:
    positions.append(board.spot_search_threat(self._position, self._color, increment[0], increment[1]))","['for increment in King.SPOT_INCREMENTS:\n    (increment_0, increment_1, *_) = increment\n    positions.append(board.spot_search_threat(self._position, self._color, increment_0, increment_1))', 'for (increment_0, increment_1, *increment_len) in King.SPOT_INCREMENTS:\n    positions.append(board.spot_search_threat(self._position, self._color, increment_0, increment_1))']",no_found,0
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/tools/fix_annotations.py,https://github.com/nlplab/brat/tree/master/tools/fix_annotations.py,,correct_annotations$28,"def correct_annotations(orig_fn, ann_fn, change_fn):
    with annotation.TextAnnotations(ann_fn) as anns:
        orig_text = anns.get_document_text()
        with annotation.open_textfile(change_fn, 'r') as f:
            changed_text = f.read()
        diffs = diff_match_patch().diff_main(orig_text, changed_text)
        orig_offset = 0
        offsets = []
        for diff in diffs:
            kind = diff[0]
            text = diff[1]
            size = len(text)
            delta = size * kind
            offsets.append((orig_offset, delta))
            if kind != 1:
                orig_offset += size
        offsets = offsets[::-1]
        tbs = list(anns.get_textbounds())
        indices = []
        for (tbi, tb) in enumerate(tbs):
            for (spani, span) in enumerate(tb.spans):
                indices.append((span[0], tbi, spani, 0))
                indices.append((span[1], tbi, spani, 1))
        indices.sort(reverse=True)
        for (orig_offset, delta) in offsets:
            for index in indices:
                if index[0] < orig_offset:
                    break
                frag = list(tbs[index[1]].spans[index[2]])
                frag[index[3]] += delta
                tbs[index[1]].spans[index[2]] = tuple(frag)
        for tb in tbs:
            if isinstance(tb, annotation.TextBoundAnnotationWithText):
                tb.text = annotation.DISCONT_SEP.join((changed_text[start:end] for (start, end) in tb.spans))
    copy(change_fn, orig_fn)","for index in indices:
    if index[0] < orig_offset:
        break
    frag = list(tbs[index[1]].spans[index[2]])
    frag[index[3]] += delta
    tbs[index[1]].spans[index[2]] = tuple(frag)","['for index in indices:\n    (index_0, *index_rindexmaining) = index\n    if index_0 < orig_offset:\n        break\n    frag = list(tbs[index[1]].spans[index[2]])\n    frag[index[3]] += delta\n    tbs[index[1]].spans[index[2]] = tuple(frag)']",no_found,0
UER-py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/UER-py/uer/utils/mask.py,https://github.com/dbiir/UER-py/tree/master/uer/utils/mask.py,,mask_seq$5,"def mask_seq(src, tokenizer, whole_word_masking, span_masking, span_geo_prob, span_max_length):
    vocab = tokenizer.vocab
    PAD_ID = vocab.get(PAD_TOKEN)
    for i in range(len(src) - 1, -1, -1):
        if src[i] != PAD_ID:
            break
    src_no_pad = src[:i + 1]
    (tokens_index, src_no_pad) = create_index(src_no_pad, tokenizer, whole_word_masking, span_masking, span_geo_prob, span_max_length)
    if len(src_no_pad) < len(src):
        src = src_no_pad + (len(src) - len(src_no_pad)) * [PAD_ID]
    else:
        src = src_no_pad
    random.shuffle(tokens_index)
    num_to_predict = max(1, int(round(len(src_no_pad) * 0.15)))
    tgt_mlm = []
    for index_set in tokens_index:
        if len(tgt_mlm) >= num_to_predict:
            break
        if whole_word_masking:
            i = index_set[0]
            mask_len = index_set[1]
            if len(tgt_mlm) + mask_len > num_to_predict:
                continue
            for j in range(mask_len):
                token = src[i + j]
                tgt_mlm.append((i + j, token))
                prob = random.random()
                if prob < 0.8:
                    src[i + j] = vocab.get(MASK_TOKEN)
                elif prob < 0.9:
                    while True:
                        rdi = random.randint(1, len(vocab) - 1)
                        if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                            break
                    src[i + j] = rdi
        elif span_masking:
            i = index_set[0]
            span_len = index_set[1]
            if len(tgt_mlm) + span_len > num_to_predict:
                continue
            for j in range(span_len):
                token = src[i + j]
                tgt_mlm.append((i + j, token))
            prob = random.random()
            if prob < 0.8:
                for j in range(span_len):
                    src[i + j] = vocab.get(MASK_TOKEN)
            elif prob < 0.9:
                for j in range(span_len):
                    while True:
                        rdi = random.randint(1, len(vocab) - 1)
                        if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                            break
                    src[i + j] = rdi
        else:
            i = index_set[0]
            token = src[i]
            tgt_mlm.append((i, token))
            prob = random.random()
            if prob < 0.8:
                src[i] = vocab.get(MASK_TOKEN)
            elif prob < 0.9:
                while True:
                    rdi = random.randint(1, len(vocab) - 1)
                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                        break
                src[i] = rdi
    tgt_mlm = sorted(tgt_mlm, key=lambda x: x[0])
    return (src, tgt_mlm)","for index_set in tokens_index:
    if len(tgt_mlm) >= num_to_predict:
        break
    if whole_word_masking:
        i = index_set[0]
        mask_len = index_set[1]
        if len(tgt_mlm) + mask_len > num_to_predict:
            continue
        for j in range(mask_len):
            token = src[i + j]
            tgt_mlm.append((i + j, token))
            prob = random.random()
            if prob < 0.8:
                src[i + j] = vocab.get(MASK_TOKEN)
            elif prob < 0.9:
                while True:
                    rdi = random.randint(1, len(vocab) - 1)
                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                        break
                src[i + j] = rdi
    elif span_masking:
        i = index_set[0]
        span_len = index_set[1]
        if len(tgt_mlm) + span_len > num_to_predict:
            continue
        for j in range(span_len):
            token = src[i + j]
            tgt_mlm.append((i + j, token))
        prob = random.random()
        if prob < 0.8:
            for j in range(span_len):
                src[i + j] = vocab.get(MASK_TOKEN)
        elif prob < 0.9:
            for j in range(span_len):
                while True:
                    rdi = random.randint(1, len(vocab) - 1)
                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                        break
                src[i + j] = rdi
    else:
        i = index_set[0]
        token = src[i]
        tgt_mlm.append((i, token))
        prob = random.random()
        if prob < 0.8:
            src[i] = vocab.get(MASK_TOKEN)
        elif prob < 0.9:
            while True:
                rdi = random.randint(1, len(vocab) - 1)
                if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:
                    break
            src[i] = rdi","['for index_set in tokens_index:\n    (index_set_0, index_set_1, *_) = index_set\n    if len(tgt_mlm) >= num_to_predict:\n        break\n    if whole_word_masking:\n        i = index_set_0\n        mask_len = index_set_1\n        if len(tgt_mlm) + mask_len > num_to_predict:\n            continue\n        for j in range(mask_len):\n            token = src[i + j]\n            tgt_mlm.append((i + j, token))\n            prob = random.random()\n            if prob < 0.8:\n                src[i + j] = vocab.get(MASK_TOKEN)\n            elif prob < 0.9:\n                while True:\n                    rdi = random.randint(1, len(vocab) - 1)\n                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:\n                        break\n                src[i + j] = rdi\n    elif span_masking:\n        i = index_set_0\n        span_len = index_set_1\n        if len(tgt_mlm) + span_len > num_to_predict:\n            continue\n        for j in range(span_len):\n            token = src[i + j]\n            tgt_mlm.append((i + j, token))\n        prob = random.random()\n        if prob < 0.8:\n            for j in range(span_len):\n                src[i + j] = vocab.get(MASK_TOKEN)\n        elif prob < 0.9:\n            for j in range(span_len):\n                while True:\n                    rdi = random.randint(1, len(vocab) - 1)\n                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:\n                        break\n                src[i + j] = rdi\n    else:\n        i = index_set_0\n        token = src[i]\n        tgt_mlm.append((i, token))\n        prob = random.random()\n        if prob < 0.8:\n            src[i] = vocab.get(MASK_TOKEN)\n        elif prob < 0.9:\n            while True:\n                rdi = random.randint(1, len(vocab) - 1)\n                if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:\n                    break\n            src[i] = rdi', 'for (index_set_0, index_set_1, *index_set_len) in tokens_index:\n    if len(tgt_mlm) >= num_to_predict:\n        break\n    if whole_word_masking:\n        i = \n        index_set_0\n        mask_len = \n        index_set_1\n        if len(tgt_mlm) + mask_len > num_to_predict:\n            continue\n        for j in range(mask_len):\n            token = src[i + j]\n            tgt_mlm.append((i + j, token))\n            prob = random.random()\n            if prob < 0.8:\n                src[i + j] = vocab.get(MASK_TOKEN)\n            elif prob < 0.9:\n                while True:\n                    rdi = random.randint(1, len(vocab) - 1)\n                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:\n                        break\n                src[i + j] = rdi\n    elif span_masking:\n        i = \n        index_set_0\n        span_len = \n        index_set_1\n        if len(tgt_mlm) + span_len > num_to_predict:\n            continue\n        for j in range(span_len):\n            token = src[i + j]\n            tgt_mlm.append((i + j, token))\n        prob = random.random()\n        if prob < 0.8:\n            for j in range(span_len):\n                src[i + j] = vocab.get(MASK_TOKEN)\n        elif prob < 0.9:\n            for j in range(span_len):\n                while True:\n                    rdi = random.randint(1, len(vocab) - 1)\n                    if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:\n                        break\n                src[i + j] = rdi\n    else:\n        i = \n        index_set_0\n        token = src[i]\n        tgt_mlm.append((i, token))\n        prob = random.random()\n        if prob < 0.8:\n            src[i] = vocab.get(MASK_TOKEN)\n        elif prob < 0.9:\n            while True:\n                rdi = random.randint(1, len(vocab) - 1)\n                if rdi not in [vocab.get(CLS_TOKEN), vocab.get(SEP_TOKEN), vocab.get(MASK_TOKEN), PAD_ID]:\n                    break\n            src[i] = rdi']",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for info in infos:
    api_path = info[0]","['for info in infos:\n    (info_0, *info_rinfomaining) = info\n    api_path = info_0', 'for (info_0, *info_len) in infos:\n    api_path = \n    info_0']",no_found,0
pgmpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgmpy/pgmpy/tests/test_factors/test_continuous/test_CustomDistribution.py,https://github.com/pgmpy/pgmpy/tree/master/pgmpy/tests/test_factors/test_continuous/test_CustomDistribution.py,TestCustomDistributionMethods,test_copy$443,"def test_copy(self):
    copy1 = self.phi1.copy()
    copy2 = self.phi3.copy()
    copy4 = copy1.copy()
    copy5 = copy2.copy()
    self.assertEqual(copy1.variables, copy4.variables)
    self.assertEqual(copy1._pdf, copy4._pdf)
    self.assertEqual(copy2.variables, copy5.variables)
    self.assertEqual(copy2._pdf, copy5._pdf)
    copy1.variables = ['A', 'B']
    self.assertEqual(copy4.variables, self.phi1.variables)

    def pdf(a, b):
        return (a + b) / (a * a + b * b)
    copy1._pdf = pdf
    copy1_pdf = pdf
    self.assertEqual(copy4._pdf, self.phi1._pdf)
    copy4.variables = ['X', 'Y']
    self.assertEqual(copy1.variables, ['A', 'B'])
    copy4._pdf = lambda a, b: a + b
    for inp in np.random.rand(4, 2):
        self.assertEqual(copy1._pdf(inp[0], inp[1]), copy1_pdf(inp[0], inp[1]))
    copy2.reduce([('x', 7.7)])

    def reduced_pdf(y, z):
        return z * (np.power(7.7, 1) * np.power(y, 2)) / beta(7.7, y)
    self.assertEqual(copy5.variables, self.phi3.variables)
    self.assertEqual(copy5._pdf, self.phi3._pdf)
    copy5.reduce([('x', 11), ('z', 13)])
    self.assertEqual(copy2.variables, ['y', 'z'])
    for inp in np.random.rand(4, 2):
        self.assertEqual(copy2._pdf(inp[0], inp[1]), reduced_pdf(inp[0], inp[1]))","for inp in np.random.rand(4, 2):
    self.assertEqual(copy1._pdf(inp[0], inp[1]), copy1_pdf(inp[0], inp[1]))","['for inp in np.random.rand(4, 2):\n    (inp_0, inp_1, *_) = inp\n    self.assertEqual(copy1._pdf(inp_0, inp_1), copy1_pdf(inp_0, inp_1))', 'for (inp_0, inp_1, *inp_len) in np.random.rand(4, 2):\n    self.assertEqual(copy1._pdf(inp_0, inp_1), copy1_pdf(inp_0, inp_1))']",no_found,0
pgmpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pgmpy/pgmpy/tests/test_factors/test_continuous/test_CustomDistribution.py,https://github.com/pgmpy/pgmpy/tree/master/pgmpy/tests/test_factors/test_continuous/test_CustomDistribution.py,TestCustomDistributionMethods,test_copy$443,"def test_copy(self):
    copy1 = self.phi1.copy()
    copy2 = self.phi3.copy()
    copy4 = copy1.copy()
    copy5 = copy2.copy()
    self.assertEqual(copy1.variables, copy4.variables)
    self.assertEqual(copy1._pdf, copy4._pdf)
    self.assertEqual(copy2.variables, copy5.variables)
    self.assertEqual(copy2._pdf, copy5._pdf)
    copy1.variables = ['A', 'B']
    self.assertEqual(copy4.variables, self.phi1.variables)

    def pdf(a, b):
        return (a + b) / (a * a + b * b)
    copy1._pdf = pdf
    copy1_pdf = pdf
    self.assertEqual(copy4._pdf, self.phi1._pdf)
    copy4.variables = ['X', 'Y']
    self.assertEqual(copy1.variables, ['A', 'B'])
    copy4._pdf = lambda a, b: a + b
    for inp in np.random.rand(4, 2):
        self.assertEqual(copy1._pdf(inp[0], inp[1]), copy1_pdf(inp[0], inp[1]))
    copy2.reduce([('x', 7.7)])

    def reduced_pdf(y, z):
        return z * (np.power(7.7, 1) * np.power(y, 2)) / beta(7.7, y)
    self.assertEqual(copy5.variables, self.phi3.variables)
    self.assertEqual(copy5._pdf, self.phi3._pdf)
    copy5.reduce([('x', 11), ('z', 13)])
    self.assertEqual(copy2.variables, ['y', 'z'])
    for inp in np.random.rand(4, 2):
        self.assertEqual(copy2._pdf(inp[0], inp[1]), reduced_pdf(inp[0], inp[1]))","for inp in np.random.rand(4, 2):
    self.assertEqual(copy2._pdf(inp[0], inp[1]), reduced_pdf(inp[0], inp[1]))","['for inp in np.random.rand(4, 2):\n    (inp_0, inp_1, *_) = inp\n    self.assertEqual(copy2._pdf(inp_0, inp_1), reduced_pdf(inp_0, inp_1))', 'for (inp_0, inp_1, *inp_len) in np.random.rand(4, 2):\n    self.assertEqual(copy2._pdf(inp_0, inp_1), reduced_pdf(inp_0, inp_1))']",no_found,0
dl-4-tsc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dl-4-tsc/classifiers/mcnn.py,https://github.com/hfawaz/dl-4-tsc/tree/master/classifiers/mcnn.py,Classifier_MCNN,split_input_for_model$410,"def split_input_for_model(self, x, input_shapes):
    res = []
    indx = 0
    for input_shape in input_shapes:
        res.append(x[:, indx:indx + input_shape[0], :])
        indx = indx + input_shape[0]
    return res","for input_shape in input_shapes:
    res.append(x[:, indx:indx + input_shape[0], :])
    indx = indx + input_shape[0]","['for input_shape in input_shapes:\n    (input_shape_0, *input_shape_rinput_shapemaining) = input_shape\n    res.append(x[:, indx:indx + input_shape_0, :])\n    indx = indx + input_shape_0', 'for (input_shape_0, *input_shape_len) in input_shapes:\n    res.append(x[:, indx:indx + \n    input_shape_0, :])\n    indx = indx + \n    input_shape_0']",no_found,0
pytorch_HMR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch_HMR/src/dataloader/AICH_dataloader.py,https://github.com/MandyMo/pytorch_HMR/tree/master/src/dataloader/AICH_dataloader.py,AICH_dataloader,_collide_heavily$89,"def _collide_heavily(box, boxs):
    for it in boxs:
        if get_rectangle_intersect_ratio(box[0], box[1], it[0], it[1]) > self.max_intersec_ratio:
            return True
    return False","for it in boxs:
    if get_rectangle_intersect_ratio(box[0], box[1], it[0], it[1]) > self.max_intersec_ratio:
        return True","['for it in boxs:\n    (it_0, it_1, *_) = it\n    if get_rectangle_intersect_ratio(box[0], box[1], it_0, it_1) > self.max_intersec_ratio:\n        return True', 'for (it_0, it_1, *it_len) in boxs:\n    if get_rectangle_intersect_ratio(box[0], box[1], it_0, it_1) > self.max_intersec_ratio:\n        return True']",no_found,0
pyradio,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyradio/pyradio/radio.py,https://github.com/coderholic/pyradio/tree/master/pyradio/radio.py,PyRadio,is_search_mode$3875,"def is_search_mode(self, a_mode):
    for it in self._search_modes.items():
        if it[1] == a_mode:
            return True
    return False","for it in self._search_modes.items():
    if it[1] == a_mode:
        return True","['for it in self._search_modes.items():\n    (_, it_1, *it_ritmaining) = it\n    if it_1 == a_mode:\n        return True', 'for (it_0, it_1, *it_len) in self._search_modes.items():\n    if \n    it_1 == a_mode:\n        return True']",no_found,0
clusterfuzz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/clusterfuzz/_internal/tests/appengine/libs/query/datastore_query_test.py,https://github.com/google/clusterfuzz/tree/master/src/clusterfuzz/_internal/tests/appengine/libs/query/datastore_query_test.py,QueryMockTest,test_third_page$253,"def test_third_page(self):
    """"""Test getting the third page with more total count.""""""
    query = datastore_query.Query(TestDatastoreModel)
    query.filter_in('tokens', ['a', 'b'])
    query.filter('boolean_value', True)
    query.order('datetime_value', is_desc=True)
    query.fetch_page(page=1, page_size=2, projection=['tokens'], more_limit=4)
    self.assertIsInstance(self.queries[0][-1].filters, ndb.AND)
    six.assertCountEqual(self, [('tokens', '=', 'a'), ('boolean_value', '=', True)], [f.__getnewargs__() for f in self.queries[0][-1].filters])
    self.assertIsInstance(self.queries[1][-1].filters, ndb.AND)
    six.assertCountEqual(self, [('tokens', '=', 'b'), ('boolean_value', '=', True)], [f.__getnewargs__() for f in self.queries[1][-1].filters])
    self.assertIsInstance(self.queries[2][-1].filters, ndb.OR)
    expected = []
    for item in [f.__getnewargs__() for f in self.queries[2][-1].filters]:
        expected.append((item[0], item[1], repr(item[2])))
    six.assertCountEqual(self, [('__key__', '=', ""<Key('TestDatastoreModel', 0), project=test-clusterfuzz>""), ('__key__', '=', ""<Key('TestDatastoreModel', 1), project=test-clusterfuzz>"")], expected)","for item in [f.__getnewargs__() for f in self.queries[2][-1].filters]:
    expected.append((item[0], item[1], repr(item[2])))","['for item in [f.__getnewargs__() for f in self.queries[2][-1].filters]:\n    (item_0, item_1, item_2, *_) = item\n    expected.append((item_0, item_1, repr(item_2)))', 'for (item_0, item_1, item_2, *item_len) in [f.__getnewargs__() for f in self.queries[2][-1].filters]:\n    expected.append((item_0, item_1, repr(item_2)))']",no_found,0
auto-editor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/auto-editor/auto_editor/__main__.py,https://github.com/WyattBlue/auto-editor/tree/master/auto_editor/__main__.py,,main$233,"def main():
    parser = vanparse.ArgumentParser('Auto-Editor', auto_editor.version, description='\nAuto-Editor is an automatic video/audio creator and editor. By default, it will detect silence and create a new video with those sections cut out. By changing some of the options, you can export to a traditional editor like Premiere Pro and adjust the edits there, adjust the pacing of the cuts, and change the method of editing like using audio loudness and video motion to judge making cuts.\nRun:\n    auto-editor --help\n\nTo get the list of options.\n')
    subcommands = ['create', 'test', 'info', 'levels', 'grep', 'subdump', 'desc']
    if len(sys.argv) > 1 and sys.argv[1] in subcommands:
        obj = __import__('auto_editor.subcommands.{}'.format(sys.argv[1]), fromlist=['subcommands'])
        obj.main(sys.argv[2:])
        sys.exit()
    else:
        parser = main_options(parser)
        args = parser.parse_args(sys.argv[1:], Log(), 'auto-editor')
    timer = Timer(args.quiet)
    exporting_to_editor = args.export_to_premiere or args.export_to_resolve or args.export_to_final_cut_pro or args.export_to_shotcut
    making_data_file = exporting_to_editor or args.export_as_json
    is64bit = '64-bit' if sys.maxsize > 2 ** 32 else '32-bit'
    ffmpeg = FFmpeg(args.ffmpeg_location, args.my_ffmpeg, args.show_ffmpeg_debug)
    if args.debug and args.input == []:
        import platform
        dirpath = os.path.dirname(os.path.realpath(__file__))
        print('Python Version: {} {}'.format(platform.python_version(), is64bit))
        print('Platform: {} {} {}'.format(platform.system(), platform.release(), platform.machine().lower()))
        print('Config File path: {}'.format(os.path.join(dirpath, 'config.txt')))
        print('FFmpeg path: {}'.format(ffmpeg.path))
        print('FFmpeg version: {}'.format(ffmpeg.version))
        print('Auto-Editor version {}'.format(auto_editor.version))
        sys.exit()
    if is64bit == '32-bit':
        Log().warning('You have the 32-bit version of Python, which may lead to memory crashes.')
    if args.version:
        print('Auto-Editor version {}'.format(auto_editor.version))
        sys.exit()
    if args.temp_dir is None:
        TEMP = tempfile.mkdtemp()
    else:
        TEMP = args.temp_dir
        if os.path.isfile(TEMP):
            Log().error('Temp directory cannot be an already existing file.')
        if os.path.isdir(TEMP):
            if len(os.listdir(TEMP)) != 0:
                Log().error('Temp directory should be empty!')
        else:
            os.mkdir(TEMP)
    log = Log(args.debug, args.quiet, temp=TEMP)
    log.debug('Temp Directory: {}'.format(TEMP))
    if args.input == []:
        log.error('You need to give auto-editor an input file or folder so it can do the work for you.')
    if [args.export_to_premiere, args.export_to_resolve, args.export_to_final_cut_pro, args.export_as_audio, args.export_to_shotcut, args.export_as_clip_sequence].count(True) > 1:
        log.error('You must choose only one export option.')
    if isinstance(args.frame_margin, str):
        try:
            if float(args.frame_margin) < 0:
                log.error('Frame margin cannot be negative.')
        except ValueError:
            log.error('Frame margin {}, is not valid.'.format(args.frame_margin))
    elif args.frame_margin < 0:
        log.error('Frame margin cannot be negative.')
    if args.constant_rate_factor != 'unset':
        if int(args.constant_rate_factor) < 0 or int(args.constant_rate_factor) > 51:
            log.error('Constant rate factor (crf) must be between 0-51.')
    if args.width < 1:
        log.error('motionOps --width cannot be less than 1.')
    if args.dilates < 0:
        log.error('motionOps --dilates cannot be less than 0')

    def write_starting_message(args):
        if args.export_to_premiere:
            return 'Exporting to Adobe Premiere Pro XML file.'
        if args.export_to_final_cut_pro:
            return 'Exporting to Final Cut Pro XML file.'
        if args.export_to_resolve:
            return 'Exporting to DaVinci Resolve XML file.'
        if args.export_to_shotcut:
            return 'Exporting to Shotcut XML Timeline file.'
        if args.export_as_audio:
            return 'Exporting as audio.'
        return 'Starting.'
    if not args.preview:
        log.conwrite(write_starting_message(args))
    if args.preview or args.export_as_clip_sequence or making_data_file:
        args.no_open = True
    if args.blur < 0:
        args.blur = 0
    if args.silent_speed <= 0 or args.silent_speed > 99999:
        args.silent_speed = 99999
    if args.video_speed <= 0 or args.video_speed > 99999:
        args.video_speed = 99999
    if args.output_file is None:
        args.output_file = []
    from auto_editor.validate_input import valid_input
    (input_list, segments) = valid_input(args.input, ffmpeg, args, log)
    if len(args.output_file) < len(input_list):
        for i in range(len(input_list) - len(args.output_file)):
            args.output_file.append(set_output_name(input_list[i], None, making_data_file, args))
    if args.combine_files:
        if exporting_to_editor:
            temp_file = 'combined.mp4'
        else:
            temp_file = os.path.join(TEMP, 'combined.mp4')
        cmd = []
        for fileref in input_list:
            cmd.extend(['-i', fileref])
        cmd.extend(['-filter_complex', '[0:v]concat=n={}:v=1:a=1'.format(len(input_list)), '-codec:v', 'h264', '-pix_fmt', 'yuv420p', '-strict', '-2', temp_file])
        ffmpeg.run(cmd)
        del cmd
        input_list = [temp_file]
    speeds = [args.silent_speed, args.video_speed]
    if args.cut_out != [] and 99999 not in speeds:
        speeds.append(99999)
    for item in args.set_speed_for_range:
        if item[0] not in speeds:
            speeds.append(float(item[0]))
    log.debug('Speeds: {}'.format(speeds))

    def main_loop(input_list, ffmpeg, args, speeds, segments, log):
        num_cuts = 0
        progress = ProgressBar(args.machine_readable_progress, args.no_progress)
        for (i, input_path) in enumerate(input_list):
            inp = ffmpeg.file_info(input_path)
            if len(input_list) > 1:
                log.conwrite('Working on {}'.format(inp.basename))
            (cuts, output_path) = edit_media(i, inp, ffmpeg, args, progress, speeds, segments[i], exporting_to_editor, making_data_file, TEMP, log)
            num_cuts += cuts
        if not args.preview and (not making_data_file):
            timer.stop()
        if not args.preview and making_data_file:
            time_save = usefulfunctions.human_readable_time(num_cuts * 30)
            s = 's' if num_cuts != 1 else ''
            log.print('Auto-Editor made {} cut{}, which would have taken about {} if edited manually.'.format(num_cuts, s, time_save))
        if not args.no_open:
            usefulfunctions.open_with_system_default(output_path, log)
    try:
        main_loop(input_list, ffmpeg, args, speeds, segments, log)
    except KeyboardInterrupt:
        log.error('Keyboard Interrupt')
    log.cleanup()","for item in args.set_speed_for_range:
    if item[0] not in speeds:
        speeds.append(float(item[0]))","['for item in args.set_speed_for_range:\n    (item_0, *item_ritemmaining) = item\n    if item_0 not in speeds:\n        speeds.append(float(item_0))', 'for (item_0, *item_len) in args.set_speed_for_range:\n    if \n    item_0 not in speeds:\n        speeds.append(float(\n        item_0))']",no_found,0
RootTheBox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RootTheBox/handlers/AdminHandlers/AdminGameObjectHandlers.py,https://github.com/moloch--/RootTheBox/tree/master/handlers/AdminHandlers/AdminGameObjectHandlers.py,AdminViewHandler,post$403,"def post(self, *args, **kwargs):
    if args[0] == 'statistics' or args[0] == 'game_objects':
        uri = {'game_objects': 'admin/view/game_objects.html', 'statistics': 'admin/view/statistics.html'}
        flag_uuid = self.get_argument('flag_uuid', '')
        team_uuid = self.get_argument('team_uuid', '')
        user_uuid = self.get_argument('user_uuid', '')
        flag = Flag.by_uuid(flag_uuid)
        team = Team.by_uuid(team_uuid)
        user = User.by_uuid(user_uuid)
        errors = []
        success = []
        if flag:
            point_restore = self.get_argument('point_restore', '')
            accept_answer = self.get_argument('accept_answer', '')
            answer_token = self.get_argument('answer_token', '')
            if point_restore == 'on' and team:
                if options.penalize_flag_value:
                    penalty = Penalty.by_team_token(flag, team, answer_token)
                    if penalty:
                        value = penalty.cost()
                        if value > 0:
                            team.money += value
                            if user:
                                user.money += value
                                self.dbsession.add(user)
                            self.dbsession.add(team)
                            self.event_manager.admin_score_update(team, '%s penalty reversed - score has been updated.' % team.name, value)
                        self.dbsession.delete(penalty)
                        self.dbsession.commit()
                if flag not in team.flags:
                    flag_value = flag.dynamic_value(team)
                    if self.config.dynamic_flag_value and self.config.dynamic_flag_type == 'decay_all':
                        for item in Flag.team_captures(flag.id):
                            tm = Team.by_id(item[0])
                            deduction = flag.dynamic_value(tm) - flag_value
                            tm.money = int(tm.money - deduction)
                            self.dbsession.add(tm)
                            self.event_manager.flag_decayed(tm, flag)
                    team.money += flag_value
                    if user:
                        user.money += flag_value
                        user.flags.append(flag)
                        self.dbsession.add(user)
                    team.flags.append(flag)
                    self.dbsession.add(team)
                    self.dbsession.commit()
                    BoxHandler.success_capture(self, user, flag, flag_value)
                    self._check_level(flag, team)
                    self.event_manager.flag_captured(team, flag)
                    if options.banking:
                        price = '$' + str(flag_value)
                    else:
                        price = str(flag_value) + ' points'
                    success.append('%s awarded flag and %s' % (team.name, price))
            if accept_answer == 'on' and (flag.type == 'static' or flag.type == 'regex') and (not flag.capture(answer_token)):
                flag.type = 'regex'
                if flag.token.startswith('(') and flag.token.endwith(')'):
                    token = '%s|(%s)' % (flag.token, answer_token)
                else:
                    token = '(%s)|(%s)' % (flag.token, answer_token)
                if len(token) < 256:
                    flag.token = token
                    self.dbsession.add(flag)
                    self.dbsession.commit()
                    success.append('Token successfully added for Flag %s' % flag.name)
                else:
                    errors.append('Flag token too long. Can not expand token.')
        self.render(uri[args[0]], errors=errors, success=success)
    else:
        self.render('public/404.html')","for item in Flag.team_captures(flag.id):
    tm = Team.by_id(item[0])
    deduction = flag.dynamic_value(tm) - flag_value
    tm.money = int(tm.money - deduction)
    self.dbsession.add(tm)
    self.event_manager.flag_decayed(tm, flag)","['for item in Flag.team_captures(flag.id):\n    (item_0, *item_ritemmaining) = item\n    tm = Team.by_id(item_0)\n    deduction = flag.dynamic_value(tm) - flag_value\n    tm.money = int(tm.money - deduction)\n    self.dbsession.add(tm)\n    self.event_manager.flag_decayed(tm, flag)', 'for (item_0, *item_len) in Flag.team_captures(flag.id):\n    tm = Team.by_id(item_0)\n    deduction = flag.dynamic_value(tm) - flag_value\n    tm.money = int(tm.money - deduction)\n    self.dbsession.add(tm)\n    self.event_manager.flag_decayed(tm, flag)']",no_found,0
TauonMusicBox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TauonMusicBox/t_modules/t_main.py,https://github.com/Taiko2k/TauonMusicBox/tree/master/t_modules/t_main.py,,update_set$2149,"def update_set():
    wid = gui.plw - round(16 * gui.scale)
    if gui.tracklist_center_mode:
        wid = gui.tracklist_highlight_width - round(16 * gui.scale)
    total = 0
    for item in gui.pl_st:
        if item[2] is False:
            total += item[1]
        else:
            wid -= item[1]
    if wid <= 75:
        wid = 75
    for i in range(len(gui.pl_st)):
        if gui.pl_st[i][2] is False and total:
            gui.pl_st[i][1] = int(round(gui.pl_st[i][1] / total * wid))","for item in gui.pl_st:
    if item[2] is False:
        total += item[1]
    else:
        wid -= item[1]","['for item in gui.pl_st:\n    (_, item_1, item_2, *item_ritemmaining) = item\n    if item_2 is False:\n        total += item_1\n    else:\n        wid -= item_1', 'for (item_0, item_1, item_2, *item_len) in gui.pl_st:\n    if \n    item_2 is False:\n        total += \n        item_1\n    else:\n        wid -= \n        item_1']",no_found,0
pororo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pororo/pororo/models/tts/utils/display.py,https://github.com/kakaobrain/pororo/tree/master/pororo/models/tts/utils/display.py,,simple_table$17,"def simple_table(item_tuples):
    border_pattern = '+---------------------------------------'
    whitespace = '                                            '
    (headings, cells) = ([], [])
    for item in item_tuples:
        (heading, cell) = (str(item[0]), str(item[1]))
        pad_head = True if len(heading) < len(cell) else False
        pad = abs(len(heading) - len(cell))
        pad = whitespace[:pad]
        pad_left = pad[:len(pad) // 2]
        pad_right = pad[len(pad) // 2:]
        if pad_head:
            heading = pad_left + heading + pad_right
        else:
            cell = pad_left + cell + pad_right
        headings += [heading]
        cells += [cell]
    (border, head, body) = ('', '', '')
    for i in range(len(item_tuples)):
        temp_head = f'| {headings[i]} '
        temp_body = f'| {cells[i]} '
        border += border_pattern[:len(temp_head)]
        head += temp_head
        body += temp_body
        if i == len(item_tuples) - 1:
            head += '|'
            body += '|'
            border += '+'
    print(border)
    print(head)
    print(border)
    print(body)
    print(border)
    print(' ')","for item in item_tuples:
    (heading, cell) = (str(item[0]), str(item[1]))
    pad_head = True if len(heading) < len(cell) else False
    pad = abs(len(heading) - len(cell))
    pad = whitespace[:pad]
    pad_left = pad[:len(pad) // 2]
    pad_right = pad[len(pad) // 2:]
    if pad_head:
        heading = pad_left + heading + pad_right
    else:
        cell = pad_left + cell + pad_right
    headings += [heading]
    cells += [cell]","['for item in item_tuples:\n    (item_0, item_1, *_) = item\n    (heading, cell) = (str(item_0), str(item_1))\n    pad_head = True if len(heading) < len(cell) else False\n    pad = abs(len(heading) - len(cell))\n    pad = whitespace[:pad]\n    pad_left = pad[:len(pad) // 2]\n    pad_right = pad[len(pad) // 2:]\n    if pad_head:\n        heading = pad_left + heading + pad_right\n    else:\n        cell = pad_left + cell + pad_right\n    headings += [heading]\n    cells += [cell]', 'for (item_0, item_1, *item_len) in item_tuples:\n    (heading, cell) = (str(item_0), str(item_1))\n    pad_head = True if len(heading) < len(cell) else False\n    pad = abs(len(heading) - len(cell))\n    pad = whitespace[:pad]\n    pad_left = pad[:len(pad) // 2]\n    pad_right = pad[len(pad) // 2:]\n    if pad_head:\n        heading = pad_left + heading + pad_right\n    else:\n        cell = pad_left + cell + pad_right\n    headings += [heading]\n    cells += [cell]']",no_found,0
mochi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mochi/mochi/core/translation.py,https://github.com/i2y/mochi/tree/master/mochi/core/translation.py,Translator,translate_with_old$1029,"def translate_with_old(self, exp):
    (keyword_with, items, *body) = exp
    pre = []
    first_with_py = None
    with_py = None
    for item in items:
        (item_pre, item_value) = self.translate(item[0], False)
        pre.extend(item_pre)
        var = item[1]
        if with_py is None:
            with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)
            first_with_py = with_py
        else:
            inner_with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)
            with_py.body = [inner_with_py]
            with_py = inner_with_py
    with_py.body = self._translate_sequence(body, True)
    pre.append(first_with_py)
    return (pre, self.translate(NONE_SYM, False)[1])","for item in items:
    (item_pre, item_value) = self.translate(item[0], False)
    pre.extend(item_pre)
    var = item[1]
    if with_py is None:
        with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)
        first_with_py = with_py
    else:
        inner_with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)
        with_py.body = [inner_with_py]
        with_py = inner_with_py","['for item in items:\n    (item_0, item_1, *_) = item\n    (item_pre, item_value) = self.translate(item_0, False)\n    pre.extend(item_pre)\n    var = item_1\n    if with_py is None:\n        with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)\n        first_with_py = with_py\n    else:\n        inner_with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)\n        with_py.body = [inner_with_py]\n        with_py = inner_with_py', 'for (item_0, item_1, *item_len) in items:\n    (item_pre, item_value) = self.translate(\n    item_0, False)\n    pre.extend(item_pre)\n    var = \n    item_1\n    if with_py is None:\n        with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)\n        first_with_py = with_py\n    else:\n        inner_with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)\n        with_py.body = [inner_with_py]\n        with_py = inner_with_py']",no_found,0
FACT_core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FACT_core/src/statistic/update.py,https://github.com/fkie-cad/FACT_core/tree/master/src/statistic/update.py,StatisticUpdater,_calculate_total_files$401,"def _calculate_total_files(list_of_stat_tuples):
    total_amount_of_files = 0
    for item in list_of_stat_tuples:
        with suppress(IndexError):
            total_amount_of_files += item[0][1]
    return total_amount_of_files","for item in list_of_stat_tuples:
    with suppress(IndexError):
        total_amount_of_files += item[0][1]","['for item in list_of_stat_tuples:\n    ((item_0_0, item_0_1, *item_0_ritemmaining), *item_ritemmaining) = item\n    with suppress(IndexError):\n        total_amount_of_files += item_0_1', 'for ((item_0_0, item_0_1, *item_0_len), *item_len) in list_of_stat_tuples:\n    with suppress(IndexError):\n        total_amount_of_files += \n        item_0_1']",no_found,0
airflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/api_connexion/endpoints/role_and_permission_endpoint.py,https://github.com/apache/airflow/tree/master/airflow/api_connexion/endpoints/role_and_permission_endpoint.py,,_check_action_and_resource$37,"def _check_action_and_resource(sm, perms):
    """"""
    Checks if the action or resource exists and raise 400 if not

    This function is intended for use in the REST API because it raise 400
    """"""
    for item in perms:
        if not sm.get_action(item[0]):
            raise BadRequest(detail=f""The specified action: '{item[0]}' was not found"")
        if not sm.get_resource(item[1]):
            raise BadRequest(detail=f""The specified resource: '{item[1]}' was not found"")","for item in perms:
    if not sm.get_action(item[0]):
        raise BadRequest(detail=f""The specified action: '{item[0]}' was not found"")
    if not sm.get_resource(item[1]):
        raise BadRequest(detail=f""The specified resource: '{item[1]}' was not found"")","['for item in perms:\n    (item_0, item_1, *_) = item\n    if not sm.get_action(item_0):\n        raise BadRequest(detail=f""The specified action: \'{item_0}\' was not found"")\n    if not sm.get_resource(item_1):\n        raise BadRequest(detail=f""The specified resource: \'{item_1}\' was not found"")', 'for (item_0, item_1, *item_len) in perms:\n    if not sm.get_action(item_0):\n        raise BadRequest(detail=f""The specified action: \'{item_0}\' was not found"")\n    if not sm.get_resource(item_1):\n        raise BadRequest(detail=f""The specified resource: \'{item_1}\' was not found"")']",no_found,0
redis-py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/redis-py/redis/commands/timeseries/utils.py,https://github.com/redis/redis-py/tree/master/redis/commands/timeseries/utils.py,,parse_m_range$13,"def parse_m_range(response):
    """"""Parse multi range response. Used by TS.MRANGE and TS.MREVRANGE.""""""
    res = []
    for item in response:
        res.append({nativestr(item[0]): [list_to_dict(item[1]), parse_range(item[2])]})
    return sorted(res, key=lambda d: list(d.keys()))","for item in response:
    res.append({nativestr(item[0]): [list_to_dict(item[1]), parse_range(item[2])]})","['for item in response:\n    (item_0, item_1, item_2, *_) = item\n    res.append({nativestr(item_0): [list_to_dict(item_1), parse_range(item_2)]})', 'for (item_0, item_1, item_2, *item_len) in response:\n    res.append({nativestr(item_0): [list_to_dict(item_1), parse_range(item_2)]})']",no_found,0
nltk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk/nltk/sem/evaluate.py,https://github.com/nltk/nltk/tree/master/nltk/sem/evaluate.py,Assignment,_addvariant$355,"def _addvariant(self):
    """"""
        Create a more pretty-printable version of the assignment.
        """"""
    list_ = []
    for item in self.items():
        pair = (item[1], item[0])
        list_.append(pair)
    self.variant = list_
    return None","for item in self.items():
    pair = (item[1], item[0])
    list_.append(pair)","['for item in self.items():\n    (item_0, item_1, *_) = item\n    pair = (item_1, item_0)\n    list_.append(pair)', 'for (item_0, item_1, *item_len) in self.items():\n    pair = (item_1, item_0)\n    list_.append(pair)']",no_found,0
taurus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/taurus/bzt/modules/aggregator.py,https://github.com/Blazemeter/taurus/tree/master/bzt/modules/aggregator.py,KPISet,items$351,"def items(self):
    for item in super(KPISet, self).items():
        yield (item[0], self.__getitem__(item[0]))","for item in super(KPISet, self).items():
    yield (item[0], self.__getitem__(item[0]))","['for item in super(KPISet, self).items():\n    (item_0, *item_ritemmaining) = item\n    yield (item_0, self.__getitem__(item_0))', 'for (item_0, *item_len) in super(KPISet, self).items():\n    yield (item_0, self.__getitem__(item_0))']",no_found,0
oppia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/platform/taskqueue/cloud_tasks_emulator.py,https://github.com/oppia/oppia/tree/master/core/platform/taskqueue/cloud_tasks_emulator.py,Emulator,get_tasks$258,"def get_tasks(self, queue_name: Optional[str]=None) -> List[Task]:
    """"""Returns a list of the tasks in a single queue if a queue name is
        specified or a list of all of the tasks in the taskqueue if no queue
        name is specified.

        Args:
            queue_name: str|None. Name of the queue. Pass in None if no specific
                queue is designated.

        Returns:
            list(Task). List of tasks in a single queue or in the entire
            taskqueue.
        """"""
    if queue_name:
        return self._queues[queue_name]
    else:
        tasks_list = []
        for items in self._queues.items():
            tasks_list.extend(items[1])
        return tasks_list","for items in self._queues.items():
    tasks_list.extend(items[1])","['for items in self._queues.items():\n    (items_0, items_1, *items_ritemsmaining) = items\n    tasks_list.extend(items_1)', 'for (items_0, items_1, *items_len) in self._queues.items():\n    tasks_list.extend(items_1)']",no_found,0
ROMP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ROMP/romp/lib/loss_funcs/maps_loss.py,https://github.com/Arthur151/ROMP/tree/master/romp/lib/loss_funcs/maps_loss.py,AELoss,singleTagLoss$78,"def singleTagLoss(self, pred_tag, joints):
    """"""
        associative embedding loss for one image
        """"""
    tags = []
    pull = 0
    for joints_per_person in joints:
        tmp = []
        for joint in joints_per_person:
            if joint[1] > 0:
                tmp.append(pred_tag[joint[0]])
        if len(tmp) == 0:
            continue
        tmp = torch.stack(tmp)
        tags.append(torch.mean(tmp, dim=0))
        pull = pull + torch.mean((tmp - tags[-1].expand_as(tmp)) ** 2)
    num_tags = len(tags)
    if num_tags == 0:
        return (make_input(torch.zeros(1).float()), make_input(torch.zeros(1).float()))
    elif num_tags == 1:
        return (make_input(torch.zeros(1).float()), pull / num_tags)
    tags = torch.stack(tags)
    size = (num_tags, num_tags)
    A = tags.expand(*size)
    B = A.permute(1, 0)
    diff = A - B
    if self.loss_type == 'exp':
        diff = torch.pow(diff, 2)
        push = torch.exp(-diff)
        push = torch.sum(push) - num_tags
    elif self.loss_type == 'max':
        diff = 1 - torch.abs(diff)
        push = torch.clamp(diff, min=0).sum() - num_tags
    else:
        raise ValueError('Unkown ae loss type')
    return (push / ((num_tags - 1) * num_tags) * 0.5, pull / num_tags)","for joint in joints_per_person:
    if joint[1] > 0:
        tmp.append(pred_tag[joint[0]])","['for joint in joints_per_person:\n    (_, joint_1, *joint_rjointmaining) = joint\n    if joint_1 > 0:\n        tmp.append(pred_tag[joint[0]])', 'for (joint_0, joint_1, *joint_len) in joints_per_person:\n    if \n    joint_1 > 0:\n        tmp.append(pred_tag[\n        joint_0])']",no_found,0
wand,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wand/wand/image.py,https://github.com/emcconville/wand/tree/master/wand/image.py,BaseImage,features$4970,"def features(self, distance):
    """"""Calculate directional image features for each color channel.
        Feature metrics including:

        - angular second moment
        - contrast
        - correlation
        - variance sum of squares
        - inverse difference moment
        - sum average
        - sum variance
        - sum entropy
        - entropy
        - difference variance
        - difference entropy
        - information measures of correlation 1
        - information measures of correlation 2
        - maximum correlation coefficient

        With each metric containing horizontal, vertical, left & right
        diagonal values.

        .. code::

            from wand.image import Image

            with Image(filename='rose:') as img:
                channel_features = img.features(distance=32)
                for channels, features in channel_features.items():
                    print(channels)
                    for feature, directions in features.items():
                        print('  ', feature)
                        for name, value in directions.items():
                            print('    ', name, value)

        :param distance: Define the distance if pixels to calculate.
        :type distance: :class:`numbers.Integral`
        :returns: a dict mapping each color channel with a dict of each
                  feature.
        :rtype: :class:`dict`

        .. versionadded:: 0.5.5
        """"""

    def build_channel(address, channel):
        feature = ChannelFeature()
        size = ctypes.sizeof(feature)
        ctypes.memmove(ctypes.addressof(feature), feature_ptr + CHANNELS[channel] * size, size)
        keys = ('horizontal', 'vertical', 'left_diagonal', 'right_diagonal')
        feature_dict = {}
        for k in feature._fields_:
            a = k[0]
            feature_dict[a] = dict(zip(keys, getattr(feature, a)))
        return feature_dict
    if MAGICK_VERSION_NUMBER < 1792:
        method = library.MagickGetImageChannelFeatures
    else:
        method = library.MagickGetImageFeatures
    assertions.assert_unsigned_integer(distance=distance)
    feature_ptr = method(self.wand, distance)
    response = {}
    if feature_ptr:
        colorspace = self.colorspace
        if self.alpha_channel:
            response['alpha'] = build_channel(feature_ptr, 'alpha')
        if colorspace == 'gray':
            response['gray'] = build_channel(feature_ptr, 'gray')
        elif colorspace == 'cmyk':
            response['cyan'] = build_channel(feature_ptr, 'cyan')
            response['magenta'] = build_channel(feature_ptr, 'magenta')
            response['yellow'] = build_channel(feature_ptr, 'yellow')
            response['black'] = build_channel(feature_ptr, 'black')
        else:
            response['red'] = build_channel(feature_ptr, 'red')
            response['green'] = build_channel(feature_ptr, 'green')
            response['blue'] = build_channel(feature_ptr, 'blue')
        feature_ptr = library.MagickRelinquishMemory(feature_ptr)
    return response","for k in feature._fields_:
    a = k[0]
    feature_dict[a] = dict(zip(keys, getattr(feature, a)))","['for k in feature._fields_:\n    (k_0, *k_rkmaining) = k\n    a = k_0\n    feature_dict[a] = dict(zip(keys, getattr(feature, a)))', 'for (k_0, *k_len) in feature._fields_:\n    a = \n    k_0\n    feature_dict[a] = dict(zip(keys, getattr(feature, a)))']",no_found,0
tensorlayer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorlayer/tensorlayer/prepro.py,https://github.com/tensorlayer/tensorlayer/tree/master/tensorlayer/prepro.py,,resize_image$3684,"def resize_image(image, annos, mask, target_width, target_height):
    """"""Reszie image

        Parameters
        -----------
        image : 3 channel image
            The given image.
        annos : list of list of floats
            Keypoints of people
        mask : single channel image or None
            The mask if available.
        target_width : int
            Expected width of returned image.
        target_height : int
            Expected height of returned image.

        Returns
        ----------
        preprocessed input image, annos, mask

        """"""
    (y, x, _) = np.shape(image)
    ratio_y = target_height / y
    ratio_x = target_width / x
    new_joints = []
    for people in annos:
        new_keypoints = []
        for keypoints in people:
            if keypoints[0] < 0 or keypoints[1] < 0:
                new_keypoints.append((-1000, -1000))
                continue
            pts = (int(keypoints[0] * ratio_x + 0.5), int(keypoints[1] * ratio_y + 0.5))
            if pts[0] > target_width - 1 or pts[1] > target_height - 1:
                new_keypoints.append((-1000, -1000))
                continue
            new_keypoints.append(pts)
        new_joints.append(new_keypoints)
    annos = new_joints
    new_image = cv2.resize(image, (target_width, target_height), interpolation=cv2.INTER_AREA)
    if mask is not None:
        new_mask = cv2.resize(mask, (target_width, target_height), interpolation=cv2.INTER_AREA)
        return (new_image, annos, new_mask)
    else:
        return (new_image, annos, None)","for keypoints in people:
    if keypoints[0] < 0 or keypoints[1] < 0:
        new_keypoints.append((-1000, -1000))
        continue
    pts = (int(keypoints[0] * ratio_x + 0.5), int(keypoints[1] * ratio_y + 0.5))
    if pts[0] > target_width - 1 or pts[1] > target_height - 1:
        new_keypoints.append((-1000, -1000))
        continue
    new_keypoints.append(pts)","['for keypoints in people:\n    (keypoints_0, keypoints_1, *_) = keypoints\n    if keypoints_0 < 0 or keypoints_1 < 0:\n        new_keypoints.append((-1000, -1000))\n        continue\n    pts = (int(keypoints_0 * ratio_x + 0.5), int(keypoints_1 * ratio_y + 0.5))\n    if pts[0] > target_width - 1 or pts[1] > target_height - 1:\n        new_keypoints.append((-1000, -1000))\n        continue\n    new_keypoints.append(pts)', 'for (keypoints_0, keypoints_1, *keypoints_len) in people:\n    if \n    keypoints_0 < 0 or \n    keypoints_1 < 0:\n        new_keypoints.append((-1000, -1000))\n        continue\n    pts = (int(\n    keypoints_0 * ratio_x + 0.5), int(\n    keypoints_1 * ratio_y + 0.5))\n    if pts[0] > target_width - 1 or pts[1] > target_height - 1:\n        new_keypoints.append((-1000, -1000))\n        continue\n    new_keypoints.append(pts)']",no_found,
gluon-nlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/scripts/processing/learn_subword.py,https://github.com/dmlc/gluon-nlp/tree/master/scripts/processing/learn_subword.py,,main$103,"def main(args):
    corpus_path_list = args.corpus
    if args.save_dir is None:
        args.save_dir = args.model
    for corpus_path in corpus_path_list:
        if not os.path.exists(corpus_path):
            raise ValueError('The path=""{}"" provided by --corpus does not exist!'.format(corpus_path))
    print('Learn the ""{}""s subword model based on {}.'.format(args.model, args.corpus))
    os.makedirs(args.save_dir, exist_ok=True)
    model_prefix = os.path.join(args.save_dir, args.model)
    print('Save the subword model to {}.model'.format(model_prefix))
    print('Save the vocabulary to {}.vocab'.format(model_prefix))
    print()
    print('------- Start Training -------------')
    special_tokens_kv = OrderedDict()
    if not args.disable_unk:
        special_tokens_kv['unk_token'] = Vocab.UNK_TOKEN
    if not args.disable_bos:
        special_tokens_kv['bos_token'] = Vocab.BOS_TOKEN
    if not args.disable_eos:
        special_tokens_kv['eos_token'] = Vocab.EOS_TOKEN
    if not args.disable_pad:
        special_tokens_kv['pad_token'] = Vocab.PAD_TOKEN
    if args.model in ['yttm'] and len(args.custom_special_tokens) > 0:
        raise ValueError('model {} do not support custom_special_tokens'.format(args.model))
    additional_custom_special_token = OrderedDict()
    for custom_special_token in args.custom_special_tokens:
        kv = custom_special_token.split('=')
        if not len(kv) == 2:
            raise ValueError('parameter {} has wrong format'.format(custom_special_token))
        (k, v) = (kv[0], kv[1])
        if k in special_tokens_kv:
            warnings.warn(f'There are overlaps between the custom special tokens and the unk, bos, eos, pad tokens. Currently, we will overwrite the default tokens. We will overwrite ""{k}"" to ""{v}""')
        special_tokens_kv[k] = v
        additional_custom_special_token[k] = v
    if args.model == 'hf_wordpiece':
        tokenizers = try_import_huggingface_tokenizers()
        if 'unk_token' not in special_tokens_kv or special_tokens_kv['unk_token'] != '[UNK]':
            special_tokens_kv['unk_token'] = '[UNK]'
        if parse_version(tokenizers.__version__) < parse_version('0.8'):
            if 'mask_token' not in special_tokens_kv:
                special_tokens_kv['mask_token'] = Vocab.MASK_TOKEN
            if 'cls_token' not in special_tokens_kv:
                special_tokens_kv['cls_token'] = Vocab.CLS_TOKEN
            if 'sep_token' not in special_tokens_kv:
                special_tokens_kv['sep_token'] = Vocab.SEP_TOKEN
    special_tokens = list(special_tokens_kv.values())
    print('special tokens: ' + ', '.join(special_tokens))
    vocab = []
    if args.model == 'spm':
        try_import_sentencepiece()
        import sentencepiece as spm
        corpus_path = ','.join(corpus_path_list)
        script = '--input={} --model_prefix={} --vocab_size={} --character_coverage={} --input_sentence_size={}'.format(corpus_path, model_prefix, args.vocab_size, args.coverage, args.input_sentence_size)
        script += ' --unk_id=' + str(list(special_tokens_kv.keys()).index('unk_token'))
        script += ' --bos_id=' + ('-1' if args.disable_bos else str(list(special_tokens_kv.keys()).index('bos_token')))
        script += ' --eos_id=' + ('-1' if args.disable_eos else str(list(special_tokens_kv.keys()).index('eos_token')))
        script += ' --pad_id=' + ('-1' if args.disable_pad else str(list(special_tokens_kv.keys()).index('pad_token')))
        if len(additional_custom_special_token) > 0:
            script += ' --control_symbols=' + ','.join(list(additional_custom_special_token.values()))
        print(script)
        spm.SentencePieceTrainer.Train(script)
        if 'bos_token' in special_tokens_kv:
            special_tokens_kv['bos_token'] = '<s>'
        if 'eos_token' in special_tokens_kv:
            special_tokens_kv['eos_token'] = '</s>'
        spm_model = spm.SentencePieceProcessor()
        spm_model.load(model_prefix + '.model')
        vocab = [spm_model.id_to_piece(i) for i in range(len(spm_model))]
        os.remove(model_prefix + '.vocab')
    elif args.model == 'subword_nmt':
        try_import_subword_nmt()
        from subword_nmt import learn_bpe
        corpus_path = cat_corpus(corpus_path_list) if len(corpus_path_list) > 1 else corpus_path_list[0]
        with open(corpus_path, 'r', encoding='utf-8') as fc, open(model_prefix + '.model', 'w', encoding='utf-8') as fm:
            learn_bpe.learn_bpe(fc, fm, args.vocab_size - len(special_tokens), total_symbols=True)
        with open(corpus_path, 'r', encoding='utf-8') as fc, open(model_prefix + '.model', 'r', encoding='utf-8') as fm:
            vocab.extend(special_tokens)
            uniq_chars_internal = set()
            uniq_chars_final = set()
            uniq_words = set()
            for line in fc:
                for word in line.strip('\r\n ').split(' '):
                    if word:
                        uniq_words.add(word)
            uniq_words = [tuple(x[:-1]) + (x[-1] + '</w>',) for x in uniq_words]
            for word in uniq_words:
                for char in word[:-1]:
                    uniq_chars_internal.add(char)
                uniq_chars_final.add(word[-1])
            vocab.extend(sorted(list(uniq_chars_internal)))
            vocab.extend(sorted(list(uniq_chars_final)))
            fm.readline()
            pair = fm.readline()
            while pair:
                vocab.append(pair.replace(' ', '', 1).strip())
                pair = fm.readline()
        if len(corpus_path_list) > 1:
            os.remove(corpus_path)
    elif args.model == 'yttm':
        try_import_yttm()
        import youtokentome as yttm
        corpus_path = cat_corpus(corpus_path_list) if len(corpus_path_list) > 1 else corpus_path_list[0]
        tokenizer = yttm.BPE.train(data=corpus_path, model=model_prefix + '.model', vocab_size=args.vocab_size, coverage=args.coverage, n_threads=args.n_threads, unk_id=special_tokens.index(Vocab.UNK_TOKEN), bos_id=-1 if args.disable_bos else special_tokens.index(Vocab.BOS_TOKEN), eos_id=-1 if args.disable_eos else special_tokens.index(Vocab.EOS_TOKEN), pad_id=-1 if args.disable_pad else special_tokens.index(Vocab.PAD_TOKEN))
        vocab = tokenizer.vocab()
        if 'unk_token' in special_tokens_kv:
            special_tokens_kv['unk_token'] = '<UNK>'
        if 'bos_token' in special_tokens_kv:
            special_tokens_kv['bos_token'] = '<BOS>'
        if 'eos_token' in special_tokens_kv:
            special_tokens_kv['eos_token'] = '<EOS>'
        if 'pad_token' in special_tokens_kv:
            special_tokens_kv['pad_token'] = '<PAD>'
        if len(corpus_path_list) > 1:
            os.remove(corpus_path)
    elif args.model in ['hf_bpe', 'hf_bytebpe', 'hf_wordpiece']:
        tokenizers = try_import_huggingface_tokenizers()
        if args.model == 'hf_bpe':
            split_on_whitespace_only = not args.split_punctuation
            tokenizer = tokenizers.CharBPETokenizer(lowercase=args.lowercase, bert_normalizer=args.bert_normalizer, split_on_whitespace_only=split_on_whitespace_only)
        elif args.model == 'hf_bytebpe':
            tokenizer = tokenizers.ByteLevelBPETokenizer(lowercase=args.lowercase)
        elif args.model == 'hf_wordpiece':
            unk_token = special_tokens_kv.get('unk_token', None)
            sep_token = special_tokens_kv.get('sep_token', None)
            cls_token = special_tokens_kv.get('cls_token', None)
            pad_token = special_tokens_kv.get('pad_token', None)
            mask_token = special_tokens_kv.get('mask_token', None)
            if args.bert_normalizer:
                strip_accents = None
                clean_text = True
                handle_chinese_chars = True
            else:
                strip_accents = False
                clean_text = False
                handle_chinese_chars = False
            tokenizer = tokenizers.BertWordPieceTokenizer(unk_token=unk_token, sep_token=sep_token, cls_token=cls_token, pad_token=pad_token, mask_token=mask_token, lowercase=args.lowercase, strip_accents=strip_accents, handle_chinese_chars=handle_chinese_chars, clean_text=clean_text)
        else:
            raise NotImplementedError
        tokenizer.train(corpus_path_list, vocab_size=args.vocab_size, show_progress=True, special_tokens=special_tokens)
        if version.parse(tokenizers.__version__) >= version.parse('0.8'):
            save_model_path = model_prefix + '.model'
            tokenizer.save(save_model_path)
            model_info = json.load(open(save_model_path, encoding='utf-8'))
            special_tokens_in_tokenizer = model_info['added_tokens']
            assert len(special_tokens_in_tokenizer) == len(special_tokens)
            hf_vocab = model_info['model']['vocab']
            hf_vocab_sorted = sorted(list(hf_vocab.items()), key=lambda x: x[1])
            hf_vocab_ids = [ele[1] for ele in hf_vocab_sorted]
            assert min(hf_vocab_ids) == 0 and max(hf_vocab_ids) == len(hf_vocab_ids) - 1
            vocab = [ele[0] for ele in hf_vocab_sorted]
        else:
            tokenizer.save(args.save_dir, args.model)
            if args.model == 'hf_wordpiece':
                hf_vocab_file = model_prefix + '-vocab.txt'
                with open(hf_vocab_file, 'r', encoding='utf-8') as fv:
                    for line in fv:
                        vocab.append(line.strip())
            else:
                os.rename(os.path.join(args.save_dir, '{}-merges.txt'.format(args.model)), os.path.join(args.save_dir, '{}.model'.format(args.model)))
                hf_vocab_file = model_prefix + '-vocab.json'
                with open(hf_vocab_file, 'r', encoding='utf-8') as fv:
                    vocab_kv = json.load(fv)
                    vocab_kv = sorted(list(vocab_kv.items()), key=lambda x: x[1])
                    for kv in vocab_kv:
                        vocab.append(kv[0])
            os.remove(hf_vocab_file)
    else:
        raise NotImplementedError
    vocab_obj = Vocab(vocab, **special_tokens_kv)
    vocab_obj.save(model_prefix + '.vocab')
    print('-------- Done Training -------------')","for kv in vocab_kv:
    vocab.append(kv[0])","['for kv in vocab_kv:\n    (kv_0, *kv_rkvmaining) = kv\n    vocab.append(kv_0)', 'for (kv_0, *kv_len) in vocab_kv:\n    vocab.append(kv_0)']",no_found,0
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/dataset/tests/flowers_test.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/dataset/tests/flowers_test.py,TestFlowers,check_reader$23,"def check_reader(self, reader):
    sum = 0
    label = 0
    size = 224 * 224 * 3
    for l in reader():
        self.assertEqual(l[0].size, size)
        if l[1] > label:
            label = l[1]
        sum += 1
    return (sum, label)","for l in reader():
    self.assertEqual(l[0].size, size)
    if l[1] > label:
        label = l[1]
    sum += 1","['for l in reader():\n    (l_0, l_1, *_) = l\n    self.assertEqual(l_0.size, size)\n    if l_1 > label:\n        label = l_1\n    sum += 1', 'for (l_0, l_1, *l_len) in reader():\n    self.assertEqual(\n    l_0.size, size)\n    if \n    l_1 > label:\n        label = \n        l_1\n    sum += 1']",no_found,0
django-rosetta,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-rosetta/rosetta/views.py,https://github.com/mbi/django-rosetta/tree/master/rosetta/views.py,TranslationFileListView,get_context_data$200,"def get_context_data(self, **kwargs):
    context = super(TranslationFileListView, self).get_context_data(**kwargs)
    third_party_apps = self.po_filter in ('all', 'third-party')
    django_apps = self.po_filter in ('all', 'django')
    project_apps = self.po_filter in ('all', 'project')
    languages = []
    has_pos = False
    for language in rosetta_settings.ROSETTA_LANGUAGES:
        if not can_translate_language(self.request.user, language[0]):
            continue
        po_paths = find_pos(language[0], project_apps=project_apps, django_apps=django_apps, third_party_apps=third_party_apps)
        po_files = [(get_app_name(lang), os.path.realpath(lang), pofile(lang)) for lang in po_paths]
        po_files.sort(key=lambda app: app[0])
        languages.append((language[0], _(language[1]), po_files))
        has_pos = has_pos or bool(po_paths)
    context['version'] = get_rosetta_version()
    context['languages'] = languages
    context['has_pos'] = has_pos
    context['po_filter'] = self.po_filter
    return context","for language in rosetta_settings.ROSETTA_LANGUAGES:
    if not can_translate_language(self.request.user, language[0]):
        continue
    po_paths = find_pos(language[0], project_apps=project_apps, django_apps=django_apps, third_party_apps=third_party_apps)
    po_files = [(get_app_name(lang), os.path.realpath(lang), pofile(lang)) for lang in po_paths]
    po_files.sort(key=lambda app: app[0])
    languages.append((language[0], _(language[1]), po_files))
    has_pos = has_pos or bool(po_paths)","['for language in rosetta_settings.ROSETTA_LANGUAGES:\n    (language_0, language_1, *_) = language\n    if not can_translate_language(self.request.user, language_0):\n        continue\n    po_paths = find_pos(language_0, project_apps=project_apps, django_apps=django_apps, third_party_apps=third_party_apps)\n    po_files = [(get_app_name(lang), os.path.realpath(lang), pofile(lang)) for lang in po_paths]\n    po_files.sort(key=lambda app: app[0])\n    languages.append((language_0, _(language_1), po_files))\n    has_pos = has_pos or bool(po_paths)', 'for (language_0, language_1, *language_len) in rosetta_settings.ROSETTA_LANGUAGES:\n    if not can_translate_language(self.request.user, language_0):\n        continue\n    po_paths = find_pos(language_0, project_apps=project_apps, django_apps=django_apps, third_party_apps=third_party_apps)\n    po_files = [(get_app_name(lang), os.path.realpath(lang), pofile(lang)) for lang in po_paths]\n    po_files.sort(key=lambda app: app[0])\n    languages.append((language_0, _(language_1), po_files))\n    has_pos = has_pos or bool(po_paths)']",no_found,0
django-extensions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-extensions/django_extensions/management/commands/show_urls.py,https://github.com/django-extensions/django-extensions/tree/master/django_extensions/management/commands/show_urls.py,Command,extract_views_from_urlpatterns$190,"def extract_views_from_urlpatterns(self, urlpatterns, base='', namespace=None):
    """"""
        Return a list of views from a list of urlpatterns.

        Each object in the returned list is a three-tuple: (view_func, regex, name)
        """"""
    views = []
    for p in urlpatterns:
        if isinstance(p, (URLPattern, RegexURLPattern)):
            try:
                if not p.name:
                    name = p.name
                elif namespace:
                    name = '{0}:{1}'.format(namespace, p.name)
                else:
                    name = p.name
                pattern = describe_pattern(p)
                views.append((p.callback, base + pattern, name))
            except ViewDoesNotExist:
                continue
        elif isinstance(p, (URLResolver, RegexURLResolver)):
            try:
                patterns = p.url_patterns
            except ImportError:
                continue
            if namespace and p.namespace:
                _namespace = '{0}:{1}'.format(namespace, p.namespace)
            else:
                _namespace = p.namespace or namespace
            pattern = describe_pattern(p)
            if isinstance(p, LocaleRegexURLResolver):
                for language in self.LANGUAGES:
                    with translation.override(language[0]):
                        views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))
            else:
                views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))
        elif hasattr(p, '_get_callback'):
            try:
                views.append((p._get_callback(), base + describe_pattern(p), p.name))
            except ViewDoesNotExist:
                continue
        elif hasattr(p, 'url_patterns') or hasattr(p, '_get_url_patterns'):
            try:
                patterns = p.url_patterns
            except ImportError:
                continue
            views.extend(self.extract_views_from_urlpatterns(patterns, base + describe_pattern(p), namespace=namespace))
        else:
            raise TypeError('%s does not appear to be a urlpattern object' % p)
    return views","for language in self.LANGUAGES:
    with translation.override(language[0]):
        views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))","['for language in self.LANGUAGES:\n    (language_0, *language_rlanguagemaining) = language\n    with translation.override(language_0):\n        views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))', 'for (language_0, *language_len) in self.LANGUAGES:\n    with translation.override(language_0):\n        views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))']",no_found,0
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""""""""
    outline = Outline()
    playlist = Playlist()
    counter = Counter()
    video_list = []
    pdf_list = []
    rich_text_list = []
    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190', 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean', 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id, 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr', data=post_data).text.encode('utf_8').decode('unicode_escape')
    chapters = re.findall('homeworks=\\w+;.+id=(\\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)
        lessons = re.findall('chapterId=' + chapter[0] + '.+contentType=1.+id=(\\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)
            videos = re.findall('contentId=(\\d+).+contentType=(1).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()
            pdfs = re.findall('contentId=(\\d+).+contentType=(3).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()
            rich_text = re.findall('contentId=(\\d+).+contentType=(4).+id=(\\d+).+jsonContent=(.+);.+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')
                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()
    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","for lesson in lessons:
    counter.add(1)
    outline.write(lesson[1], counter, 1)
    videos = re.findall('contentId=(\\d+).+contentType=(1).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
    for video in videos:
        counter.add(2)
        outline.write(video[3], counter, 2, sign='#')
        video_list.append(Video(counter, video[3], video))
    counter.reset()
    pdfs = re.findall('contentId=(\\d+).+contentType=(3).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
    for pdf in pdfs:
        counter.add(2)
        outline.write(pdf[3], counter, 2, sign='*')
        if CONFIG['doc']:
            pdf_list.append(Document(counter, pdf[3], pdf))
    counter.reset()
    rich_text = re.findall('contentId=(\\d+).+contentType=(4).+id=(\\d+).+jsonContent=(.+);.+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
    for text in rich_text:
        counter.add(2)
        outline.write(text[4], counter, 2, sign='+')
        if CONFIG['text']:
            rich_text_list.append(RichText(counter, text[4], text))
        if CONFIG['file']:
            if text[3] != 'null' and text[3] != '""""':
                params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                file_name = Resource.file_to_save(params['fileName'])
                outline.write(file_name, counter, 2, sign='!')
                WORK_DIR.change('Files')
                res_print(params['fileName'])
                file_name = '%s %s' % (counter, file_name)
                CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
    counter.reset()","['for lesson in lessons:\n    (lesson_0, lesson_1, *_) = lesson\n    counter.add(1)\n    outline.write(lesson_1, counter, 1)\n    videos = re.findall(\'contentId=(\\\\d+).+contentType=(1).+id=(\\\\d+).+lessonId=\' + lesson_0 + \'.+name=""(.+)""\', res)\n    for video in videos:\n        counter.add(2)\n        outline.write(video[3], counter, 2, sign=\'#\')\n        video_list.append(Video(counter, video[3], video))\n    counter.reset()\n    pdfs = re.findall(\'contentId=(\\\\d+).+contentType=(3).+id=(\\\\d+).+lessonId=\' + lesson_0 + \'.+name=""(.+)""\', res)\n    for pdf in pdfs:\n        counter.add(2)\n        outline.write(pdf[3], counter, 2, sign=\'*\')\n        if CONFIG[\'doc\']:\n            pdf_list.append(Document(counter, pdf[3], pdf))\n    counter.reset()\n    rich_text = re.findall(\'contentId=(\\\\d+).+contentType=(4).+id=(\\\\d+).+jsonContent=(.+);.+lessonId=\' + lesson_0 + \'.+name=""(.+)""\', res)\n    for text in rich_text:\n        counter.add(2)\n        outline.write(text[4], counter, 2, sign=\'+\')\n        if CONFIG[\'text\']:\n            rich_text_list.append(RichText(counter, text[4], text))\n        if CONFIG[\'file\']:\n            if text[3] != \'null\' and text[3] != \'""""\':\n                params = {\'nosKey\': re.search(\'nosKey"":""(.+?)""\', text[3]).group(1), \'fileName\': re.search(\'""fileName"":""(.+?)""\', text[3]).group(1)}\n                file_name = Resource.file_to_save(params[\'fileName\'])\n                outline.write(file_name, counter, 2, sign=\'!\')\n                WORK_DIR.change(\'Files\')\n                res_print(params[\'fileName\'])\n                file_name = \'%s %s\' % (counter, file_name)\n                CANDY.download_bin(\'https://www.icourse163.org/course/attachment.htm\', WORK_DIR.file(file_name), params=params, cookies={\'STUDY_SESS\': None})\n    counter.reset()', 'for (lesson_0, lesson_1, *lesson_len) in lessons:\n    counter.add(1)\n    outline.write(lesson_1, counter, 1)\n    videos = re.findall(\'contentId=(\\\\d+).+contentType=(1).+id=(\\\\d+).+lessonId=\' + lesson_0 + \'.+name=""(.+)""\', res)\n    for video in videos:\n        counter.add(2)\n        outline.write(video[3], counter, 2, sign=\'#\')\n        video_list.append(Video(counter, video[3], video))\n    counter.reset()\n    pdfs = re.findall(\'contentId=(\\\\d+).+contentType=(3).+id=(\\\\d+).+lessonId=\' + lesson_0 + \'.+name=""(.+)""\', res)\n    for pdf in pdfs:\n        counter.add(2)\n        outline.write(pdf[3], counter, 2, sign=\'*\')\n        if CONFIG[\'doc\']:\n            pdf_list.append(Document(counter, pdf[3], pdf))\n    counter.reset()\n    rich_text = re.findall(\'contentId=(\\\\d+).+contentType=(4).+id=(\\\\d+).+jsonContent=(.+);.+lessonId=\' + lesson_0 + \'.+name=""(.+)""\', res)\n    for text in rich_text:\n        counter.add(2)\n        outline.write(text[4], counter, 2, sign=\'+\')\n        if CONFIG[\'text\']:\n            rich_text_list.append(RichText(counter, text[4], text))\n        if CONFIG[\'file\']:\n            if text[3] != \'null\' and text[3] != \'""""\':\n                params = {\'nosKey\': re.search(\'nosKey"":""(.+?)""\', text[3]).group(1), \'fileName\': re.search(\'""fileName"":""(.+?)""\', text[3]).group(1)}\n                file_name = Resource.file_to_save(params[\'fileName\'])\n                outline.write(file_name, counter, 2, sign=\'!\')\n                WORK_DIR.change(\'Files\')\n                res_print(params[\'fileName\'])\n                file_name = \'%s %s\' % (counter, file_name)\n                CANDY.download_bin(\'https://www.icourse163.org/course/attachment.htm\', WORK_DIR.file(file_name), params=params, cookies={\'STUDY_SESS\': None})\n    counter.reset()']",no_found,0
ezdxf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ezdxf/src/ezdxf/path/converter.py,https://github.com/mozman/ezdxf/tree/master/src/ezdxf/path/converter.py,,to_bsplines_and_vertices$837,"def to_bsplines_and_vertices(path: Path, g1_tol: float=G1_TOL) -> Iterator[PathParts]:
    """"""Convert a :class:`Path` object into multiple cubic B-splines and
    polylines as lists of vertices. Breaks adjacent Bzier without G1
    continuity into separated B-splines.

    Args:
        path: :class:`Path` objects
        g1_tol: tolerance for G1 continuity check

    Returns:
        :class:`~ezdxf.math.BSpline` and lists of :class:`~ezdxf.math.Vec3`

    """"""
    from ezdxf.math import bezier_to_bspline

    def to_vertices():
        points = [polyline[0][0]]
        for line in polyline:
            points.append(line[1])
        return points

    def to_bspline():
        b1 = bezier[0]
        _g1_continuity_curves = [b1]
        for b2 in bezier[1:]:
            if have_bezier_curves_g1_continuity(b1, b2, g1_tol):
                _g1_continuity_curves.append(b2)
            else:
                yield bezier_to_bspline(_g1_continuity_curves)
                _g1_continuity_curves = [b2]
            b1 = b2
        if _g1_continuity_curves:
            yield bezier_to_bspline(_g1_continuity_curves)
    curves = []
    for path in tools.single_paths([path]):
        prev = path.start
        for cmd in path:
            if cmd.type == Command.CURVE3_TO:
                curve = Bezier3P([prev, cmd.ctrl, cmd.end])
            elif cmd.type == Command.CURVE4_TO:
                curve = Bezier4P([prev, cmd.ctrl1, cmd.ctrl2, cmd.end])
            elif cmd.type == Command.LINE_TO:
                curve = (prev, cmd.end)
            else:
                raise ValueError
            curves.append(curve)
            prev = cmd.end
    bezier: list = []
    polyline: list = []
    for curve in curves:
        if isinstance(curve, tuple):
            if bezier:
                yield from to_bspline()
                bezier.clear()
            polyline.append(curve)
        else:
            if polyline:
                yield to_vertices()
                polyline.clear()
            bezier.append(curve)
    if bezier:
        yield from to_bspline()
    if polyline:
        yield to_vertices()","for line in polyline:
    points.append(line[1])","['for line in polyline:\n    (_, line_1, *line_rlinemaining) = line\n    points.append(line_1)', 'for (line_0, line_1, *line_len) in polyline:\n    points.append(line_1)']",no_found,0
capirca,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capirca/capirca/lib/arista_tp.py,https://github.com/google/capirca/tree/master/capirca/lib/arista_tp.py,Term,__str__$160,"def __str__(self):
    if self.term.platform and self._PLATFORM not in self.term.platform:
        return ''
    if self.term.platform_exclude and self._PLATFORM in self.term.platform_exclude:
        return ''
    config = Config()
    term_block = []
    if self.term_type == 'inet6' and 'icmp' in self.term.protocol or (self.term_type == 'inet' and 'icmpv6' in self.term.protocol):
        logging.debug(self.NO_AF_LOG_PROTO.substitute(term=self.term.name, proto=', '.join(self.term.protocol), af=self.term_type))
        return ''
    if self.term.verbatim:
        for line in self.term.verbatim:
            if line[0] == self._PLATFORM:
                term_block.append([MATCH_INDENT, str(line[1]), True])
        for (i, s, v) in term_block:
            config.Append(i, s, verbatim=v)
        return str(config)
    flags = []
    misc_options = []
    if self.term.option:
        (flags, misc_options) = self._processTermOptions(self.term, self.term.option)
    family_keywords = self._TERM_TYPE.get(self.term_type)
    term_block.append([TERM_INDENT, 'match %s %s' % (self.term.name, family_keywords['addr_fam']), False])
    term_af = self.AF_MAP.get(self.term_type)
    if self.term.owner and (not self.noverbose):
        self.term.comment.append('owner: %s' % self.term.owner)
    if self.term.comment and (not self.noverbose):
        reflowed_comments = self._reflowComments(self.term.comment, MAX_COMMENT_LENGTH)
        for line in reflowed_comments:
            term_block.append([MATCH_INDENT, '!! ' + line, False])
    has_match_criteria = self.term.destination_address or self.term.destination_address_exclude or self.term.destination_port or self.term.destination_prefix or self.term.fragment_offset or self.term.hop_limit or self.term.port or self.term.protocol or self.term.protocol_except or self.term.source_address or self.term.source_address_exclude or self.term.source_port or self.term.source_prefix or self.term.ttl
    is_default_term = re.match('^ipv(4|6)\\-default\\-.*', self.term.name, re.IGNORECASE)
    if not has_match_criteria and (not is_default_term):
        logging.warning('WARNING: term %s has no valid match criteria and will not be rendered.', self.term.name)
        return ''
    else:
        src_addr = self.term.GetAddressOfVersion('source_address', term_af)
        src_addr_ex = self.term.GetAddressOfVersion('source_address_exclude', term_af)
        if src_addr:
            src_str = 'source prefix'
            if src_addr_ex:
                src_str += ' field-set src-%s' % self.term.name
            else:
                for addr in src_addr:
                    src_str += ' %s' % addr
            term_block.append([MATCH_INDENT, src_str, False])
        elif self.term.source_address:
            logging.debug(self.NO_AF_LOG_ADDR.substitute(term=self.term.name, direction='source', af=self.term_type))
            return ''
        dst_addr = self.term.GetAddressOfVersion('destination_address', term_af)
        dst_addr_ex = self.term.GetAddressOfVersion('destination_address_exclude', term_af)
        if dst_addr:
            dst_str = 'destination prefix'
            if dst_addr_ex:
                dst_str += ' field-set dst-%s' % self.term.name
            else:
                for addr in dst_addr:
                    dst_str += ' %s' % addr
            term_block.append([MATCH_INDENT, dst_str, False])
        elif self.term.destination_address:
            logging.debug(self.NO_AF_LOG_ADDR.substitute(term=self.term.name, direction='destination', af=self.term_type))
            return ''
        if self.term.source_prefix:
            src_pfx_str = 'source prefix field-set'
            for pfx in self.term.source_prefix:
                src_pfx_str += ' %s' % pfx
            term_block.append([MATCH_INDENT, ' %s' % src_pfx_str, False])
        if self.term.destination_prefix:
            dst_pfx_str = 'destination prefix field-set'
            for pfx in self.term.destination_prefix:
                dst_pfx_str += ' %s' % pfx
            term_block.append([MATCH_INDENT, ' %s' % dst_pfx_str, False])
        protocol_str = ''
        if self.term.protocol:
            protocol_str = self._processProtocol(self.term_type, self.term, flags)
        if self.term.protocol_except:
            protocol_str = self._processProtocolExcept(self.term_type, self.term, flags)
        port_str = self._processPorts(self.term)
        if port_str:
            protocol_str += port_str
        icmp_type_str = ''
        icmp_code_str = ''
        if self.term.protocol == ['icmp'] or self.term.protocol == ['icmpv6']:
            (icmp_type_str, icmp_code_str) = self._processICMP(self.term)
        if self.term.icmp_type:
            protocol_str += icmp_type_str
        if self.term.icmp_code:
            protocol_str += icmp_code_str
        if protocol_str:
            term_block.append([MATCH_INDENT, protocol_str, False])
        if self.term.packet_length:
            term_block.append([MATCH_INDENT, 'ip length %s' % self.term.packet_length, False])
        if self.term.fragment_offset:
            term_block.append([MATCH_INDENT, 'fragment offset %s' % self.term.fragment_offset, False])
        if self.term.hop_limit:
            term_block.append([MATCH_INDENT, 'ttl %s' % self.term.hop_limit, False])
        if self.term.ttl:
            term_block.append([MATCH_INDENT, 'ttl %s' % self.term.ttl, False])
        if misc_options:
            for mopt in misc_options:
                term_block.append([MATCH_INDENT, mopt, False])
    current_action = self._ACTIONS.get(self.term.action[0])
    has_extra_actions = self.term.logging or self.term.counter or self.term.dscp_set
    if self.term.action != ['accept']:
        term_block.append([MATCH_INDENT, 'actions', False])
        term_block.append([ACTION_INDENT, '%s' % current_action, False])
    elif self.term.action == ['accept'] and has_extra_actions:
        term_block.append([MATCH_INDENT, 'actions', False])
    if has_extra_actions:
        if self.term.logging and self.term.action != ['accept']:
            term_block.append([ACTION_INDENT, 'log', False])
        elif self.term.logging and self.term.action == ['accept']:
            logging.warning('WARNING: term %s uses logging option but is not a deny action. logging will not be added.', self.term.name)
        if self.term.counter:
            term_block.append([ACTION_INDENT, 'count %s' % self.term.counter, False])
        term_block.append([MATCH_INDENT, '!', False])
    term_block.append([TERM_INDENT, '!', False])
    for (tindent, tstr, tverb) in term_block:
        config.Append(tindent, tstr, verbatim=tverb)
    return str(config)","for line in self.term.verbatim:
    if line[0] == self._PLATFORM:
        term_block.append([MATCH_INDENT, str(line[1]), True])","['for line in self.term.verbatim:\n    (line_0, line_1, *_) = line\n    if line_0 == self._PLATFORM:\n        term_block.append([MATCH_INDENT, str(line_1), True])', 'for (line_0, line_1, *line_len) in self.term.verbatim:\n    if \n    line_0 == self._PLATFORM:\n        term_block.append([MATCH_INDENT, str(\n        line_1), True])']",no_found,0
DeepRec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRec/utils/load_data/load_data_ranking.py,https://github.com/cheungdaven/DeepRec/tree/master/utils/load_data/load_data_ranking.py,,load_data_neg$64,"def load_data_neg(path='../data/ml100k/movielens_100k.dat', header=['user_id', 'item_id', 'rating', 'category'], test_size=0.2, sep='\t'):
    df = pd.read_csv(path, sep=sep, names=header, engine='python')
    n_users = df.user_id.unique().shape[0]
    n_items = df.item_id.unique().shape[0]
    (train_data, test_data) = train_test_split(df, test_size=test_size)
    train_data = pd.DataFrame(train_data)
    test_data = pd.DataFrame(test_data)
    train_row = []
    train_col = []
    train_rating = []
    for line in train_data.itertuples():
        u = line[1] - 1
        i = line[2] - 1
        train_row.append(u)
        train_col.append(i)
        train_rating.append(1)
    train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))
    test_row = []
    test_col = []
    test_rating = []
    for line in test_data.itertuples():
        test_row.append(line[1] - 1)
        test_col.append(line[2] - 1)
        test_rating.append(1)
    test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))
    test_dict = {}
    for u in range(n_users):
        test_dict[u] = test_matrix.getrow(u).nonzero()[1]
    print('Load data finished. Number of users:', n_users, 'Number of items:', n_items)
    return (train_matrix.todok(), test_dict, n_users, n_items)","for line in test_data.itertuples():
    test_row.append(line[1] - 1)
    test_col.append(line[2] - 1)
    test_rating.append(1)","['for line in test_data.itertuples():\n    (_, line_1, line_2, *line_rlinemaining) = line\n    test_row.append(line_1 - 1)\n    test_col.append(line_2 - 1)\n    test_rating.append(1)', 'for (line_0, line_1, line_2, *line_len) in test_data.itertuples():\n    test_row.append(line_1 - 1)\n    test_col.append(line_2 - 1)\n    test_rating.append(1)']",no_found,0
DeepRec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRec/utils/load_data/load_data_ranking.py,https://github.com/cheungdaven/DeepRec/tree/master/utils/load_data/load_data_ranking.py,,load_data_neg$64,"def load_data_neg(path='../data/ml100k/movielens_100k.dat', header=['user_id', 'item_id', 'rating', 'category'], test_size=0.2, sep='\t'):
    df = pd.read_csv(path, sep=sep, names=header, engine='python')
    n_users = df.user_id.unique().shape[0]
    n_items = df.item_id.unique().shape[0]
    (train_data, test_data) = train_test_split(df, test_size=test_size)
    train_data = pd.DataFrame(train_data)
    test_data = pd.DataFrame(test_data)
    train_row = []
    train_col = []
    train_rating = []
    for line in train_data.itertuples():
        u = line[1] - 1
        i = line[2] - 1
        train_row.append(u)
        train_col.append(i)
        train_rating.append(1)
    train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))
    test_row = []
    test_col = []
    test_rating = []
    for line in test_data.itertuples():
        test_row.append(line[1] - 1)
        test_col.append(line[2] - 1)
        test_rating.append(1)
    test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))
    test_dict = {}
    for u in range(n_users):
        test_dict[u] = test_matrix.getrow(u).nonzero()[1]
    print('Load data finished. Number of users:', n_users, 'Number of items:', n_items)
    return (train_matrix.todok(), test_dict, n_users, n_items)","for line in train_data.itertuples():
    u = line[1] - 1
    i = line[2] - 1
    train_row.append(u)
    train_col.append(i)
    train_rating.append(1)","['for line in train_data.itertuples():\n    (_, line_1, line_2, *line_rlinemaining) = line\n    u = line_1 - 1\n    i = line_2 - 1\n    train_row.append(u)\n    train_col.append(i)\n    train_rating.append(1)', 'for (line_0, line_1, line_2, *line_len) in train_data.itertuples():\n    u = \n    line_1 - 1\n    i = \n    line_2 - 1\n    train_row.append(u)\n    train_col.append(i)\n    train_rating.append(1)']",no_found,0
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/ansible_test/unit/test_diff.py,https://github.com/ansible/ansible/tree/master/test/ansible_test/unit/test_diff.py,,get_parsed_diff$45,"def get_parsed_diff(base, head=None):
    """"""Return a parsed git diff between the base and head revision.
    :type base: str
    :type head: str | None
    :rtype: list[FileDiff]
    """"""
    lines = get_diff(base, head)
    items = parse_diff(lines)
    assert items
    for item in items:
        assert item.headers
        assert item.is_complete
        item.old.format_lines()
        item.new.format_lines()
        for line_range in item.old.ranges:
            assert line_range[1] >= line_range[0] > 0
        for line_range in item.new.ranges:
            assert line_range[1] >= line_range[0] > 0
    return items","for line_range in item.new.ranges:
    assert line_range[1] >= line_range[0] > 0","['for line_range in item.new.ranges:\n    (line_range_0, line_range_1, *_) = line_range\n    assert line_range_1 >= line_range_0 > 0', 'for (line_range_0, line_range_1, *line_range_len) in item.new.ranges:\n    assert \n    line_range_1 >= \n    line_range_0 > 0']",no_found,0
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/ansible_test/unit/test_diff.py,https://github.com/ansible/ansible/tree/master/test/ansible_test/unit/test_diff.py,,get_parsed_diff$45,"def get_parsed_diff(base, head=None):
    """"""Return a parsed git diff between the base and head revision.
    :type base: str
    :type head: str | None
    :rtype: list[FileDiff]
    """"""
    lines = get_diff(base, head)
    items = parse_diff(lines)
    assert items
    for item in items:
        assert item.headers
        assert item.is_complete
        item.old.format_lines()
        item.new.format_lines()
        for line_range in item.old.ranges:
            assert line_range[1] >= line_range[0] > 0
        for line_range in item.new.ranges:
            assert line_range[1] >= line_range[0] > 0
    return items","for line_range in item.old.ranges:
    assert line_range[1] >= line_range[0] > 0","['for line_range in item.old.ranges:\n    (line_range_0, line_range_1, *_) = line_range\n    assert line_range_1 >= line_range_0 > 0', 'for (line_range_0, line_range_1, *line_range_len) in item.old.ranges:\n    assert \n    line_range_1 >= \n    line_range_0 > 0']",no_found,0
playwright-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/playwright-python/playwright/_impl/_wait_helper.py,https://github.com/microsoft/playwright-python/tree/master/playwright/_impl/_wait_helper.py,WaitHelper,_cleanup$89,"def _cleanup(self) -> None:
    for task in self._pending_tasks:
        if not task.done():
            task.cancel()
    for listener in self._registered_listeners:
        listener[0].remove_listener(listener[1], listener[2])","for listener in self._registered_listeners:
    listener[0].remove_listener(listener[1], listener[2])","['for listener in self._registered_listeners:\n    (listener_0, listener_1, listener_2, *_) = listener\n    listener_0.remove_listener(listener_1, listener_2)', 'for (listener_0, listener_1, listener_2, *listener_len) in self._registered_listeners:\n    listener_0.remove_listener(listener_1, listener_2)']",no_found,0
mifthtools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mifthtools/blender/addons/2.8/mira_tools/mi_curve_stretch.py,https://github.com/mifth/mifthtools/tree/master/blender/addons/2.8/mira_tools/mi_curve_stretch.py,MI_OT_CurveStretch,start_tool$82,"def start_tool(self, context):
    args = (self, context)
    cur_stretch_settings = context.scene.mi_cur_stretch_settings
    curve_settings = context.scene.mi_settings
    active_obj = context.active_object
    bm = bmesh.from_edit_mesh(active_obj.data)
    self.manipulator = context.space_data.show_gizmo
    context.space_data.show_gizmo = False
    for loop in self.loops:
        loop_verts = [active_obj.matrix_world @ bm.verts[i].co for i in loop[0]]
        loop_line = cur_main.pass_line(loop_verts, loop[1])
        new_curve = cur_main.create_curve_to_line(cur_stretch_settings.points_number, loop_line, self.all_curves, loop[1])
        if loop[1] is True:
            new_curve.closed = True
        self.all_curves.append(new_curve)
        self.active_curve = new_curve
        cur_main.generate_bezier_points(self.active_curve, self.active_curve.display_bezier, curve_settings.curve_resolution)
        self.original_verts_data.append(cur_main.pass_line([bm.verts[i].co.copy() for i in loop[0]], loop[1]))
        for curve in self.all_curves:
            update_curve_line(active_obj, curve, self.loops, self.all_curves, bm, curve_settings.spread_mode, self.original_verts_data[self.all_curves.index(curve)])
        if curve_settings.surface_snap is True:
            meshes_array = ut_base.get_obj_dup_meshes(curve_settings.snap_objects, curve_settings.convert_instances, context)
            if meshes_array:
                self.picked_meshes = meshes_array
    self.mi_deform_handle_3d = bpy.types.SpaceView3D.draw_handler_add(mi_curve_draw_3d, args, 'WINDOW', 'POST_VIEW')
    self.mi_deform_handle_2d = bpy.types.SpaceView3D.draw_handler_add(mi_curve_draw_2d, args, 'WINDOW', 'POST_PIXEL')
    self.gh_circle_select_handle = bpy.types.SpaceView3D.draw_handler_add(gh_circle_draw_2d, args, 'WINDOW', 'POST_PIXEL')
    bm.normal_update()
    bmesh.update_edit_mesh(active_obj.data)","for loop in self.loops:
    loop_verts = [active_obj.matrix_world @ bm.verts[i].co for i in loop[0]]
    loop_line = cur_main.pass_line(loop_verts, loop[1])
    new_curve = cur_main.create_curve_to_line(cur_stretch_settings.points_number, loop_line, self.all_curves, loop[1])
    if loop[1] is True:
        new_curve.closed = True
    self.all_curves.append(new_curve)
    self.active_curve = new_curve
    cur_main.generate_bezier_points(self.active_curve, self.active_curve.display_bezier, curve_settings.curve_resolution)
    self.original_verts_data.append(cur_main.pass_line([bm.verts[i].co.copy() for i in loop[0]], loop[1]))
    for curve in self.all_curves:
        update_curve_line(active_obj, curve, self.loops, self.all_curves, bm, curve_settings.spread_mode, self.original_verts_data[self.all_curves.index(curve)])
    if curve_settings.surface_snap is True:
        meshes_array = ut_base.get_obj_dup_meshes(curve_settings.snap_objects, curve_settings.convert_instances, context)
        if meshes_array:
            self.picked_meshes = meshes_array","['for loop in self.loops:\n    (loop_0, loop_1, *_) = loop\n    loop_verts = [active_obj.matrix_world @ bm.verts[i].co for i in loop_0]\n    loop_line = cur_main.pass_line(loop_verts, loop_1)\n    new_curve = cur_main.create_curve_to_line(cur_stretch_settings.points_number, loop_line, self.all_curves, loop_1)\n    if loop_1 is True:\n        new_curve.closed = True\n    self.all_curves.append(new_curve)\n    self.active_curve = new_curve\n    cur_main.generate_bezier_points(self.active_curve, self.active_curve.display_bezier, curve_settings.curve_resolution)\n    self.original_verts_data.append(cur_main.pass_line([bm.verts[i].co.copy() for i in loop_0], loop_1))\n    for curve in self.all_curves:\n        update_curve_line(active_obj, curve, self.loops, self.all_curves, bm, curve_settings.spread_mode, self.original_verts_data[self.all_curves.index(curve)])\n    if curve_settings.surface_snap is True:\n        meshes_array = ut_base.get_obj_dup_meshes(curve_settings.snap_objects, curve_settings.convert_instances, context)\n        if meshes_array:\n            self.picked_meshes = meshes_array', 'for (loop_0, loop_1, *loop_len) in self.loops:\n    loop_verts = [active_obj.matrix_world @ bm.verts[i].co for i in \n    loop_0]\n    loop_line = cur_main.pass_line(loop_verts, \n    loop_1)\n    new_curve = cur_main.create_curve_to_line(cur_stretch_settings.points_number, loop_line, self.all_curves, \n    loop_1)\n    if \n    loop_1 is True:\n        new_curve.closed = True\n    self.all_curves.append(new_curve)\n    self.active_curve = new_curve\n    cur_main.generate_bezier_points(self.active_curve, self.active_curve.display_bezier, curve_settings.curve_resolution)\n    self.original_verts_data.append(cur_main.pass_line([bm.verts[i].co.copy() for i in \n    loop_0], \n    loop_1))\n    for curve in self.all_curves:\n        update_curve_line(active_obj, curve, self.loops, self.all_curves, bm, curve_settings.spread_mode, self.original_verts_data[self.all_curves.index(curve)])\n    if curve_settings.surface_snap is True:\n        meshes_array = ut_base.get_obj_dup_meshes(curve_settings.snap_objects, curve_settings.convert_instances, context)\n        if meshes_array:\n            self.picked_meshes = meshes_array']",no_found,0
pytorch-deeplab-xception,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch-deeplab-xception/modeling/deeplab.py,https://github.com/jfzhang95/pytorch-deeplab-xception/tree/master/modeling/deeplab.py,DeepLab,get_10x_lr_params$58,"def get_10x_lr_params(self):
    modules = [self.aspp, self.decoder]
    for i in range(len(modules)):
        for m in modules[i].named_modules():
            if self.freeze_bn:
                if isinstance(m[1], nn.Conv2d):
                    for p in m[1].parameters():
                        if p.requires_grad:
                            yield p
            elif isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) or isinstance(m[1], nn.BatchNorm2d):
                for p in m[1].parameters():
                    if p.requires_grad:
                        yield p","for m in modules[i].named_modules():
    if self.freeze_bn:
        if isinstance(m[1], nn.Conv2d):
            for p in m[1].parameters():
                if p.requires_grad:
                    yield p
    elif isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) or isinstance(m[1], nn.BatchNorm2d):
        for p in m[1].parameters():
            if p.requires_grad:
                yield p","['for m in modules[i].named_modules():\n    (_, m_1, *m_rmmaining) = m\n    if self.freeze_bn:\n        if isinstance(m_1, nn.Conv2d):\n            for p in m_1.parameters():\n                if p.requires_grad:\n                    yield p\n    elif isinstance(m_1, nn.Conv2d) or isinstance(m_1, SynchronizedBatchNorm2d) or isinstance(m_1, nn.BatchNorm2d):\n        for p in m_1.parameters():\n            if p.requires_grad:\n                yield p', 'for (m_0, m_1, *m_len) in modules[i].named_modules():\n    if self.freeze_bn:\n        if isinstance(\n        m_1, nn.Conv2d):\n            for p in \n            m_1.parameters():\n                if p.requires_grad:\n                    yield p\n    elif isinstance(\n    m_1, nn.Conv2d) or isinstance(\n    m_1, SynchronizedBatchNorm2d) or isinstance(\n    m_1, nn.BatchNorm2d):\n        for p in \n        m_1.parameters():\n            if p.requires_grad:\n                yield p']",no_found,0
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/deps/brew_exts.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/deps/brew_exts.py,,commit_for_version$237,"def commit_for_version(recipe_context, package, version):
    tap_path = recipe_context.tap_path
    commit = None
    with brew_head_at_commit('master', tap_path):
        version_to_commit = brew_versions_info(package, tap_path)
        if version is None:
            version = version_to_commit[0][0]
            commit = version_to_commit[0][1]
        else:
            for mapping in version_to_commit:
                if mapping[0] == version:
                    commit = mapping[1]
    if commit is None:
        raise Exception(f'Failed to find commit for version {version}')
    return commit","for mapping in version_to_commit:
    if mapping[0] == version:
        commit = mapping[1]","['for mapping in version_to_commit:\n    (mapping_0, mapping_1, *_) = mapping\n    if mapping_0 == version:\n        commit = mapping_1', 'for (mapping_0, mapping_1, *mapping_len) in version_to_commit:\n    if \n    mapping_0 == version:\n        commit = \n        mapping_1']",no_found,0
VTuber_Unity,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/VTuber_Unity/head_pose_estimation/visualization.py,https://github.com/kwea123/VTuber_Unity/tree/master/head_pose_estimation/visualization.py,,draw_marks$10,"def draw_marks(image, marks, color=(255, 255, 255)):
    """"""Draw mark points on image""""""
    for mark in marks:
        cv2.circle(image, (int(mark[0]), int(mark[1])), 1, color, -1, cv2.LINE_AA)","for mark in marks:
    cv2.circle(image, (int(mark[0]), int(mark[1])), 1, color, -1, cv2.LINE_AA)","['for mark in marks:\n    (mark_0, mark_1, *_) = mark\n    cv2.circle(image, (int(mark_0), int(mark_1)), 1, color, -1, cv2.LINE_AA)', 'for (mark_0, mark_1, *mark_len) in marks:\n    cv2.circle(image, (int(mark_0), int(mark_1)), 1, color, -1, cv2.LINE_AA)']",no_found,0
nfstream,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nfstream/nfstream/utils.py,https://github.com/nfstream/nfstream/tree/master/nfstream/utils.py,,update_performances$83,"def update_performances(performances, is_linux, flows_count):
    """""" Update performance report and check platform for consistency """"""
    drops = 0
    processed = 0
    ignored = 0
    load = []
    for meter in performances:
        if is_linux:
            drops += meter[0].value
            ignored += meter[2].value
        else:
            drops = max(meter[0].value, drops)
            ignored = max(meter[2].value, ignored)
        processed += meter[1].value
        load.append(meter[1].value)
    print(json.dumps({'flows_expired': flows_count.value, 'packets_processed': processed, 'packets_ignored': ignored, 'packets_dropped_filtered_by_kernel': drops, 'meters_packets_processing_balance': load}))","for meter in performances:
    if is_linux:
        drops += meter[0].value
        ignored += meter[2].value
    else:
        drops = max(meter[0].value, drops)
        ignored = max(meter[2].value, ignored)
    processed += meter[1].value
    load.append(meter[1].value)","['for meter in performances:\n    (meter_0, meter_1, meter_2, *_) = meter\n    if is_linux:\n        drops += meter_0.value\n        ignored += meter_2.value\n    else:\n        drops = max(meter_0.value, drops)\n        ignored = max(meter_2.value, ignored)\n    processed += meter_1.value\n    load.append(meter_1.value)', 'for (meter_0, meter_1, meter_2, *meter_len) in performances:\n    if is_linux:\n        drops += \n        meter_0.value\n        ignored += \n        meter_2.value\n    else:\n        drops = max(\n        meter_0.value, drops)\n        ignored = max(\n        meter_2.value, ignored)\n    processed += \n    meter_1.value\n    load.append(\n    meter_1.value)']",no_found,0
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/database/database.py,https://github.com/frappe/frappe/tree/master/frappe/database/database.py,Database,commit$915,"def commit(self):
    """"""Commit current transaction. Calls SQL `COMMIT`.""""""
    for method in frappe.local.before_commit:
        frappe.call(method[0], *(method[1] or []), **method[2] or {})
    self.sql('commit')
    self.begin()
    frappe.local.rollback_observers = []
    self.flush_realtime_log()
    enqueue_jobs_after_commit()
    flush_local_link_count()","for method in frappe.local.before_commit:
    frappe.call(method[0], *(method[1] or []), **method[2] or {})","['for method in frappe.local.before_commit:\n    (method_0, method_1, method_2, *_) = method\n    frappe.call(method_0, *(method_1 or []), **method_2 or {})', 'for (method_0, method_1, method_2, *method_len) in frappe.local.before_commit:\n    frappe.call(method_0, *(method_1 or []), **method_2 or {})']",no_found,0
tmuxomatic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tmuxomatic/windowgram/windowgram_test.py,https://github.com/oxidane/tmuxomatic/tree/master/windowgram/windowgram_test.py,SenseTestCase,runTest$123,"def runTest(self):
    try:
        methods = inspect.getmembers(self.__class__(), predicate=inspect.ismethod)
        methods = [method for method in methods if method[0].startswith('test_')]
        methods = [(method[1], inspect.getsourcelines(method[1])[1]) for method in methods]
        methods = sorted(methods, key=lambda tup: tup[1])
        for method in methods:
            method[0]()
    except AssertionError as e:
        raise e
    except Exception as e:
        error = 'An error occurred during testing: ' + repr(e)
        raise AssertionError(e)","for method in methods:
    method[0]()","['for method in methods:\n    (method_0, *method_rmethodmaining) = method\n    method_0()', 'for (method_0, *method_len) in methods:\n    method_0()']",no_found,0
detectron2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/detectron2/tests/data/test_sampler.py,https://github.com/facebookresearch/detectron2/tree/master/tests/data/test_sampler.py,TestGroupedBatchSampler,test_groups$30,"def test_groups(self):
    sampler = SequentialSampler(list(range(100)))
    group_ids = [1, 0] * 50
    samples = GroupedBatchSampler(sampler, group_ids, 2)
    for mini_batch in samples:
        self.assertEqual((mini_batch[0] + mini_batch[1]) % 2, 0)","for mini_batch in samples:
    self.assertEqual((mini_batch[0] + mini_batch[1]) % 2, 0)","['for mini_batch in samples:\n    (mini_batch_0, mini_batch_1, *_) = mini_batch\n    self.assertEqual((mini_batch_0 + mini_batch_1) % 2, 0)', 'for (mini_batch_0, mini_batch_1, *mini_batch_len) in samples:\n    self.assertEqual((mini_batch_0 + mini_batch_1) % 2, 0)']",no_found,0
tartube,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tartube/tartube/config.py,https://github.com/axcore/tartube/tree/master/tartube/config.py,SystemPrefWin,setup_downloader_paths_tab$25199,"def setup_downloader_paths_tab(self, inner_notebook):
    """"""Called by self.setup_downloader_tab().

        Sets up the 'File Paths' inner notebook tab.

        Args:

            inner_notebook (Gtk.Notebook): The container for this tab

        """"""
    (tab, grid) = self.add_inner_notebook_tab(_('File _paths'), inner_notebook)
    grid_width = 3
    self.add_label(grid, '<u>' + _('Downloader file paths') + '</u>', 0, 0, grid_width, 1)
    self.add_label(grid, _('Path to the executable'), 0, 1, 1, 1)
    combo_list = [[_('Use default path') + ' (' + self.app_obj.ytdl_path_default + ')', self.app_obj.ytdl_path_default]]
    if os.name != 'nt':
        combo_list.append([_('Use local path') + ' (' + self.app_obj.ytdl_bin + ')', self.app_obj.ytdl_bin])
    if os.name == 'nt':
        msg = _('Use custom path (not recommended on MS Windows)')
    else:
        msg = _('Use custom path')
    combo_list.append([msg, None])
    if os.name != 'nt':
        combo_list.append([_('Use PyPI path') + ' (' + self.app_obj.ytdl_path_pypi + ')', self.app_obj.ytdl_path_pypi])
    self.path_liststore = Gtk.ListStore(str, str)
    for mini_list in combo_list:
        self.path_liststore.append([mini_list[0], mini_list[1]])
    self.filepaths_combo = Gtk.ComboBox.new_with_model(self.path_liststore)
    grid.attach(self.filepaths_combo, 1, 1, grid_width - 1, 1)
    renderer_text = Gtk.CellRendererText()
    self.filepaths_combo.pack_start(renderer_text, True)
    self.filepaths_combo.add_attribute(renderer_text, 'text', 0)
    self.filepaths_combo.set_entry_text_column(0)
    entry = self.add_entry(grid, None, False, 1, 2, 1, 1)
    button = Gtk.Button(_('Set'))
    grid.attach(button, 2, 2, 1, 1)
    if os.name == 'nt':
        if self.app_obj.ytdl_path_custom_flag:
            self.filepaths_combo.set_active(1)
        else:
            self.filepaths_combo.set_active(0)
    elif self.app_obj.ytdl_path_custom_flag:
        self.filepaths_combo.set_active(2)
    elif self.app_obj.ytdl_path == self.app_obj.ytdl_path_default:
        self.filepaths_combo.set_active(0)
    elif self.app_obj.ytdl_path == self.app_obj.ytdl_path_pypi:
        self.filepaths_combo.set_active(3)
    else:
        self.filepaths_combo.set_active(1)
    if self.app_obj.ytdl_path_custom_flag:
        if self.app_obj.ytdl_path:
            entry.set_text(self.app_obj.ytdl_path)
    else:
        button.set_sensitive(False)
    self.add_label(grid, _('Command for update operations'), 0, 3, 1, 1)
    self.cmd_liststore = Gtk.ListStore(str, str)
    for item in self.app_obj.ytdl_update_list:
        self.cmd_liststore.append([item, formats.YTDL_UPDATE_DICT[item]])
    combo2 = Gtk.ComboBox.new_with_model(self.cmd_liststore)
    grid.attach(combo2, 1, 3, grid_width - 1, 1)
    renderer_text = Gtk.CellRendererText()
    combo2.pack_start(renderer_text, True)
    combo2.add_attribute(renderer_text, 'text', 1)
    combo2.set_entry_text_column(1)
    combo2.set_active(self.app_obj.ytdl_update_list.index(self.app_obj.ytdl_update_current))
    if __main__.__pkg_strict_install_flag__:
        combo2.set_sensitive(False)
    self.update_ytdl_combos()
    self.filepaths_combo.connect('changed', self.on_ytdl_path_combo_changed, entry, button)
    button.connect('clicked', self.on_ytdl_path_button_clicked, entry)
    combo2.connect('changed', self.on_update_combo_changed)","for mini_list in combo_list:
    self.path_liststore.append([mini_list[0], mini_list[1]])","['for mini_list in combo_list:\n    (mini_list_0, mini_list_1, *_) = mini_list\n    self.path_liststore.append([mini_list_0, mini_list_1])', 'for (mini_list_0, mini_list_1, *mini_list_len) in combo_list:\n    self.path_liststore.append([mini_list_0, mini_list_1])']",no_found,0
opentelemetry-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/opentelemetry-python/opentelemetry-api/tests/baggage/test_baggage_propagation.py,https://github.com/open-telemetry/opentelemetry-python/tree/master/opentelemetry-api/tests/baggage/test_baggage_propagation.py,TestBaggagePropagation,test_fields$224,"def test_fields(self, mock_format_baggage, mock_baggage):
    mock_setter = Mock()
    self.propagator.inject({}, setter=mock_setter)
    inject_fields = set()
    for mock_call in mock_setter.mock_calls:
        inject_fields.add(mock_call[1][1])
    self.assertEqual(inject_fields, self.propagator.fields)","for mock_call in mock_setter.mock_calls:
    inject_fields.add(mock_call[1][1])","['for mock_call in mock_setter.mock_calls:\n    (_, mock_call_1, *mock_call_rmock_callmaining) = mock_call\n    inject_fields.add(mock_call_1[1])']",no_found,0
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,,get_sales_payment_data$66,"def get_sales_payment_data(filters, columns):
    data = []
    show_payment_detail = False
    sales_invoice_data = get_sales_invoice_data(filters)
    mode_of_payments = get_mode_of_payments(filters)
    mode_of_payment_details = get_mode_of_payment_details(filters)
    if filters.get('payment_detail'):
        show_payment_detail = True
    else:
        show_payment_detail = False
    for inv in sales_invoice_data:
        owner_posting_date = inv['owner'] + cstr(inv['posting_date'])
        if show_payment_detail:
            row = [inv.posting_date, inv.owner, ' ', inv.net_total, inv.total_taxes, 0]
            data.append(row)
            for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
                row = [inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0]
                data.append(row)
        else:
            total_payment = 0
            for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
                total_payment = total_payment + mop_detail[1]
            row = [inv.posting_date, inv.owner, ', '.join(mode_of_payments.get(owner_posting_date, [])), inv.net_total, inv.total_taxes, total_payment]
            data.append(row)
    return data","for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
    total_payment = total_payment + mop_detail[1]","['for mop_detail in mode_of_payment_details.get(owner_posting_date, []):\n    (_, mop_detail_1, *mop_detail_rmop_detailmaining) = mop_detail\n    total_payment = total_payment + mop_detail_1', 'for (mop_detail_0, mop_detail_1, *mop_detail_len) in mode_of_payment_details.get(owner_posting_date, []):\n    total_payment = total_payment + \n    mop_detail_1']",no_found,0
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,,get_sales_payment_data$66,"def get_sales_payment_data(filters, columns):
    data = []
    show_payment_detail = False
    sales_invoice_data = get_sales_invoice_data(filters)
    mode_of_payments = get_mode_of_payments(filters)
    mode_of_payment_details = get_mode_of_payment_details(filters)
    if filters.get('payment_detail'):
        show_payment_detail = True
    else:
        show_payment_detail = False
    for inv in sales_invoice_data:
        owner_posting_date = inv['owner'] + cstr(inv['posting_date'])
        if show_payment_detail:
            row = [inv.posting_date, inv.owner, ' ', inv.net_total, inv.total_taxes, 0]
            data.append(row)
            for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
                row = [inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0]
                data.append(row)
        else:
            total_payment = 0
            for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
                total_payment = total_payment + mop_detail[1]
            row = [inv.posting_date, inv.owner, ', '.join(mode_of_payments.get(owner_posting_date, [])), inv.net_total, inv.total_taxes, total_payment]
            data.append(row)
    return data","for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
    row = [inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0]
    data.append(row)","['for mop_detail in mode_of_payment_details.get(owner_posting_date, []):\n    (mop_detail_0, mop_detail_1, *_) = mop_detail\n    row = [inv.posting_date, inv.owner, mop_detail_0, 0, 0, mop_detail_1, 0]\n    data.append(row)', 'for (mop_detail_0, mop_detail_1, *mop_detail_len) in mode_of_payment_details.get(owner_posting_date, []):\n    row = [inv.posting_date, inv.owner, mop_detail_0, 0, 0, mop_detail_1, 0]\n    data.append(row)']",no_found,0
DrQA,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DrQA/scripts/distant/generate.py,https://github.com/facebookresearch/DrQA/tree/master/scripts/distant/generate.py,,find_answer$80,"def find_answer(paragraph, q_tokens, answer, opts):
    """"""Return the best matching answer offsets from a paragraph.

    The paragraph is skipped if:
    * It is too long or short.
    * It doesn't contain the answer at all.
    * It doesn't contain named entities found in the question.
    * The answer context match score is too low.
      - This is the unigram + bigram overlap within +/- window_sz.
    """"""
    if len(paragraph) > opts['char_max'] or len(paragraph) < opts['char_min']:
        return
    if opts['regex']:
        answer = '(%s)' % answer[0]
        ans_regex = re.compile(answer, flags=re.IGNORECASE + re.UNICODE)
        answers = ans_regex.findall(paragraph)
        answers = {a[0] if isinstance(a, tuple) else a for a in answers}
        answers = {a.strip() for a in answers if len(a.strip()) > 0}
    else:
        answers = {a for a in answer if a in paragraph}
    if len(answers) == 0:
        return
    (q_tokens, q_nltk_ner) = q_tokens
    for ne in q_tokens.entity_groups():
        if ne[0] not in paragraph:
            return
    for ne in q_nltk_ner:
        if ne not in paragraph:
            return
    p_tokens = tokenize_text(paragraph)
    p_words = p_tokens.words(uncased=True)
    q_grams = Counter(q_tokens.ngrams(n=2, uncased=True, filter_fn=utils.filter_ngram))
    best_score = 0
    best_ex = None
    for ans in answers:
        try:
            a_words = tokenize_text(ans).words(uncased=True)
        except RuntimeError:
            logger.warn('Failed to tokenize answer: %s' % ans)
            continue
        for idx in range(len(p_words)):
            if p_words[idx:idx + len(a_words)] == a_words:
                w_s = max(idx - opts['window_sz'], 0)
                w_e = min(idx + opts['window_sz'] + len(a_words), len(p_words))
                w_tokens = p_tokens.slice(w_s, w_e)
                w_grams = Counter(w_tokens.ngrams(n=2, uncased=True, filter_fn=utils.filter_ngram))
                score = sum((w_grams & q_grams).values())
                if score > best_score:
                    best_score = score
                    best_ex = {'id': uuid.uuid4().hex, 'question': q_tokens.words(), 'document': p_tokens.words(), 'offsets': p_tokens.offsets(), 'answers': [(idx, idx + len(a_words) - 1)], 'qlemma': q_tokens.lemmas(), 'lemma': p_tokens.lemmas(), 'pos': p_tokens.pos(), 'ner': p_tokens.entities()}
    if best_score >= opts['match_threshold']:
        return (best_score, best_ex)","for ne in q_tokens.entity_groups():
    if ne[0] not in paragraph:
        return","['for ne in q_tokens.entity_groups():\n    (ne_0, *ne_rnemaining) = ne\n    if ne_0 not in paragraph:\n        return', 'for (ne_0, *ne_len) in q_tokens.entity_groups():\n    if \n    ne_0 not in paragraph:\n        return']",no_found,0
pygame_tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygame_tutorials/examples/pathfinding/mob_graph_test_navmesh.py,https://github.com/kidscancode/pygame_tutorials/tree/master/examples/pathfinding/mob_graph_test_navmesh.py,WeightedMesh,draw$121,"def draw(self):
    for node in self.edges.keys():
        x = int(node[0] * TILESIZE + TILESIZE / 2)
        y = int(node[1] * TILESIZE + TILESIZE / 2)
        pg.draw.circle(screen, CYAN, (x, y), 10)
        for c in self.edges[node]:
            cx = c[0] * TILESIZE + TILESIZE / 2
            cy = c[1] * TILESIZE + TILESIZE / 2
            pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)","for node in self.edges.keys():
    x = int(node[0] * TILESIZE + TILESIZE / 2)
    y = int(node[1] * TILESIZE + TILESIZE / 2)
    pg.draw.circle(screen, CYAN, (x, y), 10)
    for c in self.edges[node]:
        cx = c[0] * TILESIZE + TILESIZE / 2
        cy = c[1] * TILESIZE + TILESIZE / 2
        pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)","['for node in self.edges.keys():\n    (node_0, node_1, *_) = node\n    x = int(node_0 * TILESIZE + TILESIZE / 2)\n    y = int(node_1 * TILESIZE + TILESIZE / 2)\n    pg.draw.circle(screen, CYAN, (x, y), 10)\n    for c in self.edges[node]:\n        cx = c[0] * TILESIZE + TILESIZE / 2\n        cy = c[1] * TILESIZE + TILESIZE / 2\n        pg.draw.line(screen, CYAN, (x, y), (cx, cy), 10)']",no_found,0
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):

    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(set((i.key_number for i in midi_obj.key_signature_changes)))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start) for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(max_pos)]
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j)) for (i, j) in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)
    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for (idx, inst) in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program, note.pitch + 128 if inst.is_drum else note.pitch, enc_dur(max(1, time_to_pos(note.end - note.start))), enc_vel(note.velocity), info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) * math.log2(x / tot) for x in start_distribution))
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(start_ppl)
    encoding.sort()
    (encoding, is_major) = normalize_to_c_major(encoding)
    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]
    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry = []
    chord_int = 2
    for (chord_idx, chord) in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(boundry) >= 2, f'segement must start and end in chords: {target_chords}'
    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i] if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch + encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch + encoding[i][3] % 12, *encoding[i][4:])
    lead_notes = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    lead_chords = infer_chords_for_sequence(lead_notes, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)
    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        (src_strs, tgt_strs) = ([], [])
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for (note_idx, note) in enumerate(notes):
                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                elif dec_dur(note[4]) >= pos_resolution:
                    pitch_type = note[3] % 12
                    if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
                        src_words.append('AUT')
                    else:
                        src_words.append('HALF')
                else:
                    src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return (src_strs, tgt_strs)
    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue
        if cur_len + last_len >= target_len:
            if cur_len + last_len <= max_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    (src_strs, tgt_strs) = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment
    if max_notes >= last_len >= min_notes:
        (src_strs, tgt_strs) = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs
    assert len(src_str_list) == len(tgt_str_list)
    return (src_str_list, tgt_str_list, get_hash(encoding))","for note in encoding:
    if note[6] != allowed_ts:
        continue
    if note[3] < 0 or note[3] > 127:
        continue
    if len(tmp):
        cur_pos = bar_to_pos[note[0]] + note[1]
        last_note = tmp[-1]
        last_pos = bar_to_pos[last_note[0]] + last_note[1] + dec_dur(last_note[4])
        if cur_pos - last_pos >= 0:
            tmp.append(note)
    else:
        tmp.append(note)","['for note in encoding:\n    (_, note_1, _, note_3, _, _, note_6, *note_rnotemaining) = note\n    if note_6 != allowed_ts:\n        continue\n    if note_3 < 0 or note_3 > 127:\n        continue\n    if len(tmp):\n        cur_pos = bar_to_pos[note[0]] + note_1\n        last_note = tmp[-1]\n        last_pos = bar_to_pos[last_note[0]] + last_note_1 + dec_dur(last_note[4])\n        if cur_pos - last_pos >= 0:\n            tmp.append(note)\n    else:\n        tmp.append(note)']",no_found,0
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/telemelody/training/template2melody/gen.py,https://github.com/microsoft/muzic/tree/master/telemelody/training/template2melody/gen.py,,midi_to_encoding$204,"def midi_to_encoding(midi_obj):

    def time_to_pos(t):
        return round(t * pos_resolution / midi_obj.ticks_per_beat)
    key_signature_change_times = len(set((i.key_number for i in midi_obj.key_signature_changes)))
    assert key_signature_change_times <= 1, 'too many key signature changes: {}'.format(key_signature_change_times)
    notes_start_pos = [time_to_pos(j.start) for i in midi_obj.instruments for j in i.notes]
    assert len(notes_start_pos) != 0
    max_pos = min(max(notes_start_pos) + 1, trunc_pos)
    pos_to_info = [[None for _ in range(4)] for _ in range(max_pos)]
    tsc = midi_obj.time_signature_changes
    tpc = midi_obj.tempo_changes
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    allowed_ts_list = [enc_ts(time_signature_reduce(i, j)) for (i, j) in [(4, 4)]]
    for i in range(len(tsc)):
        for j in range(time_to_pos(tsc[i].time), time_to_pos(tsc[i + 1].time) if i < len(tsc) - 1 else max_pos):
            if j < len(pos_to_info):
                cur = enc_ts(time_signature_reduce(tsc[i].numerator, tsc[i].denominator))
                assert cur in allowed_ts_list
                pos_to_info[j][1] = allowed_ts
    for i in range(len(tpc)):
        for j in range(time_to_pos(tpc[i].time), time_to_pos(tpc[i + 1].time) if i < len(tpc) - 1 else max_pos):
            if j < len(pos_to_info):
                pos_to_info[j][3] = enc_tpo(tpc[i].tempo)
    for j in range(len(pos_to_info)):
        if pos_to_info[j][1] is None:
            pos_to_info[j][1] = enc_ts(time_signature_reduce(4, 4))
        if pos_to_info[j][3] is None:
            pos_to_info[j][3] = enc_tpo(120.0)
    cnt = 0
    bar = 0
    measure_length = None
    bar_to_pos = [0]
    for j in range(len(pos_to_info)):
        ts = dec_ts(pos_to_info[j][1])
        if cnt == 0:
            measure_length = ts[0] * beat_note_factor * pos_resolution // ts[1]
        pos_to_info[j][0] = bar
        pos_to_info[j][2] = cnt
        cnt += 1
        if cnt >= measure_length:
            assert cnt == measure_length, 'invalid time signature change: pos = {}'.format(j)
            cnt -= measure_length
            bar += 1
            bar_to_pos.append(bar_to_pos[-1] + measure_length)
    encoding = []
    start_distribution = [0] * pos_resolution
    lead_idx = None
    for (idx, inst) in enumerate(midi_obj.instruments):
        if inst.name in ['MELODY']:
            lead_idx = idx
        for note in inst.notes:
            if time_to_pos(note.start) >= trunc_pos:
                continue
            start_distribution[time_to_pos(note.start) % pos_resolution] += 1
            info = pos_to_info[time_to_pos(note.start)]
            if info[0] >= bar_max or inst.is_drum:
                continue
            encoding.append((info[0], info[2], 128 if inst.is_drum else inst.program, note.pitch + 128 if inst.is_drum else note.pitch, enc_dur(max(1, time_to_pos(note.end - note.start))), enc_vel(note.velocity), info[1], info[3], idx))
    if len(encoding) == 0:
        return list()
    tot = sum(start_distribution)
    start_ppl = 2 ** sum((0 if x == 0 else -(x / tot) * math.log2(x / tot) for x in start_distribution))
    if filter_symbolic:
        assert start_ppl <= filter_symbolic_ppl, 'filtered out by the symbolic filter: ppl = {:.2f}'.format(start_ppl)
    encoding.sort()
    (encoding, is_major) = normalize_to_c_major(encoding)
    if is_major:
        target_chords = ['C:', 'C:maj7']
        period_pitch = 0
        period_or_comma_pitchs = [4, 7]
    else:
        target_chords = ['A:m', 'A:m7']
        period_pitch = 9
        period_or_comma_pitchs = [0, 4]
    max_pos = 0
    note_items = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            note_items.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    note_items.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    chords = infer_chords_for_sequence(note_items, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry = []
    chord_int = 2
    for (chord_idx, chord) in enumerate(chords[::chord_int]):
        if chord in target_chords:
            cur_pos = chord_idx * pos_per_chord * chord_int
            boundry.append(cur_pos)
    assert len(boundry) >= 2, f'segement must start and end in chords: {target_chords}'
    pitch_sum = [0] * 128
    note_cnt = [0] * 128
    for i in encoding:
        if i[2] < 128:
            pitch_sum[i[-1]] += i[3]
            note_cnt[i[-1]] += 1
    avg_pitch = [pitch_sum[i] / note_cnt[i] if note_cnt[i] >= 50 else -1 for i in range(128)]
    if lead_idx is None:
        lead_idx = max(enumerate(avg_pitch), key=lambda x: x[1])[0]
    assert avg_pitch[lead_idx] != -1
    encoding = [i for i in encoding if i[-1] == lead_idx]
    allowed_ts = enc_ts(time_signature_reduce(4, 4))
    tmp = []
    for note in encoding:
        if note[6] != allowed_ts:
            continue
        if note[3] < 0 or note[3] > 127:
            continue
        if len(tmp):
            cur_pos = bar_to_pos[note[0]] + note[1]
            last_note = tmp[-1]
            last_pos = bar_to_pos[last_note[0]] + last_note[1] + dec_dur(last_note[4])
            if cur_pos - last_pos >= 0:
                tmp.append(note)
        else:
            tmp.append(note)
    encoding = tmp
    del tmp
    for i in range(len(encoding)):
        if encoding[i][3] < min_pitch:
            encoding[i] = (*encoding[i][:3], min_pitch + encoding[i][3] % 12, *encoding[i][4:])
        elif encoding[i][3] >= max_pitch + 12:
            encoding[i] = (*encoding[i][:3], max_pitch + encoding[i][3] % 12, *encoding[i][4:])
    lead_notes = []
    for note in encoding:
        max_pos = max(max_pos, bar_to_pos[note[0]] + note[1] + dec_dur(note[4]))
        if 0 <= note[3] < 128:
            lead_notes.append(Item(name='On', start=bar_to_pos[note[0]] + note[1], end=bar_to_pos[note[0]] + note[1] + dec_dur(note[4]), vel=dec_vel(note[5]), pitch=note[3], track=0))
    lead_notes.sort(key=lambda x: (x.start, -x.end))
    pos_per_chord = pos_resolution * 2
    max_chords = round(max_pos // pos_per_chord + 0.5)
    lead_chords = infer_chords_for_sequence(lead_notes, pos_per_chord=pos_per_chord, max_chords=max_chords, key_chord_loglik=key_chord_loglik, key_chord_transition_loglik=key_chord_transition_loglik)
    boundry_idx = 0
    segments = []
    for enc in encoding:
        if boundry_idx >= len(boundry):
            break
        cur_pos = bar_to_pos[enc[0]] + enc[1]
        while cur_pos >= boundry[boundry_idx]:
            boundry_idx += 1
            if boundry_idx >= len(boundry):
                break
            segments.append([])
        if len(segments):
            segments[-1].append(enc)
    src_str_list = []
    tgt_str_list = []
    max_notes = 200
    min_notes = 5
    last_notes = []
    last_len = 0
    target_len = random.randint(min_notes, max_notes)

    def notes_to_str(raw_notes):
        (src_strs, tgt_strs) = ([], [])
        notes_list = []
        cur_pos = None
        for note in raw_notes:
            if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
                notes_list.append([])
            notes_list[-1].append(note)
            cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])
        for notes in notes_list:
            if len(notes) < min_notes or len(notes) > max_notes:
                continue
            src_words = []
            if is_major:
                src_words.append('MAJ')
            else:
                src_words.append('MIN')
            tgt_words = []
            first_note = notes[0]
            min_bar = first_note[0]
            for (note_idx, note) in enumerate(notes):
                cur_pos = bar_to_pos[note[0]] + note[1]
                chord_idx = 2 * note[0]
                if note[1] >= 2 * pos_resolution:
                    chord_idx += 1
                cur_chord = lead_chords[chord_idx]
                src_words.append(f'Chord_{cur_chord}')
                if note_idx != len(notes) - 1:
                    nextpos = bar_to_pos[notes[note_idx + 1][0]] + notes[note_idx + 1][1]
                    if nextpos - cur_pos >= 1.5 * pos_resolution and dec_dur(note[4]) >= pos_resolution:
                        pitch_type = note[3] % 12
                        if nextpos - cur_pos >= 2 * pos_resolution and (pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords):
                            src_words.append('AUT')
                        else:
                            src_words.append('HALF')
                    else:
                        src_words.append('NOT')
                elif dec_dur(note[4]) >= pos_resolution:
                    pitch_type = note[3] % 12
                    if pitch_type == period_pitch or (pitch_type in period_or_comma_pitchs and random.random() <= 0.3) or cur_chord in target_chords:
                        src_words.append('AUT')
                    else:
                        src_words.append('HALF')
                else:
                    src_words.append('NOT')
                beat_idx = note[1] // pos_resolution
                beat_idx = np.clip(beat_idx, 0, beat_note_factor - 1)
                src_words.append(f'BEAT_{beat_idx}')
                tgt_words.append(f'Bar_{note[0] - min_bar}')
                tgt_words.append(f'Pos_{note[1]}')
                tgt_words.append(f'Pitch_{note[3]}')
                tgt_words.append(f'Dur_{note[4]}')
            src_strs.append(' '.join(src_words))
            tgt_strs.append(' '.join(tgt_words))
        return (src_strs, tgt_strs)
    for segment in segments:
        cur_len = len(segment)
        if cur_len > max_notes or cur_len == 0:
            if max_notes >= last_len >= min_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
            last_notes = []
            last_len = 0
            continue
        if cur_len + last_len >= target_len:
            if cur_len + last_len <= max_notes:
                (src_strs, tgt_strs) = notes_to_str(last_notes + segment)
                src_str_list += src_strs
                tgt_str_list += tgt_strs
                last_notes = []
                last_len = 0
                target_len = random.randint(min_notes, max_notes)
            else:
                if max_notes >= last_len >= min_notes:
                    (src_strs, tgt_strs) = notes_to_str(last_notes)
                    src_str_list += src_strs
                    tgt_str_list += tgt_strs
                last_notes = segment
                last_len = len(segment)
                target_len = random.randint(last_len, max_notes)
        else:
            last_len += cur_len
            last_notes += segment
    if max_notes >= last_len >= min_notes:
        (src_strs, tgt_strs) = notes_to_str(last_notes)
        src_str_list += src_strs
        tgt_str_list += tgt_strs
    assert len(src_str_list) == len(tgt_str_list)
    return (src_str_list, tgt_str_list, get_hash(encoding))","for note in raw_notes:
    if len(notes_list) == 0 or bar_to_pos[note[0]] + note[1] - cur_pos > 2 * pos_resolution:
        notes_list.append([])
    notes_list[-1].append(note)
    cur_pos = bar_to_pos[note[0]] + note[1] + dec_dur(note[4])","['for note in raw_notes:\n    (_, note_1, *_, note_4) = note\n    if len(notes_list) == 0 or bar_to_pos[note[0]] + note_1 - cur_pos > 2 * pos_resolution:\n        notes_list.append([])\n    notes_list[-1].append(note)\n    cur_pos = bar_to_pos[note[0]] + note_1 + dec_dur(note_4)']",no_found,0
superpaper,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/superpaper/superpaper/gui.py,https://github.com/hhannine/superpaper/tree/master/superpaper/gui.py,WallpaperPreviewPanel,export_offsets$1993,"def export_offsets(self, display_sys):
    """"""Read dragged preview positions, normalize them to be positive,
        and scale sizes up to old canvas size.""""""
    prev_canv_w = self.st_bmp_canvas.GetSize()[0]
    true_canv_w = self.get_canvas(display_sys.get_disp_list(True))[0]
    scaling = true_canv_w / prev_canv_w
    sanitzed_offs = self.sanitize_shape_offs()
    ppi_norm_offsets = []
    for off in sanitzed_offs:
        ppi_norm_offsets.append((off[0] * scaling, off[1] * scaling))
    display_sys.update_ppinorm_offsets(ppi_norm_offsets, bezels_included=False)","for off in sanitzed_offs:
    ppi_norm_offsets.append((off[0] * scaling, off[1] * scaling))","['for off in sanitzed_offs:\n    (off_0, off_1, *_) = off\n    ppi_norm_offsets.append((off_0 * scaling, off_1 * scaling))', 'for (off_0, off_1, *off_len) in sanitzed_offs:\n    ppi_norm_offsets.append((off_0 * scaling, off_1 * scaling))']",no_found,0
OpenFermion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenFermion/src/openfermion/hamiltonians/jellium_hf_state.py,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/hamiltonians/jellium_hf_state.py,,hartree_fock_state_jellium$45,"def hartree_fock_state_jellium(grid, n_electrons, spinless=True, plane_wave=False):
    """"""Give the Hartree-Fock state of jellium.

    Args:
        grid (Grid): The discretization to use.
        n_electrons (int): Number of electrons in the system.
        spinless (bool): Whether to use the spinless model or not.
        plane_wave (bool): Whether to return the Hartree-Fock state in
                           the plane wave (True) or dual basis (False).

    Notes:
        The jellium model is built up by filling the lowest-energy
        single-particle states in the plane-wave Hamiltonian until
        n_electrons states are filled.
    """"""
    hamiltonian = plane_wave_kinetic(grid, spinless=spinless)
    hamiltonian = normal_ordered(hamiltonian)
    hamiltonian.compress()
    occupied_states = lowest_single_particle_energy_states(hamiltonian, n_electrons)
    occupied_states = numpy.array(occupied_states)
    if plane_wave:
        hartree_fock_state_index = numpy.sum(2 ** occupied_states)
        hartree_fock_state = numpy.zeros(2 ** count_qubits(hamiltonian), dtype=complex)
        hartree_fock_state[hartree_fock_state_index] = 1.0
    else:
        hartree_fock_state_creation_operator = FermionOperator.identity()
        for state in occupied_states[::-1]:
            hartree_fock_state_creation_operator *= FermionOperator(((int(state), 1),))
        dual_basis_hf_creation_operator = inverse_fourier_transform(hartree_fock_state_creation_operator, grid, spinless)
        dual_basis_hf_creation = normal_ordered(dual_basis_hf_creation_operator)
        hartree_fock_state = numpy.zeros(2 ** count_qubits(hamiltonian), dtype=complex)
        for term in dual_basis_hf_creation.terms:
            index = 0
            for operator in term:
                index += 2 ** operator[0]
            hartree_fock_state[index] = dual_basis_hf_creation.terms[term]
    return hartree_fock_state","for operator in term:
    index += 2 ** operator[0]","['for operator in term:\n    (operator_0, *operator_roperatormaining) = operator\n    index += 2 ** operator_0', 'for (operator_0, *operator_len) in term:\n    index += 2 ** \n    operator_0']",no_found,0
OpenCore-Legacy-Patcher,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenCore-Legacy-Patcher/resources/cli_menu.py,https://github.com/dortania/OpenCore-Legacy-Patcher/tree/master/resources/cli_menu.py,MenuOptions,patcher_settings_security$968,"def patcher_settings_security(self):
    response = None
    while not (response and response == -1):
        title = ['Adjust Security Settings']
        menu = utilities.TUIMenu(title, 'Please select an option: ', auto_number=True, top_level=True)
        options = [[f'Set System Integrity Protection (SIP):\tCurrently {self.constants.custom_sip_value or self.constants.sip_status}', MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_sip], [f'Set Secure Boot Model (SBM):\t\tCurrently {self.constants.secure_status}', MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_sbm], [f'Set Vault Mode:\t\t\t\tCurrently {self.constants.vault}', MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_vault]]
        for option in options:
            menu.add_menu_option(option[0], function=option[1])
        response = menu.start()","for option in options:
    menu.add_menu_option(option[0], function=option[1])","['for option in options:\n    (option_0, option_1, *_) = option\n    menu.add_menu_option(option_0, function=option_1)', 'for (option_0, option_1, *option_len) in options:\n    menu.add_menu_option(option_0, function=option_1)']",no_found,0
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/tool_shed/grids/util.py,https://github.com/ansible/galaxy/tree/master/lib/tool_shed/grids/util.py,,build_approved_select_field$9,"def build_approved_select_field(trans, name, selected_value=None, for_component=True):
    options = [('No', trans.model.ComponentReview.approved_states.NO), ('Yes', trans.model.ComponentReview.approved_states.YES)]
    if for_component:
        options.append(('Not applicable', trans.model.ComponentReview.approved_states.NA))
        if selected_value is None:
            selected_value = trans.model.ComponentReview.approved_states.NA
    select_field = SelectField(name=name)
    for option_tup in options:
        selected = selected_value and option_tup[1] == selected_value
        select_field.add_option(option_tup[0], option_tup[1], selected=selected)
    return select_field","for option_tup in options:
    selected = selected_value and option_tup[1] == selected_value
    select_field.add_option(option_tup[0], option_tup[1], selected=selected)","['for option_tup in options:\n    (option_tup_0, option_tup_1, *_) = option_tup\n    selected = selected_value and option_tup_1 == selected_value\n    select_field.add_option(option_tup_0, option_tup_1, selected=selected)', 'for (option_tup_0, option_tup_1, *option_tup_len) in options:\n    selected = selected_value and \n    option_tup_1 == selected_value\n    select_field.add_option(\n    option_tup_0, \n    option_tup_1, selected=selected)']",no_found,0
pandas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandas/pandas/core/internals/concat.py,https://github.com/pandas-dev/pandas/tree/master/pandas/core/internals/concat.py,,_combine_concat_plans$704,"def _combine_concat_plans(plans, concat_axis: AxisInt):
    """"""
    Combine multiple concatenation plans into one.

    existing_plan is updated in-place.
    """"""
    if len(plans) == 1:
        for p in plans[0]:
            yield (p[0], [p[1]])
    elif concat_axis == 0:
        offset = 0
        for plan in plans:
            last_plc = None
            for (plc, unit) in plan:
                yield (plc.add(offset), [unit])
                last_plc = plc
            if last_plc is not None:
                offset += last_plc.as_slice.stop
    else:
        num_ended = [0]

        def _next_or_none(seq):
            retval = next(seq, None)
            if retval is None:
                num_ended[0] += 1
            return retval
        plans = list(map(iter, plans))
        next_items = list(map(_next_or_none, plans))
        while num_ended[0] != len(next_items):
            if num_ended[0] > 0:
                raise ValueError('Plan shapes are not aligned')
            (placements, units) = zip(*next_items)
            lengths = list(map(len, placements))
            (min_len, max_len) = (min(lengths), max(lengths))
            if min_len == max_len:
                yield (placements[0], units)
                next_items[:] = map(_next_or_none, plans)
            else:
                yielded_placement = None
                yielded_units = [None] * len(next_items)
                for (i, (plc, unit)) in enumerate(next_items):
                    yielded_units[i] = unit
                    if len(plc) > min_len:
                        next_items[i] = (plc[min_len:], _trim_join_unit(unit, min_len))
                    else:
                        yielded_placement = plc
                        next_items[i] = _next_or_none(plans[i])
                yield (yielded_placement, yielded_units)","for p in plans[0]:
    yield (p[0], [p[1]])","['for p in plans[0]:\n    (p_0, p_1, *_) = p\n    yield (p_0, [p_1])', 'for (p_0, p_1, *p_len) in plans[0]:\n    yield (p_0, [p_1])']",no_found,0
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/common.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/common.py,,__get_loop_sequence_internal$824,"def __get_loop_sequence_internal(uv_layer, pairs, island_info, closed):
    loop_sequences = []
    for pair in pairs:
        seqs = [pair]
        p = pair
        isl_grp = __get_island_group_include_pair(pair, island_info)
        if isl_grp == -1:
            return (None, 'Can not find the island or invalid island')
        while True:
            nlp = __get_next_loop_pair(p)
            if not nlp:
                break
            nlp_isl_grp = __get_island_group_include_pair(nlp, island_info)
            if nlp_isl_grp != isl_grp:
                break
            for nlpl in nlp:
                if nlpl[uv_layer].select:
                    return (None, 'Do not select UV which does not belong to the end edge')
            seqs.append(nlp)
            if len(nlp) == 1 and closed:
                break
            nplp = __get_next_poly_loop_pair(nlp)
            if not nplp:
                break
            nplp_isl_grp = __get_island_group_include_pair(nplp, island_info)
            if nplp_isl_grp != isl_grp:
                break
            matched = False
            for p1 in seqs:
                p2 = nplp
                if p1[0] == p2[0] and p1[1] == p2[1] or (p1[0] == p2[1] and p1[1] == p2[0]):
                    matched = True
            if matched:
                debug_print('This is a circular sequence')
                break
            for nlpl in nplp:
                if nlpl[uv_layer].select:
                    return (None, 'Do not select UV which does not belong to the end edge')
            seqs.append(nplp)
            p = nplp
        loop_sequences.append(seqs)
    return (loop_sequences, '')","for p1 in seqs:
    p2 = nplp
    if p1[0] == p2[0] and p1[1] == p2[1] or (p1[0] == p2[1] and p1[1] == p2[0]):
        matched = True","['for p1 in seqs:\n    (p1_0, p1_1, *_) = p1\n    p2 = nplp\n    if p1_0 == p2[0] and p1_1 == p2[1] or (p1_0 == p2[1] and p1_1 == p2[0]):\n        matched = True', 'for (p1_0, p1_1, *p1_len) in seqs:\n    p2 = nplp\n    if \n    p1_0 == p2[0] and \n    p1_1 == p2[1] or (\n    p1_0 == p2[1] and \n    p1_1 == p2[0]):\n        matched = True']",no_found,0
Listed-company-news-crawl-and-text-analysis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Listed-company-news-crawl-and-text-analysis/Crawler/crawler_nbd.py,https://github.com/DemonDamon/Listed-company-news-crawl-and-text-analysis/tree/master/Crawler/crawler_nbd.py,WebCrawlFromNBD,multi_threads_run$268,"def multi_threads_run(self):
    """"""Multi-threading running.
        """"""
    page_ranges_lst = self.GenPagesLst()
    th_lst = []
    for page_range in page_ranges_lst:
        thread = threading.Thread(target=self.CrawlCompanyNews, args=(page_range[0], page_range[1]))
        th_lst.append(thread)
    for thread in th_lst:
        thread.start()
    for thread in th_lst:
        thread.join()
    return self.url_lst_withoutNews","for page_range in page_ranges_lst:
    thread = threading.Thread(target=self.CrawlCompanyNews, args=(page_range[0], page_range[1]))
    th_lst.append(thread)","['for page_range in page_ranges_lst:\n    (page_range_0, page_range_1, *_) = page_range\n    thread = threading.Thread(target=self.CrawlCompanyNews, args=(page_range_0, page_range_1))\n    th_lst.append(thread)', 'for (page_range_0, page_range_1, *page_range_len) in page_ranges_lst:\n    thread = threading.Thread(target=self.CrawlCompanyNews, args=(page_range_0, page_range_1))\n    th_lst.append(thread)']",no_found,0
mssql-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mssql-cli/mssqlcli/mssqlcompleter.py,https://github.com/dbcli/mssql-cli/tree/master/mssqlcli/mssqlcompleter.py,MssqlCompleter,list_dict$632,"def list_dict(pairs):
    d = defaultdict(list)
    for pair in pairs:
        d[pair[0]].append(pair[1])
    return d","for pair in pairs:
    d[pair[0]].append(pair[1])","['for pair in pairs:\n    (_, pair_1, *pair_rpairmaining) = pair\n    d[pair[0]].append(pair_1)', 'for (pair_0, pair_1, *pair_len) in pairs:\n    d[pair_0].append(pair_1)']",no_found,0
electricitymap-contrib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electricitymap-contrib/parsers/CL.py,https://github.com/tmrowco/electricitymap-contrib/tree/master/parsers/CL.py,,production_processor_live$45,"def production_processor_live(json_tot, json_ren):
    """"""
    Extracts generation data and timestamp into dictionary.
    Returns a list of dictionaries for all of the available ""live"" data, usually that day.
    """"""
    gen_total = json_tot['data'][0]['values']
    if json_ren['data'][1]['key'] == 'ENERGA SOLAR':
        rawgen_sol = json_ren['data'][1]['values']
    else:
        raise RuntimeError(f""Unexpected data label. Expected 'ENERGA SOLAR' and got {json_ren['data'][1]['key']}"")
    if json_ren['data'][0]['key'] == 'ENERGA ELICA':
        rawgen_wind = json_ren['data'][0]['values']
    else:
        raise RuntimeError(f""Unexpected data label. Expected 'ENERGA ELICA' and got {json_ren['data'][0]['key']}"")
    mapped_totals = []
    for total in gen_total:
        datapoint = {}
        dt = total[0]
        for pair in rawgen_sol:
            if pair[0] == dt:
                solar = pair[1]
                break
        for pair in rawgen_wind:
            if pair[0] == dt:
                wind = pair[1]
                break
        datapoint['datetime'] = arrow.get(dt / 1000, tzinfo='Chile/Continental').datetime
        datapoint['unknown'] = total[1] - wind - solar
        datapoint['wind'] = wind
        datapoint['solar'] = solar
        mapped_totals.append(datapoint)
    return mapped_totals","for pair in rawgen_sol:
    if pair[0] == dt:
        solar = pair[1]
        break","['for pair in rawgen_sol:\n    (pair_0, pair_1, *_) = pair\n    if pair_0 == dt:\n        solar = pair_1\n        break', 'for (pair_0, pair_1, *pair_len) in rawgen_sol:\n    if \n    pair_0 == dt:\n        solar = \n        pair_1\n        break']",no_found,0
electricitymap-contrib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electricitymap-contrib/parsers/CL.py,https://github.com/tmrowco/electricitymap-contrib/tree/master/parsers/CL.py,,production_processor_live$45,"def production_processor_live(json_tot, json_ren):
    """"""
    Extracts generation data and timestamp into dictionary.
    Returns a list of dictionaries for all of the available ""live"" data, usually that day.
    """"""
    gen_total = json_tot['data'][0]['values']
    if json_ren['data'][1]['key'] == 'ENERGA SOLAR':
        rawgen_sol = json_ren['data'][1]['values']
    else:
        raise RuntimeError(f""Unexpected data label. Expected 'ENERGA SOLAR' and got {json_ren['data'][1]['key']}"")
    if json_ren['data'][0]['key'] == 'ENERGA ELICA':
        rawgen_wind = json_ren['data'][0]['values']
    else:
        raise RuntimeError(f""Unexpected data label. Expected 'ENERGA ELICA' and got {json_ren['data'][0]['key']}"")
    mapped_totals = []
    for total in gen_total:
        datapoint = {}
        dt = total[0]
        for pair in rawgen_sol:
            if pair[0] == dt:
                solar = pair[1]
                break
        for pair in rawgen_wind:
            if pair[0] == dt:
                wind = pair[1]
                break
        datapoint['datetime'] = arrow.get(dt / 1000, tzinfo='Chile/Continental').datetime
        datapoint['unknown'] = total[1] - wind - solar
        datapoint['wind'] = wind
        datapoint['solar'] = solar
        mapped_totals.append(datapoint)
    return mapped_totals","for pair in rawgen_wind:
    if pair[0] == dt:
        wind = pair[1]
        break","['for pair in rawgen_wind:\n    (pair_0, pair_1, *_) = pair\n    if pair_0 == dt:\n        wind = pair_1\n        break', 'for (pair_0, pair_1, *pair_len) in rawgen_wind:\n    if \n    pair_0 == dt:\n        wind = \n        pair_1\n        break']",no_found,0
QRec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/data/rating.py,https://github.com/Coder-Yu/QRec/tree/master/data/rating.py,Rating,matrix$158,"def matrix(self):
    m = np.zeros((len(self.user), len(self.item)))
    for u in self.user:
        (k, v) = self.userRated(u)
        vec = np.zeros(len(self.item))
        for pair in zip(k, v):
            iid = self.item[pair[0]]
            vec[iid] = pair[1]
        m[self.user[u]] = vec
    return m","for pair in zip(k, v):
    iid = self.item[pair[0]]
    vec[iid] = pair[1]","['for pair in zip(k, v):\n    (_, pair_1, *pair_rpairmaining) = pair\n    iid = self.item[pair[0]]\n    vec[iid] = pair_1', 'for (pair_0, pair_1, *pair_len) in zip(k, v):\n    iid = self.item[\n    pair_0]\n    vec[iid] = \n    pair_1']",no_found,0
cloudtracker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloudtracker/cloudtracker/datasources/athena.py,https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,Athena,__init__$164,"def __init__(self, config, account, start, end, args):
    logging.getLogger('botocore').setLevel(logging.WARN)
    logging.info('Source of CloudTrail logs: s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    if (datetime.datetime.now() - datetime.datetime.strptime(start, '%Y-%m-%d')).days > 365:
        raise Exception('Start date is over a year old. CloudTracker does not create or use partitions over a year old.')
    month_restrictions = set()
    start = start.split('-')
    end = end.split('-')
    if start[0] == end[0]:
        for month in range(int(start[1]), int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
    else:
        for month in range(int(start[1]), 12 + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
        for year in range(int(start[0]), int(end[0])):
            for month in (1, 12 + 1):
                month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month))
        for month in range(1, int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month))
    self.search_filter = '((' + ' or '.join(month_restrictions) + ') and errorcode IS NULL)'
    self.table_name = 'cloudtrail_logs_{}'.format(account['id'])
    sts = boto3.client('sts')
    identity = sts.get_caller_identity()
    logging.info('Using AWS identity: {}'.format(identity['Arn']))
    current_account_id = identity['Account']
    region = boto3.session.Session().region_name
    if 'output_s3_bucket' in config:
        self.output_bucket = config['output_s3_bucket']
    else:
        self.output_bucket = 's3://aws-athena-query-results-{}-{}'.format(current_account_id, region)
    logging.info('Using output bucket: {}'.format(self.output_bucket))
    if 'workgroup' in config:
        self.workgroup = config['workgroup']
    logging.info('Using workgroup: {}'.format(self.workgroup))
    if not config.get('org_id'):
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], account_id=account['id'])
    else:
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], org_id=config['org_id'], account_id=account['id'])
    logging.info('Account cloudtrail log path: {}'.format(cloudtrail_log_path))
    self.athena = boto3.client('athena')
    self.s3 = boto3.client('s3')
    if args.skip_setup:
        logging.info('Skipping initial table creation')
        return
    resp = self.s3.list_objects_v2(Bucket=config['s3_bucket'], Prefix=config['path'], MaxKeys=1)
    if 'Contents' not in resp or len(resp['Contents']) == 0:
        exit('ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    self.query_athena('CREATE DATABASE IF NOT EXISTS {db} {comment}'.format(db=self.database, comment=""COMMENT 'Created by CloudTracker'""), context=None)
    query = ""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (\n            `eventversion` string COMMENT 'from deserializer', \n            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', \n            `eventtime` string COMMENT 'from deserializer', \n            `eventsource` string COMMENT 'from deserializer', \n            `eventname` string COMMENT 'from deserializer', \n            `awsregion` string COMMENT 'from deserializer', \n            `sourceipaddress` string COMMENT 'from deserializer', \n            `useragent` string COMMENT 'from deserializer', \n            `errorcode` string COMMENT 'from deserializer', \n            `errormessage` string COMMENT 'from deserializer', \n            `requestparameters` string COMMENT 'from deserializer', \n            `responseelements` string COMMENT 'from deserializer', \n            `additionaleventdata` string COMMENT 'from deserializer', \n            `requestid` string COMMENT 'from deserializer', \n            `eventid` string COMMENT 'from deserializer', \n            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', \n            `eventtype` string COMMENT 'from deserializer', \n            `apiversion` string COMMENT 'from deserializer', \n            `readonly` string COMMENT 'from deserializer', \n            `recipientaccountid` string COMMENT 'from deserializer', \n            `serviceeventdetails` string COMMENT 'from deserializer', \n            `sharedeventid` string COMMENT 'from deserializer', \n            `vpcendpointid` string COMMENT 'from deserializer')\n            PARTITIONED BY (region string, year string, month string)\n            ROW FORMAT SERDE \n            'com.amazon.emr.hive.serde.CloudTrailSerde' \n            STORED AS INPUTFORMAT \n            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' \n            OUTPUTFORMAT \n            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n            LOCATION '{cloudtrail_log_path}'"".format(table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path)
    self.query_athena(query)
    logging.info('Checking if all partitions for the past {} months exist'.format(NUM_MONTHS_FOR_PARTITIONS))
    query = 'SHOW PARTITIONS {table_name}'.format(table_name=self.table_name)
    partition_list = self.query_athena(query, skip_header=False)
    partition_set = set()
    for partition in partition_list:
        partition_set.add(partition[0])
    regions = boto3.session.Session().get_available_regions('ec2')
    queries_to_make = set()
    for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
        date_of_interest = datetime.datetime.now() - relativedelta(months=num_months_ago)
        year = date_of_interest.year
        month = '{:0>2}'.format(date_of_interest.month)
        query = ''
        for region in regions:
            if 'region={region}/year={year}/month={month}'.format(region=region, year=year, month=month) in partition_set:
                continue
            query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(region=region, year=year, month=month, cloudtrail_log_path=cloudtrail_log_path)
        if query != '':
            queries_to_make.add('ALTER TABLE {table_name} ADD '.format(table_name=self.table_name) + query)
    query_count = len(queries_to_make)
    for query in queries_to_make:
        logging.info('Partition groups remaining to create: {}'.format(query_count))
        self.query_athena(query)
        query_count -= 1","for partition in partition_list:
    partition_set.add(partition[0])","['for partition in partition_list:\n    (partition_0, *partition_rpartitionmaining) = partition\n    partition_set.add(partition_0)', 'for (partition_0, *partition_len) in partition_list:\n    partition_set.add(partition_0)']",no_found,0
pyglet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglet/contrib/aseprite_codec/aseprite.py,https://github.com/pyglet/pyglet/tree/master/contrib/aseprite_codec/aseprite.py,Frame,_convert_to_rgba$193,"def _convert_to_rgba(self, cel):
    if self.color_depth == 8:
        global PALETTE_INDEX
        pixel_array = []
        for pixel in cel.pixel_data:
            if pixel == PALETTE_INDEX:
                pixel_array.extend([0, 0, 0, 0])
            else:
                pixel_array.extend(PALETTE_DICT[pixel])
        cel.pixel_data = bytes(pixel_array)
        return cel
    elif self.color_depth == 16:
        greyscale_iter = _chunked_iter(cel.pixel_data, 2)
        pixel_array = []
        for pixel in greyscale_iter:
            rgba = pixel[0] * 3 + pixel[1]
            pixel_array.append(rgba)
        cel.pixel_data = bytes(pixel_array)
        return cel
    else:
        return cel","for pixel in greyscale_iter:
    rgba = pixel[0] * 3 + pixel[1]
    pixel_array.append(rgba)","['for pixel in greyscale_iter:\n    (pixel_0, pixel_1, *_) = pixel\n    rgba = pixel_0 * 3 + pixel_1\n    pixel_array.append(rgba)', 'for (pixel_0, pixel_1, *pixel_len) in greyscale_iter:\n    rgba = \n    pixel_0 * 3 + \n    pixel_1\n    pixel_array.append(rgba)']",no_found,0
folium,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/folium/folium/plugins/heat_map.py,https://github.com/python-visualization/folium/tree/master/folium/plugins/heat_map.py,HeatMap,_get_self_bounds$87,"def _get_self_bounds(self):
    """"""
        Computes the bounds of the object itself (not including it's children)
        in the form [[lat_min, lon_min], [lat_max, lon_max]].

        """"""
    bounds = [[None, None], [None, None]]
    for point in self.data:
        bounds = [[none_min(bounds[0][0], point[0]), none_min(bounds[0][1], point[1])], [none_max(bounds[1][0], point[0]), none_max(bounds[1][1], point[1])]]
    return bounds","for point in self.data:
    bounds = [[none_min(bounds[0][0], point[0]), none_min(bounds[0][1], point[1])], [none_max(bounds[1][0], point[0]), none_max(bounds[1][1], point[1])]]","['for point in self.data:\n    (point_0, point_1, *_) = point\n    bounds = [[none_min(bounds[0][0], point_0), none_min(bounds[0][1], point_1)], [none_max(bounds[1][0], point_0), none_max(bounds[1][1], point_1)]]', 'for (point_0, point_1, *point_len) in self.data:\n    bounds = [[none_min(bounds[0][0], point_0), none_min(bounds[0][1], point_1)], [none_max(bounds[1][0], point_0), none_max(bounds[1][1], point_1)]]']",no_found,0
arcade,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/arcade/arcade/draw_commands.py,https://github.com/pythonarcade/arcade/tree/master/arcade/draw_commands.py,,draw_arc_filled$41,"def draw_arc_filled(center_x: float, center_y: float, width: float, height: float, color: Color, start_angle: float, end_angle: float, tilt_angle: float=0, num_segments: int=128):
    """"""
    Draw a filled in arc. Useful for drawing pie-wedges, or Pac-Man.

    :param float center_x: x position that is the center of the arc.
    :param float center_y: y position that is the center of the arc.
    :param float width: width of the arc.
    :param float height: height of the arc.
    :param Color color: color, specified in a list of 3 or 4 bytes in RGB or
         RGBA format.
    :param float start_angle: start angle of the arc in degrees.
    :param float end_angle: end angle of the arc in degrees.
    :param float tilt_angle: angle the arc is tilted.
    :param float num_segments: Number of line segments used to draw arc.
    """"""
    unrotated_point_list = [[0.0, 0.0]]
    start_segment = int(start_angle / 360 * num_segments)
    end_segment = int(end_angle / 360 * num_segments)
    for segment in range(start_segment, end_segment + 1):
        theta = 2.0 * 3.1415926 * segment / num_segments
        x = width * math.cos(theta) / 2
        y = height * math.sin(theta) / 2
        unrotated_point_list.append([x, y])
    if tilt_angle == 0:
        uncentered_point_list = unrotated_point_list
    else:
        uncentered_point_list = []
        for point in unrotated_point_list:
            uncentered_point_list.append(rotate_point(point[0], point[1], 0, 0, tilt_angle))
    point_list = []
    for point in uncentered_point_list:
        point_list.append((point[0] + center_x, point[1] + center_y))
    _generic_draw_line_strip(point_list, color, gl.GL_TRIANGLE_FAN)","for point in uncentered_point_list:
    point_list.append((point[0] + center_x, point[1] + center_y))","['for point in uncentered_point_list:\n    (point_0, point_1, *_) = point\n    point_list.append((point_0 + center_x, point_1 + center_y))', 'for (point_0, point_1, *point_len) in uncentered_point_list:\n    point_list.append((point_0 + center_x, point_1 + center_y))']",no_found,0
arcade,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/arcade/arcade/draw_commands.py,https://github.com/pythonarcade/arcade/tree/master/arcade/draw_commands.py,,draw_arc_filled$41,"def draw_arc_filled(center_x: float, center_y: float, width: float, height: float, color: Color, start_angle: float, end_angle: float, tilt_angle: float=0, num_segments: int=128):
    """"""
    Draw a filled in arc. Useful for drawing pie-wedges, or Pac-Man.

    :param float center_x: x position that is the center of the arc.
    :param float center_y: y position that is the center of the arc.
    :param float width: width of the arc.
    :param float height: height of the arc.
    :param Color color: color, specified in a list of 3 or 4 bytes in RGB or
         RGBA format.
    :param float start_angle: start angle of the arc in degrees.
    :param float end_angle: end angle of the arc in degrees.
    :param float tilt_angle: angle the arc is tilted.
    :param float num_segments: Number of line segments used to draw arc.
    """"""
    unrotated_point_list = [[0.0, 0.0]]
    start_segment = int(start_angle / 360 * num_segments)
    end_segment = int(end_angle / 360 * num_segments)
    for segment in range(start_segment, end_segment + 1):
        theta = 2.0 * 3.1415926 * segment / num_segments
        x = width * math.cos(theta) / 2
        y = height * math.sin(theta) / 2
        unrotated_point_list.append([x, y])
    if tilt_angle == 0:
        uncentered_point_list = unrotated_point_list
    else:
        uncentered_point_list = []
        for point in unrotated_point_list:
            uncentered_point_list.append(rotate_point(point[0], point[1], 0, 0, tilt_angle))
    point_list = []
    for point in uncentered_point_list:
        point_list.append((point[0] + center_x, point[1] + center_y))
    _generic_draw_line_strip(point_list, color, gl.GL_TRIANGLE_FAN)","for point in unrotated_point_list:
    uncentered_point_list.append(rotate_point(point[0], point[1], 0, 0, tilt_angle))","['for point in unrotated_point_list:\n    (point_0, point_1, *_) = point\n    uncentered_point_list.append(rotate_point(point_0, point_1, 0, 0, tilt_angle))', 'for (point_0, point_1, *point_len) in unrotated_point_list:\n    uncentered_point_list.append(rotate_point(point_0, point_1, 0, 0, tilt_angle))']",no_found,0
AlgorithmsByPython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsByPython/Target Offer/multiSparse.py,https://github.com/Jack-Lee-Hiter/AlgorithmsByPython/tree/master/Target Offer/multiSparse.py,,tripleToSparse$25,"def tripleToSparse(triple, m, n):
    outMatrix = zeros([m, n])
    for pointTuple in triple:
        mLocation = pointTuple[0]
        nLocation = pointTuple[1]
        value = pointTuple[2]
        outMatrix[mLocation][nLocation] = value
    return outMatrix","for pointTuple in triple:
    mLocation = pointTuple[0]
    nLocation = pointTuple[1]
    value = pointTuple[2]
    outMatrix[mLocation][nLocation] = value","['for pointTuple in triple:\n    (pointTuple_0, pointTuple_1, pointTuple_2, *_) = pointTuple\n    mLocation = pointTuple_0\n    nLocation = pointTuple_1\n    value = pointTuple_2\n    outMatrix[mLocation][nLocation] = value', 'for (pointTuple_0, pointTuple_1, pointTuple_2, *pointTuple_len) in triple:\n    mLocation = \n    pointTuple_0\n    nLocation = \n    pointTuple_1\n    value = \n    pointTuple_2\n    outMatrix[mLocation][nLocation] = value']",no_found,0
OpenSfM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenSfM/opensfm/large/tools.py,https://github.com/mapillary/OpenSfM/tree/master/opensfm/large/tools.py,,add_cluster_neighbors$43,"def add_cluster_neighbors(positions, labels, centers, max_distance) -> List[List[np.ndarray]]:
    reflla = np.mean(positions, 0)
    reference = geo.TopocentricConverter(reflla[0], reflla[1], 0)
    topocentrics = []
    for position in positions:
        (x, y, z) = reference.to_topocentric(position[0], position[1], 0)
        topocentrics.append([x, y])
    topocentrics = np.array(topocentrics)
    topo_tree = spatial.cKDTree(topocentrics)
    clusters = []
    for label in np.arange(centers.shape[0]):
        cluster_indices = np.where(labels == label)[0]
        neighbors = []
        for i in cluster_indices:
            neighbors.extend(topo_tree.query_ball_point(topocentrics[i], max_distance))
        cluster = list(np.union1d(cluster_indices, neighbors))
        clusters.append(cluster)
    return clusters","for position in positions:
    (x, y, z) = reference.to_topocentric(position[0], position[1], 0)
    topocentrics.append([x, y])","['for position in positions:\n    (position_0, position_1, *_) = position\n    (x, y, z) = reference.to_topocentric(position_0, position_1, 0)\n    topocentrics.append([x, y])', 'for (position_0, position_1, *position_len) in positions:\n    (x, y, z) = reference.to_topocentric(position_0, position_1, 0)\n    topocentrics.append([x, y])']",no_found,0
pywikibot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pywikibot/pywikibot/site/_apisite.py,https://github.com/wikimedia/pywikibot/tree/master/pywikibot/site/_apisite.py,APISite,movepage$1923,"def movepage(self, page, newtitle: str, summary, movetalk=True, noredirect=False):
    """"""Move a Page to a new title.

        :see: https://www.mediawiki.org/wiki/API:Move

        :param page: the Page to be moved (must exist)
        :param newtitle: the new title for the Page
        :param summary: edit summary (required!)
        :param movetalk: if True (default), also move the talk page if possible
        :param noredirect: if True, suppress creation of a redirect from the
            old title to the new one
        :return: Page object with the new title
        :rtype: pywikibot.Page
        """"""
    oldtitle = page.title(with_section=False)
    newlink = pywikibot.Link(newtitle, self)
    newpage = pywikibot.Page(newlink)
    if newlink.namespace:
        newtitle = self.namespace(newlink.namespace) + ':' + newlink.title
    else:
        newtitle = newlink.title
    if oldtitle == newtitle:
        raise Error('Cannot move page {} to its own title.'.format(oldtitle))
    if not page.exists():
        raise NoPageError(page, 'Cannot move page {page} because it does not exist on {site}.')
    token = self.tokens['move']
    self.lock_page(page)
    req = self._simple_request(action='move', noredirect=noredirect, reason=summary, movetalk=movetalk, token=token, to=newtitle)
    req['from'] = oldtitle
    try:
        result = req.submit()
        pywikibot.debug('movepage response: {}'.format(result), _logger)
    except APIError as err:
        if err.code.endswith('anon') and self.logged_in():
            pywikibot.debug(""movepage: received '{}' even though bot is logged in"".format(err.code), _logger)
        if err.code in self._mv_errors:
            on_error = self._mv_errors[err.code]
            if hasattr(on_error, 'exception'):
                if issubclass(on_error.exception, LockedPageError):
                    failed_page = page
                    if newpage.exists():
                        for prot in self.page_restrictions(newpage).values():
                            if not self.has_group(prot[0]):
                                failed_page = newpage
                                break
                else:
                    failed_page = newpage if on_error.on_new_page else page
                raise on_error.exception(failed_page) from None
            errdata = {'site': self, 'oldtitle': oldtitle, 'oldnamespace': self.namespace(page.namespace()), 'newtitle': newtitle, 'newnamespace': self.namespace(newlink.namespace), 'user': self.user()}
            raise Error(on_error.format_map(errdata)) from None
        pywikibot.debug(""movepage: Unexpected error code '{}' received."".format(err.code), _logger)
        raise
    finally:
        self.unlock_page(page)
    if 'move' not in result:
        pywikibot.error('movepage: {}'.format(result))
        raise Error('movepage: unexpected response')
    if 'talkmove-error-code' in result['move']:
        pywikibot.warning('movepage: Talk page {} not moved'.format(page.toggleTalkPage().title(as_link=True)))
    return pywikibot.Page(page, newtitle)","for prot in self.page_restrictions(newpage).values():
    if not self.has_group(prot[0]):
        failed_page = newpage
        break","['for prot in self.page_restrictions(newpage).values():\n    (prot_0, *prot_rprotmaining) = prot\n    if not self.has_group(prot_0):\n        failed_page = newpage\n        break', 'for (prot_0, *prot_len) in self.page_restrictions(newpage).values():\n    if not self.has_group(prot_0):\n        failed_page = newpage\n        break']",no_found,0
OpenFermion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenFermion/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,,_reduce_terms$138,"def _reduce_terms(terms, stabilizer_list, manual_input, fixed_positions):
    """"""
    Perform the term reduction using stabilizer conditions.

    Auxiliary function to reduce_number_of_terms.

    Args:
        terms (QubitOperator): Operator the number of terms is to be reduced.
        stabilizer_list (list): List of the stabilizers as QubitOperators.
        manual_input (Boolean): Option to pass the list of fixed qubits
                                positions manually. Set to False by default.
        fixed_positions (list): (optional) List of fixed qubit positions.
                                Passing a list is only effective if
                                manual_input is True.
    Returns:
        even_newer_terms (QubitOperator): Updated operator with reduced terms.
        fixed_positions (list): Positions of qubits to be used for the
                                term reduction.
    Raises:
        StabilizerError: Trivial stabilizer (identity).
        StabilizerError: Stabilizer with complex coefficient.
    """"""
    if manual_input is False:
        fixed_positions = []
    for (i, _) in enumerate(stabilizer_list):
        selected_stab = list(stabilizer_list[0].terms)[0]
        if manual_input is False:
            for qubit_pauli in selected_stab:
                if qubit_pauli[0] not in fixed_positions:
                    fixed_positions += [qubit_pauli[0]]
                    fixed_op = qubit_pauli[1]
                    break
        else:
            for qubit_pauli in selected_stab:
                if qubit_pauli[0] == fixed_positions[i]:
                    fixed_op = qubit_pauli[1]
                    break
        if fixed_op in ['X', 'Z']:
            other_op = 'Y'
        else:
            other_op = 'X'
        new_terms = QubitOperator()
        for qubit_pauli in terms:
            new_terms += fix_single_term(qubit_pauli, fixed_positions[i], fixed_op, other_op, stabilizer_list[0])
        updated_stabilizers = []
        for update_stab in stabilizer_list[1:]:
            updated_stabilizers += [fix_single_term(update_stab, fixed_positions[i], fixed_op, other_op, stabilizer_list[0])]
        terms = new_terms
        stabilizer_list = updated_stabilizers
        check_stabilizer_linearity(stabilizer_list, msg='Linearly dependent stabilizers.')
        check_commuting_stabilizers(stabilizer_list, msg='Stabilizers anti-commute.')
    return (terms, fixed_positions)","for qubit_pauli in selected_stab:
    if qubit_pauli[0] == fixed_positions[i]:
        fixed_op = qubit_pauli[1]
        break","['for qubit_pauli in selected_stab:\n    (qubit_pauli_0, qubit_pauli_1, *_) = qubit_pauli\n    if qubit_pauli_0 == fixed_positions[i]:\n        fixed_op = qubit_pauli_1\n        break', 'for (qubit_pauli_0, qubit_pauli_1, *qubit_pauli_len) in selected_stab:\n    if \n    qubit_pauli_0 == fixed_positions[i]:\n        fixed_op = \n        qubit_pauli_1\n        break']",no_found,0
OpenFermion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenFermion/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,,_reduce_terms$138,"def _reduce_terms(terms, stabilizer_list, manual_input, fixed_positions):
    """"""
    Perform the term reduction using stabilizer conditions.

    Auxiliary function to reduce_number_of_terms.

    Args:
        terms (QubitOperator): Operator the number of terms is to be reduced.
        stabilizer_list (list): List of the stabilizers as QubitOperators.
        manual_input (Boolean): Option to pass the list of fixed qubits
                                positions manually. Set to False by default.
        fixed_positions (list): (optional) List of fixed qubit positions.
                                Passing a list is only effective if
                                manual_input is True.
    Returns:
        even_newer_terms (QubitOperator): Updated operator with reduced terms.
        fixed_positions (list): Positions of qubits to be used for the
                                term reduction.
    Raises:
        StabilizerError: Trivial stabilizer (identity).
        StabilizerError: Stabilizer with complex coefficient.
    """"""
    if manual_input is False:
        fixed_positions = []
    for (i, _) in enumerate(stabilizer_list):
        selected_stab = list(stabilizer_list[0].terms)[0]
        if manual_input is False:
            for qubit_pauli in selected_stab:
                if qubit_pauli[0] not in fixed_positions:
                    fixed_positions += [qubit_pauli[0]]
                    fixed_op = qubit_pauli[1]
                    break
        else:
            for qubit_pauli in selected_stab:
                if qubit_pauli[0] == fixed_positions[i]:
                    fixed_op = qubit_pauli[1]
                    break
        if fixed_op in ['X', 'Z']:
            other_op = 'Y'
        else:
            other_op = 'X'
        new_terms = QubitOperator()
        for qubit_pauli in terms:
            new_terms += fix_single_term(qubit_pauli, fixed_positions[i], fixed_op, other_op, stabilizer_list[0])
        updated_stabilizers = []
        for update_stab in stabilizer_list[1:]:
            updated_stabilizers += [fix_single_term(update_stab, fixed_positions[i], fixed_op, other_op, stabilizer_list[0])]
        terms = new_terms
        stabilizer_list = updated_stabilizers
        check_stabilizer_linearity(stabilizer_list, msg='Linearly dependent stabilizers.')
        check_commuting_stabilizers(stabilizer_list, msg='Stabilizers anti-commute.')
    return (terms, fixed_positions)","for qubit_pauli in selected_stab:
    if qubit_pauli[0] not in fixed_positions:
        fixed_positions += [qubit_pauli[0]]
        fixed_op = qubit_pauli[1]
        break","['for qubit_pauli in selected_stab:\n    (qubit_pauli_0, qubit_pauli_1, *_) = qubit_pauli\n    if qubit_pauli_0 not in fixed_positions:\n        fixed_positions += [qubit_pauli_0]\n        fixed_op = qubit_pauli_1\n        break', 'for (qubit_pauli_0, qubit_pauli_1, *qubit_pauli_len) in selected_stab:\n    if \n    qubit_pauli_0 not in fixed_positions:\n        fixed_positions += [\n        qubit_pauli_0]\n        fixed_op = \n        qubit_pauli_1\n        break']",no_found,0
pycoin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycoin/pycoin/key/Keychain.py,https://github.com/richardkiss/pycoin/tree/master/pycoin/key/Keychain.py,Keychain,interested_hashes$128,"def interested_hashes(self):
    SQL = 'select hash160 from HASH160'
    c = self._exec_sql(SQL)
    for r in c:
        yield r[0]
    SQL = 'select hash160, hash256 from P2S'
    c = self._exec_sql(SQL)
    for r in c:
        yield r[0]
        yield r[1]","for r in c:
    yield r[0]","['for r in c:\n    (r_0, *r_rrmaining) = r\n    yield r_0', 'for (r_0, *r_len) in c:\n    yield\n    r_0']",no_found,0
pycoin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycoin/pycoin/key/Keychain.py,https://github.com/richardkiss/pycoin/tree/master/pycoin/key/Keychain.py,Keychain,interested_hashes$128,"def interested_hashes(self):
    SQL = 'select hash160 from HASH160'
    c = self._exec_sql(SQL)
    for r in c:
        yield r[0]
    SQL = 'select hash160, hash256 from P2S'
    c = self._exec_sql(SQL)
    for r in c:
        yield r[0]
        yield r[1]","for r in c:
    yield r[0]
    yield r[1]","['for r in c:\n    (r_0, r_1, *_) = r\n    yield r_0\n    yield r_1', 'for (r_0, r_1, *r_len) in c:\n    yield\n    r_0\n    yield\n    r_1']",no_found,0
PyQt5-Apps,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyQt5-Apps/words-recorder/main.py,https://github.com/taseikyo/PyQt5-Apps/tree/master/words-recorder/main.py,MainWindow,query$155,"def query(self, w):
    origin = w.input_origin.text().replace(' ', '')
    if origin:
        if db:
            try:
                sql = ""SELECT origin, trans FROM words WHERE origin = '%s'"" % origin
                print(sql)
                num = cursor.execute(sql)
                if num:
                    for r in cursor:
                        w.input_trans.setText(r[1])
            except Exception as e:
                self.messageBox('insert data failed!\nerror msg: %s' % e.args[1])
        else:
            self.messageBox(""connect to the database first!\nclick the button 'File-connect'"")","for r in cursor:
    w.input_trans.setText(r[1])","['for r in cursor:\n    (_, r_1, *r_rrmaining) = r\n    w.input_trans.setText(r_1)', 'for (r_0, r_1, *r_len) in cursor:\n    w.input_trans.setText(r_1)']",no_found,0
inception,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/inception/inception/interfaces/slotscreamer.py,https://github.com/carmaa/inception/tree/master/inception/interfaces/slotscreamer.py,SlotScreamer,readv$109,"def readv(self, req):
    for r in req:
        yield (r[0], self.read(r[0], r[1]))","for r in req:
    yield (r[0], self.read(r[0], r[1]))","['for r in req:\n    (r_0, r_1, *_) = r\n    yield (r_0, self.read(r_0, r_1))', 'for (r_0, r_1, *r_len) in req:\n    yield (r_0, self.read(r_0, r_1))']",no_found,0
dulwich,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dulwich/dulwich/tests/compat/test_client.py,https://github.com/dulwich/dulwich/tree/master/dulwich/tests/compat/test_client.py,DulwichClientTestBase,test_fetch_pack_no_side_band_64k$303,"def test_fetch_pack_no_side_band_64k(self):
    c = self._client()
    c._fetch_capabilities.remove(b'side-band-64k')
    with repo.Repo(os.path.join(self.gitroot, 'dest')) as dest:
        result = c.fetch(self._build_path('/server_new.export'), dest)
        for r in result.refs.items():
            dest.refs.set_if_equals(r[0], None, r[1])
        self.assertDestEqualsSrc()","for r in result.refs.items():
    dest.refs.set_if_equals(r[0], None, r[1])","['for r in result.refs.items():\n    (r_0, r_1, *_) = r\n    dest.refs.set_if_equals(r_0, None, r_1)', 'for (r_0, r_1, *r_len) in result.refs.items():\n    dest.refs.set_if_equals(r_0, None, r_1)']",no_found,0
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/core/chords.py,https://github.com/bspaans/python-mingus/tree/master/mingus/core/chords.py,,inversion_exhauster$1183,"def inversion_exhauster(chord, shorthand, tries, result, polychords):
    """"""Recursive helper function""""""
    if tries == 1 and (not no_polychords):
        polychords += determine_polychords(chord, shorthand)

    def add_result(short):
        result.append((short, tries, chord[0]))
    ch = determine_extended_chord5(chord[:5], True, True, True)
    intval5 = intervals.determine(chord[0], chord[5])
    for c in ch:
        c = c[len(chord[0]):]
        if c == '9':
            if intval5 == 'perfect fourth':
                add_result('11')
            elif intval5 == 'augmented fourth':
                add_result('7#11')
            elif intval5 == 'major sixth':
                add_result('13')
        elif c == 'm9':
            if intval5 == 'perfect fourth':
                add_result('m11')
            elif intval5 == 'major sixth':
                add_result('m13')
        elif c == 'M9':
            if intval5 == 'perfect fourth':
                add_result('M11')
            elif intval5 == 'major sixth':
                add_result('M13')
    if tries != 6 and (not no_inversions):
        return inversion_exhauster([chord[-1]] + chord[:-1], shorthand, tries + 1, result, polychords)
    else:
        res = []
        for r in result:
            if shorthand:
                res.append(r[2] + r[0])
            else:
                res.append(r[2] + chord_shorthand_meaning[r[0]] + int_desc(r[1]))
        return res + polychords","for r in result:
    if shorthand:
        res.append(r[2] + r[0])
    else:
        res.append(r[2] + chord_shorthand_meaning[r[0]] + int_desc(r[1]))","['for r in result:\n    (r_0, r_1, r_2, *_) = r\n    if shorthand:\n        res.append(r_2 + r_0)\n    else:\n        res.append(r_2 + chord_shorthand_meaning[r_0] + int_desc(r_1))', 'for (r_0, r_1, r_2, *r_len) in result:\n    if shorthand:\n        res.append(r_2 + r_0)\n    else:\n        res.append(r_2 + chord_shorthand_meaning[r_0] + int_desc(r_1))']",no_found,0
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/cfg/cfg_emulated.py,https://github.com/angr/angr/tree/master/angr/analyses/cfg/cfg_emulated.py,CFGEmulated,_is_address_executable$3365,"def _is_address_executable(self, address):
    """"""
        Check if the specific address is in one of the executable ranges.

        :param int address: The address
        :return: True if it's in an executable range, False otherwise
        """"""
    for r in self._executable_address_ranges:
        if r[0] <= address < r[1]:
            return True
    return False","for r in self._executable_address_ranges:
    if r[0] <= address < r[1]:
        return True","['for r in self._executable_address_ranges:\n    (r_0, r_1, *_) = r\n    if r_0 <= address < r_1:\n        return True', 'for (r_0, r_1, *r_len) in self._executable_address_ranges:\n    if \n    r_0 <= address < \n    r_1:\n        return True']",no_found,0
BracketHighlighter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BracketHighlighter/tests/validate_json_format.py,https://github.com/facelessuser/BracketHighlighter/tree/master/tests/validate_json_format.py,CheckJsonFormat,get_line$106,"def get_line(self, pt):
    """"""Get the line from char index.""""""
    line = None
    for r in self.line_range:
        if pt >= r[0] and pt <= r[1]:
            line = r[2]
            break
    return line","for r in self.line_range:
    if pt >= r[0] and pt <= r[1]:
        line = r[2]
        break","['for r in self.line_range:\n    (r_0, r_1, r_2, *_) = r\n    if pt >= r_0 and pt <= r_1:\n        line = r_2\n        break', 'for (r_0, r_1, r_2, *r_len) in self.line_range:\n    if pt >= \n    r_0 and pt <= \n    r_1:\n        line = \n        r_2\n        break']",no_found,0
GitGutter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GitGutter/tests/test_json.py,https://github.com/jisaacks/GitGutter/tree/master/tests/test_json.py,CheckJsonFormat,get_line$111,"def get_line(self, pt):
    """"""Get the line from char index.""""""
    line = None
    for r in self.line_range:
        if pt >= r[0] and pt <= r[1]:
            line = r[2]
            break
    return line","for r in self.line_range:
    if pt >= r[0] and pt <= r[1]:
        line = r[2]
        break","['for r in self.line_range:\n    (r_0, r_1, r_2, *_) = r\n    if pt >= r_0 and pt <= r_1:\n        line = r_2\n        break', 'for (r_0, r_1, r_2, *r_len) in self.line_range:\n    if pt >= \n    r_0 and pt <= \n    r_1:\n        line = \n        r_2\n        break']",no_found,0
ReAgent,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ReAgent/reagent/ope/test/yandex_web_search.py,https://github.com/facebookresearch/ReAgent/tree/master/reagent/ope/test/yandex_web_search.py,LoggedQuery,click$82,"def click(self, url_id: int, dwell_time: int):
    self._position_relevances = None
    self._url_relevances = None
    i = 0
    for r in self.list:
        if url_id == r[0]:
            self.clicks.append((i, dwell_time))
            break
        i += 1","for r in self.list:
    if url_id == r[0]:
        self.clicks.append((i, dwell_time))
        break
    i += 1","['for r in self.list:\n    (r_0, *r_rrmaining) = r\n    if url_id == r_0:\n        self.clicks.append((i, dwell_time))\n        break\n    i += 1', 'for (r_0, *r_len) in self.list:\n    if url_id == \n    r_0:\n        self.clicks.append((i, dwell_time))\n        break\n    i += 1']",no_found,0
Home-Assistant-custom-components-Xiaomi-Cloud-Map-Extractor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Home-Assistant-custom-components-Xiaomi-Cloud-Map-Extractor/custom_components/xiaomi_cloud_map_extractor/roidmi/map_data_parser.py,https://github.com/PiotrMachowski/Home-Assistant-custom-components-Xiaomi-Cloud-Map-Extractor/tree/master/custom_components/xiaomi_cloud_map_extractor/roidmi/map_data_parser.py,MapDataParserRoidmi,parse_path$85,"def parse_path(map_info: dict) -> Path:
    path_points = []
    if 'posArray' in map_info:
        raw_points = json.loads(map_info['posArray'])
        for raw_point in raw_points:
            point = Point(raw_point[0], raw_point[1])
            path_points.append(point)
    return Path(None, None, None, [path_points])","for raw_point in raw_points:
    point = Point(raw_point[0], raw_point[1])
    path_points.append(point)","['for raw_point in raw_points:\n    (raw_point_0, raw_point_1, *_) = raw_point\n    point = Point(raw_point_0, raw_point_1)\n    path_points.append(point)', 'for (raw_point_0, raw_point_1, *raw_point_len) in raw_points:\n    point = Point(raw_point_0, raw_point_1)\n    path_points.append(point)']",no_found,0
LinOTP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LinOTP/linotp/lib/tokeniterator.py,https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/tokeniterator.py,TokenIterator,_get_tokens_in_realm$359,"def _get_tokens_in_realm(self, valid_realms):
    realm_id_tuples = db.session.query(Realm.id).filter(Realm.name.in_(valid_realms)).all()
    realm_ids = set()
    for realm_tuple in realm_id_tuples:
        realm_ids.add(realm_tuple[0])
    token_id_tuples = db.session.query(TokenRealm.token_id).filter(TokenRealm.realm_id.in_(realm_ids)).all()
    token_ids = set()
    for token_tuple in token_id_tuples:
        token_ids.add(token_tuple[0])
    return token_ids","for realm_tuple in realm_id_tuples:
    realm_ids.add(realm_tuple[0])","['for realm_tuple in realm_id_tuples:\n    (realm_tuple_0, *realm_tuple_rrealm_tuplemaining) = realm_tuple\n    realm_ids.add(realm_tuple_0)', 'for (realm_tuple_0, *realm_tuple_len) in realm_id_tuples:\n    realm_ids.add(realm_tuple_0)']",no_found,0
seahub,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/seahub/seahub/api2/endpoints/group_members.py,https://github.com/haiwen/seahub/tree/master/seahub/api2/endpoints/group_members.py,GroupMembersImport,post$391,"def post(self, request, group_id):
    """""" Import members from xlsx file

        Permission checking:
        1. group admin or owner.
        """"""
    xlsx_file = request.FILES.get('file', None)
    if not xlsx_file:
        error_msg = 'file can not be found.'
        return api_error(status.HTTP_400_BAD_REQUEST, error_msg)
    (file_type, ext) = get_file_type_and_ext(xlsx_file.name)
    if ext != 'xlsx':
        error_msg = file_type_error_msg(ext, 'xlsx')
        return api_error(status.HTTP_400_BAD_REQUEST, error_msg)
    group_id = int(group_id)
    group = ccnet_api.get_group(group_id)
    if not group:
        error_msg = _('Group does not exist')
        return api_error(status.HTTP_404_NOT_FOUND, error_msg)
    username = request.user.username
    if not is_group_admin_or_owner(group_id, username):
        error_msg = 'Permission denied.'
        return api_error(status.HTTP_403_FORBIDDEN, error_msg)
    content = xlsx_file.read()
    try:
        fs = BytesIO(content)
        wb = load_workbook(filename=fs, read_only=True)
    except Exception as e:
        logger.error(e)
    rows = wb.worksheets[0].rows
    records = []
    next(rows)
    for row in rows:
        records.append([col.value for col in row])
    emails_list = []
    for record in records:
        if record[0]:
            email = record[0].strip().lower()
            emails_list.append(email)
    result = {}
    result['failed'] = []
    result['success'] = []
    emails_need_add = []
    org_id = None
    if is_org_context(request):
        org_id = request.user.org.org_id
    for email in emails_list:
        email_name = email2nickname(email)
        try:
            User.objects.get(email=email)
        except User.DoesNotExist:
            result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': 'User %s not found.' % email_name})
            continue
        if is_group_member(group_id, email, in_structure=False):
            result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': _('User %s is already a group member.') % email_name})
            continue
        if org_id and (not ccnet_api.org_user_exists(org_id, email)):
            result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': _('User %s not found in organization.') % email_name})
            continue
        if not org_id and is_org_user(email):
            result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': _('User %s is an organization user.') % email_name})
            continue
        emails_need_add.append(email)
    for email in emails_need_add:
        try:
            ccnet_api.group_add_member(group_id, username, email)
            member_info = get_group_member_info(request, group_id, email)
            result['success'].append(member_info)
        except SearpcError as e:
            logger.error(e)
            result['failed'].append({'email': email, 'error_msg': 'Internal Server Error'})
        add_user_to_group.send(sender=None, group_staff=username, group_id=group_id, added_user=email)
    return Response(result)","for record in records:
    if record[0]:
        email = record[0].strip().lower()
        emails_list.append(email)","['for record in records:\n    (record_0, *record_rrecordmaining) = record\n    if record_0:\n        email = record_0.strip().lower()\n        emails_list.append(email)', 'for (record_0, *record_len) in records:\n    if \n    record_0:\n        email = \n        record_0.strip().lower()\n        emails_list.append(email)']",no_found,0
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli-core/azure/cli/core/commands/arm.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli-core/azure/cli/core/commands/arm.py,,_get_internal_path$637,"def _get_internal_path(path):
    path = path.replace('.[', '[').replace('[', '.[')
    path_segment_pairs = internal_path_regex.findall(path)
    final_paths = []
    for regex_result in path_segment_pairs:
        segment = regex_result[0] or regex_result[1]
        final_paths.append(segment)
    return final_paths","for regex_result in path_segment_pairs:
    segment = regex_result[0] or regex_result[1]
    final_paths.append(segment)","['for regex_result in path_segment_pairs:\n    (regex_result_0, regex_result_1, *_) = regex_result\n    segment = regex_result_0 or regex_result_1\n    final_paths.append(segment)', 'for (regex_result_0, regex_result_1, *regex_result_len) in path_segment_pairs:\n    segment = \n    regex_result_0 or \n    regex_result_1\n    final_paths.append(segment)']",no_found,0
GeneticAlgorithmsWithPython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GeneticAlgorithmsWithPython/es/ch16/pruebas.py,https://github.com/handcraftsman/GeneticAlgorithmsWithPython/tree/master/es/ch16/pruebas.py,,obtener_aptitud$26,"def obtener_aptitud(genes, reglas, entradas):
    circuito = nodos_a_circuito(genes)[0]
    etiquetasDeFuente = 'ABCD'
    reglasExitosas = 0
    for regla in reglas:
        entradas.clear()
        entradas.update(zip(etiquetasDeFuente, regla[0]))
        if circuito.obtener_salida() == regla[1]:
            reglasExitosas += 1
    return reglasExitosas","for regla in reglas:
    entradas.clear()
    entradas.update(zip(etiquetasDeFuente, regla[0]))
    if circuito.obtener_salida() == regla[1]:
        reglasExitosas += 1","['for regla in reglas:\n    (regla_0, regla_1, *_) = regla\n    entradas.clear()\n    entradas.update(zip(etiquetasDeFuente, regla_0))\n    if circuito.obtener_salida() == regla_1:\n        reglasExitosas += 1', 'for (regla_0, regla_1, *regla_len) in reglas:\n    entradas.clear()\n    entradas.update(zip(etiquetasDeFuente, \n    regla_0))\n    if circuito.obtener_salida() == \n    regla_1:\n        reglasExitosas += 1']",no_found,0
osroom,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/osroom/apps/core/flask/reqparse.py,https://github.com/osroom/osroom/tree/master/apps/core/flask/reqparse.py,ArgVerify,regex_rule$84,"def regex_rule(self, **kwargs):
    vr = kwargs.get('vr')
    if vr['is_match']:
        for reqarg in kwargs.get('reqargs'):
            if not re.search(vr['rule'], reqarg[1]):
                return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})
    else:
        for reqarg in kwargs.get('reqargs'):
            if re.search(vr['rule'], reqarg[1]):
                return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})
    return (True, None)","for reqarg in kwargs.get('reqargs'):
    if not re.search(vr['rule'], reqarg[1]):
        return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})","['for reqarg in kwargs.get(\'reqargs\'):\n    (reqarg_0, reqarg_1, *_) = reqarg\n    if not re.search(vr[\'rule\'], reqarg_1):\n        return (False, {\'msg\': gettext(\'The value of parameter ""{}"" is illegal\').format(reqarg_0), \'msg_type\': \'w\', \'custom_status\': 422})', 'for (reqarg_0, reqarg_1, *reqarg_len) in kwargs.get(\'reqargs\'):\n    if not re.search(vr[\'rule\'], reqarg_1):\n        return (False, {\'msg\': gettext(\'The value of parameter ""{}"" is illegal\').format(reqarg_0), \'msg_type\': \'w\', \'custom_status\': 422})']",no_found,0
osroom,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/osroom/apps/core/flask/reqparse.py,https://github.com/osroom/osroom/tree/master/apps/core/flask/reqparse.py,ArgVerify,regex_rule$84,"def regex_rule(self, **kwargs):
    vr = kwargs.get('vr')
    if vr['is_match']:
        for reqarg in kwargs.get('reqargs'):
            if not re.search(vr['rule'], reqarg[1]):
                return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})
    else:
        for reqarg in kwargs.get('reqargs'):
            if re.search(vr['rule'], reqarg[1]):
                return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})
    return (True, None)","for reqarg in kwargs.get('reqargs'):
    if re.search(vr['rule'], reqarg[1]):
        return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})","['for reqarg in kwargs.get(\'reqargs\'):\n    (reqarg_0, reqarg_1, *_) = reqarg\n    if re.search(vr[\'rule\'], reqarg_1):\n        return (False, {\'msg\': gettext(\'The value of parameter ""{}"" is illegal\').format(reqarg_0), \'msg_type\': \'w\', \'custom_status\': 422})', 'for (reqarg_0, reqarg_1, *reqarg_len) in kwargs.get(\'reqargs\'):\n    if re.search(vr[\'rule\'], reqarg_1):\n        return (False, {\'msg\': gettext(\'The value of parameter ""{}"" is illegal\').format(reqarg_0), \'msg_type\': \'w\', \'custom_status\': 422})']",no_found,0
security_monkey,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/security_monkey/security_monkey/cloudaux_watcher.py,https://github.com/Netflix/security_monkey/tree/master/security_monkey/cloudaux_watcher.py,CloudAuxWatcher,_flatten_iter_response$59,"def _flatten_iter_response(self, response):
    """"""
        The cloudaux iter_account_region decorator returns a list of tuples.
        Each tuple contains two members.  1) The result. 2) The exception map.
        This method combines that list of tuples into a single result list and a single exception map.
        """"""
    items = list()
    exception_map = dict()
    for result in response:
        items.extend(result[0])
        exception_map.update(result[1])
    return (items, exception_map)","for result in response:
    items.extend(result[0])
    exception_map.update(result[1])","['for result in response:\n    (result_0, result_1, *_) = result\n    items.extend(result_0)\n    exception_map.update(result_1)', 'for (result_0, result_1, *result_len) in response:\n    items.extend(result_0)\n    exception_map.update(result_1)']",no_found,0
django-notifications,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-notifications/notifications/tests/tests.py,https://github.com/django-notifications/django-notifications/tree/master/notifications/tests/tests.py,NotificationManagersTest,test_notify_send_return_val_group$102,"def test_notify_send_return_val_group(self):
    results = notify.send(self.from_user, recipient=self.to_group, verb='commented', action_object=self.from_user)
    for result in results:
        if result[0] is notify_handler:
            self.assertEqual(len(result[1]), self.to_group.user_set.count())
            for notification in result[1]:
                self.assertEqual(type(notification), Notification)","for result in results:
    if result[0] is notify_handler:
        self.assertEqual(len(result[1]), self.to_group.user_set.count())
        for notification in result[1]:
            self.assertEqual(type(notification), Notification)","['for result in results:\n    (result_0, result_1, *_) = result\n    if result_0 is notify_handler:\n        self.assertEqual(len(result_1), self.to_group.user_set.count())\n        for notification in result_1:\n            self.assertEqual(type(notification), Notification)', 'for (result_0, result_1, *result_len) in results:\n    if \n    result_0 is notify_handler:\n        self.assertEqual(len(\n        result_1), self.to_group.user_set.count())\n        for notification in \n        result_1:\n            self.assertEqual(type(notification), Notification)']",no_found,0
Archery,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Archery/sql/engines/oracle.py,https://github.com/hhyo/Archery/tree/master/sql/engines/oracle.py,OracleEngine,get_kill_command$1429,"def get_kill_command(self, thread_ids):
    """"""sid+serial#kill""""""
    if [k for k in [[j for j in i if not isinstance(j, int)] for i in thread_ids] if k]:
        return None
    sql = ""select 'alter system kill session ' || '''' || s.sid || ',' || s.serial# || '''' || ' immediate' || ';'\n                 from v$process p, v$session s, v$sqlarea q\n                 where p.addr = s.paddr\n                 and s.sql_hash_value = q.hash_value\n                 and s.sid || ',' || s.serial# in ({});"".format(','.join((f""'{str(tid[0])},{str(tid[1])}'"" for tid in thread_ids)))
    all_kill_sql = self.query(sql=sql)
    kill_sql = ''
    for row in all_kill_sql.rows:
        kill_sql = kill_sql + row[0]
    return kill_sql","for row in all_kill_sql.rows:
    kill_sql = kill_sql + row[0]","['for row in all_kill_sql.rows:\n    (row_0, *row_rrowmaining) = row\n    kill_sql = kill_sql + row_0', 'for (row_0, *row_len) in all_kill_sql.rows:\n    kill_sql = kill_sql + \n    row_0']",no_found,0
coveragepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coveragepy/coverage/sqldata.py,https://github.com/nedbat/coveragepy/tree/master/coverage/sqldata.py,CoverageData,_read_db$269,"def _read_db(self):
    """"""Read the metadata from a database so that we are ready to use it.""""""
    with self._dbs[threading.get_ident()] as db:
        try:
            (schema_version,) = db.execute_one('select version from coverage_schema')
        except Exception as exc:
            if 'no such table: coverage_schema' in str(exc):
                self._init_db(db)
            else:
                raise DataError(""Data file {!r} doesn't seem to be a coverage data file: {}"".format(self._filename, exc)) from exc
        else:
            if schema_version != SCHEMA_VERSION:
                raise DataError(""Couldn't use data file {!r}: wrong schema: {} instead of {}"".format(self._filename, schema_version, SCHEMA_VERSION))
        with db.execute(""select value from meta where key = 'has_arcs'"") as cur:
            for row in cur:
                self._has_arcs = bool(int(row[0]))
                self._has_lines = not self._has_arcs
        with db.execute('select id, path from file') as cur:
            for (file_id, path) in cur:
                self._file_map[path] = file_id","for row in cur:
    self._has_arcs = bool(int(row[0]))
    self._has_lines = not self._has_arcs","['for row in cur:\n    (row_0, *row_rrowmaining) = row\n    self._has_arcs = bool(int(row_0))\n    self._has_lines = not self._has_arcs', 'for (row_0, *row_len) in cur:\n    self._has_arcs = bool(int(row_0))\n    self._has_lines = not self._has_arcs']",no_found,0
tartube,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tartube/tartube/config.py,https://github.com/axcore/tartube/tree/master/tartube/config.py,SystemPrefWin,on_data_dir_move_up_button_clicked$28440,"def on_data_dir_move_up_button_clicked(self, button, treeview, liststore, button2):
    """"""Called from callback in self.setup_files_database_tab().

        Moves the selected data directory up one position in the list of
        alternative data directories.

        Args:

            button (Gtk.Button): The widget that was clicked (the up button)

            treeview (Gtk.TreeView): The widget in which a line was selected

            liststore (Gtk.ListStore): The treeview's liststore

            button2 (Gtk.Button): The down button

        """"""
    selection = treeview.get_selection()
    (model, path_list) = selection.get_selected_rows()
    if not path_list:
        return
    first_item = None
    last_item = None
    for path in path_list:
        this_iter = model.get_iter(path)
        last_item = model[this_iter][0]
        if first_item is None:
            first_item = model[this_iter][0]
        if model.iter_previous(this_iter):
            liststore.move_before(this_iter, model.iter_previous(this_iter))
        else:
            break
    dir_list = []
    for row in liststore:
        dir_list.append(row[0])
    self.app_obj.set_data_dir_alt_list(dir_list)
    if dir_list.index(first_item) == 0:
        button.set_sensitive(False)
    else:
        button.set_sensitive(True)
    if dir_list.index(last_item) == len(dir_list) - 1:
        button2.set_sensitive(False)
    else:
        button2.set_sensitive(True)","for row in liststore:
    dir_list.append(row[0])","['for row in liststore:\n    (row_0, *row_rrowmaining) = row\n    dir_list.append(row_0)', 'for (row_0, *row_len) in liststore:\n    dir_list.append(row_0)']",no_found,0
moviepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moviepy/tests/test_compositing.py,https://github.com/Zulko/moviepy/tree/master/tests/test_compositing.py,,test_slide_out$173,"def test_slide_out():
    duration = 0.1
    size = (11, 1)
    fps = 10
    color = (255, 0, 0)
    clip = ColorClip(color=color, duration=duration, size=size).with_fps(fps)
    for side in ['left', 'right']:
        new_clip = CompositeVideoClip([slide_out(clip, duration, side)])
        for t in np.arange(0, duration, duration / fps):
            (n_reds, n_reds_expected) = (0, round(11 - t * 100, 6))
            if t:
                assert n_reds_expected
            for (r, g, b) in new_clip.get_frame(t)[0]:
                if r == color[0] and g == color[1] and (g == color[2]):
                    n_reds += 1
            assert n_reds == n_reds_expected
    clip = ColorClip(color=color, duration=duration, size=(size[1], size[0])).with_fps(fps)
    for side in ['top', 'bottom']:
        new_clip = CompositeVideoClip([slide_out(clip, duration, side)])
        for t in np.arange(0, duration, duration / fps):
            (n_reds, n_reds_expected) = (0, round(11 - t * 100, 6))
            if t:
                assert n_reds_expected
            for row in new_clip.get_frame(t):
                (r, g, b) = row[0]
                if r == color[0] and g == color[1] and (g == color[2]):
                    n_reds += 1
            assert n_reds == n_reds_expected","for row in new_clip.get_frame(t):
    (r, g, b) = row[0]
    if r == color[0] and g == color[1] and (g == color[2]):
        n_reds += 1","['for row in new_clip.get_frame(t):\n    (row_0, *row_rrowmaining) = row\n    (r, g, b) = row_0\n    if r == color[0] and g == color[1] and (g == color[2]):\n        n_reds += 1', 'for (row_0, *row_len) in new_clip.get_frame(t):\n    (r, g, b) = \n    row_0\n    if r == color[0] and g == color[1] and (g == color[2]):\n        n_reds += 1']",no_found,0
openstates-scrapers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openstates-scrapers/scrapers/md/bills.py,https://github.com/openstates/openstates-scrapers/tree/master/scrapers/md/bills.py,MDBillScraper,scrape_bill_subjects$341,"def scrape_bill_subjects(self, bill, page):
    for row in page.xpath('//a[contains(@href, ""/Legislation/SubjectIndex/annotac"")]/text()'):
        bill.add_subject(row[0])","for row in page.xpath('//a[contains(@href, ""/Legislation/SubjectIndex/annotac"")]/text()'):
    bill.add_subject(row[0])","['for row in page.xpath(\'//a[contains(@href, ""/Legislation/SubjectIndex/annotac"")]/text()\'):\n    (row_0, *row_rrowmaining) = row\n    bill.add_subject(row_0)', 'for (row_0, *row_len) in page.xpath(\'//a[contains(@href, ""/Legislation/SubjectIndex/annotac"")]/text()\'):\n    bill.add_subject(row_0)']",no_found,0
FeatureLearningRotNet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FeatureLearningRotNet/dataloader.py,https://github.com/gidariss/FeatureLearningRotNet/tree/master//dataloader.py,Places205,__init__$37,"def __init__(self, root, split, transform=None, target_transform=None):
    self.root = os.path.expanduser(root)
    self.data_folder = os.path.join(self.root, 'data', 'vision', 'torralba', 'deeplearning', 'images256')
    self.split_folder = os.path.join(self.root, 'trainvalsplit_places205')
    assert split == 'train' or split == 'val'
    split_csv_file = os.path.join(self.split_folder, split + '_places205.csv')
    self.transform = transform
    self.target_transform = target_transform
    with open(split_csv_file, 'rb') as f:
        reader = csv.reader(f, delimiter=' ')
        self.img_files = []
        self.labels = []
        for row in reader:
            self.img_files.append(row[0])
            self.labels.append(long(row[1]))","for row in reader:
    self.img_files.append(row[0])
    self.labels.append(long(row[1]))","['for row in reader:\n    (row_0, row_1, *_) = row\n    self.img_files.append(row_0)\n    self.labels.append(long(row_1))', 'for (row_0, row_1, *row_len) in reader:\n    self.img_files.append(row_0)\n    self.labels.append(long(row_1))']",no_found,0
dnsrecon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dnsrecon/tools/parser.py,https://github.com/darkoperator/dnsrecon/tree/master/tools/parser.py,,extract_hostnames$177,"def extract_hostnames(file):
    host_names = []
    hostname_pattern = re.compile('(^[^.]*)')
    file_type = detect_type(file)
    if file_type == 'xml':
        for (event, elem) in cElementTree.iterparse(file):
            if elem.tag == 'record':
                if 'address' in elem.attrib:
                    if re.search('PTR|^[A]$|AAAA', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['name']).group(1))
                    elif re.search('NS', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['target']).group(1))
                    elif re.search('SOA', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['mname']).group(1))
                    elif re.search('MX', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['exchange']).group(1))
                    elif re.search('SRV', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['target']).group(1))
    elif file_type == 'csv':
        reader = csv.reader(open(file, 'r'), delimiter=',')
        reader.next()
        for row in reader:
            host_names.append(re.search(hostname_pattern, row[1]).group(1))
    host_names = list(set(host_names))
    return filter(None, host_names)","for row in reader:
    host_names.append(re.search(hostname_pattern, row[1]).group(1))","['for row in reader:\n    (row_0, row_1, *row_rrowmaining) = row\n    host_names.append(re.search(hostname_pattern, row_1).group(1))', 'for (row_0, row_1, *row_len) in reader:\n    host_names.append(re.search(hostname_pattern, row_1).group(1))']",no_found,0
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,,downgrade$83,"def downgrade(migrate_engine):
    metadata.bind = migrate_engine
    metadata.reflect()
    Table('form_definition', metadata, autoload=True)
    Table('form_values', metadata, autoload=True)
    cmd = 'SELECT form_values.id, form_values.content, form_definition.fields FROM form_values, form_definition WHERE form_values.form_definition_id=form_definition.id ORDER BY form_values.id ASC'
    result = migrate_engine.execute(cmd)
    for row in result:
        form_values_id = int(row[0])
        if not str(row[1]).strip():
            continue
        values_dict = loads(str(row[1]))
        if not str(row[2]).strip():
            continue
        fields_list = loads(str(row[2]))
        if fields_list:
            values_list = []
            for field in fields_list:
                field_name = field['name']
                field_value = values_dict[field_name]
                values_list.append(field_value)
            cmd = ""UPDATE form_values SET content='%s' WHERE id=%i"" % (dumps(values_list), form_values_id)
            migrate_engine.execute(cmd)
    cmd = 'SELECT f.id, f.fields FROM form_definition AS f'
    result = migrate_engine.execute(cmd)
    for row in result:
        form_definition_id = row[0]
        fields = str(row[1])
        if not fields.strip():
            continue
        fields_list = loads(_sniffnfix_pg9_hex(fields))
        if len(fields_list):
            for field in fields_list:
                if 'name' in field:
                    del field['name']
            if migrate_engine.name == 'mysql':
                cmd = ""UPDATE form_definition AS f SET f.fields='%s' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)
            else:
                cmd = ""UPDATE form_definition SET fields='%s' WHERE id=%i"" % (dumps(fields_list), form_definition_id)
        migrate_engine.execute(cmd)","for row in result:
    form_definition_id = row[0]
    fields = str(row[1])
    if not fields.strip():
        continue
    fields_list = loads(_sniffnfix_pg9_hex(fields))
    if len(fields_list):
        for field in fields_list:
            if 'name' in field:
                del field['name']
        if migrate_engine.name == 'mysql':
            cmd = ""UPDATE form_definition AS f SET f.fields='%s' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)
        else:
            cmd = ""UPDATE form_definition SET fields='%s' WHERE id=%i"" % (dumps(fields_list), form_definition_id)
    migrate_engine.execute(cmd)","['for row in result:\n    (row_0, row_1, *_) = row\n    form_definition_id = row_0\n    fields = str(row_1)\n    if not fields.strip():\n        continue\n    fields_list = loads(_sniffnfix_pg9_hex(fields))\n    if len(fields_list):\n        for field in fields_list:\n            if \'name\' in field:\n                del field[\'name\']\n        if migrate_engine.name == \'mysql\':\n            cmd = ""UPDATE form_definition AS f SET f.fields=\'%s\' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)\n        else:\n            cmd = ""UPDATE form_definition SET fields=\'%s\' WHERE id=%i"" % (dumps(fields_list), form_definition_id)\n    migrate_engine.execute(cmd)', 'for (row_0, row_1, *row_len) in result:\n    form_definition_id = \n    row_0\n    fields = str(\n    row_1)\n    if not fields.strip():\n        continue\n    fields_list = loads(_sniffnfix_pg9_hex(fields))\n    if len(fields_list):\n        for field in fields_list:\n            if \'name\' in field:\n                del field[\'name\']\n        if migrate_engine.name == \'mysql\':\n            cmd = ""UPDATE form_definition AS f SET f.fields=\'%s\' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)\n        else:\n            cmd = ""UPDATE form_definition SET fields=\'%s\' WHERE id=%i"" % (dumps(fields_list), form_definition_id)\n    migrate_engine.execute(cmd)']",no_found,0
python-driver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-driver/tests/integration/standard/test_row_factories.py,https://github.com/datastax/python-driver/tree/master/tests/integration/standard/test_row_factories.py,RowFactoryTests,test_tuple_factory$93,"def test_tuple_factory(self):
    result = self._results_from_row_factory(tuple_factory)
    self.assertIsInstance(result, ResultSet)
    self.assertIsInstance(result[0], tuple)
    for row in result:
        self.assertEqual(row[0], row[1])
    self.assertEqual(result[0][0], result[0][1])
    self.assertEqual(result[0][0], 1)
    self.assertEqual(result[1][0], result[1][1])
    self.assertEqual(result[1][0], 2)","for row in result:
    self.assertEqual(row[0], row[1])","['for row in result:\n    (row_0, row_1, *_) = row\n    self.assertEqual(row_0, row_1)', 'for (row_0, row_1, *row_len) in result:\n    self.assertEqual(row_0, row_1)']",no_found,0
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,,downgrade$83,"def downgrade(migrate_engine):
    metadata.bind = migrate_engine
    metadata.reflect()
    Table('form_definition', metadata, autoload=True)
    Table('form_values', metadata, autoload=True)
    cmd = 'SELECT form_values.id, form_values.content, form_definition.fields FROM form_values, form_definition WHERE form_values.form_definition_id=form_definition.id ORDER BY form_values.id ASC'
    result = migrate_engine.execute(cmd)
    for row in result:
        form_values_id = int(row[0])
        if not str(row[1]).strip():
            continue
        values_dict = loads(str(row[1]))
        if not str(row[2]).strip():
            continue
        fields_list = loads(str(row[2]))
        if fields_list:
            values_list = []
            for field in fields_list:
                field_name = field['name']
                field_value = values_dict[field_name]
                values_list.append(field_value)
            cmd = ""UPDATE form_values SET content='%s' WHERE id=%i"" % (dumps(values_list), form_values_id)
            migrate_engine.execute(cmd)
    cmd = 'SELECT f.id, f.fields FROM form_definition AS f'
    result = migrate_engine.execute(cmd)
    for row in result:
        form_definition_id = row[0]
        fields = str(row[1])
        if not fields.strip():
            continue
        fields_list = loads(_sniffnfix_pg9_hex(fields))
        if len(fields_list):
            for field in fields_list:
                if 'name' in field:
                    del field['name']
            if migrate_engine.name == 'mysql':
                cmd = ""UPDATE form_definition AS f SET f.fields='%s' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)
            else:
                cmd = ""UPDATE form_definition SET fields='%s' WHERE id=%i"" % (dumps(fields_list), form_definition_id)
        migrate_engine.execute(cmd)","for row in result:
    form_values_id = int(row[0])
    if not str(row[1]).strip():
        continue
    values_dict = loads(str(row[1]))
    if not str(row[2]).strip():
        continue
    fields_list = loads(str(row[2]))
    if fields_list:
        values_list = []
        for field in fields_list:
            field_name = field['name']
            field_value = values_dict[field_name]
            values_list.append(field_value)
        cmd = ""UPDATE form_values SET content='%s' WHERE id=%i"" % (dumps(values_list), form_values_id)
        migrate_engine.execute(cmd)","['for row in result:\n    (row_0, row_1, row_2, *_) = row\n    form_values_id = int(row_0)\n    if not str(row_1).strip():\n        continue\n    values_dict = loads(str(row_1))\n    if not str(row_2).strip():\n        continue\n    fields_list = loads(str(row_2))\n    if fields_list:\n        values_list = []\n        for field in fields_list:\n            field_name = field[\'name\']\n            field_value = values_dict[field_name]\n            values_list.append(field_value)\n        cmd = ""UPDATE form_values SET content=\'%s\' WHERE id=%i"" % (dumps(values_list), form_values_id)\n        migrate_engine.execute(cmd)', 'for (row_0, row_1, row_2, *row_len) in result:\n    form_values_id = int(row_0)\n    if not str(row_1).strip():\n        continue\n    values_dict = loads(str(row_1))\n    if not str(row_2).strip():\n        continue\n    fields_list = loads(str(row_2))\n    if fields_list:\n        values_list = []\n        for field in fields_list:\n            field_name = field[\'name\']\n            field_value = values_dict[field_name]\n            values_list.append(field_value)\n        cmd = ""UPDATE form_values SET content=\'%s\' WHERE id=%i"" % (dumps(values_list), form_values_id)\n        migrate_engine.execute(cmd)']",no_found,0
PornHub-downloader-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PornHub-downloader-python/functions.py,https://github.com/mariosemes/PornHub-downloader-python/tree/master//functions.py,,dl_all_new_items$173,"def dl_all_new_items(conn):
    c = conn.cursor()
    try:
        c.execute(""SELECT * FROM ph_items WHERE new='1'"")
    except Error as e:
        print(e)
        sys.exit()
    rows = c.fetchall()
    for row in rows:
        if str(row[1]) == 'model':
            url_after = '/videos/upload'
        elif str(row[1]) == 'users':
            url_after = '/videos/public'
        elif str(row[1]) == 'channels':
            url_after = '/videos'
        else:
            url_after = ''
        print('-----------------------------')
        print(row[1])
        print(row[2])
        print('https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after)
        print('-----------------------------')
        outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
        ydl_opts = {'format': 'best', 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
        url = 'https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after
        with youtube_dl.YoutubeDL(ydl_opts) as ydl:
            ydl.download([url])
        try:
            c.execute(""UPDATE ph_items SET new='0', lastchecked=CURRENT_TIMESTAMP WHERE url_name=?"", (row[2],))
            conn.commit()
        except Error as e:
            print(e)
            sys.exit()","for row in rows:
    if str(row[1]) == 'model':
        url_after = '/videos/upload'
    elif str(row[1]) == 'users':
        url_after = '/videos/public'
    elif str(row[1]) == 'channels':
        url_after = '/videos'
    else:
        url_after = ''
    print('-----------------------------')
    print(row[1])
    print(row[2])
    print('https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after)
    print('-----------------------------')
    outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
    ydl_opts = {'format': 'best', 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
    url = 'https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after
    with youtube_dl.YoutubeDL(ydl_opts) as ydl:
        ydl.download([url])
    try:
        c.execute(""UPDATE ph_items SET new='0', lastchecked=CURRENT_TIMESTAMP WHERE url_name=?"", (row[2],))
        conn.commit()
    except Error as e:
        print(e)
        sys.exit()","['for row in rows:\n    (_, row_1, row_2, row_3, *_) = row\n    if str(row_1) == \'model\':\n        url_after = \'/videos/upload\'\n    elif str(row_1) == \'users\':\n        url_after = \'/videos/public\'\n    elif str(row_1) == \'channels\':\n        url_after = \'/videos\'\n    else:\n        url_after = \'\'\n    print(\'-----------------------------\')\n    print(row_1)\n    print(row_2)\n    print(\'https://www.pornhub.com/\' + str(row_1) + \'/\' + str(row_2) + url_after)\n    print(\'-----------------------------\')\n    outtmpl = get_dl_location(\'DownloadLocation\') + \'/\' + str(row_1) + \'/\' + str(row_3) + \'/%(title)s.%(ext)s\'\n    ydl_opts = {\'format\': \'best\', \'outtmpl\': outtmpl, \'nooverwrites\': True, \'no_warnings\': False, \'ignoreerrors\': True}\n    url = \'https://www.pornhub.com/\' + str(row_1) + \'/\' + str(row_2) + url_after\n    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n        ydl.download([url])\n    try:\n        c.execute(""UPDATE ph_items SET new=\'0\', lastchecked=CURRENT_TIMESTAMP WHERE url_name=?"", (row_2,))\n        conn.commit()\n    except Error as e:\n        print(e)\n        sys.exit()', 'for (row_0, row_1, row_2, row_3, *row_len) in rows:\n    if str(row_1) == \'model\':\n        url_after = \'/videos/upload\'\n    elif str(row_1) == \'users\':\n        url_after = \'/videos/public\'\n    elif str(row_1) == \'channels\':\n        url_after = \'/videos\'\n    else:\n        url_after = \'\'\n    print(\'-----------------------------\')\n    print(row_1)\n    print(row_2)\n    print(\'https://www.pornhub.com/\' + str(row_1) + \'/\' + str(row_2) + url_after)\n    print(\'-----------------------------\')\n    outtmpl = get_dl_location(\'DownloadLocation\') + \'/\' + str(row_1) + \'/\' + str(row_3) + \'/%(title)s.%(ext)s\'\n    ydl_opts = {\'format\': \'best\', \'outtmpl\': outtmpl, \'nooverwrites\': True, \'no_warnings\': False, \'ignoreerrors\': True}\n    url = \'https://www.pornhub.com/\' + str(row_1) + \'/\' + str(row_2) + url_after\n    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n        ydl.download([url])\n    try:\n        c.execute(""UPDATE ph_items SET new=\'0\', lastchecked=CURRENT_TIMESTAMP WHERE url_name=?"", (row_2,))\n        conn.commit()\n    except Error as e:\n        print(e)\n        sys.exit()']",no_found,0
deluge,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deluge/deluge/ui/gtk3/addtorrentdialog.py,https://github.com/deluge-torrent/deluge/tree/master/deluge/ui/gtk3/addtorrentdialog.py,AddTorrentDialog,_on_filename_edited$986,"def _on_filename_edited(self, renderer, path, new_text):
    index = self.files_treestore[path][3]
    new_text = new_text.strip(os.path.sep).strip()
    if new_text == self.files_treestore[path][1]:
        return
    itr = self.files_treestore.get_iter(path)
    (model, row) = self.listview_torrents.get_selection().get_selected()
    torrent_id = model[row][0]
    if 'mapped_files' not in self.options[torrent_id]:
        self.options[torrent_id]['mapped_files'] = {}
    if index > -1:
        if not new_text:
            return
        parent = self.files_treestore.iter_parent(itr)
        file_path = os.path.join(self.get_file_path(parent), new_text)
        if parent:
            for row in self.files_treestore[parent].iterchildren():
                if new_text == row[1]:
                    return
        if os.path.sep in new_text:
            split_text = new_text.split(os.path.sep)
            for s in split_text[:-1]:
                parent = self.files_treestore.append(parent, [True, s, 0, -1, False, 'folder-symbolic'])
            self.files_treestore[itr][1] = split_text[-1]
            reparent_iter(self.files_treestore, itr, parent)
        else:
            self.files_treestore[itr][1] = new_text
        self.options[torrent_id]['mapped_files'][index] = file_path
        self.files[torrent_id][index]['path'] = file_path
    else:

        def walk_tree(row):
            if not row:
                return
            file_path_base = self.get_file_path(self.files_treestore.iter_parent(row))
            while row:
                if self.files_treestore.iter_has_child(row):
                    walk_tree(self.files_treestore.iter_children(row))
                index = self.files_treestore[row][3]
                if index > -1:
                    file_path = file_path_base + self.files_treestore[row][1]
                    self.options[torrent_id]['mapped_files'][index] = file_path
                    self.files[torrent_id][index]['path'] = file_path
                row = self.files_treestore.iter_next(row)
        if os.path.sep in new_text:
            parent = self.files_treestore.iter_parent(itr)
            split_text = new_text.split(os.path.sep)
            for s in split_text[:-1]:
                parent = self.files_treestore.append(parent, [True, s + os.path.sep, 0, -1, False, 'folder-symbolic'])
            self.files_treestore[itr][1] = split_text[-1] + os.path.sep
            reparent_iter(self.files_treestore, itr, parent)
            itr = parent
            root = Gtk.TreePath.new_first()
            self.listview_files.expand_row(root, False)
        else:
            self.files_treestore[itr][1] = new_text + os.path.sep
        walk_tree(itr)","for row in self.files_treestore[parent].iterchildren():
    if new_text == row[1]:
        return","['for row in self.files_treestore[parent].iterchildren():\n    (_, row_1, *row_rrowmaining) = row\n    if new_text == row_1:\n        return', 'for (row_0, row_1, *row_len) in self.files_treestore[parent].iterchildren():\n    if new_text == \n    row_1:\n        return']",no_found,0
neutron,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neutron/neutron/db/migration/alembic_migrations/versions/ussuri/expand/Ibac91d24da2_port_forwarding_description.py,https://github.com/openstack/neutron/tree/master/neutron/db/migration/alembic_migrations/versions/ussuri/expand/Ibac91d24da2_port_forwarding_description.py,,update_existing_records$45,"def update_existing_records():
    session = sa.orm.Session(bind=op.get_bind())
    with session.begin(subtransactions=True):
        for row in session.query(TABLE_MODEL):
            res = session.execute(STDATTRS_TABLE.insert().values(resource_type=TABLE_NAME))
            session.execute(TABLE_MODEL.update().values(standard_attr_id=res.inserted_primary_key[0]).where(TABLE_MODEL.c.id == row[0]))
    session.commit()","for row in session.query(TABLE_MODEL):
    res = session.execute(STDATTRS_TABLE.insert().values(resource_type=TABLE_NAME))
    session.execute(TABLE_MODEL.update().values(standard_attr_id=res.inserted_primary_key[0]).where(TABLE_MODEL.c.id == row[0]))","['for row in session.query(TABLE_MODEL):\n    (row_0, *row_rrowmaining) = row\n    res = session.execute(STDATTRS_TABLE.insert().values(resource_type=TABLE_NAME))\n    session.execute(TABLE_MODEL.update().values(standard_attr_id=res.inserted_primary_key[0]).where(TABLE_MODEL.c.id == row_0))', 'for (row_0, *row_len) in session.query(TABLE_MODEL):\n    res = session.execute(STDATTRS_TABLE.insert().values(resource_type=TABLE_NAME))\n    session.execute(TABLE_MODEL.update().values(standard_attr_id=res.inserted_primary_key[0]).where(TABLE_MODEL.c.id == row_0))']",no_found,0
pyglossary,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglossary/pyglossary/text_utils.py,https://github.com/ilius/pyglossary/tree/master/pyglossary/text_utils.py,,replace$56,"def replace(st: str) -> str:
    for rpl in rplList:
        st = st.replace(rpl[0], rpl[1])
    return st","for rpl in rplList:
    st = st.replace(rpl[0], rpl[1])","['for rpl in rplList:\n    (rpl_0, rpl_1, *_) = rpl\n    st = st.replace(rpl_0, rpl_1)', 'for (rpl_0, rpl_1, *rpl_len) in rplList:\n    st = st.replace(rpl_0, rpl_1)']",no_found,0
PaddleVideo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleVideo/paddlevideo/utils/multigrid/multigrid.py,https://github.com/PaddlePaddle/PaddleVideo/tree/master/paddlevideo/utils/multigrid/multigrid.py,MultigridSchedule,get_long_cycle_schedule$110,"def get_long_cycle_schedule(self, cfg):
    """"""
        Based on multigrid hyperparameters, define the schedule of a long cycle.
        Args:
            cfg (configs): configs that contains training and multigrid specific
                hyperparameters.
        Returns:
            schedule (list): Specifies a list long cycle base shapes and their
                corresponding training epochs.
        """"""
    steps = cfg.OPTIMIZER.learning_rate.steps
    default_size = float(cfg.PIPELINE.train.decode_sampler.num_frames * cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size'] ** 2)
    default_iters = steps[-1]
    avg_bs = []
    all_shapes = []
    for item in cfg.MULTIGRID.long_cycle_factors:
        (t_factor, s_factor) = item['value']
        base_t = int(round(cfg.PIPELINE.train.decode_sampler.num_frames * t_factor))
        base_s = int(round(cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size'] * s_factor))
        if cfg.MULTIGRID.SHORT_CYCLE:
            shapes = [[base_t, cfg.MULTIGRID.default_crop_size * cfg.MULTIGRID.short_cycle_factors[0]], [base_t, cfg.MULTIGRID.default_crop_size * cfg.MULTIGRID.short_cycle_factors[1]], [base_t, base_s]]
        else:
            shapes = [[base_t, base_s]]
        shapes = [[int(round(default_size / (s[0] * s[1] * s[1]))), s[0], s[1]] for s in shapes]
        avg_bs.append(np.mean([s[0] for s in shapes]))
        all_shapes.append(shapes)
    total_iters = 0
    schedule = []
    for step_index in range(len(steps) - 1):
        step_epochs = steps[step_index + 1] - steps[step_index]
        for (long_cycle_index, shapes) in enumerate(all_shapes):
            cur_epochs = step_epochs * avg_bs[long_cycle_index] / sum(avg_bs)
            cur_iters = cur_epochs / avg_bs[long_cycle_index]
            total_iters += cur_iters
            schedule.append((step_index, shapes[-1], cur_epochs))
    iter_saving = default_iters / total_iters
    final_step_epochs = cfg.OPTIMIZER.learning_rate.max_epoch - steps[-1]
    ft_epochs = final_step_epochs / iter_saving * avg_bs[-1]
    schedule.append((step_index + 1, all_shapes[-1][-1], ft_epochs))
    x = cfg.OPTIMIZER.learning_rate.max_epoch * cfg.MULTIGRID.epoch_factor / sum((s[-1] for s in schedule))
    final_schedule = []
    total_epochs = 0
    for s in schedule:
        epochs = s[2] * x
        total_epochs += epochs
        final_schedule.append((s[0], s[1], int(round(total_epochs))))
    print_schedule(final_schedule)
    return final_schedule","for s in schedule:
    epochs = s[2] * x
    total_epochs += epochs
    final_schedule.append((s[0], s[1], int(round(total_epochs))))","['for s in schedule:\n    (s_0, s_1, s_2, *_) = s\n    epochs = s_2 * x\n    total_epochs += epochs\n    final_schedule.append((s_0, s_1, int(round(total_epochs))))', 'for (s_0, s_1, s_2, *s_len) in schedule:\n    epochs = \n    s_2 * x\n    total_epochs += epochs\n    final_schedule.append((\n    s_0, \n    s_1, int(round(total_epochs))))']",no_found,0
faker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/faker/tests/providers/test_ssn.py,https://github.com/joke2k/faker/tree/master/tests/providers/test_ssn.py,TestTrTr,first_part_non_zero$1041,"def first_part_non_zero(self):
    for sample in self.samples:
        assert sample[0] != 0","for sample in self.samples:
    assert sample[0] != 0","['for sample in self.samples:\n    (sample_0, *sample_rsamplemaining) = sample\n    assert sample_0 != 0', 'for (sample_0, *sample_len) in self.samples:\n    assert \n    sample_0 != 0']",no_found,0
pinax-blog,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pinax-blog/pinax/blog/migrations/0003_auto_20150529_0405.py,https://github.com/pinax/pinax-blog/tree/master/pinax/blog/migrations/0003_auto_20150529_0405.py,,seed_sections$8,"def seed_sections(apps, schema_editor):
    Section = apps.get_model('blog', 'Section')
    db_alias = schema_editor.connection.alias
    for section in settings.PINAX_BLOG_SECTIONS:
        Section.objects.using(db_alias).create(slug=section[0], name=section[1])","for section in settings.PINAX_BLOG_SECTIONS:
    Section.objects.using(db_alias).create(slug=section[0], name=section[1])","['for section in settings.PINAX_BLOG_SECTIONS:\n    (section_0, section_1, *_) = section\n    Section.objects.using(db_alias).create(slug=section_0, name=section_1)', 'for (section_0, section_1, *section_len) in settings.PINAX_BLOG_SECTIONS:\n    Section.objects.using(db_alias).create(slug=section_0, name=section_1)']",no_found,0
core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/homeassistant/components/danfoss_air/binary_sensor.py,https://github.com/home-assistant/core/tree/master/homeassistant/components/danfoss_air/binary_sensor.py,,setup_platform$12,"def setup_platform(hass, config, add_entities, discovery_info=None):
    """"""Set up the available Danfoss Air sensors etc.""""""
    data = hass.data[DANFOSS_AIR_DOMAIN]
    sensors = [['Danfoss Air Bypass Active', ReadCommand.bypass, DEVICE_CLASS_OPENING], ['Danfoss Air Away Mode Active', ReadCommand.away_mode, None]]
    dev = []
    for sensor in sensors:
        dev.append(DanfossAirBinarySensor(data, sensor[0], sensor[1], sensor[2]))
    add_entities(dev, True)","for sensor in sensors:
    dev.append(DanfossAirBinarySensor(data, sensor[0], sensor[1], sensor[2]))","['for sensor in sensors:\n    (sensor_0, sensor_1, sensor_2, *_) = sensor\n    dev.append(DanfossAirBinarySensor(data, sensor_0, sensor_1, sensor_2))', 'for (sensor_0, sensor_1, sensor_2, *sensor_len) in sensors:\n    dev.append(DanfossAirBinarySensor(data, sensor_0, sensor_1, sensor_2))']",no_found,0
pytext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytext/pytext/torchscript/seq2seq/export_model.py,https://github.com/facebookresearch/pytext/tree/master/pytext/torchscript/seq2seq/export_model.py,Seq2SeqJIT,forward$130,"def forward(self, src_tokens: List[str], dict_feat: Optional[Tuple[List[str], List[float], List[int]]]=None, contextual_token_embedding: Optional[List[float]]=None) -> List[Tuple[List[str], float, List[float]]]:
    word_ids = self.source_vocab.lookup_indices_1d(src_tokens)
    single_unk_token: Optional[str] = get_single_unk_token(src_tokens, word_ids, self.copy_unk_token, self.unk_idx)
    (words, dict_tensors, contextual_embedding_tensor, src_lengths) = self.prepare_generator_inputs(word_ids, dict_feat, contextual_token_embedding)
    hypos_etc = self.sequence_generator(words, dict_tensors, contextual_embedding_tensor, src_lengths)
    hypos_list: List[Tuple[List[str], float, List[float]]] = []
    filter_token_list: List[int] = []
    if self.filter_eos_bos:
        filter_token_list = [self.target_vocab.bos_idx, self.target_vocab.eos_idx]
    for seq in hypos_etc:
        hyopthesis = seq[0]
        stringified = self.target_vocab.lookup_words_1d(hyopthesis, filter_token_list=filter_token_list, possible_unk_token=single_unk_token)
        hypos_list.append((stringified, seq[1], seq[2]))
    return hypos_list","for seq in hypos_etc:
    hyopthesis = seq[0]
    stringified = self.target_vocab.lookup_words_1d(hyopthesis, filter_token_list=filter_token_list, possible_unk_token=single_unk_token)
    hypos_list.append((stringified, seq[1], seq[2]))","['for seq in hypos_etc:\n    (seq_0, seq_1, seq_2, *_) = seq\n    hyopthesis = seq_0\n    stringified = self.target_vocab.lookup_words_1d(hyopthesis, filter_token_list=filter_token_list, possible_unk_token=single_unk_token)\n    hypos_list.append((stringified, seq_1, seq_2))', 'for (seq_0, seq_1, seq_2, *seq_len) in hypos_etc:\n    hyopthesis = \n    seq_0\n    stringified = self.target_vocab.lookup_words_1d(hyopthesis, filter_token_list=filter_token_list, possible_unk_token=single_unk_token)\n    hypos_list.append((stringified, \n    seq_1, \n    seq_2))']",no_found,0
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/returners/django_return.py,https://github.com/saltstack/salt/tree/master/salt/returners/django_return.py,,returner$57,"def returner(ret):
    """"""
    Signal a Django server that a return is available
    """"""
    signaled = dispatch.Signal(providing_args=['ret']).send(sender='returner', ret=ret)
    for signal in signaled:
        log.debug(""Django returner function 'returner' signaled %s which responded with %s"", signal[0], signal[1])","for signal in signaled:
    log.debug(""Django returner function 'returner' signaled %s which responded with %s"", signal[0], signal[1])","['for signal in signaled:\n    (signal_0, signal_1, *_) = signal\n    log.debug(""Django returner function \'returner\' signaled %s which responded with %s"", signal_0, signal_1)', 'for (signal_0, signal_1, *signal_len) in signaled:\n    log.debug(""Django returner function \'returner\' signaled %s which responded with %s"", signal_0, signal_1)']",no_found,0
jackdaw,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jackdaw/jackdaw/credentials/credentials.py,https://github.com/skelsec/jackdaw/tree/master/jackdaw/credentials/credentials.py,JackDawCredentials,get_uncracked_hashes$129,"def get_uncracked_hashes(self, hash_type, history):
    self.get_dbsession()
    try:
        if hash_type == 'NT':
            qry = self.dbsession.query(Credential.nt_hash).outerjoin(HashEntry, Credential.nt_hash == HashEntry.nt_hash).filter(Credential.nt_hash != None).distinct(Credential.nt_hash)
        else:
            qry = self.dbsession.query(Credential.lm_hash).outerjoin(HashEntry, Credential.lm_hash == HashEntry.lm_hash).filter(Credential.lm_hash != None).distinct(Credential.lm_hash)
        if history == False:
            qry = qry.filter(Credential.history_no == 0)
        for some_hash in qry.all():
            yield some_hash[0]
    except Exception as e:
        print(e)
    finally:
        self.dbsession.close()","for some_hash in qry.all():
    yield some_hash[0]","['for some_hash in qry.all():\n    (some_hash_0, *some_hash_rsome_hashmaining) = some_hash\n    yield some_hash_0', 'for (some_hash_0, *some_hash_len) in qry.all():\n    yield\n    some_hash_0']",no_found,0
unilm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unilm/xtune/src/pequod/training/xtrainer.py,https://github.com/microsoft/unilm/tree/master/xtune/src/pequod/training/xtrainer.py,BaseTrainer,base_step$90,"def base_step(self, batches, is_qa=False, **kwargs):
    tot_loss = 0.0
    for step_batches in batches:
        batch = step_batches[0]
        batch_dict = self._parse_batch(batch)
        loss = self.model(**batch_dict)[0]
        self.backward_step(loss)
        tot_loss += loss.item()
    self.optim_step()
    return tot_loss / len(batches)","for step_batches in batches:
    batch = step_batches[0]
    batch_dict = self._parse_batch(batch)
    loss = self.model(**batch_dict)[0]
    self.backward_step(loss)
    tot_loss += loss.item()","['for step_batches in batches:\n    (step_batches_0, *step_batches_rstep_batchesmaining) = step_batches\n    batch = step_batches_0\n    batch_dict = self._parse_batch(batch)\n    loss = self.model(**batch_dict)[0]\n    self.backward_step(loss)\n    tot_loss += loss.item()', 'for (step_batches_0, *step_batches_len) in batches:\n    batch = \n    step_batches_0\n    batch_dict = self._parse_batch(batch)\n    loss = self.model(**batch_dict)[0]\n    self.backward_step(loss)\n    tot_loss += loss.item()']",no_found,0
CudaText,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CudaText/app/py/cuda_emmet/proc_snip_insert.py,https://github.com/Alexey-T/CudaText/tree/master/app/py/cuda_emmet/proc_snip_insert.py,,insert_snip_into_editor$8,"def insert_snip_into_editor(ed, snip_lines):
    items = list(snip_lines)
    if not items:
        return
    carets = ed.get_carets()
    if len(carets) != 1:
        return
    (x0, y0, x1, y1) = carets[0]
    tab_spaces = ed.get_prop(ct.PROP_TAB_SPACES)
    tab_size = ed.get_prop(ct.PROP_TAB_SIZE)
    (x_col, y_col) = ed.convert(ct.CONVERT_CHAR_TO_COL, x0, y0)
    indent = ' ' * x_col
    if not tab_spaces:
        indent = indent.replace(' ' * tab_size, '\t')
    for i in range(1, len(items)):
        items[i] = indent + items[i]
    if tab_spaces:
        indent = ' ' * tab_size
        items = [item.replace('\t', indent) for item in items]
    stops = []
    for index in range(len(items)):
        s = items[index]
        while True:
            digit = 0
            deftext = ''
            n = s.find('${')
            if n < 0:
                break
            n_close = s.find('}', n)
            if n_close < 0:
                break
            n_colon = s.find(':', n)
            digit_end = n_close
            if n_colon >= 0:
                digit_end = min(n_close, n_colon)
            try:
                digit = int(s[n + 2:digit_end])
            except ValueError:
                break
            if n_colon >= 0:
                deftext = s[n_colon + 1:n_close]
                s = s[:n] + deftext + s[n_close + 1:]
            else:
                s = s[:n] + s[n_close + 1:]
            stops += [(digit, deftext, index, n)]
            items[index] = s
    ed.insert(x0, y0, '\n'.join(items))
    mark_placed = False
    ed.markers(ct.MARKERS_DELETE_ALL)
    for digit in MARKS_INDEXES:
        for stop in reversed(stops):
            if stop[0] == digit:
                pos_x = stop[3]
                pos_y = stop[2]
                if pos_y == 0:
                    pos_x += x0
                pos_y += y0
                deftext = stop[1]
                ed.markers(ct.MARKERS_ADD, pos_x, pos_y, digit, len(deftext))
                mark_placed = True
    if mark_placed:
        ed.set_prop(ct.PROP_TAB_COLLECT_MARKERS, '1')
        ed.cmd(cudatext_cmd.cmd_Markers_GotoLastAndDelete)","for stop in reversed(stops):
    if stop[0] == digit:
        pos_x = stop[3]
        pos_y = stop[2]
        if pos_y == 0:
            pos_x += x0
        pos_y += y0
        deftext = stop[1]
        ed.markers(ct.MARKERS_ADD, pos_x, pos_y, digit, len(deftext))
        mark_placed = True","['for stop in reversed(stops):\n    (stop_0, stop_1, stop_2, stop_3, *_) = stop\n    if stop_0 == digit:\n        pos_x = stop_3\n        pos_y = stop_2\n        if pos_y == 0:\n            pos_x += x0\n        pos_y += y0\n        deftext = stop_1\n        ed.markers(ct.MARKERS_ADD, pos_x, pos_y, digit, len(deftext))\n        mark_placed = True', 'for (stop_0, stop_1, stop_2, stop_3, *stop_len) in reversed(stops):\n    if \n    stop_0 == digit:\n        pos_x = \n        stop_3\n        pos_y = \n        stop_2\n        if pos_y == 0:\n            pos_x += x0\n        pos_y += y0\n        deftext = \n        stop_1\n        ed.markers(ct.MARKERS_ADD, pos_x, pos_y, digit, len(deftext))\n        mark_placed = True']",no_found,0
capirca,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capirca/capirca/lib/policy.py,https://github.com/google/capirca/tree/master/capirca/lib/policy.py,Term,CheckPortIsContained$1379,"def CheckPortIsContained(self, superset, subset):
    """"""Check if the given list of ports is wholly contained.

    Args:
      superset: list of port tuples
      subset: list of port tuples

    Returns:
      bool: True if subset is contained in superset, false otherwise
    """"""
    if not superset:
        return True
    if not subset:
        return False
    for sub_port in subset:
        not_contains = True
        for sup_port in superset:
            if int(sub_port[0]) >= int(sup_port[0]) and int(sub_port[1]) <= int(sup_port[1]):
                not_contains = False
                break
        if not_contains:
            return False
    return True","for sub_port in subset:
    not_contains = True
    for sup_port in superset:
        if int(sub_port[0]) >= int(sup_port[0]) and int(sub_port[1]) <= int(sup_port[1]):
            not_contains = False
            break
    if not_contains:
        return False","['for sub_port in subset:\n    (sub_port_0, sub_port_1, *_) = sub_port\n    not_contains = True\n    for sup_port in superset:\n        if int(sub_port_0) >= int(sup_port[0]) and int(sub_port_1) <= int(sup_port[1]):\n            not_contains = False\n            break\n    if not_contains:\n        return False', 'for (sub_port_0, sub_port_1, *sub_port_len) in subset:\n    not_contains = True\n    for sup_port in superset:\n        if int(sub_port_0) >= int(sup_port[0]) and int(sub_port_1) <= int(sup_port[1]):\n            not_contains = False\n            break\n    if not_contains:\n        return False']",no_found,0
sympy_gamma,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy_gamma/app/views.py,https://github.com/sympy/sympy_gamma/tree/master/app/views.py,,random_example$132,"def random_example(request):
    examples = []
    for category in EXAMPLES:
        for subcategory in category[1]:
            for example in subcategory[1]:
                if isinstance(example, tuple):
                    examples.append(example[1])
                else:
                    examples.append(example)
    return redirect('input/?i=' + six.moves.urllib.parse.quote(random.choice(examples)))","for subcategory in category[1]:
    for example in subcategory[1]:
        if isinstance(example, tuple):
            examples.append(example[1])
        else:
            examples.append(example)","['for subcategory in category[1]:\n    (_, subcategory_1, *subcategory_rsubcategorymaining) = subcategory\n    for example in subcategory_1:\n        if isinstance(example, tuple):\n            examples.append(example[1])\n        else:\n            examples.append(example)', 'for (subcategory_0, subcategory_1, *subcategory_len) in category[1]:\n    for example in \n    subcategory_1:\n        if isinstance(example, tuple):\n            examples.append(example[1])\n        else:\n            examples.append(example)']",no_found,0
capirca,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capirca/capirca/lib/policy.py,https://github.com/google/capirca/tree/master/capirca/lib/policy.py,Term,CheckPortIsContained$1379,"def CheckPortIsContained(self, superset, subset):
    """"""Check if the given list of ports is wholly contained.

    Args:
      superset: list of port tuples
      subset: list of port tuples

    Returns:
      bool: True if subset is contained in superset, false otherwise
    """"""
    if not superset:
        return True
    if not subset:
        return False
    for sub_port in subset:
        not_contains = True
        for sup_port in superset:
            if int(sub_port[0]) >= int(sup_port[0]) and int(sub_port[1]) <= int(sup_port[1]):
                not_contains = False
                break
        if not_contains:
            return False
    return True","for sup_port in superset:
    if int(sub_port[0]) >= int(sup_port[0]) and int(sub_port[1]) <= int(sup_port[1]):
        not_contains = False
        break","['for sup_port in superset:\n    (sup_port_0, sup_port_1, *_) = sup_port\n    if int(sub_port[0]) >= int(sup_port_0) and int(sub_port[1]) <= int(sup_port_1):\n        not_contains = False\n        break', 'for (sup_port_0, sup_port_1, *sup_port_len) in superset:\n    if int(sub_port[0]) >= int(sup_port_0) and int(sub_port[1]) <= int(sup_port_1):\n        not_contains = False\n        break']",no_found,0
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/dygraph_to_static/test_for_enumerate.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/dygraph_to_static/test_for_enumerate.py,,for_tuple_as_enumerate_iter$258,"def for_tuple_as_enumerate_iter(x_array):
    x = paddle.to_tensor(x_array)
    x_list = [x, x, x]
    a_result = paddle.zeros([5])
    for t in enumerate(x_list):
        a_result += t[1]
    return a_result","for t in enumerate(x_list):
    a_result += t[1]","['for t in enumerate(x_list):\n    (t_0, t_1, *t_rtmaining) = t\n    a_result += t_1', 'for (t_0, t_1, *t_len) in enumerate(x_list):\n    a_result += \n    t_1']",no_found,0
Octolapse,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Octolapse/octoprint_octolapse/snapshot.py,https://github.com/FormerLurker/Octolapse/tree/master/octoprint_octolapse/snapshot.py,CaptureSnapshot,take_snapshots$111,"def take_snapshots(self, metadata={}, no_wait=False):
    logger.info('Starting snapshot acquisition')
    start_time = time()
    before_snapshot_threads = []
    snapshot_threads = []
    after_snapshot_threads = []
    results = []
    for current_camera in self.Cameras:
        camera_info = self.CameraInfos['{}'.format(current_camera.guid)]
        if current_camera.on_before_snapshot_script:
            before_snapshot_job_info = SnapshotJobInfo(self.TimelapseJobInfo, self.temporary_directory, camera_info.snapshot_attempt, current_camera, 'before-snapshot', metadata=metadata)
            thread = ExternalScriptSnapshotJob(before_snapshot_job_info, 'before-snapshot')
            thread.daemon = True
            before_snapshot_threads.append(thread)
        snapshot_job_info = SnapshotJobInfo(self.TimelapseJobInfo, self.temporary_directory, camera_info.snapshot_attempt, current_camera, 'snapshot', metadata=metadata)
        if current_camera.camera_type == 'script':
            thread = ExternalScriptSnapshotJob(snapshot_job_info, 'snapshot', on_new_thumbnail_available_callback=self.OnNewThumbnailAvailableCallback, on_post_processing_error_callback=self.on_post_processing_error_callback)
            thread.daemon = True
            snapshot_threads.append((thread, snapshot_job_info, None))
        elif current_camera.camera_type == 'webcam':
            download_started_event = Event()
            thread = WebcamSnapshotJob(snapshot_job_info, download_started_event=download_started_event, on_new_thumbnail_available_callback=self.OnNewThumbnailAvailableCallback, on_post_processing_error_callback=self.on_post_processing_error_callback)
            thread.daemon = True
            snapshot_threads.append((thread, snapshot_job_info, download_started_event))
        if current_camera.on_after_snapshot_script:
            after_snapshot_job_info = SnapshotJobInfo(self.TimelapseJobInfo, self.temporary_directory, camera_info.snapshot_attempt, current_camera, 'after-snapshot', metadata=metadata)
            thread = ExternalScriptSnapshotJob(after_snapshot_job_info, 'after-snapshot')
            thread.daemon = True
            after_snapshot_threads.append(thread)
    for current_camera in self.Cameras:
        if current_camera.on_before_snapshot_gcode:
            on_before_snapshot_gcode = Commands.string_to_gcode_array(current_camera.on_before_snapshot_gcode)
            if len(on_before_snapshot_gcode) > 0:
                logger.info('Sending on_before_snapshot_gcode for the %s camera.', current_camera.name)
                self.SendGcodeArrayCallback(on_before_snapshot_gcode, current_camera.timeout_ms / 1000.0, wait_for_completion=not no_wait, tags={'before-snapshot-gcode'})
    if len(before_snapshot_threads) > 0:
        logger.info('Starting %d before snapshot threads', len(before_snapshot_threads))
    for t in before_snapshot_threads:
        t.start()
    for t in before_snapshot_threads:
        if not no_wait:
            snapshot_job_info = t.join()
            assert isinstance(snapshot_job_info, SnapshotJobInfo)
            if t.snapshot_thread_error:
                snapshot_job_info.success = False
                snapshot_job_info.error = t.snapshot_thread_error
            else:
                snapshot_job_info.success = True
        else:
            snapshot_job_info.success = True
        results.append(snapshot_job_info)
    if len(before_snapshot_threads) > 0:
        logger.info('Before snapshot threads finished.')
    if len(snapshot_threads) > 0:
        logger.info('Starting %d snapshot threads.', len(snapshot_threads))
    for t in snapshot_threads:
        t[0].start()
    for current_camera in self.Cameras:
        if current_camera.camera_type == 'gcode':
            script_sent = False
            if current_camera.gcode_camera_script:
                gcode_camera_script = Commands.string_to_gcode_array(current_camera.gcode_camera_script)
                if len(gcode_camera_script) > 0:
                    logger.info('Sending snapshot gcode array to %s.', current_camera.name)
                    self.SendGcodeArrayCallback(Commands.string_to_gcode_array(current_camera.gcode_camera_script), current_camera.timeout_ms / 1000.0, wait_for_completion=not no_wait)
                    script_sent = True
                if not script_sent:
                    logger.warning(""The gcode camera '%s' is enabled, but failed to produce any snapshot gcode."", current_camera.name)
    for (t, snapshot_job_info, event) in snapshot_threads:
        if not no_wait:
            if event:
                event.wait()
            else:
                snapshot_job_info = t.join()
            if t.snapshot_thread_error:
                snapshot_job_info.success = False
                snapshot_job_info.error = t.snapshot_thread_error
            elif t.post_processing_error:
                snapshot_job_info.success = False
                snapshot_job_info.error = t.post_processing_error
            else:
                snapshot_job_info.success = True
        else:
            snapshot_job_info.success = True
        info = self.CameraInfos[snapshot_job_info.camera_guid]
        info.snapshot_attempt += 1
        if snapshot_job_info.success:
            info.snapshot_count += 1
            self.SnapshotsTotal += 1
        else:
            info.errors_count += 1
            self.ErrorsTotal += 1
        info.save(self.temporary_directory, self.TimelapseJobInfo.JobGuid, snapshot_job_info.camera_guid)
        results.append(snapshot_job_info)
    if len(snapshot_threads) > 0:
        logger.info('Snapshot threads complete, but may be post-processing.')
    if len(after_snapshot_threads) > 0:
        logger.info('Starting %d after snapshot threads.', len(after_snapshot_threads))
    for current_camera in self.Cameras:
        if current_camera.on_after_snapshot_gcode:
            on_after_snapshot_gcode = Commands.string_to_gcode_array(current_camera.on_after_snapshot_gcode)
            if len(on_after_snapshot_gcode) > 0:
                logger.info('Sending on_after_snapshot_gcode for the %s camera.', current_camera.name)
                self.SendGcodeArrayCallback(on_after_snapshot_gcode, current_camera.timeout_ms / 1000.0, wait_for_completion=not no_wait, tags={'after-snapshot-gcode'})
    for t in after_snapshot_threads:
        t.start()
    for t in after_snapshot_threads:
        if not no_wait:
            snapshot_job_info = t.join()
            assert isinstance(snapshot_job_info, SnapshotJobInfo)
            info = self.CameraInfos[snapshot_job_info.camera_guid]
            if t.snapshot_thread_error:
                snapshot_job_info.success = False
                snapshot_job_info.error = t.snapshot_thread_error
            else:
                snapshot_job_info.success = True
        else:
            snapshot_job_info.success = True
        results.append(snapshot_job_info)
    if len(after_snapshot_threads) > 0:
        logger.info('After snapshot threads complete.')
    logger.info('Snapshot acquisition completed in %.3f seconds.', time() - start_time)
    return results","for t in snapshot_threads:
    t[0].start()","['for t in snapshot_threads:\n    (t_0, *t_rtmaining) = t\n    t_0.start()', 'for (t_0, *t_len) in snapshot_threads:\n    t_0.start()']",no_found,0
camelot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/camelot/camelot/plotting.py,https://github.com/camelot-dev/camelot/tree/master/camelot/plotting.py,PlotMethods,text$50,"def text(self, table):
    """"""Generates a plot for all text elements present
        on the PDF page.

        Parameters
        ----------
        table : camelot.core.Table

        Returns
        -------
        fig : matplotlib.fig.Figure

        """"""
    fig = plt.figure()
    ax = fig.add_subplot(111, aspect='equal')
    (xs, ys) = ([], [])
    for t in table._text:
        xs.extend([t[0], t[2]])
        ys.extend([t[1], t[3]])
        ax.add_patch(patches.Rectangle((t[0], t[1]), t[2] - t[0], t[3] - t[1]))
    ax.set_xlim(min(xs) - 10, max(xs) + 10)
    ax.set_ylim(min(ys) - 10, max(ys) + 10)
    return fig","for t in table._text:
    xs.extend([t[0], t[2]])
    ys.extend([t[1], t[3]])
    ax.add_patch(patches.Rectangle((t[0], t[1]), t[2] - t[0], t[3] - t[1]))","['for t in table._text:\n    (t_0, t_1, t_2, t_3, *_) = t\n    xs.extend([t_0, t_2])\n    ys.extend([t_1, t_3])\n    ax.add_patch(patches.Rectangle((t_0, t_1), t_2 - t_0, t_3 - t_1))', 'for (t_0, t_1, t_2, t_3, *t_len) in table._text:\n    xs.extend([t_0, t_2])\n    ys.extend([t_1, t_3])\n    ax.add_patch(patches.Rectangle((t_0, t_1), t_2 - t_0, t_3 - t_1))']",no_found,0
stacker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stacker/stacker/tests/test_util.py,https://github.com/cloudtools/stacker/tree/master/stacker/tests/test_util.py,TestUtil,test_merge_map$82,"def test_merge_map(self):
    tests = [[{'stacks': [{'stack1': {'variables': {'a': 'b'}}}]}, {'stacks': [{'stack2': {'variables': {'c': 'd'}}}]}, {'stacks': [{'stack1': {'variables': {'a': 'b'}}}, {'stack2': {'variables': {'c': 'd'}}}]}], [{'stacks': [{'stack1': {'variables': {'a': 'b'}}}]}, {'stacks': {'stack2': {'variables': {'c': 'd'}}}}, {'stacks': {'stack2': {'variables': {'c': 'd'}}}}], [{'stacks': {'stack1': {'variables': {'a': 'b'}}}}, {'stacks': {'stack1': {'variables': {'c': 'd'}}}}, {'stacks': {'stack1': {'variables': {'a': 'b', 'c': 'd'}}}}], [{'stacks': {'stack1': {'variables': {'a': 'b'}}}}, {'stacks': {'stack1': {'variables': {'a': 'c'}}}}, {'stacks': {'stack1': {'variables': {'a': 'c'}}}}]]
    for t in tests:
        self.assertEqual(merge_map(t[0], t[1]), t[2])","for t in tests:
    self.assertEqual(merge_map(t[0], t[1]), t[2])","['for t in tests:\n    (t_0, t_1, t_2, *_) = t\n    self.assertEqual(merge_map(t_0, t_1), t_2)', 'for (t_0, t_1, t_2, *t_len) in tests:\n    self.assertEqual(merge_map(t_0, t_1), t_2)']",no_found,0
upvote_py2,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/upvote_py2/upvote/monitoring/metrics.py,https://github.com/google/upvote_py2/tree/master/upvote/monitoring/metrics.py,Namespace,__init__$35,"def __init__(self, prefix, tuples):
    self.metrics = []
    for t in tuples:
        metric = Metric(prefix + t[0], t[1])
        setattr(self, t[0].upper(), metric)
        self.metrics.append(metric)
    setattr(self, 'ALL', self.metrics)","for t in tuples:
    metric = Metric(prefix + t[0], t[1])
    setattr(self, t[0].upper(), metric)
    self.metrics.append(metric)","['for t in tuples:\n    (t_0, t_1, *_) = t\n    metric = Metric(prefix + t_0, t_1)\n    setattr(self, t_0.upper(), metric)\n    self.metrics.append(metric)', 'for (t_0, t_1, *t_len) in tuples:\n    metric = Metric(prefix + t_0, t_1)\n    setattr(self, t_0.upper(), metric)\n    self.metrics.append(metric)']",no_found,0
textflint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/textflint/textflint/common/utils/overlap_templates.py,https://github.com/textflint/textflint/tree/master/textflint/common/utils/overlap_templates.py,,template_filler$205,"def template_filler(template_list):
    probs = []
    templates = []
    for template_pair in template_list:
        probs.append(template_pair[0])
        templates.append(template_pair[1])
    template_index = np.random.choice(range(len(templates)), p=probs)
    template_tuple = templates[template_index]
    template = template_tuple[0]
    hypothesis_template = template_tuple[1]
    template_tag = template_tuple[2]
    premise_list = []
    index_dict = {}
    for (index, element) in template:
        if element == 'VP':
            (vp, vp_parse) = generate_vp()
            premise_list.append(vp)
            index_dict[index] = vp
        elif element == 'RC':
            rc = generate_rc()
            premise_list.append(rc)
            index_dict[index] = rc
        elif 'vobj' in element:
            obj = random.choice(object_dict[index_dict[int(element[-1])]])
            premise_list.append(obj)
            index_dict[index] = obj
        elif isinstance(element, str):
            premise_list.append(element)
            index_dict[index] = element
        else:
            word = random.choice(element)
            premise_list.append(word)
            index_dict[index] = word
    hypothesis_list = [index_dict[ind] for ind in hypothesis_template]
    return (postprocess(' '.join(premise_list)), postprocess(' '.join(hypothesis_list)), template_tag)","for template_pair in template_list:
    probs.append(template_pair[0])
    templates.append(template_pair[1])","['for template_pair in template_list:\n    (template_pair_0, template_pair_1, *_) = template_pair\n    probs.append(template_pair_0)\n    templates.append(template_pair_1)', 'for (template_pair_0, template_pair_1, *template_pair_len) in template_list:\n    probs.append(template_pair_0)\n    templates.append(template_pair_1)']",no_found,0
aeneas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aeneas/aeneas/tests/test_globalfunctions.py,https://github.com/readbeyond/aeneas/tree/master/aeneas/tests/test_globalfunctions.py,TestGlobalFunctions,test_safe_get$143,"def test_safe_get(self):
    tests = [(None, None, u'default', u'default'), (None, u'key', u'default', u'default'), ({}, None, u'default', u'default'), ({}, u'key', u'default', u'default'), ([], u'key', u'default', u'default'), ({u'key': u'value'}, None, u'default', u'default'), ({u'key': u'value'}, u'key', u'default', u'value')]
    for test in tests:
        self.assertEqual(gf.safe_get(test[0], test[1], test[2]), test[3])","for test in tests:
    self.assertEqual(gf.safe_get(test[0], test[1], test[2]), test[3])","['for test in tests:\n    (test_0, test_1, test_2, test_3, *_) = test\n    self.assertEqual(gf.safe_get(test_0, test_1, test_2), test_3)', 'for (test_0, test_1, test_2, test_3, *test_len) in tests:\n    self.assertEqual(gf.safe_get(test_0, test_1, test_2), test_3)']",no_found,0
XlsxWriter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/XlsxWriter/xlsxwriter/test/worksheet/test_extract_filter_tokens.py,https://github.com/jmcnamara/XlsxWriter/tree/master/xlsxwriter/test/worksheet/test_extract_filter_tokens.py,TestExtractFilterTokens,test_extract_filter_tokens$25,"def test_extract_filter_tokens(self):
    """"""Test the _extract_filter_tokens() method""""""
    testcases = [[None, []], ['', []], ['0 <  2001', ['0', '<', '2001']], ['x <  2000', ['x', '<', '2000']], ['x >  2000', ['x', '>', '2000']], ['x == 2000', ['x', '==', '2000']], ['x >  2000 and x <  5000', ['x', '>', '2000', 'and', 'x', '<', '5000']], ['x = ""goo""', ['x', '=', 'goo']], ['x = moo', ['x', '=', 'moo']], ['x = ""foo baz""', ['x', '=', 'foo baz']], ['x = ""moo """" bar""', ['x', '=', 'moo "" bar']], ['x = ""foo bar"" or x = ""bar foo""', ['x', '=', 'foo bar', 'or', 'x', '=', 'bar foo']], ['x = ""foo """" bar"" or x = ""bar """" foo""', ['x', '=', 'foo "" bar', 'or', 'x', '=', 'bar "" foo']], ['x = """"""""""""""""', ['x', '=', '""""""']], ['x = Blanks', ['x', '=', 'Blanks']], ['x = NonBlanks', ['x', '=', 'NonBlanks']], ['top 10 %', ['top', '10', '%']], ['top 10 items', ['top', '10', 'items']]]
    for testcase in testcases:
        expression = testcase[0]
        exp = testcase[1]
        got = self.worksheet._extract_filter_tokens(expression)
        self.assertEqual(got, exp)","for testcase in testcases:
    expression = testcase[0]
    exp = testcase[1]
    got = self.worksheet._extract_filter_tokens(expression)
    self.assertEqual(got, exp)","['for testcase in testcases:\n    (testcase_0, testcase_1, *_) = testcase\n    expression = testcase_0\n    exp = testcase_1\n    got = self.worksheet._extract_filter_tokens(expression)\n    self.assertEqual(got, exp)', 'for (testcase_0, testcase_1, *testcase_len) in testcases:\n    expression = \n    testcase_0\n    exp = \n    testcase_1\n    got = self.worksheet._extract_filter_tokens(expression)\n    self.assertEqual(got, exp)']",no_found,0
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""""""""
    outline = Outline()
    playlist = Playlist()
    counter = Counter()
    video_list = []
    pdf_list = []
    rich_text_list = []
    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190', 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean', 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id, 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr', data=post_data).text.encode('utf_8').decode('unicode_escape')
    chapters = re.findall('homeworks=\\w+;.+id=(\\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)
        lessons = re.findall('chapterId=' + chapter[0] + '.+contentType=1.+id=(\\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)
            videos = re.findall('contentId=(\\d+).+contentType=(1).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()
            pdfs = re.findall('contentId=(\\d+).+contentType=(3).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()
            rich_text = re.findall('contentId=(\\d+).+contentType=(4).+id=(\\d+).+jsonContent=(.+);.+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')
                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()
    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","for text in rich_text:
    counter.add(2)
    outline.write(text[4], counter, 2, sign='+')
    if CONFIG['text']:
        rich_text_list.append(RichText(counter, text[4], text))
    if CONFIG['file']:
        if text[3] != 'null' and text[3] != '""""':
            params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
            file_name = Resource.file_to_save(params['fileName'])
            outline.write(file_name, counter, 2, sign='!')
            WORK_DIR.change('Files')
            res_print(params['fileName'])
            file_name = '%s %s' % (counter, file_name)
            CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})","['for text in rich_text:\n    (_, _, _, text_3, text_4, *_) = text\n    counter.add(2)\n    outline.write(text_4, counter, 2, sign=\'+\')\n    if CONFIG[\'text\']:\n        rich_text_list.append(RichText(counter, text_4, text))\n    if CONFIG[\'file\']:\n        if text_3 != \'null\' and text_3 != \'""""\':\n            params = {\'nosKey\': re.search(\'nosKey"":""(.+?)""\', text_3).group(1), \'fileName\': re.search(\'""fileName"":""(.+?)""\', text_3).group(1)}\n            file_name = Resource.file_to_save(params[\'fileName\'])\n            outline.write(file_name, counter, 2, sign=\'!\')\n            WORK_DIR.change(\'Files\')\n            res_print(params[\'fileName\'])\n            file_name = \'%s %s\' % (counter, file_name)\n            CANDY.download_bin(\'https://www.icourse163.org/course/attachment.htm\', WORK_DIR.file(file_name), params=params, cookies={\'STUDY_SESS\': None})']",no_found,0
DeepNER,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepNER/src/utils/evaluator.py,https://github.com/z814081807/DeepNER/tree/master/src/utils/evaluator.py,,mrc_evaluation$240,"def mrc_evaluation(model, dev_info, device):
    (dev_loader, (dev_callback_info, type_weight)) = dev_info
    (start_logits, end_logits) = (None, None)
    model.eval()
    for tmp_pred in get_base_out(model, dev_loader, device):
        tmp_start_logits = tmp_pred[0].cpu().numpy()
        tmp_end_logits = tmp_pred[1].cpu().numpy()
        if start_logits is None:
            start_logits = tmp_start_logits
            end_logits = tmp_end_logits
        else:
            start_logits = np.append(start_logits, tmp_start_logits, axis=0)
            end_logits = np.append(end_logits, tmp_end_logits, axis=0)
    assert len(start_logits) == len(end_logits) == len(dev_callback_info)
    role_metric = np.zeros([13, 3])
    mirco_metrics = np.zeros(3)
    id2ent = {x: i for (i, x) in enumerate(ENTITY_TYPES)}
    for (tmp_start_logits, tmp_end_logits, tmp_callback) in zip(start_logits, end_logits, dev_callback_info):
        (text, text_offset, ent_type, gt_entities) = tmp_callback
        tmp_start_logits = tmp_start_logits[text_offset:text_offset + len(text)]
        tmp_end_logits = tmp_end_logits[text_offset:text_offset + len(text)]
        pred_entities = mrc_decode(tmp_start_logits, tmp_end_logits, text)
        role_metric[id2ent[ent_type]] += calculate_metric(gt_entities, pred_entities)
    for (idx, _type) in enumerate(ENTITY_TYPES):
        temp_metric = get_p_r_f(role_metric[idx][0], role_metric[idx][1], role_metric[idx][2])
        mirco_metrics += temp_metric * type_weight[_type]
    metric_str = f'[MIRCO] precision: {mirco_metrics[0]:.4f}, recall: {mirco_metrics[1]:.4f}, f1: {mirco_metrics[2]:.4f}'
    return (metric_str, mirco_metrics[2])","for tmp_pred in get_base_out(model, dev_loader, device):
    tmp_start_logits = tmp_pred[0].cpu().numpy()
    tmp_end_logits = tmp_pred[1].cpu().numpy()
    if start_logits is None:
        start_logits = tmp_start_logits
        end_logits = tmp_end_logits
    else:
        start_logits = np.append(start_logits, tmp_start_logits, axis=0)
        end_logits = np.append(end_logits, tmp_end_logits, axis=0)","['for tmp_pred in get_base_out(model, dev_loader, device):\n    (tmp_pred_0, tmp_pred_1, *_) = tmp_pred\n    tmp_start_logits = tmp_pred_0.cpu().numpy()\n    tmp_end_logits = tmp_pred_1.cpu().numpy()\n    if start_logits is None:\n        start_logits = tmp_start_logits\n        end_logits = tmp_end_logits\n    else:\n        start_logits = np.append(start_logits, tmp_start_logits, axis=0)\n        end_logits = np.append(end_logits, tmp_end_logits, axis=0)', 'for (tmp_pred_0, tmp_pred_1, *tmp_pred_len) in get_base_out(model, dev_loader, device):\n    tmp_start_logits = \n    tmp_pred_0.cpu().numpy()\n    tmp_end_logits = \n    tmp_pred_1.cpu().numpy()\n    if start_logits is None:\n        start_logits = tmp_start_logits\n        end_logits = tmp_end_logits\n    else:\n        start_logits = np.append(start_logits, tmp_start_logits, axis=0)\n        end_logits = np.append(end_logits, tmp_end_logits, axis=0)']",no_found,0
mindmeld,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mindmeld/mindmeld/models/taggers/taggers.py,https://github.com/cisco/mindmeld/tree/master/mindmeld/models/taggers/taggers.py,,_contains_O$461,"def _contains_O(entity):
    """"""Returns true if there is an O tag in the list of tokens we are considering
    as an entity""""""
    for token in entity:
        if token[0] == O_TAG:
            return True
    return False","for token in entity:
    if token[0] == O_TAG:
        return True","['for token in entity:\n    (token_0, *token_rtokenmaining) = token\n    if token_0 == O_TAG:\n        return True', 'for (token_0, *token_len) in entity:\n    if \n    token_0 == O_TAG:\n        return True']",no_found,0
underthesea,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/underthesea/underthesea/transformer/tagged_feature.py,https://github.com/undertheseanlp/underthesea/tree/master/underthesea/transformer/tagged_feature.py,,text_istitle$23,"def text_istitle(word):
    if len(word) == 0:
        return False
    try:
        titles = [s[0] for s in word.split(' ')]
        for token in titles:
            if token[0].istitle() is False:
                return False
        return True
    except Exception:
        return False","for token in titles:
    if token[0].istitle() is False:
        return False","['for token in titles:\n    (token_0, *token_rtokenmaining) = token\n    if token_0.istitle() is False:\n        return False', 'for (token_0, *token_len) in titles:\n    if \n    token_0.istitle() is False:\n        return False']",no_found,0
LinOTP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LinOTP/linotp/lib/tokeniterator.py,https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/tokeniterator.py,TokenIterator,_get_tokens_in_realm$359,"def _get_tokens_in_realm(self, valid_realms):
    realm_id_tuples = db.session.query(Realm.id).filter(Realm.name.in_(valid_realms)).all()
    realm_ids = set()
    for realm_tuple in realm_id_tuples:
        realm_ids.add(realm_tuple[0])
    token_id_tuples = db.session.query(TokenRealm.token_id).filter(TokenRealm.realm_id.in_(realm_ids)).all()
    token_ids = set()
    for token_tuple in token_id_tuples:
        token_ids.add(token_tuple[0])
    return token_ids","for token_tuple in token_id_tuples:
    token_ids.add(token_tuple[0])","['for token_tuple in token_id_tuples:\n    (token_tuple_0, *token_tuple_rtoken_tuplemaining) = token_tuple\n    token_ids.add(token_tuple_0)', 'for (token_tuple_0, *token_tuple_len) in token_id_tuples:\n    token_ids.add(token_tuple_0)']",no_found,0
nlp_xiaojiang,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp_xiaojiang/AugmentText/augment_eda/enhance_word2vec.py,https://github.com/yongzhuo/nlp_xiaojiang/tree/master/AugmentText/augment_eda/enhance_word2vec.py,,get_synonyms_from_word2vec$112,"def get_synonyms_from_word2vec(word2vec_model, word, topn=20, score_top=0.75):
    word_syn = []
    try:
        topn_words = word2vec_model.most_similar(word, topn=topn)
        for topn_word_num in topn_words:
            if topn_word_num[1] >= score_top:
                word_syn.append(topn_word_num[0])
    except Exception as e:
        logger.info(str(e))
    return word_syn","for topn_word_num in topn_words:
    if topn_word_num[1] >= score_top:
        word_syn.append(topn_word_num[0])","['for topn_word_num in topn_words:\n    (topn_word_num_0, topn_word_num_1, *_) = topn_word_num\n    if topn_word_num_1 >= score_top:\n        word_syn.append(topn_word_num_0)', 'for (topn_word_num_0, topn_word_num_1, *topn_word_num_len) in topn_words:\n    if \n    topn_word_num_1 >= score_top:\n        word_syn.append(\n        topn_word_num_0)']",no_found,0
DeepKE,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/name_entity_re/few_shot/module/datasets.py,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/name_entity_re/few_shot/module/datasets.py,ConllNERDataset,collate_fn$205,"def collate_fn(self, batch):
    (src_tokens, src_seq_len, first) = ([], [], [])
    (tgt_tokens, tgt_seq_len, target_span) = ([], [], [])
    if self.mode == 'test':
        raw_words = []
        for tup in batch:
            src_tokens.append(tup[0])
            src_seq_len.append(tup[1])
            first.append(tup[2])
            raw_words.append(tup[3])
        src_tokens = pad_sequence(src_tokens, batch_first=True, padding_value=self.data_processor.tokenizer.pad_token_id)
        first = pad_sequence(first, batch_first=True, padding_value=0)
        return (src_tokens, torch.stack(src_seq_len, 0), first, raw_words)
    for tup in batch:
        src_tokens.append(tup[0])
        tgt_tokens.append(tup[1])
        src_seq_len.append(tup[2])
        tgt_seq_len.append(tup[3])
        first.append(tup[4])
        target_span.append(tup[5])
    src_tokens = pad_sequence(src_tokens, batch_first=True, padding_value=self.data_processor.tokenizer.pad_token_id)
    tgt_tokens = pad_sequence(tgt_tokens, batch_first=True, padding_value=1)
    first = pad_sequence(first, batch_first=True, padding_value=0)
    return (src_tokens, tgt_tokens, torch.stack(src_seq_len, 0), torch.stack(tgt_seq_len, 0), first, target_span)","for tup in batch:
    src_tokens.append(tup[0])
    src_seq_len.append(tup[1])
    first.append(tup[2])
    raw_words.append(tup[3])","['for tup in batch:\n    (tup_0, tup_1, tup_2, tup_3, *_) = tup\n    src_tokens.append(tup_0)\n    src_seq_len.append(tup_1)\n    first.append(tup_2)\n    raw_words.append(tup_3)', 'for (tup_0, tup_1, tup_2, tup_3, *tup_len) in batch:\n    src_tokens.append(tup_0)\n    src_seq_len.append(tup_1)\n    first.append(tup_2)\n    raw_words.append(tup_3)']",no_found,
DeepKE,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/name_entity_re/few_shot/module/datasets.py,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/name_entity_re/few_shot/module/datasets.py,ConllNERDataset,collate_fn$205,"def collate_fn(self, batch):
    (src_tokens, src_seq_len, first) = ([], [], [])
    (tgt_tokens, tgt_seq_len, target_span) = ([], [], [])
    if self.mode == 'test':
        raw_words = []
        for tup in batch:
            src_tokens.append(tup[0])
            src_seq_len.append(tup[1])
            first.append(tup[2])
            raw_words.append(tup[3])
        src_tokens = pad_sequence(src_tokens, batch_first=True, padding_value=self.data_processor.tokenizer.pad_token_id)
        first = pad_sequence(first, batch_first=True, padding_value=0)
        return (src_tokens, torch.stack(src_seq_len, 0), first, raw_words)
    for tup in batch:
        src_tokens.append(tup[0])
        tgt_tokens.append(tup[1])
        src_seq_len.append(tup[2])
        tgt_seq_len.append(tup[3])
        first.append(tup[4])
        target_span.append(tup[5])
    src_tokens = pad_sequence(src_tokens, batch_first=True, padding_value=self.data_processor.tokenizer.pad_token_id)
    tgt_tokens = pad_sequence(tgt_tokens, batch_first=True, padding_value=1)
    first = pad_sequence(first, batch_first=True, padding_value=0)
    return (src_tokens, tgt_tokens, torch.stack(src_seq_len, 0), torch.stack(tgt_seq_len, 0), first, target_span)","for tup in batch:
    src_tokens.append(tup[0])
    tgt_tokens.append(tup[1])
    src_seq_len.append(tup[2])
    tgt_seq_len.append(tup[3])
    first.append(tup[4])
    target_span.append(tup[5])","['for tup in batch:\n    (tup_0, tup_1, tup_2, tup_3, tup_4, tup_5, *_) = tup\n    src_tokens.append(tup_0)\n    tgt_tokens.append(tup_1)\n    src_seq_len.append(tup_2)\n    tgt_seq_len.append(tup_3)\n    first.append(tup_4)\n    target_span.append(tup_5)', 'for (tup_0, tup_1, tup_2, tup_3, tup_4, tup_5, *tup_len) in batch:\n    src_tokens.append(tup_0)\n    tgt_tokens.append(tup_1)\n    src_seq_len.append(tup_2)\n    tgt_seq_len.append(tup_3)\n    first.append(tup_4)\n    target_span.append(tup_5)']",no_found,0
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/tools/check_op_desc.py,https://github.com/PaddlePaddle/Paddle/tree/master/tools/check_op_desc.py,,print_version_error_message$427,"def print_version_error_message(error_message):
    print('\n======================= \nOperator registration error for the changes of Inputs/Outputs/Attrs of OPs:\n')
    for op_name in error_message:
        print(""For OP '{}':"".format(op_name))
        inputs_error = error_message.get(op_name, {}).get(INPUTS, {})
        error_list = inputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print("" * The added input '{}' is not yet registered."".format(tup[1]))
        outputs_error = error_message.get(op_name, {}).get(OUTPUTS, {})
        error_list = outputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print("" * The added output '{}' is not yet registered."".format(tup[1]))
        attrs_error = error_message.get(op_name, {}).get(ATTRS, {})
        error_list = attrs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print("" * The added attribute '{}' is not yet registered."".format(tup[1]))
        error_dic = error_message.get(op_name, {}).get(ATTRS, {}).get(CHANGE, {})
        for (key, val) in error_dic.items():
            print("" * The change of attribute '{}' is not yet registered."".format(key))","for tup in error_list:
    print("" * The added attribute '{}' is not yet registered."".format(tup[1]))","['for tup in error_list:\n    (_, tup_1, *tup_rtupmaining) = tup\n    print("" * The added attribute \'{}\' is not yet registered."".format(tup_1))', 'for (tup_0, tup_1, *tup_len) in error_list:\n    print("" * The added attribute \'{}\' is not yet registered."".format(tup_1))']",no_found,0
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/tools/check_op_desc.py,https://github.com/PaddlePaddle/Paddle/tree/master/tools/check_op_desc.py,,print_version_error_message$427,"def print_version_error_message(error_message):
    print('\n======================= \nOperator registration error for the changes of Inputs/Outputs/Attrs of OPs:\n')
    for op_name in error_message:
        print(""For OP '{}':"".format(op_name))
        inputs_error = error_message.get(op_name, {}).get(INPUTS, {})
        error_list = inputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print("" * The added input '{}' is not yet registered."".format(tup[1]))
        outputs_error = error_message.get(op_name, {}).get(OUTPUTS, {})
        error_list = outputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print("" * The added output '{}' is not yet registered."".format(tup[1]))
        attrs_error = error_message.get(op_name, {}).get(ATTRS, {})
        error_list = attrs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print("" * The added attribute '{}' is not yet registered."".format(tup[1]))
        error_dic = error_message.get(op_name, {}).get(ATTRS, {}).get(CHANGE, {})
        for (key, val) in error_dic.items():
            print("" * The change of attribute '{}' is not yet registered."".format(key))","for tup in error_list:
    print("" * The added output '{}' is not yet registered."".format(tup[1]))","['for tup in error_list:\n    (_, tup_1, *tup_rtupmaining) = tup\n    print("" * The added output \'{}\' is not yet registered."".format(tup_1))', 'for (tup_0, tup_1, *tup_len) in error_list:\n    print("" * The added output \'{}\' is not yet registered."".format(tup_1))']",no_found,0
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/tools/check_op_desc.py,https://github.com/PaddlePaddle/Paddle/tree/master/tools/check_op_desc.py,,print_version_error_message$427,"def print_version_error_message(error_message):
    print('\n======================= \nOperator registration error for the changes of Inputs/Outputs/Attrs of OPs:\n')
    for op_name in error_message:
        print(""For OP '{}':"".format(op_name))
        inputs_error = error_message.get(op_name, {}).get(INPUTS, {})
        error_list = inputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print("" * The added input '{}' is not yet registered."".format(tup[1]))
        outputs_error = error_message.get(op_name, {}).get(OUTPUTS, {})
        error_list = outputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print("" * The added output '{}' is not yet registered."".format(tup[1]))
        attrs_error = error_message.get(op_name, {}).get(ATTRS, {})
        error_list = attrs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print("" * The added attribute '{}' is not yet registered."".format(tup[1]))
        error_dic = error_message.get(op_name, {}).get(ATTRS, {}).get(CHANGE, {})
        for (key, val) in error_dic.items():
            print("" * The change of attribute '{}' is not yet registered."".format(key))","for tup in error_list:
    print("" * The added input '{}' is not yet registered."".format(tup[1]))","['for tup in error_list:\n    (tup_0, tup_1, *tup_rtupmaining) = tup\n    print("" * The added input \'{}\' is not yet registered."".format(tup_1))', 'for (tup_0, tup_1, *tup_len) in error_list:\n    print("" * The added input \'{}\' is not yet registered."".format(tup_1))']",no_found,0
PGPortfolio,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGPortfolio/pgportfolio/marketdata/globaldatamatrix.py,https://github.com/ZhengyaoJiang/PGPortfolio/tree/master/pgportfolio/marketdata/globaldatamatrix.py,HistoryManager,select_coins$125,"def select_coins(self, start, end):
    if not self._online:
        logging.info('select coins offline from %s to %s' % (datetime.fromtimestamp(start).strftime('%Y-%m-%d %H:%M'), datetime.fromtimestamp(end).strftime('%Y-%m-%d %H:%M')))
        connection = sqlite3.connect(DATABASE_DIR)
        try:
            cursor = connection.cursor()
            cursor.execute('SELECT coin,SUM(volume) AS total_volume FROM History WHERE date>=? and date<=? GROUP BY coin ORDER BY total_volume DESC LIMIT ?;', (int(start), int(end), self._coin_number))
            coins_tuples = cursor.fetchall()
            if len(coins_tuples) != self._coin_number:
                logging.error('the sqlite error happend')
        finally:
            connection.commit()
            connection.close()
        coins = []
        for tuple in coins_tuples:
            coins.append(tuple[0])
    else:
        coins = list(self._coin_list.topNVolume(n=self._coin_number).index)
    logging.debug('Selected coins are: ' + str(coins))
    return coins","for tuple in coins_tuples:
    coins.append(tuple[0])","['for tuple in coins_tuples:\n    (tuple_0, *tuple_rtuplemaining) = tuple\n    coins.append(tuple_0)', 'for (tuple_0, *tuple_len) in coins_tuples:\n    coins.append(tuple_0)']",no_found,-1
quay,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quay/test/test_ldap.py,https://github.com/quay/quay/tree/master/test/test_ldap.py,TestLDAP,test_ldap_superuser_and_restricted_user_invalid_filter$810,"def test_ldap_superuser_and_restricted_user_invalid_filter(self):
    valid_user_filter = '(filterField=somevalue)'
    invalid_superuser_filter = '(filterField=notsuperuser)'
    invalid_restricted_user_filter = '(filterField=notrestricted)'
    with mock_ldap(user_filter=valid_user_filter) as ldap:
        (response, _) = ldap.verify_and_link_user('someuser', 'somepass')
        self.assertEqual(response.username, 'someuser')
    with mock_ldap(user_filter=valid_user_filter, superuser_filter=invalid_superuser_filter, restricted_user_filter=invalid_restricted_user_filter) as ldap:
        (it, err) = ldap.iterate_group_members({'group_dn': 'cn=AwesomeFolk'}, disable_pagination=True)
        self.assertIsNone(err)
        results = list(it)
        self.assertEqual(4, len(results))
        for u in results:
            user = u[0]
            is_superuser = ldap.is_superuser(user.username)
            is_restricted_user = ldap.is_restricted_user(user.username)
            self.assertFalse(is_superuser)
            self.assertFalse(is_restricted_user)
        self.assertFalse(ldap.has_superusers())
        self.assertFalse(ldap.has_restricted_users())","for u in results:
    user = u[0]
    is_superuser = ldap.is_superuser(user.username)
    is_restricted_user = ldap.is_restricted_user(user.username)
    self.assertFalse(is_superuser)
    self.assertFalse(is_restricted_user)","['for u in results:\n    (u_0, *u_rumaining) = u\n    user = u_0\n    is_superuser = ldap.is_superuser(user.username)\n    is_restricted_user = ldap.is_restricted_user(user.username)\n    self.assertFalse(is_superuser)\n    self.assertFalse(is_restricted_user)', 'for (u_0, *u_len) in results:\n    user = \n    u_0\n    is_superuser = ldap.is_superuser(user.username)\n    is_restricted_user = ldap.is_restricted_user(user.username)\n    self.assertFalse(is_superuser)\n    self.assertFalse(is_restricted_user)']",no_found,0
pyGAT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyGAT/visualize_graph.py,https://github.com/Diego999/pyGAT/tree/master//visualize_graph.py,,add_nodes$32,"def add_nodes(var):
    if var not in seen:
        if torch.is_tensor(var):
            dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')
        elif hasattr(var, 'variable'):
            u = var.variable
            node_name = '%s\n %s' % (param_map.get(id(u)), size_to_str(u.size()))
            dot.node(str(id(var)), node_name, fillcolor='lightblue')
        else:
            dot.node(str(id(var)), str(type(var).__name__))
        seen.add(var)
        if hasattr(var, 'next_functions'):
            for u in var.next_functions:
                if u[0] is not None:
                    dot.edge(str(id(u[0])), str(id(var)))
                    add_nodes(u[0])
        if hasattr(var, 'saved_tensors'):
            for t in var.saved_tensors:
                dot.edge(str(id(t)), str(id(var)))
                add_nodes(t)","for u in var.next_functions:
    if u[0] is not None:
        dot.edge(str(id(u[0])), str(id(var)))
        add_nodes(u[0])","['for u in var.next_functions:\n    (u_0, *u_rumaining) = u\n    if u_0 is not None:\n        dot.edge(str(id(u_0)), str(id(var)))\n        add_nodes(u_0)', 'for (u_0, *u_len) in var.next_functions:\n    if \n    u_0 is not None:\n        dot.edge(str(id(\n        u_0)), str(id(var)))\n        add_nodes(\n        u_0)']",no_found,0
django-js-reverse,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-js-reverse/django_js_reverse/core.py,https://github.com/ierror/django-js-reverse/tree/master/django_js_reverse/core.py,,prepare_url_list$37,"def prepare_url_list(urlresolver, namespace_path='', namespace=''):
    """"""
    returns list of tuples [(<url_name>, <url_patern_tuple> ), ...]
    """"""
    exclude_ns = getattr(settings, 'JS_REVERSE_EXCLUDE_NAMESPACES', JS_EXCLUDE_NAMESPACES)
    include_only_ns = getattr(settings, 'JS_REVERSE_INCLUDE_ONLY_NAMESPACES', JS_INCLUDE_ONLY_NAMESPACES)
    if exclude_ns and include_only_ns:
        raise ImproperlyConfigured('Neither use JS_REVERSE_EXCLUDE_NAMESPACES nor JS_REVERSE_INCLUDE_ONLY_NAMESPACES setting')
    if namespace[:-1] in exclude_ns:
        return
    include_only_allow = True
    if include_only_ns != []:
        in_on_empty_ns = False
        in_on_is_in_list = False
        in_on_null = False
        if namespace == '' and '' in include_only_ns:
            in_on_empty_ns = True
        for ns in include_only_ns:
            if ns != '' and namespace[:-1].startswith(ns):
                in_on_is_in_list = True
                break
        if namespace[:-1] + '\x00' in include_only_ns:
            in_on_null = True
        include_only_allow = in_on_empty_ns or in_on_is_in_list or in_on_null
    if include_only_allow:
        for url_name in urlresolver.reverse_dict.keys():
            if isinstance(url_name, (text_type, str)):
                url_patterns = []
                for url_pattern in urlresolver.reverse_dict.getlist(url_name):
                    url_patterns += [[namespace_path + pat[0], pat[1]] for pat in url_pattern[0]]
                yield [namespace + url_name, url_patterns]
    for (inner_ns, (inner_ns_path, inner_urlresolver)) in urlresolver.namespace_dict.items():
        inner_ns_path = namespace_path + inner_ns_path
        inner_ns = namespace + inner_ns + ':'
        if inner_ns_path:
            args = [inner_ns_path, inner_urlresolver]
            if LooseVersion(django.get_version()) >= LooseVersion('2.0.6'):
                args.append(tuple(urlresolver.pattern.converters.items()))
            inner_urlresolver = urlresolvers.get_ns_resolver(*args)
            inner_ns_path = ''
        for x in prepare_url_list(inner_urlresolver, inner_ns_path, inner_ns):
            yield x","for url_pattern in urlresolver.reverse_dict.getlist(url_name):
    url_patterns += [[namespace_path + pat[0], pat[1]] for pat in url_pattern[0]]","['for url_pattern in urlresolver.reverse_dict.getlist(url_name):\n    (url_pattern_0, *url_pattern_rurl_patternmaining) = url_pattern\n    url_patterns += [[namespace_path + pat[0], pat[1]] for pat in url_pattern_0]', 'for (url_pattern_0, *url_pattern_len) in urlresolver.reverse_dict.getlist(url_name):\n    url_patterns += [[namespace_path + pat[0], pat[1]] for pat in url_pattern_0]']",no_found,0
workalendar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/workalendar/workalendar/asia/china.py,https://github.com/workalendar/workalendar/tree/master/workalendar/asia/china.py,China,__init__$97,"def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.extra_working_days = []
    for (year, data) in workdays.items():
        for (holiday_name, day_list) in data.items():
            for v in day_list:
                self.extra_working_days.append(date(year, v[0], v[1]))","for v in day_list:
    self.extra_working_days.append(date(year, v[0], v[1]))","['for v in day_list:\n    (v_0, v_1, *_) = v\n    self.extra_working_days.append(date(year, v_0, v_1))', 'for (v_0, v_1, *v_len) in day_list:\n    self.extra_working_days.append(date(year, v_0, v_1))']",no_found,0
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/physics/vector/dyadic.py,https://github.com/sympy/sympy/tree/master/sympy/physics/vector/dyadic.py,Dyadic,simplify$496,"def simplify(self):
    """"""Returns a simplified Dyadic.""""""
    out = Dyadic(0)
    for v in self.args:
        out += Dyadic([(v[0].simplify(), v[1], v[2])])
    return out","for v in self.args:
    out += Dyadic([(v[0].simplify(), v[1], v[2])])","['for v in self.args:\n    (v_0, v_1, v_2, *_) = v\n    out += Dyadic([(v_0.simplify(), v_1, v_2)])', 'for (v_0, v_1, v_2, *v_len) in self.args:\n    out += Dyadic([(v_0.simplify(), v_1, v_2)])']",no_found,0
trezor-firmware,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/trezor-firmware/core/tests/test_trezor.strings.py,https://github.com/trezor/trezor-firmware/tree/master/core/tests/test_trezor.strings.py,TestStrings,test_format_plural$25,"def test_format_plural(self):
    VECTORS = [('We need {count} more {plural}', 1, 'share', 'We need 1 more share'), ('We need {count} more {plural}', 3, 'share', 'We need 3 more shares'), ('We need {count} more {plural}', 1, 'candy', 'We need 1 more candy'), ('We need {count} more {plural}', 7, 'candy', 'We need 7 more candies'), ('We need {count} more {plural}', 1, 'key', 'We need 1 more key'), ('We need {count} more {plural}', 5, 'key', 'We need 5 more keys'), ('We need {count} more {plural}', 1, 'hash', 'We need 1 more hash'), ('We need {count} more {plural}', 2, 'hash', 'We need 2 more hashes'), ('We need {count} more {plural}', 1, 'fuzz', 'We need 1 more fuzz'), ('We need {count} more {plural}', 2, 'fuzz', 'We need 2 more fuzzes')]
    for v in VECTORS:
        self.assertEqual(strings.format_plural(v[0], v[1], v[2]), v[3])
    with self.assertRaises(ValueError):
        strings.format_plural('Hello', 1, 'share')","for v in VECTORS:
    self.assertEqual(strings.format_plural(v[0], v[1], v[2]), v[3])","['for v in VECTORS:\n    (v_0, v_1, v_2, v_3, *_) = v\n    self.assertEqual(strings.format_plural(v_0, v_1, v_2), v_3)', 'for (v_0, v_1, v_2, v_3, *v_len) in VECTORS:\n    self.assertEqual(strings.format_plural(v_0, v_1, v_2), v_3)']",no_found,0
sparrow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sparrow/views/report.py,https://github.com/wylok/sparrow/tree/master/views/report.py,,alarm_report$222,"def alarm_report():
    try:
        INFOS = []
        total_key = 'op_totals_alarms_tmp'

        def counts_alarm(key, total_key=None):
            vals = []
            for i in range(7):
                count_key = 'op_counts_alarms_tmp'
                data_now = datetime.datetime.now() - datetime.timedelta(days=i)
                dd = data_now.strftime('%Y-%m-%d')
                alarm_count_key = '%s_%s' % (key, dd)
                if RC_CLUSTER.exists(alarm_count_key):
                    vals = RC_CLUSTER.hgetall(alarm_count_key)
                    vals = sorted(vals.items(), key=lambda item: int(item[1]))
                    for val in vals:
                        RC_CLUSTER.hincrby(count_key, val[0], val[1])
                        if total_key:
                            RC_CLUSTER.hincrby(total_key, dd, val[1])
            if RC_CLUSTER.exists(count_key):
                vals = RC_CLUSTER.hgetall(count_key)
                RC_CLUSTER.delete(count_key)
                vals = sorted(vals.items(), key=lambda item: int(item[1]), reverse=True)
            if len(vals) > 10:
                return vals[:10]
            else:
                return vals
        alarm_count = counts_alarm('op_business_alarm_count', total_key=total_key)
        vals = counts_alarm('op_business_alarm_perf')
        if vals:
            pie_perf = Pie('7TOP10', width='100%', height='100%', title_pos='center', title_text_size=14)
            attrs = [val[0] for val in vals]
            vals = [int(val[1]) for val in vals]
            pie_perf.add('', attrs, vals, is_label_show=True, is_toolbox_show=False, legend_orient='vertical', legend_pos='left', xaxis_interval=0, is_random=True, rosetype='area')
            INFOS.append(pie_perf)
        vals = counts_alarm('op_business_alarm_busi')
        if vals:
            pie_busi = Pie('7TOP10', width='100%', height='100%', title_pos='center', title_text_size=14)
            attrs = [val[0] for val in vals]
            vals = [int(val[1]) for val in vals]
            pie_busi.add('', attrs, vals, is_label_show=True, is_toolbox_show=False, legend_orient='vertical', legend_pos='left', xaxis_interval=0, is_random=True, rosetype='radius', radius=[35, 75])
            INFOS.append(pie_busi)
        if RC_CLUSTER.exists(total_key):
            vals = RC_CLUSTER.hgetall(total_key)
            vals = sorted(vals.items(), key=lambda item: item[0], reverse=True)
            RC_CLUSTER.delete(total_key)
            line = Line('7', width='100%', height='100%', title_pos='center', title_text_size=14)
            attrs = [val[0] for val in vals]
            vals = [int(val[1]) for val in vals]
            line.add('', attrs, vals, is_label_show=True, is_toolbox_show=False, is_legend_show=False, xaxis_interval=0, is_random=True)
            INFOS.append(line)
    except Exception as e:
        logging.error(e)
        return redirect(url_for('error'))
    return render_template('alarm_report.html', INFOS=INFOS, alarm_count=alarm_count)","for val in vals:
    RC_CLUSTER.hincrby(count_key, val[0], val[1])
    if total_key:
        RC_CLUSTER.hincrby(total_key, dd, val[1])","['for val in vals:\n    (val_0, val_1, *_) = val\n    RC_CLUSTER.hincrby(count_key, val_0, val_1)\n    if total_key:\n        RC_CLUSTER.hincrby(total_key, dd, val_1)', 'for (val_0, val_1, *val_len) in vals:\n    RC_CLUSTER.hincrby(count_key, val_0, val_1)\n    if total_key:\n        RC_CLUSTER.hincrby(total_key, dd, val_1)']",no_found,0
werkzeug,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/werkzeug/src/werkzeug/datastructures.py,https://github.com/pallets/werkzeug/tree/master/src/werkzeug/datastructures.py,MultiDict,values$497,"def values(self):
    """"""Returns an iterator of the first value on every key's value list.""""""
    for values in dict.values(self):
        yield values[0]","for values in dict.values(self):
    yield values[0]","['for values in dict.values(self):\n    (values_0, *values_rvaluesmaining) = values\n    yield values_0', 'for (values_0, *values_len) in dict.values(self):\n    yield\n    values_0']",no_found,0
takeover,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/takeover/takeover.py,https://github.com/m4ll0k/takeover/tree/master//takeover.py,,find$158,"def find(status, content, ok):
    for service in services:
        for values in services[service].items():
            if re.findall(str(values[1]), str(content), re.I) and int(status) in range(201 if ok is False else 200, 599):
                return (str(service), str(values[1]))","for values in services[service].items():
    if re.findall(str(values[1]), str(content), re.I) and int(status) in range(201 if ok is False else 200, 599):
        return (str(service), str(values[1]))","['for values in services[service].items():\n    (_, values_1, *values_rvaluesmaining) = values\n    if re.findall(str(values_1), str(content), re.I) and int(status) in range(201 if ok is False else 200, 599):\n        return (str(service), str(values_1))', 'for (values_0, values_1, *values_len) in services[service].items():\n    if re.findall(str(values_1), str(content), re.I) and int(status) in range(201 if ok is False else 200, 599):\n        return (str(service), str(values_1))']",no_found,0
AIDungeon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AIDungeon/story/utils.py,https://github.com/Latitude-Archives/AIDungeon/tree/master/story/utils.py,,is_first_person$151,"def is_first_person(text):
    count = 0
    for pair in first_to_second_mappings:
        variations = mapping_variation_pairs(pair)
        for variation in variations:
            reg_expr = re.compile(variation[0] + '(?=([^""]*""[^""]*"")*[^""]*$)')
            matches = re.findall(reg_expr, text)
            count += len(matches)
    if count > 3:
        return True
    else:
        return False","for variation in variations:
    reg_expr = re.compile(variation[0] + '(?=([^""]*""[^""]*"")*[^""]*$)')
    matches = re.findall(reg_expr, text)
    count += len(matches)","['for variation in variations:\n    (variation_0, *variation_rvariationmaining) = variation\n    reg_expr = re.compile(variation_0 + \'(?=([^""]*""[^""]*"")*[^""]*$)\')\n    matches = re.findall(reg_expr, text)\n    count += len(matches)', 'for (variation_0, *variation_len) in variations:\n    reg_expr = re.compile(variation_0 + \'(?=([^""]*""[^""]*"")*[^""]*$)\')\n    matches = re.findall(reg_expr, text)\n    count += len(matches)']",no_found,0
AIDungeon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AIDungeon/story/utils.py,https://github.com/Latitude-Archives/AIDungeon/tree/master/story/utils.py,,first_to_second_person$273,"def first_to_second_person(text):
    text = ' ' + text
    text = standardize_punctuation(text)
    for pair in first_to_second_mappings:
        variations = mapping_variation_pairs(pair)
        for variation in variations:
            text = replace_outside_quotes(text, variation[0], variation[1])
    return capitalize_first_letters(text[1:])","for variation in variations:
    text = replace_outside_quotes(text, variation[0], variation[1])","['for variation in variations:\n    (variation_0, variation_1, *_) = variation\n    text = replace_outside_quotes(text, variation_0, variation_1)', 'for (variation_0, variation_1, *variation_len) in variations:\n    text = replace_outside_quotes(text, variation_0, variation_1)']",no_found,0
Transformer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer/Beam.py,https://github.com/SamLynnEvans/Transformer/tree/master//Beam.py,,beam_search$55,"def beam_search(src, model, SRC, TRG, opt):
    (outputs, e_outputs, log_scores) = init_vars(src, model, SRC, TRG, opt)
    eos_tok = TRG.vocab.stoi['<eos>']
    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)
    ind = None
    for i in range(2, opt.max_len):
        trg_mask = nopeak_mask(i, opt)
        out = model.out(model.decoder(outputs[:, :i], e_outputs, src_mask, trg_mask))
        out = F.softmax(out, dim=-1)
        (outputs, log_scores) = k_best_outputs(outputs, out, log_scores, i, opt.k)
        ones = (outputs == eos_tok).nonzero()
        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()
        for vec in ones:
            i = vec[0]
            if sentence_lengths[i] == 0:
                sentence_lengths[i] = vec[1]
        num_finished_sentences = len([s for s in sentence_lengths if s > 0])
        if num_finished_sentences == opt.k:
            alpha = 0.7
            div = 1 / sentence_lengths.type_as(log_scores) ** alpha
            (_, ind) = torch.max(log_scores * div, 1)
            ind = ind.data[0]
            break
    if ind is None:
        length = (outputs[0] == eos_tok).nonzero()[0]
        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])
    else:
        length = (outputs[ind] == eos_tok).nonzero()[0]
        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])","for vec in ones:
    i = vec[0]
    if sentence_lengths[i] == 0:
        sentence_lengths[i] = vec[1]","['for vec in ones:\n    (vec_0, vec_1, *_) = vec\n    i = vec_0\n    if sentence_lengths[i] == 0:\n        sentence_lengths[i] = vec_1', 'for (vec_0, vec_1, *vec_len) in ones:\n    i = \n    vec_0\n    if sentence_lengths[i] == 0:\n        sentence_lengths[i] = \n        vec_1']",no_found,0
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/creat_vuln_detail.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/creat_vuln_detail.py,Creat_vuln_detail,creat_detail$70,"def creat_detail(self, document):
    para1 = Creat_vuln_detail(self.projectTag).locat_detail(document)
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    k = len(vuln_infos)
    try:
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = para1.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_unauth_maybe}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para2.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para2.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para2.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para2.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num = self.creat_num + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
            elif vuln_info[3] == 'INFO':
                sql = ""select * from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_infos = cursor.fetchall()
                for js_info in js_infos:
                    js_name = js_info[1]
                    js_path = js_info[2]
                    para2 = para1.insert_paragraph_before('')
                    run = para2.add_run('2.' + str(self.creat_num) + ' ' + str(js_name) + Utils().getMyWord('{r_vuln_info}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    run2 = para2.add_run(Utils().getMyWord('{r_js_path}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para2.add_run(js_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para2.add_run('\n' + Utils().getMyWord('{r_js_des}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para2.add_run(vuln_info[8])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_js_detial}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    self.creat_num = self.creat_num + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
            elif vuln_info[3] == 'CORS':
                sql = ""select vaule from info where name='%s'"" % 'host'
                cursor.execute(sql)
                infos = cursor.fetchall()
                for info in infos:
                    api_path = info[0]
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                run5 = para3.add_run('2.' + str(self.creat_num) + ' ' + str(api_path) + Utils().getMyWord('{r_vuln_CORS}') + '\n')
                run5.font.name = 'Arial'
                run5.font.size = Pt(16)
                run5.font.bold = True
                run6 = para3.add_run(':')
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                run7 = para3.add_run(api_path)
                run7.font.name = 'Arial'
                run7.font.size = Pt(10)
                run8 = para3.add_run('\n' + '{response_head}')
                run8.font.name = 'Arial'
                run8.font.size = Pt(10)
                run8.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[7], para2)
                run9 = para2.add_run('\n' + '{request_head}')
                run9.font.name = 'Arial'
                run9.font.size = Pt(10)
                run9.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        for vuln_info in vuln_infos:
            if vuln_info[3] == 'passWord':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                        run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
            elif vuln_info[3] == 'BAC':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                        info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                        Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
            elif vuln_info[3] == 'upLoad':
                sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
                cursor.execute(sql)
                api_infos = cursor.fetchall()
                for api_info in api_infos:
                    para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                    para3 = para2.insert_paragraph_before('')
                    UserLogin = api_info[2]
                    api_path = api_info[1]
                    run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
                    run.font.name = 'Arial'
                    run.font.size = Pt(16)
                    run.font.bold = True
                    sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                    cursor.execute(sql)
                    js_paths = cursor.fetchall()
                    for js_path in js_paths:
                        run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                        run2.font.name = 'Arial'
                        run2.font.size = Pt(10)
                        run2.font.bold = True
                        run3 = para3.add_run(api_path)
                        run3.font.name = 'Arial'
                        run3.font.size = Pt(10)
                        run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                        run4.font.name = 'Arial'
                        run4.font.size = Pt(10)
                        run4.font.bold = True
                        run5 = para3.add_run(js_path[0])
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                        run5.font.name = 'Arial'
                        run5.font.size = Pt(10)
                        run5.font.bold = True
                        self.creat_num1 = self.creat_num1 + 1
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                        run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                        run6.font.name = 'Arial'
                        run6.font.size = Pt(10)
                        run6.font.bold = True
                        Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
            elif vuln_info[3] == 'SQL':
                para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
                para3 = para2.insert_paragraph_before('')
                UserLogin = api_info[2]
                api_path = api_info[1]
                run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
                run.font.name = 'Arial'
                run.font.size = Pt(16)
                run.font.bold = True
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                for js_path in js_paths:
                    run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                    run2.font.name = 'Arial'
                    run2.font.size = Pt(10)
                    run2.font.bold = True
                    run3 = para3.add_run(api_path)
                    run3.font.name = 'Arial'
                    run3.font.size = Pt(10)
                    run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                    run4.font.name = 'Arial'
                    run4.font.size = Pt(10)
                    run4.font.bold = True
                    run5 = para3.add_run(js_path[0])
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                    run5.font.name = 'Arial'
                    run5.font.size = Pt(10)
                    run5.font.bold = True
                    self.creat_num1 = self.creat_num1 + 1
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                    run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                    run6.font.name = 'Arial'
                    run6.font.size = Pt(10)
                    run6.font.bold = True
                    Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
        self.log.debug('vuln_detail')
    except Exception as e:
        self.log.error('[Err] %s' % e)","for vuln_info in vuln_infos:
    if vuln_info[3] == 'passWord':
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
            para3 = para2.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_passWord}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para3.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para3.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para3.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num1 = self.creat_num1 + 1
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[6]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)
                run6 = para2.add_run(Utils().getMyWord('{request_info}'))
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                vuln_info_js_unicode = json.dumps(json.loads(vuln_info[5]), sort_keys=True, indent=4, ensure_ascii=False)
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)
    elif vuln_info[3] == 'BAC':
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
            para3 = para2.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_bac}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para3.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para3.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num1 = self.creat_num1 + 1
                info1 = '1: ' + vuln_info[5].split('')[0] + '\n\n' + '2: ' + vuln_info[5].split('')[1]
                info2 = '1: ' + vuln_info[6].split('')[0] + '\n\n' + '2: ' + vuln_info[6].split('')[1]
                Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)
                run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)
    elif vuln_info[3] == 'upLoad':
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
            para3 = para2.insert_paragraph_before('')
            UserLogin = api_info[2]
            api_path = api_info[1]
            run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_upload}') + '\n')
            run.font.name = 'Arial'
            run.font.size = Pt(16)
            run.font.bold = True
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            for js_path in js_paths:
                run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
                run2.font.name = 'Arial'
                run2.font.size = Pt(10)
                run2.font.bold = True
                run3 = para3.add_run(api_path)
                run3.font.name = 'Arial'
                run3.font.size = Pt(10)
                run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
                run4.font.name = 'Arial'
                run4.font.size = Pt(10)
                run4.font.bold = True
                run5 = para3.add_run(js_path[0])
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
                run5.font.name = 'Arial'
                run5.font.size = Pt(10)
                run5.font.bold = True
                self.creat_num1 = self.creat_num1 + 1
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
                run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
                run6.font.name = 'Arial'
                run6.font.size = Pt(10)
                run6.font.bold = True
                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)
    elif vuln_info[3] == 'SQL':
        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)
        para3 = para2.insert_paragraph_before('')
        UserLogin = api_info[2]
        api_path = api_info[1]
        run = para3.add_run('2.' + str(k - self.creat_num1 + 1) + ' ' + str(UserLogin) + Utils().getMyWord('{r_vuln_sql}') + '\n')
        run.font.name = 'Arial'
        run.font.size = Pt(16)
        run.font.bold = True
        sql = ""select path from js_file where id='%s'"" % vuln_info[2]
        cursor.execute(sql)
        js_paths = cursor.fetchall()
        for js_path in js_paths:
            run2 = para3.add_run(Utils().getMyWord('{r_api_addr}'))
            run2.font.name = 'Arial'
            run2.font.size = Pt(10)
            run2.font.bold = True
            run3 = para3.add_run(api_path)
            run3.font.name = 'Arial'
            run3.font.size = Pt(10)
            run4 = para3.add_run('\n' + Utils().getMyWord('{r_api_js}'))
            run4.font.name = 'Arial'
            run4.font.size = Pt(10)
            run4.font.bold = True
            run5 = para3.add_run(js_path[0])
            run5.font.name = 'Arial'
            run5.font.size = Pt(10)
            run5 = para3.add_run('\n' + Utils().getMyWord('{request_info}'))
            run5.font.name = 'Arial'
            run5.font.size = Pt(10)
            run5.font.bold = True
            self.creat_num1 = self.creat_num1 + 1
            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[6], para2)
            run6 = para2.add_run('\n' + Utils().getMyWord('{r_api_res}'))
            run6.font.name = 'Arial'
            run6.font.size = Pt(10)
            run6.font.bold = True
            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info[5], para3)","['for vuln_info in vuln_infos:\n    (_, vuln_info_1, vuln_info_2, vuln_info_3, _, vuln_info_5, vuln_info_6, *_) = vuln_info\n    if vuln_info_3 == \'passWord\':\n        sql = ""select * from api_tree where id=\'%s\'"" % vuln_info_1\n        cursor.execute(sql)\n        api_infos = cursor.fetchall()\n        for api_info in api_infos:\n            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n            para3 = para2.insert_paragraph_before(\'\')\n            UserLogin = api_info[2]\n            api_path = api_info[1]\n            run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_passWord}\') + \'\\n\')\n            run.font.name = \'Arial\'\n            run.font.size = Pt(16)\n            run.font.bold = True\n            sql = ""select path from js_file where id=\'%s\'"" % vuln_info_2\n            cursor.execute(sql)\n            js_paths = cursor.fetchall()\n            for js_path in js_paths:\n                run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n                run2.font.name = \'Arial\'\n                run2.font.size = Pt(10)\n                run2.font.bold = True\n                run3 = para3.add_run(api_path)\n                run3.font.name = \'Arial\'\n                run3.font.size = Pt(10)\n                run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n                run4.font.name = \'Arial\'\n                run4.font.size = Pt(10)\n                run4.font.bold = True\n                run5 = para3.add_run(js_path[0])\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5.font.bold = True\n                self.creat_num1 = self.creat_num1 + 1\n                vuln_info_js_unicode = json.dumps(json.loads(vuln_info_6), sort_keys=True, indent=4, ensure_ascii=False)\n                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)\n                run6 = para2.add_run(Utils().getMyWord(\'{request_info}\'))\n                run6.font.name = \'Arial\'\n                run6.font.size = Pt(10)\n                run6.font.bold = True\n                vuln_info_js_unicode = json.dumps(json.loads(vuln_info_5), sort_keys=True, indent=4, ensure_ascii=False)\n                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)\n    elif vuln_info_3 == \'BAC\':\n        sql = ""select * from api_tree where id=\'%s\'"" % vuln_info_1\n        cursor.execute(sql)\n        api_infos = cursor.fetchall()\n        for api_info in api_infos:\n            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n            para3 = para2.insert_paragraph_before(\'\')\n            UserLogin = api_info[2]\n            api_path = api_info[1]\n            run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_bac}\') + \'\\n\')\n            run.font.name = \'Arial\'\n            run.font.size = Pt(16)\n            run.font.bold = True\n            sql = ""select path from js_file where id=\'%s\'"" % vuln_info_2\n            cursor.execute(sql)\n            js_paths = cursor.fetchall()\n            for js_path in js_paths:\n                run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n                run2.font.name = \'Arial\'\n                run2.font.size = Pt(10)\n                run2.font.bold = True\n                run3 = para3.add_run(api_path)\n                run3.font.name = \'Arial\'\n                run3.font.size = Pt(10)\n                run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n                run4.font.name = \'Arial\'\n                run4.font.size = Pt(10)\n                run4.font.bold = True\n                run5 = para3.add_run(js_path[0])\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{request_info}\'))\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5.font.bold = True\n                self.creat_num1 = self.creat_num1 + 1\n                info1 = \'1: \' + vuln_info_5.split(\'\')[0] + \'\\n\\n\' + \'2: \' + vuln_info_5.split(\'\')[1]\n                info2 = \'1: \' + vuln_info_6.split(\'\')[0] + \'\\n\\n\' + \'2: \' + vuln_info_6.split(\'\')[1]\n                Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)\n                run6 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n                run6.font.name = \'Arial\'\n                run6.font.size = Pt(10)\n                run6.font.bold = True\n                Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)\n    elif vuln_info_3 == \'upLoad\':\n        sql = ""select * from api_tree where id=\'%s\'"" % vuln_info_1\n        cursor.execute(sql)\n        api_infos = cursor.fetchall()\n        for api_info in api_infos:\n            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n            para3 = para2.insert_paragraph_before(\'\')\n            UserLogin = api_info[2]\n            api_path = api_info[1]\n            run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_upload}\') + \'\\n\')\n            run.font.name = \'Arial\'\n            run.font.size = Pt(16)\n            run.font.bold = True\n            sql = ""select path from js_file where id=\'%s\'"" % vuln_info_2\n            cursor.execute(sql)\n            js_paths = cursor.fetchall()\n            for js_path in js_paths:\n                run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n                run2.font.name = \'Arial\'\n                run2.font.size = Pt(10)\n                run2.font.bold = True\n                run3 = para3.add_run(api_path)\n                run3.font.name = \'Arial\'\n                run3.font.size = Pt(10)\n                run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n                run4.font.name = \'Arial\'\n                run4.font.size = Pt(10)\n                run4.font.bold = True\n                run5 = para3.add_run(js_path[0])\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{request_info}\'))\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5.font.bold = True\n                self.creat_num1 = self.creat_num1 + 1\n                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_6, para2)\n                run6 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n                run6.font.name = \'Arial\'\n                run6.font.size = Pt(10)\n                run6.font.bold = True\n                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_5, para3)\n    elif vuln_info_3 == \'SQL\':\n        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n        para3 = para2.insert_paragraph_before(\'\')\n        UserLogin = api_info[2]\n        api_path = api_info[1]\n        run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_sql}\') + \'\\n\')\n        run.font.name = \'Arial\'\n        run.font.size = Pt(16)\n        run.font.bold = True\n        sql = ""select path from js_file where id=\'%s\'"" % vuln_info_2\n        cursor.execute(sql)\n        js_paths = cursor.fetchall()\n        for js_path in js_paths:\n            run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n            run2.font.name = \'Arial\'\n            run2.font.size = Pt(10)\n            run2.font.bold = True\n            run3 = para3.add_run(api_path)\n            run3.font.name = \'Arial\'\n            run3.font.size = Pt(10)\n            run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n            run4.font.name = \'Arial\'\n            run4.font.size = Pt(10)\n            run4.font.bold = True\n            run5 = para3.add_run(js_path[0])\n            run5.font.name = \'Arial\'\n            run5.font.size = Pt(10)\n            run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{request_info}\'))\n            run5.font.name = \'Arial\'\n            run5.font.size = Pt(10)\n            run5.font.bold = True\n            self.creat_num1 = self.creat_num1 + 1\n            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_6, para2)\n            run6 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n            run6.font.name = \'Arial\'\n            run6.font.size = Pt(10)\n            run6.font.bold = True\n            Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_5, para3)', 'for (vuln_info_0, vuln_info_1, vuln_info_2, vuln_info_3, vuln_info_4, vuln_info_5, vuln_info_6, *vuln_info_len) in vuln_infos:\n    if \n    vuln_info_3 == \'passWord\':\n        sql = ""select * from api_tree where id=\'%s\'"" % \n        vuln_info_1\n        cursor.execute(sql)\n        api_infos = cursor.fetchall()\n        for api_info in api_infos:\n            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n            para3 = para2.insert_paragraph_before(\'\')\n            UserLogin = api_info[2]\n            api_path = api_info[1]\n            run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_passWord}\') + \'\\n\')\n            run.font.name = \'Arial\'\n            run.font.size = Pt(16)\n            run.font.bold = True\n            sql = ""select path from js_file where id=\'%s\'"" % \n            vuln_info_2\n            cursor.execute(sql)\n            js_paths = cursor.fetchall()\n            for js_path in js_paths:\n                run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n                run2.font.name = \'Arial\'\n                run2.font.size = Pt(10)\n                run2.font.bold = True\n                run3 = para3.add_run(api_path)\n                run3.font.name = \'Arial\'\n                run3.font.size = Pt(10)\n                run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n                run4.font.name = \'Arial\'\n                run4.font.size = Pt(10)\n                run4.font.bold = True\n                run5 = para3.add_run(js_path[0])\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5.font.bold = True\n                self.creat_num1 = self.creat_num1 + 1\n                vuln_info_js_unicode = json.dumps(json.loads(\n                vuln_info_6), sort_keys=True, indent=4, ensure_ascii=False)\n                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para2)\n                run6 = para2.add_run(Utils().getMyWord(\'{request_info}\'))\n                run6.font.name = \'Arial\'\n                run6.font.size = Pt(10)\n                run6.font.bold = True\n                vuln_info_js_unicode = json.dumps(json.loads(\n                vuln_info_5), sort_keys=True, indent=4, ensure_ascii=False)\n                Creat_vuln_detail(self.projectTag).creat_table(document, vuln_info_js_unicode, para3)\n    elif \n    vuln_info_3 == \'BAC\':\n        sql = ""select * from api_tree where id=\'%s\'"" % \n        vuln_info_1\n        cursor.execute(sql)\n        api_infos = cursor.fetchall()\n        for api_info in api_infos:\n            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n            para3 = para2.insert_paragraph_before(\'\')\n            UserLogin = api_info[2]\n            api_path = api_info[1]\n            run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_bac}\') + \'\\n\')\n            run.font.name = \'Arial\'\n            run.font.size = Pt(16)\n            run.font.bold = True\n            sql = ""select path from js_file where id=\'%s\'"" % \n            vuln_info_2\n            cursor.execute(sql)\n            js_paths = cursor.fetchall()\n            for js_path in js_paths:\n                run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n                run2.font.name = \'Arial\'\n                run2.font.size = Pt(10)\n                run2.font.bold = True\n                run3 = para3.add_run(api_path)\n                run3.font.name = \'Arial\'\n                run3.font.size = Pt(10)\n                run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n                run4.font.name = \'Arial\'\n                run4.font.size = Pt(10)\n                run4.font.bold = True\n                run5 = para3.add_run(js_path[0])\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{request_info}\'))\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5.font.bold = True\n                self.creat_num1 = self.creat_num1 + 1\n                info1 = \'1: \' + \n                vuln_info_5.split(\'\')[0] + \'\\n\\n\' + \'2: \' + \n                vuln_info_5.split(\'\')[1]\n                info2 = \'1: \' + \n                vuln_info_6.split(\'\')[0] + \'\\n\\n\' + \'2: \' + \n                vuln_info_6.split(\'\')[1]\n                Creat_vuln_detail(self.projectTag).creat_table(document, info2, para2)\n                run6 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n                run6.font.name = \'Arial\'\n                run6.font.size = Pt(10)\n                run6.font.bold = True\n                Creat_vuln_detail(self.projectTag).creat_table(document, info1, para3)\n    elif \n    vuln_info_3 == \'upLoad\':\n        sql = ""select * from api_tree where id=\'%s\'"" % \n        vuln_info_1\n        cursor.execute(sql)\n        api_infos = cursor.fetchall()\n        for api_info in api_infos:\n            para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n            para3 = para2.insert_paragraph_before(\'\')\n            UserLogin = api_info[2]\n            api_path = api_info[1]\n            run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_upload}\') + \'\\n\')\n            run.font.name = \'Arial\'\n            run.font.size = Pt(16)\n            run.font.bold = True\n            sql = ""select path from js_file where id=\'%s\'"" % \n            vuln_info_2\n            cursor.execute(sql)\n            js_paths = cursor.fetchall()\n            for js_path in js_paths:\n                run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n                run2.font.name = \'Arial\'\n                run2.font.size = Pt(10)\n                run2.font.bold = True\n                run3 = para3.add_run(api_path)\n                run3.font.name = \'Arial\'\n                run3.font.size = Pt(10)\n                run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n                run4.font.name = \'Arial\'\n                run4.font.size = Pt(10)\n                run4.font.bold = True\n                run5 = para3.add_run(js_path[0])\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{request_info}\'))\n                run5.font.name = \'Arial\'\n                run5.font.size = Pt(10)\n                run5.font.bold = True\n                self.creat_num1 = self.creat_num1 + 1\n                Creat_vuln_detail(self.projectTag).creat_table(document, \n                vuln_info_6, para2)\n                run6 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n                run6.font.name = \'Arial\'\n                run6.font.size = Pt(10)\n                run6.font.bold = True\n                Creat_vuln_detail(self.projectTag).creat_table(document, \n                vuln_info_5, para3)\n    elif \n    vuln_info_3 == \'SQL\':\n        para2 = Creat_vuln_detail(self.projectTag).insert_paragraph_after(para1)\n        para3 = para2.insert_paragraph_before(\'\')\n        UserLogin = api_info[2]\n        api_path = api_info[1]\n        run = para3.add_run(\'2.\' + str(k - self.creat_num1 + 1) + \' \' + str(UserLogin) + Utils().getMyWord(\'{r_vuln_sql}\') + \'\\n\')\n        run.font.name = \'Arial\'\n        run.font.size = Pt(16)\n        run.font.bold = True\n        sql = ""select path from js_file where id=\'%s\'"" % \n        vuln_info_2\n        cursor.execute(sql)\n        js_paths = cursor.fetchall()\n        for js_path in js_paths:\n            run2 = para3.add_run(Utils().getMyWord(\'{r_api_addr}\'))\n            run2.font.name = \'Arial\'\n            run2.font.size = Pt(10)\n            run2.font.bold = True\n            run3 = para3.add_run(api_path)\n            run3.font.name = \'Arial\'\n            run3.font.size = Pt(10)\n            run4 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_js}\'))\n            run4.font.name = \'Arial\'\n            run4.font.size = Pt(10)\n            run4.font.bold = True\n            run5 = para3.add_run(js_path[0])\n            run5.font.name = \'Arial\'\n            run5.font.size = Pt(10)\n            run5 = para3.add_run(\'\\n\' + Utils().getMyWord(\'{request_info}\'))\n            run5.font.name = \'Arial\'\n            run5.font.size = Pt(10)\n            run5.font.bold = True\n            self.creat_num1 = self.creat_num1 + 1\n            Creat_vuln_detail(self.projectTag).creat_table(document, \n            vuln_info_6, para2)\n            run6 = para2.add_run(\'\\n\' + Utils().getMyWord(\'{r_api_res}\'))\n            run6.font.name = \'Arial\'\n            run6.font.size = Pt(10)\n            run6.font.bold = True\n            Creat_vuln_detail(self.projectTag).creat_table(document, \n            vuln_info_5, para3)']",no_found,0
PaddleX,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex/interpret/core/normlime_base.py,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex/interpret/core/normlime_base.py,,compute_normlime_weights$204,"def compute_normlime_weights(a_list_lime_fnames, save_dir, lime_num_samples):
    normlime_weights_all_labels = {}
    for f in a_list_lime_fnames:
        try:
            lime_weights_and_cluster = np.load(f, allow_pickle=True).item()
            lime_weights = lime_weights_and_cluster['lime_weights']
            cluster = lime_weights_and_cluster['cluster']
        except:
            logging.info('When loading precomputed LIME result, skipping' + str(f))
            continue
        logging.info('Loading precomputed LIME result,' + str(f))
        pred_labels = lime_weights.keys()
        for y in pred_labels:
            normlime_weights = normlime_weights_all_labels.get(y, {})
            w_f_y = [abs(w[1]) for w in lime_weights[y]]
            w_f_y_l1norm = sum(w_f_y)
            for w in lime_weights[y]:
                seg_label = w[0]
                weight = w[1] * w[1] / w_f_y_l1norm
                a = normlime_weights.get(cluster[seg_label], [])
                a.append(weight)
                normlime_weights[cluster[seg_label]] = a
            normlime_weights_all_labels[y] = normlime_weights
    for y in normlime_weights_all_labels:
        normlime_weights = normlime_weights_all_labels.get(y, {})
        for k in normlime_weights:
            normlime_weights[k] = sum(normlime_weights[k]) / len(normlime_weights[k])
    if len(normlime_weights_all_labels.keys()) < max(normlime_weights_all_labels.keys()) + 1:
        logging.info('\n' + 'Warning: !!! \n' + 'There are at least {} classes, '.format(max(normlime_weights_all_labels.keys()) + 1) + 'but the NormLIME has results of only {} classes. \n'.format(len(normlime_weights_all_labels.keys())) + 'It may have cause unstable results in the later computation' + ' but can be improved by computing more test samples.' + '\n')
    n = 0
    f_out = 'normlime_weights_s{}_samples_{}-{}.npy'.format(lime_num_samples, len(a_list_lime_fnames), n)
    while os.path.exists(os.path.join(save_dir, f_out)):
        n += 1
        f_out = 'normlime_weights_s{}_samples_{}-{}.npy'.format(lime_num_samples, len(a_list_lime_fnames), n)
        continue
    np.save(os.path.join(save_dir, f_out), normlime_weights_all_labels)
    return os.path.join(save_dir, f_out)","for w in lime_weights[y]:
    seg_label = w[0]
    weight = w[1] * w[1] / w_f_y_l1norm
    a = normlime_weights.get(cluster[seg_label], [])
    a.append(weight)
    normlime_weights[cluster[seg_label]] = a","['for w in lime_weights[y]:\n    (w_0, w_1, *_) = w\n    seg_label = w_0\n    weight = w_1 * w_1 / w_f_y_l1norm\n    a = normlime_weights.get(cluster[seg_label], [])\n    a.append(weight)\n    normlime_weights[cluster[seg_label]] = a', 'for (w_0, w_1, *w_len) in lime_weights[y]:\n    seg_label = \n    w_0\n    weight = \n    w_1 * \n    w_1 / w_f_y_l1norm\n    a = normlime_weights.get(cluster[seg_label], [])\n    a.append(weight)\n    normlime_weights[cluster[seg_label]] = a']",no_found,0
sublime_debugger,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sublime_debugger/debugger/interfaces/debugger_model.py,https://github.com/shuky19/sublime_debugger/tree/master/debugger/interfaces/debugger_model.py,DebuggerModel,referesh_data$123,"def referesh_data(self):
    for command in DebuggerModel.REFRESHABLE_COMMANDS:
        self.debugger.run_command(command)
    for watch_exp in self.data[DebuggerModel.DATA_WATCH]:
        self.debugger.run_result_command(DebuggerModel.COMMAND_GET_WATCH, watch_exp[0], watch_exp[0])","for watch_exp in self.data[DebuggerModel.DATA_WATCH]:
    self.debugger.run_result_command(DebuggerModel.COMMAND_GET_WATCH, watch_exp[0], watch_exp[0])","['for watch_exp in self.data[DebuggerModel.DATA_WATCH]:\n    (watch_exp_0, *watch_exp_rwatch_expmaining) = watch_exp\n    self.debugger.run_result_command(DebuggerModel.COMMAND_GET_WATCH, watch_exp_0, watch_exp_0)', 'for (watch_exp_0, *watch_exp_len) in self.data[DebuggerModel.DATA_WATCH]:\n    self.debugger.run_result_command(DebuggerModel.COMMAND_GET_WATCH, watch_exp_0, watch_exp_0)']",no_found,0
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2020/med_test.py,https://github.com/3b1b/videos/tree/master/_2020/med_test.py,SamplePopulationBreastCancer,construct$820,"def construct(self):
    title = TexText('Sample of ', '$1{,}000$', ' women', font_size=72)
    title.add(Underline(title, color=GREY_B))
    title.to_edge(UP, buff=MED_SMALL_BUFF)
    self.add(title)
    woman = WomanIcon()
    globals()['woman'] = woman
    population = VGroup(*[woman.copy() for x in range(1000)])
    population.arrange_in_grid(25, 40, buff=LARGE_BUFF, fill_rows_first=False)
    population.set_height(6)
    population.next_to(title, DOWN)
    counter = Integer(1000, edge_to_fix=UL)
    counter.replace(title[1])
    counter.set_value(0)
    title[1].set_opacity(0)
    self.play(ShowIncreasingSubsets(population), ChangeDecimalToValue(counter, 1000), run_time=5)
    self.remove(counter)
    title[1].set_opacity(1)
    self.wait()
    rects = VGroup(Rectangle(), Rectangle())
    rects.set_height(6)
    rects[0].set_width(4, stretch=True)
    rects[1].set_width(8, stretch=True)
    rects[0].set_stroke(YELLOW, 3)
    rects[1].set_stroke(GREY, 3)
    rects.arrange(RIGHT)
    rects.center().to_edge(DOWN, buff=MED_SMALL_BUFF)
    positive_cases = population[:10]
    negative_cases = population[10:]
    positive_cases.generate_target()
    positive_cases.target.move_to(rects[0])
    positive_cases.target.set_color(YELLOW)
    negative_cases.generate_target()
    negative_cases.target.set_height(rects[1].get_height() * 0.8)
    negative_cases.target.move_to(rects[1])
    positive_words = TexText('1\\% ', 'Have breast cancer', font_size=36)
    positive_words.set_color(YELLOW)
    positive_words.next_to(rects[0], UP, SMALL_BUFF)
    negative_words = TexText('99\\% ', 'Do not have cancer', font_size=36)
    negative_words.set_color(GREY_B)
    negative_words.next_to(rects[1], UP, SMALL_BUFF)
    self.play(MoveToTarget(positive_cases), MoveToTarget(negative_cases), Write(positive_words, run_time=1), Write(negative_words, run_time=1), FadeIn(rects))
    self.wait()
    scan_lines = VGroup(*(Line(FRAME_HEIGHT * DOWN / 2, icon.get_center(), stroke_width=1, stroke_color=interpolate_color(BLUE, GREEN, random.random())) for icon in population))
    self.play(LaggedStartMap(ShowCreationThenFadeOut, scan_lines, lag_ratio=1 / len(scan_lines), run_time=3))
    self.wait()
    tpr_words = TexText('9 True positives', font_size=36)
    fnr_words = TexText('1 False negative', font_size=36)
    tnr_words = TexText('901 True negatives', font_size=36)
    fpr_words = TexText('89 False positives', font_size=36)
    tpr_words.set_color(GREEN_B)
    fnr_words.set_color(RED_D)
    tnr_words.set_color(RED_B)
    fpr_words.set_color(GREEN_D)
    tp_cases = positive_cases[:9]
    fn_cases = positive_cases[9:]
    tpr_words.next_to(tp_cases, UP)
    fnr_words.next_to(fn_cases, DOWN)
    signs = VGroup()
    for woman in tp_cases:
        sign = Tex('+')
        sign.set_color(GREEN_B)
        sign.match_height(woman)
        sign.next_to(woman, RIGHT, SMALL_BUFF)
        woman.sign = sign
        signs.add(sign)
    for woman in fn_cases:
        sign = Tex('-')
        sign.set_color(RED)
        sign.match_width(signs[0])
        sign.next_to(woman, RIGHT, SMALL_BUFF)
        woman.sign = sign
        signs.add(sign)
    boxes = VGroup()
    for (n, woman) in enumerate(positive_cases):
        box = SurroundingRectangle(woman, buff=0)
        box.set_stroke(width=2)
        if woman in tp_cases:
            box.set_color(GREEN)
        else:
            box.set_color(RED)
        woman.box = box
        boxes.add(box)
    self.play(FadeIn(tpr_words, shift=0.2 * UP), ShowIncreasingSubsets(signs[:9]), ShowIncreasingSubsets(boxes[:9]))
    self.wait()
    self.play(FadeIn(fnr_words, shift=0.2 * DOWN), Write(signs[9:]), ShowCreation(boxes[9:]))
    self.wait()
    negative_cases.sort(lambda p: -p[1])
    num_fp = int(len(negative_cases) * 0.09)
    fp_cases = negative_cases[:num_fp]
    tn_cases = negative_cases[num_fp:]
    new_boxes = VGroup()
    for (n, woman) in enumerate(negative_cases):
        box = SurroundingRectangle(woman, buff=0)
        box.set_stroke(width=2)
        if woman in fp_cases:
            box.set_color(GREEN)
        else:
            box.set_color(RED)
        woman.box = box
        new_boxes.add(box)
    fpr_words.next_to(fp_cases, UP, buff=SMALL_BUFF)
    tnr_words.next_to(tn_cases, DOWN, buff=0.2)
    self.play(FadeIn(fpr_words, shift=0.2 * UP), ShowIncreasingSubsets(new_boxes[:num_fp]))
    self.wait()
    self.play(FadeIn(tnr_words, shift=0.2 * DOWN), ShowIncreasingSubsets(new_boxes[num_fp:]))
    self.wait()
    self.remove(boxes, new_boxes, population)
    for woman in population:
        woman.add(woman.box)
    self.add(population)
    for (cases, nr, rect) in zip([tp_cases, fp_cases], [3, 7], rects):
        cases.save_state()
        cases.generate_target()
        for case in cases.target:
            case[-1].set_stroke(width=3)
            case[-1].scale(1.1)
        cases.target.arrange_in_grid(n_rows=nr, buff=0.5 * cases[0].get_width())
        cases.target.scale(0.5 / cases.target[0].get_height())
        cases.target.move_to(rect)
    fp_cases.target.shift(0.4 * DOWN)
    positive_words.save_state()
    negative_words.save_state()
    tpr_words.save_state()
    fpr_words.save_state()
    self.play(MoveToTarget(tp_cases), MoveToTarget(fp_cases), tpr_words.next_to, tp_cases.target, UP, fpr_words.next_to, fp_cases.target, UP, FadeOut(signs), positive_words[0].set_opacity, 0, negative_words[0].set_opacity, 0, positive_words[1].match_x, rects[0], negative_words[1].match_x, rects[1], LaggedStart(FadeOut(fn_cases, shift=DOWN), FadeOut(fnr_words, shift=DOWN), FadeOut(tn_cases, shift=DOWN), FadeOut(tnr_words, shift=DOWN)))
    self.wait()
    self.play(ShowCreationThenFadeOut(SurroundingRectangle(tpr_words[0][:1], stroke_width=2, stroke_color=WHITE, buff=0.05)), LaggedStartMap(Indicate, tp_cases, color=YELLOW, lag_ratio=0.3, run_time=1))
    self.wait()
    self.play(ShowCreationThenFadeOut(SurroundingRectangle(fpr_words[0][:2], stroke_width=2, stroke_color=WHITE, buff=0.05)), LaggedStartMap(Indicate, fp_cases, color=GREEN_A, lag_ratio=0.05, run_time=3))
    self.wait()
    equation = Tex('P(', '\\text{Have cancer }', '|', '\\text{ positive test})', '\\approx', '\\frac{9}{9 + 89}', '\\approx \\frac{1}{11}')
    equation.set_color_by_tex('cancer', YELLOW)
    equation.set_color_by_tex('positive', GREEN)
    equation.to_edge(UP, buff=SMALL_BUFF)
    self.play(FadeIn(equation[:-1], shift=UP), FadeOut(title, shift=UP))
    self.wait()
    self.play(Write(equation[-1]))
    self.wait()
    frame = self.camera.frame
    frame.save_state()
    ppv_words = TexText('Positive\\\\', 'Predictive\\\\', 'Value\\\\', alignment='')
    ppv_words.next_to(equation, RIGHT, LARGE_BUFF, DOWN)
    for word in ppv_words:
        word[0].set_color(BLUE)
    ppv_rhs = Tex('={\\text{TP} \\over \\text{TP} + \\text{FP}}', tex_to_color_map={'\\text{TP}': GREEN_B, '\\text{FP}': GREEN_C})
    ppv_rhs.next_to(ppv_words, RIGHT)
    ppv_rhs.shift(1.5 * LEFT)
    self.play(frame.scale, 1.1, {'about_edge': DL})
    self.play(ShowIncreasingSubsets(ppv_words))
    self.wait()
    self.play(equation.shift, 1.5 * LEFT + 0.5 * UP, ppv_words.shift, 1.5 * LEFT, FadeIn(ppv_rhs, lag_ratio=0.1), frame.scale, 1.1, {'about_edge': DL})
    self.wait()
    self.play(frame.restore, frame.shift, 0.5 * DOWN, LaggedStartMap(FadeOut, VGroup(equation, ppv_words, ppv_rhs)), LaggedStartMap(Restore, VGroup(tpr_words, tp_cases, fpr_words, fp_cases)), run_time=3)
    self.play(LaggedStartMap(FadeIn, VGroup(fnr_words, fn_cases, tnr_words, tn_cases)))
    self.wait()
    fade_rects = VGroup(*(BackgroundRectangle(VGroup(rect, words), fill_opacity=0.9, fill_color=BLACK, buff=SMALL_BUFF) for (rect, words) in zip(rects, [positive_words, negative_words])))
    sens_eq = Tex('\\text{Sensitivity}', '= {9 \\over 10}', '= 90\\%')
    sens_eq.next_to(rects[0], LEFT, MED_LARGE_BUFF, aligned_edge=UP)
    sens_eq.shift(DOWN)
    fnr_eq = Tex('\\text{False Negative Rate}', '= 10\\%')
    fnr_eq.set_color(RED)
    fnr_eq.scale(0.9)
    equiv = Tex('\\Leftrightarrow')
    equiv.scale(1.5)
    equiv.rotate(90 * DEGREES)
    equiv.next_to(sens_eq, DOWN, MED_LARGE_BUFF)
    fnr_eq.next_to(equiv, DOWN, MED_LARGE_BUFF)
    self.play(frame.shift, 5 * LEFT, FadeIn(fade_rects[1]), Write(sens_eq[0]))
    self.wait()
    self.play(TransformFromCopy(tpr_words[0][0], sens_eq[1][1]), Write(sens_eq[1][0]), Write(sens_eq[1][2:]))
    self.play(Write(sens_eq[2]))
    self.wait()
    self.play(FadeIn(equiv, shift=0.5 * DOWN), FadeIn(fnr_eq, shift=1.0 * DOWN))
    self.wait()
    fade_rects[0].stretch(5, 0, about_edge=RIGHT)
    self.play(ApplyMethod(frame.shift, 10 * RIGHT, run_time=4), FadeIn(fade_rects[0], run_time=2), FadeOut(fade_rects[1], run_time=2))
    spec_eq = Tex('\\text{Specificity}', '= {901 \\over 990}', '\\approx 91\\%')
    spec_eq.next_to(rects[1], RIGHT, MED_LARGE_BUFF, aligned_edge=DOWN)
    spec_eq.shift(UP)
    fpr_eq = Tex('\\text{False Positive Rate}', '= 9\\%')
    fpr_eq.set_color(GREEN)
    fpr_eq.scale(0.9)
    equiv2 = Tex('\\Leftrightarrow')
    equiv2.scale(1.5)
    equiv2.rotate(90 * DEGREES)
    equiv2.next_to(spec_eq, UP, MED_LARGE_BUFF)
    fpr_eq.next_to(equiv2, UP, MED_LARGE_BUFF)
    self.play(Write(spec_eq[0]))
    self.wait()
    self.play(Write(spec_eq[1][0]), TransformFromCopy(tnr_words[0][:3], spec_eq[1][1:4], run_time=2, path_arc=30 * DEGREES), Write(spec_eq[1][4:]))
    self.wait()
    self.play(Write(spec_eq[2]))
    self.wait()
    self.play(FadeIn(equiv2, shift=0.5 * UP), FadeIn(fpr_eq, shift=1.0 * UP))
    self.wait()
    eqs = [sens_eq, spec_eq]
    for (eq, word) in zip(eqs, [positive_words, negative_words]):
        eq.generate_target()
        eq.target[1].set_opacity(0)
        (eq.target[2].move_to(eq.target[1], LEFT),)
        eq.target.next_to(word, UP, buff=0.3)
    self.play(FadeOut(fade_rects[0]), frame.shift, 5 * LEFT, frame.scale, 1.1, {'about_edge': DOWN}, MoveToTarget(sens_eq), MoveToTarget(spec_eq), *map(FadeOut, (fnr_eq, fpr_eq, equiv, equiv2)), run_time=2)
    self.wait()
    self.play(VGroup(fn_cases, fnr_words, fp_cases, fpr_words).set_opacity, 0.2, rate_func=there_and_back_with_pause, run_time=3)","for word in ppv_words:
    word[0].set_color(BLUE)","['for word in ppv_words:\n    (word_0, *word_rwordmaining) = word\n    word_0.set_color(BLUE)', 'for (word_0, *word_len) in ppv_words:\n    word_0.set_color(BLUE)']",no_found,0
gluon-nlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-nlp/scripts/processing/learn_subword.py,https://github.com/dmlc/gluon-nlp/tree/master/scripts/processing/learn_subword.py,,main$103,"def main(args):
    corpus_path_list = args.corpus
    if args.save_dir is None:
        args.save_dir = args.model
    for corpus_path in corpus_path_list:
        if not os.path.exists(corpus_path):
            raise ValueError('The path=""{}"" provided by --corpus does not exist!'.format(corpus_path))
    print('Learn the ""{}""s subword model based on {}.'.format(args.model, args.corpus))
    os.makedirs(args.save_dir, exist_ok=True)
    model_prefix = os.path.join(args.save_dir, args.model)
    print('Save the subword model to {}.model'.format(model_prefix))
    print('Save the vocabulary to {}.vocab'.format(model_prefix))
    print()
    print('------- Start Training -------------')
    special_tokens_kv = OrderedDict()
    if not args.disable_unk:
        special_tokens_kv['unk_token'] = Vocab.UNK_TOKEN
    if not args.disable_bos:
        special_tokens_kv['bos_token'] = Vocab.BOS_TOKEN
    if not args.disable_eos:
        special_tokens_kv['eos_token'] = Vocab.EOS_TOKEN
    if not args.disable_pad:
        special_tokens_kv['pad_token'] = Vocab.PAD_TOKEN
    if args.model in ['yttm'] and len(args.custom_special_tokens) > 0:
        raise ValueError('model {} do not support custom_special_tokens'.format(args.model))
    additional_custom_special_token = OrderedDict()
    for custom_special_token in args.custom_special_tokens:
        kv = custom_special_token.split('=')
        if not len(kv) == 2:
            raise ValueError('parameter {} has wrong format'.format(custom_special_token))
        (k, v) = (kv[0], kv[1])
        if k in special_tokens_kv:
            warnings.warn(f'There are overlaps between the custom special tokens and the unk, bos, eos, pad tokens. Currently, we will overwrite the default tokens. We will overwrite ""{k}"" to ""{v}""')
        special_tokens_kv[k] = v
        additional_custom_special_token[k] = v
    if args.model == 'hf_wordpiece':
        tokenizers = try_import_huggingface_tokenizers()
        if 'unk_token' not in special_tokens_kv or special_tokens_kv['unk_token'] != '[UNK]':
            special_tokens_kv['unk_token'] = '[UNK]'
        if parse_version(tokenizers.__version__) < parse_version('0.8'):
            if 'mask_token' not in special_tokens_kv:
                special_tokens_kv['mask_token'] = Vocab.MASK_TOKEN
            if 'cls_token' not in special_tokens_kv:
                special_tokens_kv['cls_token'] = Vocab.CLS_TOKEN
            if 'sep_token' not in special_tokens_kv:
                special_tokens_kv['sep_token'] = Vocab.SEP_TOKEN
    special_tokens = list(special_tokens_kv.values())
    print('special tokens: ' + ', '.join(special_tokens))
    vocab = []
    if args.model == 'spm':
        try_import_sentencepiece()
        import sentencepiece as spm
        corpus_path = ','.join(corpus_path_list)
        script = '--input={} --model_prefix={} --vocab_size={} --character_coverage={} --input_sentence_size={}'.format(corpus_path, model_prefix, args.vocab_size, args.coverage, args.input_sentence_size)
        script += ' --unk_id=' + str(list(special_tokens_kv.keys()).index('unk_token'))
        script += ' --bos_id=' + ('-1' if args.disable_bos else str(list(special_tokens_kv.keys()).index('bos_token')))
        script += ' --eos_id=' + ('-1' if args.disable_eos else str(list(special_tokens_kv.keys()).index('eos_token')))
        script += ' --pad_id=' + ('-1' if args.disable_pad else str(list(special_tokens_kv.keys()).index('pad_token')))
        if len(additional_custom_special_token) > 0:
            script += ' --control_symbols=' + ','.join(list(additional_custom_special_token.values()))
        print(script)
        spm.SentencePieceTrainer.Train(script)
        if 'bos_token' in special_tokens_kv:
            special_tokens_kv['bos_token'] = '<s>'
        if 'eos_token' in special_tokens_kv:
            special_tokens_kv['eos_token'] = '</s>'
        spm_model = spm.SentencePieceProcessor()
        spm_model.load(model_prefix + '.model')
        vocab = [spm_model.id_to_piece(i) for i in range(len(spm_model))]
        os.remove(model_prefix + '.vocab')
    elif args.model == 'subword_nmt':
        try_import_subword_nmt()
        from subword_nmt import learn_bpe
        corpus_path = cat_corpus(corpus_path_list) if len(corpus_path_list) > 1 else corpus_path_list[0]
        with open(corpus_path, 'r', encoding='utf-8') as fc, open(model_prefix + '.model', 'w', encoding='utf-8') as fm:
            learn_bpe.learn_bpe(fc, fm, args.vocab_size - len(special_tokens), total_symbols=True)
        with open(corpus_path, 'r', encoding='utf-8') as fc, open(model_prefix + '.model', 'r', encoding='utf-8') as fm:
            vocab.extend(special_tokens)
            uniq_chars_internal = set()
            uniq_chars_final = set()
            uniq_words = set()
            for line in fc:
                for word in line.strip('\r\n ').split(' '):
                    if word:
                        uniq_words.add(word)
            uniq_words = [tuple(x[:-1]) + (x[-1] + '</w>',) for x in uniq_words]
            for word in uniq_words:
                for char in word[:-1]:
                    uniq_chars_internal.add(char)
                uniq_chars_final.add(word[-1])
            vocab.extend(sorted(list(uniq_chars_internal)))
            vocab.extend(sorted(list(uniq_chars_final)))
            fm.readline()
            pair = fm.readline()
            while pair:
                vocab.append(pair.replace(' ', '', 1).strip())
                pair = fm.readline()
        if len(corpus_path_list) > 1:
            os.remove(corpus_path)
    elif args.model == 'yttm':
        try_import_yttm()
        import youtokentome as yttm
        corpus_path = cat_corpus(corpus_path_list) if len(corpus_path_list) > 1 else corpus_path_list[0]
        tokenizer = yttm.BPE.train(data=corpus_path, model=model_prefix + '.model', vocab_size=args.vocab_size, coverage=args.coverage, n_threads=args.n_threads, unk_id=special_tokens.index(Vocab.UNK_TOKEN), bos_id=-1 if args.disable_bos else special_tokens.index(Vocab.BOS_TOKEN), eos_id=-1 if args.disable_eos else special_tokens.index(Vocab.EOS_TOKEN), pad_id=-1 if args.disable_pad else special_tokens.index(Vocab.PAD_TOKEN))
        vocab = tokenizer.vocab()
        if 'unk_token' in special_tokens_kv:
            special_tokens_kv['unk_token'] = '<UNK>'
        if 'bos_token' in special_tokens_kv:
            special_tokens_kv['bos_token'] = '<BOS>'
        if 'eos_token' in special_tokens_kv:
            special_tokens_kv['eos_token'] = '<EOS>'
        if 'pad_token' in special_tokens_kv:
            special_tokens_kv['pad_token'] = '<PAD>'
        if len(corpus_path_list) > 1:
            os.remove(corpus_path)
    elif args.model in ['hf_bpe', 'hf_bytebpe', 'hf_wordpiece']:
        tokenizers = try_import_huggingface_tokenizers()
        if args.model == 'hf_bpe':
            split_on_whitespace_only = not args.split_punctuation
            tokenizer = tokenizers.CharBPETokenizer(lowercase=args.lowercase, bert_normalizer=args.bert_normalizer, split_on_whitespace_only=split_on_whitespace_only)
        elif args.model == 'hf_bytebpe':
            tokenizer = tokenizers.ByteLevelBPETokenizer(lowercase=args.lowercase)
        elif args.model == 'hf_wordpiece':
            unk_token = special_tokens_kv.get('unk_token', None)
            sep_token = special_tokens_kv.get('sep_token', None)
            cls_token = special_tokens_kv.get('cls_token', None)
            pad_token = special_tokens_kv.get('pad_token', None)
            mask_token = special_tokens_kv.get('mask_token', None)
            if args.bert_normalizer:
                strip_accents = None
                clean_text = True
                handle_chinese_chars = True
            else:
                strip_accents = False
                clean_text = False
                handle_chinese_chars = False
            tokenizer = tokenizers.BertWordPieceTokenizer(unk_token=unk_token, sep_token=sep_token, cls_token=cls_token, pad_token=pad_token, mask_token=mask_token, lowercase=args.lowercase, strip_accents=strip_accents, handle_chinese_chars=handle_chinese_chars, clean_text=clean_text)
        else:
            raise NotImplementedError
        tokenizer.train(corpus_path_list, vocab_size=args.vocab_size, show_progress=True, special_tokens=special_tokens)
        if version.parse(tokenizers.__version__) >= version.parse('0.8'):
            save_model_path = model_prefix + '.model'
            tokenizer.save(save_model_path)
            model_info = json.load(open(save_model_path, encoding='utf-8'))
            special_tokens_in_tokenizer = model_info['added_tokens']
            assert len(special_tokens_in_tokenizer) == len(special_tokens)
            hf_vocab = model_info['model']['vocab']
            hf_vocab_sorted = sorted(list(hf_vocab.items()), key=lambda x: x[1])
            hf_vocab_ids = [ele[1] for ele in hf_vocab_sorted]
            assert min(hf_vocab_ids) == 0 and max(hf_vocab_ids) == len(hf_vocab_ids) - 1
            vocab = [ele[0] for ele in hf_vocab_sorted]
        else:
            tokenizer.save(args.save_dir, args.model)
            if args.model == 'hf_wordpiece':
                hf_vocab_file = model_prefix + '-vocab.txt'
                with open(hf_vocab_file, 'r', encoding='utf-8') as fv:
                    for line in fv:
                        vocab.append(line.strip())
            else:
                os.rename(os.path.join(args.save_dir, '{}-merges.txt'.format(args.model)), os.path.join(args.save_dir, '{}.model'.format(args.model)))
                hf_vocab_file = model_prefix + '-vocab.json'
                with open(hf_vocab_file, 'r', encoding='utf-8') as fv:
                    vocab_kv = json.load(fv)
                    vocab_kv = sorted(list(vocab_kv.items()), key=lambda x: x[1])
                    for kv in vocab_kv:
                        vocab.append(kv[0])
            os.remove(hf_vocab_file)
    else:
        raise NotImplementedError
    vocab_obj = Vocab(vocab, **special_tokens_kv)
    vocab_obj.save(model_prefix + '.vocab')
    print('-------- Done Training -------------')","for word in uniq_words:
    for char in word[:-1]:
        uniq_chars_internal.add(char)
    uniq_chars_final.add(word[-1])","['for word in uniq_words:\n    (*word_rwordmaining, word_nwordg_1) = word\n    for char in word[:-1]:\n        uniq_chars_internal.add(char)\n    uniq_chars_final.add(word_nwordg_1)']",no_found,0
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2017/nn/part2.py,https://github.com/3b1b/videos/tree/master/_2017/nn/part2.py,EmphasizeComplexityOfCostFunction,show_network_as_a_function$1233,"def show_network_as_a_function(self):
    title = TexText('Neural network function')
    title.shift(FRAME_X_RADIUS * RIGHT / 2)
    title.to_edge(UP)
    underline = Line(LEFT, RIGHT)
    underline.stretch_to_fit_width(title.get_width())
    underline.next_to(title, DOWN, SMALL_BUFF)
    self.add(title, underline)
    words = self.get_function_description_words('784 numbers (pixels)', '10 numbers', '13{,}002 weights/biases')
    (input_words, output_words, parameter_words) = words
    for word in words:
        self.add(word[0])
    in_vect = get_organized_images()[7][8]
    activations = self.network.get_activation_of_all_layers(in_vect)
    image = MNistMobject(in_vect)
    image.set_height(1.5)
    image_label = TexText('Input')
    image_label.set_color(input_words[0].get_color())
    image_label.next_to(image, UP, SMALL_BUFF)
    arrow = Arrow(LEFT, RIGHT, color=WHITE)
    arrow.next_to(image, RIGHT)
    output = self.num_vect_to_column_vector(activations[-1], 2)
    output.next_to(arrow, RIGHT)
    group = Group(image, image_label, arrow, output)
    group.next_to(self.network_mob, UP, 0, RIGHT)
    dot = Dot()
    dot.move_to(input_words.get_right())
    dot.set_fill(opacity=0.5)
    self.play(FadeIn(input_words[1], lag_ratio=0.5))
    self.play(dot.move_to, image, dot.set_fill, None, 0, FadeIn(image), FadeIn(image_label))
    self.activate_network(in_vect, GrowArrow(arrow), FadeIn(output), FadeIn(output_words[1]))
    self.wait()
    self.play(FadeIn(parameter_words[1]), self.get_edge_animation())
    self.wait(2)
    self.to_fade = group
    self.curr_words = words
    self.title = title
    self.underline = underline","for word in words:
    self.add(word[0])","['for word in words:\n    (word_0, *word_rwordmaining) = word\n    self.add(word_0)', 'for (word_0, *word_len) in words:\n    self.add(word_0)']",no_found,0
WordOps,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WordOps/wo/cli/plugins/site_functions.py,https://github.com/WordOps/WordOps/tree/master/wo/cli/plugins/site_functions.py,,setupwordpress$243,"def setupwordpress(self, data, vhostonly=False):
    wo_domain_name = data['site_name']
    wo_site_webroot = data['webroot']
    if self.app.config.has_section('wordpress'):
        prompt_wpprefix = self.app.config.get('wordpress', 'prefix')
        wo_wp_user = self.app.config.get('wordpress', 'user')
        wo_wp_pass = self.app.config.get('wordpress', 'password')
        wo_wp_email = self.app.config.get('wordpress', 'email')
    else:
        prompt_wpprefix = False
        wo_wp_user = ''
        wo_wp_pass = ''
        wo_wp_email = ''
    wo_random_pass = ''.join(random.sample(string.ascii_uppercase + string.ascii_lowercase + string.digits, 24))
    wo_wp_prefix = ''
    if 'wp-user' in data.keys() and data['wp-user']:
        wo_wp_user = data['wp-user']
    if 'wp-email' in data.keys() and data['wp-email']:
        wo_wp_email = data['wp-email']
    if 'wp-pass' in data.keys() and data['wp-pass']:
        wo_wp_pass = data['wp-pass']
    Log.info(self, 'Downloading WordPress \t\t', end='')
    WOFileUtils.chdir(self, '{0}/htdocs/'.format(wo_site_webroot))
    try:
        if WOShellExec.cmd_exec(self, 'wp --allow-root core download'):
            pass
        else:
            Log.info(self, '[' + Log.ENDC + Log.FAIL + 'Fail' + Log.OKBLUE + ']')
            raise SiteError('download WordPress core failed')
    except CommandExecutionError:
        Log.info(self, '[' + Log.ENDC + Log.FAIL + 'Fail' + Log.OKBLUE + ']')
        raise SiteError('download WordPress core failed')
    Log.info(self, '[' + Log.ENDC + 'Done' + Log.OKBLUE + ']')
    if not (data['wo_db_name'] and data['wo_db_user'] and data['wo_db_pass']):
        data = setupdatabase(self, data)
    if prompt_wpprefix == 'True' or prompt_wpprefix == 'true':
        try:
            wo_wp_prefix = input('Enter the WordPress table prefix [wp_]: ')
            while not re.match('^[A-Za-z0-9_]*$', wo_wp_prefix):
                Log.warn(self, 'table prefix can only contain numbers, letters, and underscores')
                wo_wp_prefix = input('Enter the WordPress table prefix [wp_]: ')
        except EOFError:
            raise SiteError('input table prefix failed')
    if not wo_wp_prefix:
        wo_wp_prefix = 'wp_'
    WOFileUtils.chdir(self, '{0}/htdocs/'.format(wo_site_webroot))
    Log.debug(self, 'Setting up wp-config file')
    if not data['multisite']:
        Log.debug(self, 'Generating wp-config for WordPress Single site')
        Log.debug(self, '/bin/bash -c ""{0} --allow-root '.format(WOVar.wo_wpcli_path) + 'config create ' + ""--dbname='{0}' --dbprefix='{1}' --dbuser='{2}' --dbhost='{3}' "".format(data['wo_db_name'], wo_wp_prefix, data['wo_db_user'], data['wo_db_host']) + '--dbpass= --extra-php<<PHP \n {0}\nPHP""'.format(""\n\ndefine('WP_DEBUG', false);""))
        try:
            if WOShellExec.cmd_exec(self, '/bin/bash -c ""{0} --allow-root'.format(WOVar.wo_wpcli_path) + ' config create ' + ""--dbname='{0}' --dbprefix='{1}' --dbuser='{2}' --dbhost='{3}' "".format(data['wo_db_name'], wo_wp_prefix, data['wo_db_user'], data['wo_db_host']) + '--dbpass=\'{0}\'""'.format(data['wo_db_pass']), log=False):
                pass
            else:
                raise SiteError('generate wp-config failed for wp single site')
        except CommandExecutionError:
            raise SiteError('generate wp-config failed for wp single site')
    else:
        Log.debug(self, 'Generating wp-config for WordPress multisite')
        Log.debug(self, '/bin/bash -c ""{0} --allow-root '.format(WOVar.wo_wpcli_path) + 'config create ' + ""--dbname='{0}' --dbprefix='{1}' --dbhost='{2}' "".format(data['wo_db_name'], wo_wp_prefix, data['wo_db_host']) + '--dbuser=\'{0}\' --dbpass= --extra-php<<PHP \n {1} {2} \nPHP""'.format(data['wo_db_user'], ""\ndefine('WPMU_ACCEL_REDIRECT', true);"", ""\ndefine('CONCATENATE_SCRIPTS', false);""))
        try:
            if WOShellExec.cmd_exec(self, '/bin/bash -c ""{0} --allow-root'.format(WOVar.wo_wpcli_path) + ' config create ' + ""--dbname='{0}' --dbprefix='{1}' --dbhost='{2}' "".format(data['wo_db_name'], wo_wp_prefix, data['wo_db_host']) + '--dbuser=\'{0}\' --dbpass=\'{1}\' --extra-php<<PHP \n \n{2} \nPHP""'.format(data['wo_db_user'], data['wo_db_pass'], ""\ndefine('WPMU_ACCEL_REDIRECT', true);""), log=False):
                pass
            else:
                raise SiteError('generate wp-config failed for wp multi site')
        except CommandExecutionError:
            raise SiteError('generate wp-config failed for wp multi site')
    wp_conf_variables = [['WP_REDIS_PREFIX', '{0}:'.format(wo_domain_name)], ['WP_MEMORY_LIMIT', '128M'], ['WP_MAX_MEMORY_LIMIT', '256M'], ['CONCATENATE_SCRIPTS', 'false'], ['WP_POST_REVISIONS', '10'], ['MEDIA_TRASH', 'true'], ['EMPTY_TRASH_DAYS', '15'], ['WP_AUTO_UPDATE_CORE', 'minor'], ['WP_REDIS_DISABLE_BANNERS', 'true']]
    Log.wait(self, 'Configuring WordPress')
    for wp_conf in wp_conf_variables:
        wp_var = wp_conf[0]
        wp_val = wp_conf[1]
        var_raw = bool(wp_val == 'true' or wp_val == 'false')
        try:
            WOShellExec.cmd_exec(self, '/bin/bash -c ""{0} --allow-root '.format(WOVar.wo_wpcli_path) + 'config set {0} \'{1}\' {wp_raw}""'.format(wp_var, wp_val, wp_raw='--raw' if var_raw is True else ''))
        except CommandExecutionError as e:
            Log.failed(self, 'Configuring WordPress')
            Log.debug(self, str(e))
            Log.error(self, 'Unable to define wp-config.php variables')
    Log.valide(self, 'Configuring WordPress')
    try:
        Log.debug(self, 'Moving file from {0} to {1}'.format(os.getcwd() + '/wp-config.php', os.path.abspath(os.path.join(os.getcwd(), os.pardir))))
        shutil.move(os.getcwd() + '/wp-config.php', os.path.abspath(os.path.join(os.getcwd(), os.pardir)))
    except Exception as e:
        Log.debug(self, str(e))
        Log.error(self, 'Unable to move file from {0} to {1}'.format(os.getcwd() + '/wp-config.php', os.path.abspath(os.path.join(os.getcwd(), os.pardir))), False)
        raise SiteError('Unable to move wp-config.php')
    if not wo_wp_user:
        wo_wp_user = WOVar.wo_user
        while not wo_wp_user:
            Log.warn(self, 'Username can have only alphanumericcharacters, spaces, underscores, hyphens,periods and the @ symbol.')
            try:
                wo_wp_user = input('Enter WordPress username: ')
            except EOFError:
                raise SiteError('input WordPress username failed')
    if not wo_wp_pass:
        wo_wp_pass = wo_random_pass
    if not wo_wp_email:
        wo_wp_email = WOVar.wo_email
        while not wo_wp_email:
            try:
                wo_wp_email = input('Enter WordPress email: ')
            except EOFError:
                raise SiteError('input WordPress username failed')
    try:
        while not re.match('^[A-Za-z0-9\\.\\+_-]+@[A-Za-z0-9\\._-]+\\.[a-zA-Z]*$', wo_wp_email):
            Log.info(self, 'EMail not Valid in config, Please provide valid email id')
            wo_wp_email = input('Enter your email: ')
    except EOFError:
        raise SiteError('input WordPress user email failed')
    Log.debug(self, 'Setting up WordPress tables')
    Log.wait(self, 'Installing WordPress')
    if not data['multisite']:
        Log.debug(self, 'Creating tables for WordPress Single site')
        Log.debug(self, '{0} --allow-root core install '.format(WOVar.wo_wpcli_path) + ""--url='{0}' --title='{0}' --admin_name='{1}' "".format(data['site_name'], wo_wp_user) + ""--admin_password= --admin_email='{0}'"".format(wo_wp_email))
        try:
            if WOShellExec.cmd_exec(self, '{0} --allow-root core '.format(WOVar.wo_wpcli_path) + ""install --url='{0}' --title='{0}' --admin_name='{1}' "".format(data['site_name'], wo_wp_user) + ""--admin_password='{0}' --admin_email='{1}'"".format(wo_wp_pass, wo_wp_email), log=False):
                pass
            else:
                Log.failed(self, 'Installing WordPress')
                raise SiteError('setup WordPress tables failed for single site')
        except CommandExecutionError:
            raise SiteError('setup WordPress tables failed for single site')
    else:
        Log.debug(self, 'Creating tables for WordPress multisite')
        Log.debug(self, '{0} --allow-root '.format(WOVar.wo_wpcli_path) + ""core multisite-install --url='{0}' --title='{0}' --admin_name='{1}' "".format(data['site_name'], wo_wp_user) + ""--admin_password= --admin_email='{0}' {subdomains}"".format(wo_wp_email, subdomains='--subdomains' if not data['wpsubdir'] else ''))
        try:
            if WOShellExec.cmd_exec(self, '{0} --allow-root '.format(WOVar.wo_wpcli_path) + ""core multisite-install --url='{0}' --title='{0}' --admin_name='{1}' "".format(data['site_name'], wo_wp_user) + ""--admin_password='{0}' --admin_email='{1}' {subdomains}"".format(wo_wp_pass, wo_wp_email, subdomains='--subdomains' if not data['wpsubdir'] else ''), log=False):
                pass
            else:
                Log.failed(self, 'Installing WordPress')
                raise SiteError('setup WordPress tables failed for wp multi site')
        except CommandExecutionError:
            raise SiteError('setup WordPress tables failed for wp multi site')
    Log.valide(self, 'Installing WordPress')
    Log.debug(self, 'Updating WordPress permalink')
    try:
        WOShellExec.cmd_exec(self, ' {0} --allow-root '.format(WOVar.wo_wpcli_path) + 'rewrite structure /%postname%/')
    except CommandExecutionError as e:
        Log.debug(self, str(e))
        raise SiteError('Update wordpress permalinks failed')
    'Install nginx-helper plugin '
    installwp_plugin(self, 'nginx-helper', data)
    if data['wpfc']:
        plugin_data_object = {'log_level': 'INFO', 'log_filesize': 5, 'enable_purge': 1, 'enable_map': '0', 'enable_log': 0, 'enable_stamp': 1, 'purge_homepage_on_new': 1, 'purge_homepage_on_edit': 1, 'purge_homepage_on_del': 1, 'purge_archive_on_new': 1, 'purge_archive_on_edit': 1, 'purge_archive_on_del': 1, 'purge_archive_on_new_comment': 0, 'purge_archive_on_deleted_comment': 0, 'purge_page_on_mod': 1, 'purge_page_on_new_comment': 1, 'purge_page_on_deleted_comment': 1, 'cache_method': 'enable_fastcgi', 'purge_method': 'get_request', 'redis_hostname': '127.0.0.1', 'redis_port': '6379', 'redis_prefix': 'nginx-cache:'}
        plugin_data = json.dumps(plugin_data_object)
        setupwp_plugin(self, 'nginx-helper', 'rt_wp_nginx_helper_options', plugin_data, data)
    elif data['wpredis']:
        plugin_data_object = {'log_level': 'INFO', 'log_filesize': 5, 'enable_purge': 1, 'enable_map': '0', 'enable_log': 0, 'enable_stamp': 1, 'purge_homepage_on_new': 1, 'purge_homepage_on_edit': 1, 'purge_homepage_on_del': 1, 'purge_archive_on_new': 1, 'purge_archive_on_edit': 1, 'purge_archive_on_del': 1, 'purge_archive_on_new_comment': 0, 'purge_archive_on_deleted_comment': 0, 'purge_page_on_mod': 1, 'purge_page_on_new_comment': 1, 'purge_page_on_deleted_comment': 1, 'cache_method': 'enable_redis', 'purge_method': 'get_request', 'redis_hostname': '127.0.0.1', 'redis_port': '6379', 'redis_prefix': 'nginx-cache:'}
        plugin_data = json.dumps(plugin_data_object)
        setupwp_plugin(self, 'nginx-helper', 'rt_wp_nginx_helper_options', plugin_data, data)
    'Install Wp Super Cache'
    if data['wpsc']:
        installwp_plugin(self, 'wp-super-cache', data)
    'Install Redis Cache'
    if data['wpredis']:
        installwp_plugin(self, 'redis-cache', data)
    'Install Cache-Enabler'
    if data['wpce']:
        installwp_plugin(self, 'cache-enabler', data)
        plugin_data_object = {'expires': 24, 'new_post': 1, 'new_comment': 0, 'webp': 0, 'clear_on_upgrade': 1, 'compress': 0, 'excl_ids': '', 'excl_regexp': '', 'excl_cookies': '', 'incl_attributes': '', 'minify_html': 1}
        plugin_data = json.dumps(plugin_data_object)
        setupwp_plugin(self, 'cache-enabler', 'cache-enabler', plugin_data, data)
        WOShellExec.cmd_exec(self, '/bin/bash -c ""{0} --allow-root '.format(WOVar.wo_wpcli_path) + 'config set WP_CACHE true --raw""')
    if vhostonly:
        try:
            WOShellExec.cmd_exec(self, '/bin/bash -c ""{0} --allow-root '.format(WOVar.wo_wpcli_path) + 'db clean --yes""')
            WOFileUtils.chdir(self, '{0}'.format(wo_site_webroot))
            WOFileUtils.rm(self, '{0}/htdocs'.format(wo_site_webroot))
            WOFileUtils.mkdir(self, '{0}/htdocs'.format(wo_site_webroot))
            WOFileUtils.chown(self, '{0}/htdocs'.format(wo_site_webroot), 'www-data', 'www-data')
        except CommandExecutionError:
            raise SiteError('Cleaning WordPress install failed')
    wp_creds = dict(wp_user=wo_wp_user, wp_pass=wo_wp_pass, wp_email=wo_wp_email)
    return wp_creds","for wp_conf in wp_conf_variables:
    wp_var = wp_conf[0]
    wp_val = wp_conf[1]
    var_raw = bool(wp_val == 'true' or wp_val == 'false')
    try:
        WOShellExec.cmd_exec(self, '/bin/bash -c ""{0} --allow-root '.format(WOVar.wo_wpcli_path) + 'config set {0} \'{1}\' {wp_raw}""'.format(wp_var, wp_val, wp_raw='--raw' if var_raw is True else ''))
    except CommandExecutionError as e:
        Log.failed(self, 'Configuring WordPress')
        Log.debug(self, str(e))
        Log.error(self, 'Unable to define wp-config.php variables')","['for wp_conf in wp_conf_variables:\n    (wp_conf_0, wp_conf_1, *_) = wp_conf\n    wp_var = wp_conf_0\n    wp_val = wp_conf_1\n    var_raw = bool(wp_val == \'true\' or wp_val == \'false\')\n    try:\n        WOShellExec.cmd_exec(self, \'/bin/bash -c ""{0} --allow-root \'.format(WOVar.wo_wpcli_path) + \'config set {0} \\\'{1}\\\' {wp_raw}""\'.format(wp_var, wp_val, wp_raw=\'--raw\' if var_raw is True else \'\'))\n    except CommandExecutionError as e:\n        Log.failed(self, \'Configuring WordPress\')\n        Log.debug(self, str(e))\n        Log.error(self, \'Unable to define wp-config.php variables\')', 'for (wp_conf_0, wp_conf_1, *wp_conf_len) in wp_conf_variables:\n    wp_var = \n    wp_conf_0\n    wp_val = \n    wp_conf_1\n    var_raw = bool(wp_val == \'true\' or wp_val == \'false\')\n    try:\n        WOShellExec.cmd_exec(self, \'/bin/bash -c ""{0} --allow-root \'.format(WOVar.wo_wpcli_path) + \'config set {0} \\\'{1}\\\' {wp_raw}""\'.format(wp_var, wp_val, wp_raw=\'--raw\' if var_raw is True else \'\'))\n    except CommandExecutionError as e:\n        Log.failed(self, \'Configuring WordPress\')\n        Log.debug(self, str(e))\n        Log.error(self, \'Unable to define wp-config.php variables\')']",no_found,0
Synonyms,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Synonyms/synonyms/synonyms.py,https://github.com/chatopera/Synonyms/tree/master/synonyms/synonyms.py,,nearby$318,"def nearby(word, size=10):
    """"""
    Nearby word
    """"""
    w = any2unicode(word)
    wk = w + '-' + str(size)
    if wk in _cache_nearby:
        return _cache_nearby[wk]
    (words, scores) = ([], [])
    try:
        for x in _vectors.neighbours(w, size):
            words.append(x[0])
            scores.append(x[1])
    except:
        pass
    _cache_nearby[wk] = (words, scores)
    return (words, scores)","for x in _vectors.neighbours(w, size):
    words.append(x[0])
    scores.append(x[1])","['for x in _vectors.neighbours(w, size):\n    (x_0, x_1, *_) = x\n    words.append(x_0)\n    scores.append(x_1)', 'for (x_0, x_1, *x_len) in _vectors.neighbours(w, size):\n    words.append(x_0)\n    scores.append(x_1)']",no_found,0
dynaconf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dynaconf/dynaconf/vendor_src/ruamel/yaml/resolver.py,https://github.com/rochacbruno/dynaconf/tree/master/dynaconf/vendor_src/ruamel/yaml/resolver.py,VersionedResolver,versioned_resolver$345,"def versioned_resolver(self):
    """"""
        select the resolver based on the version we are parsing
        """"""
    version = self.processing_version
    if version not in self._version_implicit_resolver:
        for x in implicit_resolvers:
            if version in x[0]:
                self.add_version_implicit_resolver(version, x[1], x[2], x[3])
    return self._version_implicit_resolver[version]","for x in implicit_resolvers:
    if version in x[0]:
        self.add_version_implicit_resolver(version, x[1], x[2], x[3])","['for x in implicit_resolvers:\n    (x_0, x_1, x_2, x_3, *_) = x\n    if version in x_0:\n        self.add_version_implicit_resolver(version, x_1, x_2, x_3)', 'for (x_0, x_1, x_2, x_3, *x_len) in implicit_resolvers:\n    if version in \n    x_0:\n        self.add_version_implicit_resolver(version, \n        x_1, \n        x_2, \n        x_3)']",no_found,0
clearml,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clearml/clearml/utilities/gpu/pynvml.py,https://github.com/allegroai/clearml/tree/master/clearml/utilities/gpu/pynvml.py,,nvmlStructToFriendlyObject$754,"def nvmlStructToFriendlyObject(struct):
    d = {}
    for x in struct._fields_:
        key = x[0]
        value = getattr(struct, key)
        d[key] = value
    obj = nvmlFriendlyObject(d)
    return obj","for x in struct._fields_:
    key = x[0]
    value = getattr(struct, key)
    d[key] = value","['for x in struct._fields_:\n    (x_0, *x_rxmaining) = x\n    key = x_0\n    value = getattr(struct, key)\n    d[key] = value', 'for (x_0, *x_len) in struct._fields_:\n    key = \n    x_0\n    value = getattr(struct, key)\n    d[key] = value']",no_found,0
Source-Code-from-Tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Source-Code-from-Tutorials/Pygame/24_PythonGameDevelopment.py,https://github.com/buckyroberts/Source-Code-from-Tutorials/tree/master/Pygame/24_PythonGameDevelopment.py,,snake$27,"def snake(block_size, snakelist):
    for XnY in snakelist:
        pygame.draw.rect(gameDisplay, green, [XnY[0], XnY[1], block_size, block_size])","for XnY in snakelist:
    pygame.draw.rect(gameDisplay, green, [XnY[0], XnY[1], block_size, block_size])","['for XnY in snakelist:\n    (XnY_0, XnY_1, *_) = XnY\n    pygame.draw.rect(gameDisplay, green, [XnY_0, XnY_1, block_size, block_size])', 'for (XnY_0, XnY_1, *XnY_len) in snakelist:\n    pygame.draw.rect(gameDisplay, green, [XnY_0, XnY_1, block_size, block_size])']",no_found,0
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/extra/tunings.py,https://github.com/bspaans/python-mingus/tree/master/mingus/extra/tunings.py,StringTuning,find_chord_fingering$149,"def find_chord_fingering(self, notes, max_distance=4, maxfret=18, max_fingers=4, return_best_as_NoteContainer=False):
    """"""Return a list of fret lists that are considered possible fingerings.

        This function only looks at and matches on the note _names_ so it
        does more than find_fingering.

        Example:
        >>> t = get_tuning('guitar', 'standard', 6, 1)
        >>> t.find_chord_fingering(NoteContainer().from_chord('Am'))
        [[0, 0, 2, 2, 1, 0], [0, 3, 2, 2, 1, 0], ......]
        """"""

    def follow(string, next, name, prev=-1):
        """"""Follow the fret 'next' on 'string'; build result on the way.""""""
        if string >= len(self.tuning) - 1:
            return [[(next, name)]]
        result = []
        cur = res[string][next]
        if cur != []:
            for y in cur[1]:
                for sub in follow(string + 1, y[0], y[1]):
                    if prev < 0:
                        result.append([(next, name)] + sub)
                    elif sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
                        result.append([(next, name)] + sub)
        for s in follow(string + 1, maxfret + 1, None, next):
            result.append([(next, name)] + s)
        return [[(next, name)]] if result == [] else result

    def make_lookup_table():
        """"""Prepare the lookup table.

            table[string][fret] = (name, dest_frets)
            """"""
        res = [[[] for x in range(maxfret + 2)] for x in range(len(self.tuning) - 1)]
        for x in range(0, len(self.tuning) - 1):
            addedNone = -1
            next = fretdict[x + 1]
            for (fret, name) in fretdict[x]:
                for (f2, n2) in next:
                    if n2 != name and (f2 == 0 or abs(fret - f2) < max_distance):
                        if res[x][fret] != []:
                            res[x][fret][1].append((f2, n2))
                        else:
                            res[x][fret] = (name, [(f2, n2)])
                    if addedNone < x:
                        if res[x][maxfret + 1] != []:
                            res[x][maxfret + 1][1].append((f2, n2))
                        else:
                            res[x][maxfret + 1] = (None, [(f2, n2)])
                addedNone = x
        return res
    n = notes
    if notes != [] and isinstance(notes, list) and isinstance(notes[0], six.string_types):
        n = NoteContainer(notes)
    notenames = [x.name for x in n]
    if len(notenames) == 0 or len(notenames) > len(self.tuning):
        return []
    fretdict = []
    for x in range(0, len(self.tuning)):
        fretdict.append(self.find_note_names(notes, x, maxfret))
    res = make_lookup_table()
    result = []
    for (i, y) in enumerate(res[0]):
        if y != []:
            (yname, next) = (y[0], y[1])
            for (fret, name) in next:
                for s in follow(1, fret, name):
                    subresult = [(i, yname)] + s
                    (mi, ma, names) = (1000, -1000, [])
                    for (f, n) in subresult:
                        if n is not None:
                            if f != 0 and f <= mi:
                                mi = f
                            if f != 0 and f >= ma:
                                ma = f
                            names.append(n)
                    if abs(ma - mi) < max_distance:
                        covered = True
                        for n in notenames:
                            if n not in names:
                                covered = False
                        if covered and names != []:
                            result.append([y[0] if y[1] is not None else y[1] for y in subresult])
    s = sorted(result, key=lambda x: sum([t if t is not None else 1000 for (i, t) in enumerate(x)]))
    s = [a for a in s if fingers_needed(a) <= max_fingers]
    if not return_best_as_NoteContainer:
        return s
    else:
        rnotes = self.frets_to_NoteContainer(s[0])
        for (i, x) in enumerate(rnotes):
            if x.string < len(self.tuning) - 1:
                if res[x.string][x.fret] != []:
                    rnotes[i].name = res[x.string][x.fret][0]
        return rnotes","for y in cur[1]:
    for sub in follow(string + 1, y[0], y[1]):
        if prev < 0:
            result.append([(next, name)] + sub)
        elif sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
            result.append([(next, name)] + sub)","['for y in cur[1]:\n    (y_0, y_1, *_) = y\n    for sub in follow(string + 1, y_0, y_1):\n        if prev < 0:\n            result.append([(next, name)] + sub)\n        elif sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:\n            result.append([(next, name)] + sub)', 'for (y_0, y_1, *y_len) in cur[1]:\n    for sub in follow(string + 1, y_0, y_1):\n        if prev < 0:\n            result.append([(next, name)] + sub)\n        elif sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:\n            result.append([(next, name)] + sub)']",no_found,0
quodlibet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quodlibet/quodlibet/ext/songsmenu/duplicates.py,https://github.com/quodlibet/quodlibet/tree/master/quodlibet/ext/songsmenu/duplicates.py,DuplicateSongsView,get_selected_songs$36,"def get_selected_songs(self):
    selection = self.get_selection()
    if selection is None:
        return []
    (model, rows) = selection.get_selected_rows()
    if not rows:
        return []
    selected = []
    for row in rows:
        row = model[row]
        if row.parent is None:
            for child in row.iterchildren():
                selected.append(child[0])
        else:
            selected.append(row[0])
    return selected","for row in rows:
    row = model[row]
    if row.parent is None:
        for child in row.iterchildren():
            selected.append(child[0])
    else:
        selected.append(row[0])",['it can be refactored'],no_found,0
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/extra/tunings.py,https://github.com/bspaans/python-mingus/tree/master/mingus/extra/tunings.py,StringTuning,find_chord_fingering$149,"def find_chord_fingering(self, notes, max_distance=4, maxfret=18, max_fingers=4, return_best_as_NoteContainer=False):
    """"""Return a list of fret lists that are considered possible fingerings.

        This function only looks at and matches on the note _names_ so it
        does more than find_fingering.

        Example:
        >>> t = get_tuning('guitar', 'standard', 6, 1)
        >>> t.find_chord_fingering(NoteContainer().from_chord('Am'))
        [[0, 0, 2, 2, 1, 0], [0, 3, 2, 2, 1, 0], ......]
        """"""

    def follow(string, next, name, prev=-1):
        """"""Follow the fret 'next' on 'string'; build result on the way.""""""
        if string >= len(self.tuning) - 1:
            return [[(next, name)]]
        result = []
        cur = res[string][next]
        if cur != []:
            for y in cur[1]:
                for sub in follow(string + 1, y[0], y[1]):
                    if prev < 0:
                        result.append([(next, name)] + sub)
                    elif sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
                        result.append([(next, name)] + sub)
        for s in follow(string + 1, maxfret + 1, None, next):
            result.append([(next, name)] + s)
        return [[(next, name)]] if result == [] else result

    def make_lookup_table():
        """"""Prepare the lookup table.

            table[string][fret] = (name, dest_frets)
            """"""
        res = [[[] for x in range(maxfret + 2)] for x in range(len(self.tuning) - 1)]
        for x in range(0, len(self.tuning) - 1):
            addedNone = -1
            next = fretdict[x + 1]
            for (fret, name) in fretdict[x]:
                for (f2, n2) in next:
                    if n2 != name and (f2 == 0 or abs(fret - f2) < max_distance):
                        if res[x][fret] != []:
                            res[x][fret][1].append((f2, n2))
                        else:
                            res[x][fret] = (name, [(f2, n2)])
                    if addedNone < x:
                        if res[x][maxfret + 1] != []:
                            res[x][maxfret + 1][1].append((f2, n2))
                        else:
                            res[x][maxfret + 1] = (None, [(f2, n2)])
                addedNone = x
        return res
    n = notes
    if notes != [] and isinstance(notes, list) and isinstance(notes[0], six.string_types):
        n = NoteContainer(notes)
    notenames = [x.name for x in n]
    if len(notenames) == 0 or len(notenames) > len(self.tuning):
        return []
    fretdict = []
    for x in range(0, len(self.tuning)):
        fretdict.append(self.find_note_names(notes, x, maxfret))
    res = make_lookup_table()
    result = []
    for (i, y) in enumerate(res[0]):
        if y != []:
            (yname, next) = (y[0], y[1])
            for (fret, name) in next:
                for s in follow(1, fret, name):
                    subresult = [(i, yname)] + s
                    (mi, ma, names) = (1000, -1000, [])
                    for (f, n) in subresult:
                        if n is not None:
                            if f != 0 and f <= mi:
                                mi = f
                            if f != 0 and f >= ma:
                                ma = f
                            names.append(n)
                    if abs(ma - mi) < max_distance:
                        covered = True
                        for n in notenames:
                            if n not in names:
                                covered = False
                        if covered and names != []:
                            result.append([y[0] if y[1] is not None else y[1] for y in subresult])
    s = sorted(result, key=lambda x: sum([t if t is not None else 1000 for (i, t) in enumerate(x)]))
    s = [a for a in s if fingers_needed(a) <= max_fingers]
    if not return_best_as_NoteContainer:
        return s
    else:
        rnotes = self.frets_to_NoteContainer(s[0])
        for (i, x) in enumerate(rnotes):
            if x.string < len(self.tuning) - 1:
                if res[x.string][x.fret] != []:
                    rnotes[i].name = res[x.string][x.fret][0]
        return rnotes","for sub in follow(string + 1, y[0], y[1]):
    if prev < 0:
        result.append([(next, name)] + sub)
    elif sub[0][0] == 0 or abs(sub[0][0] - prev) < max_distance:
        result.append([(next, name)] + sub)",['it can be refactored'],no_found,0
congress-legislators,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/congress-legislators/scripts/icpsr_ids.py,https://github.com/unitedstates/congress-legislators/tree/master/scripts/icpsr_ids.py,,run$15,"def run():
    cache = utils.flags().get('cache', True)
    force = not cache
    only_bioguide = utils.flags().get('bioguide', None)
    congress = utils.flags().get('congress', None)
    data_files = []
    print('Loading %s...' % 'legislators-current.yaml')
    legislators = load_data('legislators-current.yaml')
    data_files.append((legislators, 'legislators-current.yaml'))
    print('Loading %s...' % 'legislators-historical.yaml')
    legislators = load_data('legislators-historical.yaml')
    data_files.append((legislators, 'legislators-historical.yaml'))
    if congress == None:
        raise Exception('the --congress flag is required')
    elif int(congress) < 10 and int(congress) > 0:
        url_senate = 'https://voteview.com/static/data/out/members/S00%s_members.csv' % congress
        url_house = 'https://voteview.com/static/data/out/members/H00%s_members.csv' % congress
    elif int(congress) < 100 and int(congress) >= 10:
        url_senate = 'https://voteview.com/static/data/out/members/S0%s_members.csv' % congress
        url_house = 'https://voteview.com/static/data/out/members/H0%s_members.csv' % congress
    elif int(congress) >= 100:
        url_senate = 'https://voteview.com/static/data/out/members/S%s_members.csv' % congress
        url_house = 'https://voteview.com/static/data/out/members/H%s_members.csv' % congress
    else:
        raise Exception('no data for congress ' + congress)
    senate_destination = 'icpsr/source/senate_rollcall%s.txt' % congress
    senate_data = utils.download(url_senate, senate_destination, force)
    house_destination = 'icpsr/source/house_rollcall%s.txt' % congress
    house_data = utils.download(url_house, house_destination, force)
    error_log = csv.writer(open('cache/errors/mismatch/mismatch_%s.csv' % congress, 'w'))
    error_log.writerow(['error_type', 'matches', 'icpsr_name', 'icpsr_state', 'is_territory', 'old_id', 'new_id'])
    read_files = [('sen', senate_data), ('rep', house_data)]
    print('Running for congress ' + congress)
    for (read_file_chamber, read_file_content) in read_files:
        for data_file in data_files:
            for legislator in data_file[0]:
                num_matches = 0
                write_id = ''
                bioguide = legislator['id'].get('bioguide', None)
                if only_bioguide and bioguide != only_bioguide:
                    continue
                chamber = legislator['terms'][len(legislator['terms']) - 1]['type']
                if chamber != read_file_chamber:
                    continue
                latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms']) - 1]['start'])))
                if chamber == 'sen':
                    congresses = [latest_congress, latest_congress + 1, latest_congress + 2]
                else:
                    congresses = [latest_congress]
                if int(congress) not in congresses:
                    continue
                last_name = legislator['name']['last'].upper()
                state = utils.states[legislator['terms'][len(legislator['terms']) - 1]['state']].upper()[:7].strip()
                content_as_file = StringIO(read_file_content)
                content_parsed = csv.reader(content_as_file, delimiter=',')
                for icpsr_member in content_parsed:
                    if bioguide == icpsr_member[10]:
                        num_matches += 1
                        write_id = int(icpsr_member[2])
                if 'icpsr' in legislator['id']:
                    if write_id == legislator['id']['icpsr'] or write_id == '':
                        continue
                    elif write_id != legislator['id']['icpsr'] and write_id != '':
                        error_log.writerow(['Incorrect_ID', 'NA', last_name[:8], state, 'NA', legislator['id']['icpsr'], write_id])
                        print('ID updated for %s' % last_name)
                if num_matches == 1:
                    legislator['id']['icpsr'] = int(write_id)
                elif state == 'GUAM' or state == 'PUERTO' or state == 'VIRGIN' or (state == 'DISTRIC') or (state == 'AMERICA') or (state == 'NORTHER') or (state == 'PHILIPP'):
                    print('error: non 1 match')
                    error_log.writerow(['Non_1_match_number', str(num_matches), last_name[:8], state, 'Y', 'NA', 'NA'])
                else:
                    print(str(num_matches) + ' matches found for ' + last_name[:8] + ', ' + state + ' in congress ' + str(congress))
                    error_log.writerow(['Non_1_match_number', str(num_matches), last_name, state, 'N', 'NA', 'NA'])
            save_data(data_file[0], data_file[1])","for icpsr_member in content_parsed:
    if bioguide == icpsr_member[10]:
        num_matches += 1
        write_id = int(icpsr_member[2])",['it can be refactored'],no_found,0
ros_comm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ros_comm/test/test_rosmaster/test/testSlave.py,https://github.com/ros/ros_comm/tree/master/test/test_rosmaster/test/testSlave.py,SlaveTestCase,_sink_StartNodes$243,"def _sink_StartNodes(self, tests):
    """"""
        Test subroutine to startup all the nodes specified in the tests
        """"""
    master = self.master
    sourceUris = {}
    sinks = {}
    (pkg, node) = testNode
    for test in tests:
        (sourceName, sinkName) = test[0]
        sourcePort = apiSuccess(master.addNode('', '', sourceName, pkg, node, TEST_MACHINE, 0))
        sinkPort = apiSuccess(master.addNode('', '', sinkName, pkg, node, TEST_MACHINE, 0))
        sourceUri = 'http://%s:%s/' % (testNodeAddr[0], sourcePort)
        sinkUri = 'http://%s:%s/' % (testNodeAddr[0], sinkPort)
        sourceUris[sourceName] = sourceUri
        sinks[sinkName] = ServerProxy(sinkUri)
    return (sourceUris, sinks)","(pkg, node) = testNode
for test in tests:
    (sourceName, sinkName) = test[0]
    sourcePort = apiSuccess(master.addNode('', '', sourceName, pkg, node, TEST_MACHINE, 0))
    sinkPort = apiSuccess(master.addNode('', '', sinkName, pkg, node, TEST_MACHINE, 0))
    sourceUri = 'http://%s:%s/' % (testNodeAddr[0], sourcePort)
    sinkUri = 'http://%s:%s/' % (testNodeAddr[0], sinkPort)
    sourceUris[sourceName] = sourceUri
    sinks[sinkName] = ServerProxy(sinkUri)","(pkg, node) = testNode
for test in tests:
    (sourceName, sinkName) = test[0]
    sourcePort = apiSuccess(master.addNode('', '', sourceName, pkg, node, TEST_MACHINE, 0))
    sinkPort = apiSuccess(master.addNode('', '', sinkName, pkg, node, TEST_MACHINE, 0))
    sourceUri = f'http://{testNodeAddr[0]}:{sourcePort}/'
    sinkUri = f'http://{testNodeAddr[0]}:{sinkPort}/'
    sourceUris[sourceName] = sourceUri
    sinks[sinkName] = ServerProxy(sinkUri)",find_wrong,1
DeepKE,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepKE/src/deepke/name_entity_re/few_shot/module/datasets.py,https://github.com/zjunlp/DeepKE/tree/master/src/deepke/name_entity_re/few_shot/module/datasets.py,ConllNERDataset,collate_fn$205,"def collate_fn(self, batch):
    (src_tokens, src_seq_len, first) = ([], [], [])
    (tgt_tokens, tgt_seq_len, target_span) = ([], [], [])
    if self.mode == 'test':
        raw_words = []
        for tup in batch:
            src_tokens.append(tup[0])
            src_seq_len.append(tup[1])
            first.append(tup[2])
            raw_words.append(tup[3])
        src_tokens = pad_sequence(src_tokens, batch_first=True, padding_value=self.data_processor.tokenizer.pad_token_id)
        first = pad_sequence(first, batch_first=True, padding_value=0)
        return (src_tokens, torch.stack(src_seq_len, 0), first, raw_words)
    for tup in batch:
        src_tokens.append(tup[0])
        tgt_tokens.append(tup[1])
        src_seq_len.append(tup[2])
        tgt_seq_len.append(tup[3])
        first.append(tup[4])
        target_span.append(tup[5])
    src_tokens = pad_sequence(src_tokens, batch_first=True, padding_value=self.data_processor.tokenizer.pad_token_id)
    tgt_tokens = pad_sequence(tgt_tokens, batch_first=True, padding_value=1)
    first = pad_sequence(first, batch_first=True, padding_value=0)
    return (src_tokens, tgt_tokens, torch.stack(src_seq_len, 0), torch.stack(tgt_seq_len, 0), first, target_span)","for tup in batch:
    src_tokens.append(tup[0])
    tgt_tokens.append(tup[1])
    src_seq_len.append(tup[2])
    tgt_seq_len.append(tup[3])
    first.append(tup[4])
    target_span.append(tup[5])
src_tokens = pad_sequence(src_tokens, batch_first=True, padding_value=self.data_processor.tokenizer.pad_token_id)
tgt_tokens = pad_sequence(tgt_tokens, batch_first=True, padding_value=1)
first = pad_sequence(first, batch_first=True, padding_value=0)
return (src_tokens, tgt_tokens, torch.stack(src_seq_len, 0), torch.stack(tgt_seq_len, 0), first, target_span)","(src_tokens, tgt_tokens, src_seq_len, tgt_seq_len, first, target_span) = zip(*batch)
src_tokens = pad_sequence(src_tokens, batch_first=True, padding_value=self.data_processor.tokenizer.pad_token_id)
tgt_tokens = pad_sequence(tgt_tokens, batch_first=True, padding_value=1)
first = pad_sequence(first, batch_first=True, padding_value=0)
return (src_tokens, tgt_tokens, torch.stack(src_seq_len, 0), torch.stack(tgt_seq_len, 0), first, target_span)",find_wrong,-1
core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/homeassistant/components/nzbget/coordinator.py,https://github.com/home-assistant/core/tree/master/homeassistant/components/nzbget/coordinator.py,NZBGetDataUpdateCoordinator,_check_completed_downloads$51,"def _check_completed_downloads(self, history):
    """"""Check history for newly completed downloads.""""""
    actual_completed_downloads = {(x['Name'], x['Category'], x['Status']) for x in history}
    if self._completed_downloads_init:
        tmp_completed_downloads = list(actual_completed_downloads.difference(self._completed_downloads))
        for download in tmp_completed_downloads:
            self.hass.bus.fire('nzbget_download_complete', {'name': download[0], 'category': download[1], 'status': download[2]})
    self._completed_downloads = actual_completed_downloads
    self._completed_downloads_init = True","actual_completed_downloads = {(x['Name'], x['Category'], x['Status']) for x in history}
if self._completed_downloads_init:
    tmp_completed_downloads = list(actual_completed_downloads.difference(self._completed_downloads))
    for download in tmp_completed_downloads:
        self.hass.bus.fire('nzbget_download_complete', {'name': download[0], 'category': download[1], 'status': download[2]})
self._completed_downloads = actual_completed_downloads
self._completed_downloads_init = True","actual_completed_downloads = [(x['Name'], x['Category'], x['Status']) for x in history]
if self._completed_downloads_init:
    tmp_completed_downloads = list(set(actual_completed_downloads) - set(self._completed_downloads))
    for (name, category, status) in tmp_completed_downloads:
        self.hass.bus.fire('nzbget_download_complete', {'name': name, 'category': category, 'status': status})
self._completed_downloads = actual_completed_downloads
self._completed_downloads_init = True",find_wrong,-1
PaddleVideo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleVideo/paddlevideo/utils/multigrid/multigrid.py,https://github.com/PaddlePaddle/PaddleVideo/tree/master/paddlevideo/utils/multigrid/multigrid.py,MultigridSchedule,get_long_cycle_schedule$110,"def get_long_cycle_schedule(self, cfg):
    """"""
        Based on multigrid hyperparameters, define the schedule of a long cycle.
        Args:
            cfg (configs): configs that contains training and multigrid specific
                hyperparameters.
        Returns:
            schedule (list): Specifies a list long cycle base shapes and their
                corresponding training epochs.
        """"""
    steps = cfg.OPTIMIZER.learning_rate.steps
    default_size = float(cfg.PIPELINE.train.decode_sampler.num_frames * cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size'] ** 2)
    default_iters = steps[-1]
    avg_bs = []
    all_shapes = []
    for item in cfg.MULTIGRID.long_cycle_factors:
        (t_factor, s_factor) = item['value']
        base_t = int(round(cfg.PIPELINE.train.decode_sampler.num_frames * t_factor))
        base_s = int(round(cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size'] * s_factor))
        if cfg.MULTIGRID.SHORT_CYCLE:
            shapes = [[base_t, cfg.MULTIGRID.default_crop_size * cfg.MULTIGRID.short_cycle_factors[0]], [base_t, cfg.MULTIGRID.default_crop_size * cfg.MULTIGRID.short_cycle_factors[1]], [base_t, base_s]]
        else:
            shapes = [[base_t, base_s]]
        shapes = [[int(round(default_size / (s[0] * s[1] * s[1]))), s[0], s[1]] for s in shapes]
        avg_bs.append(np.mean([s[0] for s in shapes]))
        all_shapes.append(shapes)
    total_iters = 0
    schedule = []
    for step_index in range(len(steps) - 1):
        step_epochs = steps[step_index + 1] - steps[step_index]
        for (long_cycle_index, shapes) in enumerate(all_shapes):
            cur_epochs = step_epochs * avg_bs[long_cycle_index] / sum(avg_bs)
            cur_iters = cur_epochs / avg_bs[long_cycle_index]
            total_iters += cur_iters
            schedule.append((step_index, shapes[-1], cur_epochs))
    iter_saving = default_iters / total_iters
    final_step_epochs = cfg.OPTIMIZER.learning_rate.max_epoch - steps[-1]
    ft_epochs = final_step_epochs / iter_saving * avg_bs[-1]
    schedule.append((step_index + 1, all_shapes[-1][-1], ft_epochs))
    x = cfg.OPTIMIZER.learning_rate.max_epoch * cfg.MULTIGRID.epoch_factor / sum((s[-1] for s in schedule))
    final_schedule = []
    total_epochs = 0
    for s in schedule:
        epochs = s[2] * x
        total_epochs += epochs
        final_schedule.append((s[0], s[1], int(round(total_epochs))))
    print_schedule(final_schedule)
    return final_schedule","for item in cfg.MULTIGRID.long_cycle_factors:
    (t_factor, s_factor) = item['value']
    base_t = int(round(cfg.PIPELINE.train.decode_sampler.num_frames * t_factor))
    base_s = int(round(cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size'] * s_factor))
    if cfg.MULTIGRID.SHORT_CYCLE:
        shapes = [[base_t, cfg.MULTIGRID.default_crop_size * cfg.MULTIGRID.short_cycle_factors[0]], [base_t, cfg.MULTIGRID.default_crop_size * cfg.MULTIGRID.short_cycle_factors[1]], [base_t, base_s]]
    else:
        shapes = [[base_t, base_s]]
    shapes = [[int(round(default_size / (s[0] * s[1] * s[1]))), s[0], s[1]] for s in shapes]
    avg_bs.append(np.mean([s[0] for s in shapes]))
    all_shapes.append(shapes)","all_shapes = []
avg_bs = []
for item in cfg.MULTIGRID.long_cycle_factors:
    (t_factor, s_factor) = item['value']
    base_t = int(round(cfg.PIPELINE.train.decode_sampler.num_frames * t_factor))
    base_s = int(round(cfg.PIPELINE.train.transform[1]['MultiCrop']['target_size'] * s_factor))
    shapes = []
    if cfg.MULTIGRID.SHORT_CYCLE:
        for factor in cfg.MULTIGRID.short_cycle_factors:
            shapes.append([base_t, cfg.MULTIGRID.default_crop_size * factor])
    shapes.append([base_t, base_s])
    shapes = [[int(round(default_size / (s[0] * s[1] * s[1]))), s[0], s[1]] for s in shapes]
    avg_bs.append(np.mean([s[0] for s in shapes]))
    all_shapes.append(shapes)",find_wrong,2
tensorlayer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorlayer/tensorlayer/prepro.py,https://github.com/tensorlayer/tensorlayer/tree/master/tensorlayer/prepro.py,,resize_image$3684,"def resize_image(image, annos, mask, target_width, target_height):
    """"""Reszie image

        Parameters
        -----------
        image : 3 channel image
            The given image.
        annos : list of list of floats
            Keypoints of people
        mask : single channel image or None
            The mask if available.
        target_width : int
            Expected width of returned image.
        target_height : int
            Expected height of returned image.

        Returns
        ----------
        preprocessed input image, annos, mask

        """"""
    (y, x, _) = np.shape(image)
    ratio_y = target_height / y
    ratio_x = target_width / x
    new_joints = []
    for people in annos:
        new_keypoints = []
        for keypoints in people:
            if keypoints[0] < 0 or keypoints[1] < 0:
                new_keypoints.append((-1000, -1000))
                continue
            pts = (int(keypoints[0] * ratio_x + 0.5), int(keypoints[1] * ratio_y + 0.5))
            if pts[0] > target_width - 1 or pts[1] > target_height - 1:
                new_keypoints.append((-1000, -1000))
                continue
            new_keypoints.append(pts)
        new_joints.append(new_keypoints)
    annos = new_joints
    new_image = cv2.resize(image, (target_width, target_height), interpolation=cv2.INTER_AREA)
    if mask is not None:
        new_mask = cv2.resize(mask, (target_width, target_height), interpolation=cv2.INTER_AREA)
        return (new_image, annos, new_mask)
    else:
        return (new_image, annos, None)","for people in annos:
    new_keypoints = []
    for keypoints in people:
        if keypoints[0] < 0 or keypoints[1] < 0:
            new_keypoints.append((-1000, -1000))
            continue
        pts = (int(keypoints[0] * ratio_x + 0.5), int(keypoints[1] * ratio_y + 0.5))
        if pts[0] > target_width - 1 or pts[1] > target_height - 1:
            new_keypoints.append((-1000, -1000))
            continue
        new_keypoints.append(pts)
    new_joints.append(new_keypoints)
annos = new_joints","annos = [[(int(keypoints[0] * ratio_x + 0.5), int(keypoints[1] * ratio_y + 0.5)) if keypoints[0] >= 0 and keypoints[1] >= 0 and (int(keypoints[0] * ratio_x + 0.5) <= target_width - 1) and (int(keypoints[1] * ratio_y + 0.5) <= target_height - 1) else (-1000, -1000) for keypoints in people] for people in annos]",find_wrong,-1
OnlyFans,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OnlyFans/modules/onlyfans.py,https://github.com/DIGITALCRIMINAL/OnlyFans/tree/master/modules/onlyfans.py,,scrape_choice$166,"def scrape_choice(authed: create_auth, subscription):
    user_id = subscription.id
    post_count = subscription.postsCount
    archived_count = subscription.archivedPostsCount
    message = 'Scrape: 0 = All | 1 = Images | 2 = Videos | 3 = Audios | 4 = Texts'
    media_types = [[['', 'All'], ['', 'Images'], ['', 'Videos'], ['', 'Audios'], ['', 'Texts']], message]
    choice_list = main_helper.choose_option(media_types, auto_media_choice)
    user_api = OnlyFans.endpoint_links(user_id).users
    message_api = OnlyFans.endpoint_links(user_id).message_api
    stories_api = OnlyFans.endpoint_links(user_id).stories_api
    list_highlights = OnlyFans.endpoint_links(user_id).list_highlights
    post_api = OnlyFans.endpoint_links(user_id).post_api
    archived_api = OnlyFans.endpoint_links(user_id).archived_posts
    only_links = False
    mandatory = [download_directory, only_links]
    y = ['photo', 'video', 'stream', 'gif', 'audio', 'text']
    u_array = ['You have chosen to scrape {}', [user_api, media_types, *mandatory, post_count], 'Profile']
    s_array = ['You have chosen to scrape {}', [stories_api, media_types, *mandatory, post_count], 'Stories']
    h_array = ['You have chosen to scrape {}', [list_highlights, media_types, *mandatory, post_count], 'Highlights']
    p_array = ['You have chosen to scrape {}', [post_api, media_types, *mandatory, post_count], 'Posts']
    m_array = ['You have chosen to scrape {}', [message_api, media_types, *mandatory, post_count], 'Messages']
    a_array = ['You have chosen to scrape {}', [archived_api, media_types, *mandatory, archived_count], 'Archived']
    array = [u_array, s_array, p_array, a_array, m_array]
    new_array = []
    valid_input = True
    for xxx in array:
        if xxx[2] == 'Mass Messages':
            if not subscription.is_me():
                continue
        new_item = dict()
        new_item['api_message'] = xxx[0]
        new_item['api_array'] = {}
        new_item['api_array']['api_link'] = xxx[1][0]
        new_item['api_array']['media_types'] = xxx[1][1]
        new_item['api_array']['directory'] = xxx[1][2]
        new_item['api_array']['only_links'] = xxx[1][3]
        new_item['api_array']['post_count'] = xxx[1][4]
        formatted = format_media_types()
        final_format = []
        for choice in choice_list:
            choice = choice[1]
            final_format.extend([result for result in formatted if result[0] == choice])
        new_item['api_array']['media_types'] = final_format
        new_item['api_type'] = xxx[2]
        if valid_input:
            new_array.append(new_item)
    return new_array","array = [u_array, s_array, p_array, a_array, m_array]
    # array = [u_array, s_array, p_array, a_array, m_array]
    # array = [s_array, h_array, p_array, a_array, m_array]
    # array = [s_array]
    # array = [u_array]
    # array = [p_array]
    # array = [a_array]
    # array = [m_array]
    new_array = []
    valid_input = True
    for xxx in array:
        if xxx[2] == ""Mass Messages"":
            if not subscription.is_me():
                continue
        new_item = dict()
        new_item[""api_message""] = xxx[0]
        new_item[""api_array""] = {}
        new_item[""api_array""][""api_link""] = xxx[1][0]
        new_item[""api_array""][""media_types""] = xxx[1][1]
        new_item[""api_array""][""directory""] = xxx[1][2]
        new_item[""api_array""][""only_links""] = xxx[1][3]
        new_item[""api_array""][""post_count""] = xxx[1][4]
        formatted = format_media_types()
        final_format = []
        for choice in choice_list:
            choice = choice[1]
            final_format.extend([result for result in formatted if result[0] == choice])
        new_item[""api_array""][""media_types""] = final_format
        new_item[""api_type""] = xxx[2]
        if valid_input:
            new_array.append(new_item)","api_links = [
        OnlyFans.endpoint_links(subscription.id).users,
        OnlyFans.endpoint_links(subscription.id).message_api,
        OnlyFans.endpoint_links(subscription.id).stories_api,
        OnlyFans.endpoint_links(subscription.id).list_highlights,
        OnlyFans.endpoint_links(subscription.id).post_api,
        OnlyFans.endpoint_links(subscription.id).archived_posts,
    ]
    api_types = [""Profile"", ""Messages"", ""Stories"", ""Highlights"", ""Posts"", ""Archived""]
    new_array = []
    valid_input = True
    for api_link, api_type in zip(api_links, api_types):
        if api_type == ""Mass Messages"" and not subscription.is_me():
            continue
        new_item = dict()
        new_item[""api_message""] = f""You have chosen to scrape {api_type}""
        new_item[""api_array""] = {}
        new_item[""api_array""][""api_link""] = api_link
        new_item[""api_array""][""media_types""] = [
            ["""", ""All""],
            ["""", ""Images""],
            ["""", ""Videos""],
            ["""", ""Audios""],
            ["""", ""Texts""],
        ]
        new_item[""api_array""][""directory""] = download_directory
        new_item[""api_array""][""only_links""] = False
        new_item[""api_array""][""post_count""] = (
            subscription.postsCount if api_type != ""Archived"" else subscription.archivedPostsCount
        )
        formatted = format_media_types()
        final_format = []
        for choice in choice_list:
            choice = choice[1]
            final_format.extend([result for result in formatted if result[0] == choice])
        new_item[""api_array""][""media_types""] = final_format
        new_item[""api_type""] = api_type
        if valid_input:
            new_array.append(new_item)",find_wrong,-1
sympy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sympy/sympy/printing/pretty/pretty.py,https://github.com/sympy/sympy/tree/master/sympy/printing/pretty/pretty.py,PrettyPrinter,_print_Integral$426,"def _print_Integral(self, integral):
    f = integral.function
    prettyF = self._print(f)
    if f.is_Add:
        prettyF = prettyForm(*prettyF.parens())
    arg = prettyF
    for x in integral.limits:
        prettyArg = self._print(x[0])
        if prettyArg.width() > 1:
            prettyArg = prettyForm(*prettyArg.parens())
        arg = prettyForm(*arg.right(' d', prettyArg))
    firstterm = True
    s = None
    for lim in integral.limits:
        h = arg.height()
        H = h + 2
        ascii_mode = not self._use_unicode
        if ascii_mode:
            H += 2
        vint = vobj('int', H)
        pform = prettyForm(vint)
        pform.baseline = arg.baseline + (H - h) // 2
        if len(lim) > 1:
            if len(lim) == 2:
                prettyA = prettyForm('')
                prettyB = self._print(lim[1])
            if len(lim) == 3:
                prettyA = self._print(lim[1])
                prettyB = self._print(lim[2])
            if ascii_mode:
                spc = max(1, 3 - prettyB.width())
                prettyB = prettyForm(*prettyB.left(' ' * spc))
                spc = max(1, 4 - prettyA.width())
                prettyA = prettyForm(*prettyA.right(' ' * spc))
            pform = prettyForm(*pform.above(prettyB))
            pform = prettyForm(*pform.below(prettyA))
        if not ascii_mode:
            pform = prettyForm(*pform.right(' '))
        if firstterm:
            s = pform
            firstterm = False
        else:
            s = prettyForm(*s.left(pform))
    pform = prettyForm(*arg.left(s))
    pform.binding = prettyForm.MUL
    return pform","arg = prettyF
for x in integral.limits:
    prettyArg = self._print(x[0])
    if prettyArg.width() > 1:
        prettyArg = prettyForm(*prettyArg.parens())
    arg = prettyForm(*arg.right(' d', prettyArg))
firstterm = True
s = None
for lim in integral.limits:
    h = arg.height()
    H = h + 2
    ascii_mode = not self._use_unicode
    if ascii_mode:
        H += 2
    vint = vobj('int', H)
    pform = prettyForm(vint)
    pform.baseline = arg.baseline + (H - h) // 2
    if len(lim) > 1:
        if len(lim) == 2:
            prettyA = prettyForm('')
            prettyB = self._print(lim[1])
        if len(lim) == 3:
            prettyA = self._print(lim[1])
            prettyB = self._print(lim[2])
        if ascii_mode:
            spc = max(1, 3 - prettyB.width())
            prettyB = prettyForm(*prettyB.left(' ' * spc))
            spc = max(1, 4 - prettyA.width())
            prettyA = prettyForm(*prettyA.right(' ' * spc))
        pform = prettyForm(*pform.above(prettyB))
        pform = prettyForm(*pform.below(prettyA))
    if not ascii_mode:
        pform = prettyForm(*pform.right(' '))
    if firstterm:
        s = pform
        firstterm = False
    else:
        s = prettyForm(*s.left(pform))
pform = prettyForm(*arg.left(s))
pform.binding = prettyForm.MUL","arg = prettyF
for x in integral.limits:
    prettyArg = self._print(x[0])
    if prettyArg.width() > 1:
        prettyArg = prettyForm(*prettyArg.parens())
    arg = prettyForm(*arg.right(' d', prettyArg))
s = prettyForm()
for lim in integral.limits:
    h = arg.height()
    H = h + 2
    ascii_mode = not self._use_unicode
    if ascii_mode:
        H += 2
    vint = vobj('int', H)
    pform = prettyForm(vint)
    pform.baseline = arg.baseline + (H - h) // 2
    if len(lim) > 1:
        if len(lim) == 2:
            prettyA = prettyForm('')
            prettyB = self._print(lim[1])
        if len(lim) == 3:
            prettyA = self._print(lim[1])
            prettyB = self._print(lim[2])
        if ascii_mode:
            spc = max(1, 3 - prettyB.width())
            prettyB = prettyForm(*prettyB.left(' ' * spc))
            spc = max(1, 4 - prettyA.width())
            prettyA = prettyForm(*prettyA.right(' ' * spc))
        pform = prettyForm(*pform.above(prettyB))
        pform = prettyForm(*pform.below(prettyA))
    if not ascii_mode:
        pform = prettyForm(*pform.right(' '))
    s = prettyForm(*s.left(pform))
pform = prettyForm(*arg.left(s))
pform.binding = prettyForm.MUL",find_wrong,-1
LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/head_detection/data_provider_farm/reformat_brainwash.py,https://github.com/YonghaoHe/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/tree/master/head_detection/data_provider_farm/reformat_brainwash.py,,generate_data_list$12,"def generate_data_list():
    txt_file_path = '/media/heyonghao/HYH-4T-WD/public_dataset/head_detection/brainwash/brainwash/brainwash_test.idl'
    image_root = '/media/heyonghao/HYH-4T-WD/public_dataset/head_detection/brainwash/brainwash'
    list_file_path = './data_folder/data_list_brainwash_test.txt'
    if not os.path.exists(os.path.dirname(list_file_path)):
        os.makedirs(os.path.dirname(list_file_path))
    fin = open(txt_file_path, 'r')
    fout = open(list_file_path, 'w')
    counter = 0
    for line in fin:
        line = line.strip(';\n')
        im_path = re.findall('[""](.*?)[""]', line)[0]
        im_path = os.path.join(image_root, im_path)
        if not os.path.exists(im_path):
            print('im file does not exist : %s' % im_path)
            continue
        bbox_str_list = re.findall('[(](.*?)[)]', line)
        bbox_list = []
        for bbox_str in bbox_str_list:
            bbox_str = bbox_str.split(', ')
            xmin = int(float(bbox_str[0]))
            ymin = int(float(bbox_str[1]))
            xmax = int(float(bbox_str[2]))
            ymax = int(float(bbox_str[3]))
            bbox_list.append((xmin, ymin, xmax - xmin + 1, ymax - ymin + 1))
        if len(bbox_list) == 0:
            line_str = im_path + ',0,0'
            fout.write(line_str + '\n')
        else:
            line_str = im_path + ',1,' + str(len(bbox_list))
            for bbox in bbox_list:
                line_str += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
            fout.write(line_str + '\n')
        counter += 1
        print(counter)
    fout.close()
    fin.close()","bbox_str_list = re.findall('[(](.*?)[)]', line)
bbox_list = []
for bbox_str in bbox_str_list:
    bbox_str = bbox_str.split(', ')
    xmin = int(float(bbox_str[0]))
    ymin = int(float(bbox_str[1]))
    xmax = int(float(bbox_str[2]))
    ymax = int(float(bbox_str[3]))
    bbox_list.append((xmin, ymin, xmax - xmin + 1, ymax - ymin + 1))
if len(bbox_list) == 0:
    line_str = im_path + ',0,0'
    fout.write(line_str + '\n')
else:
    line_str = im_path + ',1,' + str(len(bbox_list))
    for bbox in bbox_list:
        line_str += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
    fout.write(line_str + '\n')","bbox_list = []
for bbox_str in re.findall('[(](.*?)[)]', line):
    bbox_str = bbox_str.split(', ')
    (xmin, ymin, xmax, ymax) = map(float, bbox_str)
    bbox_list.append((int(xmin), int(ymin), int(xmax - xmin + 1), int(ymax - ymin + 1)))
if not bbox_list:
    line_str = f'{im_path},0,0'
else:
    bbox_str = ','.join([f'{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}' for bbox in bbox_list])
    line_str = f'{im_path},1,{len(bbox_list)},{bbox_str}'
fout.write(line_str + '\n')",find_wrong,1
ezdxf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ezdxf/src/ezdxf/path/converter.py,https://github.com/mozman/ezdxf/tree/master/src/ezdxf/path/converter.py,,to_bsplines_and_vertices$837,"def to_bsplines_and_vertices(path: Path, g1_tol: float=G1_TOL) -> Iterator[PathParts]:
    """"""Convert a :class:`Path` object into multiple cubic B-splines and
    polylines as lists of vertices. Breaks adjacent Bzier without G1
    continuity into separated B-splines.

    Args:
        path: :class:`Path` objects
        g1_tol: tolerance for G1 continuity check

    Returns:
        :class:`~ezdxf.math.BSpline` and lists of :class:`~ezdxf.math.Vec3`

    """"""
    from ezdxf.math import bezier_to_bspline

    def to_vertices():
        points = [polyline[0][0]]
        for line in polyline:
            points.append(line[1])
        return points

    def to_bspline():
        b1 = bezier[0]
        _g1_continuity_curves = [b1]
        for b2 in bezier[1:]:
            if have_bezier_curves_g1_continuity(b1, b2, g1_tol):
                _g1_continuity_curves.append(b2)
            else:
                yield bezier_to_bspline(_g1_continuity_curves)
                _g1_continuity_curves = [b2]
            b1 = b2
        if _g1_continuity_curves:
            yield bezier_to_bspline(_g1_continuity_curves)
    curves = []
    for path in tools.single_paths([path]):
        prev = path.start
        for cmd in path:
            if cmd.type == Command.CURVE3_TO:
                curve = Bezier3P([prev, cmd.ctrl, cmd.end])
            elif cmd.type == Command.CURVE4_TO:
                curve = Bezier4P([prev, cmd.ctrl1, cmd.ctrl2, cmd.end])
            elif cmd.type == Command.LINE_TO:
                curve = (prev, cmd.end)
            else:
                raise ValueError
            curves.append(curve)
            prev = cmd.end
    bezier: list = []
    polyline: list = []
    for curve in curves:
        if isinstance(curve, tuple):
            if bezier:
                yield from to_bspline()
                bezier.clear()
            polyline.append(curve)
        else:
            if polyline:
                yield to_vertices()
                polyline.clear()
            bezier.append(curve)
    if bezier:
        yield from to_bspline()
    if polyline:
        yield to_vertices()","bezier: list = []
polyline: list = []
for curve in curves:
    if isinstance(curve, tuple):
        if bezier:
            yield from to_bspline()
            bezier.clear()
        polyline.append(curve)
    else:
        if polyline:
            yield to_vertices()
            polyline.clear()
        bezier.append(curve)
if bezier:
    yield from to_bspline()
if polyline:
    yield to_vertices()","bezier: list = []
polyline: list = []
for curve in curves:
    if isinstance(curve, tuple):
        if bezier:
            yield from to_bspline()
            bezier = []
        polyline.append(curve)
    else:
        if polyline:
            yield to_vertices()
            polyline = []
        bezier.append(curve)
if bezier:
    yield from to_bspline()
if polyline:
    yield to_vertices()",find_wrong,2
PGPortfolio,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PGPortfolio/pgportfolio/marketdata/globaldatamatrix.py,https://github.com/ZhengyaoJiang/PGPortfolio/tree/master/pgportfolio/marketdata/globaldatamatrix.py,HistoryManager,select_coins$125,"def select_coins(self, start, end):
    if not self._online:
        logging.info('select coins offline from %s to %s' % (datetime.fromtimestamp(start).strftime('%Y-%m-%d %H:%M'), datetime.fromtimestamp(end).strftime('%Y-%m-%d %H:%M')))
        connection = sqlite3.connect(DATABASE_DIR)
        try:
            cursor = connection.cursor()
            cursor.execute('SELECT coin,SUM(volume) AS total_volume FROM History WHERE date>=? and date<=? GROUP BY coin ORDER BY total_volume DESC LIMIT ?;', (int(start), int(end), self._coin_number))
            coins_tuples = cursor.fetchall()
            if len(coins_tuples) != self._coin_number:
                logging.error('the sqlite error happend')
        finally:
            connection.commit()
            connection.close()
        coins = []
        for tuple in coins_tuples:
            coins.append(tuple[0])
    else:
        coins = list(self._coin_list.topNVolume(n=self._coin_number).index)
    logging.debug('Selected coins are: ' + str(coins))
    return coins","coins = []
for tuple in coins_tuples:
    coins.append(tuple[0])",coins = [coin_tuple[0] for coin_tuple in coins_tuples],find_wrong,-1
AIDungeon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AIDungeon/story/utils.py,https://github.com/Latitude-Archives/AIDungeon/tree/master/story/utils.py,,is_first_person$151,"def is_first_person(text):
    count = 0
    for pair in first_to_second_mappings:
        variations = mapping_variation_pairs(pair)
        for variation in variations:
            reg_expr = re.compile(variation[0] + '(?=([^""]*""[^""]*"")*[^""]*$)')
            matches = re.findall(reg_expr, text)
            count += len(matches)
    if count > 3:
        return True
    else:
        return False","for pair in first_to_second_mappings:
    variations = mapping_variation_pairs(pair)
    for variation in variations:
        reg_expr = re.compile(variation[0] + '(?=([^""]*""[^""]*"")*[^""]*$)')
        matches = re.findall(reg_expr, text)
        count += len(matches)","count = sum((len(re.findall(re.compile(variation[0] + '(?=([^""]*""[^""]*"")*[^""]*$)'), text)) for pair in first_to_second_mappings for variation in mapping_variation_pairs(pair)))",find_wrong,-1
pydicom,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pydicom/source/generate_cids/generate_concept_dicts.py,https://github.com/pydicom/pydicom/tree/master/source/generate_cids/generate_concept_dicts.py,,get_table_o1$176,"def get_table_o1():
    logger.info('process Table O1')
    url = 'http://dicom.nema.org/medical/dicom/current/output/chtml/part16/chapter_O.html#table_O-1'
    root = _parse_html(_download_html(url))
    namespaces = {'w3': root.tag.split('}')[0].strip('{')}
    body = root.find('w3:body', namespaces=namespaces)
    table = body.findall('.//w3:tbody', namespaces=namespaces)[0]
    rows = table.findall('./w3:tr', namespaces=namespaces)
    data = []
    for row in rows:
        data.append((_get_text(row[0].findall('.//w3:p', namespaces=namespaces)[-1]), _get_text(row[1].findall('.//w3:p', namespaces=namespaces)[0]), _get_text(row[2].findall('.//w3:p', namespaces=namespaces)[0])))
    return data","rows = table.findall('./w3:tr', namespaces=namespaces)
data = []
for row in rows:
    data.append((_get_text(row[0].findall('.//w3:p', namespaces=namespaces)[-1]), _get_text(row[1].findall('.//w3:p', namespaces=namespaces)[0]), _get_text(row[2].findall('.//w3:p', namespaces=namespaces)[0])))","data = [(_get_text(row[0].findall('.//w3:p', namespaces=namespaces)[-1]), _get_text(row[1].findall('.//w3:p', namespaces=namespaces)[0]), _get_text(row[2].findall('.//w3:p', namespaces=namespaces)[0])) for row in table.findall('./w3:tr', namespaces=namespaces)]",find_wrong,-1
rotki,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rotki/rotkehlchen/db/dbhandler.py,https://github.com/rotki/rotki/tree/master/rotkehlchen/db/dbhandler.py,DBHandler,get_manually_tracked_balances$1356,"def get_manually_tracked_balances(self, cursor: 'DBCursor', balance_type: Optional[BalanceType]=BalanceType.ASSET) -> List[ManuallyTrackedBalance]:
    """"""Returns the manually tracked balances from the DB""""""
    query_balance_type = ''
    if balance_type is not None:
        query_balance_type = f'WHERE A.category=""{balance_type.serialize_for_db()}""'
    query = cursor.execute(f'SELECT A.asset, A.label, A.amount, A.location, group_concat(B.tag_name,"",""), A.category, A.id FROM manually_tracked_balances as A LEFT OUTER JOIN tag_mappings as B on B.object_reference = A.id {query_balance_type} GROUP BY label;')
    data = []
    for entry in query:
        tags = deserialize_tags_from_db(entry[4])
        try:
            balance_type = BalanceType.deserialize_from_db(entry[5])
            data.append(ManuallyTrackedBalance(id=entry[6], asset=Asset(entry[0]).check_existence(), label=entry[1], amount=FVal(entry[2]), location=Location.deserialize_from_db(entry[3]), tags=tags, balance_type=balance_type))
        except (DeserializationError, UnknownAsset, UnsupportedAsset, ValueError) as e:
            self.msg_aggregator.add_warning(f'Unexpected data in a ManuallyTrackedBalance entry in the DB: {str(e)}')
    return data","query = cursor.execute(f'SELECT A.asset, A.label, A.amount, A.location, group_concat(B.tag_name,"",""), A.category, A.id FROM manually_tracked_balances as A LEFT OUTER JOIN tag_mappings as B on B.object_reference = A.id {query_balance_type} GROUP BY label;')
data = []
for entry in query:
    tags = deserialize_tags_from_db(entry[4])
    try:
        balance_type = BalanceType.deserialize_from_db(entry[5])
        data.append(ManuallyTrackedBalance(id=entry[6], asset=Asset(entry[0]).check_existence(), label=entry[1], amount=FVal(entry[2]), location=Location.deserialize_from_db(entry[3]), tags=tags, balance_type=balance_type))
    except (DeserializationError, UnknownAsset, UnsupportedAsset, ValueError) as e:
        self.msg_aggregator.add_warning(f'Unexpected data in a ManuallyTrackedBalance entry in the DB: {str(e)}')","data = []
for entry in cursor.execute(f'SELECT A.asset, A.label, A.amount, A.location, group_concat(B.tag_name,"",""), A.category, A.id FROM manually_tracked_balances as A LEFT OUTER JOIN tag_mappings as B on B.object_reference = A.id {query_balance_type} GROUP BY label;'):
    tags = deserialize_tags_from_db(entry[4])
    try:
        balance_type = BalanceType.deserialize_from_db(entry[5])
        data.append(ManuallyTrackedBalance(id=entry[6], asset=Asset(entry[0]).check_existence(), label=entry[1], amount=FVal(entry[2]), location=Location.deserialize_from_db(entry[3]), tags=tags, balance_type=balance_type))
    except (DeserializationError, UnknownAsset, UnsupportedAsset, ValueError) as e:
        self.msg_aggregator.add_warning(f'Unexpected data in a ManuallyTrackedBalance entry in the DB: {str(e)}')",find_wrong,-1
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/bluez_bluetooth.py,https://github.com/saltstack/salt/tree/master/salt/modules/bluez_bluetooth.py,,scan$167,"def scan():
    """"""
    Scan for bluetooth devices in the area

    CLI Example:

    .. code-block:: bash

        salt '*' bluetooth.scan
    """"""
    ret = []
    devices = bluetooth.discover_devices(lookup_names=True)
    for device in devices:
        ret.append({device[0]: device[1]})
    return ret","devices = bluetooth.discover_devices(lookup_names=True)
for device in devices:
    ret.append({device[0]: device[1]})","devices = bluetooth.discover_devices(lookup_names=True)
ret = [{device[0]: device[1]} for device in devices]",find_wrong,-1
aws-parallelcluster,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-parallelcluster/awsbatch-cli/src/awsbatch/awsbsub.py,https://github.com/aws/aws-parallelcluster/tree/master/awsbatch-cli/src/awsbatch/awsbsub.py,AWSBsubCommand,run$433,"def run(self, job_definition, job_name, job_queue, command, nodes=None, vcpus=None, memory=None, array_size=None, retry_attempts=1, timeout=None, dependencies=None, env=None):
    """"""Submit the job.""""""
    try:
        array_properties = {}
        if array_size:
            array_properties.update(size=array_size)
        retry_strategy = {'attempts': retry_attempts}
        depends_on = dependencies if dependencies else []
        container_overrides = {'command': command}
        if vcpus:
            container_overrides.update(vcpus=vcpus)
        if memory:
            container_overrides.update(memory=memory)
        environment = []
        for env_var in env:
            environment.append({'name': env_var[0], 'value': env_var[1]})
        container_overrides.update(environment=environment)
        submission_args = {'jobName': job_name, 'jobQueue': job_queue, 'dependsOn': depends_on, 'retryStrategy': retry_strategy}
        if nodes:
            submission_args.update({'jobDefinition': job_definition})
            target_nodes = '0:'
            node_overrides = {'numNodes': nodes, 'nodePropertyOverrides': [{'targetNodes': target_nodes, 'containerOverrides': container_overrides}]}
            submission_args.update({'nodeOverrides': node_overrides})
            if timeout:
                submission_args.update({'timeout': {'attemptDurationSeconds': timeout}})
        else:
            submission_args.update({'jobDefinition': job_definition})
            submission_args.update({'containerOverrides': container_overrides})
            submission_args.update({'arrayProperties': array_properties})
            if timeout:
                submission_args.update({'timeout': {'attemptDurationSeconds': timeout}})
        self.log.debug('Job submission args: %s', submission_args)
        response = self.batch_client.submit_job(**submission_args)
        print('Job %s (%s) has been submitted.' % (response['jobId'], response['jobName']))
    except Exception as e:
        fail('Error submitting job to AWS Batch. Failed with exception: %s' % e)","environment = []
for env_var in env:
    environment.append({'name': env_var[0], 'value': env_var[1]})
container_overrides.update(environment=environment)","environment = [{'name': env_var[0], 'value': env_var[1]} for env_var in env]
container_overrides.update(environment=environment)",find_wrong,-1
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli-core/azure/cli/core/commands/arm.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli-core/azure/cli/core/commands/arm.py,,_get_internal_path$637,"def _get_internal_path(path):
    path = path.replace('.[', '[').replace('[', '.[')
    path_segment_pairs = internal_path_regex.findall(path)
    final_paths = []
    for regex_result in path_segment_pairs:
        segment = regex_result[0] or regex_result[1]
        final_paths.append(segment)
    return final_paths","path_segment_pairs = internal_path_regex.findall(path)
final_paths = []
for regex_result in path_segment_pairs:
    segment = regex_result[0] or regex_result[1]
    final_paths.append(segment)",final_paths = [regex_result[0] or regex_result[1] for regex_result in internal_path_regex.findall(path)],find_wrong,-1
Transformer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Transformer/Beam.py,https://github.com/SamLynnEvans/Transformer/tree/master//Beam.py,,beam_search$55,"def beam_search(src, model, SRC, TRG, opt):
    (outputs, e_outputs, log_scores) = init_vars(src, model, SRC, TRG, opt)
    eos_tok = TRG.vocab.stoi['<eos>']
    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)
    ind = None
    for i in range(2, opt.max_len):
        trg_mask = nopeak_mask(i, opt)
        out = model.out(model.decoder(outputs[:, :i], e_outputs, src_mask, trg_mask))
        out = F.softmax(out, dim=-1)
        (outputs, log_scores) = k_best_outputs(outputs, out, log_scores, i, opt.k)
        ones = (outputs == eos_tok).nonzero()
        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()
        for vec in ones:
            i = vec[0]
            if sentence_lengths[i] == 0:
                sentence_lengths[i] = vec[1]
        num_finished_sentences = len([s for s in sentence_lengths if s > 0])
        if num_finished_sentences == opt.k:
            alpha = 0.7
            div = 1 / sentence_lengths.type_as(log_scores) ** alpha
            (_, ind) = torch.max(log_scores * div, 1)
            ind = ind.data[0]
            break
    if ind is None:
        length = (outputs[0] == eos_tok).nonzero()[0]
        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])
    else:
        length = (outputs[ind] == eos_tok).nonzero()[0]
        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])","ones = (outputs == eos_tok).nonzero()
sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()
for vec in ones:
    i = vec[0]
    if sentence_lengths[i] == 0:
        sentence_lengths[i] = vec[1]
num_finished_sentences = len([s for s in sentence_lengths if s > 0])
if num_finished_sentences == opt.k:
    alpha = 0.7
    div = 1 / sentence_lengths.type_as(log_scores) ** alpha
    (_, ind) = torch.max(log_scores * div, 1)
    ind = ind.data[0]
    break","finished_sentences = []
for i in range(opt.k):
    ones = (outputs[i] == eos_tok).nonzero()
    if len(ones) > 0:
        finished_sentences.append(i)
        sentence_lengths = ones[0][1]
        alpha = 0.7
        div = 1 / sentence_lengths.type_as(log_scores) ** alpha
        (_, ind) = torch.max(log_scores[i] * div, 0)
        ind = ind.item()
        break
if len(finished_sentences) == opt.k:
    return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:sentence_lengths]])
else:
    length = (outputs[0] == eos_tok).nonzero()[0]
    return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])",find_wrong,-1
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/CreatHtml.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/CreatHtml.py,CreatHtml,api_detail_html$1072,"def api_detail_html(self):
    api_id = 1
    whole_html = ''
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from api_tree where success = 1 or success = 2'
    cursor.execute(sql)
    api_infos = cursor.fetchall()
    for api_info in api_infos:
        api_info_html = '<div id=""api_{api_id}"" class=""modal fade"" role=""dialog"">\n            \t<div class=""modal-dialog"">\n            \t\t<div class=""modal-content"">\n            \t\t\t<div class=""modal-header"">\n            \t\t\t\t<h6 class=""modal-title"">%s...</h4>\n            \t\t\t</div>\n            \t\t\t<div class=""modal-body"">\n            \t\t\t\t<div class=""box-body no-padding"">\n            \t\t\t\t\t<p>\n            \t\t\t\t\t\t%sapi_name}<br><br>\n                        %sapi_type}<br><br>\n                        %s <a href=""{api_path}"">{api_path}</a><br><br>\n                        %s {api_js}<br><br>\n                        %s <code>{api_res}</code><br>\n            \t\t\t\t\t</p>\n            \t\t\t\t</div>\n            \t\t\t</div>\n            \t\t</div>\n            \t</div>\n            </div>' % (Utils().getMyWord('{api_detail}'), Utils().getMyWord('{api_name}'), Utils().getMyWord('{r_type}'), Utils().getMyWord('{r_api_addr}'), Utils().getMyWord('{r_api_js}'), Utils().getMyWord('{r_api_res}'))
        sql = ""select path from js_file where id='%s'"" % api_info[6]
        cursor.execute(sql)
        js_path = cursor.fetchone()
        if api_info[5] == 2:
            api_type = Utils().getMyWord('{r_post}')
        else:
            api_type = Utils().getMyWord('{r_get}')
        if api_info[4] == None:
            api_res = ''
        else:
            api_res = api_info[4]
        api_info_html = api_info_html.replace('{api_id}', str(api_id))
        api_info_html = api_info_html.replace('{api_type}', str(api_type))
        api_info_html = api_info_html.replace('{api_name}', api_info[2])
        api_info_html = api_info_html.replace('{api_path}', api_info[1])
        api_info_html = api_info_html.replace('{api_js}', js_path[0])
        api_info_html = api_info_html.replace('{api_res}', api_res)
        api_id = api_id + 1
        whole_html = whole_html + api_info_html
    return whole_html","api_infos = cursor.fetchall()
for api_info in api_infos:
    api_info_html = '<div id=""api_{api_id}"" class=""modal fade"" role=""dialog"">\n        <div class=""modal-dialog"">\n            <div class=""modal-content"">\n                <div class=""modal-header"">\n                    <h6 class=""modal-title"">%s...</h4>\n                </div>\n                <div class=""modal-body"">\n                    <div class=""box-body no-padding"">\n                        <p>\n                            %sapi_name}<br><br>\n                            %sapi_type}<br><br>\n                            %s <a href=""{api_path}"">{api_path}</a><br><br>\n                            %s {api_js}<br><br>\n                            %s <code>{api_res}</code><br>\n                        </p>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>' % (Utils().getMyWord('{api_detail}'), Utils().getMyWord('{api_name}'), Utils().getMyWord('{r_type}'), Utils().getMyWord('{r_api_addr}'), Utils().getMyWord('{r_api_js}'), Utils().getMyWord('{r_api_res}'))
    ...
    whole_html = whole_html + api_info_html","for (api_id, api_info) in enumerate(cursor.fetchall(), start=1):
    api_info_html = '<div id=""api_{api_id}"" class=""modal fade"" role=""dialog"">\n        <div class=""modal-dialog"">\n            <div class=""modal-content"">\n                <div class=""modal-header"">\n                    <h6 class=""modal-title"">%s...</h4>\n                </div>\n                <div class=""modal-body"">\n                    <div class=""box-body no-padding"">\n                        <p>\n                            %sapi_name}<br><br>\n                            %sapi_type}<br><br>\n                            %s <a href=""{api_path}"">{api_path}</a><br><br>\n                            %s {api_js}<br><br>\n                            %s <code>{api_res}</code><br>\n                        </p>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </div>' % (Utils().getMyWord('{api_detail}'), Utils().getMyWord('{api_name}'), Utils().getMyWord('{r_type}'), Utils().getMyWord('{r_api_addr}'), Utils().getMyWord('{r_api_js}'), Utils().getMyWord('{r_api_res}'))
    ...
    whole_html += api_info_html",find_wrong,-1
gluon-cv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gluon-cv/scripts/detection/ssd/train_ssd.py,https://github.com/dmlc/gluon-cv/tree/master/scripts/detection/ssd/train_ssd.py,,validate$223,"def validate(net, val_data, ctx, eval_metric):
    """"""Test on validation dataset.""""""
    eval_metric.reset()
    net.set_nms(nms_thresh=0.45, nms_topk=400)
    net.hybridize(static_alloc=True, static_shape=True)
    for batch in val_data:
        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)
        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)
        det_bboxes = []
        det_ids = []
        det_scores = []
        gt_bboxes = []
        gt_ids = []
        gt_difficults = []
        for (x, y) in zip(data, label):
            (ids, scores, bboxes) = net(x)
            det_ids.append(ids)
            det_scores.append(scores)
            det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
            gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
            gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
            gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)
        eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)
    return eval_metric.get()","for (x, y) in zip(data, label):
    (ids, scores, bboxes) = net(x)
    det_ids.append(ids)
    det_scores.append(scores)
    det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
    gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))
    gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))
    gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)","for (batch_x, batch_y) in zip(data, label):
    (ids, scores, bboxes) = net(batch_x)
    det_ids.append(ids)
    det_scores.append(scores)
    det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))
    gt_ids.append(batch_y.slice_axis(axis=-1, begin=4, end=5))
    gt_bboxes.append(batch_y.slice_axis(axis=-1, begin=0, end=4))
    gt_difficults.append(batch_y.slice_axis(axis=-1, begin=5, end=6) if batch_y.shape[-1] > 5 else None)",find_wrong,2
pyglet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglet/contrib/aseprite_codec/aseprite.py,https://github.com/pyglet/pyglet/tree/master/contrib/aseprite_codec/aseprite.py,Frame,_convert_to_rgba$193,"def _convert_to_rgba(self, cel):
    if self.color_depth == 8:
        global PALETTE_INDEX
        pixel_array = []
        for pixel in cel.pixel_data:
            if pixel == PALETTE_INDEX:
                pixel_array.extend([0, 0, 0, 0])
            else:
                pixel_array.extend(PALETTE_DICT[pixel])
        cel.pixel_data = bytes(pixel_array)
        return cel
    elif self.color_depth == 16:
        greyscale_iter = _chunked_iter(cel.pixel_data, 2)
        pixel_array = []
        for pixel in greyscale_iter:
            rgba = pixel[0] * 3 + pixel[1]
            pixel_array.append(rgba)
        cel.pixel_data = bytes(pixel_array)
        return cel
    else:
        return cel","if self.color_depth == 8:
    global PALETTE_INDEX
    pixel_array = []
    for pixel in cel.pixel_data:
        if pixel == PALETTE_INDEX:
            pixel_array.extend([0, 0, 0, 0])
        else:
            pixel_array.extend(PALETTE_DICT[pixel])
    cel.pixel_data = bytes(pixel_array)
    return cel
elif self.color_depth == 16:
    greyscale_iter = _chunked_iter(cel.pixel_data, 2)
    pixel_array = []
    for pixel in greyscale_iter:
        rgba = pixel[0] * 3 + pixel[1]
        pixel_array.append(rgba)
    cel.pixel_data = bytes(pixel_array)
    return cel","for (color_depth, chunk_size, palette_dict) in [(8, 1, PALETTE_DICT), (16, 2, None)]:
    if self.color_depth == color_depth:
        pixel_array = []
        if color_depth == 8:
            global PALETTE_INDEX
            for pixel in cel.pixel_data:
                if pixel == PALETTE_INDEX:
                    pixel_array.extend([0, 0, 0, 0])
                else:
                    pixel_array.extend(palette_dict[pixel])
        elif color_depth == 16:
            greyscale_iter = _chunked_iter(cel.pixel_data, chunk_size)
            for pixel in greyscale_iter:
                rgba = pixel[0] * 3 + pixel[1]
                pixel_array.append(rgba)
        cel.pixel_data = bytes(pixel_array)
        return cel",find_wrong,-1
Vxscan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Vxscan/analyzer.py,https://github.com/al0ne/Vxscan/tree/master//analyzer.py,,gener$97,"def gener():
    f = open('output.log', 'a', encoding='utf-8')
    webinfo = Sqldb(dbname).query('select domain,ipaddr,title,server,apps,waf,os from webinfo')
    for i in webinfo:
        (domain, ipaddr, title, server, apps, waf, os) = i
        print('\n' + '*' * 40 + ' ' + domain + ' ' + '*' * 40)
        f.write('\n' + '*' * 40 + ' ' + domain + ' ' + '*' * 40 + '\n')
        print('{}|{}|{}|{}|{}'.format(domain, ipaddr, title, server, waf))
        f.write('{}|{}|{}|{}|{}'.format(domain, ipaddr, title, server, waf) + '\n')
        print('' + str(apps))
        f.write('' + str(apps) + '\n')
        print('' + str(os))
        f.write('' + str(os) + '\n')
        ports = Sqldb(dbname).query(f""select ipaddr,service,port from ports where ipaddr = '{domain}'"")
        for port in ports:
            (domain, server, port) = port
            print(domain, server, port)
            f.write('{}\t{}\t{}'.format(domain, server, port) + '\n')
        urls = Sqldb(dbname).query(f""select title,url,contype,rsp_len,rsp_code from urls where domain = '{domain}'"")
        for url in urls:
            (title, url, contype, rsp_len, rsp_code) = url
            print('{}\t{}\t{}\t{}t{}'.format(title, url, contype, rsp_len, rsp_code))
            f.write('{}\t{}\t{}\t{}t{}'.format(title, url, contype, rsp_len, rsp_code) + '\n')
        vulns = Sqldb(dbname).query(f""select vuln from vuln where domain = '{ipaddr}'"")
        for vuln in vulns:
            print(vuln[0])
            f.write(vuln[0] + '\n')","ports = Sqldb(dbname).query(f""select ipaddr,service,port from ports where ipaddr = '{domain}'"")
for port in ports:
    (domain, server, port) = port
    print(domain, server, port)
    f.write('{}\t{}\t{}'.format(domain, server, port) + '\n')
urls = Sqldb(dbname).query(f""select title,url,contype,rsp_len,rsp_code from urls where domain = '{domain}'"")
for url in urls:
    (title, url, contype, rsp_len, rsp_code) = url
    print('{}\t{}\t{}\t{}t{}'.format(title, url, contype, rsp_len, rsp_code))
    f.write('{}\t{}\t{}\t{}t{}'.format(title, url, contype, rsp_len, rsp_code) + '\n')
vulns = Sqldb(dbname).query(f""select vuln from vuln where domain = '{ipaddr}'"")
for vuln in vulns:
    print(vuln[0])
    f.write(vuln[0] + '\n')","for (domain, server, port) in Sqldb(dbname).query(f""select ipaddr,service,port from ports where ipaddr = '{domain}'""):
    print(domain, server, port)
    f.write('{}\t{}\t{}'.format(domain, server, port) + '\n')
for (title, url, contype, rsp_len, rsp_code) in Sqldb(dbname).query(f""select title,url,contype,rsp_len,rsp_code from urls where domain = '{domain}'""):
    print('{}\t{}\t{}\t{}t{}'.format(title, url, contype, rsp_len, rsp_code))
    f.write('{}\t{}\t{}\t{}t{}'.format(title, url, contype, rsp_len, rsp_code) + '\n')
for vuln in Sqldb(dbname).query(f""select vuln from vuln where domain = '{ipaddr}'""):
    print(vuln[0])
    f.write(vuln[0] + '\n')",find_wrong,1
sktime,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sktime/sktime/benchmarking/evaluation.py,https://github.com/alan-turing-institute/sktime/tree/master/sktime/benchmarking/evaluation.py,Evaluator,wilcoxon_test$322,"def wilcoxon_test(self, metric_name=None):
    """"""Wilcoxon signed-rank test.

        http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test
        `Wilcoxon signed-rank test
        <https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test>`_.
        Tests whether two  related paired samples come from the same
        distribution. In particular, it tests whether the distribution of the
        differences x-y is symmetric about zero
        """"""
    self._check_is_evaluated()
    metric_name = self._validate_metric_name(metric_name)
    metrics_per_estimator_dataset = self._get_metrics_per_estimator_dataset(metric_name)
    wilcoxon_df = pd.DataFrame()
    prod = itertools.combinations(metrics_per_estimator_dataset.keys(), 2)
    for p in prod:
        estim_1 = p[0]
        estim_2 = p[1]
        (w, p_val) = stats.wilcoxon(metrics_per_estimator_dataset[p[0]], metrics_per_estimator_dataset[p[1]])
        w_test = {'estimator_1': estim_1, 'estimator_2': estim_2, 'statistic': w, 'p_val': p_val}
        wilcoxon_df = wilcoxon_df.append(w_test, ignore_index=True)
    return wilcoxon_df","prod = itertools.combinations(metrics_per_estimator_dataset.keys(), 2)
for p in prod:
    estim_1 = p[0]
    estim_2 = p[1]
    (w, p_val) = stats.wilcoxon(metrics_per_estimator_dataset[p[0]], metrics_per_estimator_dataset[p[1]])
    w_test = {'estimator_1': estim_1, 'estimator_2': estim_2, 'statistic': w, 'p_val': p_val}
    wilcoxon_df = wilcoxon_df.append(w_test, ignore_index=True)","for (estim_1, estim_2) in itertools.combinations(metrics_per_estimator_dataset.keys(), 2):
    (w, p_val) = stats.wilcoxon(metrics_per_estimator_dataset[estim_1], metrics_per_estimator_dataset[estim_2])
    w_test = {'estimator_1': estim_1, 'estimator_2': estim_2, 'statistic': w, 'p_val': p_val}
    wilcoxon_df = wilcoxon_df.append(w_test, ignore_index=True)",find_wrong,-1
dnsrecon,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dnsrecon/tools/parser.py,https://github.com/darkoperator/dnsrecon/tree/master/tools/parser.py,,extract_hostnames$177,"def extract_hostnames(file):
    host_names = []
    hostname_pattern = re.compile('(^[^.]*)')
    file_type = detect_type(file)
    if file_type == 'xml':
        for (event, elem) in cElementTree.iterparse(file):
            if elem.tag == 'record':
                if 'address' in elem.attrib:
                    if re.search('PTR|^[A]$|AAAA', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['name']).group(1))
                    elif re.search('NS', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['target']).group(1))
                    elif re.search('SOA', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['mname']).group(1))
                    elif re.search('MX', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['exchange']).group(1))
                    elif re.search('SRV', elem.attrib['type']):
                        host_names.append(re.search(hostname_pattern, elem.attrib['target']).group(1))
    elif file_type == 'csv':
        reader = csv.reader(open(file, 'r'), delimiter=',')
        reader.next()
        for row in reader:
            host_names.append(re.search(hostname_pattern, row[1]).group(1))
    host_names = list(set(host_names))
    return filter(None, host_names)","for (event, elem) in cElementTree.iterparse(file):
    if elem.tag == 'record':
        if 'address' in elem.attrib:
            if re.search('PTR|^[A]$|AAAA', elem.attrib['type']):
                host_names.append(re.search(hostname_pattern, elem.attrib['name']).group(1))
            elif re.search('NS', elem.attrib['type']):
                host_names.append(re.search(hostname_pattern, elem.attrib['target']).group(1))
            elif re.search('SOA', elem.attrib['type']):
                host_names.append(re.search(hostname_pattern, elem.attrib['mname']).group(1))
            elif re.search('MX', elem.attrib['type']):
                host_names.append(re.search(hostname_pattern, elem.attrib['exchange']).group(1))
            elif re.search('SRV', elem.attrib['type']):
                host_names.append(re.search(hostname_pattern, elem.attrib['target']).group(1))","for (event, elem) in cElementTree.iterparse(file):
    if elem.tag == 'record':
        if 'address' in elem.attrib:
            for record_type in ['PTR', 'A', 'AAAA']:
                if re.search(record_type, elem.attrib['type']):
                    host_names.append(re.search(hostname_pattern, elem.attrib['name']).group(1))
            if re.search('NS', elem.attrib['type']):
                host_names.append(re.search(hostname_pattern, elem.attrib['target']).group(1))
            elif re.search('SOA', elem.attrib['type']):
                host_names.append(re.search(hostname_pattern, elem.attrib['mname']).group(1))
            elif re.search('MX', elem.attrib['type']):
                host_names.append(re.search(hostname_pattern, elem.attrib['exchange']).group(1))
            elif re.search('SRV', elem.attrib['type']):
                host_names.append(re.search(hostname_pattern, elem.attrib['target']).group(1))",find_wrong,2
dcos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dcos/packages/dcos-integration-test/extra/test_metrics.py,https://github.com/dcos/dcos/tree/master/packages/dcos-integration-test/extra/test_metrics.py,,_check_calico_metrics$212,"def _check_calico_metrics() -> None:
    response = get_metrics_prom(dcos_api_session, node)
    for family in text_string_to_metric_families(response.text):
        for sample in family.samples:
            if sample[0].startswith('felix') and sample[1].get('dcos_component_name') == 'DC/OS Calico':
                return
    raise Exception('Expected DC/OS Calico felix* metric on agent nodes not found')","for family in text_string_to_metric_families(response.text):
    for sample in family.samples:
        if sample[0].startswith('felix') and sample[1].get('dcos_component_name') == 'DC/OS Calico':
            return","for (family, samples) in ((family, family.samples) for family in text_string_to_metric_families(response.text)):
    for sample in samples:
        if sample[0].startswith('felix') and sample[1].get('dcos_component_name') == 'DC/OS Calico':
            return",find_wrong,-1
coveragepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coveragepy/coverage/sqldata.py,https://github.com/nedbat/coveragepy/tree/master/coverage/sqldata.py,CoverageData,_read_db$269,"def _read_db(self):
    """"""Read the metadata from a database so that we are ready to use it.""""""
    with self._dbs[threading.get_ident()] as db:
        try:
            (schema_version,) = db.execute_one('select version from coverage_schema')
        except Exception as exc:
            if 'no such table: coverage_schema' in str(exc):
                self._init_db(db)
            else:
                raise DataError(""Data file {!r} doesn't seem to be a coverage data file: {}"".format(self._filename, exc)) from exc
        else:
            if schema_version != SCHEMA_VERSION:
                raise DataError(""Couldn't use data file {!r}: wrong schema: {} instead of {}"".format(self._filename, schema_version, SCHEMA_VERSION))
        with db.execute(""select value from meta where key = 'has_arcs'"") as cur:
            for row in cur:
                self._has_arcs = bool(int(row[0]))
                self._has_lines = not self._has_arcs
        with db.execute('select id, path from file') as cur:
            for (file_id, path) in cur:
                self._file_map[path] = file_id","with db.execute('select id, path from file') as cur:
    for (file_id, path) in cur:
        self._file_map[path] = file_id","for (file_id, path, *_) in db.execute('select id, path from file'):
    self._file_map[path] = file_id",find_wrong,1
HanLP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HanLP/hanlp/components/srl/span_rank/srl_eval_utils.py,https://github.com/hankcs/HanLP/tree/master/hanlp/components/srl/span_rank/srl_eval_utils.py,,compute_srl_f1$162,"def compute_srl_f1(sentences, gold_srl, predictions, gold_path=None) -> SRLScores:
    assert len(gold_srl) == len(predictions)
    total_gold = 0
    total_predicted = 0
    total_matched = 0
    total_unlabeled_matched = 0
    num_sents = 0
    label_confusions = Counter()
    for (gold, prediction) in zip(gold_srl, predictions):
        gold_rels = 0
        pred_rels = 0
        matched = 0
        for (pred_id, gold_args) in gold.items():
            filtered_gold_args = [a for a in gold_args if a[2] not in ['V', 'C-V']]
            total_gold += len(filtered_gold_args)
            gold_rels += len(filtered_gold_args)
            if pred_id not in prediction:
                continue
            for a0 in filtered_gold_args:
                for a1 in prediction[pred_id]:
                    if a0[0] == a1[0] and a0[1] == a1[1]:
                        total_unlabeled_matched += 1
                        label_confusions.update([(a0[2], a1[2])])
                        if a0[2] == a1[2]:
                            total_matched += 1
                            matched += 1
        for (pred_id, args) in prediction.items():
            filtered_args = [a for a in args if a[2] not in ['V']]
            total_predicted += len(filtered_args)
            pred_rels += len(filtered_args)
        if gold_rels == matched and pred_rels == matched:
            num_sents += 1
    (precision, recall, f1) = _calc_f1(total_gold, total_predicted, total_matched)
    (unlabeled_precision, unlabeled_recall, unlabeled_f1) = _calc_f1(total_gold, total_predicted, total_unlabeled_matched)
    if not gold_path:
        gold_path = tempfile.NamedTemporaryFile().name
        print_to_conll(sentences, gold_srl, gold_path, None)
        gold_predicates = None
    else:
        gold_predicates = read_gold_predicates(gold_path)
    temp_output = tempfile.NamedTemporaryFile().name
    print_to_conll(sentences, predictions, temp_output, gold_predicates)
    (conll_precision, conll_recall, conll_f1) = official_conll_05_evaluate(temp_output, gold_path)
    return SRLScores(unlabeled_precision, unlabeled_recall, unlabeled_f1, precision, recall, f1, conll_precision, conll_recall, conll_f1, label_confusions, num_sents)","for (gold, prediction) in zip(gold_srl, predictions):
    gold_rels = 0
    pred_rels = 0
    matched = 0
    for (pred_id, gold_args) in gold.items():
        filtered_gold_args = [a for a in gold_args if a[2] not in ['V', 'C-V']]
        total_gold += len(filtered_gold_args)
        gold_rels += len(filtered_gold_args)
        if pred_id not in prediction:
            continue
        for a0 in filtered_gold_args:
            for a1 in prediction[pred_id]:
                if a0[0] == a1[0] and a0[1] == a1[1]:
                    total_unlabeled_matched += 1
                    label_confusions.update([(a0[2], a1[2])])
                    if a0[2] == a1[2]:
                        total_matched += 1
                        matched += 1
    for (pred_id, args) in prediction.items():
        filtered_args = [a for a in args if a[2] not in ['V']]
        total_predicted += len(filtered_args)
        pred_rels += len(filtered_args)
    if gold_rels == matched and pred_rels == matched:
        num_sents += 1","for (gold, prediction) in zip(gold_srl, predictions):
    gold_rels = 0
    pred_rels = 0
    matched = 0
    for (pred_id, gold_args) in gold.items():
        filtered_gold_args = [a for a in gold_args if a[2] not in ['V', 'C-V']]
        total_gold += len(filtered_gold_args)
        gold_rels += len(filtered_gold_args)
        if pred_id not in prediction:
            continue
        for a0 in filtered_gold_args:
            for a1 in prediction[pred_id]:
                if a0[0] == a1[0] and a0[1] == a1[1]:
                    total_unlabeled_matched += 1
                    label_confusions.update([(a0[2], a1[2])])
                    if a0[2] == a1[2]:
                        total_matched += 1
                        matched += 1
        for a1 in prediction[pred_id]:
            if all((a0[0] != a1[0] or a0[1] != a1[1] for a0 in filtered_gold_args)):
                filtered_args = [a1 for a1 in prediction[pred_id] if a1[2] not in ['V']]
                total_predicted += len(filtered_args)
                pred_rels += len(filtered_args)
    if gold_rels == matched and pred_rels == matched:
        num_sents += 1",find_wrong,-1
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/op/smooth_uv.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/op/smooth_uv.py,MUV_OT_SmoothUV,__smooth_wo_transmission$107,"def __smooth_wo_transmission(self, loop_seqs, uv_layer):
    loops = []
    for hseq in loop_seqs:
        loops.extend([hseq[0][0], hseq[0][1]])
    full_vlen = 0
    accm_vlens = [0.0]
    full_uvlen = 0
    accm_uvlens = [0.0]
    orig_uvs = [loop_seqs[0][0][0][uv_layer].uv.copy()]
    for (l1, l2) in zip(loops[:-1], loops[1:]):
        diff_v = l2.vert.co - l1.vert.co
        full_vlen = full_vlen + diff_v.length
        accm_vlens.append(full_vlen)
        diff_uv = l2[uv_layer].uv - l1[uv_layer].uv
        full_uvlen = full_uvlen + diff_uv.length
        accm_uvlens.append(full_uvlen)
        orig_uvs.append(l2[uv_layer].uv.copy())
    for (hidx, hseq) in enumerate(loop_seqs):
        pair = hseq[0]
        for (pidx, l) in enumerate(pair):
            if self.select:
                l[uv_layer].select = True
            if hidx == 0 and pidx == 0 or (hidx == len(loop_seqs) - 1 and pidx == len(pair) - 1):
                continue
            tgt_noinfl = full_uvlen * (hidx + pidx) / len(loop_seqs)
            tgt_infl = full_uvlen * accm_vlens[hidx * 2 + pidx] / full_vlen
            target_length = tgt_noinfl * (1 - self.mesh_infl) + tgt_infl * self.mesh_infl
            for i in range(len(accm_uvlens[:-1])):
                if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
                    tgt_seg_len = target_length - accm_uvlens[i]
                    seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
                    uv1 = orig_uvs[i]
                    uv2 = orig_uvs[i + 1]
                    target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
                    break
            else:
                self.report({'ERROR'}, 'Failed to get target UV')
                return {'CANCELLED'}
            l[uv_layer].uv = target_uv","for i in range(len(accm_uvlens[:-1])):
    if accm_uvlens[i] <= target_length < accm_uvlens[i + 1]:
        tgt_seg_len = target_length - accm_uvlens[i]
        seg_len = accm_uvlens[i + 1] - accm_uvlens[i]
        uv1 = orig_uvs[i]
        uv2 = orig_uvs[i + 1]
        target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
        break
else:
    self.report({'ERROR'}, 'Failed to get target UV')
    return {'CANCELLED'}","for (i, (accm_uvlen_1, accm_uvlen_2)) in enumerate(zip(accm_uvlens[:-1], accm_uvlens[1:])):
    if accm_uvlen_1 <= target_length < accm_uvlen_2:
        tgt_seg_len = target_length - accm_uvlen_1
        seg_len = accm_uvlen_2 - accm_uvlen_1
        uv1 = orig_uvs[i]
        uv2 = orig_uvs[i + 1]
        target_uv = uv1 + (uv2 - uv1) * tgt_seg_len / seg_len
        break
else:
    self.report({'ERROR'}, 'Failed to get target UV')
    return {'CANCELLED'}",find_wrong,2
DeepNER,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepNER/src/utils/evaluator.py,https://github.com/z814081807/DeepNER/tree/master/src/utils/evaluator.py,,mrc_evaluation$240,"def mrc_evaluation(model, dev_info, device):
    (dev_loader, (dev_callback_info, type_weight)) = dev_info
    (start_logits, end_logits) = (None, None)
    model.eval()
    for tmp_pred in get_base_out(model, dev_loader, device):
        tmp_start_logits = tmp_pred[0].cpu().numpy()
        tmp_end_logits = tmp_pred[1].cpu().numpy()
        if start_logits is None:
            start_logits = tmp_start_logits
            end_logits = tmp_end_logits
        else:
            start_logits = np.append(start_logits, tmp_start_logits, axis=0)
            end_logits = np.append(end_logits, tmp_end_logits, axis=0)
    assert len(start_logits) == len(end_logits) == len(dev_callback_info)
    role_metric = np.zeros([13, 3])
    mirco_metrics = np.zeros(3)
    id2ent = {x: i for (i, x) in enumerate(ENTITY_TYPES)}
    for (tmp_start_logits, tmp_end_logits, tmp_callback) in zip(start_logits, end_logits, dev_callback_info):
        (text, text_offset, ent_type, gt_entities) = tmp_callback
        tmp_start_logits = tmp_start_logits[text_offset:text_offset + len(text)]
        tmp_end_logits = tmp_end_logits[text_offset:text_offset + len(text)]
        pred_entities = mrc_decode(tmp_start_logits, tmp_end_logits, text)
        role_metric[id2ent[ent_type]] += calculate_metric(gt_entities, pred_entities)
    for (idx, _type) in enumerate(ENTITY_TYPES):
        temp_metric = get_p_r_f(role_metric[idx][0], role_metric[idx][1], role_metric[idx][2])
        mirco_metrics += temp_metric * type_weight[_type]
    metric_str = f'[MIRCO] precision: {mirco_metrics[0]:.4f}, recall: {mirco_metrics[1]:.4f}, f1: {mirco_metrics[2]:.4f}'
    return (metric_str, mirco_metrics[2])","for (tmp_start_logits, tmp_end_logits, tmp_callback) in zip(start_logits, end_logits, dev_callback_info):
    (text, text_offset, ent_type, gt_entities) = tmp_callback
    tmp_start_logits = tmp_start_logits[text_offset:text_offset + len(text)]
    tmp_end_logits = tmp_end_logits[text_offset:text_offset + len(text)]
    pred_entities = mrc_decode(tmp_start_logits, tmp_end_logits, text)
    role_metric[id2ent[ent_type]] += calculate_metric(gt_entities, pred_entities)","for (i, (tmp_start_logits, tmp_end_logits, tmp_callback)) in enumerate(zip(start_logits, end_logits, dev_callback_info)):
    (text, text_offset, ent_type, gt_entities) = tmp_callback
    tmp_start_logits = tmp_start_logits[text_offset:text_offset + len(text)]
    tmp_end_logits = tmp_end_logits[text_offset:text_offset + len(text)]
    pred_entities = mrc_decode(tmp_start_logits, tmp_end_logits, text)
    role_metric[id2ent[ent_type]] += calculate_metric(gt_entities, pred_entities)",find_wrong,2
flow-forecast,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flow-forecast/flood_forecast/trainer.py,https://github.com/AIStream-Peelout/flow-forecast/tree/master/flood_forecast/trainer.py,,handle_model_evaluation1$16,"def handle_model_evaluation1(trained_model, params: Dict, model_type: str) -> None:
    """"""Utility function to help handle model evaluation. Primarily used at the moment for forcast

    :param trained_model: A PyTorchForecast model that has already been trained. 
    :type trained_model: PyTorchForecast
    :param params: A dictionary of the trained model parameters.
    :type params: Dict
    :param model_type: The type of model. Almost always PyTorch in practice.
    :type model_type: str
    """"""
    test_acc = evaluate_model(trained_model, model_type, params['dataset_params']['target_col'], params['metrics'], params['inference_params'], {})
    wandb.run.summary['test_accuracy'] = test_acc[0]
    df_train_and_test = test_acc[1]
    forecast_start_idx = test_acc[2]
    df_prediction_samples = test_acc[3]
    mae = (df_train_and_test.loc[forecast_start_idx:, 'preds'] - df_train_and_test.loc[forecast_start_idx:, params['dataset_params']['target_col'][0]]).abs()
    inverse_mae = 1 / mae
    i = 0
    for df in df_prediction_samples:
        pred_std = df.std(axis=1)
        average_prediction_sharpe = (inverse_mae / pred_std).mean()
        wandb.log({'average_prediction_sharpe' + str(i): average_prediction_sharpe})
        i += 1
    df_train_and_test.to_csv('temp_preds.csv')
    if 'probabilistic' in params['inference_params']:
        test_plot = plot_df_test_with_probabilistic_confidence_interval(df_train_and_test, forecast_start_idx, params)
    elif len(df_prediction_samples) > 0:
        for thing in zip(df_prediction_samples, params['dataset_params']['target_col']):
            thing[0].to_csv(thing[1] + '.csv')
            test_plot = plot_df_test_with_confidence_interval(df_train_and_test, thing[0], forecast_start_idx, params, targ_col=thing[1], ci=95, alpha=0.25)
            wandb.log({'test_plot_' + thing[1]: test_plot})
    else:
        pd.options.plotting.backend = 'plotly'
        t = params['dataset_params']['target_col'][0]
        test_plot = df_train_and_test[[t, 'preds']].plot()
        wandb.log({'test_plot_' + t: test_plot})
    print('Now plotting final plots')
    test_plot_all = go.Figure()
    for relevant_col in params['dataset_params']['relevant_cols']:
        test_plot_all.add_trace(go.Scatter(x=df_train_and_test.index, y=df_train_and_test[relevant_col], name=relevant_col))
    wandb.log({'test_plot_all': test_plot_all})","i = 0
for df in df_prediction_samples:
    pred_std = df.std(axis=1)
    average_prediction_sharpe = (inverse_mae / pred_std).mean()
    wandb.log({'average_prediction_sharpe' + str(i): average_prediction_sharpe})
    i += 1","for (i, df) in enumerate(df_prediction_samples):
    pred_std = df.std(axis=1)
    average_prediction_sharpe = (inverse_mae / pred_std).mean()
    wandb.log({'average_prediction_sharpe' + str(i): average_prediction_sharpe})",find_wrong,2
auto-editor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/auto-editor/auto_editor/__main__.py,https://github.com/WyattBlue/auto-editor/tree/master/auto_editor/__main__.py,,main$233,"def main():
    parser = vanparse.ArgumentParser('Auto-Editor', auto_editor.version, description='\nAuto-Editor is an automatic video/audio creator and editor. By default, it will detect silence and create a new video with those sections cut out. By changing some of the options, you can export to a traditional editor like Premiere Pro and adjust the edits there, adjust the pacing of the cuts, and change the method of editing like using audio loudness and video motion to judge making cuts.\nRun:\n    auto-editor --help\n\nTo get the list of options.\n')
    subcommands = ['create', 'test', 'info', 'levels', 'grep', 'subdump', 'desc']
    if len(sys.argv) > 1 and sys.argv[1] in subcommands:
        obj = __import__('auto_editor.subcommands.{}'.format(sys.argv[1]), fromlist=['subcommands'])
        obj.main(sys.argv[2:])
        sys.exit()
    else:
        parser = main_options(parser)
        args = parser.parse_args(sys.argv[1:], Log(), 'auto-editor')
    timer = Timer(args.quiet)
    exporting_to_editor = args.export_to_premiere or args.export_to_resolve or args.export_to_final_cut_pro or args.export_to_shotcut
    making_data_file = exporting_to_editor or args.export_as_json
    is64bit = '64-bit' if sys.maxsize > 2 ** 32 else '32-bit'
    ffmpeg = FFmpeg(args.ffmpeg_location, args.my_ffmpeg, args.show_ffmpeg_debug)
    if args.debug and args.input == []:
        import platform
        dirpath = os.path.dirname(os.path.realpath(__file__))
        print('Python Version: {} {}'.format(platform.python_version(), is64bit))
        print('Platform: {} {} {}'.format(platform.system(), platform.release(), platform.machine().lower()))
        print('Config File path: {}'.format(os.path.join(dirpath, 'config.txt')))
        print('FFmpeg path: {}'.format(ffmpeg.path))
        print('FFmpeg version: {}'.format(ffmpeg.version))
        print('Auto-Editor version {}'.format(auto_editor.version))
        sys.exit()
    if is64bit == '32-bit':
        Log().warning('You have the 32-bit version of Python, which may lead to memory crashes.')
    if args.version:
        print('Auto-Editor version {}'.format(auto_editor.version))
        sys.exit()
    if args.temp_dir is None:
        TEMP = tempfile.mkdtemp()
    else:
        TEMP = args.temp_dir
        if os.path.isfile(TEMP):
            Log().error('Temp directory cannot be an already existing file.')
        if os.path.isdir(TEMP):
            if len(os.listdir(TEMP)) != 0:
                Log().error('Temp directory should be empty!')
        else:
            os.mkdir(TEMP)
    log = Log(args.debug, args.quiet, temp=TEMP)
    log.debug('Temp Directory: {}'.format(TEMP))
    if args.input == []:
        log.error('You need to give auto-editor an input file or folder so it can do the work for you.')
    if [args.export_to_premiere, args.export_to_resolve, args.export_to_final_cut_pro, args.export_as_audio, args.export_to_shotcut, args.export_as_clip_sequence].count(True) > 1:
        log.error('You must choose only one export option.')
    if isinstance(args.frame_margin, str):
        try:
            if float(args.frame_margin) < 0:
                log.error('Frame margin cannot be negative.')
        except ValueError:
            log.error('Frame margin {}, is not valid.'.format(args.frame_margin))
    elif args.frame_margin < 0:
        log.error('Frame margin cannot be negative.')
    if args.constant_rate_factor != 'unset':
        if int(args.constant_rate_factor) < 0 or int(args.constant_rate_factor) > 51:
            log.error('Constant rate factor (crf) must be between 0-51.')
    if args.width < 1:
        log.error('motionOps --width cannot be less than 1.')
    if args.dilates < 0:
        log.error('motionOps --dilates cannot be less than 0')

    def write_starting_message(args):
        if args.export_to_premiere:
            return 'Exporting to Adobe Premiere Pro XML file.'
        if args.export_to_final_cut_pro:
            return 'Exporting to Final Cut Pro XML file.'
        if args.export_to_resolve:
            return 'Exporting to DaVinci Resolve XML file.'
        if args.export_to_shotcut:
            return 'Exporting to Shotcut XML Timeline file.'
        if args.export_as_audio:
            return 'Exporting as audio.'
        return 'Starting.'
    if not args.preview:
        log.conwrite(write_starting_message(args))
    if args.preview or args.export_as_clip_sequence or making_data_file:
        args.no_open = True
    if args.blur < 0:
        args.blur = 0
    if args.silent_speed <= 0 or args.silent_speed > 99999:
        args.silent_speed = 99999
    if args.video_speed <= 0 or args.video_speed > 99999:
        args.video_speed = 99999
    if args.output_file is None:
        args.output_file = []
    from auto_editor.validate_input import valid_input
    (input_list, segments) = valid_input(args.input, ffmpeg, args, log)
    if len(args.output_file) < len(input_list):
        for i in range(len(input_list) - len(args.output_file)):
            args.output_file.append(set_output_name(input_list[i], None, making_data_file, args))
    if args.combine_files:
        if exporting_to_editor:
            temp_file = 'combined.mp4'
        else:
            temp_file = os.path.join(TEMP, 'combined.mp4')
        cmd = []
        for fileref in input_list:
            cmd.extend(['-i', fileref])
        cmd.extend(['-filter_complex', '[0:v]concat=n={}:v=1:a=1'.format(len(input_list)), '-codec:v', 'h264', '-pix_fmt', 'yuv420p', '-strict', '-2', temp_file])
        ffmpeg.run(cmd)
        del cmd
        input_list = [temp_file]
    speeds = [args.silent_speed, args.video_speed]
    if args.cut_out != [] and 99999 not in speeds:
        speeds.append(99999)
    for item in args.set_speed_for_range:
        if item[0] not in speeds:
            speeds.append(float(item[0]))
    log.debug('Speeds: {}'.format(speeds))

    def main_loop(input_list, ffmpeg, args, speeds, segments, log):
        num_cuts = 0
        progress = ProgressBar(args.machine_readable_progress, args.no_progress)
        for (i, input_path) in enumerate(input_list):
            inp = ffmpeg.file_info(input_path)
            if len(input_list) > 1:
                log.conwrite('Working on {}'.format(inp.basename))
            (cuts, output_path) = edit_media(i, inp, ffmpeg, args, progress, speeds, segments[i], exporting_to_editor, making_data_file, TEMP, log)
            num_cuts += cuts
        if not args.preview and (not making_data_file):
            timer.stop()
        if not args.preview and making_data_file:
            time_save = usefulfunctions.human_readable_time(num_cuts * 30)
            s = 's' if num_cuts != 1 else ''
            log.print('Auto-Editor made {} cut{}, which would have taken about {} if edited manually.'.format(num_cuts, s, time_save))
        if not args.no_open:
            usefulfunctions.open_with_system_default(output_path, log)
    try:
        main_loop(input_list, ffmpeg, args, speeds, segments, log)
    except KeyboardInterrupt:
        log.error('Keyboard Interrupt')
    log.cleanup()","for (i, input_path) in enumerate(input_list):
    inp = ffmpeg.file_info(input_path)
    if len(input_list) > 1:
        log.conwrite('Working on {}'.format(inp.basename))
    (cuts, output_path) = edit_media(i, inp, ffmpeg, args, progress, speeds, segments[i], exporting_to_editor, making_data_file, TEMP, log)
    num_cuts += cuts","for (i, input_path) in enumerate(input_list):
    inp = ffmpeg.file_info(input_path)
    if len(input_list) > 1:
        log.conwrite('Working on {}'.format(inp.basename))
    (cuts, output_path) = edit_media(i, inp, ffmpeg, args, progress, speeds, segments[i], exporting_to_editor, making_data_file, TEMP, log)
    num_cuts += cuts
    if not args.preview and (not making_data_file):
        timer.stop()
    if not args.preview and making_data_file:
        time_save = usefulfunctions.human_readable_time(num_cuts * 30)
        s = 's' if num_cuts != 1 else ''
        log.print('Auto-Editor made {} cut{}, which would have taken about {} if edited manually.'.format(num_cuts, s, time_save))
    if not args.no_open:
        usefulfunctions.open_with_system_default(output_path, log)",find_wrong,2
OpenFermion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenFermion/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,,_reduce_terms$138,"def _reduce_terms(terms, stabilizer_list, manual_input, fixed_positions):
    """"""
    Perform the term reduction using stabilizer conditions.

    Auxiliary function to reduce_number_of_terms.

    Args:
        terms (QubitOperator): Operator the number of terms is to be reduced.
        stabilizer_list (list): List of the stabilizers as QubitOperators.
        manual_input (Boolean): Option to pass the list of fixed qubits
                                positions manually. Set to False by default.
        fixed_positions (list): (optional) List of fixed qubit positions.
                                Passing a list is only effective if
                                manual_input is True.
    Returns:
        even_newer_terms (QubitOperator): Updated operator with reduced terms.
        fixed_positions (list): Positions of qubits to be used for the
                                term reduction.
    Raises:
        StabilizerError: Trivial stabilizer (identity).
        StabilizerError: Stabilizer with complex coefficient.
    """"""
    if manual_input is False:
        fixed_positions = []
    for (i, _) in enumerate(stabilizer_list):
        selected_stab = list(stabilizer_list[0].terms)[0]
        if manual_input is False:
            for qubit_pauli in selected_stab:
                if qubit_pauli[0] not in fixed_positions:
                    fixed_positions += [qubit_pauli[0]]
                    fixed_op = qubit_pauli[1]
                    break
        else:
            for qubit_pauli in selected_stab:
                if qubit_pauli[0] == fixed_positions[i]:
                    fixed_op = qubit_pauli[1]
                    break
        if fixed_op in ['X', 'Z']:
            other_op = 'Y'
        else:
            other_op = 'X'
        new_terms = QubitOperator()
        for qubit_pauli in terms:
            new_terms += fix_single_term(qubit_pauli, fixed_positions[i], fixed_op, other_op, stabilizer_list[0])
        updated_stabilizers = []
        for update_stab in stabilizer_list[1:]:
            updated_stabilizers += [fix_single_term(update_stab, fixed_positions[i], fixed_op, other_op, stabilizer_list[0])]
        terms = new_terms
        stabilizer_list = updated_stabilizers
        check_stabilizer_linearity(stabilizer_list, msg='Linearly dependent stabilizers.')
        check_commuting_stabilizers(stabilizer_list, msg='Stabilizers anti-commute.')
    return (terms, fixed_positions)","for (i, _) in enumerate(stabilizer_list):
    selected_stab = list(stabilizer_list[0].terms)[0]
    if manual_input is False:
        for qubit_pauli in selected_stab:
            if qubit_pauli[0] not in fixed_positions:
                fixed_positions += [qubit_pauli[0]]
                fixed_op = qubit_pauli[1]
                break
    else:
        for qubit_pauli in selected_stab:
            if qubit_pauli[0] == fixed_positions[i]:
                fixed_op = qubit_pauli[1]
                break
    if fixed_op in ['X', 'Z']:
        other_op = 'Y'
    else:
        other_op = 'X'
    new_terms = QubitOperator()
    for qubit_pauli in terms:
        new_terms += fix_single_term(qubit_pauli, fixed_positions[i], fixed_op, other_op, stabilizer_list[0])
    updated_stabilizers = []
    for update_stab in stabilizer_list[1:]:
        updated_stabilizers += [fix_single_term(update_stab, fixed_positions[i], fixed_op, other_op, stabilizer_list[0])]
    terms = new_terms
    stabilizer_list = updated_stabilizers","for (i, selected_stab) in enumerate(stabilizer_list):
    if manual_input is False:
        for qubit_pauli in list(selected_stab.terms)[0]:
            if qubit_pauli[0] not in fixed_positions:
                fixed_positions += [qubit_pauli[0]]
                fixed_op = qubit_pauli[1]
                break
    else:
        for qubit_pauli in list(selected_stab.terms)[0]:
            if qubit_pauli[0] == fixed_positions[i]:
                fixed_op = qubit_pauli[1]
                break
    if fixed_op in ['X', 'Z']:
        other_op = 'Y'
    else:
        other_op = 'X'
    new_terms = QubitOperator()
    for qubit_pauli in terms:
        new_terms += fix_single_term(qubit_pauli, fixed_positions[i], fixed_op, other_op, selected_stab)
    updated_stabilizers = []
    for update_stab in stabilizer_list[1:]:
        updated_stabilizers += [fix_single_term(update_stab, fixed_positions[i], fixed_op, other_op, selected_stab)]
    terms = new_terms
    stabilizer_list = updated_stabilizers",find_wrong,-1
QRec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/data/rating.py,https://github.com/Coder-Yu/QRec/tree/master/data/rating.py,Rating,matrix$158,"def matrix(self):
    m = np.zeros((len(self.user), len(self.item)))
    for u in self.user:
        (k, v) = self.userRated(u)
        vec = np.zeros(len(self.item))
        for pair in zip(k, v):
            iid = self.item[pair[0]]
            vec[iid] = pair[1]
        m[self.user[u]] = vec
    return m","for u in self.user:
    (k, v) = self.userRated(u)
    vec = np.zeros(len(self.item))
    for pair in zip(k, v):
        iid = self.item[pair[0]]
        vec[iid] = pair[1]
    m[self.user[u]] = vec","for (i, u) in enumerate(self.user):
    (k, v) = self.userRated(u)
    vec = np.zeros(len(self.item))
    for pair in zip(k, v):
        iid = self.item[pair[0]]
        vec[iid] = pair[1]
    m[i] = vec",find_wrong,-1
textflint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/textflint/textflint/common/utils/overlap_templates.py,https://github.com/textflint/textflint/tree/master/textflint/common/utils/overlap_templates.py,,template_filler$205,"def template_filler(template_list):
    probs = []
    templates = []
    for template_pair in template_list:
        probs.append(template_pair[0])
        templates.append(template_pair[1])
    template_index = np.random.choice(range(len(templates)), p=probs)
    template_tuple = templates[template_index]
    template = template_tuple[0]
    hypothesis_template = template_tuple[1]
    template_tag = template_tuple[2]
    premise_list = []
    index_dict = {}
    for (index, element) in template:
        if element == 'VP':
            (vp, vp_parse) = generate_vp()
            premise_list.append(vp)
            index_dict[index] = vp
        elif element == 'RC':
            rc = generate_rc()
            premise_list.append(rc)
            index_dict[index] = rc
        elif 'vobj' in element:
            obj = random.choice(object_dict[index_dict[int(element[-1])]])
            premise_list.append(obj)
            index_dict[index] = obj
        elif isinstance(element, str):
            premise_list.append(element)
            index_dict[index] = element
        else:
            word = random.choice(element)
            premise_list.append(word)
            index_dict[index] = word
    hypothesis_list = [index_dict[ind] for ind in hypothesis_template]
    return (postprocess(' '.join(premise_list)), postprocess(' '.join(hypothesis_list)), template_tag)","for (index, element) in template:
    if element == 'VP':
        (vp, vp_parse) = generate_vp()
        premise_list.append(vp)
        index_dict[index] = vp
    elif element == 'RC':
        rc = generate_rc()
        premise_list.append(rc)
        index_dict[index] = rc
    elif 'vobj' in element:
        obj = random.choice(object_dict[index_dict[int(element[-1])]])
        premise_list.append(obj)
        index_dict[index] = obj
    elif isinstance(element, str):
        premise_list.append(element)
        index_dict[index] = element
    else:
        word = random.choice(element)
        premise_list.append(word)
        index_dict[index] = word
hypothesis_list = [index_dict[ind] for ind in hypothesis_template]","for (index, element) in template:
    if element == 'VP':
        (vp, vp_parse) = generate_vp()
        premise_list.append(vp)
        index_dict[index] = vp
    elif element == 'RC':
        rc = generate_rc()
        premise_list.append(rc)
        index_dict[index] = rc
    elif 'vobj' in element:
        obj = random.choice(object_dict[index_dict[int(element[-1])]])
        premise_list.append(obj)
        index_dict[index] = obj
    elif isinstance(element, str):
        premise_list.append(element)
        index_dict[index] = element
    else:
        word = random.choice(element)
        premise_list.append(word)
        index_dict[index] = word
hypothesis_list = [index_dict[ind] for ind in hypothesis_template]
return (postprocess(' '.join(premise_list)), postprocess(' '.join(hypothesis_list)), template_tag)",find_wrong,-1
graph4nlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/graph4nlp/graph4nlp/pytorch/modules/graph_construction/constituency_graph_construction.py,https://github.com/graph4ai/graph4nlp/tree/master/graph4nlp/pytorch/modules/graph_construction/constituency_graph_construction.py,ConstituencyBasedGraphConstruction,static_topology$72,"def static_topology(cls, raw_text_data, nlp_processor, processor_args, merge_strategy=None, edge_strategy=None, sequential_link=3, top_down=False, prune=2, verbose=True):
    """"""topology This function generate a graph strcuture from a raw text data.

        Parameters
        ----------
        raw_text_data : string
            A string to be used to construct a static graph, can be composed of multiple strings

        nlp_processor : object
            A parser used to parse sentence string to parsing trees like dependency parsing tree
            or constituency parsing tree

        merge_strategy : None or str, option=[None, ""tailhead"", ""user_define""]
            Strategy to merge sub-graphs into one graph
            ``None``: It will be the default option. We will do as ``""tailhead""``.
            ``""tailhead""``: Link the sub-graph  ``i``'s tail node with ``i+1``'s head node
            ``""user_define""``: We will give this option to the user. User can override the
                                method ``_graph_connnect`` to define your merge strategy.

        edge_strategy: None or str, option=[None, ""homogeneous"", ""heterogeneous"", ""as_node""]
            Strategy to process edge.
            ``None``: It will be the default option. We will do as ``""homogeneous""``.
            ``""homogeneous""``: We will drop the edge type information.
                               If there is a linkage among node ``i`` and node ``j``, we will
                               add an edge whose weight is ``1.0``. Otherwise there is no edge.
            ``heterogeneous``: We will keep the edge type information.
                               An edge will have type information like ``n_subj``.
                               It is not implemented yet.
            ``as_node``: We will view the edge as a graph node.
                         If there is an edge whose type is ``k`` between node ``i`` and node ``j``,
                         we will insert a node ``k`` into the graph and link
                         node (``i``, ``k``) and (``k``, ``j``). It is not implemented yet.

        sequential_link : int, option=[0,1,2,3]
            Strategy to add sequential links between word nodes.
            ``0``: Do not add sequential links.
            ``1``: Add unidirectional links.
            ``2``: Add bidirectional links.
            ``3``: Do not add sequential links inside each sentence and add bidirectional links
                   between adjacent sentences.

        top_down : bool
            If true, edges in constituency tree are from root nodes to leaf nodes. Otherwise,
            from leaf nodes to root nodes.

        prune : int, option=[0,1,2]
            Strategies for pruning constituency trees
            ``0``: No pruning.
            ``1``: Prune pos nodes.
            ``2``: Prune nodes with both in-degree and out-degree of 1.

        verbose : bool
            A boolean option to decide whether to print out the graph construction process.

        Returns
        -------
        GraphData
            A customized graph data structure
        """"""
    output_graph_list = []
    parsed_output = cls.parsing(raw_text_data, nlp_processor, processor_args)
    if prune == 0:
        add_pos_nodes = True
        cut_line_node = False
    elif prune == 1:
        add_pos_nodes = False
        cut_line_node = False
    elif prune == 2:
        add_pos_nodes = False
        cut_line_node = True
    else:
        raise ValueError('``prune`` should be chosen from [0,1,2].')
    if sequential_link == 0:
        seq_link = False
        biseq_link = False
    elif sequential_link == 1:
        seq_link = True
        biseq_link = False
    elif sequential_link == 2:
        seq_link = True
        biseq_link = True
    elif sequential_link == 3:
        seq_link = False
        biseq_link = True
    else:
        raise ValueError('``sequential_link`` should be chosen from [0,1,2,3].')
    if cut_line_node and sequential_link == 1:
        raise ValueError('``cut_line_node`` should not be used when edges                 between word nodes are unidirectional.')
    for index in range(len(parsed_output)):
        output_graph_list.append(cls._construct_static_graph(parsed_output[index], index, sequential_link=seq_link, bisequential_link=biseq_link, top_down=top_down, add_pos_node=add_pos_nodes, cut_line_node=cut_line_node))
    ret_graph = cls._graph_connect(output_graph_list, bisequential_link=biseq_link)
    if verbose:
        print('--------------------------------------')
        for _edge in ret_graph.get_all_edges():
            print(ret_graph.node_attributes[_edge[0]]['token'], '\t---\t', ret_graph.node_attributes[_edge[1]]['token'])
        print('--------------------------------------')
    return ret_graph","for index in range(len(parsed_output)):
    output_graph_list.append(cls._construct_static_graph(parsed_output[index], index, sequential_link=seq_link, bisequential_link=biseq_link, top_down=top_down, add_pos_node=add_pos_nodes, cut_line_node=cut_line_node))
ret_graph = cls._graph_connect(output_graph_list, bisequential_link=biseq_link)","for (index, parsed_output_item) in enumerate(parsed_output):
    output_graph_list.append(cls._construct_static_graph(parsed_output_item, index, sequential_link=seq_link, bisequential_link=biseq_link, top_down=top_down, add_pos_node=add_pos_nodes, cut_line_node=cut_line_node))
ret_graph = cls._graph_connect(output_graph_list, bisequential_link=biseq_link)",find_wrong,2
django-js-reverse,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-js-reverse/django_js_reverse/core.py,https://github.com/ierror/django-js-reverse/tree/master/django_js_reverse/core.py,,prepare_url_list$37,"def prepare_url_list(urlresolver, namespace_path='', namespace=''):
    """"""
    returns list of tuples [(<url_name>, <url_patern_tuple> ), ...]
    """"""
    exclude_ns = getattr(settings, 'JS_REVERSE_EXCLUDE_NAMESPACES', JS_EXCLUDE_NAMESPACES)
    include_only_ns = getattr(settings, 'JS_REVERSE_INCLUDE_ONLY_NAMESPACES', JS_INCLUDE_ONLY_NAMESPACES)
    if exclude_ns and include_only_ns:
        raise ImproperlyConfigured('Neither use JS_REVERSE_EXCLUDE_NAMESPACES nor JS_REVERSE_INCLUDE_ONLY_NAMESPACES setting')
    if namespace[:-1] in exclude_ns:
        return
    include_only_allow = True
    if include_only_ns != []:
        in_on_empty_ns = False
        in_on_is_in_list = False
        in_on_null = False
        if namespace == '' and '' in include_only_ns:
            in_on_empty_ns = True
        for ns in include_only_ns:
            if ns != '' and namespace[:-1].startswith(ns):
                in_on_is_in_list = True
                break
        if namespace[:-1] + '\x00' in include_only_ns:
            in_on_null = True
        include_only_allow = in_on_empty_ns or in_on_is_in_list or in_on_null
    if include_only_allow:
        for url_name in urlresolver.reverse_dict.keys():
            if isinstance(url_name, (text_type, str)):
                url_patterns = []
                for url_pattern in urlresolver.reverse_dict.getlist(url_name):
                    url_patterns += [[namespace_path + pat[0], pat[1]] for pat in url_pattern[0]]
                yield [namespace + url_name, url_patterns]
    for (inner_ns, (inner_ns_path, inner_urlresolver)) in urlresolver.namespace_dict.items():
        inner_ns_path = namespace_path + inner_ns_path
        inner_ns = namespace + inner_ns + ':'
        if inner_ns_path:
            args = [inner_ns_path, inner_urlresolver]
            if LooseVersion(django.get_version()) >= LooseVersion('2.0.6'):
                args.append(tuple(urlresolver.pattern.converters.items()))
            inner_urlresolver = urlresolvers.get_ns_resolver(*args)
            inner_ns_path = ''
        for x in prepare_url_list(inner_urlresolver, inner_ns_path, inner_ns):
            yield x","for (inner_ns, (inner_ns_path, inner_urlresolver)) in urlresolver.namespace_dict.items():
    inner_ns_path = namespace_path + inner_ns_path
    inner_ns = namespace + inner_ns + ':'
    if inner_ns_path:
        args = [inner_ns_path, inner_urlresolver]
        if LooseVersion(django.get_version()) >= LooseVersion('2.0.6'):
            args.append(tuple(urlresolver.pattern.converters.items()))
        inner_urlresolver = urlresolvers.get_ns_resolver(*args)
        inner_ns_path = ''
    for x in prepare_url_list(inner_urlresolver, inner_ns_path, inner_ns):
        yield x","for (inner_ns, (inner_ns_path, inner_urlresolver)) in urlresolver.namespace_dict.items():
    inner_ns_path = namespace_path + inner_ns_path
    inner_ns = namespace + inner_ns + ':'
    if inner_ns_path:
        args = [inner_ns_path, inner_urlresolver]
        if LooseVersion(django.get_version()) >= LooseVersion('2.0.6'):
            args.append(tuple(urlresolver.pattern.converters.items()))
        inner_urlresolver = urlresolvers.get_ns_resolver(*args)
        inner_ns_path = ''
    yield from prepare_url_list(inner_urlresolver, inner_ns_path, inner_ns)",find_wrong,2
sphinx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sphinx/sphinx/writers/latex.py,https://github.com/sphinx-doc/sphinx/tree/master/sphinx/writers/latex.py,LaTeXTranslator,generate_indices$474,"def generate_indices(self) -> str:

    def generate(content: List[Tuple[str, List[IndexEntry]]], collapsed: bool) -> None:
        ret.append('\\begin{sphinxtheindex}' + CR)
        ret.append('\\let\\bigletter\\sphinxstyleindexlettergroup' + CR)
        for (i, (letter, entries)) in enumerate(content):
            if i > 0:
                ret.append('\\indexspace' + CR)
            ret.append('\\bigletter{%s}' % self.escape(letter) + CR)
            for entry in entries:
                if not entry[3]:
                    continue
                ret.append('\\item\\relax\\sphinxstyleindexentry{%s}' % self.encode(entry[0]))
                if entry[4]:
                    ret.append('\\sphinxstyleindexextra{%s}' % self.encode(entry[4]))
                ret.append('\\sphinxstyleindexpageref{%s:%s}' % (entry[2], self.idescape(entry[3])) + CR)
        ret.append('\\end{sphinxtheindex}' + CR)
    ret = []
    indices_config = self.config.latex_domain_indices
    if indices_config:
        for domain in self.builder.env.domains.values():
            for indexcls in domain.indices:
                indexname = '%s-%s' % (domain.name, indexcls.name)
                if isinstance(indices_config, list):
                    if indexname not in indices_config:
                        continue
                (content, collapsed) = indexcls(domain).generate(self.builder.docnames)
                if not content:
                    continue
                ret.append('\\renewcommand{\\indexname}{%s}' % indexcls.localname + CR)
                generate(content, collapsed)
    return ''.join(ret)","for (i, (letter, entries)) in enumerate(content):
    if i > 0:
        ret.append('\\indexspace' + CR)
    ret.append('\\bigletter{%s}' % self.escape(letter) + CR)
    for entry in entries:
        if not entry[3]:
            continue
        ret.append('\\item\\relax\\sphinxstyleindexentry{%s}' % self.encode(entry[0]))
        if entry[4]:
            ret.append('\\sphinxstyleindexextra{%s}' % self.encode(entry[4]))
        ret.append('\\sphinxstyleindexpageref{%s:%s}' % (entry[2], self.idescape(entry[3])) + CR)","for (letter, entries) in content:
    ret.append('\\bigletter{%s}' % self.escape(letter) + CR)
    for entry in entries:
        if not entry[3]:
            continue
        ret.append('\\item\\relax\\sphinxstyleindexentry{%s}' % self.encode(entry[0]))
        if entry[4]:
            ret.append('\\sphinxstyleindexextra{%s}' % self.encode(entry[4]))
        ret.append('\\sphinxstyleindexpageref{%s:%s}' % (entry[2], self.idescape(entry[3])) + CR)
    ret.append('\\indexspace' + CR)",find_wrong,-1
mmcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmcv/tests/test_ops/test_focal_loss.py,https://github.com/open-mmlab/mmcv/tree/master/tests/test_ops/test_focal_loss.py,Testfocalloss,_test_grad_softmax$80,"def _test_grad_softmax(self, dtype=torch.float):
    if not torch.cuda.is_available():
        return
    from mmcv.ops import SoftmaxFocalLoss
    alpha = 0.25
    gamma = 2.0
    for case in inputs:
        np_x = np.array(case[0])
        np_y = np.array(case[1])
        x = torch.from_numpy(np_x).cuda().type(dtype)
        x.requires_grad_()
        y = torch.from_numpy(np_y).cuda().long()
        floss = SoftmaxFocalLoss(gamma, alpha)
        if _USING_PARROTS:
            pass
        else:
            gradcheck(floss, (x, y), eps=0.01, atol=0.01)","for case in inputs:
    np_x = np.array(case[0])
    np_y = np.array(case[1])
    x = torch.from_numpy(np_x).cuda().type(dtype)
    x.requires_grad_()
    y = torch.from_numpy(np_y).cuda().long()","for (np_x, np_y) in inputs:
    x = torch.from_numpy(np_x).cuda().type(dtype)
    x.requires_grad_()
    y = torch.from_numpy(np_y).cuda().long()",find_wrong,-1
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/tools/check_op_desc.py,https://github.com/PaddlePaddle/Paddle/tree/master/tools/check_op_desc.py,,print_version_error_message$427,"def print_version_error_message(error_message):
    print('\n======================= \nOperator registration error for the changes of Inputs/Outputs/Attrs of OPs:\n')
    for op_name in error_message:
        print(""For OP '{}':"".format(op_name))
        inputs_error = error_message.get(op_name, {}).get(INPUTS, {})
        error_list = inputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print("" * The added input '{}' is not yet registered."".format(tup[1]))
        outputs_error = error_message.get(op_name, {}).get(OUTPUTS, {})
        error_list = outputs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print("" * The added output '{}' is not yet registered."".format(tup[1]))
        attrs_error = error_message.get(op_name, {}).get(ATTRS, {})
        error_list = attrs_error.get(ADD, [])
        if error_list:
            for tup in error_list:
                print("" * The added attribute '{}' is not yet registered."".format(tup[1]))
        error_dic = error_message.get(op_name, {}).get(ATTRS, {}).get(CHANGE, {})
        for (key, val) in error_dic.items():
            print("" * The change of attribute '{}' is not yet registered."".format(key))","for op_name in error_message:
    print(""For OP '{}':"".format(op_name))
    inputs_error = error_message.get(op_name, {}).get(INPUTS, {})
    error_list = inputs_error.get(ADD, [])
    if error_list:
        for tup in error_list:
            print("" * The added input '{}' is not yet registered."".format(tup[1]))
    outputs_error = error_message.get(op_name, {}).get(OUTPUTS, {})
    error_list = outputs_error.get(ADD, [])
    if error_list:
        for tup in error_list:
            print("" * The added output '{}' is not yet registered."".format(tup[1]))
    attrs_error = error_message.get(op_name, {}).get(ATTRS, {})
    error_list = attrs_error.get(ADD, [])
    if error_list:
        for tup in error_list:
            print("" * The added attribute '{}' is not yet registered."".format(tup[1]))
    error_dic = error_message.get(op_name, {}).get(ATTRS, {}).get(CHANGE, {})
    for (key, val) in error_dic.items():
        print("" * The change of attribute '{}' is not yet registered."".format(key))","for (op_name, op_error) in error_message.items():
    print(""For OP '{}':"".format(op_name))
    inputs_error = op_error.get(INPUTS, {}).get(ADD, [])
    for tup in inputs_error:
        print("" * The added input '{}' is not yet registered."".format(tup[1]))
    outputs_error = op_error.get(OUTPUTS, {}).get(ADD, [])
    for tup in outputs_error:
        print("" * The added output '{}' is not yet registered."".format(tup[1]))
    attrs_error = op_error.get(ATTRS, {}).get(ADD, [])
    for tup in attrs_error:
        print("" * The added attribute '{}' is not yet registered."".format(tup[1]))
    error_dic = op_error.get(ATTRS, {}).get(CHANGE, {})
    for (key, val) in error_dic.items():
        print("" * The change of attribute '{}' is not yet registered."".format(key))",find_wrong,-1
LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/pedestrian_detection/data_provider_farm/reformat_caltech.py,https://github.com/YonghaoHe/LFFD-A-Light-and-Fast-Face-Detector-for-Edge-Devices/tree/master/pedestrian_detection/data_provider_farm/reformat_caltech.py,,generate_data_list$8,"def generate_data_list():
    annotation_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_new_annotations/anno_test_1xnew'
    image_root = '/media/heyonghao/HYH-4T-WD/public_dataset/Caltech/Caltech_data/extracted_data'
    list_file_path = './data_folder/data_list_caltech_test.txt'
    if not os.path.exists(os.path.dirname(list_file_path)):
        os.makedirs(os.path.dirname(list_file_path))
    fout = open(list_file_path, 'w')
    counter = 0
    for (parent, dirnames, filenames) in os.walk(annotation_root):
        for filename in filenames:
            if not filename.endswith('.txt'):
                continue
            filename_splits = filename[:-4].split('_')
            set_name = filename_splits[0]
            seq_name = filename_splits[1]
            img_name = filename_splits[2]
            img_path = os.path.join(image_root, set_name, seq_name, 'images', img_name)
            if not os.path.exists(img_path):
                print('The corresponding image does not exist! [%s]' % img_path)
                continue
            line = img_path
            fin_anno = open(os.path.join(parent, filename), 'r')
            bbox_list = []
            for (i, anno) in enumerate(fin_anno):
                if i == 0:
                    continue
                anno = anno.strip('\n').split(' ')
                if anno[0] != 'person':
                    continue
                x = math.floor(float(anno[1]))
                y = math.floor(float(anno[2]))
                width = math.ceil(float(anno[3]))
                height = math.ceil(float(anno[4]))
                width_vis = math.ceil(float(anno[8]))
                height_vis = math.ceil(float(anno[9]))
                if width_vis * height_vis / (width * height) < 0.2:
                    continue
                bbox_list.append((x, y, width, height))
            if len(bbox_list) == 0:
                line += ',0,0'
                fout.write(line + '\n')
            else:
                bbox_line = ''
                for bbox in bbox_list:
                    bbox_line += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
                line += ',1,' + str(len(bbox_list)) + bbox_line
                fout.write(line + '\n')
            counter += 1
            print(counter)
    fout.close()","for (parent, dirnames, filenames) in os.walk(annotation_root):
    for filename in filenames:
        if not filename.endswith('.txt'):
            continue
        filename_splits = filename[:-4].split('_')
        set_name = filename_splits[0]
        seq_name = filename_splits[1]
        img_name = filename_splits[2]
        img_path = os.path.join(image_root, set_name, seq_name, 'images', img_name)
        if not os.path.exists(img_path):
            print('The corresponding image does not exist! [%s]' % img_path)
            continue
        line = img_path
        fin_anno = open(os.path.join(parent, filename), 'r')
        bbox_list = []
        for (i, anno) in enumerate(fin_anno):
            if i == 0:
                continue
            anno = anno.strip('\n').split(' ')
            if anno[0] != 'person':
                continue
            x = math.floor(float(anno[1]))
            y = math.floor(float(anno[2]))
            width = math.ceil(float(anno[3]))
            height = math.ceil(float(anno[4]))
            width_vis = math.ceil(float(anno[8]))
            height_vis = math.ceil(float(anno[9]))
            if width_vis * height_vis / (width * height) < 0.2:
                continue
            bbox_list.append((x, y, width, height))
        if len(bbox_list) == 0:
            line += ',0,0'
            fout.write(line + '\n')
        else:
            bbox_line = ''
            for bbox in bbox_list:
                bbox_line += ',' + str(bbox[0]) + ',' + str(bbox[1]) + ',' + str(bbox[2]) + ',' + str(bbox[3])
            line += ',1,' + str(len(bbox_list)) + bbox_line
            fout.write(line + '\n')
        counter += 1
        print(counter)","for (parent, dirnames, filenames) in os.walk(annotation_root):
    for filename in [f for f in filenames if f.endswith('.txt')]:
        filename_splits = filename[:-4].split('_')
        (set_name, seq_name, img_name) = filename_splits
        img_path = os.path.join(image_root, set_name, seq_name, 'images', img_name)
        if not os.path.exists(img_path):
            print(f'The corresponding image does not exist! [{img_path}]')
            continue
        line = img_path
        with open(os.path.join(parent, filename), 'r') as fin_anno:
            bbox_list = []
            for (i, anno) in enumerate(fin_anno):
                if i == 0:
                    continue
                anno = anno.strip('\n').split(' ')
                if anno[0] != 'person':
                    continue
                x = math.floor(float(anno[1]))
                y = math.floor(float(anno[2]))
                width = math.ceil(float(anno[3]))
                height = math.ceil(float(anno[4]))
                width_vis = math.ceil(float(anno[8]))
                height_vis = math.ceil(float(anno[9]))
                if width_vis * height_vis / (width * height) < 0.2:
                    continue
                bbox_list.append((x, y, width, height))
            if len(bbox_list) == 0:
                line += ',0,0'
                fout.write(line + '\n')
            else:
                bbox_line = ''
                for bbox in bbox_list:
                    bbox_line += f',{bbox[0]},{bbox[1]},{bbox[2]},{bbox[3]}'
                line += f',1,{len(bbox_list)}{bbox_line}'
                fout.write(line + '\n')
            counter += 1
            print(counter)",find_wrong,-2
takeover,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/takeover/takeover.py,https://github.com/m4ll0k/takeover/tree/master//takeover.py,,find$158,"def find(status, content, ok):
    for service in services:
        for values in services[service].items():
            if re.findall(str(values[1]), str(content), re.I) and int(status) in range(201 if ok is False else 200, 599):
                return (str(service), str(values[1]))","for service in services:
    for values in services[service].items():
        if re.findall(str(values[1]), str(content), re.I) and int(status) in range(201 if ok is False else 200, 599):
            return (str(service), str(values[1]))","for (service, items) in services.items():
    for (key, value) in items.items():
        if re.findall(str(value), str(content), re.I) and int(status) in range(201 if ok is False else 200, 599):
            return (str(service), str(value))",find_wrong,-1
nicotine-plus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nicotine-plus/pynicotine/gtkgui/userbrowse.py,https://github.com/nicotine-plus/nicotine-plus/tree/master/pynicotine/gtkgui/userbrowse.py,UserBrowse,download_directory$557,"def download_directory(self, folder, prefix='', recurse=False):
    if folder is None:
        return
    self.frame.np.transfers.requested_folders[self.user][folder] = prefix
    destination = self.frame.np.transfers.get_folder_destination(self.user, folder)
    files = self.shares.get(folder)
    if files:
        if config.sections['transfers']['reverseorder']:
            files.sort(key=lambda x: x[1], reverse=True)
        for file_data in files:
            virtualpath = '\\'.join([folder, file_data[1]])
            size = file_data[2]
            (h_bitrate, bitrate, h_length, length) = get_result_bitrate_length(size, file_data[4])
            self.frame.np.transfers.get_file(self.user, virtualpath, destination, size=size, bitrate=h_bitrate, length=h_length)
    if not recurse:
        return
    for (subdir, subf) in self.shares.items():
        if folder in subdir and folder != subdir:
            self.download_directory(subdir, prefix=os.path.join(destination, ''))","for (subdir, subf) in self.shares.items():
    if folder in subdir and folder != subdir:
        self.download_directory(subdir, prefix=os.path.join(destination, ''))","for (subdir, subf) in self.shares.items():
    if folder in subdir and folder != subdir:
        self.download_directory(subdir, prefix=os.path.join(destination, ''), recurse=True)",find_wrong,2
workalendar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/workalendar/workalendar/asia/china.py,https://github.com/workalendar/workalendar/tree/master/workalendar/asia/china.py,China,__init__$97,"def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.extra_working_days = []
    for (year, data) in workdays.items():
        for (holiday_name, day_list) in data.items():
            for v in day_list:
                self.extra_working_days.append(date(year, v[0], v[1]))","for (year, data) in workdays.items():
    for (holiday_name, day_list) in data.items():
        for v in day_list:
            self.extra_working_days.append(date(year, v[0], v[1]))","for (year, data) in workdays.items():
    for (holiday_name, day_list) in data.items():
        self.extra_working_days.extend((date(year, *v) for v in day_list))",find_wrong,-1
cloudtracker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloudtracker/cloudtracker/datasources/athena.py,https://github.com/duo-labs/cloudtracker/tree/master/cloudtracker/datasources/athena.py,Athena,__init__$164,"def __init__(self, config, account, start, end, args):
    logging.getLogger('botocore').setLevel(logging.WARN)
    logging.info('Source of CloudTrail logs: s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    if (datetime.datetime.now() - datetime.datetime.strptime(start, '%Y-%m-%d')).days > 365:
        raise Exception('Start date is over a year old. CloudTracker does not create or use partitions over a year old.')
    month_restrictions = set()
    start = start.split('-')
    end = end.split('-')
    if start[0] == end[0]:
        for month in range(int(start[1]), int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
    else:
        for month in range(int(start[1]), 12 + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(start[0], month))
        for year in range(int(start[0]), int(end[0])):
            for month in (1, 12 + 1):
                month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(year, month))
        for month in range(1, int(end[1]) + 1):
            month_restrictions.add(""(year = '{:0>2}' and month = '{:0>2}')"".format(end[0], month))
    self.search_filter = '((' + ' or '.join(month_restrictions) + ') and errorcode IS NULL)'
    self.table_name = 'cloudtrail_logs_{}'.format(account['id'])
    sts = boto3.client('sts')
    identity = sts.get_caller_identity()
    logging.info('Using AWS identity: {}'.format(identity['Arn']))
    current_account_id = identity['Account']
    region = boto3.session.Session().region_name
    if 'output_s3_bucket' in config:
        self.output_bucket = config['output_s3_bucket']
    else:
        self.output_bucket = 's3://aws-athena-query-results-{}-{}'.format(current_account_id, region)
    logging.info('Using output bucket: {}'.format(self.output_bucket))
    if 'workgroup' in config:
        self.workgroup = config['workgroup']
    logging.info('Using workgroup: {}'.format(self.workgroup))
    if not config.get('org_id'):
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], account_id=account['id'])
    else:
        cloudtrail_log_path = 's3://{bucket}/{path}/AWSLogs/{org_id}/{account_id}/CloudTrail'.format(bucket=config['s3_bucket'], path=config['path'], org_id=config['org_id'], account_id=account['id'])
    logging.info('Account cloudtrail log path: {}'.format(cloudtrail_log_path))
    self.athena = boto3.client('athena')
    self.s3 = boto3.client('s3')
    if args.skip_setup:
        logging.info('Skipping initial table creation')
        return
    resp = self.s3.list_objects_v2(Bucket=config['s3_bucket'], Prefix=config['path'], MaxKeys=1)
    if 'Contents' not in resp or len(resp['Contents']) == 0:
        exit('ERROR: S3 bucket has no contents.  Ensure you have logs at s3://{bucket}/{path}'.format(bucket=config['s3_bucket'], path=config['path']))
    self.query_athena('CREATE DATABASE IF NOT EXISTS {db} {comment}'.format(db=self.database, comment=""COMMENT 'Created by CloudTracker'""), context=None)
    query = ""CREATE EXTERNAL TABLE IF NOT EXISTS `{table_name}` (\n            `eventversion` string COMMENT 'from deserializer', \n            `useridentity` struct<type:string,principalid:string,arn:string,accountid:string,invokedby:string,accesskeyid:string,username:string,sessioncontext:struct<attributes:struct<mfaauthenticated:string,creationdate:string>,sessionissuer:struct<type:string,principalid:string,arn:string,accountid:string,username:string>>> COMMENT 'from deserializer', \n            `eventtime` string COMMENT 'from deserializer', \n            `eventsource` string COMMENT 'from deserializer', \n            `eventname` string COMMENT 'from deserializer', \n            `awsregion` string COMMENT 'from deserializer', \n            `sourceipaddress` string COMMENT 'from deserializer', \n            `useragent` string COMMENT 'from deserializer', \n            `errorcode` string COMMENT 'from deserializer', \n            `errormessage` string COMMENT 'from deserializer', \n            `requestparameters` string COMMENT 'from deserializer', \n            `responseelements` string COMMENT 'from deserializer', \n            `additionaleventdata` string COMMENT 'from deserializer', \n            `requestid` string COMMENT 'from deserializer', \n            `eventid` string COMMENT 'from deserializer', \n            `resources` array<struct<arn:string,accountid:string,type:string>> COMMENT 'from deserializer', \n            `eventtype` string COMMENT 'from deserializer', \n            `apiversion` string COMMENT 'from deserializer', \n            `readonly` string COMMENT 'from deserializer', \n            `recipientaccountid` string COMMENT 'from deserializer', \n            `serviceeventdetails` string COMMENT 'from deserializer', \n            `sharedeventid` string COMMENT 'from deserializer', \n            `vpcendpointid` string COMMENT 'from deserializer')\n            PARTITIONED BY (region string, year string, month string)\n            ROW FORMAT SERDE \n            'com.amazon.emr.hive.serde.CloudTrailSerde' \n            STORED AS INPUTFORMAT \n            'com.amazon.emr.cloudtrail.CloudTrailInputFormat' \n            OUTPUTFORMAT \n            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n            LOCATION '{cloudtrail_log_path}'"".format(table_name=self.table_name, cloudtrail_log_path=cloudtrail_log_path)
    self.query_athena(query)
    logging.info('Checking if all partitions for the past {} months exist'.format(NUM_MONTHS_FOR_PARTITIONS))
    query = 'SHOW PARTITIONS {table_name}'.format(table_name=self.table_name)
    partition_list = self.query_athena(query, skip_header=False)
    partition_set = set()
    for partition in partition_list:
        partition_set.add(partition[0])
    regions = boto3.session.Session().get_available_regions('ec2')
    queries_to_make = set()
    for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
        date_of_interest = datetime.datetime.now() - relativedelta(months=num_months_ago)
        year = date_of_interest.year
        month = '{:0>2}'.format(date_of_interest.month)
        query = ''
        for region in regions:
            if 'region={region}/year={year}/month={month}'.format(region=region, year=year, month=month) in partition_set:
                continue
            query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(region=region, year=year, month=month, cloudtrail_log_path=cloudtrail_log_path)
        if query != '':
            queries_to_make.add('ALTER TABLE {table_name} ADD '.format(table_name=self.table_name) + query)
    query_count = len(queries_to_make)
    for query in queries_to_make:
        logging.info('Partition groups remaining to create: {}'.format(query_count))
        self.query_athena(query)
        query_count -= 1","for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS):
    date_of_interest = datetime.datetime.now() - relativedelta(months=num_months_ago)
    year = date_of_interest.year
    month = '{:0>2}'.format(date_of_interest.month)
    query = ''
    for region in regions:
        if 'region={region}/year={year}/month={month}'.format(region=region, year=year, month=month) in partition_set:
            continue
        query += ""PARTITION (region='{region}',year='{year}',month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'\n"".format(region=region, year=year, month=month, cloudtrail_log_path=cloudtrail_log_path)
    if query != '':
        queries_to_make.add('ALTER TABLE {table_name} ADD '.format(table_name=self.table_name) + query)","for (year, month) in [(datetime.datetime.now() - relativedelta(months=num_months_ago)).strftime('%Y-%m').split('-') for num_months_ago in range(0, NUM_MONTHS_FOR_PARTITIONS)]:
    queries_to_make |= {f""ALTER TABLE {self.table_name} ADD PARTITION (region='{region}', year='{year}', month='{month}') location '{cloudtrail_log_path}/{region}/{year}/{month}/'"" for region in regions if f'region={region}/year={year}/month={month}' not in partition_set}",find_wrong,2
BBTz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BBTz/gctexposer.py,https://github.com/m4ll0k/BBTz/tree/master//gctexposer.py,,GCTExposer$39,"def GCTExposer(domain, more):
    (domains, next_) = contentParser(getContent(domain).content)
    for i in domains:
        if args.moreinfo:
            print('{_} -> {__}'.format(_=i[1], __=i[2]))
        else:
            print(i[1])
    range_ = next_[-1]
    for i in range(range_ - 1):
        (domains, next_) = contentParser(getNextContent(next_[1]))
        next_ = next_
        for ii in domains:
            if args.moreinfo:
                print('{_} -> {__}'.format(_=ii[1], __=ii[2]))
            else:
                print(ii[1])","for i in range(range_ - 1):
    (domains, next_) = contentParser(getNextContent(next_[1]))
    next_ = next_
    for ii in domains:
        if args.moreinfo:
            print('{_} -> {__}'.format(_=ii[1], __=ii[2]))
        else:
            print(ii[1])","for _ in range(range_ - 1):
    (domains, next_) = contentParser(getNextContent(next_[1]))
    for ii in domains:
        if args.moreinfo:
            print('{_} -> {__}'.format(_=ii[1], __=ii[2]))
        else:
            print(ii[1])",find_wrong,-1
parliament,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/parliament/parliament/statement.py,https://github.com/duo-labs/parliament/tree/master/parliament/statement.py,Statement,_check_condition$496,"def _check_condition(self, operator, condition_block, expanded_actions):
    """"""
        operator is something like ""StringLike""
        condition_block is something like {""s3:prefix"":[""home/${aws:username}/*""]}
        """"""
    operator_type_requirement = None
    for documented_operator in OPERATORS:
        op = documented_operator.lower()
        if operator.lower() in [op, op + 'ifexists', 'forallvalues:' + op, 'foranyvalue:' + op, 'forallvalues:' + op + 'ifexists', 'foranyvalue:' + op + 'ifexists']:
            operator_type_requirement = OPERATORS[documented_operator]
            break
    if operator_type_requirement is None:
        self.add_finding('UNKNOWN_OPERATOR', detail=operator, location=condition_block)
    if operator_type_requirement == 'Bool':
        for c in condition_block:
            value = str(c[1].value).lower()
            if value != 'true' and value != 'false':
                self.add_finding('MISMATCHED_TYPE_OPERATION_TO_NULL', location=condition_block)
                return False
    for block in condition_block:
        key = block[0]
        values = []
        for v in make_list(block[1]):
            values.append(v.value)
        if operator.lower() == 'bool':
            if key.lower() == 'aws:MultiFactorAuthPresent'.lower() and 'false' in values:
                self.add_finding('BAD_PATTERN_FOR_MFA', detail='The condition {""Bool"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent may not exist so it does not enforce MFA. You likely want to use a Deny with BoolIfExists.', location=condition_block)
        elif operator.lower() == 'null':
            if key.lower == 'aws:MultiFactorAuthPresent'.lower() and 'false' in values:
                self.add_finding('BAD_PATTERN_FOR_MFA', detail='The condition {""Null"": {""aws:MultiFactorAuthPresent"":""false""}} is bad because aws:MultiFactorAuthPresent it does not enforce MFA, and only checks if the value exists. You likely want to use an Allow with {""Bool"": {""aws:MultiFactorAuthPresent"":""true""}}.', location=condition_block)
        if operator.lower() in ['null']:
            continue
        condition_type = get_global_key_type(key)
        if condition_type:
            if not is_value_in_correct_format_for_type(condition_type, values):
                self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)
        else:
            for action_struct in expanded_actions:
                privilege_info = get_privilege_info(action_struct['service'], action_struct['action'])
                match = None
                for resource_type in privilege_info['resource_types']:
                    for condition_key in resource_type['condition_keys']:
                        if is_condition_key_match(condition_key, key):
                            match = condition_key
                if match is None:
                    self.add_finding('UNKNOWN_CONDITION_FOR_ACTION', detail='Unknown condition {} for action {}:{}'.format(key, action_struct['service'], action_struct['action']), location=condition_block)
                    continue
                condition_type = None
                for condition in privilege_info['service_conditions']:
                    if condition['condition'] == match:
                        condition_type = condition['type']
                if condition_type is None:
                    raise Exception('Action condition not found in service definition for {}'.format(match))
                if not is_value_in_correct_format_for_type(condition_type, values):
                    self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)
            if condition_type is not None:
                documenation_condition_type = translate_documentation_types(condition_type)
                if operator_type_requirement != documenation_condition_type:
                    if operator_type_requirement == 'String' and documenation_condition_type == 'Arn':
                        self.add_finding('MISMATCHED_TYPE_BUT_USABLE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)
                    else:
                        self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(operator, operator_type_requirement, translate_documentation_types(condition_type)), location=condition_block)
    return","for action_struct in expanded_actions:
    privilege_info = get_privilege_info(action_struct['service'], action_struct['action'])
    match = None
    for resource_type in privilege_info['resource_types']:
        for condition_key in resource_type['condition_keys']:
            if is_condition_key_match(condition_key, key):
                match = condition_key
    if match is None:
        self.add_finding('UNKNOWN_CONDITION_FOR_ACTION', detail='Unknown condition {} for action {}:{}'.format(key, action_struct['service'], action_struct['action']), location=condition_block)
        continue
    condition_type = None
    for condition in privilege_info['service_conditions']:
        if condition['condition'] == match:
            condition_type = condition['type']
    if condition_type is None:
        raise Exception('Action condition not found in service definition for {}'.format(match))
    if not is_value_in_correct_format_for_type(condition_type, values):
        self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)","for action_struct in expanded_actions:
    privilege_info = get_privilege_info(action_struct['service'], action_struct['action'])
    for resource_type in privilege_info['resource_types']:
        for condition_key in resource_type['condition_keys']:
            if is_condition_key_match(condition_key, key):
                condition_type = None
                for condition in privilege_info['service_conditions']:
                    if condition['condition'] == condition_key:
                        condition_type = condition['type']
                if condition_type is None:
                    raise Exception('Action condition not found in service definition for {}'.format(condition_key))
                if not is_value_in_correct_format_for_type(condition_type, values):
                    self.add_finding('MISMATCHED_TYPE', detail='Type mismatch: {} requires a value of type {} but given {}'.format(key, condition_type, values), location=condition_block)
    else:
        self.add_finding('UNKNOWN_CONDITION_FOR_ACTION', detail='Unknown condition {} for action {}:{}'.format(key, action_struct['service'], action_struct['action']), location=condition_block)",find_wrong,2
DrQA,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DrQA/scripts/distant/generate.py,https://github.com/facebookresearch/DrQA/tree/master/scripts/distant/generate.py,,find_answer$80,"def find_answer(paragraph, q_tokens, answer, opts):
    """"""Return the best matching answer offsets from a paragraph.

    The paragraph is skipped if:
    * It is too long or short.
    * It doesn't contain the answer at all.
    * It doesn't contain named entities found in the question.
    * The answer context match score is too low.
      - This is the unigram + bigram overlap within +/- window_sz.
    """"""
    if len(paragraph) > opts['char_max'] or len(paragraph) < opts['char_min']:
        return
    if opts['regex']:
        answer = '(%s)' % answer[0]
        ans_regex = re.compile(answer, flags=re.IGNORECASE + re.UNICODE)
        answers = ans_regex.findall(paragraph)
        answers = {a[0] if isinstance(a, tuple) else a for a in answers}
        answers = {a.strip() for a in answers if len(a.strip()) > 0}
    else:
        answers = {a for a in answer if a in paragraph}
    if len(answers) == 0:
        return
    (q_tokens, q_nltk_ner) = q_tokens
    for ne in q_tokens.entity_groups():
        if ne[0] not in paragraph:
            return
    for ne in q_nltk_ner:
        if ne not in paragraph:
            return
    p_tokens = tokenize_text(paragraph)
    p_words = p_tokens.words(uncased=True)
    q_grams = Counter(q_tokens.ngrams(n=2, uncased=True, filter_fn=utils.filter_ngram))
    best_score = 0
    best_ex = None
    for ans in answers:
        try:
            a_words = tokenize_text(ans).words(uncased=True)
        except RuntimeError:
            logger.warn('Failed to tokenize answer: %s' % ans)
            continue
        for idx in range(len(p_words)):
            if p_words[idx:idx + len(a_words)] == a_words:
                w_s = max(idx - opts['window_sz'], 0)
                w_e = min(idx + opts['window_sz'] + len(a_words), len(p_words))
                w_tokens = p_tokens.slice(w_s, w_e)
                w_grams = Counter(w_tokens.ngrams(n=2, uncased=True, filter_fn=utils.filter_ngram))
                score = sum((w_grams & q_grams).values())
                if score > best_score:
                    best_score = score
                    best_ex = {'id': uuid.uuid4().hex, 'question': q_tokens.words(), 'document': p_tokens.words(), 'offsets': p_tokens.offsets(), 'answers': [(idx, idx + len(a_words) - 1)], 'qlemma': q_tokens.lemmas(), 'lemma': p_tokens.lemmas(), 'pos': p_tokens.pos(), 'ner': p_tokens.entities()}
    if best_score >= opts['match_threshold']:
        return (best_score, best_ex)","for ans in answers:
    try:
        a_words = tokenize_text(ans).words(uncased=True)
    except RuntimeError:
        logger.warn('Failed to tokenize answer: %s' % ans)
        continue
    for idx in range(len(p_words)):
        if p_words[idx:idx + len(a_words)] == a_words:
            w_s = max(idx - opts['window_sz'], 0)
            w_e = min(idx + opts['window_sz'] + len(a_words), len(p_words))
            w_tokens = p_tokens.slice(w_s, w_e)
            w_grams = Counter(w_tokens.ngrams(n=2, uncased=True, filter_fn=utils.filter_ngram))
            score = sum((w_grams & q_grams).values())
            if score > best_score:
                best_score = score
                best_ex = {'id': uuid.uuid4().hex, 'question': q_tokens.words(), 'document': p_tokens.words(), 'offsets': p_tokens.offsets(), 'answers': [(idx, idx + len(a_words) - 1)], 'qlemma': q_tokens.lemmas(), 'lemma': p_tokens.lemmas(), 'pos': p_tokens.pos(), 'ner': p_tokens.entities()}","for ans in answers:
    try:
        a_words = tokenize_text(ans).words(uncased=True)
    except RuntimeError:
        logger.warn('Failed to tokenize answer: %s' % ans)
        continue
    for (idx, _) in enumerate(p_words):
        if p_words[idx:idx + len(a_words)] == a_words:
            w_s = max(idx - opts['window_sz'], 0)
            w_e = min(idx + opts['window_sz'] + len(a_words), len(p_words))
            w_tokens = p_tokens.slice(w_s, w_e)
            w_grams = Counter(w_tokens.ngrams(n=2, uncased=True, filter_fn=utils.filter_ngram))
            score = sum((w_grams & q_grams).values())
            if score > best_score:
                best_score = score
                best_ex = {'id': uuid.uuid4().hex, 'question': q_tokens.words(), 'document': p_tokens.words(), 'offsets': p_tokens.offsets(), 'answers': [(idx, idx + len(a_words) - 1)], 'qlemma': q_tokens.lemmas(), 'lemma': p_tokens.lemmas(), 'pos': p_tokens.pos(), 'ner': p_tokens.entities()}",find_wrong,2
python-mingus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-mingus/mingus/core/chords.py,https://github.com/bspaans/python-mingus/tree/master/mingus/core/chords.py,,inversion_exhauster$1183,"def inversion_exhauster(chord, shorthand, tries, result, polychords):
    """"""Recursive helper function""""""
    if tries == 1 and (not no_polychords):
        polychords += determine_polychords(chord, shorthand)

    def add_result(short):
        result.append((short, tries, chord[0]))
    ch = determine_extended_chord5(chord[:5], True, True, True)
    intval5 = intervals.determine(chord[0], chord[5])
    for c in ch:
        c = c[len(chord[0]):]
        if c == '9':
            if intval5 == 'perfect fourth':
                add_result('11')
            elif intval5 == 'augmented fourth':
                add_result('7#11')
            elif intval5 == 'major sixth':
                add_result('13')
        elif c == 'm9':
            if intval5 == 'perfect fourth':
                add_result('m11')
            elif intval5 == 'major sixth':
                add_result('m13')
        elif c == 'M9':
            if intval5 == 'perfect fourth':
                add_result('M11')
            elif intval5 == 'major sixth':
                add_result('M13')
    if tries != 6 and (not no_inversions):
        return inversion_exhauster([chord[-1]] + chord[:-1], shorthand, tries + 1, result, polychords)
    else:
        res = []
        for r in result:
            if shorthand:
                res.append(r[2] + r[0])
            else:
                res.append(r[2] + chord_shorthand_meaning[r[0]] + int_desc(r[1]))
        return res + polychords","ch = determine_extended_chord5(chord[:5], True, True, True)
intval5 = intervals.determine(chord[0], chord[5])
for c in ch:
    c = c[len(chord[0]):]
    if c == '9':
        if intval5 == 'perfect fourth':
            add_result('11')
        elif intval5 == 'augmented fourth':
            add_result('7#11')
        elif intval5 == 'major sixth':
            add_result('13')
    elif c == 'm9':
        if intval5 == 'perfect fourth':
            add_result('m11')
        elif intval5 == 'major sixth':
            add_result('m13')
    elif c == 'M9':
        if intval5 == 'perfect fourth':
            add_result('M11')
        elif intval5 == 'major sixth':
            add_result('M13')","for c in determine_extended_chord5(chord[:5], True, True, True):
    c = c[len(chord[0]):]
    intval5 = intervals.determine(chord[0], chord[5])
    if c == '9':
        if intval5 == 'perfect fourth':
            add_result('11')
        elif intval5 == 'augmented fourth':
            add_result('7#11')
        elif intval5 == 'major sixth':
            add_result('13')
    elif c == 'm9':
        if intval5 == 'perfect fourth':
            add_result('m11')
        elif intval5 == 'major sixth':
            add_result('m13')
    elif c == 'M9':
        if intval5 == 'perfect fourth':
            add_result('M11')
        elif intval5 == 'major sixth':
            add_result('M13')",find_wrong,2
course-crawler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/course-crawler/mooc/study_mooc.py,https://github.com/Foair/course-crawler/tree/master/mooc/study_mooc.py,,get_resource$105,"def get_resource(term_id):
    """"""""""""
    outline = Outline()
    playlist = Playlist()
    counter = Counter()
    video_list = []
    pdf_list = []
    rich_text_list = []
    post_data = {'callCount': '1', 'scriptSessionId': '${scriptSessionId}190', 'httpSessionId': 'b8efd4c73fd1434896507b83de631f0f', 'c0-scriptName': 'CourseBean', 'c0-methodName': 'getLastLearnedMocTermDto', 'c0-id': '0', 'c0-param0': 'number:' + term_id, 'batchId': str(int(time.time() * 1000))}
    res = CANDY.post('https://mooc.study.163.com/dwr/call/plaincall/CourseBean.getLastLearnedMocTermDto.dwr', data=post_data).text.encode('utf_8').decode('unicode_escape')
    chapters = re.findall('homeworks=\\w+;.+id=(\\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)
        lessons = re.findall('chapterId=' + chapter[0] + '.+contentType=1.+id=(\\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)
            videos = re.findall('contentId=(\\d+).+contentType=(1).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()
            pdfs = re.findall('contentId=(\\d+).+contentType=(3).+id=(\\d+).+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()
            rich_text = re.findall('contentId=(\\d+).+contentType=(4).+id=(\\d+).+jsonContent=(.+);.+lessonId=' + lesson[0] + '.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1), 'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')
                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()
    if video_list:
        rename = WORK_DIR.file('Names.txt') if CONFIG['rename'] else False
        WORK_DIR.change('Videos')
        if CONFIG['dpl']:
            parse_res_list(video_list, rename, playlist.write, parse_resource)
        else:
            parse_res_list(video_list, rename, parse_resource)
    if pdf_list:
        WORK_DIR.change('PDFs')
        parse_res_list(pdf_list, None, parse_resource)
    if rich_text_list:
        WORK_DIR.change('Texts')
        parse_res_list(rich_text_list, None, parse_resource)","chapters = re.findall(r'homeworks=\w+;.+id=(\d+).+name=""(.+)"";', res)
    for chapter in chapters:
        counter.add(0)
        outline.write(chapter[1], counter, 0)

        lessons = re.findall(r'chapterId=' + chapter[0] + r'.+contentType=1.+id=(\d+).+name=""(.+)"".+test', res)
        for lesson in lessons:
            counter.add(1)
            outline.write(lesson[1], counter, 1)

            videos = re.findall(r'contentId=(\d+).+contentType=(1).+id=(\d+).+lessonId=' +
                                lesson[0] + r'.+name=""(.+)""', res)
            for video in videos:
                counter.add(2)
                outline.write(video[3], counter, 2, sign='#')
                video_list.append(Video(counter, video[3], video))
            counter.reset()

            pdfs = re.findall(r'contentId=(\d+).+contentType=(3).+id=(\d+).+lessonId=' +
                              lesson[0] + r'.+name=""(.+)""', res)
            for pdf in pdfs:
                counter.add(2)
                outline.write(pdf[3], counter, 2, sign='*')
                if CONFIG['doc']:
                    pdf_list.append(Document(counter, pdf[3], pdf))
            counter.reset()

            rich_text = re.findall(r'contentId=(\d+).+contentType=(4).+id=(\d+).+jsonContent=(.+);.+lessonId=' +
                                   lesson[0] + r'.+name=""(.+)""', res)
            for text in rich_text:
                counter.add(2)
                outline.write(text[4], counter, 2, sign='+')
                if CONFIG['text']:
                    rich_text_list.append(RichText(counter, text[4], text))
                if CONFIG['file']:
                    if text[3] != 'null' and text[3] != '""""':
                        params = {'nosKey': re.search('nosKey"":""(.+?)""', text[3]).group(1),
                                  'fileName': re.search('""fileName"":""(.+?)""', text[3]).group(1)}
                        file_name = Resource.file_to_save(params['fileName'])
                        outline.write(file_name, counter, 2, sign='!')

                        WORK_DIR.change('Files')
                        res_print(params['fileName'])
                        file_name = '%s %s' % (counter, file_name)
                        CANDY.download_bin('https://www.icourse163.org/course/attachment.htm',
                                           WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
            counter.reset()","for chapter_match in re.finditer('homeworks=\\w+;.+id=(\\d+).+name=""(.+)"";', res):
    (chapter_id, chapter_name) = chapter_match.groups()
    counter.add(0)
    outline.write(chapter_name, counter, 0)
    for lesson_match in re.finditer('chapterId=' + chapter_id + '.+contentType=1.+id=(\\d+).+name=""(.+)"".+test', res):
        (lesson_id, lesson_name) = lesson_match.groups()
        counter.add(1)
        outline.write(lesson_name, counter, 1)
        for video_match in re.finditer('contentId=(\\d+).+contentType=(1).+id=(\\d+).+lessonId=' + lesson_id + '.+name=""(.+)""', res):
            (_, _, video_id, video_name) = video_match.groups()
            counter.add(2)
            outline.write(video_name, counter, 2, sign='#')
            video_list.append(Video(counter, video_name, (video_id,)))
        for pdf_match in re.finditer('contentId=(\\d+).+contentType=(3).+id=(\\d+).+lessonId=' + lesson_id + '.+name=""(.+)""', res):
            (_, _, pdf_id, pdf_name) = pdf_match.groups()
            counter.add(2)
            outline.write(pdf_name, counter, 2, sign='*')
            if CONFIG['doc']:
                pdf_list.append(Document(counter, pdf_name, (pdf_id,)))
        for rich_text_match in re.finditer('contentId=(\\d+).+contentType=(4).+id=(\\d+).+jsonContent=(.+);.+lessonId=' + lesson_id + '.+name=""(.+)""', res):
            (_, _, rich_text_id, rich_text_json, rich_text_name) = rich_text_match.groups()
            counter.add(2)
            outline.write(rich_text_name, counter, 2, sign='+')
            if CONFIG['text']:
                rich_text_list.append(RichText(counter, rich_text_name, (rich_text_id, rich_text_json)))
            if CONFIG['file']:
                if rich_text_json != 'null' and rich_text_json != '""""':
                    params = {'nosKey': re.search('nosKey"":""(.+?)""', rich_text_json).group(1), 'fileName': re.search('""fileName"":""(.+?)""', rich_text_json).group(1)}
                    file_name = Resource.file_to_save(params['fileName'])
                    outline.write(file_name, counter, 2, sign='!')
                    WORK_DIR.change('Files')
                    res_print(params['fileName'])
                    file_name = '%s %s' % (counter, file_name)
                    CANDY.download_bin('https://www.icourse163.org/course/attachment.htm', WORK_DIR.file(file_name), params=params, cookies={'STUDY_SESS': None})
        counter.reset()",find_wrong,-1
Octolapse,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Octolapse/octoprint_octolapse/snapshot.py,https://github.com/FormerLurker/Octolapse/tree/master/octoprint_octolapse/snapshot.py,CaptureSnapshot,take_snapshots$111,"def take_snapshots(self, metadata={}, no_wait=False):
    logger.info('Starting snapshot acquisition')
    start_time = time()
    before_snapshot_threads = []
    snapshot_threads = []
    after_snapshot_threads = []
    results = []
    for current_camera in self.Cameras:
        camera_info = self.CameraInfos['{}'.format(current_camera.guid)]
        if current_camera.on_before_snapshot_script:
            before_snapshot_job_info = SnapshotJobInfo(self.TimelapseJobInfo, self.temporary_directory, camera_info.snapshot_attempt, current_camera, 'before-snapshot', metadata=metadata)
            thread = ExternalScriptSnapshotJob(before_snapshot_job_info, 'before-snapshot')
            thread.daemon = True
            before_snapshot_threads.append(thread)
        snapshot_job_info = SnapshotJobInfo(self.TimelapseJobInfo, self.temporary_directory, camera_info.snapshot_attempt, current_camera, 'snapshot', metadata=metadata)
        if current_camera.camera_type == 'script':
            thread = ExternalScriptSnapshotJob(snapshot_job_info, 'snapshot', on_new_thumbnail_available_callback=self.OnNewThumbnailAvailableCallback, on_post_processing_error_callback=self.on_post_processing_error_callback)
            thread.daemon = True
            snapshot_threads.append((thread, snapshot_job_info, None))
        elif current_camera.camera_type == 'webcam':
            download_started_event = Event()
            thread = WebcamSnapshotJob(snapshot_job_info, download_started_event=download_started_event, on_new_thumbnail_available_callback=self.OnNewThumbnailAvailableCallback, on_post_processing_error_callback=self.on_post_processing_error_callback)
            thread.daemon = True
            snapshot_threads.append((thread, snapshot_job_info, download_started_event))
        if current_camera.on_after_snapshot_script:
            after_snapshot_job_info = SnapshotJobInfo(self.TimelapseJobInfo, self.temporary_directory, camera_info.snapshot_attempt, current_camera, 'after-snapshot', metadata=metadata)
            thread = ExternalScriptSnapshotJob(after_snapshot_job_info, 'after-snapshot')
            thread.daemon = True
            after_snapshot_threads.append(thread)
    for current_camera in self.Cameras:
        if current_camera.on_before_snapshot_gcode:
            on_before_snapshot_gcode = Commands.string_to_gcode_array(current_camera.on_before_snapshot_gcode)
            if len(on_before_snapshot_gcode) > 0:
                logger.info('Sending on_before_snapshot_gcode for the %s camera.', current_camera.name)
                self.SendGcodeArrayCallback(on_before_snapshot_gcode, current_camera.timeout_ms / 1000.0, wait_for_completion=not no_wait, tags={'before-snapshot-gcode'})
    if len(before_snapshot_threads) > 0:
        logger.info('Starting %d before snapshot threads', len(before_snapshot_threads))
    for t in before_snapshot_threads:
        t.start()
    for t in before_snapshot_threads:
        if not no_wait:
            snapshot_job_info = t.join()
            assert isinstance(snapshot_job_info, SnapshotJobInfo)
            if t.snapshot_thread_error:
                snapshot_job_info.success = False
                snapshot_job_info.error = t.snapshot_thread_error
            else:
                snapshot_job_info.success = True
        else:
            snapshot_job_info.success = True
        results.append(snapshot_job_info)
    if len(before_snapshot_threads) > 0:
        logger.info('Before snapshot threads finished.')
    if len(snapshot_threads) > 0:
        logger.info('Starting %d snapshot threads.', len(snapshot_threads))
    for t in snapshot_threads:
        t[0].start()
    for current_camera in self.Cameras:
        if current_camera.camera_type == 'gcode':
            script_sent = False
            if current_camera.gcode_camera_script:
                gcode_camera_script = Commands.string_to_gcode_array(current_camera.gcode_camera_script)
                if len(gcode_camera_script) > 0:
                    logger.info('Sending snapshot gcode array to %s.', current_camera.name)
                    self.SendGcodeArrayCallback(Commands.string_to_gcode_array(current_camera.gcode_camera_script), current_camera.timeout_ms / 1000.0, wait_for_completion=not no_wait)
                    script_sent = True
                if not script_sent:
                    logger.warning(""The gcode camera '%s' is enabled, but failed to produce any snapshot gcode."", current_camera.name)
    for (t, snapshot_job_info, event) in snapshot_threads:
        if not no_wait:
            if event:
                event.wait()
            else:
                snapshot_job_info = t.join()
            if t.snapshot_thread_error:
                snapshot_job_info.success = False
                snapshot_job_info.error = t.snapshot_thread_error
            elif t.post_processing_error:
                snapshot_job_info.success = False
                snapshot_job_info.error = t.post_processing_error
            else:
                snapshot_job_info.success = True
        else:
            snapshot_job_info.success = True
        info = self.CameraInfos[snapshot_job_info.camera_guid]
        info.snapshot_attempt += 1
        if snapshot_job_info.success:
            info.snapshot_count += 1
            self.SnapshotsTotal += 1
        else:
            info.errors_count += 1
            self.ErrorsTotal += 1
        info.save(self.temporary_directory, self.TimelapseJobInfo.JobGuid, snapshot_job_info.camera_guid)
        results.append(snapshot_job_info)
    if len(snapshot_threads) > 0:
        logger.info('Snapshot threads complete, but may be post-processing.')
    if len(after_snapshot_threads) > 0:
        logger.info('Starting %d after snapshot threads.', len(after_snapshot_threads))
    for current_camera in self.Cameras:
        if current_camera.on_after_snapshot_gcode:
            on_after_snapshot_gcode = Commands.string_to_gcode_array(current_camera.on_after_snapshot_gcode)
            if len(on_after_snapshot_gcode) > 0:
                logger.info('Sending on_after_snapshot_gcode for the %s camera.', current_camera.name)
                self.SendGcodeArrayCallback(on_after_snapshot_gcode, current_camera.timeout_ms / 1000.0, wait_for_completion=not no_wait, tags={'after-snapshot-gcode'})
    for t in after_snapshot_threads:
        t.start()
    for t in after_snapshot_threads:
        if not no_wait:
            snapshot_job_info = t.join()
            assert isinstance(snapshot_job_info, SnapshotJobInfo)
            info = self.CameraInfos[snapshot_job_info.camera_guid]
            if t.snapshot_thread_error:
                snapshot_job_info.success = False
                snapshot_job_info.error = t.snapshot_thread_error
            else:
                snapshot_job_info.success = True
        else:
            snapshot_job_info.success = True
        results.append(snapshot_job_info)
    if len(after_snapshot_threads) > 0:
        logger.info('After snapshot threads complete.')
    logger.info('Snapshot acquisition completed in %.3f seconds.', time() - start_time)
    return results","for current_camera in self.Cameras:
    camera_info = self.CameraInfos['{}'.format(current_camera.guid)]
    if current_camera.on_before_snapshot_script:
        before_snapshot_job_info = SnapshotJobInfo(self.TimelapseJobInfo, self.temporary_directory, camera_info.snapshot_attempt, current_camera, 'before-snapshot', metadata=metadata)
        thread = ExternalScriptSnapshotJob(before_snapshot_job_info, 'before-snapshot')
        thread.daemon = True
        before_snapshot_threads.append(thread)
    snapshot_job_info = SnapshotJobInfo(self.TimelapseJobInfo, self.temporary_directory, camera_info.snapshot_attempt, current_camera, 'snapshot', metadata=metadata)
    if current_camera.camera_type == 'script':
        thread = ExternalScriptSnapshotJob(snapshot_job_info, 'snapshot', on_new_thumbnail_available_callback=self.OnNewThumbnailAvailableCallback, on_post_processing_error_callback=self.on_post_processing_error_callback)
        thread.daemon = True
        snapshot_threads.append((thread, snapshot_job_info, None))
    elif current_camera.camera_type == 'webcam':
        download_started_event = Event()
        thread = WebcamSnapshotJob(snapshot_job_info, download_started_event=download_started_event, on_new_thumbnail_available_callback=self.OnNewThumbnailAvailableCallback, on_post_processing_error_callback=self.on_post_processing_error_callback)
        thread.daemon = True
        snapshot_threads.append((thread, snapshot_job_info, download_started_event))
    if current_camera.on_after_snapshot_script:
        after_snapshot_job_info = SnapshotJobInfo(self.TimelapseJobInfo, self.temporary_directory, camera_info.snapshot_attempt, current_camera, 'after-snapshot', metadata=metadata)
        thread = ExternalScriptSnapshotJob(after_snapshot_job_info, 'after-snapshot')
        thread.daemon = True
        after_snapshot_threads.append(thread)","for current_camera in self.Cameras:
    camera_info = self.CameraInfos[""{}"".format(current_camera.guid)]

    # pre_snapshot threads
    before_snapshot_threads = [ExternalScriptSnapshotJob(
        SnapshotJobInfo(
            self.TimelapseJobInfo,
            self.temporary_directory,
            camera_info.snapshot_attempt,
            current_camera,
            'before-snapshot',
            metadata=metadata
        ),
        'before-snapshot'
    ) for current_camera in self.Cameras if current_camera.on_before_snapshot_script]

    snapshot_threads = [
        (ExternalScriptSnapshotJob(
            SnapshotJobInfo(
                self.TimelapseJobInfo,
                self.temporary_directory,
                camera_info.snapshot_attempt,
                current_camera,
                'snapshot',
                metadata=metadata
            ),
            'snapshot',
            on_new_thumbnail_available_callback=self.OnNewThumbnailAvailableCallback,
            on_post_processing_error_callback=self.on_post_processing_error_callback
        ), snapshot_job_info, None) if current_camera.camera_type == ""script"" else (
            WebcamSnapshotJob(
                snapshot_job_info,
                download_started_event=Event(),
                on_new_thumbnail_available_callback=self.OnNewThumbnailAvailableCallback,
                on_post_processing_error_callback=self.on_post_processing_error_callback
            ), snapshot_job_info, download_started_event
        ) for current_camera in self.Cameras
    ]

    # post_snapshot threads
    after_snapshot_threads = [ExternalScriptSnapshotJob(
        SnapshotJobInfo(
            self.TimelapseJobInfo,
            self.temporary_directory,
            camera_info.snapshot_attempt,
            current_camera,
            'after-snapshot',
            metadata=metadata
        ),",find_wrong,2
CudaText,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CudaText/app/py/cuda_emmet/proc_snip_insert.py,https://github.com/Alexey-T/CudaText/tree/master/app/py/cuda_emmet/proc_snip_insert.py,,insert_snip_into_editor$8,"def insert_snip_into_editor(ed, snip_lines):
    items = list(snip_lines)
    if not items:
        return
    carets = ed.get_carets()
    if len(carets) != 1:
        return
    (x0, y0, x1, y1) = carets[0]
    tab_spaces = ed.get_prop(ct.PROP_TAB_SPACES)
    tab_size = ed.get_prop(ct.PROP_TAB_SIZE)
    (x_col, y_col) = ed.convert(ct.CONVERT_CHAR_TO_COL, x0, y0)
    indent = ' ' * x_col
    if not tab_spaces:
        indent = indent.replace(' ' * tab_size, '\t')
    for i in range(1, len(items)):
        items[i] = indent + items[i]
    if tab_spaces:
        indent = ' ' * tab_size
        items = [item.replace('\t', indent) for item in items]
    stops = []
    for index in range(len(items)):
        s = items[index]
        while True:
            digit = 0
            deftext = ''
            n = s.find('${')
            if n < 0:
                break
            n_close = s.find('}', n)
            if n_close < 0:
                break
            n_colon = s.find(':', n)
            digit_end = n_close
            if n_colon >= 0:
                digit_end = min(n_close, n_colon)
            try:
                digit = int(s[n + 2:digit_end])
            except ValueError:
                break
            if n_colon >= 0:
                deftext = s[n_colon + 1:n_close]
                s = s[:n] + deftext + s[n_close + 1:]
            else:
                s = s[:n] + s[n_close + 1:]
            stops += [(digit, deftext, index, n)]
            items[index] = s
    ed.insert(x0, y0, '\n'.join(items))
    mark_placed = False
    ed.markers(ct.MARKERS_DELETE_ALL)
    for digit in MARKS_INDEXES:
        for stop in reversed(stops):
            if stop[0] == digit:
                pos_x = stop[3]
                pos_y = stop[2]
                if pos_y == 0:
                    pos_x += x0
                pos_y += y0
                deftext = stop[1]
                ed.markers(ct.MARKERS_ADD, pos_x, pos_y, digit, len(deftext))
                mark_placed = True
    if mark_placed:
        ed.set_prop(ct.PROP_TAB_COLLECT_MARKERS, '1')
        ed.cmd(cudatext_cmd.cmd_Markers_GotoLastAndDelete)","for digit in MARKS_INDEXES:
    for stop in reversed(stops):
        if stop[0] == digit:
            pos_x = stop[3]
            pos_y = stop[2]
            if pos_y == 0:
                pos_x += x0
            pos_y += y0
            deftext = stop[1]
            ed.markers(ct.MARKERS_ADD, pos_x, pos_y, digit, len(deftext))
            mark_placed = True","for digit in MARKS_INDEXES:
    for stop in reversed(stops):
        if stop[0] == digit:
            pos_x = stop[3]
            pos_y = stop[2] + y0
            if pos_y == y0:
                pos_x += x0
            deftext = stop[1]
            ed.markers(ct.MARKERS_ADD, pos_x, pos_y, digit, len(deftext))
            mark_placed = True",find_wrong,-1
seahub,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/seahub/seahub/api2/endpoints/group_members.py,https://github.com/haiwen/seahub/tree/master/seahub/api2/endpoints/group_members.py,GroupMembersImport,post$391,"def post(self, request, group_id):
    """""" Import members from xlsx file

        Permission checking:
        1. group admin or owner.
        """"""
    xlsx_file = request.FILES.get('file', None)
    if not xlsx_file:
        error_msg = 'file can not be found.'
        return api_error(status.HTTP_400_BAD_REQUEST, error_msg)
    (file_type, ext) = get_file_type_and_ext(xlsx_file.name)
    if ext != 'xlsx':
        error_msg = file_type_error_msg(ext, 'xlsx')
        return api_error(status.HTTP_400_BAD_REQUEST, error_msg)
    group_id = int(group_id)
    group = ccnet_api.get_group(group_id)
    if not group:
        error_msg = _('Group does not exist')
        return api_error(status.HTTP_404_NOT_FOUND, error_msg)
    username = request.user.username
    if not is_group_admin_or_owner(group_id, username):
        error_msg = 'Permission denied.'
        return api_error(status.HTTP_403_FORBIDDEN, error_msg)
    content = xlsx_file.read()
    try:
        fs = BytesIO(content)
        wb = load_workbook(filename=fs, read_only=True)
    except Exception as e:
        logger.error(e)
    rows = wb.worksheets[0].rows
    records = []
    next(rows)
    for row in rows:
        records.append([col.value for col in row])
    emails_list = []
    for record in records:
        if record[0]:
            email = record[0].strip().lower()
            emails_list.append(email)
    result = {}
    result['failed'] = []
    result['success'] = []
    emails_need_add = []
    org_id = None
    if is_org_context(request):
        org_id = request.user.org.org_id
    for email in emails_list:
        email_name = email2nickname(email)
        try:
            User.objects.get(email=email)
        except User.DoesNotExist:
            result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': 'User %s not found.' % email_name})
            continue
        if is_group_member(group_id, email, in_structure=False):
            result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': _('User %s is already a group member.') % email_name})
            continue
        if org_id and (not ccnet_api.org_user_exists(org_id, email)):
            result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': _('User %s not found in organization.') % email_name})
            continue
        if not org_id and is_org_user(email):
            result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': _('User %s is an organization user.') % email_name})
            continue
        emails_need_add.append(email)
    for email in emails_need_add:
        try:
            ccnet_api.group_add_member(group_id, username, email)
            member_info = get_group_member_info(request, group_id, email)
            result['success'].append(member_info)
        except SearpcError as e:
            logger.error(e)
            result['failed'].append({'email': email, 'error_msg': 'Internal Server Error'})
        add_user_to_group.send(sender=None, group_staff=username, group_id=group_id, added_user=email)
    return Response(result)","for email in emails_list:
    email_name = email2nickname(email)
    try:
        User.objects.get(email=email)
    except User.DoesNotExist:
        result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': 'User %s not found.' % email_name})
        continue
    if is_group_member(group_id, email, in_structure=False):
        result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': _('User %s is already a group member.') % email_name})
        continue
    if org_id and (not ccnet_api.org_user_exists(org_id, email)):
        result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': _('User %s not found in organization.') % email_name})
        continue
    if not org_id and is_org_user(email):
        result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': _('User %s is an organization user.') % email_name})
        continue
    emails_need_add.append(email)
for email in emails_need_add:
    try:
        ccnet_api.group_add_member(group_id, username, email)
        member_info = get_group_member_info(request, group_id, email)
        result['success'].append(member_info)
    except SearpcError as e:
        logger.error(e)
        result['failed'].append({'email': email, 'error_msg': 'Internal Server Error'})
    add_user_to_group.send(sender=None, group_staff=username, group_id=group_id, added_user=email)","for email in emails_list:
    email_name = email2nickname(email)
    try:
        User.objects.get(email=email)
    except User.DoesNotExist:
        result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': 'User %s not found.' % email_name})
        continue
    if is_group_member(group_id, email, in_structure=False):
        result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': _('User %s is already a group member.') % email_name})
        continue
    if org_id and (not ccnet_api.org_user_exists(org_id, email)):
        result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': _('User %s not found in organization.') % email_name})
        continue
    if not org_id and is_org_user(email):
        result['failed'].append({'email': email, 'email_name': email_name, 'error_msg': _('User %s is an organization user.') % email_name})
        continue
    emails_need_add.append(email)
for email in emails_need_add:
    try:
        ccnet_api.group_add_member(group_id, username, email)
        member_info = get_group_member_info(request, group_id, email)
        result['success'].append(member_info)
    except SearpcError as e:
        logger.error(e)
        result['failed'].append({'email': email, 'error_msg': 'Internal Server Error'})
        add_user_to_group.send(sender=None, group_staff=username, group_id=group_id, added_user=email)",find_wrong,2
sphinx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sphinx/sphinx/ext/autodoc/typehints.py,https://github.com/sphinx-doc/sphinx/tree/master/sphinx/ext/autodoc/typehints.py,,modify_field_list$90,"def modify_field_list(node: nodes.field_list, annotations: Dict[str, str], suppress_rtype: bool=False) -> None:
    arguments: Dict[str, Dict[str, bool]] = {}
    fields = cast(Iterable[nodes.field], node)
    for field in fields:
        field_name = field[0].astext()
        parts = re.split(' +', field_name)
        if parts[0] == 'param':
            if len(parts) == 2:
                arg = arguments.setdefault(parts[1], {})
                arg['param'] = True
            elif len(parts) > 2:
                name = ' '.join(parts[2:])
                arg = arguments.setdefault(name, {})
                arg['param'] = True
                arg['type'] = True
        elif parts[0] == 'type':
            name = ' '.join(parts[1:])
            arg = arguments.setdefault(name, {})
            arg['type'] = True
        elif parts[0] == 'rtype':
            arguments['return'] = {'type': True}
    for (name, annotation) in annotations.items():
        if name == 'return':
            continue
        if '*' + name in arguments:
            name = '*' + name
            arguments.get(name)
        elif '**' + name in arguments:
            name = '**' + name
            arguments.get(name)
        else:
            arg = arguments.get(name, {})
        if not arg.get('type'):
            field = nodes.field()
            field += nodes.field_name('', 'type ' + name)
            field += nodes.field_body('', nodes.paragraph('', annotation))
            node += field
        if not arg.get('param'):
            field = nodes.field()
            field += nodes.field_name('', 'param ' + name)
            field += nodes.field_body('', nodes.paragraph('', ''))
            node += field
    if 'return' in annotations and 'return' not in arguments:
        annotation = annotations['return']
        if annotation == 'None' and suppress_rtype:
            return
        field = nodes.field()
        field += nodes.field_name('', 'rtype')
        field += nodes.field_body('', nodes.paragraph('', annotation))
        node += field","fields = cast(Iterable[nodes.field], node)
for field in fields:
    field_name = field[0].astext()
    parts = re.split(' +', field_name)
    if parts[0] == 'param':
        if len(parts) == 2:
            arg = arguments.setdefault(parts[1], {})
            arg['param'] = True
        elif len(parts) > 2:
            name = ' '.join(parts[2:])
            arg = arguments.setdefault(name, {})
            arg['param'] = True
            arg['type'] = True
    elif parts[0] == 'type':
        name = ' '.join(parts[1:])
        arg = arguments.setdefault(name, {})
        arg['type'] = True
    elif parts[0] == 'rtype':
        arguments['return'] = {'type': True}","for field in node:
    field_name = field[0].astext()
    parts = re.split(' +', field_name)
    if parts[0] == 'param':
        if len(parts) == 2:
            arg = arguments.setdefault(parts[1], {})
            arg['param'] = True
        elif len(parts) > 2:
            name = ' '.join(parts[2:])
            arg = arguments.setdefault(name, {})
            arg['param'] = True
            arg['type'] = True
    elif parts[0] == 'type':
        name = ' '.join(parts[1:])
        arg = arguments.setdefault(name, {})
        arg['type'] = True
    elif parts[0] == 'rtype':
        arguments['return'] = {'type': True}",find_wrong,2
lbry-sdk,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lbry-sdk/lbry/error/generate.py,https://github.com/lbryio/lbry-sdk/tree/master/lbry/error/generate.py,,analyze$127,"def analyze():
    errors = {e.class_name: [] for e in get_errors() if e.is_leaf}
    here = Path(__file__).absolute().parents[0]
    module = here.parent
    for file_path in module.glob('**/*.py'):
        if here in file_path.parents:
            continue
        with open(file_path) as src_file:
            src = src_file.read()
            for error in errors.keys():
                found = src.count(error)
                if found > 0:
                    errors[error].append((file_path, found))
    print('Unused Errors:\n')
    for (error, used) in errors.items():
        if used:
            print(f' - {error}')
            for use in used:
                print(f'   {use[0].relative_to(module.parent)} {use[1]}')
            print('')
    print('')
    print('Unused Errors:')
    for (error, used) in errors.items():
        if not used:
            print(f' - {error}')","for file_path in module.glob('**/*.py'):
    if here in file_path.parents:
        continue
    with open(file_path) as src_file:
        src = src_file.read()
        for error in errors.keys():
            found = src.count(error)
            if found > 0:
                errors[error].append((file_path, found))","for file_path in module.glob('**/*.py'):
    if here not in file_path.parents:
        with open(file_path) as src_file:
            src = src_file.read()
            for error in errors.keys():
                found = src.count(error)
                if found > 0:
                    errors[error].append((file_path, found))",find_wrong,2
deluge,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deluge/deluge/ui/gtk3/addtorrentdialog.py,https://github.com/deluge-torrent/deluge/tree/master/deluge/ui/gtk3/addtorrentdialog.py,AddTorrentDialog,_on_filename_edited$986,"def _on_filename_edited(self, renderer, path, new_text):
    index = self.files_treestore[path][3]
    new_text = new_text.strip(os.path.sep).strip()
    if new_text == self.files_treestore[path][1]:
        return
    itr = self.files_treestore.get_iter(path)
    (model, row) = self.listview_torrents.get_selection().get_selected()
    torrent_id = model[row][0]
    if 'mapped_files' not in self.options[torrent_id]:
        self.options[torrent_id]['mapped_files'] = {}
    if index > -1:
        if not new_text:
            return
        parent = self.files_treestore.iter_parent(itr)
        file_path = os.path.join(self.get_file_path(parent), new_text)
        if parent:
            for row in self.files_treestore[parent].iterchildren():
                if new_text == row[1]:
                    return
        if os.path.sep in new_text:
            split_text = new_text.split(os.path.sep)
            for s in split_text[:-1]:
                parent = self.files_treestore.append(parent, [True, s, 0, -1, False, 'folder-symbolic'])
            self.files_treestore[itr][1] = split_text[-1]
            reparent_iter(self.files_treestore, itr, parent)
        else:
            self.files_treestore[itr][1] = new_text
        self.options[torrent_id]['mapped_files'][index] = file_path
        self.files[torrent_id][index]['path'] = file_path
    else:

        def walk_tree(row):
            if not row:
                return
            file_path_base = self.get_file_path(self.files_treestore.iter_parent(row))
            while row:
                if self.files_treestore.iter_has_child(row):
                    walk_tree(self.files_treestore.iter_children(row))
                index = self.files_treestore[row][3]
                if index > -1:
                    file_path = file_path_base + self.files_treestore[row][1]
                    self.options[torrent_id]['mapped_files'][index] = file_path
                    self.files[torrent_id][index]['path'] = file_path
                row = self.files_treestore.iter_next(row)
        if os.path.sep in new_text:
            parent = self.files_treestore.iter_parent(itr)
            split_text = new_text.split(os.path.sep)
            for s in split_text[:-1]:
                parent = self.files_treestore.append(parent, [True, s + os.path.sep, 0, -1, False, 'folder-symbolic'])
            self.files_treestore[itr][1] = split_text[-1] + os.path.sep
            reparent_iter(self.files_treestore, itr, parent)
            itr = parent
            root = Gtk.TreePath.new_first()
            self.listview_files.expand_row(root, False)
        else:
            self.files_treestore[itr][1] = new_text + os.path.sep
        walk_tree(itr)","if os.path.sep in new_text:
    parent = self.files_treestore.iter_parent(itr)
    split_text = new_text.split(os.path.sep)
    for s in split_text[:-1]:
        parent = self.files_treestore.append(parent, [True, s + os.path.sep, 0, -1, False, 'folder-symbolic'])
    self.files_treestore[itr][1] = split_text[-1] + os.path.sep
    reparent_iter(self.files_treestore, itr, parent)
    itr = parent
    root = Gtk.TreePath.new_first()
    self.listview_files.expand_row(root, False)
else:
    self.files_treestore[itr][1] = new_text + os.path.sep
walk_tree(itr)","for folder in new_text.split(os.path.sep):
    if not folder:
        continue
    parent = self.files_treestore.iter_parent(itr)
    itr = self.files_treestore.append(parent, [True, folder + os.path.sep, 0, -1, False, 'folder-symbolic'])
    parent = itr
root = Gtk.TreePath.new_first()
self.listview_files.expand_row(root, False)
walk_tree(itr)",find_wrong,2
PyGaze,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyGaze/pygaze/_eyetracker/libtobiilegacy.py,https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/libtobiilegacy.py,TobiiController,flushData$1922,"def flushData(self):
    """"""
        
        arguments
        None
        
        keyword arguments
        None
        
        returns
        None
        """"""
    if self.datafile == None:
        print('WARNING! libtobii.TobiiController.flushData: data file is not set.')
        return
    if len(self.gazeData) == 0:
        print('WARNING! libtobii.TobiiController.flushData: no data to write to file.')
        return
    self.datafile.write('\t'.join(['TimeStamp', 'GazePointXLeft', 'GazePointYLeft', 'ValidityLeft', 'GazePointXRight', 'GazePointYRight', 'ValidityRight', 'GazePointX', 'GazePointY', 'Event']) + '\n')
    timeStampStart = self.gazeData[0].Timestamp
    for g in self.gazeData:
        self.datafile.write('{}\t{}\t{}\t{}\t{}\t{}\t{}'.format(round((g.Timestamp - timeStampStart) / 1000.0, ndigits=1), round(g.LeftGazePoint2D.x * self.disp.dispsize[0] if g.LeftValidity != 4 else -1.0, ndigits=4), round(g.LeftGazePoint2D.y * self.disp.dispsize[1] if g.LeftValidity != 4 else -1.0, ndigits=4), g.LeftValidity, round(g.RightGazePoint2D.x * self.disp.dispsize[0] if g.RightValidity != 4 else -1.0, ndigits=4), round(g.RightGazePoint2D.y * self.disp.dispsize[1] if g.RightValidity != 4 else -1.0, ndigits=4), g.RightValidity))
        if g.LeftValidity == 4 and g.RightValidity == 4:
            ave = (-1.0, -1.0)
        elif g.LeftValidity == 4:
            ave = (g.RightGazePoint2D.x, g.RightGazePoint2D.y)
        elif g.RightValidity == 4:
            ave = (g.LeftGazePoint2D.x, g.LeftGazePoint2D.y)
        else:
            ave = ((g.LeftGazePoint2D.x + g.RightGazePoint2D.x) / 2.0, (g.LeftGazePoint2D.y + g.RightGazePoint2D.y) / 2.0)
        self.datafile.write('\t{}\t{}\t'.format(round(ave[0], ndigits=4), round(ave[1], ndigits=4)))
        self.datafile.write('\n')
    formatstr = '{}' + '\t' * 9 + '{}\n'
    for e in self.eventData:
        self.datafile.write(formatstr.format(round((e[0] - timeStampStart) / 1000.0, ndigits=4), e[1]))
    self.datafile.flush()
    os.fsync(self.datafile.fileno())","for g in self.gazeData:
    self.datafile.write('{}\t{}\t{}\t{}\t{}\t{}\t{}'.format(round((g.Timestamp - timeStampStart) / 1000.0, ndigits=1), round(g.LeftGazePoint2D.x * self.disp.dispsize[0] if g.LeftValidity != 4 else -1.0, ndigits=4), round(g.LeftGazePoint2D.y * self.disp.dispsize[1] if g.LeftValidity != 4 else -1.0, ndigits=4), g.LeftValidity, round(g.RightGazePoint2D.x * self.disp.dispsize[0] if g.RightValidity != 4 else -1.0, ndigits=4), round(g.RightGazePoint2D.y * self.disp.dispsize[1] if g.RightValidity != 4 else -1.0, ndigits=4), g.RightValidity))
    if g.LeftValidity == 4 and g.RightValidity == 4:
        ave = (-1.0, -1.0)
    elif g.LeftValidity == 4:
        ave = (g.RightGazePoint2D.x, g.RightGazePoint2D.y)
    elif g.RightValidity == 4:
        ave = (g.LeftGazePoint2D.x, g.LeftGazePoint2D.y)
    else:
        ave = ((g.LeftGazePoint2D.x + g.RightGazePoint2D.x) / 2.0, (g.LeftGazePoint2D.y + g.RightGazePoint2D.y) / 2.0)
    self.datafile.write('\t{}\t{}\t'.format(round(ave[0], ndigits=4), round(ave[1], ndigits=4)))
    self.datafile.write('\n')","for g in self.gazeData:
    row = [round((g.Timestamp - timeStampStart) / 1000.0, ndigits=1), round(g.LeftGazePoint2D.x * self.disp.dispsize[0] if g.LeftValidity != 4 else -1.0, ndigits=4), round(g.LeftGazePoint2D.y * self.disp.dispsize[1] if g.LeftValidity != 4 else -1.0, ndigits=4), g.LeftValidity, round(g.RightGazePoint2D.x * self.disp.dispsize[0] if g.RightValidity != 4 else -1.0, ndigits=4), round(g.RightGazePoint2D.y * self.disp.dispsize[1] if g.RightValidity != 4 else -1.0, ndigits=4), g.RightValidity]
    if g.LeftValidity == 4 and g.RightValidity == 4:
        ave = (-1.0, -1.0)
    elif g.LeftValidity == 4:
        ave = (g.RightGazePoint2D.x, g.RightGazePoint2D.y)
    elif g.RightValidity == 4:
        ave = (g.LeftGazePoint2D.x, g.LeftGazePoint2D.y)
    else:
        ave = ((g.LeftGazePoint2D.x + g.RightGazePoint2D.x) / 2.0, (g.LeftGazePoint2D.y + g.RightGazePoint2D.y) / 2.0)
    row.extend([round(ave[0], ndigits=4), round(ave[1], ndigits=4), '\n'])
    self.datafile.write('\t'.join(map(str, row)))",find_wrong,2
sparrow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sparrow/views/report.py,https://github.com/wylok/sparrow/tree/master/views/report.py,,alarm_report$222,"def alarm_report():
    try:
        INFOS = []
        total_key = 'op_totals_alarms_tmp'

        def counts_alarm(key, total_key=None):
            vals = []
            for i in range(7):
                count_key = 'op_counts_alarms_tmp'
                data_now = datetime.datetime.now() - datetime.timedelta(days=i)
                dd = data_now.strftime('%Y-%m-%d')
                alarm_count_key = '%s_%s' % (key, dd)
                if RC_CLUSTER.exists(alarm_count_key):
                    vals = RC_CLUSTER.hgetall(alarm_count_key)
                    vals = sorted(vals.items(), key=lambda item: int(item[1]))
                    for val in vals:
                        RC_CLUSTER.hincrby(count_key, val[0], val[1])
                        if total_key:
                            RC_CLUSTER.hincrby(total_key, dd, val[1])
            if RC_CLUSTER.exists(count_key):
                vals = RC_CLUSTER.hgetall(count_key)
                RC_CLUSTER.delete(count_key)
                vals = sorted(vals.items(), key=lambda item: int(item[1]), reverse=True)
            if len(vals) > 10:
                return vals[:10]
            else:
                return vals
        alarm_count = counts_alarm('op_business_alarm_count', total_key=total_key)
        vals = counts_alarm('op_business_alarm_perf')
        if vals:
            pie_perf = Pie('7TOP10', width='100%', height='100%', title_pos='center', title_text_size=14)
            attrs = [val[0] for val in vals]
            vals = [int(val[1]) for val in vals]
            pie_perf.add('', attrs, vals, is_label_show=True, is_toolbox_show=False, legend_orient='vertical', legend_pos='left', xaxis_interval=0, is_random=True, rosetype='area')
            INFOS.append(pie_perf)
        vals = counts_alarm('op_business_alarm_busi')
        if vals:
            pie_busi = Pie('7TOP10', width='100%', height='100%', title_pos='center', title_text_size=14)
            attrs = [val[0] for val in vals]
            vals = [int(val[1]) for val in vals]
            pie_busi.add('', attrs, vals, is_label_show=True, is_toolbox_show=False, legend_orient='vertical', legend_pos='left', xaxis_interval=0, is_random=True, rosetype='radius', radius=[35, 75])
            INFOS.append(pie_busi)
        if RC_CLUSTER.exists(total_key):
            vals = RC_CLUSTER.hgetall(total_key)
            vals = sorted(vals.items(), key=lambda item: item[0], reverse=True)
            RC_CLUSTER.delete(total_key)
            line = Line('7', width='100%', height='100%', title_pos='center', title_text_size=14)
            attrs = [val[0] for val in vals]
            vals = [int(val[1]) for val in vals]
            line.add('', attrs, vals, is_label_show=True, is_toolbox_show=False, is_legend_show=False, xaxis_interval=0, is_random=True)
            INFOS.append(line)
    except Exception as e:
        logging.error(e)
        return redirect(url_for('error'))
    return render_template('alarm_report.html', INFOS=INFOS, alarm_count=alarm_count)","for i in range(7):
    count_key = 'op_counts_alarms_tmp'
    data_now = datetime.datetime.now() - datetime.timedelta(days=i)
    dd = data_now.strftime('%Y-%m-%d')
    alarm_count_key = '%s_%s' % (key, dd)
    if RC_CLUSTER.exists(alarm_count_key):
        vals = RC_CLUSTER.hgetall(alarm_count_key)
        vals = sorted(vals.items(), key=lambda item: int(item[1]))
        for val in vals:
            RC_CLUSTER.hincrby(count_key, val[0], val[1])
            if total_key:
                RC_CLUSTER.hincrby(total_key, dd, val[1])","for i in range(7):
    count_key = 'op_counts_alarms_tmp'
    data_now = datetime.datetime.now() - datetime.timedelta(days=i)
    dd = data_now.strftime('%Y-%m-%d')
    alarm_count_key = f'{key}_{dd}'
    if RC_CLUSTER.exists(alarm_count_key):
        vals = RC_CLUSTER.hgetall(alarm_count_key)
        vals = sorted(vals.items(), key=lambda item: int(item[1]))
        for val in vals:
            RC_CLUSTER.hincrby(count_key, val[0], val[1])
            if total_key:
                RC_CLUSTER.hincrby(total_key, dd, val[1])",find_wrong,-1
yolov5-face,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yolov5-face/test_widerface.py,https://github.com/deepcam-cn/yolov5-face/tree/master//test_widerface.py,,if_main_my$113,"if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default='runs/train/exp5/weights/last.pt', help='model.pt path(s)')
    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf-thres', type=float, default=0.02, help='object confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.5, help='IOU threshold for NMS')
    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--update', action='store_true', help='update all models')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')
    parser.add_argument('--project', default='runs/detect', help='save results to project/name')
    parser.add_argument('--name', default='exp', help='save results to project/name')
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    parser.add_argument('--save_folder', default='./widerface_evaluate/widerface_txt/', type=str, help='Dir to save txt results')
    parser.add_argument('--dataset_folder', default='../WiderFace/val/images/', type=str, help='dataset path')
    parser.add_argument('--folder_pict', default='/yolov5-face/data/widerface/val/wider_val.txt', type=str, help='folder_pict')
    opt = parser.parse_args()
    print(opt)
    pict_folder = {}
    with open(opt.folder_pict, 'r') as f:
        lines = f.readlines()
        for line in lines:
            line = line.strip().split('/')
            pict_folder[line[-1]] = line[-2]
    device = select_device(opt.device)
    model = attempt_load(opt.weights, map_location=device)
    with torch.no_grad():
        testset_folder = opt.dataset_folder
        for image_path in tqdm(glob.glob(os.path.join(testset_folder, '*'))):
            if image_path.endswith('.txt'):
                continue
            img0 = cv2.imread(image_path)
            if img0 is None:
                print(f'ignore : {image_path}')
                continue
            boxes = detect(model, img0)
            image_name = os.path.basename(image_path)
            txt_name = os.path.splitext(image_name)[0] + '.txt'
            save_name = os.path.join(opt.save_folder, pict_folder[image_name], txt_name)
            dirname = os.path.dirname(save_name)
            if not os.path.isdir(dirname):
                os.makedirs(dirname)
            with open(save_name, 'w') as fd:
                file_name = os.path.basename(save_name)[:-4] + '\n'
                bboxs_num = str(len(boxes)) + '\n'
                fd.write(file_name)
                fd.write(bboxs_num)
                for box in boxes:
                    fd.write('%d %d %d %d %.03f' % (box[0], box[1], box[2], box[3], box[4] if box[4] <= 1 else 1) + '\n')
        print('done.')","for image_path in tqdm(glob.glob(os.path.join(testset_folder, '*'))):
    if image_path.endswith('.txt'):
        continue
    img0 = cv2.imread(image_path)
    if img0 is None:
        print(f'ignore : {image_path}')
        continue
    boxes = detect(model, img0)
    image_name = os.path.basename(image_path)
    txt_name = os.path.splitext(image_name)[0] + '.txt'
    save_name = os.path.join(opt.save_folder, pict_folder[image_name], txt_name)
    dirname = os.path.dirname(save_name)
    if not os.path.isdir(dirname):
        os.makedirs(dirname)
    with open(save_name, 'w') as fd:
        file_name = os.path.basename(save_name)[:-4] + '\n'
        bboxs_num = str(len(boxes)) + '\n'
        fd.write(file_name)
        fd.write(bboxs_num)
        for box in boxes:
            fd.write('%d %d %d %d %.03f' % (box[0], box[1], box[2], box[3], box[4] if box[4] <= 1 else 1) + '\n')
print('done.')","for image_path in tqdm(glob.glob(os.path.join(testset_folder, '*'))):
    if image_path.endswith('.txt'):
        continue
    img0 = cv2.imread(image_path)
    if img0 is None:
        print(f'ignore : {image_path}')
        continue
    boxes = detect(model, img0)
    image_name = os.path.basename(image_path)
    txt_name = os.path.splitext(image_name)[0] + '.txt'
    save_name = os.path.join(opt.save_folder, pict_folder[image_name], txt_name)
    dirname = os.path.dirname(save_name)
    if not os.path.isdir(dirname):
        os.makedirs(dirname)
    with open(save_name, 'w') as fd:
        file_name = os.path.basename(save_name)[:-4] + '\n'
        bboxs_num = str(len(boxes)) + '\n'
        fd.write(file_name)
        fd.write(bboxs_num)
        for box in boxes:
            fd.write('%d %d %d %d %.03f' % (box[0], box[1], box[2], box[3], box[4] if box[4] <= 1 else 1) + '\n')
print('done.')",find_wrong,-1
centerNet-deep-sort,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/centerNet-deep-sort/CenterNet/src/tools/calc_coco_overlap.py,https://github.com/kimyoon-young/centerNet-deep-sort/tree/master/CenterNet/src/tools/calc_coco_overlap.py,,count_agnostic$117,"def count_agnostic(split):
    coco = COCO.COCO(ANN_PATH + ANN_FILES[split])
    images = coco.getImgIds()
    cnt = 0
    for img_id in images:
        ann_ids = coco.getAnnIds(imgIds=[img_id])
        anns = coco.loadAnns(ids=ann_ids)
        centers = []
        for ann in anns:
            bbox = ann['bbox']
            center = ((bbox[0] + bbox[2] / 2) // 4, (bbox[1] + bbox[3] / 2) // 4)
            for c in centers:
                if center[0] == c[0] and center[1] == c[1]:
                    cnt += 1
            centers.append(center)
    print('find {} collisions!'.format(cnt))","for img_id in images:
    ann_ids = coco.getAnnIds(imgIds=[img_id])
    anns = coco.loadAnns(ids=ann_ids)
    centers = []
    for ann in anns:
        bbox = ann['bbox']
        center = ((bbox[0] + bbox[2] / 2) // 4, (bbox[1] + bbox[3] / 2) // 4)
        for c in centers:
            if center[0] == c[0] and center[1] == c[1]:
                cnt += 1
        centers.append(center)","for img_id in images:
    ann_ids = coco.getAnnIds(imgIds=[img_id])
    anns = coco.loadAnns(ids=ann_ids)
    centers = []
    for ann in anns:
        bbox = ann['bbox']
        center = ((bbox[0] + bbox[2] // 2) // 4, (bbox[1] + bbox[3] // 2) // 4)
        if center in centers:
            cnt += 1
        else:
            centers.append(center)
    cnt += len(centers) - 1
print('find {} collisions!'.format(cnt))",find_wrong,-1
videos,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/videos/_2017/nn/part2.py,https://github.com/3b1b/videos/tree/master/_2017/nn/part2.py,EmphasizeComplexityOfCostFunction,show_network_as_a_function$1233,"def show_network_as_a_function(self):
    title = TexText('Neural network function')
    title.shift(FRAME_X_RADIUS * RIGHT / 2)
    title.to_edge(UP)
    underline = Line(LEFT, RIGHT)
    underline.stretch_to_fit_width(title.get_width())
    underline.next_to(title, DOWN, SMALL_BUFF)
    self.add(title, underline)
    words = self.get_function_description_words('784 numbers (pixels)', '10 numbers', '13{,}002 weights/biases')
    (input_words, output_words, parameter_words) = words
    for word in words:
        self.add(word[0])
    in_vect = get_organized_images()[7][8]
    activations = self.network.get_activation_of_all_layers(in_vect)
    image = MNistMobject(in_vect)
    image.set_height(1.5)
    image_label = TexText('Input')
    image_label.set_color(input_words[0].get_color())
    image_label.next_to(image, UP, SMALL_BUFF)
    arrow = Arrow(LEFT, RIGHT, color=WHITE)
    arrow.next_to(image, RIGHT)
    output = self.num_vect_to_column_vector(activations[-1], 2)
    output.next_to(arrow, RIGHT)
    group = Group(image, image_label, arrow, output)
    group.next_to(self.network_mob, UP, 0, RIGHT)
    dot = Dot()
    dot.move_to(input_words.get_right())
    dot.set_fill(opacity=0.5)
    self.play(FadeIn(input_words[1], lag_ratio=0.5))
    self.play(dot.move_to, image, dot.set_fill, None, 0, FadeIn(image), FadeIn(image_label))
    self.activate_network(in_vect, GrowArrow(arrow), FadeIn(output), FadeIn(output_words[1]))
    self.wait()
    self.play(FadeIn(parameter_words[1]), self.get_edge_animation())
    self.wait(2)
    self.to_fade = group
    self.curr_words = words
    self.title = title
    self.underline = underline","in_vect = get_organized_images()[7][8]
activations = self.network.get_activation_of_all_layers(in_vect)","for in_vect in get_organized_images()[7][8]:
    activations = self.network.get_activation_of_all_layers(in_vect)",find_wrong,2
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/report/sales_payment_summary/sales_payment_summary.py,,get_sales_payment_data$66,"def get_sales_payment_data(filters, columns):
    data = []
    show_payment_detail = False
    sales_invoice_data = get_sales_invoice_data(filters)
    mode_of_payments = get_mode_of_payments(filters)
    mode_of_payment_details = get_mode_of_payment_details(filters)
    if filters.get('payment_detail'):
        show_payment_detail = True
    else:
        show_payment_detail = False
    for inv in sales_invoice_data:
        owner_posting_date = inv['owner'] + cstr(inv['posting_date'])
        if show_payment_detail:
            row = [inv.posting_date, inv.owner, ' ', inv.net_total, inv.total_taxes, 0]
            data.append(row)
            for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
                row = [inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0]
                data.append(row)
        else:
            total_payment = 0
            for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
                total_payment = total_payment + mop_detail[1]
            row = [inv.posting_date, inv.owner, ', '.join(mode_of_payments.get(owner_posting_date, [])), inv.net_total, inv.total_taxes, total_payment]
            data.append(row)
    return data","for inv in sales_invoice_data:
    owner_posting_date = inv['owner'] + cstr(inv['posting_date'])
    if show_payment_detail:
        row = [inv.posting_date, inv.owner, ' ', inv.net_total, inv.total_taxes, 0]
        data.append(row)
        for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
            row = [inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0]
            data.append(row)
    else:
        total_payment = 0
        for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
            total_payment = total_payment + mop_detail[1]
        row = [inv.posting_date, inv.owner, ', '.join(mode_of_payments.get(owner_posting_date, [])), inv.net_total, inv.total_taxes, total_payment]
        data.append(row)","for inv in sales_invoice_data:
    owner_posting_date = inv['owner'] + cstr(inv['posting_date'])
    if show_payment_detail:
        for mop_detail in mode_of_payment_details.get(owner_posting_date, []):
            data.append([inv.posting_date, inv.owner, ' ', inv.net_total, inv.total_taxes, 0])
            data.append([inv.posting_date, inv.owner, mop_detail[0], 0, 0, mop_detail[1], 0])
    else:
        total_payment = sum((mop_detail[1] for mop_detail in mode_of_payment_details.get(owner_posting_date, [])))
        data.append([inv.posting_date, inv.owner, ', '.join(mode_of_payments.get(owner_posting_date, [])), inv.net_total, inv.total_taxes, total_payment])",find_wrong,-1
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/google/cloud/forseti/scanner/scanners/groups_settings_scanner.py,https://github.com/forseti-security/forseti-security/tree/master/google/cloud/forseti/scanner/scanners/groups_settings_scanner.py,GroupsSettingsScanner,_retrieve$134,"def _retrieve(self):
    """"""Runs the data collection.

        Returns:
            tupl: 2 lists of GroupsSettings objects, 1 only for settings that
            have iam policies and 1 with all groups settings.
        Raises:
            ValueError: if resources have an unexpected type.
        """"""
    all_groups_settings = []
    iam_groups_settings = []
    model_manager = self.service_config.model_manager
    (scoped_session, data_access) = model_manager.get(self.model_name)
    with scoped_session as session:
        for settings in data_access.scanner_fetch_groups_settings(session, True):
            email = settings[0].split('group/')[1]
            iam_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))
        for settings in data_access.scanner_fetch_groups_settings(session, False):
            email = settings[0].split('group/')[1]
            all_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))
    return (all_groups_settings, iam_groups_settings)","for settings in data_access.scanner_fetch_groups_settings(session, True):
    email = settings[0].split('group/')[1]
    iam_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))
for settings in data_access.scanner_fetch_groups_settings(session, False):
    email = settings[0].split('group/')[1]
    all_groups_settings.append(groups_settings.GroupsSettings.from_json(email, settings[1]))","for is_iam in [True, False]:
    groups_settings_list = iam_groups_settings if is_iam else all_groups_settings
    for settings in data_access.scanner_fetch_groups_settings(session, is_iam):
        email = settings[0].split('group/')[1]
        groups_settings_list.append(groups_settings.GroupsSettings.from_json(email, settings[1]))",find_wrong,-1
TauonMusicBox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TauonMusicBox/t_modules/t_main.py,https://github.com/Taiko2k/TauonMusicBox/tree/master/t_modules/t_main.py,,update_set$2149,"def update_set():
    wid = gui.plw - round(16 * gui.scale)
    if gui.tracklist_center_mode:
        wid = gui.tracklist_highlight_width - round(16 * gui.scale)
    total = 0
    for item in gui.pl_st:
        if item[2] is False:
            total += item[1]
        else:
            wid -= item[1]
    if wid <= 75:
        wid = 75
    for i in range(len(gui.pl_st)):
        if gui.pl_st[i][2] is False and total:
            gui.pl_st[i][1] = int(round(gui.pl_st[i][1] / total * wid))","for i in range(len(gui.pl_st)):
    if gui.pl_st[i][2] is False and total:
        gui.pl_st[i][1] = int(round(gui.pl_st[i][1] / total * wid))","for item in gui.pl_st:
    if item[2] is False and total:
        item[1] = int(round(item[1] / total * wid))",find_wrong,-1
PersonRelationKnowledgeGraph,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PersonRelationKnowledgeGraph/collect_person_rel.py,https://github.com/liuhuanyong/PersonRelationKnowledgeGraph/tree/master//collect_person_rel.py,PersonSpider,modify_data$131,"def modify_data(self):
    f_rel = open('rel_data.txt', 'w+')
    f_reltype = open('rel_type.txt', 'w+')
    f_person = open('person2id.txt', 'w+')
    person_dict = {}
    rel_dict = {}
    rel_list = set()
    rel_types = []
    for item in self.conn['person_rel']['data2'].find():
        nodes = item['nodes']
        for node in nodes:
            id = node['id']
            name = node['name']
            person_dict[id] = name
    for item in self.conn['person_rel']['data2'].find():
        links = item['links']
        for link in links:
            from_person = person_dict.get(link['from'], '')
            to_person = person_dict.get(link['to'], '')
            if not from_person or not to_person:
                continue
            rel_name = link['name']
            rel_type = link['type']
            rel_dict[rel_name] = rel_type
            data = [from_person, to_person, rel_name, str(rel_type)]
            rel_list.add('###'.join(data))
    rels_num = len(rel_list)
    persons_num = len(person_dict.keys())
    for rel in rel_list:
        if len(rel.split('###')) != 4:
            continue
        rel_name = rel.split('###')[2]
        rel_types.append(rel_name)
    for (id, name) in person_dict.items():
        f_person.write(str(id) + '\t' + name + '\n')
    reltype_dict = Counter(rel_types).most_common()
    sum = 0.0
    for i in reltype_dict:
        rel_name = i[0]
        rel_freq = i[1]
        rel_percent = rel_freq / rels_num
        sum += rel_percent
        f_reltype.write(rel_name + '\t' + str(rel_freq) + '\t' + str(rel_percent) + '\t' + str(sum) + '\n')
    f_rel.write('\n'.join(list(rel_list)))
    f_person.close()
    f_rel.close()
    f_reltype.close()
    print('rels_num', rels_num)
    print('persons_num', persons_num)
    return","for item in self.conn['person_rel']['data2'].find():
    nodes = item['nodes']
    for node in nodes:
        id = node['id']
        name = node['name']
        person_dict[id] = name
for item in self.conn['person_rel']['data2'].find():
    links = item['links']
    for link in links:
        from_person = person_dict.get(link['from'], '')
        to_person = person_dict.get(link['to'], '')
        if not from_person or not to_person:
            continue
        rel_name = link['name']
        rel_type = link['type']
        rel_dict[rel_name] = rel_type
        data = [from_person, to_person, rel_name, str(rel_type)]
        rel_list.add('###'.join(data))
for rel in rel_list:
    if len(rel.split('###')) != 4:
        continue
    rel_name = rel.split('###')[2]
    rel_types.append(rel_name)","for item in self.conn['person_rel']['data2'].find():
    nodes = item['nodes']
    for node in nodes:
        id = node['id']
        name = node['name']
        person_dict[id] = name
    links = item['links']
    for link in links:
        from_person = person_dict.get(link['from'], '')
        to_person = person_dict.get(link['to'], '')
        if not from_person or not to_person:
            continue
        rel_name = link['name']
        rel_type = link['type']
        rel_dict[rel_name] = rel_type
        data = [from_person, to_person, rel_name, str(rel_type)]
        rel_list.add('###'.join(data))
        if len(data) == 4:
            rel_types.append(data[2])",find_wrong,2
qutip,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutip/qutip/legacy/bloch_redfield.py,https://github.com/qutip/qutip/tree/master/qutip/legacy/bloch_redfield.py,,bloch_redfield_tensor$21,"def bloch_redfield_tensor(H, a_ops, spectra_cb=None, c_ops=[], use_secular=True, sec_cutoff=0.1):
    """"""
    Calculate the Bloch-Redfield tensor for a system given a set of operators
    and corresponding spectral functions that describes the system's coupling
    to its environment.

    .. note::

        This tensor generation requires a time-independent Hamiltonian.

    Parameters
    ----------

    H : :class:`qutip.qobj`
        System Hamiltonian.

    a_ops : list of :class:`qutip.qobj`
        List of system operators that couple to the environment.

    spectra_cb : list of callback functions
        List of callback functions that evaluate the noise power spectrum
        at a given frequency.

    c_ops : list of :class:`qutip.qobj`
        List of system collapse operators.

    use_secular : bool
        Flag (True of False) that indicates if the secular approximation should
        be used.
    
    sec_cutoff : float {0.1}
        Threshold for secular approximation.

    Returns
    -------

    R, kets: :class:`qutip.Qobj`, list of :class:`qutip.Qobj`

        R is the Bloch-Redfield tensor and kets is a list eigenstates of the
        Hamiltonian.

    """"""
    if not spectra_cb is None:
        warnings.warn('The use of spectra_cb is depreciated.', DeprecationWarning)
        _a_ops = []
        for (kk, a) in enumerate(a_ops):
            _a_ops.append([a, spectra_cb[kk]])
        a_ops = _a_ops
    if not isinstance(H, Qobj):
        raise TypeError('H must be an instance of Qobj')
    for a in a_ops:
        if not isinstance(a[0], Qobj) or not a[0].isherm:
            raise TypeError('Operators in a_ops must be Hermitian Qobj.')
    if c_ops is None:
        c_ops = []
    (evals, ekets) = H.eigenstates()
    N = len(evals)
    K = len(a_ops)
    if K == 0:
        Heb = qdiags(evals, 0, dims=H.dims)
        L = liouvillian(Heb, c_ops=[c_op.transform(ekets) for c_op in c_ops])
        return (L, ekets)
    A = np.array([a_ops[k][0].transform(ekets).full() for k in range(K)])
    Jw = np.zeros((K, N, N), dtype=complex)
    W = np.real(evals[:, np.newaxis] - evals[np.newaxis, :])
    for k in range(K):
        for n in range(N):
            for m in range(N):
                Jw[k, n, m] = a_ops[k][1](W[n, m])
    dw_min = np.abs(W[W.nonzero()]).min()
    Iabs = np.empty((N * N, 3), dtype=int)
    for (I, Iab) in enumerate(Iabs):
        Iab[0] = I
        Iab[1:] = vec2mat_index(N, I)
    Heb = qdiags(evals, 0, dims=H.dims)
    L = liouvillian(Heb, c_ops=[c_op.transform(ekets) for c_op in c_ops])
    rows = []
    cols = []
    data = []
    for (I, a, b) in Iabs:
        if use_secular:
            Jcds = Iabs[np.where(np.abs(W[a, b] - W[Iabs[:, 1], Iabs[:, 2]]) < dw_min * sec_cutoff)]
        else:
            Jcds = Iabs
        for (J, c, d) in Jcds:
            elem = 0 + 0j
            elem += 0.5 * np.sum(A[:, a, c] * A[:, d, b] * (Jw[:, c, a] + Jw[:, d, b]))
            if b == d:
                elem -= 0.5 * np.sum(A[:, a, :] * A[:, :, c] * Jw[:, c, :])
            if a == c:
                elem -= 0.5 * np.sum(A[:, d, :] * A[:, :, b] * Jw[:, d, :])
            if elem != 0:
                rows.append(I)
                cols.append(J)
                data.append(elem)
    R = arr_coo2fast(np.array(data, dtype=complex), np.array(rows, dtype=np.int32), np.array(cols, dtype=np.int32), N ** 2, N ** 2)
    L.data = L.data + R
    return (L, ekets)","for k in range(K):
    for n in range(N):
        for m in range(N):
            Jw[k, n, m] = a_ops[k][1](W[n, m])
dw_min = np.abs(W[W.nonzero()]).min()
Iabs = np.empty((N * N, 3), dtype=int)
for (I, Iab) in enumerate(Iabs):
    Iab[0] = I
    Iab[1:] = vec2mat_index(N, I)
Heb = qdiags(evals, 0, dims=H.dims)
L = liouvillian(Heb, c_ops=[c_op.transform(ekets) for c_op in c_ops])
rows = []
cols = []
data = []
for (I, a, b) in Iabs:
    if use_secular:
        Jcds = Iabs[np.where(np.abs(W[a, b] - W[Iabs[:, 1], Iabs[:, 2]]) < dw_min * sec_cutoff)]
    else:
        Jcds = Iabs
    for (J, c, d) in Jcds:
        elem = 0 + 0j
        elem += 0.5 * np.sum(A[:, a, c] * A[:, d, b] * (Jw[:, c, a] + Jw[:, d, b]))
        if b == d:
            elem -= 0.5 * np.sum(A[:, a, :] * A[:, :, c] * Jw[:, c, :])
        if a == c:
            elem -= 0.5 * np.sum(A[:, d, :] * A[:, :, b] * Jw[:, d, :])
        if elem != 0:
            rows.append(I)
            cols.append(J)
            data.append(elem)
R = arr_coo2fast(np.array(data, dtype=complex), np.array(rows, dtype=np.int32), np.array(cols, dtype=np.int32), N ** 2, N ** 2)
L.data = L.data + R","for k, a_op in enumerate(a_ops):
    # do explicit loops here in case spectra_cb[k] can not deal with array arguments
    Jw[k] = np.array([[a_op[1](W[n, m]) for m in range(N)] for n in range(N)])

dw_min = np.abs(W[W.nonzero()]).min()

# pre-calculate mapping between global index I and system indices a,b
Iabs = np.array([[I, *vec2mat_index(N, I)] for I in range(N**2)], dtype=int)

# unitary part + dissipation from c_ops (if given):
Heb = qdiags(evals, 0, dims=H.dims)
L = liouvillian(Heb, c_ops=[c_op.transform(ekets) for c_op in c_ops])

# dissipative part:
rows = []
cols = []
data = []
for I, a, b in Iabs:
    # only check use_secular once per I
    if use_secular:
        # only loop over those indices J which actually contribute
        Jcds = Iabs[np.where(np.abs(W[a, b] - W[Iabs[:,1], Iabs[:,2]]) < dw_min * sec_cutoff)]
    else:
        Jcds = Iabs
    for J, c, d in Jcds:
        elem = 0+0j
        # summed over k, i.e., each operator coupling the system to the environment
        elem += 0.5 * np.sum([A[k, a, c] * A[k, d, b] * (Jw[k, c, a] + Jw[k, d, b]) for k in range(K)])
        if b==d:
            #                  sum_{k,n} A[k, a, n] * A[k, n, c] * Jw[k, c, n])
            elem -= 0.5 * np.sum([A[k, a, n] * A[k, n, c] * Jw[k, c, n] for k in range(K) for n in range(N)])
        if a==c:
            #                  sum_{k,n} A[k, d, n] * A[k, n, b] * Jw[k, d, n])
            elem -= 0.5 * np.sum([A[k, d, n] * A[k, n, b] * Jw[k, d, n] for k in range",find_wrong,-1
OpenSfM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenSfM/opensfm/large/tools.py,https://github.com/mapillary/OpenSfM/tree/master/opensfm/large/tools.py,,add_cluster_neighbors$43,"def add_cluster_neighbors(positions, labels, centers, max_distance) -> List[List[np.ndarray]]:
    reflla = np.mean(positions, 0)
    reference = geo.TopocentricConverter(reflla[0], reflla[1], 0)
    topocentrics = []
    for position in positions:
        (x, y, z) = reference.to_topocentric(position[0], position[1], 0)
        topocentrics.append([x, y])
    topocentrics = np.array(topocentrics)
    topo_tree = spatial.cKDTree(topocentrics)
    clusters = []
    for label in np.arange(centers.shape[0]):
        cluster_indices = np.where(labels == label)[0]
        neighbors = []
        for i in cluster_indices:
            neighbors.extend(topo_tree.query_ball_point(topocentrics[i], max_distance))
        cluster = list(np.union1d(cluster_indices, neighbors))
        clusters.append(cluster)
    return clusters","for label in np.arange(centers.shape[0]):
    cluster_indices = np.where(labels == label)[0]
    neighbors = []
    for i in cluster_indices:
        neighbors.extend(topo_tree.query_ball_point(topocentrics[i], max_distance))
    cluster = list(np.union1d(cluster_indices, neighbors))
    clusters.append(cluster)","for label in range(centers.shape[0]):
    cluster_indices = np.where(labels == label)[0]
    neighbors = []
    for i in cluster_indices:
        neighbors.extend(topo_tree.query_ball_point(topocentrics[i], max_distance))
    cluster = list(np.union1d(cluster_indices, neighbors))
    clusters.append(cluster)",find_wrong,-1
congress-legislators,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/congress-legislators/scripts/icpsr_ids.py,https://github.com/unitedstates/congress-legislators/tree/master/scripts/icpsr_ids.py,,run$15,"def run():
    cache = utils.flags().get('cache', True)
    force = not cache
    only_bioguide = utils.flags().get('bioguide', None)
    congress = utils.flags().get('congress', None)
    data_files = []
    print('Loading %s...' % 'legislators-current.yaml')
    legislators = load_data('legislators-current.yaml')
    data_files.append((legislators, 'legislators-current.yaml'))
    print('Loading %s...' % 'legislators-historical.yaml')
    legislators = load_data('legislators-historical.yaml')
    data_files.append((legislators, 'legislators-historical.yaml'))
    if congress == None:
        raise Exception('the --congress flag is required')
    elif int(congress) < 10 and int(congress) > 0:
        url_senate = 'https://voteview.com/static/data/out/members/S00%s_members.csv' % congress
        url_house = 'https://voteview.com/static/data/out/members/H00%s_members.csv' % congress
    elif int(congress) < 100 and int(congress) >= 10:
        url_senate = 'https://voteview.com/static/data/out/members/S0%s_members.csv' % congress
        url_house = 'https://voteview.com/static/data/out/members/H0%s_members.csv' % congress
    elif int(congress) >= 100:
        url_senate = 'https://voteview.com/static/data/out/members/S%s_members.csv' % congress
        url_house = 'https://voteview.com/static/data/out/members/H%s_members.csv' % congress
    else:
        raise Exception('no data for congress ' + congress)
    senate_destination = 'icpsr/source/senate_rollcall%s.txt' % congress
    senate_data = utils.download(url_senate, senate_destination, force)
    house_destination = 'icpsr/source/house_rollcall%s.txt' % congress
    house_data = utils.download(url_house, house_destination, force)
    error_log = csv.writer(open('cache/errors/mismatch/mismatch_%s.csv' % congress, 'w'))
    error_log.writerow(['error_type', 'matches', 'icpsr_name', 'icpsr_state', 'is_territory', 'old_id', 'new_id'])
    read_files = [('sen', senate_data), ('rep', house_data)]
    print('Running for congress ' + congress)
    for (read_file_chamber, read_file_content) in read_files:
        for data_file in data_files:
            for legislator in data_file[0]:
                num_matches = 0
                write_id = ''
                bioguide = legislator['id'].get('bioguide', None)
                if only_bioguide and bioguide != only_bioguide:
                    continue
                chamber = legislator['terms'][len(legislator['terms']) - 1]['type']
                if chamber != read_file_chamber:
                    continue
                latest_congress = utils.congress_from_legislative_year(utils.legislative_year(parse_date(legislator['terms'][len(legislator['terms']) - 1]['start'])))
                if chamber == 'sen':
                    congresses = [latest_congress, latest_congress + 1, latest_congress + 2]
                else:
                    congresses = [latest_congress]
                if int(congress) not in congresses:
                    continue
                last_name = legislator['name']['last'].upper()
                state = utils.states[legislator['terms'][len(legislator['terms']) - 1]['state']].upper()[:7].strip()
                content_as_file = StringIO(read_file_content)
                content_parsed = csv.reader(content_as_file, delimiter=',')
                for icpsr_member in content_parsed:
                    if bioguide == icpsr_member[10]:
                        num_matches += 1
                        write_id = int(icpsr_member[2])
                if 'icpsr' in legislator['id']:
                    if write_id == legislator['id']['icpsr'] or write_id == '':
                        continue
                    elif write_id != legislator['id']['icpsr'] and write_id != '':
                        error_log.writerow(['Incorrect_ID', 'NA', last_name[:8], state, 'NA', legislator['id']['icpsr'], write_id])
                        print('ID updated for %s' % last_name)
                if num_matches == 1:
                    legislator['id']['icpsr'] = int(write_id)
                elif state == 'GUAM' or state == 'PUERTO' or state == 'VIRGIN' or (state == 'DISTRIC') or (state == 'AMERICA') or (state == 'NORTHER') or (state == 'PHILIPP'):
                    print('error: non 1 match')
                    error_log.writerow(['Non_1_match_number', str(num_matches), last_name[:8], state, 'Y', 'NA', 'NA'])
                else:
                    print(str(num_matches) + ' matches found for ' + last_name[:8] + ', ' + state + ' in congress ' + str(congress))
                    error_log.writerow(['Non_1_match_number', str(num_matches), last_name, state, 'N', 'NA', 'NA'])
            save_data(data_file[0], data_file[1])","for (read_file_chamber, read_file_content) in read_files:
    for data_file in data_files:
        for legislator in data_file[0]:
            ...
            if chamber != read_file_chamber:
                continue
            ...
            if chamber == 'sen':
                congresses = [latest_congress, latest_congress + 1, latest_congress + 2]
            else:
                congresses = [latest_congress]
            if int(congress) not in congresses:
                continue
            ...
            content_as_file = StringIO(read_file_content)
            content_parsed = csv.reader(content_as_file, delimiter=',')
            for icpsr_member in content_parsed:
                ...
                if bioguide == icpsr_member[10]:
                    ...
            ...","for legislator in itertools.chain(*data_files):
    ...
    for (read_file_chamber, read_file_content) in read_files:
        if chamber != read_file_chamber:
            continue
        ...
        if int(congress) not in congresses:
            continue
        ...
        content_parsed = csv.reader(io.StringIO(read_file_content), delimiter=',')
        for icpsr_member in content_parsed:
            ...
            if bioguide == icpsr_member[10]:
                ...
    ...",find_wrong,-1
cats-blender-plugin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cats-blender-plugin/extern_tools/google_trans_new/google_trans_new.py,https://github.com/absolute-quantum/cats-blender-plugin/tree/master/extern_tools/google_trans_new/google_trans_new.py,google_translator,translate$111,"def translate(self, text, lang_tgt='auto', lang_src='auto', pronounce=False):
    print('\n\nDEBUG: Translating', text)
    try:
        lang = LANGUAGES[lang_src]
    except:
        lang_src = 'auto'
    try:
        lang = LANGUAGES[lang_tgt]
    except:
        lang_src = 'auto'
    text = str(text)
    if len(text) >= 5000:
        return 'Warning: Can only detect less than 5000 characters'
    if len(text) == 0:
        return ''
    headers = {'Referer': 'http://translate.google.{}/'.format(self.url_suffix), 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36', 'Content-Type': 'application/x-www-form-urlencoded;charset=utf-8'}
    freq = self._package_rpc(text, lang_src, lang_tgt)
    response = requests.Request(method='POST', url=self.url, data=freq, headers=headers)
    try:
        if self.proxies == None or type(self.proxies) != dict:
            self.proxies = {}
        with requests.Session() as s:
            s.proxies = self.proxies
            r = s.send(request=response.prepare(), verify=False, timeout=self.timeout)
        for line in r.iter_lines(chunk_size=1024):
            decoded_line = line.decode('utf-8')
            if 'MkEWBc' in decoded_line:
                try:
                    response = decoded_line + ']'
                    response = json.loads(response)
                    print('DEBUG1', response)
                    response = list(response)
                    response = json.loads(response[0][2])
                    response_ = list(response)
                    response = response_[1][0]
                    print('DEBUG2', response)
                    if len(response) == 1:
                        if len(response[0]) > 5:
                            sentences = response[0][5]
                            print('DEBUG3', sentences)
                        else:
                            sentences = response[0][0]
                            print('DEBUG4', sentences)
                            if pronounce == False:
                                return sentences
                            elif pronounce == True:
                                return [sentences, None, None]
                        translate_text = ''
                        translations = sentences[0]
                        if not translations:
                            return text
                        translate_text = translations[0]
                        if pronounce == False:
                            return translate_text
                        elif pronounce == True:
                            pronounce_src = response_[0][0]
                            pronounce_tgt = response_[1][0][0][1]
                            return [translate_text, pronounce_src, pronounce_tgt]
                    elif len(response) == 2:
                        sentences = []
                        for i in response:
                            sentences.append(i[0])
                        if pronounce == False:
                            return sentences
                        elif pronounce == True:
                            pronounce_src = response_[0][0]
                            pronounce_tgt = response_[1][0][0][1]
                            return [sentences, pronounce_src, pronounce_tgt]
                except Exception as e:
                    raise e
        r.raise_for_status()
    except requests.exceptions.ConnectTimeout as e:
        raise e
    except requests.exceptions.HTTPError as e:
        raise google_new_transError(tts=self, response=r)
    except requests.exceptions.RequestException as e:
        raise google_new_transError(tts=self)","for line in r.iter_lines(chunk_size=1024):
    decoded_line = line.decode('utf-8')
    if 'MkEWBc' in decoded_line:
        try:
            response = decoded_line + ']'
            response = json.loads(response)
            print('DEBUG1', response)
            response = list(response)
            response = json.loads(response[0][2])
            response_ = list(response)
            response = response_[1][0]
            print('DEBUG2', response)
            if len(response) == 1:
                if len(response[0]) > 5:
                    sentences = response[0][5]
                    print('DEBUG3', sentences)
                else:
                    sentences = response[0][0]
                    print('DEBUG4', sentences)
                    if pronounce == False:
                        return sentences
                    elif pronounce == True:
                        return [sentences, None, None]
                translate_text = ''
                translations = sentences[0]
                if not translations:
                    return text
                translate_text = translations[0]
                if pronounce == False:
                    return translate_text
                elif pronounce == True:
                    pronounce_src = response_[0][0]
                    pronounce_tgt = response_[1][0][0][1]
                    return [translate_text, pronounce_src, pronounce_tgt]
            elif len(response) == 2:
                sentences = []
                for i in response:
                    sentences.append(i[0])
                if pronounce == False:
                    return sentences
                elif pronounce == True:
                    pronounce_src = response_[0][0]
                    pronounce_tgt = response_[1][0][0][1]
                    return [sentences, pronounce_src, pronounce_tgt]
        except Exception as e:
            raise e","for line in r.iter_lines(chunk_size=1024):
    decoded_line = line.decode('utf-8')
    if 'MkEWBc' in decoded_line:
        try:
            response = decoded_line + ']'
            response = json.loads(response)
            print('DEBUG1', response)
            response = list(response)
            response = json.loads(response[0][2])
            response_ = list(response)
            response = response_[1][0]
            print('DEBUG2', response)
            if len(response) == 1:
                if len(response[0]) > 5:
                    sentences = response[0][5]
                    print('DEBUG3', sentences)
                else:
                    sentences = response[0][0]
                    print('DEBUG4', sentences)
                    if pronounce == False:
                        return sentences
                    elif pronounce == True:
                        return [sentences, None, None]
                translate_text = ''
                translations = sentences[0]
                if not translations:
                    return text
                translate_text = translations[0]
                if pronounce == False:
                    return translate_text
                elif pronounce == True:
                    pronounce_src = response_[0][0]
                    pronounce_tgt = response_[1][0][0][1]
                    return [translate_text, pronounce_src, pronounce_tgt]
            elif len(response) == 2:
                sentences = []
                for i in response:
                    sentences.append(i[0])
                if pronounce == False:
                    return sentences
                elif pronounce == True:
                    pronounce_src = response_[0][0]
                    pronounce_tgt = response_[1][0][0][1]
                    return [sentences, pronounce_src, pronounce_tgt]
        except Exception as e:
            raise e",find_wrong,2
GWSL-Source,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GWSL-Source/manager.py,https://github.com/Opticos/GWSL-Source/tree/master//manager.py,,get_version$605,"def get_version(machine):
    try:
        machines = os.popen('wsl.exe -l -v').read()
        machines = re.sub('[^a-z A-Z0-9./\\n-]', '', machines).splitlines()
        machines2 = []
        wsl_1 = True
        for i in machines:
            b = ''.join(i).split()
            if 'VERSION' in b:
                wsl_1 = False
            if 'NAME' not in b and b != [] and (b != None):
                machines2.append(b)
        if wsl_1 == True:
            print('assuming wsl 1')
            return 1
        for i in machines2:
            if i[0] == machine:
                return int(i[2])
        return 1
    except:
        return 1","for i in machines2:
    if i[0] == machine:
        return int(i[2])
return 1","for machine_info in machines2:
    if machine_info[0] == machine:
        return int(machine_info[2])
return 1",find_wrong,-1
MarkdownEditing,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MarkdownEditing/plugins/lint.py,https://github.com/SublimeText-Markdown/MarkdownEditing/tree/master/plugins/lint.py,MdeMarkdownLintCommand,run$76,"def run(self, edit):
    mddef = globals()['mddef']
    text = self.view.substr(sublime.Region(0, self.view.size()))
    st = self.view.settings().get('mde.lint', {})
    uselist = []
    disablelist = st['disable']
    for cl in mddef.__subclasses__():
        if cl.__name__ not in disablelist:
            uselist.append(cl)
    result = []
    for mddef in uselist:
        r = self.test(mddef(st[mddef.__name__] if mddef.__name__ in st else None, self.view), text)
        result.extend(r)
    window = self.view.window() or sublime.active_window()
    if len(result) > 0:
        sublime.status_message('MarkdownLint: %d error(s) found' % len(result))
        result = sorted(result, key=lambda t: t[0])
        outputtxt = ''
        for t in result:
            (row, col) = self.view.rowcol(t[0])
            outputtxt += 'line %d: %s, %s\n' % (row + 1, t[1], t[2])
        output = window.create_output_panel('mde')
        output.run_command('insert', {'characters': outputtxt})
        window.run_command('show_panel', {'panel': 'output.mde'})
    else:
        sublime.status_message('MarkdownLint: no errors found')
        window.destroy_output_panel('mde')","for mddef in uselist:
    r = self.test(mddef(st[mddef.__name__] if mddef.__name__ in st else None, self.view), text)
    result.extend(r)","for mddef_class in uselist:
    mddef_instance = mddef_class(st.get(mddef_class.__name__), self.view)
    r = self.test(mddef_instance, text)
    result.extend(r)",find_wrong,2
jellyfin-kodi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jellyfin-kodi/jellyfin_kodi/full_sync.py,https://github.com/jellyfin/jellyfin-kodi/tree/master/jellyfin_kodi/full_sync.py,FullSync,remove_library$524,"def remove_library(self, library_id, dialog):
    """""" Remove library by their id from the Kodi database.
        """"""
    direct_path = self.library.direct_path
    with Database('jellyfin') as jellyfindb:
        db = jellyfin_db.JellyfinDatabase(jellyfindb.cursor)
        library = db.get_view(library_id.replace('Mixed:', ''))
        items = db.get_item_by_media_folder(library_id.replace('Mixed:', ''))
        media = 'music' if library.media_type == 'music' else 'video'
        if media == 'music':
            settings('MusicRescan.bool', False)
        if items:
            with self.library.music_database_lock if media == 'music' else self.library.database_lock:
                with Database(media) as kodidb:
                    count = 0
                    if library.media_type == 'mixed':
                        movies = [x for x in items if x[1] == 'Movie']
                        tvshows = [x for x in items if x[1] == 'Series']
                        obj = Movies(self.server, jellyfindb, kodidb, direct_path, library).remove
                        for item in movies:
                            obj(item[0])
                            dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
                            count += 1
                        obj = TVShows(self.server, jellyfindb, kodidb, direct_path, library).remove
                        for item in tvshows:
                            obj(item[0])
                            dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
                            count += 1
                    else:
                        default_args = (self.server, jellyfindb, kodidb, direct_path)
                        for item in items:
                            if item[1] in ('Series', 'Season', 'Episode'):
                                TVShows(*default_args).remove(item[0])
                            elif item[1] in ('Movie', 'BoxSet'):
                                Movies(*default_args).remove(item[0])
                            elif item[1] in ('MusicAlbum', 'MusicArtist', 'AlbumArtist', 'Audio'):
                                Music(*default_args).remove(item[0])
                            elif item[1] == 'MusicVideo':
                                MusicVideos(*default_args).remove(item[0])
                            dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library[0]))
                            count += 1
    self.sync = get_sync()
    if library_id in self.sync['Whitelist']:
        self.sync['Whitelist'].remove(library_id)
    elif 'Mixed:%s' % library_id in self.sync['Whitelist']:
        self.sync['Whitelist'].remove('Mixed:%s' % library_id)
    save_sync(self.sync)","movies = [x for x in items if x[1] == 'Movie']
tvshows = [x for x in items if x[1] == 'Series']
obj = Movies(self.server, jellyfindb, kodidb, direct_path, library).remove
for item in movies:
    obj(item[0])
    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
    count += 1
obj = TVShows(self.server, jellyfindb, kodidb, direct_path, library).remove
for item in tvshows:
    obj(item[0])
    dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
    count += 1","for media_type in ['Movie', 'Series']:
    items = [x for x in items if x[1] == media_type]
    obj = Movies(self.server, jellyfindb, kodidb, direct_path, library).remove if media_type == 'Movie' else TVShows(self.server, jellyfindb, kodidb, direct_path, library).remove
    for item in items:
        obj(item[0])
        dialog.update(int(float(count) / float(len(items)) * 100), heading='%s: %s' % (translate('addon_name'), library.view_name))
        count += 1",find_wrong,-1
pytorch-deeplab-xception,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch-deeplab-xception/modeling/deeplab.py,https://github.com/jfzhang95/pytorch-deeplab-xception/tree/master/modeling/deeplab.py,DeepLab,get_10x_lr_params$58,"def get_10x_lr_params(self):
    modules = [self.aspp, self.decoder]
    for i in range(len(modules)):
        for m in modules[i].named_modules():
            if self.freeze_bn:
                if isinstance(m[1], nn.Conv2d):
                    for p in m[1].parameters():
                        if p.requires_grad:
                            yield p
            elif isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) or isinstance(m[1], nn.BatchNorm2d):
                for p in m[1].parameters():
                    if p.requires_grad:
                        yield p","for i in range(len(modules)):
    for m in modules[i].named_modules():
        if self.freeze_bn:
            if isinstance(m[1], nn.Conv2d):
                for p in m[1].parameters():
                    if p.requires_grad:
                        yield p
        elif isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) or isinstance(m[1], nn.BatchNorm2d):
            for p in m[1].parameters():
                if p.requires_grad:
                    yield p","for module in [self.aspp, self.decoder]:
    for (name, m) in module.named_modules():
        if self.freeze_bn:
            if isinstance(m, nn.Conv2d):
                for p in m.parameters():
                    if p.requires_grad:
                        yield p
        elif isinstance(m, (nn.Conv2d, SynchronizedBatchNorm2d, nn.BatchNorm2d)):
            for p in m.parameters():
                if p.requires_grad:
                    yield p",find_wrong,-1
mimicry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mimicry/tests/modules/test_resblocks.py,https://github.com/kwotsin/mimicry/tree/master/tests/modules/test_resblocks.py,TestResBlocks,test_GBlock$12,"def test_GBlock(self):
    num_classes_list = [0, 10]
    spectral_norm_list = [True, False]
    in_channels = 3
    out_channels = 8
    args_comb = product(num_classes_list, spectral_norm_list)
    for args in args_comb:
        num_classes = args[0]
        spectral_norm = args[1]
        if num_classes > 0:
            y = torch.ones((4,), dtype=torch.int64)
        else:
            y = None
        gen_block_up = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=True, num_classes=num_classes, spectral_norm=spectral_norm)
        gen_block = resblocks.GBlock(in_channels=in_channels, out_channels=out_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)
        gen_block_no_sc = resblocks.GBlock(in_channels=in_channels, out_channels=in_channels, upsample=False, num_classes=num_classes, spectral_norm=spectral_norm)
        assert gen_block_up(self.images, y).shape == (4, 8, 32, 32)
        assert gen_block(self.images, y).shape == (4, 8, 16, 16)
        assert gen_block_no_sc(self.images, y).shape == (4, 3, 16, 16)","num_classes_list = [0, 10]
spectral_norm_list = [True, False]
args_comb = product(num_classes_list, spectral_norm_list)
for args in args_comb:
    num_classes = args[0]
    spectral_norm = args[1]","for num_classes in [0, 10]:
    for spectral_norm in [True, False]:",find_wrong,-1
PythonRobotics,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonRobotics/ArmNavigation/arm_obstacle_navigation/arm_obstacle_navigation_2.py,https://github.com/AtsushiSakai/PythonRobotics/tree/master/ArmNavigation/arm_obstacle_navigation/arm_obstacle_navigation_2.py,NLinkArm,plot_arm$278,"def plot_arm(self, myplt, obstacles=[]):
    myplt.cla()
    for obstacle in obstacles:
        circle = myplt.Circle((obstacle[0], obstacle[1]), radius=0.5 * obstacle[2], fc='k')
        myplt.gca().add_patch(circle)
    for i in range(self.n_links + 1):
        if i is not self.n_links:
            myplt.plot([self.points[i][0], self.points[i + 1][0]], [self.points[i][1], self.points[i + 1][1]], 'r-')
        myplt.plot(self.points[i][0], self.points[i][1], 'k.')
    myplt.xlim([-self.lim, self.lim])
    myplt.ylim([-self.lim, self.lim])
    myplt.draw()","for obstacle in obstacles:
    circle = myplt.Circle((obstacle[0], obstacle[1]), radius=0.5 * obstacle[2], fc='k')
    myplt.gca().add_patch(circle)
for i in range(self.n_links + 1):
    if i is not self.n_links:
        myplt.plot([self.points[i][0], self.points[i + 1][0]], [self.points[i][1], self.points[i + 1][1]], 'r-')
    myplt.plot(self.points[i][0], self.points[i][1], 'k.')","for obstacle in obstacles:
    circle = myplt.Circle((obstacle[0], obstacle[1]), radius=0.5 * obstacle[2], fc='k')
    myplt.gca().add_patch(circle)
for (i, point) in enumerate(self.points):
    if i < self.n_links:
        myplt.plot([point[0], self.points[i + 1][0]], [point[1], self.points[i + 1][1]], 'r-')
    myplt.plot(point[0], point[1], 'k.')",find_wrong,-1
kickthemout,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kickthemout/kickthemout.py,https://github.com/k4m4/kickthemout/tree/master//kickthemout.py,,kickalloff$552,"def kickalloff():
    os.system('clear||cls')
    print('\n{}kickALLOff{} selected...{}\n'.format(RED, GREEN, END))
    global stopAnimation
    stopAnimation = False
    t = threading.Thread(target=scanningAnimation, args=('Hang on...',))
    t.daemon = True
    t.start()
    try:
        scanNetwork()
    except KeyboardInterrupt:
        shutdown()
    stopAnimation = True
    print('Target(s): ')
    for i in range(len(onlineIPs)):
        mac = ''
        for host in hostsList:
            if host[0] == onlineIPs[i]:
                mac = host[1]
        try:
            hostname = utils.socket.gethostbyaddr(onlineIPs[i])[0]
        except:
            hostname = 'N/A'
        vendor = resolveMac(mac)
        print('  [{}{}{}] {}{}{}\t{}{}\t{} ({}{}{}){}'.format(YELLOW, str(i), WHITE, RED, str(onlineIPs[i]), BLUE, mac, GREEN, vendor, YELLOW, hostname, GREEN, END))
    if options.packets is not None:
        print('\n{}Spoofing started... {}( {} pkts/min )'.format(GREEN, END, str(options.packets)))
    else:
        print('\n{}Spoofing started... {}'.format(GREEN, END))
    try:
        reScan = 0
        while True:
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    spoof.sendPacket(defaultInterfaceMac, defaultGatewayIP, host[0], host[1])
            reScan += 1
            if reScan == 4:
                reScan = 0
                scanNetwork()
            if options.packets is not None:
                time.sleep(60 / float(options.packets))
            else:
                time.sleep(10)
    except KeyboardInterrupt:
        print('\n{}Re-arping{} targets...{}'.format(RED, GREEN, END))
        reArp = 1
        while reArp != 10:
            for host in hostsList:
                if host[0] != defaultGatewayIP:
                    try:
                        spoof.sendPacket(defaultGatewayMac, defaultGatewayIP, host[0], host[1])
                    except KeyboardInterrupt:
                        pass
                    except:
                        runDebug()
            reArp += 1
            time.sleep(0.2)
        print('{}Re-arped{} targets successfully.{}'.format(RED, GREEN, END))","for i in range(len(onlineIPs)):
    mac = ''
    for host in hostsList:
        if host[0] == onlineIPs[i]:
            mac = host[1]
    try:
        hostname = utils.socket.gethostbyaddr(onlineIPs[i])[0]
    except:
        hostname = 'N/A'
    vendor = resolveMac(mac)
    print('  [{}{}{}] {}{}{}\t{}{}\t{} ({}{}{}){}'.format(YELLOW, str(i), WHITE, RED, str(onlineIPs[i]), BLUE, mac, GREEN, vendor, YELLOW, hostname, GREEN, END))","for onlineIP in onlineIPs:
    mac = ''
    for host in hostsList:
        if host[0] == onlineIP:
            mac = host[1]
    try:
        hostname = utils.socket.gethostbyaddr(onlineIP)[0]
    except:
        hostname = 'N/A'
    vendor = resolveMac(mac)
    print('  [{}{}{}] {}{}{}\t{}{}\t{} ({}{}{}){}'.format(YELLOW, str(onlineIPs.index(onlineIP)), WHITE, RED, str(onlineIP), BLUE, mac, GREEN, vendor, YELLOW, hostname, GREEN, END))",find_wrong,-1
django-extensions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-extensions/django_extensions/management/commands/show_urls.py,https://github.com/django-extensions/django-extensions/tree/master/django_extensions/management/commands/show_urls.py,Command,extract_views_from_urlpatterns$190,"def extract_views_from_urlpatterns(self, urlpatterns, base='', namespace=None):
    """"""
        Return a list of views from a list of urlpatterns.

        Each object in the returned list is a three-tuple: (view_func, regex, name)
        """"""
    views = []
    for p in urlpatterns:
        if isinstance(p, (URLPattern, RegexURLPattern)):
            try:
                if not p.name:
                    name = p.name
                elif namespace:
                    name = '{0}:{1}'.format(namespace, p.name)
                else:
                    name = p.name
                pattern = describe_pattern(p)
                views.append((p.callback, base + pattern, name))
            except ViewDoesNotExist:
                continue
        elif isinstance(p, (URLResolver, RegexURLResolver)):
            try:
                patterns = p.url_patterns
            except ImportError:
                continue
            if namespace and p.namespace:
                _namespace = '{0}:{1}'.format(namespace, p.namespace)
            else:
                _namespace = p.namespace or namespace
            pattern = describe_pattern(p)
            if isinstance(p, LocaleRegexURLResolver):
                for language in self.LANGUAGES:
                    with translation.override(language[0]):
                        views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))
            else:
                views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))
        elif hasattr(p, '_get_callback'):
            try:
                views.append((p._get_callback(), base + describe_pattern(p), p.name))
            except ViewDoesNotExist:
                continue
        elif hasattr(p, 'url_patterns') or hasattr(p, '_get_url_patterns'):
            try:
                patterns = p.url_patterns
            except ImportError:
                continue
            views.extend(self.extract_views_from_urlpatterns(patterns, base + describe_pattern(p), namespace=namespace))
        else:
            raise TypeError('%s does not appear to be a urlpattern object' % p)
    return views","for p in urlpatterns:
    if isinstance(p, (URLPattern, RegexURLPattern)):
        try:
            if not p.name:
                name = p.name
            elif namespace:
                name = '{0}:{1}'.format(namespace, p.name)
            else:
                name = p.name
            pattern = describe_pattern(p)
            views.append((p.callback, base + pattern, name))
        except ViewDoesNotExist:
            continue
    elif isinstance(p, (URLResolver, RegexURLResolver)):
        try:
            patterns = p.url_patterns
        except ImportError:
            continue
        if namespace and p.namespace:
            _namespace = '{0}:{1}'.format(namespace, p.namespace)
        else:
            _namespace = p.namespace or namespace
        pattern = describe_pattern(p)
        if isinstance(p, LocaleRegexURLResolver):
            for language in self.LANGUAGES:
                with translation.override(language[0]):
                    views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))
        else:
            views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))
    elif hasattr(p, '_get_callback'):
        try:
            views.append((p._get_callback(), base + describe_pattern(p), p.name))
        except ViewDoesNotExist:
            continue
    elif hasattr(p, 'url_patterns') or hasattr(p, '_get_url_patterns'):
        try:
            patterns = p.url_patterns
        except ImportError:
            continue
        views.extend(self.extract_views_from_urlpatterns(patterns, base + describe_pattern(p), namespace=namespace))
    else:
        raise TypeError('%s does not appear to be a urlpattern object' % p)","for p in urlpatterns:
    if isinstance(p, (URLPattern, RegexURLPattern)):
        try:
            if not p.name:
                name = p.name
            elif namespace:
                name = f'{namespace}:{p.name}'
            else:
                name = p.name
            pattern = describe_pattern(p)
            views.append((p.callback, base + pattern, name))
        except ViewDoesNotExist:
            continue
    elif isinstance(p, (URLResolver, RegexURLResolver)):
        try:
            patterns = p.url_patterns
        except ImportError:
            continue
        _namespace = f'{namespace}:{p.namespace}' if namespace and p.namespace else p.namespace or namespace
        pattern = describe_pattern(p)
        if isinstance(p, LocaleRegexURLResolver):
            for language in self.LANGUAGES:
                with translation.override(language[0]):
                    views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))
        else:
            views.extend(self.extract_views_from_urlpatterns(patterns, base + pattern, namespace=_namespace))
    elif hasattr(p, '_get_callback'):
        try:
            views.append((p._get_callback(), base + describe_pattern(p), p.name))
        except ViewDoesNotExist:
            continue
    elif hasattr(p, 'url_patterns') or hasattr(p, '_get_url_patterns'):
        try:
            patterns = p.url_patterns
        except ImportError:
            continue
        views.extend(self.extract_views_from_urlpatterns(patterns, base + describe_pattern(p), namespace=namespace))
    else:
        raise TypeError(f'{p} does not appear to be a urlpattern object')",find_wrong,-1
unrpyc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/unrpyc/decompiler/util.py,https://github.com/CensoredUsername/unrpyc/tree/master/decompiler/util.py,,reconstruct_paraminfo$177,"def reconstruct_paraminfo(paraminfo):
    if paraminfo is None:
        return ''
    rv = ['(']
    sep = First('', ', ')
    positional = [i for i in paraminfo.parameters if i[0] in paraminfo.positional]
    nameonly = [i for i in paraminfo.parameters if i not in positional]
    for parameter in positional:
        rv.append(sep())
        rv.append(parameter[0])
        if parameter[1] is not None:
            rv.append('=%s' % parameter[1])
    if paraminfo.extrapos:
        rv.append(sep())
        rv.append('*%s' % paraminfo.extrapos)
    if nameonly:
        if not paraminfo.extrapos:
            rv.append(sep())
            rv.append('*')
        for parameter in nameonly:
            rv.append(sep())
            rv.append(parameter[0])
            if parameter[1] is not None:
                rv.append('=%s' % parameter[1])
    if paraminfo.extrakw:
        rv.append(sep())
        rv.append('**%s' % paraminfo.extrakw)
    rv.append(')')
    return ''.join(rv)","positional = [i for i in paraminfo.parameters if i[0] in paraminfo.positional]
nameonly = [i for i in paraminfo.parameters if i not in positional]
for parameter in positional:
    rv.append(sep())
    rv.append(parameter[0])
    if parameter[1] is not None:
        rv.append('=%s' % parameter[1])
if nameonly:
    if not paraminfo.extrapos:
        rv.append(sep())
        rv.append('*')
    for parameter in nameonly:
        rv.append(sep())
        rv.append(parameter[0])
        if parameter[1] is not None:
            rv.append('=%s' % parameter[1])","for parameter in paraminfo.parameters:
    if parameter[0] in paraminfo.positional:
        rv.append(sep())
        rv.append(parameter[0])
        if parameter[1] is not None:
            rv.append('=%s' % parameter[1])
    elif parameter not in positional:
        if not paraminfo.extrapos:
            rv.append(sep())
            rv.append('*')
        rv.append(sep())
        rv.append(parameter[0])
        if parameter[1] is not None:
            rv.append('=%s' % parameter[1])",find_wrong,-1
tartube,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tartube/tartube/config.py,https://github.com/axcore/tartube/tree/master/tartube/config.py,SystemPrefWin,on_data_dir_move_up_button_clicked$28440,"def on_data_dir_move_up_button_clicked(self, button, treeview, liststore, button2):
    """"""Called from callback in self.setup_files_database_tab().

        Moves the selected data directory up one position in the list of
        alternative data directories.

        Args:

            button (Gtk.Button): The widget that was clicked (the up button)

            treeview (Gtk.TreeView): The widget in which a line was selected

            liststore (Gtk.ListStore): The treeview's liststore

            button2 (Gtk.Button): The down button

        """"""
    selection = treeview.get_selection()
    (model, path_list) = selection.get_selected_rows()
    if not path_list:
        return
    first_item = None
    last_item = None
    for path in path_list:
        this_iter = model.get_iter(path)
        last_item = model[this_iter][0]
        if first_item is None:
            first_item = model[this_iter][0]
        if model.iter_previous(this_iter):
            liststore.move_before(this_iter, model.iter_previous(this_iter))
        else:
            break
    dir_list = []
    for row in liststore:
        dir_list.append(row[0])
    self.app_obj.set_data_dir_alt_list(dir_list)
    if dir_list.index(first_item) == 0:
        button.set_sensitive(False)
    else:
        button.set_sensitive(True)
    if dir_list.index(last_item) == len(dir_list) - 1:
        button2.set_sensitive(False)
    else:
        button2.set_sensitive(True)","for path in path_list:
    this_iter = model.get_iter(path)
    last_item = model[this_iter][0]
    if first_item is None:
        first_item = model[this_iter][0]
    if model.iter_previous(this_iter):
        liststore.move_before(this_iter, model.iter_previous(this_iter))
    else:
        break","for path in path_list:
    this_iter = model.get_iter(path)
    last_item = model[this_iter][0]
    if first_item is None:
        first_item = model[this_iter][0]
    for prev_iter in iter(model[this_iter].iter_previous, None):
        liststore.move_before(this_iter, prev_iter)
        break
    else:
        break",find_wrong,2
pandas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandas/pandas/core/internals/concat.py,https://github.com/pandas-dev/pandas/tree/master/pandas/core/internals/concat.py,,_combine_concat_plans$704,"def _combine_concat_plans(plans, concat_axis: AxisInt):
    """"""
    Combine multiple concatenation plans into one.

    existing_plan is updated in-place.
    """"""
    if len(plans) == 1:
        for p in plans[0]:
            yield (p[0], [p[1]])
    elif concat_axis == 0:
        offset = 0
        for plan in plans:
            last_plc = None
            for (plc, unit) in plan:
                yield (plc.add(offset), [unit])
                last_plc = plc
            if last_plc is not None:
                offset += last_plc.as_slice.stop
    else:
        num_ended = [0]

        def _next_or_none(seq):
            retval = next(seq, None)
            if retval is None:
                num_ended[0] += 1
            return retval
        plans = list(map(iter, plans))
        next_items = list(map(_next_or_none, plans))
        while num_ended[0] != len(next_items):
            if num_ended[0] > 0:
                raise ValueError('Plan shapes are not aligned')
            (placements, units) = zip(*next_items)
            lengths = list(map(len, placements))
            (min_len, max_len) = (min(lengths), max(lengths))
            if min_len == max_len:
                yield (placements[0], units)
                next_items[:] = map(_next_or_none, plans)
            else:
                yielded_placement = None
                yielded_units = [None] * len(next_items)
                for (i, (plc, unit)) in enumerate(next_items):
                    yielded_units[i] = unit
                    if len(plc) > min_len:
                        next_items[i] = (plc[min_len:], _trim_join_unit(unit, min_len))
                    else:
                        yielded_placement = plc
                        next_items[i] = _next_or_none(plans[i])
                yield (yielded_placement, yielded_units)","for plan in plans:
    last_plc = None
    for (plc, unit) in plan:
        yield (plc.add(offset), [unit])
        last_plc = plc
    if last_plc is not None:
        offset += last_plc.as_slice.stop","for plan in plans:
    last_plc = None
    for (plc, unit) in plan:
        yield ((plc + offset,), [unit])
        last_plc = plc
    if last_plc is not None:
        offset += last_plc.as_slice.stop",find_wrong,2
veusz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/veusz/veusz/windows/mainwindow.py,https://github.com/veusz/veusz/tree/master/veusz/windows/mainwindow.py,MainWindow,definePlugins$378,"def definePlugins(self, pluginlist, actions, menuname):
    """"""Create menu items and actions for plugins.

        pluginlist: list of plugin classes
        actions: dict of actions to add new actions to
        menuname: string giving prefix for new menu entries (inside actions)
        """"""

    def getLoadDialog(pluginkls):

        def _loadPlugin():
            from ..dialogs.plugin import handlePlugin
            handlePlugin(self, self.document, pluginkls)
        return _loadPlugin
    menu = []
    for pluginkls in pluginlist:
        actname = menuname + '.' + '.'.join(pluginkls.menu)
        text = pluginkls.menu[-1]
        if pluginkls.has_parameters:
            text += '...'
        actions[actname] = utils.makeAction(self, pluginkls.description_short, text, getLoadDialog(pluginkls))
        menulook = menu
        namebuild = [menuname]
        for cmpt in pluginkls.menu[:-1]:
            namebuild.append(cmpt)
            name = '.'.join(namebuild)
            for c in menulook:
                if c[0] == name:
                    menulook = c[2]
                    break
            else:
                menulook.append([name, cmpt, []])
                menulook = menulook[-1][2]
        menulook.append(actname)
    return menu","for pluginkls in pluginlist:
    actname = menuname + '.' + '.'.join(pluginkls.menu)
    text = pluginkls.menu[-1]
    if pluginkls.has_parameters:
        text += '...'
    actions[actname] = utils.makeAction(self, pluginkls.description_short, text, getLoadDialog(pluginkls))
    menulook = menu
    namebuild = [menuname]
    for cmpt in pluginkls.menu[:-1]:
        namebuild.append(cmpt)
        name = '.'.join(namebuild)
        for c in menulook:
            if c[0] == name:
                menulook = c[2]
                break
        else:
            menulook.append([name, cmpt, []])
            menulook = menulook[-1][2]
    menulook.append(actname)","for pluginkls in pluginlist:
    actname = menuname + '.' + '.'.join(pluginkls.menu)
    text = pluginkls.menu[-1]
    if pluginkls.has_parameters:
        text += '...'
    actions[actname] = utils.makeAction(self, pluginkls.description_short, text, getLoadDialog(pluginkls))
    menulook = menu
    namebuild = [menuname]
    for cmpt in pluginkls.menu[:-1]:
        namebuild.append(cmpt)
        name = '.'.join(namebuild)
        for c in menulook:
            if c[0] == name:
                menulook = c[2]
                break
        else:
            menulook.append([name, cmpt, []])
            menulook = menulook[-1][2]
        menulook.append(actname)
    return menu",find_wrong,-1
osroom,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/osroom/apps/core/flask/reqparse.py,https://github.com/osroom/osroom/tree/master/apps/core/flask/reqparse.py,ArgVerify,regex_rule$84,"def regex_rule(self, **kwargs):
    vr = kwargs.get('vr')
    if vr['is_match']:
        for reqarg in kwargs.get('reqargs'):
            if not re.search(vr['rule'], reqarg[1]):
                return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})
    else:
        for reqarg in kwargs.get('reqargs'):
            if re.search(vr['rule'], reqarg[1]):
                return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})
    return (True, None)","for reqarg in kwargs.get('reqargs'):
    if not re.search(vr['rule'], reqarg[1]):
        return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})
for reqarg in kwargs.get('reqargs'):
    if re.search(vr['rule'], reqarg[1]):
        return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})","for reqarg in kwargs.get('reqargs'):
    if vr['is_match'] != bool(re.search(vr['rule'], reqarg[1])):
        return (False, {'msg': gettext('The value of parameter ""{}"" is illegal').format(reqarg[0]), 'msg_type': 'w', 'custom_status': 422})
return (True, None)",find_wrong,-1
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/model/migrate/versions/0065_add_name_to_form_fields_and_values.py,,downgrade$83,"def downgrade(migrate_engine):
    metadata.bind = migrate_engine
    metadata.reflect()
    Table('form_definition', metadata, autoload=True)
    Table('form_values', metadata, autoload=True)
    cmd = 'SELECT form_values.id, form_values.content, form_definition.fields FROM form_values, form_definition WHERE form_values.form_definition_id=form_definition.id ORDER BY form_values.id ASC'
    result = migrate_engine.execute(cmd)
    for row in result:
        form_values_id = int(row[0])
        if not str(row[1]).strip():
            continue
        values_dict = loads(str(row[1]))
        if not str(row[2]).strip():
            continue
        fields_list = loads(str(row[2]))
        if fields_list:
            values_list = []
            for field in fields_list:
                field_name = field['name']
                field_value = values_dict[field_name]
                values_list.append(field_value)
            cmd = ""UPDATE form_values SET content='%s' WHERE id=%i"" % (dumps(values_list), form_values_id)
            migrate_engine.execute(cmd)
    cmd = 'SELECT f.id, f.fields FROM form_definition AS f'
    result = migrate_engine.execute(cmd)
    for row in result:
        form_definition_id = row[0]
        fields = str(row[1])
        if not fields.strip():
            continue
        fields_list = loads(_sniffnfix_pg9_hex(fields))
        if len(fields_list):
            for field in fields_list:
                if 'name' in field:
                    del field['name']
            if migrate_engine.name == 'mysql':
                cmd = ""UPDATE form_definition AS f SET f.fields='%s' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)
            else:
                cmd = ""UPDATE form_definition SET fields='%s' WHERE id=%i"" % (dumps(fields_list), form_definition_id)
        migrate_engine.execute(cmd)","for row in result:
    form_values_id = int(row[0])
    if not str(row[1]).strip():
        continue
    values_dict = loads(str(row[1]))
    if not str(row[2]).strip():
        continue
    fields_list = loads(str(row[2]))
    if fields_list:
        values_list = []
        for field in fields_list:
            field_name = field['name']
            field_value = values_dict[field_name]
            values_list.append(field_value)
        cmd = ""UPDATE form_values SET content='%s' WHERE id=%i"" % (dumps(values_list), form_values_id)
        migrate_engine.execute(cmd)
for row in result:
    form_definition_id = row[0]
    fields = str(row[1])
    if not fields.strip():
        continue
    fields_list = loads(_sniffnfix_pg9_hex(fields))
    if len(fields_list):
        for field in fields_list:
            if 'name' in field:
                del field['name']
        if migrate_engine.name == 'mysql':
            cmd = ""UPDATE form_definition AS f SET f.fields='%s' WHERE f.id=%i"" % (dumps(fields_list), form_definition_id)
        else:
            cmd = ""UPDATE form_definition SET fields='%s' WHERE id=%i"" % (dumps(fields_list), form_definition_id)
    migrate_engine.execute(cmd)","for row in result:
    form_values_id = int(row[0])
    if not str(row[1]).strip():
        continue
    values_dict = loads(str(row[1]))
    if not str(row[2]).strip():
        continue
    fields_list = loads(str(row[2]))
    if fields_list:
        values_list = []
        for field in fields_list:
            field_name = field['name']
            field_value = values_dict[field_name]
            values_list.append(field_value)
        cmd = f""UPDATE form_values SET content='{dumps(values_list)}' WHERE id={form_values_id}""
        migrate_engine.execute(cmd)
for row in result:
    form_definition_id = row[0]
    fields = str(row[1])
    if not fields.strip():
        continue
    fields_list = loads(_sniffnfix_pg9_hex(fields))
    if len(fields_list):
        for field in fields_list:
            if 'name' in field:
                del field['name']
        if migrate_engine.name == 'mysql':
            cmd = f""UPDATE form_definition AS f SET f.fields='{dumps(fields_list)}' WHERE f.id={form_definition_id}""
        else:
            cmd = f""UPDATE form_definition SET fields='{dumps(fields_list)}' WHERE id={form_definition_id}""
        migrate_engine.execute(cmd)",find_wrong,-1
PornHub-downloader-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PornHub-downloader-python/functions.py,https://github.com/mariosemes/PornHub-downloader-python/tree/master//functions.py,,dl_all_items$121,"def dl_all_items(conn):
    c = conn.cursor()
    try:
        c.execute('SELECT * FROM ph_items')
    except Error as e:
        print(e)
        sys.exit()
    rows = c.fetchall()
    for row in rows:
        if row[1] == 'model':
            url_after = '/videos/upload'
        elif row[1] == 'users':
            url_after = '/videos/public'
        elif row[1] == 'channels':
            url_after = '/videos'
        else:
            url_after = ''
        print('-----------------------------')
        print(row[1])
        print(row[2])
        print('https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after)
        print('-----------------------------')
        outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
        ydl_opts_start = {'format': 'best', 'playliststart:': 1, 'playlistend': 4, 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
        url = 'https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2] + url_after)
        with youtube_dl.YoutubeDL(ydl_opts_start) as ydl:
            ydl.download([url])
        try:
            c.execute('UPDATE ph_items SET lastchecked=CURRENT_TIMESTAMP WHERE url_name = ?', (row[2],))
            conn.commit()
        except Error as e:
            print(e)
            sys.exit()","for row in rows:
    if row[1] == 'model':
        url_after = '/videos/upload'
    elif row[1] == 'users':
        url_after = '/videos/public'
    elif row[1] == 'channels':
        url_after = '/videos'
    else:
        url_after = ''
    print('-----------------------------')
    print(row[1])
    print(row[2])
    print('https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2]) + url_after)
    print('-----------------------------')
    outtmpl = get_dl_location('DownloadLocation') + '/' + str(row[1]) + '/' + str(row[3]) + '/%(title)s.%(ext)s'
    ydl_opts_start = {'format': 'best', 'playliststart:': 1, 'playlistend': 4, 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
    url = 'https://www.pornhub.com/' + str(row[1]) + '/' + str(row[2] + url_after)
    with youtube_dl.YoutubeDL(ydl_opts_start) as ydl:
        ydl.download([url])","for row in rows:
    url_after = ''
    if row[1] == 'model':
        url_after = '/videos/upload'
    elif row[1] == 'users':
        url_after = '/videos/public'
    elif row[1] == 'channels':
        url_after = '/videos'
    print('-----------------------------')
    print(row[1])
    print(row[2])
    print('https://www.pornhub.com/{}/{}{}'.format(row[1], row[2], url_after))
    print('-----------------------------')
    outtmpl = get_dl_location('DownloadLocation') + '/{}/{}/%(title)s.%(ext)s'.format(row[1], row[3])
    ydl_opts_start = {'format': 'best', 'playliststart:': 1, 'playlistend': 4, 'outtmpl': outtmpl, 'nooverwrites': True, 'no_warnings': False, 'ignoreerrors': True}
    url = 'https://www.pornhub.com/{}/{}{}'.format(row[1], row[2], url_after)
    with youtube_dl.YoutubeDL(ydl_opts_start) as ydl:
        ydl.download([url])",find_wrong,-1
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/mysql.py,https://github.com/saltstack/salt/tree/master/salt/modules/mysql.py,,__do_query_into_hash$2673,"def __do_query_into_hash(conn, sql_str):
    """"""
    Perform the query that is passed to it (sql_str).

    Returns:
       results in a dict.

    """"""
    mod = sys._getframe().f_code.co_name
    log.debug('%s<--(%s)', mod, sql_str)
    rtn_results = []
    try:
        cursor = conn.cursor()
    except MySQLdb.MySQLError:
        log.error(""%s: Can't get cursor for SQL->%s"", mod, sql_str)
        cursor.close()
        log.debug('%s-->', mod)
        return rtn_results
    try:
        _execute(cursor, sql_str)
    except MySQLdb.MySQLError:
        log.error('%s: try to execute : SQL->%s', mod, sql_str)
        cursor.close()
        log.debug('%s-->', mod)
        return rtn_results
    qrs = cursor.fetchall()
    for row_data in qrs:
        col_cnt = 0
        row = {}
        for col_data in cursor.description:
            col_name = col_data[0]
            row[col_name] = row_data[col_cnt]
            col_cnt += 1
        rtn_results.append(row)
    cursor.close()
    log.debug('%s-->', mod)
    return rtn_results","for row_data in qrs:
    col_cnt = 0
    row = {}
    for col_data in cursor.description:
        col_name = col_data[0]
        row[col_name] = row_data[col_cnt]
        col_cnt += 1
    rtn_results.append(row)","for row_data in qrs:
    row = {}
    for (col_cnt, col_data) in enumerate(cursor.description):
        col_name = col_data[0]
        row[col_name] = row_data[col_cnt]
    rtn_results.append(row)",find_wrong,-1
you-get,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/you-get/src/you_get/extractors/mgtv.py,https://github.com/soimort/you-get/tree/master/src/you_get/extractors/mgtv.py,MGTV,prepare$93,"def prepare(self, **kwargs):
    if self.url:
        self.vid = self.get_vid_from_url(self.url)
    content_info = get_content(self.info_endpoint.format(video_id=self.vid))
    log.d(content_info)
    content_info = loads(content_info)
    self.title = content_info['data']['info']['videoName']
    content_player = get_content(self.player_endpoint.format(did=self.did, video_id=self.vid, tk2=self.tk2()))
    log.d(content_player)
    content_player = loads(content_player)
    pm2 = content_player['data']['atc']['pm2']
    content_source = get_content(self.source_endpoint.format(video_id=self.vid, tk2=self.tk2(), pm2=pm2))
    log.d(content_source)
    content_source = loads(content_source)
    domain = content_source['data']['stream_domain'][0]
    stream_available = {}
    for i in content_source['data']['stream']:
        stream_available[i['name']] = i['url']
    for s in self.stream_types:
        if s['video_profile'] in stream_available.keys():
            quality_id = self.id_dic[s['video_profile']]
            url = stream_available[s['video_profile']]
            if url is None or url == '':
                continue
            url = domain + re.sub('(\\&arange\\=\\d+)', '', url)
            (m3u8_url, m3u8_size, segment_list_this) = self.get_mgtv_real_url(url)
            stream_fileid_list = []
            for i in segment_list_this:
                stream_fileid_list.append(os.path.basename(i).split('.')[0])
            pieces = []
            for i in zip(stream_fileid_list, segment_list_this):
                pieces.append({'fileid': i[0], 'segs': i[1]})
                self.streams[quality_id] = {'container': s['container'], 'video_profile': s['video_profile'], 'size': m3u8_size, 'pieces': pieces, 'm3u8_url': m3u8_url}
            if not kwargs['info_only']:
                self.streams[quality_id]['src'] = segment_list_this","for s in self.stream_types:
    if s['video_profile'] in stream_available.keys():
        quality_id = self.id_dic[s['video_profile']]
        url = stream_available[s['video_profile']]
        if url is None or url == '':
            continue
        url = domain + re.sub('(\\&arange\\=\\d+)', '', url)
        (m3u8_url, m3u8_size, segment_list_this) = self.get_mgtv_real_url(url)
        stream_fileid_list = []
        for i in segment_list_this:
            stream_fileid_list.append(os.path.basename(i).split('.')[0])
        pieces = []
        for i in zip(stream_fileid_list, segment_list_this):
            pieces.append({'fileid': i[0], 'segs': i[1]})
            self.streams[quality_id] = {'container': s['container'], 'video_profile': s['video_profile'], 'size': m3u8_size, 'pieces': pieces, 'm3u8_url': m3u8_url}
        if not kwargs['info_only']:
            self.streams[quality_id]['src'] = segment_list_this","for s in self.stream_types:
    if s['video_profile'] in stream_available:
        quality_id = self.id_dic[s['video_profile']]
        url = stream_available[s['video_profile']]
        if not url:
            continue
        url = domain + re.sub('(\\&arange\\=\\d+)', '', url)
        (m3u8_url, m3u8_size, segment_list_this) = self.get_mgtv_real_url(url)
        stream_fileid_list = [os.path.basename(i).split('.')[0] for i in segment_list_this]
        pieces = [{'fileid': i[0], 'segs': i[1]} for i in zip(stream_fileid_list, segment_list_this)]
        self.streams[quality_id] = {'container': s['container'], 'video_profile': s['video_profile'], 'size': m3u8_size, 'pieces': pieces, 'm3u8_url': m3u8_url}
        if not kwargs.get('info_only'):
            self.streams[quality_id]['src'] = segment_list_this",find_wrong,-1
EasyOCR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/EasyOCR/trainer/utils.py,https://github.com/JaidedAI/EasyOCR/tree/master/trainer/utils.py,,word_segmentation$176,"def word_segmentation(mat, separator_idx={'th': [1, 2], 'en': [3, 4]}, separator_idx_list=[1, 2, 3, 4]):
    result = []
    sep_list = []
    start_idx = 0
    for sep_idx in separator_idx_list:
        if sep_idx % 2 == 0:
            mode = 'first'
        else:
            mode = 'last'
        a = consecutive(np.argwhere(mat == sep_idx).flatten(), mode)
        new_sep = [[item, sep_idx] for item in a]
        sep_list += new_sep
    sep_list = sorted(sep_list, key=lambda x: x[0])
    for sep in sep_list:
        for lang in separator_idx.keys():
            if sep[1] == separator_idx[lang][0]:
                sep_lang = lang
                sep_start_idx = sep[0]
            elif sep[1] == separator_idx[lang][1]:
                if sep_lang == lang:
                    new_sep_pair = [lang, [sep_start_idx + 1, sep[0] - 1]]
                    if sep_start_idx > start_idx:
                        result.append(['', [start_idx, sep_start_idx - 1]])
                    start_idx = sep[0] + 1
                    result.append(new_sep_pair)
                else:
                    sep_lang = ''
    if start_idx <= len(mat) - 1:
        result.append(['', [start_idx, len(mat) - 1]])
    return result","for sep_idx in separator_idx_list:
    if sep_idx % 2 == 0:
        mode = 'first'
    else:
        mode = 'last'
    a = consecutive(np.argwhere(mat == sep_idx).flatten(), mode)
    new_sep = [[item, sep_idx] for item in a]
    sep_list += new_sep
sep_list = sorted(sep_list, key=lambda x: x[0])
for sep in sep_list:
    for lang in separator_idx.keys():
        if sep[1] == separator_idx[lang][0]:
            sep_lang = lang
            sep_start_idx = sep[0]
        elif sep[1] == separator_idx[lang][1]:
            if sep_lang == lang:
                new_sep_pair = [lang, [sep_start_idx + 1, sep[0] - 1]]
                if sep_start_idx > start_idx:
                    result.append(['', [start_idx, sep_start_idx - 1]])
                start_idx = sep[0] + 1
                result.append(new_sep_pair)
            else:
                sep_lang = ''
if start_idx <= len(mat) - 1:
    result.append(['', [start_idx, len(mat) - 1]])","for sep_idx in separator_idx_list:
    mode = 'first' if sep_idx % 2 == 0 else 'last'
    a = consecutive(np.argwhere(mat == sep_idx).flatten(), mode)
    new_sep = [[item, sep_idx] for item in a]
    sep_list.extend(new_sep)
sep_list = sorted(sep_list, key=lambda x: x[0])
for sep in sep_list:
    for (lang, (start_idx, end_idx)) in separator_idx.items():
        if sep[1] == start_idx:
            sep_lang = lang
            sep_start_idx = sep[0]
        elif sep[1] == end_idx:
            if sep_lang == lang:
                new_sep_pair = [lang, [sep_start_idx + 1, sep[0] - 1]]
                if sep_start_idx > start_idx:
                    result.append(['', [start_idx, sep_start_idx - 1]])
                start_idx = sep[0] + 1
                result.append(new_sep_pair)
            else:
                sep_lang = ''
    if start_idx <= len(mat) - 1:
        result.append(['', [start_idx, len(mat) - 1]])",find_wrong,-1
moviepy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moviepy/tests/test_compositing.py,https://github.com/Zulko/moviepy/tree/master/tests/test_compositing.py,,test_slide_out$173,"def test_slide_out():
    duration = 0.1
    size = (11, 1)
    fps = 10
    color = (255, 0, 0)
    clip = ColorClip(color=color, duration=duration, size=size).with_fps(fps)
    for side in ['left', 'right']:
        new_clip = CompositeVideoClip([slide_out(clip, duration, side)])
        for t in np.arange(0, duration, duration / fps):
            (n_reds, n_reds_expected) = (0, round(11 - t * 100, 6))
            if t:
                assert n_reds_expected
            for (r, g, b) in new_clip.get_frame(t)[0]:
                if r == color[0] and g == color[1] and (g == color[2]):
                    n_reds += 1
            assert n_reds == n_reds_expected
    clip = ColorClip(color=color, duration=duration, size=(size[1], size[0])).with_fps(fps)
    for side in ['top', 'bottom']:
        new_clip = CompositeVideoClip([slide_out(clip, duration, side)])
        for t in np.arange(0, duration, duration / fps):
            (n_reds, n_reds_expected) = (0, round(11 - t * 100, 6))
            if t:
                assert n_reds_expected
            for row in new_clip.get_frame(t):
                (r, g, b) = row[0]
                if r == color[0] and g == color[1] and (g == color[2]):
                    n_reds += 1
            assert n_reds == n_reds_expected","for side in ['left', 'right']:
    new_clip = CompositeVideoClip([slide_out(clip, duration, side)])
    for t in np.arange(0, duration, duration / fps):
        (n_reds, n_reds_expected) = (0, round(11 - t * 100, 6))
        if t:
            assert n_reds_expected
        for (r, g, b) in new_clip.get_frame(t)[0]:
            if r == color[0] and g == color[1] and (g == color[2]):
                n_reds += 1
        assert n_reds == n_reds_expected
for side in ['top', 'bottom']:
    new_clip = CompositeVideoClip([slide_out(clip, duration, side)])
    for t in np.arange(0, duration, duration / fps):
        (n_reds, n_reds_expected) = (0, round(11 - t * 100, 6))
        if t:
            assert n_reds_expected
        for row in new_clip.get_frame(t):
            (r, g, b) = row[0]
            if r == color[0] and g == color[1] and (g == color[2]):
                n_reds += 1
        assert n_reds == n_reds_expected","for side in ['left', 'right', 'top', 'bottom']:
    if side in ['left', 'right']:
        size = (11, 1)
    else:
        size = (1, 11)
    clip = ColorClip(color=color, duration=duration, size=size).with_fps(fps)
    new_clip = CompositeVideoClip([slide_out(clip, duration, side)])
    for t in np.arange(0, duration, duration / fps):
        (n_reds, n_reds_expected) = (0, round(11 - t * 100, 6))
        if t:
            assert n_reds_expected
        if side in ['left', 'right']:
            for (r, g, b) in new_clip.get_frame(t)[0]:
                if r == color[0] and g == color[1] and (g == color[2]):
                    n_reds += 1
        else:
            for row in new_clip.get_frame(t):
                (r, g, b) = row[0]
                if r == color[0] and g == color[1] and (g == color[2]):
                    n_reds += 1
        assert n_reds == n_reds_expected",find_wrong,-1
sparrow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sparrow/views/report.py,https://github.com/wylok/sparrow/tree/master/views/report.py,,work_order_report$287,"def work_order_report(start_time=None, end_time=None, source='all_order'):
    INFOS = []
    db_sso = db_op.user_sso
    db_work_order = db_op.work_order
    dm_key = 'op_work_order_report_dm'
    stat_key = 'op_work_order_report_status'
    dep_key = 'op_work_order_report_department'
    if not start_time or not end_time:
        tm = datetime.datetime.now() - datetime.timedelta(days=7)
        start_time = tm.strftime('%Y-%m-%d')
        end_time = time.strftime('%Y-%m-%d', time.localtime())
    try:
        infos = db_sso.query.with_entities(db_sso.dingunionid, db_sso.department, db_sso.realName).all()
        departments = {info[0]: info[1] for info in infos}
        users = {info[0]: info[-1] for info in infos}
        try:
            if source != 'all_order':
                vals = db_work_order.query.with_entities(db_work_order.status, func.count(db_work_order.status)).filter(and_(db_work_order.source == source, db_work_order.date >= start_time, db_work_order.date <= end_time)).group_by(db_work_order.status).all()
            else:
                vals = db_work_order.query.with_entities(db_work_order.status, func.count(db_work_order.status)).filter(and_(db_work_order.date >= start_time, db_work_order.date <= end_time)).group_by(db_work_order.status).all()
            pie = Pie('', width='100%', height='100%', title_pos='center', title_text_size=14)
            pie_vals = [val[0] for val in vals]
            pie_counts = [int(val[1]) for val in vals]
            pie.add('', pie_vals, pie_counts, is_label_show=True, is_toolbox_show=False, legend_orient='vertical', legend_pos='right', radius=[1, 65], is_random=True)
            INFOS.append(pie)
        except Exception as e:
            logging.error(e)
        try:
            if source != 'all_order':
                vals = db_work_order.query.with_entities(db_work_order.date, db_work_order.status).filter(and_(db_work_order.source == source, db_work_order.date >= start_time, db_work_order.date <= end_time)).all()
            else:
                vals = db_work_order.query.with_entities(db_work_order.date, db_work_order.status).filter(and_(db_work_order.date >= start_time, db_work_order.date <= end_time)).all()
            if vals:
                for val in vals:
                    (dm, status) = val
                    dm = dm.split('-')[1]
                    RC.hincrby(dm_key, dm)
                    if status not in ('', ''):
                        RC.hincrby(stat_key, dm)
            line = Line('', width='100%', height='100%', title_pos='center', title_text_size=14)
            total_vals = RC.hgetall(dm_key)
            vals = sorted(total_vals.items(), key=lambda item: int(item[0]))
            dm_vals = [val[0] for val in vals]
            dm_counts = [int(val[1]) for val in vals]
            line.add('', dm_vals, dm_counts, is_label_show=True, is_toolbox_show=False, legend_orient='vertical', legend_pos='right', xaxis_interval=0, is_random=True, xaxis_rotate=15)
            stat_vals = RC.hgetall(stat_key)
            stat_counts = [round(float(stat_vals[val]) / float(total_vals[val]) * 100, 1) for val in stat_vals]
            line.add('', dm_vals, stat_counts, is_label_show=True, is_toolbox_show=False, legend_orient='vertical', legend_pos='right', xaxis_interval=0, is_random=True, xaxis_rotate=15)
            RC.delete(stat_key)
            RC.delete(dm_key)
            INFOS.append(line)
        except Exception as e:
            logging.error(e)
        try:
            if source != 'all_order':
                vals = db_work_order.query.with_entities(db_work_order.applicant).filter(and_(db_work_order.source == source, db_work_order.date >= start_time, db_work_order.date <= end_time)).all()
            else:
                vals = db_work_order.query.with_entities(db_work_order.applicant).filter(and_(db_work_order.date >= start_time, db_work_order.date <= end_time)).all()
            if vals:
                for val in vals:
                    RC.hincrby(dep_key, departments[val[0]])
            bar = Bar('', width='100%', height='100%', title_pos='center', title_text_size=14)
            vals = RC.hgetall(dep_key)
            dep_vals = [val for val in vals]
            dep_counts = [int(vals[val]) for val in vals]
            bar.add('', dep_vals, dep_counts, is_label_show=True, is_toolbox_show=False, legend_orient='vertical', legend_pos='right', xaxis_interval=0, is_random=True, xaxis_rotate=15)
            RC.delete(dep_key)
            INFOS.append(bar)
        except Exception as e:
            logging.error(e)
        try:
            if source != 'all_order':
                vals = db_work_order.query.with_entities(db_work_order.applicant, func.count(db_work_order.applicant)).filter(and_(db_work_order.source == source, db_work_order.date >= start_time, db_work_order.date <= end_time)).group_by(db_work_order.applicant).order_by(desc(func.count(db_work_order.applicant))).limit(15).all()
            else:
                vals = db_work_order.query.with_entities(db_work_order.applicant, func.count(db_work_order.applicant)).filter(and_(db_work_order.date >= start_time, db_work_order.date <= end_time)).group_by(db_work_order.applicant).order_by(desc(func.count(db_work_order.applicant))).limit(15).all()
            vals = [list(val) for val in vals]
            for val in vals:
                val[0] = users[val[0]]
            bar = Bar('', width='100%', height='100%', title_pos='center', title_text_size=14)
            dep_vals = [val[0] for val in vals]
            dep_counts = [int(val[1]) for val in vals]
            bar.add('', dep_vals, dep_counts, is_label_show=True, is_toolbox_show=False, legend_orient='vertical', legend_pos='right', xaxis_interval=0, is_random=True, xaxis_rotate=15)
            INFOS.append(bar)
        except Exception as e:
            logging.error(e)
    except Exception as e:
        logging.error(e)
    return render_template('work_order_report.html', INFOS=INFOS, tt=(start_time, end_time))","try:
    if source != 'all_order':
        vals = db_work_order.query.with_entities(db_work_order.status, func.count(db_work_order.status)).filter(and_(db_work_order.source == source, db_work_order.date >= start_time, db_work_order.date <= end_time)).group_by(db_work_order.status).all()
    else:
        vals = db_work_order.query.with_entities(db_work_order.status, func.count(db_work_order.status)).filter(and_(db_work_order.date >= start_time, db_work_order.date <= end_time)).group_by(db_work_order.status).all()
    pie = Pie('', width='100%', height='100%', title_pos='center', title_text_size=14)
    pie_vals = [val[0] for val in vals]
    pie_counts = [int(val[1]) for val in vals]
    pie.add('', pie_vals, pie_counts, is_label_show=True, is_toolbox_show=False, legend_orient='vertical', legend_pos='right', radius=[1, 65], is_random=True)
    INFOS.append(pie)
except Exception as e:
    logging.error(e)","for source_val in ['all_order', source]:
    try:
        vals = db_work_order.query.with_entities(db_work_order.status, func.count(db_work_order.status)).filter(and_(db_work_order.source == source_val, db_work_order.date >= start_time, db_work_order.date <= end_time)).group_by(db_work_order.status).all()
        pie = Pie('', width='100%', height='100%', title_pos='center', title_text_size=14)
        pie_vals = [val[0] for val in vals]
        pie_counts = [int(val[1]) for val in vals]
        pie.add('', pie_vals, pie_counts, is_label_show=True, is_toolbox_show=False, legend_orient='vertical', legend_pos='right', radius=[1, 65], is_random=True)
        INFOS.append(pie)
    except Exception as e:
        logging.error(e)",find_wrong,-1
pycoin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycoin/pycoin/key/Keychain.py,https://github.com/richardkiss/pycoin/tree/master/pycoin/key/Keychain.py,Keychain,interested_hashes$128,"def interested_hashes(self):
    SQL = 'select hash160 from HASH160'
    c = self._exec_sql(SQL)
    for r in c:
        yield r[0]
    SQL = 'select hash160, hash256 from P2S'
    c = self._exec_sql(SQL)
    for r in c:
        yield r[0]
        yield r[1]","SQL = 'select hash160 from HASH160'
c = self._exec_sql(SQL)
for r in c:
    yield r[0]
SQL = 'select hash160, hash256 from P2S'
c = self._exec_sql(SQL)
for r in c:
    yield r[0]
    yield r[1]","for SQL in ['select hash160 from HASH160', 'select hash160, hash256 from P2S']:
    c = self._exec_sql(SQL)
    for r in c:
        yield r[0]
        if len(r) > 1:
            yield r[1]",find_wrong,-1
underthesea,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/underthesea/underthesea/transformer/tagged_feature.py,https://github.com/undertheseanlp/underthesea/tree/master/underthesea/transformer/tagged_feature.py,,text_istitle$23,"def text_istitle(word):
    if len(word) == 0:
        return False
    try:
        titles = [s[0] for s in word.split(' ')]
        for token in titles:
            if token[0].istitle() is False:
                return False
        return True
    except Exception:
        return False","titles = [s[0] for s in word.split(' ')]
for token in titles:
    if token[0].istitle() is False:
        return False","for token in word.split():
    if not token.istitle():
        return False
return True",find_wrong,-1
pyGAT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyGAT/visualize_graph.py,https://github.com/Diego999/pyGAT/tree/master//visualize_graph.py,,add_nodes$32,"def add_nodes(var):
    if var not in seen:
        if torch.is_tensor(var):
            dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')
        elif hasattr(var, 'variable'):
            u = var.variable
            node_name = '%s\n %s' % (param_map.get(id(u)), size_to_str(u.size()))
            dot.node(str(id(var)), node_name, fillcolor='lightblue')
        else:
            dot.node(str(id(var)), str(type(var).__name__))
        seen.add(var)
        if hasattr(var, 'next_functions'):
            for u in var.next_functions:
                if u[0] is not None:
                    dot.edge(str(id(u[0])), str(id(var)))
                    add_nodes(u[0])
        if hasattr(var, 'saved_tensors'):
            for t in var.saved_tensors:
                dot.edge(str(id(t)), str(id(var)))
                add_nodes(t)","if hasattr(var, 'next_functions'):
    for u in var.next_functions:
        if u[0] is not None:
            dot.edge(str(id(u[0])), str(id(var)))
            add_nodes(u[0])
if hasattr(var, 'saved_tensors'):
    for t in var.saved_tensors:
        dot.edge(str(id(t)), str(id(var)))
        add_nodes(t)","for u in getattr(var, 'next_functions', []):
    if u[0] is not None:
        dot.edge(str(id(u[0])), str(id(var)))
        add_nodes(u[0])
for t in getattr(var, 'saved_tensors', []):
    dot.edge(str(id(t)), str(id(var)))
    add_nodes(t)",find_wrong,-1
QRec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/QRec/model/rating/CUNE_MF.py,https://github.com/Coder-Yu/QRec/tree/master/model/rating/CUNE_MF.py,CUNE_MF,trainModel$155,"def trainModel(self):
    print('Kind Note: This method will probably take much time.')
    print('Building collaborative user network...')
    self.itemNet = {}
    for item in self.data.trainSet_i:
        if len(self.data.trainSet_i[item]) > 1:
            self.itemNet[item] = self.data.trainSet_i[item]
    self.filteredRatings = defaultdict(list)
    for item in self.itemNet:
        for user in self.itemNet[item]:
            if self.itemNet[item][user] >= 1:
                self.filteredRatings[user].append(item)
    self.CUNet = defaultdict(list)
    for user1 in self.filteredRatings:
        s1 = set(self.filteredRatings[user1])
        for user2 in self.filteredRatings:
            if user1 != user2:
                s2 = set(self.filteredRatings[user2])
                weight = len(s1.intersection(s2))
                if weight > 0:
                    self.CUNet[user1] += [user2] * weight
    print('Generating random deep walks...')
    self.walks = []
    self.visited = defaultdict(dict)
    for user in self.CUNet:
        for t in range(self.walkCount):
            path = [user]
            lastNode = user
            for i in range(1, self.walkLength):
                nextNode = choice(self.CUNet[lastNode])
                count = 0
                while nextNode in self.visited[lastNode]:
                    nextNode = choice(self.CUNet[lastNode])
                    count += 1
                    if count == 10:
                        break
                path.append(nextNode)
                self.visited[user][nextNode] = 1
                lastNode = nextNode
            self.walks.append(path)
    shuffle(self.walks)
    print('Generating user embedding...')
    model = w2v.Word2Vec(self.walks, size=self.walkDim, window=5, min_count=0, iter=3)
    print('User embedding generated.')
    print('Constructing similarity matrix...')
    self.W = np.random.rand(self.data.trainingSize()[0], self.walkDim) / 10
    self.topKSim = {}
    i = 0
    for user1 in self.CUNet:
        sims = []
        u1 = self.data.user[user1]
        self.W[u1] = model.wv[user1]
        for user2 in self.CUNet:
            if user1 != user2:
                u2 = self.data.user[user2]
                self.W[u2] = model.wv[user2]
                sims.append((user2, cosine(self.W[u1], self.W[u2])))
        self.topKSim[user1] = sorted(sims, key=lambda d: d[1], reverse=True)[:self.topK]
        i += 1
        if i % 200 == 0:
            print('progress:', i, '/', len(self.CUNet))
    print('Similarity matrix finished.')
    print('Decomposing...')
    epoch = 0
    while epoch < self.maxEpoch:
        self.loss = 0
        for entry in self.data.trainingData:
            (user, item, rating) = entry
            u = self.data.user[user]
            i = self.data.item[item]
            error = rating - self.P[u].dot(self.Q[i])
            self.loss += error ** 2
            p = self.P[u]
            q = self.Q[i]
            self.P[u] += self.lRate * (error * q - self.regU * p)
            self.Q[i] += self.lRate * (error * p - self.regI * q)
        for user in self.CUNet:
            u = self.data.user[user]
            friends = self.topKSim[user]
            for friend in friends:
                uf = self.data.user[friend[0]]
                self.P[u] -= self.lRate * (self.P[u] - self.P[uf]) * self.alpha
                self.loss += self.alpha * (self.P[u] - self.P[uf]).dot(self.P[u] - self.P[uf])
        self.loss += self.regU * (self.P * self.P).sum() + self.regI * (self.Q * self.Q).sum()
        epoch += 1
        if self.isConverged(epoch):
            break","for user1 in self.CUNet:
    s1 = set(self.filteredRatings[user1])
    for user2 in self.filteredRatings:
        if user1 != user2:
            s2 = set(self.filteredRatings[user2])
            weight = len(s1.intersection(s2))
            if weight > 0:
                self.CUNet[user1] += [user2] * weight","for user1 in self.CUNet:
    s1 = set(self.filteredRatings[user1])
    for user2 in self.filteredRatings:
        if user1 != user2:
            s2 = set(self.filteredRatings[user2])
            weight = len(s1.intersection(s2))
            if weight > 0:
                self.CUNet[user1].extend([user2] * weight)",find_wrong,2
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/reports/CreatHtml.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/reports/CreatHtml.py,CreatHtml,vuln_list$27,"def vuln_list(self):
    global creat_vuln_num
    creat_vuln_num = 1
    tr_whole_api_list = ''
    projectDBPath = DatabaseType(self.projectTag).getPathfromDB() + self.projectTag + '.db'
    connect = sqlite3.connect(os.sep.join(projectDBPath.split('/')))
    cursor = connect.cursor()
    connect.isolation_level = None
    sql = 'select * from vuln'
    cursor.execute(sql)
    vuln_infos = cursor.fetchall()
    for vuln_info in vuln_infos:
        if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
            sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
            cursor.execute(sql)
            api_infos = cursor.fetchall()
            for api_info in api_infos:
                tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
                if api_info[5] == 1:
                    vuln_url_type = Utils().getMyWord('{r_get}')
                else:
                    vuln_url_type = Utils().getMyWord('{r_post}')
                tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                try:
                    api_length_info = len(api_info[4])
                except:
                    api_length_info = 0
                for js_path in js_paths:
                    tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                    tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                    tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                    tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                    tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
                    tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                    self.creat_api_num = self.creat_api_num + 1
                    creat_vuln_num = creat_vuln_num + 1
                    tr_whole_api_list = tr_whole_api_list + tr_api_list
        elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
            sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
            cursor.execute(sql)
            api_infos = cursor.fetchall()
            for api_info in api_infos:
                tr_api_list = '\n                    <tr>\n                      <td>{vuln_num}</td>\n                      <td>{vuln_api_name}</td>\n                      <td>{vuln_url}</td>\n                      <td>{vuln_url_type}</td>\n                      <td>{vuln_risk}</td>\n                      <td>{vuln_length}</td>\n                      <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n                    </tr>'
                if api_info[5] == 1:
                    vuln_url_type = Utils().getMyWord('{r_get}')
                else:
                    vuln_url_type = Utils().getMyWord('{r_post}')
                tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
                sql = ""select path from js_file where id='%s'"" % vuln_info[2]
                cursor.execute(sql)
                js_paths = cursor.fetchall()
                try:
                    api_length_info = len(api_info[4])
                except:
                    api_length_info = 0
                for js_path in js_paths:
                    tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                    tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                    tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                    tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                    tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '' + Utils().getMyWord('{r_vuln_maybe}'))
                    tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                    self.creat_api_num = self.creat_api_num + 1
                    creat_vuln_num = creat_vuln_num + 1
                    tr_whole_api_list = tr_whole_api_list + tr_api_list
    return tr_whole_api_list","for vuln_info in vuln_infos:
    if vuln_info[3] == 'unAuth' and vuln_info[4] == 1:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n            <tr>\n              <td>{vuln_num}</td>\n              <td>{vuln_api_name}</td>\n              <td>{vuln_url}</td>\n              <td>{vuln_url_type}</td>\n              <td>{vuln_risk}</td>\n              <td>{vuln_length}</td>\n              <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n            </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_m}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list
    elif vuln_info[3] == 'unAuth' and vuln_info[4] == 2:
        sql = ""select * from api_tree where id='%s'"" % vuln_info[1]
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = '\n            <tr>\n              <td>{vuln_num}</td>\n              <td>{vuln_api_name}</td>\n              <td>{vuln_url}</td>\n              <td>{vuln_url_type}</td>\n              <td>{vuln_risk}</td>\n              <td>{vuln_length}</td>\n              <td><button type=""button"" class=""btn btn-primary"" data-toggle=""modal"" data-target=""#{vuln_id}"">More</button></td>\n            </tr>'
            if api_info[5] == 1:
                vuln_url_type = Utils().getMyWord('{r_get}')
            else:
                vuln_url_type = Utils().getMyWord('{r_post}')
            tr_api_list = tr_api_list.replace('{vuln_num}', str(self.creat_api_num))
            sql = ""select path from js_file where id='%s'"" % vuln_info[2]
            cursor.execute(sql)
            js_paths = cursor.fetchall()
            try:
                api_length_info = len(api_info[4])
            except:
                api_length_info = 0
            for js_path in js_paths:
                tr_api_list = tr_api_list.replace('{vuln_api_name}', api_info[2])
                tr_api_list = tr_api_list.replace('{vuln_url}', api_info[1])
                tr_api_list = tr_api_list.replace('{vuln_url_type}', vuln_url_type)
                tr_api_list = tr_api_list.replace('{vuln_length}', str(api_length_info))
                tr_api_list = tr_api_list.replace('{vuln_risk}', Utils().getMyWord('{r_l_l}') + '' + Utils().getMyWord('{r_vuln_maybe}'))
                tr_api_list = tr_api_list.replace('{vuln_id}', 'vuln_' + str(creat_vuln_num))
                self.creat_api_num = self.creat_api_num + 1
                creat_vuln_num = creat_vuln_num + 1
                tr_whole_api_list = tr_whole_api_list + tr_api_list","for vuln_info in vuln_infos:
    if vuln_info[3] == ""unAuth"" and vuln_info[4] in [1, 2]:
        sql = ""select * from api_tree where id='%s'"" % (vuln_info[1])
        cursor.execute(sql)
        api_infos = cursor.fetchall()
        for api_info in api_infos:
            tr_api_list = """"""
            <tr>
              <td>{vuln_num}</td>
              <td>{vuln_api_name}</td>
              <td>{vuln_url}</td>
              <td>{vuln_url_type}</td>
              <td>{vuln_risk}</td>
              <td>{vuln_length}</td>
              <td><button type=""button",find_wrong,-1
PaddleX,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex/interpret/core/normlime_base.py,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex/interpret/core/normlime_base.py,,compute_normlime_weights$204,"def compute_normlime_weights(a_list_lime_fnames, save_dir, lime_num_samples):
    normlime_weights_all_labels = {}
    for f in a_list_lime_fnames:
        try:
            lime_weights_and_cluster = np.load(f, allow_pickle=True).item()
            lime_weights = lime_weights_and_cluster['lime_weights']
            cluster = lime_weights_and_cluster['cluster']
        except:
            logging.info('When loading precomputed LIME result, skipping' + str(f))
            continue
        logging.info('Loading precomputed LIME result,' + str(f))
        pred_labels = lime_weights.keys()
        for y in pred_labels:
            normlime_weights = normlime_weights_all_labels.get(y, {})
            w_f_y = [abs(w[1]) for w in lime_weights[y]]
            w_f_y_l1norm = sum(w_f_y)
            for w in lime_weights[y]:
                seg_label = w[0]
                weight = w[1] * w[1] / w_f_y_l1norm
                a = normlime_weights.get(cluster[seg_label], [])
                a.append(weight)
                normlime_weights[cluster[seg_label]] = a
            normlime_weights_all_labels[y] = normlime_weights
    for y in normlime_weights_all_labels:
        normlime_weights = normlime_weights_all_labels.get(y, {})
        for k in normlime_weights:
            normlime_weights[k] = sum(normlime_weights[k]) / len(normlime_weights[k])
    if len(normlime_weights_all_labels.keys()) < max(normlime_weights_all_labels.keys()) + 1:
        logging.info('\n' + 'Warning: !!! \n' + 'There are at least {} classes, '.format(max(normlime_weights_all_labels.keys()) + 1) + 'but the NormLIME has results of only {} classes. \n'.format(len(normlime_weights_all_labels.keys())) + 'It may have cause unstable results in the later computation' + ' but can be improved by computing more test samples.' + '\n')
    n = 0
    f_out = 'normlime_weights_s{}_samples_{}-{}.npy'.format(lime_num_samples, len(a_list_lime_fnames), n)
    while os.path.exists(os.path.join(save_dir, f_out)):
        n += 1
        f_out = 'normlime_weights_s{}_samples_{}-{}.npy'.format(lime_num_samples, len(a_list_lime_fnames), n)
        continue
    np.save(os.path.join(save_dir, f_out), normlime_weights_all_labels)
    return os.path.join(save_dir, f_out)","for y in pred_labels:
    normlime_weights = normlime_weights_all_labels.get(y, {})
    w_f_y = [abs(w[1]) for w in lime_weights[y]]
    w_f_y_l1norm = sum(w_f_y)
    for w in lime_weights[y]:
        seg_label = w[0]
        weight = w[1] * w[1] / w_f_y_l1norm
        a = normlime_weights.get(cluster[seg_label], [])
        a.append(weight)
        normlime_weights[cluster[seg_label]] = a
    normlime_weights_all_labels[y] = normlime_weights","for y in pred_labels:
    normlime_weights = normlime_weights_all_labels.get(y, {})
    w_f_y = [abs(w[1]) for w in lime_weights[y]]
    w_f_y_l1norm = sum(w_f_y)
    for w in lime_weights[y]:
        seg_label = w[0]
        weight = w[1] * w[1] / w_f_y_l1norm
        a = normlime_weights.get(cluster[seg_label], [])
        a.append(weight)
        normlime_weights[cluster[seg_label]] = a
    normlime_weights_all_labels[y] = normlime_weights.copy()",find_wrong,-1
RootTheBox,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RootTheBox/handlers/AdminHandlers/AdminGameObjectHandlers.py,https://github.com/moloch--/RootTheBox/tree/master/handlers/AdminHandlers/AdminGameObjectHandlers.py,AdminViewHandler,post$403,"def post(self, *args, **kwargs):
    if args[0] == 'statistics' or args[0] == 'game_objects':
        uri = {'game_objects': 'admin/view/game_objects.html', 'statistics': 'admin/view/statistics.html'}
        flag_uuid = self.get_argument('flag_uuid', '')
        team_uuid = self.get_argument('team_uuid', '')
        user_uuid = self.get_argument('user_uuid', '')
        flag = Flag.by_uuid(flag_uuid)
        team = Team.by_uuid(team_uuid)
        user = User.by_uuid(user_uuid)
        errors = []
        success = []
        if flag:
            point_restore = self.get_argument('point_restore', '')
            accept_answer = self.get_argument('accept_answer', '')
            answer_token = self.get_argument('answer_token', '')
            if point_restore == 'on' and team:
                if options.penalize_flag_value:
                    penalty = Penalty.by_team_token(flag, team, answer_token)
                    if penalty:
                        value = penalty.cost()
                        if value > 0:
                            team.money += value
                            if user:
                                user.money += value
                                self.dbsession.add(user)
                            self.dbsession.add(team)
                            self.event_manager.admin_score_update(team, '%s penalty reversed - score has been updated.' % team.name, value)
                        self.dbsession.delete(penalty)
                        self.dbsession.commit()
                if flag not in team.flags:
                    flag_value = flag.dynamic_value(team)
                    if self.config.dynamic_flag_value and self.config.dynamic_flag_type == 'decay_all':
                        for item in Flag.team_captures(flag.id):
                            tm = Team.by_id(item[0])
                            deduction = flag.dynamic_value(tm) - flag_value
                            tm.money = int(tm.money - deduction)
                            self.dbsession.add(tm)
                            self.event_manager.flag_decayed(tm, flag)
                    team.money += flag_value
                    if user:
                        user.money += flag_value
                        user.flags.append(flag)
                        self.dbsession.add(user)
                    team.flags.append(flag)
                    self.dbsession.add(team)
                    self.dbsession.commit()
                    BoxHandler.success_capture(self, user, flag, flag_value)
                    self._check_level(flag, team)
                    self.event_manager.flag_captured(team, flag)
                    if options.banking:
                        price = '$' + str(flag_value)
                    else:
                        price = str(flag_value) + ' points'
                    success.append('%s awarded flag and %s' % (team.name, price))
            if accept_answer == 'on' and (flag.type == 'static' or flag.type == 'regex') and (not flag.capture(answer_token)):
                flag.type = 'regex'
                if flag.token.startswith('(') and flag.token.endwith(')'):
                    token = '%s|(%s)' % (flag.token, answer_token)
                else:
                    token = '(%s)|(%s)' % (flag.token, answer_token)
                if len(token) < 256:
                    flag.token = token
                    self.dbsession.add(flag)
                    self.dbsession.commit()
                    success.append('Token successfully added for Flag %s' % flag.name)
                else:
                    errors.append('Flag token too long. Can not expand token.')
        self.render(uri[args[0]], errors=errors, success=success)
    else:
        self.render('public/404.html')","if args[0] == 'statistics' or args[0] == 'game_objects':
    uri = {'game_objects': 'admin/view/game_objects.html', 'statistics': 'admin/view/statistics.html'}","if args[0] in ['statistics', 'game_objects']:
    uri = {'game_objects': 'admin/view/game_objects.html', 'statistics': 'admin/view/statistics.html'}",find_wrong,2
azure-cli,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/src/azure-cli/azure/cli/command_modules/backup/custom_wl.py,https://github.com/Azure/azure-cli/tree/master/src/azure-cli/azure/cli/command_modules/backup/custom_wl.py,,restore_azure_wl$615,"def restore_azure_wl(cmd, client, resource_group_name, vault_name, recovery_config, rehydration_duration=15, rehydration_priority=None, use_secondary_region=None):
    recovery_config_object = cust_help.get_or_read_json(recovery_config)
    restore_mode = recovery_config_object['restore_mode']
    container_uri = recovery_config_object['container_uri']
    item_uri = recovery_config_object['item_uri']
    recovery_point_id = recovery_config_object['recovery_point_id']
    log_point_in_time = recovery_config_object['log_point_in_time']
    item_type = recovery_config_object['item_type']
    workload_type = recovery_config_object['workload_type']
    source_resource_id = recovery_config_object['source_resource_id']
    database_name = recovery_config_object['database_name']
    container_id = recovery_config_object['container_id']
    alternate_directory_paths = recovery_config_object['alternate_directory_paths']
    recovery_mode = recovery_config_object['recovery_mode']
    filepath = recovery_config_object['filepath']
    item = common.show_item(cmd, backup_protected_items_cf(cmd.cli_ctx), resource_group_name, vault_name, container_uri, item_uri, 'AzureWorkload')
    cust_help.validate_item(item)
    validate_wl_restore(item, item_type, restore_mode, recovery_mode)
    trigger_restore_properties = _get_restore_request_instance(item_type, log_point_in_time, None)
    if log_point_in_time is None:
        recovery_point = common.show_recovery_point(cmd, recovery_points_cf(cmd.cli_ctx), resource_group_name, vault_name, container_uri, item_uri, recovery_point_id, workload_type, 'AzureWorkload', use_secondary_region)
        if recovery_point is None:
            raise InvalidArgumentValueError('\n            Specified recovery point not found. Please check the recovery config file\n            or try removing --use-secondary-region if provided')
        common.fetch_tier_for_rp(recovery_point)
        if recovery_point.tier_type is not None and recovery_point.tier_type == 'VaultArchive':
            if rehydration_priority is None:
                raise InvalidArgumentValueError('The selected recovery point is in archive tier, provide additional\n                parameters of rehydration duration and rehydration priority.')
            trigger_restore_properties = _get_restore_request_instance(item_type, log_point_in_time, rehydration_priority)
            rehyd_duration = 'P' + str(rehydration_duration) + 'D'
            rehydration_info = RecoveryPointRehydrationInfo(rehydration_retention_duration=rehyd_duration, rehydration_priority=rehydration_priority)
            trigger_restore_properties.recovery_point_rehydration_info = rehydration_info
    trigger_restore_properties.recovery_type = restore_mode
    if container_id is not None:
        target_container_name = cust_help.get_protection_container_uri_from_id(container_id)
        target_resource_group = cust_help.get_resource_group_from_id(container_id)
        target_vault_name = cust_help.get_vault_from_arm_id(container_id)
        target_container = common.show_container(cmd, backup_protection_containers_cf(cmd.cli_ctx), target_container_name, target_resource_group, target_vault_name, 'AzureWorkload')
        setattr(trigger_restore_properties, 'target_virtual_machine_id', target_container.properties.source_resource_id)
    if restore_mode == 'AlternateLocation':
        if recovery_mode != 'FileRecovery':
            setattr(trigger_restore_properties, 'source_resource_id', source_resource_id)
            setattr(trigger_restore_properties, 'target_info', TargetRestoreInfo(overwrite_option='Overwrite', database_name=database_name, container_id=container_id))
            if 'sql' in item_type.lower():
                directory_map = []
                for i in alternate_directory_paths:
                    directory_map.append(SQLDataDirectoryMapping(mapping_type=i[0], source_path=i[1], source_logical_name=i[2], target_path=i[3]))
                setattr(trigger_restore_properties, 'alternate_directory_paths', directory_map)
        else:
            target_info = TargetRestoreInfo(overwrite_option='Overwrite', container_id=container_id, target_directory_for_file_restore=filepath)
            setattr(trigger_restore_properties, 'target_info', target_info)
            trigger_restore_properties.recovery_mode = recovery_mode
    if log_point_in_time is not None:
        log_point_in_time = datetime_type(log_point_in_time)
        time_range_list = _get_log_time_range(cmd, resource_group_name, vault_name, item, use_secondary_region)
        validate_log_point_in_time(log_point_in_time, time_range_list)
        setattr(trigger_restore_properties, 'point_in_time', log_point_in_time)
    if 'sql' in item_type.lower():
        setattr(trigger_restore_properties, 'should_use_alternate_target_location', True)
        setattr(trigger_restore_properties, 'is_non_recoverable', False)
    trigger_restore_request = RestoreRequestResource(properties=trigger_restore_properties)
    if use_secondary_region:
        if rehydration_priority is not None:
            raise MutuallyExclusiveArgumentError(""Archive restore isn't supported for secondary region."")
        vault = vaults_cf(cmd.cli_ctx).get(resource_group_name, vault_name)
        vault_location = vault.location
        azure_region = custom.secondary_region_map[vault_location]
        aad_client = aad_properties_cf(cmd.cli_ctx)
        filter_string = cust_help.get_filter_string({'backupManagementType': 'AzureWorkload'})
        aad_result = aad_client.get(azure_region, filter_string)
        rp_client = recovery_points_passive_cf(cmd.cli_ctx)
        crr_access_token = rp_client.get_access_token(vault_name, resource_group_name, fabric_name, container_uri, item_uri, recovery_point_id, aad_result).properties
        crr_client = cross_region_restore_cf(cmd.cli_ctx)
        trigger_restore_properties.region = azure_region
        trigger_crr_request = CrossRegionRestoreRequest(cross_region_restore_access_details=crr_access_token, restore_request=trigger_restore_properties)
        result = crr_client.begin_trigger(azure_region, trigger_crr_request, cls=cust_help.get_pipeline_response, polling=False).result()
        return cust_help.track_backup_crr_job(cmd.cli_ctx, result, azure_region, vault.id)
    result = client.begin_trigger(vault_name, resource_group_name, fabric_name, container_uri, item_uri, recovery_point_id, trigger_restore_request, cls=cust_help.get_pipeline_response, polling=False).result()
    return cust_help.track_backup_job(cmd.cli_ctx, result, vault_name, resource_group_name)","if container_id is not None:
    target_container_name = cust_help.get_protection_container_uri_from_id(container_id)
    target_resource_group = cust_help.get_resource_group_from_id(container_id)
    target_vault_name = cust_help.get_vault_from_arm_id(container_id)
    target_container = common.show_container(cmd, backup_protection_containers_cf(cmd.cli_ctx), target_container_name, target_resource_group, target_vault_name, 'AzureWorkload')
    setattr(trigger_restore_properties, 'target_virtual_machine_id', target_container.properties.source_resource_id)","if container_id is not None:
    target_container_name = cust_help.get_protection_container_uri_from_id(container_id)
    target_resource_group = cust_help.get_resource_group_from_id(container_id)
    target_vault_name = cust_help.get_vault_from_arm_id(container_id)
    target_container = common.show_container(cmd, backup_protection_containers_cf(cmd.cli_ctx), target_container_name, target_resource_group, target_vault_name, 'AzureWorkload')
    for prop in ['target_virtual_machine_id']:
        setattr(trigger_restore_properties, prop, target_container.properties.source_resource_id)",find_wrong,2
natlas,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/natlas/natlas-server/app/api/routes.py,https://github.com/MJL85/natlas/tree/master/natlas-server/app/api/routes.py,,submit$94,"def submit():
    status_code = None
    response_body = None
    data = request.get_json()
    newhost = {}
    newhost = json.loads(data)
    newhost['ctime'] = dt.now(tz.utc)
    if newhost['scan_reason'] == 'requested':
        mark_scan_completed(newhost['ip'], newhost['scan_id'])
    try:
        nmap = NmapParser.parse(newhost.get('xml_data', None))
        if nmap.hosts_total != 1:
            status_code = 400
            response_body = json.dumps({'status': status_code, 'message': 'XML had too many hosts in it', 'retry': False})
        elif len(nmap.hosts) == 1 and (not current_app.ScopeManager.is_acceptable_target(nmap.hosts[0].address)):
            status_code = 400
            response_body = json.dumps({'status': status_code, 'message': 'Out of scope: ' + nmap.hosts[0].address, 'retry': False})
        elif not newhost['is_up'] or (newhost['is_up'] and newhost['port_count'] == 0):
            current_app.elastic.new_result(newhost)
            status_code = 200
            response_body = json.dumps({'status': status_code, 'message': 'Received: ' + newhost['ip']})
    except NmapParserException:
        status_code = 400
        response_body = json.dumps({'status': status_code, 'message': 'Invalid nmap xml data provided', 'retry': False})
    if status_code and response_body:
        response = Response(response=response_body, status=status_code, content_type=json_content)
        return response
    if newhost['scan_start'] and newhost['scan_stop']:
        elapsed = dateutil.parser.parse(newhost['scan_stop']) - dateutil.parser.parse(newhost['scan_start'])
        newhost['elapsed'] = elapsed.seconds
    newhost['ip'] = nmap.hosts[0].address
    if len(nmap.hosts[0].hostnames) > 0:
        newhost['hostname'] = nmap.hosts[0].hostnames[0]
    tmpports = []
    newhost['ports'] = []
    for port in nmap.hosts[0].get_open_ports():
        tmpports.append(str(port[0]))
        srv = nmap.hosts[0].get_service(port[0], port[1])
        portinfo = srv.get_dict()
        portinfo['service'] = srv.service_dict
        portinfo['scripts'] = []
        for script in srv.scripts_results:
            scriptsave = {'id': script['id'], 'output': script['output']}
            portinfo['scripts'].append(scriptsave)
            if script['id'] == 'ssl-cert':
                portinfo['ssl'] = parse_ssl_data(script)
        newhost['ports'].append(portinfo)
    newhost['port_str'] = ', '.join(tmpports)
    if 'screenshots' in newhost and newhost['screenshots']:
        (newhost['screenshots'], newhost['num_screenshots']) = process_screenshots(newhost['screenshots'])
    if len(newhost['ports']) == 0:
        status_code = 200
        response_body = json.dumps({'status': status_code, 'message': f""Expected open ports but didn't find any for {newhost['ip']}""})
    elif len(newhost['ports']) > 500:
        status_code = 200
        response_body = json.dumps({'status': status_code, 'message': 'More than 500 ports found, throwing data out'})
    else:
        status_code = 200
        current_app.elastic.new_result(newhost)
        response_body = json.dumps({'status': status_code, 'message': f""Received {len(newhost['ports'])} ports for {newhost['ip']}""})
    response = Response(response=response_body, status=status_code, content_type=json_content)
    return response","if newhost['scan_start'] and newhost['scan_stop']:
    elapsed = dateutil.parser.parse(newhost['scan_stop']) - dateutil.parser.parse(newhost['scan_start'])
    newhost['elapsed'] = elapsed.seconds","if 'scan_start' in newhost and 'scan_stop' in newhost:
    elapsed = dateutil.parser.parse(newhost['scan_stop']) - dateutil.parser.parse(newhost['scan_start'])
    newhost['elapsed'] = elapsed.seconds",find_wrong,2
kawaii-player,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/meta_engine.py,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/meta_engine.py,MetaEngine,get_epn_arr_list$195,"def get_epn_arr_list(self, site, name, video_dir):
    epn_arr = []
    if site.lower() == 'video' and video_dir:
        video_db = os.path.join(ui.home_folder, 'VideoDB', 'Video.db')
        if os.path.exists(video_db):
            epn_arr_tmp = ui.media_data.get_video_db(video_db, 'Directory', video_dir)
            for i in epn_arr_tmp:
                epn_name = i[0] + '\t' + i[1]
                logger.debug(epn_name)
                epn_arr.append(epn_name)
    elif video_dir:
        new_name_with_info = video_dir.strip()
        extra_info = ''
        if '\t' in new_name_with_info:
            name_title = new_name_with_info.split('\t')[0]
            extra_info = new_name_with_info.split('\t')[1]
        else:
            name_title = new_name_with_info
        if site.lower() == 'subbedanime' or site.lower() == 'dubbedanime':
            siteName = ui.get_parameters_value(s='siteName')['siteName']
            hist_site = os.path.join(ui.home_folder, 'History', site, siteName, name_title)
        else:
            hist_site = os.path.join(ui.home_folder, 'History', site, name_title)
        hist_epn = os.path.join(hist_site, 'Ep.txt')
        logger.info(hist_epn)
        if os.path.exists(hist_epn):
            lines = open_files(hist_epn, True)
            for i in lines:
                i = i.strip()
                j = i.split('\t')
                if len(j) == 1:
                    epn_arr.append(i + '\t' + i + '\t' + name)
                elif len(j) >= 2:
                    epn_arr.append(i + '\t' + name)
    return epn_arr",if site.lower() == 'subbedanime' or site.lower() == 'dubbedanime':,"if site.lower() in {'subbedanime', 'dubbedanime'}:",find_wrong,2
brat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/brat/tools/fix_annotations.py,https://github.com/nlplab/brat/tree/master/tools/fix_annotations.py,,correct_annotations$28,"def correct_annotations(orig_fn, ann_fn, change_fn):
    with annotation.TextAnnotations(ann_fn) as anns:
        orig_text = anns.get_document_text()
        with annotation.open_textfile(change_fn, 'r') as f:
            changed_text = f.read()
        diffs = diff_match_patch().diff_main(orig_text, changed_text)
        orig_offset = 0
        offsets = []
        for diff in diffs:
            kind = diff[0]
            text = diff[1]
            size = len(text)
            delta = size * kind
            offsets.append((orig_offset, delta))
            if kind != 1:
                orig_offset += size
        offsets = offsets[::-1]
        tbs = list(anns.get_textbounds())
        indices = []
        for (tbi, tb) in enumerate(tbs):
            for (spani, span) in enumerate(tb.spans):
                indices.append((span[0], tbi, spani, 0))
                indices.append((span[1], tbi, spani, 1))
        indices.sort(reverse=True)
        for (orig_offset, delta) in offsets:
            for index in indices:
                if index[0] < orig_offset:
                    break
                frag = list(tbs[index[1]].spans[index[2]])
                frag[index[3]] += delta
                tbs[index[1]].spans[index[2]] = tuple(frag)
        for tb in tbs:
            if isinstance(tb, annotation.TextBoundAnnotationWithText):
                tb.text = annotation.DISCONT_SEP.join((changed_text[start:end] for (start, end) in tb.spans))
    copy(change_fn, orig_fn)","indices = []
for (tbi, tb) in enumerate(tbs):
    for (spani, span) in enumerate(tb.spans):
        indices.append((span[0], tbi, spani, 0))
        indices.append((span[1], tbi, spani, 1))
indices.sort(reverse=True)
for (orig_offset, delta) in offsets:
    for index in indices:
        if index[0] < orig_offset:
            break
        frag = list(tbs[index[1]].spans[index[2]])
        frag[index[3]] += delta
        tbs[index[1]].spans[index[2]] = tuple(frag)","indices = [(span[0], tbi, spani, 0) for (tbi, tb) in enumerate(tbs) for (spani, span) in enumerate(tb.spans)] + [(span[1], tbi, spani, 1) for (tbi, tb) in enumerate(tbs) for (spani, span) in enumerate(tb.spans)]
indices.sort(reverse=True)
for (orig_offset, delta) in offsets:
    for index in indices:
        if index[0] < orig_offset:
            break
        (tbi, spani, frag_index) = (index[1], index[2], index[3])
        frag = list(tbs[tbi].spans[spani])
        frag[frag_index] += delta
        tbs[tbi].spans[spani] = tuple(frag)",find_wrong,-1
security_monkey,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/security_monkey/security_monkey/cloudaux_watcher.py,https://github.com/Netflix/security_monkey/tree/master/security_monkey/cloudaux_watcher.py,CloudAuxWatcher,_flatten_iter_response$59,"def _flatten_iter_response(self, response):
    """"""
        The cloudaux iter_account_region decorator returns a list of tuples.
        Each tuple contains two members.  1) The result. 2) The exception map.
        This method combines that list of tuples into a single result list and a single exception map.
        """"""
    items = list()
    exception_map = dict()
    for result in response:
        items.extend(result[0])
        exception_map.update(result[1])
    return (items, exception_map)","items = list()
exception_map = dict()
for result in response:
    items.extend(result[0])
    exception_map.update(result[1])
return (items, exception_map)","items = []
exception_map = {}
for (result_0, result_1, *_) in response:
    items.extend(result_0)
    exception_map.update(result_1)
return (items, exception_map)",find_wrong,1
redis-py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/redis-py/tests/test_search.py,https://github.com/redis/redis-py/tree/master/tests/test_search.py,,test_drop_index$308,"def test_drop_index(client):
    """"""
    Ensure the index gets dropped by data remains by default
    """"""
    for x in range(20):
        for keep_docs in [[True, {}], [False, {'name': 'haveit'}]]:
            idx = 'HaveIt'
            index = getClient(client)
            index.hset('index:haveit', mapping={'name': 'haveit'})
            idef = IndexDefinition(prefix=['index:'])
            index.ft(idx).create_index((TextField('name'),), definition=idef)
            waitForIndex(index, idx)
            index.ft(idx).dropindex(delete_documents=keep_docs[0])
            i = index.hgetall('index:haveit')
            assert i == keep_docs[1]","for x in range(20):
    for keep_docs in [[True, {}], [False, {'name': 'haveit'}]]:
        idx = 'HaveIt'
        index = getClient(client)
        index.hset('index:haveit', mapping={'name': 'haveit'})
        idef = IndexDefinition(prefix=['index:'])
        index.ft(idx).create_index((TextField('name'),), definition=idef)
        waitForIndex(index, idx)
        index.ft(idx).dropindex(delete_documents=keep_docs[0])
        i = index.hgetall('index:haveit')
        assert i == keep_docs[1]","keep_docs_list = [[True, {}], [False, {'name': 'haveit'}]]
for x in range(20):
    for keep_docs in keep_docs_list:
        idx = 'HaveIt'
        index = getClient(client)
        index.hset('index:haveit', mapping={'name': 'haveit'})
        idef = IndexDefinition(prefix=['index:'])
        index.ft(idx).create_index((TextField('name'),), definition=idef)
        waitForIndex(index, idx)
        index.ft(idx).dropindex(delete_documents=keep_docs[0])
        i = index.hgetall('index:haveit')
        assert i == keep_docs[1]",find_wrong,-1
django-rosetta,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-rosetta/rosetta/views.py,https://github.com/mbi/django-rosetta/tree/master/rosetta/views.py,TranslationFileListView,get_context_data$200,"def get_context_data(self, **kwargs):
    context = super(TranslationFileListView, self).get_context_data(**kwargs)
    third_party_apps = self.po_filter in ('all', 'third-party')
    django_apps = self.po_filter in ('all', 'django')
    project_apps = self.po_filter in ('all', 'project')
    languages = []
    has_pos = False
    for language in rosetta_settings.ROSETTA_LANGUAGES:
        if not can_translate_language(self.request.user, language[0]):
            continue
        po_paths = find_pos(language[0], project_apps=project_apps, django_apps=django_apps, third_party_apps=third_party_apps)
        po_files = [(get_app_name(lang), os.path.realpath(lang), pofile(lang)) for lang in po_paths]
        po_files.sort(key=lambda app: app[0])
        languages.append((language[0], _(language[1]), po_files))
        has_pos = has_pos or bool(po_paths)
    context['version'] = get_rosetta_version()
    context['languages'] = languages
    context['has_pos'] = has_pos
    context['po_filter'] = self.po_filter
    return context","languages = []
for language in rosetta_settings.ROSETTA_LANGUAGES:
    if not can_translate_language(self.request.user, language[0]):
        continue
    po_paths = find_pos(language[0], project_apps=project_apps, django_apps=django_apps, third_party_apps=third_party_apps)
    po_files = [(get_app_name(lang), os.path.realpath(lang), pofile(lang)) for lang in po_paths]
    po_files.sort(key=lambda app: app[0])
    languages.append((language[0], _(language[1]), po_files))
    has_pos = has_pos or bool(po_paths)","languages = []
for language in rosetta_settings.ROSETTA_LANGUAGES:
    if not can_translate_language(self.request.user, language[0]):
        continue
    po_paths = find_pos(language[0], project_apps=project_apps, django_apps=django_apps, third_party_apps=third_party_apps)
    po_files = [(get_app_name(lang), os.path.realpath(lang), pofile(lang)) for lang in po_paths]
    po_files.sort(key=lambda app: app[0])
    languages.append((language[0], _(language[1]), po_files))
    has_pos = has_pos or bool(po_paths)
context = {'version': get_rosetta_version(), 'languages': languages, 'has_pos': has_pos, 'po_filter': self.po_filter}
return super().get_context_data(**context)",find_wrong,-1
Mycodo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/mycodo_flask/utils/utils_general.py,https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/utils/utils_general.py,,generate_form_input_list$1635,"def generate_form_input_list(dict_inputs):
    list_tuples_sorted = sorted(dict_inputs.items(), key=lambda x: (x[1]['input_manufacturer'], x[1]['input_name']))
    list_inputs_sorted = []
    for each_input in list_tuples_sorted:
        list_inputs_sorted.append(each_input[0])
    return list_inputs_sorted","list_inputs_sorted = []
for each_input in list_tuples_sorted:
    list_inputs_sorted.append(each_input[0])",list_inputs_sorted = [each_input[0] for each_input in list_tuples_sorted],find_wrong,-1
Magic-UV,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Magic-UV/src/magic_uv/common.py,https://github.com/nutti/Magic-UV/tree/master/src/magic_uv/common.py,,__get_loop_sequence_internal$824,"def __get_loop_sequence_internal(uv_layer, pairs, island_info, closed):
    loop_sequences = []
    for pair in pairs:
        seqs = [pair]
        p = pair
        isl_grp = __get_island_group_include_pair(pair, island_info)
        if isl_grp == -1:
            return (None, 'Can not find the island or invalid island')
        while True:
            nlp = __get_next_loop_pair(p)
            if not nlp:
                break
            nlp_isl_grp = __get_island_group_include_pair(nlp, island_info)
            if nlp_isl_grp != isl_grp:
                break
            for nlpl in nlp:
                if nlpl[uv_layer].select:
                    return (None, 'Do not select UV which does not belong to the end edge')
            seqs.append(nlp)
            if len(nlp) == 1 and closed:
                break
            nplp = __get_next_poly_loop_pair(nlp)
            if not nplp:
                break
            nplp_isl_grp = __get_island_group_include_pair(nplp, island_info)
            if nplp_isl_grp != isl_grp:
                break
            matched = False
            for p1 in seqs:
                p2 = nplp
                if p1[0] == p2[0] and p1[1] == p2[1] or (p1[0] == p2[1] and p1[1] == p2[0]):
                    matched = True
            if matched:
                debug_print('This is a circular sequence')
                break
            for nlpl in nplp:
                if nlpl[uv_layer].select:
                    return (None, 'Do not select UV which does not belong to the end edge')
            seqs.append(nplp)
            p = nplp
        loop_sequences.append(seqs)
    return (loop_sequences, '')","for pair in pairs:
    seqs = [pair]
    p = pair
    isl_grp = __get_island_group_include_pair(pair, island_info)
    if isl_grp == -1:
        return (None, 'Can not find the island or invalid island')
    while True:
        nlp = __get_next_loop_pair(p)
        if not nlp:
            break
        nlp_isl_grp = __get_island_group_include_pair(nlp, island_info)
        if nlp_isl_grp != isl_grp:
            break
        for nlpl in nlp:
            if nlpl[uv_layer].select:
                return (None, 'Do not select UV which does not belong to the end edge')
        seqs.append(nlp)
        if len(nlp) == 1 and closed:
            break
        nplp = __get_next_poly_loop_pair(nlp)
        if not nplp:
            break
        nplp_isl_grp = __get_island_group_include_pair(nplp, island_info)
        if nplp_isl_grp != isl_grp:
            break
        matched = False
        for p1 in seqs:
            p2 = nplp
            if p1[0] == p2[0] and p1[1] == p2[1] or (p1[0] == p2[1] and p1[1] == p2[0]):
                matched = True
        if matched:
            debug_print('This is a circular sequence')
            break
        for nlpl in nplp:
            if nlpl[uv_layer].select:
                return (None, 'Do not select UV which does not belong to the end edge')
        seqs.append(nplp)
        p = nplp
    loop_sequences.append(seqs)","loop_sequences = []
for pair in pairs:
    seqs = [pair]
    p = pair
    isl_grp = __get_island_group_include_pair(pair, island_info)
    if isl_grp == -1:
        return (None, 'Can not find the island or invalid island')
    while True:
        nlp = __get_next_loop_pair(p)
        if not nlp:
            break
        nlp_isl_grp = __get_island_group_include_pair(nlp, island_info)
        if nlp_isl_grp != isl_grp:
            break
        for nlpl in nlp:
            if nlpl[uv_layer].select:
                return (None, 'Do not select UV which does not belong to the end edge')
        seqs.append(nlp)
        if len(nlp) == 1 and closed:
            break
        nplp = __get_next_poly_loop_pair(nlp)
        if not nplp:
            break
        nplp_isl_grp = __get_island_group_include_pair(nplp, island_info)
        if nplp_isl_grp != isl_grp:
            break
        matched = False
        for p1 in seqs:
            p2 = nplp
            if p1[0] == p2[0] and p1[1] == p2[1] or (p1[0] == p2[1] and p1[1] == p2[0]):
                matched = True
        if matched:
            debug_print('This is a circular sequence')
            break
        for nlpl in nplp:
            if nlpl[uv_layer].select:
                return (None, 'Do not select UV which does not belong to the end edge')
        seqs.append(nplp)
        p = nplp
    loop_sequences.append(seqs)",find_wrong,-1
microk8s,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/microk8s/scripts/wrappers/status.py,https://github.com/ubuntu/microk8s/tree/master/scripts/wrappers/status.py,,print_pretty$33,"def print_pretty(isReady, enabled_addons, disabled_addons):
    console_formatter = '{:>3} {:<20} # ({}) {}'
    if isReady:
        print('microk8s is running')
        if not is_ha_enabled():
            print('high-availability: no')
        else:
            info = get_dqlite_info()
            if ha_cluster_formed(info):
                print('high-availability: yes')
            else:
                print('high-availability: no')
            masters = 'none'
            standby = 'none'
            for node in info:
                if node[1] == 'voter':
                    if masters == 'none':
                        masters = '{}'.format(node[0])
                    else:
                        masters = '{} {}'.format(masters, node[0])
                if node[1] == 'standby':
                    if standby == 'none':
                        standby = '{}'.format(node[0])
                    else:
                        standby = '{} {}'.format(standby, node[0])
            print('{:>2}{} {}'.format('', 'datastore master nodes:', masters))
            print('{:>2}{} {}'.format('', 'datastore standby nodes:', standby))
        print('addons:')
        if enabled_addons and len(enabled_addons) > 0:
            print('{:>2}{}'.format('', 'enabled:'))
            for enabled in enabled_addons:
                print(console_formatter.format('', enabled['name'], enabled['repository'], enabled['description']))
        if disabled_addons and len(disabled_addons) > 0:
            print('{:>2}{}'.format('', 'disabled:'))
            for disabled in disabled_addons:
                print(console_formatter.format('', disabled['name'], disabled['repository'], disabled['description']))
    else:
        print('microk8s is not running. Use microk8s inspect for a deeper inspection.')","masters = 'none'
standby = 'none'
for node in info:
    if node[1] == 'voter':
        if masters == 'none':
            masters = '{}'.format(node[0])
        else:
            masters = '{} {}'.format(masters, node[0])
    if node[1] == 'standby':
        if standby == 'none':
            standby = '{}'.format(node[0])
        else:
            standby = '{} {}'.format(standby, node[0])
print('{:>2}{} {}'.format('', 'datastore master nodes:', masters))
print('{:>2}{} {}'.format('', 'datastore standby nodes:', standby))","masters = []
standby = []
for node in info:
    if node[1] == 'voter':
        masters.append(node[0])
    if node[1] == 'standby':
        standby.append(node[0])
print('{:>2}{} {}'.format('', 'datastore master nodes:', ' '.join(masters)))
print('{:>2}{} {}'.format('', 'datastore standby nodes:', ' '.join(standby)))",find_wrong,-1
azure-devops-cli-extension,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-devops-cli-extension/scripts/findEmptyHelpTexts.py,https://github.com/Azure/azure-devops-cli-extension/tree/master/scripts/findEmptyHelpTexts.py,,print_missing_help_files$63,"def print_missing_help_files(help_files):
    missing_help_text = False
    missing_help_list = []
    for help_file in help_files:
        if help_file.short_summary == '' and help_file.command != '':
            is_command = isinstance(help_file, CliCommandHelpFile)
            command_type = 'command' if is_command else 'group'
            missing_help_list.append((command_type, help_file.command))
            missing_help_text = True
    if not missing_help_text:
        print('No missing help texts found.')
    else:
        print('\n\nNo help texts were found for below command(s):')
        for text in missing_help_list:
            print('{} : {}'.format(text[0], text[1]))
        raise Exception('Please update the help text(s).')","for help_file in help_files:
    if help_file.short_summary == '' and help_file.command != '':
        is_command = isinstance(help_file, CliCommandHelpFile)
        command_type = 'command' if is_command else 'group'
        missing_help_list.append((command_type, help_file.command))
        missing_help_text = True","missing_help_list = [('command' if isinstance(help_file, CliCommandHelpFile) else 'group', help_file.command) for help_file in help_files if help_file.short_summary == '' and help_file.command != '']
missing_help_text = bool(missing_help_list)",find_wrong,2
joinmarket-clientserver,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/joinmarket-clientserver/scripts/snicker/snicker-recovery.py,https://github.com/JoinMarket-Org/joinmarket-clientserver/tree/master/scripts/snicker/snicker-recovery.py,,main$74,"def main():
    parser = OptionParser(usage='usage: %prog [options] walletname', description=description)
    parser.add_option('-m', '--mixdepth', action='store', type='int', dest='mixdepth', default=0, help='mixdepth to source coins from')
    parser.add_option('-a', '--amtmixdepths', action='store', type='int', dest='amtmixdepths', help='number of mixdepths in wallet, default 5', default=5)
    parser.add_option('-g', '--gap-limit', type='int', action='store', dest='gaplimit', help='gap limit for wallet, default=6', default=6)
    add_base_options(parser)
    (options, args) = parser.parse_args()
    load_program_config(config_path=options.datadir)
    check_regtest()
    if len(args) != 1:
        log.error('Invalid arguments, see --help')
        sys.exit(EXIT_ARGERROR)
    wallet_name = args[0]
    wallet_path = get_wallet_path(wallet_name, None)
    max_mix_depth = max([options.mixdepth, options.amtmixdepths - 1])
    wallet = open_test_wallet_maybe(wallet_path, wallet_name, max_mix_depth, wallet_password_stdin=options.wallet_password_stdin, gap_limit=options.gaplimit)
    wallet_service = WalletService(wallet)
    if not options.recoversync:
        jmprint('Recovery sync was not set, but using it anyway.')
    while not wallet_service.synced:
        wallet_service.sync_wallet(fast=False)
    seed_transactions = wallet_service.get_all_transactions()
    if len(seed_transactions) == 0:
        jmprint('No transactions were found for this wallet. Did you rescan?')
        return False
    new_txs = []
    current_block_heights = set()
    for tx in seed_transactions:
        if btc.is_snicker_tx(tx):
            jmprint('Found a snicker tx: {}'.format(bintohex(tx.GetTxid()[::-1])))
            equal_outs = btc.get_equal_outs(tx)
            if not equal_outs:
                continue
            if all([wallet_service.is_known_script(x.scriptPubKey) == False for x in [a[1] for a in equal_outs]]):
                my_indices = get_pubs_and_indices_of_inputs(tx, wallet_service, ours=True)
                for (mypub, mi) in my_indices:
                    for eo in equal_outs:
                        for (other_pub, i) in get_pubs_and_indices_of_inputs(tx, wallet_service, ours=False):
                            for (our_pub, j) in get_pubs_and_indices_of_ancestor_inputs(tx.vin[mi], wallet_service, ours=True):
                                our_spk = wallet_service.pubkey_to_script(our_pub)
                                our_priv = wallet_service.get_key_from_addr(wallet_service.script_to_addr(our_spk))
                                tweak_bytes = btc.ecdh(our_priv[:-1], other_pub)
                                tweaked_pub = btc.snicker_pubkey_tweak(our_pub, tweak_bytes)
                                tweaked_spk = wallet_service.pubkey_to_script(tweaked_pub)
                                if tweaked_spk == eo[1].scriptPubKey:
                                    address_found = str(btc.CCoinAddress.from_scriptPubKey(btc.CScript(tweaked_spk)))
                                    jmprint('Found a new SNICKER output belonging to us.')
                                    jmprint('Output address {} in the following transaction:'.format(address_found))
                                    jmprint(btc.human_readable_transaction(tx))
                                    jmprint('Importing the address into the joinmarket wallet...')
                                    (success, msg) = wallet_service.check_tweak_matches_and_import(wallet_service.script_to_addr(our_spk), tweak_bytes, tweaked_pub, wallet_service.mixdepth)
                                    if not success:
                                        jmprint('Failed to import SNICKER key: {}'.format(msg), 'error')
                                        return False
                                    else:
                                        jmprint('... success.')
                                    current_block_heights.add(wallet_service.get_transaction_block_height(tx))
                                    new_txs.append(tx)
    if len(new_txs) == 0:
        return True
    seed_transactions.extend(new_txs)
    earliest_new_blockheight = min(current_block_heights)
    jmprint('New SNICKER addresses were imported to the Core wallet; do rescanblockchain again, starting from block {}, before restarting this script.'.format(earliest_new_blockheight))
    return False","for tx in seed_transactions:
    if btc.is_snicker_tx(tx):
        jmprint('Found a snicker tx: {}'.format(bintohex(tx.GetTxid()[::-1])))
        equal_outs = btc.get_equal_outs(tx)
        if not equal_outs:
            continue
        if all([wallet_service.is_known_script(x.scriptPubKey) == False for x in [a[1] for a in equal_outs]]):
            my_indices = get_pubs_and_indices_of_inputs(tx, wallet_service, ours=True)
            for (mypub, mi) in my_indices:
                for eo in equal_outs:
                    for (other_pub, i) in get_pubs_and_indices_of_inputs(tx, wallet_service, ours=False):
                        for (our_pub, j) in get_pubs_and_indices_of_ancestor_inputs(tx.vin[mi], wallet_service, ours=True):
                            our_spk = wallet_service.pubkey_to_script(our_pub)
                            our_priv = wallet_service.get_key_from_addr(wallet_service.script_to_addr(our_spk))
                            tweak_bytes = btc.ecdh(our_priv[:-1], other_pub)
                            tweaked_pub = btc.snicker_pubkey_tweak(our_pub, tweak_bytes)
                            tweaked_spk = wallet_service.pubkey_to_script(tweaked_pub)
                            if tweaked_spk == eo[1].scriptPubKey:
                                address_found = str(btc.CCoinAddress.from_scriptPubKey(btc.CScript(tweaked_spk)))
                                jmprint('Found a new SNICKER output belonging to us.')
                                jmprint('Output address {} in the following transaction:'.format(address_found))
                                jmprint(btc.human_readable_transaction(tx))
                                jmprint('Importing the address into the joinmarket wallet...')
                                (success, msg) = wallet_service.check_tweak_matches_and_import(wallet_service.script_to_addr(our_spk), tweak_bytes, tweaked_pub, wallet_service.mixdepth)
                                if not success:
                                    jmprint('Failed to import SNICKER key: {}'.format(msg), 'error')
                                    return False
                                else:
                                    jmprint('... success.')
                                current_block_heights.add(wallet_service.get_transaction_block_height(tx))
                                new_txs.append(tx)","new_txs = []
current_block_heights = set()
for tx in [tx for tx in seed_transactions if btc.is_snicker_tx(tx)]:
    jmprint(""Found a snicker tx: {}"".format(bintohex(tx.GetTxid()[::-1])))
    equal_outs = btc.get_equal_outs(tx)
    if not equal_outs:
        continue
    if all([wallet_service.is_known_script(x.scriptPubKey) == False for x in [a[1] for a in equal_outs]]):
        # it is now *very* likely that one of the two equal
        # outputs is our SNICKER custom output
        # script; notice that in this case, the transaction *must*
        # have spent our inputs, since it didn't recognize ownership
        # of either coinjoin output (and if it did recognize the change,
        # it would have recognized the cj output also).
        # We try to regenerate one of the outputs, but warn if
        # we can't.
        my_indices = get_pubs_and_indices_of_inputs(tx, wallet_service, ours=True)
        for mypub, mi in my_indices:
            for eo in equal_outs:
                for (other_pub, i) in get_pubs_and_indices_of_inputs(tx, wallet_service, ours=False):
                    for (our_pub, j) in get_pubs_and",find_wrong,-1
MultiQC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MultiQC/multiqc/modules/bbmap/plot_bhist.py,https://github.com/ewels/MultiQC/tree/master/multiqc/modules/bbmap/plot_bhist.py,,plot_bhist$6,"def plot_bhist(samples, file_type, **plot_args):
    """"""Create line graph plot of histogram data for BBMap 'bhist' output.

    The 'samples' parameter could be from the bbmap mod_data dictionary:
    samples = bbmap.MultiqcModule.mod_data[file_type]
    """"""
    all_x = set()
    for item in sorted(chain(*[samples[sample]['data'].items() for sample in samples])):
        all_x.add(item[0])
    columns_to_plot = {'GC': {1: 'C', 2: 'G'}, 'AT': {0: 'A', 3: 'T'}, 'N': {4: 'N'}}
    nucleotide_data = []
    for column_type in columns_to_plot:
        nucleotide_data.append({sample + '.' + column_name: {x: samples[sample]['data'][x][column] * 100 if x in samples[sample]['data'] else 0 for x in all_x} for sample in samples for (column, column_name) in columns_to_plot[column_type].items()})
    plot_params = {'id': 'bbmap-' + file_type + '_plot', 'title': 'BBTools: ' + plot_args['plot_title'], 'xlab': 'Read position', 'ylab': 'Percentage of G+C bases', 'ymin': 0, 'ymax': 100, 'data_labels': [{'name': 'Percentage of G+C bases'}, {'name': 'Percentage of A+T bases'}, {'name': 'Percentage of N bases'}]}
    plot_params.update(plot_args['plot_params'])
    plot = linegraph.plot(nucleotide_data, plot_params)
    return plot","for column_type in columns_to_plot:
    nucleotide_data.append({sample + '.' + column_name: {x: samples[sample]['data'][x][column] * 100 if x in samples[sample]['data'] else 0 for x in all_x} for sample in samples for (column, column_name) in columns_to_plot[column_type].items()})","nucleotide_data = []
for column_type in columns_to_plot:
    for (column, column_name) in columns_to_plot[column_type].items():
        for sample in samples:
            nucleotide_data.append({sample + '.' + column_name: {x: samples[sample]['data'][x][column] * 100 if x in samples[sample]['data'] else 0 for x in all_x}})",find_wrong,2
OpenCore-Legacy-Patcher,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenCore-Legacy-Patcher/resources/cli_menu.py,https://github.com/dortania/OpenCore-Legacy-Patcher/tree/master/resources/cli_menu.py,MenuOptions,patcher_settings_security$968,"def patcher_settings_security(self):
    response = None
    while not (response and response == -1):
        title = ['Adjust Security Settings']
        menu = utilities.TUIMenu(title, 'Please select an option: ', auto_number=True, top_level=True)
        options = [[f'Set System Integrity Protection (SIP):\tCurrently {self.constants.custom_sip_value or self.constants.sip_status}', MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_sip], [f'Set Secure Boot Model (SBM):\t\tCurrently {self.constants.secure_status}', MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_sbm], [f'Set Vault Mode:\t\t\t\tCurrently {self.constants.vault}', MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_vault]]
        for option in options:
            menu.add_menu_option(option[0], function=option[1])
        response = menu.start()","options = [[f'Set System Integrity Protection (SIP):\tCurrently {self.constants.custom_sip_value or self.constants.sip_status}', MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_sip], [f'Set Secure Boot Model (SBM):\t\tCurrently {self.constants.secure_status}', MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_sbm], [f'Set Vault Mode:\t\t\t\tCurrently {self.constants.vault}', MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_vault]]","options = [{'text': f'Set System Integrity Protection (SIP):\tCurrently {self.constants.custom_sip_value or self.constants.sip_status}', 'function': MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_sip}, {'text': f'Set Secure Boot Model (SBM):\t\tCurrently {self.constants.secure_status}', 'function': MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_sbm}, {'text': f'Set Vault Mode:\t\t\t\tCurrently {self.constants.vault}', 'function': MenuOptions(self.constants.custom_model or self.constants.computer.real_model, self.constants).change_vault}]",find_wrong,2
sparkup,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sparkup/sparkup.py,https://github.com/rstacruz/sparkup/tree/master//sparkup.py,Parser,_textmatify$404,"def _textmatify(self, output):
    """"""Returns a version of the output with TextMate placeholders in it.
        """"""
    matches = re.findall('(></)|("""")|(\\n\\s+)\\n|(.|\\s)', output)
    output = ''
    n = 1
    for i in matches:
        if i[0]:
            output += '>$%i</' % n
            n += 1
        elif i[1]:
            output += '""$%i""' % n
            n += 1
        elif i[2]:
            output += i[2] + '$%i\n' % n
            n += 1
        elif i[3]:
            output += i[3]
    output += '$0'
    return output","matches = re.findall(r'(></)|("""")|(\n\s+)\n|(.|\s)', output)
        output = ''
        n = 1
        for i in matches:
            if i[0]:
                output += '>$%i</' % n
                n += 1
            elif i[1]:
                output += '""$%i""' % n
                n += 1
            elif i[2]:
                output += i[2] + '$%i\n' % n
                n += 1
            elif i[3]:
                output += i[3]
        output += ""$0""","output = ''
        n = 1
        for match in re.finditer(r'(></)|("""")|(\n\s+)\n|(.|\s)', output):
            if match.group(1):
                output += '>$%i</' % n
                n += 1
            elif match.group(2):
                output += '""$%i""' % n
                n += 1
            elif match.group(3):
                output += match.group(3) + '$%i\n' % n
                n += 1
            elif match.group(4):
                output += match.group(4)
        output += ""$0""",find_wrong,-1
Home-Assistant-custom-components-Xiaomi-Cloud-Map-Extractor,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Home-Assistant-custom-components-Xiaomi-Cloud-Map-Extractor/custom_components/xiaomi_cloud_map_extractor/roidmi/map_data_parser.py,https://github.com/PiotrMachowski/Home-Assistant-custom-components-Xiaomi-Cloud-Map-Extractor/tree/master/custom_components/xiaomi_cloud_map_extractor/roidmi/map_data_parser.py,MapDataParserRoidmi,parse_path$85,"def parse_path(map_info: dict) -> Path:
    path_points = []
    if 'posArray' in map_info:
        raw_points = json.loads(map_info['posArray'])
        for raw_point in raw_points:
            point = Point(raw_point[0], raw_point[1])
            path_points.append(point)
    return Path(None, None, None, [path_points])","raw_points = json.loads(map_info['posArray'])
for raw_point in raw_points:
    point = Point(raw_point[0], raw_point[1])
    path_points.append(point)","path_points = [Point(raw_point[0], raw_point[1]) for raw_point in json.loads(map_info['posArray'])]",find_wrong,-1
cocoNLP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cocoNLP/dist/cocoNLP-0.0.9/cocoNLP/config/phrase/rake.py,https://github.com/fighting41love/cocoNLP/tree/master/dist/cocoNLP-0.0.9/cocoNLP/config/phrase/rake.py,Rake,_get_phrase_list_from_words$218,"def _get_phrase_list_from_words(self, word_list, min_len, max_len):
    """"""Method to create contender phrases from the list of words that form
        a sentence by dropping stopwords and punctuations and grouping the left
        words into phrases. Only phrases in the given length range (both limits
        inclusive) would be considered to build co-occurrence matrix. Ex:

        Sentence: Red apples, are good in flavour.
        List of words: ['red', 'apples', "","", 'are', 'good', 'in', 'flavour']
        List after dropping punctuations and stopwords.
        List of words: ['red', 'apples', *, *, good, *, 'flavour']
        List of phrases: [('red', 'apples'), ('good',), ('flavour',)]

        List of phrases with a correct length:
        For the range [1, 2]: [('red', 'apples'), ('good',), ('flavour',)]
        For the range [1, 1]: [('good',), ('flavour',)]
        For the range [2, 2]: [('red', 'apples')]

        :param word_list: List of words which form a sentence when joined in
                          the same order.
        :return: List of contender phrases that are formed after dropping
                 stopwords and punctuations.
        """"""
    groups = groupby(word_list, lambda x: x not in self.to_ignore)
    phrases = []
    for group in groups:
        tmp = tuple(group[1])
        len_g1 = len(list(tmp))
        if group[0] and len_g1 >= min_len and (len_g1 <= max_len):
            phrases.append(tuple(tmp))
    return list(filter(lambda x: self.min_length <= len(x) <= self.max_length, phrases))","groups = groupby(word_list, lambda x: x not in self.to_ignore)
phrases = []
for group in groups:
    tmp = tuple(group[1])
    len_g1 = len(list(tmp))
    if group[0] and len_g1 >= min_len and (len_g1 <= max_len):
        phrases.append(tuple(tmp))","phrases = [tuple(g[1]) for (k, g) in groupby(word_list, lambda x: x not in self.to_ignore) if k and min_len <= len(g[1]) <= max_len]",find_wrong,-1
rtfm,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rtfm/rtfm.py,https://github.com/leostat/rtfm/tree/master//rtfm.py,,dbInsertCmd$386,"def dbInsertCmd(conn, cmds):
    cur = conn.cursor()
    cur.execute('SELECT max(cmdid) from tblcommand')
    max_id = cur.fetchall()
    if options.debug:
        for cmd in cmds:
            debug(""I: INSERT INTO tblcommand VALUES (NULL, '"" + str(cmd[0]) + ""', '"" + str(cmd[1]) + ""', '"" + str(cmd[2]) + ""', "" + ""date('now'))"")
    cur.executemany('INSERT INTO tblcommand VALUES (NULL, ?, ?, ?, date(""now""));', cmds)
    conn.commit()
    ok('Added Rows : ' + str(cur.rowcount))
    cur.execute('SELECT max(cmdid) FROM tblcommand')
    new_max_id = cur.fetchall()
    ok('New Top ID : ' + str(new_max_id[0][0]) + "" | Number of CMD's Added : "" + str(new_max_id[0][0] - max_id[0][0]))","cur.executemany('INSERT INTO tblcommand VALUES (NULL, ?, ?, ?, date(""now""));', cmds)","placeholders = ', '.join(['?' for _ in range(len(cmds[0]))])
query = f'INSERT INTO tblcommand VALUES (NULL, {placeholders}, date(""now""))'
cur.executemany(query, cmds)",find_wrong,2
superpaper,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/superpaper/superpaper/gui.py,https://github.com/hhannine/superpaper/tree/master/superpaper/gui.py,WallpaperPreviewPanel,export_offsets$1993,"def export_offsets(self, display_sys):
    """"""Read dragged preview positions, normalize them to be positive,
        and scale sizes up to old canvas size.""""""
    prev_canv_w = self.st_bmp_canvas.GetSize()[0]
    true_canv_w = self.get_canvas(display_sys.get_disp_list(True))[0]
    scaling = true_canv_w / prev_canv_w
    sanitzed_offs = self.sanitize_shape_offs()
    ppi_norm_offsets = []
    for off in sanitzed_offs:
        ppi_norm_offsets.append((off[0] * scaling, off[1] * scaling))
    display_sys.update_ppinorm_offsets(ppi_norm_offsets, bezels_included=False)","ppi_norm_offsets = []
for off in sanitzed_offs:
    ppi_norm_offsets.append((off[0] * scaling, off[1] * scaling))","ppi_norm_offsets = [(off[0] * scaling, off[1] * scaling) for off in sanitzed_offs]",find_wrong,-1
sentry,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/utils/snuba.py,https://github.com/getsentry/sentry/tree/master/src/sentry/utils/snuba.py,,get_query_params_to_update_for_projects$441,"def get_query_params_to_update_for_projects(query_params, with_org=False):
    """"""
    Get the project ID and query params that need to be updated for project
    based datasets, before we send the query to Snuba.
    """"""
    if 'project_id' in query_params.filter_keys:
        project_ids = list(set(query_params.filter_keys['project_id']))
    elif query_params.filter_keys:
        with timer('get_related_project_ids'):
            ids = [set(get_related_project_ids(k, query_params.filter_keys[k])) for k in query_params.filter_keys]
            project_ids = list(set.union(*ids))
    elif query_params.conditions:
        project_ids = []
        for cond in query_params.conditions:
            if cond[0] == 'project_id':
                project_ids = [cond[2]] if cond[1] == '=' else cond[2]
    else:
        project_ids = []
    if not project_ids:
        raise UnqualifiedQueryError('No project_id filter, or none could be inferred from other filters.')
    try:
        organization_id = Project.objects.get_from_cache(pk=project_ids[0]).organization_id
    except Project.DoesNotExist:
        project = Project.objects.filter(pk__in=project_ids).values('organization_id').first()
        if project is None:
            raise UnqualifiedQueryError('All project_ids from the filter no longer exist')
        organization_id = project.get('organization_id')
    params = {'project': project_ids}
    if with_org:
        params['organization'] = organization_id
    return (organization_id, params)","for k in query_params.filter_keys:
    ids = [set(get_related_project_ids(k, query_params.filter_keys[k])) for k in query_params.filter_keys]
    project_ids = list(set.union(*ids))","project_ids = []
for (k, v) in query_params.filter_keys.items():
    project_ids.extend(get_related_project_ids(k, v))
project_ids = list(set(project_ids))",find_wrong,2
python-ternary,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-ternary/examples/scatter_colorbar.py,https://github.com/marcharper/python-ternary/tree/master/examples/scatter_colorbar.py,,_energy_to_enthalpy$37,"def _energy_to_enthalpy(energy):
    """"""Converts energy to enthalpy.
    
    This function take the energies stored in the energy array and
    converts them to formation enthalpy.

    Parameters
    ---------
    energy : list of lists of floats
    
    Returns
    -------
    enthalpy : list of lists containing the enthalpies.
    """"""
    pureA = [energy[0][0], energy[0][1]]
    pureB = [energy[1][0], energy[1][1]]
    pureC = [energy[2][0], energy[2][1]]
    enthalpy = []
    for en in energy:
        c = en[2]
        conc = [float(i) / sum(c) for i in c]
        CE = _en_to_enth(en[0], conc, pureA[0], pureB[0], pureC[0])
        VASP = _en_to_enth(en[1], conc, pureA[1], pureB[1], pureC[1])
        enthalpy.append([CE, VASP, c])
    return enthalpy","pureA = [energy[0][0], energy[0][1]]
pureB = [energy[1][0], energy[1][1]]
pureC = [energy[2][0], energy[2][1]]
enthalpy = []
for en in energy:
    c = en[2]
    conc = [float(i) / sum(c) for i in c]
    CE = _en_to_enth(en[0], conc, pureA[0], pureB[0], pureC[0])
    VASP = _en_to_enth(en[1], conc, pureA[1], pureB[1], pureC[1])
    enthalpy.append([CE, VASP, c])","pure = [[energy[i][0], energy[i][1]] for i in range(3)]
enthalpy = []
for en in energy:
    c = en[2]
    conc = [float(i) / sum(c) for i in c]
    enthalpies = [_en_to_enth(en[i], conc, *pure[i]) for i in range(2)]
    enthalpy.append([*enthalpies, c])",find_wrong,-1
OpenFermion,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenFermion/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,https://github.com/quantumlib/OpenFermion/tree/master/src/openfermion/transforms/repconversions/qubit_tapering_from_stabilizer.py,,taper_off_qubits$404,"def taper_off_qubits(operator, stabilizers, manual_input=False, fixed_positions=None, output_tapered_positions=False):
    """"""
    Remove qubits from given operator.

    Qubits are removed by eliminating an equivalent number of
    stabilizer conditions. Which qubits that are can either be determined
    automatically or their positions can be set manually.

    Qubits can be disregarded from the Hamiltonian when the effect of all its
    terms on them is rendered trivial. This algorithm employs a stabilizers
    like $\\pm X \\otimes p$ to fix the action of every Pauli
    string on the first qubit to $Z$ or the identity. A string
    $X \\otimes h$ would for instance be multiplied with the stabilizer
    to obtain $1 \\otimes (\\pm h\\cdot p)$ while a string
    $Z \\otimes h^\\prime$ would pass without correction. The first
    qubit can subsequently be removed as it must be in the computational basis
    in Hamiltonian eigenstates.
    For stabilizers acting as $Y$ ($Z$) on selected qubits,
    the algorithm would fix the action of every Hamiltonian string to
    $Z$ ($X$). Updating also the list of remaining stabilizer
    generators, the algorithm is run iteratively.

    Args:
        operator (QubitOperator): Operator of which qubits will be removed.
        stabilizers (QubitOperator): Stabilizer generators for the tapering.
                                     Can also be passed as a list of
                                     QubitOperator.
        manual_input (Boolean): Option to pass the list of fixed qubits
                                positions manually. Set to False by default.
        fixed_positions (list): (optional) List of fixed qubit positions.
                                Passing a list is only effective if
                                manual_input is True.
        output_tapered_positions (Boolean): Option to output the positions of
                                            qubits that have been removed.
    Returns:
        skimmed_operator (QubitOperator): Operator with fewer qubits.
        removed_positions (list): (optional) List of removed qubit positions.
                                  For the qubits to be gone in the qubit count,
                                  the remaining qubits have been moved up to
                                  those indices.
    """"""
    if isinstance(stabilizers, (list, tuple, numpy.ndarray)):
        n_qbits_stabs = 0
        for ent in stabilizers:
            if op_utils.count_qubits(ent) > n_qbits_stabs:
                n_qbits_stabs = op_utils.count_qubits(ent)
    else:
        n_qbits_stabs = op_utils.count_qubits(stabilizers)
    n_qbits = max(op_utils.count_qubits(operator), n_qbits_stabs)
    (ham_to_update, qbts_to_rm) = reduce_number_of_terms(operator, stabilizers, maintain_length=False, manual_input=manual_input, fixed_positions=fixed_positions, output_fixed_positions=True)
    qbit_order = list(numpy.arange(n_qbits - len(qbts_to_rm), dtype=int))
    removed_positions = qbts_to_rm
    qbts_to_rm.sort()
    for x in qbts_to_rm:
        qbit_order.insert(x, 'remove')
    skimmed_operator = QubitOperator()
    for (term, coef) in ham_to_update.terms.items():
        if term == ():
            skimmed_operator += QubitOperator('', coef)
            continue
        tap_tpls = []
        for p in term:
            if qbit_order[p[0]] != 'remove':
                tap_tpls.append((qbit_order[p[0]].item(), p[1]))
        skimmed_operator += QubitOperator(tuple(tap_tpls), coef)
    if output_tapered_positions:
        return (skimmed_operator, removed_positions)
    else:
        return skimmed_operator","for x in qbts_to_rm:
    qbit_order.insert(x, 'remove')
skimmed_operator = QubitOperator()
for (term, coef) in ham_to_update.terms.items():
    if term == ():
        skimmed_operator += QubitOperator('', coef)
        continue
    tap_tpls = []
    for p in term:
        if qbit_order[p[0]] != 'remove':
            tap_tpls.append((qbit_order[p[0]].item(), p[1]))
    skimmed_operator += QubitOperator(tuple(tap_tpls), coef)","qbit_order = ['remove' if x in qbts_to_rm else x for x in range(n_qbits)]
skimmed_operator = QubitOperator()
for (term, coef) in ham_to_update.terms.items():
    if term == ():
        skimmed_operator += QubitOperator('', coef)
        continue
    tap_tpls = []
    for p in term:
        if qbit_order[p[0]] != 'remove':
            tap_tpls.append((qbit_order[p[0]].item(), p[1]))
    skimmed_operator += QubitOperator(tuple(tap_tpls), coef)",find_wrong,-1
Vulmap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Vulmap/Vulmap-Linux/vulmap-linux.py,https://github.com/vulmon/Vulmap/tree/master/Vulmap-Linux/vulmap-linux.py,,ReadFromFile$207,"def ReadFromFile(InventoryOutFile):
    count = 0
    print('Reading software inventory from ' + InventoryOutFile)
    with open(InventoryOutFile) as json_file:
        products = json.load(json_file)
    for a in products:
        if count == 0:
            queryData = '['
        queryData += '{'
        queryData += '""product"": ""' + a[0] + '"",'
        queryData += '""version"": ""' + a[1] + '"",'
        queryData += '""arc"": ""' + a[2] + '""'
        queryData += '},'
        count += 1
        if count == 100:
            count = 0
            outResults(queryData)
    outResults(queryData)","for a in products:
    if count == 0:
        queryData = '['
    queryData += '{'
    queryData += '""product"": ""' + a[0] + '"",'
    queryData += '""version"": ""' + a[1] + '"",'
    queryData += '""arc"": ""' + a[2] + '""'
    queryData += '},'
    count += 1
    if count == 100:
        count = 0
        outResults(queryData)
outResults(queryData)","queryData = '['
for (i, a) in enumerate(products):
    queryData += '{'
    queryData += f'""product"": ""{a[0]}"",'
    queryData += f'""version"": ""{a[1]}"",'
    queryData += f'""arc"": ""{a[2]}""'
    queryData += '},'
    if (i + 1) % 100 == 0:
        outResults(queryData)
        queryData = ''
if queryData:
    outResults(queryData)",find_wrong,-1
LinOTP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LinOTP/linotp/lib/tokeniterator.py,https://github.com/LinOTP/LinOTP/tree/master/linotp/lib/tokeniterator.py,TokenIterator,_get_tokens_in_realm$359,"def _get_tokens_in_realm(self, valid_realms):
    realm_id_tuples = db.session.query(Realm.id).filter(Realm.name.in_(valid_realms)).all()
    realm_ids = set()
    for realm_tuple in realm_id_tuples:
        realm_ids.add(realm_tuple[0])
    token_id_tuples = db.session.query(TokenRealm.token_id).filter(TokenRealm.realm_id.in_(realm_ids)).all()
    token_ids = set()
    for token_tuple in token_id_tuples:
        token_ids.add(token_tuple[0])
    return token_ids","realm_id_tuples = db.session.query(Realm.id).filter(Realm.name.in_(valid_realms)).all()
realm_ids = set()
for realm_tuple in realm_id_tuples:
    realm_ids.add(realm_tuple[0])
token_id_tuples = db.session.query(TokenRealm.token_id).filter(TokenRealm.realm_id.in_(realm_ids)).all()
token_ids = set()
for token_tuple in token_id_tuples:
    token_ids.add(token_tuple[0])","realm_ids = {realm[0] for realm in db.session.query(Realm.id).filter(Realm.name.in_(valid_realms)).all()}
token_ids = {token[0] for token in db.session.query(TokenRealm.token_id).filter(TokenRealm.realm_id.in_(realm_ids)).all()}",find_wrong,-1
jieba,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jieba/test/jieba_test.py,https://github.com/fxsjy/jieba/tree/master/test/jieba_test.py,JiebaTestCase,testTokenize_NOHMM$185,"def testTokenize_NOHMM(self):
    for content in test_contents:
        result = jieba.tokenize(content, HMM=False)
        assert isinstance(result, types.GeneratorType), 'Test Tokenize Generator error'
        result = list(result)
        assert isinstance(result, list), 'Test Tokenize error on content: %s' % content
        for tk in result:
            print('word %s\t\t start: %d \t\t end:%d' % (tk[0], tk[1], tk[2]), file=sys.stderr)
    print('testTokenize_NOHMM', file=sys.stderr)","for content in test_contents:
    result = jieba.tokenize(content, HMM=False)
    assert isinstance(result, types.GeneratorType), 'Test Tokenize Generator error'
    result = list(result)
    assert isinstance(result, list), 'Test Tokenize error on content: %s' % content
    for tk in result:
        print('word %s\t\t start: %d \t\t end:%d' % (tk[0], tk[1], tk[2]), file=sys.stderr)","result = [tk for content in test_contents for tk in jieba.tokenize(content, HMM=False)]
assert isinstance(result, list), 'Test Tokenize error on content: %s' % content
for tk in result:
    print('word %s\t\t start: %d \t\t end:%d' % (tk[0], tk[1], tk[2]), file=sys.stderr)",find_wrong,-1
Packer-Fuzzer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Packer-Fuzzer/lib/FuzzParam.py,https://github.com/rtcatc/Packer-Fuzzer/tree/master/lib/FuzzParam.py,FuzzerParam,result_method_1$355,"def result_method_1(self, str):
    result_key_list = []
    result_value_list = []
    regxs_1 = 'method\\:.*?\\,url\\:.*?\\,data\\:({.*?})'
    if re.findall(regxs_1, str, re.S):
        result_json = re.findall(regxs_1, str, re.S)[0].replace(' ', '').replace('\n', '').replace('{', '').replace('\t', '')
        regx_key = '(.*?)\\:.*?\\,|(.*?)\\:.*?\\}'
        result_keys = re.findall(regx_key, result_json, re.S)
        for result_key in result_keys:
            if result_key[0] != '':
                result_key_list.append(result_key[0])
            if result_key[1] != '':
                result_key_list.append(result_key[1])
        regx_value = '\\:(.*?)\\,|\\:(.*?)\\}'
        result_values = re.findall(regx_value, result_json, re.S)
        for para in result_values:
            if para[0] != '':
                result_value_list.append(para[0])
            if para[1] != '':
                result_value_list.append(para[1])
    result_list = [result_key_list, result_value_list]
    return result_list","result_key_list = []
result_value_list = []
regxs_1 = 'method\\:.*?\\,url\\:.*?\\,data\\:({.*?})'
if re.findall(regxs_1, str, re.S):
    result_json = re.findall(regxs_1, str, re.S)[0].replace(' ', '').replace('\n', '').replace('{', '').replace('\t', '')
    regx_key = '(.*?)\\:.*?\\,|(.*?)\\:.*?\\}'
    result_keys = re.findall(regx_key, result_json, re.S)
    for result_key in result_keys:
        if result_key[0] != '':
            result_key_list.append(result_key[0])
        if result_key[1] != '':
            result_key_list.append(result_key[1])
    regx_value = '\\:(.*?)\\,|\\:(.*?)\\}'
    result_values = re.findall(regx_value, result_json, re.S)
    for para in result_values:
        if para[0] != '':
            result_value_list.append(para[0])
        if para[1] != '':
            result_value_list.append(para[1])
result_list = [result_key_list, result_value_list]","result_key_list = []
result_value_list = []
regxs_1 = 'method\\:.*?\\,url\\:.*?\\,data\\:({.*?})'
if re.findall(regxs_1, str, re.S):
    result_json = re.findall(regxs_1, str, re.S)[0].replace(' ', '').replace('\n', '').replace('{', '').replace('\t', '')
    result_list = [[], []]
    for (key, value) in json.loads(result_json).items():
        result_list[0].append(key)
        result_list[1].append(value)",find_wrong,-1
flask-profiler,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flask-profiler/flask_profiler/storage/sqlite.py,https://github.com/muatik/flask-profiler/tree/master/flask_profiler/storage/sqlite.py,Sqlite,getSummary$281,"def getSummary(self, kwds={}):
    filters = Sqlite.getFilters(kwds)
    conditions = 'WHERE 1=1 and '
    if filters['startedAt']:
        conditions = conditions + 'startedAt>={0} AND '.format(filters['startedAt'])
    if filters['endedAt']:
        conditions = conditions + 'endedAt<={0} AND '.format(filters['endedAt'])
    if filters['elapsed']:
        conditions = conditions + 'elapsed>={0} AND'.format(filters['elapsed'])
    conditions = conditions.rstrip(' AND')
    with self.lock:
        sql = 'SELECT\n                    method, name,\n                    count(id) as count,\n                    min(elapsed) as minElapsed,\n                    max(elapsed) as maxElapsed,\n                    avg(elapsed) as avgElapsed\n                FROM ""{table_name}"" {conditions}\n                group by method, name\n                order by {sort_field} {sort_direction}\n                '.format(table_name=self.table_name, conditions=conditions, sort_field=filters['sort'][0], sort_direction=filters['sort'][1])
        self.cursor.execute(sql)
        rows = self.cursor.fetchall()
    result = []
    for r in rows:
        result.append({'method': r[0], 'name': r[1], 'count': r[2], 'minElapsed': r[3], 'maxElapsed': r[4], 'avgElapsed': r[5]})
    return result","result = []
for r in rows:
    result.append({'method': r[0], 'name': r[1], 'count': r[2], 'minElapsed': r[3], 'maxElapsed': r[4], 'avgElapsed': r[5]})
return result","return [{'method': r[0], 'name': r[1], 'count': r[2], 'minElapsed': r[3], 'maxElapsed': r[4], 'avgElapsed': r[5]} for r in rows]",find_wrong,-1
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/cfg/cfg_emulated.py,https://github.com/angr/angr/tree/master/angr/analyses/cfg/cfg_emulated.py,CFGEmulated,_is_address_executable$3365,"def _is_address_executable(self, address):
    """"""
        Check if the specific address is in one of the executable ranges.

        :param int address: The address
        :return: True if it's in an executable range, False otherwise
        """"""
    for r in self._executable_address_ranges:
        if r[0] <= address < r[1]:
            return True
    return False","for r in self._executable_address_ranges:
    if r[0] <= address < r[1]:
        return True
return False",return any((r[0] <= address < r[1] for r in self._executable_address_ranges)),find_wrong,-1
quodlibet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/quodlibet/quodlibet/ext/songsmenu/duplicates.py,https://github.com/quodlibet/quodlibet/tree/master/quodlibet/ext/songsmenu/duplicates.py,DuplicateSongsView,get_selected_songs$36,"def get_selected_songs(self):
    selection = self.get_selection()
    if selection is None:
        return []
    (model, rows) = selection.get_selected_rows()
    if not rows:
        return []
    selected = []
    for row in rows:
        row = model[row]
        if row.parent is None:
            for child in row.iterchildren():
                selected.append(child[0])
        else:
            selected.append(row[0])
    return selected","selected = []
for row in rows:
    row = model[row]
    if row.parent is None:
        for child in row.iterchildren():
            selected.append(child[0])
    else:
        selected.append(row[0])",selected = [child[0] for row in rows for child in ([row.iterchildren()] if row.parent is None else [row])],find_wrong,-1
tartube,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tartube/tartube/config.py,https://github.com/axcore/tartube/tree/master/tartube/config.py,SystemPrefWin,setup_downloader_paths_tab$25199,"def setup_downloader_paths_tab(self, inner_notebook):
    """"""Called by self.setup_downloader_tab().

        Sets up the 'File Paths' inner notebook tab.

        Args:

            inner_notebook (Gtk.Notebook): The container for this tab

        """"""
    (tab, grid) = self.add_inner_notebook_tab(_('File _paths'), inner_notebook)
    grid_width = 3
    self.add_label(grid, '<u>' + _('Downloader file paths') + '</u>', 0, 0, grid_width, 1)
    self.add_label(grid, _('Path to the executable'), 0, 1, 1, 1)
    combo_list = [[_('Use default path') + ' (' + self.app_obj.ytdl_path_default + ')', self.app_obj.ytdl_path_default]]
    if os.name != 'nt':
        combo_list.append([_('Use local path') + ' (' + self.app_obj.ytdl_bin + ')', self.app_obj.ytdl_bin])
    if os.name == 'nt':
        msg = _('Use custom path (not recommended on MS Windows)')
    else:
        msg = _('Use custom path')
    combo_list.append([msg, None])
    if os.name != 'nt':
        combo_list.append([_('Use PyPI path') + ' (' + self.app_obj.ytdl_path_pypi + ')', self.app_obj.ytdl_path_pypi])
    self.path_liststore = Gtk.ListStore(str, str)
    for mini_list in combo_list:
        self.path_liststore.append([mini_list[0], mini_list[1]])
    self.filepaths_combo = Gtk.ComboBox.new_with_model(self.path_liststore)
    grid.attach(self.filepaths_combo, 1, 1, grid_width - 1, 1)
    renderer_text = Gtk.CellRendererText()
    self.filepaths_combo.pack_start(renderer_text, True)
    self.filepaths_combo.add_attribute(renderer_text, 'text', 0)
    self.filepaths_combo.set_entry_text_column(0)
    entry = self.add_entry(grid, None, False, 1, 2, 1, 1)
    button = Gtk.Button(_('Set'))
    grid.attach(button, 2, 2, 1, 1)
    if os.name == 'nt':
        if self.app_obj.ytdl_path_custom_flag:
            self.filepaths_combo.set_active(1)
        else:
            self.filepaths_combo.set_active(0)
    elif self.app_obj.ytdl_path_custom_flag:
        self.filepaths_combo.set_active(2)
    elif self.app_obj.ytdl_path == self.app_obj.ytdl_path_default:
        self.filepaths_combo.set_active(0)
    elif self.app_obj.ytdl_path == self.app_obj.ytdl_path_pypi:
        self.filepaths_combo.set_active(3)
    else:
        self.filepaths_combo.set_active(1)
    if self.app_obj.ytdl_path_custom_flag:
        if self.app_obj.ytdl_path:
            entry.set_text(self.app_obj.ytdl_path)
    else:
        button.set_sensitive(False)
    self.add_label(grid, _('Command for update operations'), 0, 3, 1, 1)
    self.cmd_liststore = Gtk.ListStore(str, str)
    for item in self.app_obj.ytdl_update_list:
        self.cmd_liststore.append([item, formats.YTDL_UPDATE_DICT[item]])
    combo2 = Gtk.ComboBox.new_with_model(self.cmd_liststore)
    grid.attach(combo2, 1, 3, grid_width - 1, 1)
    renderer_text = Gtk.CellRendererText()
    combo2.pack_start(renderer_text, True)
    combo2.add_attribute(renderer_text, 'text', 1)
    combo2.set_entry_text_column(1)
    combo2.set_active(self.app_obj.ytdl_update_list.index(self.app_obj.ytdl_update_current))
    if __main__.__pkg_strict_install_flag__:
        combo2.set_sensitive(False)
    self.update_ytdl_combos()
    self.filepaths_combo.connect('changed', self.on_ytdl_path_combo_changed, entry, button)
    button.connect('clicked', self.on_ytdl_path_button_clicked, entry)
    combo2.connect('changed', self.on_update_combo_changed)","self.path_liststore = Gtk.ListStore(str, str)
for mini_list in combo_list:
    self.path_liststore.append([mini_list[0], mini_list[1]])
self.filepaths_combo = Gtk.ComboBox.new_with_model(self.path_liststore)
grid.attach(self.filepaths_combo, 1, 1, grid_width - 1, 1)
renderer_text = Gtk.CellRendererText()
self.filepaths_combo.pack_start(renderer_text, True)
self.filepaths_combo.add_attribute(renderer_text, 'text', 0)
self.filepaths_combo.set_entry_text_column(0)","self.filepaths_combo = Gtk.ComboBoxText()
for mini_list in combo_list:
    self.filepaths_combo.append_text(mini_list[0])
    self.filepaths_combo.set_active(0)
grid.attach(self.filepaths_combo, 1, 1, grid_width - 1, 1)",find_wrong,-1
core,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/homeassistant/components/danfoss_air/binary_sensor.py,https://github.com/home-assistant/core/tree/master/homeassistant/components/danfoss_air/binary_sensor.py,,setup_platform$12,"def setup_platform(hass, config, add_entities, discovery_info=None):
    """"""Set up the available Danfoss Air sensors etc.""""""
    data = hass.data[DANFOSS_AIR_DOMAIN]
    sensors = [['Danfoss Air Bypass Active', ReadCommand.bypass, DEVICE_CLASS_OPENING], ['Danfoss Air Away Mode Active', ReadCommand.away_mode, None]]
    dev = []
    for sensor in sensors:
        dev.append(DanfossAirBinarySensor(data, sensor[0], sensor[1], sensor[2]))
    add_entities(dev, True)","sensors = [['Danfoss Air Bypass Active', ReadCommand.bypass, DEVICE_CLASS_OPENING], ['Danfoss Air Away Mode Active', ReadCommand.away_mode, None]]
dev = []
for sensor in sensors:
    dev.append(DanfossAirBinarySensor(data, sensor[0], sensor[1], sensor[2]))
add_entities(dev, True)","sensor_names = ['Danfoss Air Bypass Active', 'Danfoss Air Away Mode Active']
sensor_commands = [ReadCommand.bypass, ReadCommand.away_mode]
sensor_classes = [DEVICE_CLASS_OPENING, None]
dev = [DanfossAirBinarySensor(data, name, command, class_) for (name, command, class_) in zip(sensor_names, sensor_commands, sensor_classes)]
add_entities(dev, True)",find_wrong,-1
espnet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espnet/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,https://github.com/espnet/espnet/tree/master/egs/yoloxochitl_mixtec/asr1/local/data_prep.py,,TraverseData$276,"def TraverseData(sound_dir, annotation_dir, target_dir, mode, speaker_info, new_data_dir, speaker_details, text_format):
    if not os.path.exists(target_dir):
        os.makedirs(target_dir)
    segments = open(os.path.join(target_dir, 'segments'), 'w', encoding='utf-8')
    wavscp = open(os.path.join(target_dir, 'wav.scp'), 'w', encoding='utf-8')
    utt2spk = open(os.path.join(target_dir, 'utt2spk'), 'w', encoding='utf-8')
    spk2utt = open(os.path.join(target_dir, 'spk2utt'), 'w', encoding='utf-8')
    text = open(os.path.join(target_dir, 'text'), 'w', encoding='utf-8')
    name2spk = open(os.path.join(target_dir, 'name2spk'), 'w', encoding='utf-8')
    remix_script = open(os.path.join(target_dir, 'remix_script.sh'), 'w', encoding='utf-8')
    sound_files = {}
    annotation_files = {}
    spk_id = 1
    spk2utt_prep = {}
    name2spk_prep = {}
    if mode == 'trs':
        if not os.path.exists(os.path.join(target_dir, 'temp')):
            os.mkdir(os.path.join(target_dir, 'temp'))
        audio_set = set()
        for (root, dirs, files) in os.walk(sound_dir):
            for file in files:
                if file[-4:] == '.wav':
                    sound_files[ExtractAudioID(file)] = os.path.join(root, file)
        for (root, dirs, files) in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == '.trs':
                    XMLRefine(os.path.join(root, file), os.path.join(target_dir, 'temp', file))
                    annotation_files[file] = os.path.join(target_dir, 'temp', file)
        for afile in annotation_files.keys():
            if afile == 'error':
                continue
            try:
                (audio_name, speakers, segment_info) = XMLProcessing(annotation_files[afile])
            except Exception:
                print('error process %s' % annotation_files[afile])
            audio_name = audio_name.replace(' ', '')
            audio_name = ExtractAudioID(audio_name)
            if audio_name in audio_set:
                continue
            audio_set.add(audio_name)
            if '%s.wav' % audio_name not in sound_files.keys():
                print('no audio found for annotation: %s' % afile)
                continue
            print('%s sox -t wavpcm %s -c 1 -r 16000 -t wavpcm - |' % (audio_name, sound_files['%s.wav' % audio_name]), file=wavscp)
            segment_number = 1
            temp_speaker_id = {}
            for speaker in speakers.keys():
                name2spk_prep[speakers[speaker]['name']] = name2spk_prep.get(speakers[speaker]['name'], spk_id)
                temp_speaker_id[speaker] = name2spk_prep[speakers[speaker]['name']]
                if name2spk_prep[speakers[speaker]['name']] == spk_id:
                    print('%s %s' % (speakers[speaker]['name'], PackZero(spk_id)), file=name2spk)
                    spk_id += 1
            for segment in segment_info:
                if segment[0] == 'None':
                    spk = spk_id
                    spk_id += 1
                else:
                    spk = temp_speaker_id[segment[0]]
                segment_id = '%s_%s_%s' % (PackZero(spk), audio_name, PackZero(segment_number))
                skip = False
                for seg in segment:
                    if len(seg) < 1:
                        print('warning segment %s in %s' % (segment_id, audio_name))
                        skip = True
                if skip:
                    continue
                print('%s %s %s %s' % (segment_id, audio_name, segment[2], segment[3]), file=segments)
                print('%s %s' % (segment_id, PackZero(spk)), file=utt2spk)
                print('%s %s' % (segment_id, segment[1]), file=text)
                spk2utt_prep[spk] = spk2utt_prep.get(spk, '') + ' %s' % segment_id
                segment_number += 1
            for spk in spk2utt_prep.keys():
                print('%s %s' % (spk, spk2utt_prep[spk]), file=spk2utt)
            print('successfully processing %s' % afile)
        shutil.rmtree(os.path.join(target_dir, 'temp'))
    else:
        wav_spk_info = LoadWavSpeakerInfo(speaker_info)
        spk_details = LoadSpeakerDetails(speaker_details)
        for (root, dirs, files) in os.walk(sound_dir):
            for file in files:
                if file[-4:] == '.wav':
                    sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)
        for (root, dirs, files) in os.walk(annotation_dir):
            for file in files:
                if file[-4:] == '.eaf':
                    annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)
        for afile in annotation_files.keys():
            afile_path = annotation_files[afile]
            if afile == 'error':
                continue
            spk_info = wav_spk_info[afile]
            segment_info = ELANProcess(afile_path, spk_info, spk_details, text_format)
            if segment_info is None:
                continue
            (left_channel_segments, right_channel_segments) = segment_info
            f = sf.SoundFile(sound_files[afile])
            max_length = len(f) / f.samplerate
            print('sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-L.wav remix 1' % (sound_files[afile], os.path.join(new_data_dir, afile)), file=remix_script)
            print('%s-L %s-L.wav' % (afile, os.path.join(new_data_dir, afile)), file=wavscp)
            segment_number = 0
            for segment in left_channel_segments:
                segment_id = '%s_%s-L_%s' % (spk_info[0], afile, PackZero(segment_number))
                if float(segment[1]) > max_length:
                    continue
                print('%s %s-L %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
                print('%s %s' % (segment_id, spk_info[0]), file=utt2spk)
                print('%s %s' % (segment_id, segment[2]), file=text)
                spk2utt_prep[spk_info[0]] = spk2utt_prep.get(spk_info[0], '') + ' %s' % segment_id
                segment_number += 1
            if len(right_channel_segments) > 0:
                print('sox -t wavpcm ""%s"" -c 1 -r 16000 -t wavpcm %s-R.wav remix 2' % (sound_files[afile], os.path.join(new_data_dir, afile)), file=remix_script)
                print('%s-R %s-R.wav' % (afile, os.path.join(new_data_dir, afile)), file=wavscp)
                for segment in right_channel_segments:
                    segment_id = '%s_%s-R_%s' % (spk_info[1], afile, PackZero(segment_number))
                    if float(segment[1]) > max_length:
                        continue
                    print('%s %s-R %s %s' % (segment_id, afile, segment[0], segment[1]), file=segments)
                    print('%s %s' % (segment_id, spk_info[1]), file=utt2spk)
                    print('%s %s' % (segment_id, segment[2]), file=text)
                    spk2utt_prep[spk_info[1]] = spk2utt_prep.get(spk_info[1], '') + ' %s' % segment_id
                    segment_number += 1
            print('successfully processing %s' % afile)
        for spk in spk2utt_prep.keys():
            print('%s %s' % (spk, spk2utt_prep[spk]), file=spk2utt)
    segments.close()
    wavscp.close()
    utt2spk.close()
    spk2utt.close()
    text.close()","for (root, dirs, files) in os.walk(sound_dir):
    for file in files:
        if file[-4:] == '.wav':
            sound_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)
for (root, dirs, files) in os.walk(annotation_dir):
    for file in files:
        if file[-4:] == '.eaf':
            annotation_files[ExtractAudioID(file, wav_spk_info)] = os.path.join(root, file)","sound_files = {ExtractAudioID(file, wav_spk_info): os.path.join(root, file) for (root, dirs, files) in os.walk(sound_dir) for file in files if file.endswith('.wav')}
annotation_files = {ExtractAudioID(file, wav_spk_info): os.path.join(root, file) for (root, dirs, files) in os.walk(annotation_dir) for file in files if file.endswith('.eaf')}",find_wrong,-1
PyQt5-Apps,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyQt5-Apps/words-recorder/main.py,https://github.com/taseikyo/PyQt5-Apps/tree/master/words-recorder/main.py,MainWindow,query$155,"def query(self, w):
    origin = w.input_origin.text().replace(' ', '')
    if origin:
        if db:
            try:
                sql = ""SELECT origin, trans FROM words WHERE origin = '%s'"" % origin
                print(sql)
                num = cursor.execute(sql)
                if num:
                    for r in cursor:
                        w.input_trans.setText(r[1])
            except Exception as e:
                self.messageBox('insert data failed!\nerror msg: %s' % e.args[1])
        else:
            self.messageBox(""connect to the database first!\nclick the button 'File-connect'"")","sql = ""SELECT origin, trans FROM words WHERE origin = '%s'"" % origin
print(sql)
num = cursor.execute(sql)
if num:
    for r in cursor:
        w.input_trans.setText(r[1])","sql = 'SELECT origin, trans FROM words WHERE origin = %s'
print(sql)
num = cursor.execute(sql, (origin,))
if num:
    for r in cursor:
        w.input_trans.setText(r[1])",find_wrong,-1
capirca,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capirca/capirca/lib/arista_tp.py,https://github.com/google/capirca/tree/master/capirca/lib/arista_tp.py,Term,__str__$160,"def __str__(self):
    if self.term.platform and self._PLATFORM not in self.term.platform:
        return ''
    if self.term.platform_exclude and self._PLATFORM in self.term.platform_exclude:
        return ''
    config = Config()
    term_block = []
    if self.term_type == 'inet6' and 'icmp' in self.term.protocol or (self.term_type == 'inet' and 'icmpv6' in self.term.protocol):
        logging.debug(self.NO_AF_LOG_PROTO.substitute(term=self.term.name, proto=', '.join(self.term.protocol), af=self.term_type))
        return ''
    if self.term.verbatim:
        for line in self.term.verbatim:
            if line[0] == self._PLATFORM:
                term_block.append([MATCH_INDENT, str(line[1]), True])
        for (i, s, v) in term_block:
            config.Append(i, s, verbatim=v)
        return str(config)
    flags = []
    misc_options = []
    if self.term.option:
        (flags, misc_options) = self._processTermOptions(self.term, self.term.option)
    family_keywords = self._TERM_TYPE.get(self.term_type)
    term_block.append([TERM_INDENT, 'match %s %s' % (self.term.name, family_keywords['addr_fam']), False])
    term_af = self.AF_MAP.get(self.term_type)
    if self.term.owner and (not self.noverbose):
        self.term.comment.append('owner: %s' % self.term.owner)
    if self.term.comment and (not self.noverbose):
        reflowed_comments = self._reflowComments(self.term.comment, MAX_COMMENT_LENGTH)
        for line in reflowed_comments:
            term_block.append([MATCH_INDENT, '!! ' + line, False])
    has_match_criteria = self.term.destination_address or self.term.destination_address_exclude or self.term.destination_port or self.term.destination_prefix or self.term.fragment_offset or self.term.hop_limit or self.term.port or self.term.protocol or self.term.protocol_except or self.term.source_address or self.term.source_address_exclude or self.term.source_port or self.term.source_prefix or self.term.ttl
    is_default_term = re.match('^ipv(4|6)\\-default\\-.*', self.term.name, re.IGNORECASE)
    if not has_match_criteria and (not is_default_term):
        logging.warning('WARNING: term %s has no valid match criteria and will not be rendered.', self.term.name)
        return ''
    else:
        src_addr = self.term.GetAddressOfVersion('source_address', term_af)
        src_addr_ex = self.term.GetAddressOfVersion('source_address_exclude', term_af)
        if src_addr:
            src_str = 'source prefix'
            if src_addr_ex:
                src_str += ' field-set src-%s' % self.term.name
            else:
                for addr in src_addr:
                    src_str += ' %s' % addr
            term_block.append([MATCH_INDENT, src_str, False])
        elif self.term.source_address:
            logging.debug(self.NO_AF_LOG_ADDR.substitute(term=self.term.name, direction='source', af=self.term_type))
            return ''
        dst_addr = self.term.GetAddressOfVersion('destination_address', term_af)
        dst_addr_ex = self.term.GetAddressOfVersion('destination_address_exclude', term_af)
        if dst_addr:
            dst_str = 'destination prefix'
            if dst_addr_ex:
                dst_str += ' field-set dst-%s' % self.term.name
            else:
                for addr in dst_addr:
                    dst_str += ' %s' % addr
            term_block.append([MATCH_INDENT, dst_str, False])
        elif self.term.destination_address:
            logging.debug(self.NO_AF_LOG_ADDR.substitute(term=self.term.name, direction='destination', af=self.term_type))
            return ''
        if self.term.source_prefix:
            src_pfx_str = 'source prefix field-set'
            for pfx in self.term.source_prefix:
                src_pfx_str += ' %s' % pfx
            term_block.append([MATCH_INDENT, ' %s' % src_pfx_str, False])
        if self.term.destination_prefix:
            dst_pfx_str = 'destination prefix field-set'
            for pfx in self.term.destination_prefix:
                dst_pfx_str += ' %s' % pfx
            term_block.append([MATCH_INDENT, ' %s' % dst_pfx_str, False])
        protocol_str = ''
        if self.term.protocol:
            protocol_str = self._processProtocol(self.term_type, self.term, flags)
        if self.term.protocol_except:
            protocol_str = self._processProtocolExcept(self.term_type, self.term, flags)
        port_str = self._processPorts(self.term)
        if port_str:
            protocol_str += port_str
        icmp_type_str = ''
        icmp_code_str = ''
        if self.term.protocol == ['icmp'] or self.term.protocol == ['icmpv6']:
            (icmp_type_str, icmp_code_str) = self._processICMP(self.term)
        if self.term.icmp_type:
            protocol_str += icmp_type_str
        if self.term.icmp_code:
            protocol_str += icmp_code_str
        if protocol_str:
            term_block.append([MATCH_INDENT, protocol_str, False])
        if self.term.packet_length:
            term_block.append([MATCH_INDENT, 'ip length %s' % self.term.packet_length, False])
        if self.term.fragment_offset:
            term_block.append([MATCH_INDENT, 'fragment offset %s' % self.term.fragment_offset, False])
        if self.term.hop_limit:
            term_block.append([MATCH_INDENT, 'ttl %s' % self.term.hop_limit, False])
        if self.term.ttl:
            term_block.append([MATCH_INDENT, 'ttl %s' % self.term.ttl, False])
        if misc_options:
            for mopt in misc_options:
                term_block.append([MATCH_INDENT, mopt, False])
    current_action = self._ACTIONS.get(self.term.action[0])
    has_extra_actions = self.term.logging or self.term.counter or self.term.dscp_set
    if self.term.action != ['accept']:
        term_block.append([MATCH_INDENT, 'actions', False])
        term_block.append([ACTION_INDENT, '%s' % current_action, False])
    elif self.term.action == ['accept'] and has_extra_actions:
        term_block.append([MATCH_INDENT, 'actions', False])
    if has_extra_actions:
        if self.term.logging and self.term.action != ['accept']:
            term_block.append([ACTION_INDENT, 'log', False])
        elif self.term.logging and self.term.action == ['accept']:
            logging.warning('WARNING: term %s uses logging option but is not a deny action. logging will not be added.', self.term.name)
        if self.term.counter:
            term_block.append([ACTION_INDENT, 'count %s' % self.term.counter, False])
        term_block.append([MATCH_INDENT, '!', False])
    term_block.append([TERM_INDENT, '!', False])
    for (tindent, tstr, tverb) in term_block:
        config.Append(tindent, tstr, verbatim=tverb)
    return str(config)","for addr in src_addr:
    src_str += ' %s' % addr
term_block.append([MATCH_INDENT, src_str, False])
for addr in dst_addr:
    dst_str += ' %s' % addr
term_block.append([MATCH_INDENT, dst_str, False])
for pfx in self.term.source_prefix:
    src_pfx_str += ' %s' % pfx
term_block.append([MATCH_INDENT, ' %s' % src_pfx_str, False])
for pfx in self.term.destination_prefix:
    dst_pfx_str += ' %s' % pfx
term_block.append([MATCH_INDENT, ' %s' % dst_pfx_str, False])","src_str = ""source prefix""
if src_addr:
    src_str += "" "" + "" "".join(src_addr)
elif self.term.source_address:
    logging.debug(
        self.NO_AF_LOG_ADDR.substitute(
            term=self.term.name, direction=""source"", af=self.term_type))
    return """"

if src_addr_ex:
    src_str += "" field-set src-%s"" % self.term.name

term_block.append([MATCH_INDENT, src_str, False])

dst_str = ""destination prefix""
if dst_addr:
    dst_str += "" "" + "" "".join(dst_addr)
elif self.term.destination_address:
    logging.debug(
        self.NO_AF_LOG_ADDR.substitute(
            term=self.term.name, direction=""destination"",
            af=self.term_type))
    return """"

if dst_addr_ex:
    dst_str += "" field-set dst-%s"" % self.term.name

term_block.append([MATCH_INDENT, dst_str, False])

if self.term.source_prefix:
    src_pfx_str = ""source prefix field-set""
    src_pfx_str += "" "" + "" "".join(self.term.source_prefix)

    term_block.append([MATCH_INDENT, """,find_wrong,2
mysql-connector-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mysql-connector-python/examples/dates.py,https://github.com/mysql/mysql-connector-python/tree/master/examples/dates.py,,main$56,"def main(config):
    output = []
    db = mysql.connector.Connect(**config)
    cursor = db.cursor()
    tbl = 'myconnpy_dates'
    cursor.execute('SET sql_mode = """"')
    stmt_drop = 'DROP TABLE IF EXISTS {0}'.format(tbl)
    cursor.execute(stmt_drop)
    stmt_create = 'CREATE TABLE {0} (   `id` tinyint(4) NOT NULL AUTO_INCREMENT,   `c1` date DEFAULT NULL,   `c2` datetime NOT NULL,   `c3` time DEFAULT NULL,   `changed` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP     ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`))'.format(tbl)
    cursor.execute(stmt_create)
    stmt_insert = 'INSERT INTO {0} (c1,c2,c3) VALUES (%s,%s,%s)'.format(tbl)
    for data in DATA:
        try:
            cursor.execute(stmt_insert, data)
        except (mysql.connector.errors.Error, TypeError) as exc:
            output.append('Failed inserting {0}\nError: {1}\n'.format(data, exc))
            cursor.execute(stmt_drop)
            raise
    stmt_select = 'SELECT * FROM {0} ORDER BY id'.format(tbl)
    cursor.execute(stmt_select)
    for row in cursor.fetchall():
        output.append('%3s | %10s | %19s | %8s |' % (row[0], row[1], row[2], row[3]))
    cursor.execute(stmt_drop)
    cursor.close()
    db.close()
    return output","stmt_insert = 'INSERT INTO {0} (c1,c2,c3) VALUES (%s,%s,%s)'.format(tbl)
for data in DATA:
    try:
        cursor.execute(stmt_insert, data)
    except (mysql.connector.errors.Error, TypeError) as exc:
        output.append('Failed inserting {0}\nError: {1}\n'.format(data, exc))
        cursor.execute(stmt_drop)
        raise","stmt_insert = 'INSERT INTO {0} (c1,c2,c3) VALUES (%s,%s,%s)'.format(tbl)
for data in DATA:
    try:
        cursor.execute(stmt_insert, tuple(data))
    except (mysql.connector.errors.Error, TypeError) as exc:
        output.append('Failed inserting {0}\nError: {1}\n'.format(data, exc))
        cursor.execute(stmt_drop)
        raise",find_wrong,2
coa_tools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/operators/edit_mesh.py,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/operators/edit_mesh.py,,average_edge_cuts$91,"def average_edge_cuts(bm, obj, cuts=1):
    (edges_len_average, shortest_edge) = get_average_edge_length(bm, obj)
    subdivide_edges = []
    for edge in bm.edges:
        cut_count = int(edge.calc_length() / shortest_edge) * cuts
        if cut_count < 0:
            cut_count = 0
        if not edge.is_boundary:
            subdivide_edges.append([edge, cut_count])
    for edge in subdivide_edges:
        bmesh.ops.subdivide_edges(bm, edges=[edge[0]], cuts=edge[1])
        bmesh.update_edit_mesh(obj.data)","for edge in bm.edges:
    cut_count = int(edge.calc_length() / shortest_edge) * cuts
    if cut_count < 0:
        cut_count = 0
    if not edge.is_boundary:
        subdivide_edges.append([edge, cut_count])
for edge in subdivide_edges:
    bmesh.ops.subdivide_edges(bm, edges=[edge[0]], cuts=edge[1])
    bmesh.update_edit_mesh(obj.data)","subdivide_edges = []
for edge in bm.edges:
    cut_count = int(edge.calc_length() / shortest_edge) * cuts
    if cut_count < 0:
        cut_count = 0
    if not edge.is_boundary:
        subdivide_edges.append([edge, cut_count])
for (edge, cut_count) in subdivide_edges:
    bmesh.ops.subdivide_edges(bm, edges=[edge], cuts=cut_count)
    bmesh.update_edit_mesh(obj.data)",find_wrong,1
ShuiZe_0x727,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ShuiZe_0x727/Plugins/infoGather/subdomain/Spider/Baidu/baidu.py,https://github.com/0x727/ShuiZe_0x727/tree/master/Plugins/infoGather/subdomain/Spider/Baidu/baidu.py,BaiduSpider,get_subdomain$41,"def get_subdomain(self, each_wd, i):
    for page in range(1, self.PAGES + 1):
        wd = 'site:{} {}'.format(self.domain, each_wd)
        print('[{}] -> [page: {}]'.format(wd, page))
        wd = quote(wd)
        bd_link_titles = self.keyword(wd=wd, page=page)
        if bd_link_titles:
            for bd_link_title in bd_link_titles:
                (title, link) = (bd_link_title[0], bd_link_title[1])
                subdomain = self.location(each_wd, link, title)
                self.bdSubdomains.append(urlparse(subdomain).netloc)","subdomain = self.location(each_wd, link, title)
self.bdSubdomains.append(urlparse(subdomain).netloc)","subdomains = [urlparse(self.location(each_wd, link, title)).netloc for (link, title) in bd_link_titles]
self.bdSubdomains.extend(subdomains)",find_wrong,2
ROMP,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ROMP/romp/lib/loss_funcs/maps_loss.py,https://github.com/Arthur151/ROMP/tree/master/romp/lib/loss_funcs/maps_loss.py,AELoss,singleTagLoss$78,"def singleTagLoss(self, pred_tag, joints):
    """"""
        associative embedding loss for one image
        """"""
    tags = []
    pull = 0
    for joints_per_person in joints:
        tmp = []
        for joint in joints_per_person:
            if joint[1] > 0:
                tmp.append(pred_tag[joint[0]])
        if len(tmp) == 0:
            continue
        tmp = torch.stack(tmp)
        tags.append(torch.mean(tmp, dim=0))
        pull = pull + torch.mean((tmp - tags[-1].expand_as(tmp)) ** 2)
    num_tags = len(tags)
    if num_tags == 0:
        return (make_input(torch.zeros(1).float()), make_input(torch.zeros(1).float()))
    elif num_tags == 1:
        return (make_input(torch.zeros(1).float()), pull / num_tags)
    tags = torch.stack(tags)
    size = (num_tags, num_tags)
    A = tags.expand(*size)
    B = A.permute(1, 0)
    diff = A - B
    if self.loss_type == 'exp':
        diff = torch.pow(diff, 2)
        push = torch.exp(-diff)
        push = torch.sum(push) - num_tags
    elif self.loss_type == 'max':
        diff = 1 - torch.abs(diff)
        push = torch.clamp(diff, min=0).sum() - num_tags
    else:
        raise ValueError('Unkown ae loss type')
    return (push / ((num_tags - 1) * num_tags) * 0.5, pull / num_tags)","for joints_per_person in joints:
    tmp = []
    for joint in joints_per_person:
        if joint[1] > 0:
            tmp.append(pred_tag[joint[0]])
    if len(tmp) == 0:
        continue
    tmp = torch.stack(tmp)
    tags.append(torch.mean(tmp, dim=0))
    pull = pull + torch.mean((tmp - tags[-1].expand_as(tmp)) ** 2)","tags = []
pull = 0
for joints_per_person in joints:
    tmp = []
    for joint in joints_per_person:
        if joint[1] > 0:
            tmp.append(pred_tag[joint[0]])
    if len(tmp) > 0:
        tmp = torch.stack(tmp)
        tags.append(torch.mean(tmp, dim=0))
        pull = pull + torch.mean((tmp - tags[-1].expand_as(tmp)) ** 2)",find_wrong,-1
Listed-company-news-crawl-and-text-analysis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Listed-company-news-crawl-and-text-analysis/Crawler/crawler_nbd.py,https://github.com/DemonDamon/Listed-company-news-crawl-and-text-analysis/tree/master/Crawler/crawler_nbd.py,WebCrawlFromNBD,multi_threads_run$268,"def multi_threads_run(self):
    """"""Multi-threading running.
        """"""
    page_ranges_lst = self.GenPagesLst()
    th_lst = []
    for page_range in page_ranges_lst:
        thread = threading.Thread(target=self.CrawlCompanyNews, args=(page_range[0], page_range[1]))
        th_lst.append(thread)
    for thread in th_lst:
        thread.start()
    for thread in th_lst:
        thread.join()
    return self.url_lst_withoutNews","for page_range in page_ranges_lst:
    thread = threading.Thread(target=self.CrawlCompanyNews, args=(page_range[0], page_range[1]))
    th_lst.append(thread)
for thread in th_lst:
    thread.start()
for thread in th_lst:
    thread.join()","th_lst = [threading.Thread(target=self.CrawlCompanyNews, args=(page_range[0], page_range[1])) for page_range in page_ranges_lst]
for thread in th_lst:
    thread.start()
for thread in th_lst:
    thread.join()",find_wrong,-1
DeepRec,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepRec/utils/load_data/load_data_ranking.py,https://github.com/cheungdaven/DeepRec/tree/master/utils/load_data/load_data_ranking.py,,load_data_neg$64,"def load_data_neg(path='../data/ml100k/movielens_100k.dat', header=['user_id', 'item_id', 'rating', 'category'], test_size=0.2, sep='\t'):
    df = pd.read_csv(path, sep=sep, names=header, engine='python')
    n_users = df.user_id.unique().shape[0]
    n_items = df.item_id.unique().shape[0]
    (train_data, test_data) = train_test_split(df, test_size=test_size)
    train_data = pd.DataFrame(train_data)
    test_data = pd.DataFrame(test_data)
    train_row = []
    train_col = []
    train_rating = []
    for line in train_data.itertuples():
        u = line[1] - 1
        i = line[2] - 1
        train_row.append(u)
        train_col.append(i)
        train_rating.append(1)
    train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))
    test_row = []
    test_col = []
    test_rating = []
    for line in test_data.itertuples():
        test_row.append(line[1] - 1)
        test_col.append(line[2] - 1)
        test_rating.append(1)
    test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))
    test_dict = {}
    for u in range(n_users):
        test_dict[u] = test_matrix.getrow(u).nonzero()[1]
    print('Load data finished. Number of users:', n_users, 'Number of items:', n_items)
    return (train_matrix.todok(), test_dict, n_users, n_items)","train_row = []
train_col = []
train_rating = []
for line in train_data.itertuples():
    u = line[1] - 1
    i = line[2] - 1
    train_row.append(u)
    train_col.append(i)
    train_rating.append(1)
train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))
test_row = []
test_col = []
test_rating = []
for line in test_data.itertuples():
    test_row.append(line[1] - 1)
    test_col.append(line[2] - 1)
    test_rating.append(1)
test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))
test_dict = {}
for u in range(n_users):
    test_dict[u] = test_matrix.getrow(u).nonzero()[1]","train_matrix = csr_matrix(([1] * len(train_data), (train_data['user_id'].values - 1, train_data['item_id'].values - 1)), shape=(n_users, n_items))
test_matrix = csr_matrix(([1] * len(test_data), (test_data['user_id'].values - 1, test_data['item_id'].values - 1)), shape=(n_users, n_items))
test_dict = {}
for u in range(n_users):
    test_dict[u] = test_matrix.getrow(u).nonzero()[1]",find_wrong,-1
capirca,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/capirca/capirca/lib/cisco.py,https://github.com/google/capirca/tree/master/capirca/lib/cisco.py,TermStandard,__str__$106,"def __str__(self):
    if self.term.platform:
        if self.platform not in self.term.platform:
            return ''
    if self.term.platform_exclude:
        if self.platform in self.term.platform_exclude:
            return ''
    ret_str = []
    if self.term.verbatim:
        for next_verbatim in self.term.verbatim:
            if next_verbatim[0] == self.platform:
                ret_str.append(str(next_verbatim[1]))
            return '\n'.join(ret_str)
    v4_addresses = [x for x in self.term.address if not isinstance(x, nacaddr.IPv6)]
    if self.filter_name.isdigit():
        if self.verbose:
            ret_str.append('access-list %s remark %s' % (self.filter_name, self.term.name))
            comments = aclgenerator.WrapWords(self.term.comment, _COMMENT_MAX_WIDTH)
            for comment in comments:
                ret_str.append('access-list %s remark %s' % (self.filter_name, comment))
        action = _ACTION_TABLE.get(str(self.term.action[0]))
        if v4_addresses:
            for addr in v4_addresses:
                if addr.prefixlen == 32:
                    ret_str.append('access-list %s %s %s%s%s' % (self.filter_name, action, addr.network_address, self.logstring, self.dscpstring))
                else:
                    ret_str.append('access-list %s %s %s %s%s%s' % (self.filter_name, action, addr.network_address, addr.hostmask, self.logstring, self.dscpstring))
        else:
            ret_str.append('access-list %s %s %s%s%s' % (self.filter_name, action, 'any', self.logstring, self.dscpstring))
    else:
        if self.verbose:
            ret_str.append(' remark ' + self.term.name)
            comments = aclgenerator.WrapWords(self.term.comment, _COMMENT_MAX_WIDTH)
            if comments and comments[0]:
                for comment in comments:
                    ret_str.append(' remark ' + str(comment))
        action = _ACTION_TABLE.get(str(self.term.action[0]))
        if v4_addresses:
            for addr in v4_addresses:
                if addr.prefixlen == 32:
                    ret_str.append(' %s host %s%s%s' % (action, addr.network_address, self.logstring, self.dscpstring))
                elif self.platform == 'arista':
                    ret_str.append(' %s %s/%s%s%s' % (action, addr.network_address, addr.prefixlen, self.logstring, self.dscpstring))
                else:
                    ret_str.append(' %s %s %s%s%s' % (action, addr.network_address, addr.hostmask, self.logstring, self.dscpstring))
        else:
            ret_str.append(' %s %s%s%s' % (action, 'any', self.logstring, self.dscpstring))
    return '\n'.join(ret_str)","v4_addresses = [x for x in self.term.address if
                not isinstance(x, nacaddr.IPv6)]
    if self.filter_name.isdigit():
      if self.verbose:
        ret_str.append('access-list %s remark %s' % (self.filter_name,
                                                     self.term.name))
        comments = aclgenerator.WrapWords(self.term.comment,
                                          _COMMENT_MAX_WIDTH)
        for comment in comments:
          ret_str.append('access-list %s remark %s' % (self.filter_name,
                                                       comment))

      action = _ACTION_TABLE.get(str(self.term.action[0]))
      if v4_addresses:
        for addr in v4_addresses:
          if addr.prefixlen == 32:
            ret_str.append('access-list %s %s %s%s%s' % (self.filter_name,
                                                         action,
                                                         addr.network_address,
                                                         self.logstring,
                                                         self.dscpstring))
          else:
            ret_str.append('access-list %s %s %s %s%s%s' % (
                self.filter_name,
                action,
                addr.network_address,
                addr.hostmask,
                self.logstring,
                self.dscpstring))
      else:
        ret_str.append('access-list %s %s %s%s%s' % (self.filter_name,
                                                     action,
                                                     'any',
                                                     self.logstring,
                                                     self.dscpstring))
    else:
      if self.verbose:
        ret_str.append(' remark ' + self.term.name)
        comments = aclgenerator.WrapWords(self.term.comment,
                                          _COMMENT_MAX_WIDTH)
        if comments and comments[0]:
          for comment in comments:
            ret_str.append(' remark ' + str(comment))

      action = _ACTION_TABLE.get(str(self.term.action[0]))
      if v4_addresses:
        for addr in v4_addresses:
          if addr.prefixlen == 32:
            ret_str.append(' %s host %s%s%s' % (action,
                                                addr.network_address,
                                                self.logstring,
                                                self.dscpstring))
          elif self.platform == 'arista':
            ret_str.append(' %s %s/%s%s%s' % (action,
                                              addr.network_address,
                                              addr.prefixlen,
                                              self.logstring,
                                              self.dscpstring))
          else:
            ret_str.append(' %s %s %s%s%s' % (action,
                                              addr.network_address,
                                              addr.hostmask,
                                              self.logstring,
                                              self.dscpstring))
      else:
        ret_str.append(' %s %s%s%s' % (action,
                                       'any',
                                       self.logstring,
                                       self.dscpstring))","v4_addresses = [x for x in self.term.address if not isinstance(x, nacaddr.IPv6)]
    if self.filter_name.isdigit():
        if self.verbose:
            ret_str.append(f'access-list {self.filter_name} remark {self.term.name}')
            comments = aclgenerator.WrapWords(self.term.comment, _COMMENT_MAX_WIDTH)
            for comment in comments:
                ret_str.append(f'access-list {self.filter_name} remark {comment}')

        action = _ACTION_TABLE.get(str(self.term.action[0]))
        for addr in v4_addresses:
            if addr.prefixlen == 32:
                ret_str.append(f'access-list {self.filter_name} {action} {addr.network_address}{self.logstring}{self.dscpstring}')
            else:
                ret_str.append(f'access-list {self.filter_name} {action} {addr.network_address} {addr.hostmask}{self.logstring}{self.dscpstring}')
        if not v4_addresses:
            ret_str.append(f'access-list {self.filter_name} {action} any{self.logstring}{self.dscpstring}')
    else:
        if self.verbose:
            ret_str.append(f' remark {self.term.name}')
            comments = aclgenerator.WrapWords(self.term.comment, _COMMENT_MAX_WIDTH)
            if comments and comments[0]:
                for comment in comments:
                    ret_str.append(f' remark {comment}')

        action = _ACTION_TABLE.get(str(self.term.action[0]))
        for addr in v4_addresses:
            if addr.prefixlen == 32:
                ret_str.append(f' {action} host {addr.network_address}{self.logstring}{self.dscpstring}')
            elif self.platform == 'arista':
                ret_str.append(f' {action} {addr.network_address}/{addr.prefixlen}{self.logstring}{self.dscpstring}')
            else:
                ret_str.append(f' {action} {addr.network_address} {addr.hostmask}{self.logstring}{self.dscpstring}')
        if not v4_addresses:
            ret_str.append(f' {action} any{self.logstring}{self.dscpstring}')",find_wrong,2
qiskit-terra,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qiskit-terra/tools/verify_headers.py,https://github.com/Qiskit/qiskit-terra/tree/master/tools/verify_headers.py,,_main$81,"def _main():
    default_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'qiskit')
    parser = argparse.ArgumentParser(description='Check file headers.')
    parser.add_argument('paths', type=str, nargs='*', default=[default_path], help='Paths to scan by default uses ../qiskit from the script')
    args = parser.parse_args()
    files = discover_files(args.paths)
    with multiprocessing.Pool() as pool:
        res = pool.map(validate_header, files)
    failed_files = [x for x in res if x[1] is False]
    if len(failed_files) > 0:
        for failed_file in failed_files:
            sys.stderr.write('%s failed header check because:\n' % failed_file[0])
            sys.stderr.write('%s\n\n' % failed_file[2])
        sys.exit(1)
    sys.exit(0)","files = discover_files(args.paths)
with multiprocessing.Pool() as pool:
    res = pool.map(validate_header, files)","with multiprocessing.Pool() as pool:
    res = pool.map(validate_header, discover_files(args.paths))",find_wrong,2
FeatureLearningRotNet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FeatureLearningRotNet/dataloader.py,https://github.com/gidariss/FeatureLearningRotNet/tree/master//dataloader.py,Places205,__init__$37,"def __init__(self, root, split, transform=None, target_transform=None):
    self.root = os.path.expanduser(root)
    self.data_folder = os.path.join(self.root, 'data', 'vision', 'torralba', 'deeplearning', 'images256')
    self.split_folder = os.path.join(self.root, 'trainvalsplit_places205')
    assert split == 'train' or split == 'val'
    split_csv_file = os.path.join(self.split_folder, split + '_places205.csv')
    self.transform = transform
    self.target_transform = target_transform
    with open(split_csv_file, 'rb') as f:
        reader = csv.reader(f, delimiter=' ')
        self.img_files = []
        self.labels = []
        for row in reader:
            self.img_files.append(row[0])
            self.labels.append(long(row[1]))","with open(split_csv_file, 'rb') as f:
    reader = csv.reader(f, delimiter=' ')
    self.img_files = []
    self.labels = []
    for row in reader:
        self.img_files.append(row[0])
        self.labels.append(long(row[1]))","with open(split_csv_file, 'r') as f:
    reader = csv.reader(f, delimiter=' ')
    self.img_files = []
    self.labels = []
    for row in reader:
        self.img_files.append(row[0])
        self.labels.append(int(row[1]))",find_wrong,-1
mochi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mochi/mochi/core/translation.py,https://github.com/i2y/mochi/tree/master/mochi/core/translation.py,Translator,translate_with_old$1029,"def translate_with_old(self, exp):
    (keyword_with, items, *body) = exp
    pre = []
    first_with_py = None
    with_py = None
    for item in items:
        (item_pre, item_value) = self.translate(item[0], False)
        pre.extend(item_pre)
        var = item[1]
        if with_py is None:
            with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)
            first_with_py = with_py
        else:
            inner_with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)
            with_py.body = [inner_with_py]
            with_py = inner_with_py
    with_py.body = self._translate_sequence(body, True)
    pre.append(first_with_py)
    return (pre, self.translate(NONE_SYM, False)[1])","with_py = None
for item in items:
    ...
    if with_py is None:
        with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)
        first_with_py = with_py
    else:
        inner_with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)
        with_py.body = [inner_with_py]
        with_py = inner_with_py","with_pys = []
for item in items:
    ...
    with_py = ast.With(context_expr=item_value, optional_vars=ast.Name(id=var.name, ctx=ast.Store(), lineno=var.lineno, col_offset=0), lineno=var.lineno, col_offset=0)
    with_pys.append(with_py)
for i in range(len(with_pys) - 1):
    with_pys[i].body = [with_pys[i + 1]]
with_pys[-1].body = self._translate_sequence(body, True)
return (pre + with_pys, self.translate(NONE_SYM, False)[1])",find_wrong,2
