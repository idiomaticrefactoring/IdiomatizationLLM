repo_name,file_path,file_html,class_name,me_name,me_code,old_code,new_code,bool_code,chatGPT_code,if_correct,reversed_code,non_replace_var_refactored_code,refactored_code,acc,instruction,sys_msg,exam_msg,user_msg
tf-encrypted,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tf-encrypted/tf_encrypted/protocol/aby3/aby3.py,https://github.com/tf-encrypted/tf-encrypted/tree/master/tf_encrypted/protocol/aby3/aby3.py,,_sort_private$5708,"def _sort_private(
    prot: ABY3,
    x: ABY3PrivateTensor,
    axis: int,
    acc: bool = True,
) -> ABY3PrivateTensor:

    with tf.name_scope(""sort""):

        def bitonic_index(n, stage, sub_stage):
            assert sub_stage <= stage, ""The i-th stage can have at most i+1 sub stages.""
            # In bitonic sorting network, the 0-th sub stage in each stage has a different pattern from
            # other sub stages.
            if sub_stage == 0:
                a = np.arange(n)
                b = np.split(a, n / (2**stage))
                left = np.concatenate(b[0::2])
                right = np.concatenate([np.flip(x) for x in b[1::2]])
                return (left, right)
            else:
                a = np.arange(n)
                b = np.split(a, n / (2 ** (stage - sub_stage)))
                left = np.concatenate(b[0::2])
                right = np.concatenate(b[1::2])
                return (left, right)

        # def bitonic_sort(x):
        # n = int(x.shape[0])
        # n_stages = ceil(log2(n))
        # for stage in range(n_stages):
        # print(""building stage: "", stage)
        # for sub_stage in range(stage + 1):
        # left_idx, right_idx = bitonic_index(n, stage, sub_stage)
        # left = prot.gather(x, left_idx)
        # right = prot.gather(x, right_idx)
        # left, right = prot.cmp_swap(left, right)
        # z0 = prot.scatter_nd(
        # np.expand_dims(left_idx, axis=1),
        # left,
        # x.shape)
        # z1 = prot.scatter_nd(
        # np.expand_dims(right_idx, axis=1),
        # right,
        # x.shape)
        # x = z0 + z1
        # return x

        # return bitonic_sort(x)

        def build_bitonic_index(n):
            indices = []
            n_stages = ceil(log2(n))
            for stage in range(n_stages):
                for sub_stage in range(stage + 1):
                    left_idx, right_idx = bitonic_index(n, stage, sub_stage)
                    indices.append(np.stack([left_idx, right_idx]))
            return np.stack(indices)

        axis = axis % len(x.shape)
        if axis < 0:
            axis += len(x.shape)
        if axis != 0:
            x = prot.transpose(
                x,
                perm=[axis]
                + list(range(0, axis))
                + list(range(axis + 1, len(x.shape))),
            )

        unpadded_n = int(x.shape[0])
        n = next_power_of_two(unpadded_n)
        if n > unpadded_n:
            # We can only handle numbers of bit length k-2 for comparison
            max_bound = (1 << (x.backing_dtype.nbits - 2)) - 1
            pad = prot.define_constant(
                np.ones([n - unpadded_n] + x.shape[1:]) * max_bound, apply_scaling=False
            )
            pad = pad.to_private(x.share_type)
            pad.is_scaled = x.is_scaled
            x = prot.concat([x, pad], axis=0)

        n_stages = ceil(log2(n))
        n_sub_stages = int((1 + n_stages) * n_stages / 2)

        indices = build_bitonic_index(n)
        indices = prot.define_constant(indices, apply_scaling=False)

        def cond(i, x):
            return i < n_sub_stages

        def body(i, x):
            left_idx = indices[i][0]
            right_idx = indices[i][1]

            left = prot.gather(x, left_idx, axis=0)
            right = prot.gather(x, right_idx, axis=0)
            min_, max_ = prot.cmp_swap(left, right)

            if acc:
                min_idx, max_idx = (left_idx, right_idx)
            else:
                min_idx, max_idx = (right_idx, left_idx)

            z0 = prot.scatter_nd(prot.expand_dims(min_idx, axis=1), min_, x.shape)
            z1 = prot.scatter_nd(prot.expand_dims(max_idx, axis=1), max_, x.shape)
            x = z0 + z1

            return (i + 1, x)

        _, x = prot.while_loop(cond, body, [tf.constant(0), x])
        if n > unpadded_n:
            if acc:
                x = x[:unpadded_n]
            else:
                x = x[(n - unpadded_n) :]
        if axis != 0:
            x = prot.transpose(
                x,
                perm=list(range(1, axis + 1))
                + [0]
                + list(range(axis + 1, len(x.shape))),
            )
        return x",axis != 0,axis,axis,1,"v1: axis
v2: 0
v3: != (not equal to)"
tf-encrypted,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tf-encrypted/tf_encrypted/protocol/aby3/aby3.py,https://github.com/tf-encrypted/tf-encrypted/tree/master/tf_encrypted/protocol/aby3/aby3.py,,_sort_private$5708,"def _sort_private(
    prot: ABY3,
    x: ABY3PrivateTensor,
    axis: int,
    acc: bool = True,
) -> ABY3PrivateTensor:

    with tf.name_scope(""sort""):

        def bitonic_index(n, stage, sub_stage):
            assert sub_stage <= stage, ""The i-th stage can have at most i+1 sub stages.""
            # In bitonic sorting network, the 0-th sub stage in each stage has a different pattern from
            # other sub stages.
            if sub_stage == 0:
                a = np.arange(n)
                b = np.split(a, n / (2**stage))
                left = np.concatenate(b[0::2])
                right = np.concatenate([np.flip(x) for x in b[1::2]])
                return (left, right)
            else:
                a = np.arange(n)
                b = np.split(a, n / (2 ** (stage - sub_stage)))
                left = np.concatenate(b[0::2])
                right = np.concatenate(b[1::2])
                return (left, right)

        # def bitonic_sort(x):
        # n = int(x.shape[0])
        # n_stages = ceil(log2(n))
        # for stage in range(n_stages):
        # print(""building stage: "", stage)
        # for sub_stage in range(stage + 1):
        # left_idx, right_idx = bitonic_index(n, stage, sub_stage)
        # left = prot.gather(x, left_idx)
        # right = prot.gather(x, right_idx)
        # left, right = prot.cmp_swap(left, right)
        # z0 = prot.scatter_nd(
        # np.expand_dims(left_idx, axis=1),
        # left,
        # x.shape)
        # z1 = prot.scatter_nd(
        # np.expand_dims(right_idx, axis=1),
        # right,
        # x.shape)
        # x = z0 + z1
        # return x

        # return bitonic_sort(x)

        def build_bitonic_index(n):
            indices = []
            n_stages = ceil(log2(n))
            for stage in range(n_stages):
                for sub_stage in range(stage + 1):
                    left_idx, right_idx = bitonic_index(n, stage, sub_stage)
                    indices.append(np.stack([left_idx, right_idx]))
            return np.stack(indices)

        axis = axis % len(x.shape)
        if axis < 0:
            axis += len(x.shape)
        if axis != 0:
            x = prot.transpose(
                x,
                perm=[axis]
                + list(range(0, axis))
                + list(range(axis + 1, len(x.shape))),
            )

        unpadded_n = int(x.shape[0])
        n = next_power_of_two(unpadded_n)
        if n > unpadded_n:
            # We can only handle numbers of bit length k-2 for comparison
            max_bound = (1 << (x.backing_dtype.nbits - 2)) - 1
            pad = prot.define_constant(
                np.ones([n - unpadded_n] + x.shape[1:]) * max_bound, apply_scaling=False
            )
            pad = pad.to_private(x.share_type)
            pad.is_scaled = x.is_scaled
            x = prot.concat([x, pad], axis=0)

        n_stages = ceil(log2(n))
        n_sub_stages = int((1 + n_stages) * n_stages / 2)

        indices = build_bitonic_index(n)
        indices = prot.define_constant(indices, apply_scaling=False)

        def cond(i, x):
            return i < n_sub_stages

        def body(i, x):
            left_idx = indices[i][0]
            right_idx = indices[i][1]

            left = prot.gather(x, left_idx, axis=0)
            right = prot.gather(x, right_idx, axis=0)
            min_, max_ = prot.cmp_swap(left, right)

            if acc:
                min_idx, max_idx = (left_idx, right_idx)
            else:
                min_idx, max_idx = (right_idx, left_idx)

            z0 = prot.scatter_nd(prot.expand_dims(min_idx, axis=1), min_, x.shape)
            z1 = prot.scatter_nd(prot.expand_dims(max_idx, axis=1), max_, x.shape)
            x = z0 + z1

            return (i + 1, x)

        _, x = prot.while_loop(cond, body, [tf.constant(0), x])
        if n > unpadded_n:
            if acc:
                x = x[:unpadded_n]
            else:
                x = x[(n - unpadded_n) :]
        if axis != 0:
            x = prot.transpose(
                x,
                perm=list(range(1, axis + 1))
                + [0]
                + list(range(axis + 1, len(x.shape))),
            )
        return x",sub_stage == 0,not sub_stage,not sub_stage,1,"v1: sub_stage
v2: 0"
tf-encrypted,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tf-encrypted/tf_encrypted/protocol/aby3/aby3.py,https://github.com/tf-encrypted/tf-encrypted/tree/master/tf_encrypted/protocol/aby3/aby3.py,,_sort_private$5708,"def _sort_private(
    prot: ABY3,
    x: ABY3PrivateTensor,
    axis: int,
    acc: bool = True,
) -> ABY3PrivateTensor:

    with tf.name_scope(""sort""):

        def bitonic_index(n, stage, sub_stage):
            assert sub_stage <= stage, ""The i-th stage can have at most i+1 sub stages.""
            # In bitonic sorting network, the 0-th sub stage in each stage has a different pattern from
            # other sub stages.
            if sub_stage == 0:
                a = np.arange(n)
                b = np.split(a, n / (2**stage))
                left = np.concatenate(b[0::2])
                right = np.concatenate([np.flip(x) for x in b[1::2]])
                return (left, right)
            else:
                a = np.arange(n)
                b = np.split(a, n / (2 ** (stage - sub_stage)))
                left = np.concatenate(b[0::2])
                right = np.concatenate(b[1::2])
                return (left, right)

        # def bitonic_sort(x):
        # n = int(x.shape[0])
        # n_stages = ceil(log2(n))
        # for stage in range(n_stages):
        # print(""building stage: "", stage)
        # for sub_stage in range(stage + 1):
        # left_idx, right_idx = bitonic_index(n, stage, sub_stage)
        # left = prot.gather(x, left_idx)
        # right = prot.gather(x, right_idx)
        # left, right = prot.cmp_swap(left, right)
        # z0 = prot.scatter_nd(
        # np.expand_dims(left_idx, axis=1),
        # left,
        # x.shape)
        # z1 = prot.scatter_nd(
        # np.expand_dims(right_idx, axis=1),
        # right,
        # x.shape)
        # x = z0 + z1
        # return x

        # return bitonic_sort(x)

        def build_bitonic_index(n):
            indices = []
            n_stages = ceil(log2(n))
            for stage in range(n_stages):
                for sub_stage in range(stage + 1):
                    left_idx, right_idx = bitonic_index(n, stage, sub_stage)
                    indices.append(np.stack([left_idx, right_idx]))
            return np.stack(indices)

        axis = axis % len(x.shape)
        if axis < 0:
            axis += len(x.shape)
        if axis != 0:
            x = prot.transpose(
                x,
                perm=[axis]
                + list(range(0, axis))
                + list(range(axis + 1, len(x.shape))),
            )

        unpadded_n = int(x.shape[0])
        n = next_power_of_two(unpadded_n)
        if n > unpadded_n:
            # We can only handle numbers of bit length k-2 for comparison
            max_bound = (1 << (x.backing_dtype.nbits - 2)) - 1
            pad = prot.define_constant(
                np.ones([n - unpadded_n] + x.shape[1:]) * max_bound, apply_scaling=False
            )
            pad = pad.to_private(x.share_type)
            pad.is_scaled = x.is_scaled
            x = prot.concat([x, pad], axis=0)

        n_stages = ceil(log2(n))
        n_sub_stages = int((1 + n_stages) * n_stages / 2)

        indices = build_bitonic_index(n)
        indices = prot.define_constant(indices, apply_scaling=False)

        def cond(i, x):
            return i < n_sub_stages

        def body(i, x):
            left_idx = indices[i][0]
            right_idx = indices[i][1]

            left = prot.gather(x, left_idx, axis=0)
            right = prot.gather(x, right_idx, axis=0)
            min_, max_ = prot.cmp_swap(left, right)

            if acc:
                min_idx, max_idx = (left_idx, right_idx)
            else:
                min_idx, max_idx = (right_idx, left_idx)

            z0 = prot.scatter_nd(prot.expand_dims(min_idx, axis=1), min_, x.shape)
            z1 = prot.scatter_nd(prot.expand_dims(max_idx, axis=1), max_, x.shape)
            x = z0 + z1

            return (i + 1, x)

        _, x = prot.while_loop(cond, body, [tf.constant(0), x])
        if n > unpadded_n:
            if acc:
                x = x[:unpadded_n]
            else:
                x = x[(n - unpadded_n) :]
        if axis != 0:
            x = prot.transpose(
                x,
                perm=list(range(1, axis + 1))
                + [0]
                + list(range(axis + 1, len(x.shape))),
            )
        return x",axis != 0,axis,axis,1,"v1: axis
v2: 0"
zvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/tests/api/test_trading.py,https://github.com/zvtvz/zvt/tree/master/tests/api/test_trading.py,,test_000778_manager_trading$29,"def test_000778_manager_trading():
    result: List[ManagerTrading] = ManagerTrading.query_data(session=session, provider='eastmoney',
                                                             return_type='domain',
                                                             codes=['000778'],
                                                             end_timestamp='2018-09-30',
                                                             start_timestamp='2017-09-30',
                                                             order=ManagerTrading.holding.desc())
    assert len(result) == 1
    assert result[0].trading_person == '巩国平'
    assert result[0].volume == 8400
    assert result[0].price == None
    assert result[0].holding == 18700
    assert result[0].trading_way == '增持'
    assert result[0].manager_position == '职工监事'
    assert result[0].manager == '巩国平'
    assert result[0].relationship_with_manager == '本人'",result[0].volume == 8400,cannot be refactored by truth value test,Cannot refactor,1,"v1: result[0].volume
v2: 8400"
zvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/tests/api/test_trading.py,https://github.com/zvtvz/zvt/tree/master/tests/api/test_trading.py,,test_000778_manager_trading$29,"def test_000778_manager_trading():
    result: List[ManagerTrading] = ManagerTrading.query_data(session=session, provider='eastmoney',
                                                             return_type='domain',
                                                             codes=['000778'],
                                                             end_timestamp='2018-09-30',
                                                             start_timestamp='2017-09-30',
                                                             order=ManagerTrading.holding.desc())
    assert len(result) == 1
    assert result[0].trading_person == '巩国平'
    assert result[0].volume == 8400
    assert result[0].price == None
    assert result[0].holding == 18700
    assert result[0].trading_way == '增持'
    assert result[0].manager_position == '职工监事'
    assert result[0].manager == '巩国平'
    assert result[0].relationship_with_manager == '本人'",result[0].relationship_with_manager == '本人',cannot be refactored by truth value test,Cannot refactor,1,"v1: result[0].relationship_with_manager
v2: '本人'"
zvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/tests/api/test_trading.py,https://github.com/zvtvz/zvt/tree/master/tests/api/test_trading.py,,test_000778_manager_trading$29,"def test_000778_manager_trading():
    result: List[ManagerTrading] = ManagerTrading.query_data(session=session, provider='eastmoney',
                                                             return_type='domain',
                                                             codes=['000778'],
                                                             end_timestamp='2018-09-30',
                                                             start_timestamp='2017-09-30',
                                                             order=ManagerTrading.holding.desc())
    assert len(result) == 1
    assert result[0].trading_person == '巩国平'
    assert result[0].volume == 8400
    assert result[0].price == None
    assert result[0].holding == 18700
    assert result[0].trading_way == '增持'
    assert result[0].manager_position == '职工监事'
    assert result[0].manager == '巩国平'
    assert result[0].relationship_with_manager == '本人'",result[0].trading_person == '巩国平',cannot be refactored by truth value test,Cannot refactor,1,"v1: result[0].trading_person
v2: '巩国平'"
zvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/tests/api/test_trading.py,https://github.com/zvtvz/zvt/tree/master/tests/api/test_trading.py,,test_000778_manager_trading$29,"def test_000778_manager_trading():
    result: List[ManagerTrading] = ManagerTrading.query_data(session=session, provider='eastmoney',
                                                             return_type='domain',
                                                             codes=['000778'],
                                                             end_timestamp='2018-09-30',
                                                             start_timestamp='2017-09-30',
                                                             order=ManagerTrading.holding.desc())
    assert len(result) == 1
    assert result[0].trading_person == '巩国平'
    assert result[0].volume == 8400
    assert result[0].price == None
    assert result[0].holding == 18700
    assert result[0].trading_way == '增持'
    assert result[0].manager_position == '职工监事'
    assert result[0].manager == '巩国平'
    assert result[0].relationship_with_manager == '本人'",result[0].holding == 18700,cannot be refactored by truth value test,Cannot refactor,1,"v1: result[0].holding
v2: 18700"
zvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/tests/api/test_trading.py,https://github.com/zvtvz/zvt/tree/master/tests/api/test_trading.py,,test_000778_manager_trading$29,"def test_000778_manager_trading():
    result: List[ManagerTrading] = ManagerTrading.query_data(session=session, provider='eastmoney',
                                                             return_type='domain',
                                                             codes=['000778'],
                                                             end_timestamp='2018-09-30',
                                                             start_timestamp='2017-09-30',
                                                             order=ManagerTrading.holding.desc())
    assert len(result) == 1
    assert result[0].trading_person == '巩国平'
    assert result[0].volume == 8400
    assert result[0].price == None
    assert result[0].holding == 18700
    assert result[0].trading_way == '增持'
    assert result[0].manager_position == '职工监事'
    assert result[0].manager == '巩国平'
    assert result[0].relationship_with_manager == '本人'",result[0].trading_way == '增持',cannot be refactored by truth value test,Cannot refactor,1,"v1: result[0].trading_way
v2: '增持'"
zvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/tests/api/test_trading.py,https://github.com/zvtvz/zvt/tree/master/tests/api/test_trading.py,,test_000778_manager_trading$29,"def test_000778_manager_trading():
    result: List[ManagerTrading] = ManagerTrading.query_data(session=session, provider='eastmoney',
                                                             return_type='domain',
                                                             codes=['000778'],
                                                             end_timestamp='2018-09-30',
                                                             start_timestamp='2017-09-30',
                                                             order=ManagerTrading.holding.desc())
    assert len(result) == 1
    assert result[0].trading_person == '巩国平'
    assert result[0].volume == 8400
    assert result[0].price == None
    assert result[0].holding == 18700
    assert result[0].trading_way == '增持'
    assert result[0].manager_position == '职工监事'
    assert result[0].manager == '巩国平'
    assert result[0].relationship_with_manager == '本人'",result[0].manager_position == '职工监事',cannot be refactored by truth value test,Cannot refactor,1,"v1: result[0].manager_position
v2: '职工监事'"
zvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/tests/api/test_trading.py,https://github.com/zvtvz/zvt/tree/master/tests/api/test_trading.py,,test_000778_manager_trading$29,"def test_000778_manager_trading():
    result: List[ManagerTrading] = ManagerTrading.query_data(session=session, provider='eastmoney',
                                                             return_type='domain',
                                                             codes=['000778'],
                                                             end_timestamp='2018-09-30',
                                                             start_timestamp='2017-09-30',
                                                             order=ManagerTrading.holding.desc())
    assert len(result) == 1
    assert result[0].trading_person == '巩国平'
    assert result[0].volume == 8400
    assert result[0].price == None
    assert result[0].holding == 18700
    assert result[0].trading_way == '增持'
    assert result[0].manager_position == '职工监事'
    assert result[0].manager == '巩国平'
    assert result[0].relationship_with_manager == '本人'",result[0].price == None,not result[0].price,not result[0].price,1,"v1: result[0].price
v2: None"
zvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/tests/api/test_trading.py,https://github.com/zvtvz/zvt/tree/master/tests/api/test_trading.py,,test_000778_manager_trading$29,"def test_000778_manager_trading():
    result: List[ManagerTrading] = ManagerTrading.query_data(session=session, provider='eastmoney',
                                                             return_type='domain',
                                                             codes=['000778'],
                                                             end_timestamp='2018-09-30',
                                                             start_timestamp='2017-09-30',
                                                             order=ManagerTrading.holding.desc())
    assert len(result) == 1
    assert result[0].trading_person == '巩国平'
    assert result[0].volume == 8400
    assert result[0].price == None
    assert result[0].holding == 18700
    assert result[0].trading_way == '增持'
    assert result[0].manager_position == '职工监事'
    assert result[0].manager == '巩国平'
    assert result[0].relationship_with_manager == '本人'",len(result) == 1,cannot be refactored by truth value test,Cannot refactor,1,"v1: len(result)
v2: 1"
zvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/tests/api/test_trading.py,https://github.com/zvtvz/zvt/tree/master/tests/api/test_trading.py,,test_000778_manager_trading$29,"def test_000778_manager_trading():
    result: List[ManagerTrading] = ManagerTrading.query_data(session=session, provider='eastmoney',
                                                             return_type='domain',
                                                             codes=['000778'],
                                                             end_timestamp='2018-09-30',
                                                             start_timestamp='2017-09-30',
                                                             order=ManagerTrading.holding.desc())
    assert len(result) == 1
    assert result[0].trading_person == '巩国平'
    assert result[0].volume == 8400
    assert result[0].price == None
    assert result[0].holding == 18700
    assert result[0].trading_way == '增持'
    assert result[0].manager_position == '职工监事'
    assert result[0].manager == '巩国平'
    assert result[0].relationship_with_manager == '本人'",result[0].manager == '巩国平',cannot be refactored by truth value test,Cannot refactor,1,"v1: result[0].manager
v2: '巩国平'"
xonsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xonsh/xonsh/completers/commands.py,https://github.com/xonsh/xonsh/tree/master/xonsh/completers/commands.py,,complete_skipper$44,"def complete_skipper(command_context: CommandContext):
    """"""
    Skip over several tokens (e.g., sudo) and complete based on the rest of the command.
    """"""

    # Contextual completers don't need us to skip tokens since they get the correct completion context -
    # meaning we only need to skip commands like ``sudo``.
    skip_part_num = 0
    # all the args before the current argument
    for arg in command_context.args[: command_context.arg_index]:
        if arg.value not in SKIP_TOKENS:
            break
        skip_part_num += 1

    if skip_part_num == 0:
        return None

    skipped_command_context = command_context._replace(
        args=command_context.args[skip_part_num:],
        arg_index=command_context.arg_index - skip_part_num,
    )

    if skipped_command_context.arg_index == 0:
        # completing the command after a SKIP_TOKEN
        return complete_command(skipped_command_context)

    completer: Completer = XSH.shell.shell.completer  # type: ignore
    return completer.complete_from_context(CompletionContext(skipped_command_context))",skip_part_num == 0,not skip_part_num,not skip_part_num,1,"v1: skip_part_num
v2: 0"
xonsh,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/xonsh/xonsh/completers/commands.py,https://github.com/xonsh/xonsh/tree/master/xonsh/completers/commands.py,,complete_skipper$44,"def complete_skipper(command_context: CommandContext):
    """"""
    Skip over several tokens (e.g., sudo) and complete based on the rest of the command.
    """"""

    # Contextual completers don't need us to skip tokens since they get the correct completion context -
    # meaning we only need to skip commands like ``sudo``.
    skip_part_num = 0
    # all the args before the current argument
    for arg in command_context.args[: command_context.arg_index]:
        if arg.value not in SKIP_TOKENS:
            break
        skip_part_num += 1

    if skip_part_num == 0:
        return None

    skipped_command_context = command_context._replace(
        args=command_context.args[skip_part_num:],
        arg_index=command_context.arg_index - skip_part_num,
    )

    if skipped_command_context.arg_index == 0:
        # completing the command after a SKIP_TOKEN
        return complete_command(skipped_command_context)

    completer: Completer = XSH.shell.shell.completer  # type: ignore
    return completer.complete_from_context(CompletionContext(skipped_command_context))",skipped_command_context.arg_index == 0,not skipped_command_context.arg_index,not skipped_command_context.arg_index,1,"v1: skipped_command_context.arg_index
v2: 0"
self-critical.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/self-critical.pytorch/captioning/models/BertCapModel.py,https://github.com/ruotianluo/self-critical.pytorch/tree/master/captioning/models/BertCapModel.py,BertCapModel,core$92,"def core(self, it, fc_feats_ph, att_feats_ph, memory, state, mask):
        """"""
        state = [ys.unsqueeze(0)]
        """"""
        if len(state) == 0:
            ys = it.unsqueeze(1)
        else:
            ys = torch.cat([state[0][0], it.unsqueeze(1)], dim=1)
        out = self.model.decode(memory, mask, 
                               ys, 
                               subsequent_mask(ys.size(1))
                                        .to(memory.device))
        return out[:, -1], [ys.unsqueeze(0)]",len(state) == 0,not state,not state,1,"v1: len(state)
v2: 0"
pytorch_tabular,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pytorch_tabular/tests/test_mdn.py,https://github.com/manujosephv/pytorch_tabular/tree/master/tests/test_mdn.py,,test_ssl$150,"def test_ssl(
    classification_data,
    continuous_cols,
    categorical_cols,
    continuous_feature_transform,
    normalize_continuous_features,
    num_gaussian,
    ssl_task,
    aug_task,
):
    (train, test, target) = classification_data
    if len(continuous_cols) + len(categorical_cols) == 0:
        assert True
    else:
        data_config = DataConfig(
            target=target,
            continuous_cols=continuous_cols,
            categorical_cols=categorical_cols,
            continuous_feature_transform=continuous_feature_transform,
            normalize_continuous_features=normalize_continuous_features,
        )
        model_config_params = dict(task=""ssl"", ssl_task=ssl_task, aug_task=aug_task)
        mdn_config = MixtureDensityHeadConfig(num_gaussian=num_gaussian)
        model_config_params[""mdn_config""] = mdn_config
        model_config = CategoryEmbeddingMDNConfig(**model_config_params)
        trainer_config = TrainerConfig(
            max_epochs=3,
            checkpoints=None,
            early_stopping=None,
            gpus=None,
            fast_dev_run=True,
        )
        optimizer_config = OptimizerConfig()
        with pytest.raises(AssertionError):
            tabular_model = TabularModel(
                data_config=data_config,
                model_config=model_config,
                optimizer_config=optimizer_config,
                trainer_config=trainer_config,
            )
            tabular_model.fit(train=train, test=test)",len(continuous_cols) + len(categorical_cols) == 0,not continuous_cols) + len(categorical_cols,not len(continuous_cols) + len(categorical_cols),0,"v1: len(continuous_cols) + len(categorical_cols)
v2: 0"
interfacegan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/interfacegan/models/pggan_tf_official/train.py,https://github.com/genforce/interfacegan/tree/master/models/pggan_tf_official/train.py,,train_progressive_gan$133,"def train_progressive_gan(
    G_smoothing             = 0.999,        # Exponential running average of generator weights.
    D_repeats               = 1,            # How many times the discriminator is trained per G iteration.
    minibatch_repeats       = 4,            # Number of minibatches to run before adjusting training parameters.
    reset_opt_for_new_lod   = True,         # Reset optimizer internal state (e.g. Adam moments) when new layers are introduced?
    total_kimg              = 15000,        # Total length of the training, measured in thousands of real images.
    mirror_augment          = False,        # Enable mirror augment?
    drange_net              = [-1,1],       # Dynamic range used when feeding image data to the networks.
    image_snapshot_ticks    = 1,            # How often to export image snapshots?
    network_snapshot_ticks  = 10,           # How often to export network snapshots?
    save_tf_graph           = False,        # Include full TensorFlow computation graph in the tfevents file?
    save_weight_histograms  = False,        # Include weight histograms in the tfevents file?
    resume_run_id           = None,         # Run ID or network pkl to resume training from, None = start from scratch.
    resume_snapshot         = None,         # Snapshot index to resume training from, None = autodetect.
    resume_kimg             = 0.0,          # Assumed training progress at the beginning. Affects reporting and training schedule.
    resume_time             = 0.0):         # Assumed wallclock time at the beginning. Affects reporting.

    maintenance_start_time = time.time()
    training_set = dataset.load_dataset(data_dir=config.data_dir, verbose=True, **config.dataset)

    # Construct networks.
    with tf.device('/gpu:0'):
        if resume_run_id is not None:
            network_pkl = misc.locate_network_pkl(resume_run_id, resume_snapshot)
            print('Loading networks from ""%s""...' % network_pkl)
            G, D, Gs = misc.load_pkl(network_pkl)
        else:
            print('Constructing networks...')
            G = tfutil.Network('G', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.G)
            D = tfutil.Network('D', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.D)
            Gs = G.clone('Gs')
        Gs_update_op = Gs.setup_as_moving_average_of(G, beta=G_smoothing)
    G.print_layers(); D.print_layers()

    print('Building TensorFlow graph...')
    with tf.name_scope('Inputs'):
        lod_in          = tf.placeholder(tf.float32, name='lod_in', shape=[])
        lrate_in        = tf.placeholder(tf.float32, name='lrate_in', shape=[])
        minibatch_in    = tf.placeholder(tf.int32, name='minibatch_in', shape=[])
        minibatch_split = minibatch_in // config.num_gpus
        reals, labels   = training_set.get_minibatch_tf()
        reals_split     = tf.split(reals, config.num_gpus)
        labels_split    = tf.split(labels, config.num_gpus)
    G_opt = tfutil.Optimizer(name='TrainG', learning_rate=lrate_in, **config.G_opt)
    D_opt = tfutil.Optimizer(name='TrainD', learning_rate=lrate_in, **config.D_opt)
    for gpu in range(config.num_gpus):
        with tf.name_scope('GPU%d' % gpu), tf.device('/gpu:%d' % gpu):
            G_gpu = G if gpu == 0 else G.clone(G.name + '_shadow')
            D_gpu = D if gpu == 0 else D.clone(D.name + '_shadow')
            lod_assign_ops = [tf.assign(G_gpu.find_var('lod'), lod_in), tf.assign(D_gpu.find_var('lod'), lod_in)]
            reals_gpu = process_reals(reals_split[gpu], lod_in, mirror_augment, training_set.dynamic_range, drange_net)
            labels_gpu = labels_split[gpu]
            with tf.name_scope('G_loss'), tf.control_dependencies(lod_assign_ops):
                G_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=G_opt, training_set=training_set, minibatch_size=minibatch_split, **config.G_loss)
            with tf.name_scope('D_loss'), tf.control_dependencies(lod_assign_ops):
                D_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=D_opt, training_set=training_set, minibatch_size=minibatch_split, reals=reals_gpu, labels=labels_gpu, **config.D_loss)
            G_opt.register_gradients(tf.reduce_mean(G_loss), G_gpu.trainables)
            D_opt.register_gradients(tf.reduce_mean(D_loss), D_gpu.trainables)
    G_train_op = G_opt.apply_updates()
    D_train_op = D_opt.apply_updates()

    print('Setting up snapshot image grid...')
    grid_size, grid_reals, grid_labels, grid_latents = setup_snapshot_image_grid(G, training_set, **config.grid)
    sched = TrainingSchedule(total_kimg * 1000, training_set, **config.sched)
    grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)

    print('Setting up result dir...')
    result_subdir = misc.create_result_subdir(config.result_dir, config.desc)
    misc.save_image_grid(grid_reals, os.path.join(result_subdir, 'reals.png'), drange=training_set.dynamic_range, grid_size=grid_size)
    misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % 0), drange=drange_net, grid_size=grid_size)
    summary_log = tf.summary.FileWriter(result_subdir)
    if save_tf_graph:
        summary_log.add_graph(tf.get_default_graph())
    if save_weight_histograms:
        G.setup_weight_histograms(); D.setup_weight_histograms()

    print('Training...')
    cur_nimg = int(resume_kimg * 1000)
    cur_tick = 0
    tick_start_nimg = cur_nimg
    tick_start_time = time.time()
    train_start_time = tick_start_time - resume_time
    prev_lod = -1.0
    while cur_nimg < total_kimg * 1000:

        # Choose training parameters and configure training ops.
        sched = TrainingSchedule(cur_nimg, training_set, **config.sched)
        training_set.configure(sched.minibatch, sched.lod)
        if reset_opt_for_new_lod:
            if np.floor(sched.lod) != np.floor(prev_lod) or np.ceil(sched.lod) != np.ceil(prev_lod):
                G_opt.reset_optimizer_state(); D_opt.reset_optimizer_state()
        prev_lod = sched.lod

        # Run training ops.
        for repeat in range(minibatch_repeats):
            for _ in range(D_repeats):
                tfutil.run([D_train_op, Gs_update_op], {lod_in: sched.lod, lrate_in: sched.D_lrate, minibatch_in: sched.minibatch})
                cur_nimg += sched.minibatch
            tfutil.run([G_train_op], {lod_in: sched.lod, lrate_in: sched.G_lrate, minibatch_in: sched.minibatch})

        # Perform maintenance tasks once per tick.
        done = (cur_nimg >= total_kimg * 1000)
        if cur_nimg >= tick_start_nimg + sched.tick_kimg * 1000 or done:
            cur_tick += 1
            cur_time = time.time()
            tick_kimg = (cur_nimg - tick_start_nimg) / 1000.0
            tick_start_nimg = cur_nimg
            tick_time = cur_time - tick_start_time
            total_time = cur_time - train_start_time
            maintenance_time = tick_start_time - maintenance_start_time
            maintenance_start_time = cur_time

            # Report progress.
            print('tick %-5d kimg %-8.1f lod %-5.2f minibatch %-4d time %-12s sec/tick %-7.1f sec/kimg %-7.2f maintenance %.1f' % (
                tfutil.autosummary('Progress/tick', cur_tick),
                tfutil.autosummary('Progress/kimg', cur_nimg / 1000.0),
                tfutil.autosummary('Progress/lod', sched.lod),
                tfutil.autosummary('Progress/minibatch', sched.minibatch),
                misc.format_time(tfutil.autosummary('Timing/total_sec', total_time)),
                tfutil.autosummary('Timing/sec_per_tick', tick_time),
                tfutil.autosummary('Timing/sec_per_kimg', tick_time / tick_kimg),
                tfutil.autosummary('Timing/maintenance_sec', maintenance_time)))
            tfutil.autosummary('Timing/total_hours', total_time / (60.0 * 60.0))
            tfutil.autosummary('Timing/total_days', total_time / (24.0 * 60.0 * 60.0))
            tfutil.save_summaries(summary_log, cur_nimg)

            # Save snapshots.
            if cur_tick % image_snapshot_ticks == 0 or done:
                grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)
                misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % (cur_nimg // 1000)), drange=drange_net, grid_size=grid_size)
            if cur_tick % network_snapshot_ticks == 0 or done:
                misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-snapshot-%06d.pkl' % (cur_nimg // 1000)))

            # Record start time of the next tick.
            tick_start_time = time.time()

    # Write final results.
    misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-final.pkl'))
    summary_log.close()
    open(os.path.join(result_subdir, '_training-done.txt'), 'wt').close()",np.floor(sched.lod) != np.floor(prev_lod),cannot be refactored by truth value test,Cannot refactor,1,"v1: np.floor(sched.lod)
v2: np.floor(prev_lod)"
interfacegan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/interfacegan/models/pggan_tf_official/train.py,https://github.com/genforce/interfacegan/tree/master/models/pggan_tf_official/train.py,,train_progressive_gan$133,"def train_progressive_gan(
    G_smoothing             = 0.999,        # Exponential running average of generator weights.
    D_repeats               = 1,            # How many times the discriminator is trained per G iteration.
    minibatch_repeats       = 4,            # Number of minibatches to run before adjusting training parameters.
    reset_opt_for_new_lod   = True,         # Reset optimizer internal state (e.g. Adam moments) when new layers are introduced?
    total_kimg              = 15000,        # Total length of the training, measured in thousands of real images.
    mirror_augment          = False,        # Enable mirror augment?
    drange_net              = [-1,1],       # Dynamic range used when feeding image data to the networks.
    image_snapshot_ticks    = 1,            # How often to export image snapshots?
    network_snapshot_ticks  = 10,           # How often to export network snapshots?
    save_tf_graph           = False,        # Include full TensorFlow computation graph in the tfevents file?
    save_weight_histograms  = False,        # Include weight histograms in the tfevents file?
    resume_run_id           = None,         # Run ID or network pkl to resume training from, None = start from scratch.
    resume_snapshot         = None,         # Snapshot index to resume training from, None = autodetect.
    resume_kimg             = 0.0,          # Assumed training progress at the beginning. Affects reporting and training schedule.
    resume_time             = 0.0):         # Assumed wallclock time at the beginning. Affects reporting.

    maintenance_start_time = time.time()
    training_set = dataset.load_dataset(data_dir=config.data_dir, verbose=True, **config.dataset)

    # Construct networks.
    with tf.device('/gpu:0'):
        if resume_run_id is not None:
            network_pkl = misc.locate_network_pkl(resume_run_id, resume_snapshot)
            print('Loading networks from ""%s""...' % network_pkl)
            G, D, Gs = misc.load_pkl(network_pkl)
        else:
            print('Constructing networks...')
            G = tfutil.Network('G', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.G)
            D = tfutil.Network('D', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.D)
            Gs = G.clone('Gs')
        Gs_update_op = Gs.setup_as_moving_average_of(G, beta=G_smoothing)
    G.print_layers(); D.print_layers()

    print('Building TensorFlow graph...')
    with tf.name_scope('Inputs'):
        lod_in          = tf.placeholder(tf.float32, name='lod_in', shape=[])
        lrate_in        = tf.placeholder(tf.float32, name='lrate_in', shape=[])
        minibatch_in    = tf.placeholder(tf.int32, name='minibatch_in', shape=[])
        minibatch_split = minibatch_in // config.num_gpus
        reals, labels   = training_set.get_minibatch_tf()
        reals_split     = tf.split(reals, config.num_gpus)
        labels_split    = tf.split(labels, config.num_gpus)
    G_opt = tfutil.Optimizer(name='TrainG', learning_rate=lrate_in, **config.G_opt)
    D_opt = tfutil.Optimizer(name='TrainD', learning_rate=lrate_in, **config.D_opt)
    for gpu in range(config.num_gpus):
        with tf.name_scope('GPU%d' % gpu), tf.device('/gpu:%d' % gpu):
            G_gpu = G if gpu == 0 else G.clone(G.name + '_shadow')
            D_gpu = D if gpu == 0 else D.clone(D.name + '_shadow')
            lod_assign_ops = [tf.assign(G_gpu.find_var('lod'), lod_in), tf.assign(D_gpu.find_var('lod'), lod_in)]
            reals_gpu = process_reals(reals_split[gpu], lod_in, mirror_augment, training_set.dynamic_range, drange_net)
            labels_gpu = labels_split[gpu]
            with tf.name_scope('G_loss'), tf.control_dependencies(lod_assign_ops):
                G_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=G_opt, training_set=training_set, minibatch_size=minibatch_split, **config.G_loss)
            with tf.name_scope('D_loss'), tf.control_dependencies(lod_assign_ops):
                D_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=D_opt, training_set=training_set, minibatch_size=minibatch_split, reals=reals_gpu, labels=labels_gpu, **config.D_loss)
            G_opt.register_gradients(tf.reduce_mean(G_loss), G_gpu.trainables)
            D_opt.register_gradients(tf.reduce_mean(D_loss), D_gpu.trainables)
    G_train_op = G_opt.apply_updates()
    D_train_op = D_opt.apply_updates()

    print('Setting up snapshot image grid...')
    grid_size, grid_reals, grid_labels, grid_latents = setup_snapshot_image_grid(G, training_set, **config.grid)
    sched = TrainingSchedule(total_kimg * 1000, training_set, **config.sched)
    grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)

    print('Setting up result dir...')
    result_subdir = misc.create_result_subdir(config.result_dir, config.desc)
    misc.save_image_grid(grid_reals, os.path.join(result_subdir, 'reals.png'), drange=training_set.dynamic_range, grid_size=grid_size)
    misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % 0), drange=drange_net, grid_size=grid_size)
    summary_log = tf.summary.FileWriter(result_subdir)
    if save_tf_graph:
        summary_log.add_graph(tf.get_default_graph())
    if save_weight_histograms:
        G.setup_weight_histograms(); D.setup_weight_histograms()

    print('Training...')
    cur_nimg = int(resume_kimg * 1000)
    cur_tick = 0
    tick_start_nimg = cur_nimg
    tick_start_time = time.time()
    train_start_time = tick_start_time - resume_time
    prev_lod = -1.0
    while cur_nimg < total_kimg * 1000:

        # Choose training parameters and configure training ops.
        sched = TrainingSchedule(cur_nimg, training_set, **config.sched)
        training_set.configure(sched.minibatch, sched.lod)
        if reset_opt_for_new_lod:
            if np.floor(sched.lod) != np.floor(prev_lod) or np.ceil(sched.lod) != np.ceil(prev_lod):
                G_opt.reset_optimizer_state(); D_opt.reset_optimizer_state()
        prev_lod = sched.lod

        # Run training ops.
        for repeat in range(minibatch_repeats):
            for _ in range(D_repeats):
                tfutil.run([D_train_op, Gs_update_op], {lod_in: sched.lod, lrate_in: sched.D_lrate, minibatch_in: sched.minibatch})
                cur_nimg += sched.minibatch
            tfutil.run([G_train_op], {lod_in: sched.lod, lrate_in: sched.G_lrate, minibatch_in: sched.minibatch})

        # Perform maintenance tasks once per tick.
        done = (cur_nimg >= total_kimg * 1000)
        if cur_nimg >= tick_start_nimg + sched.tick_kimg * 1000 or done:
            cur_tick += 1
            cur_time = time.time()
            tick_kimg = (cur_nimg - tick_start_nimg) / 1000.0
            tick_start_nimg = cur_nimg
            tick_time = cur_time - tick_start_time
            total_time = cur_time - train_start_time
            maintenance_time = tick_start_time - maintenance_start_time
            maintenance_start_time = cur_time

            # Report progress.
            print('tick %-5d kimg %-8.1f lod %-5.2f minibatch %-4d time %-12s sec/tick %-7.1f sec/kimg %-7.2f maintenance %.1f' % (
                tfutil.autosummary('Progress/tick', cur_tick),
                tfutil.autosummary('Progress/kimg', cur_nimg / 1000.0),
                tfutil.autosummary('Progress/lod', sched.lod),
                tfutil.autosummary('Progress/minibatch', sched.minibatch),
                misc.format_time(tfutil.autosummary('Timing/total_sec', total_time)),
                tfutil.autosummary('Timing/sec_per_tick', tick_time),
                tfutil.autosummary('Timing/sec_per_kimg', tick_time / tick_kimg),
                tfutil.autosummary('Timing/maintenance_sec', maintenance_time)))
            tfutil.autosummary('Timing/total_hours', total_time / (60.0 * 60.0))
            tfutil.autosummary('Timing/total_days', total_time / (24.0 * 60.0 * 60.0))
            tfutil.save_summaries(summary_log, cur_nimg)

            # Save snapshots.
            if cur_tick % image_snapshot_ticks == 0 or done:
                grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)
                misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % (cur_nimg // 1000)), drange=drange_net, grid_size=grid_size)
            if cur_tick % network_snapshot_ticks == 0 or done:
                misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-snapshot-%06d.pkl' % (cur_nimg // 1000)))

            # Record start time of the next tick.
            tick_start_time = time.time()

    # Write final results.
    misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-final.pkl'))
    summary_log.close()
    open(os.path.join(result_subdir, '_training-done.txt'), 'wt').close()",np.ceil(sched.lod) != np.ceil(prev_lod),cannot be refactored by truth value test,Cannot refactor,1,"v1: np.ceil(sched.lod)
v2: np.ceil(prev_lod)"
interfacegan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/interfacegan/models/pggan_tf_official/train.py,https://github.com/genforce/interfacegan/tree/master/models/pggan_tf_official/train.py,,train_progressive_gan$133,"def train_progressive_gan(
    G_smoothing             = 0.999,        # Exponential running average of generator weights.
    D_repeats               = 1,            # How many times the discriminator is trained per G iteration.
    minibatch_repeats       = 4,            # Number of minibatches to run before adjusting training parameters.
    reset_opt_for_new_lod   = True,         # Reset optimizer internal state (e.g. Adam moments) when new layers are introduced?
    total_kimg              = 15000,        # Total length of the training, measured in thousands of real images.
    mirror_augment          = False,        # Enable mirror augment?
    drange_net              = [-1,1],       # Dynamic range used when feeding image data to the networks.
    image_snapshot_ticks    = 1,            # How often to export image snapshots?
    network_snapshot_ticks  = 10,           # How often to export network snapshots?
    save_tf_graph           = False,        # Include full TensorFlow computation graph in the tfevents file?
    save_weight_histograms  = False,        # Include weight histograms in the tfevents file?
    resume_run_id           = None,         # Run ID or network pkl to resume training from, None = start from scratch.
    resume_snapshot         = None,         # Snapshot index to resume training from, None = autodetect.
    resume_kimg             = 0.0,          # Assumed training progress at the beginning. Affects reporting and training schedule.
    resume_time             = 0.0):         # Assumed wallclock time at the beginning. Affects reporting.

    maintenance_start_time = time.time()
    training_set = dataset.load_dataset(data_dir=config.data_dir, verbose=True, **config.dataset)

    # Construct networks.
    with tf.device('/gpu:0'):
        if resume_run_id is not None:
            network_pkl = misc.locate_network_pkl(resume_run_id, resume_snapshot)
            print('Loading networks from ""%s""...' % network_pkl)
            G, D, Gs = misc.load_pkl(network_pkl)
        else:
            print('Constructing networks...')
            G = tfutil.Network('G', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.G)
            D = tfutil.Network('D', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.D)
            Gs = G.clone('Gs')
        Gs_update_op = Gs.setup_as_moving_average_of(G, beta=G_smoothing)
    G.print_layers(); D.print_layers()

    print('Building TensorFlow graph...')
    with tf.name_scope('Inputs'):
        lod_in          = tf.placeholder(tf.float32, name='lod_in', shape=[])
        lrate_in        = tf.placeholder(tf.float32, name='lrate_in', shape=[])
        minibatch_in    = tf.placeholder(tf.int32, name='minibatch_in', shape=[])
        minibatch_split = minibatch_in // config.num_gpus
        reals, labels   = training_set.get_minibatch_tf()
        reals_split     = tf.split(reals, config.num_gpus)
        labels_split    = tf.split(labels, config.num_gpus)
    G_opt = tfutil.Optimizer(name='TrainG', learning_rate=lrate_in, **config.G_opt)
    D_opt = tfutil.Optimizer(name='TrainD', learning_rate=lrate_in, **config.D_opt)
    for gpu in range(config.num_gpus):
        with tf.name_scope('GPU%d' % gpu), tf.device('/gpu:%d' % gpu):
            G_gpu = G if gpu == 0 else G.clone(G.name + '_shadow')
            D_gpu = D if gpu == 0 else D.clone(D.name + '_shadow')
            lod_assign_ops = [tf.assign(G_gpu.find_var('lod'), lod_in), tf.assign(D_gpu.find_var('lod'), lod_in)]
            reals_gpu = process_reals(reals_split[gpu], lod_in, mirror_augment, training_set.dynamic_range, drange_net)
            labels_gpu = labels_split[gpu]
            with tf.name_scope('G_loss'), tf.control_dependencies(lod_assign_ops):
                G_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=G_opt, training_set=training_set, minibatch_size=minibatch_split, **config.G_loss)
            with tf.name_scope('D_loss'), tf.control_dependencies(lod_assign_ops):
                D_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=D_opt, training_set=training_set, minibatch_size=minibatch_split, reals=reals_gpu, labels=labels_gpu, **config.D_loss)
            G_opt.register_gradients(tf.reduce_mean(G_loss), G_gpu.trainables)
            D_opt.register_gradients(tf.reduce_mean(D_loss), D_gpu.trainables)
    G_train_op = G_opt.apply_updates()
    D_train_op = D_opt.apply_updates()

    print('Setting up snapshot image grid...')
    grid_size, grid_reals, grid_labels, grid_latents = setup_snapshot_image_grid(G, training_set, **config.grid)
    sched = TrainingSchedule(total_kimg * 1000, training_set, **config.sched)
    grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)

    print('Setting up result dir...')
    result_subdir = misc.create_result_subdir(config.result_dir, config.desc)
    misc.save_image_grid(grid_reals, os.path.join(result_subdir, 'reals.png'), drange=training_set.dynamic_range, grid_size=grid_size)
    misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % 0), drange=drange_net, grid_size=grid_size)
    summary_log = tf.summary.FileWriter(result_subdir)
    if save_tf_graph:
        summary_log.add_graph(tf.get_default_graph())
    if save_weight_histograms:
        G.setup_weight_histograms(); D.setup_weight_histograms()

    print('Training...')
    cur_nimg = int(resume_kimg * 1000)
    cur_tick = 0
    tick_start_nimg = cur_nimg
    tick_start_time = time.time()
    train_start_time = tick_start_time - resume_time
    prev_lod = -1.0
    while cur_nimg < total_kimg * 1000:

        # Choose training parameters and configure training ops.
        sched = TrainingSchedule(cur_nimg, training_set, **config.sched)
        training_set.configure(sched.minibatch, sched.lod)
        if reset_opt_for_new_lod:
            if np.floor(sched.lod) != np.floor(prev_lod) or np.ceil(sched.lod) != np.ceil(prev_lod):
                G_opt.reset_optimizer_state(); D_opt.reset_optimizer_state()
        prev_lod = sched.lod

        # Run training ops.
        for repeat in range(minibatch_repeats):
            for _ in range(D_repeats):
                tfutil.run([D_train_op, Gs_update_op], {lod_in: sched.lod, lrate_in: sched.D_lrate, minibatch_in: sched.minibatch})
                cur_nimg += sched.minibatch
            tfutil.run([G_train_op], {lod_in: sched.lod, lrate_in: sched.G_lrate, minibatch_in: sched.minibatch})

        # Perform maintenance tasks once per tick.
        done = (cur_nimg >= total_kimg * 1000)
        if cur_nimg >= tick_start_nimg + sched.tick_kimg * 1000 or done:
            cur_tick += 1
            cur_time = time.time()
            tick_kimg = (cur_nimg - tick_start_nimg) / 1000.0
            tick_start_nimg = cur_nimg
            tick_time = cur_time - tick_start_time
            total_time = cur_time - train_start_time
            maintenance_time = tick_start_time - maintenance_start_time
            maintenance_start_time = cur_time

            # Report progress.
            print('tick %-5d kimg %-8.1f lod %-5.2f minibatch %-4d time %-12s sec/tick %-7.1f sec/kimg %-7.2f maintenance %.1f' % (
                tfutil.autosummary('Progress/tick', cur_tick),
                tfutil.autosummary('Progress/kimg', cur_nimg / 1000.0),
                tfutil.autosummary('Progress/lod', sched.lod),
                tfutil.autosummary('Progress/minibatch', sched.minibatch),
                misc.format_time(tfutil.autosummary('Timing/total_sec', total_time)),
                tfutil.autosummary('Timing/sec_per_tick', tick_time),
                tfutil.autosummary('Timing/sec_per_kimg', tick_time / tick_kimg),
                tfutil.autosummary('Timing/maintenance_sec', maintenance_time)))
            tfutil.autosummary('Timing/total_hours', total_time / (60.0 * 60.0))
            tfutil.autosummary('Timing/total_days', total_time / (24.0 * 60.0 * 60.0))
            tfutil.save_summaries(summary_log, cur_nimg)

            # Save snapshots.
            if cur_tick % image_snapshot_ticks == 0 or done:
                grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)
                misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % (cur_nimg // 1000)), drange=drange_net, grid_size=grid_size)
            if cur_tick % network_snapshot_ticks == 0 or done:
                misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-snapshot-%06d.pkl' % (cur_nimg // 1000)))

            # Record start time of the next tick.
            tick_start_time = time.time()

    # Write final results.
    misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-final.pkl'))
    summary_log.close()
    open(os.path.join(result_subdir, '_training-done.txt'), 'wt').close()",cur_tick % network_snapshot_ticks == 0,not cur_tick % network_snapshot_ticks,not cur_tick % network_snapshot_ticks,1,"v1: cur_tick % network_snapshot_ticks
v2: 0"
interfacegan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/interfacegan/models/pggan_tf_official/train.py,https://github.com/genforce/interfacegan/tree/master/models/pggan_tf_official/train.py,,train_progressive_gan$133,"def train_progressive_gan(
    G_smoothing             = 0.999,        # Exponential running average of generator weights.
    D_repeats               = 1,            # How many times the discriminator is trained per G iteration.
    minibatch_repeats       = 4,            # Number of minibatches to run before adjusting training parameters.
    reset_opt_for_new_lod   = True,         # Reset optimizer internal state (e.g. Adam moments) when new layers are introduced?
    total_kimg              = 15000,        # Total length of the training, measured in thousands of real images.
    mirror_augment          = False,        # Enable mirror augment?
    drange_net              = [-1,1],       # Dynamic range used when feeding image data to the networks.
    image_snapshot_ticks    = 1,            # How often to export image snapshots?
    network_snapshot_ticks  = 10,           # How often to export network snapshots?
    save_tf_graph           = False,        # Include full TensorFlow computation graph in the tfevents file?
    save_weight_histograms  = False,        # Include weight histograms in the tfevents file?
    resume_run_id           = None,         # Run ID or network pkl to resume training from, None = start from scratch.
    resume_snapshot         = None,         # Snapshot index to resume training from, None = autodetect.
    resume_kimg             = 0.0,          # Assumed training progress at the beginning. Affects reporting and training schedule.
    resume_time             = 0.0):         # Assumed wallclock time at the beginning. Affects reporting.

    maintenance_start_time = time.time()
    training_set = dataset.load_dataset(data_dir=config.data_dir, verbose=True, **config.dataset)

    # Construct networks.
    with tf.device('/gpu:0'):
        if resume_run_id is not None:
            network_pkl = misc.locate_network_pkl(resume_run_id, resume_snapshot)
            print('Loading networks from ""%s""...' % network_pkl)
            G, D, Gs = misc.load_pkl(network_pkl)
        else:
            print('Constructing networks...')
            G = tfutil.Network('G', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.G)
            D = tfutil.Network('D', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.D)
            Gs = G.clone('Gs')
        Gs_update_op = Gs.setup_as_moving_average_of(G, beta=G_smoothing)
    G.print_layers(); D.print_layers()

    print('Building TensorFlow graph...')
    with tf.name_scope('Inputs'):
        lod_in          = tf.placeholder(tf.float32, name='lod_in', shape=[])
        lrate_in        = tf.placeholder(tf.float32, name='lrate_in', shape=[])
        minibatch_in    = tf.placeholder(tf.int32, name='minibatch_in', shape=[])
        minibatch_split = minibatch_in // config.num_gpus
        reals, labels   = training_set.get_minibatch_tf()
        reals_split     = tf.split(reals, config.num_gpus)
        labels_split    = tf.split(labels, config.num_gpus)
    G_opt = tfutil.Optimizer(name='TrainG', learning_rate=lrate_in, **config.G_opt)
    D_opt = tfutil.Optimizer(name='TrainD', learning_rate=lrate_in, **config.D_opt)
    for gpu in range(config.num_gpus):
        with tf.name_scope('GPU%d' % gpu), tf.device('/gpu:%d' % gpu):
            G_gpu = G if gpu == 0 else G.clone(G.name + '_shadow')
            D_gpu = D if gpu == 0 else D.clone(D.name + '_shadow')
            lod_assign_ops = [tf.assign(G_gpu.find_var('lod'), lod_in), tf.assign(D_gpu.find_var('lod'), lod_in)]
            reals_gpu = process_reals(reals_split[gpu], lod_in, mirror_augment, training_set.dynamic_range, drange_net)
            labels_gpu = labels_split[gpu]
            with tf.name_scope('G_loss'), tf.control_dependencies(lod_assign_ops):
                G_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=G_opt, training_set=training_set, minibatch_size=minibatch_split, **config.G_loss)
            with tf.name_scope('D_loss'), tf.control_dependencies(lod_assign_ops):
                D_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=D_opt, training_set=training_set, minibatch_size=minibatch_split, reals=reals_gpu, labels=labels_gpu, **config.D_loss)
            G_opt.register_gradients(tf.reduce_mean(G_loss), G_gpu.trainables)
            D_opt.register_gradients(tf.reduce_mean(D_loss), D_gpu.trainables)
    G_train_op = G_opt.apply_updates()
    D_train_op = D_opt.apply_updates()

    print('Setting up snapshot image grid...')
    grid_size, grid_reals, grid_labels, grid_latents = setup_snapshot_image_grid(G, training_set, **config.grid)
    sched = TrainingSchedule(total_kimg * 1000, training_set, **config.sched)
    grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)

    print('Setting up result dir...')
    result_subdir = misc.create_result_subdir(config.result_dir, config.desc)
    misc.save_image_grid(grid_reals, os.path.join(result_subdir, 'reals.png'), drange=training_set.dynamic_range, grid_size=grid_size)
    misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % 0), drange=drange_net, grid_size=grid_size)
    summary_log = tf.summary.FileWriter(result_subdir)
    if save_tf_graph:
        summary_log.add_graph(tf.get_default_graph())
    if save_weight_histograms:
        G.setup_weight_histograms(); D.setup_weight_histograms()

    print('Training...')
    cur_nimg = int(resume_kimg * 1000)
    cur_tick = 0
    tick_start_nimg = cur_nimg
    tick_start_time = time.time()
    train_start_time = tick_start_time - resume_time
    prev_lod = -1.0
    while cur_nimg < total_kimg * 1000:

        # Choose training parameters and configure training ops.
        sched = TrainingSchedule(cur_nimg, training_set, **config.sched)
        training_set.configure(sched.minibatch, sched.lod)
        if reset_opt_for_new_lod:
            if np.floor(sched.lod) != np.floor(prev_lod) or np.ceil(sched.lod) != np.ceil(prev_lod):
                G_opt.reset_optimizer_state(); D_opt.reset_optimizer_state()
        prev_lod = sched.lod

        # Run training ops.
        for repeat in range(minibatch_repeats):
            for _ in range(D_repeats):
                tfutil.run([D_train_op, Gs_update_op], {lod_in: sched.lod, lrate_in: sched.D_lrate, minibatch_in: sched.minibatch})
                cur_nimg += sched.minibatch
            tfutil.run([G_train_op], {lod_in: sched.lod, lrate_in: sched.G_lrate, minibatch_in: sched.minibatch})

        # Perform maintenance tasks once per tick.
        done = (cur_nimg >= total_kimg * 1000)
        if cur_nimg >= tick_start_nimg + sched.tick_kimg * 1000 or done:
            cur_tick += 1
            cur_time = time.time()
            tick_kimg = (cur_nimg - tick_start_nimg) / 1000.0
            tick_start_nimg = cur_nimg
            tick_time = cur_time - tick_start_time
            total_time = cur_time - train_start_time
            maintenance_time = tick_start_time - maintenance_start_time
            maintenance_start_time = cur_time

            # Report progress.
            print('tick %-5d kimg %-8.1f lod %-5.2f minibatch %-4d time %-12s sec/tick %-7.1f sec/kimg %-7.2f maintenance %.1f' % (
                tfutil.autosummary('Progress/tick', cur_tick),
                tfutil.autosummary('Progress/kimg', cur_nimg / 1000.0),
                tfutil.autosummary('Progress/lod', sched.lod),
                tfutil.autosummary('Progress/minibatch', sched.minibatch),
                misc.format_time(tfutil.autosummary('Timing/total_sec', total_time)),
                tfutil.autosummary('Timing/sec_per_tick', tick_time),
                tfutil.autosummary('Timing/sec_per_kimg', tick_time / tick_kimg),
                tfutil.autosummary('Timing/maintenance_sec', maintenance_time)))
            tfutil.autosummary('Timing/total_hours', total_time / (60.0 * 60.0))
            tfutil.autosummary('Timing/total_days', total_time / (24.0 * 60.0 * 60.0))
            tfutil.save_summaries(summary_log, cur_nimg)

            # Save snapshots.
            if cur_tick % image_snapshot_ticks == 0 or done:
                grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)
                misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % (cur_nimg // 1000)), drange=drange_net, grid_size=grid_size)
            if cur_tick % network_snapshot_ticks == 0 or done:
                misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-snapshot-%06d.pkl' % (cur_nimg // 1000)))

            # Record start time of the next tick.
            tick_start_time = time.time()

    # Write final results.
    misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-final.pkl'))
    summary_log.close()
    open(os.path.join(result_subdir, '_training-done.txt'), 'wt').close()",gpu == 0,not gpu,Cannot refactor,1,"v1: gpu
v2: 0"
interfacegan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/interfacegan/models/pggan_tf_official/train.py,https://github.com/genforce/interfacegan/tree/master/models/pggan_tf_official/train.py,,train_progressive_gan$133,"def train_progressive_gan(
    G_smoothing             = 0.999,        # Exponential running average of generator weights.
    D_repeats               = 1,            # How many times the discriminator is trained per G iteration.
    minibatch_repeats       = 4,            # Number of minibatches to run before adjusting training parameters.
    reset_opt_for_new_lod   = True,         # Reset optimizer internal state (e.g. Adam moments) when new layers are introduced?
    total_kimg              = 15000,        # Total length of the training, measured in thousands of real images.
    mirror_augment          = False,        # Enable mirror augment?
    drange_net              = [-1,1],       # Dynamic range used when feeding image data to the networks.
    image_snapshot_ticks    = 1,            # How often to export image snapshots?
    network_snapshot_ticks  = 10,           # How often to export network snapshots?
    save_tf_graph           = False,        # Include full TensorFlow computation graph in the tfevents file?
    save_weight_histograms  = False,        # Include weight histograms in the tfevents file?
    resume_run_id           = None,         # Run ID or network pkl to resume training from, None = start from scratch.
    resume_snapshot         = None,         # Snapshot index to resume training from, None = autodetect.
    resume_kimg             = 0.0,          # Assumed training progress at the beginning. Affects reporting and training schedule.
    resume_time             = 0.0):         # Assumed wallclock time at the beginning. Affects reporting.

    maintenance_start_time = time.time()
    training_set = dataset.load_dataset(data_dir=config.data_dir, verbose=True, **config.dataset)

    # Construct networks.
    with tf.device('/gpu:0'):
        if resume_run_id is not None:
            network_pkl = misc.locate_network_pkl(resume_run_id, resume_snapshot)
            print('Loading networks from ""%s""...' % network_pkl)
            G, D, Gs = misc.load_pkl(network_pkl)
        else:
            print('Constructing networks...')
            G = tfutil.Network('G', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.G)
            D = tfutil.Network('D', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.D)
            Gs = G.clone('Gs')
        Gs_update_op = Gs.setup_as_moving_average_of(G, beta=G_smoothing)
    G.print_layers(); D.print_layers()

    print('Building TensorFlow graph...')
    with tf.name_scope('Inputs'):
        lod_in          = tf.placeholder(tf.float32, name='lod_in', shape=[])
        lrate_in        = tf.placeholder(tf.float32, name='lrate_in', shape=[])
        minibatch_in    = tf.placeholder(tf.int32, name='minibatch_in', shape=[])
        minibatch_split = minibatch_in // config.num_gpus
        reals, labels   = training_set.get_minibatch_tf()
        reals_split     = tf.split(reals, config.num_gpus)
        labels_split    = tf.split(labels, config.num_gpus)
    G_opt = tfutil.Optimizer(name='TrainG', learning_rate=lrate_in, **config.G_opt)
    D_opt = tfutil.Optimizer(name='TrainD', learning_rate=lrate_in, **config.D_opt)
    for gpu in range(config.num_gpus):
        with tf.name_scope('GPU%d' % gpu), tf.device('/gpu:%d' % gpu):
            G_gpu = G if gpu == 0 else G.clone(G.name + '_shadow')
            D_gpu = D if gpu == 0 else D.clone(D.name + '_shadow')
            lod_assign_ops = [tf.assign(G_gpu.find_var('lod'), lod_in), tf.assign(D_gpu.find_var('lod'), lod_in)]
            reals_gpu = process_reals(reals_split[gpu], lod_in, mirror_augment, training_set.dynamic_range, drange_net)
            labels_gpu = labels_split[gpu]
            with tf.name_scope('G_loss'), tf.control_dependencies(lod_assign_ops):
                G_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=G_opt, training_set=training_set, minibatch_size=minibatch_split, **config.G_loss)
            with tf.name_scope('D_loss'), tf.control_dependencies(lod_assign_ops):
                D_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=D_opt, training_set=training_set, minibatch_size=minibatch_split, reals=reals_gpu, labels=labels_gpu, **config.D_loss)
            G_opt.register_gradients(tf.reduce_mean(G_loss), G_gpu.trainables)
            D_opt.register_gradients(tf.reduce_mean(D_loss), D_gpu.trainables)
    G_train_op = G_opt.apply_updates()
    D_train_op = D_opt.apply_updates()

    print('Setting up snapshot image grid...')
    grid_size, grid_reals, grid_labels, grid_latents = setup_snapshot_image_grid(G, training_set, **config.grid)
    sched = TrainingSchedule(total_kimg * 1000, training_set, **config.sched)
    grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)

    print('Setting up result dir...')
    result_subdir = misc.create_result_subdir(config.result_dir, config.desc)
    misc.save_image_grid(grid_reals, os.path.join(result_subdir, 'reals.png'), drange=training_set.dynamic_range, grid_size=grid_size)
    misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % 0), drange=drange_net, grid_size=grid_size)
    summary_log = tf.summary.FileWriter(result_subdir)
    if save_tf_graph:
        summary_log.add_graph(tf.get_default_graph())
    if save_weight_histograms:
        G.setup_weight_histograms(); D.setup_weight_histograms()

    print('Training...')
    cur_nimg = int(resume_kimg * 1000)
    cur_tick = 0
    tick_start_nimg = cur_nimg
    tick_start_time = time.time()
    train_start_time = tick_start_time - resume_time
    prev_lod = -1.0
    while cur_nimg < total_kimg * 1000:

        # Choose training parameters and configure training ops.
        sched = TrainingSchedule(cur_nimg, training_set, **config.sched)
        training_set.configure(sched.minibatch, sched.lod)
        if reset_opt_for_new_lod:
            if np.floor(sched.lod) != np.floor(prev_lod) or np.ceil(sched.lod) != np.ceil(prev_lod):
                G_opt.reset_optimizer_state(); D_opt.reset_optimizer_state()
        prev_lod = sched.lod

        # Run training ops.
        for repeat in range(minibatch_repeats):
            for _ in range(D_repeats):
                tfutil.run([D_train_op, Gs_update_op], {lod_in: sched.lod, lrate_in: sched.D_lrate, minibatch_in: sched.minibatch})
                cur_nimg += sched.minibatch
            tfutil.run([G_train_op], {lod_in: sched.lod, lrate_in: sched.G_lrate, minibatch_in: sched.minibatch})

        # Perform maintenance tasks once per tick.
        done = (cur_nimg >= total_kimg * 1000)
        if cur_nimg >= tick_start_nimg + sched.tick_kimg * 1000 or done:
            cur_tick += 1
            cur_time = time.time()
            tick_kimg = (cur_nimg - tick_start_nimg) / 1000.0
            tick_start_nimg = cur_nimg
            tick_time = cur_time - tick_start_time
            total_time = cur_time - train_start_time
            maintenance_time = tick_start_time - maintenance_start_time
            maintenance_start_time = cur_time

            # Report progress.
            print('tick %-5d kimg %-8.1f lod %-5.2f minibatch %-4d time %-12s sec/tick %-7.1f sec/kimg %-7.2f maintenance %.1f' % (
                tfutil.autosummary('Progress/tick', cur_tick),
                tfutil.autosummary('Progress/kimg', cur_nimg / 1000.0),
                tfutil.autosummary('Progress/lod', sched.lod),
                tfutil.autosummary('Progress/minibatch', sched.minibatch),
                misc.format_time(tfutil.autosummary('Timing/total_sec', total_time)),
                tfutil.autosummary('Timing/sec_per_tick', tick_time),
                tfutil.autosummary('Timing/sec_per_kimg', tick_time / tick_kimg),
                tfutil.autosummary('Timing/maintenance_sec', maintenance_time)))
            tfutil.autosummary('Timing/total_hours', total_time / (60.0 * 60.0))
            tfutil.autosummary('Timing/total_days', total_time / (24.0 * 60.0 * 60.0))
            tfutil.save_summaries(summary_log, cur_nimg)

            # Save snapshots.
            if cur_tick % image_snapshot_ticks == 0 or done:
                grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)
                misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % (cur_nimg // 1000)), drange=drange_net, grid_size=grid_size)
            if cur_tick % network_snapshot_ticks == 0 or done:
                misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-snapshot-%06d.pkl' % (cur_nimg // 1000)))

            # Record start time of the next tick.
            tick_start_time = time.time()

    # Write final results.
    misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-final.pkl'))
    summary_log.close()
    open(os.path.join(result_subdir, '_training-done.txt'), 'wt').close()",gpu == 0,not gpu,Cannot refactor,1,"v1: gpu
v2: 0"
interfacegan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/interfacegan/models/pggan_tf_official/train.py,https://github.com/genforce/interfacegan/tree/master/models/pggan_tf_official/train.py,,train_progressive_gan$133,"def train_progressive_gan(
    G_smoothing             = 0.999,        # Exponential running average of generator weights.
    D_repeats               = 1,            # How many times the discriminator is trained per G iteration.
    minibatch_repeats       = 4,            # Number of minibatches to run before adjusting training parameters.
    reset_opt_for_new_lod   = True,         # Reset optimizer internal state (e.g. Adam moments) when new layers are introduced?
    total_kimg              = 15000,        # Total length of the training, measured in thousands of real images.
    mirror_augment          = False,        # Enable mirror augment?
    drange_net              = [-1,1],       # Dynamic range used when feeding image data to the networks.
    image_snapshot_ticks    = 1,            # How often to export image snapshots?
    network_snapshot_ticks  = 10,           # How often to export network snapshots?
    save_tf_graph           = False,        # Include full TensorFlow computation graph in the tfevents file?
    save_weight_histograms  = False,        # Include weight histograms in the tfevents file?
    resume_run_id           = None,         # Run ID or network pkl to resume training from, None = start from scratch.
    resume_snapshot         = None,         # Snapshot index to resume training from, None = autodetect.
    resume_kimg             = 0.0,          # Assumed training progress at the beginning. Affects reporting and training schedule.
    resume_time             = 0.0):         # Assumed wallclock time at the beginning. Affects reporting.

    maintenance_start_time = time.time()
    training_set = dataset.load_dataset(data_dir=config.data_dir, verbose=True, **config.dataset)

    # Construct networks.
    with tf.device('/gpu:0'):
        if resume_run_id is not None:
            network_pkl = misc.locate_network_pkl(resume_run_id, resume_snapshot)
            print('Loading networks from ""%s""...' % network_pkl)
            G, D, Gs = misc.load_pkl(network_pkl)
        else:
            print('Constructing networks...')
            G = tfutil.Network('G', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.G)
            D = tfutil.Network('D', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.D)
            Gs = G.clone('Gs')
        Gs_update_op = Gs.setup_as_moving_average_of(G, beta=G_smoothing)
    G.print_layers(); D.print_layers()

    print('Building TensorFlow graph...')
    with tf.name_scope('Inputs'):
        lod_in          = tf.placeholder(tf.float32, name='lod_in', shape=[])
        lrate_in        = tf.placeholder(tf.float32, name='lrate_in', shape=[])
        minibatch_in    = tf.placeholder(tf.int32, name='minibatch_in', shape=[])
        minibatch_split = minibatch_in // config.num_gpus
        reals, labels   = training_set.get_minibatch_tf()
        reals_split     = tf.split(reals, config.num_gpus)
        labels_split    = tf.split(labels, config.num_gpus)
    G_opt = tfutil.Optimizer(name='TrainG', learning_rate=lrate_in, **config.G_opt)
    D_opt = tfutil.Optimizer(name='TrainD', learning_rate=lrate_in, **config.D_opt)
    for gpu in range(config.num_gpus):
        with tf.name_scope('GPU%d' % gpu), tf.device('/gpu:%d' % gpu):
            G_gpu = G if gpu == 0 else G.clone(G.name + '_shadow')
            D_gpu = D if gpu == 0 else D.clone(D.name + '_shadow')
            lod_assign_ops = [tf.assign(G_gpu.find_var('lod'), lod_in), tf.assign(D_gpu.find_var('lod'), lod_in)]
            reals_gpu = process_reals(reals_split[gpu], lod_in, mirror_augment, training_set.dynamic_range, drange_net)
            labels_gpu = labels_split[gpu]
            with tf.name_scope('G_loss'), tf.control_dependencies(lod_assign_ops):
                G_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=G_opt, training_set=training_set, minibatch_size=minibatch_split, **config.G_loss)
            with tf.name_scope('D_loss'), tf.control_dependencies(lod_assign_ops):
                D_loss = tfutil.call_func_by_name(G=G_gpu, D=D_gpu, opt=D_opt, training_set=training_set, minibatch_size=minibatch_split, reals=reals_gpu, labels=labels_gpu, **config.D_loss)
            G_opt.register_gradients(tf.reduce_mean(G_loss), G_gpu.trainables)
            D_opt.register_gradients(tf.reduce_mean(D_loss), D_gpu.trainables)
    G_train_op = G_opt.apply_updates()
    D_train_op = D_opt.apply_updates()

    print('Setting up snapshot image grid...')
    grid_size, grid_reals, grid_labels, grid_latents = setup_snapshot_image_grid(G, training_set, **config.grid)
    sched = TrainingSchedule(total_kimg * 1000, training_set, **config.sched)
    grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)

    print('Setting up result dir...')
    result_subdir = misc.create_result_subdir(config.result_dir, config.desc)
    misc.save_image_grid(grid_reals, os.path.join(result_subdir, 'reals.png'), drange=training_set.dynamic_range, grid_size=grid_size)
    misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % 0), drange=drange_net, grid_size=grid_size)
    summary_log = tf.summary.FileWriter(result_subdir)
    if save_tf_graph:
        summary_log.add_graph(tf.get_default_graph())
    if save_weight_histograms:
        G.setup_weight_histograms(); D.setup_weight_histograms()

    print('Training...')
    cur_nimg = int(resume_kimg * 1000)
    cur_tick = 0
    tick_start_nimg = cur_nimg
    tick_start_time = time.time()
    train_start_time = tick_start_time - resume_time
    prev_lod = -1.0
    while cur_nimg < total_kimg * 1000:

        # Choose training parameters and configure training ops.
        sched = TrainingSchedule(cur_nimg, training_set, **config.sched)
        training_set.configure(sched.minibatch, sched.lod)
        if reset_opt_for_new_lod:
            if np.floor(sched.lod) != np.floor(prev_lod) or np.ceil(sched.lod) != np.ceil(prev_lod):
                G_opt.reset_optimizer_state(); D_opt.reset_optimizer_state()
        prev_lod = sched.lod

        # Run training ops.
        for repeat in range(minibatch_repeats):
            for _ in range(D_repeats):
                tfutil.run([D_train_op, Gs_update_op], {lod_in: sched.lod, lrate_in: sched.D_lrate, minibatch_in: sched.minibatch})
                cur_nimg += sched.minibatch
            tfutil.run([G_train_op], {lod_in: sched.lod, lrate_in: sched.G_lrate, minibatch_in: sched.minibatch})

        # Perform maintenance tasks once per tick.
        done = (cur_nimg >= total_kimg * 1000)
        if cur_nimg >= tick_start_nimg + sched.tick_kimg * 1000 or done:
            cur_tick += 1
            cur_time = time.time()
            tick_kimg = (cur_nimg - tick_start_nimg) / 1000.0
            tick_start_nimg = cur_nimg
            tick_time = cur_time - tick_start_time
            total_time = cur_time - train_start_time
            maintenance_time = tick_start_time - maintenance_start_time
            maintenance_start_time = cur_time

            # Report progress.
            print('tick %-5d kimg %-8.1f lod %-5.2f minibatch %-4d time %-12s sec/tick %-7.1f sec/kimg %-7.2f maintenance %.1f' % (
                tfutil.autosummary('Progress/tick', cur_tick),
                tfutil.autosummary('Progress/kimg', cur_nimg / 1000.0),
                tfutil.autosummary('Progress/lod', sched.lod),
                tfutil.autosummary('Progress/minibatch', sched.minibatch),
                misc.format_time(tfutil.autosummary('Timing/total_sec', total_time)),
                tfutil.autosummary('Timing/sec_per_tick', tick_time),
                tfutil.autosummary('Timing/sec_per_kimg', tick_time / tick_kimg),
                tfutil.autosummary('Timing/maintenance_sec', maintenance_time)))
            tfutil.autosummary('Timing/total_hours', total_time / (60.0 * 60.0))
            tfutil.autosummary('Timing/total_days', total_time / (24.0 * 60.0 * 60.0))
            tfutil.save_summaries(summary_log, cur_nimg)

            # Save snapshots.
            if cur_tick % image_snapshot_ticks == 0 or done:
                grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)
                misc.save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % (cur_nimg // 1000)), drange=drange_net, grid_size=grid_size)
            if cur_tick % network_snapshot_ticks == 0 or done:
                misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-snapshot-%06d.pkl' % (cur_nimg // 1000)))

            # Record start time of the next tick.
            tick_start_time = time.time()

    # Write final results.
    misc.save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-final.pkl'))
    summary_log.close()
    open(os.path.join(result_subdir, '_training-done.txt'), 'wt').close()",cur_tick % image_snapshot_ticks == 0,not cur_tick % image_snapshot_ticks,not cur_tick % image_snapshot_ticks,1,"v1: cur_tick % image_snapshot_ticks
v2: 0"
mbrl-lib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mbrl-lib/tests/core/test_replay_buffer.py,https://github.com/facebookresearch/mbrl-lib/tree/master/tests/core/test_replay_buffer.py,,test_sac_buffer_batched_add$118,"def test_sac_buffer_batched_add():
    def compare_batch_to_buffer_slice(
        start_idx, batch_size, obs, act, next_obs, reward, done
    ):
        for i in range(batch_size):
            buffer_idx = (start_idx + i) % buffer.capacity
            np.testing.assert_array_equal(buffer.obses[buffer_idx], obs[i])
            np.testing.assert_array_equal(buffer.actions[buffer_idx], act[i])
            np.testing.assert_array_equal(buffer.next_obses[buffer_idx], next_obs[i])
            np.testing.assert_array_equal(buffer.rewards[buffer_idx], reward[i])
            np.testing.assert_array_equal(
                buffer.not_dones[buffer_idx], np.logical_not(done[i])
            )
            np.testing.assert_array_equal(buffer.not_dones_no_max[buffer_idx], done[i])

    buffer = sac_buffer.ReplayBuffer((2,), (1,), 20, torch.device(""cpu""))

    # Test adding less than capacity
    batch_size_ = 10
    obs_, act_, next_obs_, reward_, done_ = _create_batch(batch_size_)
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    assert buffer.idx == batch_size_
    assert not buffer.full
    compare_batch_to_buffer_slice(0, batch_size_, obs_, act_, next_obs_, reward_, done_)

    # Test adding up to capacity
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    assert buffer.idx == 0
    assert buffer.full
    compare_batch_to_buffer_slice(
        batch_size_, batch_size_, obs_, act_, next_obs_, reward_, done_
    )  # new additions
    compare_batch_to_buffer_slice(
        0, batch_size_, obs_, act_, next_obs_, reward_, done_
    )  # Check that nothing changed here

    # Test adding beyond capacity
    start = 4
    buffer = sac_buffer.ReplayBuffer((2,), (1,), 20, torch.device(""cpu""))
    # first add a few elements to set buffer.idx != 0
    obs_, act_, next_obs_, reward_, done_ = _create_batch(start, mult=3)
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    # now add a batch larger than capacity
    batch_size_ = 27
    obs_, act_, next_obs_, reward_, done_ = _create_batch(batch_size_, mult=7)
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    assert buffer.idx == 11
    assert buffer.full
    # The last 11 observations loop around and overwrite the first 11
    compare_batch_to_buffer_slice(
        0, 11, obs_[16:], act_[16:], next_obs_[16:], reward_[16:], done_[16:]
    )
    # Now check that the last 9 observations are correct
    compare_batch_to_buffer_slice(
        11, 9, obs_[7:16], act_[7:16], next_obs_[7:16], reward_[7:16], done_[7:16]
    )",buffer.idx == 0,not buffer.idx,not buffer.idx,1,"v1: buffer.idx
v2: 0"
mbrl-lib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mbrl-lib/tests/core/test_replay_buffer.py,https://github.com/facebookresearch/mbrl-lib/tree/master/tests/core/test_replay_buffer.py,,test_sac_buffer_batched_add$118,"def test_sac_buffer_batched_add():
    def compare_batch_to_buffer_slice(
        start_idx, batch_size, obs, act, next_obs, reward, done
    ):
        for i in range(batch_size):
            buffer_idx = (start_idx + i) % buffer.capacity
            np.testing.assert_array_equal(buffer.obses[buffer_idx], obs[i])
            np.testing.assert_array_equal(buffer.actions[buffer_idx], act[i])
            np.testing.assert_array_equal(buffer.next_obses[buffer_idx], next_obs[i])
            np.testing.assert_array_equal(buffer.rewards[buffer_idx], reward[i])
            np.testing.assert_array_equal(
                buffer.not_dones[buffer_idx], np.logical_not(done[i])
            )
            np.testing.assert_array_equal(buffer.not_dones_no_max[buffer_idx], done[i])

    buffer = sac_buffer.ReplayBuffer((2,), (1,), 20, torch.device(""cpu""))

    # Test adding less than capacity
    batch_size_ = 10
    obs_, act_, next_obs_, reward_, done_ = _create_batch(batch_size_)
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    assert buffer.idx == batch_size_
    assert not buffer.full
    compare_batch_to_buffer_slice(0, batch_size_, obs_, act_, next_obs_, reward_, done_)

    # Test adding up to capacity
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    assert buffer.idx == 0
    assert buffer.full
    compare_batch_to_buffer_slice(
        batch_size_, batch_size_, obs_, act_, next_obs_, reward_, done_
    )  # new additions
    compare_batch_to_buffer_slice(
        0, batch_size_, obs_, act_, next_obs_, reward_, done_
    )  # Check that nothing changed here

    # Test adding beyond capacity
    start = 4
    buffer = sac_buffer.ReplayBuffer((2,), (1,), 20, torch.device(""cpu""))
    # first add a few elements to set buffer.idx != 0
    obs_, act_, next_obs_, reward_, done_ = _create_batch(start, mult=3)
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    # now add a batch larger than capacity
    batch_size_ = 27
    obs_, act_, next_obs_, reward_, done_ = _create_batch(batch_size_, mult=7)
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    assert buffer.idx == 11
    assert buffer.full
    # The last 11 observations loop around and overwrite the first 11
    compare_batch_to_buffer_slice(
        0, 11, obs_[16:], act_[16:], next_obs_[16:], reward_[16:], done_[16:]
    )
    # Now check that the last 9 observations are correct
    compare_batch_to_buffer_slice(
        11, 9, obs_[7:16], act_[7:16], next_obs_[7:16], reward_[7:16], done_[7:16]
    )",buffer.idx == 11,cannot be refactored by truth value test,Cannot refactor,1,"v1: buffer.idx
v2: 11"
mbrl-lib,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mbrl-lib/tests/core/test_replay_buffer.py,https://github.com/facebookresearch/mbrl-lib/tree/master/tests/core/test_replay_buffer.py,,test_sac_buffer_batched_add$118,"def test_sac_buffer_batched_add():
    def compare_batch_to_buffer_slice(
        start_idx, batch_size, obs, act, next_obs, reward, done
    ):
        for i in range(batch_size):
            buffer_idx = (start_idx + i) % buffer.capacity
            np.testing.assert_array_equal(buffer.obses[buffer_idx], obs[i])
            np.testing.assert_array_equal(buffer.actions[buffer_idx], act[i])
            np.testing.assert_array_equal(buffer.next_obses[buffer_idx], next_obs[i])
            np.testing.assert_array_equal(buffer.rewards[buffer_idx], reward[i])
            np.testing.assert_array_equal(
                buffer.not_dones[buffer_idx], np.logical_not(done[i])
            )
            np.testing.assert_array_equal(buffer.not_dones_no_max[buffer_idx], done[i])

    buffer = sac_buffer.ReplayBuffer((2,), (1,), 20, torch.device(""cpu""))

    # Test adding less than capacity
    batch_size_ = 10
    obs_, act_, next_obs_, reward_, done_ = _create_batch(batch_size_)
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    assert buffer.idx == batch_size_
    assert not buffer.full
    compare_batch_to_buffer_slice(0, batch_size_, obs_, act_, next_obs_, reward_, done_)

    # Test adding up to capacity
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    assert buffer.idx == 0
    assert buffer.full
    compare_batch_to_buffer_slice(
        batch_size_, batch_size_, obs_, act_, next_obs_, reward_, done_
    )  # new additions
    compare_batch_to_buffer_slice(
        0, batch_size_, obs_, act_, next_obs_, reward_, done_
    )  # Check that nothing changed here

    # Test adding beyond capacity
    start = 4
    buffer = sac_buffer.ReplayBuffer((2,), (1,), 20, torch.device(""cpu""))
    # first add a few elements to set buffer.idx != 0
    obs_, act_, next_obs_, reward_, done_ = _create_batch(start, mult=3)
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    # now add a batch larger than capacity
    batch_size_ = 27
    obs_, act_, next_obs_, reward_, done_ = _create_batch(batch_size_, mult=7)
    buffer.add_batch(obs_, act_, reward_, next_obs_, done_, np.logical_not(done_))
    assert buffer.idx == 11
    assert buffer.full
    # The last 11 observations loop around and overwrite the first 11
    compare_batch_to_buffer_slice(
        0, 11, obs_[16:], act_[16:], next_obs_[16:], reward_[16:], done_[16:]
    )
    # Now check that the last 9 observations are correct
    compare_batch_to_buffer_slice(
        11, 9, obs_[7:16], act_[7:16], next_obs_[7:16], reward_[7:16], done_[7:16]
    )",buffer.idx == batch_size_,cannot be refactored by truth value test,Cannot refactor,1,"v1: buffer.idx
v2: batch_size_"
blam,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/blam/src/blam.py,https://github.com/stuffmatic/blam/tree/master/src/blam.py,CameraCalibrationOperator,execute$1789,"def execute(self, context):
        '''Executes the operator.
        \param context The context in which the operator was executed.
        '''
        scn = bpy.context.scene
        singleVp = scn.calibration_type == 'one_vp'
        useHorizonSegment = scn.use_horizon_segment
        setBgImg = scn.set_cambg
        
        '''
        get the active camera
        '''
        cam = scn.camera     
        if not cam:
            self.report({'ERROR'}, ""No active camera."")
            return{'CANCELLED'}
        
        '''
        check settings
        '''
        if singleVp:
            upAxisIndex = ['x', 'y', 'z'].index(scn.up_axis)
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            
            if upAxisIndex == vp1AxisIndex:
                self.report({'ERROR'}, ""The up axis cannot be parallel to the axis of the line set."")
                return{'CANCELLED'}    
            vp2AxisIndex = (set([0, 1, 2]) ^ set([upAxisIndex, vp1AxisIndex])).pop()
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
        else:
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            vp2AxisIndex = ['x', 'y', 'z'].index(scn.vp2_axis)
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
            setBgImg = scn.set_cambg
            
            if vpAxisIndices[0] == vpAxisIndices[1]:
                self.report({'ERROR'}, ""The two line sets cannot be parallel to the same axis."")
                return{'CANCELLED'}
        
        '''
        gather lines for each vanishing point
        '''        
        activeSpace = bpy.context.area.spaces.active
        
        if not activeSpace.clip:
            self.report({'ERROR'}, ""There is no active movie clip."")
            return{'CANCELLED'}
        
        #check that we have the number of layers we need
        if not activeSpace.clip.grease_pencil:
            self.report({'ERROR'}, ""There is no grease pencil datablock."")
            return{'CANCELLED'}
        gpl = activeSpace.clip.grease_pencil.layers
        if len(gpl) == 0:
            self.report({'ERROR'}, ""There are no grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and not singleVp:
            self.report({'ERROR'}, ""Calibration using two vanishing points requires two grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and singleVp and useHorizonSegment:
            self.report({'ERROR'}, ""Single vanishing point calibration with a custom horizon line requires two grease pencil layers"")
            return{'CANCELLED'}
       
        vpLineSets = self.gatherGreasePencilSegments()
        
        #check that we have the expected number of line segment strokes
        if len(vpLineSets[0]) < 2:
            self.report({'ERROR'}, ""The first grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if not singleVp and len(vpLineSets[1]) < 2:
            self.report({'ERROR'}, ""The second grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if singleVp and useHorizonSegment and len(vpLineSets[1]) != 1:
            self.report({'ERROR'}, ""The second grease pencil layer must contain exactly one line segment stroke (the horizon line)."")
            return{'CANCELLED'}    
         
        '''
        get the principal point P in image plane coordinates
        TODO: get the value from the camera data panel, 
        currently always using the image center
        '''
        imageWidth = activeSpace.clip.size[0]
        imageHeight = activeSpace.clip.size[1]
        
        #principal point in image plane coordinates. 
        #in the middle of the image by default
        P = [0, 0]
        
        if singleVp:
            '''
            calibration using a single vanishing point
            '''
            imgAspect = imageWidth / float(imageHeight)
            
            #compute the horizon direction
            horizDir = normalize([1.0, 0.0]) #flat horizon by default
            if useHorizonSegment:
                xHorizDir = imgAspect * (vpLineSets[1][0][1][0] - vpLineSets[1][0][0][0])
                yHorizDir = vpLineSets[1][0][1][1] - vpLineSets[1][0][0][1]
                horizDir = normalize([-xHorizDir, -yHorizDir])
            #print(""horizDir"", horizDir)
            
            #compute the vanishing point location
            vp1 = self.computeIntersectionPointForLineSegments(vpLineSets[0])
            
            #get the current relative focal length
            fAbs = activeSpace.clip.tracking.camera.focal_length
            sensorWidth = activeSpace.clip.tracking.camera.sensor_width
            
            f = fAbs / sensorWidth * imgAspect
            #print(""fAbs"", fAbs, ""f rel"", f)
            Fu = self.relImgCoords2ImgPlaneCoords(vp1, imageWidth, imageHeight)
            Fv = self.computeSecondVanishingPoint(Fu, f, P, horizDir)
        else:
            '''
            calibration using two vanishing points
            '''
            if scn.optical_center_type == 'camdata':
                #get the principal point location from camera data
                P = [x for x in  activeSpace.clip.tracking.camera.principal]
                #print(""camera data optical center"", P[:])
                P[0] /= imageWidth
                P[1] /= imageHeight
                #print(""normlz. optical center"", P[:])
                P = self.relImgCoords2ImgPlaneCoords(P, imageWidth, imageHeight)
            elif scn.optical_center_type == 'compute':
                if len(vpLineSets) < 3:
                    self.report({'ERROR'}, ""A third grease pencil layer is needed to compute the optical center."")
                    return{'CANCELLED'}
                #compute the principal point using a vanishing point from a third gp layer.
                #this computation does not rely on the order of the line sets
                vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(len(vpLineSets))]
                vps = [self.relImgCoords2ImgPlaneCoords(vps[i], imageWidth, imageHeight) for i in range(len(vps))]
                P = self.computeTriangleOrthocenter(vps)
            else:
                #assume optical center in image midpoint
                pass
            
            #compute the two vanishing points
            vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(2)]    
        
            #order vanishing points along the image x axis
            if vps[1][0] < vps[0][0]:
                vps.reverse()
                vpLineSets.reverse()
                vpAxisIndices.reverse()            
        
            '''
            compute focal length
            '''
            Fu = self.relImgCoords2ImgPlaneCoords(vps[0], imageWidth, imageHeight)
            Fv = self.relImgCoords2ImgPlaneCoords(vps[1], imageWidth, imageHeight)
            
            f = self.computeFocalLength(Fu, Fv, P)
            
            if f == None:
                self.report({'ERROR'}, ""Failed to compute focal length. Invalid vanishing point constellation."")
                return{'CANCELLED'}
        
        '''
        compute camera orientation
        '''
        print(Fu, Fv, f)
        #initial orientation based on the vanishing points and focal length
        M = self.computeCameraRotationMatrix(Fu, Fv, f, P)
        
        #sanity check: M should be a pure rotation matrix, 
        #so its determinant should be 1
        eps = 0.00001
        if 1.0 - M.determinant() < -eps or 1.0 - M.determinant() > eps:
            self.report({'ERROR'}, ""Non unit rotation matrix determinant: "" + str(M.determinant()))    
            #return{'CANCELLED'} 
        
        #align the camera to the coordinate axes as specified
        M = self.alignCoordinateAxes(M, vpAxisIndices[0], vpAxisIndices[1])
        #apply the transform to the camera
        cam.matrix_world = M
        
        '''
        move the camera an arbitrary distance away from the ground plane
        TODO: focus on the origin or something
        '''
        cam.location = (0, 0, 2)
    
        #compute an absolute focal length in mm based 
        #on the current camera settings
        #TODO: make sure this works for all combinations of
        #image dimensions and camera sensor settings
        if imageWidth >= imageHeight:
            fMm = cam.data.sensor_height * f
        else:
            fMm = cam.data.sensor_width * f
        cam.data.lens = fMm
        self.report({'INFO'}, ""Camera focal length set to "" + str(fMm))
        
        #move principal point of the blender camera
        r = imageWidth / float(imageHeight)
        cam.data.shift_x = -1 * P[0] / r
        cam.data.shift_y = -1 * P[1] / r
        
        '''
        set the camera background image
        '''
        bpy.context.scene.render.resolution_x = imageWidth
        bpy.context.scene.render.resolution_y = imageHeight
        
        if setBgImg:
            bpy.ops.clip.set_viewport_background()
            
        return{'FINISHED'}",upAxisIndex == vp1AxisIndex,cannot be refactored by truth value test,Cannot refactor,1,"v1: upAxisIndex
v2: vp1AxisIndex"
blam,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/blam/src/blam.py,https://github.com/stuffmatic/blam/tree/master/src/blam.py,CameraCalibrationOperator,execute$1789,"def execute(self, context):
        '''Executes the operator.
        \param context The context in which the operator was executed.
        '''
        scn = bpy.context.scene
        singleVp = scn.calibration_type == 'one_vp'
        useHorizonSegment = scn.use_horizon_segment
        setBgImg = scn.set_cambg
        
        '''
        get the active camera
        '''
        cam = scn.camera     
        if not cam:
            self.report({'ERROR'}, ""No active camera."")
            return{'CANCELLED'}
        
        '''
        check settings
        '''
        if singleVp:
            upAxisIndex = ['x', 'y', 'z'].index(scn.up_axis)
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            
            if upAxisIndex == vp1AxisIndex:
                self.report({'ERROR'}, ""The up axis cannot be parallel to the axis of the line set."")
                return{'CANCELLED'}    
            vp2AxisIndex = (set([0, 1, 2]) ^ set([upAxisIndex, vp1AxisIndex])).pop()
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
        else:
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            vp2AxisIndex = ['x', 'y', 'z'].index(scn.vp2_axis)
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
            setBgImg = scn.set_cambg
            
            if vpAxisIndices[0] == vpAxisIndices[1]:
                self.report({'ERROR'}, ""The two line sets cannot be parallel to the same axis."")
                return{'CANCELLED'}
        
        '''
        gather lines for each vanishing point
        '''        
        activeSpace = bpy.context.area.spaces.active
        
        if not activeSpace.clip:
            self.report({'ERROR'}, ""There is no active movie clip."")
            return{'CANCELLED'}
        
        #check that we have the number of layers we need
        if not activeSpace.clip.grease_pencil:
            self.report({'ERROR'}, ""There is no grease pencil datablock."")
            return{'CANCELLED'}
        gpl = activeSpace.clip.grease_pencil.layers
        if len(gpl) == 0:
            self.report({'ERROR'}, ""There are no grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and not singleVp:
            self.report({'ERROR'}, ""Calibration using two vanishing points requires two grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and singleVp and useHorizonSegment:
            self.report({'ERROR'}, ""Single vanishing point calibration with a custom horizon line requires two grease pencil layers"")
            return{'CANCELLED'}
       
        vpLineSets = self.gatherGreasePencilSegments()
        
        #check that we have the expected number of line segment strokes
        if len(vpLineSets[0]) < 2:
            self.report({'ERROR'}, ""The first grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if not singleVp and len(vpLineSets[1]) < 2:
            self.report({'ERROR'}, ""The second grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if singleVp and useHorizonSegment and len(vpLineSets[1]) != 1:
            self.report({'ERROR'}, ""The second grease pencil layer must contain exactly one line segment stroke (the horizon line)."")
            return{'CANCELLED'}    
         
        '''
        get the principal point P in image plane coordinates
        TODO: get the value from the camera data panel, 
        currently always using the image center
        '''
        imageWidth = activeSpace.clip.size[0]
        imageHeight = activeSpace.clip.size[1]
        
        #principal point in image plane coordinates. 
        #in the middle of the image by default
        P = [0, 0]
        
        if singleVp:
            '''
            calibration using a single vanishing point
            '''
            imgAspect = imageWidth / float(imageHeight)
            
            #compute the horizon direction
            horizDir = normalize([1.0, 0.0]) #flat horizon by default
            if useHorizonSegment:
                xHorizDir = imgAspect * (vpLineSets[1][0][1][0] - vpLineSets[1][0][0][0])
                yHorizDir = vpLineSets[1][0][1][1] - vpLineSets[1][0][0][1]
                horizDir = normalize([-xHorizDir, -yHorizDir])
            #print(""horizDir"", horizDir)
            
            #compute the vanishing point location
            vp1 = self.computeIntersectionPointForLineSegments(vpLineSets[0])
            
            #get the current relative focal length
            fAbs = activeSpace.clip.tracking.camera.focal_length
            sensorWidth = activeSpace.clip.tracking.camera.sensor_width
            
            f = fAbs / sensorWidth * imgAspect
            #print(""fAbs"", fAbs, ""f rel"", f)
            Fu = self.relImgCoords2ImgPlaneCoords(vp1, imageWidth, imageHeight)
            Fv = self.computeSecondVanishingPoint(Fu, f, P, horizDir)
        else:
            '''
            calibration using two vanishing points
            '''
            if scn.optical_center_type == 'camdata':
                #get the principal point location from camera data
                P = [x for x in  activeSpace.clip.tracking.camera.principal]
                #print(""camera data optical center"", P[:])
                P[0] /= imageWidth
                P[1] /= imageHeight
                #print(""normlz. optical center"", P[:])
                P = self.relImgCoords2ImgPlaneCoords(P, imageWidth, imageHeight)
            elif scn.optical_center_type == 'compute':
                if len(vpLineSets) < 3:
                    self.report({'ERROR'}, ""A third grease pencil layer is needed to compute the optical center."")
                    return{'CANCELLED'}
                #compute the principal point using a vanishing point from a third gp layer.
                #this computation does not rely on the order of the line sets
                vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(len(vpLineSets))]
                vps = [self.relImgCoords2ImgPlaneCoords(vps[i], imageWidth, imageHeight) for i in range(len(vps))]
                P = self.computeTriangleOrthocenter(vps)
            else:
                #assume optical center in image midpoint
                pass
            
            #compute the two vanishing points
            vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(2)]    
        
            #order vanishing points along the image x axis
            if vps[1][0] < vps[0][0]:
                vps.reverse()
                vpLineSets.reverse()
                vpAxisIndices.reverse()            
        
            '''
            compute focal length
            '''
            Fu = self.relImgCoords2ImgPlaneCoords(vps[0], imageWidth, imageHeight)
            Fv = self.relImgCoords2ImgPlaneCoords(vps[1], imageWidth, imageHeight)
            
            f = self.computeFocalLength(Fu, Fv, P)
            
            if f == None:
                self.report({'ERROR'}, ""Failed to compute focal length. Invalid vanishing point constellation."")
                return{'CANCELLED'}
        
        '''
        compute camera orientation
        '''
        print(Fu, Fv, f)
        #initial orientation based on the vanishing points and focal length
        M = self.computeCameraRotationMatrix(Fu, Fv, f, P)
        
        #sanity check: M should be a pure rotation matrix, 
        #so its determinant should be 1
        eps = 0.00001
        if 1.0 - M.determinant() < -eps or 1.0 - M.determinant() > eps:
            self.report({'ERROR'}, ""Non unit rotation matrix determinant: "" + str(M.determinant()))    
            #return{'CANCELLED'} 
        
        #align the camera to the coordinate axes as specified
        M = self.alignCoordinateAxes(M, vpAxisIndices[0], vpAxisIndices[1])
        #apply the transform to the camera
        cam.matrix_world = M
        
        '''
        move the camera an arbitrary distance away from the ground plane
        TODO: focus on the origin or something
        '''
        cam.location = (0, 0, 2)
    
        #compute an absolute focal length in mm based 
        #on the current camera settings
        #TODO: make sure this works for all combinations of
        #image dimensions and camera sensor settings
        if imageWidth >= imageHeight:
            fMm = cam.data.sensor_height * f
        else:
            fMm = cam.data.sensor_width * f
        cam.data.lens = fMm
        self.report({'INFO'}, ""Camera focal length set to "" + str(fMm))
        
        #move principal point of the blender camera
        r = imageWidth / float(imageHeight)
        cam.data.shift_x = -1 * P[0] / r
        cam.data.shift_y = -1 * P[1] / r
        
        '''
        set the camera background image
        '''
        bpy.context.scene.render.resolution_x = imageWidth
        bpy.context.scene.render.resolution_y = imageHeight
        
        if setBgImg:
            bpy.ops.clip.set_viewport_background()
            
        return{'FINISHED'}",f == None,not f,not f,1,"v1: f
v2: None"
blam,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/blam/src/blam.py,https://github.com/stuffmatic/blam/tree/master/src/blam.py,CameraCalibrationOperator,execute$1789,"def execute(self, context):
        '''Executes the operator.
        \param context The context in which the operator was executed.
        '''
        scn = bpy.context.scene
        singleVp = scn.calibration_type == 'one_vp'
        useHorizonSegment = scn.use_horizon_segment
        setBgImg = scn.set_cambg
        
        '''
        get the active camera
        '''
        cam = scn.camera     
        if not cam:
            self.report({'ERROR'}, ""No active camera."")
            return{'CANCELLED'}
        
        '''
        check settings
        '''
        if singleVp:
            upAxisIndex = ['x', 'y', 'z'].index(scn.up_axis)
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            
            if upAxisIndex == vp1AxisIndex:
                self.report({'ERROR'}, ""The up axis cannot be parallel to the axis of the line set."")
                return{'CANCELLED'}    
            vp2AxisIndex = (set([0, 1, 2]) ^ set([upAxisIndex, vp1AxisIndex])).pop()
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
        else:
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            vp2AxisIndex = ['x', 'y', 'z'].index(scn.vp2_axis)
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
            setBgImg = scn.set_cambg
            
            if vpAxisIndices[0] == vpAxisIndices[1]:
                self.report({'ERROR'}, ""The two line sets cannot be parallel to the same axis."")
                return{'CANCELLED'}
        
        '''
        gather lines for each vanishing point
        '''        
        activeSpace = bpy.context.area.spaces.active
        
        if not activeSpace.clip:
            self.report({'ERROR'}, ""There is no active movie clip."")
            return{'CANCELLED'}
        
        #check that we have the number of layers we need
        if not activeSpace.clip.grease_pencil:
            self.report({'ERROR'}, ""There is no grease pencil datablock."")
            return{'CANCELLED'}
        gpl = activeSpace.clip.grease_pencil.layers
        if len(gpl) == 0:
            self.report({'ERROR'}, ""There are no grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and not singleVp:
            self.report({'ERROR'}, ""Calibration using two vanishing points requires two grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and singleVp and useHorizonSegment:
            self.report({'ERROR'}, ""Single vanishing point calibration with a custom horizon line requires two grease pencil layers"")
            return{'CANCELLED'}
       
        vpLineSets = self.gatherGreasePencilSegments()
        
        #check that we have the expected number of line segment strokes
        if len(vpLineSets[0]) < 2:
            self.report({'ERROR'}, ""The first grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if not singleVp and len(vpLineSets[1]) < 2:
            self.report({'ERROR'}, ""The second grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if singleVp and useHorizonSegment and len(vpLineSets[1]) != 1:
            self.report({'ERROR'}, ""The second grease pencil layer must contain exactly one line segment stroke (the horizon line)."")
            return{'CANCELLED'}    
         
        '''
        get the principal point P in image plane coordinates
        TODO: get the value from the camera data panel, 
        currently always using the image center
        '''
        imageWidth = activeSpace.clip.size[0]
        imageHeight = activeSpace.clip.size[1]
        
        #principal point in image plane coordinates. 
        #in the middle of the image by default
        P = [0, 0]
        
        if singleVp:
            '''
            calibration using a single vanishing point
            '''
            imgAspect = imageWidth / float(imageHeight)
            
            #compute the horizon direction
            horizDir = normalize([1.0, 0.0]) #flat horizon by default
            if useHorizonSegment:
                xHorizDir = imgAspect * (vpLineSets[1][0][1][0] - vpLineSets[1][0][0][0])
                yHorizDir = vpLineSets[1][0][1][1] - vpLineSets[1][0][0][1]
                horizDir = normalize([-xHorizDir, -yHorizDir])
            #print(""horizDir"", horizDir)
            
            #compute the vanishing point location
            vp1 = self.computeIntersectionPointForLineSegments(vpLineSets[0])
            
            #get the current relative focal length
            fAbs = activeSpace.clip.tracking.camera.focal_length
            sensorWidth = activeSpace.clip.tracking.camera.sensor_width
            
            f = fAbs / sensorWidth * imgAspect
            #print(""fAbs"", fAbs, ""f rel"", f)
            Fu = self.relImgCoords2ImgPlaneCoords(vp1, imageWidth, imageHeight)
            Fv = self.computeSecondVanishingPoint(Fu, f, P, horizDir)
        else:
            '''
            calibration using two vanishing points
            '''
            if scn.optical_center_type == 'camdata':
                #get the principal point location from camera data
                P = [x for x in  activeSpace.clip.tracking.camera.principal]
                #print(""camera data optical center"", P[:])
                P[0] /= imageWidth
                P[1] /= imageHeight
                #print(""normlz. optical center"", P[:])
                P = self.relImgCoords2ImgPlaneCoords(P, imageWidth, imageHeight)
            elif scn.optical_center_type == 'compute':
                if len(vpLineSets) < 3:
                    self.report({'ERROR'}, ""A third grease pencil layer is needed to compute the optical center."")
                    return{'CANCELLED'}
                #compute the principal point using a vanishing point from a third gp layer.
                #this computation does not rely on the order of the line sets
                vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(len(vpLineSets))]
                vps = [self.relImgCoords2ImgPlaneCoords(vps[i], imageWidth, imageHeight) for i in range(len(vps))]
                P = self.computeTriangleOrthocenter(vps)
            else:
                #assume optical center in image midpoint
                pass
            
            #compute the two vanishing points
            vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(2)]    
        
            #order vanishing points along the image x axis
            if vps[1][0] < vps[0][0]:
                vps.reverse()
                vpLineSets.reverse()
                vpAxisIndices.reverse()            
        
            '''
            compute focal length
            '''
            Fu = self.relImgCoords2ImgPlaneCoords(vps[0], imageWidth, imageHeight)
            Fv = self.relImgCoords2ImgPlaneCoords(vps[1], imageWidth, imageHeight)
            
            f = self.computeFocalLength(Fu, Fv, P)
            
            if f == None:
                self.report({'ERROR'}, ""Failed to compute focal length. Invalid vanishing point constellation."")
                return{'CANCELLED'}
        
        '''
        compute camera orientation
        '''
        print(Fu, Fv, f)
        #initial orientation based on the vanishing points and focal length
        M = self.computeCameraRotationMatrix(Fu, Fv, f, P)
        
        #sanity check: M should be a pure rotation matrix, 
        #so its determinant should be 1
        eps = 0.00001
        if 1.0 - M.determinant() < -eps or 1.0 - M.determinant() > eps:
            self.report({'ERROR'}, ""Non unit rotation matrix determinant: "" + str(M.determinant()))    
            #return{'CANCELLED'} 
        
        #align the camera to the coordinate axes as specified
        M = self.alignCoordinateAxes(M, vpAxisIndices[0], vpAxisIndices[1])
        #apply the transform to the camera
        cam.matrix_world = M
        
        '''
        move the camera an arbitrary distance away from the ground plane
        TODO: focus on the origin or something
        '''
        cam.location = (0, 0, 2)
    
        #compute an absolute focal length in mm based 
        #on the current camera settings
        #TODO: make sure this works for all combinations of
        #image dimensions and camera sensor settings
        if imageWidth >= imageHeight:
            fMm = cam.data.sensor_height * f
        else:
            fMm = cam.data.sensor_width * f
        cam.data.lens = fMm
        self.report({'INFO'}, ""Camera focal length set to "" + str(fMm))
        
        #move principal point of the blender camera
        r = imageWidth / float(imageHeight)
        cam.data.shift_x = -1 * P[0] / r
        cam.data.shift_y = -1 * P[1] / r
        
        '''
        set the camera background image
        '''
        bpy.context.scene.render.resolution_x = imageWidth
        bpy.context.scene.render.resolution_y = imageHeight
        
        if setBgImg:
            bpy.ops.clip.set_viewport_background()
            
        return{'FINISHED'}",scn.optical_center_type == 'compute',cannot be refactored by truth value test,Cannot refactor,1,"v1: scn.optical_center_type
v2: 'compute'"
blam,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/blam/src/blam.py,https://github.com/stuffmatic/blam/tree/master/src/blam.py,CameraCalibrationOperator,execute$1789,"def execute(self, context):
        '''Executes the operator.
        \param context The context in which the operator was executed.
        '''
        scn = bpy.context.scene
        singleVp = scn.calibration_type == 'one_vp'
        useHorizonSegment = scn.use_horizon_segment
        setBgImg = scn.set_cambg
        
        '''
        get the active camera
        '''
        cam = scn.camera     
        if not cam:
            self.report({'ERROR'}, ""No active camera."")
            return{'CANCELLED'}
        
        '''
        check settings
        '''
        if singleVp:
            upAxisIndex = ['x', 'y', 'z'].index(scn.up_axis)
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            
            if upAxisIndex == vp1AxisIndex:
                self.report({'ERROR'}, ""The up axis cannot be parallel to the axis of the line set."")
                return{'CANCELLED'}    
            vp2AxisIndex = (set([0, 1, 2]) ^ set([upAxisIndex, vp1AxisIndex])).pop()
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
        else:
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            vp2AxisIndex = ['x', 'y', 'z'].index(scn.vp2_axis)
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
            setBgImg = scn.set_cambg
            
            if vpAxisIndices[0] == vpAxisIndices[1]:
                self.report({'ERROR'}, ""The two line sets cannot be parallel to the same axis."")
                return{'CANCELLED'}
        
        '''
        gather lines for each vanishing point
        '''        
        activeSpace = bpy.context.area.spaces.active
        
        if not activeSpace.clip:
            self.report({'ERROR'}, ""There is no active movie clip."")
            return{'CANCELLED'}
        
        #check that we have the number of layers we need
        if not activeSpace.clip.grease_pencil:
            self.report({'ERROR'}, ""There is no grease pencil datablock."")
            return{'CANCELLED'}
        gpl = activeSpace.clip.grease_pencil.layers
        if len(gpl) == 0:
            self.report({'ERROR'}, ""There are no grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and not singleVp:
            self.report({'ERROR'}, ""Calibration using two vanishing points requires two grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and singleVp and useHorizonSegment:
            self.report({'ERROR'}, ""Single vanishing point calibration with a custom horizon line requires two grease pencil layers"")
            return{'CANCELLED'}
       
        vpLineSets = self.gatherGreasePencilSegments()
        
        #check that we have the expected number of line segment strokes
        if len(vpLineSets[0]) < 2:
            self.report({'ERROR'}, ""The first grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if not singleVp and len(vpLineSets[1]) < 2:
            self.report({'ERROR'}, ""The second grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if singleVp and useHorizonSegment and len(vpLineSets[1]) != 1:
            self.report({'ERROR'}, ""The second grease pencil layer must contain exactly one line segment stroke (the horizon line)."")
            return{'CANCELLED'}    
         
        '''
        get the principal point P in image plane coordinates
        TODO: get the value from the camera data panel, 
        currently always using the image center
        '''
        imageWidth = activeSpace.clip.size[0]
        imageHeight = activeSpace.clip.size[1]
        
        #principal point in image plane coordinates. 
        #in the middle of the image by default
        P = [0, 0]
        
        if singleVp:
            '''
            calibration using a single vanishing point
            '''
            imgAspect = imageWidth / float(imageHeight)
            
            #compute the horizon direction
            horizDir = normalize([1.0, 0.0]) #flat horizon by default
            if useHorizonSegment:
                xHorizDir = imgAspect * (vpLineSets[1][0][1][0] - vpLineSets[1][0][0][0])
                yHorizDir = vpLineSets[1][0][1][1] - vpLineSets[1][0][0][1]
                horizDir = normalize([-xHorizDir, -yHorizDir])
            #print(""horizDir"", horizDir)
            
            #compute the vanishing point location
            vp1 = self.computeIntersectionPointForLineSegments(vpLineSets[0])
            
            #get the current relative focal length
            fAbs = activeSpace.clip.tracking.camera.focal_length
            sensorWidth = activeSpace.clip.tracking.camera.sensor_width
            
            f = fAbs / sensorWidth * imgAspect
            #print(""fAbs"", fAbs, ""f rel"", f)
            Fu = self.relImgCoords2ImgPlaneCoords(vp1, imageWidth, imageHeight)
            Fv = self.computeSecondVanishingPoint(Fu, f, P, horizDir)
        else:
            '''
            calibration using two vanishing points
            '''
            if scn.optical_center_type == 'camdata':
                #get the principal point location from camera data
                P = [x for x in  activeSpace.clip.tracking.camera.principal]
                #print(""camera data optical center"", P[:])
                P[0] /= imageWidth
                P[1] /= imageHeight
                #print(""normlz. optical center"", P[:])
                P = self.relImgCoords2ImgPlaneCoords(P, imageWidth, imageHeight)
            elif scn.optical_center_type == 'compute':
                if len(vpLineSets) < 3:
                    self.report({'ERROR'}, ""A third grease pencil layer is needed to compute the optical center."")
                    return{'CANCELLED'}
                #compute the principal point using a vanishing point from a third gp layer.
                #this computation does not rely on the order of the line sets
                vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(len(vpLineSets))]
                vps = [self.relImgCoords2ImgPlaneCoords(vps[i], imageWidth, imageHeight) for i in range(len(vps))]
                P = self.computeTriangleOrthocenter(vps)
            else:
                #assume optical center in image midpoint
                pass
            
            #compute the two vanishing points
            vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(2)]    
        
            #order vanishing points along the image x axis
            if vps[1][0] < vps[0][0]:
                vps.reverse()
                vpLineSets.reverse()
                vpAxisIndices.reverse()            
        
            '''
            compute focal length
            '''
            Fu = self.relImgCoords2ImgPlaneCoords(vps[0], imageWidth, imageHeight)
            Fv = self.relImgCoords2ImgPlaneCoords(vps[1], imageWidth, imageHeight)
            
            f = self.computeFocalLength(Fu, Fv, P)
            
            if f == None:
                self.report({'ERROR'}, ""Failed to compute focal length. Invalid vanishing point constellation."")
                return{'CANCELLED'}
        
        '''
        compute camera orientation
        '''
        print(Fu, Fv, f)
        #initial orientation based on the vanishing points and focal length
        M = self.computeCameraRotationMatrix(Fu, Fv, f, P)
        
        #sanity check: M should be a pure rotation matrix, 
        #so its determinant should be 1
        eps = 0.00001
        if 1.0 - M.determinant() < -eps or 1.0 - M.determinant() > eps:
            self.report({'ERROR'}, ""Non unit rotation matrix determinant: "" + str(M.determinant()))    
            #return{'CANCELLED'} 
        
        #align the camera to the coordinate axes as specified
        M = self.alignCoordinateAxes(M, vpAxisIndices[0], vpAxisIndices[1])
        #apply the transform to the camera
        cam.matrix_world = M
        
        '''
        move the camera an arbitrary distance away from the ground plane
        TODO: focus on the origin or something
        '''
        cam.location = (0, 0, 2)
    
        #compute an absolute focal length in mm based 
        #on the current camera settings
        #TODO: make sure this works for all combinations of
        #image dimensions and camera sensor settings
        if imageWidth >= imageHeight:
            fMm = cam.data.sensor_height * f
        else:
            fMm = cam.data.sensor_width * f
        cam.data.lens = fMm
        self.report({'INFO'}, ""Camera focal length set to "" + str(fMm))
        
        #move principal point of the blender camera
        r = imageWidth / float(imageHeight)
        cam.data.shift_x = -1 * P[0] / r
        cam.data.shift_y = -1 * P[1] / r
        
        '''
        set the camera background image
        '''
        bpy.context.scene.render.resolution_x = imageWidth
        bpy.context.scene.render.resolution_y = imageHeight
        
        if setBgImg:
            bpy.ops.clip.set_viewport_background()
            
        return{'FINISHED'}",vpAxisIndices[0] == vpAxisIndices[1],cannot be refactored by truth value test,Cannot refactor,1,"v1: vpAxisIndices[0]
v2: vpAxisIndices[1]"
blam,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/blam/src/blam.py,https://github.com/stuffmatic/blam/tree/master/src/blam.py,CameraCalibrationOperator,execute$1789,"def execute(self, context):
        '''Executes the operator.
        \param context The context in which the operator was executed.
        '''
        scn = bpy.context.scene
        singleVp = scn.calibration_type == 'one_vp'
        useHorizonSegment = scn.use_horizon_segment
        setBgImg = scn.set_cambg
        
        '''
        get the active camera
        '''
        cam = scn.camera     
        if not cam:
            self.report({'ERROR'}, ""No active camera."")
            return{'CANCELLED'}
        
        '''
        check settings
        '''
        if singleVp:
            upAxisIndex = ['x', 'y', 'z'].index(scn.up_axis)
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            
            if upAxisIndex == vp1AxisIndex:
                self.report({'ERROR'}, ""The up axis cannot be parallel to the axis of the line set."")
                return{'CANCELLED'}    
            vp2AxisIndex = (set([0, 1, 2]) ^ set([upAxisIndex, vp1AxisIndex])).pop()
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
        else:
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            vp2AxisIndex = ['x', 'y', 'z'].index(scn.vp2_axis)
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
            setBgImg = scn.set_cambg
            
            if vpAxisIndices[0] == vpAxisIndices[1]:
                self.report({'ERROR'}, ""The two line sets cannot be parallel to the same axis."")
                return{'CANCELLED'}
        
        '''
        gather lines for each vanishing point
        '''        
        activeSpace = bpy.context.area.spaces.active
        
        if not activeSpace.clip:
            self.report({'ERROR'}, ""There is no active movie clip."")
            return{'CANCELLED'}
        
        #check that we have the number of layers we need
        if not activeSpace.clip.grease_pencil:
            self.report({'ERROR'}, ""There is no grease pencil datablock."")
            return{'CANCELLED'}
        gpl = activeSpace.clip.grease_pencil.layers
        if len(gpl) == 0:
            self.report({'ERROR'}, ""There are no grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and not singleVp:
            self.report({'ERROR'}, ""Calibration using two vanishing points requires two grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and singleVp and useHorizonSegment:
            self.report({'ERROR'}, ""Single vanishing point calibration with a custom horizon line requires two grease pencil layers"")
            return{'CANCELLED'}
       
        vpLineSets = self.gatherGreasePencilSegments()
        
        #check that we have the expected number of line segment strokes
        if len(vpLineSets[0]) < 2:
            self.report({'ERROR'}, ""The first grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if not singleVp and len(vpLineSets[1]) < 2:
            self.report({'ERROR'}, ""The second grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if singleVp and useHorizonSegment and len(vpLineSets[1]) != 1:
            self.report({'ERROR'}, ""The second grease pencil layer must contain exactly one line segment stroke (the horizon line)."")
            return{'CANCELLED'}    
         
        '''
        get the principal point P in image plane coordinates
        TODO: get the value from the camera data panel, 
        currently always using the image center
        '''
        imageWidth = activeSpace.clip.size[0]
        imageHeight = activeSpace.clip.size[1]
        
        #principal point in image plane coordinates. 
        #in the middle of the image by default
        P = [0, 0]
        
        if singleVp:
            '''
            calibration using a single vanishing point
            '''
            imgAspect = imageWidth / float(imageHeight)
            
            #compute the horizon direction
            horizDir = normalize([1.0, 0.0]) #flat horizon by default
            if useHorizonSegment:
                xHorizDir = imgAspect * (vpLineSets[1][0][1][0] - vpLineSets[1][0][0][0])
                yHorizDir = vpLineSets[1][0][1][1] - vpLineSets[1][0][0][1]
                horizDir = normalize([-xHorizDir, -yHorizDir])
            #print(""horizDir"", horizDir)
            
            #compute the vanishing point location
            vp1 = self.computeIntersectionPointForLineSegments(vpLineSets[0])
            
            #get the current relative focal length
            fAbs = activeSpace.clip.tracking.camera.focal_length
            sensorWidth = activeSpace.clip.tracking.camera.sensor_width
            
            f = fAbs / sensorWidth * imgAspect
            #print(""fAbs"", fAbs, ""f rel"", f)
            Fu = self.relImgCoords2ImgPlaneCoords(vp1, imageWidth, imageHeight)
            Fv = self.computeSecondVanishingPoint(Fu, f, P, horizDir)
        else:
            '''
            calibration using two vanishing points
            '''
            if scn.optical_center_type == 'camdata':
                #get the principal point location from camera data
                P = [x for x in  activeSpace.clip.tracking.camera.principal]
                #print(""camera data optical center"", P[:])
                P[0] /= imageWidth
                P[1] /= imageHeight
                #print(""normlz. optical center"", P[:])
                P = self.relImgCoords2ImgPlaneCoords(P, imageWidth, imageHeight)
            elif scn.optical_center_type == 'compute':
                if len(vpLineSets) < 3:
                    self.report({'ERROR'}, ""A third grease pencil layer is needed to compute the optical center."")
                    return{'CANCELLED'}
                #compute the principal point using a vanishing point from a third gp layer.
                #this computation does not rely on the order of the line sets
                vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(len(vpLineSets))]
                vps = [self.relImgCoords2ImgPlaneCoords(vps[i], imageWidth, imageHeight) for i in range(len(vps))]
                P = self.computeTriangleOrthocenter(vps)
            else:
                #assume optical center in image midpoint
                pass
            
            #compute the two vanishing points
            vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(2)]    
        
            #order vanishing points along the image x axis
            if vps[1][0] < vps[0][0]:
                vps.reverse()
                vpLineSets.reverse()
                vpAxisIndices.reverse()            
        
            '''
            compute focal length
            '''
            Fu = self.relImgCoords2ImgPlaneCoords(vps[0], imageWidth, imageHeight)
            Fv = self.relImgCoords2ImgPlaneCoords(vps[1], imageWidth, imageHeight)
            
            f = self.computeFocalLength(Fu, Fv, P)
            
            if f == None:
                self.report({'ERROR'}, ""Failed to compute focal length. Invalid vanishing point constellation."")
                return{'CANCELLED'}
        
        '''
        compute camera orientation
        '''
        print(Fu, Fv, f)
        #initial orientation based on the vanishing points and focal length
        M = self.computeCameraRotationMatrix(Fu, Fv, f, P)
        
        #sanity check: M should be a pure rotation matrix, 
        #so its determinant should be 1
        eps = 0.00001
        if 1.0 - M.determinant() < -eps or 1.0 - M.determinant() > eps:
            self.report({'ERROR'}, ""Non unit rotation matrix determinant: "" + str(M.determinant()))    
            #return{'CANCELLED'} 
        
        #align the camera to the coordinate axes as specified
        M = self.alignCoordinateAxes(M, vpAxisIndices[0], vpAxisIndices[1])
        #apply the transform to the camera
        cam.matrix_world = M
        
        '''
        move the camera an arbitrary distance away from the ground plane
        TODO: focus on the origin or something
        '''
        cam.location = (0, 0, 2)
    
        #compute an absolute focal length in mm based 
        #on the current camera settings
        #TODO: make sure this works for all combinations of
        #image dimensions and camera sensor settings
        if imageWidth >= imageHeight:
            fMm = cam.data.sensor_height * f
        else:
            fMm = cam.data.sensor_width * f
        cam.data.lens = fMm
        self.report({'INFO'}, ""Camera focal length set to "" + str(fMm))
        
        #move principal point of the blender camera
        r = imageWidth / float(imageHeight)
        cam.data.shift_x = -1 * P[0] / r
        cam.data.shift_y = -1 * P[1] / r
        
        '''
        set the camera background image
        '''
        bpy.context.scene.render.resolution_x = imageWidth
        bpy.context.scene.render.resolution_y = imageHeight
        
        if setBgImg:
            bpy.ops.clip.set_viewport_background()
            
        return{'FINISHED'}",len(gpl) == 0,not gpl,not gpl,1,"v1: len(gpl)
v2: 0"
blam,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/blam/src/blam.py,https://github.com/stuffmatic/blam/tree/master/src/blam.py,CameraCalibrationOperator,execute$1789,"def execute(self, context):
        '''Executes the operator.
        \param context The context in which the operator was executed.
        '''
        scn = bpy.context.scene
        singleVp = scn.calibration_type == 'one_vp'
        useHorizonSegment = scn.use_horizon_segment
        setBgImg = scn.set_cambg
        
        '''
        get the active camera
        '''
        cam = scn.camera     
        if not cam:
            self.report({'ERROR'}, ""No active camera."")
            return{'CANCELLED'}
        
        '''
        check settings
        '''
        if singleVp:
            upAxisIndex = ['x', 'y', 'z'].index(scn.up_axis)
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            
            if upAxisIndex == vp1AxisIndex:
                self.report({'ERROR'}, ""The up axis cannot be parallel to the axis of the line set."")
                return{'CANCELLED'}    
            vp2AxisIndex = (set([0, 1, 2]) ^ set([upAxisIndex, vp1AxisIndex])).pop()
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
        else:
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            vp2AxisIndex = ['x', 'y', 'z'].index(scn.vp2_axis)
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
            setBgImg = scn.set_cambg
            
            if vpAxisIndices[0] == vpAxisIndices[1]:
                self.report({'ERROR'}, ""The two line sets cannot be parallel to the same axis."")
                return{'CANCELLED'}
        
        '''
        gather lines for each vanishing point
        '''        
        activeSpace = bpy.context.area.spaces.active
        
        if not activeSpace.clip:
            self.report({'ERROR'}, ""There is no active movie clip."")
            return{'CANCELLED'}
        
        #check that we have the number of layers we need
        if not activeSpace.clip.grease_pencil:
            self.report({'ERROR'}, ""There is no grease pencil datablock."")
            return{'CANCELLED'}
        gpl = activeSpace.clip.grease_pencil.layers
        if len(gpl) == 0:
            self.report({'ERROR'}, ""There are no grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and not singleVp:
            self.report({'ERROR'}, ""Calibration using two vanishing points requires two grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and singleVp and useHorizonSegment:
            self.report({'ERROR'}, ""Single vanishing point calibration with a custom horizon line requires two grease pencil layers"")
            return{'CANCELLED'}
       
        vpLineSets = self.gatherGreasePencilSegments()
        
        #check that we have the expected number of line segment strokes
        if len(vpLineSets[0]) < 2:
            self.report({'ERROR'}, ""The first grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if not singleVp and len(vpLineSets[1]) < 2:
            self.report({'ERROR'}, ""The second grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if singleVp and useHorizonSegment and len(vpLineSets[1]) != 1:
            self.report({'ERROR'}, ""The second grease pencil layer must contain exactly one line segment stroke (the horizon line)."")
            return{'CANCELLED'}    
         
        '''
        get the principal point P in image plane coordinates
        TODO: get the value from the camera data panel, 
        currently always using the image center
        '''
        imageWidth = activeSpace.clip.size[0]
        imageHeight = activeSpace.clip.size[1]
        
        #principal point in image plane coordinates. 
        #in the middle of the image by default
        P = [0, 0]
        
        if singleVp:
            '''
            calibration using a single vanishing point
            '''
            imgAspect = imageWidth / float(imageHeight)
            
            #compute the horizon direction
            horizDir = normalize([1.0, 0.0]) #flat horizon by default
            if useHorizonSegment:
                xHorizDir = imgAspect * (vpLineSets[1][0][1][0] - vpLineSets[1][0][0][0])
                yHorizDir = vpLineSets[1][0][1][1] - vpLineSets[1][0][0][1]
                horizDir = normalize([-xHorizDir, -yHorizDir])
            #print(""horizDir"", horizDir)
            
            #compute the vanishing point location
            vp1 = self.computeIntersectionPointForLineSegments(vpLineSets[0])
            
            #get the current relative focal length
            fAbs = activeSpace.clip.tracking.camera.focal_length
            sensorWidth = activeSpace.clip.tracking.camera.sensor_width
            
            f = fAbs / sensorWidth * imgAspect
            #print(""fAbs"", fAbs, ""f rel"", f)
            Fu = self.relImgCoords2ImgPlaneCoords(vp1, imageWidth, imageHeight)
            Fv = self.computeSecondVanishingPoint(Fu, f, P, horizDir)
        else:
            '''
            calibration using two vanishing points
            '''
            if scn.optical_center_type == 'camdata':
                #get the principal point location from camera data
                P = [x for x in  activeSpace.clip.tracking.camera.principal]
                #print(""camera data optical center"", P[:])
                P[0] /= imageWidth
                P[1] /= imageHeight
                #print(""normlz. optical center"", P[:])
                P = self.relImgCoords2ImgPlaneCoords(P, imageWidth, imageHeight)
            elif scn.optical_center_type == 'compute':
                if len(vpLineSets) < 3:
                    self.report({'ERROR'}, ""A third grease pencil layer is needed to compute the optical center."")
                    return{'CANCELLED'}
                #compute the principal point using a vanishing point from a third gp layer.
                #this computation does not rely on the order of the line sets
                vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(len(vpLineSets))]
                vps = [self.relImgCoords2ImgPlaneCoords(vps[i], imageWidth, imageHeight) for i in range(len(vps))]
                P = self.computeTriangleOrthocenter(vps)
            else:
                #assume optical center in image midpoint
                pass
            
            #compute the two vanishing points
            vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(2)]    
        
            #order vanishing points along the image x axis
            if vps[1][0] < vps[0][0]:
                vps.reverse()
                vpLineSets.reverse()
                vpAxisIndices.reverse()            
        
            '''
            compute focal length
            '''
            Fu = self.relImgCoords2ImgPlaneCoords(vps[0], imageWidth, imageHeight)
            Fv = self.relImgCoords2ImgPlaneCoords(vps[1], imageWidth, imageHeight)
            
            f = self.computeFocalLength(Fu, Fv, P)
            
            if f == None:
                self.report({'ERROR'}, ""Failed to compute focal length. Invalid vanishing point constellation."")
                return{'CANCELLED'}
        
        '''
        compute camera orientation
        '''
        print(Fu, Fv, f)
        #initial orientation based on the vanishing points and focal length
        M = self.computeCameraRotationMatrix(Fu, Fv, f, P)
        
        #sanity check: M should be a pure rotation matrix, 
        #so its determinant should be 1
        eps = 0.00001
        if 1.0 - M.determinant() < -eps or 1.0 - M.determinant() > eps:
            self.report({'ERROR'}, ""Non unit rotation matrix determinant: "" + str(M.determinant()))    
            #return{'CANCELLED'} 
        
        #align the camera to the coordinate axes as specified
        M = self.alignCoordinateAxes(M, vpAxisIndices[0], vpAxisIndices[1])
        #apply the transform to the camera
        cam.matrix_world = M
        
        '''
        move the camera an arbitrary distance away from the ground plane
        TODO: focus on the origin or something
        '''
        cam.location = (0, 0, 2)
    
        #compute an absolute focal length in mm based 
        #on the current camera settings
        #TODO: make sure this works for all combinations of
        #image dimensions and camera sensor settings
        if imageWidth >= imageHeight:
            fMm = cam.data.sensor_height * f
        else:
            fMm = cam.data.sensor_width * f
        cam.data.lens = fMm
        self.report({'INFO'}, ""Camera focal length set to "" + str(fMm))
        
        #move principal point of the blender camera
        r = imageWidth / float(imageHeight)
        cam.data.shift_x = -1 * P[0] / r
        cam.data.shift_y = -1 * P[1] / r
        
        '''
        set the camera background image
        '''
        bpy.context.scene.render.resolution_x = imageWidth
        bpy.context.scene.render.resolution_y = imageHeight
        
        if setBgImg:
            bpy.ops.clip.set_viewport_background()
            
        return{'FINISHED'}",scn.optical_center_type == 'camdata',cannot be refactored by truth value test,Cannot refactor,1,"v1: scn.optical_center_type
v2: 'camdata'"
blam,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/blam/src/blam.py,https://github.com/stuffmatic/blam/tree/master/src/blam.py,CameraCalibrationOperator,execute$1789,"def execute(self, context):
        '''Executes the operator.
        \param context The context in which the operator was executed.
        '''
        scn = bpy.context.scene
        singleVp = scn.calibration_type == 'one_vp'
        useHorizonSegment = scn.use_horizon_segment
        setBgImg = scn.set_cambg
        
        '''
        get the active camera
        '''
        cam = scn.camera     
        if not cam:
            self.report({'ERROR'}, ""No active camera."")
            return{'CANCELLED'}
        
        '''
        check settings
        '''
        if singleVp:
            upAxisIndex = ['x', 'y', 'z'].index(scn.up_axis)
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            
            if upAxisIndex == vp1AxisIndex:
                self.report({'ERROR'}, ""The up axis cannot be parallel to the axis of the line set."")
                return{'CANCELLED'}    
            vp2AxisIndex = (set([0, 1, 2]) ^ set([upAxisIndex, vp1AxisIndex])).pop()
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
        else:
            vp1AxisIndex = ['x', 'y', 'z'].index(scn.vp1_axis)
            vp2AxisIndex = ['x', 'y', 'z'].index(scn.vp2_axis)
            vpAxisIndices = [vp1AxisIndex, vp2AxisIndex]
            setBgImg = scn.set_cambg
            
            if vpAxisIndices[0] == vpAxisIndices[1]:
                self.report({'ERROR'}, ""The two line sets cannot be parallel to the same axis."")
                return{'CANCELLED'}
        
        '''
        gather lines for each vanishing point
        '''        
        activeSpace = bpy.context.area.spaces.active
        
        if not activeSpace.clip:
            self.report({'ERROR'}, ""There is no active movie clip."")
            return{'CANCELLED'}
        
        #check that we have the number of layers we need
        if not activeSpace.clip.grease_pencil:
            self.report({'ERROR'}, ""There is no grease pencil datablock."")
            return{'CANCELLED'}
        gpl = activeSpace.clip.grease_pencil.layers
        if len(gpl) == 0:
            self.report({'ERROR'}, ""There are no grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and not singleVp:
            self.report({'ERROR'}, ""Calibration using two vanishing points requires two grease pencil layers."")
            return{'CANCELLED'}
        if len(gpl) < 2 and singleVp and useHorizonSegment:
            self.report({'ERROR'}, ""Single vanishing point calibration with a custom horizon line requires two grease pencil layers"")
            return{'CANCELLED'}
       
        vpLineSets = self.gatherGreasePencilSegments()
        
        #check that we have the expected number of line segment strokes
        if len(vpLineSets[0]) < 2:
            self.report({'ERROR'}, ""The first grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if not singleVp and len(vpLineSets[1]) < 2:
            self.report({'ERROR'}, ""The second grease pencil layer must contain at least two line segment strokes."")
            return{'CANCELLED'}
        if singleVp and useHorizonSegment and len(vpLineSets[1]) != 1:
            self.report({'ERROR'}, ""The second grease pencil layer must contain exactly one line segment stroke (the horizon line)."")
            return{'CANCELLED'}    
         
        '''
        get the principal point P in image plane coordinates
        TODO: get the value from the camera data panel, 
        currently always using the image center
        '''
        imageWidth = activeSpace.clip.size[0]
        imageHeight = activeSpace.clip.size[1]
        
        #principal point in image plane coordinates. 
        #in the middle of the image by default
        P = [0, 0]
        
        if singleVp:
            '''
            calibration using a single vanishing point
            '''
            imgAspect = imageWidth / float(imageHeight)
            
            #compute the horizon direction
            horizDir = normalize([1.0, 0.0]) #flat horizon by default
            if useHorizonSegment:
                xHorizDir = imgAspect * (vpLineSets[1][0][1][0] - vpLineSets[1][0][0][0])
                yHorizDir = vpLineSets[1][0][1][1] - vpLineSets[1][0][0][1]
                horizDir = normalize([-xHorizDir, -yHorizDir])
            #print(""horizDir"", horizDir)
            
            #compute the vanishing point location
            vp1 = self.computeIntersectionPointForLineSegments(vpLineSets[0])
            
            #get the current relative focal length
            fAbs = activeSpace.clip.tracking.camera.focal_length
            sensorWidth = activeSpace.clip.tracking.camera.sensor_width
            
            f = fAbs / sensorWidth * imgAspect
            #print(""fAbs"", fAbs, ""f rel"", f)
            Fu = self.relImgCoords2ImgPlaneCoords(vp1, imageWidth, imageHeight)
            Fv = self.computeSecondVanishingPoint(Fu, f, P, horizDir)
        else:
            '''
            calibration using two vanishing points
            '''
            if scn.optical_center_type == 'camdata':
                #get the principal point location from camera data
                P = [x for x in  activeSpace.clip.tracking.camera.principal]
                #print(""camera data optical center"", P[:])
                P[0] /= imageWidth
                P[1] /= imageHeight
                #print(""normlz. optical center"", P[:])
                P = self.relImgCoords2ImgPlaneCoords(P, imageWidth, imageHeight)
            elif scn.optical_center_type == 'compute':
                if len(vpLineSets) < 3:
                    self.report({'ERROR'}, ""A third grease pencil layer is needed to compute the optical center."")
                    return{'CANCELLED'}
                #compute the principal point using a vanishing point from a third gp layer.
                #this computation does not rely on the order of the line sets
                vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(len(vpLineSets))]
                vps = [self.relImgCoords2ImgPlaneCoords(vps[i], imageWidth, imageHeight) for i in range(len(vps))]
                P = self.computeTriangleOrthocenter(vps)
            else:
                #assume optical center in image midpoint
                pass
            
            #compute the two vanishing points
            vps = [self.computeIntersectionPointForLineSegments(vpLineSets[i]) for i in range(2)]    
        
            #order vanishing points along the image x axis
            if vps[1][0] < vps[0][0]:
                vps.reverse()
                vpLineSets.reverse()
                vpAxisIndices.reverse()            
        
            '''
            compute focal length
            '''
            Fu = self.relImgCoords2ImgPlaneCoords(vps[0], imageWidth, imageHeight)
            Fv = self.relImgCoords2ImgPlaneCoords(vps[1], imageWidth, imageHeight)
            
            f = self.computeFocalLength(Fu, Fv, P)
            
            if f == None:
                self.report({'ERROR'}, ""Failed to compute focal length. Invalid vanishing point constellation."")
                return{'CANCELLED'}
        
        '''
        compute camera orientation
        '''
        print(Fu, Fv, f)
        #initial orientation based on the vanishing points and focal length
        M = self.computeCameraRotationMatrix(Fu, Fv, f, P)
        
        #sanity check: M should be a pure rotation matrix, 
        #so its determinant should be 1
        eps = 0.00001
        if 1.0 - M.determinant() < -eps or 1.0 - M.determinant() > eps:
            self.report({'ERROR'}, ""Non unit rotation matrix determinant: "" + str(M.determinant()))    
            #return{'CANCELLED'} 
        
        #align the camera to the coordinate axes as specified
        M = self.alignCoordinateAxes(M, vpAxisIndices[0], vpAxisIndices[1])
        #apply the transform to the camera
        cam.matrix_world = M
        
        '''
        move the camera an arbitrary distance away from the ground plane
        TODO: focus on the origin or something
        '''
        cam.location = (0, 0, 2)
    
        #compute an absolute focal length in mm based 
        #on the current camera settings
        #TODO: make sure this works for all combinations of
        #image dimensions and camera sensor settings
        if imageWidth >= imageHeight:
            fMm = cam.data.sensor_height * f
        else:
            fMm = cam.data.sensor_width * f
        cam.data.lens = fMm
        self.report({'INFO'}, ""Camera focal length set to "" + str(fMm))
        
        #move principal point of the blender camera
        r = imageWidth / float(imageHeight)
        cam.data.shift_x = -1 * P[0] / r
        cam.data.shift_y = -1 * P[1] / r
        
        '''
        set the camera background image
        '''
        bpy.context.scene.render.resolution_x = imageWidth
        bpy.context.scene.render.resolution_y = imageHeight
        
        if setBgImg:
            bpy.ops.clip.set_viewport_background()
            
        return{'FINISHED'}",len(vpLineSets[1]) != 1,cannot be refactored by truth value test,Cannot refactor,1,"v1: len(vpLineSets[1])
v2: 1"
barman,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/barman/barman/cloud.py,https://github.com/EnterpriseDB/barman/tree/master/barman/cloud.py,,copyfileobj_pad_truncate$76,"def copyfileobj_pad_truncate(src, dst, length=None):
    """"""
    Copy length bytes from fileobj src to fileobj dst.
    If length is None, copy the entire content.
    This method is used by the TarFileIgnoringTruncate.addfile().
    """"""
    if length == 0:
        return

    if length is None:
        shutil.copyfileobj(src, dst, BUFSIZE)
        return

    blocks, remainder = divmod(length, BUFSIZE)
    for _ in range(blocks):
        buf = src.read(BUFSIZE)
        dst.write(buf)
        if len(buf) < BUFSIZE:
            # End of file reached
            # The file must have  been truncated, so pad with zeroes
            dst.write(tarfile.NUL * (BUFSIZE - len(buf)))

    if remainder != 0:
        buf = src.read(remainder)
        dst.write(buf)
        if len(buf) < remainder:
            # End of file reached
            # The file must have  been truncated, so pad with zeroes
            dst.write(tarfile.NUL * (remainder - len(buf)))",length == 0,not length,not length,1,"v1: length
v2: 0"
barman,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/barman/barman/cloud.py,https://github.com/EnterpriseDB/barman/tree/master/barman/cloud.py,,copyfileobj_pad_truncate$76,"def copyfileobj_pad_truncate(src, dst, length=None):
    """"""
    Copy length bytes from fileobj src to fileobj dst.
    If length is None, copy the entire content.
    This method is used by the TarFileIgnoringTruncate.addfile().
    """"""
    if length == 0:
        return

    if length is None:
        shutil.copyfileobj(src, dst, BUFSIZE)
        return

    blocks, remainder = divmod(length, BUFSIZE)
    for _ in range(blocks):
        buf = src.read(BUFSIZE)
        dst.write(buf)
        if len(buf) < BUFSIZE:
            # End of file reached
            # The file must have  been truncated, so pad with zeroes
            dst.write(tarfile.NUL * (BUFSIZE - len(buf)))

    if remainder != 0:
        buf = src.read(remainder)
        dst.write(buf)
        if len(buf) < remainder:
            # End of file reached
            # The file must have  been truncated, so pad with zeroes
            dst.write(tarfile.NUL * (remainder - len(buf)))",remainder != 0,remainder,remainder,1,"v1: remainder
v2: 0"
cfn-lint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cfn-lint/src/cfnlint/rules/resources/properties/AtLeastOne.py,https://github.com/aws-cloudformation/cfn-lint/tree/master/src/cfnlint/rules/resources/properties/AtLeastOne.py,AtLeastOne,check$36,"def check(self, properties, atleastoneprops, path, cfn):
        """"""Check itself""""""
        matches = []

        for atleastoneprop in atleastoneprops:
            for (safe_properties, safe_path) in properties.items_safe(path):
                property_sets = cfn.get_object_without_conditions(
                    safe_properties, atleastoneprop
                )
                for property_set in property_sets:
                    count = 0
                    for prop in atleastoneprop:
                        if prop in property_set[""Object""]:
                            count += 1

                    if count == 0:
                        if property_set[""Scenario""] is None:
                            message = (
                                ""At least one of [{0}] should be specified for {1}""
                            )
                            matches.append(
                                RuleMatch(
                                    path,
                                    message.format(
                                        "", "".join(map(str, atleastoneprop)),
                                        ""/"".join(map(str, safe_path)),
                                    ),
                                )
                            )
                        else:
                            scenario_text = "" and "".join(
                                [
                                    f'when condition ""{k}"" is {v}'
                                    for (k, v) in property_set[""Scenario""].items()
                                ]
                            )
                            message = (
                                ""At least one of [{0}] should be specified {1} at {2}""
                            )
                            matches.append(
                                RuleMatch(
                                    path,
                                    message.format(
                                        "", "".join(map(str, atleastoneprop)),
                                        scenario_text,
                                        ""/"".join(map(str, safe_path)),
                                    ),
                                )
                            )

        return matches",count == 0,not count,not count,1,"v1: count
v2: 0"
vega,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/algorithms/hpo/ea/ga.py,https://github.com/huawei-noah/vega/tree/master/vega/algorithms/hpo/ea/ga.py,GeneticAlgorithm,selection$89,"def selection(self):
        """"""Select pareto front individual.""""""
        ids = [id for id, value in self.scores.items() if ""score"" in value]
        if len(ids) == 0:
            return None
        rewards = [self.scores[id][""score""] for id in ids]
        indexes = get_pareto_index(np.array(rewards)).tolist()
        pareto = [id for i, id in enumerate(ids) if indexes[i]]

        if len(pareto) < 2:
            others = [id for id in self.scores if id not in pareto]
            pareto = pareto + random.sample(others, 2 - len(pareto))
        else:
            pareto = random.sample(pareto, 2)

        return [value[""config""] for id, value in self.scores.items() if id in pareto]",len(ids) == 0,not ids,not ids,1,"v1: len(ids)
v2: 0"
python-telegram-bot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-telegram-bot/tests/test_messageautodeletetimerchanged.py,https://github.com/python-telegram-bot/python-telegram-bot/tree/master/tests/test_messageautodeletetimerchanged.py,TestMessageAutoDeleteTimerChanged,test_de_json$31,"def test_de_json(self):
        json_dict = {""message_auto_delete_time"": self.message_auto_delete_time}
        madtc = MessageAutoDeleteTimerChanged.de_json(json_dict, None)
        assert madtc.api_kwargs == {}

        assert madtc.message_auto_delete_time == self.message_auto_delete_time",madtc.api_kwargs == {},not madtc.api_kwargs,not madtc.api_kwargs,1,"v1: madtc.api_kwargs
v2: {}"
python-telegram-bot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-telegram-bot/tests/test_messageautodeletetimerchanged.py,https://github.com/python-telegram-bot/python-telegram-bot/tree/master/tests/test_messageautodeletetimerchanged.py,TestMessageAutoDeleteTimerChanged,test_de_json$31,"def test_de_json(self):
        json_dict = {""message_auto_delete_time"": self.message_auto_delete_time}
        madtc = MessageAutoDeleteTimerChanged.de_json(json_dict, None)
        assert madtc.api_kwargs == {}

        assert madtc.message_auto_delete_time == self.message_auto_delete_time",madtc.message_auto_delete_time == self.message_auto_delete_time,cannot be refactored by truth value test,Cannot refactor,1,"v1: madtc.message_auto_delete_time
v2: self.message_auto_delete_time"
JsFormat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/JsFormat/libs/merge_utils.py,https://github.com/jdavisclark/JsFormat/tree/master/libs/merge_utils.py,,_merge_code$34,"def _merge_code(view, edit, code, formatted):
    def ss(start, end):
        return view.substr(sublime.Region(start, end))

    dmp = diff_match_patch()
    diffs = dmp.diff_main(code, formatted)
    dmp.diff_cleanupEfficiency(diffs)
    i = 0
    dirty = False
    for k, s in diffs:
        l = len(s)
        if k == 0:
            # match
            l = len(s)
            if ss(i, i + l) != s:
                raise MergeException('mismatch', dirty)
            i += l
        else:
            dirty = True
            if k > 0:
                # insert
                view.insert(edit, i, s)
                i += l
            else:
                # delete
                if ss(i, i + l) != s:
                    raise MergeException('mismatch', dirty)
                view.erase(edit, sublime.Region(i, i + l))
    return dirty","ss(i, i + l) != s",cannot be refactored by truth value test,Cannot refactor,1,"v1: ss(i, i + l)
v2: s"
JsFormat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/JsFormat/libs/merge_utils.py,https://github.com/jdavisclark/JsFormat/tree/master/libs/merge_utils.py,,_merge_code$34,"def _merge_code(view, edit, code, formatted):
    def ss(start, end):
        return view.substr(sublime.Region(start, end))

    dmp = diff_match_patch()
    diffs = dmp.diff_main(code, formatted)
    dmp.diff_cleanupEfficiency(diffs)
    i = 0
    dirty = False
    for k, s in diffs:
        l = len(s)
        if k == 0:
            # match
            l = len(s)
            if ss(i, i + l) != s:
                raise MergeException('mismatch', dirty)
            i += l
        else:
            dirty = True
            if k > 0:
                # insert
                view.insert(edit, i, s)
                i += l
            else:
                # delete
                if ss(i, i + l) != s:
                    raise MergeException('mismatch', dirty)
                view.erase(edit, sublime.Region(i, i + l))
    return dirty",k == 0,not k,not k,1,"v1: k
v2: 0"
JsFormat,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/JsFormat/libs/merge_utils.py,https://github.com/jdavisclark/JsFormat/tree/master/libs/merge_utils.py,,_merge_code$34,"def _merge_code(view, edit, code, formatted):
    def ss(start, end):
        return view.substr(sublime.Region(start, end))

    dmp = diff_match_patch()
    diffs = dmp.diff_main(code, formatted)
    dmp.diff_cleanupEfficiency(diffs)
    i = 0
    dirty = False
    for k, s in diffs:
        l = len(s)
        if k == 0:
            # match
            l = len(s)
            if ss(i, i + l) != s:
                raise MergeException('mismatch', dirty)
            i += l
        else:
            dirty = True
            if k > 0:
                # insert
                view.insert(edit, i, s)
                i += l
            else:
                # delete
                if ss(i, i + l) != s:
                    raise MergeException('mismatch', dirty)
                view.erase(edit, sublime.Region(i, i + l))
    return dirty","ss(i, i + l) != s",cannot be refactored by truth value test,Cannot refactor,1,"v1: ss(i, i + l)
v2: s"
clusterfuzz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/appengine/handlers/testcase_detail/show.py,https://github.com/google/clusterfuzz/tree/master/src/appengine/handlers/testcase_detail/show.py,,highlight_common_stack_frames$99,"def highlight_common_stack_frames(crash_stacktrace):
  """"""Highlights common stack frames between first two stacks.""""""
  crash_stacks = [[]]
  highlighted_crash_stacktrace_lines = []
  old_frame_no = 0
  stack_index = 0
  stack_trace_line_format = '^ *#([0-9]+) *0x[0-9a-f]+ (.*)'

  for line in crash_stacktrace.splitlines():
    # Stacktrace separator prefix.
    if stack_index and line.startswith('+-'):
      break

    match = re.match(stack_trace_line_format, line)
    if match:
      frame_no = int(match.group(1))

      # This means we encountered another stack like free or alloc stack.
      if old_frame_no > frame_no:
        stack_index += 1
        crash_stacks.append([])

      crash_stacks[stack_index].append(match.group(2))
      old_frame_no = frame_no

  # If we have just one crash stack and no other stack,
  # then nothing to highlight.
  if stack_index == 0:
    return crash_stacktrace

  # Compare stack frames between first two stacks.
  match_index = -1
  start_index_crash_stack_1 = len(crash_stacks[0]) - 1
  start_index_crash_stack_2 = len(crash_stacks[1]) - 1
  while True:
    if (crash_stacks[0][start_index_crash_stack_1] !=
        crash_stacks[1][start_index_crash_stack_2]):
      break

    match_index = [start_index_crash_stack_1, start_index_crash_stack_2]

    if not start_index_crash_stack_1:
      break
    if not start_index_crash_stack_2:
      break

    start_index_crash_stack_1 -= 1
    start_index_crash_stack_2 -= 1

  # No match found, nothing to highlight.
  if match_index == -1:
    return crash_stacktrace

  old_frame_no = 0
  stack_index = 0
  frame_index = -1
  for line in crash_stacktrace.splitlines():
    match = re.match(stack_trace_line_format, line)
    if match:
      frame_no = int(match.group(1))

      # This means we encountered another stack like free or alloc stack.
      if old_frame_no > frame_no:
        stack_index += 1
        frame_index = -1

      frame_index += 1
      old_frame_no = frame_no

      # We only care about highlighting the first two stacks.
      if stack_index <= 1 and frame_index >= match_index[stack_index]:
        line = '<b>%s</b>' % line

    highlighted_crash_stacktrace_lines.append(line)

  return '\n'.join(highlighted_crash_stacktrace_lines)",match_index == -1,cannot be refactored by truth value test,Cannot refactor,1,"v1: match_index
v2: -1"
clusterfuzz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/appengine/handlers/testcase_detail/show.py,https://github.com/google/clusterfuzz/tree/master/src/appengine/handlers/testcase_detail/show.py,,highlight_common_stack_frames$99,"def highlight_common_stack_frames(crash_stacktrace):
  """"""Highlights common stack frames between first two stacks.""""""
  crash_stacks = [[]]
  highlighted_crash_stacktrace_lines = []
  old_frame_no = 0
  stack_index = 0
  stack_trace_line_format = '^ *#([0-9]+) *0x[0-9a-f]+ (.*)'

  for line in crash_stacktrace.splitlines():
    # Stacktrace separator prefix.
    if stack_index and line.startswith('+-'):
      break

    match = re.match(stack_trace_line_format, line)
    if match:
      frame_no = int(match.group(1))

      # This means we encountered another stack like free or alloc stack.
      if old_frame_no > frame_no:
        stack_index += 1
        crash_stacks.append([])

      crash_stacks[stack_index].append(match.group(2))
      old_frame_no = frame_no

  # If we have just one crash stack and no other stack,
  # then nothing to highlight.
  if stack_index == 0:
    return crash_stacktrace

  # Compare stack frames between first two stacks.
  match_index = -1
  start_index_crash_stack_1 = len(crash_stacks[0]) - 1
  start_index_crash_stack_2 = len(crash_stacks[1]) - 1
  while True:
    if (crash_stacks[0][start_index_crash_stack_1] !=
        crash_stacks[1][start_index_crash_stack_2]):
      break

    match_index = [start_index_crash_stack_1, start_index_crash_stack_2]

    if not start_index_crash_stack_1:
      break
    if not start_index_crash_stack_2:
      break

    start_index_crash_stack_1 -= 1
    start_index_crash_stack_2 -= 1

  # No match found, nothing to highlight.
  if match_index == -1:
    return crash_stacktrace

  old_frame_no = 0
  stack_index = 0
  frame_index = -1
  for line in crash_stacktrace.splitlines():
    match = re.match(stack_trace_line_format, line)
    if match:
      frame_no = int(match.group(1))

      # This means we encountered another stack like free or alloc stack.
      if old_frame_no > frame_no:
        stack_index += 1
        frame_index = -1

      frame_index += 1
      old_frame_no = frame_no

      # We only care about highlighting the first two stacks.
      if stack_index <= 1 and frame_index >= match_index[stack_index]:
        line = '<b>%s</b>' % line

    highlighted_crash_stacktrace_lines.append(line)

  return '\n'.join(highlighted_crash_stacktrace_lines)",crash_stacks[0][start_index_crash_stack_1] != crash_stacks[1][start_index_crash_stack_2],cannot be refactored by truth value test,Cannot refactor,1,"v1: crash_stacks[0][start_index_crash_stack_1]
v2: crash_stacks[1][start_index_crash_stack_2]"
clusterfuzz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/clusterfuzz/src/appengine/handlers/testcase_detail/show.py,https://github.com/google/clusterfuzz/tree/master/src/appengine/handlers/testcase_detail/show.py,,highlight_common_stack_frames$99,"def highlight_common_stack_frames(crash_stacktrace):
  """"""Highlights common stack frames between first two stacks.""""""
  crash_stacks = [[]]
  highlighted_crash_stacktrace_lines = []
  old_frame_no = 0
  stack_index = 0
  stack_trace_line_format = '^ *#([0-9]+) *0x[0-9a-f]+ (.*)'

  for line in crash_stacktrace.splitlines():
    # Stacktrace separator prefix.
    if stack_index and line.startswith('+-'):
      break

    match = re.match(stack_trace_line_format, line)
    if match:
      frame_no = int(match.group(1))

      # This means we encountered another stack like free or alloc stack.
      if old_frame_no > frame_no:
        stack_index += 1
        crash_stacks.append([])

      crash_stacks[stack_index].append(match.group(2))
      old_frame_no = frame_no

  # If we have just one crash stack and no other stack,
  # then nothing to highlight.
  if stack_index == 0:
    return crash_stacktrace

  # Compare stack frames between first two stacks.
  match_index = -1
  start_index_crash_stack_1 = len(crash_stacks[0]) - 1
  start_index_crash_stack_2 = len(crash_stacks[1]) - 1
  while True:
    if (crash_stacks[0][start_index_crash_stack_1] !=
        crash_stacks[1][start_index_crash_stack_2]):
      break

    match_index = [start_index_crash_stack_1, start_index_crash_stack_2]

    if not start_index_crash_stack_1:
      break
    if not start_index_crash_stack_2:
      break

    start_index_crash_stack_1 -= 1
    start_index_crash_stack_2 -= 1

  # No match found, nothing to highlight.
  if match_index == -1:
    return crash_stacktrace

  old_frame_no = 0
  stack_index = 0
  frame_index = -1
  for line in crash_stacktrace.splitlines():
    match = re.match(stack_trace_line_format, line)
    if match:
      frame_no = int(match.group(1))

      # This means we encountered another stack like free or alloc stack.
      if old_frame_no > frame_no:
        stack_index += 1
        frame_index = -1

      frame_index += 1
      old_frame_no = frame_no

      # We only care about highlighting the first two stacks.
      if stack_index <= 1 and frame_index >= match_index[stack_index]:
        line = '<b>%s</b>' % line

    highlighted_crash_stacktrace_lines.append(line)

  return '\n'.join(highlighted_crash_stacktrace_lines)",stack_index == 0,not stack_index,not stack_index,1,"v1: stack_index
v2: 0"
enterprise_gateway,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/enterprise_gateway/enterprise_gateway/services/processproxies/yarn.py,https://github.com/jupyter/enterprise_gateway/tree/master/enterprise_gateway/services/processproxies/yarn.py,YarnClusterProcessProxy,send_signal$281,"def send_signal(self, signum: int) -> bool | None:
        """"""Currently only support 0 as poll and other as kill.

        :param signum
        :return:
        """"""
        if signum == 0:
            return self.poll()
        elif signum == signal.SIGKILL:
            return self.kill()
        else:
            # Yarn api doesn't support the equivalent to interrupts, so take our chances
            # via a remote signal.  Note that this condition cannot check against the
            # signum value because altternate interrupt signals might be in play.
            return super().send_signal(signum)",signum == 0,not signum,not signum,1,"v1: signum
v2: 0"
enterprise_gateway,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/enterprise_gateway/enterprise_gateway/services/processproxies/yarn.py,https://github.com/jupyter/enterprise_gateway/tree/master/enterprise_gateway/services/processproxies/yarn.py,YarnClusterProcessProxy,send_signal$281,"def send_signal(self, signum: int) -> bool | None:
        """"""Currently only support 0 as poll and other as kill.

        :param signum
        :return:
        """"""
        if signum == 0:
            return self.poll()
        elif signum == signal.SIGKILL:
            return self.kill()
        else:
            # Yarn api doesn't support the equivalent to interrupts, so take our chances
            # via a remote signal.  Note that this condition cannot check against the
            # signum value because altternate interrupt signals might be in play.
            return super().send_signal(signum)",signum == signal.SIGKILL,cannot be refactored by truth value test,Cannot refactor,1,"v1: signum
v2: signal.SIGKILL"
vcrpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vcrpy/tests/integration/test_tornado.py,https://github.com/kevin1024/vcrpy/tree/master/tests/integration/test_tornado.py,,test_cross_scheme$169,"def test_cross_scheme(get_client, tmpdir, scheme):
    """"""Ensure that requests between schemes are treated separately""""""
    # First fetch a url under http, and then again under https and then
    # ensure that we haven't served anything out of cache, and we have two
    # requests / response pairs in the cassette
    with vcr.use_cassette(str(tmpdir.join(""cross_scheme.yaml""))) as cass:
        yield get(get_client(), ""https://httpbin.org/"")
        yield get(get_client(), ""http://httpbin.org/"")
        assert cass.play_count == 0
        assert len(cass) == 2

    # Then repeat the same requests and ensure both were replayed.
    with vcr.use_cassette(str(tmpdir.join(""cross_scheme.yaml""))) as cass:
        yield get(get_client(), ""https://httpbin.org/"")
        yield get(get_client(), ""http://httpbin.org/"")
        assert cass.play_count == 2",len(cass) == 2,cannot be refactored by truth value test,Cannot refactor,1,"v1: len(cass)
v2: 2"
vcrpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vcrpy/tests/integration/test_tornado.py,https://github.com/kevin1024/vcrpy/tree/master/tests/integration/test_tornado.py,,test_cross_scheme$169,"def test_cross_scheme(get_client, tmpdir, scheme):
    """"""Ensure that requests between schemes are treated separately""""""
    # First fetch a url under http, and then again under https and then
    # ensure that we haven't served anything out of cache, and we have two
    # requests / response pairs in the cassette
    with vcr.use_cassette(str(tmpdir.join(""cross_scheme.yaml""))) as cass:
        yield get(get_client(), ""https://httpbin.org/"")
        yield get(get_client(), ""http://httpbin.org/"")
        assert cass.play_count == 0
        assert len(cass) == 2

    # Then repeat the same requests and ensure both were replayed.
    with vcr.use_cassette(str(tmpdir.join(""cross_scheme.yaml""))) as cass:
        yield get(get_client(), ""https://httpbin.org/"")
        yield get(get_client(), ""http://httpbin.org/"")
        assert cass.play_count == 2",cass.play_count == 2,cannot be refactored by truth value test,Cannot refactor,1,"v1: cass.play_count
v2: 2"
vcrpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vcrpy/tests/integration/test_tornado.py,https://github.com/kevin1024/vcrpy/tree/master/tests/integration/test_tornado.py,,test_cross_scheme$169,"def test_cross_scheme(get_client, tmpdir, scheme):
    """"""Ensure that requests between schemes are treated separately""""""
    # First fetch a url under http, and then again under https and then
    # ensure that we haven't served anything out of cache, and we have two
    # requests / response pairs in the cassette
    with vcr.use_cassette(str(tmpdir.join(""cross_scheme.yaml""))) as cass:
        yield get(get_client(), ""https://httpbin.org/"")
        yield get(get_client(), ""http://httpbin.org/"")
        assert cass.play_count == 0
        assert len(cass) == 2

    # Then repeat the same requests and ensure both were replayed.
    with vcr.use_cassette(str(tmpdir.join(""cross_scheme.yaml""))) as cass:
        yield get(get_client(), ""https://httpbin.org/"")
        yield get(get_client(), ""http://httpbin.org/"")
        assert cass.play_count == 2",cass.play_count == 0,not cass.play_count,not cass.play_count,1,"v1: cass.play_count
v2: 0"
textflint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/textflint/test/common/utils/test_list_op.py,https://github.com/textflint/textflint/tree/master/test/common/utils/test_list_op.py,TestListOp,test_insert_after_index$240,"def test_insert_after_index(self):
        self.assertEqual(
            [1, 0, 2, 3, 4], insert_after_index([1, 2, 3, 4], 0, [0]))
        self.assertEqual(
            [1, 0, 2, 3, 4], insert_after_index([1, 2, 3, 4], 0, 0))
        self.assertEqual(
            [1, 2, 3, 4, 0, 1, 2],
            insert_after_index([1, 2, 3, 4], 3, [0, 1, 2]))
        self.assertEqual(
            [1, 2, 3, 4, 5], insert_after_index([1, 2, 3, 4], 3, [5]))

        # Test successive replacement of individual elements
        # TODO merge
        num = random.randint(200, 1000)
        n = random.randint(20, 99)
        pos = random.sample(list(range(1, num - 1)), n)
        pos += [0, num]
        n += 2
        pos.sort()

        if n % 2 == 0:
            pos = pos[:-1]
            n -= 1
            pos[n - 1] = num
        test_pos = [[pos[i * 2 + 1], pos[i * 2 + 2]] for i in range(int(n / 2))]
        test_list = []
        origin = list(range(num))

        for i in range(int(n / 2)):
            test_list += origin[pos[2 * i]:pos[2 * i + 1]]
        random.shuffle(test_pos)
        for i in range(len(test_pos) - 1):
            test_list = insert_after_index(
                test_list,
                test_list.index(test_pos[i][0] - 1),
                list(range(test_pos[i][0], test_pos[i][1])))
        self.assertEqual(
            origin,
            insert_after_index(
                test_list,
                test_list.index(test_pos[len(test_pos) - 1][0] - 1),
                list(range(test_pos[len(test_pos) - 1][0],
                           test_pos[len(test_pos) - 1][1]))))",n % 2 == 0,not n % 2,not n % 2,1,"v1: n % 2
v2: 0"
scikit-survival,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scikit-survival/tests/test_survival_svm.py,https://github.com/sebp/scikit-survival/tree/master/tests/test_survival_svm.py,TestFastSurvivalSVM,test_fit_and_predict_regression_no_intercept$368,"def test_fit_and_predict_regression_no_intercept(make_whas500, optimizer_regression):
        whas500 = make_whas500(to_numeric=True)

        ssvm = FastSurvivalSVM(optimizer=optimizer_regression, rank_ratio=0.0,
                               max_iter=50, fit_intercept=False, random_state=0)
        ssvm.fit(whas500.x, whas500.y)

        assert not hasattr(ssvm, ""intercept_"")
        expected_coef = numpy.array([1.39989875, -1.16903161, -0.40195857, -0.05848903, -0.08421557, 4.11924729,
                                     0.25135451, 1.89067276, -0.25751401, -0.10213143, 1.56333622, 3.10136873,
                                     -2.23644848, -0.11620715])
        assert_array_almost_equal(expected_coef, ssvm.coef_)

        pred = ssvm.predict(whas500.x)
        rmse = numpy.sqrt(mean_squared_error(whas500.y['lenfol'], pred))
        assert round(abs(15838.510668936022 - rmse), 7) == 0","round(abs(15838.510668936022 - rmse), 7) == 0","not round(abs(15838.510668936022 - rmse), 7)","not round(abs(15838.510668936022 - rmse), 7)",1,"v1: round(abs(15838.510668936022 - rmse), 7)
v2: 0"
mmdetection3d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection3d/mmdet3d/models/necks/imvoxel_neck.py,https://github.com/open-mmlab/mmdetection3d/tree/master/mmdet3d/models/necks/imvoxel_neck.py,IndoorImVoxelNeck,_make_layer$119,"def _make_layer(stride, n_channels, n_blocks):
        """"""Make a layer from several residual blocks.

        Args:
            stride (int): Stride of the first residual block.
            n_channels (int): Number of channels of the first residual block.
            n_blocks (int): Number of residual blocks.

        Returns:
            torch.nn.Module: With several residual blocks.
        """"""
        blocks = []
        for i in range(n_blocks):
            if i == 0 and stride != 1:
                blocks.append(ResModule(n_channels, n_channels * 2, stride))
                n_channels = n_channels * 2
            else:
                blocks.append(ResModule(n_channels, n_channels))
        return nn.Sequential(*blocks)",i == 0,not i,not i,1,"v1: i
v2: 0"
mmdetection3d,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection3d/mmdet3d/models/necks/imvoxel_neck.py,https://github.com/open-mmlab/mmdetection3d/tree/master/mmdet3d/models/necks/imvoxel_neck.py,IndoorImVoxelNeck,_make_layer$119,"def _make_layer(stride, n_channels, n_blocks):
        """"""Make a layer from several residual blocks.

        Args:
            stride (int): Stride of the first residual block.
            n_channels (int): Number of channels of the first residual block.
            n_blocks (int): Number of residual blocks.

        Returns:
            torch.nn.Module: With several residual blocks.
        """"""
        blocks = []
        for i in range(n_blocks):
            if i == 0 and stride != 1:
                blocks.append(ResModule(n_channels, n_channels * 2, stride))
                n_channels = n_channels * 2
            else:
                blocks.append(ResModule(n_channels, n_channels))
        return nn.Sequential(*blocks)",stride != 1,cannot be refactored by truth value test,Cannot refactor,1,"v1: stride
v2: 1"
fedlearner,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fedlearner/fedlearner/data_join/rsa_psi/rsa_psi_component.py,https://github.com/bytedance/fedlearner/tree/master/fedlearner/data_join/rsa_psi/rsa_psi_component.py,FollowerPsiRsaSigner,_rpc_sign_callback$649,"def _rpc_sign_callback(self, ctx, stub, rpc_future):
        try:
            response = rpc_future.result()
            if response.status.code != 0:
                raise RuntimeError(""Failed to call rpc for psi sign, ""\
                                   ""error code: {}, error message: {}"".format(
                                        response.status.code,
                                        response.status.error_message))
            ctx.trigger_rpc_finished()
            self._add_sign_stats(ctx.rpc_sign_duration(),
                                 ctx.rpc_pending_duration(),
                                 ctx.retry_cnt)
            self._revert_stub(stub, False)
            signed_blinded_hashed_ids = [bytes2int(item) for
                                         item in response.signed_ids]
            assert len(ctx.raw_id_batch) == len(signed_blinded_hashed_ids)
            self._callback_submitter.submit(self._deblind_signed_id_func,
                                            ctx, signed_blinded_hashed_ids)
            next_ctxs = []
            with self._lock:
                assert self._flying_rpc_num > 0
                self._flying_rpc_num -= 1
                req_num = self._flying_sign_rpc_threshold - self._flying_rpc_num
                if req_num > 0:
                    next_ctxs = self._pending_rpc_sign_ctx[:req_num]
                    self._pending_rpc_sign_ctx = \
                            self._pending_rpc_sign_ctx[req_num:]
            for nctx in next_ctxs:
                self._rpc_sign_func(nctx)
        except Exception as e: # pylint: disable=broad-except
            self._revert_stub(stub, True)
            begin_index = ctx.raw_id_batch.begin_index
            end_index = begin_index + len(ctx.raw_id_batch)
            logging.warning(""psi signer batch[%d, %d) sign ""\
                            ""failed for %d times, reson:%s. ""\
                            ""retry again"", begin_index,
                            end_index, ctx.retry_cnt, e)
            with self._lock:
                assert self._flying_rpc_num > 0
                self._flying_rpc_num -= 1
            ctx.trigger_retry()
            self._rpc_sign_func(ctx)",response.status.code != 0,response.status.code,response.status.code,1,"v1: response.status.code
v2: 0"
fedlearner,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fedlearner/fedlearner/data_join/rsa_psi/rsa_psi_component.py,https://github.com/bytedance/fedlearner/tree/master/fedlearner/data_join/rsa_psi/rsa_psi_component.py,FollowerPsiRsaSigner,_rpc_sign_callback$649,"def _rpc_sign_callback(self, ctx, stub, rpc_future):
        try:
            response = rpc_future.result()
            if response.status.code != 0:
                raise RuntimeError(""Failed to call rpc for psi sign, ""\
                                   ""error code: {}, error message: {}"".format(
                                        response.status.code,
                                        response.status.error_message))
            ctx.trigger_rpc_finished()
            self._add_sign_stats(ctx.rpc_sign_duration(),
                                 ctx.rpc_pending_duration(),
                                 ctx.retry_cnt)
            self._revert_stub(stub, False)
            signed_blinded_hashed_ids = [bytes2int(item) for
                                         item in response.signed_ids]
            assert len(ctx.raw_id_batch) == len(signed_blinded_hashed_ids)
            self._callback_submitter.submit(self._deblind_signed_id_func,
                                            ctx, signed_blinded_hashed_ids)
            next_ctxs = []
            with self._lock:
                assert self._flying_rpc_num > 0
                self._flying_rpc_num -= 1
                req_num = self._flying_sign_rpc_threshold - self._flying_rpc_num
                if req_num > 0:
                    next_ctxs = self._pending_rpc_sign_ctx[:req_num]
                    self._pending_rpc_sign_ctx = \
                            self._pending_rpc_sign_ctx[req_num:]
            for nctx in next_ctxs:
                self._rpc_sign_func(nctx)
        except Exception as e: # pylint: disable=broad-except
            self._revert_stub(stub, True)
            begin_index = ctx.raw_id_batch.begin_index
            end_index = begin_index + len(ctx.raw_id_batch)
            logging.warning(""psi signer batch[%d, %d) sign ""\
                            ""failed for %d times, reson:%s. ""\
                            ""retry again"", begin_index,
                            end_index, ctx.retry_cnt, e)
            with self._lock:
                assert self._flying_rpc_num > 0
                self._flying_rpc_num -= 1
            ctx.trigger_retry()
            self._rpc_sign_func(ctx)",len(ctx.raw_id_batch) == len(signed_blinded_hashed_ids),cannot be refactored by truth value test,Cannot refactor,1,"v1: len(ctx.raw_id_batch)
v2: len(signed_blinded_hashed_ids)"
eralchemy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/eralchemy/eralchemy/helpers.py,https://github.com/Alexis-benoist/eralchemy/tree/master/eralchemy/helpers.py,,check_args$11,"def check_args(args):
    """"""Checks that the args are coherent.""""""
    check_args_has_attributes(args)
    if args.v:
        non_version_attrs = [v for k, v in args.__dict__.items() if k != 'v']
        print('non_version_attrs', non_version_attrs)
        if len([v for v in non_version_attrs if v is not None]) != 0:
            fail('Cannot show the version number with another command.')
        return
    if args.i is None:
        fail('Cannot draw ER diagram of no database.')
    if args.o is None:
        fail('Cannot draw ER diagram with no output file.')",len([v for v in non_version_attrs if v is not None]) != 0,[v for v in non_version_attrs if v is not None],[v for v in non_version_attrs if v is not None],1,"v1: len([v for v in non_version_attrs if v is not None])
v2: 0"
dcc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dcc/androguard/core/bytecodes/dvm.py,https://github.com/amimo/dcc/tree/master/androguard/core/bytecodes/dvm.py,ParameterAnnotation,get_obj$926,"def get_obj(self):
        if self.annotations_off != 0:
            self.annotations_off = self.CM.get_obj_by_offset(
                self.annotations_off).get_off()

        return pack(""=I"", self.method_idx) + pack(""=I"", self.annotations_off)",self.annotations_off != 0,self.annotations_off,self.annotations_off,1,"v1: self.annotations_off
v2: 0"
great_expectations,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/great_expectations/tests/datasource/data_connector/test_configured_asset_filesystem_data_connector.py,https://github.com/great-expectations/great_expectations/tree/master/tests/datasource/data_connector/test_configured_asset_filesystem_data_connector.py,,test_basic_instantiation$28,"def test_basic_instantiation(tmp_path_factory):
    base_directory = str(tmp_path_factory.mktemp(""test_basic_instantiation""))
    create_files_in_directory(
        directory=base_directory,
        file_name_list=[
            ""alpha-1.csv"",
            ""alpha-2.csv"",
            ""alpha-3.csv"",
        ],
    )

    my_data_connector = ConfiguredAssetFilesystemDataConnector(
        name=""my_data_connector"",
        datasource_name=""FAKE_DATASOURCE_NAME"",
        execution_engine=PandasExecutionEngine(),
        default_regex={
            ""pattern"": ""alpha-(.*)\\.csv"",
            ""group_names"": [""index""],
        },
        base_directory=base_directory,
        assets={""alpha"": {}},
    )

    assert my_data_connector.self_check() == {
        ""class_name"": ""ConfiguredAssetFilesystemDataConnector"",
        ""data_asset_count"": 1,
        ""example_data_asset_names"": [
            ""alpha"",
        ],
        ""data_assets"": {
            ""alpha"": {
                ""example_data_references"": [
                    ""alpha-1.csv"",
                    ""alpha-2.csv"",
                    ""alpha-3.csv"",
                ],
                ""batch_definition_count"": 3,
            },
        },
        ""example_unmatched_data_references"": [],
        ""unmatched_data_reference_count"": 0,
        # FIXME: (Sam) example_data_reference removed temporarily in PR #2590:
        # ""example_data_reference"": {},
    }

    # noinspection PyProtectedMember
    my_data_connector._refresh_data_references_cache()
    assert my_data_connector.get_data_reference_list_count() == 3
    assert my_data_connector.get_unmatched_data_references() == []

    # Illegal execution environment name
    with pytest.raises(ValueError):
        print(
            my_data_connector.get_batch_definition_list_from_batch_request(
                BatchRequest(
                    datasource_name=""something"",
                    data_connector_name=""my_data_connector"",
                    data_asset_name=""something"",
                )
            )
        )",my_data_connector.get_unmatched_data_references() == [],not my_data_connector.get_unmatched_data_references(),not my_data_connector.get_unmatched_data_references(),1,"v1: my_data_connector.get_unmatched_data_references()
v2: []"
great_expectations,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/great_expectations/tests/datasource/data_connector/test_configured_asset_filesystem_data_connector.py,https://github.com/great-expectations/great_expectations/tree/master/tests/datasource/data_connector/test_configured_asset_filesystem_data_connector.py,,test_basic_instantiation$28,"def test_basic_instantiation(tmp_path_factory):
    base_directory = str(tmp_path_factory.mktemp(""test_basic_instantiation""))
    create_files_in_directory(
        directory=base_directory,
        file_name_list=[
            ""alpha-1.csv"",
            ""alpha-2.csv"",
            ""alpha-3.csv"",
        ],
    )

    my_data_connector = ConfiguredAssetFilesystemDataConnector(
        name=""my_data_connector"",
        datasource_name=""FAKE_DATASOURCE_NAME"",
        execution_engine=PandasExecutionEngine(),
        default_regex={
            ""pattern"": ""alpha-(.*)\\.csv"",
            ""group_names"": [""index""],
        },
        base_directory=base_directory,
        assets={""alpha"": {}},
    )

    assert my_data_connector.self_check() == {
        ""class_name"": ""ConfiguredAssetFilesystemDataConnector"",
        ""data_asset_count"": 1,
        ""example_data_asset_names"": [
            ""alpha"",
        ],
        ""data_assets"": {
            ""alpha"": {
                ""example_data_references"": [
                    ""alpha-1.csv"",
                    ""alpha-2.csv"",
                    ""alpha-3.csv"",
                ],
                ""batch_definition_count"": 3,
            },
        },
        ""example_unmatched_data_references"": [],
        ""unmatched_data_reference_count"": 0,
        # FIXME: (Sam) example_data_reference removed temporarily in PR #2590:
        # ""example_data_reference"": {},
    }

    # noinspection PyProtectedMember
    my_data_connector._refresh_data_references_cache()
    assert my_data_connector.get_data_reference_list_count() == 3
    assert my_data_connector.get_unmatched_data_references() == []

    # Illegal execution environment name
    with pytest.raises(ValueError):
        print(
            my_data_connector.get_batch_definition_list_from_batch_request(
                BatchRequest(
                    datasource_name=""something"",
                    data_connector_name=""my_data_connector"",
                    data_asset_name=""something"",
                )
            )
        )",my_data_connector.get_data_reference_list_count() == 3,cannot be refactored by truth value test,Cannot refactor,1,"v1: my_data_connector.get_data_reference_list_count()
v2: 3"
great_expectations,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/great_expectations/tests/datasource/data_connector/test_configured_asset_filesystem_data_connector.py,https://github.com/great-expectations/great_expectations/tree/master/tests/datasource/data_connector/test_configured_asset_filesystem_data_connector.py,,test_basic_instantiation$28,"def test_basic_instantiation(tmp_path_factory):
    base_directory = str(tmp_path_factory.mktemp(""test_basic_instantiation""))
    create_files_in_directory(
        directory=base_directory,
        file_name_list=[
            ""alpha-1.csv"",
            ""alpha-2.csv"",
            ""alpha-3.csv"",
        ],
    )

    my_data_connector = ConfiguredAssetFilesystemDataConnector(
        name=""my_data_connector"",
        datasource_name=""FAKE_DATASOURCE_NAME"",
        execution_engine=PandasExecutionEngine(),
        default_regex={
            ""pattern"": ""alpha-(.*)\\.csv"",
            ""group_names"": [""index""],
        },
        base_directory=base_directory,
        assets={""alpha"": {}},
    )

    assert my_data_connector.self_check() == {
        ""class_name"": ""ConfiguredAssetFilesystemDataConnector"",
        ""data_asset_count"": 1,
        ""example_data_asset_names"": [
            ""alpha"",
        ],
        ""data_assets"": {
            ""alpha"": {
                ""example_data_references"": [
                    ""alpha-1.csv"",
                    ""alpha-2.csv"",
                    ""alpha-3.csv"",
                ],
                ""batch_definition_count"": 3,
            },
        },
        ""example_unmatched_data_references"": [],
        ""unmatched_data_reference_count"": 0,
        # FIXME: (Sam) example_data_reference removed temporarily in PR #2590:
        # ""example_data_reference"": {},
    }

    # noinspection PyProtectedMember
    my_data_connector._refresh_data_references_cache()
    assert my_data_connector.get_data_reference_list_count() == 3
    assert my_data_connector.get_unmatched_data_references() == []

    # Illegal execution environment name
    with pytest.raises(ValueError):
        print(
            my_data_connector.get_batch_definition_list_from_batch_request(
                BatchRequest(
                    datasource_name=""something"",
                    data_connector_name=""my_data_connector"",
                    data_asset_name=""something"",
                )
            )
        )","my_data_connector.self_check() == {'class_name': 'ConfiguredAssetFilesystemDataConnector', 'data_asset_count': 1, 'example_data_asset_names': ['alpha'], 'data_assets': {'alpha': {'example_data_references': ['alpha-1.csv', 'alpha-2.csv', 'alpha-3.csv'], 'batch_definition_count': 3}}, 'example_unmatched_data_references': [], 'unmatched_data_reference_count': 0}",cannot be refactored by truth value test,Cannot refactor,1,"v1: my_data_connector.self_check()

v2: {'class_name': 'ConfiguredAssetFilesystemDataConnector', 'data_asset_count': 1, 'example_data_asset_names': ['alpha'], 'data_assets': {'alpha': {'example_data_references': ['alpha-1.csv', 'alpha-2.csv', 'alpha-3.csv'], 'batch_definition_count': 3}}, 'example_unmatched_data_references': [], 'unmatched_data_reference_count': 0}"
hydrus,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydrus/hydrus/client/ClientServices.py,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/ClientServices.py,ServiceIPFS,PinDirectory$2971,"def PinDirectory( self, hashes, note ):
        
        job_key = ClientThreading.JobKey( pausable = True, cancellable = True )
        
        job_key.SetStatusTitle( 'creating ipfs directory on ' + self._name )
        
        HG.client_controller.pub( 'message', job_key )
        
        try:
            
            file_info = []
            
            hashes = sorted( hashes )
            
            for ( i, hash ) in enumerate( hashes ):
                
                ( i_paused, should_quit ) = job_key.WaitIfNeeded()
                
                if should_quit:
                    
                    job_key.SetVariable( 'popup_text_1', 'cancelled!' )
                    
                    return
                    
                
                job_key.SetVariable( 'popup_text_1', 'ensuring files are pinned: ' + HydrusData.ConvertValueRangeToPrettyString( i + 1, len( hashes ) ) )
                job_key.SetVariable( 'popup_gauge_1', ( i + 1, len( hashes ) ) )
                
                media_result = HG.client_controller.Read( 'media_result', hash )
                
                mime = media_result.GetMime()
                
                result = HG.client_controller.Read( 'service_filenames', self._service_key, { hash } )
                
                if len( result ) == 0:
                    
                    try:
                        
                        multihash = self.PinFile( hash, mime )
                        
                    except HydrusExceptions.DataMissing:
                        
                        HydrusData.ShowText( 'File {} could not be pinned!'.format( hash.hex() ) )
                        
                        continue
                        
                    
                else:
                    
                    ( multihash, ) = result
                    
                
                file_info.append( ( hash, mime, multihash ) )
                
            
            with self._lock:
                
                api_base_url = self._GetAPIBaseURL()
                
            
            url = api_base_url + 'object/new?arg=unixfs-dir'
            
            network_job = ClientNetworkingJobs.NetworkJobIPFS( url )
            
            HG.client_controller.network_engine.AddJob( network_job )
            
            network_job.WaitUntilDone()
            
            parsing_text = network_job.GetContentText()
            
            response_json = json.loads( parsing_text )
            
            for ( i, ( hash, mime, multihash ) ) in enumerate( file_info ):
                
                ( i_paused, should_quit ) = job_key.WaitIfNeeded()
                
                if should_quit:
                    
                    job_key.SetVariable( 'popup_text_1', 'cancelled!' )
                    
                    return
                    
                
                job_key.SetVariable( 'popup_text_1', 'creating directory: ' + HydrusData.ConvertValueRangeToPrettyString( i + 1, len( file_info ) ) )
                job_key.SetVariable( 'popup_gauge_1', ( i + 1, len( file_info ) ) )
                
                object_multihash = response_json[ 'Hash' ]
                
                filename = hash.hex() + HC.mime_ext_lookup[ mime ]
                
                url = api_base_url + 'object/patch/add-link?arg=' + object_multihash + '&arg=' + filename + '&arg=' + multihash
                
                network_job = ClientNetworkingJobs.NetworkJobIPFS( url )
                
                HG.client_controller.network_engine.AddJob( network_job )
                
                network_job.WaitUntilDone()
                
                parsing_text = network_job.GetContentText()
                
                response_json = json.loads( parsing_text )
                
            
            directory_multihash = response_json[ 'Hash' ]
            
            url = api_base_url + 'pin/add?arg=' + directory_multihash
            
            network_job = ClientNetworkingJobs.NetworkJobIPFS( url )
            
            HG.client_controller.network_engine.AddJob( network_job )
            
            network_job.WaitUntilDone()
            
            content_update_row = ( hashes, directory_multihash, note )
            
            content_updates = [ HydrusData.ContentUpdate( HC.CONTENT_TYPE_DIRECTORIES, HC.CONTENT_UPDATE_ADD, content_update_row ) ]
            
            HG.client_controller.WriteSynchronous( 'content_updates', { self._service_key : content_updates } )
            
            job_key.SetVariable( 'popup_text_1', 'done!' )
            
            with self._lock:
                
                text = self._multihash_prefix + directory_multihash
                
            
            job_key.SetVariable( 'popup_clipboard', ( 'copy multihash to clipboard', text ) )
            
            return directory_multihash
            
        except Exception as e:
            
            HydrusData.ShowException( e )
            
            job_key.SetErrorException( e )
            
            job_key.Cancel()
            
        finally:
            
            job_key.DeleteVariable( 'popup_gauge_1' )
            
            job_key.Finish()",len(result) == 0,not result,not result,1,"v1: len(result)
v2: 0"
SSDTTime,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SSDTTime/Scripts/dsdt.py,https://github.com/corpnewt/SSDTTime/tree/master/Scripts/dsdt.py,DSDT,get_devices$340,"def get_devices(self,search=None,types=(""Device ("",""Scope (""),strip_comments=False):
        # Returns a list of tuples organized as (Device/Scope,d_s_index,matched_index)
        if search == None:
            return []
        last_device = None
        device_index = 0
        devices = []
        for index,line in enumerate(self.dsdt_lines):
            if self.is_hex(line):
                continue
            line = self.get_line(line) if strip_comments else line
            if any ((x for x in types if x in line)):
                # Got a last_device match
                last_device = line
                device_index = index
            if search in line:
                # Got a search hit - add it
                devices.append((last_device,device_index,index))
        return devices",search == None,search,not search,0,"v1: search
v2: None"
faker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/faker/tests/providers/test_ssn.py,https://github.com/joke2k/faker/tree/master/tests/providers/test_ssn.py,TestPtBR,test_pt_BR_ssn_checksum$858,"def test_pt_BR_ssn_checksum(self):
        assert pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2]) == 2
        assert pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2, 2]) == 0","pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2]) == 2",cannot be refactored by truth value test,Cannot refactor,1,"v1: pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2])
v2: 2"
faker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/faker/tests/providers/test_ssn.py,https://github.com/joke2k/faker/tree/master/tests/providers/test_ssn.py,TestPtBR,test_pt_BR_ssn_checksum$858,"def test_pt_BR_ssn_checksum(self):
        assert pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2]) == 2
        assert pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2, 2]) == 0","pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2, 2]) == 0","not pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2, 2])","not pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2, 2])",1,"v1: pt_checksum([8, 8, 2, 8, 2, 1, 6, 5, 2, 2])
v2: 0"
explainerdashboard,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/explainerdashboard/tests/integration_tests/test_dashboards.py,https://github.com/oegedijk/explainerdashboard/tree/master/tests/integration_tests/test_dashboards.py,,test_xgboost_multiclass_dashboard$77,"def test_xgboost_multiclass_dashboard(dash_duo, precalculated_xgb_multiclass_explainer):
    db = ExplainerDashboard(precalculated_xgb_multiclass_explainer, title=""testing"", responsive=False)
    html = db.to_html()
    assert html.startswith('\n<!DOCTYPE html>\n<html'), ""failed to generate dashboard to_html""

    dash_duo.start_server(db.app)
    dash_duo.wait_for_text_to_equal(""h1"", ""testing"", timeout=30)
    assert dash_duo.get_logs() == [], ""browser console should contain no error""",dash_duo.get_logs() == [],not dash_duo.get_logs(),not dash_duo.get_logs(),1,"v1: dash_duo.get_logs()
v2: []"
torchdistill,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torchdistill/torchdistill/models/classification/resnet.py,https://github.com/yoshitomo-matsubara/torchdistill/tree/master/torchdistill/models/classification/resnet.py,,resnet$122,"def resnet(
        depth: int,
        num_classes: int,
        pretrained: bool,
        progress: bool,
        **kwargs: Any
) -> ResNet4Cifar:
    assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110, 1202'
    n = (depth - 2) // 6
    model = ResNet4Cifar(BasicBlock, [n, n, n], num_classes, **kwargs)
    model_key = 'cifar{}-resnet{}'.format(num_classes, depth)
    if pretrained and model_key in MODEL_URL_DICT:
        state_dict = torch.hub.load_state_dict_from_url(MODEL_URL_DICT[model_key], progress=progress)
        model.load_state_dict(state_dict)
    return model",(depth - 2) % 6 == 0,not (depth - 2) % 6,not (depth - 2) % 6,1,"v1: (depth - 2) % 6
v2: 0"
scvi-tools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scvi-tools/scvi/nn/_base_components.py,https://github.com/YosefLab/scvi-tools/tree/master/scvi/nn/_base_components.py,FCLayers,inject_into_layer$107,"def inject_into_layer(self, layer_num) -> bool:
        """"""Helper to determine if covariates should be injected.""""""
        user_cond = layer_num == 0 or (layer_num > 0 and self.inject_covariates)
        return user_cond",layer_num == 0,not layer_num,not layer_num,1,"v1: layer_num
v2: 0"
socorro,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/socorro/webapp-django/crashstats/authentication/tests/test_auditgroups.py,https://github.com/mozilla-services/socorro/tree/master/webapp-django/crashstats/authentication/tests/test_auditgroups.py,TestAuditGroupsCommand,test_user_with_invalid_email_removed$57,"def test_user_with_invalid_email_removed(self, db):
        hackers_group = Group.objects.get(name=""Hackers"")

        bob = User.objects.create(username=""bob"", email=""bob@example.com"")
        bob.last_login = timezone.now()
        bob.groups.add(hackers_group)
        bob.save()

        buffer = StringIO()
        call_command(""auditgroups"", dry_run=False, stdout=buffer)
        assert [u.email for u in hackers_group.user_set.all()] == []
        assert (
            ""Removing: bob@example.com (not employee or exception)"" in buffer.getvalue()
        )",[u.email for u in hackers_group.user_set.all()] == [],not [u.email for u in hackers_group.user_set.all()],not [u.email for u in hackers_group.user_set.all()],1,"v1: [u.email for u in hackers_group.user_set.all()]
v2: []"
