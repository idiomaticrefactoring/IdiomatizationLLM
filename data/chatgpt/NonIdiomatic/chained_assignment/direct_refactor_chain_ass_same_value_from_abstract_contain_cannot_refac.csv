repo_name,file_path,file_html,class_name,me_code,old_code,new_code,bool_code,chatGPT_code,if_correct,reversed_code,non_replace_var_refactored_code,refactored_code,acc,instruction,sys_msg,exam_msg,user_msg
hangoutsbot,https://github.com/hangoutsbot/hangoutsbot/tree/master/hangupsbot/plugins/lottery.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hangoutsbot/hangupsbot/plugins/lottery.py,,"def perform_drawing(bot, event, *args):
    """"""draw handling:
        /me draw[s] [a[n]] number[s] => draws from ""number"", ""numbers"" or ""numberes""
        /me draw[s] [a[n]] sticks[s] => draws from ""stick"", ""sticks"" or ""stickses""
        /me draws[s]<unrecognised> => draws from ""default""

        note: to prepare lotteries/drawings, see /bot prepare ...

        XXX: check is for singular, plural ""-s"" and plural ""-es""
    """"""
    draw_lists = _load_lottery_state(bot)
    pattern = re.compile('.+ draws?( +(a +|an +|from +)?([a-z0-9\\-_]+))?$', re.IGNORECASE)
    if pattern.match(event.text):
        listname = 'default'
        matches = pattern.search(event.text)
        groups = matches.groups()
        if groups[2] is not None:
            listname = groups[2]
        if listname.endswith('s'):
            _plurality = (listname[:-1], listname, listname + 'es')
        else:
            _plurality = (listname, listname + 's', listname + 'es')
        global_draw_name = None
        _test_name = None
        for word in _plurality:
            _test_name = _get_global_lottery_name(bot, event.conv.id_, word)
            if _test_name in draw_lists:
                global_draw_name = _test_name
                logger.debug('{} is valid lottery'.format(global_draw_name))
                break
        if global_draw_name is not None:
            if len(draw_lists[global_draw_name]['box']) > 0:
                if event.user.id_.chat_id in draw_lists[global_draw_name]['users']:
                    yield from bot.coro_send_message(event.conv, _('<b>{}</b>, you have already drawn <b>{}</b> from the <b>{}</b> box').format(event.user.full_name, draw_lists[global_draw_name]['users'][event.user.id_.chat_id], word))
                else:
                    _thing = str(draw_lists[global_draw_name]['box'].pop())
                    text_drawn = _('<b>{}</b> draws <b>{}</b> from the <b>{}</b> box. ').format(event.user.full_name, _thing, word)
                    if len(draw_lists[global_draw_name]['box']) == 0:
                        text_drawn = text_drawn + _('...AAAAAND its all gone! The <b>{}</b> lottery is over folks.').format(word)
                    yield from bot.coro_send_message(event.conv, text_drawn)
                    draw_lists[global_draw_name]['users'][event.user.id_.chat_id] = _thing
            else:
                text_finished = _('<b>{}</b>, the <b>{}</b> lottery is over. ').format(event.user.full_name, word)
                if event.user.id_.chat_id in draw_lists[global_draw_name]['users']:
                    text_finished = _('You drew a {} previously.').format(draw_lists[global_draw_name]['users'][event.user.id_.chat_id])
                yield from bot.coro_send_message(event.conv, text_finished)
    _save_lottery_state(bot, draw_lists)","global_draw_name = None
_test_name = None",global_draw_name = _test_name = None,1,,,,,,,,,,
tfc,https://github.com/maqp/tfc/tree/master/tests/common/test_crypto.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tfc/tests/common/test_crypto.py,TestArgon2KDF,"def test_argon2_cffi_using_the_official_command_line_utility(self) -> None:
    min_password_length = 1
    max_password_length = 127
    min_salt_length = 8
    min_parallelism = 1
    max_parallelism = multiprocessing.cpu_count()
    min_time_cost = 1
    min_memory_cost = 7
    min_key_length = 4
    max_salt_length = 128
    max_time_cost = 3
    max_memory_cost = 15
    max_key_length = 64
    sys_rand = random.SystemRandom()
    for _ in range(self.number_of_tests):
        len_password = sys_rand.randint(min_password_length, max_password_length)
        len_salt = sys_rand.randint(min_salt_length, max_salt_length)
        parallelism = sys_rand.randint(min_parallelism, max_parallelism)
        time_cost = sys_rand.randint(min_time_cost, max_time_cost)
        memory_cost = sys_rand.randint(min_memory_cost, max_memory_cost)
        key_length = sys_rand.randint(min_key_length, max_key_length)
        password = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_password)])
        salt = ''.join([sys_rand.choice(ascii_letters + digits) for _ in range(len_salt)])
        output = subprocess.check_output(f'echo -n ""{password}"" | ./argon2 {salt} -t {time_cost} -m {memory_cost} -p {parallelism} -l {key_length} -id', shell=True).decode()
        key_test_vector = output.split('\n')[4].split('\t')[-1]
        purported_key = argon2.low_level.hash_secret_raw(secret=password.encode(), salt=salt.encode(), time_cost=time_cost, memory_cost=2 ** memory_cost, parallelism=parallelism, hash_len=key_length, type=argon2.Type.ID).hex()
        self.assertEqual(purported_key, key_test_vector)","min_password_length = 1
min_parallelism = 1
min_time_cost = 1",min_password_length = min_parallelism = min_time_cost = 1,1,,,,,,,,,,
JBOPS,https://github.com/blacktwin/JBOPS/tree/master/fun/plexapi_haiku.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/JBOPS/fun/plexapi_haiku.py,,"def sylco(word):
    word = word.lower()
    exception_add = ['serious', 'crucial']
    exception_del = ['fortunately', 'unfortunately']
    co_one = ['cool', 'coach', 'coat', 'coal', 'count', 'coin', 'coarse', 'coup', 'coif', 'cook', 'coign', 'coiffe', 'coof', 'court']
    co_two = ['coapt', 'coed', 'coinci']
    pre_one = ['preach']
    syls = 0
    disc = 0
    if len(word) <= 3:
        syls = 1
        return syls
    if word[-2:] == 'es' or word[-2:] == 'ed':
        doubleAndtripple_1 = len(re.findall('[eaoui][eaoui]', word))
        if doubleAndtripple_1 > 1 or len(re.findall('[eaoui][^eaoui]', word)) > 1:
            if word[-3:] == 'ted' or word[-3:] == 'tes' or word[-3:] == 'ses' or (word[-3:] == 'ied') or (word[-3:] == 'ies'):
                pass
            else:
                disc += 1
    le_except = ['whole', 'mobile', 'pole', 'male', 'female', 'hale', 'pale', 'tale', 'sale', 'aisle', 'whale', 'while']
    if word[-1:] == 'e':
        if word[-2:] == 'le' and word not in le_except:
            pass
        else:
            disc += 1
    doubleAndtripple = len(re.findall('[eaoui][eaoui]', word))
    tripple = len(re.findall('[eaoui][eaoui][eaoui]', word))
    disc += doubleAndtripple + tripple
    numVowels = len(re.findall('[eaoui]', word))
    if word[:2] == 'mc':
        syls += 1
    if word[-1:] == 'y' and word[-2] not in 'aeoui':
        syls += 1
    for (i, j) in enumerate(word):
        if j == 'y':
            if i != 0 and i != len(word) - 1:
                if word[i - 1] not in 'aeoui' and word[i + 1] not in 'aeoui':
                    syls += 1
    if word[:3] == 'tri' and word[3] in 'aeoui':
        syls += 1
    if word[:2] == 'bi' and word[2] in 'aeoui':
        syls += 1
    if word[-3:] == 'ian':
        if word[-4:] == 'cian' or word[-4:] == 'tian':
            pass
        else:
            syls += 1
    if word[:2] == 'co' and word[2] in 'eaoui':
        if word[:4] in co_two or word[:5] in co_two or word[:6] in co_two:
            syls += 1
        elif word[:4] in co_one or word[:5] in co_one or word[:6] in co_one:
            pass
        else:
            syls += 1
    if word[:3] == 'pre' and word[3] in 'eaoui':
        if word[:6] in pre_one:
            pass
        else:
            syls += 1
    negative = [""doesn't"", ""isn't"", ""shouldn't"", ""couldn't"", ""wouldn't""]
    if word[-3:] == ""n't"":
        if word in negative:
            syls += 1
        else:
            pass
    if word in exception_del:
        disc += 1
    if word in exception_add:
        syls += 1
    return numVowels - disc + syls","syls = 0
disc = 0",syls = disc = 0,1,,,,,,,,,,
torba,https://github.com/lbryio/torba/tree/master/torba/orchstr8/node.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torba/torba/orchstr8/node.py,BlockchainNode,"def __init__(self, url, daemon, cli):
    self.latest_release_url = url
    self.project_dir = os.path.dirname(os.path.dirname(__file__))
    self.bin_dir = os.path.join(self.project_dir, 'bin')
    self.daemon_bin = os.path.join(self.bin_dir, daemon)
    self.cli_bin = os.path.join(self.bin_dir, cli)
    self.log = log.getChild('blockchain')
    self.data_path = None
    self.protocol = None
    self.transport = None
    self._block_expected = 0
    self.hostname = 'localhost'
    self.peerport = 9246 + 2
    self.rpcport = 9245 + 2
    self.rpcuser = 'rpcuser'
    self.rpcpassword = 'rpcpassword'","self.data_path = None
self.protocol = None
self.transport = None",self.data_path = self.protocol = self.transport = None,1,,,,,,,,,,
myscan,https://github.com/amcai/myscan/tree/master/myscan/reverse/reverse_dns.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/myscan/myscan/reverse/reverse_dns.py,SinDNSAnswer,"def __init__(self, ip):
    self.type = 1
    self.classify = 1
    self.name = 49164
    self.datalength = 4
    self.timetolive = 190
    self.ip = ip","self.type = 1
self.classify = 1",self.type = self.classify = 1,1,,,,,,,,,,
nltk-trainer,https://github.com/japerk/nltk-trainer/tree/master/nltk_trainer/featx/phonetics.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nltk-trainer/nltk_trainer/featx/phonetics.py,,"def metaphone(term):
    """"""returns metaphone code for a given string""""""
    code = ''
    i = 0
    term_length = len(term)
    if term_length == 0:
        return code
    term = string.lower(term)
    term = re.sub('[^a-z]', '', term)
    if len(term) == 0:
        return code
    firstChar = term[0]
    str2 = firstChar
    for x in term:
        if x != str2[-1]:
            str2 = str2 + x
    firstChar = str2[0]
    str3 = firstChar
    for x in str2[1:]:
        if re.search('[^aeiou]', x):
            str3 = str3 + x
    term = str3
    term_length = len(term)
    if term_length == 0:
        return code
    if term_length > 1:
        first_chars = term[0:2]
        table = {'ae': 'e', 'gn': 'n', 'kn': 'n', 'pn': 'n', 'wr': 'n', 'wh': 'w'}
        if first_chars in table.keys():
            term = term[2:]
            code = table[first_chars]
            term_length = len(term)
    elif term[0] == 'x':
        term = ''
        code = 's'
        term_length = 0
    st_trans = {'b': 'b', 'c': 'k', 'd': 't', 'g': 'k', 'h': 'h', 'k': 'k', 'p': 'p', 'q': 'k', 's': 's', 't': 't', 'v': 'f', 'w': 'w', 'x': 'ks', 'y': 'y', 'z': 's'}
    i = 0
    while i < term_length:
        add_char = ''
        part_n_2 = ''
        part_n_3 = ''
        part_n_4 = ''
        part_c_2 = ''
        part_c_3 = ''
        if i < term_length - 1:
            part_n_2 = term[i:i + 2]
            if i > 0:
                part_c_2 = term[i - 1:i + 1]
                part_c_3 = term[i - 1:i + 2]
        if i < term_length - 2:
            part_n_3 = term[i:i + 3]
        if i < term_length - 3:
            part_n_4 = term[i:i + 4]
        if term[i] == 'b':
            add_char = st_trans['b']
            if i == term_length - 1:
                if i > 0:
                    if term[i - 1] == 'm':
                        add_char = ''
        elif term[i] == 'c':
            add_char = st_trans['c']
            if part_n_2 == 'ch':
                add_char = 'x'
            elif re.search('c[iey]', part_n_2):
                add_char = 's'
            if part_n_3 == 'cia':
                add_char = 'x'
            if re.search('sc[iey]', part_c_3):
                add_char = ''
        elif term[i] == 'd':
            add_char = st_trans['d']
            if re.search('dg[eyi]', part_n_3):
                add_char = 'j'
        elif term[i] == 'g':
            add_char = st_trans['g']
            if part_n_2 == 'gh':
                if i == term_length - 2:
                    add_char = ''
            elif re.search('gh[aeiouy]', part_n_3):
                add_char = ''
            elif part_n_2 == 'gn':
                add_char = ''
            elif part_n_4 == 'gned':
                add_char = ''
            elif re.search('dg[eyi]', part_c_3):
                add_char = ''
            elif part_n_2 == 'gi':
                if part_c_3 != 'ggi':
                    add_char = 'j'
            elif part_n_2 == 'ge':
                if part_c_3 != 'gge':
                    add_char = 'j'
            elif part_n_2 == 'gy':
                if part_c_3 != 'ggy':
                    add_char = 'j'
            elif part_n_2 == 'gg':
                add_char = ''
        elif term[i] == 'h':
            add_char = st_trans['h']
            if re.search('[aeiouy]h[^aeiouy]', part_c_3):
                add_char = ''
            elif re.search('[csptg]h', part_c_2):
                add_char = ''
        elif term[i] == 'k':
            add_char = st_trans['k']
            if part_c_2 == 'ck':
                add_char = ''
        elif term[i] == 'p':
            add_char = st_trans['p']
            if part_n_2 == 'ph':
                add_char = 'f'
        elif term[i] == 'q':
            add_char = st_trans['q']
        elif term[i] == 's':
            add_char = st_trans['s']
            if part_n_2 == 'sh':
                add_char = 'x'
            if re.search('si[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 't':
            add_char = st_trans['t']
            if part_n_2 == 'th':
                add_char = '0'
            if re.search('ti[ao]', part_n_3):
                add_char = 'x'
        elif term[i] == 'v':
            add_char = st_trans['v']
        elif term[i] == 'w':
            add_char = st_trans['w']
            if re.search('w[^aeiouy]', part_n_2):
                add_char = ''
        elif term[i] == 'x':
            add_char = st_trans['x']
        elif term[i] == 'y':
            add_char = st_trans['y']
        elif term[i] == 'z':
            add_char = st_trans['z']
        else:
            add_char = term[i]
        code = code + add_char
        i += 1
    return code","add_char = ''
part_n_2 = ''
part_n_3 = ''
part_n_4 = ''
part_c_2 = ''
part_c_3 = ''",add_char = part_n_2 = part_n_3 = part_n_4 = part_c_2 = part_c_3 = '',1,,,,,,,,,,
gpodder,https://github.com/gpodder/gpodder/tree/master/src/gpodder/download.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gpodder/src/gpodder/download.py,DownloadTask,"def __init__(self, episode, config, downloader=None):
    assert episode.download_task is None
    self.__lock = threading.RLock()
    self.__status = DownloadTask.NEW
    self.__activity = DownloadTask.ACTIVITY_DOWNLOAD
    self.__status_changed = True
    self.__episode = episode
    self._config = config
    self.__downloader = downloader
    self.filename = self.__episode.local_filename(create=True)
    self.tempname = self.filename + '.partial'
    self.total_size = self.__episode.file_size
    self.speed = 0.0
    self.progress = 0.0
    self.error_message = None
    self._notification_shown = False
    self.__start_time = 0
    self.__start_blocks = 0
    self.__limit_rate_value = self._config.limit_rate_value
    self.__limit_rate = self._config.limit_rate
    self._progress_updated = None
    self._last_progress_updated = 0.0
    if os.path.exists(self.tempname):
        try:
            already_downloaded = os.path.getsize(self.tempname)
            if self.total_size > 0:
                self.progress = max(0.0, min(1.0, already_downloaded / self.total_size))
        except OSError as os_error:
            logger.error('Cannot get size for %s', os_error)
    else:
        open(self.tempname, 'w').close()
    episode.download_task = self","self.speed = 0.0
self.progress = 0.0
self._notification_shown = False
self.__start_time = 0
self.__start_blocks = 0
self._last_progress_updated = 0.0","self.speed = self.progress = self._last_progress_updated = 0.0
self._notification_shown = False
self.__start_time = self.__start_blocks = 0",1,,,,,,,,,,
gpodder,https://github.com/gpodder/gpodder/tree/master/src/gpodder/download.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gpodder/src/gpodder/download.py,DownloadTask,"def __init__(self, episode, config, downloader=None):
    assert episode.download_task is None
    self.__lock = threading.RLock()
    self.__status = DownloadTask.NEW
    self.__activity = DownloadTask.ACTIVITY_DOWNLOAD
    self.__status_changed = True
    self.__episode = episode
    self._config = config
    self.__downloader = downloader
    self.filename = self.__episode.local_filename(create=True)
    self.tempname = self.filename + '.partial'
    self.total_size = self.__episode.file_size
    self.speed = 0.0
    self.progress = 0.0
    self.error_message = None
    self._notification_shown = False
    self.__start_time = 0
    self.__start_blocks = 0
    self.__limit_rate_value = self._config.limit_rate_value
    self.__limit_rate = self._config.limit_rate
    self._progress_updated = None
    self._last_progress_updated = 0.0
    if os.path.exists(self.tempname):
        try:
            already_downloaded = os.path.getsize(self.tempname)
            if self.total_size > 0:
                self.progress = max(0.0, min(1.0, already_downloaded / self.total_size))
        except OSError as os_error:
            logger.error('Cannot get size for %s', os_error)
    else:
        open(self.tempname, 'w').close()
    episode.download_task = self","self.error_message = None
self._progress_updated = None",self.error_message = self._progress_updated = None,1,,,,,,,,,,
sunpy,https://github.com/sunpy/sunpy/tree/master/examples/time_series/timeseries_peak_finding.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sunpy/examples/time_series/timeseries_peak_finding.py,,"def findpeaks(series, DELTA):
    """"""
    Finds extrema in a pandas series data.

    Parameters
    ----------
    series : `pandas.Series`
        The data series from which we need to find extrema.

    DELTA : `float`
        The minimum difference between data values that defines a peak.

    Returns
    -------
    minpeaks, maxpeaks : `list`
        Lists consisting of pos, val pairs for both local minima points and
        local maxima points.
    """"""
    (mn, mx) = (np.Inf, -np.Inf)
    minpeaks = []
    maxpeaks = []
    lookformax = True
    start = True
    for (time_pos, value) in series.iteritems():
        if value > mx:
            mx = value
            mxpos = time_pos
        if value < mn:
            mn = value
            mnpos = time_pos
        if lookformax:
            if value < mx - DELTA:
                maxpeaks.append((mxpos, mx))
                mn = value
                mnpos = time_pos
                lookformax = False
            elif start:
                minpeaks.append((mnpos, mn))
                mx = value
                mxpos = time_pos
                start = False
        elif value > mn + DELTA:
            minpeaks.append((mnpos, mn))
            mx = value
            mxpos = time_pos
            lookformax = True
    if value > mn + DELTA:
        maxpeaks.append((mxpos, mx))
    elif value < mx - DELTA:
        minpeaks.append((mnpos, mn))
    return (minpeaks, maxpeaks)","lookformax = True
start = True",lookformax = start = True,1,,,,,,,,,,
projects,https://github.com/explosion/projects/tree/master/tutorials/ner_fashion_brands/scripts/visualize_data.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/projects/tutorials/ner_fashion_brands/scripts/visualize_data.py,,"def load_data(filepath):
    examples = list(srsly.read_jsonl(filepath))
    rows = []
    n_total_ents = 0
    n_no_ents = 0
    labels = set()
    for eg in examples:
        row = {'text': eg['text'], 'ents': eg.get('spans', [])}
        n_total_ents += len(row['ents'])
        if not row['ents']:
            n_no_ents += 1
        labels.update([span['label'] for span in row['ents']])
        rows.append(row)
    return (rows, labels, n_total_ents, n_no_ents)","n_total_ents = 0
n_no_ents = 0",n_total_ents = n_no_ents = 0,1,,,,,,,,,,
flair,https://github.com/flairNLP/flair/tree/master/flair/datasets/sequence_labeling.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flair/flair/datasets/sequence_labeling.py,NER_ENGLISH_STACKOVERFLOW,"def __init__(self, base_path: Union[str, Path]=None, in_memory: bool=True, **corpusargs):
    """"""
        Initialize the STACKOVERFLOW_NER corpus. The first time you call this constructor it will automatically
        download the dataset.
        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this
        to point to a different folder but typically this should not be necessary.
        POS tags instead
        :param in_memory: If True, keeps dataset in memory giving speedups in training.
        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object
        """"""
    if not base_path:
        base_path = flair.cache_root / 'datasets'
    else:
        base_path = Path(base_path)
    '\n        The Datasets are represented in the Conll format.\n           In this format each line of the Dataset is in the following format:\n           <word>+""\t""+<NE>""\t""+<word>+""\t""<markdown>\n           The end of sentence is marked with an empty line.\n           In each line NE represented the human annotated named entity\n           and <markdown> represented the code tags provided by the users who wrote the posts.\n           '
    columns = {0: 'word', 1: 'ner', 3: 'markdown'}
    entity_mapping = {'Library_Function': 'Function', 'Function_Name': 'Function', 'Class_Name': 'Class', 'Library_Class': 'Class', 'Organization': 'Website', 'Library_Variable': 'Variable', 'Variable_Name': 'Variable', 'Error_Name': 'O', 'Keyboard_IP': 'O', 'Value': 'O', 'Output_Block': 'O'}
    dataset_name = self.__class__.__name__.lower()
    data_folder = base_path / dataset_name
    STACKOVERFLOW_NER_path = 'https://raw.githubusercontent.com/jeniyat/StackOverflowNER/master/resources/annotated_ner_data/StackOverflow/'
    banned_sentences = ['code omitted for annotation', 'omitted for annotation', 'CODE_BLOCK :', 'OP_BLOCK :', 'Question_URL :', 'Question_ID :']
    files = ['train', 'test', 'dev']
    for file in files:
        questions = 0
        answers = 0
        cached_path(f'{STACKOVERFLOW_NER_path}{file}.txt', Path('datasets') / dataset_name)
        for line in open(data_folder / (file + '.txt'), mode='r', encoding='utf-8'):
            if line.startswith('Question_ID'):
                questions += 1
            if line.startswith('Answer_to_Question_ID'):
                answers += 1
        log.info(f'File {file} has {questions} questions and {answers} answers.')
    super(NER_ENGLISH_STACKOVERFLOW, self).__init__(data_folder, columns, train_file='train.txt', test_file='test.txt', dev_file='dev.txt', encoding='utf-8', banned_sentences=banned_sentences, in_memory=in_memory, label_name_map=entity_mapping, **corpusargs)","questions = 0
answers = 0",questions = answers = 0,1,,,,,,,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_dist_base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_dist_base.py,TestDistRunnerBase,"def run_use_fleet_api_trainer(self, args):
    assert args.update_method == 'nccl2' or 'bkcl'
    self.lr = args.lr
    exec_strategy = fluid.ExecutionStrategy()
    exec_strategy.num_threads = 1
    dist_strategy = DistributedStrategy()
    dist_strategy.exec_strategy = exec_strategy
    dist_strategy.fuse_memory_size = 1
    dist_strategy.fuse_laryer_size = 1
    if args.use_local_sgd:
        dist_strategy.use_local_sgd = True
    if args.ut4grad_allreduce:
        dist_strategy._ut4grad_allreduce = True
    if args.sync_batch_norm:
        dist_strategy.sync_batch_norm = True
    role = role_maker.PaddleCloudRoleMaker(is_collective=True)
    fleet.init(role)
    print_to_err('use_fleet', 'fleet.node_num:')
    (test_program, avg_cost, train_reader, test_reader, batch_acc, predict) = self.get_model(batch_size=args.batch_size, dist_strategy=dist_strategy)
    trainer_prog = fleet._origin_program
    dist_prog = fleet.main_program
    if fluid.core.is_compiled_with_cuda():
        device_id = int(os.getenv('FLAGS_selected_gpus', '0'))
        place = fluid.CUDAPlace(device_id)
    elif fluid.core.is_compiled_with_xpu():
        device_id = int(os.getenv('FLAGS_selected_xpus', '0'))
        place = fluid.XPUPlace(device_id)
    else:
        raise ValueError('fleet dygraph api must in paddlepaddle-xpu or paddlepaddle-gpu.')
    exe = fluid.Executor(place)
    exe.run(fluid.default_startup_program())
    eprint(type(self).__name__, 'run worker startup program done.')
    feed_var_list = [var for var in trainer_prog.global_block().vars.values() if var.is_data]
    eprint('feed_var_list:', feed_var_list)
    if feed_var_list[0].name == 'label':
        feed_var_list = feed_var_list[::-1]
    feeder = fluid.DataFeeder(feed_var_list, place)
    reader_generator = train_reader()

    def get_data():
        origin_batch = next(reader_generator)
        if args.update_method != 'local' and args.use_reader_alloc:
            new_batch = []
            for (offset, item) in enumerate(origin_batch):
                if offset % 2 == args.trainer_id:
                    new_batch.append(item)
            return new_batch
        else:
            return origin_batch
    print_to_err(type(self).__name__, 'begin to train on trainer')
    out_losses = []
    for i in range(RUN_STEP):
        (loss,) = exe.run(dist_prog, fetch_list=[avg_cost.name], feed=feeder.feed(get_data()))
        out_losses.append(loss[0])
        print_to_err(type(self).__name__, 'run step %d finished' % i)
    print_to_err(type(self).__name__, 'trainer run finished')
    sys.stdout.buffer.write(pickle.dumps(out_losses))
    if args.save_model:
        model_save_dir = '/tmp'
        if fleet.worker_index() == 0:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer')
        else:
            model_save_dir_fluid = os.path.join(model_save_dir, 'fluid_persistables_2')
            model_save_dir_fleet = os.path.join(model_save_dir, 'fleet_persistables_2')
            infer_save_dir_fluid = os.path.join(model_save_dir, 'fluid_infer_2')
            infer_save_dir_fleet = os.path.join(model_save_dir, 'fleet_infer_2')
        paddle.distributed.io.save_persistables(exe, model_save_dir_fluid, fleet._origin_program)
        fleet.save_persistables(executor=exe, dirname=model_save_dir_fleet)
        feeded_var_names = [var.name for var in feed_var_list]
        fluid.io.save_inference_model(infer_save_dir_fluid, feeded_var_names, [avg_cost], exe, fleet._origin_program)
        fleet.save_inference_model(exe, infer_save_dir_fleet, feeded_var_names, [avg_cost])","exec_strategy.num_threads = 1
dist_strategy.fuse_memory_size = 1
dist_strategy.fuse_laryer_size = 1",exec_strategy.num_threads = dist_strategy.fuse_memory_size = dist_strategy.fuse_laryer_size = 1,1,,,,,,,,,,
metaworld,https://github.com/rlworkgroup/metaworld/tree/master/metaworld/envs/mujoco/env_dict.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/metaworld/metaworld/envs/mujoco/env_dict.py,,"def create_hidden_goal_envs():
    hidden_goal_envs = {}
    for (env_name, env_cls) in ALL_V2_ENVIRONMENTS.items():
        d = {}

        def initialize(env, seed=None):
            if seed is not None:
                st0 = np.random.get_state()
                np.random.seed(seed)
            super(type(env), env).__init__()
            env._partially_observable = True
            env._freeze_rand_vec = False
            env._set_task_called = True
            env.reset()
            env._freeze_rand_vec = True
            if seed is not None:
                env.seed(seed)
                np.random.set_state(st0)
        d['__init__'] = initialize
        hg_env_name = re.sub('(^|[-])\\s*([a-zA-Z])', lambda p: p.group(0).upper(), env_name)
        hg_env_name = hg_env_name.replace('-', '')
        hg_env_key = '{}-goal-hidden'.format(env_name)
        hg_env_name = '{}GoalHidden'.format(hg_env_name)
        HiddenGoalEnvCls = type(hg_env_name, (env_cls,), d)
        hidden_goal_envs[hg_env_key] = HiddenGoalEnvCls
    return OrderedDict(hidden_goal_envs)","env._partially_observable = True
env._set_task_called = True",env._partially_observable = env._set_task_called = True,1,,,,,,,,,,
hydrus,https://github.com/hydrusnetwork/hydrus/tree/master/hydrus/client/gui/pages/ClientGUIManagement.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hydrus/hydrus/client/gui/pages/ClientGUIManagement.py,ManagementPanelDuplicateFilter,"def qt_code(potential_duplicates_count):
    if not self or not QP.isValid(self):
        return
    self._currently_refreshing_dupe_count_numbers = False
    self._dupe_count_numbers_dirty = False
    self._refresh_dupe_counts_button.setEnabled(True)
    self._UpdatePotentialDuplicatesCount(potential_duplicates_count)","self._currently_refreshing_dupe_count_numbers = False
self._dupe_count_numbers_dirty = False",self._currently_refreshing_dupe_count_numbers = self._dupe_count_numbers_dirty = False,1,,,,,,,,,,
core,https://github.com/home-assistant/core/tree/master/homeassistant/components/imap_email_content/sensor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/homeassistant/components/imap_email_content/sensor.py,EmailReader,"def __init__(self, user, password, server, port, folder):
    """"""Initialize the Email Reader.""""""
    self._user = user
    self._password = password
    self._server = server
    self._port = port
    self._folder = folder
    self._last_id = None
    self._unread_ids = deque([])
    self.connection = None","self._last_id = None
self.connection = None",self._last_id = self.connection = None,1,,,,,,,,,,
XwareDesktop,https://github.com/Xinkai/XwareDesktop/tree/master/src/frontend/models/TaskTreeModel.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/XwareDesktop/src/frontend/models/TaskTreeModel.py,TaskTreeModel,"def __init__(self, parent=None):
    super().__init__(parent)
    self._root = None
    self._creation = None
    self._taskItem = None","self._root = None
self._creation = None
self._taskItem = None",self._root = self._creation = self._taskItem = None,1,,,,,,,,,,
cloud189,https://github.com/Aruelius/cloud189/tree/master/cloud189/cli/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud189/cloud189/cli/utils.py,,"def parsing_up_params(arg: str, follow, force, mkdir) -> (bool, bool, bool, bool):
    """"""解析文件上传参数
    :param str arg: 解析参数
    :param bool follow: 实时任务
    :param bool force: 强制上传
    :param bool mkdir: 不创建父文件夹
    :return: follow, force, mkdir, match(标识是否需要删除 arg)
    """"""
    match = False
    if len(arg) > 1:
        if arg.startswith('--'):
            if arg == '--follow':
                follow = True
                match = True
            elif arg == '--force':
                force = True
                match = True
            elif arg == '--nodir':
                mkdir = False
                match = True
        elif arg.startswith('-'):
            for i in arg[1:]:
                if i == 'f':
                    follow = True
                    match = True
                elif i == 'F':
                    force = True
                    match = True
                elif i == 'n':
                    mkdir = False
                    match = True
    return (follow, force, mkdir, match)","follow = True
match = True",follow = match = True,1,,,,,,,,,,
cloud189,https://github.com/Aruelius/cloud189/tree/master/cloud189/cli/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud189/cloud189/cli/utils.py,,"def parsing_up_params(arg: str, follow, force, mkdir) -> (bool, bool, bool, bool):
    """"""解析文件上传参数
    :param str arg: 解析参数
    :param bool follow: 实时任务
    :param bool force: 强制上传
    :param bool mkdir: 不创建父文件夹
    :return: follow, force, mkdir, match(标识是否需要删除 arg)
    """"""
    match = False
    if len(arg) > 1:
        if arg.startswith('--'):
            if arg == '--follow':
                follow = True
                match = True
            elif arg == '--force':
                force = True
                match = True
            elif arg == '--nodir':
                mkdir = False
                match = True
        elif arg.startswith('-'):
            for i in arg[1:]:
                if i == 'f':
                    follow = True
                    match = True
                elif i == 'F':
                    force = True
                    match = True
                elif i == 'n':
                    mkdir = False
                    match = True
    return (follow, force, mkdir, match)","force = True
match = True",force = match = True,1,,,,,,,,,,
cloud189,https://github.com/Aruelius/cloud189/tree/master/cloud189/cli/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud189/cloud189/cli/utils.py,,"def parsing_up_params(arg: str, follow, force, mkdir) -> (bool, bool, bool, bool):
    """"""解析文件上传参数
    :param str arg: 解析参数
    :param bool follow: 实时任务
    :param bool force: 强制上传
    :param bool mkdir: 不创建父文件夹
    :return: follow, force, mkdir, match(标识是否需要删除 arg)
    """"""
    match = False
    if len(arg) > 1:
        if arg.startswith('--'):
            if arg == '--follow':
                follow = True
                match = True
            elif arg == '--force':
                force = True
                match = True
            elif arg == '--nodir':
                mkdir = False
                match = True
        elif arg.startswith('-'):
            for i in arg[1:]:
                if i == 'f':
                    follow = True
                    match = True
                elif i == 'F':
                    force = True
                    match = True
                elif i == 'n':
                    mkdir = False
                    match = True
    return (follow, force, mkdir, match)","follow = True
match = True",follow = match = True,1,,,,,,,,,,
cloud189,https://github.com/Aruelius/cloud189/tree/master/cloud189/cli/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cloud189/cloud189/cli/utils.py,,"def parsing_up_params(arg: str, follow, force, mkdir) -> (bool, bool, bool, bool):
    """"""解析文件上传参数
    :param str arg: 解析参数
    :param bool follow: 实时任务
    :param bool force: 强制上传
    :param bool mkdir: 不创建父文件夹
    :return: follow, force, mkdir, match(标识是否需要删除 arg)
    """"""
    match = False
    if len(arg) > 1:
        if arg.startswith('--'):
            if arg == '--follow':
                follow = True
                match = True
            elif arg == '--force':
                force = True
                match = True
            elif arg == '--nodir':
                mkdir = False
                match = True
        elif arg.startswith('-'):
            for i in arg[1:]:
                if i == 'f':
                    follow = True
                    match = True
                elif i == 'F':
                    force = True
                    match = True
                elif i == 'n':
                    mkdir = False
                    match = True
    return (follow, force, mkdir, match)","force = True
match = True",force = match = True,1,,,,,,,,,,
glance,https://github.com/openstack/glance/tree/master/glance/tests/unit/test_data_migration_framework.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/glance/glance/tests/unit/test_data_migration_framework.py,TestDataMigrationFramework,"def test_has_pending_migrations_mult_migration_one_pending(self, mock_find):
    mock_migration1 = mock.Mock()
    mock_migration1.has_migrations.return_value = False
    mock_migration2 = mock.Mock()
    mock_migration2.has_migrations.return_value = True
    mock_migration3 = mock.Mock()
    mock_migration3.has_migrations.return_value = False
    mock_find.return_value = [mock_migration1, mock_migration2, mock_migration3]
    self.assertTrue(data_migrations.has_pending_migrations(mock.Mock()))","mock_migration1.has_migrations.return_value = False
mock_migration3.has_migrations.return_value = False",mock_migration1.has_migrations.return_value = mock_migration3.has_migrations.return_value = False,1,,,,,,,,,,
numpy-ml,https://github.com/ddbourgin/numpy-ml/tree/master/numpy_ml/neural_nets/models/wgan_gp.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpy-ml/numpy_ml/neural_nets/models/wgan_gp.py,WGAN_GP,"def __init__(self, g_hidden=512, init='he_uniform', optimizer='RMSProp(lr=0.0001)', debug=False):
    """"""
        Wasserstein generative adversarial network with gradient penalty.

        Parameters
        ----------
        g_hidden : int
            The number of units in the critic and generator hidden layers.
            Default is 512.
        init : str
            The weight initialization strategy. Valid entries are
            {'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform',
            'std_normal', 'trunc_normal'}. Default is ""he_uniform"".
        optimizer : str or :doc:`Optimizer <numpy_ml.neural_nets.optimizers>` object or None
            The optimization strategy to use when performing gradient updates.
            If None, use the :class:`~numpy_ml.neural_nets.optimizers.SGD`
            optimizer with default parameters. Default is ""RMSProp(lr=0.0001)"".
        debug : bool
            Whether to store additional intermediate output within
            ``self.derived_variables``. Default is False.
        """"""
    self.init = init
    self.debug = debug
    self.g_hidden = g_hidden
    self.optimizer = optimizer
    self.lambda_ = None
    self.n_steps = None
    self.batchsize = None
    self.is_initialized = False","self.lambda_ = None
self.n_steps = None
self.batchsize = None",self.lambda_ = self.n_steps = self.batchsize = None,1,,,,,,,,,,
aeneas,https://github.com/readbeyond/aeneas/tree/master/aeneas/tests/test_cew.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aeneas/aeneas/tests/test_cew.py,TestCEW,"def test_cew_synthesize_multiple(self):
    (handler, output_file_path) = gf.tmp_file(suffix='.wav')
    try:
        c_quit_after = 0.0
        c_backwards = 0
        c_text = [(u'en', u'Dummy 1'), (u'en', u'Dummy 2'), (u'en', u'Dummy 3')]
        import aeneas.cew.cew
        (sr, sf, intervals) = aeneas.cew.cew.synthesize_multiple(output_file_path, c_quit_after, c_backwards, c_text)
        self.assertEqual(sr, 22050)
        self.assertEqual(sf, 3)
        self.assertEqual(len(intervals), 3)
    except ImportError:
        pass
    gf.delete_file(handler, output_file_path)","c_quit_after = 0.0
c_backwards = 0",c_quit_after = c_backwards = 0,1,,,,,,,,,,
WatchAD,https://github.com/Qianlitp/WatchAD/tree/master/models/Log.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WatchAD/models/Log.py,TargetInfo,"def __init__(self, event_data):
    self.domain_name = None
    self.user_name = None
    self.user_sid = None
    self.logon_id = None
    self.info = None
    self.server_name = None
    self.sid = None
    self.full_user_name = None
    self._field_map = {'TargetDomainName': 'domain_name', 'TargetUserName': 'user_name', 'TargetUserSid': 'user_sid', 'TargetSid': 'sid', 'TargetLogonId': 'logon_id', 'TargetInfo': 'info', 'TargetServerName': 'server_name'}
    for (key, value) in self._field_map.items():
        if key not in event_data:
            continue
        if key == 'TargetUserName':
            if '@' in event_data[key]:
                user_name = event_data[key].split('@')[0]
                self.__dict__.update({value: user_name})
            else:
                self.__dict__.update({value: event_data[key]})
            self.__dict__.update({'full_user_name': event_data[key]})
        elif key in event_data:
            self.__dict__.update({value: event_data[key]})","self.domain_name = None
self.user_name = None
self.user_sid = None
self.logon_id = None
self.info = None
self.server_name = None
self.sid = None
self.full_user_name = None",self.domain_name = self.user_name = self.user_sid = self.logon_id = self.info = self.server_name = self.sid = self.full_user_name = None,1,,,,,,,,,,
astroquery,https://github.com/astropy/astroquery/tree/master/astroquery/lamda/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astroquery/astroquery/lamda/core.py,,"def parse_lamda_lines(data):
    """"""
    Extract a LAMDA datafile into a dictionary of tables

    (non-pythonic!  more like, fortranic)
    """"""
    meta_rad = {}
    meta_mol = {}
    meta_coll = {}
    levels = []
    radtrans = []
    collider = None
    ncolltrans = None
    for (ii, line) in enumerate(data):
        if line[0] == '!':
            continue
        if 'molecule' not in meta_mol:
            meta_mol['molecule'] = _cln(line)
            continue
        if 'molwt' not in meta_mol:
            meta_mol['molwt'] = float(_cln(line))
            continue
        if 'nenergylevels' not in meta_mol:
            meta_mol['nenergylevels'] = int(_cln(line))
            continue
        if len(levels) < meta_mol['nenergylevels']:
            (lev, en, wt) = _cln(line).split()[:3]
            jul = ' '.join(_cln(line).split()[3:])
            levels.append([int(lev), float(en), int(float(wt)), jul])
            continue
        if 'radtrans' not in meta_rad:
            meta_rad['radtrans'] = int(_cln(line))
            continue
        if len(radtrans) < meta_rad['radtrans']:
            (trans, up, low, aval, freq, eu) = _cln(line).split()[:6]
            radtrans.append([int(trans), int(up), int(low), float(aval), float(freq), float(eu)])
            continue
        if 'ncoll' not in meta_coll:
            meta_coll['ncoll'] = int(_cln(line))
            collrates = {}
            continue
        if collider is None:
            collider = int(line[0])
            collname = collider_ids[collider]
            collrates[collider] = []
            meta_coll[collname] = {'collider': collname, 'collider_id': collider}
            continue
        if ncolltrans is None:
            ncolltrans = int(_cln(line))
            meta_coll[collname]['ntrans'] = ncolltrans
            continue
        if 'ntemp' not in meta_coll[collname]:
            meta_coll[collname]['ntemp'] = int(_cln(line))
            continue
        if 'temperatures' not in meta_coll[collname]:
            meta_coll[collname]['temperatures'] = [int(float(x)) for x in _cln(line).split()]
            continue
        if len(collrates[collider]) < meta_coll[collname]['ntrans']:
            (trans, up, low) = [int(x) for x in _cln(line).split()[:3]]
            temperatures = [float(x) for x in _cln(line).split()[3:]]
            collrates[collider].append([trans, up, low] + temperatures)
        if len(collrates[collider]) == meta_coll[collname]['ntrans']:
            log.debug('{ii} Finished loading collider {0:d}: {1}'.format(collider, collider_ids[collider], ii=ii))
            collider = None
            ncolltrans = None
            if len(collrates) == meta_coll['ncoll']:
                break
    if len(levels[0]) == 4:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J']
    elif len(levels[0]) == 5:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J', 'F']
    else:
        raise ValueError('Unrecognized levels structure.')
    mol_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(mol_table_names, zip(*levels))]
    mol_table = table.Table(data=mol_table_columns, meta=meta_mol)
    rad_table_names = ['Transition', 'Upper', 'Lower', 'EinsteinA', 'Frequency', 'E_u(K)']
    rad_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(rad_table_names, zip(*radtrans))]
    rad_table = table.Table(data=rad_table_columns, meta=meta_rad)
    coll_tables = {collider_ids[collider]: None for collider in collrates}
    for collider in collrates:
        collname = collider_ids[collider]
        coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
        coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
        coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
        coll_tables[collname] = coll_table
    return (coll_tables, rad_table, mol_table)","collider = None
ncolltrans = None",collider = ncolltrans = None,1,,,,,,,,,,
astroquery,https://github.com/astropy/astroquery/tree/master/astroquery/lamda/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astroquery/astroquery/lamda/core.py,,"def parse_lamda_lines(data):
    """"""
    Extract a LAMDA datafile into a dictionary of tables

    (non-pythonic!  more like, fortranic)
    """"""
    meta_rad = {}
    meta_mol = {}
    meta_coll = {}
    levels = []
    radtrans = []
    collider = None
    ncolltrans = None
    for (ii, line) in enumerate(data):
        if line[0] == '!':
            continue
        if 'molecule' not in meta_mol:
            meta_mol['molecule'] = _cln(line)
            continue
        if 'molwt' not in meta_mol:
            meta_mol['molwt'] = float(_cln(line))
            continue
        if 'nenergylevels' not in meta_mol:
            meta_mol['nenergylevels'] = int(_cln(line))
            continue
        if len(levels) < meta_mol['nenergylevels']:
            (lev, en, wt) = _cln(line).split()[:3]
            jul = ' '.join(_cln(line).split()[3:])
            levels.append([int(lev), float(en), int(float(wt)), jul])
            continue
        if 'radtrans' not in meta_rad:
            meta_rad['radtrans'] = int(_cln(line))
            continue
        if len(radtrans) < meta_rad['radtrans']:
            (trans, up, low, aval, freq, eu) = _cln(line).split()[:6]
            radtrans.append([int(trans), int(up), int(low), float(aval), float(freq), float(eu)])
            continue
        if 'ncoll' not in meta_coll:
            meta_coll['ncoll'] = int(_cln(line))
            collrates = {}
            continue
        if collider is None:
            collider = int(line[0])
            collname = collider_ids[collider]
            collrates[collider] = []
            meta_coll[collname] = {'collider': collname, 'collider_id': collider}
            continue
        if ncolltrans is None:
            ncolltrans = int(_cln(line))
            meta_coll[collname]['ntrans'] = ncolltrans
            continue
        if 'ntemp' not in meta_coll[collname]:
            meta_coll[collname]['ntemp'] = int(_cln(line))
            continue
        if 'temperatures' not in meta_coll[collname]:
            meta_coll[collname]['temperatures'] = [int(float(x)) for x in _cln(line).split()]
            continue
        if len(collrates[collider]) < meta_coll[collname]['ntrans']:
            (trans, up, low) = [int(x) for x in _cln(line).split()[:3]]
            temperatures = [float(x) for x in _cln(line).split()[3:]]
            collrates[collider].append([trans, up, low] + temperatures)
        if len(collrates[collider]) == meta_coll[collname]['ntrans']:
            log.debug('{ii} Finished loading collider {0:d}: {1}'.format(collider, collider_ids[collider], ii=ii))
            collider = None
            ncolltrans = None
            if len(collrates) == meta_coll['ncoll']:
                break
    if len(levels[0]) == 4:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J']
    elif len(levels[0]) == 5:
        mol_table_names = ['Level', 'Energy', 'Weight', 'J', 'F']
    else:
        raise ValueError('Unrecognized levels structure.')
    mol_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(mol_table_names, zip(*levels))]
    mol_table = table.Table(data=mol_table_columns, meta=meta_mol)
    rad_table_names = ['Transition', 'Upper', 'Lower', 'EinsteinA', 'Frequency', 'E_u(K)']
    rad_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(rad_table_names, zip(*radtrans))]
    rad_table = table.Table(data=rad_table_columns, meta=meta_rad)
    coll_tables = {collider_ids[collider]: None for collider in collrates}
    for collider in collrates:
        collname = collider_ids[collider]
        coll_table_names = ['Transition', 'Upper', 'Lower'] + ['C_ij(T={0:d})'.format(tem) for tem in meta_coll[collname]['temperatures']]
        coll_table_columns = [table.Column(name=name, data=data) for (name, data) in zip(coll_table_names, zip(*collrates[collider]))]
        coll_table = table.Table(data=coll_table_columns, meta=meta_coll[collname])
        coll_tables[collname] = coll_table
    return (coll_tables, rad_table, mol_table)","collider = None
ncolltrans = None",collider = ncolltrans = None,1,,,,,,,,,,
eiten,https://github.com/tradytics/eiten/tree/master/strategies/genetic_algo_strategy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/eiten/strategies/genetic_algo_strategy.py,GeneticAlgoStrategy,"def __init__(self):
    print('Genetic algo strategy has been created')
    self.initial_genes = 100
    self.selection_top = 25
    self.mutation_iterations = 50
    self.weight_update_factor = 0.1
    self.gene_length = None
    self.genes_in_each_iteration = 250
    self.iterations = 50
    self.crossover_probability = 0.05","self.mutation_iterations = 50
self.iterations = 50",self.mutation_iterations = self.iterations = 50,1,,,,,,,,,,
core,https://github.com/home-assistant/core/tree/master/tests/components/xiaomi_miio/test_vacuum.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/tests/components/xiaomi_miio/test_vacuum.py,,"def mirobo_is_on_fixture():
    """"""Mock mock_mirobo.""""""
    mock_vacuum = MagicMock()
    mock_vacuum.status().data = {'test': 'raw'}
    mock_vacuum.status().is_on = True
    mock_vacuum.fan_speed_presets.return_value = new_fanspeeds
    mock_vacuum.status().fanspeed = list(new_fanspeeds.values())[0]
    mock_vacuum.status().got_error = False
    mock_vacuum.status().battery = 32
    mock_vacuum.status().clean_area = 133.43218
    mock_vacuum.status().clean_time = timedelta(hours=2, minutes=55, seconds=34)
    mock_vacuum.consumable_status().main_brush_left = timedelta(hours=11, minutes=35, seconds=34)
    mock_vacuum.consumable_status().side_brush_left = timedelta(hours=11, minutes=35, seconds=34)
    mock_vacuum.consumable_status().filter_left = timedelta(hours=11, minutes=35, seconds=34)
    mock_vacuum.clean_history().count = '41'
    mock_vacuum.clean_history().total_area = 323.43218
    mock_vacuum.clean_history().total_duration = timedelta(hours=11, minutes=15, seconds=34)
    mock_vacuum.status().state = 'Test Xiaomi Cleaning'
    mock_vacuum.status().state_code = 5
    mock_vacuum.dnd_status().enabled = False
    mock_vacuum.last_clean_details().start = datetime(2020, 4, 1, 13, 21, 10, tzinfo=dt_util.UTC)
    mock_vacuum.last_clean_details().end = datetime(2020, 4, 1, 13, 21, 10, tzinfo=dt_util.UTC)
    mock_vacuum.last_clean_details().duration = timedelta(hours=11, minutes=15, seconds=34)
    mock_vacuum.last_clean_details().area = 133.43218
    mock_vacuum.last_clean_details().error_code = 1
    mock_vacuum.last_clean_details().error = 'test_error_code'
    mock_vacuum.last_clean_details().complete = True
    mock_timer_1 = MagicMock()
    mock_timer_1.enabled = True
    mock_timer_1.cron = '5 5 1 8 1'
    mock_timer_1.next_schedule = datetime(2020, 5, 23, 13, 21, 10, tzinfo=dt_util.UTC)
    mock_timer_2 = MagicMock()
    mock_timer_2.enabled = False
    mock_timer_2.cron = '5 5 1 8 2'
    mock_timer_2.next_schedule = datetime(2020, 5, 23, 13, 21, 10, tzinfo=dt_util.UTC)
    mock_vacuum.timer.return_value = [mock_timer_1, mock_timer_2]
    with patch('homeassistant.components.xiaomi_miio.RoborockVacuum') as mock_vacuum_cls:
        mock_vacuum_cls.return_value = mock_vacuum
        yield mock_vacuum","mock_vacuum.status().is_on = True
mock_vacuum.last_clean_details().error_code = 1
mock_vacuum.last_clean_details().complete = True
mock_timer_1.enabled = True",mock_vacuum.status().is_on = mock_vacuum.last_clean_details().error_code = mock_vacuum.last_clean_details().complete = mock_timer_1.enabled = True,1,,,,,,,,,,
core,https://github.com/home-assistant/core/tree/master/tests/components/xiaomi_miio/test_vacuum.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/tests/components/xiaomi_miio/test_vacuum.py,,"def mirobo_is_on_fixture():
    """"""Mock mock_mirobo.""""""
    mock_vacuum = MagicMock()
    mock_vacuum.status().data = {'test': 'raw'}
    mock_vacuum.status().is_on = True
    mock_vacuum.fan_speed_presets.return_value = new_fanspeeds
    mock_vacuum.status().fanspeed = list(new_fanspeeds.values())[0]
    mock_vacuum.status().got_error = False
    mock_vacuum.status().battery = 32
    mock_vacuum.status().clean_area = 133.43218
    mock_vacuum.status().clean_time = timedelta(hours=2, minutes=55, seconds=34)
    mock_vacuum.consumable_status().main_brush_left = timedelta(hours=11, minutes=35, seconds=34)
    mock_vacuum.consumable_status().side_brush_left = timedelta(hours=11, minutes=35, seconds=34)
    mock_vacuum.consumable_status().filter_left = timedelta(hours=11, minutes=35, seconds=34)
    mock_vacuum.clean_history().count = '41'
    mock_vacuum.clean_history().total_area = 323.43218
    mock_vacuum.clean_history().total_duration = timedelta(hours=11, minutes=15, seconds=34)
    mock_vacuum.status().state = 'Test Xiaomi Cleaning'
    mock_vacuum.status().state_code = 5
    mock_vacuum.dnd_status().enabled = False
    mock_vacuum.last_clean_details().start = datetime(2020, 4, 1, 13, 21, 10, tzinfo=dt_util.UTC)
    mock_vacuum.last_clean_details().end = datetime(2020, 4, 1, 13, 21, 10, tzinfo=dt_util.UTC)
    mock_vacuum.last_clean_details().duration = timedelta(hours=11, minutes=15, seconds=34)
    mock_vacuum.last_clean_details().area = 133.43218
    mock_vacuum.last_clean_details().error_code = 1
    mock_vacuum.last_clean_details().error = 'test_error_code'
    mock_vacuum.last_clean_details().complete = True
    mock_timer_1 = MagicMock()
    mock_timer_1.enabled = True
    mock_timer_1.cron = '5 5 1 8 1'
    mock_timer_1.next_schedule = datetime(2020, 5, 23, 13, 21, 10, tzinfo=dt_util.UTC)
    mock_timer_2 = MagicMock()
    mock_timer_2.enabled = False
    mock_timer_2.cron = '5 5 1 8 2'
    mock_timer_2.next_schedule = datetime(2020, 5, 23, 13, 21, 10, tzinfo=dt_util.UTC)
    mock_vacuum.timer.return_value = [mock_timer_1, mock_timer_2]
    with patch('homeassistant.components.xiaomi_miio.RoborockVacuum') as mock_vacuum_cls:
        mock_vacuum_cls.return_value = mock_vacuum
        yield mock_vacuum","mock_vacuum.status().got_error = False
mock_vacuum.dnd_status().enabled = False
mock_timer_2.enabled = False",mock_vacuum.status().got_error = mock_vacuum.dnd_status().enabled = mock_timer_2.enabled = False,1,,,,,,,,,,
core,https://github.com/home-assistant/core/tree/master/tests/components/xiaomi_miio/test_vacuum.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/tests/components/xiaomi_miio/test_vacuum.py,,"def mirobo_is_on_fixture():
    """"""Mock mock_mirobo.""""""
    mock_vacuum = MagicMock()
    mock_vacuum.status().data = {'test': 'raw'}
    mock_vacuum.status().is_on = True
    mock_vacuum.fan_speed_presets.return_value = new_fanspeeds
    mock_vacuum.status().fanspeed = list(new_fanspeeds.values())[0]
    mock_vacuum.status().got_error = False
    mock_vacuum.status().battery = 32
    mock_vacuum.status().clean_area = 133.43218
    mock_vacuum.status().clean_time = timedelta(hours=2, minutes=55, seconds=34)
    mock_vacuum.consumable_status().main_brush_left = timedelta(hours=11, minutes=35, seconds=34)
    mock_vacuum.consumable_status().side_brush_left = timedelta(hours=11, minutes=35, seconds=34)
    mock_vacuum.consumable_status().filter_left = timedelta(hours=11, minutes=35, seconds=34)
    mock_vacuum.clean_history().count = '41'
    mock_vacuum.clean_history().total_area = 323.43218
    mock_vacuum.clean_history().total_duration = timedelta(hours=11, minutes=15, seconds=34)
    mock_vacuum.status().state = 'Test Xiaomi Cleaning'
    mock_vacuum.status().state_code = 5
    mock_vacuum.dnd_status().enabled = False
    mock_vacuum.last_clean_details().start = datetime(2020, 4, 1, 13, 21, 10, tzinfo=dt_util.UTC)
    mock_vacuum.last_clean_details().end = datetime(2020, 4, 1, 13, 21, 10, tzinfo=dt_util.UTC)
    mock_vacuum.last_clean_details().duration = timedelta(hours=11, minutes=15, seconds=34)
    mock_vacuum.last_clean_details().area = 133.43218
    mock_vacuum.last_clean_details().error_code = 1
    mock_vacuum.last_clean_details().error = 'test_error_code'
    mock_vacuum.last_clean_details().complete = True
    mock_timer_1 = MagicMock()
    mock_timer_1.enabled = True
    mock_timer_1.cron = '5 5 1 8 1'
    mock_timer_1.next_schedule = datetime(2020, 5, 23, 13, 21, 10, tzinfo=dt_util.UTC)
    mock_timer_2 = MagicMock()
    mock_timer_2.enabled = False
    mock_timer_2.cron = '5 5 1 8 2'
    mock_timer_2.next_schedule = datetime(2020, 5, 23, 13, 21, 10, tzinfo=dt_util.UTC)
    mock_vacuum.timer.return_value = [mock_timer_1, mock_timer_2]
    with patch('homeassistant.components.xiaomi_miio.RoborockVacuum') as mock_vacuum_cls:
        mock_vacuum_cls.return_value = mock_vacuum
        yield mock_vacuum","mock_vacuum.status().clean_area = 133.43218
mock_vacuum.last_clean_details().area = 133.43218",mock_vacuum.status().clean_area = mock_vacuum.last_clean_details().area = 133.43218,1,,,,,,,,,,
sagemaker-python-sdk,https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/analytics.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sagemaker-python-sdk/src/sagemaker/analytics.py,HyperparameterTuningJobAnalytics,"def clear_cache(self):
    """"""Clear the object of all local caches of API methods.""""""
    super(HyperparameterTuningJobAnalytics, self).clear_cache()
    self._tuning_job_describe_result = None
    self._training_job_summaries = None","self._tuning_job_describe_result = None
self._training_job_summaries = None",self._tuning_job_describe_result = self._training_job_summaries = None,1,,,,,,,,,,
airflow,https://github.com/apache/airflow/tree/master/airflow/models/dag.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/models/dag.py,DAG,"def __init__(self, dag_id: str, description: Optional[str]=None, schedule_interval: ScheduleIntervalArg=ScheduleIntervalArgNotSet, timetable: Optional[Timetable]=None, start_date: Optional[datetime]=None, end_date: Optional[datetime]=None, full_filepath: Optional[str]=None, template_searchpath: Optional[Union[str, Iterable[str]]]=None, template_undefined: Type[jinja2.StrictUndefined]=jinja2.StrictUndefined, user_defined_macros: Optional[Dict]=None, user_defined_filters: Optional[Dict]=None, default_args: Optional[Dict]=None, concurrency: Optional[int]=None, max_active_tasks: int=conf.getint('core', 'max_active_tasks_per_dag'), max_active_runs: int=conf.getint('core', 'max_active_runs_per_dag'), dagrun_timeout: Optional[timedelta]=None, sla_miss_callback: Optional[Callable[['DAG', str, str, List[str], List[TaskInstance]], None]]=None, default_view: str=conf.get('webserver', 'dag_default_view').lower(), orientation: str=conf.get('webserver', 'dag_orientation'), catchup: bool=conf.getboolean('scheduler', 'catchup_by_default'), on_success_callback: Optional[DagStateChangeCallback]=None, on_failure_callback: Optional[DagStateChangeCallback]=None, doc_md: Optional[str]=None, params: Optional[Dict]=None, access_control: Optional[Dict]=None, is_paused_upon_creation: Optional[bool]=None, jinja_environment_kwargs: Optional[Dict]=None, render_template_as_native_obj: bool=False, tags: Optional[List[str]]=None):
    from airflow.utils.task_group import TaskGroup
    self.user_defined_macros = user_defined_macros
    self.user_defined_filters = user_defined_filters
    self.default_args = copy.deepcopy(default_args or {})
    self.params = params or {}
    if 'params' in self.default_args:
        self.params.update(self.default_args['params'])
        del self.default_args['params']
    self.params = ParamsDict(self.params)
    if full_filepath:
        warnings.warn('Passing full_filepath to DAG() is deprecated and has no effect', DeprecationWarning, stacklevel=2)
    validate_key(dag_id)
    self._dag_id = dag_id
    if concurrency:
        warnings.warn(""The 'concurrency' parameter is deprecated. Please use 'max_active_tasks'."", DeprecationWarning, stacklevel=2)
        max_active_tasks = concurrency
    self._max_active_tasks = max_active_tasks
    self._pickle_id: Optional[int] = None
    self._description = description
    back = sys._getframe().f_back
    self.fileloc = back.f_code.co_filename if back else ''
    self.task_dict: Dict[str, BaseOperator] = {}
    if start_date and start_date.tzinfo:
        self.timezone = start_date.tzinfo
    elif 'start_date' in self.default_args and self.default_args['start_date']:
        if isinstance(self.default_args['start_date'], str):
            self.default_args['start_date'] = timezone.parse(self.default_args['start_date'])
        self.timezone = self.default_args['start_date'].tzinfo
    if not hasattr(self, 'timezone') or not self.timezone:
        self.timezone = settings.TIMEZONE
    if 'end_date' in self.default_args and self.default_args['end_date']:
        if isinstance(self.default_args['end_date'], str):
            self.default_args['end_date'] = timezone.parse(self.default_args['end_date'], timezone=self.timezone)
    self.start_date = timezone.convert_to_utc(start_date)
    self.end_date = timezone.convert_to_utc(end_date)
    if 'start_date' in self.default_args:
        self.default_args['start_date'] = timezone.convert_to_utc(self.default_args['start_date'])
    if 'end_date' in self.default_args:
        self.default_args['end_date'] = timezone.convert_to_utc(self.default_args['end_date'])
    if timetable is None:
        self.timetable = create_timetable(schedule_interval, self.timezone)
        if schedule_interval is ScheduleIntervalArgNotSet:
            schedule_interval = DEFAULT_SCHEDULE_INTERVAL
        self.schedule_interval: ScheduleInterval = schedule_interval
    elif schedule_interval is ScheduleIntervalArgNotSet:
        self.timetable = timetable
        self.schedule_interval = self.timetable.summary
    else:
        raise TypeError(""cannot specify both 'schedule_interval' and 'timetable'"")
    if isinstance(template_searchpath, str):
        template_searchpath = [template_searchpath]
    self.template_searchpath = template_searchpath
    self.template_undefined = template_undefined
    self.parent_dag: Optional[DAG] = None
    self.last_loaded = timezone.utcnow()
    self.safe_dag_id = dag_id.replace('.', '__dot__')
    self.max_active_runs = max_active_runs
    self.dagrun_timeout = dagrun_timeout
    self.sla_miss_callback = sla_miss_callback
    if default_view in DEFAULT_VIEW_PRESETS:
        self._default_view: str = default_view
    else:
        raise AirflowException(f'Invalid values of dag.default_view: only support {DEFAULT_VIEW_PRESETS}, but get {default_view}')
    if orientation in ORIENTATION_PRESETS:
        self.orientation = orientation
    else:
        raise AirflowException(f'Invalid values of dag.orientation: only support {ORIENTATION_PRESETS}, but get {orientation}')
    self.catchup = catchup
    self.is_subdag = False
    self.partial = False
    self.on_success_callback = on_success_callback
    self.on_failure_callback = on_failure_callback
    self.edge_info: Dict[str, Dict[str, EdgeInfoType]] = {}
    self.has_on_success_callback = self.on_success_callback is not None
    self.has_on_failure_callback = self.on_failure_callback is not None
    self.doc_md = doc_md
    self._access_control = DAG._upgrade_outdated_dag_access_control(access_control)
    self.is_paused_upon_creation = is_paused_upon_creation
    self.jinja_environment_kwargs = jinja_environment_kwargs
    self.render_template_as_native_obj = render_template_as_native_obj
    self.tags = tags
    self._task_group = TaskGroup.create_root(self)
    self.validate_schedule_and_params()","self.is_subdag = False
self.partial = False",self.is_subdag = self.partial = False,1,,,,,,,,,,
diff-match-patch,https://github.com/google/diff-match-patch/tree/master/python3/diff_match_patch.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diff-match-patch/python3/diff_match_patch.py,diff_match_patch,"def __init__(self):
    """"""Inits a diff_match_patch object with default settings.
    Redefine these in your program to override the defaults.
    """"""
    self.Diff_Timeout = 1.0
    self.Diff_EditCost = 4
    self.Match_Threshold = 0.5
    self.Match_Distance = 1000
    self.Patch_DeleteThreshold = 0.5
    self.Patch_Margin = 4
    self.Match_MaxBits = 32","self.Diff_EditCost = 4
self.Patch_Margin = 4",self.Diff_EditCost = self.Patch_Margin = 4,1,,,,,,,,,,
diff-match-patch,https://github.com/google/diff-match-patch/tree/master/python3/diff_match_patch.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diff-match-patch/python3/diff_match_patch.py,diff_match_patch,"def __init__(self):
    """"""Inits a diff_match_patch object with default settings.
    Redefine these in your program to override the defaults.
    """"""
    self.Diff_Timeout = 1.0
    self.Diff_EditCost = 4
    self.Match_Threshold = 0.5
    self.Match_Distance = 1000
    self.Patch_DeleteThreshold = 0.5
    self.Patch_Margin = 4
    self.Match_MaxBits = 32","self.Match_Threshold = 0.5
self.Patch_DeleteThreshold = 0.5",self.Match_Threshold = self.Patch_DeleteThreshold = 0.5,1,,,,,,,,,,
alerta,https://github.com/alerta/alerta/tree/master/alerta/models/alert.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/alerta/alerta/models/alert.py,Alert,"def update(self, correlate_with) -> 'Alert':
    now = datetime.utcnow()
    self.previous_severity = db.get_severity(self)
    self.trend_indication = alarm_model.trend(self.previous_severity, self.severity)
    (status, _, previous_status, _) = self._get_hist_info()
    (_, new_status) = alarm_model.transition(alert=self, current_status=status, previous_status=previous_status)
    self.duplicate_count = 0
    self.repeat = False
    self.receive_time = now
    self.last_receive_id = self.id
    self.last_receive_time = now
    if new_status != status:
        r = status_change_hook.send(correlate_with, status=new_status, text=self.text)
        (_, (_, new_status, text)) = r[0]
        self.update_time = now
    else:
        text = self.text
    history = [History(id=self.id, event=self.event, severity=self.severity, status=new_status, value=self.value, text=text, change_type=ChangeType.severity, update_time=self.create_time, user=g.login, timeout=self.timeout)]
    self.status = new_status
    return Alert.from_db(db.correlate_alert(self, history))","self.duplicate_count = 0
self.repeat = False",self.duplicate_count = self.repeat = 0,1,,,,,,,,,,
pyatv,https://github.com/postlund/pyatv/tree/master/pyatv/protocols/mrp/messages.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyatv/pyatv/protocols/mrp/messages.py,,"def device_information(name, identifier, update=False):
    """"""Create a new DEVICE_INFO_MESSAGE.""""""
    msg_type = protobuf.DEVICE_INFO_UPDATE_MESSAGE if update else protobuf.DEVICE_INFO_MESSAGE
    message = create(msg_type)
    info = message.inner()
    info.allowsPairing = True
    info.applicationBundleIdentifier = 'com.apple.TVRemote'
    info.applicationBundleVersion = '344.28'
    info.lastSupportedMessageType = 108
    info.localizedModelName = 'iPhone'
    info.name = name
    info.protocolVersion = 1
    info.sharedQueueVersion = 2
    info.supportsACL = True
    info.supportsExtendedMotion = True
    info.supportsSharedQueue = True
    info.supportsSystemPairing = True
    info.systemBuildVersion = '18A393'
    info.systemMediaApplication = 'com.apple.TVMusic'
    info.uniqueIdentifier = identifier
    info.deviceClass = protobuf.DeviceClass.iPhone
    info.logicalDeviceCount = 1
    return message","info.allowsPairing = True
info.protocolVersion = 1
info.supportsACL = True
info.supportsExtendedMotion = True
info.supportsSharedQueue = True
info.supportsSystemPairing = True
info.logicalDeviceCount = 1","info.allowsPairing = info.supportsACL = info.supportsExtendedMotion = info.supportsSharedQueue = info.supportsSystemPairing = True
info.protocolVersion = info.logicalDeviceCount = 1",1,,,,,,,,,,
conan-center-index,https://github.com/conan-io/conan-center-index/tree/master/recipes/opencv/3.x/conanfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan-center-index/recipes/opencv/3.x/conanfile.py,OpenCVConan,"def _configure_cmake(self):
    if self._cmake:
        return self._cmake
    self._cmake = CMake(self)
    self._cmake.definitions['OPENCV_CONFIG_INSTALL_PATH'] = 'cmake'
    self._cmake.definitions['OPENCV_BIN_INSTALL_PATH'] = 'bin'
    self._cmake.definitions['OPENCV_LIB_INSTALL_PATH'] = 'lib'
    self._cmake.definitions['OPENCV_3P_LIB_INSTALL_PATH'] = 'lib'
    self._cmake.definitions['OPENCV_OTHER_INSTALL_PATH'] = 'res'
    self._cmake.definitions['OPENCV_LICENSES_INSTALL_PATH'] = 'licenses'
    self._cmake.definitions['BUILD_EXAMPLES'] = False
    self._cmake.definitions['BUILD_DOCS'] = False
    self._cmake.definitions['BUILD_TESTS'] = False
    self._cmake.definitions['BUILD_PACKAGE'] = False
    self._cmake.definitions['BUILD_PERF_TESTS'] = False
    self._cmake.definitions['BUILD_JAVA'] = False
    self._cmake.definitions['BUILD_FAT_JAVA_LIB'] = False
    self._cmake.definitions['BUILD_PERF_TESTS'] = False
    self._cmake.definitions['BUILD_ZLIB'] = False
    self._cmake.definitions['BUILD_JPEG'] = False
    self._cmake.definitions['BUILD_PNG'] = False
    self._cmake.definitions['BUILD_TIFF'] = False
    self._cmake.definitions['BUILD_JASPER'] = False
    self._cmake.definitions['BUILD_OPENEXR'] = False
    self._cmake.definitions['BUILD_WEBP'] = False
    self._cmake.definitions['BUILD_TBB'] = False
    self._cmake.definitions['BUILD_JPEG_TURBO_DISABLE'] = True
    self._cmake.definitions['BUILD_IPP_IW'] = False
    self._cmake.definitions['BUILD_ITT'] = False
    self._cmake.definitions['BUILD_PROTOBUF'] = False
    self._cmake.definitions['BUILD_USE_SYMLINKS'] = False
    self._cmake.definitions['OPENCV_FORCE_3RDPARTY_BUILD'] = False
    self._cmake.definitions['BUILD_opencv_java_bindings_gen'] = False
    self._cmake.definitions['BUILD_opencv_js'] = False
    self._cmake.definitions['BUILD_opencv_apps'] = False
    self._cmake.definitions['BUILD_opencv_java'] = False
    self._cmake.definitions['OPENCV_PYTHON_SKIP_DETECTION'] = True
    self._cmake.definitions['BUILD_opencv_python2'] = False
    self._cmake.definitions['BUILD_opencv_python3'] = False
    self._cmake.definitions['BUILD_opencv_python_bindings_g'] = False
    self._cmake.definitions['BUILD_opencv_python_tests'] = False
    self._cmake.definitions['BUILD_opencv_ts'] = False
    self._cmake.definitions['WITH_CUFFT'] = False
    self._cmake.definitions['WITH_CUBLAS'] = False
    self._cmake.definitions['WITH_NVCUVID'] = False
    self._cmake.definitions['WITH_FFMPEG'] = False
    self._cmake.definitions['WITH_GSTREAMER'] = False
    self._cmake.definitions['WITH_OPENCL'] = False
    self._cmake.definitions['WITH_CUDA'] = False
    self._cmake.definitions['WITH_1394'] = False
    self._cmake.definitions['WITH_ADE'] = False
    self._cmake.definitions['WITH_ARAVIS'] = False
    self._cmake.definitions['WITH_CLP'] = False
    self._cmake.definitions['WITH_HALIDE'] = False
    self._cmake.definitions['WITH_HPX'] = False
    self._cmake.definitions['WITH_IMGCODEC_HDR'] = False
    self._cmake.definitions['WITH_IMGCODEC_PFM'] = False
    self._cmake.definitions['WITH_IMGCODEC_PXM'] = False
    self._cmake.definitions['WITH_IMGCODEC_SUNRASTER'] = False
    self._cmake.definitions['WITH_INF_ENGINE'] = False
    self._cmake.definitions['WITH_IPP'] = False
    self._cmake.definitions['WITH_ITT'] = False
    self._cmake.definitions['WITH_LIBREALSENSE'] = False
    self._cmake.definitions['WITH_MFX'] = False
    self._cmake.definitions['WITH_NGRAPH'] = False
    self._cmake.definitions['WITH_OPENCLAMDBLAS'] = False
    self._cmake.definitions['WITH_OPENCLAMDFFT'] = False
    self._cmake.definitions['WITH_OPENCL_SVM'] = False
    self._cmake.definitions['WITH_OPENGL'] = False
    self._cmake.definitions['WITH_OPENNI'] = False
    self._cmake.definitions['WITH_OPENNI2'] = False
    self._cmake.definitions['WITH_OPENVX'] = False
    self._cmake.definitions['WITH_PLAIDML'] = False
    self._cmake.definitions['WITH_PROTOBUF'] = False
    self._cmake.definitions['WITH_PTHREADS_PF'] = False
    self._cmake.definitions['WITH_PVAPI'] = False
    self._cmake.definitions['WITH_QT'] = False
    self._cmake.definitions['WITH_QUIRC'] = False
    self._cmake.definitions['WITH_V4L'] = False
    self._cmake.definitions['WITH_VA'] = False
    self._cmake.definitions['WITH_VA_INTEL'] = False
    self._cmake.definitions['WITH_VTK'] = False
    self._cmake.definitions['WITH_VULKAN'] = False
    self._cmake.definitions['WITH_XIMEA'] = False
    self._cmake.definitions['WITH_XINE'] = False
    self._cmake.definitions['WITH_LAPACK'] = False
    self._cmake.definitions['WITH_IPP_IW'] = False
    self._cmake.definitions['WITH_CAROTENE'] = False
    self._cmake.definitions['WITH_PROTOBUF'] = False
    self._cmake.definitions['WITH_LAPACK'] = False
    self._cmake.definitions['WITH_JPEG'] = self.options.with_jpeg
    self._cmake.definitions['WITH_PNG'] = self.options.with_png
    self._cmake.definitions['WITH_TIFF'] = self.options.with_tiff
    self._cmake.definitions['WITH_JASPER'] = self.options.with_jasper
    self._cmake.definitions['WITH_OPENEXR'] = self.options.with_openexr
    self._cmake.definitions['WITH_EIGEN'] = self.options.with_eigen
    self._cmake.definitions['WITH_WEBP'] = self.options.with_webp
    self._cmake.definitions['WITH_DSHOW'] = self._is_msvc
    self._cmake.definitions['WITH_MSMF'] = self._is_msvc
    self._cmake.definitions['WITH_MSMF_DXVA'] = self._is_msvc
    self._cmake.definitions['WITH_GTK'] = self.options.get_safe('with_gtk', False)
    self._cmake.definitions['WITH_GTK_2_X'] = self.options.get_safe('with_gtk', False)
    self._cmake.definitions['OPENCV_MODULES_PUBLIC'] = 'opencv'
    self._cmake.definitions['OPENCV_ENABLE_NONFREE'] = self.options.nonfree
    if self.options.parallel:
        self._cmake.definitions['WITH_TBB'] = self.options.parallel == 'tbb'
        self._cmake.definitions['WITH_OPENMP'] = self.options.parallel == 'openmp'
    if self.options.contrib:
        self._cmake.definitions['OPENCV_EXTRA_MODULES_PATH'] = os.path.join(self.build_folder, self._contrib_folder, 'modules')
    if self._is_msvc:
        self._cmake.definitions['BUILD_WITH_STATIC_CRT'] = 'MT' in msvc_runtime_flag(self)
    if self.options.with_openexr:
        self._cmake.definitions['OPENEXR_ROOT'] = self.deps_cpp_info['openexr'].rootpath.replace('\\', '/')
    self._cmake.definitions['ENABLE_PIC'] = self.options.get_safe('fPIC', True)
    self._cmake.definitions['ENABLE_CCACHE'] = False
    self._cmake.configure(build_folder=self._build_subfolder)
    return self._cmake","self._cmake.definitions['OPENCV_LIB_INSTALL_PATH'] = 'lib'
self._cmake.definitions['OPENCV_3P_LIB_INSTALL_PATH'] = 'lib'",self._cmake.definitions['OPENCV_LIB_INSTALL_PATH'] = self._cmake.definitions['OPENCV_3P_LIB_INSTALL_PATH'] = 'lib',1,,,,,,,,,,
conan-center-index,https://github.com/conan-io/conan-center-index/tree/master/recipes/opencv/3.x/conanfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan-center-index/recipes/opencv/3.x/conanfile.py,OpenCVConan,"def _configure_cmake(self):
    if self._cmake:
        return self._cmake
    self._cmake = CMake(self)
    self._cmake.definitions['OPENCV_CONFIG_INSTALL_PATH'] = 'cmake'
    self._cmake.definitions['OPENCV_BIN_INSTALL_PATH'] = 'bin'
    self._cmake.definitions['OPENCV_LIB_INSTALL_PATH'] = 'lib'
    self._cmake.definitions['OPENCV_3P_LIB_INSTALL_PATH'] = 'lib'
    self._cmake.definitions['OPENCV_OTHER_INSTALL_PATH'] = 'res'
    self._cmake.definitions['OPENCV_LICENSES_INSTALL_PATH'] = 'licenses'
    self._cmake.definitions['BUILD_EXAMPLES'] = False
    self._cmake.definitions['BUILD_DOCS'] = False
    self._cmake.definitions['BUILD_TESTS'] = False
    self._cmake.definitions['BUILD_PACKAGE'] = False
    self._cmake.definitions['BUILD_PERF_TESTS'] = False
    self._cmake.definitions['BUILD_JAVA'] = False
    self._cmake.definitions['BUILD_FAT_JAVA_LIB'] = False
    self._cmake.definitions['BUILD_PERF_TESTS'] = False
    self._cmake.definitions['BUILD_ZLIB'] = False
    self._cmake.definitions['BUILD_JPEG'] = False
    self._cmake.definitions['BUILD_PNG'] = False
    self._cmake.definitions['BUILD_TIFF'] = False
    self._cmake.definitions['BUILD_JASPER'] = False
    self._cmake.definitions['BUILD_OPENEXR'] = False
    self._cmake.definitions['BUILD_WEBP'] = False
    self._cmake.definitions['BUILD_TBB'] = False
    self._cmake.definitions['BUILD_JPEG_TURBO_DISABLE'] = True
    self._cmake.definitions['BUILD_IPP_IW'] = False
    self._cmake.definitions['BUILD_ITT'] = False
    self._cmake.definitions['BUILD_PROTOBUF'] = False
    self._cmake.definitions['BUILD_USE_SYMLINKS'] = False
    self._cmake.definitions['OPENCV_FORCE_3RDPARTY_BUILD'] = False
    self._cmake.definitions['BUILD_opencv_java_bindings_gen'] = False
    self._cmake.definitions['BUILD_opencv_js'] = False
    self._cmake.definitions['BUILD_opencv_apps'] = False
    self._cmake.definitions['BUILD_opencv_java'] = False
    self._cmake.definitions['OPENCV_PYTHON_SKIP_DETECTION'] = True
    self._cmake.definitions['BUILD_opencv_python2'] = False
    self._cmake.definitions['BUILD_opencv_python3'] = False
    self._cmake.definitions['BUILD_opencv_python_bindings_g'] = False
    self._cmake.definitions['BUILD_opencv_python_tests'] = False
    self._cmake.definitions['BUILD_opencv_ts'] = False
    self._cmake.definitions['WITH_CUFFT'] = False
    self._cmake.definitions['WITH_CUBLAS'] = False
    self._cmake.definitions['WITH_NVCUVID'] = False
    self._cmake.definitions['WITH_FFMPEG'] = False
    self._cmake.definitions['WITH_GSTREAMER'] = False
    self._cmake.definitions['WITH_OPENCL'] = False
    self._cmake.definitions['WITH_CUDA'] = False
    self._cmake.definitions['WITH_1394'] = False
    self._cmake.definitions['WITH_ADE'] = False
    self._cmake.definitions['WITH_ARAVIS'] = False
    self._cmake.definitions['WITH_CLP'] = False
    self._cmake.definitions['WITH_HALIDE'] = False
    self._cmake.definitions['WITH_HPX'] = False
    self._cmake.definitions['WITH_IMGCODEC_HDR'] = False
    self._cmake.definitions['WITH_IMGCODEC_PFM'] = False
    self._cmake.definitions['WITH_IMGCODEC_PXM'] = False
    self._cmake.definitions['WITH_IMGCODEC_SUNRASTER'] = False
    self._cmake.definitions['WITH_INF_ENGINE'] = False
    self._cmake.definitions['WITH_IPP'] = False
    self._cmake.definitions['WITH_ITT'] = False
    self._cmake.definitions['WITH_LIBREALSENSE'] = False
    self._cmake.definitions['WITH_MFX'] = False
    self._cmake.definitions['WITH_NGRAPH'] = False
    self._cmake.definitions['WITH_OPENCLAMDBLAS'] = False
    self._cmake.definitions['WITH_OPENCLAMDFFT'] = False
    self._cmake.definitions['WITH_OPENCL_SVM'] = False
    self._cmake.definitions['WITH_OPENGL'] = False
    self._cmake.definitions['WITH_OPENNI'] = False
    self._cmake.definitions['WITH_OPENNI2'] = False
    self._cmake.definitions['WITH_OPENVX'] = False
    self._cmake.definitions['WITH_PLAIDML'] = False
    self._cmake.definitions['WITH_PROTOBUF'] = False
    self._cmake.definitions['WITH_PTHREADS_PF'] = False
    self._cmake.definitions['WITH_PVAPI'] = False
    self._cmake.definitions['WITH_QT'] = False
    self._cmake.definitions['WITH_QUIRC'] = False
    self._cmake.definitions['WITH_V4L'] = False
    self._cmake.definitions['WITH_VA'] = False
    self._cmake.definitions['WITH_VA_INTEL'] = False
    self._cmake.definitions['WITH_VTK'] = False
    self._cmake.definitions['WITH_VULKAN'] = False
    self._cmake.definitions['WITH_XIMEA'] = False
    self._cmake.definitions['WITH_XINE'] = False
    self._cmake.definitions['WITH_LAPACK'] = False
    self._cmake.definitions['WITH_IPP_IW'] = False
    self._cmake.definitions['WITH_CAROTENE'] = False
    self._cmake.definitions['WITH_PROTOBUF'] = False
    self._cmake.definitions['WITH_LAPACK'] = False
    self._cmake.definitions['WITH_JPEG'] = self.options.with_jpeg
    self._cmake.definitions['WITH_PNG'] = self.options.with_png
    self._cmake.definitions['WITH_TIFF'] = self.options.with_tiff
    self._cmake.definitions['WITH_JASPER'] = self.options.with_jasper
    self._cmake.definitions['WITH_OPENEXR'] = self.options.with_openexr
    self._cmake.definitions['WITH_EIGEN'] = self.options.with_eigen
    self._cmake.definitions['WITH_WEBP'] = self.options.with_webp
    self._cmake.definitions['WITH_DSHOW'] = self._is_msvc
    self._cmake.definitions['WITH_MSMF'] = self._is_msvc
    self._cmake.definitions['WITH_MSMF_DXVA'] = self._is_msvc
    self._cmake.definitions['WITH_GTK'] = self.options.get_safe('with_gtk', False)
    self._cmake.definitions['WITH_GTK_2_X'] = self.options.get_safe('with_gtk', False)
    self._cmake.definitions['OPENCV_MODULES_PUBLIC'] = 'opencv'
    self._cmake.definitions['OPENCV_ENABLE_NONFREE'] = self.options.nonfree
    if self.options.parallel:
        self._cmake.definitions['WITH_TBB'] = self.options.parallel == 'tbb'
        self._cmake.definitions['WITH_OPENMP'] = self.options.parallel == 'openmp'
    if self.options.contrib:
        self._cmake.definitions['OPENCV_EXTRA_MODULES_PATH'] = os.path.join(self.build_folder, self._contrib_folder, 'modules')
    if self._is_msvc:
        self._cmake.definitions['BUILD_WITH_STATIC_CRT'] = 'MT' in msvc_runtime_flag(self)
    if self.options.with_openexr:
        self._cmake.definitions['OPENEXR_ROOT'] = self.deps_cpp_info['openexr'].rootpath.replace('\\', '/')
    self._cmake.definitions['ENABLE_PIC'] = self.options.get_safe('fPIC', True)
    self._cmake.definitions['ENABLE_CCACHE'] = False
    self._cmake.configure(build_folder=self._build_subfolder)
    return self._cmake","self._cmake.definitions['BUILD_EXAMPLES'] = False
self._cmake.definitions['BUILD_DOCS'] = False
self._cmake.definitions['BUILD_TESTS'] = False
self._cmake.definitions['BUILD_PACKAGE'] = False
self._cmake.definitions['BUILD_PERF_TESTS'] = False
self._cmake.definitions['BUILD_JAVA'] = False
self._cmake.definitions['BUILD_FAT_JAVA_LIB'] = False
self._cmake.definitions['BUILD_PERF_TESTS'] = False
self._cmake.definitions['BUILD_ZLIB'] = False
self._cmake.definitions['BUILD_JPEG'] = False
self._cmake.definitions['BUILD_PNG'] = False
self._cmake.definitions['BUILD_TIFF'] = False
self._cmake.definitions['BUILD_JASPER'] = False
self._cmake.definitions['BUILD_OPENEXR'] = False
self._cmake.definitions['BUILD_WEBP'] = False
self._cmake.definitions['BUILD_TBB'] = False
self._cmake.definitions['BUILD_IPP_IW'] = False
self._cmake.definitions['BUILD_ITT'] = False
self._cmake.definitions['BUILD_PROTOBUF'] = False
self._cmake.definitions['BUILD_USE_SYMLINKS'] = False
self._cmake.definitions['OPENCV_FORCE_3RDPARTY_BUILD'] = False
self._cmake.definitions['BUILD_opencv_java_bindings_gen'] = False
self._cmake.definitions['BUILD_opencv_js'] = False
self._cmake.definitions['BUILD_opencv_apps'] = False
self._cmake.definitions['BUILD_opencv_java'] = False
self._cmake.definitions['BUILD_opencv_python2'] = False
self._cmake.definitions['BUILD_opencv_python3'] = False
self._cmake.definitions['BUILD_opencv_python_bindings_g'] = False
self._cmake.definitions['BUILD_opencv_python_tests'] = False
self._cmake.definitions['BUILD_opencv_ts'] = False
self._cmake.definitions['WITH_CUFFT'] = False
self._cmake.definitions['WITH_CUBLAS'] = False
self._cmake.definitions['WITH_NVCUVID'] = False
self._cmake.definitions['WITH_FFMPEG'] = False
self._cmake.definitions['WITH_GSTREAMER'] = False
self._cmake.definitions['WITH_OPENCL'] = False
self._cmake.definitions['WITH_CUDA'] = False
self._cmake.definitions['WITH_1394'] = False
self._cmake.definitions['WITH_ADE'] = False
self._cmake.definitions['WITH_ARAVIS'] = False
self._cmake.definitions['WITH_CLP'] = False
self._cmake.definitions['WITH_HALIDE'] = False
self._cmake.definitions['WITH_HPX'] = False
self._cmake.definitions['WITH_IMGCODEC_HDR'] = False
self._cmake.definitions['WITH_IMGCODEC_PFM'] = False
self._cmake.definitions['WITH_IMGCODEC_PXM'] = False
self._cmake.definitions['WITH_IMGCODEC_SUNRASTER'] = False
self._cmake.definitions['WITH_INF_ENGINE'] = False
self._cmake.definitions['WITH_IPP'] = False
self._cmake.definitions['WITH_ITT'] = False
self._cmake.definitions['WITH_LIBREALSENSE'] = False
self._cmake.definitions['WITH_MFX'] = False
self._cmake.definitions['WITH_NGRAPH'] = False
self._cmake.definitions['WITH_OPENCLAMDBLAS'] = False
self._cmake.definitions['WITH_OPENCLAMDFFT'] = False
self._cmake.definitions['WITH_OPENCL_SVM'] = False
self._cmake.definitions['WITH_OPENGL'] = False
self._cmake.definitions['WITH_OPENNI'] = False
self._cmake.definitions['WITH_OPENNI2'] = False
self._cmake.definitions['WITH_OPENVX'] = False
self._cmake.definitions['WITH_PLAIDML'] = False
self._cmake.definitions['WITH_PROTOBUF'] = False
self._cmake.definitions['WITH_PTHREADS_PF'] = False
self._cmake.definitions['WITH_PVAPI'] = False
self._cmake.definitions['WITH_QT'] = False
self._cmake.definitions['WITH_QUIRC'] = False
self._cmake.definitions['WITH_V4L'] = False
self._cmake.definitions['WITH_VA'] = False
self._cmake.definitions['WITH_VA_INTEL'] = False
self._cmake.definitions['WITH_VTK'] = False
self._cmake.definitions['WITH_VULKAN'] = False
self._cmake.definitions['WITH_XIMEA'] = False
self._cmake.definitions['WITH_XINE'] = False
self._cmake.definitions['WITH_LAPACK'] = False
self._cmake.definitions['WITH_IPP_IW'] = False
self._cmake.definitions['WITH_CAROTENE'] = False
self._cmake.definitions['WITH_PROTOBUF'] = False
self._cmake.definitions['WITH_LAPACK'] = False",The code cannot be refactored with chained assignment because it involves a loop that iterates over the keys of a dictionary and assigns a value to each key.,0,,,,,,,,,,
conan-center-index,https://github.com/conan-io/conan-center-index/tree/master/recipes/opencv/3.x/conanfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan-center-index/recipes/opencv/3.x/conanfile.py,OpenCVConan,"def _configure_cmake(self):
    if self._cmake:
        return self._cmake
    self._cmake = CMake(self)
    self._cmake.definitions['OPENCV_CONFIG_INSTALL_PATH'] = 'cmake'
    self._cmake.definitions['OPENCV_BIN_INSTALL_PATH'] = 'bin'
    self._cmake.definitions['OPENCV_LIB_INSTALL_PATH'] = 'lib'
    self._cmake.definitions['OPENCV_3P_LIB_INSTALL_PATH'] = 'lib'
    self._cmake.definitions['OPENCV_OTHER_INSTALL_PATH'] = 'res'
    self._cmake.definitions['OPENCV_LICENSES_INSTALL_PATH'] = 'licenses'
    self._cmake.definitions['BUILD_EXAMPLES'] = False
    self._cmake.definitions['BUILD_DOCS'] = False
    self._cmake.definitions['BUILD_TESTS'] = False
    self._cmake.definitions['BUILD_PACKAGE'] = False
    self._cmake.definitions['BUILD_PERF_TESTS'] = False
    self._cmake.definitions['BUILD_JAVA'] = False
    self._cmake.definitions['BUILD_FAT_JAVA_LIB'] = False
    self._cmake.definitions['BUILD_PERF_TESTS'] = False
    self._cmake.definitions['BUILD_ZLIB'] = False
    self._cmake.definitions['BUILD_JPEG'] = False
    self._cmake.definitions['BUILD_PNG'] = False
    self._cmake.definitions['BUILD_TIFF'] = False
    self._cmake.definitions['BUILD_JASPER'] = False
    self._cmake.definitions['BUILD_OPENEXR'] = False
    self._cmake.definitions['BUILD_WEBP'] = False
    self._cmake.definitions['BUILD_TBB'] = False
    self._cmake.definitions['BUILD_JPEG_TURBO_DISABLE'] = True
    self._cmake.definitions['BUILD_IPP_IW'] = False
    self._cmake.definitions['BUILD_ITT'] = False
    self._cmake.definitions['BUILD_PROTOBUF'] = False
    self._cmake.definitions['BUILD_USE_SYMLINKS'] = False
    self._cmake.definitions['OPENCV_FORCE_3RDPARTY_BUILD'] = False
    self._cmake.definitions['BUILD_opencv_java_bindings_gen'] = False
    self._cmake.definitions['BUILD_opencv_js'] = False
    self._cmake.definitions['BUILD_opencv_apps'] = False
    self._cmake.definitions['BUILD_opencv_java'] = False
    self._cmake.definitions['OPENCV_PYTHON_SKIP_DETECTION'] = True
    self._cmake.definitions['BUILD_opencv_python2'] = False
    self._cmake.definitions['BUILD_opencv_python3'] = False
    self._cmake.definitions['BUILD_opencv_python_bindings_g'] = False
    self._cmake.definitions['BUILD_opencv_python_tests'] = False
    self._cmake.definitions['BUILD_opencv_ts'] = False
    self._cmake.definitions['WITH_CUFFT'] = False
    self._cmake.definitions['WITH_CUBLAS'] = False
    self._cmake.definitions['WITH_NVCUVID'] = False
    self._cmake.definitions['WITH_FFMPEG'] = False
    self._cmake.definitions['WITH_GSTREAMER'] = False
    self._cmake.definitions['WITH_OPENCL'] = False
    self._cmake.definitions['WITH_CUDA'] = False
    self._cmake.definitions['WITH_1394'] = False
    self._cmake.definitions['WITH_ADE'] = False
    self._cmake.definitions['WITH_ARAVIS'] = False
    self._cmake.definitions['WITH_CLP'] = False
    self._cmake.definitions['WITH_HALIDE'] = False
    self._cmake.definitions['WITH_HPX'] = False
    self._cmake.definitions['WITH_IMGCODEC_HDR'] = False
    self._cmake.definitions['WITH_IMGCODEC_PFM'] = False
    self._cmake.definitions['WITH_IMGCODEC_PXM'] = False
    self._cmake.definitions['WITH_IMGCODEC_SUNRASTER'] = False
    self._cmake.definitions['WITH_INF_ENGINE'] = False
    self._cmake.definitions['WITH_IPP'] = False
    self._cmake.definitions['WITH_ITT'] = False
    self._cmake.definitions['WITH_LIBREALSENSE'] = False
    self._cmake.definitions['WITH_MFX'] = False
    self._cmake.definitions['WITH_NGRAPH'] = False
    self._cmake.definitions['WITH_OPENCLAMDBLAS'] = False
    self._cmake.definitions['WITH_OPENCLAMDFFT'] = False
    self._cmake.definitions['WITH_OPENCL_SVM'] = False
    self._cmake.definitions['WITH_OPENGL'] = False
    self._cmake.definitions['WITH_OPENNI'] = False
    self._cmake.definitions['WITH_OPENNI2'] = False
    self._cmake.definitions['WITH_OPENVX'] = False
    self._cmake.definitions['WITH_PLAIDML'] = False
    self._cmake.definitions['WITH_PROTOBUF'] = False
    self._cmake.definitions['WITH_PTHREADS_PF'] = False
    self._cmake.definitions['WITH_PVAPI'] = False
    self._cmake.definitions['WITH_QT'] = False
    self._cmake.definitions['WITH_QUIRC'] = False
    self._cmake.definitions['WITH_V4L'] = False
    self._cmake.definitions['WITH_VA'] = False
    self._cmake.definitions['WITH_VA_INTEL'] = False
    self._cmake.definitions['WITH_VTK'] = False
    self._cmake.definitions['WITH_VULKAN'] = False
    self._cmake.definitions['WITH_XIMEA'] = False
    self._cmake.definitions['WITH_XINE'] = False
    self._cmake.definitions['WITH_LAPACK'] = False
    self._cmake.definitions['WITH_IPP_IW'] = False
    self._cmake.definitions['WITH_CAROTENE'] = False
    self._cmake.definitions['WITH_PROTOBUF'] = False
    self._cmake.definitions['WITH_LAPACK'] = False
    self._cmake.definitions['WITH_JPEG'] = self.options.with_jpeg
    self._cmake.definitions['WITH_PNG'] = self.options.with_png
    self._cmake.definitions['WITH_TIFF'] = self.options.with_tiff
    self._cmake.definitions['WITH_JASPER'] = self.options.with_jasper
    self._cmake.definitions['WITH_OPENEXR'] = self.options.with_openexr
    self._cmake.definitions['WITH_EIGEN'] = self.options.with_eigen
    self._cmake.definitions['WITH_WEBP'] = self.options.with_webp
    self._cmake.definitions['WITH_DSHOW'] = self._is_msvc
    self._cmake.definitions['WITH_MSMF'] = self._is_msvc
    self._cmake.definitions['WITH_MSMF_DXVA'] = self._is_msvc
    self._cmake.definitions['WITH_GTK'] = self.options.get_safe('with_gtk', False)
    self._cmake.definitions['WITH_GTK_2_X'] = self.options.get_safe('with_gtk', False)
    self._cmake.definitions['OPENCV_MODULES_PUBLIC'] = 'opencv'
    self._cmake.definitions['OPENCV_ENABLE_NONFREE'] = self.options.nonfree
    if self.options.parallel:
        self._cmake.definitions['WITH_TBB'] = self.options.parallel == 'tbb'
        self._cmake.definitions['WITH_OPENMP'] = self.options.parallel == 'openmp'
    if self.options.contrib:
        self._cmake.definitions['OPENCV_EXTRA_MODULES_PATH'] = os.path.join(self.build_folder, self._contrib_folder, 'modules')
    if self._is_msvc:
        self._cmake.definitions['BUILD_WITH_STATIC_CRT'] = 'MT' in msvc_runtime_flag(self)
    if self.options.with_openexr:
        self._cmake.definitions['OPENEXR_ROOT'] = self.deps_cpp_info['openexr'].rootpath.replace('\\', '/')
    self._cmake.definitions['ENABLE_PIC'] = self.options.get_safe('fPIC', True)
    self._cmake.definitions['ENABLE_CCACHE'] = False
    self._cmake.configure(build_folder=self._build_subfolder)
    return self._cmake","self._cmake.definitions['BUILD_JPEG_TURBO_DISABLE'] = True
self._cmake.definitions['OPENCV_PYTHON_SKIP_DETECTION'] = True",self._cmake.definitions['BUILD_JPEG_TURBO_DISABLE'] = self._cmake.definitions['OPENCV_PYTHON_SKIP_DETECTION'] = True,1,,,,,,,,,,
Algoritmos-e-Estruturas-de-Dados,https://github.com/kelvins/Algoritmos-e-Estruturas-de-Dados/tree/master/src/python/arvore_binaria_de_busca.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Algoritmos-e-Estruturas-de-Dados/src/python/arvore_binaria_de_busca.py,Arvore,"def __init__(self, chave):
    self.chave = chave
    self.esquerda = None
    self.direita = None","self.esquerda = None
self.direita = None",self.esquerda = self.direita = None,11,,,,,,,,,,
LightNet,https://github.com/linksense/LightNet/tree/master/scripts/model_measure.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LightNet/scripts/model_measure.py,,"def measure_layer(layer, x):
    global count_ops, count_params
    delta_ops = 0
    delta_params = 0
    multi_add = 1
    type_name = get_layer_info(layer)
    if type_name in ['Conv2d']:
        out_h = int((x.size()[2] + 2 * layer.padding[0] - layer.kernel_size[0]) / layer.stride[0] + 1)
        out_w = int((x.size()[3] + 2 * layer.padding[1] - layer.kernel_size[1]) / layer.stride[1] + 1)
        delta_ops = layer.in_channels * layer.out_channels * layer.kernel_size[0] * layer.kernel_size[1] * out_h * out_w / layer.groups * multi_add
        delta_params = get_layer_param(layer)
    elif type_name in ['ReLU', 'ReLU6', 'LeakyReLU', 'Sigmoid']:
        delta_ops = x.numel()
        delta_params = get_layer_param(layer)
    elif type_name in ['AvgPool2d']:
        in_w = x.size()[2]
        kernel_ops = layer.kernel_size * layer.kernel_size
        out_w = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)
        out_h = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)
        delta_ops = x.size()[0] * x.size()[1] * out_w * out_h * kernel_ops
        delta_params = get_layer_param(layer)
    elif type_name in ['AdaptiveAvgPool2d']:
        delta_ops = x.size()[0] * x.size()[1] * x.size()[2] * x.size()[3]
        delta_params = get_layer_param(layer)
    elif type_name in ['Linear']:
        weight_ops = layer.weight.numel() * multi_add
        bias_ops = layer.bias.numel()
        delta_ops = x.size()[0] * (weight_ops + bias_ops)
        delta_params = get_layer_param(layer)
    elif type_name in ['BatchNorm2d', 'Dropout2d', 'DropChannel', 'Dropout', 'InPlaceABN', 'InPlaceABNSync', 'Upsample', 'MaxPool2d']:
        delta_params = get_layer_param(layer)
    else:
        raise TypeError('unknown layer type: %s' % type_name)
    count_ops += delta_ops
    count_params += delta_params
    return","delta_ops = 0
delta_params = 0",delta_ops = delta_params = 0,1,,,,,,,,,,
mushroom-rl,https://github.com/MushroomRL/mushroom-rl/tree/master/mushroom_rl/environments/minigrid_env.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mushroom-rl/mushroom_rl/environments/minigrid_env.py,MiniGrid,"def __init__(self, name, horizon=None, gamma=0.99, history_length=4, fixed_seed=None, use_pixels=False):
    """"""
        Constructor.

        Args:
             name (str): name of the environment;
             horizon (int, None): the horizon;
             gamma (float, 0.99): the discount factor;
             history_length (int, 4): number of frames to form a state;
             fixed_seed (int, None): if passed, it fixes the seed of the
                environment at every reset. This way, the environment is fixed
                rather than procedurally generated;
             use_pixels (bool, False): if True, MiniGrid's default 7x7x3
                observations is converted to an image of resolution 56x56x3.

        """"""
    self._not_pybullet = True
    self._first = True
    env = gym.make(name)
    obs_high = 10.0
    if use_pixels:
        env = RGBImgPartialObsWrapper(env)
        obs_high = 255.0
    env = ImgObsWrapper(env)
    self.env = env
    self._fixed_seed = fixed_seed
    self._img_size = env.observation_space.shape[0:2]
    self._history_length = history_length
    if horizon is None:
        horizon = self.env.max_steps
    action_space = Discrete(self.env.action_space.n)
    observation_space = Box(low=0.0, high=obs_high, shape=(history_length, self._img_size[1], self._img_size[0]))
    self.env.max_steps = horizon + 1
    mdp_info = MDPInfo(observation_space, action_space, gamma, horizon)
    Environment.__init__(self, mdp_info)
    self._state = None","self._not_pybullet = True
self._first = True",self._not_pybullet = self._first = True,1,,,,,,,,,,
gradslam,https://github.com/gradslam/gradslam/tree/master/tests/datasets/test_datautils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gradslam/tests/datasets/test_datautils.py,TestDataUtils,"def single_ptquat2transform(pq):
    (tx, ty, tz) = pq[:3]
    (X, Y, Z, W) = pq[3:7]
    sqw = W * W
    sqx = X * X
    sqy = Y * Y
    sqz = Z * Z
    invs = 1 / (sqx + sqy + sqz + sqw)
    m00 = (sqx - sqy - sqz + sqw) * invs
    m11 = (-sqx + sqy - sqz + sqw) * invs
    m22 = (-sqx - sqy + sqz + sqw) * invs
    tmp1 = X * Y
    tmp2 = Z * W
    m10 = 2.0 * (tmp1 + tmp2) * invs
    m01 = 2.0 * (tmp1 - tmp2) * invs
    tmp1 = X * Z
    tmp2 = Y * W
    m20 = 2.0 * (tmp1 - tmp2) * invs
    m02 = 2.0 * (tmp1 + tmp2) * invs
    tmp1 = Y * Z
    tmp2 = X * W
    m21 = 2.0 * (tmp1 + tmp2) * invs
    m12 = 2.0 * (tmp1 - tmp2) * invs
    m03 = tx
    m13 = ty
    m23 = tz
    m30 = 0
    m31 = 0
    m32 = 0
    m33 = 1
    transform = np.array(((m00, m01, m02, m03), (m10, m11, m12, m13), (m20, m21, m22, m23), (m30, m31, m32, m33)))
    if torch.is_tensor(pq):
        return torch.tensor(transform).float()
    else:
        return transform","m30 = 0
m31 = 0
m32 = 0",m30 = m31 = m32 = 0,1,,,,,,,,,,
TradingGym,https://github.com/Yvictor/TradingGym/tree/master/trading_env/envs/backtest_v0.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TradingGym/trading_env/envs/backtest_v0.py,trading_env,"def reset(self):
    self.gameover = None
    self.df_sample = self.df
    self.step_st = 0
    self.price = self.df_sample[self.price_name].as_matrix()
    self.obs_features = self.df_sample[self.using_feature].as_matrix()
    self.obs_res = self.obs_features[self.step_st:self.step_st + self.obs_len]
    self.posi_l = [0] * self.obs_len
    self.reward_sum = 0
    self.reward_fluctuant = 0
    self.reward_ret = 0
    self.transaction_details = pd.DataFrame()
    self.reward_curve = []
    self.t_index = 0
    (self.buy_color, self.sell_color) = (1, 2)
    (self.new_rotation, self.cover_rotation) = (1, 2)
    return self.obs_res","self.step_st = 0
self.reward_sum = 0
self.reward_fluctuant = 0
self.reward_ret = 0
self.t_index = 0",self.step_st = self.reward_sum = self.reward_fluctuant = self.reward_ret = self.t_index = 0,1,,,,,,,,,,
KiKit,https://github.com/yaqwsx/KiKit/tree/master/kikit/present.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/KiKit/kikit/present.py,Template,"def __init__(self, directory):
    self.directory = directory
    with open(os.path.join(directory, 'template.json')) as jsonFile:
        self.parameters = json.load(jsonFile)
    self.extraResources = []
    self.boards = []
    self.name = None
    self.repository = None","self.name = None
self.repository = None",self.name = self.repository = None,1,,,,,,,,,,
FastFCN,https://github.com/wuhuikai/FastFCN/tree/master/encoding/nn/syncbn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FastFCN/encoding/nn/syncbn.py,SyncBatchNorm,"def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True):
    super(SyncBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)
    self._sync_master = SyncMaster(self._data_parallel_master)
    self._parallel_id = None
    self._slave_pipe = None","self._parallel_id = None
self._slave_pipe = None",self._parallel_id = self._slave_pipe = None,1,,,,,,,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_compare_comply_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_compare_comply_v1.py,TestListFeedback,"def test_list_feedback_all_params(self):
    """"""
        list_feedback()
        """"""
    url = self.preprocess_url(_base_url + '/v1/feedback')
    mock_response = '{""feedback"": [{""feedback_id"": ""feedback_id"", ""created"": ""2019-01-01T12:00:00.000Z"", ""comment"": ""comment"", ""feedback_data"": {""feedback_type"": ""feedback_type"", ""document"": {""title"": ""title"", ""hash"": ""hash""}, ""model_id"": ""model_id"", ""model_version"": ""model_version"", ""location"": {""begin"": 5, ""end"": 3}, ""text"": ""text"", ""original_labels"": {""types"": [{""label"": {""nature"": ""nature"", ""party"": ""party""}, ""provenance_ids"": [""provenance_ids""], ""modification"": ""added""}], ""categories"": [{""label"": ""Amendments"", ""provenance_ids"": [""provenance_ids""], ""modification"": ""added""}]}, ""updated_labels"": {""types"": [{""label"": {""nature"": ""nature"", ""party"": ""party""}, ""provenance_ids"": [""provenance_ids""], ""modification"": ""added""}], ""categories"": [{""label"": ""Amendments"", ""provenance_ids"": [""provenance_ids""], ""modification"": ""added""}]}, ""pagination"": {""refresh_cursor"": ""refresh_cursor"", ""next_cursor"": ""next_cursor"", ""refresh_url"": ""refresh_url"", ""next_url"": ""next_url"", ""total"": 5}}}]}'
    responses.add(responses.GET, url, body=mock_response, content_type='application/json', status=200)
    feedback_type = 'testString'
    document_title = 'testString'
    model_id = 'testString'
    model_version = 'testString'
    category_removed = 'testString'
    category_added = 'testString'
    category_not_changed = 'testString'
    type_removed = 'testString'
    type_added = 'testString'
    type_not_changed = 'testString'
    page_limit = 100
    cursor = 'testString'
    sort = 'testString'
    include_total = True
    response = _service.list_feedback(feedback_type=feedback_type, document_title=document_title, model_id=model_id, model_version=model_version, category_removed=category_removed, category_added=category_added, category_not_changed=category_not_changed, type_removed=type_removed, type_added=type_added, type_not_changed=type_not_changed, page_limit=page_limit, cursor=cursor, sort=sort, include_total=include_total, headers={})
    assert len(responses.calls) == 1
    assert response.status_code == 200
    query_string = responses.calls[0].request.url.split('?', 1)[1]
    query_string = urllib.parse.unquote_plus(query_string)
    assert 'feedback_type={}'.format(feedback_type) in query_string
    assert 'document_title={}'.format(document_title) in query_string
    assert 'model_id={}'.format(model_id) in query_string
    assert 'model_version={}'.format(model_version) in query_string
    assert 'category_removed={}'.format(category_removed) in query_string
    assert 'category_added={}'.format(category_added) in query_string
    assert 'category_not_changed={}'.format(category_not_changed) in query_string
    assert 'type_removed={}'.format(type_removed) in query_string
    assert 'type_added={}'.format(type_added) in query_string
    assert 'type_not_changed={}'.format(type_not_changed) in query_string
    assert 'page_limit={}'.format(page_limit) in query_string
    assert 'cursor={}'.format(cursor) in query_string
    assert 'sort={}'.format(sort) in query_string
    assert 'include_total={}'.format('true' if include_total else 'false') in query_string","feedback_type = 'testString'
document_title = 'testString'
model_id = 'testString'
model_version = 'testString'
category_removed = 'testString'
category_added = 'testString'
category_not_changed = 'testString'
type_removed = 'testString'
type_added = 'testString'
type_not_changed = 'testString'
cursor = 'testString'
sort = 'testString'",feedback_type = document_title = model_id = model_version = category_removed = category_added = category_not_changed = type_removed = type_added = feedback_type0 = feedback_type1 = feedback_type2 = 'testString',0,,,,,,,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_argsort_op.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_argsort_op.py,TestArgsortOpCPU,"def setUp(self):
    self.init_axis()
    self.init_datatype()
    self.init_direction()
    self.init_inputshape()
    self.setup_program()
    self.feed_data_field = {'x', 'label'}
    self.grad_data_field = {'x'}
    self.py_argsort = PyArgsort(self.input_shape, self.axis, self.descending, self.dtype)
    with fluid.program_guard(self.main_program, self.startup_program):
        x = fluid.layers.data(name='x', shape=self.input_shape, dtype=self.dtype)
        x.stop_gradient = False
        label = fluid.layers.data(name='label', shape=self.input_shape, dtype=self.dtype)
        self.index = paddle.argsort(x=x, axis=self.axis, descending=self.descending)
        self.sorted_x = paddle.sort(x=x, axis=self.axis, descending=self.descending)
        self.sorted_x.stop_gradient = False
        loss = paddle.multiply(self.sorted_x, label)
        self.loss = paddle.sum(loss)","x.stop_gradient = False
self.sorted_x.stop_gradient = False",x.stop_gradient = self.sorted_x.stop_gradient = False,1,,,,,,,,,,
MONAI,https://github.com/Project-MONAI/MONAI/tree/master/tests/test_bilateral_approx_cpu.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MONAI/tests/test_bilateral_approx_cpu.py,BilateralFilterTestCaseCpuApprox,"def test_cpu_approx_backwards(self, test_case_description, sigmas, input, expected):
    device = torch.device('cpu')
    fast_approx = True
    input_tensor = torch.from_numpy(np.array(input)).to(dtype=torch.float, device=device)
    input_tensor.requires_grad = True
    args = (input_tensor, *sigmas, fast_approx)
    gradcheck(BilateralFilter.apply, args, raise_exception=False)","fast_approx = True
input_tensor.requires_grad = True",fast_approx = input_tensor.requires_grad = True,1,,,,,,,,,,
deep_gcns,https://github.com/lightaime/deep_gcns/tree/master/sem_seg/indoor3d_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deep_gcns/sem_seg/indoor3d_util.py,,"def bbox_label_to_obj_room(input_filename, out_filename_prefix, easy_view=False, permute=None, center=False, exclude_table=False):
    """""" Visualization of bounding boxes.

  Args:
    input_filename: each line is x1 y1 z1 x2 y2 z2 label
    out_filename_prefix: OBJ filename prefix,
      visualize object by g_label2color
    easy_view: if True, only visualize furniture and floor
    permute: if not None, permute XYZ for rendering, e.g. [0 2 1]
    center: if True, move obj to have zero origin
  Returns:
    output a list of OBJ file and MTL files with the same prefix
  """"""
    bbox_label = np.loadtxt(input_filename)
    bbox = bbox_label[:, 0:6]
    if permute is not None:
        assert len(permute) == 3
        permute = np.array(permute)
        bbox[:, 0:3] = bbox[:, permute]
        bbox[:, 3:6] = bbox[:, permute + 3]
    if center:
        xyz_max = np.amax(bbox[:, 3:6], 0)
        bbox[:, 0:3] -= xyz_max / 2.0
        bbox[:, 3:6] -= xyz_max / 2.0
        bbox /= np.max(xyz_max / 2.0)
    label = bbox_label[:, -1].astype(int)
    obj_filename = out_filename_prefix + '.obj'
    mtl_filename = out_filename_prefix + '.mtl'
    fout_obj = open(obj_filename, 'w')
    fout_mtl = open(mtl_filename, 'w')
    fout_obj.write('mtllib %s\n' % os.path.basename(mtl_filename))
    v_cnt = 0
    ins_cnt = 0
    for i in range(bbox.shape[0]):
        if easy_view and label[i] not in g_easy_view_labels:
            continue
        if exclude_table and label[i] == g_classes.index('table'):
            continue
        length = bbox[i, 3:6] - bbox[i, 0:3]
        a = length[0]
        b = length[1]
        c = length[2]
        x = bbox[i, 0]
        y = bbox[i, 1]
        z = bbox[i, 2]
        color = np.array(g_label2color[label[i]], dtype=float) / 255.0
        material = 'material%d' % ins_cnt
        fout_obj.write('usemtl %s\n' % material)
        fout_obj.write('v %f %f %f\n' % (x, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z + c))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z + c))
        fout_obj.write('v %f %f %f\n' % (x, y, z))
        fout_obj.write('v %f %f %f\n' % (x, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y + b, z))
        fout_obj.write('v %f %f %f\n' % (x + a, y, z))
        fout_obj.write('g default\n')
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 3 + v_cnt, 2 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (1 + v_cnt, 2 + v_cnt, 6 + v_cnt, 5 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (7 + v_cnt, 6 + v_cnt, 2 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (4 + v_cnt, 8 + v_cnt, 7 + v_cnt, 3 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 8 + v_cnt, 4 + v_cnt, 1 + v_cnt))
        fout_obj.write('f %d %d %d %d\n' % (5 + v_cnt, 6 + v_cnt, 7 + v_cnt, 8 + v_cnt))
        fout_obj.write('\n')
        fout_mtl.write('newmtl %s\n' % material)
        fout_mtl.write('Kd %f %f %f\n' % (color[0], color[1], color[2]))
        fout_mtl.write('\n')
        v_cnt += 8
        ins_cnt += 1
    fout_obj.close()
    fout_mtl.close()","v_cnt = 0
ins_cnt = 0",v_cnt = ins_cnt = 0,1,,,,,,,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_discovery_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_discovery_v1.py,TestFederatedQuery,"def test_federated_query_required_params(self):
    """"""
        test_federated_query_required_params()
        """"""
    url = self.preprocess_url(_base_url + '/v1/environments/testString/query')
    mock_response = '{""matching_results"": 16, ""results"": [{""id"": ""id"", ""metadata"": {""mapKey"": ""anyValue""}, ""collection_id"": ""collection_id"", ""result_metadata"": {""score"": 5, ""confidence"": 10}}], ""aggregations"": [{""type"": ""histogram"", ""matching_results"": 16, ""field"": ""field"", ""interval"": 8}], ""passages"": [{""document_id"": ""document_id"", ""passage_score"": 13, ""passage_text"": ""passage_text"", ""start_offset"": 12, ""end_offset"": 10, ""field"": ""field""}], ""duplicates_removed"": 18, ""session_token"": ""session_token"", ""retrieval_details"": {""document_retrieval_strategy"": ""untrained""}, ""suggested_query"": ""suggested_query""}'
    responses.add(responses.POST, url, body=mock_response, content_type='application/json', status=200)
    environment_id = 'testString'
    collection_ids = 'testString'
    filter = 'testString'
    query = 'testString'
    natural_language_query = 'testString'
    passages = True
    aggregation = 'testString'
    count = 38
    return_ = 'testString'
    offset = 38
    sort = 'testString'
    highlight = False
    passages_fields = 'testString'
    passages_count = 100
    passages_characters = 50
    deduplicate = False
    deduplicate_field = 'testString'
    similar = False
    similar_document_ids = 'testString'
    similar_fields = 'testString'
    bias = 'testString'
    response = _service.federated_query(environment_id, collection_ids, filter=filter, query=query, natural_language_query=natural_language_query, passages=passages, aggregation=aggregation, count=count, return_=return_, offset=offset, sort=sort, highlight=highlight, passages_fields=passages_fields, passages_count=passages_count, passages_characters=passages_characters, deduplicate=deduplicate, deduplicate_field=deduplicate_field, similar=similar, similar_document_ids=similar_document_ids, similar_fields=similar_fields, bias=bias, headers={})
    assert len(responses.calls) == 1
    assert response.status_code == 200
    req_body = json.loads(str(responses.calls[0].request.body, 'utf-8'))
    assert req_body['collection_ids'] == 'testString'
    assert req_body['filter'] == 'testString'
    assert req_body['query'] == 'testString'
    assert req_body['natural_language_query'] == 'testString'
    assert req_body['passages'] == True
    assert req_body['aggregation'] == 'testString'
    assert req_body['count'] == 38
    assert req_body['return'] == 'testString'
    assert req_body['offset'] == 38
    assert req_body['sort'] == 'testString'
    assert req_body['highlight'] == False
    assert req_body['passages.fields'] == 'testString'
    assert req_body['passages.count'] == 100
    assert req_body['passages.characters'] == 50
    assert req_body['deduplicate'] == False
    assert req_body['deduplicate.field'] == 'testString'
    assert req_body['similar'] == False
    assert req_body['similar.document_ids'] == 'testString'
    assert req_body['similar.fields'] == 'testString'
    assert req_body['bias'] == 'testString'","environment_id = 'testString'
collection_ids = 'testString'
filter = 'testString'
query = 'testString'
natural_language_query = 'testString'
aggregation = 'testString'
return_ = 'testString'
sort = 'testString'
passages_fields = 'testString'
deduplicate_field = 'testString'
similar_document_ids = 'testString'
similar_fields = 'testString'
bias = 'testString'","You can use chained assignment to assign the same value to multiple variables in a single line. Here's the refactored code:

environment_id = collection_ids = filter = query = natural_language_query = aggregation = return_ = sort = passages_fields = environment_id0 = environment_id1 = environment_id2 = environment_id3 = 'testString'",0,,,,,,,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_discovery_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_discovery_v1.py,TestFederatedQuery,"def test_federated_query_required_params(self):
    """"""
        test_federated_query_required_params()
        """"""
    url = self.preprocess_url(_base_url + '/v1/environments/testString/query')
    mock_response = '{""matching_results"": 16, ""results"": [{""id"": ""id"", ""metadata"": {""mapKey"": ""anyValue""}, ""collection_id"": ""collection_id"", ""result_metadata"": {""score"": 5, ""confidence"": 10}}], ""aggregations"": [{""type"": ""histogram"", ""matching_results"": 16, ""field"": ""field"", ""interval"": 8}], ""passages"": [{""document_id"": ""document_id"", ""passage_score"": 13, ""passage_text"": ""passage_text"", ""start_offset"": 12, ""end_offset"": 10, ""field"": ""field""}], ""duplicates_removed"": 18, ""session_token"": ""session_token"", ""retrieval_details"": {""document_retrieval_strategy"": ""untrained""}, ""suggested_query"": ""suggested_query""}'
    responses.add(responses.POST, url, body=mock_response, content_type='application/json', status=200)
    environment_id = 'testString'
    collection_ids = 'testString'
    filter = 'testString'
    query = 'testString'
    natural_language_query = 'testString'
    passages = True
    aggregation = 'testString'
    count = 38
    return_ = 'testString'
    offset = 38
    sort = 'testString'
    highlight = False
    passages_fields = 'testString'
    passages_count = 100
    passages_characters = 50
    deduplicate = False
    deduplicate_field = 'testString'
    similar = False
    similar_document_ids = 'testString'
    similar_fields = 'testString'
    bias = 'testString'
    response = _service.federated_query(environment_id, collection_ids, filter=filter, query=query, natural_language_query=natural_language_query, passages=passages, aggregation=aggregation, count=count, return_=return_, offset=offset, sort=sort, highlight=highlight, passages_fields=passages_fields, passages_count=passages_count, passages_characters=passages_characters, deduplicate=deduplicate, deduplicate_field=deduplicate_field, similar=similar, similar_document_ids=similar_document_ids, similar_fields=similar_fields, bias=bias, headers={})
    assert len(responses.calls) == 1
    assert response.status_code == 200
    req_body = json.loads(str(responses.calls[0].request.body, 'utf-8'))
    assert req_body['collection_ids'] == 'testString'
    assert req_body['filter'] == 'testString'
    assert req_body['query'] == 'testString'
    assert req_body['natural_language_query'] == 'testString'
    assert req_body['passages'] == True
    assert req_body['aggregation'] == 'testString'
    assert req_body['count'] == 38
    assert req_body['return'] == 'testString'
    assert req_body['offset'] == 38
    assert req_body['sort'] == 'testString'
    assert req_body['highlight'] == False
    assert req_body['passages.fields'] == 'testString'
    assert req_body['passages.count'] == 100
    assert req_body['passages.characters'] == 50
    assert req_body['deduplicate'] == False
    assert req_body['deduplicate.field'] == 'testString'
    assert req_body['similar'] == False
    assert req_body['similar.document_ids'] == 'testString'
    assert req_body['similar.fields'] == 'testString'
    assert req_body['bias'] == 'testString'","count = 38
offset = 38",count = offset = 38,1,,,,,,,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_discovery_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_discovery_v1.py,TestFederatedQuery,"def test_federated_query_required_params(self):
    """"""
        test_federated_query_required_params()
        """"""
    url = self.preprocess_url(_base_url + '/v1/environments/testString/query')
    mock_response = '{""matching_results"": 16, ""results"": [{""id"": ""id"", ""metadata"": {""mapKey"": ""anyValue""}, ""collection_id"": ""collection_id"", ""result_metadata"": {""score"": 5, ""confidence"": 10}}], ""aggregations"": [{""type"": ""histogram"", ""matching_results"": 16, ""field"": ""field"", ""interval"": 8}], ""passages"": [{""document_id"": ""document_id"", ""passage_score"": 13, ""passage_text"": ""passage_text"", ""start_offset"": 12, ""end_offset"": 10, ""field"": ""field""}], ""duplicates_removed"": 18, ""session_token"": ""session_token"", ""retrieval_details"": {""document_retrieval_strategy"": ""untrained""}, ""suggested_query"": ""suggested_query""}'
    responses.add(responses.POST, url, body=mock_response, content_type='application/json', status=200)
    environment_id = 'testString'
    collection_ids = 'testString'
    filter = 'testString'
    query = 'testString'
    natural_language_query = 'testString'
    passages = True
    aggregation = 'testString'
    count = 38
    return_ = 'testString'
    offset = 38
    sort = 'testString'
    highlight = False
    passages_fields = 'testString'
    passages_count = 100
    passages_characters = 50
    deduplicate = False
    deduplicate_field = 'testString'
    similar = False
    similar_document_ids = 'testString'
    similar_fields = 'testString'
    bias = 'testString'
    response = _service.federated_query(environment_id, collection_ids, filter=filter, query=query, natural_language_query=natural_language_query, passages=passages, aggregation=aggregation, count=count, return_=return_, offset=offset, sort=sort, highlight=highlight, passages_fields=passages_fields, passages_count=passages_count, passages_characters=passages_characters, deduplicate=deduplicate, deduplicate_field=deduplicate_field, similar=similar, similar_document_ids=similar_document_ids, similar_fields=similar_fields, bias=bias, headers={})
    assert len(responses.calls) == 1
    assert response.status_code == 200
    req_body = json.loads(str(responses.calls[0].request.body, 'utf-8'))
    assert req_body['collection_ids'] == 'testString'
    assert req_body['filter'] == 'testString'
    assert req_body['query'] == 'testString'
    assert req_body['natural_language_query'] == 'testString'
    assert req_body['passages'] == True
    assert req_body['aggregation'] == 'testString'
    assert req_body['count'] == 38
    assert req_body['return'] == 'testString'
    assert req_body['offset'] == 38
    assert req_body['sort'] == 'testString'
    assert req_body['highlight'] == False
    assert req_body['passages.fields'] == 'testString'
    assert req_body['passages.count'] == 100
    assert req_body['passages.characters'] == 50
    assert req_body['deduplicate'] == False
    assert req_body['deduplicate.field'] == 'testString'
    assert req_body['similar'] == False
    assert req_body['similar.document_ids'] == 'testString'
    assert req_body['similar.fields'] == 'testString'
    assert req_body['bias'] == 'testString'","highlight = False
deduplicate = False
similar = False",highlight = deduplicate = similar = False,1,,,,,,,,,,
pygmsh,https://github.com/nschloe/pygmsh/tree/master/tests/occ/test_logo.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pygmsh/tests/occ/test_logo.py,,"def test():
    with pygmsh.occ.Geometry() as geom:
        print(geom.characteristic_length_min)
        print(geom.characteristic_length_max)
        geom.characteristic_length_min = 2.0
        geom.characteristic_length_max = 2.0
        rect1 = geom.add_rectangle([10.0, 0.0, 0.0], 20.0, 40.0, corner_radius=5.0)
        rect2 = geom.add_rectangle([0.0, 10.0, 0.0], 40.0, 20.0, corner_radius=5.0)
        disk1 = geom.add_disk([14.5, 35.0, 0.0], 1.85)
        disk2 = geom.add_disk([25.5, 5.0, 0.0], 1.85)
        rect3 = geom.add_rectangle([10.0, 30.0, 0.0], 10.0, 1.0)
        rect4 = geom.add_rectangle([20.0, 9.0, 0.0], 10.0, 1.0)
        r1 = geom.add_rectangle([9.0, 0.0, 0.0], 21.0, 20.5, corner_radius=8.0)
        r2 = geom.add_rectangle([10.0, 0.0, 0.0], 20.0, 19.5, corner_radius=7.0)
        diff1 = geom.boolean_difference(r1, r2)
        r22 = geom.add_rectangle([9.0, 10.0, 0.0], 11.0, 11.0)
        inter1 = geom.boolean_intersection([diff1, r22])
        r3 = geom.add_rectangle([10.0, 19.5, 0.0], 21.0, 21.0, corner_radius=8.0)
        r4 = geom.add_rectangle([10.0, 20.5, 0.0], 20.0, 20.0, corner_radius=7.0)
        diff2 = geom.boolean_difference(r3, r4)
        r33 = geom.add_rectangle([20.0, 19.0, 0.0], 11.0, 11.0)
        inter2 = geom.boolean_intersection([diff2, r33])
        geom.boolean_difference(geom.boolean_union([rect1, rect2]), geom.boolean_union([disk1, disk2, rect3, rect4, inter1, inter2]))
        mesh = geom.generate_mesh()
    ref = 1082.4470502181903
    assert abs(compute_volume(mesh) - ref) < 0.01 * ref
    return mesh","geom.characteristic_length_min = 2.0
geom.characteristic_length_max = 2.0",geom.characteristic_length_min = geom.characteristic_length_max = 2.0,1,,,,,,,,,,
azure-cli,https://github.com/Azure/azure-cli/tree/master/scripts/release/homebrew/docker/formula_generate.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/azure-cli/scripts/release/homebrew/docker/formula_generate.py,,"def last_bottle_hash():
    """"""Fetch the bottle do ... end from the latest brew formula""""""
    resp = requests.get(HOMEBREW_FORMULAR_LATEST)
    resp.raise_for_status()
    lines = resp.text.split('\n')
    look_for_end = False
    start = 0
    end = 0
    for (idx, content) in enumerate(lines):
        if look_for_end:
            if 'end' in content:
                end = idx
                break
        elif 'bottle do' in content:
            start = idx
            look_for_end = True
    return '\n'.join(lines[start:end + 1])","look_for_end = False
start = 0
end = 0",look_for_end = start = end = False,1,,,,,,,,,,
fame,https://github.com/certsocietegenerale/fame/tree/master/fame/core/user.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fame/fame/core/user.py,User,"def __init__(self, values):
    self['permissions'] = []
    self['api_key'] = User.generate_api_key()
    MongoDict.__init__(self, values)
    self.is_authenticated = True
    self.is_active = True
    self.is_anonymous = False
    self.is_api = False
    self.files = FilteredCollection(store.files, self.filters())
    self.analyses = FilteredCollection(store.analysis, self.filters())","self.is_authenticated = True
self.is_active = True",self.is_authenticated = self.is_active = True,1,,,,,,,,,,
fame,https://github.com/certsocietegenerale/fame/tree/master/fame/core/user.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fame/fame/core/user.py,User,"def __init__(self, values):
    self['permissions'] = []
    self['api_key'] = User.generate_api_key()
    MongoDict.__init__(self, values)
    self.is_authenticated = True
    self.is_active = True
    self.is_anonymous = False
    self.is_api = False
    self.files = FilteredCollection(store.files, self.filters())
    self.analyses = FilteredCollection(store.analysis, self.filters())","self.is_anonymous = False
self.is_api = False",self.is_anonymous = self.is_api = False,1,,,,,,,,,,
tvm,https://github.com/apache/tvm/tree/master/python/tvm/autotvm/tuner/sa_model_optimizer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/autotvm/tuner/sa_model_optimizer.py,SimulatedAnnealingOptimizer,"def find_maximums(self, model, num, exclusive):
    tic = time.time()
    (temp, n_iter, early_stop, log_interval) = (self.temp, self.n_iter, self.early_stop, self.log_interval)
    if self.persistent and self.points is not None:
        points = self.points
    else:
        points = self.task.config_space.sample_ints(self.parallel_size)
    scores = model.predict(points)
    heap_items = [(float('-inf'), -1 - i) for i in range(num)]
    heapq.heapify(heap_items)
    in_heap = set(exclusive)
    in_heap.update([x[1] for x in heap_items])
    for (s, p) in zip(scores, points):
        if s > heap_items[0][0] and p not in in_heap:
            pop = heapq.heapreplace(heap_items, (s, p))
            in_heap.remove(pop[1])
            in_heap.add(p)
    k = 0
    k_last_modify = 0
    if isinstance(temp, (tuple, list, np.ndarray)):
        t = temp[0]
        cool = 1.0 * (temp[0] - temp[1]) / (n_iter + 1)
    else:
        t = temp
        cool = 0
    while k < n_iter and k < k_last_modify + early_stop:
        new_points = np.empty_like(points)
        for (i, p) in enumerate(points):
            new_points[i] = self.task.config_space.random_walk(p)
        new_scores = model.predict(new_points)
        ac_prob = np.exp(np.minimum((new_scores - scores) / (t + 1e-05), 1))
        ac_index = np.random.random(len(ac_prob)) < ac_prob
        points[ac_index] = new_points[ac_index]
        scores[ac_index] = new_scores[ac_index]
        for (s, p) in zip(new_scores, new_points):
            if s > heap_items[0][0] and p not in in_heap:
                pop = heapq.heapreplace(heap_items, (s, p))
                in_heap.remove(pop[1])
                in_heap.add(p)
                k_last_modify = k
        k += 1
        t -= cool
        if log_interval and k % log_interval == 0:
            t_str = '%.2f' % t
            logger.debug('SA iter: %d\tlast_update: %d\tmax-0: %.2f\tmax-1: %.2f\ttemp: %s\telapsed: %.2f', k, k_last_modify, heap_items[0][0], np.max([v for (v, _) in heap_items]), t_str, time.time() - tic)
    heap_items.sort(key=lambda item: -item[0])
    heap_items = [x for x in heap_items if x[0] >= 0]
    logger.debug('SA iter: %d\tlast_update: %d\telapsed: %.2f', k, k_last_modify, time.time() - tic)
    logger.debug('SA Maximums: %s', heap_items)
    if self.persistent:
        self.points = points
    return [x[1] for x in heap_items]","k = 0
k_last_modify = 0",k = k_last_modify = 0,1,,,,,,,,,,
Fabrik,https://github.com/Cloud-CV/Fabrik/tree/master/tests/unit/keras_app/test_views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Fabrik/tests/unit/keras_app/test_views.py,DenseExportTest,"def test_keras_export(self):
    tests = open(os.path.join(settings.BASE_DIR, 'tests', 'unit', 'keras_app', 'keras_export_test.json'), 'r')
    response = json.load(tests)
    tests.close()
    net = yaml.safe_load(json.dumps(response['net']))
    net = {'l0': net['Input2'], 'l1': net['InnerProduct']}
    net['l0']['connection']['output'].append('l1')
    inp = data(net['l0'], '', 'l0')['l0']
    temp = dense(net['l1'], [inp], 'l1')
    model = Model(inp, temp['l1'])
    self.assertEqual(model.layers[2].__class__.__name__, 'Dense')
    net['l1']['params']['weight_filler'] = 'glorot_normal'
    net['l1']['params']['bias_filler'] = 'glorot_normal'
    inp = data(net['l0'], '', 'l0')['l0']
    temp = dense(net['l1'], [inp], 'l1')
    model = Model(inp, temp['l1'])
    self.assertEqual(model.layers[2].__class__.__name__, 'Dense')","net['l1']['params']['weight_filler'] = 'glorot_normal'
net['l1']['params']['bias_filler'] = 'glorot_normal'",net['l1']['params']['weight_filler'] = net['l1']['params']['bias_filler'] = 'glorot_normal',1,,,,,,,,,,
galaxy,https://github.com/ansible/galaxy/tree/master/test/unit/app/jobs/test_command_factory.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/test/unit/app/jobs/test_command_factory.py,MockJobWrapper,"def __init__(self, job_dir):
    self.strict_shell = False
    self.command_line = MOCK_COMMAND_LINE
    self.dependency_shell_commands = []
    self.metadata_line = None
    self.configured_external_metadata_kwds = None
    self.working_directory = job_dir
    self.prepare_input_files_cmds = None
    self.commands_in_new_shell = False
    self.app = Bunch(config=Bunch(check_job_script_integrity=False))
    self.shell = '/bin/sh'
    self.use_metadata_binary = False","self.strict_shell = False
self.commands_in_new_shell = False
self.use_metadata_binary = False",self.strict_shell = self.commands_in_new_shell = self.use_metadata_binary = False,1,,,,,,,,,,
galaxy,https://github.com/ansible/galaxy/tree/master/test/unit/app/jobs/test_command_factory.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/test/unit/app/jobs/test_command_factory.py,MockJobWrapper,"def __init__(self, job_dir):
    self.strict_shell = False
    self.command_line = MOCK_COMMAND_LINE
    self.dependency_shell_commands = []
    self.metadata_line = None
    self.configured_external_metadata_kwds = None
    self.working_directory = job_dir
    self.prepare_input_files_cmds = None
    self.commands_in_new_shell = False
    self.app = Bunch(config=Bunch(check_job_script_integrity=False))
    self.shell = '/bin/sh'
    self.use_metadata_binary = False","self.metadata_line = None
self.configured_external_metadata_kwds = None
self.prepare_input_files_cmds = None",self.metadata_line = self.configured_external_metadata_kwds = self.prepare_input_files_cmds = None,1,,,,,,,,,,
openfold,https://github.com/aqlaboratory/openfold/tree/master/openfold/np/relax/amber_minimize.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openfold/openfold/np/relax/amber_minimize.py,,"def _run_one_iteration(*, pdb_string: str, max_iterations: int, tolerance: float, stiffness: float, restraint_set: str, max_attempts: int, exclude_residues: Optional[Collection[int]]=None, use_gpu: bool):
    """"""Runs the minimization pipeline.

    Args:
      pdb_string: A pdb string.
      max_iterations: An `int` specifying the maximum number of L-BFGS iterations.
      A value of 0 specifies no limit.
      tolerance: kcal/mol, the energy tolerance of L-BFGS.
      stiffness: kcal/mol A**2, spring constant of heavy atom restraining
        potential.
      restraint_set: The set of atoms to restrain.
      max_attempts: The maximum number of minimization attempts.
      exclude_residues: An optional list of zero-indexed residues to exclude from
          restraints.
      use_gpu: Whether to run relaxation on GPU
    Returns:
      A `dict` of minimization info.
    """"""
    exclude_residues = exclude_residues or []
    tolerance = tolerance * ENERGY
    stiffness = stiffness * ENERGY / LENGTH ** 2
    start = time.perf_counter()
    minimized = False
    attempts = 0
    while not minimized and attempts < max_attempts:
        attempts += 1
        try:
            logging.info('Minimizing protein, attempt %d of %d.', attempts, max_attempts)
            ret = _openmm_minimize(pdb_string, max_iterations=max_iterations, tolerance=tolerance, stiffness=stiffness, restraint_set=restraint_set, exclude_residues=exclude_residues, use_gpu=use_gpu)
            minimized = True
        except Exception as e:
            print(e)
            logging.info(e)
    if not minimized:
        raise ValueError(f'Minimization failed after {max_attempts} attempts.')
    ret['opt_time'] = time.perf_counter() - start
    ret['min_attempts'] = attempts
    return ret","minimized = False
attempts = 0",minimized = attempts = False,0,,,,,,,,,,
fiftyone,https://github.com/voxel51/fiftyone/tree/master/fiftyone/core/clips.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fiftyone/fiftyone/core/clips.py,_Bounds,"def __init__(self):
    self.min = None
    self.max = None","self.min = None
self.max = None",self.min = self.max = None,1,,,,,,,,,,
tvm,https://github.com/apache/tvm/tree/master/tests/python/unittest/test_tir_transform_loop_partition.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/tests/python/unittest/test_tir_transform_loop_partition.py,,"def test_cce_loop_1():
    ib = tvm.tir.ir_builder.create()
    dtype = 'float16'
    n = 514
    m = 514
    _A = te.placeholder((n * m,), name='A')
    Ab = tvm.tir.decl_buffer((n * m,), dtype, name='A')
    A = ib.buffer_ptr(Ab)
    _B = te.placeholder((n * m,), name='B')
    Bb = tvm.tir.decl_buffer((n * m,), dtype, name='B')
    B = ib.buffer_ptr(Bb)
    with ib.for_range(0, 11, name='i') as i:
        with ib.for_range(0, 160, name='j') as j:
            with ib.if_scope(ib.likely(i * 160 + j < 1600)):
                A[(i + 1) * m + j + 1] = B[i * m + j + 1] + B[(i + 1) * m + j + 1] + B[(i + 2) * m + j + 1]
    stmt = ib.get()
    mod = tvm.IRModule.from_expr(tvm.tir.PrimFunc([Ab, Bb], stmt))
    with tvm.transform.PassContext(config={'tir.LoopPartition': {'partition_const_loop': True}}):
        mod = tvm.tir.transform.LoopPartition()(mod)
        stmt = tvm.tir.transform.Simplify()(mod)['main'].body
    assert not any(collect_visit(stmt, lambda x: isinstance(x, tvm.tir.IfThenElse)))","n = 514
m = 514",n = m = 514,1,,,,,,,,,,
SPADE,https://github.com/NVlabs/SPADE/tree/master/models/networks/loss.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SPADE/models/networks/loss.py,GANLoss,"def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0, tensor=torch.FloatTensor, opt=None):
    super(GANLoss, self).__init__()
    self.real_label = target_real_label
    self.fake_label = target_fake_label
    self.real_label_tensor = None
    self.fake_label_tensor = None
    self.zero_tensor = None
    self.Tensor = tensor
    self.gan_mode = gan_mode
    self.opt = opt
    if gan_mode == 'ls':
        pass
    elif gan_mode == 'original':
        pass
    elif gan_mode == 'w':
        pass
    elif gan_mode == 'hinge':
        pass
    else:
        raise ValueError('Unexpected gan_mode {}'.format(gan_mode))","self.real_label_tensor = None
self.fake_label_tensor = None
self.zero_tensor = None",self.real_label_tensor = self.fake_label_tensor = self.zero_tensor = None,1,,,,,,,,,,
gistandard,https://github.com/RobbieHan/gistandard/tree/master/extra_apps/xadmin/views/list.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gistandard/extra_apps/xadmin/views/list.py,ResultHeader,"def __init__(self, field_name, row):
    super(ResultHeader, self).__init__(field_name, row)
    self.tag = 'th'
    self.tag_attrs = ['scope=""col""']
    self.sortable = False
    self.allow_tags = True
    self.sorted = False
    self.ascending = None
    self.sort_priority = None
    self.url_primary = None
    self.url_remove = None
    self.url_toggle = None","self.sortable = False
self.sorted = False",self.sortable = self.sorted = False,1,,,,,,,,,,
gistandard,https://github.com/RobbieHan/gistandard/tree/master/extra_apps/xadmin/views/list.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gistandard/extra_apps/xadmin/views/list.py,ResultHeader,"def __init__(self, field_name, row):
    super(ResultHeader, self).__init__(field_name, row)
    self.tag = 'th'
    self.tag_attrs = ['scope=""col""']
    self.sortable = False
    self.allow_tags = True
    self.sorted = False
    self.ascending = None
    self.sort_priority = None
    self.url_primary = None
    self.url_remove = None
    self.url_toggle = None","self.ascending = None
self.sort_priority = None
self.url_primary = None
self.url_remove = None
self.url_toggle = None",self.ascending = self.sort_priority = self.url_primary = self.url_remove = self.url_toggle = None,1,,,,,,,,,,
hivemind,https://github.com/learning-at-home/hivemind/tree/master/hivemind/optim/experimental/grad_averager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/hivemind/hivemind/optim/experimental/grad_averager.py,GradientAverager,"def reset_accumulated_grads_(self):
    """"""reset averager-internal gradient accumulators and the denominator""""""
    self._accumulators_used_in_step = False
    self.local_samples_accumulated = self.local_times_accumulated = 0
    self._anchor_batch_size = None
    for grad_buf in self._grad_accumulators():
        grad_buf.zero_()","self._accumulators_used_in_step = False
self.local_samples_accumulated = self.local_times_accumulated = 0","self._accumulators_used_in_step, self.local_samples_accumulated, self.local_times_accumulated = False, 0, 0",1,,,,,,,,,,
chia-rosechain,https://github.com/snight1983/chia-rosechain/tree/master/chia/full_node/weight_proof.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chia-rosechain/chia/full_node/weight_proof.py,,"def __get_rc_sub_slot(constants: ConsensusConstants, segment: SubEpochChallengeSegment, summaries: List[SubEpochSummary], curr_ssi: uint64) -> RewardChainSubSlot:
    ses = summaries[uint32(segment.sub_epoch_n - 1)]
    first_idx = None
    first = None
    for (idx, curr) in enumerate(segment.sub_slots):
        if curr.cc_slot_end is None:
            first_idx = idx
            first = curr
            break
    assert first_idx
    idx = first_idx
    slots = segment.sub_slots
    slots_n = 1
    assert first
    assert first.signage_point_index is not None
    if is_overflow_block(constants, first.signage_point_index):
        if idx >= 2 and slots[idx - 2].cc_slot_end is None:
            slots_n = 2
    new_diff = None if ses is None else ses.new_difficulty
    new_ssi = None if ses is None else ses.new_sub_slot_iters
    ses_hash = None if ses is None else ses.get_hash()
    overflow = is_overflow_block(constants, first.signage_point_index)
    if overflow:
        if idx >= 2 and slots[idx - 2].cc_slot_end is not None and (slots[idx - 1].cc_slot_end is not None):
            ses_hash = None
            new_ssi = None
            new_diff = None
    sub_slot = slots[idx]
    while True:
        if sub_slot.cc_slot_end:
            slots_n -= 1
            if slots_n == 0:
                break
        idx -= 1
        sub_slot = slots[idx]
    icc_sub_slot_hash: Optional[bytes32] = None
    assert sub_slot is not None
    assert sub_slot.cc_slot_end_info is not None
    assert segment.rc_slot_end_info is not None
    if idx != 0:
        cc_vdf_info = VDFInfo(sub_slot.cc_slot_end_info.challenge, curr_ssi, sub_slot.cc_slot_end_info.output)
        if sub_slot.icc_slot_end_info is not None:
            icc_slot_end_info = VDFInfo(sub_slot.icc_slot_end_info.challenge, curr_ssi, sub_slot.icc_slot_end_info.output)
            icc_sub_slot_hash = icc_slot_end_info.get_hash()
    else:
        cc_vdf_info = sub_slot.cc_slot_end_info
        if sub_slot.icc_slot_end_info is not None:
            icc_sub_slot_hash = sub_slot.icc_slot_end_info.get_hash()
    cc_sub_slot = ChallengeChainSubSlot(cc_vdf_info, icc_sub_slot_hash, ses_hash, new_ssi, new_diff)
    rc_sub_slot = RewardChainSubSlot(segment.rc_slot_end_info, cc_sub_slot.get_hash(), icc_sub_slot_hash, constants.MIN_BLOCKS_PER_CHALLENGE_BLOCK)
    return rc_sub_slot","first_idx = None
first = None",first_idx = first = None,1,,,,,,,,,,
chia-rosechain,https://github.com/snight1983/chia-rosechain/tree/master/chia/full_node/weight_proof.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chia-rosechain/chia/full_node/weight_proof.py,,"def __get_rc_sub_slot(constants: ConsensusConstants, segment: SubEpochChallengeSegment, summaries: List[SubEpochSummary], curr_ssi: uint64) -> RewardChainSubSlot:
    ses = summaries[uint32(segment.sub_epoch_n - 1)]
    first_idx = None
    first = None
    for (idx, curr) in enumerate(segment.sub_slots):
        if curr.cc_slot_end is None:
            first_idx = idx
            first = curr
            break
    assert first_idx
    idx = first_idx
    slots = segment.sub_slots
    slots_n = 1
    assert first
    assert first.signage_point_index is not None
    if is_overflow_block(constants, first.signage_point_index):
        if idx >= 2 and slots[idx - 2].cc_slot_end is None:
            slots_n = 2
    new_diff = None if ses is None else ses.new_difficulty
    new_ssi = None if ses is None else ses.new_sub_slot_iters
    ses_hash = None if ses is None else ses.get_hash()
    overflow = is_overflow_block(constants, first.signage_point_index)
    if overflow:
        if idx >= 2 and slots[idx - 2].cc_slot_end is not None and (slots[idx - 1].cc_slot_end is not None):
            ses_hash = None
            new_ssi = None
            new_diff = None
    sub_slot = slots[idx]
    while True:
        if sub_slot.cc_slot_end:
            slots_n -= 1
            if slots_n == 0:
                break
        idx -= 1
        sub_slot = slots[idx]
    icc_sub_slot_hash: Optional[bytes32] = None
    assert sub_slot is not None
    assert sub_slot.cc_slot_end_info is not None
    assert segment.rc_slot_end_info is not None
    if idx != 0:
        cc_vdf_info = VDFInfo(sub_slot.cc_slot_end_info.challenge, curr_ssi, sub_slot.cc_slot_end_info.output)
        if sub_slot.icc_slot_end_info is not None:
            icc_slot_end_info = VDFInfo(sub_slot.icc_slot_end_info.challenge, curr_ssi, sub_slot.icc_slot_end_info.output)
            icc_sub_slot_hash = icc_slot_end_info.get_hash()
    else:
        cc_vdf_info = sub_slot.cc_slot_end_info
        if sub_slot.icc_slot_end_info is not None:
            icc_sub_slot_hash = sub_slot.icc_slot_end_info.get_hash()
    cc_sub_slot = ChallengeChainSubSlot(cc_vdf_info, icc_sub_slot_hash, ses_hash, new_ssi, new_diff)
    rc_sub_slot = RewardChainSubSlot(segment.rc_slot_end_info, cc_sub_slot.get_hash(), icc_sub_slot_hash, constants.MIN_BLOCKS_PER_CHALLENGE_BLOCK)
    return rc_sub_slot","ses_hash = None
new_ssi = None
new_diff = None",ses_hash = new_ssi = new_diff = None,1,,,,,,,,,,
djongo,https://github.com/nesdis/djongo/tree/master/tests/django_tests/tests/v21/tests/datatypes/tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/djongo/tests/django_tests/tests/v21/tests/datatypes/tests.py,DataTypesTestCase,"def test_boolean_type(self):
    d = Donut(name='Apple Fritter')
    self.assertFalse(d.is_frosted)
    self.assertIsNone(d.has_sprinkles)
    self.assertIsNone(d.has_sprinkles_old)
    d.has_sprinkles = True
    d.has_sprinkles_old = True
    self.assertTrue(d.has_sprinkles)
    self.assertTrue(d.has_sprinkles_old)
    d.save()
    d2 = Donut.objects.get(name='Apple Fritter')
    self.assertFalse(d2.is_frosted)
    self.assertTrue(d2.has_sprinkles)
    self.assertTrue(d2.has_sprinkles_old)","d.has_sprinkles = True
d.has_sprinkles_old = True",d.has_sprinkles = d.has_sprinkles_old = True,1,,,,,,,,,,
gyroflow,https://github.com/ElvinC/gyroflow/tree/master//stabilizer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/gyroflow//stabilizer.py,OpticalStabilizer,"def __init__(self, videopath, calibrationfile):
    self.cap = cv2.VideoCapture(videopath)
    self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    self.fps = self.cap.get(cv2.CAP_PROP_FPS)
    self.num_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
    self.undistort = StandardCalibrator()
    self.undistort.load_calibration_json(calibrationfile, True)
    (self.map1, self.map2) = self.undistort.get_maps(1.6, new_img_dim=(self.width, self.height))
    self.times = None
    self.stab_transform = None","self.times = None
self.stab_transform = None",self.times = self.stab_transform = None,1,,,,,,,,,,
core,https://github.com/home-assistant/core/tree/master/tests/components/blebox/test_switch.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/tests/components/blebox/test_switch.py,,"def initial_update_any():
    feature_mocks[0].is_on = True
    feature_mocks[1].is_on = True","feature_mocks[0].is_on = True
feature_mocks[1].is_on = True",feature_mocks[0].is_on = feature_mocks[1].is_on = True,1,,,,,,,,,,
dask,https://github.com/dask/dask/tree/master/dask/array/svg.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dask/dask/array/svg.py,,"def svg_nd(chunks, size=200):
    if len(chunks) % 3 == 1:
        chunks = ((1,),) + chunks
    shape = tuple(map(sum, chunks))
    sizes = draw_sizes(shape, size=size)
    chunks2 = chunks
    sizes2 = sizes
    out = []
    left = 0
    total_height = 0
    while chunks2:
        n = len(chunks2) % 3 or 3
        o = svg(chunks2[:n], sizes=sizes2[:n], offset=(left, 0))
        chunks2 = chunks2[n:]
        sizes2 = sizes2[n:]
        lines = o.split('\n')
        header = lines[0]
        height = float(re.search('height=""(\\d*\\.?\\d*)""', header).groups()[0])
        total_height = max(total_height, height)
        width = float(re.search('width=""(\\d*\\.?\\d*)""', header).groups()[0])
        left += width + 10
        o = '\n'.join(lines[1:-1])
        out.append(o)
    header = '<svg width=""%d"" height=""%d"" style=""stroke:rgb(0,0,0);stroke-width:1"" >\n' % (left, total_height)
    footer = '\n</svg>'
    return header + '\n\n'.join(out) + footer","left = 0
total_height = 0",left = total_height = 0,1,,,,,,,,,,
galaxy,https://github.com/ansible/galaxy/tree/master/lib/galaxy/webapps/reports/controllers/system.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/webapps/reports/controllers/system.py,System,"def deleted_histories(self, trans, **kwd):
    """"""
        The number of histories that were deleted more than the specified number of days ago, but have not yet been purged.
        Also included is the number of datasets associated with the histories.
        """"""
    params = util.Params(kwd)
    message = ''
    if params.deleted_histories_days:
        deleted_histories_days = int(params.deleted_histories_days)
        cutoff_time = datetime.utcnow() - timedelta(days=deleted_histories_days)
        history_count = 0
        dataset_count = 0
        disk_space = 0
        histories = trans.sa_session.query(model.History).filter(and_(model.History.table.c.deleted == true(), model.History.table.c.purged == false(), model.History.update_time < cutoff_time)).options(eagerload('datasets'))
        for history in histories:
            for hda in history.datasets:
                if not hda.dataset.purged:
                    dataset_count += 1
                    try:
                        disk_space += hda.dataset.file_size
                    except Exception:
                        pass
            history_count += 1
        message = '%d histories ( including a total of %d datasets ) were deleted more than %d days ago, but have not yet been purged, disk space: %s.' % (history_count, dataset_count, deleted_histories_days, nice_size(disk_space, True))
    else:
        message = 'Enter the number of days.'
    return (str(deleted_histories_days), message)","history_count = 0
dataset_count = 0
disk_space = 0",history_count = dataset_count = disk_space = 0,1,,,,,,,,,,
DeepSpeed,https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/moe/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepSpeed/deepspeed/moe/utils.py,,"def has_moe_layers(m):
    has_moe = False
    num_experts = 0
    for (_, module) in m.named_modules():
        if isinstance(module, MoE):
            has_moe = True
            num_experts = module.num_experts
            break
    return (has_moe, num_experts)","has_moe = False
num_experts = 0",has_moe = num_experts = False,1,,,,,,,,,,
AutoDL-Projects,https://github.com/D-X-Y/AutoDL-Projects/tree/master/xautodl/procedures/metric_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoDL-Projects/xautodl/procedures/metric_utils.py,AverageMeter,"def reset(self):
    self.val = 0.0
    self.avg = 0.0
    self.sum = 0.0
    self.count = 0.0","self.val = 0.0
self.avg = 0.0
self.sum = 0.0
self.count = 0.0",self.val = self.avg = self.sum = self.count = 0.0,1,,,,,,,,,,
mypaint,https://github.com/mypaint/mypaint/tree/master/gui/colors/hcywheel.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mypaint/gui/colors/hcywheel.py,HCYMaskEditorWheel,"def __button_release_cb(self, widget, event):
    if self.__drag_func is None:
        if self.__last_cursor is self.__add_cursor:
            self.__add_void(event.x, event.y)
    else:
        self.__drag_func = None
        self.__drag_start_pos = None
        self.__cleanup_mask()
    self.__update_active_objects(event.x, event.y)","self.__drag_func = None
self.__drag_start_pos = None",self.__drag_func = self.__drag_start_pos = None,1,,,,,,,,,,
PythonRobotics,https://github.com/AtsushiSakai/PythonRobotics/tree/master/PathPlanning/AStar/a_star_variants.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PythonRobotics/PathPlanning/AStar/a_star_variants.py,,"def main():
    obs_dict = {}
    for i in range(51):
        for j in range(51):
            obs_dict[i, j] = False
    (o_x, o_y) = ([], [])
    s_x = 5.0
    s_y = 5.0
    g_x = 35.0
    g_y = 45.0
    draw_vertical_line(0, 0, 50, o_x, o_y, obs_dict)
    draw_vertical_line(48, 0, 50, o_x, o_y, obs_dict)
    draw_horizontal_line(0, 0, 50, o_x, o_y, obs_dict)
    draw_horizontal_line(0, 48, 50, o_x, o_y, obs_dict)
    all_x = [10, 10, 10, 15, 20, 20, 30, 30, 35, 30, 40, 45]
    all_y = [10, 30, 45, 20, 5, 40, 10, 40, 5, 40, 10, 25]
    all_len = [10, 10, 5, 10, 10, 5, 20, 10, 25, 10, 35, 15]
    for (x, y, l) in zip(all_x, all_y, all_len):
        draw_vertical_line(x, y, l, o_x, o_y, obs_dict)
    (all_x[:], all_y[:], all_len[:]) = ([], [], [])
    all_x = [35, 40, 15, 10, 45, 20, 10, 15, 25, 45, 10, 30, 10, 40]
    all_y = [5, 10, 15, 20, 20, 25, 30, 35, 35, 35, 40, 40, 45, 45]
    all_len = [10, 5, 10, 10, 5, 5, 10, 5, 10, 5, 10, 5, 5, 5]
    for (x, y, l) in zip(all_x, all_y, all_len):
        draw_horizontal_line(x, y, l, o_x, o_y, obs_dict)
    if show_animation:
        plt.plot(o_x, o_y, '.k')
        plt.plot(s_x, s_y, 'og')
        plt.plot(g_x, g_y, 'xb')
        plt.grid(True)
    if use_jump_point:
        keypoint_list = key_points(obs_dict)
        search_obj = SearchAlgo(obs_dict, g_x, g_y, s_x, s_y, 101, 101, keypoint_list)
        search_obj.jump_point()
    else:
        search_obj = SearchAlgo(obs_dict, g_x, g_y, s_x, s_y, 101, 101)
        search_obj.a_star()","s_x = 5.0
s_y = 5.0",s_x = s_y = 5.0,1,,,,,,,,,,
texar,https://github.com/asyml/texar/tree/master/examples/vae_text/vae_train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/texar/examples/vae_text/vae_train.py,,"def _run_epoch(sess, epoch, mode_string, display=10):
    if mode_string == 'train':
        iterator.switch_to_train_data(sess)
    elif mode_string == 'valid':
        iterator.switch_to_val_data(sess)
    elif mode_string == 'test':
        iterator.switch_to_test_data(sess)
    step = 0
    start_time = time.time()
    num_words = num_sents = 0
    nll_ = 0.0
    kl_loss_ = rc_loss_ = 0.0
    while True:
        try:
            fetches = {'nll': nll, 'kl_loss': kl_loss, 'rc_loss': rc_loss, 'lengths': seq_lengths}
            if mode_string == 'train':
                fetches['train_op'] = train_op
                opt_vars['kl_weight'] = min(1.0, opt_vars['kl_weight'] + anneal_r)
                kl_weight_ = opt_vars['kl_weight']
            else:
                kl_weight_ = 1.0
            mode = tf.estimator.ModeKeys.TRAIN if mode_string == 'train' else tf.estimator.ModeKeys.EVAL
            feed = {tx.global_mode(): mode, kl_weight: kl_weight_, learning_rate: opt_vars['learning_rate']}
            fetches_ = sess.run(fetches, feed_dict=feed)
            batch_size_ = len(fetches_['lengths'])
            num_sents += batch_size_
            num_words += sum(fetches_['lengths'])
            nll_ += fetches_['nll'] * batch_size_
            kl_loss_ += fetches_['kl_loss'] * batch_size_
            rc_loss_ += fetches_['rc_loss'] * batch_size_
            if step % display == 0 and mode_string == 'train':
                print('%s: epoch %d, step %d, nll %.4f, klw: %.4f, KL %.4f,  rc %.4f, log_ppl %.4f, ppl %.4f, time elapsed: %.1fs' % (mode_string, epoch, step, nll_ / num_sents, opt_vars['kl_weight'], kl_loss_ / num_sents, rc_loss_ / num_sents, nll_ / num_words, np.exp(nll_ / num_words), time.time() - start_time))
                sys.stdout.flush()
            step += 1
        except tf.errors.OutOfRangeError:
            print('\n%s: epoch %d, nll %.4f, KL %.4f, rc %.4f, log_ppl %.4f, ppl %.4f\n' % (mode_string, epoch, nll_ / num_sents, kl_loss_ / num_sents, rc_loss_ / num_sents, nll_ / num_words, np.exp(nll_ / num_words)))
            break
    return (nll_ / num_sents, np.exp(nll_ / num_words))","step = 0
num_words = num_sents = 0
nll_ = 0.0
kl_loss_ = rc_loss_ = 0.0","step, num_words, num_sents, nll_, kl_loss_, rc_loss_ = 0, 0, 0, 0.0, 0.0, 0.0",1,,,,,,,,,,
cement,https://github.com/datafolklabs/cement/tree/master/cement/cli/contrib/yaml/emitter.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cement/cement/cli/contrib/yaml/emitter.py,Emitter,"def write_plain(self, text, split=True):
    if self.root_context:
        self.open_ended = True
    if not text:
        return
    if not self.whitespace:
        data = ' '
        self.column += len(data)
        if self.encoding:
            data = data.encode(self.encoding)
        self.stream.write(data)
    self.whitespace = False
    self.indention = False
    spaces = False
    breaks = False
    start = end = 0
    while end <= len(text):
        ch = None
        if end < len(text):
            ch = text[end]
        if spaces:
            if ch != ' ':
                if start + 1 == end and self.column > self.best_width and split:
                    self.write_indent()
                    self.whitespace = False
                    self.indention = False
                else:
                    data = text[start:end]
                    self.column += len(data)
                    if self.encoding:
                        data = data.encode(self.encoding)
                    self.stream.write(data)
                start = end
        elif breaks:
            if ch not in '\n\x85\u2028\u2029':
                if text[start] == '\n':
                    self.write_line_break()
                for br in text[start:end]:
                    if br == '\n':
                        self.write_line_break()
                    else:
                        self.write_line_break(br)
                self.write_indent()
                self.whitespace = False
                self.indention = False
                start = end
        elif ch is None or ch in ' \n\x85\u2028\u2029':
            data = text[start:end]
            self.column += len(data)
            if self.encoding:
                data = data.encode(self.encoding)
            self.stream.write(data)
            start = end
        if ch is not None:
            spaces = ch == ' '
            breaks = ch in '\n\x85\u2028\u2029'
        end += 1","self.whitespace = False
self.indention = False
spaces = False
breaks = False
start = end = 0","self.whitespace = self.indention = spaces = breaks = False
start = end = 0",1,,,,,,,,,,
cement,https://github.com/datafolklabs/cement/tree/master/cement/cli/contrib/yaml/emitter.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cement/cement/cli/contrib/yaml/emitter.py,Emitter,"def write_plain(self, text, split=True):
    if self.root_context:
        self.open_ended = True
    if not text:
        return
    if not self.whitespace:
        data = ' '
        self.column += len(data)
        if self.encoding:
            data = data.encode(self.encoding)
        self.stream.write(data)
    self.whitespace = False
    self.indention = False
    spaces = False
    breaks = False
    start = end = 0
    while end <= len(text):
        ch = None
        if end < len(text):
            ch = text[end]
        if spaces:
            if ch != ' ':
                if start + 1 == end and self.column > self.best_width and split:
                    self.write_indent()
                    self.whitespace = False
                    self.indention = False
                else:
                    data = text[start:end]
                    self.column += len(data)
                    if self.encoding:
                        data = data.encode(self.encoding)
                    self.stream.write(data)
                start = end
        elif breaks:
            if ch not in '\n\x85\u2028\u2029':
                if text[start] == '\n':
                    self.write_line_break()
                for br in text[start:end]:
                    if br == '\n':
                        self.write_line_break()
                    else:
                        self.write_line_break(br)
                self.write_indent()
                self.whitespace = False
                self.indention = False
                start = end
        elif ch is None or ch in ' \n\x85\u2028\u2029':
            data = text[start:end]
            self.column += len(data)
            if self.encoding:
                data = data.encode(self.encoding)
            self.stream.write(data)
            start = end
        if ch is not None:
            spaces = ch == ' '
            breaks = ch in '\n\x85\u2028\u2029'
        end += 1","self.whitespace = False
self.indention = False",self.whitespace = self.indention = False,1,,,,,,,,,,
cement,https://github.com/datafolklabs/cement/tree/master/cement/cli/contrib/yaml/emitter.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cement/cement/cli/contrib/yaml/emitter.py,Emitter,"def write_plain(self, text, split=True):
    if self.root_context:
        self.open_ended = True
    if not text:
        return
    if not self.whitespace:
        data = ' '
        self.column += len(data)
        if self.encoding:
            data = data.encode(self.encoding)
        self.stream.write(data)
    self.whitespace = False
    self.indention = False
    spaces = False
    breaks = False
    start = end = 0
    while end <= len(text):
        ch = None
        if end < len(text):
            ch = text[end]
        if spaces:
            if ch != ' ':
                if start + 1 == end and self.column > self.best_width and split:
                    self.write_indent()
                    self.whitespace = False
                    self.indention = False
                else:
                    data = text[start:end]
                    self.column += len(data)
                    if self.encoding:
                        data = data.encode(self.encoding)
                    self.stream.write(data)
                start = end
        elif breaks:
            if ch not in '\n\x85\u2028\u2029':
                if text[start] == '\n':
                    self.write_line_break()
                for br in text[start:end]:
                    if br == '\n':
                        self.write_line_break()
                    else:
                        self.write_line_break(br)
                self.write_indent()
                self.whitespace = False
                self.indention = False
                start = end
        elif ch is None or ch in ' \n\x85\u2028\u2029':
            data = text[start:end]
            self.column += len(data)
            if self.encoding:
                data = data.encode(self.encoding)
            self.stream.write(data)
            start = end
        if ch is not None:
            spaces = ch == ' '
            breaks = ch in '\n\x85\u2028\u2029'
        end += 1","self.whitespace = False
self.indention = False",self.whitespace = self.indention = False,1,,,,,,,,,,
allennlp,https://github.com/allenai/allennlp/tree/master/allennlp/fairness/bias_metrics.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/allennlp/allennlp/fairness/bias_metrics.py,NaturalLanguageInference,"def __init__(self, neutral_label: int=2, taus: List[float]=[0.5, 0.7]):
    self.neutral_label = neutral_label
    self.taus = taus
    self._nli_probs_sum = 0.0
    self._num_neutral_predictions = 0.0
    self._num_neutral_above_taus = {tau: 0.0 for tau in taus}
    self._total_predictions = 0","self._nli_probs_sum = 0.0
self._num_neutral_predictions = 0.0
self._total_predictions = 0","self._nli_probs_sum = self._num_neutral_predictions = 0.0
self._total_predictions = 0",1,,,,,,,,,,
luigi,https://github.com/spotify/luigi/tree/master/luigi/contrib/scalding.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/luigi/luigi/contrib/scalding.py,ScaldingJobRunner,"def get_job_class(self, source):
    job_name = os.path.splitext(os.path.basename(source))[0]
    package = None
    job_class = None
    for line in open(source).readlines():
        p = re.search('package\\s+([^\\s\\(]+)', line)
        if p:
            package = p.groups()[0]
        p = re.search('class\\s+([^\\s\\(]+).*extends\\s+.*Job', line)
        if p:
            job_class = p.groups()[0]
            if job_class == job_name:
                break
    if job_class:
        if package:
            job_class = package + '.' + job_class
        logger.debug('Found scalding job class: %s', job_class)
        return job_class
    else:
        raise luigi.contrib.hadoop.HadoopJobError('Coudl not find scalding job class.')","package = None
job_class = None",package = job_class = None,1,,,,,,,,,,
coa_tools,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/operators/exporter/export_creature.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/operators/exporter/export_creature.py,CreatureExport,"def __init__(self):
    self.json_data = self.setup_json_data()
    self.texture_export_scale = 1.0
    self.armature_export_scale = 1.0
    self.sprite_object = None
    self.armature_orig = None
    self.armature = None
    self.sprite_data = []
    self.reduce_size = False
    self.scene = None
    self.init_bone_positions = {}
    self.export_progress_total = 0
    self.export_progress_current = 0
    self.root_bone_name = '__Creature RootBone__'
    self.bone_weights = {}
    self.bone_scaled = {}
    self.mesh_deformed = {}
    self.remapped_indices = {}","self.texture_export_scale = 1.0
self.armature_export_scale = 1.0",self.texture_export_scale = self.armature_export_scale = 1.0,1,,,,,,,,,,
coa_tools,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/operators/exporter/export_creature.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/operators/exporter/export_creature.py,CreatureExport,"def __init__(self):
    self.json_data = self.setup_json_data()
    self.texture_export_scale = 1.0
    self.armature_export_scale = 1.0
    self.sprite_object = None
    self.armature_orig = None
    self.armature = None
    self.sprite_data = []
    self.reduce_size = False
    self.scene = None
    self.init_bone_positions = {}
    self.export_progress_total = 0
    self.export_progress_current = 0
    self.root_bone_name = '__Creature RootBone__'
    self.bone_weights = {}
    self.bone_scaled = {}
    self.mesh_deformed = {}
    self.remapped_indices = {}","self.sprite_object = None
self.armature_orig = None
self.armature = None
self.scene = None",self.sprite_object = self.armature_orig = self.armature = self.scene = None,1,,,,,,,,,,
coa_tools,https://github.com/ndee85/coa_tools/tree/master/Blender/coa_tools/operators/exporter/export_creature.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coa_tools/Blender/coa_tools/operators/exporter/export_creature.py,CreatureExport,"def __init__(self):
    self.json_data = self.setup_json_data()
    self.texture_export_scale = 1.0
    self.armature_export_scale = 1.0
    self.sprite_object = None
    self.armature_orig = None
    self.armature = None
    self.sprite_data = []
    self.reduce_size = False
    self.scene = None
    self.init_bone_positions = {}
    self.export_progress_total = 0
    self.export_progress_current = 0
    self.root_bone_name = '__Creature RootBone__'
    self.bone_weights = {}
    self.bone_scaled = {}
    self.mesh_deformed = {}
    self.remapped_indices = {}","self.reduce_size = False
self.export_progress_total = 0
self.export_progress_current = 0",self.reduce_size = self.export_progress_total = self.export_progress_current = False,1,,,,,,,,,,
espresso,https://github.com/freewym/espresso/tree/master/fairseq/models/speech_to_text/modules/emformer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/fairseq/models/speech_to_text/modules/emformer.py,NoSegAugmentedMemoryMultiheadAttentionBmm,"def __init__(self, input_dim, num_heads, dropout=0.0, std_scale=None, scaled_init=False, tanh_on_mem=False, use_mem=True, mini_batches=False, negative_inf='-inf', layer_index=-1, max_relative_position=0, rpe_old_option=True):
    if input_dim % num_heads:
        raise ValueError('input_dim ({}) must be divisible by num_heads ({})'.format(input_dim, num_heads))
    super().__init__()
    embed_dim = input_dim
    self.e2h_kv = torch.nn.Linear(input_dim, 2 * input_dim, bias=True)
    self.e2h_q = torch.nn.Linear(input_dim, input_dim, bias=True)
    self.rpe_old_option = rpe_old_option
    if max_relative_position > 0:
        self.use_rpe = True
        self.rpe_k = RelativePositionEmbedding(head_dim=input_dim // num_heads, max_position=max_relative_position)
        self.rpe_v = RelativePositionEmbedding(head_dim=input_dim // num_heads, max_position=max_relative_position)
    else:
        self.use_rpe = False
        self.rpe_k = None
        self.rpe_v = None
    if scaled_init:
        if layer_index == -1:
            gain = 1.0 / math.sqrt(2)
        else:
            gain = 1.0 / math.sqrt(layer_index + 1)
        torch.nn.init.xavier_uniform_(self.e2h_kv.weight, gain=gain)
        torch.nn.init.xavier_uniform_(self.e2h_q.weight, gain=gain)
    self.out_proj = torch.nn.Linear(embed_dim, embed_dim, bias=True)
    self.embed_dim = embed_dim
    self.num_heads = num_heads
    self.dropout = dropout
    self.head_dim = embed_dim // num_heads
    self.scaling = self.head_dim ** (-0.5)
    self.std_scale = std_scale
    self.use_mem = use_mem
    self.mini_batches = mini_batches
    self.negative_inf = negative_inf
    if tanh_on_mem:
        self.squash_mem = torch.tanh
        self.nonlinear_squash_mem = True
    else:
        self.squash_mem = NoOp()
        self.nonlinear_squash_mem = False","self.rpe_k = None
self.rpe_v = None",self.rpe_k = self.rpe_v = None,1,,,,,,,,,,
homework,https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3/dqn_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/homework/hw3/dqn_utils.py,ReplayBuffer,"def __init__(self, size, frame_history_len, lander=False):
    """"""This is a memory efficient implementation of the replay buffer.

        The sepecific memory optimizations use here are:
            - only store each frame once rather than k times
              even if every observation normally consists of k last frames
            - store frames as np.uint8 (actually it is most time-performance
              to cast them back to float32 on GPU to minimize memory transfer
              time)
            - store frame_t and frame_(t+1) in the same buffer.

        For the tipical use case in Atari Deep RL buffer with 1M frames the total
        memory footprint of this buffer is 10^6 * 84 * 84 bytes ~= 7 gigabytes

        Warning! Assumes that returning frame of zeros at the beginning
        of the episode, when there is less frames than `frame_history_len`,
        is acceptable.

        Parameters
        ----------
        size: int
            Max number of transitions to store in the buffer. When the buffer
            overflows the old memories are dropped.
        frame_history_len: int
            Number of memories to be retried for each observation.
        """"""
    self.lander = lander
    self.size = size
    self.frame_history_len = frame_history_len
    self.next_idx = 0
    self.num_in_buffer = 0
    self.obs = None
    self.action = None
    self.reward = None
    self.done = None","self.next_idx = 0
self.num_in_buffer = 0",self.next_idx = self.num_in_buffer = 0,1,,,,,,,,,,
homework,https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3/dqn_utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/homework/hw3/dqn_utils.py,ReplayBuffer,"def __init__(self, size, frame_history_len, lander=False):
    """"""This is a memory efficient implementation of the replay buffer.

        The sepecific memory optimizations use here are:
            - only store each frame once rather than k times
              even if every observation normally consists of k last frames
            - store frames as np.uint8 (actually it is most time-performance
              to cast them back to float32 on GPU to minimize memory transfer
              time)
            - store frame_t and frame_(t+1) in the same buffer.

        For the tipical use case in Atari Deep RL buffer with 1M frames the total
        memory footprint of this buffer is 10^6 * 84 * 84 bytes ~= 7 gigabytes

        Warning! Assumes that returning frame of zeros at the beginning
        of the episode, when there is less frames than `frame_history_len`,
        is acceptable.

        Parameters
        ----------
        size: int
            Max number of transitions to store in the buffer. When the buffer
            overflows the old memories are dropped.
        frame_history_len: int
            Number of memories to be retried for each observation.
        """"""
    self.lander = lander
    self.size = size
    self.frame_history_len = frame_history_len
    self.next_idx = 0
    self.num_in_buffer = 0
    self.obs = None
    self.action = None
    self.reward = None
    self.done = None","self.obs = None
self.action = None
self.reward = None
self.done = None",self.obs = self.action = self.reward = self.done = None,1,,,,,,,,,,
cats-blender-plugin,https://github.com/absolute-quantum/cats-blender-plugin/tree/master/tools/atlas.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cats-blender-plugin/tools/atlas.py,InstallShotariya,"def draw(self, context):
    layout = self.layout
    col = layout.column(align=True)
    if self.action == 'INSTALL':
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.install1'))
        row.scale_y = 0.75
        row = col.row(align=True)
        row.scale_y = 0.75
        row.label(text=t('InstallShotariya.error.install2'))
        col.separator()
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.install3'))
        row.scale_y = 0.75
        col.separator()
        row = col.row(align=True)
        row.operator(ShotariyaButton.bl_idname, icon=globs.ICON_URL)
        col.separator()
    elif self.action == 'ENABLE':
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.enable1'))
        row.scale_y = 0.75
        row = col.row(align=True)
        row.scale_y = 0.75
        row.label(text=t('InstallShotariya.error.enable2'))
        col.separator()
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.enable3'))
        row.scale_y = 0.75
        col.separator()
    elif self.action == 'VERSION':
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.version1'))
        row.scale_y = 0.75
        row = col.row(align=True)
        row.scale_y = 0.75
        row.label(text=t('InstallShotariya.error.version2'))
        col.separator()
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.version3'))
        row.scale_y = 0.75
        col.separator()
        row = col.row(align=True)
        row.operator(ShotariyaButton.bl_idname, icon=globs.ICON_URL)
        col.separator()","row.scale_y = 0.75
row.scale_y = 0.75",row.scale_y = 0.75 # both variables can be assigned in a single line,1,,,,,,,,,,
cats-blender-plugin,https://github.com/absolute-quantum/cats-blender-plugin/tree/master/tools/atlas.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cats-blender-plugin/tools/atlas.py,InstallShotariya,"def draw(self, context):
    layout = self.layout
    col = layout.column(align=True)
    if self.action == 'INSTALL':
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.install1'))
        row.scale_y = 0.75
        row = col.row(align=True)
        row.scale_y = 0.75
        row.label(text=t('InstallShotariya.error.install2'))
        col.separator()
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.install3'))
        row.scale_y = 0.75
        col.separator()
        row = col.row(align=True)
        row.operator(ShotariyaButton.bl_idname, icon=globs.ICON_URL)
        col.separator()
    elif self.action == 'ENABLE':
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.enable1'))
        row.scale_y = 0.75
        row = col.row(align=True)
        row.scale_y = 0.75
        row.label(text=t('InstallShotariya.error.enable2'))
        col.separator()
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.enable3'))
        row.scale_y = 0.75
        col.separator()
    elif self.action == 'VERSION':
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.version1'))
        row.scale_y = 0.75
        row = col.row(align=True)
        row.scale_y = 0.75
        row.label(text=t('InstallShotariya.error.version2'))
        col.separator()
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.version3'))
        row.scale_y = 0.75
        col.separator()
        row = col.row(align=True)
        row.operator(ShotariyaButton.bl_idname, icon=globs.ICON_URL)
        col.separator()","row.scale_y = 0.75
row.scale_y = 0.75",row.scale_y = 0.75 # both variables can be assigned in a single line,1,,,,,,,,,,
cats-blender-plugin,https://github.com/absolute-quantum/cats-blender-plugin/tree/master/tools/atlas.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cats-blender-plugin/tools/atlas.py,InstallShotariya,"def draw(self, context):
    layout = self.layout
    col = layout.column(align=True)
    if self.action == 'INSTALL':
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.install1'))
        row.scale_y = 0.75
        row = col.row(align=True)
        row.scale_y = 0.75
        row.label(text=t('InstallShotariya.error.install2'))
        col.separator()
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.install3'))
        row.scale_y = 0.75
        col.separator()
        row = col.row(align=True)
        row.operator(ShotariyaButton.bl_idname, icon=globs.ICON_URL)
        col.separator()
    elif self.action == 'ENABLE':
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.enable1'))
        row.scale_y = 0.75
        row = col.row(align=True)
        row.scale_y = 0.75
        row.label(text=t('InstallShotariya.error.enable2'))
        col.separator()
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.enable3'))
        row.scale_y = 0.75
        col.separator()
    elif self.action == 'VERSION':
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.version1'))
        row.scale_y = 0.75
        row = col.row(align=True)
        row.scale_y = 0.75
        row.label(text=t('InstallShotariya.error.version2'))
        col.separator()
        row = col.row(align=True)
        row.label(text=t('InstallShotariya.error.version3'))
        row.scale_y = 0.75
        col.separator()
        row = col.row(align=True)
        row.operator(ShotariyaButton.bl_idname, icon=globs.ICON_URL)
        col.separator()","row.scale_y = 0.75
row.scale_y = 0.75",row.scale_y = 0.75 # both variables assigned the same value in a single line,1,,,,,,,,,,
numpyro,https://github.com/pyro-ppl/numpyro/tree/master/examples/hsgp.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/numpyro/examples/hsgp.py,,"def make_birthdays_data_dict(data):
    x = data['id'].values
    y = data['births_relative'].values
    dates = data['date']
    xsd = jnp.array((x - x.mean()) / x.std())
    ysd = jnp.array((y - y.mean()) / y.std())
    day_of_week = jnp.array((data['day_of_week'] - 1).values)
    day_of_year = jnp.array((data['day_of_year'] - 1).values)
    floating_days = get_floating_days_indicators(dates)
    period = 365.25
    w0 = x.std() * (jnp.pi * 2 / period)
    L = 1.5 * max(xsd)
    M1 = 10
    M2 = 10
    M3 = 5
    return {'x': xsd, 'day_of_week': day_of_week, 'day_of_year': day_of_year, 'w0': w0, 'L': L, 'M1': M1, 'M2': M2, 'M3': M3, **floating_days, 'y': ysd}","M1 = 10
M2 = 10",M1 = M2 = 10,1,,,,,,,,,,
Cura,https://github.com/Ultimaker/Cura/tree/master/plugins/UM3NetworkPrinting/src/Cloud/CloudOutputDevice.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Cura/plugins/UM3NetworkPrinting/src/Cloud/CloudOutputDevice.py,CloudOutputDevice,"def _onPrintUploadSpecificError(self, reply: 'QNetworkReply', _: 'QNetworkReply.NetworkError'):
    """"""
        Displays a message when an error occurs specific to uploading print job (i.e. queue is full).
        """"""
    error_code = reply.attribute(QNetworkRequest.Attribute.HttpStatusCodeAttribute)
    if error_code == 409:
        PrintJobUploadQueueFullMessage().show()
    else:
        PrintJobUploadErrorMessage(I18N_CATALOG.i18nc('@error:send', 'Unknown error code when uploading print job: {0}', error_code)).show()
    Logger.log('w', 'Upload of print job failed specifically with error code {}'.format(error_code))
    self._progress.hide()
    self._pre_upload_print_job = None
    self._uploaded_print_job = None
    self.writeError.emit()","self._pre_upload_print_job = None
self._uploaded_print_job = None",self._pre_upload_print_job = self._uploaded_print_job = None,1,,,,,,,,,,
whipper,https://github.com/whipper-team/whipper/tree/master/whipper/test/test_common_accurip.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/whipper/whipper/test/test_common_accurip.py,TestAccurateRipReport,"def test_report_v2_only(self):
    self.result.tracks[0].AR['v1']['DBCRC'] = None
    self.result.tracks[0].AR['v1']['DBConfidence'] = None
    print_report(self.result)
    self.assertEqual(sys.stdout.getvalue(), 'track  1: rip accurate     (confidence   4 of  12) v1 [284fc705], v2 [dc77f9ab], DB [dc77f9ab]\n')","self.result.tracks[0].AR['v1']['DBCRC'] = None
self.result.tracks[0].AR['v1']['DBConfidence'] = None",self.result.tracks[0].AR['v1']['DBCRC'] = self.result.tracks[0].AR['v1']['DBConfidence'] = None,1,,,,,,,,,,
freeipa,https://github.com/freeipa/freeipa/tree/master/ipaserver/plugins/baseldap.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/freeipa/ipaserver/plugins/baseldap.py,LDAPUpdate,"def execute(self, *keys, **options):
    ldap = self.obj.backend
    if len(options) == 2:
        raise errors.EmptyModlist()
    dn = self.obj.get_dn(*keys, **options)
    entry_attrs = ldap.make_entry(dn, self.args_options_2_entry(**options))
    self.process_attr_options(entry_attrs, dn, keys, options)
    if options.get('all', False):
        attrs_list = ['*'] + self.obj.default_attributes
    else:
        attrs_list = set(self.obj.default_attributes)
        attrs_list.update(entry_attrs.keys())
        if options.get('no_members', False):
            attrs_list.difference_update(self.obj.attribute_members)
        attrs_list = list(attrs_list)
    _check_single_value_attrs(self.params, entry_attrs)
    _check_empty_attrs(self.obj.params, entry_attrs)
    for callback in self.get_callbacks('pre'):
        entry_attrs.dn = callback(self, ldap, entry_attrs.dn, entry_attrs, attrs_list, *keys, **options)
    _check_limit_object_class(self.api.Backend.ldap2.schema.attribute_types(self.obj.limit_object_classes), list(entry_attrs), allow_only=True)
    _check_limit_object_class(self.api.Backend.ldap2.schema.attribute_types(self.obj.disallow_object_classes), list(entry_attrs), allow_only=False)
    rdnupdate = False
    if 'rename' in options:
        if not options['rename']:
            raise errors.ValidationError(name='rename', error=u""can't be empty"")
        entry_attrs[self.obj.primary_key.name] = options['rename']
    if self.obj.allow_rename and self.obj.primary_key.name in entry_attrs:
        if RDN((self.obj.primary_key.name, keys[-1])) == entry_attrs.dn[0]:
            try:
                new_dn = DN((self.obj.primary_key.name, entry_attrs[self.obj.primary_key.name]), *entry_attrs.dn[1:])
                self._exc_wrapper(keys, options, ldap.move_entry)(entry_attrs.dn, new_dn)
                rdnkeys = keys[:-1] + (entry_attrs[self.obj.primary_key.name],)
                entry_attrs.dn = self.obj.get_dn(*rdnkeys)
                options['rdnupdate'] = True
                rdnupdate = True
            except errors.EmptyModlist:
                pass
            except errors.NotFound:
                raise self.obj.handle_not_found(*keys)
            finally:
                del entry_attrs[self.obj.primary_key.name]
    try:
        update = self._exc_wrapper(keys, options, ldap.get_entry)(entry_attrs.dn, list(entry_attrs))
        update.update(entry_attrs)
        self._exc_wrapper(keys, options, ldap.update_entry)(update)
    except errors.EmptyModlist as e:
        if not rdnupdate:
            raise e
    except errors.NotFound:
        raise self.obj.handle_not_found(*keys)
    try:
        entry_attrs = self._exc_wrapper(keys, options, ldap.get_entry)(entry_attrs.dn, attrs_list)
    except errors.NotFound:
        raise errors.MidairCollision(message=_('the entry was deleted while being modified'))
    self.obj.get_indirect_members(entry_attrs, attrs_list)
    if options.get('rights', False) and options.get('all', False):
        entry_attrs['attributelevelrights'] = get_effective_rights(ldap, entry_attrs.dn)
    for callback in self.get_callbacks('post'):
        entry_attrs.dn = callback(self, ldap, entry_attrs.dn, entry_attrs, *keys, **options)
    self.obj.convert_attribute_members(entry_attrs, *keys, **options)
    entry_attrs = entry_to_dict(entry_attrs, **options)
    if self.obj.primary_key:
        pkey = keys[-1]
    else:
        pkey = None
    return dict(result=entry_attrs, value=pkey_to_value(pkey, options))","options['rdnupdate'] = True
rdnupdate = True",options['rdnupdate'] = rdnupdate = True,1,,,,,,,,,,
Bert-Multi-Label-Text-Classification,https://github.com/lonePatient/Bert-Multi-Label-Text-Classification/tree/master/pybert/callback/lr_schedulers.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Bert-Multi-Label-Text-Classification/pybert/callback/lr_schedulers.py,ReduceLROnPlateau,"def _reset(self):
    """"""Resets wait counter and cooldown counter.
        """"""
    if self.mode not in ['min', 'max']:
        raise RuntimeError('Learning Rate Plateau Reducing mode %s is unknown!')
    if self.mode == 'min':
        self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)
        self.best = np.Inf
    else:
        self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)
        self.best = -np.Inf
    self.cooldown_counter = 0
    self.wait = 0","self.cooldown_counter = 0
self.wait = 0",self.cooldown_counter = self.wait = 0,1,,,,,,,,,,
networkx,https://github.com/networkx/networkx/tree/master/networkx/tests/test_all_random_functions.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/networkx/networkx/tests/test_all_random_functions.py,,"def run_all_random_functions(seed):
    n = 20
    m = 10
    k = l = 2
    s = v = 10
    p = q = p1 = p2 = p_in = p_out = 0.4
    alpha = radius = theta = 0.75
    sizes = (20, 20, 10)
    colors = [1, 2, 3]
    G = nx.barbell_graph(12, 20)
    H = nx.cycle_graph(3)
    H.add_weighted_edges_from(((u, v, 0.2) for (u, v) in H.edges))
    deg_sequence = [3, 2, 1, 3, 2, 1, 3, 2, 1, 2, 1, 2, 1]
    in_degree_sequence = w = sequence = aseq = bseq = deg_sequence
    t(nx.maximal_independent_set, G, seed=seed)
    t(nx.rich_club_coefficient, G, seed=seed, normalized=False)
    t(nx.random_reference, G, seed=seed)
    t(nx.lattice_reference, G, seed=seed)
    t(nx.sigma, G, 1, 2, seed=seed)
    t(nx.omega, G, 1, 2, seed=seed)
    t(nx.double_edge_swap, G, seed=seed)
    t(nx.connected_double_edge_swap, nx.complete_graph(9), seed=seed)
    t(nx.random_layout, G, seed=seed)
    t(nx.fruchterman_reingold_layout, G, seed=seed)
    t(nx.algebraic_connectivity, G, seed=seed)
    t(nx.fiedler_vector, G, seed=seed)
    t(nx.spectral_ordering, G, seed=seed)
    t(approx.average_clustering, G, seed=seed)
    t(approx.simulated_annealing_tsp, H, 'greedy', source=1, seed=seed)
    t(approx.threshold_accepting_tsp, H, 'greedy', source=1, seed=seed)
    t(approx.traveling_salesman_problem, H, method=lambda G, wt: approx.simulated_annealing_tsp(G, 'greedy', wt, seed=seed))
    t(approx.traveling_salesman_problem, H, method=lambda G, wt: approx.threshold_accepting_tsp(G, 'greedy', wt, seed=seed))
    t(nx.betweenness_centrality, G, seed=seed)
    t(nx.edge_betweenness_centrality, G, seed=seed)
    t(nx.edge_betweenness, G, seed=seed)
    t(nx.approximate_current_flow_betweenness_centrality, G, seed=seed)
    t(nx.algorithms.community.kernighan_lin_bisection, G, seed=seed)
    t(nx.algorithms.tree.greedy_branching, G, seed=seed)
    t(nx.algorithms.tree.Edmonds, G, seed=seed)
    t(nx.spectral_graph_forge, G, alpha, seed=seed)
    t(nx.algorithms.community.asyn_fluidc, G, k, max_iter=1, seed=seed)
    t(nx.algorithms.connectivity.edge_augmentation.greedy_k_edge_augmentation, G, k, seed=seed)
    t(nx.algorithms.coloring.strategy_random_sequential, G, colors, seed=seed)
    cs = ['d', 'i', 'i', 'd', 'd', 'i']
    t(threshold.swap_d, cs, seed=seed)
    t(nx.configuration_model, deg_sequence, seed=seed)
    t(nx.directed_configuration_model, in_degree_sequence, in_degree_sequence, seed=seed)
    t(nx.expected_degree_graph, w, seed=seed)
    t(nx.random_degree_sequence_graph, sequence, seed=seed)
    joint_degrees = {1: {4: 1}, 2: {2: 2, 3: 2, 4: 2}, 3: {2: 2, 4: 1}, 4: {1: 1, 2: 2, 3: 1}}
    t(nx.joint_degree_graph, joint_degrees, seed=seed)
    joint_degree_sequence = [(1, 0), (1, 0), (1, 0), (2, 0), (1, 0), (2, 1), (0, 1), (0, 1)]
    t(nx.random_clustered_graph, joint_degree_sequence, seed=seed)
    constructor = [(3, 3, 0.5), (10, 10, 0.7)]
    t(nx.random_shell_graph, constructor, seed=seed)
    mapping = {1: 0.4, 2: 0.3, 3: 0.3}
    t(nx.utils.random_weighted_sample, mapping, k, seed=seed)
    t(nx.utils.weighted_choice, mapping, seed=seed)
    t(nx.algorithms.bipartite.configuration_model, aseq, bseq, seed=seed)
    t(nx.algorithms.bipartite.preferential_attachment_graph, aseq, p, seed=seed)

    def kernel_integral(u, w, z):
        return z - w
    t(nx.random_kernel_graph, n, kernel_integral, seed=seed)
    sizes = [75, 75, 300]
    probs = [[0.25, 0.05, 0.02], [0.05, 0.35, 0.07], [0.02, 0.07, 0.4]]
    t(nx.stochastic_block_model, sizes, probs, seed=seed)
    t(nx.random_partition_graph, sizes, p_in, p_out, seed=seed)
    t(threshold.random_threshold_sequence, n, p, seed=seed)
    t(nx.tournament.random_tournament, n, seed=seed)
    t(nx.relaxed_caveman_graph, l, k, p, seed=seed)
    t(nx.planted_partition_graph, l, k, p_in, p_out, seed=seed)
    t(nx.gaussian_random_partition_graph, n, s, v, p_in, p_out, seed=seed)
    t(nx.gn_graph, n, seed=seed)
    t(nx.gnr_graph, n, p, seed=seed)
    t(nx.gnc_graph, n, seed=seed)
    t(nx.scale_free_graph, n, seed=seed)
    t(nx.directed.random_uniform_k_out_graph, n, k, seed=seed)
    t(nx.random_k_out_graph, n, k, alpha, seed=seed)
    N = 1000
    t(nx.partial_duplication_graph, N, n, p, q, seed=seed)
    t(nx.duplication_divergence_graph, n, p, seed=seed)
    t(nx.random_geometric_graph, n, radius, seed=seed)
    t(nx.soft_random_geometric_graph, n, radius, seed=seed)
    t(nx.geographical_threshold_graph, n, theta, seed=seed)
    t(nx.waxman_graph, n, seed=seed)
    t(nx.navigable_small_world_graph, n, seed=seed)
    t(nx.thresholded_random_geometric_graph, n, radius, theta, seed=seed)
    t(nx.uniform_random_intersection_graph, n, m, p, seed=seed)
    t(nx.k_random_intersection_graph, n, m, k, seed=seed)
    t(nx.general_random_intersection_graph, n, 2, [0.1, 0.5], seed=seed)
    t(nx.fast_gnp_random_graph, n, p, seed=seed)
    t(nx.gnp_random_graph, n, p, seed=seed)
    t(nx.dense_gnm_random_graph, n, m, seed=seed)
    t(nx.gnm_random_graph, n, m, seed=seed)
    t(nx.newman_watts_strogatz_graph, n, k, p, seed=seed)
    t(nx.watts_strogatz_graph, n, k, p, seed=seed)
    t(nx.connected_watts_strogatz_graph, n, k, p, seed=seed)
    t(nx.random_regular_graph, 3, n, seed=seed)
    t(nx.barabasi_albert_graph, n, m, seed=seed)
    t(nx.extended_barabasi_albert_graph, n, m, p, q, seed=seed)
    t(nx.powerlaw_cluster_graph, n, m, p, seed=seed)
    t(nx.random_lobster, n, p1, p2, seed=seed)
    t(nx.random_powerlaw_tree, n, seed=seed, tries=5000)
    t(nx.random_powerlaw_tree_sequence, 10, seed=seed, tries=5000)
    t(nx.random_tree, n, seed=seed)
    t(nx.utils.powerlaw_sequence, n, seed=seed)
    t(nx.utils.zipf_rv, 2.3, seed=seed)
    cdist = [0.2, 0.4, 0.5, 0.7, 0.9, 1.0]
    t(nx.utils.discrete_sequence, n, cdistribution=cdist, seed=seed)
    t(nx.algorithms.bipartite.random_graph, n, m, p, seed=seed)
    t(nx.algorithms.bipartite.gnmk_random_graph, n, m, k, seed=seed)
    LFR = nx.generators.LFR_benchmark_graph
    t(LFR, 25, 3, 1.5, 0.1, average_degree=3, min_community=10, seed=seed, max_community=20)
    t(nx.random_internet_as_graph, n, seed=seed)","m = 10
s = v = 10",m = s = v = 10,1,,,,,,,,,,
spiderfoot,https://github.com/smicallef/spiderfoot/tree/master/test/unit/modules/test_sfp_phone.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/spiderfoot/test/unit/modules/test_sfp_phone.py,TestModulePhone,"def test_handleEvent_phone_number_event_data_containing_phone_string_should_return_provider_telco_event(self):
    sf = SpiderFoot(self.default_options)
    module = sfp_phone()
    module.setup(sf, dict())
    target_value = 'spiderfoot.net'
    target_type = 'INTERNET_NAME'
    target = SpiderFootTarget(target_value, target_type)
    module.setTarget(target)

    def new_notifyListeners(self, event):
        expected = 'PROVIDER_TELCO'
        if str(event.eventType) != expected:
            raise Exception(f'{event.eventType} != {expected}')
        expected = 'Swisscom'
        if str(event.data) != expected:
            raise Exception(f'{event.data} != {expected}')
        raise Exception('OK')
    module.notifyListeners = new_notifyListeners.__get__(module, sfp_phone)
    event_type = 'ROOT'
    event_data = 'example data'
    event_module = ''
    source_event = ''
    evt = SpiderFootEvent(event_type, event_data, event_module, source_event)
    event_type = 'PHONE_NUMBER'
    event_data = '+41798765432'
    event_module = 'example module'
    source_event = evt
    evt = SpiderFootEvent(event_type, event_data, event_module, source_event)
    with self.assertRaises(Exception) as cm:
        module.handleEvent(evt)
    self.assertEqual('OK', str(cm.exception))","event_module = ''
source_event = ''",event_module = source_event = '',1,,,,,,,,,,
MultiQC,https://github.com/ewels/MultiQC/tree/master/multiqc/utils/log.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MultiQC/multiqc/utils/log.py,,"def get_log_stream(logger):
    """"""
    Returns a stream to the root log file.
    If there is no logfile return the stderr log stream

    Returns:
        A stream to the root log file or stderr stream.
    """"""
    file_stream = None
    log_stream = None
    for handler in logger.handlers:
        if isinstance(handler, logging.FileHandler):
            file_stream = handler.stream
        else:
            log_stream = handler.stream
    if file_stream:
        return file_stream
    return log_stream","file_stream = None
log_stream = None",file_stream = log_stream = None,1,,,,,,,,,,
conan-center-index,https://github.com/conan-io/conan-center-index/tree/master/recipes/rocksdb/all/conanfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan-center-index/recipes/rocksdb/all/conanfile.py,RocksDB,"def package_info(self):
    cmake_target = 'rocksdb-shared' if self.options.shared else 'rocksdb'
    self.cpp_info.set_property('cmake_file_name', 'RocksDB')
    self.cpp_info.set_property('cmake_target_name', 'RocksDB::{}'.format(cmake_target))
    self.cpp_info.components['librocksdb'].libs = tools.collect_libs(self)
    if self.settings.os == 'Windows':
        self.cpp_info.components['librocksdb'].system_libs = ['shlwapi', 'rpcrt4']
        if self.options.shared:
            self.cpp_info.components['librocksdb'].defines = ['ROCKSDB_DLL']
    elif self.settings.os in ['Linux', 'FreeBSD']:
        self.cpp_info.components['librocksdb'].system_libs = ['pthread', 'm']
    if self.options.lite:
        self.cpp_info.components['librocksdb'].defines.append('ROCKSDB_LITE')
    self.cpp_info.names['cmake_find_package'] = 'RocksDB'
    self.cpp_info.names['cmake_find_package_multi'] = 'RocksDB'
    self.cpp_info.components['librocksdb'].names['cmake_find_package'] = cmake_target
    self.cpp_info.components['librocksdb'].names['cmake_find_package_multi'] = cmake_target
    self.cpp_info.components['librocksdb'].set_property('cmake_target_name', 'RocksDB::{}'.format(cmake_target))
    if self.options.with_gflags:
        self.cpp_info.components['librocksdb'].requires.append('gflags::gflags')
    if self.options.with_snappy:
        self.cpp_info.components['librocksdb'].requires.append('snappy::snappy')
    if self.options.with_lz4:
        self.cpp_info.components['librocksdb'].requires.append('lz4::lz4')
    if self.options.with_zlib:
        self.cpp_info.components['librocksdb'].requires.append('zlib::zlib')
    if self.options.with_zstd:
        self.cpp_info.components['librocksdb'].requires.append('zstd::zstd')
    if self.options.get_safe('with_tbb'):
        self.cpp_info.components['librocksdb'].requires.append('onetbb::onetbb')
    if self.options.with_jemalloc:
        self.cpp_info.components['librocksdb'].requires.append('jemalloc::jemalloc')","self.cpp_info.names['cmake_find_package'] = 'RocksDB'
self.cpp_info.names['cmake_find_package_multi'] = 'RocksDB'",self.cpp_info.names['cmake_find_package'] = self.cpp_info.names['cmake_find_package_multi'] = 'RocksDB',1,,,,,,,,,,
dagster,https://github.com/dagster-io/dagster/tree/master/python_modules/libraries/dagstermill/dagstermill/manager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dagster/python_modules/libraries/dagstermill/dagstermill/manager.py,Manager,"def __init__(self):
    self.pipeline = None
    self.solid_def = None
    self.in_pipeline = False
    self.marshal_dir = None
    self.context = None
    self.resource_manager = None","self.pipeline = None
self.solid_def = None
self.marshal_dir = None
self.context = None
self.resource_manager = None",self.pipeline = self.solid_def = self.marshal_dir = self.context = self.resource_manager = None,1,,,,,,,,,,
Machine-Learning-Collection,https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/others/default_setups/CV - Image Classification/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Machine-Learning-Collection/ML/Pytorch/others/default_setups/CV - Image Classification/utils.py,,"def check_accuracy(loader, model, device='cuda'):
    num_correct = 0
    num_samples = 0
    model.eval()
    with torch.no_grad():
        for (x, y) in loader:
            x = x.to(device=device)
            y = y.to(device=device)
            scores = torch.sigmoid(model(x))
            predictions = (scores > 0.5).float()
            num_correct += (predictions == y).sum()
            num_samples += predictions.shape[0]
        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100:.2f}')
    model.train()","num_correct = 0
num_samples = 0",num_correct = num_samples = 0,1,,,,,,,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json","message_input_model['text'] = 'testString'
message_input_model['suggested_text'] = 'testString'
message_input_model['original_text'] = 'testString'
message_input_model['foo'] = 'testString'
runtime_intent_model['intent'] = 'testString'
capture_group_model['group'] = 'testString'
runtime_entity_interpretation_model['calendar_type'] = 'testString'
runtime_entity_interpretation_model['datetime_link'] = 'testString'
runtime_entity_interpretation_model['festival'] = 'testString'
runtime_entity_interpretation_model['range_link'] = 'testString'
runtime_entity_interpretation_model['range_modifier'] = 'testString'
runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
runtime_entity_interpretation_model['subtype'] = 'testString'
runtime_entity_interpretation_model['part_of_day'] = 'testString'
runtime_entity_interpretation_model['timezone'] = 'testString'
runtime_entity_alternative_model['value'] = 'testString'
runtime_entity_model['entity'] = 'testString'
runtime_entity_model['value'] = 'testString'
message_context_metadata_model['deployment'] = 'testString'
message_context_metadata_model['user_id'] = 'testString'
context_model['conversation_id'] = 'testString'
context_model['foo'] = 'testString'
dialog_node_visited_details_model['dialog_node'] = 'testString'
dialog_node_visited_details_model['title'] = 'testString'
dialog_node_visited_details_model['conditions'] = 'testString'
log_message_source_model['dialog_node'] = 'testString'
log_message_model['msg'] = 'testString'
log_message_model['code'] = 'testString'
dialog_node_output_options_element_model['label'] = 'testString'
runtime_response_generic_model['title'] = 'testString'
runtime_response_generic_model['description'] = 'testString'
output_data_model['foo'] = 'testString'
dialog_node_action_model['name'] = 'testString'
dialog_node_action_model['result_variable'] = 'testString'
dialog_node_action_model['credentials'] = 'testString'
message_response_model_json['user_id'] = 'testString'","You can use a for loop to assign the same value to multiple variables in a single line. Here's the refactored code:

testString = 'testString'
for i in range(1, 37):
    globals()['zejun{}'.format(i)] = testString

Note: Using `globals()` is not recommended in most cases, but it can be used to assign values to multiple variables with dynamically generated names.",0,1,,,,,,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json","message_input_model['spelling_suggestions'] = False
message_input_model['spelling_auto_correct'] = False
message_response_model_json['alternate_intents'] = False",message_input_model['spelling_suggestions'] = message_input_model['spelling_auto_correct'] = message_response_model_json['alternate_intents'] = False,1,,,,,,,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_assistant_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_assistant_v1.py,TestModel_MessageResponse,"def test_message_response_serialization(self):
    """"""
        Test serialization/deserialization for MessageResponse
        """"""
    message_input_model = {}
    message_input_model['text'] = 'testString'
    message_input_model['spelling_suggestions'] = False
    message_input_model['spelling_auto_correct'] = False
    message_input_model['suggested_text'] = 'testString'
    message_input_model['original_text'] = 'testString'
    message_input_model['foo'] = 'testString'
    runtime_intent_model = {}
    runtime_intent_model['intent'] = 'testString'
    runtime_intent_model['confidence'] = 72.5
    capture_group_model = {}
    capture_group_model['group'] = 'testString'
    capture_group_model['location'] = [38]
    runtime_entity_interpretation_model = {}
    runtime_entity_interpretation_model['calendar_type'] = 'testString'
    runtime_entity_interpretation_model['datetime_link'] = 'testString'
    runtime_entity_interpretation_model['festival'] = 'testString'
    runtime_entity_interpretation_model['granularity'] = 'day'
    runtime_entity_interpretation_model['range_link'] = 'testString'
    runtime_entity_interpretation_model['range_modifier'] = 'testString'
    runtime_entity_interpretation_model['relative_day'] = 72.5
    runtime_entity_interpretation_model['relative_month'] = 72.5
    runtime_entity_interpretation_model['relative_week'] = 72.5
    runtime_entity_interpretation_model['relative_weekend'] = 72.5
    runtime_entity_interpretation_model['relative_year'] = 72.5
    runtime_entity_interpretation_model['specific_day'] = 72.5
    runtime_entity_interpretation_model['specific_day_of_week'] = 'testString'
    runtime_entity_interpretation_model['specific_month'] = 72.5
    runtime_entity_interpretation_model['specific_quarter'] = 72.5
    runtime_entity_interpretation_model['specific_year'] = 72.5
    runtime_entity_interpretation_model['numeric_value'] = 72.5
    runtime_entity_interpretation_model['subtype'] = 'testString'
    runtime_entity_interpretation_model['part_of_day'] = 'testString'
    runtime_entity_interpretation_model['relative_hour'] = 72.5
    runtime_entity_interpretation_model['relative_minute'] = 72.5
    runtime_entity_interpretation_model['relative_second'] = 72.5
    runtime_entity_interpretation_model['specific_hour'] = 72.5
    runtime_entity_interpretation_model['specific_minute'] = 72.5
    runtime_entity_interpretation_model['specific_second'] = 72.5
    runtime_entity_interpretation_model['timezone'] = 'testString'
    runtime_entity_alternative_model = {}
    runtime_entity_alternative_model['value'] = 'testString'
    runtime_entity_alternative_model['confidence'] = 72.5
    runtime_entity_role_model = {}
    runtime_entity_role_model['type'] = 'date_from'
    runtime_entity_model = {}
    runtime_entity_model['entity'] = 'testString'
    runtime_entity_model['location'] = [38]
    runtime_entity_model['value'] = 'testString'
    runtime_entity_model['confidence'] = 72.5
    runtime_entity_model['metadata'] = {}
    runtime_entity_model['groups'] = [capture_group_model]
    runtime_entity_model['interpretation'] = runtime_entity_interpretation_model
    runtime_entity_model['alternatives'] = [runtime_entity_alternative_model]
    runtime_entity_model['role'] = runtime_entity_role_model
    message_context_metadata_model = {}
    message_context_metadata_model['deployment'] = 'testString'
    message_context_metadata_model['user_id'] = 'testString'
    context_model = {}
    context_model['conversation_id'] = 'testString'
    context_model['system'] = {}
    context_model['metadata'] = message_context_metadata_model
    context_model['foo'] = 'testString'
    dialog_node_visited_details_model = {}
    dialog_node_visited_details_model['dialog_node'] = 'testString'
    dialog_node_visited_details_model['title'] = 'testString'
    dialog_node_visited_details_model['conditions'] = 'testString'
    log_message_source_model = {}
    log_message_source_model['type'] = 'dialog_node'
    log_message_source_model['dialog_node'] = 'testString'
    log_message_model = {}
    log_message_model['level'] = 'info'
    log_message_model['msg'] = 'testString'
    log_message_model['code'] = 'testString'
    log_message_model['source'] = log_message_source_model
    dialog_node_output_options_element_value_model = {}
    dialog_node_output_options_element_value_model['input'] = message_input_model
    dialog_node_output_options_element_value_model['intents'] = [runtime_intent_model]
    dialog_node_output_options_element_value_model['entities'] = [runtime_entity_model]
    dialog_node_output_options_element_model = {}
    dialog_node_output_options_element_model['label'] = 'testString'
    dialog_node_output_options_element_model['value'] = dialog_node_output_options_element_value_model
    response_generic_channel_model = {}
    response_generic_channel_model['channel'] = 'chat'
    runtime_response_generic_model = {}
    runtime_response_generic_model['response_type'] = 'option'
    runtime_response_generic_model['title'] = 'testString'
    runtime_response_generic_model['description'] = 'testString'
    runtime_response_generic_model['preference'] = 'dropdown'
    runtime_response_generic_model['options'] = [dialog_node_output_options_element_model]
    runtime_response_generic_model['channels'] = [response_generic_channel_model]
    output_data_model = {}
    output_data_model['nodes_visited'] = ['testString']
    output_data_model['nodes_visited_details'] = [dialog_node_visited_details_model]
    output_data_model['log_messages'] = [log_message_model]
    output_data_model['text'] = ['testString']
    output_data_model['generic'] = [runtime_response_generic_model]
    output_data_model['foo'] = 'testString'
    dialog_node_action_model = {}
    dialog_node_action_model['name'] = 'testString'
    dialog_node_action_model['type'] = 'client'
    dialog_node_action_model['parameters'] = {}
    dialog_node_action_model['result_variable'] = 'testString'
    dialog_node_action_model['credentials'] = 'testString'
    message_response_model_json = {}
    message_response_model_json['input'] = message_input_model
    message_response_model_json['intents'] = [runtime_intent_model]
    message_response_model_json['entities'] = [runtime_entity_model]
    message_response_model_json['alternate_intents'] = False
    message_response_model_json['context'] = context_model
    message_response_model_json['output'] = output_data_model
    message_response_model_json['actions'] = [dialog_node_action_model]
    message_response_model_json['user_id'] = 'testString'
    message_response_model = MessageResponse.from_dict(message_response_model_json)
    assert message_response_model != False
    message_response_model_dict = MessageResponse.from_dict(message_response_model_json).__dict__
    message_response_model2 = MessageResponse(**message_response_model_dict)
    assert message_response_model == message_response_model2
    message_response_model_json2 = message_response_model.to_dict()
    assert message_response_model_json2 == message_response_model_json","runtime_intent_model['confidence'] = 72.5
runtime_entity_interpretation_model['relative_day'] = 72.5
runtime_entity_interpretation_model['relative_month'] = 72.5
runtime_entity_interpretation_model['relative_week'] = 72.5
runtime_entity_interpretation_model['relative_weekend'] = 72.5
runtime_entity_interpretation_model['relative_year'] = 72.5
runtime_entity_interpretation_model['specific_day'] = 72.5
runtime_entity_interpretation_model['specific_month'] = 72.5
runtime_entity_interpretation_model['specific_quarter'] = 72.5
runtime_entity_interpretation_model['specific_year'] = 72.5
runtime_entity_interpretation_model['numeric_value'] = 72.5
runtime_entity_interpretation_model['relative_hour'] = 72.5
runtime_entity_interpretation_model['relative_minute'] = 72.5
runtime_entity_interpretation_model['relative_second'] = 72.5
runtime_entity_interpretation_model['specific_hour'] = 72.5
runtime_entity_interpretation_model['specific_minute'] = 72.5
runtime_entity_interpretation_model['specific_second'] = 72.5
runtime_entity_alternative_model['confidence'] = 72.5
runtime_entity_model['confidence'] = 72.5",runtime_intent_model['confidence'] = runtime_entity_interpretation_model['relative_day'] = runtime_entity_interpretation_model['relative_month'] = runtime_entity_interpretation_model['relative_week'] = runtime_entity_interpretation_model['relative_weekend'] = runtime_entity_interpretation_model['relative_year'] = runtime_entity_interpretation_model['specific_day'] = runtime_entity_interpretation_model['specific_month'] = runtime_entity_interpretation_model['specific_quarter'] = runtime_intent_model['confidence']0 = runtime_intent_model['confidence']1 = runtime_intent_model['confidence']2 = runtime_intent_model['confidence']3 = runtime_intent_model['confidence']4 = runtime_intent_model['confidence']5 = runtime_intent_model['confidence']6 = runtime_intent_model['confidence']7 = runtime_intent_model['confidence']8 = runtime_intent_model['confidence']9 = 72.5 = runtime_entity_interpretation_model['relative_day']0,0,1,,,,,,,,,
angr,https://github.com/angr/angr/tree/master/angr/exploration_techniques/tracer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/exploration_techniques/tracer.py,Tracer,"def _update_state_tracking(self, state: 'angr.SimState'):
    idx = state.globals['trace_idx']
    sync = state.globals['sync_idx']
    timer = state.globals['sync_timer']
    if state.history.recent_block_count > 1:
        assert state.history.recent_block_count == len(state.history.recent_bbl_addrs)
        for (addr_idx, addr) in enumerate(state.history.recent_bbl_addrs):
            if addr == state.unicorn.transmit_addr:
                continue
            if sync is not None and sync != 'entry':
                if self._compare_addr(self._trace[sync], addr):
                    idx = sync + 1
                    state.globals['sync_idx'] = None
                    sync = None
                continue
            if self._compare_addr(self._trace[idx], addr) or self._check_qemu_unicorn_large_block_split(state, idx, addr_idx):
                idx += 1
            else:
                (is_contained, increment) = self._check_qemu_block_in_unicorn_block(state, idx, addr_idx)
                if is_contained:
                    idx += increment
                    if self._compare_addr(self._trace[idx], addr):
                        idx += 1
                        continue
                raise TracerDesyncError('Oops! angr did not follow the trace', deviating_addr=addr, deviating_trace_idx=idx)
        idx -= 1
    if sync == 'entry':
        trace_addr = self._translate_state_addr(state.addr)
        idx = self._trace.index(trace_addr)
        state.globals['trace_idx'] = idx
        state.globals['sync_idx'] = None
    elif sync is not None:
        timer -= 1
        if self._compare_addr(self._trace[sync], state.addr):
            state.globals['trace_idx'] = sync
            state.globals['sync_idx'] = None
            state.globals['sync_timer'] = 0
        elif timer > 0:
            state.globals['sync_timer'] = timer
        else:
            raise Exception('Trace failed to synchronize! We expected it to hit %#x (trace addr), but it failed to do this within a timeout' % self._trace[sync])
    elif state.history.jumpkind.startswith('Ijk_Exit'):
        pass
    elif self.project.is_hooked(state.addr) and (not self.project.loader.extern_object.contains_addr(state.addr)):
        self._sync_return(state, idx)
    elif self._compare_addr(self._trace[idx + 1], state.addr):
        state.globals['trace_idx'] = idx + 1
    elif self.project.loader._extern_object is not None and self.project.loader.extern_object.contains_addr(state.addr):
        proc = self.project.hooked_by(state.addr)
        if proc is None:
            raise Exception(""Extremely bad news: we're executing an unhooked address in the externs space"")
        if proc.display_name == 'LinuxLoader':
            state.globals['sync_idx'] = 'entry'
        elif proc.is_continuation:
            orig_addr = self.project.loader.find_symbol(proc.display_name).rebased_addr
            obj = self.project.loader.find_object_containing(orig_addr)
            orig_trace_addr = self._translate_state_addr(orig_addr, obj)
            if 0 <= self._trace[idx + 1] - orig_trace_addr <= 65536:
                pass
            else:
                raise Exception(""BUG: State is returning to a continuation that isn't its own???"")
        elif state.addr == getattr(self.project.simos, 'vsyscall_addr', None):
            if not self._sync_callsite(state, idx, state.history.addr):
                raise AngrTracerError('Could not synchronize following vsyscall')
        elif self.project.hooked_by(state.addr).display_name.startswith('IFuncResolver'):
            if not self._sync_return(state, idx):
                raise AngrTracerError('Could not synchronize at ifunc return address')
        else:
            pass
    elif state.history.jumpkind.startswith('Ijk_Sys'):
        state.globals['sync_idx'] = idx + 1
        state.globals['sync_timer'] = 1
    elif self.project.is_hooked(state.history.addr):
        self._fast_forward(state)
    elif state.addr == self._trace[-1]:
        state.globals['sync_idx'] = idx + 1
        state.globals['sync_timer'] = 1
    elif self.project.is_hooked(state.addr) and self.project.loader.find_symbol(self.project.hooked_by(state.addr).display_name) is not None and (self.project.loader.find_symbol(self.project.hooked_by(state.addr).display_name).subtype.value[0] == 10):
        if not self._sync_return(state, idx):
            raise AngrTracerError('Could not synchronize at ifunc return address')
    elif self._analyze_misfollow(state, idx):
        pass
    else:
        raise TracerDesyncError('Oops! angr did not follow the trace', deviating_addr=state.addr, deviating_trace_idx=idx + 1)
    if state.globals['sync_idx'] is not None:
        l.debug('Trace: %s-%s/%s synchronizing %s', state.globals['trace_idx'], state.globals['sync_idx'], len(self._trace), state.globals['sync_timer'])
    else:
        l.debug('Trace: %s/%s', state.globals['trace_idx'], len(self._trace))","state.globals['sync_idx'] = None
sync = None",state.globals['sync_idx'] = sync = None,1,,,,,,,,,,
dirsearch,https://github.com/maurosoria/dirsearch/tree/master/lib/core/fuzzer.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dirsearch/lib/core/fuzzer.py,Fuzzer,"def __init__(self, requester, dictionary, **kwargs):
    self._threads = []
    self._scanned = set()
    self._requester = requester
    self._dictionary = dictionary
    self._play_event = threading.Event()
    self._quit_event = threading.Event()
    self._pause_semaphore = threading.Semaphore(0)
    self._base_path = None
    self.exc = None
    self.match_callbacks = kwargs.get('match_callbacks', [])
    self.not_found_callbacks = kwargs.get('not_found_callbacks', [])
    self.error_callbacks = kwargs.get('error_callbacks', [])","self._base_path = None
self.exc = None",self._base_path = self.exc = None,1,,,,,,,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_matmul_v2_op.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_matmul_v2_op.py,TestMatMulOp15,"def config(self):
    self.x_shape = (3, 1, 6, 6)
    self.y_shape = (1, 2, 6, 9)
    self.trans_x = False
    self.trans_y = False","self.trans_x = False
self.trans_y = False",self.trans_x = self.trans_y = False,1,,,,,,,,,,
nonoCAPTCHA,https://github.com/mikeyy/nonoCAPTCHA/tree/master/nonocaptcha/image.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nonoCAPTCHA/nonocaptcha/image.py,SolveImage,"def __init__(self, browser, image_frame, proxy, proxy_auth, proc_id):
    self.browser = browser
    self.image_frame = image_frame
    self.proxy = proxy
    self.proxy_auth = proxy_auth
    self.proc_id = proc_id
    self.cur_image_path = None
    self.title = None
    self.pieces = None","self.cur_image_path = None
self.title = None
self.pieces = None",self.cur_image_path = self.title = self.pieces = None,1,,,,,,,,,,
prjxray,https://github.com/SymbiFlow/prjxray/tree/master/fuzzers/100-dsp-mskpat/top.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/fuzzers/100-dsp-mskpat/top.py,,"def run():
    verilog.top_harness(48, 48)
    print('module roi(input clk, input [47:0] din, output [47:0] dout);')
    data = {}
    data['instances'] = []
    sites = list(gen_sites())
    for (i, (tile, site)) in enumerate(sites):
        synthesis = '(* KEEP, DONT_TOUCH, LOC = ""%s"" *)' % site
        module = 'DSP48E1'
        instance = 'INST_%s' % site
        ports = {}
        params = {}
        ports['A'] = ""{30{1'b1}}""
        ports['ACIN'] = ""{30{1'b1}}""
        ports['ACOUT'] = ""30'b0""
        ports['ALUMODE'] = 'din[3:0]'
        ports['B'] = ""{18{1'b1}}""
        ports['BCIN'] = ""{18{1'b1}}""
        ports['BCOUT'] = ""18'b0""
        ports['C'] = ""{48{1'b1}}""
        ports['CARRYCASCIN'] = ""1'b1""
        ports['CARRYCASCOUT'] = ""1'b0""
        ports['CARRYIN'] = 'din[4]'
        ports['CARRYINSEL'] = ""3'b000""
        ports['CARRYOUT'] = ""4'b0""
        ports['CEA1'] = ""1'b1""
        ports['CEA2'] = ""1'b1""
        ports['CEAD'] = ""1'b1""
        ports['CEALUMODE'] = ""1'b1""
        ports['CEB1'] = ""1'b1""
        ports['CEB2'] = ""1'b1""
        ports['CEC'] = ""1'b1""
        ports['CECARRYIN'] = ""1'b1""
        ports['CECTRL'] = ""1'b1""
        ports['CED'] = ""1'b1""
        ports['CEINMODE'] = ""1'b1""
        ports['CEM'] = ""1'b1""
        ports['CEP'] = ""1'b1""
        ports['CLK'] = 'clk'
        ports['D'] = ""{25{1'b1}}""
        ports['INMODE'] = 'din[9:5]'
        ports['OPMODE'] = 'din[16:10]'
        ports['OVERFLOW'] = ""1'b0""
        ports['P'] = ""48'b0""
        ports['PATTERNBDETECT'] = ""1'b0""
        ports['PATTERNDETECT'] = ""1'b0""
        ports['PCIN'] = ""{48{1'b1}}""
        ports['PCOUT'] = ""48'b0""
        ports['RSTA'] = ""1'b1""
        ports['RSTALLCARRYIN'] = ""1'b1""
        ports['RSTALUMODE'] = ""1'b1""
        ports['RSTB'] = ""1'b1""
        ports['RSTC'] = ""1'b1""
        ports['RSTCTRL'] = ""1'b1""
        ports['RSTD'] = ""1'b1""
        ports['RSTINMODE'] = ""1'b1""
        ports['RSTM'] = ""1'b1""
        ports['RSTP'] = ""1'b1""
        ports['UNDERFLOW'] = ""1'b0""
        params['ADREG'] = fuzz((0, 1))
        params['ALUMODEREG'] = fuzz((0, 1))
        params['AREG'] = fuzz((0, 1, 2))
        if params['AREG'] == 0 or params['AREG'] == 1:
            params['ACASCREG'] = params['AREG']
        else:
            params['ACASCREG'] = fuzz((1, 2))
        params['BREG'] = fuzz((0, 1, 2))
        if params['BREG'] == 0 or params['BREG'] == 1:
            params['BCASCREG'] = params['BREG']
        else:
            params['BCASCREG'] = fuzz((1, 2))
        params['CARRYINREG'] = fuzz((0, 1))
        params['CARRYINSELREG'] = fuzz((0, 1))
        params['CREG'] = fuzz((0, 1))
        params['DREG'] = fuzz((0, 1))
        params['INMODEREG'] = fuzz((0, 1))
        params['OPMODEREG'] = fuzz((0, 1))
        params['PREG'] = fuzz((0, 1))
        params['A_INPUT'] = verilog.quote(fuzz(('DIRECT', 'CASCADE')))
        params['B_INPUT'] = verilog.quote(fuzz(('DIRECT', 'CASCADE')))
        params['USE_DPORT'] = verilog.quote(fuzz(('TRUE', 'FALSE')))
        params['USE_SIMD'] = verilog.quote(fuzz(('ONE48', 'TWO24', 'FOUR12')))
        params['USE_MULT'] = verilog.quote('NONE' if params['USE_SIMD'] != verilog.quote('ONE48') else fuzz(('NONE', 'MULTIPLY', 'DYNAMIC')))
        params['MREG'] = 0 if params['USE_MULT'] == verilog.quote('NONE') else fuzz((0, 1))
        params['AUTORESET_PATDET'] = verilog.quote(fuzz(('NO_RESET', 'RESET_MATCH', 'RESET_NOT_MATCH')))
        params['MASK'] = ""48'd%s"" % fuzz(48)
        params['PATTERN'] = ""48'd%s"" % fuzz(48)
        params['SEL_MASK'] = verilog.quote(fuzz(('MASK', 'C', 'ROUNDING_MODE1', 'ROUNDING_MODE2')))
        params['USE_PATTERN_DETECT'] = verilog.quote(fuzz(('NO_PATDET', 'PATDET')))
        params['IS_ALUMODE_INVERTED'] = fuzz(4)
        params['IS_CARRYIN_INVERTED'] = fuzz((0, 1))
        params['IS_CLK_INVERTED'] = fuzz((0, 1))
        params['IS_INMODE_INVERTED'] = fuzz(5)
        params['IS_OPMODE_INVERTED'] = fuzz(7)
        verilog.instance(synthesis + ' ' + module, instance, ports, params)
        params['TILE'] = tile
        params['SITE'] = site
        data['instances'].append(params)
    with open('params.json', 'w') as fp:
        json.dump(data, fp)
    print('endmodule')","ports['A'] = ""{30{1'b1}}""
ports['ACIN'] = ""{30{1'b1}}""",ports['A'] = ports['ACIN'] = "{30{1'b1}}",1,,,,,,,,,,
prjxray,https://github.com/SymbiFlow/prjxray/tree/master/fuzzers/100-dsp-mskpat/top.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/fuzzers/100-dsp-mskpat/top.py,,"def run():
    verilog.top_harness(48, 48)
    print('module roi(input clk, input [47:0] din, output [47:0] dout);')
    data = {}
    data['instances'] = []
    sites = list(gen_sites())
    for (i, (tile, site)) in enumerate(sites):
        synthesis = '(* KEEP, DONT_TOUCH, LOC = ""%s"" *)' % site
        module = 'DSP48E1'
        instance = 'INST_%s' % site
        ports = {}
        params = {}
        ports['A'] = ""{30{1'b1}}""
        ports['ACIN'] = ""{30{1'b1}}""
        ports['ACOUT'] = ""30'b0""
        ports['ALUMODE'] = 'din[3:0]'
        ports['B'] = ""{18{1'b1}}""
        ports['BCIN'] = ""{18{1'b1}}""
        ports['BCOUT'] = ""18'b0""
        ports['C'] = ""{48{1'b1}}""
        ports['CARRYCASCIN'] = ""1'b1""
        ports['CARRYCASCOUT'] = ""1'b0""
        ports['CARRYIN'] = 'din[4]'
        ports['CARRYINSEL'] = ""3'b000""
        ports['CARRYOUT'] = ""4'b0""
        ports['CEA1'] = ""1'b1""
        ports['CEA2'] = ""1'b1""
        ports['CEAD'] = ""1'b1""
        ports['CEALUMODE'] = ""1'b1""
        ports['CEB1'] = ""1'b1""
        ports['CEB2'] = ""1'b1""
        ports['CEC'] = ""1'b1""
        ports['CECARRYIN'] = ""1'b1""
        ports['CECTRL'] = ""1'b1""
        ports['CED'] = ""1'b1""
        ports['CEINMODE'] = ""1'b1""
        ports['CEM'] = ""1'b1""
        ports['CEP'] = ""1'b1""
        ports['CLK'] = 'clk'
        ports['D'] = ""{25{1'b1}}""
        ports['INMODE'] = 'din[9:5]'
        ports['OPMODE'] = 'din[16:10]'
        ports['OVERFLOW'] = ""1'b0""
        ports['P'] = ""48'b0""
        ports['PATTERNBDETECT'] = ""1'b0""
        ports['PATTERNDETECT'] = ""1'b0""
        ports['PCIN'] = ""{48{1'b1}}""
        ports['PCOUT'] = ""48'b0""
        ports['RSTA'] = ""1'b1""
        ports['RSTALLCARRYIN'] = ""1'b1""
        ports['RSTALUMODE'] = ""1'b1""
        ports['RSTB'] = ""1'b1""
        ports['RSTC'] = ""1'b1""
        ports['RSTCTRL'] = ""1'b1""
        ports['RSTD'] = ""1'b1""
        ports['RSTINMODE'] = ""1'b1""
        ports['RSTM'] = ""1'b1""
        ports['RSTP'] = ""1'b1""
        ports['UNDERFLOW'] = ""1'b0""
        params['ADREG'] = fuzz((0, 1))
        params['ALUMODEREG'] = fuzz((0, 1))
        params['AREG'] = fuzz((0, 1, 2))
        if params['AREG'] == 0 or params['AREG'] == 1:
            params['ACASCREG'] = params['AREG']
        else:
            params['ACASCREG'] = fuzz((1, 2))
        params['BREG'] = fuzz((0, 1, 2))
        if params['BREG'] == 0 or params['BREG'] == 1:
            params['BCASCREG'] = params['BREG']
        else:
            params['BCASCREG'] = fuzz((1, 2))
        params['CARRYINREG'] = fuzz((0, 1))
        params['CARRYINSELREG'] = fuzz((0, 1))
        params['CREG'] = fuzz((0, 1))
        params['DREG'] = fuzz((0, 1))
        params['INMODEREG'] = fuzz((0, 1))
        params['OPMODEREG'] = fuzz((0, 1))
        params['PREG'] = fuzz((0, 1))
        params['A_INPUT'] = verilog.quote(fuzz(('DIRECT', 'CASCADE')))
        params['B_INPUT'] = verilog.quote(fuzz(('DIRECT', 'CASCADE')))
        params['USE_DPORT'] = verilog.quote(fuzz(('TRUE', 'FALSE')))
        params['USE_SIMD'] = verilog.quote(fuzz(('ONE48', 'TWO24', 'FOUR12')))
        params['USE_MULT'] = verilog.quote('NONE' if params['USE_SIMD'] != verilog.quote('ONE48') else fuzz(('NONE', 'MULTIPLY', 'DYNAMIC')))
        params['MREG'] = 0 if params['USE_MULT'] == verilog.quote('NONE') else fuzz((0, 1))
        params['AUTORESET_PATDET'] = verilog.quote(fuzz(('NO_RESET', 'RESET_MATCH', 'RESET_NOT_MATCH')))
        params['MASK'] = ""48'd%s"" % fuzz(48)
        params['PATTERN'] = ""48'd%s"" % fuzz(48)
        params['SEL_MASK'] = verilog.quote(fuzz(('MASK', 'C', 'ROUNDING_MODE1', 'ROUNDING_MODE2')))
        params['USE_PATTERN_DETECT'] = verilog.quote(fuzz(('NO_PATDET', 'PATDET')))
        params['IS_ALUMODE_INVERTED'] = fuzz(4)
        params['IS_CARRYIN_INVERTED'] = fuzz((0, 1))
        params['IS_CLK_INVERTED'] = fuzz((0, 1))
        params['IS_INMODE_INVERTED'] = fuzz(5)
        params['IS_OPMODE_INVERTED'] = fuzz(7)
        verilog.instance(synthesis + ' ' + module, instance, ports, params)
        params['TILE'] = tile
        params['SITE'] = site
        data['instances'].append(params)
    with open('params.json', 'w') as fp:
        json.dump(data, fp)
    print('endmodule')","ports['B'] = ""{18{1'b1}}""
ports['BCIN'] = ""{18{1'b1}}""",ports['B'] = ports['BCIN'] = "{18{1'b1}}",1,,,,,,,,,,
prjxray,https://github.com/SymbiFlow/prjxray/tree/master/fuzzers/100-dsp-mskpat/top.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/fuzzers/100-dsp-mskpat/top.py,,"def run():
    verilog.top_harness(48, 48)
    print('module roi(input clk, input [47:0] din, output [47:0] dout);')
    data = {}
    data['instances'] = []
    sites = list(gen_sites())
    for (i, (tile, site)) in enumerate(sites):
        synthesis = '(* KEEP, DONT_TOUCH, LOC = ""%s"" *)' % site
        module = 'DSP48E1'
        instance = 'INST_%s' % site
        ports = {}
        params = {}
        ports['A'] = ""{30{1'b1}}""
        ports['ACIN'] = ""{30{1'b1}}""
        ports['ACOUT'] = ""30'b0""
        ports['ALUMODE'] = 'din[3:0]'
        ports['B'] = ""{18{1'b1}}""
        ports['BCIN'] = ""{18{1'b1}}""
        ports['BCOUT'] = ""18'b0""
        ports['C'] = ""{48{1'b1}}""
        ports['CARRYCASCIN'] = ""1'b1""
        ports['CARRYCASCOUT'] = ""1'b0""
        ports['CARRYIN'] = 'din[4]'
        ports['CARRYINSEL'] = ""3'b000""
        ports['CARRYOUT'] = ""4'b0""
        ports['CEA1'] = ""1'b1""
        ports['CEA2'] = ""1'b1""
        ports['CEAD'] = ""1'b1""
        ports['CEALUMODE'] = ""1'b1""
        ports['CEB1'] = ""1'b1""
        ports['CEB2'] = ""1'b1""
        ports['CEC'] = ""1'b1""
        ports['CECARRYIN'] = ""1'b1""
        ports['CECTRL'] = ""1'b1""
        ports['CED'] = ""1'b1""
        ports['CEINMODE'] = ""1'b1""
        ports['CEM'] = ""1'b1""
        ports['CEP'] = ""1'b1""
        ports['CLK'] = 'clk'
        ports['D'] = ""{25{1'b1}}""
        ports['INMODE'] = 'din[9:5]'
        ports['OPMODE'] = 'din[16:10]'
        ports['OVERFLOW'] = ""1'b0""
        ports['P'] = ""48'b0""
        ports['PATTERNBDETECT'] = ""1'b0""
        ports['PATTERNDETECT'] = ""1'b0""
        ports['PCIN'] = ""{48{1'b1}}""
        ports['PCOUT'] = ""48'b0""
        ports['RSTA'] = ""1'b1""
        ports['RSTALLCARRYIN'] = ""1'b1""
        ports['RSTALUMODE'] = ""1'b1""
        ports['RSTB'] = ""1'b1""
        ports['RSTC'] = ""1'b1""
        ports['RSTCTRL'] = ""1'b1""
        ports['RSTD'] = ""1'b1""
        ports['RSTINMODE'] = ""1'b1""
        ports['RSTM'] = ""1'b1""
        ports['RSTP'] = ""1'b1""
        ports['UNDERFLOW'] = ""1'b0""
        params['ADREG'] = fuzz((0, 1))
        params['ALUMODEREG'] = fuzz((0, 1))
        params['AREG'] = fuzz((0, 1, 2))
        if params['AREG'] == 0 or params['AREG'] == 1:
            params['ACASCREG'] = params['AREG']
        else:
            params['ACASCREG'] = fuzz((1, 2))
        params['BREG'] = fuzz((0, 1, 2))
        if params['BREG'] == 0 or params['BREG'] == 1:
            params['BCASCREG'] = params['BREG']
        else:
            params['BCASCREG'] = fuzz((1, 2))
        params['CARRYINREG'] = fuzz((0, 1))
        params['CARRYINSELREG'] = fuzz((0, 1))
        params['CREG'] = fuzz((0, 1))
        params['DREG'] = fuzz((0, 1))
        params['INMODEREG'] = fuzz((0, 1))
        params['OPMODEREG'] = fuzz((0, 1))
        params['PREG'] = fuzz((0, 1))
        params['A_INPUT'] = verilog.quote(fuzz(('DIRECT', 'CASCADE')))
        params['B_INPUT'] = verilog.quote(fuzz(('DIRECT', 'CASCADE')))
        params['USE_DPORT'] = verilog.quote(fuzz(('TRUE', 'FALSE')))
        params['USE_SIMD'] = verilog.quote(fuzz(('ONE48', 'TWO24', 'FOUR12')))
        params['USE_MULT'] = verilog.quote('NONE' if params['USE_SIMD'] != verilog.quote('ONE48') else fuzz(('NONE', 'MULTIPLY', 'DYNAMIC')))
        params['MREG'] = 0 if params['USE_MULT'] == verilog.quote('NONE') else fuzz((0, 1))
        params['AUTORESET_PATDET'] = verilog.quote(fuzz(('NO_RESET', 'RESET_MATCH', 'RESET_NOT_MATCH')))
        params['MASK'] = ""48'd%s"" % fuzz(48)
        params['PATTERN'] = ""48'd%s"" % fuzz(48)
        params['SEL_MASK'] = verilog.quote(fuzz(('MASK', 'C', 'ROUNDING_MODE1', 'ROUNDING_MODE2')))
        params['USE_PATTERN_DETECT'] = verilog.quote(fuzz(('NO_PATDET', 'PATDET')))
        params['IS_ALUMODE_INVERTED'] = fuzz(4)
        params['IS_CARRYIN_INVERTED'] = fuzz((0, 1))
        params['IS_CLK_INVERTED'] = fuzz((0, 1))
        params['IS_INMODE_INVERTED'] = fuzz(5)
        params['IS_OPMODE_INVERTED'] = fuzz(7)
        verilog.instance(synthesis + ' ' + module, instance, ports, params)
        params['TILE'] = tile
        params['SITE'] = site
        data['instances'].append(params)
    with open('params.json', 'w') as fp:
        json.dump(data, fp)
    print('endmodule')","ports['C'] = ""{48{1'b1}}""
ports['PCIN'] = ""{48{1'b1}}""",ports['C'] = ports['PCIN'] = "{48{1'b1}}",1,,,,,,,,,,
prjxray,https://github.com/SymbiFlow/prjxray/tree/master/fuzzers/100-dsp-mskpat/top.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/fuzzers/100-dsp-mskpat/top.py,,"def run():
    verilog.top_harness(48, 48)
    print('module roi(input clk, input [47:0] din, output [47:0] dout);')
    data = {}
    data['instances'] = []
    sites = list(gen_sites())
    for (i, (tile, site)) in enumerate(sites):
        synthesis = '(* KEEP, DONT_TOUCH, LOC = ""%s"" *)' % site
        module = 'DSP48E1'
        instance = 'INST_%s' % site
        ports = {}
        params = {}
        ports['A'] = ""{30{1'b1}}""
        ports['ACIN'] = ""{30{1'b1}}""
        ports['ACOUT'] = ""30'b0""
        ports['ALUMODE'] = 'din[3:0]'
        ports['B'] = ""{18{1'b1}}""
        ports['BCIN'] = ""{18{1'b1}}""
        ports['BCOUT'] = ""18'b0""
        ports['C'] = ""{48{1'b1}}""
        ports['CARRYCASCIN'] = ""1'b1""
        ports['CARRYCASCOUT'] = ""1'b0""
        ports['CARRYIN'] = 'din[4]'
        ports['CARRYINSEL'] = ""3'b000""
        ports['CARRYOUT'] = ""4'b0""
        ports['CEA1'] = ""1'b1""
        ports['CEA2'] = ""1'b1""
        ports['CEAD'] = ""1'b1""
        ports['CEALUMODE'] = ""1'b1""
        ports['CEB1'] = ""1'b1""
        ports['CEB2'] = ""1'b1""
        ports['CEC'] = ""1'b1""
        ports['CECARRYIN'] = ""1'b1""
        ports['CECTRL'] = ""1'b1""
        ports['CED'] = ""1'b1""
        ports['CEINMODE'] = ""1'b1""
        ports['CEM'] = ""1'b1""
        ports['CEP'] = ""1'b1""
        ports['CLK'] = 'clk'
        ports['D'] = ""{25{1'b1}}""
        ports['INMODE'] = 'din[9:5]'
        ports['OPMODE'] = 'din[16:10]'
        ports['OVERFLOW'] = ""1'b0""
        ports['P'] = ""48'b0""
        ports['PATTERNBDETECT'] = ""1'b0""
        ports['PATTERNDETECT'] = ""1'b0""
        ports['PCIN'] = ""{48{1'b1}}""
        ports['PCOUT'] = ""48'b0""
        ports['RSTA'] = ""1'b1""
        ports['RSTALLCARRYIN'] = ""1'b1""
        ports['RSTALUMODE'] = ""1'b1""
        ports['RSTB'] = ""1'b1""
        ports['RSTC'] = ""1'b1""
        ports['RSTCTRL'] = ""1'b1""
        ports['RSTD'] = ""1'b1""
        ports['RSTINMODE'] = ""1'b1""
        ports['RSTM'] = ""1'b1""
        ports['RSTP'] = ""1'b1""
        ports['UNDERFLOW'] = ""1'b0""
        params['ADREG'] = fuzz((0, 1))
        params['ALUMODEREG'] = fuzz((0, 1))
        params['AREG'] = fuzz((0, 1, 2))
        if params['AREG'] == 0 or params['AREG'] == 1:
            params['ACASCREG'] = params['AREG']
        else:
            params['ACASCREG'] = fuzz((1, 2))
        params['BREG'] = fuzz((0, 1, 2))
        if params['BREG'] == 0 or params['BREG'] == 1:
            params['BCASCREG'] = params['BREG']
        else:
            params['BCASCREG'] = fuzz((1, 2))
        params['CARRYINREG'] = fuzz((0, 1))
        params['CARRYINSELREG'] = fuzz((0, 1))
        params['CREG'] = fuzz((0, 1))
        params['DREG'] = fuzz((0, 1))
        params['INMODEREG'] = fuzz((0, 1))
        params['OPMODEREG'] = fuzz((0, 1))
        params['PREG'] = fuzz((0, 1))
        params['A_INPUT'] = verilog.quote(fuzz(('DIRECT', 'CASCADE')))
        params['B_INPUT'] = verilog.quote(fuzz(('DIRECT', 'CASCADE')))
        params['USE_DPORT'] = verilog.quote(fuzz(('TRUE', 'FALSE')))
        params['USE_SIMD'] = verilog.quote(fuzz(('ONE48', 'TWO24', 'FOUR12')))
        params['USE_MULT'] = verilog.quote('NONE' if params['USE_SIMD'] != verilog.quote('ONE48') else fuzz(('NONE', 'MULTIPLY', 'DYNAMIC')))
        params['MREG'] = 0 if params['USE_MULT'] == verilog.quote('NONE') else fuzz((0, 1))
        params['AUTORESET_PATDET'] = verilog.quote(fuzz(('NO_RESET', 'RESET_MATCH', 'RESET_NOT_MATCH')))
        params['MASK'] = ""48'd%s"" % fuzz(48)
        params['PATTERN'] = ""48'd%s"" % fuzz(48)
        params['SEL_MASK'] = verilog.quote(fuzz(('MASK', 'C', 'ROUNDING_MODE1', 'ROUNDING_MODE2')))
        params['USE_PATTERN_DETECT'] = verilog.quote(fuzz(('NO_PATDET', 'PATDET')))
        params['IS_ALUMODE_INVERTED'] = fuzz(4)
        params['IS_CARRYIN_INVERTED'] = fuzz((0, 1))
        params['IS_CLK_INVERTED'] = fuzz((0, 1))
        params['IS_INMODE_INVERTED'] = fuzz(5)
        params['IS_OPMODE_INVERTED'] = fuzz(7)
        verilog.instance(synthesis + ' ' + module, instance, ports, params)
        params['TILE'] = tile
        params['SITE'] = site
        data['instances'].append(params)
    with open('params.json', 'w') as fp:
        json.dump(data, fp)
    print('endmodule')","ports['CARRYCASCIN'] = ""1'b1""
ports['CEA1'] = ""1'b1""
ports['CEA2'] = ""1'b1""
ports['CEAD'] = ""1'b1""
ports['CEALUMODE'] = ""1'b1""
ports['CEB1'] = ""1'b1""
ports['CEB2'] = ""1'b1""
ports['CEC'] = ""1'b1""
ports['CECARRYIN'] = ""1'b1""
ports['CECTRL'] = ""1'b1""
ports['CED'] = ""1'b1""
ports['CEINMODE'] = ""1'b1""
ports['CEM'] = ""1'b1""
ports['CEP'] = ""1'b1""
ports['RSTA'] = ""1'b1""
ports['RSTALLCARRYIN'] = ""1'b1""
ports['RSTALUMODE'] = ""1'b1""
ports['RSTB'] = ""1'b1""
ports['RSTC'] = ""1'b1""
ports['RSTCTRL'] = ""1'b1""
ports['RSTD'] = ""1'b1""
ports['RSTINMODE'] = ""1'b1""
ports['RSTM'] = ""1'b1""
ports['RSTP'] = ""1'b1""","You can use a for loop to assign the same value to multiple variables in a single line. Here's the refactored code:

```
value = ""1'b1""
for i in range(1, 25):
    globals()[""zejun"" + str(i)] = value
```

This code assigns the value ""1'b1"" to the variables ports['CARRYCASCIN'] through ports['CEA1']4 using a for loop and the `globals()` function to dynamically create variable names.",0,1,,,,,,,,,
prjxray,https://github.com/SymbiFlow/prjxray/tree/master/fuzzers/100-dsp-mskpat/top.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/fuzzers/100-dsp-mskpat/top.py,,"def run():
    verilog.top_harness(48, 48)
    print('module roi(input clk, input [47:0] din, output [47:0] dout);')
    data = {}
    data['instances'] = []
    sites = list(gen_sites())
    for (i, (tile, site)) in enumerate(sites):
        synthesis = '(* KEEP, DONT_TOUCH, LOC = ""%s"" *)' % site
        module = 'DSP48E1'
        instance = 'INST_%s' % site
        ports = {}
        params = {}
        ports['A'] = ""{30{1'b1}}""
        ports['ACIN'] = ""{30{1'b1}}""
        ports['ACOUT'] = ""30'b0""
        ports['ALUMODE'] = 'din[3:0]'
        ports['B'] = ""{18{1'b1}}""
        ports['BCIN'] = ""{18{1'b1}}""
        ports['BCOUT'] = ""18'b0""
        ports['C'] = ""{48{1'b1}}""
        ports['CARRYCASCIN'] = ""1'b1""
        ports['CARRYCASCOUT'] = ""1'b0""
        ports['CARRYIN'] = 'din[4]'
        ports['CARRYINSEL'] = ""3'b000""
        ports['CARRYOUT'] = ""4'b0""
        ports['CEA1'] = ""1'b1""
        ports['CEA2'] = ""1'b1""
        ports['CEAD'] = ""1'b1""
        ports['CEALUMODE'] = ""1'b1""
        ports['CEB1'] = ""1'b1""
        ports['CEB2'] = ""1'b1""
        ports['CEC'] = ""1'b1""
        ports['CECARRYIN'] = ""1'b1""
        ports['CECTRL'] = ""1'b1""
        ports['CED'] = ""1'b1""
        ports['CEINMODE'] = ""1'b1""
        ports['CEM'] = ""1'b1""
        ports['CEP'] = ""1'b1""
        ports['CLK'] = 'clk'
        ports['D'] = ""{25{1'b1}}""
        ports['INMODE'] = 'din[9:5]'
        ports['OPMODE'] = 'din[16:10]'
        ports['OVERFLOW'] = ""1'b0""
        ports['P'] = ""48'b0""
        ports['PATTERNBDETECT'] = ""1'b0""
        ports['PATTERNDETECT'] = ""1'b0""
        ports['PCIN'] = ""{48{1'b1}}""
        ports['PCOUT'] = ""48'b0""
        ports['RSTA'] = ""1'b1""
        ports['RSTALLCARRYIN'] = ""1'b1""
        ports['RSTALUMODE'] = ""1'b1""
        ports['RSTB'] = ""1'b1""
        ports['RSTC'] = ""1'b1""
        ports['RSTCTRL'] = ""1'b1""
        ports['RSTD'] = ""1'b1""
        ports['RSTINMODE'] = ""1'b1""
        ports['RSTM'] = ""1'b1""
        ports['RSTP'] = ""1'b1""
        ports['UNDERFLOW'] = ""1'b0""
        params['ADREG'] = fuzz((0, 1))
        params['ALUMODEREG'] = fuzz((0, 1))
        params['AREG'] = fuzz((0, 1, 2))
        if params['AREG'] == 0 or params['AREG'] == 1:
            params['ACASCREG'] = params['AREG']
        else:
            params['ACASCREG'] = fuzz((1, 2))
        params['BREG'] = fuzz((0, 1, 2))
        if params['BREG'] == 0 or params['BREG'] == 1:
            params['BCASCREG'] = params['BREG']
        else:
            params['BCASCREG'] = fuzz((1, 2))
        params['CARRYINREG'] = fuzz((0, 1))
        params['CARRYINSELREG'] = fuzz((0, 1))
        params['CREG'] = fuzz((0, 1))
        params['DREG'] = fuzz((0, 1))
        params['INMODEREG'] = fuzz((0, 1))
        params['OPMODEREG'] = fuzz((0, 1))
        params['PREG'] = fuzz((0, 1))
        params['A_INPUT'] = verilog.quote(fuzz(('DIRECT', 'CASCADE')))
        params['B_INPUT'] = verilog.quote(fuzz(('DIRECT', 'CASCADE')))
        params['USE_DPORT'] = verilog.quote(fuzz(('TRUE', 'FALSE')))
        params['USE_SIMD'] = verilog.quote(fuzz(('ONE48', 'TWO24', 'FOUR12')))
        params['USE_MULT'] = verilog.quote('NONE' if params['USE_SIMD'] != verilog.quote('ONE48') else fuzz(('NONE', 'MULTIPLY', 'DYNAMIC')))
        params['MREG'] = 0 if params['USE_MULT'] == verilog.quote('NONE') else fuzz((0, 1))
        params['AUTORESET_PATDET'] = verilog.quote(fuzz(('NO_RESET', 'RESET_MATCH', 'RESET_NOT_MATCH')))
        params['MASK'] = ""48'd%s"" % fuzz(48)
        params['PATTERN'] = ""48'd%s"" % fuzz(48)
        params['SEL_MASK'] = verilog.quote(fuzz(('MASK', 'C', 'ROUNDING_MODE1', 'ROUNDING_MODE2')))
        params['USE_PATTERN_DETECT'] = verilog.quote(fuzz(('NO_PATDET', 'PATDET')))
        params['IS_ALUMODE_INVERTED'] = fuzz(4)
        params['IS_CARRYIN_INVERTED'] = fuzz((0, 1))
        params['IS_CLK_INVERTED'] = fuzz((0, 1))
        params['IS_INMODE_INVERTED'] = fuzz(5)
        params['IS_OPMODE_INVERTED'] = fuzz(7)
        verilog.instance(synthesis + ' ' + module, instance, ports, params)
        params['TILE'] = tile
        params['SITE'] = site
        data['instances'].append(params)
    with open('params.json', 'w') as fp:
        json.dump(data, fp)
    print('endmodule')","ports['CARRYCASCOUT'] = ""1'b0""
ports['OVERFLOW'] = ""1'b0""
ports['PATTERNBDETECT'] = ""1'b0""
ports['PATTERNDETECT'] = ""1'b0""
ports['UNDERFLOW'] = ""1'b0""",ports['CARRYCASCOUT'] = ports['OVERFLOW'] = ports['PATTERNBDETECT'] = ports['PATTERNDETECT'] = ports['UNDERFLOW'] = "1'b0",1,,,,,,,,,,
prjxray,https://github.com/SymbiFlow/prjxray/tree/master/fuzzers/100-dsp-mskpat/top.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/prjxray/fuzzers/100-dsp-mskpat/top.py,,"def run():
    verilog.top_harness(48, 48)
    print('module roi(input clk, input [47:0] din, output [47:0] dout);')
    data = {}
    data['instances'] = []
    sites = list(gen_sites())
    for (i, (tile, site)) in enumerate(sites):
        synthesis = '(* KEEP, DONT_TOUCH, LOC = ""%s"" *)' % site
        module = 'DSP48E1'
        instance = 'INST_%s' % site
        ports = {}
        params = {}
        ports['A'] = ""{30{1'b1}}""
        ports['ACIN'] = ""{30{1'b1}}""
        ports['ACOUT'] = ""30'b0""
        ports['ALUMODE'] = 'din[3:0]'
        ports['B'] = ""{18{1'b1}}""
        ports['BCIN'] = ""{18{1'b1}}""
        ports['BCOUT'] = ""18'b0""
        ports['C'] = ""{48{1'b1}}""
        ports['CARRYCASCIN'] = ""1'b1""
        ports['CARRYCASCOUT'] = ""1'b0""
        ports['CARRYIN'] = 'din[4]'
        ports['CARRYINSEL'] = ""3'b000""
        ports['CARRYOUT'] = ""4'b0""
        ports['CEA1'] = ""1'b1""
        ports['CEA2'] = ""1'b1""
        ports['CEAD'] = ""1'b1""
        ports['CEALUMODE'] = ""1'b1""
        ports['CEB1'] = ""1'b1""
        ports['CEB2'] = ""1'b1""
        ports['CEC'] = ""1'b1""
        ports['CECARRYIN'] = ""1'b1""
        ports['CECTRL'] = ""1'b1""
        ports['CED'] = ""1'b1""
        ports['CEINMODE'] = ""1'b1""
        ports['CEM'] = ""1'b1""
        ports['CEP'] = ""1'b1""
        ports['CLK'] = 'clk'
        ports['D'] = ""{25{1'b1}}""
        ports['INMODE'] = 'din[9:5]'
        ports['OPMODE'] = 'din[16:10]'
        ports['OVERFLOW'] = ""1'b0""
        ports['P'] = ""48'b0""
        ports['PATTERNBDETECT'] = ""1'b0""
        ports['PATTERNDETECT'] = ""1'b0""
        ports['PCIN'] = ""{48{1'b1}}""
        ports['PCOUT'] = ""48'b0""
        ports['RSTA'] = ""1'b1""
        ports['RSTALLCARRYIN'] = ""1'b1""
        ports['RSTALUMODE'] = ""1'b1""
        ports['RSTB'] = ""1'b1""
        ports['RSTC'] = ""1'b1""
        ports['RSTCTRL'] = ""1'b1""
        ports['RSTD'] = ""1'b1""
        ports['RSTINMODE'] = ""1'b1""
        ports['RSTM'] = ""1'b1""
        ports['RSTP'] = ""1'b1""
        ports['UNDERFLOW'] = ""1'b0""
        params['ADREG'] = fuzz((0, 1))
        params['ALUMODEREG'] = fuzz((0, 1))
        params['AREG'] = fuzz((0, 1, 2))
        if params['AREG'] == 0 or params['AREG'] == 1:
            params['ACASCREG'] = params['AREG']
        else:
            params['ACASCREG'] = fuzz((1, 2))
        params['BREG'] = fuzz((0, 1, 2))
        if params['BREG'] == 0 or params['BREG'] == 1:
            params['BCASCREG'] = params['BREG']
        else:
            params['BCASCREG'] = fuzz((1, 2))
        params['CARRYINREG'] = fuzz((0, 1))
        params['CARRYINSELREG'] = fuzz((0, 1))
        params['CREG'] = fuzz((0, 1))
        params['DREG'] = fuzz((0, 1))
        params['INMODEREG'] = fuzz((0, 1))
        params['OPMODEREG'] = fuzz((0, 1))
        params['PREG'] = fuzz((0, 1))
        params['A_INPUT'] = verilog.quote(fuzz(('DIRECT', 'CASCADE')))
        params['B_INPUT'] = verilog.quote(fuzz(('DIRECT', 'CASCADE')))
        params['USE_DPORT'] = verilog.quote(fuzz(('TRUE', 'FALSE')))
        params['USE_SIMD'] = verilog.quote(fuzz(('ONE48', 'TWO24', 'FOUR12')))
        params['USE_MULT'] = verilog.quote('NONE' if params['USE_SIMD'] != verilog.quote('ONE48') else fuzz(('NONE', 'MULTIPLY', 'DYNAMIC')))
        params['MREG'] = 0 if params['USE_MULT'] == verilog.quote('NONE') else fuzz((0, 1))
        params['AUTORESET_PATDET'] = verilog.quote(fuzz(('NO_RESET', 'RESET_MATCH', 'RESET_NOT_MATCH')))
        params['MASK'] = ""48'd%s"" % fuzz(48)
        params['PATTERN'] = ""48'd%s"" % fuzz(48)
        params['SEL_MASK'] = verilog.quote(fuzz(('MASK', 'C', 'ROUNDING_MODE1', 'ROUNDING_MODE2')))
        params['USE_PATTERN_DETECT'] = verilog.quote(fuzz(('NO_PATDET', 'PATDET')))
        params['IS_ALUMODE_INVERTED'] = fuzz(4)
        params['IS_CARRYIN_INVERTED'] = fuzz((0, 1))
        params['IS_CLK_INVERTED'] = fuzz((0, 1))
        params['IS_INMODE_INVERTED'] = fuzz(5)
        params['IS_OPMODE_INVERTED'] = fuzz(7)
        verilog.instance(synthesis + ' ' + module, instance, ports, params)
        params['TILE'] = tile
        params['SITE'] = site
        data['instances'].append(params)
    with open('params.json', 'w') as fp:
        json.dump(data, fp)
    print('endmodule')","ports['P'] = ""48'b0""
ports['PCOUT'] = ""48'b0""",ports['P'] = ports['PCOUT'] = "48'b0",,,,,,,,,,,
pyglossary,https://github.com/ilius/pyglossary/tree/master/pyglossary/ui/ui_cmd_interactive.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglossary/pyglossary/ui/ui_cmd_interactive.py,UI,"def __init__(self):
    self._inputFilename = ''
    self._outputFilename = ''
    self._inputFormat = ''
    self._outputFormat = ''
    self.config = None
    self._readOptions = None
    self._writeOptions = None
    self._convertOptions = None
    ui_cmd.UI.__init__(self)
    self.ls_parser = argparse.ArgumentParser(add_help=False)
    self.ls_parser.add_argument('-l', '--long', action='store_true', dest='long', help='use a long listing format')
    self.ls_parser.add_argument('--help', action='store_true', dest='help', help='display help')
    self.ls_usage = 'Usage: !ls [--help] [-l] [FILE/DIRECTORY]...\n\noptional arguments:\n    --help      show this help message and exit\n    -l, --long  use a long listing format\n'
    self._fsActions = OrderedDict([('!pwd', (self.fs_pwd, '')), ('!ls', (self.fs_ls, self.ls_usage)), ('!..', (self.fs_cd_parent, '')), ('!cd', (self.fs_cd, ''))])
    self._finalActions = OrderedDict([('formats', self.askFormats), ('read-options', self.askReadOptions), ('write-options', self.askWriteOptions), ('reset-read-options', self.resetReadOptions), ('reset-write-options', self.resetWriteOptions), ('config', self.askConfig), ('indirect', self.setIndirect), ('sqlite', self.setSQLite), ('no-progressbar', self.setNoProgressbar), ('sort', self.setSort), ('sort-key', self.setSortKey), ('show-options', self.showOptions), ('back', None)])","self._inputFilename = ''
self._outputFilename = ''
self._inputFormat = ''
self._outputFormat = ''",self._inputFilename = self._outputFilename = self._inputFormat = self._outputFormat = '',,,,,,,,,,,
pyglossary,https://github.com/ilius/pyglossary/tree/master/pyglossary/ui/ui_cmd_interactive.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pyglossary/pyglossary/ui/ui_cmd_interactive.py,UI,"def __init__(self):
    self._inputFilename = ''
    self._outputFilename = ''
    self._inputFormat = ''
    self._outputFormat = ''
    self.config = None
    self._readOptions = None
    self._writeOptions = None
    self._convertOptions = None
    ui_cmd.UI.__init__(self)
    self.ls_parser = argparse.ArgumentParser(add_help=False)
    self.ls_parser.add_argument('-l', '--long', action='store_true', dest='long', help='use a long listing format')
    self.ls_parser.add_argument('--help', action='store_true', dest='help', help='display help')
    self.ls_usage = 'Usage: !ls [--help] [-l] [FILE/DIRECTORY]...\n\noptional arguments:\n    --help      show this help message and exit\n    -l, --long  use a long listing format\n'
    self._fsActions = OrderedDict([('!pwd', (self.fs_pwd, '')), ('!ls', (self.fs_ls, self.ls_usage)), ('!..', (self.fs_cd_parent, '')), ('!cd', (self.fs_cd, ''))])
    self._finalActions = OrderedDict([('formats', self.askFormats), ('read-options', self.askReadOptions), ('write-options', self.askWriteOptions), ('reset-read-options', self.resetReadOptions), ('reset-write-options', self.resetWriteOptions), ('config', self.askConfig), ('indirect', self.setIndirect), ('sqlite', self.setSQLite), ('no-progressbar', self.setNoProgressbar), ('sort', self.setSort), ('sort-key', self.setSortKey), ('show-options', self.showOptions), ('back', None)])","self.config = None
self._readOptions = None
self._writeOptions = None
self._convertOptions = None",self.config = self._readOptions = self._writeOptions = self._convertOptions = None,,,,,,,,,,,
mushroom-rl,https://github.com/MushroomRL/mushroom-rl/tree/master/mushroom_rl/solvers/car_on_hill.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mushroom-rl/mushroom_rl/solvers/car_on_hill.py,,"def solve_car_on_hill(mdp, states, actions, gamma, max_k=50):
    """"""
    Solver of the Car-On-Hill environment.

    Args:
        mdp (CarOnHill): the Car-On-Hill environment;
        states (np.ndarray): the states;
        actions (np.ndarray): the actions;
        gamma (float): the discount factor;
        max_k (int, 50): maximum depth to consider.

    Returns:
        The Q-value for each ``state``-``action`` tuple.

    """"""
    q = list()
    for (s, a) in zip(states, actions):
        mdp.reset(s)
        (state, reward, _, _) = mdp.step(a)
        if reward == 1:
            k = 1
            success = True
        elif reward == -1:
            k = 1
            success = False
        else:
            (success, k) = bfs(mdp, [state], 2, max_k)
        if success:
            q.append(gamma ** (k - 1))
        else:
            q.append(-gamma ** (k - 1))
    return q","k = 1
success = True",k = success = 1,,,,,,,,,,,
erpnext,https://github.com/frappe/erpnext/tree/master/erpnext/setup/doctype/email_digest/email_digest.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/setup/doctype/email_digest/email_digest.py,EmailDigest,"def get_period_amounts(self, accounts, fieldname):
    """"""Get amounts for current and past periods""""""
    balance = past_balance = 0.0
    count = 0
    for account in accounts:
        balance += get_incomes_expenses_for_period(account, self.future_from_date, self.future_to_date)
        past_balance += get_incomes_expenses_for_period(account, self.past_from_date, self.past_to_date)
        count += get_count_for_period(account, fieldname, self.future_from_date, self.future_to_date)
    return (balance, past_balance, count)","balance = past_balance = 0.0
count = 0","balance = past_balance = 0.0
count = 0

can be refactored as:

balance, past_balance, count = 0.0, 0.0, 0",,,,,,,,,,,
OpenBCI_Python,https://github.com/openbci-archive/OpenBCI_Python/tree/master/tests/test_wifi.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpenBCI_Python/tests/test_wifi.py,TestOpenBCIWiFi,"def test_wifi_init(self, mock_on_shield_found):
    expected_ip_address = '192.168.0.1'
    expected_shield_name = 'OpenBCI-E218'
    expected_sample_rate = 500
    expected_log = False
    expected_timeout = 5
    expected_max_packets_to_skip = 10
    expected_latency = 5000
    expected_high_speed = False
    expected_ssdp_attempts = 2
    wifi = OpenBCIWiFi(ip_address=expected_ip_address, shield_name=expected_shield_name, sample_rate=expected_sample_rate, log=expected_log, timeout=expected_timeout, max_packets_to_skip=expected_max_packets_to_skip, latency=expected_latency, high_speed=expected_high_speed, ssdp_attempts=expected_ssdp_attempts)
    self.assertEqual(wifi.ip_address, expected_ip_address)
    self.assertEqual(wifi.shield_name, expected_shield_name)
    self.assertEqual(wifi.sample_rate, expected_sample_rate)
    self.assertEqual(wifi.log, expected_log)
    self.assertEqual(wifi.timeout, expected_timeout)
    self.assertEqual(wifi.max_packets_to_skip, expected_max_packets_to_skip)
    self.assertEqual(wifi.latency, expected_latency)
    self.assertEqual(wifi.high_speed, expected_high_speed)
    self.assertEqual(wifi.ssdp_attempts, expected_ssdp_attempts)
    mock_on_shield_found.assert_called_with(expected_ip_address)","expected_log = False
expected_high_speed = False",expected_log = expected_high_speed = False,,,,,,,,,,,
leetCode,https://github.com/HuberTRoy/leetCode/tree/master/Sorted/SortList.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/leetCode/Sorted/SortList.py,Solution,"def merge_sort(l, r):
    _l = 0
    _r = 0
    _l_length = len(l)
    _r_length = len(r)
    result = []
    while _l < _l_length and _r < _r_length:
        if l[_l] < r[_r]:
            result.append(l[_l])
            _l += 1
        else:
            result.append(r[_r])
            _r += 1
    if _l == _l_length:
        while _r < _r_length:
            result.append(r[_r])
            _r += 1
    else:
        while _l < _l_length:
            result.append(l[_l])
            _l += 1
    return result","_l = 0
_r = 0",_l = _r = 0,,,,,,,,,,,
youtube-dl,https://github.com/lrvick/youtube-dl/tree/master/test/test_subtitles.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/youtube-dl/test/test_subtitles.py,TestWallaSubtitles,"def test_nosubtitles(self):
    self.DL.expect_warning(""video doesn't have subtitles"")
    self.url = 'http://vod.walla.co.il/movie/2642630/one-direction-all-for-one'
    self.DL.params['writesubtitles'] = True
    self.DL.params['allsubtitles'] = True
    subtitles = self.getSubtitles()
    self.assertFalse(subtitles)","self.DL.params['writesubtitles'] = True
self.DL.params['allsubtitles'] = True",self.DL.params['writesubtitles'] = self.DL.params['allsubtitles'] = True,,,,,,,,,,,
suzieq,https://github.com/netenglabs/suzieq/tree/master/suzieq/db/parquet/pq_coalesce.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/suzieq/suzieq/db/parquet/pq_coalesce.py,,"def coalesce_resource_table(infolder: str, outfolder: str, archive_folder: str, table: str, state: SqCoalesceState) -> None:
    """"""This routine coalesces all the parquet data in the folder provided

    This function MUST be called with sqPoller as the table the first time to
    build the polling period sample. Without this, its not possible to compute
    the records to be written for a period accurately. The polling periods are
    computed when this function is called the first time with None as the
    state field. This function stuffs the sqPoller timeblocks as the polling
    period in the state block and returns it. The state object returned also
    has some statistics written such as number of files written, number of
    records written and so on.

    :param infolder: str, folder to read data in from
    :param outfolder: str, folder to write data to
    :param archive_folder: str, folder to store the archived files in
    :param table: str, name of table we're coalesceing
    :param state: SqCoalesceState, state about this coalesceion run
    :returns: Nothing
    """"""

    def compute_block_start(start):
        return start - timedelta(seconds=start.timestamp() % state.period.total_seconds())
    partition_cols = ['sqvers', 'namespace']
    dodel = True
    if table == 'sqPoller':
        wr_polling_period = True
        state.poller_periods = set()
    else:
        wr_polling_period = False
    state.wrfile_count = 0
    state.wrrec_count = 0
    state.table_name = table
    schema = state.schema
    if state.schema.type == 'record':
        state.keys = schema.key_fields()
        if state.current_df.empty:
            state.current_df = get_last_update_df(table, outfolder, state)
    dataset = ds.dataset(infolder, partitioning='hive', format='parquet', ignore_prefixes=state.ign_pfx, schema=schema.get_arrow_schema())
    state.logger.info(f'Examining {len(dataset.files)} {table} files for coalescing')
    fdf = get_file_timestamps(dataset.files)
    if fdf.empty:
        if table == 'sqPoller' or not state.poller_periods:
            return
    polled_periods = sorted(state.poller_periods)
    if fdf.empty:
        state.logger.info(f'No updates for {table} to coalesce')
        start = polled_periods[0]
    else:
        start = fdf.timestamp.iloc[0]
    utcnow = datetime.now(timezone.utc)
    if utcnow < start:
        logging.error('ERROR: Something is off, now is earlier than dates on files')
        return
    block_start = compute_block_start(start)
    block_end = block_start + state.period
    if block_end > utcnow:
        return
    readblock = []
    wrfile_count = 0
    if schema.type == 'record':
        for interval in polled_periods:
            if not fdf.empty and block_end < interval:
                break
            pre_block_start = compute_block_start(interval)
            pre_block_end = pre_block_start + state.period
            write_files(table, readblock, infolder, outfolder, partition_cols, state, pre_block_start, pre_block_end)
    for row in fdf.itertuples():
        if block_start <= row.timestamp < block_end:
            readblock.append(row.file)
            continue
        if readblock or (schema.type == 'record' and block_start in state.poller_periods):
            write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            wrfile_count += len(readblock)
        if wr_polling_period and readblock:
            state.poller_periods.add(block_start)
        if readblock:
            archive_coalesced_files(readblock, archive_folder, state, dodel)
        block_start = block_end
        block_end = block_start + state.period
        readblock = []
        if schema.type != 'record':
            block_start = compute_block_start(row.timestamp)
            block_end = block_start + state.period
            if row.timestamp > block_end or block_end > utcnow:
                break
            readblock = [row.file]
            continue
        while row.timestamp > block_end:
            if block_start in state.poller_periods:
                write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
            block_start = block_end
            block_end = block_start + state.period
        if block_end > utcnow:
            break
        readblock = [row.file]
    if readblock or (fdf.empty and schema.type == 'record' and (block_start in state.poller_periods)):
        write_files(table, readblock, infolder, outfolder, partition_cols, state, block_start, block_end)
        wrfile_count += len(readblock)
        if wr_polling_period:
            state.poller_periods.add(block_start)
        archive_coalesced_files(readblock, archive_folder, state, dodel)
    state.wrfile_count = wrfile_count
    return","state.wrfile_count = 0
state.wrrec_count = 0",state.wrfile_count = state.wrrec_count = 0,,,,,,,,,,,
qutip,https://github.com/qutip/qutip/tree/master/qutip/tests/test_piqs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/qutip/qutip/tests/test_piqs.py,TestDicke,"def test_gamma2(self):
    """"""
        PIQS: Test the calculation of gamma2. PIQS: Test performed for N = 4.
        """"""
    true_gamma_1 = 2
    true_gamma_2 = 1.5
    true_gamma_3 = 5.5
    true_gamma_4 = 0
    true_gamma_5 = 0
    true_gamma_6 = 0
    N = 4
    test_dicke = _Dicke(N=N, collective_emission=1)
    test_gamma_1 = test_dicke.gamma2((1, 1, 1))
    test_dicke = _Dicke(N=N, emission=1)
    test_gamma_2 = test_dicke.gamma2((1, 1, 1))
    test_dicke = _Dicke(N=N, emission=1, collective_emission=2)
    test_gamma_3 = test_dicke.gamma2((1, 1, 1))
    test_dicke = _Dicke(N=N, dephasing=4)
    test_gamma_4 = test_dicke.gamma2((1, 1, 1))
    test_dicke = _Dicke(N=N, collective_pumping=2)
    test_gamma_5 = test_dicke.gamma2((1, 1, 1))
    test_dicke = _Dicke(N=N, collective_dephasing=2)
    test_gamma_6 = test_dicke.gamma2((1, 1, 1))
    assert_almost_equal(true_gamma_1, test_gamma_1)
    assert_almost_equal(true_gamma_2, test_gamma_2)
    assert_almost_equal(true_gamma_3, test_gamma_3)
    assert_almost_equal(true_gamma_4, test_gamma_4)
    assert_almost_equal(true_gamma_5, test_gamma_5)
    assert_almost_equal(true_gamma_6, test_gamma_6)","true_gamma_4 = 0
true_gamma_5 = 0
true_gamma_6 = 0",true_gamma_4 = true_gamma_5 = true_gamma_6 = 0,,,,,,,,,,,
forseti-security,https://github.com/forseti-security/forseti-security/tree/master/tests/services/cli_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/tests/services/cli_test.py,RunExplainerTest,"def test_list_permissions_no_roles_and_no_role_prefixes(self):
    ignored = mock.MagicMock()
    mock_client = mock.MagicMock()
    mock_config = mock.MagicMock()
    mock_config.action = 'list_permissions'
    mock_config.roles = None
    mock_config.role_prefixes = None
    mock_output = mock.MagicMock()
    with self.assertRaises(ValueError) as ctxt:
        cli.run_explainer(mock_client, mock_config, mock_output, ignored)
    self.assertEqual('please specify either a role or a role prefix', str(ctxt.exception))","mock_config.roles = None
mock_config.role_prefixes = None",mock_config.roles = mock_config.role_prefixes = None,,,,,,,,,,,
MegaDepth,https://github.com/zl548/MegaDepth/tree/master/models/HG_model.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MegaDepth/models/HG_model.py,HGModel,"def evaluate_RMSE(self, input_images, prediction_d, targets):
    count = 0
    total_loss = Variable(torch.cuda.FloatTensor(1))
    total_loss[0] = 0
    mask_0 = Variable(targets['mask_0'].cuda(), requires_grad=False)
    d_gt_0 = torch.log(Variable(targets['gt_0'].cuda(), requires_grad=False))
    for i in range(0, mask_0.size(0)):
        total_loss += self.rmse_Loss(prediction_d[i, :, :], mask_0[i, :, :], d_gt_0[i, :, :])
        count += 1
    return (total_loss.data[0], count)","count = 0
total_loss[0] = 0",count = total_loss[0] = 0,,,,,,,,,,,
open_model_zoo,https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker/openvino/tools/accuracy_checker/metrics/speech_recognition.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open_model_zoo/tools/accuracy_checker/openvino/tools/accuracy_checker/metrics/speech_recognition.py,SpeechRecognitionWER,"def configure(self):
    if isinstance(editdistance, UnsupportedPackage):
        editdistance.raise_error(self.__provider__)
    self.words = 0
    self.score = 0
    self.meta['target'] = 'higher-worse'","self.words = 0
self.score = 0",self.words = self.score = 0,,,,,,,,,,,
glance,https://github.com/openstack/glance/tree/master/glance/tests/unit/v2/test_v2_policy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/glance/glance/tests/unit/v2/test_v2_policy.py,APIPolicyBase,"def test_check_is_image_mutable(self):
    context = mock.MagicMock()
    image = mock.MagicMock()
    context.is_admin = True
    context.owner = 'someuser'
    self.assertIsNone(policy.check_is_image_mutable(context, image))
    context.is_admin = False
    image.owner = None
    self.assertRaises(exception.Forbidden, policy.check_is_image_mutable, context, image)
    image.owner = 'someoneelse'
    self.assertRaises(exception.Forbidden, policy.check_is_image_mutable, context, image)
    image.owner = 'someoneelse'
    context.owner = None
    self.assertRaises(exception.Forbidden, policy.check_is_image_mutable, context, image)
    image.owner = 'someuser'
    context.owner = 'someuser'
    self.assertIsNone(policy.check_is_image_mutable(context, image))","image.owner = 'someuser'
context.owner = 'someuser'",image.owner = context.owner = 'someuser',,,,,,,,,,,
MB-Lab,https://github.com/animate1978/MB-Lab/tree/master//animationengine.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MB-Lab//animationengine.py,RetargetEngine,"def add_copy_location(self, target_armat, source_armat, bones_to_move):
    for b in target_armat.pose.bones:
        if b.name in self.skeleton_mapped and b.name in bones_to_move:
            if 'mbastlab_loc' not in b.constraints:
                cstr = b.constraints.new('COPY_LOCATION')
                cstr.target = source_armat
                cstr.subtarget = self.skeleton_mapped[b.name]
                cstr.target_space = 'WORLD'
                cstr.owner_space = 'WORLD'
                cstr.name = 'mbastlab_loc'","cstr.target_space = 'WORLD'
cstr.owner_space = 'WORLD'",cstr.target_space = cstr.owner_space = 'WORLD',,,,,,,,,,,
aws-shell,https://github.com/awslabs/aws-shell/tree/master/tests/unit/test_toolbar.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/aws-shell/tests/unit/test_toolbar.py,ToolbarTest,"def test_toolbar_off(self):
    self.aws_shell.model_completer.match_fuzzy = False
    self.aws_shell.enable_vi_bindings = False
    self.aws_shell.show_completion_columns = False
    self.aws_shell.show_help = False
    self.cli.current_buffer_name = 'DEFAULT_BUFFER'
    expected = [(Token.Toolbar.Off, ' [F2] Fuzzy: OFF '), (Token.Toolbar.On, ' [F3] Keys: Emacs '), (Token.Toolbar.On, ' [F4] Single Column '), (Token.Toolbar.Off, ' [F5] Help: OFF '), (Token.Toolbar, ' [F9] Focus: cli '), (Token.Toolbar, ' [F10] Exit ')]
    assert expected == self.toolbar.handler(self.cli)","self.aws_shell.model_completer.match_fuzzy = False
self.aws_shell.enable_vi_bindings = False
self.aws_shell.show_completion_columns = False
self.aws_shell.show_help = False",self.aws_shell.model_completer.match_fuzzy = self.aws_shell.enable_vi_bindings = self.aws_shell.show_completion_columns = self.aws_shell.show_help = False,,,,,,,,,,,
OpsManage,https://github.com/welliamcao/OpsManage/tree/master/libs/ansible/runner.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OpsManage/libs/ansible/runner.py,ANSRunner,"def run_playbook(self, host_list, playbook_path, extra_vars=dict()):
    """""" 
        run ansible palybook 
        """"""
    total_tasks = self.get_playbook_total_tasks(host_list, playbook_path, extra_vars)
    try:
        self.callback = Playbookcallback(websocket=self.websocket, background=self.background, deplytask=self.deplytask, total_tasks=total_tasks)
        extra_vars['host'] = ','.join(host_list)
        self.variable_manager.extra_vars = extra_vars
        executor = PlaybookExecutor(playbooks=[playbook_path], inventory=self.inventory, variable_manager=self.variable_manager, loader=self.loader, options=self.options, passwords=self.passwords)
        executor._tqm._stdout_callback = self.callback
        constants.HOST_KEY_CHECKING = False
        constants.DEPRECATION_WARNINGS = False
        constants.RETRY_FILES_ENABLED = False
        if extra_vars.get('roles_path'):
            constants.DEFAULT_ROLES_PATH.append(extra_vars.get('roles_path'))
        if extra_vars.get('module_path'):
            constants.DEFAULT_MODULE_PATH.append(extra_vars.get('module_path'))
        executor.run()
    except Exception as err:
        msg = 'run playbook failed: {err}'.format(err=str(err))
        logger.error(msg)
        if self.websocket:
            self.websocket.send(str(err))
        return str(msg)
    return True","constants.HOST_KEY_CHECKING = False
constants.DEPRECATION_WARNINGS = False
constants.RETRY_FILES_ENABLED = False",constants.HOST_KEY_CHECKING = constants.DEPRECATION_WARNINGS = constants.RETRY_FILES_ENABLED = False,,,,,,,,,,,
anki,https://github.com/ankitects/anki/tree/master/qt/aqt/webview.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/anki/qt/aqt/webview.py,AnkiWebView,"def force_load_hack(self) -> None:
    """"""Force process to initialize.
        Must be done on Windows prior to changing current working directory.""""""
    self.requiresCol = False
    self._domReady = False
    self._page.setContent(cast(QByteArray, bytes('', 'ascii')))","self.requiresCol = False
self._domReady = False",self.requiresCol = self._domReady = False,,,,,,,,,,,
cms,https://github.com/amfoss/cms/tree/master/cms/conf.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cms/cms/conf.py,Config,"def __init__(self):
    """"""Default values for configuration, plus decide if this
        instance is running from the system path or from the source
        directory.

        """"""
    self.async_config = async_config
    self.cmsuser = 'cmsuser'
    self.temp_dir = '/tmp'
    self.backdoor = False
    self.file_log_debug = False
    self.stream_log_detailed = False
    self.database = 'postgresql+psycopg2://cmsuser@localhost/cms'
    self.database_debug = False
    self.twophase_commit = False
    self.keep_sandbox = True
    self.use_cgroups = True
    self.sandbox_implementation = 'isolate'
    self.max_file_size = 1024 * 1024
    self.compilation_sandbox_max_processes = 1000
    self.compilation_sandbox_max_time_s = 10.0
    self.compilation_sandbox_max_memory_kib = 512 * 1024
    self.trusted_sandbox_max_processes = 1000
    self.trusted_sandbox_max_time_s = 10.0
    self.trusted_sandbox_max_memory_kib = 4 * 1024 * 1024
    self.secret_key_default = '8e045a51e4b102ea803c06f92841a1fb'
    self.secret_key = self.secret_key_default
    self.tornado_debug = False
    self.contest_listen_address = ['']
    self.contest_listen_port = [8888]
    self.cookie_duration = 30 * 60
    self.submit_local_copy = True
    self.submit_local_copy_path = '%s/submissions/'
    self.tests_local_copy = True
    self.tests_local_copy_path = '%s/tests/'
    self.is_proxy_used = None
    self.num_proxies_used = None
    self.max_submission_length = 100000
    self.max_input_length = 5000000
    self.stl_path = '/usr/share/cppreference/doc/html/'
    self.shared_mime_info_prefix = '/usr'
    self.admin_listen_address = ''
    self.admin_listen_port = 8889
    self.admin_cookie_duration = 10 * 60 * 60
    self.admin_num_proxies_used = None
    self.rankings = ['http://usern4me:passw0rd@localhost:8890/']
    self.https_certfile = None
    self.max_print_length = 10000000
    self.printer = None
    self.paper_size = 'A4'
    self.max_pages_per_job = 10
    self.max_jobs_per_user = 10
    self.pdf_printing_allowed = False
    bin_path = os.path.join(os.getcwd(), sys.argv[0])
    bin_name = os.path.basename(bin_path)
    bin_is_python = bin_name in ['ipython', 'python', 'python2', 'python3']
    bin_in_installed_path = bin_path.startswith(sys.prefix) or (hasattr(sys, 'real_prefix') and bin_path.startswith(sys.real_prefix))
    self.installed = bin_in_installed_path and (not bin_is_python)
    if self.installed:
        self.log_dir = os.path.join('/', 'var', 'local', 'log', 'cms')
        self.cache_dir = os.path.join('/', 'var', 'local', 'cache', 'cms')
        self.data_dir = os.path.join('/', 'var', 'local', 'lib', 'cms')
        self.run_dir = os.path.join('/', 'var', 'local', 'run', 'cms')
        paths = [os.path.join('/', 'usr', 'local', 'etc', 'cms.conf'), os.path.join('/', 'etc', 'cms.conf')]
    else:
        self.log_dir = 'log'
        self.cache_dir = 'cache'
        self.data_dir = 'lib'
        self.run_dir = 'run'
        paths = [os.path.join('.', 'config', 'cms.conf')]
        if '__file__' in globals():
            paths += [os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'config', 'cms.conf'))]
        paths += [os.path.join('/', 'usr', 'local', 'etc', 'cms.conf'), os.path.join('/', 'etc', 'cms.conf')]
    CMS_CONFIG_ENV_VAR = 'CMS_CONFIG'
    if CMS_CONFIG_ENV_VAR in os.environ:
        paths = [os.environ[CMS_CONFIG_ENV_VAR]] + paths
    self._load(paths)
    set_detailed_logs(self.stream_log_detailed)","self.backdoor = False
self.file_log_debug = False
self.stream_log_detailed = False
self.database_debug = False
self.twophase_commit = False
self.tornado_debug = False
self.pdf_printing_allowed = False",self.backdoor = self.file_log_debug = self.stream_log_detailed = self.database_debug = self.twophase_commit = self.tornado_debug = self.pdf_printing_allowed = False,,,,,,,,,,,
cms,https://github.com/amfoss/cms/tree/master/cms/conf.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cms/cms/conf.py,Config,"def __init__(self):
    """"""Default values for configuration, plus decide if this
        instance is running from the system path or from the source
        directory.

        """"""
    self.async_config = async_config
    self.cmsuser = 'cmsuser'
    self.temp_dir = '/tmp'
    self.backdoor = False
    self.file_log_debug = False
    self.stream_log_detailed = False
    self.database = 'postgresql+psycopg2://cmsuser@localhost/cms'
    self.database_debug = False
    self.twophase_commit = False
    self.keep_sandbox = True
    self.use_cgroups = True
    self.sandbox_implementation = 'isolate'
    self.max_file_size = 1024 * 1024
    self.compilation_sandbox_max_processes = 1000
    self.compilation_sandbox_max_time_s = 10.0
    self.compilation_sandbox_max_memory_kib = 512 * 1024
    self.trusted_sandbox_max_processes = 1000
    self.trusted_sandbox_max_time_s = 10.0
    self.trusted_sandbox_max_memory_kib = 4 * 1024 * 1024
    self.secret_key_default = '8e045a51e4b102ea803c06f92841a1fb'
    self.secret_key = self.secret_key_default
    self.tornado_debug = False
    self.contest_listen_address = ['']
    self.contest_listen_port = [8888]
    self.cookie_duration = 30 * 60
    self.submit_local_copy = True
    self.submit_local_copy_path = '%s/submissions/'
    self.tests_local_copy = True
    self.tests_local_copy_path = '%s/tests/'
    self.is_proxy_used = None
    self.num_proxies_used = None
    self.max_submission_length = 100000
    self.max_input_length = 5000000
    self.stl_path = '/usr/share/cppreference/doc/html/'
    self.shared_mime_info_prefix = '/usr'
    self.admin_listen_address = ''
    self.admin_listen_port = 8889
    self.admin_cookie_duration = 10 * 60 * 60
    self.admin_num_proxies_used = None
    self.rankings = ['http://usern4me:passw0rd@localhost:8890/']
    self.https_certfile = None
    self.max_print_length = 10000000
    self.printer = None
    self.paper_size = 'A4'
    self.max_pages_per_job = 10
    self.max_jobs_per_user = 10
    self.pdf_printing_allowed = False
    bin_path = os.path.join(os.getcwd(), sys.argv[0])
    bin_name = os.path.basename(bin_path)
    bin_is_python = bin_name in ['ipython', 'python', 'python2', 'python3']
    bin_in_installed_path = bin_path.startswith(sys.prefix) or (hasattr(sys, 'real_prefix') and bin_path.startswith(sys.real_prefix))
    self.installed = bin_in_installed_path and (not bin_is_python)
    if self.installed:
        self.log_dir = os.path.join('/', 'var', 'local', 'log', 'cms')
        self.cache_dir = os.path.join('/', 'var', 'local', 'cache', 'cms')
        self.data_dir = os.path.join('/', 'var', 'local', 'lib', 'cms')
        self.run_dir = os.path.join('/', 'var', 'local', 'run', 'cms')
        paths = [os.path.join('/', 'usr', 'local', 'etc', 'cms.conf'), os.path.join('/', 'etc', 'cms.conf')]
    else:
        self.log_dir = 'log'
        self.cache_dir = 'cache'
        self.data_dir = 'lib'
        self.run_dir = 'run'
        paths = [os.path.join('.', 'config', 'cms.conf')]
        if '__file__' in globals():
            paths += [os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'config', 'cms.conf'))]
        paths += [os.path.join('/', 'usr', 'local', 'etc', 'cms.conf'), os.path.join('/', 'etc', 'cms.conf')]
    CMS_CONFIG_ENV_VAR = 'CMS_CONFIG'
    if CMS_CONFIG_ENV_VAR in os.environ:
        paths = [os.environ[CMS_CONFIG_ENV_VAR]] + paths
    self._load(paths)
    set_detailed_logs(self.stream_log_detailed)","self.keep_sandbox = True
self.use_cgroups = True
self.submit_local_copy = True
self.tests_local_copy = True",self.keep_sandbox = self.use_cgroups = self.submit_local_copy = self.tests_local_copy = True,,,,,,,,,,,
cms,https://github.com/amfoss/cms/tree/master/cms/conf.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cms/cms/conf.py,Config,"def __init__(self):
    """"""Default values for configuration, plus decide if this
        instance is running from the system path or from the source
        directory.

        """"""
    self.async_config = async_config
    self.cmsuser = 'cmsuser'
    self.temp_dir = '/tmp'
    self.backdoor = False
    self.file_log_debug = False
    self.stream_log_detailed = False
    self.database = 'postgresql+psycopg2://cmsuser@localhost/cms'
    self.database_debug = False
    self.twophase_commit = False
    self.keep_sandbox = True
    self.use_cgroups = True
    self.sandbox_implementation = 'isolate'
    self.max_file_size = 1024 * 1024
    self.compilation_sandbox_max_processes = 1000
    self.compilation_sandbox_max_time_s = 10.0
    self.compilation_sandbox_max_memory_kib = 512 * 1024
    self.trusted_sandbox_max_processes = 1000
    self.trusted_sandbox_max_time_s = 10.0
    self.trusted_sandbox_max_memory_kib = 4 * 1024 * 1024
    self.secret_key_default = '8e045a51e4b102ea803c06f92841a1fb'
    self.secret_key = self.secret_key_default
    self.tornado_debug = False
    self.contest_listen_address = ['']
    self.contest_listen_port = [8888]
    self.cookie_duration = 30 * 60
    self.submit_local_copy = True
    self.submit_local_copy_path = '%s/submissions/'
    self.tests_local_copy = True
    self.tests_local_copy_path = '%s/tests/'
    self.is_proxy_used = None
    self.num_proxies_used = None
    self.max_submission_length = 100000
    self.max_input_length = 5000000
    self.stl_path = '/usr/share/cppreference/doc/html/'
    self.shared_mime_info_prefix = '/usr'
    self.admin_listen_address = ''
    self.admin_listen_port = 8889
    self.admin_cookie_duration = 10 * 60 * 60
    self.admin_num_proxies_used = None
    self.rankings = ['http://usern4me:passw0rd@localhost:8890/']
    self.https_certfile = None
    self.max_print_length = 10000000
    self.printer = None
    self.paper_size = 'A4'
    self.max_pages_per_job = 10
    self.max_jobs_per_user = 10
    self.pdf_printing_allowed = False
    bin_path = os.path.join(os.getcwd(), sys.argv[0])
    bin_name = os.path.basename(bin_path)
    bin_is_python = bin_name in ['ipython', 'python', 'python2', 'python3']
    bin_in_installed_path = bin_path.startswith(sys.prefix) or (hasattr(sys, 'real_prefix') and bin_path.startswith(sys.real_prefix))
    self.installed = bin_in_installed_path and (not bin_is_python)
    if self.installed:
        self.log_dir = os.path.join('/', 'var', 'local', 'log', 'cms')
        self.cache_dir = os.path.join('/', 'var', 'local', 'cache', 'cms')
        self.data_dir = os.path.join('/', 'var', 'local', 'lib', 'cms')
        self.run_dir = os.path.join('/', 'var', 'local', 'run', 'cms')
        paths = [os.path.join('/', 'usr', 'local', 'etc', 'cms.conf'), os.path.join('/', 'etc', 'cms.conf')]
    else:
        self.log_dir = 'log'
        self.cache_dir = 'cache'
        self.data_dir = 'lib'
        self.run_dir = 'run'
        paths = [os.path.join('.', 'config', 'cms.conf')]
        if '__file__' in globals():
            paths += [os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'config', 'cms.conf'))]
        paths += [os.path.join('/', 'usr', 'local', 'etc', 'cms.conf'), os.path.join('/', 'etc', 'cms.conf')]
    CMS_CONFIG_ENV_VAR = 'CMS_CONFIG'
    if CMS_CONFIG_ENV_VAR in os.environ:
        paths = [os.environ[CMS_CONFIG_ENV_VAR]] + paths
    self._load(paths)
    set_detailed_logs(self.stream_log_detailed)","self.compilation_sandbox_max_processes = 1000
self.trusted_sandbox_max_processes = 1000",self.compilation_sandbox_max_processes = self.trusted_sandbox_max_processes = 1000,,,,,,,,,,,
cms,https://github.com/amfoss/cms/tree/master/cms/conf.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cms/cms/conf.py,Config,"def __init__(self):
    """"""Default values for configuration, plus decide if this
        instance is running from the system path or from the source
        directory.

        """"""
    self.async_config = async_config
    self.cmsuser = 'cmsuser'
    self.temp_dir = '/tmp'
    self.backdoor = False
    self.file_log_debug = False
    self.stream_log_detailed = False
    self.database = 'postgresql+psycopg2://cmsuser@localhost/cms'
    self.database_debug = False
    self.twophase_commit = False
    self.keep_sandbox = True
    self.use_cgroups = True
    self.sandbox_implementation = 'isolate'
    self.max_file_size = 1024 * 1024
    self.compilation_sandbox_max_processes = 1000
    self.compilation_sandbox_max_time_s = 10.0
    self.compilation_sandbox_max_memory_kib = 512 * 1024
    self.trusted_sandbox_max_processes = 1000
    self.trusted_sandbox_max_time_s = 10.0
    self.trusted_sandbox_max_memory_kib = 4 * 1024 * 1024
    self.secret_key_default = '8e045a51e4b102ea803c06f92841a1fb'
    self.secret_key = self.secret_key_default
    self.tornado_debug = False
    self.contest_listen_address = ['']
    self.contest_listen_port = [8888]
    self.cookie_duration = 30 * 60
    self.submit_local_copy = True
    self.submit_local_copy_path = '%s/submissions/'
    self.tests_local_copy = True
    self.tests_local_copy_path = '%s/tests/'
    self.is_proxy_used = None
    self.num_proxies_used = None
    self.max_submission_length = 100000
    self.max_input_length = 5000000
    self.stl_path = '/usr/share/cppreference/doc/html/'
    self.shared_mime_info_prefix = '/usr'
    self.admin_listen_address = ''
    self.admin_listen_port = 8889
    self.admin_cookie_duration = 10 * 60 * 60
    self.admin_num_proxies_used = None
    self.rankings = ['http://usern4me:passw0rd@localhost:8890/']
    self.https_certfile = None
    self.max_print_length = 10000000
    self.printer = None
    self.paper_size = 'A4'
    self.max_pages_per_job = 10
    self.max_jobs_per_user = 10
    self.pdf_printing_allowed = False
    bin_path = os.path.join(os.getcwd(), sys.argv[0])
    bin_name = os.path.basename(bin_path)
    bin_is_python = bin_name in ['ipython', 'python', 'python2', 'python3']
    bin_in_installed_path = bin_path.startswith(sys.prefix) or (hasattr(sys, 'real_prefix') and bin_path.startswith(sys.real_prefix))
    self.installed = bin_in_installed_path and (not bin_is_python)
    if self.installed:
        self.log_dir = os.path.join('/', 'var', 'local', 'log', 'cms')
        self.cache_dir = os.path.join('/', 'var', 'local', 'cache', 'cms')
        self.data_dir = os.path.join('/', 'var', 'local', 'lib', 'cms')
        self.run_dir = os.path.join('/', 'var', 'local', 'run', 'cms')
        paths = [os.path.join('/', 'usr', 'local', 'etc', 'cms.conf'), os.path.join('/', 'etc', 'cms.conf')]
    else:
        self.log_dir = 'log'
        self.cache_dir = 'cache'
        self.data_dir = 'lib'
        self.run_dir = 'run'
        paths = [os.path.join('.', 'config', 'cms.conf')]
        if '__file__' in globals():
            paths += [os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'config', 'cms.conf'))]
        paths += [os.path.join('/', 'usr', 'local', 'etc', 'cms.conf'), os.path.join('/', 'etc', 'cms.conf')]
    CMS_CONFIG_ENV_VAR = 'CMS_CONFIG'
    if CMS_CONFIG_ENV_VAR in os.environ:
        paths = [os.environ[CMS_CONFIG_ENV_VAR]] + paths
    self._load(paths)
    set_detailed_logs(self.stream_log_detailed)","self.compilation_sandbox_max_time_s = 10.0
self.trusted_sandbox_max_time_s = 10.0
self.max_pages_per_job = 10
self.max_jobs_per_user = 10","self.compilation_sandbox_max_time_s = self.trusted_sandbox_max_time_s = 10.0
self.max_pages_per_job = self.max_jobs_per_user = 10",,,,,,,,,,,
cms,https://github.com/amfoss/cms/tree/master/cms/conf.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cms/cms/conf.py,Config,"def __init__(self):
    """"""Default values for configuration, plus decide if this
        instance is running from the system path or from the source
        directory.

        """"""
    self.async_config = async_config
    self.cmsuser = 'cmsuser'
    self.temp_dir = '/tmp'
    self.backdoor = False
    self.file_log_debug = False
    self.stream_log_detailed = False
    self.database = 'postgresql+psycopg2://cmsuser@localhost/cms'
    self.database_debug = False
    self.twophase_commit = False
    self.keep_sandbox = True
    self.use_cgroups = True
    self.sandbox_implementation = 'isolate'
    self.max_file_size = 1024 * 1024
    self.compilation_sandbox_max_processes = 1000
    self.compilation_sandbox_max_time_s = 10.0
    self.compilation_sandbox_max_memory_kib = 512 * 1024
    self.trusted_sandbox_max_processes = 1000
    self.trusted_sandbox_max_time_s = 10.0
    self.trusted_sandbox_max_memory_kib = 4 * 1024 * 1024
    self.secret_key_default = '8e045a51e4b102ea803c06f92841a1fb'
    self.secret_key = self.secret_key_default
    self.tornado_debug = False
    self.contest_listen_address = ['']
    self.contest_listen_port = [8888]
    self.cookie_duration = 30 * 60
    self.submit_local_copy = True
    self.submit_local_copy_path = '%s/submissions/'
    self.tests_local_copy = True
    self.tests_local_copy_path = '%s/tests/'
    self.is_proxy_used = None
    self.num_proxies_used = None
    self.max_submission_length = 100000
    self.max_input_length = 5000000
    self.stl_path = '/usr/share/cppreference/doc/html/'
    self.shared_mime_info_prefix = '/usr'
    self.admin_listen_address = ''
    self.admin_listen_port = 8889
    self.admin_cookie_duration = 10 * 60 * 60
    self.admin_num_proxies_used = None
    self.rankings = ['http://usern4me:passw0rd@localhost:8890/']
    self.https_certfile = None
    self.max_print_length = 10000000
    self.printer = None
    self.paper_size = 'A4'
    self.max_pages_per_job = 10
    self.max_jobs_per_user = 10
    self.pdf_printing_allowed = False
    bin_path = os.path.join(os.getcwd(), sys.argv[0])
    bin_name = os.path.basename(bin_path)
    bin_is_python = bin_name in ['ipython', 'python', 'python2', 'python3']
    bin_in_installed_path = bin_path.startswith(sys.prefix) or (hasattr(sys, 'real_prefix') and bin_path.startswith(sys.real_prefix))
    self.installed = bin_in_installed_path and (not bin_is_python)
    if self.installed:
        self.log_dir = os.path.join('/', 'var', 'local', 'log', 'cms')
        self.cache_dir = os.path.join('/', 'var', 'local', 'cache', 'cms')
        self.data_dir = os.path.join('/', 'var', 'local', 'lib', 'cms')
        self.run_dir = os.path.join('/', 'var', 'local', 'run', 'cms')
        paths = [os.path.join('/', 'usr', 'local', 'etc', 'cms.conf'), os.path.join('/', 'etc', 'cms.conf')]
    else:
        self.log_dir = 'log'
        self.cache_dir = 'cache'
        self.data_dir = 'lib'
        self.run_dir = 'run'
        paths = [os.path.join('.', 'config', 'cms.conf')]
        if '__file__' in globals():
            paths += [os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'config', 'cms.conf'))]
        paths += [os.path.join('/', 'usr', 'local', 'etc', 'cms.conf'), os.path.join('/', 'etc', 'cms.conf')]
    CMS_CONFIG_ENV_VAR = 'CMS_CONFIG'
    if CMS_CONFIG_ENV_VAR in os.environ:
        paths = [os.environ[CMS_CONFIG_ENV_VAR]] + paths
    self._load(paths)
    set_detailed_logs(self.stream_log_detailed)","self.is_proxy_used = None
self.num_proxies_used = None
self.admin_num_proxies_used = None
self.https_certfile = None
self.printer = None",self.is_proxy_used = self.num_proxies_used = self.admin_num_proxies_used = self.https_certfile = self.printer = None,,,,,,,,,,,
horizon,https://github.com/openstack/horizon/tree/master/openstack_dashboard/dashboards/admin/defaults/tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/horizon/openstack_dashboard/dashboards/admin/defaults/tests.py,ServicesViewTests,"def test_index(self):
    self.mock_is_volume_service_enabled.return_value = True
    self.mock_is_service_enabled.return_value = True
    compute_quotas = [q.name for q in self.quotas.nova]
    self.mock_enabled_quotas.return_value = compute_quotas
    self.mock_nova_default_quota_get.return_value = self.quotas.nova
    self.mock_cinder_default_quota_get.return_value = self.cinder_quotas.first()
    self.mock_neutron_default_quota_get.return_value = self.neutron_quotas.first()
    res = self.client.get(INDEX_URL)
    self.assertTemplateUsed(res, 'admin/defaults/index.html')
    expected_data = ['<Quota: (injected_file_content_bytes, 1)>', '<Quota: (metadata_items, 1)>', '<Quota: (injected_files, 1)>', '<Quota: (ram, 10000)>', '<Quota: (instances, 10)>', '<Quota: (cores, 10)>', '<Quota: (key_pairs, 100)>', '<Quota: (server_groups, 10)>', '<Quota: (server_group_members, 10)>', '<Quota: (injected_file_path_bytes, 255)>']
    self._check_quotas_data(res, 'compute_quotas', expected_data)
    expected_data = ['<Quota: (gigabytes, 1000)>', '<Quota: (snapshots, 1)>', '<Quota: (volumes, 1)>']
    self._check_quotas_data(res, 'volume_quotas', expected_data)
    expected_data = ['<Quota: (network, 10)>', '<Quota: (subnet, 10)>', '<Quota: (port, 50)>', '<Quota: (router, 10)>', '<Quota: (floatingip, 50)>', '<Quota: (security_group, 20)>', '<Quota: (security_group_rule, 100)>']
    self._check_quotas_data(res, 'network_quotas', expected_data)
    self.mock_is_volume_service_enabled.assert_called_once_with(test.IsHttpRequest())
    self.assertEqual(2, self.mock_is_service_enabled.call_count)
    self.mock_is_service_enabled.assert_has_calls([mock.call(test.IsHttpRequest(), 'compute'), mock.call(test.IsHttpRequest(), 'network')])
    self.assert_mock_multiple_calls_with_same_arguments(self.mock_enabled_quotas, 4, mock.call(test.IsHttpRequest()))
    self.mock_nova_default_quota_get.assert_called_once_with(test.IsHttpRequest(), self.tenant.id)
    self.mock_cinder_default_quota_get.assert_called_once_with(test.IsHttpRequest(), self.tenant.id)
    self.mock_neutron_default_quota_get.assert_called_once_with(test.IsHttpRequest())","self.mock_is_volume_service_enabled.return_value = True
self.mock_is_service_enabled.return_value = True",self.mock_is_volume_service_enabled.return_value = self.mock_is_service_enabled.return_value = True,,,,,,,,,,,
Project_CodeNet,https://github.com/IBM/Project_CodeNet/tree/master/model-experiments/token-based-similarity-classification/src/Dataset/TokensSimilDS.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Project_CodeNet/model-experiments/token-based-similarity-classification/src/Dataset/TokensSimilDS.py,SimilarityDSMaker,"def __init__(self, dir_name, min_n_solutions=1, problem_list=None, max_n_problems=None, short_code_th=4, long_code_th=None, test=0, labels01=True):
    """"""
        Initialize Dataset maker
        Parameters:
        - dir_name  -- directory with files of tokenized source code files
        - min_n_solutions -- min number of solutions of problem 
                             required to load that problem
        - problem_list    -- list of problem to process
                             If it is not defined the problem are
                             selected from all tokenized problems
                             using specified criteria
        - max_n_problems  -- maximum number of problems to load
                             If it is None 
                             all the specified problems are loaded
        - short_code_th   -- minimum length of code to load
        - long_code_th    -- max length of solution code to load
                              if it is None sys.maxsize is used
        - test            -- amount of problems reserved for test dataset
                             if test = 0 no problems test dataset is not created
                             if test < 1 it defines fraction of all problems 
                             if test > 1 it defines number of all problems
        - labels01        -- label types flag:
                             True:   0/1 labels
                             Flase:  -1/+1 labels
        """"""
    super(SimilarityDSMaker, self).__init__(dir_name, min_n_solutions=min_n_solutions, problem_list=problem_list, max_n_problems=max_n_problems, short_code_th=short_code_th, long_code_th=long_code_th)
    self.labels01 = labels01
    (self.problems_solutions, self.solution_names) = self.getPartitionedSampes()
    self.n_problems = len(self.problems_solutions)
    self.n_test_problems = int(float(self.n_problems) * test) if test < 1 else int(test)
    if test and self.n_test_problems == 1:
        sys.exit(f'Too few {test} problems for test dataset')
    self.n_tran_ds_problems = self.n_problems - self.n_test_problems
    if self.n_test_problems > 0:
        self.test_problems = self.problems[-self.n_test_problems:]
        self.writeProblemList(self.test_problems, 'test_problems.txt')
        self.train_ds_problems = self.problems[:-self.n_test_problems]
        self.test_problem_solutions = self.problems_solutions[-self.n_test_problems:]
        self.train_ds_probl_solutions = self.problems_solutions[:-self.n_test_problems]
        print(f'Dataset of {self.n_problems} problems is split ' + f'into {self.n_test_problems} and ' + f'{self.n_tran_ds_problems} for test and training with validation')
    else:
        self.test_problems = None
        self.train_ds_problems = self.problems
        self.test_problem_solutions = None
        self.train_ds_probl_solutions = self.problems_solutions
        print('No test dataset was defined')
    self._SEL_SIZE = 128 * 1024
    self.writeProblemList(self.train_ds_problems, 'train_valid_problems.txt')
    self.writeProblemList(self.problems, 'all_problems.txt')","self.test_problems = None
self.test_problem_solutions = None",self.test_problems = self.test_problem_solutions = None,,,,,,,,,,,
virt-manager,https://github.com/virt-manager/virt-manager/tree/master/virtManager/virtmanager.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/virt-manager/virtManager/virtmanager.py,,"def main():
    (options, leftovers) = parse_commandline()
    cli.setupLogging('virt-manager', options.debug, False, False)
    log.debug('virt-manager version: %s', BuildConfig.version)
    log.debug('virtManager import: %s', os.path.dirname(__file__))
    if BuildConfig.running_from_srcdir:
        _setup_gsettings_path(BuildConfig.gsettings_dir)
    if options.trace_libvirt:
        log.debug('Libvirt tracing requested')
        from .lib import module_trace
        import libvirt
        module_trace.wrap_module(libvirt, mainloop=options.trace_libvirt == 'mainloop', regex=None)
    CLITestOptions = CLITestOptionsClass(options.test_options)
    os.environ['GSETTINGS_SCHEMA_DIR'] = BuildConfig.gsettings_dir
    do_drop_stdio = False
    if not options.no_fork and (not options.debug):
        drop_tty()
        do_drop_stdio = True
        signal.signal(signal.SIGHUP, signal.SIG_IGN)
    leftovers = _import_gtk(leftovers)
    Gtk = globals()['Gtk']
    if do_drop_stdio:
        drop_stdio()
    if leftovers:
        raise RuntimeError(""Unhandled command line options '%s'"" % leftovers)
    log.debug('PyGObject version: %d.%d.%d', gi.version_info[0], gi.version_info[1], gi.version_info[2])
    log.debug('GTK version: %d.%d.%d', Gtk.get_major_version(), Gtk.get_minor_version(), Gtk.get_micro_version())
    from . import config
    config.vmmConfig.get_instance(BuildConfig, CLITestOptions)
    icon_theme = Gtk.IconTheme.get_default()
    icon_theme.prepend_search_path(BuildConfig.icon_dir)
    from .engine import vmmEngine
    Gtk.Window.set_default_icon_name('virt-manager')
    show_window = None
    domain = None
    if options.show_domain_creator:
        show_window = vmmEngine.CLI_SHOW_DOMAIN_CREATOR
    elif options.show_host_summary:
        show_window = vmmEngine.CLI_SHOW_HOST_SUMMARY
    elif options.show_domain_editor:
        show_window = vmmEngine.CLI_SHOW_DOMAIN_EDITOR
        domain = options.show_domain_editor
    elif options.show_domain_performance:
        show_window = vmmEngine.CLI_SHOW_DOMAIN_PERFORMANCE
        domain = options.show_domain_performance
    elif options.show_domain_console:
        show_window = vmmEngine.CLI_SHOW_DOMAIN_CONSOLE
        domain = options.show_domain_console
    elif options.show_domain_delete:
        show_window = vmmEngine.CLI_SHOW_DOMAIN_DELETE
        domain = options.show_domain_delete
    if show_window and options.uri is None:
        raise RuntimeError(""can't use --show-* options without --connect"")
    skip_autostart = False
    if show_window:
        skip_autostart = True
    LibvirtGLib.init(None)
    LibvirtGLib.event_register()
    engine = vmmEngine.get_instance()
    from gi.repository import GLib

    def _sigint_handler(user_data):
        ignore = user_data
        log.debug('Received KeyboardInterrupt. Exiting application.')
        engine.exit_app()
    GLib.unix_signal_add(GLib.PRIORITY_DEFAULT, signal.SIGINT, _sigint_handler, None)
    engine.start(options.uri, show_window, domain, skip_autostart)","show_window = None
domain = None",show_window = domain = None,,,,,,,,,,,
pydicom,https://github.com/pydicom/pydicom/tree/master/pydicom/tests/test_handler_util.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pydicom/pydicom/tests/test_handler_util.py,TestNumpy_ApplyVOILUT,"def test_voi_windowing(self):
    """"""Test both LUT and windowing operation elements present.""""""
    ds = Dataset()
    ds.PhotometricInterpretation = 'MONOCHROME1'
    ds.PixelRepresentation = 0
    ds.BitsStored = 8
    ds.WindowWidth = 1
    ds.WindowCenter = 0
    ds.VOILUTSequence = [Dataset()]
    item = ds.VOILUTSequence[0]
    item.LUTDescriptor = [4, 0, 8]
    item.LUTData = [0, 127, 128, 255]
    arr = np.asarray([0, 1, 128, 254, 255], dtype='uint8')
    out = apply_voi_lut(arr, ds)
    assert [0, 127, 255, 255, 255] == out.tolist()
    out = apply_voi_lut(arr, ds, prefer_lut=False)
    assert [255, 255, 255, 255, 255] == out.tolist()","ds.PixelRepresentation = 0
ds.WindowCenter = 0",ds.PixelRepresentation = ds.WindowCenter = 0,,,,,,,,,,,
conan-center-index,https://github.com/conan-io/conan-center-index/tree/master/recipes/kealib/all/conanfile.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/conan-center-index/recipes/kealib/all/conanfile.py,KealibConan,"def _configure_cmake(self):
    if self._cmake:
        return self._cmake
    self._cmake = CMake(self)
    self._cmake.definitions['HDF5_USE_STATIC_LIBRARIES'] = not self.options['hdf5'].shared
    self._cmake.definitions['HDF5_PREFER_PARALLEL'] = False
    self._cmake.definitions['LIBKEA_WITH_GDAL'] = False
    self._cmake.configure(build_folder=self._build_subfolder)
    return self._cmake","self._cmake.definitions['HDF5_PREFER_PARALLEL'] = False
self._cmake.definitions['LIBKEA_WITH_GDAL'] = False",self._cmake.definitions['HDF5_PREFER_PARALLEL'] = self._cmake.definitions['LIBKEA_WITH_GDAL'] = False,,,,,,,,,,,
pvcnn,https://github.com/mit-han-lab/pvcnn/tree/master/meters/kitti/frustum.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pvcnn/meters/kitti/frustum.py,MeterFrustumKitti,"def reset(self):
    self.total_seen_num = 0
    self.total_correct_num = 0
    self.iou_3d_corrent_num = 0
    self.iou_2d_sum = 0
    self.iou_3d_sum = 0
    self.iou_3d_corrent_num_per_class = {cls: 0 for cls in self.class_name_to_class_id.keys()}
    self.total_seen_num_per_class = {cls: 0 for cls in self.class_name_to_class_id.keys()}","self.total_seen_num = 0
self.total_correct_num = 0
self.iou_3d_corrent_num = 0
self.iou_2d_sum = 0
self.iou_3d_sum = 0",self.total_seen_num = self.total_correct_num = self.iou_3d_corrent_num = self.iou_2d_sum = self.iou_3d_sum = 0,,,,,,,,,,,
pfrl,https://github.com/pfnet/pfrl/tree/master/pfrl/agents/state_q_function_actor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pfrl/pfrl/agents/state_q_function_actor.py,StateQFunctionActor,"def __init__(self, pipe, model, explorer, phi=lambda x: x, recurrent=False, logger=getLogger(__name__), batch_states=batch_states):
    self.pipe = pipe
    self.model = model
    self.explorer = explorer
    self.phi = phi
    self.recurrent = recurrent
    self.logger = logger
    self.batch_states = batch_states
    self.t = 0
    self.last_state = None
    self.last_action = None
    self.train_recurrent_states = None
    self.train_prev_recurrent_states = None
    self.test_recurrent_states = None","self.last_state = None
self.last_action = None
self.train_recurrent_states = None
self.train_prev_recurrent_states = None
self.test_recurrent_states = None",self.last_state = self.last_action = self.train_recurrent_states = self.train_prev_recurrent_states = self.test_recurrent_states = None,,,,,,,,,,,
edx-platform,https://github.com/edx/edx-platform/tree/master/openedx/core/djangoapps/programs/tests/test_tasks.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/edx-platform/openedx/core/djangoapps/programs/tests/test_tasks.py,UpdateCertificateAvailableDateOnCourseUpdateTestCase,"def test_update_certificate_available_date_with_expect_no_update(self):
    """"""
        This test verifies that we do _not_ queue a task to update the course certificate configuration in Credentials
        if the course-run does not meet the required criteria.
        """"""
    self.credentials_api_config.enabled = True
    self.credentials_api_config.enable_learner_issuance = True
    available_date = datetime.now(pytz.UTC) + timedelta(days=1)
    course = CourseOverviewFactory.create(self_paced=False, certificate_available_date=available_date, certificates_display_behavior=CertificatesDisplayBehaviors.EARLY_NO_INFO)
    expected_message = f'Skipping update of the `certificate_available_date` for course {course.id} in the Credentials service. This course-run does not meet the required criteria for an update.'
    with LogCapture(level=logging.WARNING) as log_capture:
        tasks.update_certificate_available_date_on_course_update(course.id)
    assert len(log_capture.records) == 1
    assert log_capture.records[0].getMessage() == expected_message","self.credentials_api_config.enabled = True
self.credentials_api_config.enable_learner_issuance = True",self.credentials_api_config.enabled = self.credentials_api_config.enable_learner_issuance = True,,,,,,,,,,,
crossbar,https://github.com/crossbario/crossbar/tree/master/crossbar/router/session.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/crossbar/crossbar/router/session.py,RouterSession,"def __init__(self, router_factory):
    """"""

        :param router_factory: The router factory this session is created from. This is different from
            the :class:`crossbar.router.session.RouterSessionFactory` stored in ``self.factory``.
        :type router_factory: Instance of :class:`crossbar.router.router.RouterFactory`.
        """"""
    super(RouterSession, self).__init__()
    self._transport = None
    self._router_factory = router_factory
    self._router = None
    self._realm = None
    self._testaments = {'destroyed': [], 'detached': []}
    self._goodbye_sent = False
    self._transport_is_closing = False
    self._session_details = None
    self._service_session = None","self._transport = None
self._router = None
self._realm = None
self._session_details = None
self._service_session = None",self._transport = self._router = self._realm = self._session_details = self._service_session = None,,,,,,,,,,,
crossbar,https://github.com/crossbario/crossbar/tree/master/crossbar/router/session.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/crossbar/crossbar/router/session.py,RouterSession,"def __init__(self, router_factory):
    """"""

        :param router_factory: The router factory this session is created from. This is different from
            the :class:`crossbar.router.session.RouterSessionFactory` stored in ``self.factory``.
        :type router_factory: Instance of :class:`crossbar.router.router.RouterFactory`.
        """"""
    super(RouterSession, self).__init__()
    self._transport = None
    self._router_factory = router_factory
    self._router = None
    self._realm = None
    self._testaments = {'destroyed': [], 'detached': []}
    self._goodbye_sent = False
    self._transport_is_closing = False
    self._session_details = None
    self._service_session = None","self._goodbye_sent = False
self._transport_is_closing = False",self._goodbye_sent = self._transport_is_closing = False,,,,,,,,,,,
OWOD,https://github.com/JosephKJ/OWOD/tree/master/tests/data/test_coco.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/OWOD/tests/data/test_coco.py,,"def uncompressed_rle(mask):
    l = mask.flatten(order='F').tolist()
    counts = []
    p = False
    cnt = 0
    for i in l:
        if i == p:
            cnt += 1
        else:
            counts.append(cnt)
            p = i
            cnt = 1
    counts.append(cnt)
    return {'counts': counts, 'size': [mask.shape[0], mask.shape[1]]}","p = False
cnt = 0",p = cnt = False,,,,,,,,,,,
yt-dlp,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/videa.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlp/yt_dlp/extractor/videa.py,VideaIE,"def rc4(cipher_text, key):
    res = b''
    key_len = len(key)
    S = list(range(256))
    j = 0
    for i in range(256):
        j = (j + S[i] + ord(key[i % key_len])) % 256
        (S[i], S[j]) = (S[j], S[i])
    i = 0
    j = 0
    for m in range(len(cipher_text)):
        i = (i + 1) % 256
        j = (j + S[i]) % 256
        (S[i], S[j]) = (S[j], S[i])
        k = S[(S[i] + S[j]) % 256]
        res += compat_struct_pack('B', k ^ compat_ord(cipher_text[m]))
    return res.decode()","i = 0
j = 0",i = j = 0,,,,,,,,,,,
CornerNet-Lite,https://github.com/princeton-vl/CornerNet-Lite/tree/master/core/dbs/base.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/CornerNet-Lite/core/dbs/base.py,BASE,"def __init__(self):
    self._split = None
    self._db_inds = []
    self._image_ids = []
    self._mean = np.zeros((3,), dtype=np.float32)
    self._std = np.ones((3,), dtype=np.float32)
    self._eig_val = np.ones((3,), dtype=np.float32)
    self._eig_vec = np.zeros((3, 3), dtype=np.float32)
    self._configs = {}
    self._configs['data_aug'] = True
    self._data_rng = None","self._split = None
self._data_rng = None",self._split = self._data_rng = None,,,,,,,,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/tools/sampcd_processor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/tools/sampcd_processor.py,,"def insert_codes_into_codeblock(codeblock, apiname='not-specified'):
    """"""
    insert some codes in the frontend and backend into the code-block.
    """"""
    global ENV_KEY_CODES_FRONTEND, GPU_ID, RUN_ON_DEVICE
    inserted_codes_f = ''
    inserted_codes_b = ''
    if ENV_KEY_CODES_FRONTEND in os.environ and os.environ[ENV_KEY_CODES_FRONTEND]:
        inserted_codes_f = os.environ[ENV_KEY_CODES_FRONTEND]
    else:
        cpu_str = '\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = """"\n'
        gpu_str = '\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""{}""\n'.format(GPU_ID)
        if 'required' in codeblock and codeblock['required']:
            if codeblock['required'] == 'cpu':
                inserted_codes_f = cpu_str
            elif codeblock['required'] == 'gpu':
                inserted_codes_f = gpu_str
        elif RUN_ON_DEVICE == 'cpu':
            inserted_codes_f = cpu_str
        elif RUN_ON_DEVICE == 'gpu':
            inserted_codes_f = gpu_str
    inserted_codes_b = '\nprint(""{}\'s sample code (name:{}, id:{}) is executed successfully!"")'.format(apiname, codeblock['name'], codeblock['id'])
    cb = codeblock['codes']
    last_future_line_end = find_last_future_line_end(cb)
    if last_future_line_end:
        return cb[:last_future_line_end] + inserted_codes_f + cb[last_future_line_end:] + inserted_codes_b
    else:
        return inserted_codes_f + cb + inserted_codes_b","inserted_codes_f = ''
inserted_codes_b = ''",inserted_codes_f = inserted_codes_b = '',,,,,,,,,,,
torba,https://github.com/lbryio/torba/tree/master/torba/server/coins.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/torba/torba/server/coins.py,Namecoin,"def split_name_script(cls, script):
    from torba.server.script import _match_ops, Script, ScriptError
    try:
        ops = Script.get_ops(script)
    except ScriptError:
        return (None, script)
    match = _match_ops
    OP_NAME_NEW = OpCodes.OP_1
    OP_NAME_FIRSTUPDATE = OpCodes.OP_2
    OP_NAME_UPDATE = OpCodes.OP_3
    NAME_NEW_OPS = [OP_NAME_NEW, -1, OpCodes.OP_2DROP]
    NAME_FIRSTUPDATE_OPS = [OP_NAME_FIRSTUPDATE, -1, -1, -1, OpCodes.OP_2DROP, OpCodes.OP_2DROP]
    NAME_UPDATE_OPS = [OP_NAME_UPDATE, -1, -1, OpCodes.OP_2DROP, OpCodes.OP_DROP]
    name_script_op_count = None
    name_pushdata = None
    if match(ops[:len(NAME_NEW_OPS)], NAME_NEW_OPS):
        name_script_op_count = len(NAME_NEW_OPS)
    elif match(ops[:len(NAME_FIRSTUPDATE_OPS)], NAME_FIRSTUPDATE_OPS):
        name_script_op_count = len(NAME_FIRSTUPDATE_OPS)
        name_pushdata = ops[1]
    elif match(ops[:len(NAME_UPDATE_OPS)], NAME_UPDATE_OPS):
        name_script_op_count = len(NAME_UPDATE_OPS)
        name_pushdata = ops[1]
    if name_script_op_count is None:
        return (None, script)
    n = 0
    for i in range(name_script_op_count):
        op = script[n]
        n += 1
        if op <= OpCodes.OP_PUSHDATA4:
            if op < OpCodes.OP_PUSHDATA1:
                dlen = op
            elif op == OpCodes.OP_PUSHDATA1:
                dlen = script[n]
                n += 1
            elif op == OpCodes.OP_PUSHDATA2:
                (dlen,) = struct.unpack('<H', script[n:n + 2])
                n += 2
            else:
                (dlen,) = struct.unpack('<I', script[n:n + 4])
                n += 4
            if n + dlen > len(script):
                raise IndexError
            op = (op, script[n:n + dlen])
            n += dlen
    address_script = script[n:]
    if name_pushdata is None:
        return (None, address_script)
    normalized_name_op_script = bytearray()
    normalized_name_op_script.append(OP_NAME_UPDATE)
    normalized_name_op_script.extend(Script.push_data(name_pushdata[1]))
    normalized_name_op_script.extend(Script.push_data(bytes([])))
    normalized_name_op_script.append(OpCodes.OP_2DROP)
    normalized_name_op_script.append(OpCodes.OP_DROP)
    normalized_name_op_script.append(OpCodes.OP_RETURN)
    return (bytes(normalized_name_op_script), address_script)","name_script_op_count = None
name_pushdata = None",name_script_op_count = name_pushdata = None,,,,,,,,,,,
kamene,https://github.com/phaethon/kamene/tree/master/kamene/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/utils.py,,"def hexdiff(x, y):
    """"""Show differences between 2 binary strings""""""
    x = any2b(x)[::-1]
    y = any2b(y)[::-1]
    SUBST = 1
    INSERT = 1
    d = {}
    d[-1, -1] = (0, (-1, -1))
    for j in range(len(y)):
        d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))
    for i in range(len(x)):
        d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))
    for j in range(len(y)):
        for i in range(len(x)):
            d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))
    backtrackx = []
    backtracky = []
    i = len(x) - 1
    j = len(y) - 1
    while not i == j == -1:
        (i2, j2) = d[i, j][1]
        backtrackx.append(x[i2 + 1:i + 1])
        backtracky.append(y[j2 + 1:j + 1])
        (i, j) = (i2, j2)
    x = y = i = 0
    colorize = {0: lambda x: x, -1: conf.color_theme.left, 1: conf.color_theme.right}
    dox = 1
    doy = 0
    l = len(backtrackx)
    while i < l:
        separate = 0
        linex = backtrackx[i:i + 16]
        liney = backtracky[i:i + 16]
        xx = sum((len(k) for k in linex))
        yy = sum((len(k) for k in liney))
        if dox and (not xx):
            dox = 0
            doy = 1
        if dox and linex == liney:
            doy = 1
        if dox:
            xd = y
            j = 0
            while not linex[j]:
                j += 1
                xd -= 1
            print(colorize[doy - dox]('%04x' % xd), end=' ')
            x += xx
            line = linex
        else:
            print('    ', end=' ')
        if doy:
            yd = y
            j = 0
            while not liney[j]:
                j += 1
                yd -= 1
            print(colorize[doy - dox]('%04x' % yd), end=' ')
            y += yy
            line = liney
        else:
            print('    ', end=' ')
        print(' ', end=' ')
        cl = ''
        for j in range(16):
            if i + j < l:
                if line[j]:
                    col = colorize[(linex[j] != liney[j]) * (doy - dox)]
                    print(col('%02X' % line[j][0]), end=' ')
                    if linex[j] == liney[j]:
                        cl += sane_color(line[j])
                    else:
                        cl += col(sane(line[j]))
                else:
                    print('  ', end=' ')
                    cl += ' '
            else:
                print('  ', end=' ')
            if j == 7:
                print('', end=' ')
        print(' ', cl)
        if doy or not yy:
            doy = 0
            dox = 1
            i += 16
        elif yy:
            dox = 0
            doy = 1
        else:
            i += 16","SUBST = 1
INSERT = 1",SUBST = INSERT = 1,,,,,,,,,,,
kamene,https://github.com/phaethon/kamene/tree/master/kamene/utils.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kamene/kamene/utils.py,,"def hexdiff(x, y):
    """"""Show differences between 2 binary strings""""""
    x = any2b(x)[::-1]
    y = any2b(y)[::-1]
    SUBST = 1
    INSERT = 1
    d = {}
    d[-1, -1] = (0, (-1, -1))
    for j in range(len(y)):
        d[-1, j] = (d[-1, j - 1][0] + INSERT, (-1, j - 1))
    for i in range(len(x)):
        d[i, -1] = (d[i - 1, -1][0] + INSERT, (i - 1, -1))
    for j in range(len(y)):
        for i in range(len(x)):
            d[i, j] = min((d[i - 1, j - 1][0] + SUBST * (x[i] != y[j]), (i - 1, j - 1)), (d[i - 1, j][0] + INSERT, (i - 1, j)), (d[i, j - 1][0] + INSERT, (i, j - 1)))
    backtrackx = []
    backtracky = []
    i = len(x) - 1
    j = len(y) - 1
    while not i == j == -1:
        (i2, j2) = d[i, j][1]
        backtrackx.append(x[i2 + 1:i + 1])
        backtracky.append(y[j2 + 1:j + 1])
        (i, j) = (i2, j2)
    x = y = i = 0
    colorize = {0: lambda x: x, -1: conf.color_theme.left, 1: conf.color_theme.right}
    dox = 1
    doy = 0
    l = len(backtrackx)
    while i < l:
        separate = 0
        linex = backtrackx[i:i + 16]
        liney = backtracky[i:i + 16]
        xx = sum((len(k) for k in linex))
        yy = sum((len(k) for k in liney))
        if dox and (not xx):
            dox = 0
            doy = 1
        if dox and linex == liney:
            doy = 1
        if dox:
            xd = y
            j = 0
            while not linex[j]:
                j += 1
                xd -= 1
            print(colorize[doy - dox]('%04x' % xd), end=' ')
            x += xx
            line = linex
        else:
            print('    ', end=' ')
        if doy:
            yd = y
            j = 0
            while not liney[j]:
                j += 1
                yd -= 1
            print(colorize[doy - dox]('%04x' % yd), end=' ')
            y += yy
            line = liney
        else:
            print('    ', end=' ')
        print(' ', end=' ')
        cl = ''
        for j in range(16):
            if i + j < l:
                if line[j]:
                    col = colorize[(linex[j] != liney[j]) * (doy - dox)]
                    print(col('%02X' % line[j][0]), end=' ')
                    if linex[j] == liney[j]:
                        cl += sane_color(line[j])
                    else:
                        cl += col(sane(line[j]))
                else:
                    print('  ', end=' ')
                    cl += ' '
            else:
                print('  ', end=' ')
            if j == 7:
                print('', end=' ')
        print(' ', cl)
        if doy or not yy:
            doy = 0
            dox = 1
            i += 16
        elif yy:
            dox = 0
            doy = 1
        else:
            i += 16","x = y = i = 0
doy = 0",x = y = i = doy = 0,,,,,,,,,,,
tensorflow-rnn-shakespeare,https://github.com/martin-gorner/tensorflow-rnn-shakespeare/tree/master//tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tensorflow-rnn-shakespeare//tests.py,RnnMinibatchSequencerTest,"def test_batches(self):
    start = True
    prev_x = np.zeros([TST_BATCHSIZE, TST_SEQLEN], np.int32)
    prev_y = np.zeros([TST_BATCHSIZE, TST_SEQLEN], np.int32)
    nb_errors = 0
    nb_batches = 0
    for (x, y, epoch) in txt.rnn_minibatch_sequencer(self.data, TST_BATCHSIZE, TST_SEQLEN, TST_EPOCHS):
        if not start:
            nb_errors += self.check_seq_batch(prev_x, x)
            nb_errors += self.check_seq_batch(prev_y, y)
        prev_x = x
        prev_y = y
        start = False
        nb_batches += 1
    self.assertLessEqual(nb_errors, 2 * TST_EPOCHS, msg='Sequences should be correctly continued, even between epochs. Only one sequence is allowed to not continue from one epoch to the next.')
    self.assertLess(TST_TXTSIZE - nb_batches * TST_BATCHSIZE * TST_SEQLEN, TST_BATCHSIZE * TST_SEQLEN * TST_EPOCHS, msg='Text ignored at the end of an epoch must be smaller than one batch of sequences')","nb_errors = 0
nb_batches = 0",nb_errors = nb_batches = 0,,,,,,,,,,,
robotics-rl-srl,https://github.com/araffin/robotics-rl-srl/tree/master/environments/kuka_gym/kuka_button_gym_env.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/robotics-rl-srl/environments/kuka_gym/kuka_button_gym_env.py,KukaButtonGymEnv,"def __init__(self, urdf_root=pybullet_data.getDataPath(), renders=False, is_discrete=True, multi_view=False, name='kuka_button_gym', max_distance=0.8, action_repeat=1, shape_reward=False, action_joints=False, record_data=False, random_target=False, force_down=True, state_dim=-1, learn_states=False, verbose=False, save_path='srl_zoo/data/', env_rank=0, srl_pipe=None, srl_model='raw_pixels', **_):
    super(KukaButtonGymEnv, self).__init__(srl_model=srl_model, relative_pos=RELATIVE_POS, env_rank=env_rank, srl_pipe=srl_pipe)
    self._timestep = 1.0 / 240.0
    self._urdf_root = urdf_root
    self._action_repeat = action_repeat
    self._observation = []
    self._env_step_counter = 0
    self._renders = renders
    self._width = RENDER_WIDTH
    self._height = RENDER_HEIGHT
    self._cam_dist = 1.1
    self._cam_yaw = 145
    self._cam_pitch = -36
    self._cam_roll = 0
    self._max_distance = max_distance
    self._shape_reward = shape_reward
    self._random_target = random_target
    self._force_down = force_down
    self.camera_target_pos = (0.316, -0.2, -0.1)
    self._is_discrete = is_discrete
    self.terminated = False
    self.renderer = p.ER_TINY_RENDERER
    self.debug = False
    self.n_contacts = 0
    self.state_dim = state_dim
    self.action_joints = action_joints
    self.relative_pos = RELATIVE_POS
    self.cuda = th.cuda.is_available()
    self.saver = None
    self.multi_view = multi_view
    self.verbose = verbose
    self.max_steps = MAX_STEPS
    self.n_steps_outside = 0
    self.table_uid = None
    self.button_pos = None
    self.button_uid = None
    self._kuka = None
    self.action = None
    self.srl_model = srl_model
    if record_data:
        self.saver = EpisodeSaver(name, max_distance, state_dim, globals_=getGlobals(), relative_pos=RELATIVE_POS, learn_states=learn_states, path=save_path)
    if self._renders:
        client_id = p.connect(p.SHARED_MEMORY)
        if client_id < 0:
            p.connect(p.GUI)
        p.resetDebugVisualizerCamera(1.3, 180, -41, [0.52, -0.2, -0.33])
        self.debug = True
        self.x_slider = p.addUserDebugParameter('x_slider', -10, 10, self.camera_target_pos[0])
        self.y_slider = p.addUserDebugParameter('y_slider', -10, 10, self.camera_target_pos[1])
        self.z_slider = p.addUserDebugParameter('z_slider', -10, 10, self.camera_target_pos[2])
        self.dist_slider = p.addUserDebugParameter('cam_dist', 0, 10, self._cam_dist)
        self.yaw_slider = p.addUserDebugParameter('cam_yaw', -180, 180, self._cam_yaw)
        self.pitch_slider = p.addUserDebugParameter('cam_pitch', -180, 180, self._cam_pitch)
    else:
        p.connect(p.DIRECT)
    global CONNECTED_TO_SIMULATOR
    CONNECTED_TO_SIMULATOR = True
    if self._is_discrete:
        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)
    else:
        if self.action_joints:
            action_dim = 7
            self._action_bound = 1
        else:
            action_dim = 3
            self._action_bound = 1
        action_high = np.array([self._action_bound] * action_dim)
        self.action_space = spaces.Box(-action_high, action_high, dtype=np.float32)
    if self.srl_model == 'ground_truth':
        self.state_dim = self.getGroundTruthDim()
    elif self.srl_model == 'joints':
        self.state_dim = self.getJointsDim()
    elif self.srl_model == 'joints_position':
        self.state_dim = self.getGroundTruthDim() + self.getJointsDim()
    if self.srl_model == 'raw_pixels':
        self.observation_space = spaces.Box(low=0, high=255, shape=(self._height, self._width, 3), dtype=np.uint8)
    else:
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.state_dim,), dtype=np.float32)","self._env_step_counter = 0
self._cam_roll = 0
self.terminated = False
self.debug = False
self.n_contacts = 0
self.n_steps_outside = 0","self._env_step_counter = self._cam_roll = self.n_contacts = self.n_steps_outside = 0
self.terminated = self.debug = False",,,,,,,,,,,
robotics-rl-srl,https://github.com/araffin/robotics-rl-srl/tree/master/environments/kuka_gym/kuka_button_gym_env.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/robotics-rl-srl/environments/kuka_gym/kuka_button_gym_env.py,KukaButtonGymEnv,"def __init__(self, urdf_root=pybullet_data.getDataPath(), renders=False, is_discrete=True, multi_view=False, name='kuka_button_gym', max_distance=0.8, action_repeat=1, shape_reward=False, action_joints=False, record_data=False, random_target=False, force_down=True, state_dim=-1, learn_states=False, verbose=False, save_path='srl_zoo/data/', env_rank=0, srl_pipe=None, srl_model='raw_pixels', **_):
    super(KukaButtonGymEnv, self).__init__(srl_model=srl_model, relative_pos=RELATIVE_POS, env_rank=env_rank, srl_pipe=srl_pipe)
    self._timestep = 1.0 / 240.0
    self._urdf_root = urdf_root
    self._action_repeat = action_repeat
    self._observation = []
    self._env_step_counter = 0
    self._renders = renders
    self._width = RENDER_WIDTH
    self._height = RENDER_HEIGHT
    self._cam_dist = 1.1
    self._cam_yaw = 145
    self._cam_pitch = -36
    self._cam_roll = 0
    self._max_distance = max_distance
    self._shape_reward = shape_reward
    self._random_target = random_target
    self._force_down = force_down
    self.camera_target_pos = (0.316, -0.2, -0.1)
    self._is_discrete = is_discrete
    self.terminated = False
    self.renderer = p.ER_TINY_RENDERER
    self.debug = False
    self.n_contacts = 0
    self.state_dim = state_dim
    self.action_joints = action_joints
    self.relative_pos = RELATIVE_POS
    self.cuda = th.cuda.is_available()
    self.saver = None
    self.multi_view = multi_view
    self.verbose = verbose
    self.max_steps = MAX_STEPS
    self.n_steps_outside = 0
    self.table_uid = None
    self.button_pos = None
    self.button_uid = None
    self._kuka = None
    self.action = None
    self.srl_model = srl_model
    if record_data:
        self.saver = EpisodeSaver(name, max_distance, state_dim, globals_=getGlobals(), relative_pos=RELATIVE_POS, learn_states=learn_states, path=save_path)
    if self._renders:
        client_id = p.connect(p.SHARED_MEMORY)
        if client_id < 0:
            p.connect(p.GUI)
        p.resetDebugVisualizerCamera(1.3, 180, -41, [0.52, -0.2, -0.33])
        self.debug = True
        self.x_slider = p.addUserDebugParameter('x_slider', -10, 10, self.camera_target_pos[0])
        self.y_slider = p.addUserDebugParameter('y_slider', -10, 10, self.camera_target_pos[1])
        self.z_slider = p.addUserDebugParameter('z_slider', -10, 10, self.camera_target_pos[2])
        self.dist_slider = p.addUserDebugParameter('cam_dist', 0, 10, self._cam_dist)
        self.yaw_slider = p.addUserDebugParameter('cam_yaw', -180, 180, self._cam_yaw)
        self.pitch_slider = p.addUserDebugParameter('cam_pitch', -180, 180, self._cam_pitch)
    else:
        p.connect(p.DIRECT)
    global CONNECTED_TO_SIMULATOR
    CONNECTED_TO_SIMULATOR = True
    if self._is_discrete:
        self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)
    else:
        if self.action_joints:
            action_dim = 7
            self._action_bound = 1
        else:
            action_dim = 3
            self._action_bound = 1
        action_high = np.array([self._action_bound] * action_dim)
        self.action_space = spaces.Box(-action_high, action_high, dtype=np.float32)
    if self.srl_model == 'ground_truth':
        self.state_dim = self.getGroundTruthDim()
    elif self.srl_model == 'joints':
        self.state_dim = self.getJointsDim()
    elif self.srl_model == 'joints_position':
        self.state_dim = self.getGroundTruthDim() + self.getJointsDim()
    if self.srl_model == 'raw_pixels':
        self.observation_space = spaces.Box(low=0, high=255, shape=(self._height, self._width, 3), dtype=np.uint8)
    else:
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.state_dim,), dtype=np.float32)","self.saver = None
self.table_uid = None
self.button_pos = None
self.button_uid = None
self._kuka = None
self.action = None",self.saver = self.table_uid = self.button_pos = self.button_uid = self._kuka = self.action = None,,,,,,,,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/npu/test_top_k_v2_op_npu.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/npu/test_top_k_v2_op_npu.py,TestTopkV2OP1Int32,"def set_attrs(self):
    self.k = 3
    self.axis = 0
    self.largest = False","self.axis = 0
self.largest = False",self.axis = self.largest = 0,,,,,,,,,,,
InstaPy,https://github.com/InstaPy/InstaPy/tree/master/instapy/instapy.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/InstaPy/instapy/instapy.py,InstaPy,"def interact_user_likers(self, usernames: list, posts_grab_amount: int=3, interact_likers_per_post: int=3, randomize: bool=False):
    """"""
        Interact with the likers of given user's posts.

        set_do_comment, set_do_follow and set_do_like are applicable.

        :param usernames: List of users with whose likers to interact.
        :param posts_grab_amount: Amount of posts to get the likers from per given user.
        :param interact_likers_per_post: Amount of likers to be interacted with per post.
        :param randomize: If followers should be chosen randomly.
        """"""
    if self.aborting:
        return self
    if self.do_follow is not True and self.do_like is not True:
        self.logger.info('Please enable following or liking in settings in order to do interactions.')
        return self
    elif self.user_interact_amount <= 0:
        self.logger.info('Please choose an amount higher than zero in `set_user_interact` in order to do interactions.')
        return self
    if not isinstance(usernames, list):
        usernames = [usernames]
    if posts_grab_amount > 12:
        self.logger.info('Sorry, you can only grab likers from first 12 posts for given username now.\n')
        posts_grab_amount = 12
    interacted_all = 0
    not_valid_users = 0
    simulated_unfollow = 0
    liked_init = self.liked_img
    already_liked_init = self.already_liked
    commented_init = self.commented
    followed_init = self.followed
    inap_img_init = self.inap_img
    self.quotient_breach = False
    for (index, username) in enumerate(usernames):
        if self.quotient_breach:
            break
        self.logger.info(""User '{}' [{}/{}]"".format(username, index + 1, len(usernames)))
        try:
            post_urls = get_photo_urls_from_profile(self.browser, username, posts_grab_amount, randomize, self.logger)
            if not isinstance(post_urls, list):
                post_urls = [post_urls]
        except (TypeError, RuntimeWarning) as err:
            if isinstance(err, RuntimeWarning):
                self.logger.warning('Warning: {} , skipping to next user'.format(err))
                continue
            else:
                self.logger.error('Sorry, an error occurred: {}'.format(err))
                self.aborting = True
                return self
        print('')
        self.logger.info(""Grabbed {} posts from '{}'s profile to do interaction."".format(len(post_urls), username))
        interacted_personal = 0
        for (post_index, post_url) in enumerate(post_urls):
            if self.quotient_breach:
                break
            likers = users_liked(self.browser, post_url, interact_likers_per_post, self.logger)
            random.shuffle(likers)
            self.logger.info(""Post '{}' [{}/{}]"".format(post_url, post_index + 1, len(post_urls)))
            for (liker_index, person) in enumerate(likers):
                if self.quotient_breach:
                    self.logger.warning('--> Like quotient reached its peak!\t~leaving Interact-Likers activity\n')
                    break
                self.logger.info(""Liker '{}' [{}/{}]"".format(person, liker_index + 1, len(likers)))
                (validation, details) = self.validate_user_call(person)
                if not validation:
                    self.logger.info(details)
                    not_valid_users += 1
                    continue
                do_interact = random.randint(0, 100) <= self.user_interact_percentage
                if not do_interact:
                    self.logger.info(""Skipping user '{}' due to the interaction percentage of {}"".format(person, self.user_interact_percentage))
                    continue
                else:
                    interacted_all += 1
                    interacted_personal += 1
                    self.logger.info('Interaction [{}/{}]  |  Total Interaction: {}'.format(interacted_personal, len(likers), interacted_all))
                    with self.feature_in_feature('interact_by_users', False):
                        self.interact_by_users(person, self.user_interact_amount, self.user_interact_random, self.user_interact_media)
                    if self.aborting:
                        return self
                    sleep(1)
    self.logger.info(""Finished interacting {} people from {} users' `Followers`! xD\n"".format(interacted_all, len(usernames)))
    liked = self.liked_img - liked_init
    already_liked = self.already_liked - already_liked_init
    commented = self.commented - commented_init
    followed = self.followed - followed_init
    inap_img = self.inap_img - inap_img_init
    self.logger.info('Liked: {}'.format(liked))
    self.logger.info('Already Liked: {}'.format(already_liked))
    self.logger.info('Commented: {}'.format(commented))
    self.logger.info('Followed: {}'.format(followed))
    self.logger.info('Inappropriate: {}'.format(inap_img))
    self.logger.info('Not valid users: {}\n'.format(not_valid_users))
    self.not_valid_users += not_valid_users
    return self","interacted_all = 0
not_valid_users = 0
simulated_unfollow = 0
self.quotient_breach = False","interacted_all = not_valid_users = simulated_unfollow = 0
self.quotient_breach = False",,,,,,,,,,,
core,https://github.com/home-assistant/core/tree/master/homeassistant/components/opentherm_gw/sensor.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/core/homeassistant/components/opentherm_gw/sensor.py,DeprecatedOpenThermSensor,"def __init__(self, gw_dev, var, device_class, unit, friendly_name_format):
    """"""Initialize the OpenTherm Gateway sensor.""""""
    self.entity_id = async_generate_entity_id(ENTITY_ID_FORMAT, f'{var}_{gw_dev.gw_id}', hass=gw_dev.hass)
    self._gateway = gw_dev
    self._var = var
    self._source = DEPRECATED_SENSOR_SOURCE_LOOKUP[var]
    self._value = None
    self._device_class = device_class
    self._unit = unit
    self._friendly_name = friendly_name_format.format(gw_dev.name)
    self._unsub_updates = None","self._value = None
self._unsub_updates = None",self._value = self._unsub_updates = None,,,,,,,,,,,
pybossa,https://github.com/Scifabric/pybossa/tree/master/test/test_jobs/test_leaderboard_jobs.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pybossa/test/test_jobs/test_leaderboard_jobs.py,TestLeaderboard,"def test_materialized_view_refreshed(self, db_mock, exists_mock):
    """"""Test JOB leaderboard materialized view is refreshed.""""""
    result = MagicMock()
    result.exists = True
    results = [result]
    exists_mock.return_value = True
    db_mock.slave_session.execute.side_effect = results
    db_mock.session.execute.side_effect = [ProgrammingError('foo', 'bar', 'bar'), True]
    res = leaderboard()
    assert db_mock.session.execute.called
    assert res == 'Materialized view refreshed'","result.exists = True
exists_mock.return_value = True",result.exists = exists_mock.return_value = True,,,,,,,,,,,
lingvo,https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/car/waymo/tools/waymo_proto_to_tfe.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lingvo/lingvo/tasks/car/waymo/tools/waymo_proto_to_tfe.py,FrameToTFE,"def add_point_cloud(self, feature, laser_names, range_image_pose):
    """"""Convert the range images in `feature` to 3D point clouds.

    Adds the point cloud data to the tf.Example feature map.

    Args:
      feature: A tf.Example feature map.
      laser_names: A list of laser names (e.g., 'TOP', 'REAR', 'SIDE_LEFT').
      range_image_pose: A range image pose Tensor for the top laser.
    """"""
    self.laser_info = {}
    for laser_name in laser_names:
        beam_inclinations = np.array(feature['%s_beam_inclinations' % laser_name].float_list.value[:])
        if beam_inclinations.size == 0:
            beam_inclination_min = feature['%s_beam_inclination_min' % laser_name].float_list.value[:]
            beam_inclination_max = feature['%s_beam_inclination_max' % laser_name].float_list.value[:]
            laser_ri_name = '%s_ri1' % laser_name
            range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
            height = tf.cast(range_image_shape[0], tf.float32)
            beam_inclinations = tf.constant([beam_inclination_min[0], beam_inclination_max[0]])
            beam_inclinations = range_image_utils.compute_inclination(beam_inclinations, height)
        beam_extrinsics = np.array(feature['%s_extrinsics' % laser_name].float_list.value[:]).reshape(4, 4)
        for ri_type in ['ri1', 'ri2']:
            laser_ri_name = '%s_%s' % (laser_name, ri_type)
            range_image = np.array(feature[laser_ri_name].float_list.value[:])
            range_image_shape = feature[laser_ri_name + '_shape'].int64_list.value[:]
            range_image = range_image.reshape(range_image_shape)
            range_image_mask = range_image[..., 0] >= 0
            range_image_range = range_image[..., 0]
            batched_pixel_pose = None
            batched_frame_pose = None
            if laser_name == 'TOP' and range_image_pose is not None:
                batched_pixel_pose = range_image_pose[tf.newaxis, ...]
                batched_frame_pose = self.frame_pose[tf.newaxis, ...]
            batched_range_image_range = tf.convert_to_tensor(range_image_range[np.newaxis, ...], dtype=tf.float32)
            batched_extrinsics = tf.convert_to_tensor(beam_extrinsics[np.newaxis, ...], dtype=tf.float32)
            batched_inclinations = tf.convert_to_tensor(beam_inclinations[np.newaxis, ...], dtype=tf.float32)
            batched_inclinations = tf.reverse(batched_inclinations, axis=[-1])
            range_image_cartesian = range_image_utils.extract_point_cloud_from_range_image(batched_range_image_range, batched_extrinsics, batched_inclinations, pixel_pose=batched_pixel_pose, frame_pose=batched_frame_pose)
            info = py_utils.NestedMap()
            self.laser_info[laser_ri_name] = info
            info.range_image = range_image
            info.range_image_shape = range_image_shape
            ri_indices = tf.where(range_image_mask)
            points_xyz = tf.gather_nd(range_image_cartesian[0], ri_indices)
            info.num_points = tf.shape(points_xyz).numpy()[0]
            points_features = tf.cast(tf.gather_nd(range_image[..., 1:], ri_indices), tf.float32)
            if self._use_range_image_index_as_lidar_feature:
                points_data = tf.concat([points_xyz, tf.cast(ri_indices, tf.float32), points_features[..., 2:]], axis=-1)
            else:
                points_data = tf.concat([points_xyz, points_features], axis=-1)
            points_list = list(points_data.numpy().reshape([-1]))
            feature['laser_%s' % laser_ri_name].float_list.value[:] = points_list
            laser_ri_flow_name = '%s_flow' % laser_ri_name
            if laser_ri_flow_name in feature:
                range_image_flow = np.array(feature[laser_ri_flow_name].float_list.value[:])
                range_image_flow_shape = feature[laser_ri_flow_name + '_shape'].int64_list.value[:]
                range_image_flow = range_image_flow.reshape(range_image_flow_shape)
                flow_data = tf.cast(tf.gather_nd(range_image_flow, ri_indices), tf.float32)
                flow_list = list(flow_data.numpy().reshape([-1]))
                feature['laser_%s' % laser_ri_flow_name].float_list.value[:] = flow_list","batched_pixel_pose = None
batched_frame_pose = None",batched_pixel_pose = batched_frame_pose = None,,,,,,,,,,,
kawaii-player,https://github.com/kanishka-linux/kawaii-player/tree/master/kawaii_player/kawaii_player.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/kawaii-player/kawaii_player/kawaii_player.py,Ui_MainWindow,"def IconView(self):
    global total_till, browse_cnt
    global thumbnail_indicator, total_till_epn
    if self.list_poster is not None:
        self.list_poster.title_clicked = False
    if self.list1.count() == 0:
        return 0
    self.view_mode = 'thumbnail'
    thumbnail_indicator[:] = []
    self.scrollArea1.hide()
    self.scrollArea.show()
    browse_cnt = 0
    num = self.list2.currentRow()
    i = 0
    if total_till > 0:
        while i < total_till:
            t = 'self.label_' + str(i) + '.deleteLater()'
            exec(t)
            i = i + 1
        total_till = 0
    i = 0
    if total_till_epn > 0:
        while i < total_till_epn:
            t = 'self.label_epn_' + str(i) + '.deleteLater()'
            exec(t)
            i = i + 1
        total_till_epn = 0
    if self.tab_6.isHidden():
        self.list1.hide()
        self.list2.hide()
        self.tab_5.hide()
        self.label.hide()
        self.label_new.hide()
        self.text.hide()
        self.frame.hide()
        self.frame1.hide()
        self.goto_epn.hide()
        if ui.auto_hide_dock:
            self.dockWidget_3.hide()
        self.tab_6.show()
        self.next_page('deleted')
        self.tab_2.hide()
    else:
        self.tab_6.hide()
        self.list1.show()
        self.list2.show()
        self.label.show()
        self.label_new.show()
        self.list1.setFocus()
        self.text.show()
        self.frame1.show()","browse_cnt = 0
i = 0",browse_cnt = i = 0,,,,,,,,,,,
scikit-image,https://github.com/scikit-image/scikit-image/tree/master/skimage/morphology/tests/test_extrema.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scikit-image/skimage/morphology/tests/test_extrema.py,TestLocalMaxima,"def test_nd(self):
    """"""Test one- and three-dimensional case.""""""
    x_1d = np.array([1, 1, 0, 1, 2, 3, 0, 2, 1, 2, 0])
    expected_1d = np.array([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0], dtype=bool)
    result_1d = extrema.local_maxima(x_1d)
    assert result_1d.dtype == bool
    assert_equal(result_1d, expected_1d)
    x_3d = np.zeros((8, 8, 8), dtype=np.uint8)
    expected_3d = np.zeros((8, 8, 8), dtype=bool)
    x_3d[1, 1:3, 1:3] = 100
    x_3d[2, 2, 2] = 200
    x_3d[3, 1:3, 1:3] = 100
    expected_3d[2, 2, 2] = 1
    x_3d[5:8, 1, 1] = 200
    expected_3d[5:8, 1, 1] = 1
    x_3d[0, 5:8, 5:8] = 200
    x_3d[1, 6, 6] = 100
    x_3d[2, 5:7, 5:7] = 200
    x_3d[0:3, 5:8, 5:8] += 50
    expected_3d[0, 5:8, 5:8] = 1
    expected_3d[2, 5:7, 5:7] = 1
    x_3d[6:8, 6:8, 6:8] = 200
    x_3d[7, 7, 7] = 255
    expected_3d[7, 7, 7] = 1
    result_3d = extrema.local_maxima(x_3d)
    assert result_3d.dtype == bool
    assert_equal(result_3d, expected_3d)","x_3d[1, 1:3, 1:3] = 100
x_3d[3, 1:3, 1:3] = 100
x_3d[1, 6, 6] = 100","x_3d[1, 1:3, 1:3] = x_3d[3, 1:3, 1:3] = x_3d[1, 6, 6] = 100",,,,,,,,,,,
scikit-image,https://github.com/scikit-image/scikit-image/tree/master/skimage/morphology/tests/test_extrema.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scikit-image/skimage/morphology/tests/test_extrema.py,TestLocalMaxima,"def test_nd(self):
    """"""Test one- and three-dimensional case.""""""
    x_1d = np.array([1, 1, 0, 1, 2, 3, 0, 2, 1, 2, 0])
    expected_1d = np.array([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0], dtype=bool)
    result_1d = extrema.local_maxima(x_1d)
    assert result_1d.dtype == bool
    assert_equal(result_1d, expected_1d)
    x_3d = np.zeros((8, 8, 8), dtype=np.uint8)
    expected_3d = np.zeros((8, 8, 8), dtype=bool)
    x_3d[1, 1:3, 1:3] = 100
    x_3d[2, 2, 2] = 200
    x_3d[3, 1:3, 1:3] = 100
    expected_3d[2, 2, 2] = 1
    x_3d[5:8, 1, 1] = 200
    expected_3d[5:8, 1, 1] = 1
    x_3d[0, 5:8, 5:8] = 200
    x_3d[1, 6, 6] = 100
    x_3d[2, 5:7, 5:7] = 200
    x_3d[0:3, 5:8, 5:8] += 50
    expected_3d[0, 5:8, 5:8] = 1
    expected_3d[2, 5:7, 5:7] = 1
    x_3d[6:8, 6:8, 6:8] = 200
    x_3d[7, 7, 7] = 255
    expected_3d[7, 7, 7] = 1
    result_3d = extrema.local_maxima(x_3d)
    assert result_3d.dtype == bool
    assert_equal(result_3d, expected_3d)","x_3d[2, 2, 2] = 200
x_3d[5:8, 1, 1] = 200
x_3d[0, 5:8, 5:8] = 200
x_3d[2, 5:7, 5:7] = 200","x_3d[2, 2, 2] = x_3d[5:8, 1, 1] = x_3d[0, 5:8, 5:8] = x_3d[2, 5:7, 5:7] = 200",,,,,,,,,,,
scikit-image,https://github.com/scikit-image/scikit-image/tree/master/skimage/morphology/tests/test_extrema.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scikit-image/skimage/morphology/tests/test_extrema.py,TestLocalMaxima,"def test_nd(self):
    """"""Test one- and three-dimensional case.""""""
    x_1d = np.array([1, 1, 0, 1, 2, 3, 0, 2, 1, 2, 0])
    expected_1d = np.array([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0], dtype=bool)
    result_1d = extrema.local_maxima(x_1d)
    assert result_1d.dtype == bool
    assert_equal(result_1d, expected_1d)
    x_3d = np.zeros((8, 8, 8), dtype=np.uint8)
    expected_3d = np.zeros((8, 8, 8), dtype=bool)
    x_3d[1, 1:3, 1:3] = 100
    x_3d[2, 2, 2] = 200
    x_3d[3, 1:3, 1:3] = 100
    expected_3d[2, 2, 2] = 1
    x_3d[5:8, 1, 1] = 200
    expected_3d[5:8, 1, 1] = 1
    x_3d[0, 5:8, 5:8] = 200
    x_3d[1, 6, 6] = 100
    x_3d[2, 5:7, 5:7] = 200
    x_3d[0:3, 5:8, 5:8] += 50
    expected_3d[0, 5:8, 5:8] = 1
    expected_3d[2, 5:7, 5:7] = 1
    x_3d[6:8, 6:8, 6:8] = 200
    x_3d[7, 7, 7] = 255
    expected_3d[7, 7, 7] = 1
    result_3d = extrema.local_maxima(x_3d)
    assert result_3d.dtype == bool
    assert_equal(result_3d, expected_3d)","expected_3d[2, 2, 2] = 1
expected_3d[5:8, 1, 1] = 1","expected_3d[2, 2, 2] = expected_3d[5:8, 1, 1] = 1",,,,,,,,,,,
scikit-image,https://github.com/scikit-image/scikit-image/tree/master/skimage/morphology/tests/test_extrema.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scikit-image/skimage/morphology/tests/test_extrema.py,TestLocalMaxima,"def test_nd(self):
    """"""Test one- and three-dimensional case.""""""
    x_1d = np.array([1, 1, 0, 1, 2, 3, 0, 2, 1, 2, 0])
    expected_1d = np.array([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0], dtype=bool)
    result_1d = extrema.local_maxima(x_1d)
    assert result_1d.dtype == bool
    assert_equal(result_1d, expected_1d)
    x_3d = np.zeros((8, 8, 8), dtype=np.uint8)
    expected_3d = np.zeros((8, 8, 8), dtype=bool)
    x_3d[1, 1:3, 1:3] = 100
    x_3d[2, 2, 2] = 200
    x_3d[3, 1:3, 1:3] = 100
    expected_3d[2, 2, 2] = 1
    x_3d[5:8, 1, 1] = 200
    expected_3d[5:8, 1, 1] = 1
    x_3d[0, 5:8, 5:8] = 200
    x_3d[1, 6, 6] = 100
    x_3d[2, 5:7, 5:7] = 200
    x_3d[0:3, 5:8, 5:8] += 50
    expected_3d[0, 5:8, 5:8] = 1
    expected_3d[2, 5:7, 5:7] = 1
    x_3d[6:8, 6:8, 6:8] = 200
    x_3d[7, 7, 7] = 255
    expected_3d[7, 7, 7] = 1
    result_3d = extrema.local_maxima(x_3d)
    assert result_3d.dtype == bool
    assert_equal(result_3d, expected_3d)","expected_3d[0, 5:8, 5:8] = 1
expected_3d[2, 5:7, 5:7] = 1
expected_3d[7, 7, 7] = 1","expected_3d[0, 5:8, 5:8] = expected_3d[2, 5:7, 5:7] = expected_3d[7, 7, 7] = 1",,,,,,,,,,,
neural_sp,https://github.com/hirofumi0810/neural_sp/tree/master/test/decoders/test_las_decoder.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neural_sp/test/decoders/test_las_decoder.py,,"def test_decoding(backward, lm_fusion, params):
    args = make_args()
    args['backward'] = backward
    args['lm_fusion'] = lm_fusion
    params = make_decode_params(**params)
    bs = params['recog_batch_size']
    emax = 40
    device = 'cpu'
    eouts = np.random.randn(bs, emax, ENC_N_UNITS).astype(np.float32)
    elens = torch.IntTensor([len(x) for x in eouts])
    eouts = pad_list([np2tensor(x, device).float() for x in eouts], 0.0)
    ylens = [4, 5, 3, 7]
    ys = [np.random.randint(0, VOCAB, ylen).astype(np.int32) for ylen in ylens]
    ctc_log_probs = None
    if params['recog_ctc_weight'] > 0:
        ctc_logits = torch.FloatTensor(bs, emax, VOCAB, device=device)
        ctc_log_probs = torch.softmax(ctc_logits, dim=-1)
    if params['recog_ctc_weight'] == 1:
        args['ctc_weight'] = 1
    args_lm = make_args_rnnlm()
    module_rnnlm = importlib.import_module('neural_sp.models.lm.rnnlm')
    lm = None
    lm_second = None
    lm_second_bwd = None
    if params['recog_lm_weight'] > 0:
        lm = module_rnnlm.RNNLM(args_lm).to(device)
    if params['recog_lm_second_weight'] > 0:
        lm_second = module_rnnlm.RNNLM(args_lm).to(device)
    if params['recog_lm_bwd_weight'] > 0:
        lm_second_bwd = module_rnnlm.RNNLM(args_lm).to(device)
    if args['lm_fusion']:
        args['external_lm'] = module_rnnlm.RNNLM(args_lm).to(device)
    module = importlib.import_module('neural_sp.models.seq2seq.decoders.las')
    dec = module.RNNDecoder(**args)
    dec = dec.to(device)
    dec.eval()
    with torch.no_grad():
        if params['recog_ctc_weight'] == 1:
            if params['recog_beam_width'] == 1:
                nbest_hyps = dec.ctc.greedy(eouts, elens)
            else:
                nbest_hyps = dec.ctc.beam_search(eouts, elens, params, idx2token, lm, lm_second, lm_second_bwd, nbest=1)
            assert isinstance(nbest_hyps, list)
            assert len(nbest_hyps) == bs
        elif params['recog_beam_width'] == 1:
            out = dec.greedy(eouts, elens, max_len_ratio=1.0, idx2token=idx2token, exclude_eos=params['exclude_eos'], refs_id=ys, utt_ids=None, speakers=None)
            assert len(out) == 2
            (nbest_hyps, aws) = out
            assert isinstance(nbest_hyps, list)
            assert len(nbest_hyps) == bs
            assert isinstance(aws, list)
            assert aws[0].shape == (args['attn_n_heads'], len(nbest_hyps[0]), emax)
        else:
            out = dec.beam_search(eouts, elens, params, idx2token, lm, lm_second, lm_second_bwd, ctc_log_probs, nbest=params['nbest'], exclude_eos=params['exclude_eos'], refs_id=None, utt_ids=None, speakers=None, cache_states=True)
            assert len(out) == 3
            (nbest_hyps, aws, scores) = out
            assert isinstance(nbest_hyps, list)
            assert len(nbest_hyps) == bs
            assert len(nbest_hyps[0]) == params['nbest']
            ymax = len(nbest_hyps[0][0])
            assert isinstance(aws, list)
            assert aws[0][0].shape == (args['attn_n_heads'], ymax, emax)
            assert isinstance(scores, list)
            assert len(scores) == bs
            assert len(scores[0]) == params['nbest']
            (ensmbl_eouts, ensmbl_elens, ensmbl_decs) = ([], [], [])
            for _ in range(3):
                ensmbl_eouts += [eouts]
                ensmbl_elens += [elens]
                ensmbl_decs += [dec]
            out = dec.beam_search(eouts, elens, params, idx2token=idx2token, lm=lm, lm_second=lm_second, lm_second_bwd=lm_second_bwd, ctc_log_probs=ctc_log_probs, nbest=params['nbest'], exclude_eos=params['exclude_eos'], refs_id=None, utt_ids=None, speakers=None, ensmbl_eouts=ensmbl_eouts, ensmbl_elens=ensmbl_elens, ensmbl_decs=ensmbl_decs, cache_states=True)
            assert len(out) == 3
            (nbest_hyps, aws, scores) = out
            assert isinstance(nbest_hyps, list)
            assert len(nbest_hyps) == bs
            assert len(nbest_hyps[0]) == params['nbest']
            ymax = len(nbest_hyps[0][0])
            assert isinstance(aws, list)
            assert aws[0][0].shape == (args['attn_n_heads'], ymax, emax)
            assert isinstance(scores, list)
            assert len(scores) == bs
            assert len(scores[0]) == params['nbest']","lm = None
lm_second = None
lm_second_bwd = None",lm = lm_second = lm_second_bwd = None,,,,,,,,,,,
pycord,https://github.com/Pycord-Development/pycord/tree/master/discord/abc.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycord/discord/abc.py,GuildChannel,"def permissions_for(self, obj: Member | Role, /) -> Permissions:
    """"""Handles permission resolution for the :class:`~discord.Member`
        or :class:`~discord.Role`.

        This function takes into consideration the following cases:

        - Guild owner
        - Guild roles
        - Channel overrides
        - Member overrides

        If a :class:`~discord.Role` is passed, then it checks the permissions
        someone with that role would have, which is essentially:

        - The default role permissions
        - The permissions of the role used as a parameter
        - The default role permission overwrites
        - The permission overwrites of the role used as a parameter

        .. versionchanged:: 2.0
            The object passed in can now be a role object.

        Parameters
        ----------
        obj: Union[:class:`~discord.Member`, :class:`~discord.Role`]
            The object to resolve permissions for. This could be either
            a member or a role. If it's a role then member overwrites
            are not computed.

        Returns
        -------
        :class:`~discord.Permissions`
            The resolved permissions for the member or role.
        """"""
    if self.guild.owner_id == obj.id:
        return Permissions.all()
    default = self.guild.default_role
    base = Permissions(default.permissions.value)
    if isinstance(obj, Role):
        base.value |= obj._permissions
        if base.administrator:
            return Permissions.all()
        try:
            maybe_everyone = self._overwrites[0]
            if maybe_everyone.id == self.guild.id:
                base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
        except IndexError:
            pass
        if obj.is_default():
            return base
        overwrite = utils.get(self._overwrites, type=_Overwrites.ROLE, id=obj.id)
        if overwrite is not None:
            base.handle_overwrite(overwrite.allow, overwrite.deny)
        return base
    roles = obj._roles
    get_role = self.guild.get_role
    for role_id in roles:
        role = get_role(role_id)
        if role is not None:
            base.value |= role._permissions
    if base.administrator:
        return Permissions.all()
    try:
        maybe_everyone = self._overwrites[0]
        if maybe_everyone.id == self.guild.id:
            base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
            remaining_overwrites = self._overwrites[1:]
        else:
            remaining_overwrites = self._overwrites
    except IndexError:
        remaining_overwrites = self._overwrites
    denies = 0
    allows = 0
    for overwrite in remaining_overwrites:
        if overwrite.is_role() and roles.has(overwrite.id):
            denies |= overwrite.deny
            allows |= overwrite.allow
    base.handle_overwrite(allow=allows, deny=denies)
    for overwrite in remaining_overwrites:
        if overwrite.is_member() and overwrite.id == obj.id:
            base.handle_overwrite(allow=overwrite.allow, deny=overwrite.deny)
            break
    if not base.send_messages:
        base.send_tts_messages = False
        base.mention_everyone = False
        base.embed_links = False
        base.attach_files = False
    if not base.read_messages:
        denied = Permissions.all_channel()
        base.value &= ~denied.value
    return base","denies = 0
allows = 0",denies = allows = 0,,,,,,,,,,,
pycord,https://github.com/Pycord-Development/pycord/tree/master/discord/abc.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycord/discord/abc.py,GuildChannel,"def permissions_for(self, obj: Member | Role, /) -> Permissions:
    """"""Handles permission resolution for the :class:`~discord.Member`
        or :class:`~discord.Role`.

        This function takes into consideration the following cases:

        - Guild owner
        - Guild roles
        - Channel overrides
        - Member overrides

        If a :class:`~discord.Role` is passed, then it checks the permissions
        someone with that role would have, which is essentially:

        - The default role permissions
        - The permissions of the role used as a parameter
        - The default role permission overwrites
        - The permission overwrites of the role used as a parameter

        .. versionchanged:: 2.0
            The object passed in can now be a role object.

        Parameters
        ----------
        obj: Union[:class:`~discord.Member`, :class:`~discord.Role`]
            The object to resolve permissions for. This could be either
            a member or a role. If it's a role then member overwrites
            are not computed.

        Returns
        -------
        :class:`~discord.Permissions`
            The resolved permissions for the member or role.
        """"""
    if self.guild.owner_id == obj.id:
        return Permissions.all()
    default = self.guild.default_role
    base = Permissions(default.permissions.value)
    if isinstance(obj, Role):
        base.value |= obj._permissions
        if base.administrator:
            return Permissions.all()
        try:
            maybe_everyone = self._overwrites[0]
            if maybe_everyone.id == self.guild.id:
                base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
        except IndexError:
            pass
        if obj.is_default():
            return base
        overwrite = utils.get(self._overwrites, type=_Overwrites.ROLE, id=obj.id)
        if overwrite is not None:
            base.handle_overwrite(overwrite.allow, overwrite.deny)
        return base
    roles = obj._roles
    get_role = self.guild.get_role
    for role_id in roles:
        role = get_role(role_id)
        if role is not None:
            base.value |= role._permissions
    if base.administrator:
        return Permissions.all()
    try:
        maybe_everyone = self._overwrites[0]
        if maybe_everyone.id == self.guild.id:
            base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
            remaining_overwrites = self._overwrites[1:]
        else:
            remaining_overwrites = self._overwrites
    except IndexError:
        remaining_overwrites = self._overwrites
    denies = 0
    allows = 0
    for overwrite in remaining_overwrites:
        if overwrite.is_role() and roles.has(overwrite.id):
            denies |= overwrite.deny
            allows |= overwrite.allow
    base.handle_overwrite(allow=allows, deny=denies)
    for overwrite in remaining_overwrites:
        if overwrite.is_member() and overwrite.id == obj.id:
            base.handle_overwrite(allow=overwrite.allow, deny=overwrite.deny)
            break
    if not base.send_messages:
        base.send_tts_messages = False
        base.mention_everyone = False
        base.embed_links = False
        base.attach_files = False
    if not base.read_messages:
        denied = Permissions.all_channel()
        base.value &= ~denied.value
    return base","base.send_tts_messages = False
base.mention_everyone = False
base.embed_links = False
base.attach_files = False",base.send_tts_messages = base.mention_everyone = base.embed_links = base.attach_files = False,,,,,,,,,,,
django-cms,https://github.com/django-cms/django-cms/tree/master/cms/tests/test_admin.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-cms/cms/tests/test_admin.py,AdminTestsBase,"def _get_staff_user(self, use_global_permissions=True):
    USERNAME = 'test'
    if get_user_model().USERNAME_FIELD == 'email':
        normal_guy = get_user_model().objects.create_user(USERNAME, 'test@test.com', 'test@test.com')
    else:
        normal_guy = get_user_model().objects.create_user(USERNAME, 'test@test.com', USERNAME)
    normal_guy.is_staff = True
    normal_guy.is_active = True
    perms = Permission.objects.filter(codename__in=['change_page', 'change_title', 'add_page', 'add_title', 'delete_page', 'delete_title'])
    normal_guy.save()
    normal_guy.user_permissions.set(perms)
    if use_global_permissions:
        gpp = GlobalPagePermission.objects.create(user=normal_guy, can_change=True, can_delete=True, can_change_advanced_settings=False, can_publish=True, can_change_permissions=False, can_move_page=True)
        gpp.sites.set(Site.objects.all())
    return normal_guy","normal_guy.is_staff = True
normal_guy.is_active = True",normal_guy.is_staff = normal_guy.is_active = True,,,,,,,,,,,
toapi,https://github.com/gaojiuli/toapi/tree/master/toapi/item.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/toapi/toapi/item.py,ItemType,"def __new__(cls, what, bases=None, attrdict=None):
    __fields__ = OrderedDict()
    for (name, selector) in attrdict.items():
        if isinstance(selector, Selector):
            __fields__[name] = selector
    for name in __fields__.keys():
        del attrdict[name]
    instance = type.__new__(cls, what, bases, attrdict)
    instance._list = None
    instance._site = None
    instance._selector = None
    instance.__fields__ = __fields__
    return instance","instance._list = None
instance._site = None
instance._selector = None",instance._list = instance._site = instance._selector = None,,,,,,,,,,,
webterminal,https://github.com/jimmy201602/webterminal/tree/master/elfinder/views.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/webterminal/elfinder/views.py,ElfinderConnectorView,"def post(self, request, *args, **kwargs):
    """"""
        called in post method calls.
        It only allows for the 'upload' command
        """"""
    u_id = str(uuid.uuid4())
    kwargs['u_id'] = u_id
    loginuser = kwargs.get('loginuser', None)
    if kwargs['optionset'] == 'sftp':
        server_object = get_object_or_404(ServerInfor, id=kwargs['start_path'])
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}-{1}'.format(server_object.name, server_object.ip)
        key_label = '%s::%s' % (server_object.ip, loginuser)
        port = None
        method = None
        key = None
        password = None
        for credential in server_object.credentials.all():
            if credential.username == loginuser:
                port = credential.port
                method = credential.method
                if method == 'password':
                    password = credential.password
                else:
                    password = credential.password
                    key = credential.key
        if method == 'password':
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'password': password, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        else:
            private_key = StringIO(key)
            if 'RSA' in key:
                private_key = paramiko.RSAKey.from_private_key(private_key, password=password)
            elif 'DSA' in key:
                private_key = paramiko.DSSKey.from_private_key(private_key, password=password)
            elif 'EC' in key:
                private_key = paramiko.ECDSAKey.from_private_key(private_key, password=password)
            elif 'OPENSSH' in key:
                private_key = paramiko.Ed25519Key.from_private_key(private_key, password=password)
            optinon_sets['roots'][u_id][0]['storageKwArgs'] = {'host': server_object.ip, 'params': {'port': port, 'username': loginuser, 'pkey': private_key, 'timeout': 30}, 'root_path': '/', 'interactive': False, 'key_label': key_label}
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    else:
        optinon_sets = self.get_optionset(**kwargs)
        optinon_sets['roots'][u_id][0]['alias'] = '{0}_tmp_dir'.format(request.user.username)
        optinon_sets['roots'][u_id][0]['path'] = os.path.join(settings.MEDIA_ROOT, request.user.username, 'Download')
        optinon_sets['roots'][u_id][0]['URL'] = '{0}{1}/{2}/'.format(settings.MEDIA_URL, request.user.username, 'Download')
        self.elfinder = ElfinderConnector(optinon_sets, u_id, request.session)
    cmd = self.get_command(request.POST)
    if not cmd in ['upload']:
        self.render_to_response({'error': self.elfinder.error(ElfinderErrorMessages.ERROR_UPLOAD, ElfinderErrorMessages.ERROR_UPLOAD_TOTAL_SIZE)})
    return self.output(cmd, request.POST)","port = None
method = None
key = None
password = None",port = method = key = password = None,,,,,,,,,,,
electrum,https://github.com/spesmilo/electrum/tree/master/electrum/lnrater.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/electrum/electrum/lnrater.py,LNRater,"def _rate_nodes(self):
    """"""Rate nodes by collected statistics.""""""
    max_capacity = 0
    max_num_chan = 0
    min_fee_rate = float('inf')
    for stats in self._node_stats.values():
        max_capacity = max(max_capacity, stats.total_capacity_msat)
        max_num_chan = max(max_num_chan, stats.number_channels)
        min_fee_rate = min(min_fee_rate, stats.mean_fee_rate)
    for (n, stats) in self._node_stats.items():
        heuristics = []
        heuristics_weights = []
        heuristics.append(stats.number_channels / max_num_chan)
        heuristics_weights.append(0.2)
        heuristics.append(stats.total_capacity_msat / max_capacity)
        heuristics_weights.append(0.8)
        fees = min(1e-06, min_fee_rate) / max(1e-10, stats.mean_fee_rate)
        heuristics.append(fees)
        heuristics_weights.append(1.0)
        self._node_ratings[n] = weighted_sum(heuristics, heuristics_weights)","max_capacity = 0
max_num_chan = 0",max_capacity = max_num_chan = 0,,,,,,,,,,,
volatility3,https://github.com/volatilityfoundation/volatility3/tree/master/volatility3/framework/plugins/windows/hashdump.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/volatility3/volatility3/framework/plugins/windows/hashdump.py,Hashdump,"def run(self):
    offset = self.config.get('offset', None)
    syshive = None
    samhive = None
    kernel = self.context.modules[self.config['kernel']]
    for hive in hivelist.HiveList.list_hives(self.context, self.config_path, kernel.layer_name, kernel.symbol_table_name, hive_offsets=None if offset is None else [offset]):
        if hive.get_name().split('\\')[-1].upper() == 'SYSTEM':
            syshive = hive
        if hive.get_name().split('\\')[-1].upper() == 'SAM':
            samhive = hive
    return renderers.TreeGrid([('User', str), ('rid', int), ('lmhash', str), ('nthash', str)], self._generator(syshive, samhive))","syshive = None
samhive = None",syshive = samhive = None,,,,,,,,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/distributed/auto_parallel/operators/dist_transpose.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/distributed/auto_parallel/operators/dist_transpose.py,DistributedTranspose2Impl,"def __init__(self, name):
    super().__init__(name)
    self._forward_implemented = False
    self._backward_implemented = False","self._forward_implemented = False
self._backward_implemented = False",self._forward_implemented = self._backward_implemented = False,,,,,,,,,,,
veusz,https://github.com/veusz/veusz/tree/master/veusz/datasets/histo.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/veusz/veusz/datasets/histo.py,DatasetHistoBins,"def __init__(self, generator, document):
    Dataset1DBase.__init__(self)
    self.generator = generator
    self.document = document
    self.linked = None
    self._invalidpoints = None
    self.changeset = -1","self.linked = None
self._invalidpoints = None",self.linked = self._invalidpoints = None,,,,,,,,,,,
neural_sp,https://github.com/hirofumi0810/neural_sp/tree/master/neural_sp/models/seq2seq/speech2text.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neural_sp/neural_sp/models/seq2seq/speech2text.py,Speech2Text,"def __init__(self, args, save_path=None, idx2token=None):
    super(ModelBase, self).__init__()
    self.save_path = save_path
    self.input_type = args.input_type
    self.input_dim = args.input_dim
    self.enc_type = args.enc_type
    self.dec_type = args.dec_type
    self.enc_n_layers = args.enc_n_layers
    self.enc_n_layers_sub1 = args.enc_n_layers_sub1
    self.subsample = [int(s) for s in args.subsample.split('_')]
    self.vocab = args.vocab
    self.vocab_sub1 = args.vocab_sub1
    self.vocab_sub2 = args.vocab_sub2
    self.blank = 0
    self.unk = 1
    self.eos = 2
    self.pad = 3
    self.main_weight = args.total_weight - args.sub1_weight - args.sub2_weight
    self.sub1_weight = args.sub1_weight
    self.sub2_weight = args.sub2_weight
    self.mtl_per_batch = args.mtl_per_batch
    self.task_specific_layer = args.task_specific_layer
    self.ctc_weight = min(args.ctc_weight, self.main_weight)
    self.ctc_weight_sub1 = min(args.ctc_weight_sub1, self.sub1_weight)
    self.ctc_weight_sub2 = min(args.ctc_weight_sub2, self.sub2_weight)
    self.bwd_weight = min(args.bwd_weight, self.main_weight)
    self.fwd_weight = self.main_weight - self.bwd_weight - self.ctc_weight
    self.fwd_weight_sub1 = self.sub1_weight - self.ctc_weight_sub1
    self.fwd_weight_sub2 = self.sub2_weight - self.ctc_weight_sub2
    self.mbr_training = args.mbr_training
    self.recog_params = vars(args)
    self.idx2token = idx2token
    self.utt_id_prev = None
    self.input_noise_std = args.input_noise_std
    self.n_stacks = args.n_stacks
    self.n_skips = args.n_skips
    self.n_splices = args.n_splices
    self.weight_noise_std = args.weight_noise_std
    self.specaug = None
    if args.n_freq_masks > 0 or args.n_time_masks > 0:
        assert args.n_stacks == 1 and args.n_skips == 1
        assert args.n_splices == 1
        self.specaug = SpecAugment(F=args.freq_width, T=args.time_width, n_freq_masks=args.n_freq_masks, n_time_masks=args.n_time_masks, p=args.time_width_upper, adaptive_number_ratio=args.adaptive_number_ratio, adaptive_size_ratio=args.adaptive_size_ratio, max_n_time_masks=args.max_n_time_masks)
    self.ssn = None
    if args.sequence_summary_network:
        assert args.input_type == 'speech'
        self.ssn = SequenceSummaryNetwork(args.input_dim, n_units=512, n_layers=3, bottleneck_dim=100, dropout=0, param_init=args.param_init)
    self.enc = build_encoder(args)
    if args.freeze_encoder:
        for (n, p) in self.enc.named_parameters():
            if 'bridge' in n or 'sub1' in n:
                continue
            p.requires_grad = False
            logger.info('freeze %s' % n)
    special_symbols = {'blank': self.blank, 'unk': self.unk, 'eos': self.eos, 'pad': self.pad}
    external_lm = None
    directions = []
    if self.fwd_weight > 0 or (self.bwd_weight == 0 and self.ctc_weight > 0):
        directions.append('fwd')
    if self.bwd_weight > 0:
        directions.append('bwd')
    for dir in directions:
        if args.external_lm and dir == 'fwd':
            external_lm = RNNLM(args.lm_conf)
            load_checkpoint(args.external_lm, external_lm)
            for (n, p) in external_lm.named_parameters():
                p.requires_grad = False
        dec = build_decoder(args, special_symbols, self.enc.output_dim, args.vocab, self.ctc_weight, self.main_weight - self.bwd_weight if dir == 'fwd' else self.bwd_weight, external_lm)
        setattr(self, 'dec_' + dir, dec)
    for sub in ['sub1', 'sub2']:
        if getattr(self, sub + '_weight') > 0:
            args_sub = copy.deepcopy(args)
            if hasattr(args, 'dec_config_' + sub):
                for (k, v) in getattr(args, 'dec_config_' + sub).items():
                    setattr(args_sub, k, v)
            dec_sub = build_decoder(args_sub, special_symbols, getattr(self.enc, 'output_dim_' + sub), getattr(self, 'vocab_' + sub), getattr(self, 'ctc_weight_' + sub), getattr(self, sub + '_weight'), external_lm)
            setattr(self, 'dec_fwd_' + sub, dec_sub)
    if args.input_type == 'text':
        if args.vocab == args.vocab_sub1:
            self.embed = dec.embed
        else:
            self.embed = nn.Embedding(args.vocab_sub1, args.emb_dim, padding_idx=self.pad)
            self.dropout_emb = nn.Dropout(p=args.dropout_emb)
    if args.lm_fusion == 'deep' and external_lm is not None:
        for (n, p) in self.named_parameters():
            if 'output' in n or 'output_bn' in n or 'linear' in n:
                p.requires_grad = True
            else:
                p.requires_grad = False","self.utt_id_prev = None
self.specaug = None",self.utt_id_prev = self.specaug = None,,,,,,,,,,,
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","p_mask[pad_token_indices] = 1
p_mask[special_token_indices] = 1","p_mask, pad_token_indices, special_token_indices = ([1 if i in pad_token_indices or i in special_token_indices else 0 for i in range(len(p_mask))], pad_token_indices, special_token_indices)",,,,,,,,,,,
jiant,https://github.com/nyu-mll/jiant/tree/master/jiant/tasks/lib/templates/squad_style/core.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/jiant/jiant/tasks/lib/templates/squad_style/core.py,Example,"def to_feature_list(self, tokenizer, max_seq_length, doc_stride, max_query_length, set_type):
    is_training = set_type == PHASE.TRAIN
    features = []
    if is_training and (not self.is_impossible):
        start_position = self.start_position
        end_position = self.end_position
        actual_text = ' '.join(self.doc_tokens[start_position:end_position + 1])
        cleaned_answer_text = ' '.join(whitespace_tokenize(self.answer_text))
        if actual_text.find(cleaned_answer_text) == -1:
            logger.warning(""Could not find answer: '%s' vs. '%s'"", actual_text, cleaned_answer_text)
            return []
    tok_to_orig_index = []
    orig_to_tok_index = []
    all_doc_tokens = []
    for (i, token) in enumerate(self.doc_tokens):
        orig_to_tok_index.append(len(all_doc_tokens))
        if tokenizer.__class__.__name__ in ['RobertaTokenizer', 'LongformerTokenizer', 'BartTokenizer', 'RobertaTokenizerFast', 'LongformerTokenizerFast', 'BartTokenizerFast']:
            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
        else:
            sub_tokens = tokenizer.tokenize(token)
        for sub_token in sub_tokens:
            tok_to_orig_index.append(i)
            all_doc_tokens.append(sub_token)
    if is_training and (not self.is_impossible):
        tok_start_position = orig_to_tok_index[self.start_position]
        if self.end_position < len(self.doc_tokens) - 1:
            tok_end_position = orig_to_tok_index[self.end_position + 1] - 1
        else:
            tok_end_position = len(all_doc_tokens) - 1
        (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, tok_start_position, tok_end_position, tokenizer, self.answer_text)
    spans = []
    truncated_query = tokenizer.encode(self.question_text, add_special_tokens=False, truncation=True, max_length=max_query_length)
    tokenizer_type = type(tokenizer).__name__.replace('Tokenizer', '').lower()
    sequence_added_tokens = tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1 if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET else tokenizer.model_max_length - tokenizer.max_len_single_sentence
    sequence_pair_added_tokens = tokenizer.model_max_length - tokenizer.max_len_sentences_pair
    span_doc_tokens = all_doc_tokens
    while len(spans) * doc_stride < len(all_doc_tokens):
        if tokenizer.padding_side == 'right':
            texts = truncated_query
            pairs = span_doc_tokens
            truncation = TruncationStrategy.ONLY_SECOND.value
        else:
            texts = span_doc_tokens
            pairs = truncated_query
            truncation = TruncationStrategy.ONLY_FIRST.value
        encoded_dict = tokenizer.encode_plus(texts, pairs, truncation=truncation, padding='max_length', max_length=max_seq_length, return_overflowing_tokens=True, stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens, return_token_type_ids=True)
        paragraph_len = min(len(all_doc_tokens) - len(spans) * doc_stride, max_seq_length - len(truncated_query) - sequence_pair_added_tokens)
        if tokenizer.pad_token_id in encoded_dict['input_ids']:
            if tokenizer.padding_side == 'right':
                non_padded_ids = encoded_dict['input_ids'][:encoded_dict['input_ids'].index(tokenizer.pad_token_id)]
            else:
                last_padding_id_position = len(encoded_dict['input_ids']) - 1 - encoded_dict['input_ids'][::-1].index(tokenizer.pad_token_id)
                non_padded_ids = encoded_dict['input_ids'][last_padding_id_position + 1:]
        else:
            non_padded_ids = encoded_dict['input_ids']
        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
        token_to_orig_map = {}
        for i in range(paragraph_len):
            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == 'right' else i
            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
        encoded_dict['paragraph_len'] = paragraph_len
        encoded_dict['tokens'] = tokens
        encoded_dict['token_to_orig_map'] = token_to_orig_map
        encoded_dict['truncated_query_with_special_tokens_length'] = len(truncated_query) + sequence_added_tokens
        encoded_dict['token_is_max_context'] = {}
        encoded_dict['start'] = len(spans) * doc_stride
        encoded_dict['length'] = paragraph_len
        spans.append(encoded_dict)
        if 'overflowing_tokens' not in encoded_dict or ('overflowing_tokens' in encoded_dict and len(encoded_dict['overflowing_tokens']) == 0):
            break
        span_doc_tokens = encoded_dict['overflowing_tokens']
    for doc_span_index in range(len(spans)):
        for j in range(spans[doc_span_index]['paragraph_len']):
            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)
            index = j if tokenizer.padding_side == 'left' else spans[doc_span_index]['truncated_query_with_special_tokens_length'] + j
            spans[doc_span_index]['token_is_max_context'][index] = is_max_context
    for span in spans:
        cls_index = span['input_ids'].index(tokenizer.cls_token_id)
        p_mask = np.ones_like(span['token_type_ids'])
        if tokenizer.padding_side == 'right':
            p_mask[len(truncated_query) + sequence_added_tokens:] = 0
        else:
            p_mask[-len(span['tokens']):-(len(truncated_query) + sequence_added_tokens)] = 0
        pad_token_indices = np.where(span['input_ids'] == tokenizer.pad_token_id)
        special_token_indices = np.asarray(tokenizer.get_special_tokens_mask(span['input_ids'], already_has_special_tokens=True)).nonzero()
        p_mask[pad_token_indices] = 1
        p_mask[special_token_indices] = 1
        p_mask[cls_index] = 0
        span_is_impossible = self.is_impossible
        start_position = 0
        end_position = 0
        if is_training and (not span_is_impossible):
            doc_start = span['start']
            doc_end = span['start'] + span['length'] - 1
            out_of_span = False
            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
                out_of_span = True
            if out_of_span:
                start_position = cls_index
                end_position = cls_index
                span_is_impossible = True
            else:
                if tokenizer.padding_side == 'left':
                    doc_offset = 0
                else:
                    doc_offset = len(truncated_query) + sequence_added_tokens
                start_position = tok_start_position - doc_start + doc_offset
                end_position = tok_end_position - doc_start + doc_offset
        features.append(DataRow(unique_id='', qas_id=self.qas_id, tokens=span['tokens'], token_to_orig_map=span['token_to_orig_map'], token_is_max_context=span['token_is_max_context'], input_ids=np.array(span['input_ids']), input_mask=np.array(span['attention_mask']), segment_ids=np.array(span['token_type_ids']), cls_index=np.array(cls_index), p_mask=np.array(p_mask.tolist()), paragraph_len=span['paragraph_len'], start_position=start_position, end_position=end_position, answers=self.answers, doc_tokens=self.doc_tokens))
    return features","p_mask[cls_index] = 0
start_position = 0
end_position = 0",p_mask[cls_index] = start_position = end_position = 0,,,,,,,,,,,
mushroom-rl,https://github.com/MushroomRL/mushroom-rl/tree/master/tests/algorithms/test_sac.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mushroom-rl/tests/algorithms/test_sac.py,,"def learn_sac():
    horizon = 200
    gamma = 0.99
    mdp = Gym('Pendulum-v1', horizon, gamma)
    mdp.seed(1)
    np.random.seed(1)
    torch.manual_seed(1)
    torch.cuda.manual_seed(1)
    initial_replay_size = 64
    max_replay_size = 50000
    batch_size = 64
    n_features = 64
    warmup_transitions = 10
    tau = 0.005
    lr_alpha = 0.0003
    actor_input_shape = mdp.info.observation_space.shape
    actor_mu_params = dict(network=ActorNetwork, n_features=n_features, input_shape=actor_input_shape, output_shape=mdp.info.action_space.shape, use_cuda=False)
    actor_sigma_params = dict(network=ActorNetwork, n_features=n_features, input_shape=actor_input_shape, output_shape=mdp.info.action_space.shape, use_cuda=False)
    actor_optimizer = {'class': optim.Adam, 'params': {'lr': 0.0003}}
    critic_input_shape = (actor_input_shape[0] + mdp.info.action_space.shape[0],)
    critic_params = dict(network=CriticNetwork, optimizer={'class': optim.Adam, 'params': {'lr': 0.0003}}, loss=F.mse_loss, n_features=n_features, input_shape=critic_input_shape, output_shape=(1,), use_cuda=False)
    agent = SAC(mdp.info, actor_mu_params, actor_sigma_params, actor_optimizer, critic_params, batch_size, initial_replay_size, max_replay_size, warmup_transitions, tau, lr_alpha, critic_fit_params=None)
    core = Core(agent, mdp)
    core.learn(n_steps=2 * initial_replay_size, n_steps_per_fit=initial_replay_size)
    return agent","initial_replay_size = 64
batch_size = 64
n_features = 64",initial_replay_size = batch_size = n_features = 64,,,,,,,,,,,
erpnext,https://github.com/frappe/erpnext/tree/master/erpnext/selling/doctype/sales_order/sales_order.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/selling/doctype/sales_order/sales_order.py,,"def set_missing_values(source, target):
    target.supplier = supplier
    target.currency = frappe.db.get_value('Supplier', filters={'name': supplier}, fieldname=['default_currency'])
    company_currency = frappe.db.get_value('Company', filters={'name': target.company}, fieldname=['default_currency'])
    target.conversion_rate = get_exchange_rate(target.currency, company_currency, args='for_buying')
    target.apply_discount_on = ''
    target.additional_discount_percentage = 0.0
    target.discount_amount = 0.0
    target.inter_company_order_reference = ''
    target.shipping_rule = ''
    default_price_list = frappe.get_value('Supplier', supplier, 'default_price_list')
    if default_price_list:
        target.buying_price_list = default_price_list
    if any((item.delivered_by_supplier == 1 for item in source.items)):
        if source.shipping_address_name:
            target.shipping_address = source.shipping_address_name
            target.shipping_address_display = source.shipping_address
        else:
            target.shipping_address = source.customer_address
            target.shipping_address_display = source.address_display
        target.customer_contact_person = source.contact_person
        target.customer_contact_display = source.contact_display
        target.customer_contact_mobile = source.contact_mobile
        target.customer_contact_email = source.contact_email
    else:
        target.customer = ''
        target.customer_name = ''
    target.run_method('set_missing_values')
    target.run_method('calculate_taxes_and_totals')","target.apply_discount_on = ''
target.inter_company_order_reference = ''
target.shipping_rule = ''",target.apply_discount_on = target.inter_company_order_reference = target.shipping_rule = '',,,,,,,,,,,
erpnext,https://github.com/frappe/erpnext/tree/master/erpnext/selling/doctype/sales_order/sales_order.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/selling/doctype/sales_order/sales_order.py,,"def set_missing_values(source, target):
    target.supplier = supplier
    target.currency = frappe.db.get_value('Supplier', filters={'name': supplier}, fieldname=['default_currency'])
    company_currency = frappe.db.get_value('Company', filters={'name': target.company}, fieldname=['default_currency'])
    target.conversion_rate = get_exchange_rate(target.currency, company_currency, args='for_buying')
    target.apply_discount_on = ''
    target.additional_discount_percentage = 0.0
    target.discount_amount = 0.0
    target.inter_company_order_reference = ''
    target.shipping_rule = ''
    default_price_list = frappe.get_value('Supplier', supplier, 'default_price_list')
    if default_price_list:
        target.buying_price_list = default_price_list
    if any((item.delivered_by_supplier == 1 for item in source.items)):
        if source.shipping_address_name:
            target.shipping_address = source.shipping_address_name
            target.shipping_address_display = source.shipping_address
        else:
            target.shipping_address = source.customer_address
            target.shipping_address_display = source.address_display
        target.customer_contact_person = source.contact_person
        target.customer_contact_display = source.contact_display
        target.customer_contact_mobile = source.contact_mobile
        target.customer_contact_email = source.contact_email
    else:
        target.customer = ''
        target.customer_name = ''
    target.run_method('set_missing_values')
    target.run_method('calculate_taxes_and_totals')","target.additional_discount_percentage = 0.0
target.discount_amount = 0.0",target.additional_discount_percentage = target.discount_amount = 0.0,,,,,,,,,,,
erpnext,https://github.com/frappe/erpnext/tree/master/erpnext/selling/doctype/sales_order/sales_order.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/selling/doctype/sales_order/sales_order.py,,"def set_missing_values(source, target):
    target.supplier = supplier
    target.currency = frappe.db.get_value('Supplier', filters={'name': supplier}, fieldname=['default_currency'])
    company_currency = frappe.db.get_value('Company', filters={'name': target.company}, fieldname=['default_currency'])
    target.conversion_rate = get_exchange_rate(target.currency, company_currency, args='for_buying')
    target.apply_discount_on = ''
    target.additional_discount_percentage = 0.0
    target.discount_amount = 0.0
    target.inter_company_order_reference = ''
    target.shipping_rule = ''
    default_price_list = frappe.get_value('Supplier', supplier, 'default_price_list')
    if default_price_list:
        target.buying_price_list = default_price_list
    if any((item.delivered_by_supplier == 1 for item in source.items)):
        if source.shipping_address_name:
            target.shipping_address = source.shipping_address_name
            target.shipping_address_display = source.shipping_address
        else:
            target.shipping_address = source.customer_address
            target.shipping_address_display = source.address_display
        target.customer_contact_person = source.contact_person
        target.customer_contact_display = source.contact_display
        target.customer_contact_mobile = source.contact_mobile
        target.customer_contact_email = source.contact_email
    else:
        target.customer = ''
        target.customer_name = ''
    target.run_method('set_missing_values')
    target.run_method('calculate_taxes_and_totals')","target.customer = ''
target.customer_name = ''",target.customer = target.customer_name = '',,,,,,,,,,,
micropython-lib,https://github.com/micropython/micropython-lib/tree/master/python-stdlib/quopri/quopri.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/micropython-lib/python-stdlib/quopri/quopri.py,,"def main():
    import sys
    import getopt
    try:
        (opts, args) = getopt.getopt(sys.argv[1:], 'td')
    except getopt.error as msg:
        sys.stdout = sys.stderr
        print(msg)
        print('usage: quopri [-t | -d] [file] ...')
        print('-t: quote tabs')
        print('-d: decode; default encode')
        sys.exit(2)
    deco = 0
    tabs = 0
    for (o, a) in opts:
        if o == '-t':
            tabs = 1
        if o == '-d':
            deco = 1
    if tabs and deco:
        sys.stdout = sys.stderr
        print('-t and -d are mutually exclusive')
        sys.exit(2)
    if not args:
        args = ['-']
    sts = 0
    for file in args:
        if file == '-':
            fp = sys.stdin.buffer
        else:
            try:
                fp = open(file, 'rb')
            except IOError as msg:
                sys.stderr.write(""%s: can't open (%s)\n"" % (file, msg))
                sts = 1
                continue
        try:
            if deco:
                decode(fp, sys.stdout.buffer)
            else:
                encode(fp, sys.stdout.buffer, tabs)
        finally:
            if file != '-':
                fp.close()
    if sts:
        sys.exit(sts)","deco = 0
tabs = 0",deco = tabs = 0,,,,,,,,,,,
Mycodo,https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/utils/utils_method.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/mycodo_flask/utils/utils_method.py,,"def method_create(form_create_method):
    """"""Create new method table entry (all data stored in method_data table)""""""
    action = '{action} {controller}'.format(action=TRANSLATIONS['add']['title'], controller=TRANSLATIONS['method']['title'])
    error = []
    (dep_unmet, _, _) = return_dependencies(form_create_method.method_type.data)
    if dep_unmet:
        list_unmet_deps = []
        for each_dep in dep_unmet:
            list_unmet_deps.append(each_dep[3])
        error.append(""The {dev} device you're trying to add has unmet dependencies: {dep}"".format(dev=form_create_method.method_type.data, dep=', '.join(list_unmet_deps)))
    try:
        new_method = Method()
        new_method.name = form_create_method.name.data
        new_method.method_type = form_create_method.method_type.data
        if not error:
            db.session.add(new_method)
            db.session.commit()
            method_order = DisplayOrder.query.first()
            display_order = csv_to_list_of_str(method_order.method)
            method_order.method = add_display_order(display_order, new_method.unique_id)
            db.session.commit()
        if new_method.method_type in ['DailyBezier', 'DailySine']:
            new_method_data = MethodData()
            new_method_data.method_id = new_method.unique_id
            if new_method.method_type == 'DailySine':
                new_method_data.amplitude = 1.0
                new_method_data.frequency = 1.0
                new_method_data.shift_angle = 0
                new_method_data.shift_y = 1.0
            elif new_method.method_type == 'DailyBezier':
                new_method_data = MethodData()
                new_method_data.method_id = new_method.unique_id
                new_method_data.shift_angle = 0.0
                new_method_data.x0 = 20.0
                new_method_data.y0 = 20.0
                new_method_data.x1 = 10.0
                new_method_data.y1 = 13.5
                new_method_data.x2 = 22.5
                new_method_data.y2 = 30.0
                new_method_data.x3 = 0.0
                new_method_data.y3 = 20.0
            if not error:
                db.session.add(new_method_data)
                db.session.commit()
                display_order = csv_to_list_of_str(new_method.method_order)
                method = Method.query.filter(Method.unique_id == new_method.unique_id).first()
                method.method_order = add_display_order(display_order, new_method_data.unique_id)
                db.session.commit()
    except Exception as except_msg:
        error.append(except_msg)
    flash_success_errors(error, action, url_for('routes_method.method_list'))
    if dep_unmet:
        return 1","new_method_data.amplitude = 1.0
new_method_data.frequency = 1.0
new_method_data.shift_y = 1.0",new_method_data.amplitude = new_method_data.frequency = new_method_data.shift_y = 1.0,,,,,,,,,,,
Mycodo,https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/utils/utils_method.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/mycodo_flask/utils/utils_method.py,,"def method_create(form_create_method):
    """"""Create new method table entry (all data stored in method_data table)""""""
    action = '{action} {controller}'.format(action=TRANSLATIONS['add']['title'], controller=TRANSLATIONS['method']['title'])
    error = []
    (dep_unmet, _, _) = return_dependencies(form_create_method.method_type.data)
    if dep_unmet:
        list_unmet_deps = []
        for each_dep in dep_unmet:
            list_unmet_deps.append(each_dep[3])
        error.append(""The {dev} device you're trying to add has unmet dependencies: {dep}"".format(dev=form_create_method.method_type.data, dep=', '.join(list_unmet_deps)))
    try:
        new_method = Method()
        new_method.name = form_create_method.name.data
        new_method.method_type = form_create_method.method_type.data
        if not error:
            db.session.add(new_method)
            db.session.commit()
            method_order = DisplayOrder.query.first()
            display_order = csv_to_list_of_str(method_order.method)
            method_order.method = add_display_order(display_order, new_method.unique_id)
            db.session.commit()
        if new_method.method_type in ['DailyBezier', 'DailySine']:
            new_method_data = MethodData()
            new_method_data.method_id = new_method.unique_id
            if new_method.method_type == 'DailySine':
                new_method_data.amplitude = 1.0
                new_method_data.frequency = 1.0
                new_method_data.shift_angle = 0
                new_method_data.shift_y = 1.0
            elif new_method.method_type == 'DailyBezier':
                new_method_data = MethodData()
                new_method_data.method_id = new_method.unique_id
                new_method_data.shift_angle = 0.0
                new_method_data.x0 = 20.0
                new_method_data.y0 = 20.0
                new_method_data.x1 = 10.0
                new_method_data.y1 = 13.5
                new_method_data.x2 = 22.5
                new_method_data.y2 = 30.0
                new_method_data.x3 = 0.0
                new_method_data.y3 = 20.0
            if not error:
                db.session.add(new_method_data)
                db.session.commit()
                display_order = csv_to_list_of_str(new_method.method_order)
                method = Method.query.filter(Method.unique_id == new_method.unique_id).first()
                method.method_order = add_display_order(display_order, new_method_data.unique_id)
                db.session.commit()
    except Exception as except_msg:
        error.append(except_msg)
    flash_success_errors(error, action, url_for('routes_method.method_list'))
    if dep_unmet:
        return 1","new_method_data.shift_angle = 0.0
new_method_data.x3 = 0.0",new_method_data.shift_angle = new_method_data.x3 = 0.0,,,,,,,,,,,
Mycodo,https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/utils/utils_method.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/mycodo_flask/utils/utils_method.py,,"def method_create(form_create_method):
    """"""Create new method table entry (all data stored in method_data table)""""""
    action = '{action} {controller}'.format(action=TRANSLATIONS['add']['title'], controller=TRANSLATIONS['method']['title'])
    error = []
    (dep_unmet, _, _) = return_dependencies(form_create_method.method_type.data)
    if dep_unmet:
        list_unmet_deps = []
        for each_dep in dep_unmet:
            list_unmet_deps.append(each_dep[3])
        error.append(""The {dev} device you're trying to add has unmet dependencies: {dep}"".format(dev=form_create_method.method_type.data, dep=', '.join(list_unmet_deps)))
    try:
        new_method = Method()
        new_method.name = form_create_method.name.data
        new_method.method_type = form_create_method.method_type.data
        if not error:
            db.session.add(new_method)
            db.session.commit()
            method_order = DisplayOrder.query.first()
            display_order = csv_to_list_of_str(method_order.method)
            method_order.method = add_display_order(display_order, new_method.unique_id)
            db.session.commit()
        if new_method.method_type in ['DailyBezier', 'DailySine']:
            new_method_data = MethodData()
            new_method_data.method_id = new_method.unique_id
            if new_method.method_type == 'DailySine':
                new_method_data.amplitude = 1.0
                new_method_data.frequency = 1.0
                new_method_data.shift_angle = 0
                new_method_data.shift_y = 1.0
            elif new_method.method_type == 'DailyBezier':
                new_method_data = MethodData()
                new_method_data.method_id = new_method.unique_id
                new_method_data.shift_angle = 0.0
                new_method_data.x0 = 20.0
                new_method_data.y0 = 20.0
                new_method_data.x1 = 10.0
                new_method_data.y1 = 13.5
                new_method_data.x2 = 22.5
                new_method_data.y2 = 30.0
                new_method_data.x3 = 0.0
                new_method_data.y3 = 20.0
            if not error:
                db.session.add(new_method_data)
                db.session.commit()
                display_order = csv_to_list_of_str(new_method.method_order)
                method = Method.query.filter(Method.unique_id == new_method.unique_id).first()
                method.method_order = add_display_order(display_order, new_method_data.unique_id)
                db.session.commit()
    except Exception as except_msg:
        error.append(except_msg)
    flash_success_errors(error, action, url_for('routes_method.method_list'))
    if dep_unmet:
        return 1","new_method_data.x0 = 20.0
new_method_data.y0 = 20.0
new_method_data.y3 = 20.0",new_method_data.x0 = new_method_data.y0 = new_method_data.y3 = 20.0,,,,,,,,,,,
rdflib,https://github.com/RDFLib/rdflib/tree/master/rdflib/compare.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rdflib/rdflib/compare.py,_TripleCanonicalizer,"def _traces(self, coloring: List[Color], stats: Optional[Stats]=None, depth: List[int]=[0]) -> List[Color]:
    if stats is not None and 'prunings' not in stats:
        stats['prunings'] = 0
    depth[0] += 1
    candidates = self._get_candidates(coloring)
    best: List[List[Color]] = []
    best_score = None
    best_experimental = None
    best_experimental_score = None
    last_coloring = None
    generator: Dict[Node, Set[Node]] = defaultdict(set)
    visited: Set[Node] = set()
    for (candidate, color) in candidates:
        if candidate in generator:
            v = generator[candidate] & visited
            if len(v) > 0:
                visited.add(candidate)
                continue
        visited.add(candidate)
        coloring_copy: List[Color] = []
        color_copy = None
        for c in coloring:
            c_copy = c.copy()
            coloring_copy.append(c_copy)
            if c == color:
                color_copy = c_copy
        new_color = self._individuate(color_copy, candidate)
        coloring_copy.append(new_color)
        refined_coloring = self._refine(coloring_copy, [new_color])
        color_score = tuple([c.key() for c in refined_coloring])
        experimental = self._experimental_path(coloring_copy)
        experimental_score = set([c.key() for c in experimental])
        if last_coloring:
            generator = self._create_generator([last_coloring, experimental], generator)
        last_coloring = experimental
        if best_score is None or best_score < color_score:
            best = [refined_coloring]
            best_score = color_score
            best_experimental_score = experimental_score
        elif best_score > color_score:
            if stats is not None:
                stats['prunings'] += 1
        elif experimental_score != best_experimental_score:
            best.append(refined_coloring)
        elif stats is not None:
            stats['prunings'] += 1
    discrete: List[List[Color]] = [x for x in best if self._discrete(x)]
    if len(discrete) == 0:
        best_score = None
        best_depth = None
        for coloring in best:
            d = [depth[0]]
            new_color = self._traces(coloring, stats=stats, depth=d)
            color_score = tuple([c.key() for c in refined_coloring])
            if best_score is None or color_score > best_score:
                discrete = [new_color]
                best_score = color_score
                best_depth = d[0]
        depth[0] = best_depth
    return discrete[0]","best_score = None
best_experimental = None
best_experimental_score = None
last_coloring = None",best_score = best_experimental = best_experimental_score = last_coloring = None,,,,,,,,,,,
rdflib,https://github.com/RDFLib/rdflib/tree/master/rdflib/compare.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/rdflib/rdflib/compare.py,_TripleCanonicalizer,"def _traces(self, coloring: List[Color], stats: Optional[Stats]=None, depth: List[int]=[0]) -> List[Color]:
    if stats is not None and 'prunings' not in stats:
        stats['prunings'] = 0
    depth[0] += 1
    candidates = self._get_candidates(coloring)
    best: List[List[Color]] = []
    best_score = None
    best_experimental = None
    best_experimental_score = None
    last_coloring = None
    generator: Dict[Node, Set[Node]] = defaultdict(set)
    visited: Set[Node] = set()
    for (candidate, color) in candidates:
        if candidate in generator:
            v = generator[candidate] & visited
            if len(v) > 0:
                visited.add(candidate)
                continue
        visited.add(candidate)
        coloring_copy: List[Color] = []
        color_copy = None
        for c in coloring:
            c_copy = c.copy()
            coloring_copy.append(c_copy)
            if c == color:
                color_copy = c_copy
        new_color = self._individuate(color_copy, candidate)
        coloring_copy.append(new_color)
        refined_coloring = self._refine(coloring_copy, [new_color])
        color_score = tuple([c.key() for c in refined_coloring])
        experimental = self._experimental_path(coloring_copy)
        experimental_score = set([c.key() for c in experimental])
        if last_coloring:
            generator = self._create_generator([last_coloring, experimental], generator)
        last_coloring = experimental
        if best_score is None or best_score < color_score:
            best = [refined_coloring]
            best_score = color_score
            best_experimental_score = experimental_score
        elif best_score > color_score:
            if stats is not None:
                stats['prunings'] += 1
        elif experimental_score != best_experimental_score:
            best.append(refined_coloring)
        elif stats is not None:
            stats['prunings'] += 1
    discrete: List[List[Color]] = [x for x in best if self._discrete(x)]
    if len(discrete) == 0:
        best_score = None
        best_depth = None
        for coloring in best:
            d = [depth[0]]
            new_color = self._traces(coloring, stats=stats, depth=d)
            color_score = tuple([c.key() for c in refined_coloring])
            if best_score is None or color_score > best_score:
                discrete = [new_color]
                best_score = color_score
                best_depth = d[0]
        depth[0] = best_depth
    return discrete[0]","best_score = None
best_depth = None",best_score = best_depth = None,,,,,,,,,,,
sparrow-wifi,https://github.com/ghostop14/sparrow-wifi/tree/master/plugins/falconwifi.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sparrow-wifi/plugins/falconwifi.py,FalconDeauth,"def __init__(self):
    self.processid = 0
    self.channel = 0
    self.stationMacAddr = ''
    self.apMacAddr = ''
    self.interface = ''","self.processid = 0
self.channel = 0",self.processid = self.channel = 0,,,,,,,,,,,
sparrow-wifi,https://github.com/ghostop14/sparrow-wifi/tree/master/plugins/falconwifi.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sparrow-wifi/plugins/falconwifi.py,FalconDeauth,"def __init__(self):
    self.processid = 0
    self.channel = 0
    self.stationMacAddr = ''
    self.apMacAddr = ''
    self.interface = ''","self.stationMacAddr = ''
self.apMacAddr = ''
self.interface = ''",self.stationMacAddr = self.apMacAddr = self.interface = '',,,,,,,,,,,
pybossa,https://github.com/Scifabric/pybossa/tree/master/test/test_newsletter.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pybossa/test/test_newsletter.py,TestNewsletterViewFunctions,"def test_newsletter_not_subscribe(self, newsletter):
    """"""Test NEWSLETTER view not subcribe works.""""""
    newsletter.is_initialized.return_value = True
    newsletter.ask_user_to_subscribe.return_value = True
    self.register()
    res = self.app.get('/account/newsletter?subscribe=False', follow_redirects=True)
    err_msg = 'User should not be subscribed'
    assert 'You are subscribed' not in str(res.data), err_msg
    assert newsletter.subscribe_user.called is False, err_msg","newsletter.is_initialized.return_value = True
newsletter.ask_user_to_subscribe.return_value = True",newsletter.is_initialized.return_value = newsletter.ask_user_to_subscribe.return_value = True,,,,,,,,,,,
social_mapper,https://github.com/Greenwolf/social_mapper/tree/master//social_mapper.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/social_mapper//social_mapper.py,,"def fill_linkedin(peoplelist):
    LinkedinfinderObject = linkedinfinder.Linkedinfinder(showbrowser)
    LinkedinfinderObject.doLogin(linkedin_username, linkedin_password)
    if args.waitafterlogin:
        input('Press Enter to continue after verifying you are logged in...')
    count = 1
    ammount = len(peoplelist)
    for person in peoplelist:
        if args.vv == True or args.debug == True:
            print('LinkedIn Check %i/%i : %s' % (count, ammount, person.full_name))
        else:
            sys.stdout.write('\rLinkedIn Check %i/%i : %s                                ' % (count, ammount, person.full_name))
            sys.stdout.flush()
        count = count + 1
        if person.person_image:
            try:
                target_image = face_recognition.load_image_file(person.person_image)
                target_encoding = face_recognition.face_encodings(target_image)[0]
                profilelist = LinkedinfinderObject.getLinkedinProfiles(person.first_name, person.last_name, linkedin_username, linkedin_password)
                if args.debug == True:
                    print(profilelist)
            except:
                continue
        else:
            continue
        early_break = False
        updatedlist = []
        for (profilelink, profilepic, distance) in profilelist:
            try:
                os.remove('potential_target_image.jpg')
            except:
                pass
            if early_break:
                break
            image_link = profilepic
            if image_link:
                try:
                    urllib.request.urlretrieve(image_link, 'potential_target_image.jpg')
                    potential_target_image = face_recognition.load_image_file('potential_target_image.jpg')
                    try:
                        potential_target_encoding = face_recognition.face_encodings(potential_target_image)[0]
                    except:
                        continue
                    results = face_recognition.face_distance([target_encoding], potential_target_encoding)
                    for result in results:
                        if args.mode == 'fast':
                            if result < threshold:
                                person.linkedin = encoding.smart_str(profilelink, encoding='ascii', errors='ignore')
                                person.linkedinimage = encoding.smart_str(image_link, encoding='ascii', errors='ignore')
                                if args.vv == True:
                                    print('\tMatch found: ' + person.full_name)
                                    print('\tLinkedIn: ' + person.linkedin)
                                early_break = True
                                break
                        elif args.mode == 'accurate':
                            if result < threshold:
                                updatedlist.append([profilelink, image_link, result])
                except Exception as e:
                    print(e)
        if args.mode == 'accurate':
            highestdistance = 1.0
            bestprofilelink = ''
            bestimagelink = ''
            for (profilelink, image_link, distance) in updatedlist:
                if distance < highestdistance:
                    highestdistance = distance
                    bestprofilelink = profilelink
                    bestimagelink = image_link
            if highestdistance < threshold:
                person.linkedin = encoding.smart_str(bestprofilelink, encoding='ascii', errors='ignore')
                person.linkedinimage = encoding.smart_str(bestimagelink, encoding='ascii', errors='ignore')
                if args.vv == True:
                    print('\tMatch found: ' + person.full_name)
                    print('\tLinkedIn: ' + person.linkedin)
    try:
        LinkedinfinderObject.kill()
    except:
        print('Error Killing LinkedIn Selenium instance')
    return peoplelist","bestprofilelink = ''
bestimagelink = ''",bestprofilelink = bestimagelink = '',,,,,,,,,,,
nnabla,https://github.com/sony/nnabla/tree/master/python/src/nnabla/utils/converter/commands.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nnabla/python/src/nnabla/utils/converter/commands.py,,"def get_ranges_from_func_set(support_set):
    pos_start = 0
    pos_end = 0
    ranges = []
    for (pos, func) in enumerate(network.function):
        if func.type in support_set:
            pos_end = pos
        else:
            if pos_end >= pos_start:
                ranges.append((pos_start, pos_end))
            pos_start = pos + 1
    if pos_end >= pos_start:
        ranges.append((pos_start, pos_end))
    return ranges","pos_start = 0
pos_end = 0",pos_start = pos_end = 0,,,,,,,,,,,
plantcv,https://github.com/danforthcenter/plantcv/tree/master/tests/tests.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plantcv/tests/tests.py,,"def test_plantcv_parallel_check_date_range_wrongdateformat():
    start_date = 10
    end_date = 10
    img_time = '2010-10-10'
    with pytest.raises(SystemExit, match='does not match format'):
        date_format = '%Y%m%d'
        _ = plantcv.parallel.check_date_range(start_date, end_date, img_time, date_format)","start_date = 10
end_date = 10",start_date = end_date = 10,,,,,,,,,,,
synapse,https://github.com/matrix-org/synapse/tree/master/tests/replication/test_federation_sender_shard.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/synapse/tests/replication/test_federation_sender_shard.py,FederationSenderTestCase,"def test_send_event_sharded(self):
    """"""Test that using two federation sender workers correctly sends
        new events.
        """"""
    mock_client1 = Mock(spec=['put_json'])
    mock_client1.put_json.return_value = make_awaitable({})
    self.make_worker_hs('synapse.app.generic_worker', {'worker_name': 'federation_sender1', 'federation_sender_instances': ['federation_sender1', 'federation_sender2']}, federation_http_client=mock_client1)
    mock_client2 = Mock(spec=['put_json'])
    mock_client2.put_json.return_value = make_awaitable({})
    self.make_worker_hs('synapse.app.generic_worker', {'worker_name': 'federation_sender2', 'federation_sender_instances': ['federation_sender1', 'federation_sender2']}, federation_http_client=mock_client2)
    user = self.register_user('user2', 'pass')
    token = self.login('user2', 'pass')
    sent_on_1 = False
    sent_on_2 = False
    for i in range(20):
        server_name = 'other_server_%d' % (i,)
        room = self.create_room_with_remote_server(user, token, server_name)
        mock_client1.reset_mock()
        mock_client2.reset_mock()
        self.create_and_send_event(room, UserID.from_string(user))
        self.replicate()
        if mock_client1.put_json.called:
            sent_on_1 = True
            mock_client2.put_json.assert_not_called()
            self.assertEqual(mock_client1.put_json.call_args[0][0], server_name)
            self.assertTrue(mock_client1.put_json.call_args[1]['data'].get('pdus'))
        elif mock_client2.put_json.called:
            sent_on_2 = True
            mock_client1.put_json.assert_not_called()
            self.assertEqual(mock_client2.put_json.call_args[0][0], server_name)
            self.assertTrue(mock_client2.put_json.call_args[1]['data'].get('pdus'))
        else:
            raise AssertionError('Expected send transaction from one or the other sender')
        if sent_on_1 and sent_on_2:
            break
    self.assertTrue(sent_on_1)
    self.assertTrue(sent_on_2)","sent_on_1 = False
sent_on_2 = False",sent_on_1 = sent_on_2 = False,,,,,,,,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_discovery_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_discovery_v1.py,TestModel_QueryNoticesResult,"def test_query_notices_result_serialization(self):
    """"""
        Test serialization/deserialization for QueryNoticesResult
        """"""
    query_result_metadata_model = {}
    query_result_metadata_model['score'] = 72.5
    query_result_metadata_model['confidence'] = 72.5
    notice_model = {}
    notice_model['notice_id'] = 'testString'
    notice_model['created'] = '2019-01-01T12:00:00Z'
    notice_model['document_id'] = 'testString'
    notice_model['query_id'] = 'testString'
    notice_model['severity'] = 'warning'
    notice_model['step'] = 'testString'
    notice_model['description'] = 'testString'
    query_notices_result_model_json = {}
    query_notices_result_model_json['id'] = 'testString'
    query_notices_result_model_json['metadata'] = {}
    query_notices_result_model_json['collection_id'] = 'testString'
    query_notices_result_model_json['result_metadata'] = query_result_metadata_model
    query_notices_result_model_json['code'] = 38
    query_notices_result_model_json['filename'] = 'testString'
    query_notices_result_model_json['file_type'] = 'pdf'
    query_notices_result_model_json['sha1'] = 'testString'
    query_notices_result_model_json['notices'] = [notice_model]
    query_notices_result_model_json['foo'] = {'foo': 'bar'}
    query_notices_result_model = QueryNoticesResult.from_dict(query_notices_result_model_json)
    assert query_notices_result_model != False
    query_notices_result_model_dict = QueryNoticesResult.from_dict(query_notices_result_model_json).__dict__
    query_notices_result_model2 = QueryNoticesResult(**query_notices_result_model_dict)
    assert query_notices_result_model == query_notices_result_model2
    query_notices_result_model_json2 = query_notices_result_model.to_dict()
    assert query_notices_result_model_json2 == query_notices_result_model_json
    query_notices_result_model.set_properties({})
    actual_dict = query_notices_result_model.get_properties()
    assert actual_dict == {}
    expected_dict = {'foo': {'foo': 'bar'}}
    query_notices_result_model.set_properties(expected_dict)
    actual_dict = query_notices_result_model.get_properties()
    assert actual_dict == expected_dict","query_result_metadata_model['score'] = 72.5
query_result_metadata_model['confidence'] = 72.5",query_result_metadata_model['score'] = query_result_metadata_model['confidence'] = 72.5,,,,,,,,,,,
python-sdk,https://github.com/qiniu/python-sdk/tree/master/test/unit/test_discovery_v1.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/python-sdk/test/unit/test_discovery_v1.py,TestModel_QueryNoticesResult,"def test_query_notices_result_serialization(self):
    """"""
        Test serialization/deserialization for QueryNoticesResult
        """"""
    query_result_metadata_model = {}
    query_result_metadata_model['score'] = 72.5
    query_result_metadata_model['confidence'] = 72.5
    notice_model = {}
    notice_model['notice_id'] = 'testString'
    notice_model['created'] = '2019-01-01T12:00:00Z'
    notice_model['document_id'] = 'testString'
    notice_model['query_id'] = 'testString'
    notice_model['severity'] = 'warning'
    notice_model['step'] = 'testString'
    notice_model['description'] = 'testString'
    query_notices_result_model_json = {}
    query_notices_result_model_json['id'] = 'testString'
    query_notices_result_model_json['metadata'] = {}
    query_notices_result_model_json['collection_id'] = 'testString'
    query_notices_result_model_json['result_metadata'] = query_result_metadata_model
    query_notices_result_model_json['code'] = 38
    query_notices_result_model_json['filename'] = 'testString'
    query_notices_result_model_json['file_type'] = 'pdf'
    query_notices_result_model_json['sha1'] = 'testString'
    query_notices_result_model_json['notices'] = [notice_model]
    query_notices_result_model_json['foo'] = {'foo': 'bar'}
    query_notices_result_model = QueryNoticesResult.from_dict(query_notices_result_model_json)
    assert query_notices_result_model != False
    query_notices_result_model_dict = QueryNoticesResult.from_dict(query_notices_result_model_json).__dict__
    query_notices_result_model2 = QueryNoticesResult(**query_notices_result_model_dict)
    assert query_notices_result_model == query_notices_result_model2
    query_notices_result_model_json2 = query_notices_result_model.to_dict()
    assert query_notices_result_model_json2 == query_notices_result_model_json
    query_notices_result_model.set_properties({})
    actual_dict = query_notices_result_model.get_properties()
    assert actual_dict == {}
    expected_dict = {'foo': {'foo': 'bar'}}
    query_notices_result_model.set_properties(expected_dict)
    actual_dict = query_notices_result_model.get_properties()
    assert actual_dict == expected_dict","notice_model['notice_id'] = 'testString'
notice_model['document_id'] = 'testString'
notice_model['query_id'] = 'testString'
notice_model['step'] = 'testString'
notice_model['description'] = 'testString'
query_notices_result_model_json['id'] = 'testString'
query_notices_result_model_json['collection_id'] = 'testString'
query_notices_result_model_json['filename'] = 'testString'
query_notices_result_model_json['sha1'] = 'testString'",notice_model['notice_id'] = notice_model['document_id'] = notice_model['query_id'] = notice_model['step'] = notice_model['description'] = query_notices_result_model_json['id'] = query_notices_result_model_json['collection_id'] = query_notices_result_model_json['filename'] = query_notices_result_model_json['sha1'] = 'testString',,,,,,,,,,,
not-youtube-dl,https://github.com/scastillo/not-youtube-dl/tree/master/test/test_subtitles.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/test/test_subtitles.py,TestMTVSubtitles,"def test_allsubtitles(self):
    self.DL.params['writesubtitles'] = True
    self.DL.params['allsubtitles'] = True
    subtitles = self.getSubtitles()
    self.assertEqual(set(subtitles.keys()), set(['en']))
    self.assertEqual(md5(subtitles['en']), '78206b8d8a0cfa9da64dc026eea48961')","self.DL.params['writesubtitles'] = True
self.DL.params['allsubtitles'] = True",self.DL.params['writesubtitles'] = self.DL.params['allsubtitles'] = True,,,,,,,,,,,
sentry,https://github.com/getsentry/sentry/tree/master/src/sentry/tagstore/snuba/backend.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sentry/src/sentry/tagstore/snuba/backend.py,SnubaTagStorage,"def __get_tag_keys_for_projects(self, projects, group, environments, start, end, limit=1000, keys=None, include_values_seen=True, use_cache=False, include_transactions=False, denylist=None, **kwargs):
    """"""Query snuba for tag keys based on projects

        When use_cache is passed, we'll attempt to use the cache. There's an exception if group_id was passed
        which refines the query enough caching isn't required.
        The cache key is based on the filters being passed so that different queries don't hit the same cache, with
        exceptions for start and end dates. Since even a microsecond passing would result in a different caching
        key, which means always missing the cache.
        Instead, to keep the cache key the same for a short period we append the duration, and the end time rounded
        with a certain jitter to the cache key.
        This jitter is based on the hash of the key before duration/end time is added for consistency per query.
        The jitter's intent is to avoid a dogpile effect of many queries being invalidated at the same time.
        This is done by changing the rounding of the end key to a random offset. See snuba.quantize_time for
        further explanation of how that is done.
        """"""
    (default_start, default_end) = default_start_end_dates()
    if start is None:
        start = default_start
    if end is None:
        end = default_end
    dataset = Dataset.Events
    if include_transactions:
        dataset = Dataset.Discover
    conditions = []
    aggregations = [['count()', '', 'count']]
    filters = {'project_id': sorted(projects)}
    if environments:
        filters['environment'] = sorted(environments)
    if group is not None:
        (dataset, conditions, filters) = self.apply_group_filters_conditions(group, conditions, filters)
    if keys is not None:
        filters['tags_key'] = sorted(keys)
    if include_values_seen:
        aggregations.append(['uniq', 'tags_value', 'values_seen'])
    should_cache = use_cache and group is None
    result = None
    cache_key = None
    if should_cache:
        filtering_strings = [f'{key}={value}' for (key, value) in filters.items()]
        filtering_strings.append(f'dataset={dataset.name}')
        cache_key = 'tagstore.__get_tag_keys:{}'.format(md5_text(*filtering_strings).hexdigest())
        key_hash = hash(cache_key)
        duration = (end - start).total_seconds()
        end = snuba.quantize_time(end, key_hash)
        cache_key += f':{duration}@{end.isoformat()}'
        result = cache.get(cache_key, None)
        if result is not None:
            metrics.incr('testing.tagstore.cache_tag_key.hit')
        else:
            metrics.incr('testing.tagstore.cache_tag_key.miss')
    if result is None:
        result = snuba.query(dataset=dataset, start=start, end=end, groupby=['tags_key'], conditions=conditions, filter_keys=filters, aggregations=aggregations, limit=limit, orderby='-count', referrer='tagstore.__get_tag_keys', **kwargs)
        if should_cache:
            cache.set(cache_key, result, 300)
            metrics.incr('testing.tagstore.cache_tag_key.len', amount=len(result))
    if group is None:
        ctor = TagKey
    else:
        ctor = functools.partial(GroupTagKey, group_id=group.id)
    results = set()
    for (key, data) in result.items():
        if denylist is not None and key in denylist:
            continue
        params = {'key': key}
        if include_values_seen:
            params['values_seen'] = data['values_seen']
            params['count'] = data['count']
        else:
            params['count'] = data
        results.add(ctor(**params))
    return results","result = None
cache_key = None",result = cache_key = None,,,,,,,,,,,
doit,https://github.com/pydoit/doit/tree/master/doit/runner.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/doit/doit/runner.py,MRunner,"def __init__(self, dep_manager, reporter, continue_=False, always_execute=False, stream=None, num_process=1):
    Runner.__init__(self, dep_manager, reporter, continue_=continue_, always_execute=always_execute, stream=stream)
    self.num_process = num_process
    self.free_proc = 0
    self.task_dispatcher = None
    self.tasks = None
    self.result_q = None","self.task_dispatcher = None
self.tasks = None
self.result_q = None",self.task_dispatcher = self.tasks = self.result_q = None,,,,,,,,,,,
Realistic-Neural-Talking-Head-Models,https://github.com/vincent-thevenin/Realistic-Neural-Talking-Head-Models/tree/master/dataset/preprocess.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Realistic-Neural-Talking-Head-Models/dataset/preprocess.py,,"def pick_images(video_path, pic_folder, num_images):
    cap = cv2.VideoCapture(video_path)
    n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    idxes = [1 if i % (n_frames // num_images + 1) == 0 else 0 for i in range(n_frames)]
    frames_list = []
    ret = True
    frame_idx = 0
    frame_counter = 0
    while ret and frame_idx < n_frames:
        (ret, frame) = cap.read()
        if ret and idxes[frame_idx] == 1:
            RGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames_list.append(RGB)
        frame_idx += 1
    cap.release()
    return frames_list","frame_idx = 0
frame_counter = 0",frame_idx = frame_counter = 0,,,,,,,,,,,
saleor,https://github.com/saleor/saleor/tree/master/saleor/product/utils/availability.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/saleor/saleor/product/utils/availability.py,,"def get_product_availability(*, product: Product, product_channel_listing: Optional[ProductChannelListing], variants: Iterable[ProductVariant], variants_channel_listing: List[ProductVariantChannelListing], collections: Iterable[Collection], discounts: Iterable[DiscountInfo], channel: Channel, local_currency: Optional[str]=None, prices_entered_with_tax: bool, tax_calculation_strategy: str, tax_rate: Decimal) -> ProductAvailability:
    discounted = None
    discounted_net_range = get_product_price_range(product=product, variants=variants, variants_channel_listing=variants_channel_listing, collections=collections, discounts=discounts, channel=channel)
    if discounted_net_range is not None:
        discounted = TaxedMoneyRange(start=_calculate_product_price_with_taxes(discounted_net_range.start, tax_rate, tax_calculation_strategy, prices_entered_with_tax), stop=_calculate_product_price_with_taxes(discounted_net_range.stop, tax_rate, tax_calculation_strategy, prices_entered_with_tax))
    undiscounted = None
    undiscounted_net_range = get_product_price_range(product=product, variants=variants, variants_channel_listing=variants_channel_listing, collections=collections, discounts=[], channel=channel)
    if undiscounted_net_range is not None:
        undiscounted = TaxedMoneyRange(start=_calculate_product_price_with_taxes(undiscounted_net_range.start, tax_rate, tax_calculation_strategy, prices_entered_with_tax), stop=_calculate_product_price_with_taxes(undiscounted_net_range.stop, tax_rate, tax_calculation_strategy, prices_entered_with_tax))
    discount = None
    price_range_local = None
    discount_local_currency = None
    if undiscounted_net_range is not None and discounted_net_range is not None:
        discount = _get_total_discount_from_range(undiscounted, discounted)
        (price_range_local, discount_local_currency) = _get_product_price_range(discounted, undiscounted, local_currency)
    is_visible = product_channel_listing is not None and product_channel_listing.is_visible
    is_on_sale = is_visible and discount is not None
    return ProductAvailability(on_sale=is_on_sale, price_range=discounted, price_range_undiscounted=undiscounted, discount=discount, price_range_local_currency=price_range_local, discount_local_currency=discount_local_currency)","discount = None
price_range_local = None
discount_local_currency = None",discount = price_range_local = discount_local_currency = None,,,,,,,,,,,
plaso,https://github.com/log2timeline/plaso/tree/master/plaso/parsers/winreg_plugins/appcompatcache.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plaso/plaso/parsers/winreg_plugins/appcompatcache.py,AppCompatCacheHeader,"def __init__(self):
    """"""Initializes the header object.""""""
    super(AppCompatCacheHeader, self).__init__()
    self.number_of_cached_entries = 0
    self.header_size = 0","self.number_of_cached_entries = 0
self.header_size = 0",self.number_of_cached_entries = self.header_size = 0,,,,,,,,,,,
imgclsmob,https://github.com/osmr/imgclsmob/tree/master/chainer_/chainercv2/models/resnet_cub.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/imgclsmob/chainer_/chainercv2/models/resnet_cub.py,,"def _test():
    import numpy as np
    import chainer
    chainer.global_config.train = False
    pretrained = False
    models = [resnet10_cub, resnet12_cub, resnet14_cub, resnetbc14b_cub, resnet16_cub, resnet18_cub, resnet26_cub, resnetbc26b_cub, resnet34_cub, resnetbc38b_cub, resnet50_cub, resnet50b_cub, resnet101_cub, resnet101b_cub, resnet152_cub, resnet152b_cub, resnet200_cub, resnet200b_cub]
    for model in models:
        net = model(pretrained=pretrained)
        weight_count = net.count_params()
        print('m={}, {}'.format(model.__name__, weight_count))
        assert model != resnet10_cub or weight_count == 5008392
        assert model != resnet12_cub or weight_count == 5082376
        assert model != resnet14_cub or weight_count == 5377800
        assert model != resnetbc14b_cub or weight_count == 8425736
        assert model != resnet16_cub or weight_count == 6558472
        assert model != resnet18_cub or weight_count == 11279112
        assert model != resnet26_cub or weight_count == 17549832
        assert model != resnetbc26b_cub or weight_count == 14355976
        assert model != resnet34_cub or weight_count == 21387272
        assert model != resnetbc38b_cub or weight_count == 20286216
        assert model != resnet50_cub or weight_count == 23917832
        assert model != resnet50b_cub or weight_count == 23917832
        assert model != resnet101_cub or weight_count == 42909960
        assert model != resnet101b_cub or weight_count == 42909960
        assert model != resnet152_cub or weight_count == 58553608
        assert model != resnet152b_cub or weight_count == 58553608
        assert model != resnet200_cub or weight_count == 63034632
        assert model != resnet200b_cub or weight_count == 63034632
        x = np.zeros((1, 3, 224, 224), np.float32)
        y = net(x)
        assert y.shape == (1, 200)","chainer.global_config.train = False
pretrained = False",chainer.global_config.train = pretrained = False,,,,,,,,,,,
Paddle,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_tdm_sampler_op.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_tdm_sampler_op.py,TestCase5,"def config(self):
    """"""test normal neg""""""
    self.neg_samples_num_list = [1, 2, 3, 4]
    self.x_shape = (10, 1)
    self.x_type = 'int64'
    self.tree_dtype = 'int32'
    self.out_dtype = 'int64'","self.x_type = 'int64'
self.out_dtype = 'int64'",self.x_type = self.out_dtype = 'int64',,,,,,,,,,,
trezor-firmware,https://github.com/trezor/trezor-firmware/tree/master/python/src/trezorlib/transport/webusb.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/trezor-firmware/python/src/trezorlib/transport/webusb.py,,"def is_vendor_class(dev: 'usb1.USBDevice') -> bool:
    configurationId = 0
    altSettingId = 0
    return dev[configurationId][INTERFACE][altSettingId].getClass() == usb1.libusb1.LIBUSB_CLASS_VENDOR_SPEC","configurationId = 0
altSettingId = 0",configurationId = altSettingId = 0,,,,,,,,,,,
PyContrast,https://github.com/HobbitLong/PyContrast/tree/master/pycontrast/options/base_options.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyContrast/pycontrast/options/base_options.py,BaseOptions,"def __init__(self):
    self.initialized = False
    self.parser = None
    self.opt = None
    self.override_dict = {'InsDis': ['RGB', False, 'bank', 'A', 'linear', 0.07], 'CMC': ['CMC', False, 'bank', 'C', 'linear', 0.07], 'MoCo': ['RGB', False, 'moco', 'A', 'linear', 0.07], 'PIRL': ['RGB', True, 'bank', 'A', 'linear', 0.07], 'MoCov2': ['RGB', False, 'moco', 'B', 'mlp', 0.2], 'CMCv2': ['CMC', False, 'moco', 'E', 'mlp', 0.2], 'InfoMin': ['RGB', True, 'moco', 'D', 'mlp', 0.15]}","self.parser = None
self.opt = None",self.parser = self.opt = None,,,,,,,,,,,
nlp-recipes,https://github.com/microsoft/nlp-recipes/tree/master/examples/sentence_similarity/gensen_train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp-recipes/examples/sentence_similarity/gensen_train.py,,"def train(config, data_folder, learning_rate=0.0001, max_epoch=None):
    """""" Train the Gensen model.

    Args:
        max_epoch(int): Limit training to specified number of epochs.
        config(dict): Loaded json file as a python object.
        data_folder(str): Path to the folder containing the data.
        learning_rate(float): Learning rate for the model.
    """"""
    owd = os.getcwd()
    os.chdir(data_folder)
    try:
        with mlflow.start_run():
            save_dir = config['data']['save_dir']
            if not os.path.exists('./log'):
                os.makedirs('./log')
            os.makedirs(save_dir, exist_ok=True)
            setup_logging(config)
            batch_size = config['training']['batch_size']
            src_vocab_size = config['model']['n_words_src']
            trg_vocab_size = config['model']['n_words_trg']
            max_len_src = config['data']['max_src_length']
            max_len_trg = config['data']['max_trg_length']
            model_state = {}
            train_src = [item['train_src'] for item in config['data']['paths']]
            train_trg = [item['train_trg'] for item in config['data']['paths']]
            tasknames = [item['taskname'] for item in config['data']['paths']]
            if 'skipthought_next' in tasknames and 'skipthought_previous' in tasknames:
                skipthought_idx = tasknames.index('skipthought_next')
                skipthought_backward_idx = tasknames.index('skipthought_previous')
                paired_tasks = {skipthought_idx: skipthought_backward_idx, skipthought_backward_idx: skipthought_idx}
            else:
                paired_tasks = None
                skipthought_idx = None
                skipthought_backward_idx = None
            train_iterator = BufferedDataIterator(train_src, train_trg, src_vocab_size, trg_vocab_size, tasknames, save_dir, buffer_size=1000000.0, lowercase=True, seed=(hvd.rank() + 1) * 12345)
            nli_iterator = NLIIterator(train=config['data']['nli_train'], dev=config['data']['nli_dev'], test=config['data']['nli_test'], vocab_size=-1, vocab=os.path.join(save_dir, 'src_vocab.pkl'), seed=(hvd.rank() + 1) * 12345)
            src_vocab_size = len(train_iterator.src[0]['word2id'])
            trg_vocab_size = len(train_iterator.trg[0]['word2id'])
            logging.info('Finished creating iterator ...')
            log_config(config)
            logging.info('Found %d words in source : ' % len(train_iterator.src[0]['id2word']))
            for (idx, taskname) in enumerate(tasknames):
                logging.info('Found %d target words in task %s ' % (len(train_iterator.trg[idx]['id2word']), taskname))
            logging.info('Found %d words in src ' % src_vocab_size)
            logging.info('Found %d words in trg ' % trg_vocab_size)
            weight_mask = torch.ones(trg_vocab_size).cuda()
            weight_mask[train_iterator.trg[0]['word2id']['<pad>']] = 0
            loss_criterion = nn.CrossEntropyLoss(weight=weight_mask).cuda()
            nli_criterion = nn.CrossEntropyLoss().cuda()
            model = MultitaskModel(src_emb_dim=config['model']['dim_word_src'], trg_emb_dim=config['model']['dim_word_trg'], src_vocab_size=src_vocab_size, trg_vocab_size=trg_vocab_size, src_hidden_dim=config['model']['dim_src'], trg_hidden_dim=config['model']['dim_trg'], bidirectional=config['model']['bidirectional'], pad_token_src=train_iterator.src[0]['word2id']['<pad>'], pad_token_trg=train_iterator.trg[0]['word2id']['<pad>'], nlayers_src=config['model']['n_layers_src'], dropout=config['model']['dropout'], num_tasks=len(train_iterator.src), paired_tasks=paired_tasks).cuda()
            optimizer = setup_horovod(model, learning_rate=learning_rate)
            logging.info(model)
            n_gpus = config['training']['n_gpus']
            model = torch.nn.DataParallel(model, device_ids=range(n_gpus))
            task_losses = [[] for _ in tasknames]
            task_idxs = [0 for _ in tasknames]
            nli_losses = []
            updates = 0
            nli_ctr = 0
            nli_epoch = 0
            monitor_epoch = 0
            nli_mbatch_ctr = 0
            mbatch_times = []
            min_val_loss = 10000000
            min_val_loss_epoch = -1
            rng_num_tasks = len(tasknames) - 1 if paired_tasks else len(tasknames)
            logging.info('OS Environ: \n {} \n\n'.format(os.environ))
            mlflow.log_param('learning_rate', learning_rate)
            logging.info('Commencing Training ...')
            start = time.time()
            while True:
                batch_start_time = time.time()
                if nli_ctr % 10 == 0:
                    minibatch = nli_iterator.get_parallel_minibatch(nli_mbatch_ctr, batch_size * n_gpus)
                    optimizer.zero_grad()
                    class_logits = model(minibatch, -1, return_hidden=False, paired_trg=None)
                    loss = nli_criterion(class_logits.contiguous().view(-1, class_logits.size(1)), minibatch['labels'].contiguous().view(-1))
                    nli_losses.append(loss.item())
                    loss.backward()
                    torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)
                    optimizer.step()
                    nli_mbatch_ctr += batch_size * n_gpus
                    if nli_mbatch_ctr >= len(nli_iterator.train_lines):
                        nli_mbatch_ctr = 0
                        nli_epoch += 1
                else:
                    task_idx = np.random.randint(low=0, high=rng_num_tasks)
                    minibatch = train_iterator.get_parallel_minibatch(task_idx, task_idxs[task_idx], batch_size * n_gpus, max_len_src, max_len_trg)
                    'Increment pointer into task and if current buffer is\n                    exhausted, fetch new buffer. '
                    task_idxs[task_idx] += batch_size * n_gpus
                    if task_idxs[task_idx] >= train_iterator.buffer_size:
                        train_iterator.fetch_buffer(task_idx)
                        task_idxs[task_idx] = 0
                    if task_idx == skipthought_idx:
                        minibatch_back = train_iterator.get_parallel_minibatch(skipthought_backward_idx, task_idxs[skipthought_backward_idx], batch_size * n_gpus, max_len_src, max_len_trg)
                        task_idxs[skipthought_backward_idx] += batch_size * n_gpus
                        if task_idxs[skipthought_backward_idx] >= train_iterator.buffer_size:
                            train_iterator.fetch_buffer(skipthought_backward_idx)
                            task_idxs[skipthought_backward_idx] = 0
                        optimizer.zero_grad()
                        (decoder_logit, decoder_logit_2) = model(minibatch, task_idx, paired_trg=minibatch_back['input_trg'])
                        loss_f = loss_criterion(decoder_logit.contiguous().view(-1, decoder_logit.size(2)), minibatch['output_trg'].contiguous().view(-1))
                        loss_b = loss_criterion(decoder_logit_2.contiguous().view(-1, decoder_logit_2.size(2)), minibatch_back['output_trg'].contiguous().view(-1))
                        task_losses[task_idx].append(loss_f.data[0])
                        task_losses[skipthought_backward_idx].append(loss_b.data[0])
                        loss = loss_f + loss_b
                    else:
                        optimizer.zero_grad()
                        decoder_logit = model(minibatch, task_idx)
                        loss = loss_criterion(decoder_logit.contiguous().view(-1, decoder_logit.size(2)), minibatch['output_trg'].contiguous().view(-1))
                        task_losses[task_idx].append(loss.item())
                    loss.backward()
                    optimizer.synchronize()
                    torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)
                    optimizer.step()
                end = time.time()
                mbatch_times.append(end - batch_start_time)
                if updates % config['management']['monitor_loss'] == 0 and updates != 0:
                    monitor_epoch += 1
                    for (idx, task) in enumerate(tasknames):
                        logging.info('Seq2Seq Examples Processed : %d %s Loss : %.5f Num %s minibatches : %d' % (updates, task, np.mean(task_losses[idx]), task, len(task_losses[idx])))
                        mlflow.log_metric('validation_loss', np.mean(task_losses[idx]), step=monitor_epoch)
                    logging.info('Round: %d NLI Epoch : %d NLI Examples Processed : %d NLI Loss : %.5f ' % (nli_ctr, nli_epoch, nli_mbatch_ctr, np.mean(nli_losses)))
                    mlflow.log_metric('nli_loss', np.mean(nli_losses), step=nli_epoch)
                    logging.info('Average time per minibatch : %.5f' % np.mean(mbatch_times))
                    mlflow.log_metric('minibatch_avg_duration', np.mean(mbatch_times))
                    task_losses = [[] for _ in tasknames]
                    mbatch_times = []
                    nli_losses = []
                    logging.info('############################')
                    logging.info('##### Evaluating model #####')
                    logging.info('############################')
                    (training_complete, min_val_loss_epoch, min_val_loss, model_state) = evaluate(config=config, train_iterator=train_iterator, model=model, loss_criterion=loss_criterion, monitor_epoch=monitor_epoch, min_val_loss=min_val_loss, min_val_loss_epoch=min_val_loss_epoch, save_dir=save_dir, starting_time=start, model_state=model_state, max_epoch=max_epoch)
                    if training_complete:
                        mlflow.log_metric('min_val_loss', float(min_val_loss))
                        mlflow.log_metric('learning_rate', learning_rate)
                        break
                    logging.info('Evaluating on NLI')
                    evaluate_nli(nli_iterator=nli_iterator, model=model, n_gpus=n_gpus, batch_size=batch_size)
                updates += batch_size * n_gpus
                nli_ctr += 1
                logging.info('Updates: %d' % updates)
    finally:
        os.chdir(owd)","updates = 0
nli_ctr = 0
nli_epoch = 0
monitor_epoch = 0
nli_mbatch_ctr = 0",updates = nli_ctr = nli_epoch = monitor_epoch = nli_mbatch_ctr = 0,,,,,,,,,,,
nlp-recipes,https://github.com/microsoft/nlp-recipes/tree/master/examples/sentence_similarity/gensen_train.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp-recipes/examples/sentence_similarity/gensen_train.py,,"def train(config, data_folder, learning_rate=0.0001, max_epoch=None):
    """""" Train the Gensen model.

    Args:
        max_epoch(int): Limit training to specified number of epochs.
        config(dict): Loaded json file as a python object.
        data_folder(str): Path to the folder containing the data.
        learning_rate(float): Learning rate for the model.
    """"""
    owd = os.getcwd()
    os.chdir(data_folder)
    try:
        with mlflow.start_run():
            save_dir = config['data']['save_dir']
            if not os.path.exists('./log'):
                os.makedirs('./log')
            os.makedirs(save_dir, exist_ok=True)
            setup_logging(config)
            batch_size = config['training']['batch_size']
            src_vocab_size = config['model']['n_words_src']
            trg_vocab_size = config['model']['n_words_trg']
            max_len_src = config['data']['max_src_length']
            max_len_trg = config['data']['max_trg_length']
            model_state = {}
            train_src = [item['train_src'] for item in config['data']['paths']]
            train_trg = [item['train_trg'] for item in config['data']['paths']]
            tasknames = [item['taskname'] for item in config['data']['paths']]
            if 'skipthought_next' in tasknames and 'skipthought_previous' in tasknames:
                skipthought_idx = tasknames.index('skipthought_next')
                skipthought_backward_idx = tasknames.index('skipthought_previous')
                paired_tasks = {skipthought_idx: skipthought_backward_idx, skipthought_backward_idx: skipthought_idx}
            else:
                paired_tasks = None
                skipthought_idx = None
                skipthought_backward_idx = None
            train_iterator = BufferedDataIterator(train_src, train_trg, src_vocab_size, trg_vocab_size, tasknames, save_dir, buffer_size=1000000.0, lowercase=True, seed=(hvd.rank() + 1) * 12345)
            nli_iterator = NLIIterator(train=config['data']['nli_train'], dev=config['data']['nli_dev'], test=config['data']['nli_test'], vocab_size=-1, vocab=os.path.join(save_dir, 'src_vocab.pkl'), seed=(hvd.rank() + 1) * 12345)
            src_vocab_size = len(train_iterator.src[0]['word2id'])
            trg_vocab_size = len(train_iterator.trg[0]['word2id'])
            logging.info('Finished creating iterator ...')
            log_config(config)
            logging.info('Found %d words in source : ' % len(train_iterator.src[0]['id2word']))
            for (idx, taskname) in enumerate(tasknames):
                logging.info('Found %d target words in task %s ' % (len(train_iterator.trg[idx]['id2word']), taskname))
            logging.info('Found %d words in src ' % src_vocab_size)
            logging.info('Found %d words in trg ' % trg_vocab_size)
            weight_mask = torch.ones(trg_vocab_size).cuda()
            weight_mask[train_iterator.trg[0]['word2id']['<pad>']] = 0
            loss_criterion = nn.CrossEntropyLoss(weight=weight_mask).cuda()
            nli_criterion = nn.CrossEntropyLoss().cuda()
            model = MultitaskModel(src_emb_dim=config['model']['dim_word_src'], trg_emb_dim=config['model']['dim_word_trg'], src_vocab_size=src_vocab_size, trg_vocab_size=trg_vocab_size, src_hidden_dim=config['model']['dim_src'], trg_hidden_dim=config['model']['dim_trg'], bidirectional=config['model']['bidirectional'], pad_token_src=train_iterator.src[0]['word2id']['<pad>'], pad_token_trg=train_iterator.trg[0]['word2id']['<pad>'], nlayers_src=config['model']['n_layers_src'], dropout=config['model']['dropout'], num_tasks=len(train_iterator.src), paired_tasks=paired_tasks).cuda()
            optimizer = setup_horovod(model, learning_rate=learning_rate)
            logging.info(model)
            n_gpus = config['training']['n_gpus']
            model = torch.nn.DataParallel(model, device_ids=range(n_gpus))
            task_losses = [[] for _ in tasknames]
            task_idxs = [0 for _ in tasknames]
            nli_losses = []
            updates = 0
            nli_ctr = 0
            nli_epoch = 0
            monitor_epoch = 0
            nli_mbatch_ctr = 0
            mbatch_times = []
            min_val_loss = 10000000
            min_val_loss_epoch = -1
            rng_num_tasks = len(tasknames) - 1 if paired_tasks else len(tasknames)
            logging.info('OS Environ: \n {} \n\n'.format(os.environ))
            mlflow.log_param('learning_rate', learning_rate)
            logging.info('Commencing Training ...')
            start = time.time()
            while True:
                batch_start_time = time.time()
                if nli_ctr % 10 == 0:
                    minibatch = nli_iterator.get_parallel_minibatch(nli_mbatch_ctr, batch_size * n_gpus)
                    optimizer.zero_grad()
                    class_logits = model(minibatch, -1, return_hidden=False, paired_trg=None)
                    loss = nli_criterion(class_logits.contiguous().view(-1, class_logits.size(1)), minibatch['labels'].contiguous().view(-1))
                    nli_losses.append(loss.item())
                    loss.backward()
                    torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)
                    optimizer.step()
                    nli_mbatch_ctr += batch_size * n_gpus
                    if nli_mbatch_ctr >= len(nli_iterator.train_lines):
                        nli_mbatch_ctr = 0
                        nli_epoch += 1
                else:
                    task_idx = np.random.randint(low=0, high=rng_num_tasks)
                    minibatch = train_iterator.get_parallel_minibatch(task_idx, task_idxs[task_idx], batch_size * n_gpus, max_len_src, max_len_trg)
                    'Increment pointer into task and if current buffer is\n                    exhausted, fetch new buffer. '
                    task_idxs[task_idx] += batch_size * n_gpus
                    if task_idxs[task_idx] >= train_iterator.buffer_size:
                        train_iterator.fetch_buffer(task_idx)
                        task_idxs[task_idx] = 0
                    if task_idx == skipthought_idx:
                        minibatch_back = train_iterator.get_parallel_minibatch(skipthought_backward_idx, task_idxs[skipthought_backward_idx], batch_size * n_gpus, max_len_src, max_len_trg)
                        task_idxs[skipthought_backward_idx] += batch_size * n_gpus
                        if task_idxs[skipthought_backward_idx] >= train_iterator.buffer_size:
                            train_iterator.fetch_buffer(skipthought_backward_idx)
                            task_idxs[skipthought_backward_idx] = 0
                        optimizer.zero_grad()
                        (decoder_logit, decoder_logit_2) = model(minibatch, task_idx, paired_trg=minibatch_back['input_trg'])
                        loss_f = loss_criterion(decoder_logit.contiguous().view(-1, decoder_logit.size(2)), minibatch['output_trg'].contiguous().view(-1))
                        loss_b = loss_criterion(decoder_logit_2.contiguous().view(-1, decoder_logit_2.size(2)), minibatch_back['output_trg'].contiguous().view(-1))
                        task_losses[task_idx].append(loss_f.data[0])
                        task_losses[skipthought_backward_idx].append(loss_b.data[0])
                        loss = loss_f + loss_b
                    else:
                        optimizer.zero_grad()
                        decoder_logit = model(minibatch, task_idx)
                        loss = loss_criterion(decoder_logit.contiguous().view(-1, decoder_logit.size(2)), minibatch['output_trg'].contiguous().view(-1))
                        task_losses[task_idx].append(loss.item())
                    loss.backward()
                    optimizer.synchronize()
                    torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)
                    optimizer.step()
                end = time.time()
                mbatch_times.append(end - batch_start_time)
                if updates % config['management']['monitor_loss'] == 0 and updates != 0:
                    monitor_epoch += 1
                    for (idx, task) in enumerate(tasknames):
                        logging.info('Seq2Seq Examples Processed : %d %s Loss : %.5f Num %s minibatches : %d' % (updates, task, np.mean(task_losses[idx]), task, len(task_losses[idx])))
                        mlflow.log_metric('validation_loss', np.mean(task_losses[idx]), step=monitor_epoch)
                    logging.info('Round: %d NLI Epoch : %d NLI Examples Processed : %d NLI Loss : %.5f ' % (nli_ctr, nli_epoch, nli_mbatch_ctr, np.mean(nli_losses)))
                    mlflow.log_metric('nli_loss', np.mean(nli_losses), step=nli_epoch)
                    logging.info('Average time per minibatch : %.5f' % np.mean(mbatch_times))
                    mlflow.log_metric('minibatch_avg_duration', np.mean(mbatch_times))
                    task_losses = [[] for _ in tasknames]
                    mbatch_times = []
                    nli_losses = []
                    logging.info('############################')
                    logging.info('##### Evaluating model #####')
                    logging.info('############################')
                    (training_complete, min_val_loss_epoch, min_val_loss, model_state) = evaluate(config=config, train_iterator=train_iterator, model=model, loss_criterion=loss_criterion, monitor_epoch=monitor_epoch, min_val_loss=min_val_loss, min_val_loss_epoch=min_val_loss_epoch, save_dir=save_dir, starting_time=start, model_state=model_state, max_epoch=max_epoch)
                    if training_complete:
                        mlflow.log_metric('min_val_loss', float(min_val_loss))
                        mlflow.log_metric('learning_rate', learning_rate)
                        break
                    logging.info('Evaluating on NLI')
                    evaluate_nli(nli_iterator=nli_iterator, model=model, n_gpus=n_gpus, batch_size=batch_size)
                updates += batch_size * n_gpus
                nli_ctr += 1
                logging.info('Updates: %d' % updates)
    finally:
        os.chdir(owd)","paired_tasks = None
skipthought_idx = None
skipthought_backward_idx = None",paired_tasks = skipthought_idx = skipthought_backward_idx = None,,,,,,,,,,,
tvm,https://github.com/apache/tvm/tree/master/python/tvm/te/hybrid/parser.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/python/tvm/te/hybrid/parser.py,HybridParser,"def __init__(self, args, usage, symbols, closure_vars, func_name=None):
    """"""
        Parameters
        ----------
        args: A list of tvm.te.placeholder or te.var
            Provided by the user, the argument list of the function to be lowered.

        usage: A dict of variables used in last in this function
            Provided by last lower pass, which collects this information

        symbols : list of str
            The symbol list of the global context of the function.

        closure_vars: dict
            A dict of external name reference captured by this function.

        Returns
        -------
        func_name: str
            The name of the function to be lowered; if not provided,
            the compiler will use the name in the AST
        """"""
    self.args = list(args)
    self.usage = usage.copy()
    self.symbols = {}
    for (k, v) in symbols.items():
        if isinstance(v, types.FunctionType):
            self.add_symbol(k, Symbol.Callable, v)
    self.closure_vars = closure_vars
    self.binds = {}
    self.device = 0
    self.func_name = func_name
    self.outputs = []
    self.side_effect = set()
    self.parsed_body = None
    self.analyzer = tvm.arith.Analyzer()
    self.returned = False","self.device = 0
self.returned = False",self.device = self.returned = 0,,,,,,,,,,,
scipy,https://github.com/scipy/scipy/tree/master/scipy/stats/_continuous_distns.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/scipy/scipy/stats/_continuous_distns.py,,"def _norm_logcdfprime(z):
    assert np.all(z <= -10)
    lhs = -z - 1 / z
    denom_cons = 1 / z ** 2
    numerator = 1
    pwr = 1.0
    (denom_total, numerator_total) = (0, 0)
    sign = -1
    for i in range(1, 11):
        pwr *= denom_cons
        numerator *= 2 * i - 1
        term = sign * numerator * pwr
        denom_total += term
        numerator_total += term * (2 * i) / z
        sign = -sign
    return lhs - numerator_total / (1 + denom_total)","numerator = 1
pwr = 1.0",numerator = pwr = 1.0,,,,,,,,,,,
keras,https://github.com/keras-team/keras/tree/master/keras/optimizer_v2/optimizer_v2_test.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keras/keras/optimizer_v2/optimizer_v2_test.py,OptimizerTest,"def testOptimizerWithCallableVarList(self):
    train_samples = 20
    input_dim = 1
    num_classes = 2
    ((x, y), _) = testing_utils.get_test_data(train_samples=train_samples, test_samples=10, input_shape=(input_dim,), num_classes=num_classes)
    y = np_utils.to_categorical(y)
    num_hidden = 1
    model = testing_utils.get_small_sequential_mlp(num_hidden=num_hidden, num_classes=num_classes)
    opt = adam.Adam()
    loss = lambda : losses.mean_squared_error(model(x), y)
    var_list = lambda : model.trainable_weights
    with self.assertRaisesRegex(ValueError, 'Weights for model .* have not yet been created'):
        var_list()
    train_op = opt.minimize(loss, var_list)
    if not tf.executing_eagerly():
        self.evaluate(tf.compat.v1.global_variables_initializer())
        self.assertEqual([[0.0]], self.evaluate(opt.get_slot(var_list()[0], 'm')))
        self.evaluate(train_op)
    self.assertNotEqual([[0.0]], self.evaluate(opt.get_slot(var_list()[0], 'm')))
    self.assertLen(var_list(), 4)","input_dim = 1
num_hidden = 1",input_dim = num_hidden = 1,,,,,,,,,,,
plaso,https://github.com/log2timeline/plaso/tree/master/plaso/parsers/winreg_plugins/usb.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plaso/plaso/parsers/winreg_plugins/usb.py,USBPlugin,"def ExtractEvents(self, parser_mediator, registry_key, **kwargs):
    """"""Extracts events from a Windows Registry key.

    Args:
      parser_mediator (ParserMediator): mediates interactions between parsers
          and other components, such as storage and dfVFS.
      registry_key (dfwinreg.WinRegistryKey): Windows Registry key.
    """"""
    event_data = WindowsUSBDeviceEventData()
    event_data.key_path = registry_key.path
    for subkey in registry_key.GetSubkeys():
        event_data.subkey_name = subkey.name
        vendor_identification = None
        product_identification = None
        try:
            subkey_name_parts = subkey.name.split('&')
            if len(subkey_name_parts) >= 2:
                vendor_identification = subkey_name_parts[0]
                product_identification = subkey_name_parts[1]
        except ValueError as exception:
            logger.warning('Unable to split string: {0:s} with error: {1!s}'.format(subkey.name, exception))
        event_data.vendor = vendor_identification
        event_data.product = product_identification
        for devicekey in subkey.GetSubkeys():
            event_data.last_written_time = devicekey.last_written_time
            event_data.serial = devicekey.name
            parser_mediator.ProduceEventData(event_data)","vendor_identification = None
product_identification = None",vendor_identification = product_identification = None,,,,,,,,,,,
violent-python3,https://github.com/EONRaider/violent-python3/tree/master/chapter02/ftp_default_pages.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/violent-python3/chapter02/ftp_default_pages.py,,"if __name__ == '__main__':
    tgt_host = '192.168.95.179'
    username = 'guest'
    password = 'guest'
    ftp_conn = ftplib.FTP(tgt_host)
    ftp_conn.login(username, password)
    return_default(ftp_conn)","username = 'guest'
password = 'guest'",username = password = 'guest',,,,,,,,,,,
pychess,https://github.com/pychess/pychess/tree/master/lib/pychess/widgets/enginesDialog.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pychess/lib/pychess/widgets/enginesDialog.py,EnginesDialog,"def __init__(self, widgets):
    self.widgets = widgets
    self.dialog = self.widgets['manage_engines_dialog']
    self.cur_engine = None
    self.default_workdir = getEngineDataPrefix()
    uistuff.keepWindowSize('engineswindow', self.dialog)
    self.allstore = Gtk.ListStore(Pixbuf, str)
    self.tv = self.widgets['engines_treeview']
    self.tv.set_model(self.allstore)
    self.tv.append_column(Gtk.TreeViewColumn('Flag', Gtk.CellRendererPixbuf(), pixbuf=0))
    name_renderer = Gtk.CellRendererText()
    name_renderer.set_property('editable', False)
    self.tv.append_column(Gtk.TreeViewColumn('Name', name_renderer, text=1))
    protocol_combo = self.widgets['engine_protocol_combo']
    protocol_combo.set_name('engine_protocol_combo')
    cell = Gtk.CellRendererText()
    protocol_combo.pack_start(cell, True)
    protocol_combo.add_attribute(cell, 'text', 0)
    self.options_store = Gtk.ListStore(str, str, GObject.TYPE_PYOBJECT)
    optv = self.widgets['options_treeview']
    optv.set_model(self.options_store)
    optv.append_column(Gtk.TreeViewColumn('  ', Gtk.CellRendererText(), text=0))
    optv.append_column(Gtk.TreeViewColumn(_('Option'), Gtk.CellRendererText(), text=1))
    optv.append_column(Gtk.TreeViewColumn(_('Value'), KeyValueCellRenderer(self.options_store), data=2))
    self.update_store()

    def do_update_store(self, *args):
        GLib.idle_add(engine_dialog.update_store)
    discoverer.connect_after('engine_discovered', do_update_store)

    def remove(button):
        if self.cur_engine is not None:
            self.widgets['remove_engine_button'].set_sensitive(False)
            discoverer.removeEngine(self.cur_engine)
            selection = self.tv.get_selection()
            result = selection.get_selected()
            if result is not None:
                (model, ts_iter) = result
                model.remove(ts_iter)
            if model.iter_n_children() == 0:
                clearView()
            discoverer.emit('all_engines_discovered')
    self.widgets['remove_engine_button'].connect('clicked', remove)
    engine_chooser_dialog = Gtk.FileChooserDialog(_('Select engine'), mainwindow(), Gtk.FileChooserAction.OPEN, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    filter = Gtk.FileFilter()
    filter.set_name(_('Executable files'))
    filter.add_mime_type('application/x-executable')
    filter.add_mime_type('application/x-sharedlib')
    filter.add_mime_type('application/x-ms-dos-executable')
    filter.add_mime_type('application/x-msdownload')
    filter.add_pattern('*.exe')
    for vm in VM_LIST:
        filter.add_pattern('*%s' % vm.ext)
    engine_chooser_dialog.add_filter(filter)
    self.add = False

    def add(button):
        self.add = True
        response = engine_chooser_dialog.run()
        if response == Gtk.ResponseType.OK:
            new_engine = engine_chooser_dialog.get_filename()
            binname = os.path.split(new_engine)[1]
            ext = os.path.splitext(new_engine)[1]
            if new_engine != '':
                for eng in discoverer.getEngines():
                    if eng['command'] == new_engine:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('The engine is already installed under the same name'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
                        break
            if new_engine != '':
                vm_name = None
                vm_args = None
                vmpath = ''
                for vm in VM_LIST:
                    if ext == vm.ext:
                        vm_name = vm.name
                        vm_args = vm.args
                        break
                if vm_name is None and new_engine.lower().endswith('.exe') and (sys.platform != 'win32'):
                    vm_name = 'wine'
                if vm_name is not None:
                    vmpath = shutil.which(vm_name, mode=os.R_OK | os.X_OK)
                    if vmpath is None:
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(vm_name + _(' is not installed'))
                        msg_dia.run()
                        msg_dia.hide()
                        new_engine = ''
            if new_engine:
                vm_ext_list = [vm.ext for vm in VM_LIST]
                if ext not in vm_ext_list and (not os.access(new_engine, os.X_OK)):
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>%s is not marked executable in the filesystem</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('Try chmod a+x %s' % new_engine))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
                try:
                    engine_command = []
                    if vmpath:
                        engine_command.append(vmpath)
                    if vm_args is not None:
                        engine_command += vm_args
                    engine_command.append(new_engine)
                    refeng = discoverer.getReferencedEngine(binname)
                    if refeng is not None and refeng['protocol'] == 'xboard':
                        checkers = [is_cecp, is_uci]
                    else:
                        checkers = [is_uci, is_cecp]
                    uci = False
                    for checker in checkers:
                        check_ok = checker(engine_command)
                        if check_ok:
                            uci = checker is is_uci
                            break
                    if not check_ok:
                        engine = discoverer.getEngineByName(self.cur_engine)
                        engine_chooser_dialog.set_filename(engine['command'])
                        msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                        msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                        msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                        msg_dia.run()
                        msg_dia.hide()
                        engine_chooser_dialog.hide()
                        self.add = False
                        engine_chooser_dialog.hide()
                        return
                    self.widgets['engine_command_entry'].set_text(new_engine)
                    self.widgets['engine_protocol_combo'].set_active(0 if uci else 1)
                    self.widgets['engine_args_entry'].set_text('')
                    protocol = 'uci' if uci else 'xboard'
                    discoverer.addEngine(binname, new_engine, protocol, vm_name, vm_args)
                    self.cur_engine = binname
                    self.add = False
                    discoverer.discover()
                except Exception:
                    msg_dia = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.ERROR, buttons=Gtk.ButtonsType.OK)
                    msg_dia.set_markup(_('<big><b>Unable to add %s</b></big>' % new_engine))
                    msg_dia.format_secondary_text(_('There is something wrong with this executable'))
                    msg_dia.run()
                    msg_dia.hide()
                    self.add = False
                    engine_chooser_dialog.hide()
                    return
            else:
                engine = discoverer.getEngineByName(self.cur_engine)
                engine_chooser_dialog.set_filename(engine['command'])
        engine_chooser_dialog.hide()
    self.widgets['add_engine_button'].connect('clicked', add)

    def addInMass(button):
        folder_dlg = Gtk.FileChooserDialog(_('Choose a folder'), None, Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
        answer = folder_dlg.run()
        path = folder_dlg.get_filename()
        folder_dlg.destroy()
        if answer != Gtk.ResponseType.OK:
            return False
        possibleFiles = listEnginesFromPath(path)

        def isNewEngine(path):
            sfn = os.path.basename(path)
            for engine in discoverer.getEngines():
                if sfn in engine.get('command'):
                    return False
            return True
        possibleFiles = [fn for fn in possibleFiles if isNewEngine(fn)]
        if len(possibleFiles) == 0:
            return False
        mass_dialog = self.widgets['engine_list_dialog']
        self.widgets['mass_path_label'].set_text(path)
        mass_list = self.widgets['mass_list_treeview']
        if len(mass_list.get_columns()) == 0:
            mass_store = Gtk.ListStore(bool, str)
            mass_list.set_model(mass_store)

            def checkbox_renderer_cb(cell, path, model):
                model[path][0] = not model[path][0]
                return
            checkbox_renderer = Gtk.CellRendererToggle()
            checkbox_renderer.set_property('activatable', True)
            checkbox_renderer.connect('toggled', checkbox_renderer_cb, mass_store)
            mass_list.append_column(Gtk.TreeViewColumn(_('Import'), checkbox_renderer, active=0))
            mass_list.append_column(Gtk.TreeViewColumn(_('File name'), Gtk.CellRendererText(), text=1))
        else:
            mass_store = mass_list.get_model()
        mass_store.clear()
        for fn in possibleFiles:
            mass_store.append([False, fn[len(path):]])
        answer = mass_dialog.run()
        mass_dialog.hide()
        if answer != Gtk.ResponseType.OK.real:
            return False
        self.add = True
        found = False
        for entry in mass_store:
            if entry[0]:
                newengine = discoverer.getReferencedEngine(path + entry[1])
                if newengine is not None:
                    discoverer.addEngineFromReference(newengine)
                    found = True
        self.add = False
        if found:
            discoverer.discover()
        return True
    self.widgets['mass_engine_button'].connect('clicked', addInMass)

    def clearView():
        self.selection = True
        self.cur_engine = None
        self.widgets['vm_command_entry'].set_text('')
        self.widgets['vm_args_entry'].set_text('')
        self.widgets['engine_command_entry'].set_text('')
        self.widgets['engine_args_entry'].set_text('')
        self.widgets['engine_protocol_combo'].set_active(0)
        self.widgets['engine_country_combo'].set_active(0)
        self.widgets['engine_comment_entry'].set_text('')
        self.widgets['engine_level_scale'].set_value(ENGINE_DEFAULT_LEVEL)
        self.options_store.clear()
        self.selection = False

    def vm_args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['vm_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('vm_args')
            if new_args != old_args:
                engine['vm_args'] = new_args.split()
    self.widgets['vm_args_entry'].connect('changed', vm_args_changed)

    def args_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_args = self.widgets['engine_args_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_args = engine.get('args')
            if new_args != old_args:
                engine['args'] = new_args.split()
    self.widgets['engine_args_entry'].connect('changed', args_changed)
    dir_chooser_dialog = Gtk.FileChooserDialog(_('Select working directory'), mainwindow(), Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OPEN, Gtk.ResponseType.OK))
    dir_chooser_button = Gtk.FileChooserButton.new_with_dialog(dir_chooser_dialog)
    self.widgets['dirChooserDock'].add(dir_chooser_button)
    dir_chooser_button.show()

    def select_dir(button):
        new_directory = dir_chooser_dialog.get_filename()
        engine = discoverer.getEngineByName(self.cur_engine)
        old_directory = engine.get('workingDirectory')
        if new_directory != old_directory and new_directory != self.default_workdir:
            engine['workingDirectory'] = new_directory
    dir_chooser_button.connect('current-folder-changed', select_dir)

    def protocol_changed(widget):
        if self.cur_engine is not None and (not self.add) and (not self.selection):
            active = self.widgets['engine_protocol_combo'].get_active()
            new_protocol = 'uci' if active == 0 else 'xboard'
            engine = discoverer.getEngineByName(self.cur_engine)
            old_protocol = engine['protocol']
            if new_protocol != old_protocol:
                command = engine.get('command')
                engine_command = []
                vm_command = engine.get('vm_command')
                if vm_command is not None:
                    engine_command.append(vm_command)
                    vm_args = engine.get('vm_args')
                    if vm_args is not None:
                        engine_command.append(', '.join(vm_args))
                engine_command.append(command)
                if new_protocol == 'uci':
                    check_ok = is_uci(engine_command)
                else:
                    check_ok = is_cecp(engine_command)
                if check_ok:
                    engine['protocol'] = new_protocol
                    engine['recheck'] = True
                    discoverer.discover()
                else:
                    widgets['engine_protocol_combo'].set_active(0 if old_protocol == 'uci' else 1)
    self.widgets['engine_protocol_combo'].connect('changed', protocol_changed)

    def country_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            old_country = discoverer.getCountry(engine)
            new_country = ISO3166_LIST[widget.get_active()].iso2
            if old_country != new_country:
                engine['country'] = new_country
                path = addDataPrefix('flags/%s.png' % new_country)
                if not os.path.isfile(path):
                    path = addDataPrefix('flags/unknown.png')
                item = self.tv.get_selection().get_selected()
                if item is not None:
                    (model, ts_iter) = item
                    model[ts_iter][0] = get_pixbuf(path)
                    discoverer.emit('all_engines_discovered')
    self.widgets['engine_country_combo'].connect('changed', country_changed)

    def country_keypressed(widget, event):
        idx = 0
        for iso in ISO3166_LIST:
            if idx != 0 and (ord(iso.country[0].lower()) == event.keyval or ord(iso.country[0].upper()) == event.keyval):
                widget.set_active(idx)
                break
            idx += 1
    self.widgets['engine_country_combo'].connect('key-press-event', country_keypressed)

    def comment_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_comment = self.widgets['engine_comment_entry'].get_text().strip()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_comment = engine.get('comment')
            if new_comment != old_comment:
                engine['comment'] = new_comment
    self.widgets['engine_comment_entry'].connect('changed', comment_changed)

    def level_changed(widget):
        if self.cur_engine is not None and (not self.selection):
            new_level = widget.get_value()
            engine = discoverer.getEngineByName(self.cur_engine)
            old_level = engine.get('level')
            if new_level != old_level:
                engine['level'] = int(new_level)
    self.widgets['engine_level_scale'].connect('value-changed', level_changed)
    self.selection = False

    def selection_changed(treeselection):
        (store, tv_iter) = self.tv.get_selection().get_selected()
        if tv_iter:
            self.selection = True
            path = store.get_path(tv_iter)
            indices = path.get_indices()
            row = indices[0]
            name = store[row][1]
            self.cur_engine = name
            engine = discoverer.getEngineByName(name)
            if 'PyChess.py' in engine['command']:
                self.widgets['remove_engine_button'].set_sensitive(False)
            else:
                self.widgets['remove_engine_button'].set_sensitive(True)
            self.widgets['engine_command_entry'].set_text(engine['command'])
            engine_chooser_dialog.set_filename(engine['command'])
            args = [] if engine.get('args') is None else engine.get('args')
            self.widgets['engine_args_entry'].set_text(' '.join(args))
            vm = engine.get('vm_command')
            self.widgets['vm_command_entry'].set_text(vm if vm is not None else '')
            args = [] if engine.get('vm_args') is None else engine.get('vm_args')
            self.widgets['vm_args_entry'].set_text(' '.join(args))
            directory = engine.get('workingDirectory')
            dir_choice = directory if directory is not None else self.default_workdir
            dir_chooser_dialog.set_current_folder(dir_choice)
            self.widgets['engine_protocol_combo'].set_active(0 if engine['protocol'] == 'uci' else 1)
            self.widgets['engine_country_combo'].set_active(0)
            country = discoverer.getCountry(engine)
            idx = 0
            for iso in ISO3166_LIST:
                if iso.iso2 == country:
                    self.widgets['engine_country_combo'].set_active(idx)
                    break
                idx += 1
            comment = engine.get('comment')
            self.widgets['engine_comment_entry'].set_text(comment if comment is not None else '')
            level = engine.get('level')
            try:
                level = int(level)
            except Exception:
                level = ENGINE_DEFAULT_LEVEL
            self.widgets['engine_level_scale'].set_value(level)
            self.update_options()
            self.selection = False
    tree_selection = self.tv.get_selection()
    tree_selection.connect('changed', selection_changed)
    tree_selection.select_path((0,))
    selection_changed(tree_selection)

    def engine_default_options(button):
        if self.cur_engine is not None and (not self.selection):
            engine = discoverer.getEngineByName(self.cur_engine)
            options = engine.get('options')
            if options:
                dialog = Gtk.MessageDialog(mainwindow(), type=Gtk.MessageType.QUESTION, buttons=Gtk.ButtonsType.YES_NO)
                dialog.set_markup(_('Do you really want to restore the default options of the engine ?'))
                response = dialog.run()
                dialog.destroy()
                if response == Gtk.ResponseType.YES:
                    for option in options:
                        if 'default' in option:
                            option['value'] = option['default']
                    self.update_options()
    self.widgets['engine_default_options_button'].connect('clicked', engine_default_options)","vm_name = None
vm_args = None",vm_name = vm_args = None,,,,,,,,,,,
PaddleSlim,https://github.com/PaddlePaddle/PaddleSlim/tree/master/demo/mkldnn_quant/sample_tester.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSlim/demo/mkldnn_quant/sample_tester.py,SampleTester,"def reader():
    with open(data_file, 'rb') as fp:
        num = fp.read(8)
        num = struct.unpack('q', num)[0]
        imgs_offset = 8
        img_ch = 3
        img_w = 224
        img_h = 224
        img_pixel_size = 4
        img_size = img_ch * img_h * img_w * img_pixel_size
        label_size = 8
        labels_offset = imgs_offset + num * img_size
        step = 0
        while step < num:
            fp.seek(imgs_offset + img_size * step)
            img = fp.read(img_size)
            img = struct.unpack_from('{}f'.format(img_ch * img_w * img_h), img)
            img = np.array(img)
            img.shape = (img_ch, img_w, img_h)
            fp.seek(labels_offset + label_size * step)
            label = fp.read(label_size)
            label = struct.unpack('q', label)[0]
            yield (img, int(label))
            step += 1","imgs_offset = 8
label_size = 8",imgs_offset = label_size = 8,,,,,,,,,,,
PaddleSlim,https://github.com/PaddlePaddle/PaddleSlim/tree/master/demo/mkldnn_quant/sample_tester.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleSlim/demo/mkldnn_quant/sample_tester.py,SampleTester,"def reader():
    with open(data_file, 'rb') as fp:
        num = fp.read(8)
        num = struct.unpack('q', num)[0]
        imgs_offset = 8
        img_ch = 3
        img_w = 224
        img_h = 224
        img_pixel_size = 4
        img_size = img_ch * img_h * img_w * img_pixel_size
        label_size = 8
        labels_offset = imgs_offset + num * img_size
        step = 0
        while step < num:
            fp.seek(imgs_offset + img_size * step)
            img = fp.read(img_size)
            img = struct.unpack_from('{}f'.format(img_ch * img_w * img_h), img)
            img = np.array(img)
            img.shape = (img_ch, img_w, img_h)
            fp.seek(labels_offset + label_size * step)
            label = fp.read(label_size)
            label = struct.unpack('q', label)[0]
            yield (img, int(label))
            step += 1","img_w = 224
img_h = 224",img_w = img_h = 224,,,,,,,,,,,
airflow,https://github.com/apache/airflow/tree/master/airflow/providers/mysql/transfers/vertica_to_mysql.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/providers/mysql/transfers/vertica_to_mysql.py,VerticaToMySqlOperator,"def execute(self, context):
    vertica = VerticaHook(vertica_conn_id=self.vertica_conn_id)
    mysql = MySqlHook(mysql_conn_id=self.mysql_conn_id)
    tmpfile = None
    result = None
    selected_columns = []
    count = 0
    with closing(vertica.get_conn()) as conn:
        with closing(conn.cursor()) as cursor:
            cursor.execute(self.sql)
            selected_columns = [d.name for d in cursor.description]
            if self.bulk_load:
                with NamedTemporaryFile('w') as tmpfile:
                    self.log.info('Selecting rows from Vertica to local file %s...', tmpfile.name)
                    self.log.info(self.sql)
                    csv_writer = csv.writer(tmpfile, delimiter='\t', encoding='utf-8')
                    for row in cursor.iterate():
                        csv_writer.writerow(row)
                        count += 1
                    tmpfile.flush()
            else:
                self.log.info('Selecting rows from Vertica...')
                self.log.info(self.sql)
                result = cursor.fetchall()
                count = len(result)
            self.log.info('Selected rows from Vertica %s', count)
    if self.mysql_preoperator:
        self.log.info('Running MySQL preoperator...')
        mysql.run(self.mysql_preoperator)
    try:
        if self.bulk_load:
            self.log.info('Bulk inserting rows into MySQL...')
            with closing(mysql.get_conn()) as conn:
                with closing(conn.cursor()) as cursor:
                    cursor.execute(f""LOAD DATA LOCAL INFILE '{tmpfile.name}' INTO TABLE {self.mysql_table} LINES TERMINATED BY '\r\n' ({', '.join(selected_columns)})"")
                    conn.commit()
            tmpfile.close()
        else:
            self.log.info('Inserting rows into MySQL...')
            mysql.insert_rows(table=self.mysql_table, rows=result, target_fields=selected_columns)
        self.log.info('Inserted rows into MySQL %s', count)
    except (MySQLdb.Error, MySQLdb.Warning):
        self.log.info('Inserted rows into MySQL 0')
        raise
    if self.mysql_postoperator:
        self.log.info('Running MySQL postoperator...')
        mysql.run(self.mysql_postoperator)
    self.log.info('Done')","tmpfile = None
result = None",tmpfile = result = None,,,,,,,,,,,
dynaconf,https://github.com/rochacbruno/dynaconf/tree/master/dynaconf/vendor_src/ruamel/yaml/comments.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dynaconf/dynaconf/vendor_src/ruamel/yaml/comments.py,CommentedKeySeq,"def _yaml_get_column(self, key):
    column = None
    sel_idx = None
    (pre, post) = (key - 1, key + 1)
    if pre in self.ca.items:
        sel_idx = pre
    elif post in self.ca.items:
        sel_idx = post
    else:
        for (row_idx, _k1) in enumerate(self):
            if row_idx >= key:
                break
            if row_idx not in self.ca.items:
                continue
            sel_idx = row_idx
    if sel_idx is not None:
        column = self._yaml_get_columnX(sel_idx)
    return column","column = None
sel_idx = None",column = sel_idx = None,,,,,,,,,,,
Bert-Chinese-Text-Classification-Pytorch,https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch/tree/master/pytorch_pretrained/tokenization.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Bert-Chinese-Text-Classification-Pytorch/pytorch_pretrained/tokenization.py,WordpieceTokenizer,"def tokenize(self, text):
    """"""Tokenizes a piece of text into its word pieces.

        This uses a greedy longest-match-first algorithm to perform tokenization
        using the given vocabulary.

        For example:
          input = ""unaffable""
          output = [""un"", ""##aff"", ""##able""]

        Args:
          text: A single token or whitespace separated tokens. This should have
            already been passed through `BasicTokenizer`.

        Returns:
          A list of wordpiece tokens.
        """"""
    output_tokens = []
    for token in whitespace_tokenize(text):
        chars = list(token)
        if len(chars) > self.max_input_chars_per_word:
            output_tokens.append(self.unk_token)
            continue
        is_bad = False
        start = 0
        sub_tokens = []
        while start < len(chars):
            end = len(chars)
            cur_substr = None
            while start < end:
                substr = ''.join(chars[start:end])
                if start > 0:
                    substr = '##' + substr
                if substr in self.vocab:
                    cur_substr = substr
                    break
                end -= 1
            if cur_substr is None:
                is_bad = True
                break
            sub_tokens.append(cur_substr)
            start = end
        if is_bad:
            output_tokens.append(self.unk_token)
        else:
            output_tokens.extend(sub_tokens)
    return output_tokens","is_bad = False
start = 0",is_bad = start = False,,,,,,,,,,,
dupeguru,https://github.com/arsenetar/dupeguru/tree/master/hscommon/pygettext.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dupeguru/hscommon/pygettext.py,,"def main(source_files, outpath, keywords=None):
    global default_keywords

    class Options:
        GNU = 1
        SOLARIS = 2
        extractall = 0
        escape = 0
        keywords = []
        outfile = 'messages.pot'
        writelocations = 1
        locationstyle = GNU
        verbose = 0
        width = 78
        excludefilename = ''
        docstrings = 0
        nodocstrings = {}
    options = Options()
    options.outfile = outpath
    if keywords:
        options.keywords = keywords
    make_escapes(options.escape)
    options.keywords.extend(default_keywords)
    if options.excludefilename:
        try:
            fp = open(options.excludefilename, encoding='utf-8')
            options.toexclude = fp.readlines()
            fp.close()
        except IOError:
            print(""Can't read --exclude-file: %s"" % options.excludefilename, file=sys.stderr)
            sys.exit(1)
    else:
        options.toexclude = []
    eater = TokenEater(options)
    for filename in source_files:
        if options.verbose:
            print('Working on %s' % filename)
        fp = open(filename, encoding='utf-8')
        closep = 1
        try:
            eater.set_filename(filename)
            try:
                tokens = tokenize.generate_tokens(fp.readline)
                for _token in tokens:
                    eater(*_token)
            except tokenize.TokenError as e:
                print('%s: %s, line %d, column %d' % (e.args[0], filename, e.args[1][0], e.args[1][1]), file=sys.stderr)
        finally:
            if closep:
                fp.close()
    fp = open(options.outfile, 'w', encoding='utf-8')
    closep = 1
    try:
        eater.write(fp)
    finally:
        if closep:
            fp.close()","GNU = 1
writelocations = 1",GNU = writelocations = 1,,,,,,,,,,,
dupeguru,https://github.com/arsenetar/dupeguru/tree/master/hscommon/pygettext.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dupeguru/hscommon/pygettext.py,,"def main(source_files, outpath, keywords=None):
    global default_keywords

    class Options:
        GNU = 1
        SOLARIS = 2
        extractall = 0
        escape = 0
        keywords = []
        outfile = 'messages.pot'
        writelocations = 1
        locationstyle = GNU
        verbose = 0
        width = 78
        excludefilename = ''
        docstrings = 0
        nodocstrings = {}
    options = Options()
    options.outfile = outpath
    if keywords:
        options.keywords = keywords
    make_escapes(options.escape)
    options.keywords.extend(default_keywords)
    if options.excludefilename:
        try:
            fp = open(options.excludefilename, encoding='utf-8')
            options.toexclude = fp.readlines()
            fp.close()
        except IOError:
            print(""Can't read --exclude-file: %s"" % options.excludefilename, file=sys.stderr)
            sys.exit(1)
    else:
        options.toexclude = []
    eater = TokenEater(options)
    for filename in source_files:
        if options.verbose:
            print('Working on %s' % filename)
        fp = open(filename, encoding='utf-8')
        closep = 1
        try:
            eater.set_filename(filename)
            try:
                tokens = tokenize.generate_tokens(fp.readline)
                for _token in tokens:
                    eater(*_token)
            except tokenize.TokenError as e:
                print('%s: %s, line %d, column %d' % (e.args[0], filename, e.args[1][0], e.args[1][1]), file=sys.stderr)
        finally:
            if closep:
                fp.close()
    fp = open(options.outfile, 'w', encoding='utf-8')
    closep = 1
    try:
        eater.write(fp)
    finally:
        if closep:
            fp.close()","extractall = 0
escape = 0
verbose = 0
docstrings = 0",extractall = escape = verbose = docstrings = 0,,,,,,,,,,,
tvm,https://github.com/apache/tvm/tree/master/vta/tests/python/unittest/test_vta_insn.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tvm/vta/tests/python/unittest/test_vta_insn.py,,"def _run(env, remote):

    def check_alu(tvm_op, np_op=None, use_imm=False, test_name=None):
        """"""Test ALU""""""
        m = 8
        n = 8
        imm = np.random.randint(1, 5)
        a = te.placeholder((m, n, env.BATCH, env.BLOCK_OUT), name='a', dtype=env.acc_dtype)
        a_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: a(*i), 'a_buf')
        if use_imm:
            res_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: tvm_op(a_buf(*i), imm), 'res_buf')
        else:
            b = te.placeholder((m, n, env.BATCH, env.BLOCK_OUT), name='b', dtype=env.acc_dtype)
            b_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: b(*i), 'b_buf')
            res_buf = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: tvm_op(a_buf(*i), b_buf(*i)), 'res_buf')
        res = te.compute((m, n, env.BATCH, env.BLOCK_OUT), lambda *i: res_buf(*i).astype(env.inp_dtype), 'res')
        s = te.create_schedule(res.op)
        s[a_buf].set_scope(env.acc_scope)
        s[a_buf].pragma(a_buf.op.axis[0], env.dma_copy)
        s[res_buf].set_scope(env.acc_scope)
        s[res_buf].pragma(res_buf.op.axis[0], env.alu)
        s[res].pragma(res.op.axis[0], env.dma_copy)
        if not use_imm:
            s[b_buf].set_scope(env.acc_scope)
            s[b_buf].pragma(b_buf.op.axis[0], env.dma_copy)
        if not remote:
            return
        with vta.build_config():
            if use_imm:
                mod = vta.build(s, [a, res], tvm.target.Target('ext_dev', host=env.target_host))
            else:
                mod = vta.build(s, [a, b, res], tvm.target.Target('ext_dev', host=env.target_host))
        temp = utils.tempdir()
        mod.save(temp.relpath('load_act.o'))
        remote.upload(temp.relpath('load_act.o'))
        f = remote.load_module('load_act.o')
        dev = remote.ext_dev(0)
        a_np = np.random.randint(-16, 16, size=(m, n, env.BATCH, env.BLOCK_OUT)).astype(a.dtype)
        if use_imm:
            res_np = np_op(a_np, imm) if np_op else tvm_op(a_np, imm)
        else:
            b_np = np.random.randint(-16, 16, size=(m, n, env.BATCH, env.BLOCK_OUT)).astype(b.dtype)
            res_np = np_op(a_np, b_np) if np_op else tvm_op(a_np, b_np)
        res_np = res_np.astype(res.dtype)
        a_nd = tvm.nd.array(a_np, dev)
        res_nd = tvm.nd.array(np.zeros((m, n, env.BATCH, env.BLOCK_OUT)).astype(res.dtype), dev)
        if env.TARGET in ['sim', 'tsim']:
            simulator.clear_stats()
        if use_imm:
            f(a_nd, res_nd)
        else:
            b_nd = tvm.nd.array(b_np, dev)
            f(a_nd, b_nd, res_nd)
        np.testing.assert_equal(res_np, res_nd.numpy())
        if env.TARGET in ['sim', 'tsim']:
            sim_stats = simulator.stats()
            print('ALU {} execution statistics:'.format(test_name))
            for (k, v) in sim_stats.items():
                print('\t{:<16}: {:>16}'.format(k, v))
    check_alu(lambda x, y: x << y, np.left_shift, use_imm=True, test_name='SHL')
    check_alu(tvm.te.max, np.maximum, use_imm=True, test_name='MAX')
    check_alu(tvm.te.max, np.maximum, test_name='MAX')
    check_alu(lambda x, y: x + y, use_imm=True, test_name='ADD')
    check_alu(lambda x, y: x + y, test_name='ADD')
    check_alu(lambda x, y: x >> y, np.right_shift, use_imm=True, test_name='SHR')","m = 8
n = 8",m = n = 8,,,,,,,,,,,
dbt-core,https://github.com/dbt-labs/dbt-core/tree/master/core/dbt/adapters/base/relation.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dbt-core/core/dbt/adapters/base/relation.py,BaseRelation,"def matches(self, database: Optional[str]=None, schema: Optional[str]=None, identifier: Optional[str]=None) -> bool:
    search = filter_null_values({ComponentName.Database: database, ComponentName.Schema: schema, ComponentName.Identifier: identifier})
    if not search:
        raise dbt.exceptions.RuntimeException('Tried to match relation, but no search path was passed!')
    exact_match = True
    approximate_match = True
    for (k, v) in search.items():
        if not self._is_exactish_match(k, v):
            exact_match = False
        if self.path.get_lowered_part(k).strip(self.quote_character) != v.lower().strip(self.quote_character):
            approximate_match = False
    if approximate_match and (not exact_match):
        target = self.create(database=database, schema=schema, identifier=identifier)
        dbt.exceptions.approximate_relation_match(target, self)
    return exact_match","exact_match = True
approximate_match = True",exact_match = approximate_match = True,,,,,,,,,,,
orator,https://github.com/sdispater/orator/tree/master/tests/orm/relations/test_belongs_to_many.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/orator/tests/orm/relations/test_belongs_to_many.py,OrmBelongsToTestCase,"def test_models_are_properly_matched_to_parents(self):
    relation = self._get_relation()
    result1 = OrmBelongsToManyModelPivotStub()
    result1.pivot.user_id = 1
    result2 = OrmBelongsToManyModelPivotStub()
    result2.pivot.user_id = 2
    result3 = OrmBelongsToManyModelPivotStub()
    result3.pivot.user_id = 2
    model1 = OrmBelongsToManyModelStub()
    model1.id = 1
    model2 = OrmBelongsToManyModelStub()
    model2.id = 2
    model3 = OrmBelongsToManyModelStub()
    model3.id = 3
    relation.get_related().should_receive('new_collection').replace_with(lambda l=None: Collection(l))
    models = relation.match([model1, model2, model3], Collection([result1, result2, result3]), 'foo')
    self.assertEqual(1, models[0].foo[0].pivot.user_id)
    self.assertEqual(1, len(models[0].foo))
    self.assertEqual(2, models[1].foo[0].pivot.user_id)
    self.assertEqual(2, models[1].foo[1].pivot.user_id)
    self.assertEqual(2, len(models[1].foo))
    self.assertTrue(models[2].foo.is_empty())","result1.pivot.user_id = 1
model1.id = 1",result1.pivot.user_id = model1.id = 1,,,,,,,,,,,
orator,https://github.com/sdispater/orator/tree/master/tests/orm/relations/test_belongs_to_many.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/orator/tests/orm/relations/test_belongs_to_many.py,OrmBelongsToTestCase,"def test_models_are_properly_matched_to_parents(self):
    relation = self._get_relation()
    result1 = OrmBelongsToManyModelPivotStub()
    result1.pivot.user_id = 1
    result2 = OrmBelongsToManyModelPivotStub()
    result2.pivot.user_id = 2
    result3 = OrmBelongsToManyModelPivotStub()
    result3.pivot.user_id = 2
    model1 = OrmBelongsToManyModelStub()
    model1.id = 1
    model2 = OrmBelongsToManyModelStub()
    model2.id = 2
    model3 = OrmBelongsToManyModelStub()
    model3.id = 3
    relation.get_related().should_receive('new_collection').replace_with(lambda l=None: Collection(l))
    models = relation.match([model1, model2, model3], Collection([result1, result2, result3]), 'foo')
    self.assertEqual(1, models[0].foo[0].pivot.user_id)
    self.assertEqual(1, len(models[0].foo))
    self.assertEqual(2, models[1].foo[0].pivot.user_id)
    self.assertEqual(2, models[1].foo[1].pivot.user_id)
    self.assertEqual(2, len(models[1].foo))
    self.assertTrue(models[2].foo.is_empty())","result2.pivot.user_id = 2
result3.pivot.user_id = 2
model2.id = 2",result2.pivot.user_id = result3.pivot.user_id = model2.id = 2,,,,,,,,,,,
seahub,https://github.com/haiwen/seahub/tree/master/tests/api/endpoints/test_repo_share_invitations.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/seahub/tests/api/endpoints/test_repo_share_invitations.py,RepoShareInvitationsTest,"def test_can_list(self, mock_can_invite_guest, mock_has_permission):
    mock_can_invite_guest.return_val = True
    mock_has_permission.return_val = True
    invitation_obj_1 = Invitation.objects.add(inviter=self.username, accepter='1@qq.com')
    invitation_obj_2 = Invitation.objects.add(inviter=self.username, accepter='1@qq.com')
    RepoShareInvitation.objects.add(invitation_obj_1, self.repo_id, '/', 'r')
    RepoShareInvitation.objects.add(invitation_obj_2, self.repo_id, '/', 'rw')
    resp = self.client.get(self.url + '?path=/')
    self.assertEqual(200, resp.status_code)
    json_resp = json.loads(resp.content)
    assert len(json_resp.get('repo_share_invitation_list')) == 2","mock_can_invite_guest.return_val = True
mock_has_permission.return_val = True",mock_can_invite_guest.return_val = mock_has_permission.return_val = True,,,,,,,,,,,
nstools,https://github.com/skydark/nstools/tree/master/ons/saver.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nstools/ons/saver.py,ONSSaver,"def reset(self):
    self.p = 0
    self.stack_offset = None
    self.num_nest = 0
    self.nest_info = []
    self.current_offset = (None,)","self.p = 0
self.num_nest = 0",self.p = self.num_nest = 0,,,,,,,,,,,
Mobile-Security-Framework-MobSF,https://github.com/MobSF/Mobile-Security-Framework-MobSF/tree/master/mobsf/MobSF/views/apk_downloader.py,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mobile-Security-Framework-MobSF/mobsf/MobSF/views/apk_downloader.py,,"def try_provider(package, provider, domain):
    """"""Try using a provider.""""""
    downloaded_file = None
    data = None
    apk_name = f'{package}.apk'
    temp_file = Path(gettempdir()) / apk_name
    link = find_apk_link(provider, domain)
    if link:
        downloaded_file = download_file(link, temp_file)
    if downloaded_file:
        data = add_apk(downloaded_file, apk_name)
    if data:
        return data
    return None","downloaded_file = None
data = None",downloaded_file = data = None,,,,,,,,,,,
