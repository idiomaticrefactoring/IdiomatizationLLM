repo_name,file_path,file_html,class_name,me_name,me_code,old_code,chatGPT_code,element_str,slice_str,truth_code
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_fusion_repeated_fc_relu_op.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_fusion_repeated_fc_relu_op.py,TestFusionRepeatedFCReluOp,setUp$23,"def setUp(self):
        self.bs = 3
        self.ic = 9
        self.oc = [2, 4, 3]
        assert len(self.oc) > 1, 'Should larger than 1'
        self.set_conf()
        self.op_type = 'fusion_repeated_fc_relu'
        sz = len(self.oc)
        ics = [self.ic] + self.oc[0 : sz - 1]
        assert len(ics) == len(self.oc)
        weights = []
        biases = []
        outs = []

        i = 0
        matrix = MatrixGenerate(self.bs, ics[i], self.oc[i], 1, 1)
        inp = np.reshape(matrix.input, [self.bs, ics[i]])
        weights.append(
            (
                'W_{0}'.format(i),
                np.reshape(matrix.weights, [ics[i], self.oc[i]]),
            )
        )
        biases.append(('B_{0}'.format(i), matrix.bias))
        outs.append(
            np.reshape(
                np.maximum(fc_refer(matrix, True), 0), [self.bs, self.oc[i]]
            )
        )

        for i in range(sz - 1):
            matrix = MatrixGenerate(self.bs, ics[i + 1], self.oc[i + 1], 1, 1)
            matrix.input = np.reshape(outs[i], [self.bs, ics[i + 1], 1, 1])
            out = fc_refer(matrix, True)
            weights.append(
                (
                    'W_{0}'.format(i + 1),
                    np.reshape(matrix.weights, [ics[i + 1], self.oc[i + 1]]),
                )
            )
            biases.append(('B_{0}'.format(i + 1), matrix.bias))
            outs.append(
                np.reshape(np.maximum(out, 0), [self.bs, self.oc[i + 1]])
            )

        relu_outs = []
        for i in range(sz - 1):
            relu_outs.append(('ReluOut_{0}'.format(i), outs[i]))

        self.inputs = {
            'X': inp,
            'W': weights,
            'Bias': biases,
        }

        self.outputs = {'Out': outs[-1], 'ReluOut': relu_outs}","for i in range(sz - 1):
    relu_outs.append(('ReluOut_{0}'.format(i), outs[i]))","relu_outs = [('ReluOut_{0}'.format(i), outs[i]) for i in range(sz - 1)]","relu_outs = [('ReluOut_{0}'.format(i), outs[i]) for i in range(sz - 1)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
airflow,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/airflow/airflow/www/views.py,https://github.com/apache/airflow/tree/master/airflow/www/views.py,Airflow,xcom$1542,"def xcom(self, session=None):
        """"""Retrieve XCOM.""""""
        dag_id = request.args.get('dag_id')
        task_id = request.args.get('task_id')
        # Carrying execution_date through, even though it's irrelevant for
        # this context
        execution_date = request.args.get('execution_date')
        dttm = timezone.parse(execution_date)
        form = DateTimeForm(data={'execution_date': dttm})
        root = request.args.get('root', '')
        ti_db = models.TaskInstance
        dag = DagModel.get_dagmodel(dag_id)
        ti = session.query(ti_db).filter(and_(ti_db.dag_id == dag_id, ti_db.task_id == task_id)).first()

        if not ti:
            flash(f""Task [{dag_id}.{task_id}] doesn't seem to exist at the moment"", ""error"")
            return redirect(url_for('Airflow.index'))

        xcomlist = (
            session.query(XCom)
            .filter(XCom.dag_id == dag_id, XCom.task_id == task_id, XCom.execution_date == dttm)
            .all()
        )

        attributes = []
        for xcom in xcomlist:
            if not xcom.key.startswith('_'):
                attributes.append((xcom.key, xcom.value))

        title = ""XCom""
        return self.render_template(
            'airflow/xcom.html',
            attributes=attributes,
            task_id=task_id,
            execution_date=execution_date,
            form=form,
            root=root,
            dag=dag,
            title=title,
        )","for xcom in xcomlist:
    if not xcom.key.startswith('_'):
        attributes.append((xcom.key, xcom.value))","attributes = [(xcom.key, xcom.value) for xcom in xcomlist if not xcom.key.startswith('_')]","attributes = [(xcom.key, xcom.value) for xcom in xcomlist if not xcom.key.startswith('_')]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/containers/docker_model.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/containers/docker_model.py,DockerContainer,ports$132,"def ports(self):
        # {
        #     ""NetworkSettings"" : {
        #         ""Ports"" : {
        #             ""3306/tcp"" : [
        #                 {
        #                     ""HostIp"" : ""127.0.0.1"",
        #                     ""HostPort"" : ""3306""
        #                 }
        #             ]
        rval = []
        try:
            port_mappings = self.inspect['NetworkSettings']['Ports']
        except KeyError:
            log.warning(""Failed to get ports for container %s from `docker inspect` output at ""
                        ""['NetworkSettings']['Ports']: %s: %s"", self.id, exc_info=True)
            return None
        for port_name in port_mappings:
            for binding in port_mappings[port_name]:
                rval.append(ContainerPort(
                    int(port_name.split('/')[0]),
                    port_name.split('/')[1],
                    self.address,
                    int(binding['HostPort']),
                ))
        return rval","for port_name in port_mappings:
    for binding in port_mappings[port_name]:
        rval.append(ContainerPort(int(port_name.split('/')[0]), port_name.split('/')[1], self.address, int(binding['HostPort'])))","rval = [ContainerPort(int(port_name.split('/')[0]), port_name.split('/')[1], self.address, int(binding['HostPort'])) for port_name in port_mappings for binding in port_mappings[port_name]]","rval = [ContainerPort(int(port_name.split('/')[0]), port_name.split('/')[1], self.address, int(binding['HostPort'])) for port_name in port_mappings for binding in port_mappings[port_name]]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/mkldnn/test_matmul_v2_mkldnn_op.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/mkldnn/test_matmul_v2_mkldnn_op.py,,create_bf16_test_class$318,"def create_bf16_test_class(parent):
    @OpTestTool.skip_if_not_cpu_bf16()
    class TestMatMulV2Bf16OneDNNOp(parent):
        def set_inputs(self, x, y):
            self.inputs = {
                'X': convert_float_to_uint16(x),
                'Y': convert_float_to_uint16(y),
            }
            self.x_fp32 = x
            self.y_fp32 = y

        def set_dtype_attr(self):
            self.attrs['mkldnn_data_type'] = ""bfloat16""

        def test_check_output(self):
            self.check_output_with_place(core.CPUPlace())

        def test_check_grad(self):
            self.calculate_grads()
            self.check_grad_with_place(
                core.CPUPlace(),
                [""X"", ""Y""],
                ""Out"",
                user_defined_grads=[self.dx, self.dy],
                user_defined_grad_outputs=[convert_float_to_uint16(self.dout)],
            )

        def matmul_grad(self, x, transpose_x, y, transpose_y):
            x = (
                np.transpose(x, self.shape_transpose_axes[x.ndim])
                if transpose_x
                else x
            )
            y = (
                np.transpose(y, self.shape_transpose_axes[y.ndim])
                if transpose_y
                else y
            )

            return np.matmul(x, y)

        def calculate_grads(self):
            self.shape_transpose_axes = {
                2: [1, 0],
                3: [0, 2, 1],
                4: [0, 1, 3, 2],
                5: [0, 1, 2, 4, 3],
                6: [0, 1, 2, 3, 5, 4],
            }

            # expand vector so it will be a valid matrix for multiplication
            if self.x_fp32.ndim == 1:
                self.x_fp32 = np.expand_dims(self.x_fp32, axis=0)
            if self.y_fp32.ndim == 1:
                self.y_fp32 = np.expand_dims(self.y_fp32, axis=1)

            x_transpose_axes = self.shape_transpose_axes[self.x_fp32.ndim]
            y_transpose_axes = self.shape_transpose_axes[self.y_fp32.ndim]

            x = (
                np.transpose(self.x_fp32, x_transpose_axes)
                if self.attrs['trans_x'] is True
                else self.x_fp32
            )
            y = (
                np.transpose(self.y_fp32, y_transpose_axes)
                if self.attrs['trans_y'] is True
                else self.y_fp32
            )

            dout = np.matmul(x, y)

            x_shape = x.shape
            y_shape = y.shape

            if x.ndim <= 2 or y.ndim <= 2:
                is_broadcast = False
            elif x.ndim != y.ndim:
                is_broadcast = True
            else:
                is_broadcast = x.shape[0:-2] != y.shape[0:-2]

            if self.attrs['trans_x'] is True and self.attrs['trans_y'] is True:
                self.dx = self.matmul_grad(self.y_fp32, True, dout, True)
                self.dy = self.matmul_grad(dout, True, self.x_fp32, True)
            elif (
                self.attrs['trans_x'] is True and self.attrs['trans_y'] is False
            ):
                self.dx = self.matmul_grad(self.y_fp32, False, dout, True)
                self.dy = self.matmul_grad(self.x_fp32, False, dout, False)
            elif (
                self.attrs['trans_x'] is False and self.attrs['trans_y'] is True
            ):
                self.dx = self.matmul_grad(dout, False, self.y_fp32, False)
                self.dy = self.matmul_grad(dout, True, self.x_fp32, False)
            else:
                self.dx = self.matmul_grad(dout, False, self.y_fp32, True)
                self.dy = self.matmul_grad(self.x_fp32, True, dout, False)

            if is_broadcast:
                x_reduce_axis = []
                y_reduce_axis = []
                for index, (first, second) in enumerate(
                    zip(x_shape[0:-2], self.dx.shape[0:-2])
                ):
                    if first != second:
                        x_reduce_axis.append(index)

                for index, (first, second) in enumerate(
                    zip(y_shape[0:-2], self.dy.shape[0:-2])
                ):
                    if first != second:
                        y_reduce_axis.append(index)

                if x_reduce_axis:
                    self.dx = self.dx.sum(
                        axis=tuple(x_reduce_axis), keepdims=True
                    )
                if y_reduce_axis:
                    self.dy = self.dy.sum(
                        axis=tuple(y_reduce_axis), keepdims=True
                    )

            # after multiplying with vector one dimension is deleted from tensor
            if len(x_shape) == 2 and x_shape[0] == 1:
                dout = dout.sum(axis=-2)
            if len(y_shape) == 2 and y_shape[1] == 1:
                dout = dout.sum(axis=-1)

            self.dout = dout

    cls_name = ""{0}_{1}"".format(parent.__name__, ""BF16"")
    TestMatMulV2Bf16OneDNNOp.__name__ = cls_name
    globals()[cls_name] = TestMatMulV2Bf16OneDNNOp","for (index, (first, second)) in enumerate(zip(x_shape[0:-2], self.dx.shape[0:-2])):
    if first != second:
        x_reduce_axis.append(index)","x_reduce_axis = [index for (index, (first, second)) in enumerate(zip(x_shape[0:-2], self.dx.shape[0:-2])) if first != second]","x_reduce_axis = [index for (index, (first, second)) in enumerate(zip(x_shape[0:-2], self.dx.shape[0:-2])) if first != second]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/mkldnn/test_matmul_v2_mkldnn_op.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/mkldnn/test_matmul_v2_mkldnn_op.py,,create_bf16_test_class$318,"def create_bf16_test_class(parent):
    @OpTestTool.skip_if_not_cpu_bf16()
    class TestMatMulV2Bf16OneDNNOp(parent):
        def set_inputs(self, x, y):
            self.inputs = {
                'X': convert_float_to_uint16(x),
                'Y': convert_float_to_uint16(y),
            }
            self.x_fp32 = x
            self.y_fp32 = y

        def set_dtype_attr(self):
            self.attrs['mkldnn_data_type'] = ""bfloat16""

        def test_check_output(self):
            self.check_output_with_place(core.CPUPlace())

        def test_check_grad(self):
            self.calculate_grads()
            self.check_grad_with_place(
                core.CPUPlace(),
                [""X"", ""Y""],
                ""Out"",
                user_defined_grads=[self.dx, self.dy],
                user_defined_grad_outputs=[convert_float_to_uint16(self.dout)],
            )

        def matmul_grad(self, x, transpose_x, y, transpose_y):
            x = (
                np.transpose(x, self.shape_transpose_axes[x.ndim])
                if transpose_x
                else x
            )
            y = (
                np.transpose(y, self.shape_transpose_axes[y.ndim])
                if transpose_y
                else y
            )

            return np.matmul(x, y)

        def calculate_grads(self):
            self.shape_transpose_axes = {
                2: [1, 0],
                3: [0, 2, 1],
                4: [0, 1, 3, 2],
                5: [0, 1, 2, 4, 3],
                6: [0, 1, 2, 3, 5, 4],
            }

            # expand vector so it will be a valid matrix for multiplication
            if self.x_fp32.ndim == 1:
                self.x_fp32 = np.expand_dims(self.x_fp32, axis=0)
            if self.y_fp32.ndim == 1:
                self.y_fp32 = np.expand_dims(self.y_fp32, axis=1)

            x_transpose_axes = self.shape_transpose_axes[self.x_fp32.ndim]
            y_transpose_axes = self.shape_transpose_axes[self.y_fp32.ndim]

            x = (
                np.transpose(self.x_fp32, x_transpose_axes)
                if self.attrs['trans_x'] is True
                else self.x_fp32
            )
            y = (
                np.transpose(self.y_fp32, y_transpose_axes)
                if self.attrs['trans_y'] is True
                else self.y_fp32
            )

            dout = np.matmul(x, y)

            x_shape = x.shape
            y_shape = y.shape

            if x.ndim <= 2 or y.ndim <= 2:
                is_broadcast = False
            elif x.ndim != y.ndim:
                is_broadcast = True
            else:
                is_broadcast = x.shape[0:-2] != y.shape[0:-2]

            if self.attrs['trans_x'] is True and self.attrs['trans_y'] is True:
                self.dx = self.matmul_grad(self.y_fp32, True, dout, True)
                self.dy = self.matmul_grad(dout, True, self.x_fp32, True)
            elif (
                self.attrs['trans_x'] is True and self.attrs['trans_y'] is False
            ):
                self.dx = self.matmul_grad(self.y_fp32, False, dout, True)
                self.dy = self.matmul_grad(self.x_fp32, False, dout, False)
            elif (
                self.attrs['trans_x'] is False and self.attrs['trans_y'] is True
            ):
                self.dx = self.matmul_grad(dout, False, self.y_fp32, False)
                self.dy = self.matmul_grad(dout, True, self.x_fp32, False)
            else:
                self.dx = self.matmul_grad(dout, False, self.y_fp32, True)
                self.dy = self.matmul_grad(self.x_fp32, True, dout, False)

            if is_broadcast:
                x_reduce_axis = []
                y_reduce_axis = []
                for index, (first, second) in enumerate(
                    zip(x_shape[0:-2], self.dx.shape[0:-2])
                ):
                    if first != second:
                        x_reduce_axis.append(index)

                for index, (first, second) in enumerate(
                    zip(y_shape[0:-2], self.dy.shape[0:-2])
                ):
                    if first != second:
                        y_reduce_axis.append(index)

                if x_reduce_axis:
                    self.dx = self.dx.sum(
                        axis=tuple(x_reduce_axis), keepdims=True
                    )
                if y_reduce_axis:
                    self.dy = self.dy.sum(
                        axis=tuple(y_reduce_axis), keepdims=True
                    )

            # after multiplying with vector one dimension is deleted from tensor
            if len(x_shape) == 2 and x_shape[0] == 1:
                dout = dout.sum(axis=-2)
            if len(y_shape) == 2 and y_shape[1] == 1:
                dout = dout.sum(axis=-1)

            self.dout = dout

    cls_name = ""{0}_{1}"".format(parent.__name__, ""BF16"")
    TestMatMulV2Bf16OneDNNOp.__name__ = cls_name
    globals()[cls_name] = TestMatMulV2Bf16OneDNNOp","for (index, (first, second)) in enumerate(zip(y_shape[0:-2], self.dy.shape[0:-2])):
    if first != second:
        y_reduce_axis.append(index)","y_reduce_axis = [index for (index, (first, second)) in enumerate(zip(y_shape[0:-2], self.dy.shape[0:-2])) if first != second]","y_reduce_axis = [index for (index, (first, second)) in enumerate(zip(y_shape[0:-2], self.dy.shape[0:-2])) if first != second]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
pigar,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pigar/pigar/unpack.py,https://github.com/damnever/pigar/tree/master/pigar/unpack.py,Archive,_safe_extractall$67,"def _safe_extractall(self, to_path='.'):
        unsafe = []
        for name in self.names:
            if not self.is_safe(name):
                unsafe.append(name)
        if unsafe:
            raise ValueError(""unsafe to unpack: {}"".format(unsafe))
        self._file.extractall(to_path)","for name in self.names:
    if not self.is_safe(name):
        unsafe.append(name)",unsafe = [name for name in self.names if not self.is_safe(name)],unsafe = [name for name in self.names if not self.is_safe(name)],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
bili2.0,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bili2.0/substance/substance_raffle_sql.py,https://github.com/yjqiang/bili2.0/tree/master/substance/substance_raffle_sql.py,SubstanceRaffleJoinedTable,select_all$138,"def select_all(self):
        results = []
        for row in self.conn.execute('SELECT * FROM substanceraffle_joined'):
            results.append(self.as_bili_data(row))
        return results","for row in self.conn.execute('SELECT * FROM substanceraffle_joined'):
    results.append(self.as_bili_data(row))",results = [self.as_bili_data(row) for row in self.conn.execute('SELECT * FROM substanceraffle_joined')],results = [self.as_bili_data(row) for row in self.conn.execute('SELECT * FROM substanceraffle_joined')],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
not-youtube-dl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/not-youtube-dl/youtube_dl/extractor/tagesschau.py,https://github.com/scastillo/not-youtube-dl/tree/master/youtube_dl/extractor/tagesschau.py,TagesschauIE,_real_extract$265,"def _real_extract(self, url):
        mobj = re.match(self._VALID_URL, url)
        video_id = mobj.group('id') or mobj.group('path')
        display_id = video_id.lstrip('-')

        webpage = self._download_webpage(url, display_id)

        title = self._html_search_regex(
            r'<span[^>]*class=""headline""[^>]*>(.+?)</span>',
            webpage, 'title', default=None) or self._og_search_title(webpage)

        DOWNLOAD_REGEX = r'(?s)<p>Wir bieten dieses (?P<kind>Video|Audio) in folgenden Formaten zum Download an:</p>\s*<div class=""controls"">(?P<links>.*?)</div>\s*<p>'

        webpage_type = self._og_search_property('type', webpage, default=None)
        if webpage_type == 'website':  # Article
            entries = []
            for num, (entry_title, media_kind, download_text) in enumerate(re.findall(
                    r'(?s)<p[^>]+class=""infotext""[^>]*>\s*(?:<a[^>]+>)?\s*<strong>(.+?)</strong>.*?</p>.*?%s' % DOWNLOAD_REGEX,
                    webpage), 1):
                entries.append({
                    'id': '%s-%d' % (display_id, num),
                    'title': '%s' % entry_title,
                    'formats': self._extract_formats(download_text, media_kind),
                })
            if len(entries) > 1:
                return self.playlist_result(entries, display_id, title)
            formats = entries[0]['formats']
        else:  # Assume single video
            download_text = self._search_regex(
                DOWNLOAD_REGEX, webpage, 'download links', group='links')
            media_kind = self._search_regex(
                DOWNLOAD_REGEX, webpage, 'media kind', default='Video', group='kind')
            formats = self._extract_formats(download_text, media_kind)
        thumbnail = self._og_search_thumbnail(webpage)
        description = self._html_search_regex(
            r'(?s)<p class=""teasertext"">(.*?)</p>',
            webpage, 'description', default=None)

        self._sort_formats(formats)

        return {
            'id': display_id,
            'title': title,
            'thumbnail': thumbnail,
            'formats': formats,
            'description': description,
        }","for (num, (entry_title, media_kind, download_text)) in enumerate(re.findall('(?s)<p[^>]+class=""infotext""[^>]*>\\s*(?:<a[^>]+>)?\\s*<strong>(.+?)</strong>.*?</p>.*?%s' % DOWNLOAD_REGEX, webpage), 1):
    entries.append({'id': '%s-%d' % (display_id, num), 'title': '%s' % entry_title, 'formats': self._extract_formats(download_text, media_kind)})","entries = [{'id': f'{display_id}-{num}', 'title': f'{entry_title}', 'formats': self._extract_formats(download_text, media_kind)} for (num, (entry_title, media_kind, download_text)) in enumerate(re.findall('(?s)<p[^>]+class=""infotext""[^>]*>\\s*(?:<a[^>]+>)?\\s*<strong>(.+?)</strong>.*?</p>.*?%s' % DOWNLOAD_REGEX, webpage), 1)]","entries = [{'id': '%s-%d' % (display_id, num), 'title': '%s' % entry_title, 'formats': self._extract_formats(download_text, media_kind)} for (num, (entry_title, media_kind, download_text)) in enumerate(re.findall('(?s)<p[^>]+class=""infotext""[^>]*>\\s*(?:<a[^>]+>)?\\s*<strong>(.+?)</strong>.*?</p>.*?%s' % DOWNLOAD_REGEX, webpage), 1)]",0,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
pandera,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandera/pandera/strategies.py,https://github.com/pandera-dev/pandera/tree/master/pandera/strategies.py,,dataframe_strategy$933,"def dataframe_strategy(
    pandera_dtype: Optional[DataType] = None,
    strategy: Optional[SearchStrategy] = None,
    *,
    columns: Optional[Dict] = None,
    checks: Optional[Sequence] = None,
    unique: Optional[List[str]] = None,
    index: Optional[IndexComponent] = None,
    size: Optional[int] = None,
    n_regex_columns: int = 1,
):
    """"""Strategy to generate a pandas DataFrame.

    :param pandera_dtype: :class:`pandera.dtypes.DataType` instance.
    :param strategy: if specified, this will raise a BaseStrategyOnlyError,
        since it cannot be chained to a prior strategy.
    :param columns: a dictionary where keys are column names and values
        are :class:`~pandera.schema_components.Column` objects.
    :param checks: sequence of :class:`~pandera.checks.Check` s to constrain
        the values of the data at the dataframe level.
    :param unique: a list of column names that should be jointly unique.
    :param index: Index or MultiIndex schema component.
    :param size: number of elements in the Series.
    :param n_regex_columns: number of regex columns to generate.
    :returns: ``hypothesis`` strategy.
    """"""
    # pylint: disable=too-many-locals,too-many-branches,too-many-statements
    if n_regex_columns < 1:
        raise ValueError(
            ""`n_regex_columns` must be a positive integer, found: ""
            f""{n_regex_columns}""
        )
    if strategy:
        raise BaseStrategyOnlyError(
            ""The dataframe strategy is a base strategy. You cannot specify ""
            ""the strategy argument to chain it to a parent strategy.""
        )

    columns = {} if columns is None else columns
    checks = [] if checks is None else checks

    def undefined_check_strategy(strategy, check, column=None):
        """"""Strategy for checks with undefined strategies.""""""

        def _element_wise_check_fn(element):
            return check._check_fn(element)

        def _column_check_fn(dataframe):
            return check(dataframe[column]).check_passed

        def _dataframe_check_fn(dataframe):
            return check(dataframe).check_passed

        if check.element_wise:
            check_fn = _element_wise_check_fn
            warning_type = ""Element-wise""
        elif column is None:
            check_fn = _dataframe_check_fn
            warning_type = ""Dataframe""
        else:
            check_fn = _column_check_fn
            warning_type = ""Column""

        warnings.warn(
            f""{warning_type} check doesn't have a defined strategy. ""
            ""Falling back to filtering drawn values based on the check ""
            ""definition. This can considerably slow down data-generation.""
        )

        return strategy.filter(check_fn)

    def make_row_strategy(col, checks):
        strategy = None
        for check in checks:
            if hasattr(check, ""strategy""):
                strategy = check.strategy(col.dtype, strategy)
            else:
                strategy = undefined_check_strategy(
                    strategy=(
                        pandas_dtype_strategy(col.dtype)
                        if strategy is None
                        else strategy
                    ),
                    check=check,
                )
        if strategy is None:
            strategy = pandas_dtype_strategy(col.dtype)
        return strategy

    @composite
    def _dataframe_strategy(draw):
        row_strategy_checks = []
        undefined_strat_df_checks = []
        for check in checks:
            if hasattr(check, ""strategy"") or check.element_wise:
                # we can apply element-wise checks defined at the dataframe
                # level to the row strategy
                row_strategy_checks.append(check)
            else:
                undefined_strat_df_checks.append(check)

        # expand column set to generate column names for columns where
        # regex=True.
        expanded_columns = {}
        for col_name, column in columns.items():
            if unique and col_name in unique:
                # if the column is in the set of columns specified in `unique`,
                # make the column strategy independently unique. This is
                # technically stricter than it should be, since the list of
                # columns in `unique` are required to be jointly unique, but
                # this is a simple solution that produces synthetic data that
                # fulfills the uniqueness constraints of the dataframe.
                column = deepcopy(column)
                column.unique = True
            if not column.regex:
                expanded_columns[col_name] = column
            else:
                regex_columns = draw(
                    st.lists(
                        st.from_regex(column.name, fullmatch=True),
                        min_size=n_regex_columns,
                        max_size=n_regex_columns,
                        unique=True,
                    )
                )
                for regex_col in regex_columns:
                    expanded_columns[regex_col] = deepcopy(column).set_name(
                        regex_col
                    )

        # collect all non-element-wise column checks with undefined strategies
        undefined_strat_column_checks: Dict[str, list] = defaultdict(list)
        for col_name, column in expanded_columns.items():
            undefined_strat_column_checks[col_name].extend(
                check
                for check in column.checks
                if not hasattr(check, ""strategy"") and not check.element_wise
            )

        # override the column datatype with dataframe-level datatype if
        # specified
        col_dtypes = {
            col_name: str(col.dtype)
            if pandera_dtype is None
            else str(pandera_dtype)
            for col_name, col in expanded_columns.items()
        }
        nullable_columns = {
            col_name: col.nullable
            for col_name, col in expanded_columns.items()
        }

        row_strategy = None
        if row_strategy_checks:
            row_strategy = st.fixed_dictionaries(
                {
                    col_name: make_row_strategy(col, row_strategy_checks)
                    for col_name, col in expanded_columns.items()
                }
            )

        strategy = pdst.data_frames(
            columns=[
                column.strategy_component()
                for column in expanded_columns.values()
            ],
            rows=row_strategy,
            index=pdst.range_indexes(
                min_size=0 if size is None else size, max_size=size
            ),
        )

        # this is a hack to convert np.str_ data values into native python str.
        string_columns = []
        for col_name, col_dtype in col_dtypes.items():
            if col_dtype in {""object"", ""str""} or col_dtype.startswith(
                ""string""
            ):
                string_columns.append(col_name)

        if string_columns:
            # pylint: disable=cell-var-from-loop,undefined-loop-variable
            strategy = strategy.map(
                lambda df: df.assign(
                    **{
                        col_name: df[col_name].map(str)
                        for col_name in string_columns
                    }
                )
            )

        strategy = strategy.map(
            lambda df: df if df.empty else df.astype(col_dtypes)
        )

        if size is not None and size > 0 and any(nullable_columns.values()):
            strategy = null_dataframe_masks(strategy, nullable_columns)

        if index is not None:
            strategy = set_pandas_index(strategy, index)

        for check in undefined_strat_df_checks:
            strategy = undefined_check_strategy(strategy, check)

        for col_name, column_checks in undefined_strat_column_checks.items():
            for check in column_checks:  # type: ignore
                strategy = undefined_check_strategy(
                    strategy, check, column=col_name
                )

        return draw(strategy)

    return _dataframe_strategy()","for (col_name, col_dtype) in col_dtypes.items():
    if col_dtype in {'object', 'str'} or col_dtype.startswith('string'):
        string_columns.append(col_name)","string_columns = [col_name for (col_name, col_dtype) in col_dtypes.items() if col_dtype in {'object', 'str'} or col_dtype.startswith('string')]","string_columns = [col_name for (col_name, col_dtype) in col_dtypes.items() if col_dtype in {'object', 'str'} or col_dtype.startswith('string')]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
mmocr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmocr/mmocr/datasets/pipelines/dbnet_transforms.py,https://github.com/open-mmlab/mmocr/tree/master/mmocr/datasets/pipelines/dbnet_transforms.py,ImgAug,may_augment_bbox$110,"def may_augment_bbox(self, aug, ori_shape, bboxes):
        imgaug_bboxes = []
        for bbox in bboxes:
            x1, y1, x2, y2 = bbox
            imgaug_bboxes.append(
                imgaug.BoundingBox(x1=x1, y1=y1, x2=x2, y2=y2))
        imgaug_bboxes = aug.augment_bounding_boxes([
            imgaug.BoundingBoxesOnImage(imgaug_bboxes, shape=ori_shape)
        ])[0].clip_out_of_image()

        new_bboxes = []
        for box in imgaug_bboxes.bounding_boxes:
            new_bboxes.append(
                np.array([box.x1, box.y1, box.x2, box.y2], dtype=np.float32))

        return new_bboxes","for box in imgaug_bboxes.bounding_boxes:
    new_bboxes.append(np.array([box.x1, box.y1, box.x2, box.y2], dtype=np.float32))","new_bboxes = [np.array([box.x1, box.y1, box.x2, box.y2], dtype=np.float32) for box in imgaug_bboxes.bounding_boxes]","new_bboxes = [np.array([box.x1, box.y1, box.x2, box.y2], dtype=np.float32) for box in imgaug_bboxes.bounding_boxes]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/pdaugment/midi_preprocess/steps/track_separate.py,https://github.com/microsoft/muzic/tree/master/pdaugment/midi_preprocess/steps/track_separate.py,,remove_empty_track$13,"def remove_empty_track(midi_file):
    '''
    1. read pretty midi data
    2. remove empty track,
    also remove track with fewer than 10% notes of the track
    with most notes
    ********
    Return: pretty_midi object, pypianoroll object
    '''
    try:
        pretty_midi_data = pretty_midi.PrettyMIDI(midi_file)
    except Exception as e:
        print(f'exceptions in reading the file {midi_file}')
        return None, None

    #     print('I00:', pretty_midi_data.instruments)
    pypiano_data = pypianoroll.Multitrack()

    try:
        pypiano_data.parse_pretty_midi(pretty_midi_data, skip_empty_tracks=False)
    except Exception as e:
        print(f'exceptions for pypianoroll in reading the file {midi_file}')
        return None, None

    drum_idx = []
    for i, instrument in enumerate(pretty_midi_data.instruments):
        if instrument.is_drum:
            drum_idx.append(i)

    note_count = [np.count_nonzero(np.any(track.pianoroll, axis=1)) \
                  for track in pypiano_data.tracks]

    empty_indices = np.array(note_count) < 10
    remove_indices = np.arange(len(pypiano_data.tracks))[empty_indices]

    for index in sorted(remove_indices, reverse=True):
        del pypiano_data.tracks[index]
        del pretty_midi_data.instruments[index]
    return pretty_midi_data, pypiano_data","for (i, instrument) in enumerate(pretty_midi_data.instruments):
    if instrument.is_drum:
        drum_idx.append(i)","drum_idx = [i for (i, instrument) in enumerate(pretty_midi_data.instruments) if instrument.is_drum]","drum_idx = [i for (i, instrument) in enumerate(pretty_midi_data.instruments) if instrument.is_drum]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
neural_sp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/neural_sp/neural_sp/datasets/token_converter/character.py,https://github.com/hirofumi0810/neural_sp/tree/master/neural_sp/datasets/token_converter/character.py,Char2idx,__init__$19,"def __init__(self, dict_path, nlsyms=False, remove_space=False, remove_list=[]):
        self.remove_space = remove_space
        self.remove_list = remove_list

        # Load a dictionary file
        self.token2idx = {'<blank>': 0}
        with codecs.open(dict_path, 'r', encoding='utf-8') as f:
            for line in f:
                c, idx = line.strip().split(' ')
                if c in remove_list:
                    continue
                self.token2idx[c] = int(idx)
        self.vocab = len(self.token2idx.keys())

        self.nlsyms_list = []
        if nlsyms and os.path.isfile(nlsyms):
            with codecs.open(nlsyms, 'r', encoding='utf-8') as f:
                for line in f:
                    self.nlsyms_list.append(line.strip())","for line in f:
    self.nlsyms_list.append(line.strip())",self.nlsyms_list = [line.strip() for line in f],self.nlsyms_list = [line.strip() for line in f],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
dm-haiku,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dm-haiku/haiku/_src/nets/resnet.py,https://github.com/deepmind/dm-haiku/tree/master/haiku/_src/nets/resnet.py,BlockGroup,__init__$203,"def __init__(
      self,
      channels: int,
      num_blocks: int,
      stride: Union[int, Sequence[int]],
      bn_config: Mapping[str, FloatStrOrBool],
      resnet_v2: bool,
      bottleneck: bool,
      use_projection: bool,
      name: Optional[str] = None,
  ):
    super().__init__(name=name)

    block_cls = BlockV2 if resnet_v2 else BlockV1

    self.blocks = []
    for i in range(num_blocks):
      self.blocks.append(
          block_cls(channels=channels,
                    stride=(1 if i else stride),
                    use_projection=(i == 0 and use_projection),
                    bottleneck=bottleneck,
                    bn_config=bn_config,
                    name=""block_%d"" % (i)))","for i in range(num_blocks):
    self.blocks.append(block_cls(channels=channels, stride=1 if i else stride, use_projection=i == 0 and use_projection, bottleneck=bottleneck, bn_config=bn_config, name='block_%d' % i))","blocks = [block_cls(channels=channels, stride=1 if i else stride, use_projection=i == 0 and use_projection, bottleneck=bottleneck, bn_config=bn_config, name='block_%d' % i) for i in range(num_blocks)]","self.blocks = [block_cls(channels=channels, stride=1 if i else stride, use_projection=i == 0 and use_projection, bottleneck=bottleneck, bn_config=bn_config, name='block_%d' % i) for i in range(num_blocks)]",0,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
astropy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/astropy/astropy/io/votable/exceptions.py,https://github.com/astropy/astropy/tree/master/astropy/io/votable/exceptions.py,,_get_warning_and_exception_classes$1520,"def _get_warning_and_exception_classes(prefix):
    classes = []
    for key, val in globals().items():
        if re.match(prefix + ""[0-9]{2}"", key):
            classes.append((key, val))
    classes.sort()
    return classes","for (key, val) in globals().items():
    if re.match(prefix + '[0-9]{2}', key):
        classes.append((key, val))","classes = [(key, val) for (key, val) in globals().items() if re.match(prefix + '[0-9]{2}', key)]","classes = [(key, val) for (key, val) in globals().items() if re.match(prefix + '[0-9]{2}', key)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
RATDecoders,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/RATDecoders/malwareconfig/yarascanner.py,https://github.com/kevthehermit/RATDecoders/tree/master/malwareconfig/yarascanner.py,YaraScanner,yara_scan$17,"def yara_scan(self, raw_data):
        matches = self.compiled_rules.match(data=raw_data)
        rule_list = []

        for match in matches:
            rule_list.append(match.rule)
        self.rule_list = rule_list","for match in matches:
    rule_list.append(match.rule)",rule_list = [match.rule for match in matches],rule_list = [match.rule for match in matches],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
moto,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/moto/moto/ec2/responses/instances.py,https://github.com/spulec/moto/tree/master/moto/ec2/responses/instances.py,InstanceResponse,_security_grp_instance_attribute_handler$281,"def _security_grp_instance_attribute_handler(self):
        new_security_grp_list = []
        for key, value in self.querystring.items():
            if ""GroupId."" in key:
                new_security_grp_list.append(self.querystring.get(key)[0])

        instance_id = self._get_param(""InstanceId"")
        if self.is_not_dryrun(""ModifyInstanceSecurityGroups""):
            self.ec2_backend.modify_instance_security_groups(
                instance_id, new_security_grp_list
            )
            return EC2_MODIFY_INSTANCE_ATTRIBUTE","for (key, value) in self.querystring.items():
    if 'GroupId.' in key:
        new_security_grp_list.append(self.querystring.get(key)[0])","new_security_grp_list = [self.querystring.get(key)[0] for (key, value) in self.querystring.items() if 'GroupId.' in key]","new_security_grp_list = [self.querystring.get(key)[0] for (key, value) in self.querystring.items() if 'GroupId.' in key]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
diff-match-patch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/diff-match-patch/python3/diff_match_patch.py,https://github.com/google/diff-match-patch/tree/master/python3/diff_match_patch.py,diff_match_patch,diff_charsToLines$444,"def diff_charsToLines(self, diffs, lineArray):
    """"""Rehydrate the text in a diff from a string of line hashes to real lines
    of text.

    Args:
      diffs: Array of diff tuples.
      lineArray: Array of unique strings.
    """"""
    for i in range(len(diffs)):
      text = []
      for char in diffs[i][1]:
        text.append(lineArray[ord(char)])
      diffs[i] = (diffs[i][0], """".join(text))","for char in diffs[i][1]:
    text.append(lineArray[ord(char)])",text = [lineArray[ord(char)] for char in diffs[i][1]],text = [lineArray[ord(char)] for char in diffs[i][1]],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
amazing-qr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/amazing-qr/amzqr/amzqr.py,https://github.com/x-hw/amazing-qr/tree/master/amzqr/amzqr.py,,combine$51,"def combine(ver, qr_name, bg_name, colorized, contrast, brightness, save_dir, save_name=None):
        from amzqr.mylibs.constant import alig_location
        from PIL import ImageEnhance, ImageFilter
        
        qr = Image.open(qr_name)
        qr = qr.convert('RGBA') if colorized else qr
        
        bg0 = Image.open(bg_name).convert('RGBA')
        bg0 = ImageEnhance.Contrast(bg0).enhance(contrast)
        bg0 = ImageEnhance.Brightness(bg0).enhance(brightness)

        if bg0.size[0] < bg0.size[1]:
            bg0 = bg0.resize((qr.size[0]-24, (qr.size[0]-24)*int(bg0.size[1]/bg0.size[0])))
        else:
            bg0 = bg0.resize(((qr.size[1]-24)*int(bg0.size[0]/bg0.size[1]), qr.size[1]-24))    
            
        bg = bg0 if colorized else bg0.convert('1')
        
        aligs = []
        if ver > 1:
            aloc = alig_location[ver-2]
            for a in range(len(aloc)):
                for b in range(len(aloc)):
                    if not ((a==b==0) or (a==len(aloc)-1 and b==0) or (a==0 and b==len(aloc)-1)):
                        for i in range(3*(aloc[a]-2), 3*(aloc[a]+3)):
                            for j in range(3*(aloc[b]-2), 3*(aloc[b]+3)):
                                aligs.append((i,j))

        for i in range(qr.size[0]-24):
            for j in range(qr.size[1]-24):
                if not ((i in (18,19,20)) or (j in (18,19,20)) or (i<24 and j<24) or (i<24 and j>qr.size[1]-49) or (i>qr.size[0]-49 and j<24) or ((i,j) in aligs) or (i%3==1 and j%3==1) or (bg0.getpixel((i,j))[3]==0)):
                    qr.putpixel((i+12,j+12), bg.getpixel((i,j)))
        
        qr_name = os.path.join(save_dir, os.path.splitext(os.path.basename(bg_name))[0] + '_qrcode.png') if not save_name else os.path.join(save_dir, save_name)
        qr.resize((qr.size[0]*3, qr.size[1]*3)).save(qr_name)
        return qr_name","for a in range(len(aloc)):
    for b in range(len(aloc)):
        if not (a == b == 0 or (a == len(aloc) - 1 and b == 0) or (a == 0 and b == len(aloc) - 1)):
            for i in range(3 * (aloc[a] - 2), 3 * (aloc[a] + 3)):
                for j in range(3 * (aloc[b] - 2), 3 * (aloc[b] + 3)):
                    aligs.append((i, j))","aligs = [(i, j) for a in range(len(aloc)) for b in range(len(aloc)) if not (a == b == 0 or (a == len(aloc) - 1 and b == 0) or (a == 0 and b == len(aloc) - 1)) for i in range(3 * (aloc[a] - 2), 3 * (aloc[a] + 3)) for j in range(3 * (aloc[b] - 2), 3 * (aloc[b] + 3))]","aligs = [(i, j) for a in range(len(aloc)) for b in range(len(aloc)) if not (a == b == 0 or (a == len(aloc) - 1 and b == 0) or (a == 0 and b == len(aloc) - 1)) for i in range(3 * (aloc[a] - 2), 3 * (aloc[a] + 3)) for j in range(3 * (aloc[b] - 2), 3 * (aloc[b] + 3))]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
frappe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/frappe/frappe/core/doctype/user/user.py,https://github.com/frappe/frappe/tree/master/frappe/core/doctype/user/user.py,User,after_rename$453,"def after_rename(self, old_name, new_name, merge=False):
		tables = frappe.db.get_tables()
		for tab in tables:
			desc = frappe.db.get_table_columns_description(tab)
			has_fields = []
			for d in desc:
				if d.get(""name"") in [""owner"", ""modified_by""]:
					has_fields.append(d.get(""name""))
			for field in has_fields:
				frappe.db.sql(
					""""""UPDATE `%s`
					SET `%s` = %s
					WHERE `%s` = %s""""""
					% (tab, field, ""%s"", field, ""%s""),
					(new_name, old_name),
				)

		if frappe.db.exists(""Notification Settings"", old_name):
			frappe.rename_doc(""Notification Settings"", old_name, new_name, force=True, show_alert=False)

		# set email
		frappe.db.set_value(""User"", new_name, ""email"", new_name)","for d in desc:
    if d.get('name') in ['owner', 'modified_by']:
        has_fields.append(d.get('name'))","has_fields = [d.get('name') for d in desc if d.get('name') in ['owner', 'modified_by']]","has_fields = [d.get('name') for d in desc if d.get('name') in ['owner', 'modified_by']]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
satpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/satpy/satpy/readers/viirs_compact.py,https://github.com/pytroll/satpy/tree/master/satpy/readers/viirs_compact.py,VIIRSCompactFileHandler,expand_angle_and_nav$285,"def expand_angle_and_nav(self, arrays):
        """"""Expand angle and navigation datasets.""""""
        res = []
        for array in arrays:
            res.append(da.map_blocks(expand, array[:, :, np.newaxis], self.expansion_coefs,
                                     scans=self.scans, scan_size=self.scan_size,
                                     dtype=array.dtype, drop_axis=2, chunks=self.expansion_coefs.chunks[:-1]))
        return res","for array in arrays:
    res.append(da.map_blocks(expand, array[:, :, np.newaxis], self.expansion_coefs, scans=self.scans, scan_size=self.scan_size, dtype=array.dtype, drop_axis=2, chunks=self.expansion_coefs.chunks[:-1]))","res = [da.map_blocks(expand, array[:, :, np.newaxis], self.expansion_coefs, scans=self.scans, scan_size=self.scan_size, dtype=array.dtype, drop_axis=2, chunks=self.expansion_coefs.chunks[:-1]) for array in arrays]","res = [da.map_blocks(expand, array[:, :, np.newaxis], self.expansion_coefs, scans=self.scans, scan_size=self.scan_size, dtype=array.dtype, drop_axis=2, chunks=self.expansion_coefs.chunks[:-1]) for array in arrays]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
LibreASR,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/LibreASR/libreasr/lib/learner.py,https://github.com/iceychris/LibreASR/tree/master/libreasr/lib/learner.py,ASRLearner,from_config$173,"def from_config(conf, db, m):
        cbs = [
            CudaCallback(),
            TerminateOnNaNCallback(),
            SaveModelCallback(),
            ReduceLROnPlateau(patience=1, min_lr=1e-5, factor=1.5),
        ]
        optim = conf[""training""][""optimizer""].lower()
        if optim == ""ranger"":
            opt_func = ranger
        elif optim == ""ranger_adabelief"":
            opt_func = ranger_adabelief
        elif optim == ""adam"":
            opt_func = Adam
        elif optim == ""lamb"":
            opt_func = Lamb
        elif optim == ""apollo"":
            from fastai2.optimizer import OptimWrapper

            def of(param_groups, **kwargs):
                lr_init = 1e-4
                lr = 1e-3
                warmup = 10  # 1000
                wd = 4e-4
                apollo = Apollo(param_groups, lr=lr, warmup=warmup)
                new_pgs = []
                for pg in param_groups:
                    new_pgs.append(
                        {
                            ""params"": pg,
                            ""lr"": lr,
                            ""wd"": wd,
                            ""mom"": 0.99,
                            ""eps"": 1e-4,
                            ""beta"": 0.9,
                            ""init_lr"": lr_init,
                            ""base_lr"": lr,
                            ""warmup"": warmup,
                        }
                    )
                apollo.param_groups = new_pgs
                opt = OptimWrapper(apollo)
                return opt

            opt_func = of
        elif optim == ""adahessian"":
            opt_func = AdaHessian
            cbs.append(HutchinsonTraceCallback())

            @patch
            def _backward(self: Learner):
                if self.opt._hutch_iter % HESSIAN_EVERY == 0:
                    self.loss.backward(create_graph=True)
                else:
                    self.loss.backward()

        else:
            raise Exception(""No such optimizer"")
        acnb = conf[""accumulate_n_batches""]
        if acnb > 1 and not optim == ""adahessian"":
            cbs.append(GradAccumCallback(num_batches=acnb))
        extra_cbs = []
        if conf[""mp""]:
            extra_cbs.append(MixedPrecision(clip=conf[""mp_clip""]))
            _ = m.half()
        if conf[""tensorboard""]:
            _tb = partial(
                Tensorboard,
                wandb=conf[""wandb""],
                test=True,
                tests_per_epoch=conf[""tests_per_epoch""],
                mp=conf[""mp""],
            )()
            extra_cbs.append(_tb)
        learn = Learner(
            db,
            m,
            loss_func=get_loss_func(
                ""rnnt"",
                conf[""cuda""][""device""],
                conf[""model""][""encoder""][""reduction_factor""],
                debug=False,
                perf=False,
                div_by_len=False,
                entropy_loss=False,
            ),
            opt_func=opt_func,
            splitter=partial(transducer_splitter, adahessian=(optim == ""adahessian"")),
            cbs=cbs,
        )
        learn.extra_cbs = extra_cbs
        if conf[""mp""]:
            # dirty fix
            learn.dls.device = torch.device(""cuda:0"")
        return learn","for pg in param_groups:
    new_pgs.append({'params': pg, 'lr': lr, 'wd': wd, 'mom': 0.99, 'eps': 0.0001, 'beta': 0.9, 'init_lr': lr_init, 'base_lr': lr, 'warmup': warmup})","new_pgs = [{'params': pg, 'lr': lr, 'wd': wd, 'mom': 0.99, 'eps': 0.0001, 'beta': 0.9, 'init_lr': lr_init, 'base_lr': lr, 'warmup': warmup} for pg in param_groups]","new_pgs = [{'params': pg, 'lr': lr, 'wd': wd, 'mom': 0.99, 'eps': 0.0001, 'beta': 0.9, 'init_lr': lr_init, 'base_lr': lr, 'warmup': warmup} for pg in param_groups]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
shinysdr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shinysdr/shinysdr/telemetry.py,https://github.com/kpreid/shinysdr/tree/master/shinysdr/telemetry.py,TelemetryStore,__flush_expired$203,"def __flush_expired(self):
        current_time = self.__time_source.seconds()
        deletes = []
        for object_id, expiry in six.iteritems(self.__expiry_times):
            if expiry <= current_time:
                deletes.append(object_id)
        for object_id in deletes:
            del self.__objects[object_id]
            del self.__expiry_times[object_id]
            if object_id in self.__interesting_objects:
                del self.__interesting_objects[object_id]

        self.__maybe_schedule_flush()","for (object_id, expiry) in six.iteritems(self.__expiry_times):
    if expiry <= current_time:
        deletes.append(object_id)","deletes = [object_id for (object_id, expiry) in six.iteritems(self.__expiry_times) if expiry <= current_time]","deletes = [object_id for (object_id, expiry) in six.iteritems(self.__expiry_times) if expiry <= current_time]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
Metis,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Metis/app/dao/time_series_detector/train_op.py,https://github.com/Tencent/Metis/tree/master/app/dao/time_series_detector/train_op.py,TrainOperation,query_train_source$87,"def query_train_source(self):
        command = ""select distinct source from train_task""
        num = self.__cur.execute(command)
        source_list = []
        query_res = self.__cur.fetchmany(num)
        for row in query_res:
            source_list.append(row[0])
        return OP_SUCCESS, {
            ""source"": source_list
        }","for row in query_res:
    source_list.append(row[0])",source_list = [row[0] for row in query_res],source_list = [row[0] for row in query_res],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
MB-Lab,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/MB-Lab/algorithms.py,https://github.com/animate1978/MB-Lab/tree/master//algorithms.py,,collect_existing_meshes$611,"def collect_existing_meshes():
    existing_mesh_names = []
    for mesh in bpy.data.meshes:
        existing_mesh_names.append(mesh.name)
    return existing_mesh_names","for mesh in bpy.data.meshes:
    existing_mesh_names.append(mesh.name)",existing_mesh_names = [mesh.name for mesh in bpy.data.meshes],existing_mesh_names = [mesh.name for mesh in bpy.data.meshes],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
pybossa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pybossa/test/test_jobs/test_engage_old_users.py,https://github.com/Scifabric/pybossa/tree/master/test/test_jobs/test_engage_old_users.py,TestEngageUsers,test_get_inactive_users_jobs_with_users$42,"def test_get_inactive_users_jobs_with_users(self):
        """"""Test JOB get with users returns empty list.""""""
        TaskRunFactory.create()
        jobs_generator = get_inactive_users_jobs()
        jobs = []
        for job in jobs_generator:
            jobs.append(job)

        msg = ""There should not be any job.""
        assert len(jobs) == 0,  msg","for job in jobs_generator:
    jobs.append(job)",jobs = [job for job in jobs_generator],jobs = [job for job in jobs_generator],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
AlgorithmsAndDataStructure,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsAndDataStructure/Python/Algorithms/OperatingSystem/bankers_algorithm.py,https://github.com/codePerfectPlus/AlgorithmsAndDataStructure/tree/master/Python/Algorithms/OperatingSystem/bankers_algorithm.py,,isSafe$22,"def isSafe(P, R, processes, available_array, max_R, alloted):
    need = []
    for i in range(P):
        temp = []
        for j in range(R):
            temp.append(0)
        need.append(temp)

    calculateNeed(P, R, need, max_R, alloted)

    # mark all processes as unfinished
    finish = [0] * P

    # safe sequence array
    safeSeq = [0] * P

    # make a copy of available resources
    work = [0] * R
    for i in range(R):
        work[i] = available_array[i]

    # while all processes not finished or system not in safe state
    count = 0
    while(count < P):
        # find an unfinished process whose
        # needs can be satisfied
        found = False
        for p in range(P):

            if(finish[p] == 0):

                for j in range(R):
                    if(need[p][j] > work[j]):
                        break

                if(j == R - 1):

                    for k in range(R):
                        work[k] += alloted[p][k]
                    safeSeq[count] = p
                    count += 1

                    finish[p] = 1
                    found = True
        if found is False:
            print(""System not in safe state"")
            return False

    print(""System is in safe state"")
    print(""Safe Sequence is: "")
    print(*safeSeq)
    return True","for j in range(R):
    temp.append(0)",temp = [0 for j in range(R)],temp = [0 for j in range(R)],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
lemur,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lemur/lemur/certificates/utils.py,https://github.com/Netflix/lemur/tree/master/lemur/certificates/utils.py,,get_sans_from_csr$19,"def get_sans_from_csr(data):
    """"""
    Fetches SubjectAlternativeNames from CSR.
    Works with any kind of SubjectAlternativeName
    :param data: PEM-encoded string with CSR
    :return: List of LemurAPI-compatible subAltNames
    """"""
    sub_alt_names = []
    try:
        request = x509.load_pem_x509_csr(data.encode(""utf-8""), default_backend())
    except Exception:
        raise ValidationError(""CSR presented is not valid."")

    try:
        alt_names = request.extensions.get_extension_for_class(
            x509.SubjectAlternativeName
        )
        for alt_name in alt_names.value:
            sub_alt_names.append(
                {""nameType"": type(alt_name).__name__, ""value"": alt_name.value}
            )
    except x509.ExtensionNotFound:
        pass

    return sub_alt_names","for alt_name in alt_names.value:
    sub_alt_names.append({'nameType': type(alt_name).__name__, 'value': alt_name.value})","sub_alt_names = [{'nameType': type(alt_name).__name__, 'value': alt_name.value} for alt_name in alt_names.value]","sub_alt_names = [{'nameType': type(alt_name).__name__, 'value': alt_name.value} for alt_name in alt_names.value]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
DG-Net,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DG-Net/reIDmodel.py,https://github.com/NVlabs/DG-Net/tree/master//reIDmodel.py,PCB,forward$219,"def forward(self, x):
        x = self.model.conv1(x)
        x = self.model.bn1(x)
        x = self.model.relu(x)
        x = self.model.maxpool(x)
        
        x = self.model.layer1(x)
        x = self.model.layer2(x)
        x = self.model.layer3(x)
        x = self.model.layer4(x)
        x = self.avgpool(x)
        f = x
        f = f.view(f.size(0),f.size(1)*self.part)
        x = self.dropout(x)
        part = {}
        predict = {}
        # get part feature batchsize*2048*4
        for i in range(self.part):
            part[i] = x[:,:,i].contiguous()
            part[i] = part[i].view(x.size(0), x.size(1))
            name = 'classifier'+str(i)
            c = getattr(self,name)
            predict[i] = c(part[i])

        y=[]
        for i in range(self.part):
            y.append(predict[i])

        return f, y","for i in range(self.part):
    y.append(predict[i])",y = [predict[i] for i in range(self.part)],y = [predict[i] for i in range(self.part)],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
stacker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stacker/stacker/lookups/handlers/dynamodb.py,https://github.com/cloudtools/stacker/tree/master/stacker/lookups/handlers/dynamodb.py,,_convert_ddb_list_to_list$180,"def _convert_ddb_list_to_list(conversion_list):
    """"""Given a dynamodb list, it will return a python list without the dynamodb
        datatypes

    Args:
        conversion_list (dict): a dynamodb list which includes the
            datatypes

    Returns:
        list: Returns a sanitized list without the dynamodb datatypes
    """"""
    ret_list = []
    for v in conversion_list:
        for v1 in v:
            ret_list.append(v[v1])
    return ret_list","for v in conversion_list:
    for v1 in v:
        ret_list.append(v[v1])",ret_list = [v[v1] for v in conversion_list for v1 in v],ret_list = [v[v1] for v in conversion_list for v1 in v],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
django-haystack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-haystack/haystack/backends/solr_backend.py,https://github.com/django-haystack/django-haystack/tree/master/haystack/backends/solr_backend.py,SolrSearchBackend,extract_spelling_suggestions$592,"def extract_spelling_suggestions(self, raw_results):
        # There are many different formats for Legacy, 6.4, and 6.5 e.g.
        # https://issues.apache.org/jira/browse/SOLR-3029 and depending on the
        # version and configuration the response format may be a dict of dicts,
        # a list of dicts, or a list of strings.

        collations = raw_results.spellcheck.get(""collations"", None)
        suggestions = raw_results.spellcheck.get(""suggestions"", None)

        # We'll collect multiple suggestions here. For backwards
        # compatibility with older versions of Haystack we'll still return
        # only a single suggestion but in the future we can expose all of
        # them.

        spelling_suggestions = []

        if collations:
            if isinstance(collations, dict):
                # Solr 6.5
                collation_values = collations[""collation""]
                if isinstance(collation_values, str):
                    collation_values = [collation_values]
                elif isinstance(collation_values, dict):
                    # spellcheck.collateExtendedResults changes the format to a dictionary:
                    collation_values = [collation_values[""collationQuery""]]
            elif isinstance(collations[1], dict):
                # Solr 6.4
                collation_values = collations
            else:
                # Older versions of Solr
                collation_values = collations[-1:]

            for i in collation_values:
                # Depending on the options the values are either simple strings or dictionaries:
                spelling_suggestions.append(
                    i[""collationQuery""] if isinstance(i, dict) else i
                )
        elif suggestions:
            if isinstance(suggestions, dict):
                for i in suggestions.values():
                    for j in i[""suggestion""]:
                        if isinstance(j, dict):
                            spelling_suggestions.append(j[""word""])
                        else:
                            spelling_suggestions.append(j)
            elif isinstance(suggestions[0], str) and isinstance(suggestions[1], dict):
                # Solr 6.4 uses a list of paired (word, dictionary) pairs:
                for suggestion in suggestions:
                    if isinstance(suggestion, dict):
                        for i in suggestion[""suggestion""]:
                            if isinstance(i, dict):
                                spelling_suggestions.append(i[""word""])
                            else:
                                spelling_suggestions.append(i)
            else:
                # Legacy Solr
                spelling_suggestions.append(suggestions[-1])

        return spelling_suggestions","for i in collation_values:
    spelling_suggestions.append(i['collationQuery'] if isinstance(i, dict) else i)","spelling_suggestions = [i['collationQuery'] if isinstance(i, dict) else i for i in collation_values]","spelling_suggestions = [i['collationQuery'] if isinstance(i, dict) else i for i in collation_values]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
nova,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nova/nova/tests/unit/scheduler/test_filters.py,https://github.com/openstack/nova/tree/master/nova/tests/unit/scheduler/test_filters.py,FiltersTestCase,test_filter_all_recursive_yields$59,"def test_filter_all_recursive_yields(self, mock_filter_one):
        # Test filter_all() allows generators from previous filter_all()s.
        # filter_all() yields results.  We want to make sure that we can
        # call filter_all() with generators returned from previous calls
        # to filter_all().
        filter_obj_list = ['obj1', 'obj2', 'obj3']
        spec_obj = objects.RequestSpec()
        base_filter = filters.BaseFilter()

        # The order that _filter_one is going to get called gets
        # confusing because we will be recursively yielding things..
        # We are going to simulate the first call to filter_all()
        # returning False for 'obj2'.  So, 'obj1' will get yielded
        # 'total_iterations' number of times before the first filter_all()
        # call gets to processing 'obj2'.  We then return 'False' for it.
        # After that, 'obj3' gets yielded 'total_iterations' number of
        # times.
        mock_results = []
        total_iterations = 200
        for x in range(total_iterations):
            mock_results.append(True)
        mock_results.append(False)
        for x in range(total_iterations):
            mock_results.append(True)
        mock_filter_one.side_effect = mock_results

        objs = iter(filter_obj_list)
        for x in range(total_iterations):
            # Pass in generators returned from previous calls.
            objs = base_filter.filter_all(objs, spec_obj)
        self.assertTrue(inspect.isgenerator(objs))
        self.assertEqual(['obj1', 'obj3'], list(objs))","for x in range(total_iterations):
    mock_results.append(True)",mock_results = [True for x in range(total_iterations)],mock_results = [True for x in range(total_iterations)],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
data-driven-web-apps-with-flask,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/data-driven-web-apps-with-flask/app/ch11_migrations/starter/pypi_org/bin/load_data.py,https://github.com/talkpython/data-driven-web-apps-with-flask/tree/master/app/ch11_migrations/starter/pypi_org/bin/load_data.py,,get_file_names$350,"def get_file_names(data_path: str) -> List[str]:
    files = []
    for f in os.listdir(data_path):
        if f.endswith('.json'):
            files.append(
                os.path.abspath(os.path.join(data_path, f))
            )

    files.sort()
    return files","for f in os.listdir(data_path):
    if f.endswith('.json'):
        files.append(os.path.abspath(os.path.join(data_path, f)))","files = [os.path.abspath(os.path.join(data_path, f)) for f in os.listdir(data_path) if f.endswith('.json')]","files = [os.path.abspath(os.path.join(data_path, f)) for f in os.listdir(data_path) if f.endswith('.json')]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
forseti-security,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/forseti-security/tests/scanner/scanners/forwarding_rule_rules_scanner_test.py,https://github.com/forseti-security/forseti-security/tree/master/tests/scanner/scanners/forwarding_rule_rules_scanner_test.py,ForwardingRuleScannerTest,test_forwarding_rules_scanner_all_match$31,"def test_forwarding_rules_scanner_all_match(self):
        rules_local_path = get_datafile_path(__file__,
            'forward_rule_test_1.yaml')
        scanner = forwarding_rule_scanner.ForwardingRuleScanner(
            {}, {}, mock.MagicMock(), '', '', rules_local_path)

        project_id = ""abc-123""

        gcp_forwarding_rules_resource_data = [
            {
                ""id"": ""46"",
                ""creationTimestamp"": ""2017-06-01 04:19:37"",
                ""name"": ""abc-123"",
                ""description"": """",
                ""region"": ""https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1"",
                ""IPAddress"": ""198.51.100.99"",
                ""IPProtocol"": ""UDP"",
                ""portRange"": ""4500-4500"",
                ""ports"": [],
                ""target"": ""https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1/abc-123/abc-123"",
                ""loadBalancingScheme"": ""EXTERNAL"",
            },
            {
                ""id"": ""23"",
                ""creationTimestamp"": ""2017-06-01 04:19:37"",
                ""name"": ""abc-123"",
                ""description"": """",
                ""region"": ""https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1"",
                ""IPAddress"": ""198.51.100.23"",
                ""IPProtocol"": ""TCP"",
                ""ports"": [8080],
                ""target"": ""https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1/abc-123/abc-123"",
                ""loadBalancingScheme"": ""INTERNAL"",
            },
            {
                ""id"": ""46"",
                ""creationTimestamp"": ""2017-06-01 04:19:37"",
                ""name"": ""abc-123"",
                ""description"": """",
                ""region"": ""https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1"",
                ""IPAddress"": ""198.51.100.46"",
                ""IPProtocol"":  ""ESP"",
                ""ports"": [],
                ""target"": ""https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1/abc-123/abc-123"",
                ""loadBalancingScheme"":  ""EXTERNAL"",
            },
            {
                ""id"": ""46"",
                ""creationTimestamp"": ""2017-06-01 04:19:37"",
                ""name"": ""abc-123"",
                ""description"": """",
                ""region"": ""https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1"",
                ""IPAddress"": ""198.51.100.35"",
                ""IPProtocol"":  ""TCP"",
                ""portRange"": ""4500-4500"",
                ""target"": ""https://www.googleapis.com/compute/v1/projects/abc-123/regions/asia-east1/abc-123/abc-123"",
                ""loadBalancingScheme"":  ""EXTERNAL"",
            }
        ]
        gcp_forwarding_rules_resource_objs = []
        for gcp_forwarding_rule_resource_data in gcp_forwarding_rules_resource_data:
            gcp_forwarding_rules_resource_objs.append(
                fr.ForwardingRule.from_dict(
                    project_id, '', gcp_forwarding_rule_resource_data))

        violations = scanner._find_violations(gcp_forwarding_rules_resource_objs)
        self.assertEqual(0, len(violations))","for gcp_forwarding_rule_resource_data in gcp_forwarding_rules_resource_data:
    gcp_forwarding_rules_resource_objs.append(fr.ForwardingRule.from_dict(project_id, '', gcp_forwarding_rule_resource_data))","gcp_forwarding_rules_resource_objs = [fr.ForwardingRule.from_dict(project_id, '', gcp_forwarding_rule_resource_data) for gcp_forwarding_rule_resource_data in gcp_forwarding_rules_resource_data]","gcp_forwarding_rules_resource_objs = [fr.ForwardingRule.from_dict(project_id, '', gcp_forwarding_rule_resource_data) for gcp_forwarding_rule_resource_data in gcp_forwarding_rules_resource_data]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
openstates-scrapers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openstates-scrapers/scrapers_next/md/people.py,https://github.com/openstates/openstates-scrapers/tree/master/scrapers_next/md/people.py,PersonDetail,parse_address_block$7,"def parse_address_block(self, block):
        state = ""address""
        # group lines by type
        values = {""address"": [], ""phone"": [], ""fax"": []}
        for line in block.splitlines():
            line = line.strip()
            if not line:
                continue
            if line.startswith(""Phone""):
                state = ""phone""
            elif line.startswith(""Fax""):
                state = ""fax""

            values[state].append(line)

        # postprocess values

        phones = []
        for line in values[""phone""]:
            for match in re.findall(r""\d{3}-\d{3}-\d{4}"", line):
                phones.append(match)

        faxes = []
        for line in values[""fax""]:
            for match in re.findall(r""\d{3}-\d{3}-\d{4}"", line):
                faxes.append(match)

        return {
            ""address"": ""; "".join(values[""address""]),
            ""phones"": phones,
            ""faxes"": faxes,
        }","for line in values['phone']:
    for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line):
        phones.append(match)","phones = [match for line in values['phone'] for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line)]","phones = [match for line in values['phone'] for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
openstates-scrapers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openstates-scrapers/scrapers_next/md/people.py,https://github.com/openstates/openstates-scrapers/tree/master/scrapers_next/md/people.py,PersonDetail,parse_address_block$7,"def parse_address_block(self, block):
        state = ""address""
        # group lines by type
        values = {""address"": [], ""phone"": [], ""fax"": []}
        for line in block.splitlines():
            line = line.strip()
            if not line:
                continue
            if line.startswith(""Phone""):
                state = ""phone""
            elif line.startswith(""Fax""):
                state = ""fax""

            values[state].append(line)

        # postprocess values

        phones = []
        for line in values[""phone""]:
            for match in re.findall(r""\d{3}-\d{3}-\d{4}"", line):
                phones.append(match)

        faxes = []
        for line in values[""fax""]:
            for match in re.findall(r""\d{3}-\d{3}-\d{4}"", line):
                faxes.append(match)

        return {
            ""address"": ""; "".join(values[""address""]),
            ""phones"": phones,
            ""faxes"": faxes,
        }","for line in values['fax']:
    for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line):
        faxes.append(match)","faxes = [match for line in values['fax'] for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line)]","faxes = [match for line in values['fax'] for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
espresso,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/espresso/tests/test_dictionary.py,https://github.com/freewym/espresso/tree/master/tests/test_dictionary.py,TestDictionary,test_finalize$18,"def test_finalize(self):
        txt = [
            ""A B C D"",
            ""B C D"",
            ""C D"",
            ""D"",
        ]
        ref_ids1 = list(
            map(
                torch.IntTensor,
                [
                    [4, 5, 6, 7, 2],
                    [5, 6, 7, 2],
                    [6, 7, 2],
                    [7, 2],
                ],
            )
        )
        ref_ids2 = list(
            map(
                torch.IntTensor,
                [
                    [7, 6, 5, 4, 2],
                    [6, 5, 4, 2],
                    [5, 4, 2],
                    [4, 2],
                ],
            )
        )

        # build dictionary
        d = Dictionary()
        for line in txt:
            d.encode_line(line, add_if_not_exist=True)

        def get_ids(dictionary):
            ids = []
            for line in txt:
                ids.append(dictionary.encode_line(line, add_if_not_exist=False))
            return ids

        def assertMatch(ids, ref_ids):
            for toks, ref_toks in zip(ids, ref_ids):
                self.assertEqual(toks.size(), ref_toks.size())
                self.assertEqual(0, (toks != ref_toks).sum().item())

        ids = get_ids(d)
        assertMatch(ids, ref_ids1)

        # check finalized dictionary
        d.finalize()
        finalized_ids = get_ids(d)
        assertMatch(finalized_ids, ref_ids2)

        # write to disk and reload
        with tempfile.NamedTemporaryFile(mode=""w"") as tmp_dict:
            d.save(tmp_dict.name)
            d = Dictionary.load(tmp_dict.name)
            reload_ids = get_ids(d)
            assertMatch(reload_ids, ref_ids2)
            assertMatch(finalized_ids, reload_ids)","for line in txt:
    ids.append(dictionary.encode_line(line, add_if_not_exist=False))","ids = [dictionary.encode_line(line, add_if_not_exist=False) for line in txt]","ids = [dictionary.encode_line(line, add_if_not_exist=False) for line in txt]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
reid-strong-baseline,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/reid-strong-baseline/utils/re_ranking.py,https://github.com/michuanhaohao/reid-strong-baseline/tree/master/utils/re_ranking.py,,re_ranking$29,"def re_ranking(probFea, galFea, k1, k2, lambda_value, local_distmat=None, only_local=False):
    # if feature vector is numpy, you should use 'torch.tensor' transform it to tensor
    query_num = probFea.size(0)
    all_num = query_num + galFea.size(0)
    if only_local:
        original_dist = local_distmat
    else:
        feat = torch.cat([probFea,galFea])
        print('using GPU to compute original distance')
        distmat = torch.pow(feat,2).sum(dim=1, keepdim=True).expand(all_num,all_num) + \
                      torch.pow(feat, 2).sum(dim=1, keepdim=True).expand(all_num, all_num).t()
        distmat.addmm_(1,-2,feat,feat.t())
        original_dist = distmat.cpu().numpy()
        del feat
        if not local_distmat is None:
            original_dist = original_dist + local_distmat
    gallery_num = original_dist.shape[0]
    original_dist = np.transpose(original_dist / np.max(original_dist, axis=0))
    V = np.zeros_like(original_dist).astype(np.float16)
    initial_rank = np.argsort(original_dist).astype(np.int32)

    print('starting re_ranking')
    for i in range(all_num):
        # k-reciprocal neighbors
        forward_k_neigh_index = initial_rank[i, :k1 + 1]
        backward_k_neigh_index = initial_rank[forward_k_neigh_index, :k1 + 1]
        fi = np.where(backward_k_neigh_index == i)[0]
        k_reciprocal_index = forward_k_neigh_index[fi]
        k_reciprocal_expansion_index = k_reciprocal_index
        for j in range(len(k_reciprocal_index)):
            candidate = k_reciprocal_index[j]
            candidate_forward_k_neigh_index = initial_rank[candidate, :int(np.around(k1 / 2)) + 1]
            candidate_backward_k_neigh_index = initial_rank[candidate_forward_k_neigh_index,
                                               :int(np.around(k1 / 2)) + 1]
            fi_candidate = np.where(candidate_backward_k_neigh_index == candidate)[0]
            candidate_k_reciprocal_index = candidate_forward_k_neigh_index[fi_candidate]
            if len(np.intersect1d(candidate_k_reciprocal_index, k_reciprocal_index)) > 2 / 3 * len(
                    candidate_k_reciprocal_index):
                k_reciprocal_expansion_index = np.append(k_reciprocal_expansion_index, candidate_k_reciprocal_index)

        k_reciprocal_expansion_index = np.unique(k_reciprocal_expansion_index)
        weight = np.exp(-original_dist[i, k_reciprocal_expansion_index])
        V[i, k_reciprocal_expansion_index] = weight / np.sum(weight)
    original_dist = original_dist[:query_num, ]
    if k2 != 1:
        V_qe = np.zeros_like(V, dtype=np.float16)
        for i in range(all_num):
            V_qe[i, :] = np.mean(V[initial_rank[i, :k2], :], axis=0)
        V = V_qe
        del V_qe
    del initial_rank
    invIndex = []
    for i in range(gallery_num):
        invIndex.append(np.where(V[:, i] != 0)[0])

    jaccard_dist = np.zeros_like(original_dist, dtype=np.float16)

    for i in range(query_num):
        temp_min = np.zeros(shape=[1, gallery_num], dtype=np.float16)
        indNonZero = np.where(V[i, :] != 0)[0]
        indImages = [invIndex[ind] for ind in indNonZero]
        for j in range(len(indNonZero)):
            temp_min[0, indImages[j]] = temp_min[0, indImages[j]] + np.minimum(V[i, indNonZero[j]],
                                                                               V[indImages[j], indNonZero[j]])
        jaccard_dist[i] = 1 - temp_min / (2 - temp_min)

    final_dist = jaccard_dist * (1 - lambda_value) + original_dist * lambda_value
    del original_dist
    del V
    del jaccard_dist
    final_dist = final_dist[:query_num, query_num:]
    return final_dist","for i in range(gallery_num):
    invIndex.append(np.where(V[:, i] != 0)[0])","invIndex = [np.where(V[:, i] != 0)[0] for i in range(gallery_num)]","invIndex = [np.where(V[:, i] != 0)[0] for i in range(gallery_num)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/tensor/search.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/tensor/search.py,,nonzero$383,"def nonzero(x, as_tuple=False):
    """"""
    Return a tensor containing the indices of all non-zero elements of the `input`
    tensor. If as_tuple is True, return a tuple of 1-D tensors, one for each dimension
    in `input`, each containing the indices (in that dimension) of all non-zero elements
    of `input`. Given a n-Dimensional `input` tensor with shape [x_1, x_2, ..., x_n], If
    as_tuple is False, we can get a output tensor with shape [z, n], where `z` is the
    number of all non-zero elements in the `input` tensor. If as_tuple is True, we can get
    a 1-D tensor tuple of length `n`, and the shape of each 1-D tensor is [z, 1].

    Args:
        x (Tensor): The input tensor variable.
        as_tuple (bool, optional): Return type, Tensor or tuple of Tensor.

    Returns:
        Tensor. The data type is int64.

    Examples:

        .. code-block:: python

            import paddle

            x1 = paddle.to_tensor([[1.0, 0.0, 0.0],
                                   [0.0, 2.0, 0.0],
                                   [0.0, 0.0, 3.0]])
            x2 = paddle.to_tensor([0.0, 1.0, 0.0, 3.0])
            out_z1 = paddle.nonzero(x1)
            print(out_z1)
            #[[0 0]
            # [1 1]
            # [2 2]]
            out_z1_tuple = paddle.nonzero(x1, as_tuple=True)
            for out in out_z1_tuple:
                print(out)
            #[[0]
            # [1]
            # [2]]
            #[[0]
            # [1]
            # [2]]
            out_z2 = paddle.nonzero(x2)
            print(out_z2)
            #[[1]
            # [3]]
            out_z2_tuple = paddle.nonzero(x2, as_tuple=True)
            for out in out_z2_tuple:
                print(out)
            #[[1]
            # [3]]

    """"""
    list_out = []
    shape = x.shape
    rank = len(shape)

    if in_dygraph_mode():
        outs = _C_ops.nonzero(x)
    elif paddle.in_dynamic_mode():
        outs = _legacy_C_ops.where_index(x)
    else:
        helper = LayerHelper(""where_index"", **locals())

        outs = helper.create_variable_for_type_inference(
            dtype=core.VarDesc.VarType.INT64
        )

        helper.append_op(
            type='where_index', inputs={'Condition': x}, outputs={'Out': [outs]}
        )

    if not as_tuple:
        return outs
    elif rank == 1:
        return tuple([outs])
    else:
        for i in range(rank):
            list_out.append(
                paddle.slice(outs, axes=[1], starts=[i], ends=[i + 1])
            )
        return tuple(list_out)","for i in range(rank):
    list_out.append(paddle.slice(outs, axes=[1], starts=[i], ends=[i + 1]))","list_out = [paddle.slice(outs, axes=[1], starts=[i], ends=[i + 1]) for i in range(rank)]","list_out = [paddle.slice(outs, axes=[1], starts=[i], ends=[i + 1]) for i in range(rank)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
football,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/football/gfootball/env/football_env_core.py,https://github.com/google-research/football/tree/master/gfootball/env/football_env_core.py,FootballEnvCore,sticky_actions_state$378,"def sticky_actions_state(self, left_team, player_id):
    result = []
    for a in self._sticky_actions:
      result.append(
          self._env.sticky_action_state(a._backend_action, left_team,
                                        player_id))
    return np.uint8(result)","for a in self._sticky_actions:
    result.append(self._env.sticky_action_state(a._backend_action, left_team, player_id))","result = [self._env.sticky_action_state(a._backend_action, left_team, player_id) for a in self._sticky_actions]","result = [self._env.sticky_action_state(a._backend_action, left_team, player_id) for a in self._sticky_actions]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
zvt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/zvt/zvt/contract/register.py,https://github.com/zvtvz/zvt/tree/master/zvt/contract/register.py,,register_schema$42,"def register_schema(providers: List[str],
                    db_name: str,
                    schema_base: DeclarativeMeta,
                    entity_type: str = None):
    """"""
    function for register schema,please declare them before register

    :param providers: the supported providers for the schema
    :type providers:
    :param db_name: database name for the schema
    :type db_name:
    :param schema_base:
    :type schema_base:
    :param entity_type: the schema related entity_type
    :type entity_type:
    :return:
    :rtype:
    """"""
    schemas = []
    for item in schema_base.registry.mappers:
        cls = item.class_
        if type(cls) == DeclarativeMeta:
            # register provider to the schema
            for provider in providers:
                if issubclass(cls, Mixin):
                    cls.register_provider(provider)

            if zvt_context.dbname_map_schemas.get(db_name):
                schemas = zvt_context.dbname_map_schemas[db_name]
            zvt_context.schemas.append(cls)
            if entity_type:
                add_to_map_list(the_map=zvt_context.entity_map_schemas, key=entity_type, value=cls)
            schemas.append(cls)

    zvt_context.dbname_map_schemas[db_name] = schemas

    for provider in providers:
        if provider not in zvt_context.providers:
            zvt_context.providers.append(provider)

        if not zvt_context.provider_map_dbnames.get(provider):
            zvt_context.provider_map_dbnames[provider] = []
        zvt_context.provider_map_dbnames[provider].append(db_name)
        zvt_context.dbname_map_base[db_name] = schema_base

        # create the db & table
        engine = get_db_engine(provider, db_name=db_name)
        schema_base.metadata.create_all(engine)

        session_fac = get_db_session_factory(provider, db_name=db_name)
        session_fac.configure(bind=engine)

    for provider in providers:
        engine = get_db_engine(provider, db_name=db_name)

        # create index for 'timestamp','entity_id','code','report_period','updated_timestamp
        for table_name, table in iter(schema_base.metadata.tables.items()):
            index_list = []
            with engine.connect() as con:
                rs = con.execute(""PRAGMA INDEX_LIST('{}')"".format(table_name))
                for row in rs:
                    index_list.append(row[1])

            logger.debug('engine:{},table:{},index:{}'.format(engine, table_name, index_list))

            for col in ['timestamp', 'entity_id', 'code', 'report_period', 'created_timestamp', 'updated_timestamp']:
                if col in table.c:
                    column = eval('table.c.{}'.format(col))
                    index_name = '{}_{}_index'.format(table_name, col)
                    if index_name not in index_list:
                        index = sqlalchemy.schema.Index(index_name, column)
                        index.create(engine)
            for cols in [('timestamp', 'entity_id'), ('timestamp', 'code')]:
                if (cols[0] in table.c) and (col[1] in table.c):
                    column0 = eval('table.c.{}'.format(col[0]))
                    column1 = eval('table.c.{}'.format(col[1]))
                    index_name = '{}_{}_{}_index'.format(table_name, col[0], col[1])
                    if index_name not in index_list:
                        index = sqlalchemy.schema.Index(index_name, column0,
                                                        column1)
                        index.create(engine)","for row in rs:
    index_list.append(row[1])",index_list = [row[1] for row in rs],index_list = [row[1] for row in rs],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
bubbles,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bubbles/bubbles/backends/sql/ops.py,https://github.com/Stiivi/bubbles/tree/master/bubbles/backends/sql/ops.py,,_$435,"def _(ctx, master, details, joins):
    """"""Creates left inner master-detail join (star schema) where `master` is an
    iterator if the ""bigger"" table `details` are details. `joins` is a list of
    tuples `(master, detail)` where the master is index of master key and
    detail is index of detail key to be matched.

    If `inner` is `True` then inner join is performed. That means that only
    rows from master that have corresponding details are returned.

    .. warning::

        all detail iterators are consumed and result is held in memory. Do not
        use for large datasets.
    """"""
    # TODO: update documentation

    if not details:
        raise ArgumentError(""No details provided, nothing to join"")

    if not joins:
        raise ArgumentError(""No joins specified"")

    if len(details) != len(joins):
        raise ArgumentError(""For every detail there should be a join ""
                            ""(%d:%d)."" % (len(details), len(joins)))

    if not all(master.can_compose(detail) for detail in details):
        raise RetryOperation([""rows"", ""rows[]""], reason=""Can not compose"")

    out_fields = master.fields.clone()

    master_stmt = master.sql_statement().alias(""master"")
    selection = list(master_stmt.columns)

    joined = master_stmt
    i = 0
    for detail, join in zip(details, joins):
        alias = ""detail%s"" % i
        det_stmt = detail.sql_statement().alias(alias)
        master_key = join[""master""]
        detail_key = join[""detail""]

        onclause = master_stmt.c[master_key] == det_stmt.c[detail_key]
        # Skip detail key in the output

        for field, col in zip(detail.fields, det_stmt.columns):
            if str(field) != str(detail_key):
                selection.append(col)
                out_fields.append(field.clone())

        joined = sql.expression.join(joined,
                                     det_stmt,
                                     onclause=onclause)

    aliased = []
    for col, field in zip(selection, out_fields):
        aliased.append(col.label(field.name))

    select = sql.expression.select(aliased,
                                from_obj=joined,
                                use_labels=True)

    return master.clone_statement(statement=select, fields=out_fields)","for (col, field) in zip(selection, out_fields):
    aliased.append(col.label(field.name))","aliased = [col.label(field.name) for (col, field) in zip(selection, out_fields)]","aliased = [col.label(field.name) for (col, field) in zip(selection, out_fields)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
meta-dataset,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/meta-dataset/meta_dataset/learners/experimental/optimization_learners.py,https://github.com/google-research/meta-dataset/tree/master/meta_dataset/learners/experimental/optimization_learners.py,ExperimentalOptimizationLearner,detailed_forward_pass$328,"def detailed_forward_pass(self, data):
    """"""Returns all information from a forward pass of the `OptimizationLearner`.

    Args:
      data: A `meta_dataset.providers.Episode` containing the data for the
        episode.

    Returns:
      A `collections.NamedTuple` that contains the results of the forward pass.
    """"""
    # Loop initialization.
    init_loop_variables = self.task_parameters
    init_loop_variable_refs = [
        v.experimental_ref() for v in init_loop_variables
    ]

    # Construct ops for data-dependent episodic initialization.
    episodic_init_ops = self.episodic_init_ops(
        labels=data.support_labels,
        embeddings=self.embedding_fn(data.support_images, training=True),
        task_parameters=init_loop_variables,
    )

    def _forward_pass(iteration_idx_, variables_mapping_, images_,
                      onehot_labels_):
      """"""Helper function to compute the outputs of a forward pass.""""""

      with self.embedding_fn.reparameterize(variables_mapping_):
        # TODO(eringrant): Implement non-transductive batch normalization (i.e.,
        # pass the support set statistics through the query set forward pass.
        embeddings_ = self.embedding_fn(images_, training=True)

      # TODO(eringrant): `head_fn` is an attribute of the subclass.
      with self.head_fn.reparameterize(variables_mapping_):
        predictions_ = self.head_fn(embeddings_)[:, :data.way]

      accuracy_ = tf.reduce_mean(
          input_tensor=self.compute_accuracy(
              onehot_labels=onehot_labels_, predictions=predictions_))

      inner_objective_ = self.inner_objective(
          onehot_labels=onehot_labels_,
          predictions=predictions_,
          iteration_idx=iteration_idx_)

      outer_objective_ = self.outer_objective(
          onehot_labels=onehot_labels_,
          predictions=predictions_,
      )

      return ForwardPass(
          embeddings=embeddings_,
          predictions=predictions_,
          inner_objective_value=inner_objective_,
          outer_objective_value=outer_objective_,
          accuracy=accuracy_,
      )

    def _objective_fn(loop_variables_, iteration_idx_):
      """"""Evaluate the support set objective given `loop_variables_`.""""""

      # Get attribute paths for the loop_variables.
      loop_variables_mapping_ = dict(
          zip(init_loop_variable_refs, loop_variables_))

      adaptation_support_results = _forward_pass(
          iteration_idx_=iteration_idx_,
          variables_mapping_=loop_variables_mapping_,
          images_=data.support_images,
          onehot_labels_=data.onehot_support_labels)

      return adaptation_support_results.inner_objective_value

    def _e_step(loop_variables_):
      """"""Evaluate expectations given `loop_variables_`.""""""

      # Get attribute paths for the loop_variables.
      loop_variables_dict_ = dict(zip(init_loop_variable_refs, loop_variables_))

      with self.embedding_fn.reparameterize(loop_variables_dict_):
        # TODO(eringrant): training to True for normalization with batch stats.
        # Figure out the appropriate way to pass this around.
        train_embeddings_ = self.embedding_fn(data.train_images, training=True)

      class_embeddings_ = learner_base.class_specific_data(
          data.onehot_train_labels, train_embeddings_, self.logit_dim)

      def _compute_responsibilities(examples_, class_idx):
        train_predictions_ = tf.squeeze(
            self.head_fn(
                embeddings=examples_, components=True, class_idx=[class_idx]),
            axis=1)
        return tf.nn.softmax(train_predictions_, axis=-1)

      with self.head_fn.reparameterize(loop_variables_dict_):
        class_responsibilities_ = [
            _compute_responsibilities(embeddings_, class_idx=i)
            for i, embeddings_ in enumerate(class_embeddings_)
        ]

      return class_embeddings_, class_responsibilities_

    def _m_step(preupdate_vars, all_embeddings_, all_responsibilities_):
      """"""Compute parameter estimates given `loop_variables_`.""""""

      means, log_scales, logits = zip(*map(
          reparameterizable_distributions.fit_gaussian_mixture, all_embeddings_,
          all_responsibilities_, itertools.repeat(self.head_fn.damping)))

      def flatten(x):
        return list(itertools.chain.from_iterable(x))

      means = flatten(means)
      log_scales = flatten(log_scales)
      logits = flatten(logits)

      if not self.head_fn.estimate_loc:
        means = [None for _ in means]

      if not self.head_fn.estimate_scale:
        log_scales = [None for _ in log_scales]

      if not self.head_fn.estimate_logits:
        logits = [None for _ in logits]

      updated_vars = means + log_scales + logits

      # Replace constant variables.
      # TODO(eringrant): This interface differs from just excluding these
      # variables from `task_variables`.
      no_none_updated_vars = []
      for preupdate_var, updated_var in zip(preupdate_vars, updated_vars):
        if updated_var is None:
          no_none_updated_vars.append(preupdate_var)
        else:
          no_none_updated_vars.append(updated_var)

      # TODO(eringrant): This assumes an ordering of mean, log_scales,
      # mixing_logits.
      return no_none_updated_vars

    # Loop body.
    with tf.control_dependencies(episodic_init_ops):

      # Inner loop of expectation maximization.
      num_em_steps = getattr(self, 'num_em_steps', 0)
      if num_em_steps > 0:
        loop_variables = em_loop(
            num_updates=self.num_em_steps,
            e_step=_e_step,
            m_step=_m_step,
            variables=loop_variables)

      # Inner loop of gradient-based optimization.
      num_optimizer_steps = (
          self.num_update_steps + (self.additional_evaluation_update_steps
                                   if not self.is_training else 0))
      if num_optimizer_steps > 0:
        # pylint: disable=no-value-for-parameter
        final_loop_variables = optimizer_loop(
            num_updates=num_optimizer_steps,
            objective_fn=_objective_fn,
            update_fn=self.update_fn,
            variables=init_loop_variables,
            first_order=self.first_order,
            clip_grad_norm=self.clip_grad_norm,
        )
        # pylint: enable=no-value-for-parameter

      # If no inner loop adaptation is performed, ensure the episodic
      # initialization is still part of the graph via a control dependency.
      if num_optimizer_steps + num_em_steps == 0:
        loop_variables = [tf.identity(v) for v in init_loop_variables]

    # Get variable references to use when remapping the loop_variables.
    init_loop_variables_mapping = dict(
        zip(init_loop_variable_refs, init_loop_variables))
    final_loop_variables_mapping = dict(
        zip(init_loop_variable_refs, final_loop_variables))

    # Collect statistics about the inner optimization.
    with tf.compat.v1.name_scope('pre-adaptation'):
      with tf.compat.v1.name_scope('support'):
        pre_adaptation_support_results = _forward_pass(
            iteration_idx_=0,
            variables_mapping_=init_loop_variables_mapping,
            images_=data.support_images,
            onehot_labels_=data.onehot_support_labels)

      with tf.compat.v1.name_scope('query'):
        pre_adaptation_query_results = _forward_pass(
            iteration_idx_=0,
            variables_mapping_=init_loop_variables_mapping,
            images_=data.query_images,
            onehot_labels_=data.onehot_query_labels)

    with tf.compat.v1.name_scope('post-adaptation'):
      with tf.compat.v1.name_scope('support'):
        post_adaptation_support_results = _forward_pass(
            iteration_idx_=num_optimizer_steps,
            variables_mapping_=final_loop_variables_mapping,
            images_=data.support_images,
            onehot_labels_=data.onehot_support_labels,
        )

      with tf.compat.v1.name_scope('query'):
        post_adaptation_query_results = _forward_pass(
            iteration_idx_=num_optimizer_steps,
            variables_mapping_=final_loop_variables_mapping,
            images_=data.query_images,
            onehot_labels_=data.onehot_query_labels,
        )

    def _support_module_objective_fn(module_variables_, module_variable_refs_):
      """"""Evaluate the query set objective given `module_variables_`.""""""
      # Use the values of the parameters at convergence as the default value.
      variables_mapping_ = final_loop_variables_mapping.copy()

      # Loop over and replace the module-specific variables.
      for module_variable_ref, module_variable in zip(module_variable_refs_,
                                                      module_variables_):
        variables_mapping_[module_variable_ref] = module_variable

      adaptation_query_results = _forward_pass(
          iteration_idx_=num_optimizer_steps,
          variables_mapping_=variables_mapping_,
          images_=data.support_images,
          onehot_labels_=data.onehot_support_labels,
      )

      return adaptation_query_results.inner_objective_value

    def _query_module_objective_fn(module_variables_, module_variable_refs_):
      """"""Evaluate the query set objective given `module_variables_`.""""""
      # Use the values of the parameters at convergence as the default value.
      variables_mapping_ = final_loop_variables_mapping.copy()

      # Loop over and replace the module-specific variables.
      for module_variable_ref, module_variable in zip(module_variable_refs_,
                                                      module_variables_):
        variables_mapping_[module_variable_ref] = module_variable

      adaptation_query_results = _forward_pass(
          iteration_idx_=num_optimizer_steps,
          variables_mapping_=variables_mapping_,
          images_=data.query_images,
          onehot_labels_=data.onehot_query_labels)

      return adaptation_query_results.inner_objective_value

    return Adaptation(
        pre_adaptation_support_results=pre_adaptation_support_results,
        post_adaptation_support_results=post_adaptation_support_results,
        pre_adaptation_query_results=pre_adaptation_query_results,
        post_adaptation_query_results=post_adaptation_query_results,
        objective_fn=_objective_fn,
        support_module_objective_fn=_support_module_objective_fn,
        query_module_objective_fn=_query_module_objective_fn,
        forward_pass_fn=_forward_pass,
        init_loop_variables_mapping=init_loop_variables_mapping,
        final_loop_variables_mapping=final_loop_variables_mapping,
    )","for (preupdate_var, updated_var) in zip(preupdate_vars, updated_vars):
    if updated_var is None:
        no_none_updated_vars.append(preupdate_var)
    else:
        no_none_updated_vars.append(updated_var)","no_none_updated_vars = [preupdate_var if updated_var is None else updated_var for (preupdate_var, updated_var) in zip(preupdate_vars, updated_vars)]","no_none_updated_vars = [preupdate_var if updated_var is None else updated_var for (preupdate_var, updated_var) in zip(preupdate_vars, updated_vars)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
nanodet,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nanodet/nanodet/data/dataset/xml_dataset.py,https://github.com/RangiLyu/nanodet/tree/master/nanodet/data/dataset/xml_dataset.py,XMLDataset,xml_to_coco$61,"def xml_to_coco(self, ann_path):
        """"""
        convert xml annotations to coco_api
        :param ann_path:
        :return:
        """"""
        logging.info(""loading annotations into memory..."")
        tic = time.time()
        ann_file_names = get_file_list(ann_path, type="".xml"")
        logging.info(""Found {} annotation files."".format(len(ann_file_names)))
        image_info = []
        categories = []
        annotations = []
        for idx, supercat in enumerate(self.class_names):
            categories.append(
                {""supercategory"": supercat, ""id"": idx + 1, ""name"": supercat}
            )
        ann_id = 1
        for idx, xml_name in enumerate(ann_file_names):
            tree = ET.parse(os.path.join(ann_path, xml_name))
            root = tree.getroot()
            file_name = root.find(""filename"").text
            width = int(root.find(""size"").find(""width"").text)
            height = int(root.find(""size"").find(""height"").text)
            info = {
                ""file_name"": file_name,
                ""height"": height,
                ""width"": width,
                ""id"": idx + 1,
            }
            image_info.append(info)
            for _object in root.findall(""object""):
                category = _object.find(""name"").text
                if category not in self.class_names:
                    logging.warning(
                        ""WARNING! {} is not in class_names! ""
                        ""Pass this box annotation."".format(category)
                    )
                    continue
                for cat in categories:
                    if category == cat[""name""]:
                        cat_id = cat[""id""]
                xmin = int(_object.find(""bndbox"").find(""xmin"").text)
                ymin = int(_object.find(""bndbox"").find(""ymin"").text)
                xmax = int(_object.find(""bndbox"").find(""xmax"").text)
                ymax = int(_object.find(""bndbox"").find(""ymax"").text)
                w = xmax - xmin
                h = ymax - ymin
                if w < 0 or h < 0:
                    logging.warning(
                        ""WARNING! Find error data in file {}! Box w and ""
                        ""h should > 0. Pass this box annotation."".format(xml_name)
                    )
                    continue
                coco_box = [max(xmin, 0), max(ymin, 0), min(w, width), min(h, height)]
                ann = {
                    ""image_id"": idx + 1,
                    ""bbox"": coco_box,
                    ""category_id"": cat_id,
                    ""iscrowd"": 0,
                    ""id"": ann_id,
                    ""area"": coco_box[2] * coco_box[3],
                }
                annotations.append(ann)
                ann_id += 1

        coco_dict = {
            ""images"": image_info,
            ""categories"": categories,
            ""annotations"": annotations,
        }
        logging.info(
            ""Load {} xml files and {} boxes"".format(len(image_info), len(annotations))
        )
        logging.info(""Done (t={:0.2f}s)"".format(time.time() - tic))
        return coco_dict","for (idx, supercat) in enumerate(self.class_names):
    categories.append({'supercategory': supercat, 'id': idx + 1, 'name': supercat})","categories = [{'supercategory': supercat, 'id': idx + 1, 'name': supercat} for (idx, supercat) in enumerate(self.class_names)]","categories = [{'supercategory': supercat, 'id': idx + 1, 'name': supercat} for (idx, supercat) in enumerate(self.class_names)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
GAM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GAM/src/gam/gapi/vault.py,https://github.com/jay0lee/GAM/tree/master/src/gam/gapi/vault.py,,printExports$851,"def printExports():
    v = buildGAPIObject()
    todrive = False
    csvRows = []
    initialTitles = ['matterId', 'id', 'name', 'createTime', 'status']
    titles = initialTitles[:]
    matters = []
    matterIds = []
    i = 3
    while i < len(sys.argv):
        myarg = sys.argv[i].lower().replace('_', '')
        if myarg == 'todrive':
            todrive = True
            i += 1
        elif myarg in ['matter', 'matters']:
            matters = sys.argv[i + 1].split(',')
            i += 2
        else:
            controlflow.invalid_argument_exit(myarg, 'gam print exports')
    if not matters:
        fields = 'matters(matterId),nextPageToken'
        matters_results = gapi.get_all_pages(v.matters(),
                                             'list',
                                             'matters',
                                             view='BASIC',
                                             state='OPEN',
                                             fields=fields)
        for matter in matters_results:
            matterIds.append(matter['matterId'])
    else:
        for matter in matters:
            matterIds.append(getMatterItem(v, matter))
    for matterId in matterIds:
        sys.stderr.write(f'Retrieving exports for matter {matterId}\n')
        exports = gapi.get_all_pages(v.matters().exports(),
                                     'list',
                                     'exports',
                                     matterId=matterId)
        for export in exports:
            display.add_row_titles_to_csv_file(
                utils.flatten_json(export, flattened={'matterId': matterId}),
                csvRows, titles)
    display.sort_csv_titles(initialTitles, titles)
    display.write_csv_file(csvRows, titles, 'Vault Exports', todrive)","for matter in matters_results:
    matterIds.append(matter['matterId'])",matterIds = [matter['matterId'] for matter in matters_results],matterIds = [matter['matterId'] for matter in matters_results],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
dataprep,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dataprep/dataprep/eda/distribution/render.py,https://github.com/sfu-db/dataprep/tree/master/dataprep/eda/distribution/render.py,,geo_viz$661,"def geo_viz(
    df: pd.DataFrame,
    plot_width: int,
    y: Optional[str] = None,
) -> Panel:
    """"""
    Render a geo plot visualization
    """"""
    # pylint: disable=too-many-arguments,too-many-locals,too-many-statements
    # pylint: disable=too-many-function-args

    # title = f""{y} by {x}""

    minimum = min(df[y])
    maximum = max(df[y])

    # no_name=[]
    value = {}
    names = NAME_DICT.keys()
    for i in range(df[y].shape[0]):
        if df.index[i].lower().strip() in names:
            value[NAME_DICT[df.index[i].lower().strip()]] = df[y][i]
        # else:
        #     no_name.append(df.index[i])

    temp_list = []
    for itr in range(len(MAPS[""name""])):
        temp_list.append(value.get(MAPS[""fip""][itr], ""unknown""))
    MAPS[""value""] = temp_list

    mapper = LinearColorMapper(
        palette=YlGnBu[33:233], low=minimum, high=maximum, nan_color=""#cccccc""
    )
    tools = ""pan,wheel_zoom,box_zoom,reset,hover""

    fig = Figure(
        plot_width=plot_width,
        plot_height=plot_width // 10 * 7,
        tools=tools,
        tooltips=[
            (""Name"", ""@name""),
            (y, ""@value""),
            (""(Long, Lat)"", ""($x, $y)""),
        ],
    )
    fig.grid.grid_line_color = None
    fig.hover.point_policy = ""follow_mouse""
    fig.background_fill_color = ""white""
    fig.x_range = Range1d(start=-180, end=180)
    fig.y_range = Range1d(start=-90, end=90)
    fig.patches(
        ""xs"",
        ""ys"",
        line_color=""white"",
        source=MAPS,
        fill_color={""field"": ""value"", ""transform"": mapper},
        line_width=0.5,
    )

    color_bar = ColorBar(
        color_mapper=mapper,
        major_label_text_font_size=""7px"",
        ticker=BasicTicker(desired_num_ticks=11),
        formatter=PrintfTickFormatter(format=""%10.2f""),
        label_standoff=6,
        border_line_color=None,
        location=(0, 0),
    )
    if minimum < maximum:
        fig.add_layout(color_bar, ""right"")

    return Panel(child=row(fig), title=""World Map"")","for itr in range(len(MAPS['name'])):
    temp_list.append(value.get(MAPS['fip'][itr], 'unknown'))","temp_list = [value.get(MAPS['fip'][itr], 'unknown') for itr in range(len(MAPS['name']))]","temp_list = [value.get(MAPS['fip'][itr], 'unknown') for itr in range(len(MAPS['name']))]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
my_first_tic_tac_toe,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/my_first_tic_tac_toe/codegenerator/generate_ttt.py,https://github.com/asweigart/my_first_tic_tac_toe/tree/master/codegenerator/generate_ttt.py,,chooseRandomMoveFromList$61,"def chooseRandomMoveFromList(board, movesList):
    # Returns a valid move from the passed list on the passed board.
    # Returns None if there is no valid move.
    possibleMoves = []
    for i in movesList:
        if isSpaceFree(board, i):
            possibleMoves.append(i)

    if len(possibleMoves) != 0:
        return random.choice(possibleMoves)
    else:
        return None","for i in movesList:
    if isSpaceFree(board, i):
        possibleMoves.append(i)","possibleMoves = [i for i in movesList if isSpaceFree(board, i)]","possibleMoves = [i for i in movesList if isSpaceFree(board, i)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
alibi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/alibi/alibi/explainers/shap_wrappers.py,https://github.com/SeldonIO/alibi/tree/master/alibi/explainers/shap_wrappers.py,TreeShap,_build_explanation$1476,"def _build_explanation(self,
                           X: Union[np.ndarray, pd.DataFrame, 'catboost.Pool'],
                           shap_output: List[np.ndarray],
                           expected_value: List[float],
                           **kwargs) -> Explanation:

        """"""
        Create an explanation object. If output summarisation is required and all inputs necessary for this operation
        are passed, the raw shap values are summed first so that a single shap value is returned for each categorical
        variable, as opposed to a shap value per dimension of categorical variable encoding. Similarly, the
        shap interaction values are summed such that they represent the interaction between categorical variables as
        opposed to levels of categorical variables. If the interaction option has been specified during `explain`,
        this method computes the shap values given the interactions prior to creating the response.

        Parameters
        ----------
        X
            Instances to be explained.
        shap_output
            If `explain` is callled with ``interactions=True`` then the list contains arrays of dimensionality
            `n_instances x n_features x n_features` of shap interaction values. Otherwise, it contains arrays of
            dimension `n_instances x n_features` representing shap values. The length of the list equals the number of
            model outputs.
        expected_value
            A list containing the expected value of the prediction for each class. Its length is equal to that of
            `shap_output`.

        Returns
        -------
        explanation
            An `Explanation` object containing the shap values and prediction in the `data` field, along with a
            `meta` field containing additional data. See usage at `TreeSHAP examples`_ for details.

            .. _TreeSHAP examples:
               https://docs.seldon.io/projects/alibi/en/stable/methods/TreeSHAP.html

        """"""

        y = kwargs.get('y')
        if y is None:
            y = np.array([])
        cat_vars_start_idx = kwargs.get('cat_vars_start_idx', ())  # type: Sequence[int]
        cat_vars_enc_dim = kwargs.get('cat_vars_enc_dim', ())  # type: Sequence[int]
        summarise_result = kwargs.get('summarise_result', False)  # type: bool

        # check if interactions were computed
        if len(shap_output[0].shape) == 3:
            shap_interaction_values = shap_output
            # shap values are the sum over all shap interaction values for each instance
            shap_values = [interactions.sum(axis=2) for interactions in shap_output]
        else:
            shap_interaction_values = [np.array([])]
            shap_values = shap_output  # type: ignore
        if summarise_result:
            self._check_result_summarisation(summarise_result, cat_vars_start_idx, cat_vars_enc_dim)
        if self.summarise_result:
            summarised_shap = []
            for shap_array in shap_values:
                summarised_shap.append(sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim))  # type: ignore
            shap_values = summarised_shap
            if shap_interaction_values[0].size != 0:
                summarised_shap_interactions = []
                for shap_array in shap_interaction_values:
                    summarised_shap_interactions.append(
                        sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim)
                    )
                shap_interaction_values = summarised_shap_interactions

        # NB: Can't get the raw prediction from model when model_output = 'log_loss` as shap library does
        # not support this (issue raised). We may be able to support this if there's a compelling need.
        # NB: raw output of a regression or classification task will not work for pyspark (predict not implemented)
        if self.model_output == 'log_loss':
            loss = self._explainer.model.predict(X, y, tree_limit=self.tree_limit)
            raw_predictions = []  # type: Any
        else:
            loss = []
            raw_predictions = self._explainer.model.predict(X, tree_limit=self.tree_limit)
            # flatten array of predictions if the trailing dimension is 1
            if raw_predictions.shape[-1] == 1:
                raw_predictions = raw_predictions.squeeze(-1)

        # predicted class
        argmax_pred = []  # type: Any
        if self.task != 'regression':
            if not isinstance(raw_predictions, list):
                if self.scalar_output:
                    if self.model_output == 'raw':
                        probas = expit(raw_predictions)
                    else:
                        probas = raw_predictions
                    argmax_pred = (probas > 0.5).astype(int)
                else:
                    argmax_pred = np.argmax(np.atleast_2d(raw_predictions), axis=1)

        importances = rank_by_importance(shap_values, feature_names=self.feature_names)  # type: ignore

        if self._explainer.model.model_type == 'catboost':
            import catboost  # noqa: F811
            if isinstance(X, catboost.Pool):
                X = X.get_features()
        # output explanation dictionary
        data = copy.deepcopy(DEFAULT_DATA_TREE_SHAP)
        data.update(
            shap_values=shap_values,
            shap_interaction_values=shap_interaction_values,
            expected_value=expected_value,
            categorical_names=self.categorical_names,
            feature_names=self.feature_names,
        )
        data['raw'].update(
            raw_prediction=raw_predictions,
            loss=loss,
            prediction=argmax_pred,
            instances=np.array(X),
            labels=y,
            importances=importances,
        )

        self._update_metadata({""summarise_result"": self.summarise_result}, params=True)

        return Explanation(meta=copy.deepcopy(self.meta), data=data)","for shap_array in shap_values:
    summarised_shap.append(sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim))","summarised_shap = [sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim) for shap_array in shap_values]","summarised_shap = [sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim) for shap_array in shap_values]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
alibi,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/alibi/alibi/explainers/shap_wrappers.py,https://github.com/SeldonIO/alibi/tree/master/alibi/explainers/shap_wrappers.py,TreeShap,_build_explanation$1476,"def _build_explanation(self,
                           X: Union[np.ndarray, pd.DataFrame, 'catboost.Pool'],
                           shap_output: List[np.ndarray],
                           expected_value: List[float],
                           **kwargs) -> Explanation:

        """"""
        Create an explanation object. If output summarisation is required and all inputs necessary for this operation
        are passed, the raw shap values are summed first so that a single shap value is returned for each categorical
        variable, as opposed to a shap value per dimension of categorical variable encoding. Similarly, the
        shap interaction values are summed such that they represent the interaction between categorical variables as
        opposed to levels of categorical variables. If the interaction option has been specified during `explain`,
        this method computes the shap values given the interactions prior to creating the response.

        Parameters
        ----------
        X
            Instances to be explained.
        shap_output
            If `explain` is callled with ``interactions=True`` then the list contains arrays of dimensionality
            `n_instances x n_features x n_features` of shap interaction values. Otherwise, it contains arrays of
            dimension `n_instances x n_features` representing shap values. The length of the list equals the number of
            model outputs.
        expected_value
            A list containing the expected value of the prediction for each class. Its length is equal to that of
            `shap_output`.

        Returns
        -------
        explanation
            An `Explanation` object containing the shap values and prediction in the `data` field, along with a
            `meta` field containing additional data. See usage at `TreeSHAP examples`_ for details.

            .. _TreeSHAP examples:
               https://docs.seldon.io/projects/alibi/en/stable/methods/TreeSHAP.html

        """"""

        y = kwargs.get('y')
        if y is None:
            y = np.array([])
        cat_vars_start_idx = kwargs.get('cat_vars_start_idx', ())  # type: Sequence[int]
        cat_vars_enc_dim = kwargs.get('cat_vars_enc_dim', ())  # type: Sequence[int]
        summarise_result = kwargs.get('summarise_result', False)  # type: bool

        # check if interactions were computed
        if len(shap_output[0].shape) == 3:
            shap_interaction_values = shap_output
            # shap values are the sum over all shap interaction values for each instance
            shap_values = [interactions.sum(axis=2) for interactions in shap_output]
        else:
            shap_interaction_values = [np.array([])]
            shap_values = shap_output  # type: ignore
        if summarise_result:
            self._check_result_summarisation(summarise_result, cat_vars_start_idx, cat_vars_enc_dim)
        if self.summarise_result:
            summarised_shap = []
            for shap_array in shap_values:
                summarised_shap.append(sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim))  # type: ignore
            shap_values = summarised_shap
            if shap_interaction_values[0].size != 0:
                summarised_shap_interactions = []
                for shap_array in shap_interaction_values:
                    summarised_shap_interactions.append(
                        sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim)
                    )
                shap_interaction_values = summarised_shap_interactions

        # NB: Can't get the raw prediction from model when model_output = 'log_loss` as shap library does
        # not support this (issue raised). We may be able to support this if there's a compelling need.
        # NB: raw output of a regression or classification task will not work for pyspark (predict not implemented)
        if self.model_output == 'log_loss':
            loss = self._explainer.model.predict(X, y, tree_limit=self.tree_limit)
            raw_predictions = []  # type: Any
        else:
            loss = []
            raw_predictions = self._explainer.model.predict(X, tree_limit=self.tree_limit)
            # flatten array of predictions if the trailing dimension is 1
            if raw_predictions.shape[-1] == 1:
                raw_predictions = raw_predictions.squeeze(-1)

        # predicted class
        argmax_pred = []  # type: Any
        if self.task != 'regression':
            if not isinstance(raw_predictions, list):
                if self.scalar_output:
                    if self.model_output == 'raw':
                        probas = expit(raw_predictions)
                    else:
                        probas = raw_predictions
                    argmax_pred = (probas > 0.5).astype(int)
                else:
                    argmax_pred = np.argmax(np.atleast_2d(raw_predictions), axis=1)

        importances = rank_by_importance(shap_values, feature_names=self.feature_names)  # type: ignore

        if self._explainer.model.model_type == 'catboost':
            import catboost  # noqa: F811
            if isinstance(X, catboost.Pool):
                X = X.get_features()
        # output explanation dictionary
        data = copy.deepcopy(DEFAULT_DATA_TREE_SHAP)
        data.update(
            shap_values=shap_values,
            shap_interaction_values=shap_interaction_values,
            expected_value=expected_value,
            categorical_names=self.categorical_names,
            feature_names=self.feature_names,
        )
        data['raw'].update(
            raw_prediction=raw_predictions,
            loss=loss,
            prediction=argmax_pred,
            instances=np.array(X),
            labels=y,
            importances=importances,
        )

        self._update_metadata({""summarise_result"": self.summarise_result}, params=True)

        return Explanation(meta=copy.deepcopy(self.meta), data=data)","for shap_array in shap_interaction_values:
    summarised_shap_interactions.append(sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim))","summarised_shap_interactions = [sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim) for shap_array in shap_interaction_values]","summarised_shap_interactions = [sum_categories(shap_array, cat_vars_start_idx, cat_vars_enc_dim) for shap_array in shap_interaction_values]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
dulwich,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dulwich/dulwich/objectspec.py,https://github.com/dulwich/dulwich/tree/master/dulwich/objectspec.py,,scan_for_short_id$194,"def scan_for_short_id(object_store, prefix):
    """"""Scan an object store for a short id.""""""
    # TODO(jelmer): This could short-circuit looking for objects
    # starting with a certain prefix.
    ret = []
    for object_id in object_store:
        if object_id.startswith(prefix):
            ret.append(object_store[object_id])
    if not ret:
        raise KeyError(prefix)
    if len(ret) == 1:
        return ret[0]
    raise AmbiguousShortId(prefix, ret)","for object_id in object_store:
    if object_id.startswith(prefix):
        ret.append(object_store[object_id])",ret = [object_store[object_id] for object_id in object_store if object_id.startswith(prefix)],ret = [object_store[object_id] for object_id in object_store if object_id.startswith(prefix)],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
mifthtools,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mifthtools/blender/addons/2.7/super_grouper/grouper_main.py,https://github.com/mifth/mifthtools/tree/master/blender/addons/2.7/super_grouper/grouper_main.py,,generate_id$351,"def generate_id():
    # Generate unique id
    other_ids = []
    for scene in bpy.data.scenes:
        if scene != bpy.context.scene and scene.name.endswith(SCENE_SGR) is False:
            for s_group in scene.super_groups:
                other_ids.append(s_group.unique_id)

    while True:
        uni_numb = None
        uniq_id_temp = ''.join(random.choice(string.ascii_uppercase + string.digits)
                               for _ in range(10))
        if uniq_id_temp not in other_ids:
            uni_numb = uniq_id_temp
            break

    other_ids = None  # clean
    return uni_numb","for scene in bpy.data.scenes:
    if scene != bpy.context.scene and scene.name.endswith(SCENE_SGR) is False:
        for s_group in scene.super_groups:
            other_ids.append(s_group.unique_id)",other_ids = [s_group.unique_id for scene in bpy.data.scenes if scene != bpy.context.scene and scene.name.endswith(SCENE_SGR) is False for s_group in scene.super_groups],other_ids = [s_group.unique_id for scene in bpy.data.scenes if scene != bpy.context.scene and scene.name.endswith(SCENE_SGR) is False for s_group in scene.super_groups],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
AutoDL-Projects,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoDL-Projects/exps/NATS-Bench/draw-fig2_5.py,https://github.com/D-X-Y/AutoDL-Projects/tree/master/exps/NATS-Bench/draw-fig2_5.py,,calculate_correlation$517,"def calculate_correlation(*vectors):
    matrix = []
    for i, vectori in enumerate(vectors):
        x = []
        for j, vectorj in enumerate(vectors):
            # x.append(np.corrcoef(vectori, vectorj)[0,1])
            x.append(compute_kendalltau(vectori, vectorj))
        matrix.append(x)
    return np.array(matrix)","for (j, vectorj) in enumerate(vectors):
    x.append(compute_kendalltau(vectori, vectorj))","x = [compute_kendalltau(vectori, vectorj) for (j, vectorj) in enumerate(vectors)]","x = [compute_kendalltau(vectori, vectorj) for (j, vectorj) in enumerate(vectors)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
open-paperless,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/open-paperless/mayan/apps/navigation/templatetags/navigation_tags.py,https://github.com/zhoubear/open-paperless/tree/master/mayan/apps/navigation/templatetags/navigation_tags.py,,get_menus_links$17,"def get_menus_links(context, names, source=None):
    result = []

    for name in names.split(','):
        for links in Menu.get(name=name).resolve(context=context):
            if links:
                result.append(links)

    return result","for name in names.split(','):
    for links in Menu.get(name=name).resolve(context=context):
        if links:
            result.append(links)","result = [links for name in names.split(',') for links in Menu.get(name=name).resolve(context=context) if links]","result = [links for name in names.split(',') for links in Menu.get(name=name).resolve(context=context) if links]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
bCNC,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bCNC/bCNC/lib/python_utils/formatters.py,https://github.com/vlachoudis/bCNC/tree/master/bCNC/lib/python_utils/formatters.py,,timesince$42,"def timesince(dt, default=""just now""):
    """"""
    Returns string representing 'time since' e.g.
    3 days ago, 5 hours ago etc.

    >>> now = datetime.datetime.now()
    >>> timesince(now)
    'just now'
    >>> timesince(now - datetime.timedelta(seconds=1))
    '1 second ago'
    >>> timesince(now - datetime.timedelta(seconds=2))
    '2 seconds ago'
    >>> timesince(now - datetime.timedelta(seconds=60))
    '1 minute ago'
    >>> timesince(now - datetime.timedelta(seconds=61))
    '1 minute and 1 second ago'
    >>> timesince(now - datetime.timedelta(seconds=62))
    '1 minute and 2 seconds ago'
    >>> timesince(now - datetime.timedelta(seconds=120))
    '2 minutes ago'
    >>> timesince(now - datetime.timedelta(seconds=121))
    '2 minutes and 1 second ago'
    >>> timesince(now - datetime.timedelta(seconds=122))
    '2 minutes and 2 seconds ago'
    >>> timesince(now - datetime.timedelta(seconds=3599))
    '59 minutes and 59 seconds ago'
    >>> timesince(now - datetime.timedelta(seconds=3600))
    '1 hour ago'
    >>> timesince(now - datetime.timedelta(seconds=3601))
    '1 hour and 1 second ago'
    >>> timesince(now - datetime.timedelta(seconds=3602))
    '1 hour and 2 seconds ago'
    >>> timesince(now - datetime.timedelta(seconds=3660))
    '1 hour and 1 minute ago'
    >>> timesince(now - datetime.timedelta(seconds=3661))
    '1 hour and 1 minute ago'
    >>> timesince(now - datetime.timedelta(seconds=3720))
    '1 hour and 2 minutes ago'
    >>> timesince(now - datetime.timedelta(seconds=3721))
    '1 hour and 2 minutes ago'
    >>> timesince(datetime.timedelta(seconds=3721))
    '1 hour and 2 minutes ago'
    """"""
    if isinstance(dt, datetime.timedelta):
        diff = dt
    else:
        now = datetime.datetime.now()
        diff = abs(now - dt)

    periods = (
        (diff.days / 365, ""year"", ""years""),
        (diff.days % 365 / 30, ""month"", ""months""),
        (diff.days % 30 / 7, ""week"", ""weeks""),
        (diff.days % 7, ""day"", ""days""),
        (diff.seconds / 3600, ""hour"", ""hours""),
        (diff.seconds % 3600 / 60, ""minute"", ""minutes""),
        (diff.seconds % 60, ""second"", ""seconds""),
    )

    output = []
    for period, singular, plural in periods:
        if int(period):
            if int(period) == 1:
                output.append(f""{int(period)} {singular}"")
            else:
                output.append(f""{int(period)} {plural}"")

    if output:
        return f""{' and '.join(output[:2])} ago""

    return default","for (period, singular, plural) in periods:
    if int(period):
        if int(period) == 1:
            output.append(f'{int(period)} {singular}')
        else:
            output.append(f'{int(period)} {plural}')","output = [f'{int(period)} {singular}' if int(period) == 1 else f'{int(period)} {plural}' for (period, singular, plural) in periods if int(period)]","output = [f'{int(period)} {singular}' if int(period) == 1 else f'{int(period)} {plural}' for (period, singular, plural) in periods if int(period)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
Minecraft-Overviewer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Minecraft-Overviewer/overviewer.py,https://github.com/overviewer/Minecraft-Overviewer/tree/master//overviewer.py,,main$52,"def main():
    # bootstrap the logger with defaults
    logger.configure()

    if os.name == ""posix"":
        if os.geteuid() == 0:
            logging.warning(""You are running Overviewer as root. ""
                            ""It is recommended that you never do this, ""
                            ""as it is dangerous for your system. If you are running ""
                            ""into permission errors, fix your file/directory ""
                            ""permissions instead. Overviewer does not need access to ""
                            ""critical system resources and therefore does not require ""
                            ""root access."")
        try:
            with open(""/etc/redhat-release"", ""r"") as release_f:
                rel_contents = release_f.read()
                try:
                    major_rel = re.search(r'\d(\.\d+)?', rel_contents).group(0).split('.')[0]
                    if major_rel == ""6"":
                        logging.warning(
                            ""We will be dropping support for this release of your distribution ""
                            ""soon. Please upgrade as soon as possible, or you will not receive ""
                            ""future Overviewer updates."")
                except AttributeError:
                    pass
        except IOError:
            pass

    try:
        cpus = multiprocessing.cpu_count()
    except NotImplementedError:
        cpus = 1

    avail_north_dirs = ['lower-left', 'upper-left', 'upper-right', 'lower-right', 'auto']

    # Parse for basic options
    parser = ArgumentParser(usage=helptext)
    parser.add_argument(""-c"", ""--config"", dest=""config"", action=""store"",
                        help=""Specify the config file to use."")
    parser.add_argument(""-p"", ""--processes"", dest=""procs"", action=""store"", type=int,
                        help=""The number of local worker processes to spawn. Defaults to the ""
                        ""number of CPU cores your computer has."")

    parser.add_argument(""--pid"", dest=""pid"", action=""store"", help=""Specify the pid file to use."")
    # Options that only apply to the config-less render usage
    parser.add_argument(""--rendermodes"", dest=""rendermodes"", action=""store"",
                        help=""If you're not using a config file, specify which rendermodes to ""
                        ""render with this option. This is a comma-separated list."")
    parser.add_argument(""world"", nargs='?',
                        help=""Path or name of the world you want to render."")
    parser.add_argument(""output"", nargs='?',
                        help=""Output directory for the rendered map."")

    # Useful one-time render modifiers:
    render_modifiers = parser.add_mutually_exclusive_group()
    render_modifiers.add_argument(""--forcerender"", dest=""forcerender"", action=""store_true"",
                                  help=""Force re-render the entire map."")
    render_modifiers.add_argument(""--check-tiles"", dest=""checktiles"", action=""store_true"",
                                  help=""Check each tile on disk and re-render old tiles."")
    render_modifiers.add_argument(""--no-tile-checks"", dest=""notilechecks"", action=""store_true"",
                                  help=""Only render tiles that come from chunks that have changed ""
                                  ""since the last render (the default)."")

    # Useful one-time debugging options:
    parser.add_argument(""--check-terrain"", dest=""check_terrain"", action=""store_true"",
                        help=""Try to locate the texture files. Useful for debugging texture""
                        "" problems."")
    parser.add_argument(""-V"", ""--version"", dest=""version"",
                        help=""Display version information and then exits."", action=""store_true"")
    parser.add_argument(""--check-version"", dest=""checkversion"",
                        help=""Fetch information about the latest version of Overviewer."",
                        action=""store_true"")
    parser.add_argument(""--update-web-assets"", dest='update_web_assets', action=""store_true"",
                        help=""Update web assets. Will *not* render tiles or update ""
                        ""overviewerConfig.js."")

    # Log level options:
    parser.add_argument(""-q"", ""--quiet"", dest=""quiet"", action=""count"", default=0,
                        help=""Print less output. You can specify this option multiple times."")
    parser.add_argument(""-v"", ""--verbose"", dest=""verbose"", action=""count"", default=0,
                        help=""Print more output. You can specify this option multiple times."")
    parser.add_argument(""--simple-output"", dest=""simple"", action=""store_true"", default=False,
                        help=""Use a simple output format, with no colors or progress bars."")

    # create a group for ""plugin exes""
    # (the concept of a plugin exe is only loosely defined at this point)
    exegroup = parser.add_argument_group(""Other Scripts"", ""These scripts may accept different ""
                                         ""arguments than the ones listed above."")
    exegroup.add_argument(""--genpoi"", dest=""genpoi"", action=""store_true"",
                          help=""Run the genPOI script."")
    exegroup.add_argument(""--skip-scan"", dest=""skipscan"", action=""store_true"",
                          help=""When running GenPOI, don't scan for entities."")
    exegroup.add_argument(""--skip-players"", dest=""skipplayers"", action=""store_true"",
                          help=""When running GenPOI, don't scan player data."")

    args, unknowns = parser.parse_known_args()

    # Check for possible shell quoting issues
    if len(unknowns) > 0 and args.world and args.output:
        possible_mistakes = []
        for i in range(len(unknowns) + 1):
            possible_mistakes.append("" "".join([args.world, args.output] + unknowns[:i]))
            possible_mistakes.append("" "".join([args.output] + unknowns[:i]))
        for mistake in possible_mistakes:
            if os.path.exists(mistake):
                logging.warning(""Looks like you tried to make me use {0} as an argument, but ""
                                ""forgot to quote the argument correctly. Try using \""{0}\"" ""
                                ""instead if the spaces are part of the path."".format(mistake))
                parser.error(""Too many arguments."")
        parser.error(""Too many arguments."")

    # first thing to do is check for stuff in the exegroup:
    if args.genpoi:
        # remove the ""--genpoi"" option from sys.argv before running genPI
        sys.argv.remove(""--genpoi"")
        g = __import__(""overviewer_core.aux_files"", {}, {}, [""genPOI""])
        g.genPOI.main()
        return 0

    # re-configure the logger now that we've processed the command line options
    logger.configure(logging.INFO + 10 * args.quiet - 10 * args.verbose,
                     verbose=args.verbose > 0, simple=args.simple)

    ##########################################################################
    # This section of main() runs in response to any one-time options we have,
    # such as -V for version reporting
    if args.version:
        print(""Minecraft Overviewer %s"" % util.findGitVersion() +
              "" (%s)"" % util.findGitHash()[:7])
        try:
            import overviewer_core.overviewer_version as overviewer_version
            print(""built on %s"" % overviewer_version.BUILD_DATE)
            if args.verbose > 0:
                print(""Build machine: %s %s"" % (overviewer_version.BUILD_PLATFORM,
                                                overviewer_version.BUILD_OS))
                print(""Read version information from %r"" % overviewer_version.__file__)
        except ImportError:
            print(""(build info not found)"")
        if args.verbose > 0:
            print(""Python executable: %r"" % sys.executable)
            print(sys.version)
        if not args.checkversion:
            return 0
    if args.checkversion:
        print(""Currently running Minecraft Overviewer %s"" % util.findGitVersion() +
              "" (%s)"" % util.findGitHash()[:7])
        try:
            from urllib import request
            import json
            latest_ver = json.loads(request.urlopen(""http://overviewer.org/download.json"")
                                    .read())['src']
            print(""Latest version of Minecraft Overviewer %s (%s)"" % (latest_ver['version'],
                                                                      latest_ver['commit'][:7]))
            print(""See https://overviewer.org/downloads for more information."")
        except Exception:
            print(""Failed to fetch latest version info."")
            if args.verbose > 0:
                import traceback
                traceback.print_exc()
            else:
                print(""Re-run with --verbose for more details."")
            return 1
        return 0

    if args.pid:
        if os.path.exists(args.pid):
            try:
                with open(args.pid, 'r') as fpid:
                    pid = int(fpid.read())
                    if util.pid_exists(pid):
                        print(""Overviewer is already running (pid exists) - exiting."")
                        return 0
            except (IOError, ValueError):
                pass
        with open(args.pid, ""w"") as f:
            f.write(str(os.getpid()))
    # if --check-terrain was specified, but we have NO config file, then we cannot
    # operate on a custom texture path.  we do terrain checking with a custom texture
    # pack later on, after we've parsed the config file
    if args.check_terrain and not args.config:
        import hashlib
        from overviewer_core.textures import Textures
        tex = Textures()

        logging.info(""Looking for a few common texture files..."")
        try:
            f = tex.find_file(""assets/minecraft/textures/block/sandstone_top.png"", verbose=True)
            f = tex.find_file(""assets/minecraft/textures/block/grass_block_top.png"", verbose=True)
            f = tex.find_file(""assets/minecraft/textures/block/diamond_ore.png"", verbose=True)
            f = tex.find_file(""assets/minecraft/textures/block/acacia_planks.png"", verbose=True)
            # 1.16
            f = tex.find_file(""assets/minecraft/textures/block/ancient_debris_top.png"",
                              verbose=True)
        except IOError:
            logging.error(""Could not find any texture files."")
            return 1

        return 0

    # if no arguments are provided, print out a helpful message
    if not (args.world and args.output) and not args.config:
        # first provide an appropriate error for bare-console users
        # that don't provide any options
        if util.is_bare_console():
            print(""\n"")
            print(""The Overviewer is a console program.  Please open a Windows command prompt"")
            print(""first and run Overviewer from there.   Further documentation is available at"")
            print(""http://docs.overviewer.org/\n"")
            print(""\n"")
            print(""For a quick-start guide on Windows, visit the following URL:\n"")
            print(""http://docs.overviewer.org/en/latest/win_tut/windowsguide/\n"")

        else:
            # more helpful message for users who know what they're doing
            logging.error(""You must either specify --config or give me a world directory ""
                          ""and output directory."")
            parser.print_help()
            list_worlds()
        return 1

    ##########################################################################
    # This section does some sanity checking on the command line options passed
    # in. It checks to see if --config was given that no worldname/destdir were
    # given, and vice versa
    if args.config and (args.world and args.output):
        print()
        print(""If you specify --config, you need to specify the world to render as well as ""
              ""the destination in the config file, not on the command line."")
        print(""Put something like this in your config file:"")
        print(""worlds['myworld'] = %r"" % args[0])
        print(""outputdir = %r"" % (args[1] if len(args) > 1 else ""/path/to/output""))
        print()
        logging.error(""You cannot specify both --config AND a world + output directory on the ""
                      ""command line."")
        parser.print_help()
        return 1

    if not args.config and (args.world or args.output) and not (args.world and args.output):
        logging.error(""You must specify both the world directory and an output directory"")
        parser.print_help()
        return 1

    #########################################################################
    # These two halfs of this if statement unify config-file mode and
    # command-line mode.
    mw_parser = config_parser.MultiWorldParser()

    if not args.config:
        # No config file mode.
        worldpath, destdir = map(os.path.expanduser, [args.world, args.output])
        logging.debug(""Using %r as the world directory"", worldpath)
        logging.debug(""Using %r as the output directory"", destdir)

        mw_parser.set_config_item(""worlds"", {'world': worldpath})
        mw_parser.set_config_item(""outputdir"", destdir)

        rendermodes = ['lighting']
        if args.rendermodes:
            rendermodes = args.rendermodes.replace(""-"", ""_"").split("","")

        # Now for some good defaults
        renders = OrderedDict()
        for rm in rendermodes:
            renders[""world-"" + rm] = {
                ""world"": ""world"",
                ""title"": ""Overviewer Render (%s)"" % rm,
                ""rendermode"": rm,
            }
        mw_parser.set_config_item(""renders"", renders)

    else:
        if args.rendermodes:
            logging.error(""You cannot specify --rendermodes if you give a config file. ""
                          ""Configure your rendermodes in the config file instead."")
            parser.print_help()
            return 1

        # Parse the config file
        try:
            mw_parser.parse(os.path.expanduser(args.config))
        except config_parser.MissingConfigException as e:
            # this isn't a ""bug"", so don't print scary traceback
            logging.error(str(e))
            util.nice_exit(1)

    # Add in the command options here, perhaps overriding values specified in
    # the config
    if args.procs:
        mw_parser.set_config_item(""processes"", args.procs)

    # Now parse and return the validated config
    try:
        config = mw_parser.get_validated_config()
    except Exception as ex:
        if args.verbose:
            logging.exception(""An error was encountered with your configuration. ""
                              ""See the information below."")
        else:   # no need to print scary traceback!
            logging.error(""An error was encountered with your configuration."")
            logging.error(str(ex))
        return 1

    if args.check_terrain:   # we are already in the ""if configfile"" branch
        logging.info(""Looking for a few common texture files..."")
        for render_name, render in config['renders'].items():
            logging.info(""Looking at render %r."", render_name)

            # find or create the textures object
            texopts = util.dict_subset(render, [""texturepath""])

            tex = textures.Textures(**texopts)
            f = tex.find_file(""assets/minecraft/textures/block/sandstone_top.png"", verbose=True)
            f = tex.find_file(""assets/minecraft/textures/block/grass_block_top.png"", verbose=True)
            f = tex.find_file(""assets/minecraft/textures/block/diamond_ore.png"", verbose=True)
            f = tex.find_file(""assets/minecraft/textures/block/oak_planks.png"", verbose=True)
        return 0

    ############################################################
    # Final validation steps and creation of the destination directory
    logging.info(""Welcome to Minecraft Overviewer version %s (%s)!"" % (util.findGitVersion(), util.findGitHash()[:7]))
    logging.debug(""Current log level: {0}."".format(logging.getLogger().level))

    def set_renderchecks(checkname, num):
        for name, render in config['renders'].items():
            if render.get('renderchecks', 0) == 3:
                logging.warning(checkname + "" ignoring render "" + repr(name) + "" since it's ""
                                ""marked as \""don't render\""."")
            else:
                render['renderchecks'] = num

    if args.forcerender:
        logging.info(""Forcerender mode activated. ALL tiles will be rendered."")
        set_renderchecks(""forcerender"", 2)
    elif args.checktiles:
        logging.info(""Checking all tiles for updates manually."")
        set_renderchecks(""checktiles"", 1)
    elif args.notilechecks:
        logging.info(""Disabling all tile mtime checks. Only rendering tiles ""
                     ""that need updating since last render."")
        set_renderchecks(""notilechecks"", 0)

    if not config['renders']:
        logging.error(""You must specify at least one render in your config file. Check the ""
                      ""documentation at http://docs.overviewer.org if you're having trouble."")
        return 1

    #####################
    # Do a few last minute things to each render dictionary here
    for rname, render in config['renders'].items():
        # Convert render['world'] to the world path, and store the original
        # in render['worldname_orig']
        try:
            worldpath = config['worlds'][render['world']]
        except KeyError:
            logging.error(""Render %s's world is '%s', but I could not find a corresponding entry ""
                          ""in the worlds dictionary."", rname, render['world'])
            return 1
        render['worldname_orig'] = render['world']
        render['world'] = worldpath

        # If 'forcerender' is set, change renderchecks to 2
        if render.get('forcerender', False):
            render['renderchecks'] = 2

        # check if overlays are set, if so, make sure that those renders exist
        if render.get('overlay', []) != []:
            for x in render.get('overlay'):
                if x != rname:
                    try:
                        renderLink = config['renders'][x]
                    except KeyError:
                        logging.error(""Render %s's overlay is '%s', but I could not find a ""
                                      ""corresponding entry in the renders dictionary."", rname, x)
                        return 1
                else:
                    logging.error(""Render %s's overlay contains itself."", rname)
                    return 1

    destdir = config['outputdir']
    if not destdir:
        logging.error(""You must specify the output directory in your config file."")
        logging.error(""e.g. outputdir = '/path/to/outputdir'"")
        return 1
    if not os.path.exists(destdir):
        try:
            os.mkdir(destdir)
        except OSError:
            logging.exception(""Could not create the output directory."")
            return 1

    ########################################################################
    # Now we start the actual processing, now that all the configuration has
    # been gathered and validated
    # create our asset manager... ASSMAN
    assetMrg = assetmanager.AssetManager(destdir, config.get('customwebassets', None))

    # If we've been asked to update web assets, do that and then exit
    if args.update_web_assets:
        assetMrg.output_noconfig()
        logging.info(""Web assets have been updated."")
        return 0

    # The changelist support.
    changelists = {}
    for render in config['renders'].values():
        if 'changelist' in render:
            path = render['changelist']
            if path not in changelists:
                out = open(path, ""w"")
                logging.debug(""Opening changelist %s (%s)."", out, out.fileno())
                changelists[path] = out
            else:
                out = changelists[path]
            render['changelist'] = out.fileno()

    tilesets = []

    # saves us from creating the same World object over and over again
    worldcache = {}
    # same for textures
    texcache = {}

    # Set up the cache objects to use
    caches = []
    caches.append(cache.LRUCache(size=100))
    # TODO: optionally more caching layers here

    renders = config['renders']
    for render_name, render in renders.items():
        logging.debug(""Found the following render thing: %r"", render)

        # find or create the world object
        try:
            w = worldcache[render['world']]
        except KeyError:
            try:
                w = world.World(render['world'])
            except CorruptNBTError as e:
                logging.error(""Failed to open world %r."", render['world'])
                raise e
            except world.UnsupportedVersion as e:
                for ln in str(e).split('\n'):
                    logging.error(ln)
                sys.exit(1)

            worldcache[render['world']] = w

        # find or create the textures object
        texopts = util.dict_subset(render, [""texturepath"", ""bgcolor"", ""northdirection""])
        texopts_key = tuple(texopts.items())
        if texopts_key not in texcache:
            tex = textures.Textures(**texopts)
            logging.info(""Generating textures..."")
            tex.generate()
            logging.debug(""Finished generating textures."")
            texcache[texopts_key] = tex
        else:
            tex = texcache[texopts_key]

        try:
            logging.debug(""Asking for regionset %r."" % render['dimension'][1])
            rset = w.get_regionset(render['dimension'][1])
        except IndexError:
            logging.error(""Sorry, I can't find anything to render!  Are you sure there are .mca ""
                          ""files in the world directory of %s?"" % render['world'])
            return 1
        if rset is None:    # indicates no such dimension was found
            logging.warning(""Sorry, you requested dimension '%s' for %s, but I couldn't find it."",
                         render['dimension'][0], render_name)
            continue

        #################
        # Apply any regionset transformations here

        # Insert a layer of caching above the real regionset. Any world
        # tranformations will pull from this cache, but their results will not
        # be cached by this layer. This uses a common pool of caches; each
        # regionset cache pulls from the same underlying cache object.
        rset = world.CachedRegionSet(rset, caches)

        # If a crop is requested, wrap the regionset here
        if ""crop"" in render:
            rsets = []
            for zone in render['crop']:
                rsets.append(world.CroppedRegionSet(rset, *zone))
        else:
            rsets = [rset]

        # If this is to be a rotated regionset, wrap it in a RotatedRegionSet
        # object
        if (render['northdirection'] > 0):
            newrsets = []
            for r in rsets:
                r = world.RotatedRegionSet(r, render['northdirection'])
                newrsets.append(r)
            rsets = newrsets

        ###############################
        # Do the final prep and create the TileSet object

        # create our TileSet from this RegionSet
        tileset_dir = os.path.abspath(os.path.join(destdir, render_name))

        # only pass to the TileSet the options it really cares about
        render['name'] = render_name    # perhaps a hack. This is stored here for the asset manager
        tileSetOpts = util.dict_subset(render, [
            ""name"", ""imgformat"", ""renderchecks"", ""rerenderprob"", ""bgcolor"", ""defaultzoom"",
            ""imgquality"", ""imglossless"", ""optimizeimg"", ""rendermode"", ""worldname_orig"", ""title"",
            ""dimension"", ""changelist"", ""showspawn"", ""overlay"", ""base"", ""poititle"", ""maxzoom"",
            ""showlocationmarker"", ""minzoom"", ""center""])
        tileSetOpts.update({""spawn"": w.find_true_spawn()})  # TODO find a better way to do this
        for rset in rsets:
            tset = tileset.TileSet(w, rset, assetMrg, tex, tileSetOpts, tileset_dir)
            tilesets.append(tset)

    # If none of the requested dimenstions exist, tilesets will be empty
    if not tilesets:
        logging.error(""There are no tilesets to render! There's nothing to do, so exiting."")
        return 1

    # Do tileset preprocessing here, before we start dispatching jobs
    logging.info(""Preprocessing..."")
    for ts in tilesets:
        ts.do_preprocessing()

    # Output initial static data and configuration
    assetMrg.initialize(tilesets)

    # multiprocessing dispatcher
    if config['processes'] == 1:
        dispatch = dispatcher.Dispatcher()
    else:
        dispatch = dispatcher.MultiprocessingDispatcher(
            local_procs=config['processes'])
    dispatch.render_all(tilesets, config['observer'])
    dispatch.close()

    assetMrg.finalize(tilesets)

    for out in changelists.values():
        logging.debug(""Closing %s (%s)."", out, out.fileno())
        out.close()

    if config['processes'] == 1:
        logging.debug(""Final cache stats:"")
        for c in caches:
            logging.debug(""\t%s: %s hits, %s misses"", c.__class__.__name__, c.hits, c.misses)
    if args.pid:
        os.remove(args.pid)

    logging.info(""Your render has been written to '%s', open index.html to view it."" % destdir)

    return 0","for zone in render['crop']:
    rsets.append(world.CroppedRegionSet(rset, *zone))","rsets = [world.CroppedRegionSet(rset, *zone) for zone in render['crop']]","rsets = [world.CroppedRegionSet(rset, *zone) for zone in render['crop']]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
graph-rcnn.pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/graph-rcnn.pytorch/lib/data/samplers/grouped_batch_sampler.py,https://github.com/jwyang/graph-rcnn.pytorch/tree/master/lib/data/samplers/grouped_batch_sampler.py,GroupedBatchSampler,_prepare_batches$40,"def _prepare_batches(self):
        dataset_size = len(self.group_ids)
        # get the sampled indices from the sampler
        sampled_ids = torch.as_tensor(list(self.sampler))
        # potentially not all elements of the dataset were sampled
        # by the sampler (e.g., DistributedSampler).
        # construct a tensor which contains -1 if the element was
        # not sampled, and a non-negative number indicating the
        # order where the element was sampled.
        # for example. if sampled_ids = [3, 1] and dataset_size = 5,
        # the order is [-1, 1, -1, 0, -1]
        order = torch.full((dataset_size,), -1, dtype=torch.int64)
        order[sampled_ids] = torch.arange(len(sampled_ids))

        # get a mask with the elements that were sampled
        mask = order >= 0

        # find the elements that belong to each individual cluster
        clusters = [(self.group_ids == i) & mask for i in self.groups]
        # get relative order of the elements inside each cluster
        # that follows the order from the sampler
        relative_order = [order[cluster] for cluster in clusters]
        # with the relative order, find the absolute order in the
        # sampled space
        permutation_ids = [s[s.sort()[1]] for s in relative_order]
        # permute each cluster so that they follow the order from
        # the sampler
        permuted_clusters = [sampled_ids[idx] for idx in permutation_ids]

        # splits each cluster in batch_size, and merge as a list of tensors
        splits = [c.split(self.batch_size) for c in permuted_clusters]
        merged = tuple(itertools.chain.from_iterable(splits))

        # now each batch internally has the right order, but
        # they are grouped by clusters. Find the permutation between
        # different batches that brings them as close as possible to
        # the order that we have in the sampler. For that, we will consider the
        # ordering as coming from the first element of each batch, and sort
        # correspondingly
        first_element_of_batch = [t[0].item() for t in merged]
        # get and inverse mapping from sampled indices and the position where
        # they occur (as returned by the sampler)
        inv_sampled_ids_map = {v: k for k, v in enumerate(sampled_ids.tolist())}
        # from the first element in each batch, get a relative ordering
        first_index_of_batch = torch.as_tensor(
            [inv_sampled_ids_map[s] for s in first_element_of_batch]
        )

        # permute the batches so that they approximately follow the order
        # from the sampler
        permutation_order = first_index_of_batch.sort(0)[1].tolist()
        # finally, permute the batches
        batches = [merged[i].tolist() for i in permutation_order]

        if self.drop_uneven:
            kept = []
            for batch in batches:
                if len(batch) == self.batch_size:
                    kept.append(batch)
            batches = kept
        return batches","for batch in batches:
    if len(batch) == self.batch_size:
        kept.append(batch)",kept = [batch for batch in batches if len(batch) == self.batch_size],kept = [batch for batch in batches if len(batch) == self.batch_size],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
angr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/angr/angr/analyses/reassembler.py,https://github.com/angr/angr/tree/master/angr/analyses/reassembler.py,Reassembler,remove_cgc_attachments$2113,"def remove_cgc_attachments(self):
        """"""
        Remove CGC attachments.

        :return: True if CGC attachments are found and removed, False otherwise
        :rtype: bool
        """"""

        cgc_package_list = None
        cgc_extended_application = None

        for data in self.data:
            if data.sort == 'cgc-package-list':
                cgc_package_list = data
            elif data.sort == 'cgc-extended-application':
                cgc_extended_application = data

        if not cgc_package_list or not cgc_extended_application:
            return False

        if cgc_package_list.skip or cgc_extended_application.skip:
            # they have already been removed
            # so we still return True to indicate that CGC attachments have been removed
            return True

        # there is a single function referencing them
        cgcpl_memory_data = self.cfg.memory_data.get(cgc_package_list.addr, None)
        cgcea_memory_data = self.cfg.memory_data.get(cgc_extended_application.addr, None)
        refs = self.cfg.kb.xrefs

        if cgcpl_memory_data is None or cgcea_memory_data is None:
            return False

        if len(refs.get_xrefs_by_dst(cgcpl_memory_data.addr)) != 1:
            return False
        if len(refs.get_xrefs_by_dst(cgcea_memory_data.addr)) != 1:
            return False

        # check if the irsb addresses are the same
        if next(iter(refs.get_xrefs_by_dst(cgcpl_memory_data.addr))).block_addr != \
                next(iter(refs.get_xrefs_by_dst(cgcea_memory_data.addr))).block_addr:
            return False

        insn_addr = next(iter(refs.get_xrefs_by_dst(cgcpl_memory_data.addr))).ins_addr
        # get the basic block
        cfg_node = self.cfg.model.get_any_node(insn_addr, anyaddr=True)
        if not cfg_node:
            return False

        func_addr = cfg_node.function_address

        # this function should be calling another function
        sub_func_addr = None
        if func_addr not in self.cfg.functions:
            return False
        function = self.cfg.functions[func_addr]
        # traverse the graph and make sure there is only one call edge
        calling_targets = [ ]
        for _, dst, data in function.transition_graph.edges(data=True):
            if 'type' in data and data['type'] == 'call':
                calling_targets.append(dst.addr)

        if len(calling_targets) != 1:
            return False

        sub_func_addr = calling_targets[0]

        # alright. We want to nop this function, as well as the subfunction
        proc = next((p for p in self.procedures if p.addr == func_addr), None)
        if proc is None:
            return False

        subproc = next((p for p in self.procedures if p.addr == sub_func_addr), None)
        if subproc is None:
            return False

        # if those two data entries have any label, we should properly modify them
        # at this point, we are fairly confident that none of those labels are direct data references to either package
        # list or extended application
        has_label = True
        lowest_address = min(cgc_package_list.addr, cgc_extended_application.addr)
        for obj in (cgc_package_list, cgc_extended_application):
            labels = obj.labels
            for addr, label in labels:
                if addr != lowest_address:
                    label.base_addr = lowest_address

        if has_label:
            # is there any memory data entry that ends right at the lowest address?
            data = next((d for d in self.data if d.addr is not None and d.addr + d.size == lowest_address), None)
            if data is None:
                # since there is no gap between memory data entries (we guarantee that), this can only be that no other
                # data resides in the same memory region that CGC attachments are in
                pass
            else:
                lbl = self.symbol_manager.addr_to_label[lowest_address][0]
                if lbl not in data.end_labels:
                    data.end_labels.append(lbl)

        # practically nop the function
        proc.asm_code = ""\tret\n""
        subproc.asm_code = ""\tret\n""

        # remove those two data entries
        cgc_package_list.skip = True
        cgc_extended_application.skip = True

        l.info('CGC attachments are removed.')

        return True","for (_, dst, data) in function.transition_graph.edges(data=True):
    if 'type' in data and data['type'] == 'call':
        calling_targets.append(dst.addr)","calling_targets = [data['dst'].addr for (_, data) in function.transition_graph.edges(data=True) if 'type' in data and data['type'] == 'call']","calling_targets = [dst.addr for (_, dst, data) in function.transition_graph.edges(data=True) if 'type' in data and data['type'] == 'call']",0,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
lc-all-solutions,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/lc-all-solutions/366.find-leaves-of-binary-tree/find-leaves-of-binary-tree.py,https://github.com/csujedihy/lc-all-solutions/tree/master/366.find-leaves-of-binary-tree/find-leaves-of-binary-tree.py,Solution,findLeaves$11,"def findLeaves(self, root):
    """"""
    :type root: TreeNode
    :rtype: List[List[int]]
    """"""

    def helper(p, res):
      if not p:
        return 0
      left = helper(p.left, res)
      right = helper(p.right, res)
      depth = max(left, right) + 1
      res[depth].append(p.val)
      return depth

    ans = []
    res = collections.defaultdict(list)
    helper(root, res)
    for i in range(1, len(res) + 1):
      ans.append(res[i])
    return ans","for i in range(1, len(res) + 1):
    ans.append(res[i])","ans = [res[i] for i in range(1, len(res) + 1)]","ans = [res[i] for i in range(1, len(res) + 1)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
bcloud,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bcloud/bcloud/IconWindow.py,https://github.com/XuShaohua/bcloud/tree/master/bcloud/IconWindow.py,IconWindow,on_trash_activated$652,"def on_trash_activated(self, menu_item):
        tree_paths = self.iconview.get_selected_items()
        if not tree_paths:
            return
        path_list = []
        for tree_path in tree_paths:
            path_list.append(self.liststore[tree_path][PATH_COL])
        gutil.async_call(pcs.delete_files, self.app.cookie, self.app.tokens,
                         path_list, callback=self.parent.reload)
        self.app.blink_page(self.app.trash_page)
        self.app.trash_page.reload()","for tree_path in tree_paths:
    path_list.append(self.liststore[tree_path][PATH_COL])",path_list = [self.liststore[tree_path][PATH_COL] for tree_path in tree_paths],path_list = [self.liststore[tree_path][PATH_COL] for tree_path in tree_paths],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
flexx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flexx/flexx/ui/layouts/_hv.py,https://github.com/flexxui/flexx/tree/master/flexx/ui/layouts/_hv.py,HVLayout,set_from_flex_values$327,"def set_from_flex_values(self):
        """""" Set the divider positions corresponding to the children's flex values.
        Only has a visual effect in split-mode.
        """"""
        # Note that we still use it for fix mode to initialize it, and in box
        # mode to set splitter_positions prop, for consistency.

        # Collect flexes
        sizes = []
        dim = 0 if 'h' in self.orientation else 1
        for widget in self.children:
            sizes.append(widget.flex[dim])

        # Normalize size, so that total is one
        size_sum = 0 if len(sizes) == 0 else sum(sizes)
        if size_sum == 0:
            # Convenience: all zeros probably means to divide equally
            sizes = [1/len(sizes) for i in sizes]
        else:
            sizes = [i/size_sum for i in sizes]

        # Turn sizes into positions
        positions = []
        pos = 0
        for i in range(len(sizes) - 1):
            pos = pos + sizes[i]
            positions.append(pos)

        # Apply
        self._mutate_splitter_positions(positions)","for widget in self.children:
    sizes.append(widget.flex[dim])",sizes = [widget.flex[dim] for widget in self.children],sizes = [widget.flex[dim] for widget in self.children],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
coding-interview-gym,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coding-interview-gym/leetcode.com/python/448_Find_All_Numbers_Disappeared_in_an_Array.py,https://github.com/partho-maple/coding-interview-gym/tree/master/leetcode.com/python/448_Find_All_Numbers_Disappeared_in_an_Array.py,Solution,findDisappearedNumbers$2,"def findDisappearedNumbers(self, nums):
        """"""
        :type nums: List[int]
        :rtype: List[int]
        """"""
        currentIndex, length = 0, len(nums)
        missingNumbers = []
        while currentIndex < length:
            finalIndex = nums[currentIndex] - 1
            if nums[currentIndex] != nums[finalIndex]:
                nums[currentIndex], nums[finalIndex] = nums[finalIndex], nums[currentIndex]
            else:
                currentIndex += 1
        for index in range(length):     # find the first number missing from its index, that will be our required number
            if nums[index] != index + 1:
                missingNumbers.append(index + 1)
        return missingNumbers","for index in range(length):
    if nums[index] != index + 1:
        missingNumbers.append(index + 1)",missingNumbers = [index + 1 for index in range(length) if nums[index] != index + 1],missingNumbers = [index + 1 for index in range(length) if nums[index] != index + 1],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
pybossa,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pybossa/test/test_jobs/test_leaderboard_jobs.py,https://github.com/Scifabric/pybossa/tree/master/test/test_jobs/test_leaderboard_jobs.py,TestLeaderboard,test_leaderboard_foo_dash_key_current_user$166,"def test_leaderboard_foo_dash_key_current_user(self):
        """"""Test JOB leaderboard returns users for foo-dash key with current user.""""""
        users = []
        for score in range(1, 11):
            users.append(UserFactory.create(info={'foo-dash': score}))

        users.append(UserFactory.create(restrict=True, info={'foo-dash': 11}))

        leaderboard(info='foo-dash')
        top_users = get_leaderboard(user_id=users[0].id, info='foo-dash')
        assert len(top_users) == 11, len(top_users)
        score = 10
        for user in top_users[0:10]:
            user['score'] == score, user
            score = score - 1
        assert top_users[-1]['name'] == users[0].name
        assert top_users[-1]['score'] == users[0].info.get('foo-dash')

        results = db.session.execute('select * from ""users_rank_foo-dash""');
        for r in results:
            assert r.restrict is False, r","for score in range(1, 11):
    users.append(UserFactory.create(info={'foo-dash': score}))","users = [UserFactory.create(info={'foo-dash': score}) for score in range(1, 11)]","users = [UserFactory.create(info={'foo-dash': score}) for score in range(1, 11)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
sdc,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/sdc/sdc/tests/test_tbb_hashmap.py,https://github.com/IntelPython/sdc/tree/master/sdc/tests/test_tbb_hashmap.py,TestHashmapGeneric,test_hashmap_generic_values$1000,"def test_hashmap_generic_values(self):

        @self.jit
        def test_impl(keys, values):
            a_dict = ConcurrentDict.fromkeys(keys, values[0])
            for k, v in zip(keys, values):
                a_dict[k] = v

            res = []
            for k, v in a_dict.items():
                res.append((k, v))
            return res

        n = 47
        np.random.seed(0)

        for key_type, value_type in self.key_value_combinations():
            keys = self.get_random_sequence(key_type, n)
            values = self.get_random_sequence(value_type, n)
            source_kv_pairs = list(zip(keys, values))
            with self.subTest(key_type=key_type, value_type=value_type, keys=keys, values=values):
                result = test_impl(keys, values)
                assert_dict_correct(self, dict(result), source_kv_pairs)","for (k, v) in a_dict.items():
    res.append((k, v))","res = [(k, v) for (k, v) in a_dict.items()]","res = [(k, v) for (k, v) in a_dict.items()]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
package_control,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/package_control/dev/deps.py,https://github.com/wbond/package_control/tree/master/dev/deps.py,,_pep425tags$813,"def _pep425tags():
    """"""
    :return:
        A list of 3-element tuples with unicode strings or None:
         [0] implementation tag - cp33, pp27, cp26, py2, py2.py3
         [1] abi tag - cp26m, None
         [2] arch tag - linux_x86_64, macosx_10_10_x85_64, etc
    """"""

    tags = []

    versions = []
    version_info = _pep425_version()
    major = version_info[:-1]
    for minor in range(version_info[-1], -1, -1):
        versions.append(''.join(map(str, major + (minor,))))

    impl = _pep425_implementation()

    abis = []
    abi = _pep425_get_abi()
    if abi:
        abis.append(abi)
    abi3 = _pep425_implementation() == 'cp' and sys.version_info >= (3,)
    if abi3:
        abis.append('abi3')
    abis.append('none')

    if sys.platform == 'darwin':
        plat_ver = platform.mac_ver()
        ver_parts = plat_ver[0].split('.')
        minor = int(ver_parts[1])
        arch = plat_ver[2]
        if sys.maxsize == 2147483647:
            arch = 'i386'
        arches = []
        while minor > 5:
            arches.append('macosx_10_%s_%s' % (minor, arch))
            arches.append('macosx_10_%s_intel' % (minor,))
            arches.append('macosx_10_%s_universal' % (minor,))
            minor -= 1
    else:
        if sys.platform == 'win32':
            if 'amd64' in sys.version.lower():
                arches = ['win_amd64']
            else:
                arches = [sys.platform]
        elif hasattr(os, 'uname'):
            (plat, _, _, _, machine) = os.uname()
            plat = plat.lower().replace('/', '')
            machine.replace(' ', '_').replace('/', '_')
            if plat == 'linux' and sys.maxsize == 2147483647 and 'arm' not in machine:
                machine = 'i686'
            arch = '%s_%s' % (plat, machine)
            if _pep425_supports_manylinux():
                arches = [arch.replace('linux', 'manylinux1'), arch]
            else:
                arches = [arch]

    for abi in abis:
        for arch in arches:
            tags.append(('%s%s' % (impl, versions[0]), abi, arch))

    if abi3:
        for version in versions[1:]:
            for arch in arches:
                tags.append(('%s%s' % (impl, version), 'abi3', arch))

    for arch in arches:
        tags.append(('py%s' % (versions[0][0]), 'none', arch))

    tags.append(('%s%s' % (impl, versions[0]), 'none', 'any'))
    tags.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))

    for i, version in enumerate(versions):
        tags.append(('py%s' % (version,), 'none', 'any'))
        if i == 0:
            tags.append(('py%s' % (version[0]), 'none', 'any'))

    tags.append(('py2.py3', 'none', 'any'))

    return tags","for minor in range(version_info[-1], -1, -1):
    versions.append(''.join(map(str, major + (minor,))))","versions = [''.join(map(str, major + (minor,))) for minor in range(version_info[-1], -1, -1)]","versions = [''.join(map(str, major + (minor,))) for minor in range(version_info[-1], -1, -1)]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
package_control,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/package_control/dev/deps.py,https://github.com/wbond/package_control/tree/master/dev/deps.py,,_pep425tags$813,"def _pep425tags():
    """"""
    :return:
        A list of 3-element tuples with unicode strings or None:
         [0] implementation tag - cp33, pp27, cp26, py2, py2.py3
         [1] abi tag - cp26m, None
         [2] arch tag - linux_x86_64, macosx_10_10_x85_64, etc
    """"""

    tags = []

    versions = []
    version_info = _pep425_version()
    major = version_info[:-1]
    for minor in range(version_info[-1], -1, -1):
        versions.append(''.join(map(str, major + (minor,))))

    impl = _pep425_implementation()

    abis = []
    abi = _pep425_get_abi()
    if abi:
        abis.append(abi)
    abi3 = _pep425_implementation() == 'cp' and sys.version_info >= (3,)
    if abi3:
        abis.append('abi3')
    abis.append('none')

    if sys.platform == 'darwin':
        plat_ver = platform.mac_ver()
        ver_parts = plat_ver[0].split('.')
        minor = int(ver_parts[1])
        arch = plat_ver[2]
        if sys.maxsize == 2147483647:
            arch = 'i386'
        arches = []
        while minor > 5:
            arches.append('macosx_10_%s_%s' % (minor, arch))
            arches.append('macosx_10_%s_intel' % (minor,))
            arches.append('macosx_10_%s_universal' % (minor,))
            minor -= 1
    else:
        if sys.platform == 'win32':
            if 'amd64' in sys.version.lower():
                arches = ['win_amd64']
            else:
                arches = [sys.platform]
        elif hasattr(os, 'uname'):
            (plat, _, _, _, machine) = os.uname()
            plat = plat.lower().replace('/', '')
            machine.replace(' ', '_').replace('/', '_')
            if plat == 'linux' and sys.maxsize == 2147483647 and 'arm' not in machine:
                machine = 'i686'
            arch = '%s_%s' % (plat, machine)
            if _pep425_supports_manylinux():
                arches = [arch.replace('linux', 'manylinux1'), arch]
            else:
                arches = [arch]

    for abi in abis:
        for arch in arches:
            tags.append(('%s%s' % (impl, versions[0]), abi, arch))

    if abi3:
        for version in versions[1:]:
            for arch in arches:
                tags.append(('%s%s' % (impl, version), 'abi3', arch))

    for arch in arches:
        tags.append(('py%s' % (versions[0][0]), 'none', arch))

    tags.append(('%s%s' % (impl, versions[0]), 'none', 'any'))
    tags.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))

    for i, version in enumerate(versions):
        tags.append(('py%s' % (version,), 'none', 'any'))
        if i == 0:
            tags.append(('py%s' % (version[0]), 'none', 'any'))

    tags.append(('py2.py3', 'none', 'any'))

    return tags","for abi in abis:
    for arch in arches:
        tags.append(('%s%s' % (impl, versions[0]), abi, arch))","tags = [('%s%s' % (impl, versions[0]), abi, arch) for abi in abis for arch in arches]","tags = [('%s%s' % (impl, versions[0]), abi, arch) for abi in abis for arch in arches]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
WireViz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/WireViz/src/wireviz/wv_bom.py,https://github.com/formatc1702/WireViz/tree/master/src/wireviz/wv_bom.py,,get_additional_component_bom$47,"def get_additional_component_bom(component: Union[Connector, Cable]) -> List[BOMEntry]:
    """"""Return a list of BOM entries with additional components.""""""
    bom_entries = []
    for part in component.additional_components:
        bom_entries.append({
            'description': part.description,
            'qty': part.qty * component.get_qty_multiplier(part.qty_multiplier),
            'unit': part.unit,
            'designators': component.name if component.show_name else None,
            **optional_fields(part),
        })
    return bom_entries","for part in component.additional_components:
    bom_entries.append({'description': part.description, 'qty': part.qty * component.get_qty_multiplier(part.qty_multiplier), 'unit': part.unit, 'designators': component.name if component.show_name else None, **optional_fields(part)})","bom_entries = [{'description': part.description, 'qty': part.qty * component.get_qty_multiplier(part.qty_multiplier), 'unit': part.unit, 'designators': component.name if component.show_name else None, **optional_fields(part)} for part in component.additional_components]","bom_entries = [{'description': part.description, 'qty': part.qty * component.get_qty_multiplier(part.qty_multiplier), 'unit': part.unit, 'designators': component.name if component.show_name else None, **optional_fields(part)} for part in component.additional_components]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
pycorrector,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycorrector/pycorrector/macbert/preprocess.py,https://github.com/shibing624/pycorrector/tree/master/pycorrector/macbert/preprocess.py,,proc_item$33,"def proc_item(item):
    """"""
    
    Args:
        item:
    Returns:
        list
    """"""
    root = etree.XML(item)
    passages = dict()
    mistakes = []
    for passage in root.xpath('/ESSAY/TEXT/PASSAGE'):
        passages[passage.get('id')] = traditional2simplified(passage.text)
    for mistake in root.xpath('/ESSAY/MISTAKE'):
        mistakes.append({'id': mistake.get('id'),
                         'location': int(mistake.get('location')) - 1,
                         'wrong': traditional2simplified(mistake.xpath('./WRONG/text()')[0].strip()),
                         'correction': traditional2simplified(mistake.xpath('./CORRECTION/text()')[0].strip())})

    rst_items = dict()

    def get_passages_by_id(pgs, _id):
        p = pgs.get(_id)
        if p:
            return p
        _id = _id[:-1] + str(int(_id[-1]) + 1)
        p = pgs.get(_id)
        if p:
            return p
        raise ValueError(f'passage not found by {_id}')

    for mistake in mistakes:
        if mistake['id'] not in rst_items.keys():
            rst_items[mistake['id']] = {'original_text': get_passages_by_id(passages, mistake['id']),
                                        'wrong_ids': [],
                                        'correct_text': get_passages_by_id(passages, mistake['id'])}

        # todo 
        ori_text = rst_items[mistake['id']]['original_text']
        cor_text = rst_items[mistake['id']]['correct_text']
        if len(ori_text) == len(cor_text):
            if ori_text[mistake['location']] in mistake['wrong']:
                rst_items[mistake['id']]['wrong_ids'].append(mistake['location'])
                wrong_char_idx = mistake['wrong'].index(ori_text[mistake['location']])
                start = mistake['location'] - wrong_char_idx
                end = start + len(mistake['wrong'])
                rst_items[mistake['id']][
                    'correct_text'] = f'{cor_text[:start]}{mistake[""correction""]}{cor_text[end:]}'
        else:
            print(f'error line:\n{mistake[""id""]}\n{ori_text}\n{cor_text}')
    rst = []
    for k in rst_items.keys():
        if len(rst_items[k]['correct_text']) == len(rst_items[k]['original_text']):
            rst.append({'id': k, **rst_items[k]})
        else:
            text = rst_items[k]['correct_text']
            rst.append({'id': k, 'correct_text': text, 'original_text': text, 'wrong_ids': []})
    return rst","for mistake in root.xpath('/ESSAY/MISTAKE'):
    mistakes.append({'id': mistake.get('id'), 'location': int(mistake.get('location')) - 1, 'wrong': traditional2simplified(mistake.xpath('./WRONG/text()')[0].strip()), 'correction': traditional2simplified(mistake.xpath('./CORRECTION/text()')[0].strip())})","mistakes = [{'id': mistake.get('id'), 'location': int(mistake.get('location')) - 1, 'wrong': traditional2simplified(mistake.xpath('./WRONG/text()')[0].strip()), 'correction': traditional2simplified(mistake.xpath('./CORRECTION/text()')[0].strip())} for mistake in root.xpath('/ESSAY/MISTAKE')]","mistakes = [{'id': mistake.get('id'), 'location': int(mistake.get('location')) - 1, 'wrong': traditional2simplified(mistake.xpath('./WRONG/text()')[0].strip()), 'correction': traditional2simplified(mistake.xpath('./CORRECTION/text()')[0].strip())} for mistake in root.xpath('/ESSAY/MISTAKE')]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/lib/ansible_test/_util/controller/sanity/validate-modules/validate_modules/main.py,https://github.com/ansible/ansible/tree/master/test/lib/ansible_test/_util/controller/sanity/validate-modules/validate_modules/main.py,ModuleValidator,_validate_argument_spec$1376,"def _validate_argument_spec(self, docs, spec, kwargs, context=None, last_context_spec=None):
        if not self.analyze_arg_spec:
            return

        if docs is None:
            docs = {}

        if context is None:
            context = []

        if last_context_spec is None:
            last_context_spec = kwargs

        try:
            if not context:
                add_fragments(docs, self.object_path, fragment_loader=fragment_loader, is_module=True)
        except Exception:
            # Cannot merge fragments
            return

        # Use this to access type checkers later
        module = NoArgsAnsibleModule({})

        self._validate_list_of_module_args('mutually_exclusive', last_context_spec.get('mutually_exclusive'), spec, context)
        self._validate_list_of_module_args('required_together', last_context_spec.get('required_together'), spec, context)
        self._validate_list_of_module_args('required_one_of', last_context_spec.get('required_one_of'), spec, context)
        self._validate_required_if(last_context_spec.get('required_if'), spec, context, module)
        self._validate_required_by(last_context_spec.get('required_by'), spec, context)

        provider_args = set()
        args_from_argspec = set()
        deprecated_args_from_argspec = set()
        doc_options = docs.get('options', {})
        if doc_options is None:
            doc_options = {}
        for arg, data in spec.items():
            restricted_argument_names = ('message', 'syslog_facility')
            if arg.lower() in restricted_argument_names:
                msg = ""Argument '%s' in argument_spec "" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                msg += ""must not be one of %s as it is used "" \
                       ""internally by Ansible Core Engine"" % ("","".join(restricted_argument_names))
                self.reporter.error(
                    path=self.object_path,
                    code='invalid-argument-name',
                    msg=msg,
                )
                continue
            if 'aliases' in data:
                for al in data['aliases']:
                    if al.lower() in restricted_argument_names:
                        msg = ""Argument alias '%s' in argument_spec "" % al
                        if context:
                            msg += "" found in %s"" % "" -> "".join(context)
                        msg += ""must not be one of %s as it is used "" \
                               ""internally by Ansible Core Engine"" % ("","".join(restricted_argument_names))
                        self.reporter.error(
                            path=self.object_path,
                            code='invalid-argument-name',
                            msg=msg,
                        )
                        continue

            # Could this a place where secrets are leaked?
            # If it is type: path we know it's not a secret key as it's a file path.
            # If it is type: bool it is more likely a flag indicating that something is secret, than an actual secret.
            if all((
                    data.get('no_log') is None, is_potential_secret_option(arg),
                    data.get('type') not in (""path"", ""bool""), data.get('choices') is None,
            )):
                msg = ""Argument '%s' in argument_spec could be a secret, though doesn't have `no_log` set"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                self.reporter.error(
                    path=self.object_path,
                    code='no-log-needed',
                    msg=msg,
                )

            if not isinstance(data, dict):
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                msg += "" must be a dictionary/hash when used""
                self.reporter.error(
                    path=self.object_path,
                    code='invalid-argument-spec',
                    msg=msg,
                )
                continue

            removed_at_date = data.get('removed_at_date', None)
            if removed_at_date is not None:
                try:
                    if parse_isodate(removed_at_date, allow_date=False) < datetime.date.today():
                        msg = ""Argument '%s' in argument_spec"" % arg
                        if context:
                            msg += "" found in %s"" % "" -> "".join(context)
                        msg += "" has a removed_at_date '%s' before today"" % removed_at_date
                        self.reporter.error(
                            path=self.object_path,
                            code='deprecated-date',
                            msg=msg,
                        )
                except ValueError:
                    # This should only happen when removed_at_date is not in ISO format. Since schema
                    # validation already reported this as an error, don't report it a second time.
                    pass

            deprecated_aliases = data.get('deprecated_aliases', None)
            if deprecated_aliases is not None:
                for deprecated_alias in deprecated_aliases:
                    if 'name' in deprecated_alias and 'date' in deprecated_alias:
                        try:
                            date = deprecated_alias['date']
                            if parse_isodate(date, allow_date=False) < datetime.date.today():
                                msg = ""Argument '%s' in argument_spec"" % arg
                                if context:
                                    msg += "" found in %s"" % "" -> "".join(context)
                                msg += "" has deprecated aliases '%s' with removal date '%s' before today"" % (
                                    deprecated_alias['name'], deprecated_alias['date'])
                                self.reporter.error(
                                    path=self.object_path,
                                    code='deprecated-date',
                                    msg=msg,
                                )
                        except ValueError:
                            # This should only happen when deprecated_alias['date'] is not in ISO format. Since
                            # schema validation already reported this as an error, don't report it a second
                            # time.
                            pass

            has_version = False
            if self.collection and self.collection_version is not None:
                compare_version = self.collection_version
                version_of_what = ""this collection (%s)"" % self.collection_version_str
                code_prefix = 'collection'
                has_version = True
            elif not self.collection:
                compare_version = LOOSE_ANSIBLE_VERSION
                version_of_what = ""Ansible (%s)"" % ansible_version
                code_prefix = 'ansible'
                has_version = True

            removed_in_version = data.get('removed_in_version', None)
            if removed_in_version is not None:
                try:
                    collection_name = data.get('removed_from_collection')
                    removed_in = self._create_version(str(removed_in_version), collection_name=collection_name)
                    if has_version and collection_name == self.collection_name and compare_version >= removed_in:
                        msg = ""Argument '%s' in argument_spec"" % arg
                        if context:
                            msg += "" found in %s"" % "" -> "".join(context)
                        msg += "" has a deprecated removed_in_version %r,"" % removed_in_version
                        msg += "" i.e. the version is less than or equal to the current version of %s"" % version_of_what
                        self.reporter.error(
                            path=self.object_path,
                            code=code_prefix + '-deprecated-version',
                            msg=msg,
                        )
                except ValueError as e:
                    msg = ""Argument '%s' in argument_spec"" % arg
                    if context:
                        msg += "" found in %s"" % "" -> "".join(context)
                    msg += "" has an invalid removed_in_version number %r: %s"" % (removed_in_version, e)
                    self.reporter.error(
                        path=self.object_path,
                        code='invalid-deprecated-version',
                        msg=msg,
                    )
                except TypeError:
                    msg = ""Argument '%s' in argument_spec"" % arg
                    if context:
                        msg += "" found in %s"" % "" -> "".join(context)
                    msg += "" has an invalid removed_in_version number %r: "" % (removed_in_version, )
                    msg += "" error while comparing to version of %s"" % version_of_what
                    self.reporter.error(
                        path=self.object_path,
                        code='invalid-deprecated-version',
                        msg=msg,
                    )

            if deprecated_aliases is not None:
                for deprecated_alias in deprecated_aliases:
                    if 'name' in deprecated_alias and 'version' in deprecated_alias:
                        try:
                            collection_name = deprecated_alias.get('collection_name')
                            version = self._create_version(str(deprecated_alias['version']), collection_name=collection_name)
                            if has_version and collection_name == self.collection_name and compare_version >= version:
                                msg = ""Argument '%s' in argument_spec"" % arg
                                if context:
                                    msg += "" found in %s"" % "" -> "".join(context)
                                msg += "" has deprecated aliases '%s' with removal in version %r,"" % (
                                    deprecated_alias['name'], deprecated_alias['version'])
                                msg += "" i.e. the version is less than or equal to the current version of %s"" % version_of_what
                                self.reporter.error(
                                    path=self.object_path,
                                    code=code_prefix + '-deprecated-version',
                                    msg=msg,
                                )
                        except ValueError as e:
                            msg = ""Argument '%s' in argument_spec"" % arg
                            if context:
                                msg += "" found in %s"" % "" -> "".join(context)
                            msg += "" has deprecated aliases '%s' with invalid removal version %r: %s"" % (
                                deprecated_alias['name'], deprecated_alias['version'], e)
                            self.reporter.error(
                                path=self.object_path,
                                code='invalid-deprecated-version',
                                msg=msg,
                            )
                        except TypeError:
                            msg = ""Argument '%s' in argument_spec"" % arg
                            if context:
                                msg += "" found in %s"" % "" -> "".join(context)
                            msg += "" has deprecated aliases '%s' with invalid removal version %r:"" % (
                                deprecated_alias['name'], deprecated_alias['version'])
                            msg += "" error while comparing to version of %s"" % version_of_what
                            self.reporter.error(
                                path=self.object_path,
                                code='invalid-deprecated-version',
                                msg=msg,
                            )

            aliases = data.get('aliases', [])
            if arg in aliases:
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                msg += "" is specified as its own alias""
                self.reporter.error(
                    path=self.object_path,
                    code='parameter-alias-self',
                    msg=msg
                )
            if len(aliases) > len(set(aliases)):
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                msg += "" has at least one alias specified multiple times in aliases""
                self.reporter.error(
                    path=self.object_path,
                    code='parameter-alias-repeated',
                    msg=msg
                )
            if not context and arg == 'state':
                bad_states = set(['list', 'info', 'get']) & set(data.get('choices', set()))
                for bad_state in bad_states:
                    self.reporter.error(
                        path=self.object_path,
                        code='parameter-state-invalid-choice',
                        msg=""Argument 'state' includes the value '%s' as a choice"" % bad_state)
            if not data.get('removed_in_version', None) and not data.get('removed_at_date', None):
                args_from_argspec.add(arg)
                args_from_argspec.update(aliases)
            else:
                deprecated_args_from_argspec.add(arg)
                deprecated_args_from_argspec.update(aliases)
            if arg == 'provider' and self.object_path.startswith('lib/ansible/modules/network/'):
                if data.get('options') is not None and not isinstance(data.get('options'), Mapping):
                    self.reporter.error(
                        path=self.object_path,
                        code='invalid-argument-spec-options',
                        msg=""Argument 'options' in argument_spec['provider'] must be a dictionary/hash when used"",
                    )
                elif data.get('options'):
                    # Record provider options from network modules, for later comparison
                    for provider_arg, provider_data in data.get('options', {}).items():
                        provider_args.add(provider_arg)
                        provider_args.update(provider_data.get('aliases', []))

            if data.get('required') and data.get('default', object) != object:
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                msg += "" is marked as required but specifies a default. Arguments with a"" \
                       "" default should not be marked as required""
                self.reporter.error(
                    path=self.object_path,
                    code='no-default-for-required-parameter',
                    msg=msg
                )

            if arg in provider_args:
                # Provider args are being removed from network module top level
                # don't validate docs<->arg_spec checks below
                continue

            _type = data.get('type', 'str')
            if callable(_type):
                _type_checker = _type
            else:
                _type_checker = DEFAULT_TYPE_VALIDATORS.get(_type)

            _elements = data.get('elements')
            if (_type == 'list') and not _elements:
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                msg += "" defines type as list but elements is not defined""
                self.reporter.error(
                    path=self.object_path,
                    code='parameter-list-no-elements',
                    msg=msg
                )
            if _elements:
                if not callable(_elements):
                    DEFAULT_TYPE_VALIDATORS.get(_elements)
                if _type != 'list':
                    msg = ""Argument '%s' in argument_spec"" % arg
                    if context:
                        msg += "" found in %s"" % "" -> "".join(context)
                    msg += "" defines elements as %s but it is valid only when value of parameter type is list"" % _elements
                    self.reporter.error(
                        path=self.object_path,
                        code='parameter-invalid-elements',
                        msg=msg
                    )

            arg_default = None
            if 'default' in data and not is_empty(data['default']):
                try:
                    with CaptureStd():
                        arg_default = _type_checker(data['default'])
                except (Exception, SystemExit):
                    msg = ""Argument '%s' in argument_spec"" % arg
                    if context:
                        msg += "" found in %s"" % "" -> "".join(context)
                    msg += "" defines default as (%r) but this is incompatible with parameter type %r"" % (data['default'], _type)
                    self.reporter.error(
                        path=self.object_path,
                        code='incompatible-default-type',
                        msg=msg
                    )
                    continue

            doc_options_args = []
            for alias in sorted(set([arg] + list(aliases))):
                if alias in doc_options:
                    doc_options_args.append(alias)
            if len(doc_options_args) == 0:
                # Undocumented arguments will be handled later (search for undocumented-parameter)
                doc_options_arg = {}
            else:
                doc_options_arg = doc_options[doc_options_args[0]]
                if len(doc_options_args) > 1:
                    msg = ""Argument '%s' in argument_spec"" % arg
                    if context:
                        msg += "" found in %s"" % "" -> "".join(context)
                    msg += "" with aliases %s is documented multiple times, namely as %s"" % (
                        "", "".join([(""'%s'"" % alias) for alias in aliases]),
                        "", "".join([(""'%s'"" % alias) for alias in doc_options_args])
                    )
                    self.reporter.error(
                        path=self.object_path,
                        code='parameter-documented-multiple-times',
                        msg=msg
                    )

            try:
                doc_default = None
                if 'default' in doc_options_arg and not is_empty(doc_options_arg['default']):
                    with CaptureStd():
                        doc_default = _type_checker(doc_options_arg['default'])
            except (Exception, SystemExit):
                msg = ""Argument '%s' in documentation"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                msg += "" defines default as (%r) but this is incompatible with parameter type %r"" % (doc_options_arg.get('default'), _type)
                self.reporter.error(
                    path=self.object_path,
                    code='doc-default-incompatible-type',
                    msg=msg
                )
                continue

            if arg_default != doc_default:
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                msg += "" defines default as (%r) but documentation defines default as (%r)"" % (arg_default, doc_default)
                self.reporter.error(
                    path=self.object_path,
                    code='doc-default-does-not-match-spec',
                    msg=msg
                )

            doc_type = doc_options_arg.get('type')
            if 'type' in data and data['type'] is not None:
                if doc_type is None:
                    if not arg.startswith('_'):  # hidden parameter, for example _raw_params
                        msg = ""Argument '%s' in argument_spec"" % arg
                        if context:
                            msg += "" found in %s"" % "" -> "".join(context)
                        msg += "" defines type as %r but documentation doesn't define type"" % (data['type'])
                        self.reporter.error(
                            path=self.object_path,
                            code='parameter-type-not-in-doc',
                            msg=msg
                        )
                elif data['type'] != doc_type:
                    msg = ""Argument '%s' in argument_spec"" % arg
                    if context:
                        msg += "" found in %s"" % "" -> "".join(context)
                    msg += "" defines type as %r but documentation defines type as %r"" % (data['type'], doc_type)
                    self.reporter.error(
                        path=self.object_path,
                        code='doc-type-does-not-match-spec',
                        msg=msg
                    )
            else:
                if doc_type is None:
                    msg = ""Argument '%s' in argument_spec"" % arg
                    if context:
                        msg += "" found in %s"" % "" -> "".join(context)
                    msg += "" uses default type ('str') but documentation doesn't define type""
                    self.reporter.error(
                        path=self.object_path,
                        code='doc-missing-type',
                        msg=msg
                    )
                elif doc_type != 'str':
                    msg = ""Argument '%s' in argument_spec"" % arg
                    if context:
                        msg += "" found in %s"" % "" -> "".join(context)
                    msg += "" implies type as 'str' but documentation defines as %r"" % doc_type
                    self.reporter.error(
                        path=self.object_path,
                        code='implied-parameter-type-mismatch',
                        msg=msg
                    )

            doc_choices = []
            try:
                for choice in doc_options_arg.get('choices', []):
                    try:
                        with CaptureStd():
                            doc_choices.append(_type_checker(choice))
                    except (Exception, SystemExit):
                        msg = ""Argument '%s' in documentation"" % arg
                        if context:
                            msg += "" found in %s"" % "" -> "".join(context)
                        msg += "" defines choices as (%r) but this is incompatible with argument type %r"" % (choice, _type)
                        self.reporter.error(
                            path=self.object_path,
                            code='doc-choices-incompatible-type',
                            msg=msg
                        )
                        raise StopIteration()
            except StopIteration:
                continue

            arg_choices = []
            try:
                for choice in data.get('choices', []):
                    try:
                        with CaptureStd():
                            arg_choices.append(_type_checker(choice))
                    except (Exception, SystemExit):
                        msg = ""Argument '%s' in argument_spec"" % arg
                        if context:
                            msg += "" found in %s"" % "" -> "".join(context)
                        msg += "" defines choices as (%r) but this is incompatible with argument type %r"" % (choice, _type)
                        self.reporter.error(
                            path=self.object_path,
                            code='incompatible-choices',
                            msg=msg
                        )
                        raise StopIteration()
            except StopIteration:
                continue

            if not compare_unordered_lists(arg_choices, doc_choices):
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                msg += "" defines choices as (%r) but documentation defines choices as (%r)"" % (arg_choices, doc_choices)
                self.reporter.error(
                    path=self.object_path,
                    code='doc-choices-do-not-match-spec',
                    msg=msg
                )

            doc_required = doc_options_arg.get('required', False)
            data_required = data.get('required', False)
            if (doc_required or data_required) and not (doc_required and data_required):
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                if doc_required:
                    msg += "" is not required, but is documented as being required""
                else:
                    msg += "" is required, but is not documented as being required""
                self.reporter.error(
                    path=self.object_path,
                    code='doc-required-mismatch',
                    msg=msg
                )

            doc_elements = doc_options_arg.get('elements', None)
            doc_type = doc_options_arg.get('type', 'str')
            data_elements = data.get('elements', None)
            if (doc_elements and not doc_type == 'list'):
                msg = ""Argument '%s' "" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                msg += "" defines parameter elements as %s but it is valid only when value of parameter type is list"" % doc_elements
                self.reporter.error(
                    path=self.object_path,
                    code='doc-elements-invalid',
                    msg=msg
                )
            if (doc_elements or data_elements) and not (doc_elements == data_elements):
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                if data_elements:
                    msg += "" specifies elements as %s,"" % data_elements
                else:
                    msg += "" does not specify elements,""
                if doc_elements:
                    msg += ""but elements is documented as being %s"" % doc_elements
                else:
                    msg += ""but elements is not documented""
                self.reporter.error(
                    path=self.object_path,
                    code='doc-elements-mismatch',
                    msg=msg
                )

            spec_suboptions = data.get('options')
            doc_suboptions = doc_options_arg.get('suboptions', {})
            if spec_suboptions:
                if not doc_suboptions:
                    msg = ""Argument '%s' in argument_spec"" % arg
                    if context:
                        msg += "" found in %s"" % "" -> "".join(context)
                    msg += "" has sub-options but documentation does not define it""
                    self.reporter.error(
                        path=self.object_path,
                        code='missing-suboption-docs',
                        msg=msg
                    )
                self._validate_argument_spec({'options': doc_suboptions}, spec_suboptions, kwargs,
                                             context=context + [arg], last_context_spec=data)

        for arg in args_from_argspec:
            if not str(arg).isidentifier():
                msg = ""Argument '%s' in argument_spec"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                msg += "" is not a valid python identifier""
                self.reporter.error(
                    path=self.object_path,
                    code='parameter-invalid',
                    msg=msg
                )

        if docs:
            args_from_docs = set()
            for arg, data in doc_options.items():
                args_from_docs.add(arg)
                args_from_docs.update(data.get('aliases', []))

            args_missing_from_docs = args_from_argspec.difference(args_from_docs)
            docs_missing_from_args = args_from_docs.difference(args_from_argspec | deprecated_args_from_argspec)
            for arg in args_missing_from_docs:
                if arg in provider_args:
                    # Provider args are being removed from network module top level
                    # So they are likely not documented on purpose
                    continue
                msg = ""Argument '%s'"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                msg += "" is listed in the argument_spec, but not documented in the module documentation""
                self.reporter.error(
                    path=self.object_path,
                    code='undocumented-parameter',
                    msg=msg
                )
            for arg in docs_missing_from_args:
                msg = ""Argument '%s'"" % arg
                if context:
                    msg += "" found in %s"" % "" -> "".join(context)
                msg += "" is listed in DOCUMENTATION.options, but not accepted by the module argument_spec""
                self.reporter.error(
                    path=self.object_path,
                    code='nonexistent-parameter-documented',
                    msg=msg
                )","for alias in sorted(set([arg] + list(aliases))):
    if alias in doc_options:
        doc_options_args.append(alias)",doc_options_args = [alias for alias in sorted(set([arg] + list(aliases))) if alias in doc_options],doc_options_args = [alias for alias in sorted(set([arg] + list(aliases))) if alias in doc_options],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
s3prl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/s3prl/s3prl/downstream/ctc/corpus/libriphone.py,https://github.com/s3prl/s3prl/tree/master/s3prl/downstream/ctc/corpus/libriphone.py,LibriPhoneDataset,__init__$35,"def __init__(self, split, tokenizer, bucket_size, path, lexicon, ascending=False, **kwargs):
        # Setup
        self.path = path
        self.bucket_size = bucket_size

        # create word -> phonemes mapping
        word2phonemes_all = defaultdict(list)
        for lexicon_file in lexicon:
            with open(lexicon_file, 'r') as file:
                lines = [line.strip() for line in file.readlines()]
                for line in lines:
                    word, phonemes = parse_lexicon(line, tokenizer)
                    word2phonemes_all[word].append(phonemes)

        # check mapping number of each word
        word2phonemes = {}
        for word, phonemes_all in word2phonemes_all.items():
            if len(phonemes_all) > 1:
                print(f'[LibriPhone] - {len(phonemes_all)} of phoneme sequences found for {word}.')
                for idx, phonemes in enumerate(phonemes_all):
                    print(f'{idx}. {phonemes}')
            word2phonemes[word] = phonemes_all[0]
        print(f'[LibriPhone] - Taking the first phoneme sequences for a deterministic behavior.')

        # List all wave files
        file_list = []
        for s in split:
            split_list = list(Path(join(path, s)).rglob(""*.flac""))
            assert len(split_list) > 0, ""No data found @ {}"".format(join(path,s))
            file_list += split_list
        
        text = []
        for f in tqdm(file_list, desc='word -> phonemes'):
            text.append(read_text(str(f), word2phonemes, tokenizer))

        self.file_list, self.text = zip(*[(f_name, txt)
                                          for f_name, txt in sorted(zip(file_list, text), reverse=not ascending, key=lambda x:len(x[1]))])","for f in tqdm(file_list, desc='word -> phonemes'):
    text.append(read_text(str(f), word2phonemes, tokenizer))","text = [read_text(str(f), word2phonemes, tokenizer) for f in tqdm(file_list, desc='word -> phonemes')]","text = [read_text(str(f), word2phonemes, tokenizer) for f in tqdm(file_list, desc='word -> phonemes')]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
mmdetection-mini,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection-mini/mmdet/cv_core/runner/hooks/logger/text.py,https://github.com/hhaAndroid/mmdetection-mini/tree/master/mmdet/cv_core/runner/hooks/logger/text.py,TextLoggerHook,_log_info$60,"def _log_info(self, log_dict, runner):
        # print exp name for users to distinguish experiments
        # at every ``interval_exp_name`` iterations and the end of each epoch
        if runner.meta is not None and 'exp_name' in runner.meta:
            if (self.every_n_iters(runner, self.interval_exp_name)) or (
                    self.by_epoch and self.end_of_epoch(runner)):
                exp_info = f'Exp name: {runner.meta[""exp_name""]}'
                runner.logger.info(exp_info)

        if log_dict['mode'] == 'train':
            if isinstance(log_dict['lr'], dict):
                lr_str = []
                for k, val in log_dict['lr'].items():
                    lr_str.append(f'lr_{k}: {val:.3e}')
                lr_str = ' '.join(lr_str)
            else:
                lr_str = f'lr: {log_dict[""lr""]:.3e}'

            # by epoch: Epoch [4][100/1000]
            # by iter:  Iter [100/100000]
            if self.by_epoch:
                log_str = f'Epoch [{log_dict[""epoch""]}]' \
                          f'[{log_dict[""iter""]}/{len(runner.data_loader)}]\t'
            else:
                log_str = f'Iter [{log_dict[""iter""]}/{runner.max_iters}]\t'
            log_str += f'{lr_str}, '

            if 'time' in log_dict.keys():
                self.time_sec_tot += (log_dict['time'] * self.interval)
                time_sec_avg = self.time_sec_tot / (
                    runner.iter - self.start_iter + 1)
                eta_sec = time_sec_avg * (runner.max_iters - runner.iter - 1)
                eta_str = str(datetime.timedelta(seconds=int(eta_sec)))
                log_str += f'eta: {eta_str}, '
                log_str += f'time: {log_dict[""time""]:.3f}, ' \
                           f'data_time: {log_dict[""data_time""]:.3f}, '
                # statistic memory
                if torch.cuda.is_available():
                    log_str += f'memory: {log_dict[""memory""]}, '
        else:
            if self.by_epoch:
                log_str = f'Epoch({log_dict[""mode""]}) ' \
                    f'[{log_dict[""epoch""]}][{log_dict[""iter""]}]\t'
            else:
                log_str = f'Iter({log_dict[""mode""]}) [{log_dict[""iter""]}]\t'

        log_items = []
        for name, val in log_dict.items():
            # TODO: resolve this hack
            # these items have been in log_str
            if name in [
                    'mode', 'Epoch', 'iter', 'lr', 'time', 'data_time',
                    'memory', 'epoch'
            ]:
                continue
            if isinstance(val, float):
                val = f'{val:.4f}'
            log_items.append(f'{name}: {val}')
        log_str += ', '.join(log_items)

        runner.logger.info(log_str)","for (k, val) in log_dict['lr'].items():
    lr_str.append(f'lr_{k}: {val:.3e}')","lr_str = [f'lr_{k}: {val:.3e}' for (k, val) in log_dict['lr'].items()]","lr_str = [f'lr_{k}: {val:.3e}' for (k, val) in log_dict['lr'].items()]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/accounts/report/financial_statements.py,https://github.com/frappe/erpnext/tree/master/erpnext/accounts/report/financial_statements.py,,get_filtered_list_for_consolidated_report$570,"def get_filtered_list_for_consolidated_report(filters, period_list):
	filtered_summary_list = []
	for period in period_list:
		if period == filters.get(""company""):
			filtered_summary_list.append(period)

	return filtered_summary_list","for period in period_list:
    if period == filters.get('company'):
        filtered_summary_list.append(period)",filtered_summary_list = [period for period in period_list if period == filters.get('company')],filtered_summary_list = [period for period in period_list if period == filters.get('company')],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
investpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/investpy/investpy/currency_crosses.py,https://github.com/alvarobartt/investpy/tree/master/investpy/currency_crosses.py,,get_currency_cross_recent_data$187,"def get_currency_cross_recent_data(
    currency_cross, as_json=False, order=""ascending"", interval=""Daily""
):
    """"""
    This function retrieves recent historical data from the introduced `currency_cross` as indexed in Investing.com
    via Web Scraping. The resulting data can it either be stored in a :obj:`pandas.DataFrame` or in a
    :obj:`json` file, with `ascending` or `descending` order.

    Args:
        currency_cross (:obj:`str`): name of the currency_cross to retrieve recent historical data from.
        as_json (:obj:`bool`, optional):
            optional argument to determine the format of the output data (:obj:`pandas.DataFrame` or :obj:`json`).
        order (:obj:`str`, optional):
            optional argument to define the order of the retrieved data (`ascending`, `asc` or `descending`, `desc`).
        interval (:obj:`str`, optional):
            value to define the historical data interval to retrieve, by default `Daily`, but it can also be `Weekly` or `Monthly`.

    Returns:
        :obj:`pandas.DataFrame` or :obj:`json`:
            The function returns a either a :obj:`pandas.DataFrame` or a :obj:`json` file containing the retrieved
            recent data from the specified currency_cross via argument. The dataset contains the open, high, low, close,
            volume and currency values for the selected currency_cross on market days.

            The return data is in case we use default arguments will look like::

                Date || Open | High | Low | Close | Currency
                -----||------|------|-----|-------|---------
                xxxx || xxxx | xxxx | xxx | xxxxx | xxxxxxxx

            but if we define `as_json=True`, then the output will be::

                {
                    name: name,
                    recent: [
                        dd/mm/yyyy: {
                            'open': x,
                            'high': x,
                            'low': x,
                            'close': x,
                            'currency' : x
                        },
                        ...
                    ]
                }

    Raises:
        ValueError: raised if any of the introduced arguments was not valid or errored.
        IOError: raised if currency_crosses object/file not found or unable to retrieve.
        RuntimeError: raised introduced currency_cross does not match any of the indexed ones.
        ConnectionError: raised if GET request did not return 200 status code.
        IndexError: raised if currency_cross information was unavailable or not found.

    Examples:
        >>> data = investpy.get_currency_cross_recent_data(currency_cross='EUR/USD')
        >>> data.head()
                      Open    High     Low   Close Currency
        Date
        2019-08-27  1.1101  1.1116  1.1084  1.1091      USD
        2019-08-28  1.1090  1.1099  1.1072  1.1078      USD
        2019-08-29  1.1078  1.1093  1.1042  1.1057      USD
        2019-08-30  1.1058  1.1062  1.0963  1.0991      USD
        2019-09-02  1.0990  1.1000  1.0958  1.0968      USD

    """"""

    if not currency_cross:
        raise ValueError(
            ""ERR#0052: currency_cross param is mandatory and should be a str.""
        )

    if not isinstance(currency_cross, str):
        raise ValueError(
            ""ERR#0052: currency_cross param is mandatory and should be a str.""
        )

    if not isinstance(as_json, bool):
        raise ValueError(
            ""ERR#0002: as_json argument can just be True or False, bool type.""
        )

    if order not in [""ascending"", ""asc"", ""descending"", ""desc""]:
        raise ValueError(
            ""ERR#0003: order argument can just be ascending (asc) or descending (desc),""
            "" str type.""
        )

    if not interval:
        raise ValueError(
            ""ERR#0073: interval value should be a str type and it can just be either""
            "" 'Daily', 'Weekly' or 'Monthly'.""
        )

    if not isinstance(interval, str):
        raise ValueError(
            ""ERR#0073: interval value should be a str type and it can just be either""
            "" 'Daily', 'Weekly' or 'Monthly'.""
        )

    interval = interval.lower()

    if interval not in [""daily"", ""weekly"", ""monthly""]:
        raise ValueError(
            ""ERR#0073: interval value should be a str type and it can just be either""
            "" 'Daily', 'Weekly' or 'Monthly'.""
        )

    resource_package = ""investpy""
    resource_path = ""/"".join((""resources"", ""currency_crosses.csv""))
    if pkg_resources.resource_exists(resource_package, resource_path):
        currency_crosses = pd.read_csv(
            pkg_resources.resource_filename(resource_package, resource_path),
            keep_default_na=False,
        )
    else:
        raise FileNotFoundError(""ERR#0060: currency_crosses file not found or errored."")

    if currency_crosses is None:
        raise IOError(""ERR#0050: currency_crosses not found or unable to retrieve."")

    currency_cross = unidecode(currency_cross.strip().lower())

    if currency_cross not in list(
        currency_crosses[""name""].apply(unidecode).str.lower()
    ):
        raise RuntimeError(
            ""ERR#0054: the introduced currency_cross ""
            + str(currency_cross)
            + "" does not exist.""
        )

    id_ = currency_crosses.loc[
        (
            currency_crosses[""name""].apply(unidecode).str.lower() == currency_cross
        ).idxmax(),
        ""id"",
    ]
    name = currency_crosses.loc[
        (
            currency_crosses[""name""].apply(unidecode).str.lower() == currency_cross
        ).idxmax(),
        ""name"",
    ]
    currency = currency_crosses.loc[
        (
            currency_crosses[""name""].apply(unidecode).str.lower() == currency_cross
        ).idxmax(),
        ""second"",
    ]

    header = name + "" Historical Data""

    params = {
        ""curr_id"": id_,
        ""smlID"": str(randint(1000000, 99999999)),
        ""header"": header,
        ""interval_sec"": interval.capitalize(),
        ""sort_col"": ""date"",
        ""sort_ord"": ""DESC"",
        ""action"": ""historical_data"",
    }

    head = {
        ""User-Agent"": random_user_agent(),
        ""X-Requested-With"": ""XMLHttpRequest"",
        ""Accept"": ""text/html"",
        ""Accept-Encoding"": ""gzip, deflate"",
        ""Connection"": ""keep-alive"",
    }

    url = ""https://www.investing.com/instruments/HistoricalDataAjax""

    req = requests.post(url, headers=head, data=params)

    if req.status_code != 200:
        raise ConnectionError(
            ""ERR#0015: error "" + str(req.status_code) + "", try again later.""
        )

    root_ = fromstring(req.text)
    path_ = root_.xpath("".//table[@id='curr_table']/tbody/tr"")
    result = list()

    if path_:
        for elements_ in path_:
            if elements_.xpath("".//td"")[0].text_content() == ""No results found"":
                raise IndexError(
                    ""ERR#0055: currency_cross information unavailable or not found.""
                )

            info = []

            for nested_ in elements_.xpath("".//td""):
                info.append(nested_.get(""data-real-value""))

            currency_cross_date = datetime.strptime(
                str(
                    datetime.fromtimestamp(int(info[0]), tz=pytz.timezone(""GMT"")).date()
                ),
                ""%Y-%m-%d"",
            )

            currency_cross_close = float(info[1].replace("","", """"))
            currency_cross_open = float(info[2].replace("","", """"))
            currency_cross_high = float(info[3].replace("","", """"))
            currency_cross_low = float(info[4].replace("","", """"))

            result.insert(
                len(result),
                Data(
                    currency_cross_date,
                    currency_cross_open,
                    currency_cross_high,
                    currency_cross_low,
                    currency_cross_close,
                    None,
                    currency,
                    None,
                ),
            )

        if order in [""ascending"", ""asc""]:
            result = result[::-1]
        elif order in [""descending"", ""desc""]:
            result = result

        if as_json is True:
            json_ = {
                ""name"": name,
                ""recent"": [value.currency_cross_as_json() for value in result],
            }

            return json.dumps(json_, sort_keys=False)
        elif as_json is False:
            df = pd.DataFrame.from_records(
                [value.currency_cross_to_dict() for value in result]
            )
            df.set_index(""Date"", inplace=True)

            return df
    else:
        raise RuntimeError(""ERR#0004: data retrieval error while scraping."")","for nested_ in elements_.xpath('.//td'):
    info.append(nested_.get('data-real-value'))",info = [nested_.get('data-real-value') for nested_ in elements_.xpath('.//td')],info = [nested_.get('data-real-value') for nested_ in elements_.xpath('.//td')],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
tf-encrypted,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tf-encrypted/tf_encrypted/convert/convert.py,https://github.com/tf-encrypted/tf-encrypted/tree/master/tf_encrypted/convert/convert.py,Converter,forward_function$213,"def forward_function(x):
            node_outputs = {}
            for index, input in enumerate(model_proto.graph.input):
                node_outputs[input.name] = x[index]
            for node in nodes:
                inputs = []
                for input in node.input:
                    if input in node_outputs.keys():
                        inputs.append(node_outputs[input])
                with tf.name_scope(node.name + ""/forward""):
                    res = tfe_nodes[node.name].forward(inputs)
                for i, output in enumerate(node.output):
                    node_outputs[output] = res[i]
            res = []
            for output in model_proto.graph.output:
                res.append(node_outputs[output.name])
            return res","for output in model_proto.graph.output:
    res.append(node_outputs[output.name])",res = [node_outputs[output.name] for output in model_proto.graph.output],res = [node_outputs[output.name] for output in model_proto.graph.output],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
tf-encrypted,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tf-encrypted/tf_encrypted/convert/convert.py,https://github.com/tf-encrypted/tf-encrypted/tree/master/tf_encrypted/convert/convert.py,Converter,forward_function$213,"def forward_function(x):
            node_outputs = {}
            for index, input in enumerate(model_proto.graph.input):
                node_outputs[input.name] = x[index]
            for node in nodes:
                inputs = []
                for input in node.input:
                    if input in node_outputs.keys():
                        inputs.append(node_outputs[input])
                with tf.name_scope(node.name + ""/forward""):
                    res = tfe_nodes[node.name].forward(inputs)
                for i, output in enumerate(node.output):
                    node_outputs[output] = res[i]
            res = []
            for output in model_proto.graph.output:
                res.append(node_outputs[output.name])
            return res","for input in node.input:
    if input in node_outputs.keys():
        inputs.append(node_outputs[input])",inputs = [node_outputs[input] for input in node.input if input in node_outputs.keys()],inputs = [node_outputs[input] for input in node.input if input in node_outputs.keys()],1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
PaddleHub,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleHub/modules/text/language_model/lda_news/module.py,https://github.com/PaddlePaddle/PaddleHub/tree/master/modules/text/language_model/lda_news/module.py,TopicModel,infer_doc_topic_distribution$150,"def infer_doc_topic_distribution(self, document):
        """"""
        This interface infers the topic distribution of document.

        Args:
            document(str): the input document text.

        Returns:
            results(list): returns the topic distribution of document.
        """"""
        tokens = self.__tokenizer.tokenize(document)
        if tokens == []:
            return []
        results = []
        doc = LDADoc()
        self.__engine.infer(tokens, doc)
        topics = doc.sparse_topic_dist()
        for topic in topics:
            results.append({""topic id"": topic.tid, ""distribution"": topic.prob})
        return results","for topic in topics:
    results.append({'topic id': topic.tid, 'distribution': topic.prob})","results = [{'topic id': topic.tid, 'distribution': topic.prob} for topic in topics]","results = [{'topic id': topic.tid, 'distribution': topic.prob} for topic in topics]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161,"def get_all_funcs_async(file_list):

    async_group = []
    done_list = []
    for file in file_list:
        async_group.append(
            async_get_funcs.apply_async(
                args=[file],
                time_limit=3600,
            ))

    bar = tqdm.tqdm(total=len(async_group),
                    desc=""[~] Recovering function prototypes"")
    while not all([x.successful() or x.failed() for x in async_group]):
        done_count = len([x.successful() or x.failed() for x in async_group if x.successful() or x.failed()])
        bar.update(done_count - bar.n)
        # done_list = check_files(async_group, done_list)
        time.sleep(1)
    bar.close()

    return [x.get(propagate=False) for x in async_group if not x.failed()]","for file in file_list:
    async_group.append(async_get_funcs.apply_async(args=[file], time_limit=3600))","async_group = [async_get_funcs.apply_async(args=[file], time_limit=3600) for file in file_list]","async_group = [async_get_funcs.apply_async(args=[file], time_limit=3600) for file in file_list]",1,Firmware_Slap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Firmware_Slap/bin/Pwn_Firmware_CGI.py,https://github.com/ChrisTheCoolHut/Firmware_Slap/tree/master/bin/Pwn_Firmware_CGI.py,,get_all_funcs_async$161
NOFOUND
