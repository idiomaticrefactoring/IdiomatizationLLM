repo_name,file_path,file_html,class_name,me_name,me_code,old_code,chatGPT_code,element_str,slice_str,chatgpt_real_acc,ridiom_acc,real_acc,double sen,truth_code,,,,,
find_or_refactor_wrong,,,,,,,,,,,,,,,,,,,
PyGaze,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PyGaze/pygaze/_eyetracker/libopengaze.py,https://github.com/esdalmaijer/PyGaze/tree/master/pygaze/_eyetracker/libopengaze.py,OpenGazeTracker,calibrate$184,"def calibrate(self):
    """"""Calibrates the eye tracking system
        
        arguments
        None
        
        keyword arguments
        None

        returns
        success    -- returns True if calibration succeeded, or False if
                   not; in addition a calibration log is added to the
                   log file and some properties are updated (i.e. the
                   thresholds for detection algorithms)
        """"""
    self.screen.clear()
    self.screen.draw_text(text='Preparing the calibration, please wait...', fontsize=20)
    self.disp.fill(self.screen)
    self.disp.show()
    caldur = {'animation': 1.5, 'point': 1.0, 'timeout': 10.0}
    self.opengaze.calibrate_delay(caldur['animation'])
    self.opengaze.calibrate_timeout(caldur['point'])
    calibpoints = []
    for x in [0.1, 0.5, 0.9]:
        for y in [0.1, 0.5, 0.9]:
            calibpoints.append((x, y))
    random.shuffle(calibpoints)
    self.opengaze.calibrate_clear()
    for (x, y) in calibpoints:
        self.opengaze.calibrate_addpoint(x, y)
    self.screen.clear()
    self.screen.draw_text(text='Press Space to calibrate, S to skip, and Q to quit', fontsize=20)
    self.disp.fill(self.screen)
    self.disp.show()
    (key, keytime) = self.kb.get_key(keylist=['q', 's', 'space'], timeout=None, flush=True)
    self.screen.clear()
    self.disp.fill(self.screen)
    self.disp.show()
    if key == 's':
        return True
    if key == 'q':
        quited = True
    else:
        quited = False
    x = self.dispsize[0] // 2
    y = self.dispsize[1] // 2
    self.screen.draw_circle(colour=(255, 255, 255), pos=(x, y), r=30, fill=True)
    self.screen.draw_circle(colour=(255, 0, 0), pos=(x, y), r=3, fill=True)
    self.disp.fill(self.screen)
    self.disp.show()
    calibrated = False
    while not quited and (not calibrated):
        self.opengaze.clear_calibration_result()
        while True:
            calibpoints = self.opengaze.get_calibration_points()
            if calibpoints is not None:
                break
        self.opengaze.calibrate_start(True)
        pointnr = 0
        n_points = len(calibpoints)
        if self.has_been_calibrated_before:
            n_points += 1
        for i in range(n_points):
            (pointnr, pos) = self.opengaze.wait_for_calibration_point_start(timeout=caldur['timeout'])
            if pointnr is None:
                quited = True
                break
            x = int(pos[0] * self.dispsize[0])
            y = int(pos[1] * self.dispsize[1])
            t1 = clock.get_time()
            t = clock.get_time()
            while t - t1 < caldur['animation'] * 1000:
                if self.kb.get_key(keylist=['q'], timeout=10, flush=False)[0] == 'q':
                    quited = True
                    break
                self.screen.clear(colour=(0, 0, 0))
                p = 1.0 - float(t - t1) / (caldur['animation'] * 1000)
                self.screen.draw_circle(colour=(255, 255, 255), pos=(x, y), r=max(1, int(30 * p)), fill=True)
                self.screen.draw_circle(colour=(255, 0, 0), pos=(x, y), r=3, fill=True)
                self.disp.fill(self.screen)
                t = self.disp.show()
            if self.kb.get_key(keylist=['q'], timeout=1, flush=False)[0] == 'q':
                quited = True
            if quited:
                break
        calibresult = None
        while calibresult is None and (not quited):
            calibresult = self.opengaze.get_calibration_result()
            if self.kb.get_key(keylist=['q'], timeout=100, flush=False)[0] == 'q':
                quited = True
                break
        if quited:
            self.screen.clear()
            self.screen.draw_text(text=""Calibration aborted. Press Space to restart or 'Q' to quit"", fontsize=20)
            self.disp.fill(self.screen)
            self.disp.show()
            (key, keytime) = self.kb.get_key(keylist=['q', 'space'], timeout=None, flush=True)
            if key == 'space':
                quited = False
            continue
        self.disp.fill()
        self.disp.show()
        self.screen.clear()
        if calibresult is not None:
            for p in calibresult:
                for param in ['CALX', 'LX', 'RX']:
                    p[param] *= self.dispsize[0]
                for param in ['CALY', 'LY', 'RY']:
                    p[param] *= self.dispsize[1]
                self.screen.draw_fixation(fixtype='dot', colour=(115, 210, 22), pos=(p['CALX'], p['CALY']))
                col = {'L': (32, 74, 135), 'R': (92, 53, 102)}
                for eye in ['L', 'R']:
                    if p['{}V'.format(eye)]:
                        x = p['{}X'.format(eye)]
                        y = p['{}Y'.format(eye)]
                        c = col[eye]
                    else:
                        x = p['CALX']
                        y = p['CALY']
                        c = (204, 0, 0)
                    if p['{}V'.format(eye)]:
                        self.screen.draw_line(colour=c, spos=(p['CALX'], p['CALY']), epos=(x, y), pw=3)
                    self.screen.draw_fixation(fixtype='dot', pos=(x, y), colour=c)
                    self.screen.draw_text(text=eye, pos=(x + 10, y + 10), colour=c, fontsize=20)
            self.screen.draw_text(text=""Press Space to continue or 'R' to restart"", pos=(int(self.dispsize[0] * 0.5), int(self.dispsize[1] * 0.25 + 60)), fontsize=20)
        else:
            self.screen.draw_text(text=""Calibration failed. Press 'R' to try again."", fontsize=20)
        self.disp.fill(self.screen)
        self.disp.show()
        (key, keytime) = self.kb.get_key(keylist=['space', 'r'], timeout=None, flush=True)
        if key == 'space':
            calibrated = True
        self.has_been_calibrated_before = True
    if quited:
        return False
    err = {'LX': [], 'LY': [], 'RX': [], 'RY': []}
    var = {'LX': [], 'LY': [], 'RX': [], 'RY': []}
    for p in calibresult:
        for eye in ['L', 'R']:
            for dim in ['X', 'Y']:
                if p['{}V'.format(eye)]:
                    d = p['{}{}'.format(eye, dim)] - p['CAL{}'.format(dim)]
                    err['{}{}'.format(eye, dim)].append(abs(d))
                    var['{}{}'.format(eye, dim)].append(d ** 2)
    xnoise = []
    ynoise = []
    if var['LX'] and var['LY']:
        xnoise.append(math.sqrt(mean(var['LX'])))
        ynoise.append(math.sqrt(mean(var['LY'])))
    if var['RX'] and var['RY']:
        xnoise.append(math.sqrt(mean(var['RX'])))
        ynoise.append(math.sqrt(mean(var['RY'])))
    xnoise = mean(xnoise)
    ynoise = mean(ynoise)
    self.pxdsttresh = (xnoise, ynoise)
    pixpercm = (self.dispsize[0] / float(self.screensize[0]) + self.dispsize[1] / float(self.screensize[1])) / 2
    screendist = settings.SCREENDIST
    self.accuracy = []
    self.pxaccuracy = []
    if err['LX'] and err['LY']:
        self.accuracy.append([pix2deg(screendist, mean(err['LX']), pixpercm), pix2deg(screendist, mean(err['LY']), pixpercm)])
        self.pxaccuracy.append([mean(err['LX']), mean(err['LY'])])
    else:
        self.accuracy.append([0, 0])
        self.pxaccuracy.append([0, 0])
    if err['RX'] and err['RY']:
        self.accuracy.append([pix2deg(screendist, mean(err['RX']), pixpercm), pix2deg(screendist, mean(err['RY']), pixpercm)])
        self.pxaccuracy.append([mean(err['RX']), mean(err['RY'])])
    else:
        self.accuracy.append([0, 0])
        self.pxaccuracy.append([0, 0])
    self.pxerrdist = deg2pix(screendist, self.errdist, pixpercm)
    self.pxfixtresh = deg2pix(screendist, self.fixtresh, pixpercm)
    self.pxspdtresh = deg2pix(screendist, self.spdtresh / 1000.0, pixpercm)
    self.pxacctresh = deg2pix(screendist, self.accthresh / 1000.0, pixpercm)
    self._elog('pygaze calibration report start')
    self._elog('accuracy (degrees): LX={}, LY={}, RX={}, RY={}'.format(self.accuracy[0][0], self.accuracy[0][1], self.accuracy[1][0], self.accuracy[1][1]))
    self._elog('accuracy (in pixels): LX={}, LY={}, RX={}, RY={}'.format(self.pxaccuracy[0][0], self.pxaccuracy[0][1], self.pxaccuracy[1][0], self.pxaccuracy[1][1]))
    self._elog('precision (RMS noise in pixels): X={}, Y={}'.format(self.pxdsttresh[0], self.pxdsttresh[1]))
    self._elog('distance between participant and display: {} cm'.format(screendist))
    self._elog('fixation threshold: {} pixels'.format(self.pxfixtresh))
    self._elog('speed threshold: {} pixels/ms'.format(self.pxspdtresh))
    self._elog('acceleration threshold: {} pixels/ms**2'.format(self.pxacctresh))
    self._elog('pygaze calibration report end')
    return True","for p in calibresult:
    for eye in ['L', 'R']:
        for dim in ['X', 'Y']:
            if p['{}V'.format(eye)]:
                d = p['{}{}'.format(eye, dim)] - p['CAL{}'.format(dim)]
                err['{}{}'.format(eye, dim)].append(abs(d))
                var['{}{}'.format(eye, dim)].append(d ** 2)","var['{}{}'.format(eye, dim)] = [pow(p[f'{eye}{dim}'] - p[f'CAL{dim}'], 2) for p in calibresult for eye in ['L', 'R'] for dim in ['X', 'Y'] if p[f'{eye}V']]",Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/detr/feature_extraction_detr.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/detr/feature_extraction_detr.py,DetrFeatureExtractor,post_process_panoptic$973,"def post_process_panoptic(self, outputs, processed_sizes, target_sizes=None, is_thing_map=None, threshold=0.85):
    """"""
        Converts the output of [`DetrForSegmentation`] into actual panoptic predictions. Only supports PyTorch.

        Args:
            outputs ([`DetrSegmentationOutput`]):
                Raw outputs of the model.
            processed_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `List[Tuple]` of length `batch_size`):
                Torch Tensor (or list) containing the size (h, w) of each image of the batch, i.e. the size after data
                augmentation but before batching.
            target_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `List[Tuple]` of length `batch_size`, *optional*):
                Torch Tensor (or list) corresponding to the requested final size (h, w) of each prediction. If left to
                None, it will default to the `processed_sizes`.
            is_thing_map (`torch.Tensor` of shape `(batch_size, 2)`, *optional*):
                Dictionary mapping class indices to either True or False, depending on whether or not they are a thing.
                If not set, defaults to the `is_thing_map` of COCO panoptic.
            threshold (`float`, *optional*, defaults to 0.85):
                Threshold to use to filter out queries.

        Returns:
            `List[Dict]`: A list of dictionaries, each dictionary containing a PNG string and segments_info values for
            an image in the batch as predicted by the model.
        """"""
    warnings.warn('`post_process_panoptic is deprecated and will be removed in v5 of Transformers, please use `post_process_panoptic_segmentation`.', FutureWarning)
    if target_sizes is None:
        target_sizes = processed_sizes
    if len(processed_sizes) != len(target_sizes):
        raise ValueError('Make sure to pass in as many processed_sizes as target_sizes')
    if is_thing_map is None:
        is_thing_map = {i: i <= 90 for i in range(201)}
    (out_logits, raw_masks, raw_boxes) = (outputs.logits, outputs.pred_masks, outputs.pred_boxes)
    if not len(out_logits) == len(raw_masks) == len(target_sizes):
        raise ValueError('Make sure that you pass in as many target sizes as the batch dimension of the logits and masks')
    preds = []

    def to_tuple(tup):
        if isinstance(tup, tuple):
            return tup
        return tuple(tup.cpu().tolist())
    for (cur_logits, cur_masks, cur_boxes, size, target_size) in zip(out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes):
        (scores, labels) = cur_logits.softmax(-1).max(-1)
        keep = labels.ne(outputs.logits.shape[-1] - 1) & (scores > threshold)
        (cur_scores, cur_classes) = cur_logits.softmax(-1).max(-1)
        cur_scores = cur_scores[keep]
        cur_classes = cur_classes[keep]
        cur_masks = cur_masks[keep]
        cur_masks = nn.functional.interpolate(cur_masks[:, None], to_tuple(size), mode='bilinear').squeeze(1)
        cur_boxes = center_to_corners_format(cur_boxes[keep])
        (h, w) = cur_masks.shape[-2:]
        if len(cur_boxes) != len(cur_classes):
            raise ValueError('Not as many boxes as there are classes')
        cur_masks = cur_masks.flatten(1)
        stuff_equiv_classes = defaultdict(lambda : [])
        for (k, label) in enumerate(cur_classes):
            if not is_thing_map[label.item()]:
                stuff_equiv_classes[label.item()].append(k)

        def get_ids_area(masks, scores, dedup=False):
            m_id = masks.transpose(0, 1).softmax(-1)
            if m_id.shape[-1] == 0:
                m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)
            else:
                m_id = m_id.argmax(-1).view(h, w)
            if dedup:
                for equiv in stuff_equiv_classes.values():
                    if len(equiv) > 1:
                        for eq_id in equiv:
                            m_id.masked_fill_(m_id.eq(eq_id), equiv[0])
            (final_h, final_w) = to_tuple(target_size)
            seg_img = Image.fromarray(id_to_rgb(m_id.view(h, w).cpu().numpy()))
            seg_img = seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST)
            np_seg_img = torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes()))
            np_seg_img = np_seg_img.view(final_h, final_w, 3)
            np_seg_img = np_seg_img.numpy()
            m_id = torch.from_numpy(rgb_to_id(np_seg_img))
            area = []
            for i in range(len(scores)):
                area.append(m_id.eq(i).sum().item())
            return (area, seg_img)
        (area, seg_img) = get_ids_area(cur_masks, cur_scores, dedup=True)
        if cur_classes.numel() > 0:
            while True:
                filtered_small = torch.as_tensor([area[i] <= 4 for (i, c) in enumerate(cur_classes)], dtype=torch.bool, device=keep.device)
                if filtered_small.any().item():
                    cur_scores = cur_scores[~filtered_small]
                    cur_classes = cur_classes[~filtered_small]
                    cur_masks = cur_masks[~filtered_small]
                    (area, seg_img) = get_ids_area(cur_masks, cur_scores)
                else:
                    break
        else:
            cur_classes = torch.ones(1, dtype=torch.long, device=cur_classes.device)
        segments_info = []
        for (i, a) in enumerate(area):
            cat = cur_classes[i].item()
            segments_info.append({'id': i, 'isthing': is_thing_map[cat], 'category_id': cat, 'area': a})
        del cur_classes
        with io.BytesIO() as out:
            seg_img.save(out, format='PNG')
            predictions = {'png_string': out.getvalue(), 'segments_info': segments_info}
        preds.append(predictions)
    return preds","for (k, label) in enumerate(cur_classes):
    if not is_thing_map[label.item()]:
        stuff_equiv_classes[label.item()].append(k)","stuff_equiv_classes[label.item()] = [k for (k, label) in enumerate(cur_classes) if not is_thing_map[label.item()]]",Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/toolbox/views/sources.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/toolbox/views/sources.py,StaticToolBoxViewSources,get_definitions$23,"def get_definitions(self) -> List[StaticToolBoxView]:
    view_definitions = []
    for view_dict in self.view_dicts:
        view_definitions.append(StaticToolBoxView.from_dict(view_dict))
    for view_directory in self.view_directories:
        if not os.path.exists(view_directory):
            log.warning(f'Failed to find toolbox view directory {view_directory}')
        for filename in os.listdir(view_directory):
            if not looks_like_view_source_filename(filename):
                continue
            view_path = os.path.join(view_directory, filename)
            with open(view_path) as f:
                view_dict = yaml.safe_load(f)
            if 'id' not in view_dict:
                file_id = os.path.splitext(filename)[0]
                view_dict['id'] = file_id
            view_definitions.append(StaticToolBoxView.from_dict(view_dict))
    return view_definitions","for view_directory in self.view_directories:
    if not os.path.exists(view_directory):
        log.warning(f'Failed to find toolbox view directory {view_directory}')
    for filename in os.listdir(view_directory):
        if not looks_like_view_source_filename(filename):
            continue
        view_path = os.path.join(view_directory, filename)
        with open(view_path) as f:
            view_dict = yaml.safe_load(f)
        if 'id' not in view_dict:
            file_id = os.path.splitext(filename)[0]
            view_dict['id'] = file_id
        view_definitions.append(StaticToolBoxView.from_dict(view_dict))","view_definitions += [StaticToolBoxView.from_dict(view_dict) for view_directory in self.view_directories if os.path.exists(view_directory) for filename in os.listdir(view_directory) if looks_like_view_source_filename(filename) for view_path in [os.path.join(view_directory, filename)] for file_id in [os.path.splitext(filename)[0]] for view_dict in [{'id': file_id, **yaml.safe_load(open(view_path))} if 'id' not in yaml.safe_load(open(view_path)) else yaml.safe_load(open(view_path))]]",Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
AutoDL-Projects,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoDL-Projects/exps/NATS-Bench/draw-ranks.py,https://github.com/D-X-Y/AutoDL-Projects/tree/master/exps/NATS-Bench/draw-ranks.py,,visualize_relative_info$37,"def visualize_relative_info(vis_save_dir, search_space, indicator, topk):
    vis_save_dir = vis_save_dir.resolve()
    print('{:} start to visualize {:} with top-{:} information'.format(time_string(), search_space, topk))
    vis_save_dir.mkdir(parents=True, exist_ok=True)
    cache_file_path = vis_save_dir / 'cache-{:}-info.pth'.format(search_space)
    datasets = ['cifar10', 'cifar100', 'ImageNet16-120']
    if not cache_file_path.exists():
        api = create(None, search_space, fast_mode=False, verbose=False)
        all_infos = OrderedDict()
        for index in range(len(api)):
            all_info = OrderedDict()
            for dataset in datasets:
                info_less = api.get_more_info(index, dataset, hp='12', is_random=False)
                info_more = api.get_more_info(index, dataset, hp=api.full_train_epochs, is_random=False)
                all_info[dataset] = dict(less=info_less['test-accuracy'], more=info_more['test-accuracy'])
            all_infos[index] = all_info
        torch.save(all_infos, cache_file_path)
        print('{:} save all cache data into {:}'.format(time_string(), cache_file_path))
    else:
        api = create(None, search_space, fast_mode=True, verbose=False)
        all_infos = torch.load(cache_file_path)
    (dpi, width, height) = (250, 5000, 1300)
    figsize = (width / float(dpi), height / float(dpi))
    (LabelSize, LegendFontsize) = (16, 16)
    (fig, axs) = plt.subplots(1, 3, figsize=figsize)
    datasets = ['cifar10', 'cifar100', 'ImageNet16-120']

    def sub_plot_fn(ax, dataset, indicator):
        performances = []
        for _index in range(len(api)):
            performances.append((all_infos[_index][dataset][indicator], _index))
        performances = sorted(performances, reverse=True)
        performances = performances[:int(len(api) * topk * 0.01)]
        selected_indexes = [x[1] for x in performances]
        print('{:} plot {:10s} with {:}, {:} architectures'.format(time_string(), dataset, indicator, len(selected_indexes)))
        standard_scores = []
        random_scores = []
        for idx in selected_indexes:
            standard_scores.append(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=False)['test-accuracy'])
            random_scores.append(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=True)['test-accuracy'])
        indexes = list(range(len(selected_indexes)))
        standard_indexes = sorted(indexes, key=lambda i: standard_scores[i])
        random_indexes = sorted(indexes, key=lambda i: random_scores[i])
        random_labels = []
        for idx in standard_indexes:
            random_labels.append(random_indexes.index(idx))
        for tick in ax.get_xticklabels():
            tick.set_fontsize(LabelSize - 3)
        for tick in ax.get_yticklabels():
            tick.set_rotation(25)
            tick.set_fontsize(LabelSize - 3)
        ax.set_xlim(0, len(indexes))
        ax.set_ylim(0, len(indexes))
        ax.set_yticks(np.arange(min(indexes), max(indexes), max(indexes) // 3))
        ax.set_xticks(np.arange(min(indexes), max(indexes), max(indexes) // 5))
        ax.scatter(indexes, random_labels, marker='^', s=0.5, c='tab:green', alpha=0.8)
        ax.scatter(indexes, indexes, marker='o', s=0.5, c='tab:blue', alpha=0.8)
        ax.scatter([-1], [-1], marker='o', s=100, c='tab:blue', label='Average Over Multi-Trials')
        ax.scatter([-1], [-1], marker='^', s=100, c='tab:green', label='Randomly Selected Trial')
        (coef, p) = scipy.stats.kendalltau(standard_scores, random_scores)
        ax.set_xlabel('architecture ranking in {:}'.format(name2label[dataset]), fontsize=LabelSize)
        if dataset == 'cifar10':
            ax.set_ylabel('architecture ranking', fontsize=LabelSize)
        ax.legend(loc=4, fontsize=LegendFontsize)
        return coef
    for (dataset, ax) in zip(datasets, axs):
        rank_coef = sub_plot_fn(ax, dataset, indicator)
        print('sub-plot {:} on {:} done, the ranking coefficient is {:.4f}.'.format(dataset, search_space, rank_coef))
    save_path = (vis_save_dir / '{:}-rank-{:}-top{:}.pdf'.format(search_space, indicator, topk)).resolve()
    fig.savefig(save_path, dpi=dpi, bbox_inches='tight', format='pdf')
    save_path = (vis_save_dir / '{:}-rank-{:}-top{:}.png'.format(search_space, indicator, topk)).resolve()
    fig.savefig(save_path, dpi=dpi, bbox_inches='tight', format='png')
    print('Save into {:}'.format(save_path))","for idx in selected_indexes:
    standard_scores.append(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=False)['test-accuracy'])
    random_scores.append(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=True)['test-accuracy'])","(standard_scores, random_scores) = [(api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=False)['test-accuracy'], api.get_more_info(idx, dataset, hp=api.full_train_epochs if indicator == 'more' else '12', is_random=True)['test-accuracy']) for idx in selected_indexes]",Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
veusz,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/veusz/veusz/dataimport/fits_hdf5_helpers.py,https://github.com/veusz/veusz/tree/master/veusz/dataimport/fits_hdf5_helpers.py,,convertSliceToText$96,"def convertSliceToText(slice):
    """"""Convert tuple slice into text.""""""
    if slice is None:
        return ''
    out = []
    for spart in slice:
        if isinstance(spart, int):
            out.append(str(spart))
            continue
        sparttxt = []
        for p in spart:
            if p is not None:
                sparttxt.append(str(p))
            else:
                sparttxt.append('')
        if sparttxt[-1] == '':
            del sparttxt[-1]
        out.append(':'.join(sparttxt))
    return ', '.join(out)","for spart in slice:
    if isinstance(spart, int):
        out.append(str(spart))
        continue
    sparttxt = []
    for p in spart:
        if p is not None:
            sparttxt.append(str(p))
        else:
            sparttxt.append('')
    if sparttxt[-1] == '':
        del sparttxt[-1]
    out.append(':'.join(sparttxt))","out = [str(spart) if isinstance(spart, int) else ':'.join([str(p) if p is not None else '' for p in spart][:-1]) for spart in slice]",Cannot refactor,-1,0,,2,,robosuite,,,,,it actually cannot refactor
s3prl,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/s3prl/s3prl/downstream/ctc/corpus/libriphone.py,https://github.com/s3prl/s3prl/tree/master/s3prl/downstream/ctc/corpus/libriphone.py,LibriPhoneDataset,__init__$35,"def __init__(self, split, tokenizer, bucket_size, path, lexicon, ascending=False, **kwargs):
    self.path = path
    self.bucket_size = bucket_size
    word2phonemes_all = defaultdict(list)
    for lexicon_file in lexicon:
        with open(lexicon_file, 'r') as file:
            lines = [line.strip() for line in file.readlines()]
            for line in lines:
                (word, phonemes) = parse_lexicon(line, tokenizer)
                word2phonemes_all[word].append(phonemes)
    word2phonemes = {}
    for (word, phonemes_all) in word2phonemes_all.items():
        if len(phonemes_all) > 1:
            print(f'[LibriPhone] - {len(phonemes_all)} of phoneme sequences found for {word}.')
            for (idx, phonemes) in enumerate(phonemes_all):
                print(f'{idx}. {phonemes}')
        word2phonemes[word] = phonemes_all[0]
    print(f'[LibriPhone] - Taking the first phoneme sequences for a deterministic behavior.')
    file_list = []
    for s in split:
        split_list = list(Path(join(path, s)).rglob('*.flac'))
        assert len(split_list) > 0, 'No data found @ {}'.format(join(path, s))
        file_list += split_list
    text = []
    for f in tqdm(file_list, desc='word -> phonemes'):
        text.append(read_text(str(f), word2phonemes, tokenizer))
    (self.file_list, self.text) = zip(*[(f_name, txt) for (f_name, txt) in sorted(zip(file_list, text), reverse=not ascending, key=lambda x: len(x[1]))])","for lexicon_file in lexicon:
    with open(lexicon_file, 'r') as file:
        lines = [line.strip() for line in file.readlines()]
        for line in lines:
            (word, phonemes) = parse_lexicon(line, tokenizer)
            word2phonemes_all[word].append(phonemes)","word2phonemes_all[word] = [phonemes for lexicon_file in lexicon for line in [line.strip() for line in open(lexicon_file, 'r').readlines()] for (word, phonemes) in [parse_lexicon(line, tokenizer)]]",Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
Paddle,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Paddle/python/paddle/fluid/tests/unittests/test_reorder_lod_tensor.py,https://github.com/PaddlePaddle/Paddle/tree/master/python/paddle/fluid/tests/unittests/test_reorder_lod_tensor.py,TestReorderLoDTensor,reorder$112,"def reorder(self):
    level = 0
    ref_lod = self.data[self.data_desc[1][0]][1][level]
    rank_table = []
    for i in range(len(ref_lod)):
        rank_table.append((i, ref_lod[i]))
    rank_table = sorted(rank_table, key=functools.cmp_to_key(lambda x, y: y[1] - x[1]))
    (input_value, input_lod) = self.data[self.data_desc[0][0]]
    offset_lod = convert_to_offset(input_lod)
    input_table = []
    if offset_lod:
        for i in range(len(offset_lod[level]) - 1):
            start_idx = i
            end_idx = i + 1
            sub_lod = []
            for lod_level_i in offset_lod[level:]:
                sub_lod_i = []
                for idx in range(start_idx, end_idx):
                    sub_lod_i.append(lod_level_i[idx + 1] - lod_level_i[idx])
                sub_lod.append(sub_lod_i)
                start_idx = lod_level_i[start_idx]
                end_idx = lod_level_i[end_idx]
            input_table.append((start_idx, end_idx - start_idx, sub_lod))
    else:
        input_table = [(i, 1, []) for i in range(len(rank_table))]
    output_value = np.zeros_like(input_value)
    output_lod = []
    offset = 0
    for (index, length) in rank_table:
        input_seq_start = input_table[index][0]
        input_seq_len = input_table[index][1]
        input_seq_end = input_seq_start + input_seq_len
        output_value[offset:offset + input_seq_len] = input_value[input_seq_start:input_seq_end]
        offset += input_seq_len
        input_seq_sub_lod = input_table[index][2]
        if len(output_lod) == 0:
            output_lod = [[] for i in input_seq_sub_lod]
        for (i, level) in enumerate(input_seq_sub_lod):
            output_lod[i].extend(level)
    return (output_value, output_lod)","for i in range(len(offset_lod[level]) - 1):
    start_idx = i
    end_idx = i + 1
    sub_lod = []
    for lod_level_i in offset_lod[level:]:
        sub_lod_i = []
        for idx in range(start_idx, end_idx):
            sub_lod_i.append(lod_level_i[idx + 1] - lod_level_i[idx])
        sub_lod.append(sub_lod_i)
        start_idx = lod_level_i[start_idx]
        end_idx = lod_level_i[end_idx]
    input_table.append((start_idx, end_idx - start_idx, sub_lod))",,Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
pixelsort,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pixelsort/pixelsort/sorter.py,https://github.com/satyarth/pixelsort/tree/master/pixelsort/sorter.py,,sort_image$4,"def sort_image(size, image_data, mask_data, intervals, randomness, sorting_function):
    sorted_pixels = []
    for y in range(size[1]):
        row = []
        x_min = 0
        for x_max in intervals[y] + [size[0]]:
            interval = []
            for x in range(x_min, x_max):
                if mask_data[x, y]:
                    interval.append(image_data[x, y])
            if random.random() < randomness / 100:
                row += interval
            else:
                row += sort_interval(interval, sorting_function)
            x_min = x_max
        sorted_pixels.append(row)
    return sorted_pixels","for y in range(size[1]):
    row = []
    x_min = 0
    for x_max in intervals[y] + [size[0]]:
        interval = []
        for x in range(x_min, x_max):
            if mask_data[x, y]:
                interval.append(image_data[x, y])
        if random.random() < randomness / 100:
            row += interval
        else:
            row += sort_interval(interval, sorting_function)
        x_min = x_max
    sorted_pixels.append(row)","sorted_pixels = [[image_data[x, y] for x in range(x_min, x_max) if mask_data[x, y]] if random.random() < randomness / 100 else sort_interval([image_data[x, y] for x in range(x_min, x_max) if mask_data[x, y]], sorting_function) for y in range(size[1]) for (x_min, x_max) in zip([0] + intervals[y], intervals[y] + [size[0]])]",Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/cwl/util.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/cwl/util.py,,galactic_job_json$115,"def galactic_job_json(job, test_data_directory, upload_func, collection_create_func, tool_or_workflow='workflow'):
    """"""Adapt a CWL job object to the Galaxy API.

    CWL derived tools in Galaxy can consume a job description sort of like
    CWL job objects via the API but paths need to be replaced with datasets
    and records and arrays with collection references. This function will
    stage files and modify the job description to adapt to these changes
    for Galaxy.
    """"""
    datasets = []
    dataset_collections = []

    def response_to_hda(target, upload_response):
        assert isinstance(upload_response, dict), upload_response
        assert 'outputs' in upload_response, upload_response
        assert len(upload_response['outputs']) > 0, upload_response
        dataset = upload_response['outputs'][0]
        datasets.append((dataset, target))
        dataset_id = dataset['id']
        return {'src': 'hda', 'id': dataset_id}

    def upload_file(file_path, secondary_files, **kwargs):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = FileUploadTarget(file_path, secondary_files, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_literal(contents, **kwd):
        target = FileLiteralTarget(contents, **kwd)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_tar(file_path):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = DirectoryUploadTarget(file_path)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_with_composite_data(file_path, composite_data, **kwargs):
        if file_path is not None:
            file_path = abs_path_or_uri(file_path, test_data_directory)
        composite_data_resolved = []
        for cd in composite_data:
            composite_data_resolved.append(abs_path_or_uri(cd, test_data_directory))
        target = FileUploadTarget(file_path, composite_data=composite_data_resolved, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_object(the_object):
        target = ObjectUploadTarget(the_object)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def replacement_item(value, force_to_file=False):
        is_dict = isinstance(value, dict)
        item_class = None if not is_dict else value.get('class', None)
        is_file = item_class == 'File'
        is_directory = item_class == 'Directory'
        is_collection = item_class == 'Collection'
        if force_to_file:
            if is_file:
                return replacement_file(value)
            else:
                return upload_object(value)
        if isinstance(value, list):
            return replacement_list(value)
        elif not isinstance(value, dict):
            if tool_or_workflow == 'workflow':
                return upload_object(value)
            else:
                return value
        if is_file:
            return replacement_file(value)
        elif is_directory:
            return replacement_directory(value)
        elif is_collection:
            return replacement_collection(value)
        else:
            return replacement_record(value)

    def replacement_file(value):
        if value.get('galaxy_id'):
            return {'src': 'hda', 'id': str(value['galaxy_id'])}
        file_path = value.get('location', None) or value.get('path', None)
        filetype = value.get('filetype', None) or value.get('format', None)
        composite_data_raw = value.get('composite_data', None)
        kwd = {}
        if 'tags' in value:
            kwd['tags'] = value.get('tags')
        if 'dbkey' in value:
            kwd['dbkey'] = value.get('dbkey')
        if composite_data_raw:
            composite_data = []
            for entry in composite_data_raw:
                path = None
                if isinstance(entry, dict):
                    path = entry.get('location', None) or entry.get('path', None)
                else:
                    path = entry
                composite_data.append(path)
            rval_c = upload_file_with_composite_data(None, composite_data, filetype=filetype, **kwd)
            return rval_c
        if file_path is None:
            contents = value.get('contents', None)
            if contents is not None:
                return upload_file_literal(contents, **kwd)
            return value
        secondary_files = value.get('secondaryFiles', [])
        secondary_files_tar_path = None
        if secondary_files:
            tmp = tempfile.NamedTemporaryFile(delete=False)
            tf = tarfile.open(fileobj=tmp, mode='w:')
            order: List[str] = []
            index_contents = {'order': order}
            for secondary_file in secondary_files:
                secondary_file_path = secondary_file.get('location', None) or secondary_file.get('path', None)
                assert secondary_file_path, f'Invalid secondaryFile entry found [{secondary_file}]'
                full_secondary_file_path = os.path.join(test_data_directory, secondary_file_path)
                basename = secondary_file.get('basename') or os.path.basename(secondary_file_path)
                order.append(unicodify(basename))
                tf.add(full_secondary_file_path, os.path.join(SECONDARY_FILES_EXTRA_PREFIX, basename))
            tmp_index = tempfile.NamedTemporaryFile(delete=False, mode='w')
            json.dump(index_contents, tmp_index)
            tmp_index.close()
            tf.add(tmp_index.name, SECONDARY_FILES_INDEX_PATH)
            tf.close()
            secondary_files_tar_path = tmp.name
        return upload_file(file_path, secondary_files_tar_path, filetype=filetype, **kwd)

    def replacement_directory(value):
        file_path = value.get('location', None) or value.get('path', None)
        if file_path is None:
            return value
        if not os.path.isabs(file_path):
            file_path = os.path.join(test_data_directory, file_path)
        tmp = tempfile.NamedTemporaryFile(delete=False)
        tf = tarfile.open(fileobj=tmp, mode='w:')
        tf.add(file_path, '.')
        tf.close()
        return upload_tar(tmp.name)

    def replacement_list(value):
        collection_element_identifiers = []
        for (i, item) in enumerate(value):
            dataset = replacement_item(item, force_to_file=True)
            collection_element = dataset.copy()
            collection_element['name'] = str(i)
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'list')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def to_elements(value, rank_collection_type):
        collection_element_identifiers = []
        assert 'elements' in value
        elements = value['elements']
        is_nested_collection = ':' in rank_collection_type
        for element in elements:
            if not is_nested_collection:
                dataset = replacement_item(element, force_to_file=True)
                collection_element = dataset.copy()
                collection_element['name'] = element['identifier']
                collection_element_identifiers.append(collection_element)
            else:
                sub_collection_type = rank_collection_type[rank_collection_type.find(':') + 1:]
                collection_element = {'name': element['identifier'], 'src': 'new_collection', 'collection_type': sub_collection_type, 'element_identifiers': to_elements(element, sub_collection_type)}
                collection_element_identifiers.append(collection_element)
        return collection_element_identifiers

    def replacement_collection(value):
        if value.get('galaxy_id'):
            return {'src': 'hdca', 'id': str(value['galaxy_id'])}
        assert 'collection_type' in value
        collection_type = value['collection_type']
        elements = to_elements(value, collection_type)
        collection = collection_create_func(elements, collection_type)
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def replacement_record(value):
        collection_element_identifiers = []
        for (record_key, record_value) in value.items():
            if not isinstance(record_value, dict) or record_value.get('class') != 'File':
                dataset = replacement_item(record_value, force_to_file=True)
                collection_element = dataset.copy()
            else:
                dataset = upload_file(record_value['location'], [])
                collection_element = dataset.copy()
            collection_element['name'] = record_key
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'record')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}
    replace_keys = {}
    for (key, value) in job.items():
        replace_keys[key] = replacement_item(value)
    job.update(replace_keys)
    return (job, datasets)","for (i, item) in enumerate(value):
    dataset = replacement_item(item, force_to_file=True)
    collection_element = dataset.copy()
    collection_element['name'] = str(i)
    collection_element_identifiers.append(collection_element)","collection_element_identifiers = [{'name': str(i), **replacement_item(item, force_to_file=True)} for (i, item) in enumerate(value)]",Cannot refactor,-1,0,0,2,1,robosuite,,,,,it actually cannot refactor
Minecraft-Overviewer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Minecraft-Overviewer/overviewer.py,https://github.com/overviewer/Minecraft-Overviewer/tree/master//overviewer.py,,main$52,"def main():
    logger.configure()
    if os.name == 'posix':
        if os.geteuid() == 0:
            logging.warning('You are running Overviewer as root. It is recommended that you never do this, as it is dangerous for your system. If you are running into permission errors, fix your file/directory permissions instead. Overviewer does not need access to critical system resources and therefore does not require root access.')
        try:
            with open('/etc/redhat-release', 'r') as release_f:
                rel_contents = release_f.read()
                try:
                    major_rel = re.search('\\d(\\.\\d+)?', rel_contents).group(0).split('.')[0]
                    if major_rel == '6':
                        logging.warning('We will be dropping support for this release of your distribution soon. Please upgrade as soon as possible, or you will not receive future Overviewer updates.')
                except AttributeError:
                    pass
        except IOError:
            pass
    try:
        cpus = multiprocessing.cpu_count()
    except NotImplementedError:
        cpus = 1
    avail_north_dirs = ['lower-left', 'upper-left', 'upper-right', 'lower-right', 'auto']
    parser = ArgumentParser(usage=helptext)
    parser.add_argument('-c', '--config', dest='config', action='store', help='Specify the config file to use.')
    parser.add_argument('-p', '--processes', dest='procs', action='store', type=int, help='The number of local worker processes to spawn. Defaults to the number of CPU cores your computer has.')
    parser.add_argument('--pid', dest='pid', action='store', help='Specify the pid file to use.')
    parser.add_argument('--rendermodes', dest='rendermodes', action='store', help=""If you're not using a config file, specify which rendermodes to render with this option. This is a comma-separated list."")
    parser.add_argument('world', nargs='?', help='Path or name of the world you want to render.')
    parser.add_argument('output', nargs='?', help='Output directory for the rendered map.')
    render_modifiers = parser.add_mutually_exclusive_group()
    render_modifiers.add_argument('--forcerender', dest='forcerender', action='store_true', help='Force re-render the entire map.')
    render_modifiers.add_argument('--check-tiles', dest='checktiles', action='store_true', help='Check each tile on disk and re-render old tiles.')
    render_modifiers.add_argument('--no-tile-checks', dest='notilechecks', action='store_true', help='Only render tiles that come from chunks that have changed since the last render (the default).')
    parser.add_argument('--check-terrain', dest='check_terrain', action='store_true', help='Try to locate the texture files. Useful for debugging texture problems.')
    parser.add_argument('-V', '--version', dest='version', help='Display version information and then exits.', action='store_true')
    parser.add_argument('--check-version', dest='checkversion', help='Fetch information about the latest version of Overviewer.', action='store_true')
    parser.add_argument('--update-web-assets', dest='update_web_assets', action='store_true', help='Update web assets. Will *not* render tiles or update overviewerConfig.js.')
    parser.add_argument('-q', '--quiet', dest='quiet', action='count', default=0, help='Print less output. You can specify this option multiple times.')
    parser.add_argument('-v', '--verbose', dest='verbose', action='count', default=0, help='Print more output. You can specify this option multiple times.')
    parser.add_argument('--simple-output', dest='simple', action='store_true', default=False, help='Use a simple output format, with no colors or progress bars.')
    exegroup = parser.add_argument_group('Other Scripts', 'These scripts may accept different arguments than the ones listed above.')
    exegroup.add_argument('--genpoi', dest='genpoi', action='store_true', help='Run the genPOI script.')
    exegroup.add_argument('--skip-scan', dest='skipscan', action='store_true', help=""When running GenPOI, don't scan for entities."")
    exegroup.add_argument('--skip-players', dest='skipplayers', action='store_true', help=""When running GenPOI, don't scan player data."")
    (args, unknowns) = parser.parse_known_args()
    if len(unknowns) > 0 and args.world and args.output:
        possible_mistakes = []
        for i in range(len(unknowns) + 1):
            possible_mistakes.append(' '.join([args.world, args.output] + unknowns[:i]))
            possible_mistakes.append(' '.join([args.output] + unknowns[:i]))
        for mistake in possible_mistakes:
            if os.path.exists(mistake):
                logging.warning('Looks like you tried to make me use {0} as an argument, but forgot to quote the argument correctly. Try using ""{0}"" instead if the spaces are part of the path.'.format(mistake))
                parser.error('Too many arguments.')
        parser.error('Too many arguments.')
    if args.genpoi:
        sys.argv.remove('--genpoi')
        g = __import__('overviewer_core.aux_files', {}, {}, ['genPOI'])
        g.genPOI.main()
        return 0
    logger.configure(logging.INFO + 10 * args.quiet - 10 * args.verbose, verbose=args.verbose > 0, simple=args.simple)
    if args.version:
        print('Minecraft Overviewer %s' % util.findGitVersion() + ' (%s)' % util.findGitHash()[:7])
        try:
            import overviewer_core.overviewer_version as overviewer_version
            print('built on %s' % overviewer_version.BUILD_DATE)
            if args.verbose > 0:
                print('Build machine: %s %s' % (overviewer_version.BUILD_PLATFORM, overviewer_version.BUILD_OS))
                print('Read version information from %r' % overviewer_version.__file__)
        except ImportError:
            print('(build info not found)')
        if args.verbose > 0:
            print('Python executable: %r' % sys.executable)
            print(sys.version)
        if not args.checkversion:
            return 0
    if args.checkversion:
        print('Currently running Minecraft Overviewer %s' % util.findGitVersion() + ' (%s)' % util.findGitHash()[:7])
        try:
            from urllib import request
            import json
            latest_ver = json.loads(request.urlopen('http://overviewer.org/download.json').read())['src']
            print('Latest version of Minecraft Overviewer %s (%s)' % (latest_ver['version'], latest_ver['commit'][:7]))
            print('See https://overviewer.org/downloads for more information.')
        except Exception:
            print('Failed to fetch latest version info.')
            if args.verbose > 0:
                import traceback
                traceback.print_exc()
            else:
                print('Re-run with --verbose for more details.')
            return 1
        return 0
    if args.pid:
        if os.path.exists(args.pid):
            try:
                with open(args.pid, 'r') as fpid:
                    pid = int(fpid.read())
                    if util.pid_exists(pid):
                        print('Overviewer is already running (pid exists) - exiting.')
                        return 0
            except (IOError, ValueError):
                pass
        with open(args.pid, 'w') as f:
            f.write(str(os.getpid()))
    if args.check_terrain and (not args.config):
        import hashlib
        from overviewer_core.textures import Textures
        tex = Textures()
        logging.info('Looking for a few common texture files...')
        try:
            f = tex.find_file('assets/minecraft/textures/block/sandstone_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/grass_block_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/diamond_ore.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/acacia_planks.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/ancient_debris_top.png', verbose=True)
        except IOError:
            logging.error('Could not find any texture files.')
            return 1
        return 0
    if not (args.world and args.output) and (not args.config):
        if util.is_bare_console():
            print('\n')
            print('The Overviewer is a console program.  Please open a Windows command prompt')
            print('first and run Overviewer from there.   Further documentation is available at')
            print('http://docs.overviewer.org/\n')
            print('\n')
            print('For a quick-start guide on Windows, visit the following URL:\n')
            print('http://docs.overviewer.org/en/latest/win_tut/windowsguide/\n')
        else:
            logging.error('You must either specify --config or give me a world directory and output directory.')
            parser.print_help()
            list_worlds()
        return 1
    if args.config and (args.world and args.output):
        print()
        print('If you specify --config, you need to specify the world to render as well as the destination in the config file, not on the command line.')
        print('Put something like this in your config file:')
        print(""worlds['myworld'] = %r"" % args[0])
        print('outputdir = %r' % (args[1] if len(args) > 1 else '/path/to/output'))
        print()
        logging.error('You cannot specify both --config AND a world + output directory on the command line.')
        parser.print_help()
        return 1
    if not args.config and (args.world or args.output) and (not (args.world and args.output)):
        logging.error('You must specify both the world directory and an output directory')
        parser.print_help()
        return 1
    mw_parser = config_parser.MultiWorldParser()
    if not args.config:
        (worldpath, destdir) = map(os.path.expanduser, [args.world, args.output])
        logging.debug('Using %r as the world directory', worldpath)
        logging.debug('Using %r as the output directory', destdir)
        mw_parser.set_config_item('worlds', {'world': worldpath})
        mw_parser.set_config_item('outputdir', destdir)
        rendermodes = ['lighting']
        if args.rendermodes:
            rendermodes = args.rendermodes.replace('-', '_').split(',')
        renders = OrderedDict()
        for rm in rendermodes:
            renders['world-' + rm] = {'world': 'world', 'title': 'Overviewer Render (%s)' % rm, 'rendermode': rm}
        mw_parser.set_config_item('renders', renders)
    else:
        if args.rendermodes:
            logging.error('You cannot specify --rendermodes if you give a config file. Configure your rendermodes in the config file instead.')
            parser.print_help()
            return 1
        try:
            mw_parser.parse(os.path.expanduser(args.config))
        except config_parser.MissingConfigException as e:
            logging.error(str(e))
            util.nice_exit(1)
    if args.procs:
        mw_parser.set_config_item('processes', args.procs)
    try:
        config = mw_parser.get_validated_config()
    except Exception as ex:
        if args.verbose:
            logging.exception('An error was encountered with your configuration. See the information below.')
        else:
            logging.error('An error was encountered with your configuration.')
            logging.error(str(ex))
        return 1
    if args.check_terrain:
        logging.info('Looking for a few common texture files...')
        for (render_name, render) in config['renders'].items():
            logging.info('Looking at render %r.', render_name)
            texopts = util.dict_subset(render, ['texturepath'])
            tex = textures.Textures(**texopts)
            f = tex.find_file('assets/minecraft/textures/block/sandstone_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/grass_block_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/diamond_ore.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/oak_planks.png', verbose=True)
        return 0
    logging.info('Welcome to Minecraft Overviewer version %s (%s)!' % (util.findGitVersion(), util.findGitHash()[:7]))
    logging.debug('Current log level: {0}.'.format(logging.getLogger().level))

    def set_renderchecks(checkname, num):
        for (name, render) in config['renders'].items():
            if render.get('renderchecks', 0) == 3:
                logging.warning(checkname + ' ignoring render ' + repr(name) + ' since it\'s marked as ""don\'t render"".')
            else:
                render['renderchecks'] = num
    if args.forcerender:
        logging.info('Forcerender mode activated. ALL tiles will be rendered.')
        set_renderchecks('forcerender', 2)
    elif args.checktiles:
        logging.info('Checking all tiles for updates manually.')
        set_renderchecks('checktiles', 1)
    elif args.notilechecks:
        logging.info('Disabling all tile mtime checks. Only rendering tiles that need updating since last render.')
        set_renderchecks('notilechecks', 0)
    if not config['renders']:
        logging.error(""You must specify at least one render in your config file. Check the documentation at http://docs.overviewer.org if you're having trouble."")
        return 1
    for (rname, render) in config['renders'].items():
        try:
            worldpath = config['worlds'][render['world']]
        except KeyError:
            logging.error(""Render %s's world is '%s', but I could not find a corresponding entry in the worlds dictionary."", rname, render['world'])
            return 1
        render['worldname_orig'] = render['world']
        render['world'] = worldpath
        if render.get('forcerender', False):
            render['renderchecks'] = 2
        if render.get('overlay', []) != []:
            for x in render.get('overlay'):
                if x != rname:
                    try:
                        renderLink = config['renders'][x]
                    except KeyError:
                        logging.error(""Render %s's overlay is '%s', but I could not find a corresponding entry in the renders dictionary."", rname, x)
                        return 1
                else:
                    logging.error(""Render %s's overlay contains itself."", rname)
                    return 1
    destdir = config['outputdir']
    if not destdir:
        logging.error('You must specify the output directory in your config file.')
        logging.error(""e.g. outputdir = '/path/to/outputdir'"")
        return 1
    if not os.path.exists(destdir):
        try:
            os.mkdir(destdir)
        except OSError:
            logging.exception('Could not create the output directory.')
            return 1
    assetMrg = assetmanager.AssetManager(destdir, config.get('customwebassets', None))
    if args.update_web_assets:
        assetMrg.output_noconfig()
        logging.info('Web assets have been updated.')
        return 0
    changelists = {}
    for render in config['renders'].values():
        if 'changelist' in render:
            path = render['changelist']
            if path not in changelists:
                out = open(path, 'w')
                logging.debug('Opening changelist %s (%s).', out, out.fileno())
                changelists[path] = out
            else:
                out = changelists[path]
            render['changelist'] = out.fileno()
    tilesets = []
    worldcache = {}
    texcache = {}
    caches = []
    caches.append(cache.LRUCache(size=100))
    renders = config['renders']
    for (render_name, render) in renders.items():
        logging.debug('Found the following render thing: %r', render)
        try:
            w = worldcache[render['world']]
        except KeyError:
            try:
                w = world.World(render['world'])
            except CorruptNBTError as e:
                logging.error('Failed to open world %r.', render['world'])
                raise e
            except world.UnsupportedVersion as e:
                for ln in str(e).split('\n'):
                    logging.error(ln)
                sys.exit(1)
            worldcache[render['world']] = w
        texopts = util.dict_subset(render, ['texturepath', 'bgcolor', 'northdirection'])
        texopts_key = tuple(texopts.items())
        if texopts_key not in texcache:
            tex = textures.Textures(**texopts)
            logging.info('Generating textures...')
            tex.generate()
            logging.debug('Finished generating textures.')
            texcache[texopts_key] = tex
        else:
            tex = texcache[texopts_key]
        try:
            logging.debug('Asking for regionset %r.' % render['dimension'][1])
            rset = w.get_regionset(render['dimension'][1])
        except IndexError:
            logging.error(""Sorry, I can't find anything to render!  Are you sure there are .mca files in the world directory of %s?"" % render['world'])
            return 1
        if rset is None:
            logging.warning(""Sorry, you requested dimension '%s' for %s, but I couldn't find it."", render['dimension'][0], render_name)
            continue
        rset = world.CachedRegionSet(rset, caches)
        if 'crop' in render:
            rsets = []
            for zone in render['crop']:
                rsets.append(world.CroppedRegionSet(rset, *zone))
        else:
            rsets = [rset]
        if render['northdirection'] > 0:
            newrsets = []
            for r in rsets:
                r = world.RotatedRegionSet(r, render['northdirection'])
                newrsets.append(r)
            rsets = newrsets
        tileset_dir = os.path.abspath(os.path.join(destdir, render_name))
        render['name'] = render_name
        tileSetOpts = util.dict_subset(render, ['name', 'imgformat', 'renderchecks', 'rerenderprob', 'bgcolor', 'defaultzoom', 'imgquality', 'imglossless', 'optimizeimg', 'rendermode', 'worldname_orig', 'title', 'dimension', 'changelist', 'showspawn', 'overlay', 'base', 'poititle', 'maxzoom', 'showlocationmarker', 'minzoom', 'center'])
        tileSetOpts.update({'spawn': w.find_true_spawn()})
        for rset in rsets:
            tset = tileset.TileSet(w, rset, assetMrg, tex, tileSetOpts, tileset_dir)
            tilesets.append(tset)
    if not tilesets:
        logging.error(""There are no tilesets to render! There's nothing to do, so exiting."")
        return 1
    logging.info('Preprocessing...')
    for ts in tilesets:
        ts.do_preprocessing()
    assetMrg.initialize(tilesets)
    if config['processes'] == 1:
        dispatch = dispatcher.Dispatcher()
    else:
        dispatch = dispatcher.MultiprocessingDispatcher(local_procs=config['processes'])
    dispatch.render_all(tilesets, config['observer'])
    dispatch.close()
    assetMrg.finalize(tilesets)
    for out in changelists.values():
        logging.debug('Closing %s (%s).', out, out.fileno())
        out.close()
    if config['processes'] == 1:
        logging.debug('Final cache stats:')
        for c in caches:
            logging.debug('\t%s: %s hits, %s misses', c.__class__.__name__, c.hits, c.misses)
    if args.pid:
        os.remove(args.pid)
    logging.info(""Your render has been written to '%s', open index.html to view it."" % destdir)
    return 0","for i in range(len(unknowns) + 1):
    possible_mistakes.append(' '.join([args.world, args.output] + unknowns[:i]))
    possible_mistakes.append(' '.join([args.output] + unknowns[:i]))","possible_mistakes = [' '.join([args.world, args.output] + unknowns[:i]) for i in range(len(unknowns) + 1)] + [' '.join([args.output] + unknowns[:i]) for i in range(len(unknowns) + 1)]",Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
plantcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plantcv/plantcv/plantcv/y_axis_pseudolandmarks.py,https://github.com/danforthcenter/plantcv/tree/master/plantcv/plantcv/y_axis_pseudolandmarks.py,,y_axis_pseudolandmarks$12,"def y_axis_pseudolandmarks(img, obj, mask, label='default'):
    """"""
    Divide up object contour into 19 equidistant segments and generate landmarks for each

    Inputs:
    img      = This is a copy of the original plant image generated using np.copy if debug is true it will be drawn on
    obj      = a contour of the plant object (this should be output from the object_composition.py fxn)
    mask     = this is a binary image. The object should be white and the background should be black
    label        = optional label parameter, modifies the variable name of observations recorded

    Returns:
    left      = List of landmarks within the left side
    right   = List of landmarks within the right side
    center_h = List of landmarks within the center

    :param img: numpy.ndarray
    :param obj: list
    :param mask: numpy.ndarray
    :param label: str
    :return left: list
    :return right: list
    :return center_h: list
    """"""
    if not np.any(obj):
        return (('NA', 'NA'), ('NA', 'NA'), ('NA', 'NA'))
    (x, y, width, height) = cv2.boundingRect(obj)
    extent = height
    left = []
    right = []
    center_h = []
    left_list = []
    right_list = []
    center_h_list = []
    if extent >= 21:
        inc = int(extent / 21)
        pts_max = []
        pts_min = []
        for i in range(1, 21):
            if i == 1:
                pt_max = y
                pt_min = y + inc * i
            else:
                pt_max = y + inc * (i - 1)
                pt_min = y + inc * i
            pts_max.append(pt_max)
            pts_min.append(pt_min)
        point_range = list(zip(pts_max, pts_min))
        row_median = []
        row_ave = []
        max_width = []
        left_points = []
        right_points = []
        y_vals = []
        x_centroids = []
        y_centroids = []
        for pt in point_range:
            (low_point, high_point) = pt
            rows = []
            lps = []
            rps = []
            vals = list(range(low_point, high_point))
            for v in vals:
                value = obj[v == obj[:, 0, 1]]
                if len(value) > 0:
                    largest = value[:, 0, 0].max()
                    smallest = value[:, 0, 0].min()
                    row_width = largest - smallest
                    rows.append(row_width)
                    lps.append(smallest)
                    rps.append(largest)
                if len(value) == 0:
                    row_width = 1
                    rows.append(row_width)
                    lps.append(1)
                    rps.append(1)
            row_median.append(np.median(np.array(rows)))
            row_ave.append(np.mean(np.array(rows)))
            max_width.append(np.max(np.array(rows)))
            left_points.append(np.mean(smallest))
            right_points.append(np.mean(largest))
            yval = int((high_point + low_point) / 2)
            y_vals.append(yval)
            window = np.copy(mask)
            window[:low_point] = 0
            window[high_point:] = 0
            s = cv2.moments(window)
            if largest - smallest > 3:
                if s['m00'] > 0.001:
                    (smx, smy) = (s['m10'] / s['m00'], s['m01'] / s['m00'])
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
                if s['m00'] < 0.001:
                    (smx, smy) = (s['m10'] / 0.001, s['m01'] / 0.001)
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
            else:
                smx = (largest + smallest) / 2
                smy = yval
                x_centroids.append(int(smx))
                y_centroids.append(int(smy))
        left = list(zip(left_points, y_vals))
        left = np.array(left)
        left.shape = (20, 1, 2)
        right = list(zip(right_points, y_vals))
        right = np.array(right)
        right.shape = (20, 1, 2)
        center_h = list(zip(x_centroids, y_centroids))
        center_h = np.array(center_h)
        center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    elif extent < 21:
        (x, y, width, height) = cv2.boundingRect(obj)
        y_coords = list(range(y, y + 20))
        l_points = [x] * 20
        left = list(zip(l_points, y_coords))
        left = np.array(left)
        left.shape = (20, 1, 2)
        r_points = [x + width] * 20
        right = list(zip(r_points, y_coords))
        right = np.array(right)
        right.shape = (20, 1, 2)
        m = cv2.moments(mask, binaryImage=True)
        if m['m00'] == 0:
            fatal_error('Check input parameters, first moment=0')
        else:
            (cmx, cmy) = (m['m10'] / m['m00'], m['m01'] / m['m00'])
            c_points = [cmx] * 20
            center_h = list(zip(c_points, y_coords))
            center_h = np.array(center_h)
            center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    for pt in left:
        left_list.append(pt[0].tolist())
    for pt in right:
        right_list.append(pt[0].tolist())
    for pt in center_h:
        center_h_list.append(pt[0].tolist())
    outputs.add_observation(sample=label, variable='left_lmk', trait='left landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(left_list), label='none')
    outputs.add_observation(sample=label, variable='right_lmk', trait='right landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(right_list), label='none')
    outputs.add_observation(sample=label, variable='center_h_lmk', trait='center horizontal landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(center_h_list), label='none')
    return (left, right, center_h)","for i in range(1, 21):
    if i == 1:
        pt_max = y
        pt_min = y + inc * i
    else:
        pt_max = y + inc * (i - 1)
        pt_min = y + inc * i
    pts_max.append(pt_max)
    pts_min.append(pt_min)","pts_min = [y + inc * i for i in range(1, 21)]",Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
plantcv,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/plantcv/plantcv/plantcv/y_axis_pseudolandmarks.py,https://github.com/danforthcenter/plantcv/tree/master/plantcv/plantcv/y_axis_pseudolandmarks.py,,y_axis_pseudolandmarks$12,"def y_axis_pseudolandmarks(img, obj, mask, label='default'):
    """"""
    Divide up object contour into 19 equidistant segments and generate landmarks for each

    Inputs:
    img      = This is a copy of the original plant image generated using np.copy if debug is true it will be drawn on
    obj      = a contour of the plant object (this should be output from the object_composition.py fxn)
    mask     = this is a binary image. The object should be white and the background should be black
    label        = optional label parameter, modifies the variable name of observations recorded

    Returns:
    left      = List of landmarks within the left side
    right   = List of landmarks within the right side
    center_h = List of landmarks within the center

    :param img: numpy.ndarray
    :param obj: list
    :param mask: numpy.ndarray
    :param label: str
    :return left: list
    :return right: list
    :return center_h: list
    """"""
    if not np.any(obj):
        return (('NA', 'NA'), ('NA', 'NA'), ('NA', 'NA'))
    (x, y, width, height) = cv2.boundingRect(obj)
    extent = height
    left = []
    right = []
    center_h = []
    left_list = []
    right_list = []
    center_h_list = []
    if extent >= 21:
        inc = int(extent / 21)
        pts_max = []
        pts_min = []
        for i in range(1, 21):
            if i == 1:
                pt_max = y
                pt_min = y + inc * i
            else:
                pt_max = y + inc * (i - 1)
                pt_min = y + inc * i
            pts_max.append(pt_max)
            pts_min.append(pt_min)
        point_range = list(zip(pts_max, pts_min))
        row_median = []
        row_ave = []
        max_width = []
        left_points = []
        right_points = []
        y_vals = []
        x_centroids = []
        y_centroids = []
        for pt in point_range:
            (low_point, high_point) = pt
            rows = []
            lps = []
            rps = []
            vals = list(range(low_point, high_point))
            for v in vals:
                value = obj[v == obj[:, 0, 1]]
                if len(value) > 0:
                    largest = value[:, 0, 0].max()
                    smallest = value[:, 0, 0].min()
                    row_width = largest - smallest
                    rows.append(row_width)
                    lps.append(smallest)
                    rps.append(largest)
                if len(value) == 0:
                    row_width = 1
                    rows.append(row_width)
                    lps.append(1)
                    rps.append(1)
            row_median.append(np.median(np.array(rows)))
            row_ave.append(np.mean(np.array(rows)))
            max_width.append(np.max(np.array(rows)))
            left_points.append(np.mean(smallest))
            right_points.append(np.mean(largest))
            yval = int((high_point + low_point) / 2)
            y_vals.append(yval)
            window = np.copy(mask)
            window[:low_point] = 0
            window[high_point:] = 0
            s = cv2.moments(window)
            if largest - smallest > 3:
                if s['m00'] > 0.001:
                    (smx, smy) = (s['m10'] / s['m00'], s['m01'] / s['m00'])
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
                if s['m00'] < 0.001:
                    (smx, smy) = (s['m10'] / 0.001, s['m01'] / 0.001)
                    x_centroids.append(int(smx))
                    y_centroids.append(int(smy))
            else:
                smx = (largest + smallest) / 2
                smy = yval
                x_centroids.append(int(smx))
                y_centroids.append(int(smy))
        left = list(zip(left_points, y_vals))
        left = np.array(left)
        left.shape = (20, 1, 2)
        right = list(zip(right_points, y_vals))
        right = np.array(right)
        right.shape = (20, 1, 2)
        center_h = list(zip(x_centroids, y_centroids))
        center_h = np.array(center_h)
        center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    elif extent < 21:
        (x, y, width, height) = cv2.boundingRect(obj)
        y_coords = list(range(y, y + 20))
        l_points = [x] * 20
        left = list(zip(l_points, y_coords))
        left = np.array(left)
        left.shape = (20, 1, 2)
        r_points = [x + width] * 20
        right = list(zip(r_points, y_coords))
        right = np.array(right)
        right.shape = (20, 1, 2)
        m = cv2.moments(mask, binaryImage=True)
        if m['m00'] == 0:
            fatal_error('Check input parameters, first moment=0')
        else:
            (cmx, cmy) = (m['m10'] / m['m00'], m['m01'] / m['m00'])
            c_points = [cmx] * 20
            center_h = list(zip(c_points, y_coords))
            center_h = np.array(center_h)
            center_h.shape = (20, 1, 2)
        img2 = np.copy(img)
        for i in left:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 0), -1)
        for i in right:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (255, 0, 255), -1)
        for i in center_h:
            x = i[0, 0]
            y = i[0, 1]
            cv2.circle(img2, (int(x), int(y)), params.line_thickness, (0, 79, 255), -1)
        _debug(visual=img2, filename=os.path.join(params.debug_outdir, str(params.device) + '_y_axis_pseudolandmarks.png'))
    for pt in left:
        left_list.append(pt[0].tolist())
    for pt in right:
        right_list.append(pt[0].tolist())
    for pt in center_h:
        center_h_list.append(pt[0].tolist())
    outputs.add_observation(sample=label, variable='left_lmk', trait='left landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(left_list), label='none')
    outputs.add_observation(sample=label, variable='right_lmk', trait='right landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(right_list), label='none')
    outputs.add_observation(sample=label, variable='center_h_lmk', trait='center horizontal landmark coordinates', method='plantcv.plantcv.x_axis_pseudolandmarks', scale='none', datatype=tuple, value=tuple(center_h_list), label='none')
    return (left, right, center_h)","for v in vals:
    value = obj[v == obj[:, 0, 1]]
    if len(value) > 0:
        largest = value[:, 0, 0].max()
        smallest = value[:, 0, 0].min()
        row_width = largest - smallest
        rows.append(row_width)
        lps.append(smallest)
        rps.append(largest)
    if len(value) == 0:
        row_width = 1
        rows.append(row_width)
        lps.append(1)
        rps.append(1)","rps = [value[:, 0, 0].max() if len(value) > 0 else 1 for v in vals for value in [obj[v == obj[:, 0, 1]]]]",Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
snscrape,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/snscrape/snscrape/modules/twitter.py,https://github.com/JustAnotherArchivist/snscrape/tree/master/snscrape/modules/twitter.py,TwitterAPIScraper,_tweet_to_tweet$347,"def _tweet_to_tweet(self, tweet, obj):
    kwargs = {}
    kwargs['id'] = tweet['id'] if 'id' in tweet else int(tweet['id_str'])
    kwargs['content'] = tweet['full_text']
    kwargs['renderedContent'] = self._render_text_with_urls(tweet['full_text'], tweet['entities'].get('urls'))
    kwargs['user'] = self._user_to_user(obj['globalObjects']['users'][tweet['user_id_str']])
    kwargs['date'] = email.utils.parsedate_to_datetime(tweet['created_at'])
    if tweet['entities'].get('urls'):
        kwargs['outlinks'] = [u['expanded_url'] for u in tweet['entities']['urls']]
        kwargs['tcooutlinks'] = [u['url'] for u in tweet['entities']['urls']]
    kwargs['url'] = f""https://twitter.com/{obj['globalObjects']['users'][tweet['user_id_str']]['screen_name']}/status/{kwargs['id']}""
    kwargs['replyCount'] = tweet['reply_count']
    kwargs['retweetCount'] = tweet['retweet_count']
    kwargs['likeCount'] = tweet['favorite_count']
    kwargs['quoteCount'] = tweet['quote_count']
    kwargs['conversationId'] = tweet['conversation_id'] if 'conversation_id' in tweet else int(tweet['conversation_id_str'])
    kwargs['lang'] = tweet['lang']
    kwargs['source'] = tweet['source']
    if (match := re.search('href=[\\\'""]?([^\\\'"" >]+)', tweet['source'])):
        kwargs['sourceUrl'] = match.group(1)
    if (match := re.search('>([^<]*)<', tweet['source'])):
        kwargs['sourceLabel'] = match.group(1)
    if 'extended_entities' in tweet and 'media' in tweet['extended_entities']:
        media = []
        for medium in tweet['extended_entities']['media']:
            if medium['type'] == 'photo':
                if '.' not in medium['media_url_https']:
                    logger.warning(f""Skipping malformed medium URL on tweet {kwargs['id']}: {medium['media_url_https']!r} contains no dot"")
                    continue
                (baseUrl, format) = medium['media_url_https'].rsplit('.', 1)
                if format not in ('jpg', 'png'):
                    logger.warning(f""Skipping photo with unknown format on tweet {kwargs['id']}: {format!r}"")
                    continue
                media.append(Photo(previewUrl=f'{baseUrl}?format={format}&name=small', fullUrl=f'{baseUrl}?format={format}&name=large'))
            elif medium['type'] == 'video' or medium['type'] == 'animated_gif':
                variants = []
                for variant in medium['video_info']['variants']:
                    variants.append(VideoVariant(contentType=variant['content_type'], url=variant['url'], bitrate=variant.get('bitrate')))
                mKwargs = {'thumbnailUrl': medium['media_url_https'], 'variants': variants}
                if medium['type'] == 'video':
                    mKwargs['duration'] = medium['video_info']['duration_millis'] / 1000
                    if (ext := medium['ext']) and (mediaStats := ext['mediaStats']) and isinstance((r := mediaStats['r']), dict) and ('ok' in r) and isinstance(r['ok'], dict):
                        mKwargs['views'] = int(r['ok']['viewCount'])
                    cls = Video
                elif medium['type'] == 'animated_gif':
                    cls = Gif
                media.append(cls(**mKwargs))
        if media:
            kwargs['media'] = media
    if 'retweeted_status_id_str' in tweet:
        kwargs['retweetedTweet'] = self._tweet_to_tweet(obj['globalObjects']['tweets'][tweet['retweeted_status_id_str']], obj)
    if 'quoted_status_id_str' in tweet and tweet['quoted_status_id_str'] in obj['globalObjects']['tweets']:
        kwargs['quotedTweet'] = self._tweet_to_tweet(obj['globalObjects']['tweets'][tweet['quoted_status_id_str']], obj)
    if (inReplyToTweetId := tweet.get('in_reply_to_status_id_str')):
        kwargs['inReplyToTweetId'] = int(inReplyToTweetId)
        inReplyToUserId = int(tweet['in_reply_to_user_id_str'])
        if inReplyToUserId == kwargs['user'].id:
            kwargs['inReplyToUser'] = kwargs['user']
        elif tweet['entities'].get('user_mentions'):
            for u in tweet['entities']['user_mentions']:
                if u['id_str'] == tweet['in_reply_to_user_id_str']:
                    kwargs['inReplyToUser'] = User(username=u['screen_name'], id=u['id'] if 'id' in u else int(u['id_str']), displayname=u['name'])
        if 'inReplyToUser' not in kwargs:
            kwargs['inReplyToUser'] = User(username=tweet['in_reply_to_screen_name'], id=inReplyToUserId)
    if tweet['entities'].get('user_mentions'):
        kwargs['mentionedUsers'] = [User(username=u['screen_name'], id=u['id'] if 'id' in u else int(u['id_str']), displayname=u['name']) for u in tweet['entities']['user_mentions']]
    if tweet.get('coordinates'):
        if (coords := tweet['coordinates']['coordinates']) and len(coords) == 2:
            kwargs['coordinates'] = Coordinates(coords[0], coords[1])
    elif tweet.get('geo'):
        if (coords := tweet['geo']['coordinates']) and len(coords) == 2:
            kwargs['coordinates'] = Coordinates(coords[1], coords[0])
    if tweet.get('place'):
        kwargs['place'] = Place(tweet['place']['full_name'], tweet['place']['name'], tweet['place']['place_type'], tweet['place']['country'], tweet['place']['country_code'])
        if 'coordinates' not in kwargs and tweet['place']['bounding_box'] and (coords := tweet['place']['bounding_box']['coordinates']) and coords[0] and (len(coords[0][0]) == 2):
            kwargs['coordinates'] = Coordinates(coords[0][0][0], coords[0][0][1])
    if tweet['entities'].get('hashtags'):
        kwargs['hashtags'] = [o['text'] for o in tweet['entities']['hashtags']]
    if tweet['entities'].get('symbols'):
        kwargs['cashtags'] = [o['text'] for o in tweet['entities']['symbols']]
    return Tweet(**kwargs)","for medium in tweet['extended_entities']['media']:
    if medium['type'] == 'photo':
        if '.' not in medium['media_url_https']:
            logger.warning(f""Skipping malformed medium URL on tweet {kwargs['id']}: {medium['media_url_https']!r} contains no dot"")
            continue
        (baseUrl, format) = medium['media_url_https'].rsplit('.', 1)
        if format not in ('jpg', 'png'):
            logger.warning(f""Skipping photo with unknown format on tweet {kwargs['id']}: {format!r}"")
            continue
        media.append(Photo(previewUrl=f'{baseUrl}?format={format}&name=small', fullUrl=f'{baseUrl}?format={format}&name=large'))
    elif medium['type'] == 'video' or medium['type'] == 'animated_gif':
        variants = []
        for variant in medium['video_info']['variants']:
            variants.append(VideoVariant(contentType=variant['content_type'], url=variant['url'], bitrate=variant.get('bitrate')))
        mKwargs = {'thumbnailUrl': medium['media_url_https'], 'variants': variants}
        if medium['type'] == 'video':
            mKwargs['duration'] = medium['video_info']['duration_millis'] / 1000
            if (ext := medium['ext']) and (mediaStats := ext['mediaStats']) and isinstance((r := mediaStats['r']), dict) and ('ok' in r) and isinstance(r['ok'], dict):
                mKwargs['views'] = int(r['ok']['viewCount'])
            cls = Video
        elif medium['type'] == 'animated_gif':
            cls = Gif
        media.append(cls(**mKwargs))","media += [Photo(previewUrl=f""{medium['media_url_https'].rsplit('.', 1)[0]}?format={medium['media_url_https'].rsplit('.', 1)[1]}&name=small"", fullUrl=f""{medium['media_url_https'].rsplit('.', 1)[0]}?format={medium['media_url_https'].rsplit('.', 1)[1]}&name=large"") if medium['type'] == 'photo' and '.' in medium['media_url_https'] and (medium['media_url_https'].rsplit('.', 1)[1] in ('jpg', 'png')) else Gif(thumbnailUrl=medium['media_url_https'], variants=[VideoVariant(contentType=variant['content_type'], url=variant['url'], bitrate=variant.get('bitrate')) for variant in medium['video_info']['variants']]) if medium['type'] == 'animated_gif' else Video(thumbnailUrl=medium['media_url_https'], variants=[VideoVariant(contentType=variant['content_type'], url=variant['url'], bitrate=variant.get('bitrate')) for variant in medium['video_info']['variants']], duration=medium['video_info']['duration_millis'] / 1000, views=int(medium['ext']['mediaStats']['r']['ok']['viewCount']) if 'ext' in medium and 'mediaStats' in medium['ext'] and isinstance(medium['ext']['mediaStats']['r'], dict) and ('ok' in medium['ext']['mediaStats']['r']) and isinstance(medium['ext']['mediaStats']['r']['ok'], dict) else None) if medium['type'] == 'video' else logger.warning(f""Skipping unknown medium type on tweet {kwargs['id']}: {medium['type']}"") for medium in tweet['extended_entities']['media']]",Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
vega,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/datasets/common/utils/auto_lane_codec_utils.py,https://github.com/huawei-noah/vega/tree/master/vega/datasets/common/utils/auto_lane_codec_utils.py,,delete_repeat_y$301,"def delete_repeat_y(cur_line):
    """"""Avoid same y with multi x.

    :param cur_line: the raw line
    :type cur_line:list
    :return: the deduplicated line
    :rtype:list
    """"""
    list_x = []
    list_y = []
    for pt in cur_line:
        list_x.append(pt.x)
        list_y.append(pt.y)
    sorted_y = sorted(list_y)
    sorted_x = []
    for i in range(len(sorted_y)):
        sorted_x.append(list_x[list_y.index(sorted_y[i])])
    set_sorted_y = []
    set_sorted_x = []
    index = 0
    for i in sorted_y:
        if not i in set_sorted_y:
            set_sorted_y.append(i)
            set_sorted_x.append(sorted_x[index])
        index += 1
    new_lane = []
    if len(set_sorted_y) < 2:
        return new_lane
    for i in range(len(set_sorted_y)):
        new_lane.append({'x': set_sorted_x[i], 'y': set_sorted_y[i]})
    if new_lane[0]['y'] < new_lane[1]['y']:
        new_lane = new_lane[::-1]
    return new_lane","for pt in cur_line:
    list_x.append(pt.x)
    list_y.append(pt.y)",list_y = [pt.y for pt in cur_line],Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for op in graph.get_operations():
    for x in op.inputs:
        op_to_all[op.name].append(x.name)
    for y in op.outputs:
        output_to_op[y.name].append(op.name)
        op_to_all[op.name].append(y.name)
    if str(op.type) == 'Assign':
        for y in op.outputs:
            for x in op.inputs:
                assign_out_to_in[y.name].append(x.name)",assign_out_to_in[y.name] = [x.name for op in graph.get_operations() if str(op.type) == 'Assign' for x in op.inputs for y in op.outputs if y.name == x.name],Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for out_name in assign_out_to_in.keys():
    name_group = assign_out_to_in[out_name]
    for n1 in name_group:
        assign_groups[n1].append(out_name)
        for n2 in name_group:
            if n1 != n2:
                assign_groups[n1].append(n2)",assign_groups[n1] = [out_name for out_name in assign_out_to_in.keys() for n1 in assign_out_to_in[out_name] for n2 in assign_out_to_in[out_name] if n1 != n2] + [n2 for out_name in assign_out_to_in.keys() for n1 in assign_out_to_in[out_name] for n2 in assign_out_to_in[out_name] if n1 != n2],Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for y in op.outputs:
    output_to_op[y.name].append(op.name)
    op_to_all[op.name].append(y.name)",op_to_all[op.name] += [y.name for y in op.outputs],Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for expanded_name in expanded_names:
    if expanded_name not in stack:
        stack.append(expanded_name)",stack += [expanded_name for expanded_name in expanded_names if expanded_name not in stack] + stack,Cannot refactor,-1,,,,,robosuite,,,,,it actually cannot refactor
TensorNetwork,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/TensorNetwork/tensornetwork/network_operations.py,https://github.com/google/TensorNetwork/tree/master/tensornetwork/network_operations.py,,split_node$130,"def split_node(node: AbstractNode, left_edges: List[Edge], right_edges: List[Edge], max_singular_values: Optional[int]=None, max_truncation_err: Optional[float]=None, relative: Optional[bool]=False, left_name: Optional[Text]=None, right_name: Optional[Text]=None, edge_name: Optional[Text]=None) -> Tuple[AbstractNode, AbstractNode, Tensor]:
    """"""Split a `node` using Singular Value Decomposition.

  Let :math:`M` be the matrix created by flattening `left_edges` and 
  `right_edges` into 2 axes. 
  Let :math:`U S V^* = M` be the SVD of :math:`M`. 
  This will split the network into 2 nodes. 
  The left node's tensor will be :math:`U \\sqrt{S}` 
  and the right node's tensor will be
  :math:`\\sqrt{S} V^*` where :math:`V^*` is the adjoint of :math:`V`.

  The singular value decomposition is truncated if `max_singular_values` or
  `max_truncation_err` is not `None`.

  The truncation error is the 2-norm of the vector of truncated singular
  values. If only `max_truncation_err` is set, as many singular values will
  be truncated as possible while maintaining:
  `norm(truncated_singular_values) <= max_truncation_err`.
  If `relative` is set `True` then `max_truncation_err` is understood
  relative to the largest singular value.

  If only `max_singular_values` is set, the number of singular values kept
  will be `min(max_singular_values, number_of_singular_values)`, so that
  `max(0, number_of_singular_values - max_singular_values)` are truncated.

  If both `max_truncation_err` and `max_singular_values` are set,
  `max_singular_values` takes priority: The truncation error may be larger
  than `max_truncation_err` if required to satisfy `max_singular_values`.

  Args:
    node: The node you want to split.
    left_edges: The edges you want connected to the new left node.
    right_edges: The edges you want connected to the new right node.
    max_singular_values: The maximum number of singular values to keep.
    max_truncation_err: The maximum allowed truncation error.
    relative: Multiply `max_truncation_err` with the largest singular value.
    left_name: The name of the new left node. If `None`, a name will be 
      generated automatically.
    right_name: The name of the new right node. If `None`, a name will be
      generated automatically.
    edge_name: The name of the new `Edge` connecting the new left and
      right node. If `None`, a name will be generated automatically.
      The new axis will get the same name as the edge.

  Returns:
    A tuple containing:
      left_node:
        A new node created that connects to all of the `left_edges`.
        Its underlying tensor is :math:`U \\sqrt{S}`
      right_node:
        A new node created that connects to all of the `right_edges`.
        Its underlying tensor is :math:`\\sqrt{S} V^*`
      truncated_singular_values:
        The vector of truncated singular values.
  Raises:
    AttributeError: If `node` has no backend attribute
  """"""
    if not hasattr(node, 'backend'):
        raise AttributeError('Node {} of type {} has no `backend`'.format(node, type(node)))
    if node.axis_names and edge_name:
        left_axis_names = []
        right_axis_names = [edge_name]
        for edge in left_edges:
            left_axis_names.append(node.axis_names[edge.axis1] if edge.node1 is node else node.axis_names[edge.axis2])
        for edge in right_edges:
            right_axis_names.append(node.axis_names[edge.axis1] if edge.node1 is node else node.axis_names[edge.axis2])
        left_axis_names.append(edge_name)
    else:
        left_axis_names = None
        right_axis_names = None
    backend = node.backend
    transp_tensor = node.tensor_from_edge_order(left_edges + right_edges)
    (u, s, vh, trun_vals) = backend.svd(transp_tensor, len(left_edges), max_singular_values, max_truncation_err, relative=relative)
    sqrt_s = backend.sqrt(s)
    u_s = backend.broadcast_right_multiplication(u, sqrt_s)
    vh_s = backend.broadcast_left_multiplication(sqrt_s, vh)
    left_node = Node(u_s, name=left_name, axis_names=left_axis_names, backend=backend)
    left_axes_order = [edge.axis1 if edge.node1 is node else edge.axis2 for edge in left_edges]
    for (i, edge) in enumerate(left_edges):
        left_node.add_edge(edge, i)
        edge.update_axis(left_axes_order[i], node, i, left_node)
    right_node = Node(vh_s, name=right_name, axis_names=right_axis_names, backend=backend)
    right_axes_order = [edge.axis1 if edge.node1 is node else edge.axis2 for edge in right_edges]
    for (i, edge) in enumerate(right_edges):
        right_node.add_edge(edge, i + 1)
        edge.update_axis(right_axes_order[i], node, i + 1, right_node)
    connect(left_node.edges[-1], right_node.edges[0], name=edge_name)
    node.fresh_edges(node.axis_names)
    return (left_node, right_node, trun_vals)","for edge in right_edges:
    right_axis_names.append(node.axis_names[edge.axis1] if edge.node1 is node else node.axis_names[edge.axis2])",right_axis_names = [node.axis_names[edge.axis1] if edge.node1 is node else node.axis_names[edge.axis2] for edge in right_edges],Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
package_control,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/package_control/dev/deps.py,https://github.com/wbond/package_control/tree/master/dev/deps.py,,_pep425tags$813,"def _pep425tags():
    """"""
    :return:
        A list of 3-element tuples with unicode strings or None:
         [0] implementation tag - cp33, pp27, cp26, py2, py2.py3
         [1] abi tag - cp26m, None
         [2] arch tag - linux_x86_64, macosx_10_10_x85_64, etc
    """"""
    tags = []
    versions = []
    version_info = _pep425_version()
    major = version_info[:-1]
    for minor in range(version_info[-1], -1, -1):
        versions.append(''.join(map(str, major + (minor,))))
    impl = _pep425_implementation()
    abis = []
    abi = _pep425_get_abi()
    if abi:
        abis.append(abi)
    abi3 = _pep425_implementation() == 'cp' and sys.version_info >= (3,)
    if abi3:
        abis.append('abi3')
    abis.append('none')
    if sys.platform == 'darwin':
        plat_ver = platform.mac_ver()
        ver_parts = plat_ver[0].split('.')
        minor = int(ver_parts[1])
        arch = plat_ver[2]
        if sys.maxsize == 2147483647:
            arch = 'i386'
        arches = []
        while minor > 5:
            arches.append('macosx_10_%s_%s' % (minor, arch))
            arches.append('macosx_10_%s_intel' % (minor,))
            arches.append('macosx_10_%s_universal' % (minor,))
            minor -= 1
    elif sys.platform == 'win32':
        if 'amd64' in sys.version.lower():
            arches = ['win_amd64']
        else:
            arches = [sys.platform]
    elif hasattr(os, 'uname'):
        (plat, _, _, _, machine) = os.uname()
        plat = plat.lower().replace('/', '')
        machine.replace(' ', '_').replace('/', '_')
        if plat == 'linux' and sys.maxsize == 2147483647 and ('arm' not in machine):
            machine = 'i686'
        arch = '%s_%s' % (plat, machine)
        if _pep425_supports_manylinux():
            arches = [arch.replace('linux', 'manylinux1'), arch]
        else:
            arches = [arch]
    for abi in abis:
        for arch in arches:
            tags.append(('%s%s' % (impl, versions[0]), abi, arch))
    if abi3:
        for version in versions[1:]:
            for arch in arches:
                tags.append(('%s%s' % (impl, version), 'abi3', arch))
    for arch in arches:
        tags.append(('py%s' % versions[0][0], 'none', arch))
    tags.append(('%s%s' % (impl, versions[0]), 'none', 'any'))
    tags.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))
    for (i, version) in enumerate(versions):
        tags.append(('py%s' % (version,), 'none', 'any'))
        if i == 0:
            tags.append(('py%s' % version[0], 'none', 'any'))
    tags.append(('py2.py3', 'none', 'any'))
    return tags","for (i, version) in enumerate(versions):
    tags.append(('py%s' % (version,), 'none', 'any'))
    if i == 0:
        tags.append(('py%s' % version[0], 'none', 'any'))","tags += [('py%s' % (version,), 'none', 'any') if i != 0 else ('py%s' % version[0], 'none', 'any') for (i, version) in enumerate(versions)]",Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
yt-dlp,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yt-dlp/yt_dlp/extractor/acast.py,https://github.com/yt-dlp/yt-dlp/tree/master/yt_dlp/extractor/acast.py,ACastChannelIE,_real_extract$117,"def _real_extract(self, url):
    show_slug = self._match_id(url)
    show = self._call_api(show_slug, show_slug)
    show_info = self._extract_show_info(show)
    entries = []
    for episode in show.get('episodes') or []:
        entries.append(self._extract_episode(episode, show_info))
    return self.playlist_result(entries, show.get('id'), show.get('title'), show.get('description'))","for episode in show.get('episodes') or []:
    entries.append(self._extract_episode(episode, show_info))","entries = [self._extract_episode(episode, show_info) for episode in show.get('episodes', [])]","entries = [self._extract_episode(episode, show_info) for episode in show.get('episodes') or []]",0,0,1,1,,robosuite,,,,,
pandera,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pandera/pandera/strategies.py,https://github.com/pandera-dev/pandera/tree/master/pandera/strategies.py,,dataframe_strategy$933,"def dataframe_strategy(pandera_dtype: Optional[DataType]=None, strategy: Optional[SearchStrategy]=None, *, columns: Optional[Dict]=None, checks: Optional[Sequence]=None, unique: Optional[List[str]]=None, index: Optional[IndexComponent]=None, size: Optional[int]=None, n_regex_columns: int=1):
    """"""Strategy to generate a pandas DataFrame.

    :param pandera_dtype: :class:`pandera.dtypes.DataType` instance.
    :param strategy: if specified, this will raise a BaseStrategyOnlyError,
        since it cannot be chained to a prior strategy.
    :param columns: a dictionary where keys are column names and values
        are :class:`~pandera.schema_components.Column` objects.
    :param checks: sequence of :class:`~pandera.checks.Check` s to constrain
        the values of the data at the dataframe level.
    :param unique: a list of column names that should be jointly unique.
    :param index: Index or MultiIndex schema component.
    :param size: number of elements in the Series.
    :param n_regex_columns: number of regex columns to generate.
    :returns: ``hypothesis`` strategy.
    """"""
    if n_regex_columns < 1:
        raise ValueError(f'`n_regex_columns` must be a positive integer, found: {n_regex_columns}')
    if strategy:
        raise BaseStrategyOnlyError('The dataframe strategy is a base strategy. You cannot specify the strategy argument to chain it to a parent strategy.')
    columns = {} if columns is None else columns
    checks = [] if checks is None else checks

    def undefined_check_strategy(strategy, check, column=None):
        """"""Strategy for checks with undefined strategies.""""""

        def _element_wise_check_fn(element):
            return check._check_fn(element)

        def _column_check_fn(dataframe):
            return check(dataframe[column]).check_passed

        def _dataframe_check_fn(dataframe):
            return check(dataframe).check_passed
        if check.element_wise:
            check_fn = _element_wise_check_fn
            warning_type = 'Element-wise'
        elif column is None:
            check_fn = _dataframe_check_fn
            warning_type = 'Dataframe'
        else:
            check_fn = _column_check_fn
            warning_type = 'Column'
        warnings.warn(f""{warning_type} check doesn't have a defined strategy. Falling back to filtering drawn values based on the check definition. This can considerably slow down data-generation."")
        return strategy.filter(check_fn)

    def make_row_strategy(col, checks):
        strategy = None
        for check in checks:
            if hasattr(check, 'strategy'):
                strategy = check.strategy(col.dtype, strategy)
            else:
                strategy = undefined_check_strategy(strategy=pandas_dtype_strategy(col.dtype) if strategy is None else strategy, check=check)
        if strategy is None:
            strategy = pandas_dtype_strategy(col.dtype)
        return strategy

    @composite
    def _dataframe_strategy(draw):
        row_strategy_checks = []
        undefined_strat_df_checks = []
        for check in checks:
            if hasattr(check, 'strategy') or check.element_wise:
                row_strategy_checks.append(check)
            else:
                undefined_strat_df_checks.append(check)
        expanded_columns = {}
        for (col_name, column) in columns.items():
            if unique and col_name in unique:
                column = deepcopy(column)
                column.unique = True
            if not column.regex:
                expanded_columns[col_name] = column
            else:
                regex_columns = draw(st.lists(st.from_regex(column.name, fullmatch=True), min_size=n_regex_columns, max_size=n_regex_columns, unique=True))
                for regex_col in regex_columns:
                    expanded_columns[regex_col] = deepcopy(column).set_name(regex_col)
        undefined_strat_column_checks: Dict[str, list] = defaultdict(list)
        for (col_name, column) in expanded_columns.items():
            undefined_strat_column_checks[col_name].extend((check for check in column.checks if not hasattr(check, 'strategy') and (not check.element_wise)))
        col_dtypes = {col_name: str(col.dtype) if pandera_dtype is None else str(pandera_dtype) for (col_name, col) in expanded_columns.items()}
        nullable_columns = {col_name: col.nullable for (col_name, col) in expanded_columns.items()}
        row_strategy = None
        if row_strategy_checks:
            row_strategy = st.fixed_dictionaries({col_name: make_row_strategy(col, row_strategy_checks) for (col_name, col) in expanded_columns.items()})
        strategy = pdst.data_frames(columns=[column.strategy_component() for column in expanded_columns.values()], rows=row_strategy, index=pdst.range_indexes(min_size=0 if size is None else size, max_size=size))
        string_columns = []
        for (col_name, col_dtype) in col_dtypes.items():
            if col_dtype in {'object', 'str'} or col_dtype.startswith('string'):
                string_columns.append(col_name)
        if string_columns:
            strategy = strategy.map(lambda df: df.assign(**{col_name: df[col_name].map(str) for col_name in string_columns}))
        strategy = strategy.map(lambda df: df if df.empty else df.astype(col_dtypes))
        if size is not None and size > 0 and any(nullable_columns.values()):
            strategy = null_dataframe_masks(strategy, nullable_columns)
        if index is not None:
            strategy = set_pandas_index(strategy, index)
        for check in undefined_strat_df_checks:
            strategy = undefined_check_strategy(strategy, check)
        for (col_name, column_checks) in undefined_strat_column_checks.items():
            for check in column_checks:
                strategy = undefined_check_strategy(strategy, check, column=col_name)
        return draw(strategy)
    return _dataframe_strategy()","for check in checks:
    if hasattr(check, 'strategy') or check.element_wise:
        row_strategy_checks.append(check)
    else:
        undefined_strat_df_checks.append(check)","undefined_strat_df_checks = [check for check in checks if not hasattr(check, 'strategy') and (not check.element_wise)]",Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
Wudao-dict,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Wudao-dict/soup/parse_21c.py,https://github.com/ChestnutHeng/Wudao-dict/tree/master/soup/parse_21c.py,,get_text$31,"def get_text(word):
    content = get_html(word)
    word_struct = {'word': word}
    root = bs4.BeautifulSoup(content, 'lxml')
    pron = {}
    pron_fallback = False
    for pron_item in ss(root, '.pronounce'):
        pron_lang = None
        pron_phonetic = None
        for sub_item in pron_item.children:
            if isinstance(sub_item, str) and pron_lang is None:
                pron_lang = sub_item
                continue
            if isinstance(sub_item, bs4.Tag) and sub_item.name.lower() == 'span' and sub_item.has_attr('class') and ('phonetic' in sub_item.get('class')):
                pron_phonetic = sub_item
                continue
        if pron_phonetic is None:
            pron_fallback = True
            break
        if pron_lang is None:
            pron_lang = ''
        pron_lang = pron_lang.strip()
        pron_phonetic = pron_phonetic.text
        pron[pron_lang] = pron_phonetic
    if pron_fallback:
        for item in ss(root, '.phonetic'):
            if item.name.lower() == 'span':
                pron[''] = item.text
                break
    word_struct['pronunciation'] = pron
    if pron_fallback:
        d = root.select('.wordGroup.def')
        p = root.select('.wordGroup.pos')
        ds = ''
        dp = ''
        if len(d) > 0:
            ds = d[0].text.strip()
        if len(p) > 0:
            dp = p[0].text.strip()
        word_struct['paraphrase'] = (dp + ' ' + ds).strip()
    else:
        nodes = ss(root, '#phrsListTab .trans-container ul')
        basic_desc = []
        if len(nodes) != 0:
            ul = nodes[0]
            for li in ul.children:
                if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
                    continue
                basic_desc.append(li.text.strip())
        word_struct['paraphrase'] = basic_desc
    rank = ''
    nodes = ss(root, '.rank')
    if len(nodes) != 0:
        rank = nodes[0].text.strip()
    word_struct['rank'] = rank
    pattern = ''
    nodes = ss(root, '.collinsToggle .pattern')
    if len(nodes) != 0:
        pattern = multi_space_to_single(nodes[0].text.strip())
    word_struct['pattern'] = pattern
    word_struct['sentence'] = []
    for child in ss(root, '.collinsToggle .ol li'):
        p = ss(child, 'p')
        if len(p) == 0:
            continue
        p = p[0]
        desc = ''
        cx = ''
        for node in p.children:
            if isinstance(node, str):
                desc += node
            if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
                cx = node.text
        desc = multi_space_to_single(desc.strip())
        examples = []
        for el in ss(child, '.exampleLists'):
            examp = []
            for p in ss(el, '.examples p'):
                examp.append(p.text.strip())
            examples.append(examp)
        word_struct['sentence'].append([desc, cx, examples])
    if not word_struct['sentence']:
        for v in root.select('#bilingual ul li'):
            p = ss(v, 'p')
            ll = []
            for p in ss(v, 'p'):
                if len(p) == 0:
                    continue
                if 'class' not in p.attrs:
                    ll.append(p.text.strip())
            if len(ll) != 0:
                word_struct['sentence'].append(ll)
    return word_struct","for child in ss(root, '.collinsToggle .ol li'):
    p = ss(child, 'p')
    if len(p) == 0:
        continue
    p = p[0]
    desc = ''
    cx = ''
    for node in p.children:
        if isinstance(node, str):
            desc += node
        if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
            cx = node.text
    desc = multi_space_to_single(desc.strip())
    examples = []
    for el in ss(child, '.exampleLists'):
        examp = []
        for p in ss(el, '.examples p'):
            examp.append(p.text.strip())
        examples.append(examp)
    word_struct['sentence'].append([desc, cx, examples])","word_struct['sentence'] += [[multi_space_to_single(node.strip()) for node in p.children if isinstance(node, str)] + [node.text for node in ss(p, 'span')] + [[p.text.strip() for p in ss(el, '.examples p')] for el in ss(child, '.exampleLists')] for child in ss(root, '.collinsToggle .ol li') if len(ss(child, 'p')) > 0]",Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
mmdetection,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection/mmdet/models/dense_heads/autoassign_head.py,https://github.com/open-mmlab/mmdetection/tree/master/mmdet/models/dense_heads/autoassign_head.py,AutoAssignHead,loss$307,"def loss(self, cls_scores, bbox_preds, objectnesses, gt_bboxes, gt_labels, img_metas, gt_bboxes_ignore=None):
    """"""Compute loss of the head.

        Args:
            cls_scores (list[Tensor]): Box scores for each scale level,
                each is a 4D-tensor, the channel number is
                num_points * num_classes.
            bbox_preds (list[Tensor]): Box energies / deltas for each scale
                level, each is a 4D-tensor, the channel number is
                num_points * 4.
            objectnesses (list[Tensor]): objectness for each scale level, each
                is a 4D-tensor, the channel number is num_points * 1.
            gt_bboxes (list[Tensor]): Ground truth bboxes for each image with
                shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.
            gt_labels (list[Tensor]): class indices corresponding to each box
            img_metas (list[dict]): Meta information of each image, e.g.,
                image size, scaling factor, etc.
            gt_bboxes_ignore (None | list[Tensor]): specify which bounding
                boxes can be ignored when computing the loss.

        Returns:
            dict[str, Tensor]: A dictionary of loss components.
        """"""
    assert len(cls_scores) == len(bbox_preds) == len(objectnesses)
    all_num_gt = sum([len(item) for item in gt_bboxes])
    featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]
    all_level_points = self.prior_generator.grid_priors(featmap_sizes, dtype=bbox_preds[0].dtype, device=bbox_preds[0].device)
    (inside_gt_bbox_mask_list, bbox_targets_list) = self.get_targets(all_level_points, gt_bboxes)
    center_prior_weight_list = []
    temp_inside_gt_bbox_mask_list = []
    for (gt_bboxe, gt_label, inside_gt_bbox_mask) in zip(gt_bboxes, gt_labels, inside_gt_bbox_mask_list):
        (center_prior_weight, inside_gt_bbox_mask) = self.center_prior(all_level_points, gt_bboxe, gt_label, inside_gt_bbox_mask)
        center_prior_weight_list.append(center_prior_weight)
        temp_inside_gt_bbox_mask_list.append(inside_gt_bbox_mask)
    inside_gt_bbox_mask_list = temp_inside_gt_bbox_mask_list
    mlvl_points = torch.cat(all_level_points, dim=0)
    bbox_preds = levels_to_images(bbox_preds)
    cls_scores = levels_to_images(cls_scores)
    objectnesses = levels_to_images(objectnesses)
    reg_loss_list = []
    ious_list = []
    num_points = len(mlvl_points)
    for (bbox_pred, encoded_targets, inside_gt_bbox_mask) in zip(bbox_preds, bbox_targets_list, inside_gt_bbox_mask_list):
        temp_num_gt = encoded_targets.size(1)
        expand_mlvl_points = mlvl_points[:, None, :].expand(num_points, temp_num_gt, 2).reshape(-1, 2)
        encoded_targets = encoded_targets.reshape(-1, 4)
        expand_bbox_pred = bbox_pred[:, None, :].expand(num_points, temp_num_gt, 4).reshape(-1, 4)
        decoded_bbox_preds = self.bbox_coder.decode(expand_mlvl_points, expand_bbox_pred)
        decoded_target_preds = self.bbox_coder.decode(expand_mlvl_points, encoded_targets)
        with torch.no_grad():
            ious = bbox_overlaps(decoded_bbox_preds, decoded_target_preds, is_aligned=True)
            ious = ious.reshape(num_points, temp_num_gt)
            if temp_num_gt:
                ious = ious.max(dim=-1, keepdim=True).values.repeat(1, temp_num_gt)
            else:
                ious = ious.new_zeros(num_points, temp_num_gt)
            ious[~inside_gt_bbox_mask] = 0
            ious_list.append(ious)
        loss_bbox = self.loss_bbox(decoded_bbox_preds, decoded_target_preds, weight=None, reduction_override='none')
        reg_loss_list.append(loss_bbox.reshape(num_points, temp_num_gt))
    cls_scores = [item.sigmoid() for item in cls_scores]
    objectnesses = [item.sigmoid() for item in objectnesses]
    (pos_loss_list,) = multi_apply(self.get_pos_loss_single, cls_scores, objectnesses, reg_loss_list, gt_labels, center_prior_weight_list)
    pos_avg_factor = reduce_mean(bbox_pred.new_tensor(all_num_gt)).clamp_(min=1)
    pos_loss = sum(pos_loss_list) / pos_avg_factor
    (neg_loss_list,) = multi_apply(self.get_neg_loss_single, cls_scores, objectnesses, gt_labels, ious_list, inside_gt_bbox_mask_list)
    neg_avg_factor = sum((item.data.sum() for item in center_prior_weight_list))
    neg_avg_factor = reduce_mean(neg_avg_factor).clamp_(min=1)
    neg_loss = sum(neg_loss_list) / neg_avg_factor
    center_loss = []
    for i in range(len(img_metas)):
        if inside_gt_bbox_mask_list[i].any():
            center_loss.append(len(gt_bboxes[i]) / center_prior_weight_list[i].sum().clamp_(min=EPS))
        else:
            center_loss.append(center_prior_weight_list[i].sum() * 0)
    center_loss = torch.stack(center_loss).mean() * self.center_loss_weight
    if all_num_gt == 0:
        pos_loss = bbox_preds[0].sum() * 0
        dummy_center_prior_loss = self.center_prior.mean.sum() * 0 + self.center_prior.sigma.sum() * 0
        center_loss = objectnesses[0].sum() * 0 + dummy_center_prior_loss
    loss = dict(loss_pos=pos_loss, loss_neg=neg_loss, loss_center=center_loss)
    return loss","for (gt_bboxe, gt_label, inside_gt_bbox_mask) in zip(gt_bboxes, gt_labels, inside_gt_bbox_mask_list):
    (center_prior_weight, inside_gt_bbox_mask) = self.center_prior(all_level_points, gt_bboxe, gt_label, inside_gt_bbox_mask)
    center_prior_weight_list.append(center_prior_weight)
    temp_inside_gt_bbox_mask_list.append(inside_gt_bbox_mask)","temp_inside_gt_bbox_mask_list = [self.center_prior(all_level_points, gt_bboxe, gt_label, inside_gt_bbox_mask)[1] for (gt_bboxe, gt_label, inside_gt_bbox_mask) in zip(gt_bboxes, gt_labels, inside_gt_bbox_mask_list)]",Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
bubbles,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/bubbles/bubbles/backends/sql/ops.py,https://github.com/Stiivi/bubbles/tree/master/bubbles/backends/sql/ops.py,,_$435,"def _(ctx, master, details, joins):
    """"""Creates left inner master-detail join (star schema) where `master` is an
    iterator if the ""bigger"" table `details` are details. `joins` is a list of
    tuples `(master, detail)` where the master is index of master key and
    detail is index of detail key to be matched.

    If `inner` is `True` then inner join is performed. That means that only
    rows from master that have corresponding details are returned.

    .. warning::

        all detail iterators are consumed and result is held in memory. Do not
        use for large datasets.
    """"""
    if not details:
        raise ArgumentError('No details provided, nothing to join')
    if not joins:
        raise ArgumentError('No joins specified')
    if len(details) != len(joins):
        raise ArgumentError('For every detail there should be a join (%d:%d).' % (len(details), len(joins)))
    if not all((master.can_compose(detail) for detail in details)):
        raise RetryOperation(['rows', 'rows[]'], reason='Can not compose')
    out_fields = master.fields.clone()
    master_stmt = master.sql_statement().alias('master')
    selection = list(master_stmt.columns)
    joined = master_stmt
    i = 0
    for (detail, join) in zip(details, joins):
        alias = 'detail%s' % i
        det_stmt = detail.sql_statement().alias(alias)
        master_key = join['master']
        detail_key = join['detail']
        onclause = master_stmt.c[master_key] == det_stmt.c[detail_key]
        for (field, col) in zip(detail.fields, det_stmt.columns):
            if str(field) != str(detail_key):
                selection.append(col)
                out_fields.append(field.clone())
        joined = sql.expression.join(joined, det_stmt, onclause=onclause)
    aliased = []
    for (col, field) in zip(selection, out_fields):
        aliased.append(col.label(field.name))
    select = sql.expression.select(aliased, from_obj=joined, use_labels=True)
    return master.clone_statement(statement=select, fields=out_fields)","for (field, col) in zip(detail.fields, det_stmt.columns):
    if str(field) != str(detail_key):
        selection.append(col)
        out_fields.append(field.clone())","out_fields = [field.clone() for (field, col) in zip(detail.fields, det_stmt.columns) if str(field) != str(detail_key)]",Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
openstates-scrapers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/openstates-scrapers/scrapers_next/md/people.py,https://github.com/openstates/openstates-scrapers/tree/master/scrapers_next/md/people.py,PersonDetail,parse_address_block$7,"def parse_address_block(self, block):
    state = 'address'
    values = {'address': [], 'phone': [], 'fax': []}
    for line in block.splitlines():
        line = line.strip()
        if not line:
            continue
        if line.startswith('Phone'):
            state = 'phone'
        elif line.startswith('Fax'):
            state = 'fax'
        values[state].append(line)
    phones = []
    for line in values['phone']:
        for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line):
            phones.append(match)
    faxes = []
    for line in values['fax']:
        for match in re.findall('\\d{3}-\\d{3}-\\d{4}', line):
            faxes.append(match)
    return {'address': '; '.join(values['address']), 'phones': phones, 'faxes': faxes}","for line in block.splitlines():
    line = line.strip()
    if not line:
        continue
    if line.startswith('Phone'):
        state = 'phone'
    elif line.startswith('Fax'):
        state = 'fax'
    values[state].append(line)",values[state] = [line for line in block.splitlines() if line.strip() and (state := ('phone' if line.startswith('Phone') else 'fax' if line.startswith('Fax') else True))],Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
shutit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shutit/shutit_class.py,https://github.com/ianmiell/shutit/tree/master//shutit_class.py,ShutIt,do_list_modules$3248,"def do_list_modules(self, long_output=None, sort_order=None):
    """"""Display a list of loaded modules.

		Config items:
			- shutit.list_modules['long']
			  If set, also print each module's run order value

			- shutit.list_modules['sort']
			  Select the column by which the list is ordered:
				- id: sort the list by module id
				- run_order: sort the list by module run order

		The output is also saved to ['build']['log_config_path']/module_order.txt

		Dependencies: operator
		""""""
    shutit_global.shutit_global_object.yield_to_draw()
    cfg = self.cfg
    table_list = []
    if long_output is None:
        long_output = self.list_modules['long']
    if sort_order is None:
        sort_order = self.list_modules['sort']
    if long_output:
        table_list.append(['Order', 'Module ID', 'Description', 'Run Order', 'Built', 'Compatible'])
    else:
        table_list.append(['Module ID', 'Description', 'Built', 'Compatible'])
    if sort_order == 'run_order':
        d = {}
        for m in self.shutit_modules:
            d.update({m.module_id: m.run_order})
        b = sorted(d.items(), key=operator.itemgetter(1))
        count = 0
        for pair in b:
            k = pair[0]
            for m in self.shutit_modules:
                if m.module_id == k:
                    count += 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                        cfg[m.module_id]['shutit.core.module.build'] = False
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    elif sort_order == 'id':
        l = []
        for m in self.shutit_modules:
            l.append(m.module_id)
        l.sort()
        for k in l:
            for m in self.shutit_modules:
                if m.module_id == k:
                    count = 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    table = texttable.Texttable()
    table.add_rows(table_list)
    colwidths = []
    for item in table_list:
        for n in range(0, len(item)):
            colwidths.append(10)
        break
    for item in table_list:
        for n in range(0, len(item) - 1):
            if len(str(item[n])) > colwidths[n]:
                colwidths[n] = len(str(item[n]))
    table.set_cols_width(colwidths)
    msg = table.draw()
    shutit_global.shutit_global_object.shutit_print('\n' + msg)","for pair in b:
    k = pair[0]
    for m in self.shutit_modules:
        if m.module_id == k:
            count += 1
            compatible = True
            if not cfg[m.module_id]['shutit.core.module.build']:
                cfg[m.module_id]['shutit.core.module.build'] = True
                compatible = self.determine_compatibility(m.module_id) == 0
                cfg[m.module_id]['shutit.core.module.build'] = False
            if long_output:
                table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
            else:
                table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])","table_list += [[str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(self.determine_compatibility(m.module_id) == 0)] if long_output else [m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(self.determine_compatibility(m.module_id) == 0)] for pair in b for m in self.shutit_modules if m.module_id == pair[0] and (count := (count + 1))]",Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
shutit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shutit/shutit_class.py,https://github.com/ianmiell/shutit/tree/master//shutit_class.py,ShutIt,do_list_modules$3248,"def do_list_modules(self, long_output=None, sort_order=None):
    """"""Display a list of loaded modules.

		Config items:
			- shutit.list_modules['long']
			  If set, also print each module's run order value

			- shutit.list_modules['sort']
			  Select the column by which the list is ordered:
				- id: sort the list by module id
				- run_order: sort the list by module run order

		The output is also saved to ['build']['log_config_path']/module_order.txt

		Dependencies: operator
		""""""
    shutit_global.shutit_global_object.yield_to_draw()
    cfg = self.cfg
    table_list = []
    if long_output is None:
        long_output = self.list_modules['long']
    if sort_order is None:
        sort_order = self.list_modules['sort']
    if long_output:
        table_list.append(['Order', 'Module ID', 'Description', 'Run Order', 'Built', 'Compatible'])
    else:
        table_list.append(['Module ID', 'Description', 'Built', 'Compatible'])
    if sort_order == 'run_order':
        d = {}
        for m in self.shutit_modules:
            d.update({m.module_id: m.run_order})
        b = sorted(d.items(), key=operator.itemgetter(1))
        count = 0
        for pair in b:
            k = pair[0]
            for m in self.shutit_modules:
                if m.module_id == k:
                    count += 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                        cfg[m.module_id]['shutit.core.module.build'] = False
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    elif sort_order == 'id':
        l = []
        for m in self.shutit_modules:
            l.append(m.module_id)
        l.sort()
        for k in l:
            for m in self.shutit_modules:
                if m.module_id == k:
                    count = 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    table = texttable.Texttable()
    table.add_rows(table_list)
    colwidths = []
    for item in table_list:
        for n in range(0, len(item)):
            colwidths.append(10)
        break
    for item in table_list:
        for n in range(0, len(item) - 1):
            if len(str(item[n])) > colwidths[n]:
                colwidths[n] = len(str(item[n]))
    table.set_cols_width(colwidths)
    msg = table.draw()
    shutit_global.shutit_global_object.shutit_print('\n' + msg)","for k in l:
    for m in self.shutit_modules:
        if m.module_id == k:
            count = 1
            compatible = True
            if not cfg[m.module_id]['shutit.core.module.build']:
                cfg[m.module_id]['shutit.core.module.build'] = True
                compatible = self.determine_compatibility(m.module_id) == 0
            if long_output:
                table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
            else:
                table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])","table_list += [[str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(self.determine_compatibility(m.module_id) == 0)] if not cfg[m.module_id]['shutit.core.module.build'] else [m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(self.determine_compatibility(m.module_id) == 0)] for (count, k) in enumerate(l) for m in self.shutit_modules if m.module_id == k]",Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
DeepLabCut,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DeepLabCut/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py,https://github.com/DeepLabCut/DeepLabCut/tree/master/deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py,SORTBox,match_detections_to_trackers$708,"def match_detections_to_trackers(detections, trackers, iou_threshold):
    """"""
        Assigns detections to tracked object (both represented as bounding boxes)

        Returns 3 lists of matches, unmatched_detections and unmatched_trackers
        """"""
    if not len(trackers):
        return (np.empty((0, 2), dtype=int), np.arange(len(detections)), np.empty((0, 5), dtype=int))
    iou_matrix = np.zeros((len(detections), len(trackers)), dtype=np.float32)
    for (d, det) in enumerate(detections):
        for (t, trk) in enumerate(trackers):
            iou_matrix[d, t] = calc_iou(det, trk)
    (row_indices, col_indices) = linear_sum_assignment(-iou_matrix)
    unmatched_detections = []
    for (d, det) in enumerate(detections):
        if d not in row_indices:
            unmatched_detections.append(d)
    unmatched_trackers = []
    for (t, trk) in enumerate(trackers):
        if t not in col_indices:
            unmatched_trackers.append(t)
    matches = []
    for (row, col) in zip(row_indices, col_indices):
        if iou_matrix[row, col] < iou_threshold:
            unmatched_detections.append(row)
            unmatched_trackers.append(col)
        else:
            matches.append([row, col])
    if not len(matches):
        matches = np.empty((0, 2), dtype=int)
    else:
        matches = np.stack(matches)
    return (matches, np.array(unmatched_detections), np.array(unmatched_trackers))","for (row, col) in zip(row_indices, col_indices):
    if iou_matrix[row, col] < iou_threshold:
        unmatched_detections.append(row)
        unmatched_trackers.append(col)
    else:
        matches.append([row, col])","unmatched_trackers = [col for (row, col) in zip(row_indices, col_indices) if iou_matrix[row, col] < iou_threshold]",Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_shed/galaxy_install/tools/tool_panel_manager.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_shed/galaxy_install/tools/tool_panel_manager.py,ToolPanelManager,add_to_shed_tool_config$25,"def add_to_shed_tool_config(self, shed_tool_conf_dict, elem_list):
    """"""
        ""A tool shed repository is being installed so change the shed_tool_conf file.  Parse the
        config file to generate the entire list of config_elems instead of using the in-memory list
        since it will be a subset of the entire list if one or more repositories have been deactivated.
        """"""
    if not elem_list:
        return
    old_toolbox = self.app.toolbox
    shed_tool_conf = shed_tool_conf_dict['config_filename']
    tool_cache_data_dir = shed_tool_conf_dict.get('tool_cache_data_dir')
    tool_path = shed_tool_conf_dict['tool_path']
    config_elems = []
    try:
        (tree, error_message) = parse_xml(shed_tool_conf, check_exists=False)
    except OSError as exc:
        if exc.errno == errno.ENOENT and shed_tool_conf_dict.get('create', None) is not None:
            log.info('Creating shed tool config with default contents: %s', shed_tool_conf)
            with open(shed_tool_conf, 'w') as fh:
                fh.write(shed_tool_conf_dict['create'])
            (tree, error_message) = parse_xml(shed_tool_conf)
        else:
            log.error('Unable to load shed tool config: %s', shed_tool_conf)
            raise
    if tree:
        root = tree.getroot()
        for elem in root:
            config_elems.append(elem)
        for elem_entry in elem_list:
            if elem_entry.tag == 'section':
                for existing_elem in config_elems:
                    if existing_elem.tag == 'section' and existing_elem.attrib.get('id', None) == elem_entry.attrib.get('id', None):
                        for child in elem_entry:
                            existing_elem.append(child)
                        break
                else:
                    config_elems.append(elem_entry)
            else:
                config_elems.append(elem_entry)
        self.config_elems_to_xml_file(config_elems, shed_tool_conf, tool_path, tool_cache_data_dir)
        self.app.wait_for_toolbox_reload(old_toolbox)
    else:
        log.error(error_message)","for elem_entry in elem_list:
    if elem_entry.tag == 'section':
        for existing_elem in config_elems:
            if existing_elem.tag == 'section' and existing_elem.attrib.get('id', None) == elem_entry.attrib.get('id', None):
                for child in elem_entry:
                    existing_elem.append(child)
                break
        else:
            config_elems.append(elem_entry)
    else:
        config_elems.append(elem_entry)","config_elems += [existing_elem.append(child) if existing_elem.tag == 'section' and existing_elem.attrib.get('id', None) == elem_entry.attrib.get('id', None) else config_elems.append(elem_entry) if elem_entry.tag == 'section' else config_elems.append(elem_entry) for elem_entry in elem_list for existing_elem in config_elems if elem_entry.tag == 'section' and existing_elem.tag == 'section' and (existing_elem.attrib.get('id', None) == elem_entry.attrib.get('id', None))]",Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
flexx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/flexx/flexx/ui/layouts/_hv.py,https://github.com/flexxui/flexx/tree/master/flexx/ui/layouts/_hv.py,HVLayout,set_from_flex_values$327,"def set_from_flex_values(self):
    """""" Set the divider positions corresponding to the children's flex values.
        Only has a visual effect in split-mode.
        """"""
    sizes = []
    dim = 0 if 'h' in self.orientation else 1
    for widget in self.children:
        sizes.append(widget.flex[dim])
    size_sum = 0 if len(sizes) == 0 else sum(sizes)
    if size_sum == 0:
        sizes = [1 / len(sizes) for i in sizes]
    else:
        sizes = [i / size_sum for i in sizes]
    positions = []
    pos = 0
    for i in range(len(sizes) - 1):
        pos = pos + sizes[i]
        positions.append(pos)
    self._mutate_splitter_positions(positions)","for i in range(len(sizes) - 1):
    pos = pos + sizes[i]
    positions.append(pos)",positions = [pos + sizes[i] for i in range(len(sizes) - 1)],Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
horovod,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/horovod/horovod/ray/strategy.py,https://github.com/horovod/horovod/tree/master/horovod/ray/strategy.py,ColocatedStrategy,create_workers$88,"def create_workers(self):
    (self.placement_group, bundles) = create_placement_group(resources_per_bundle=self._resources_per_host(), num_bundles=self.num_hosts, pg_timeout=self.settings.placement_group_timeout_s, pg_strategy='STRICT_SPREAD')
    self.workers = []
    for bundle_index in range(len(bundles)):
        gpu_id_futures = []
        curr_node_workers = []
        remote_cls = ray.remote(BaseHorovodWorker)
        for i in range(self.num_workers_per_host):
            remote_cls_with_options = remote_cls.options(num_cpus=self.cpus_per_worker, num_gpus=self.gpus_per_worker * int(self.use_gpu), placement_group_capture_child_tasks=False, placement_group=self.placement_group, placement_group_bundle_index=bundle_index)
            worker = remote_cls_with_options.remote(world_rank=self.num_workers_per_host * bundle_index + i, world_size=self.num_workers)
            if self.use_gpu:
                gpu_id_futures.append(worker.get_gpu_ids.remote())
            self.workers.append(worker)
            curr_node_workers.append(worker)
        if len(gpu_id_futures) > 0:
            gpu_ids = sum(ray.get(gpu_id_futures), [])
            assert len(gpu_ids) == len(set(gpu_ids)) == self.num_workers_per_host, gpu_ids
            all_ids = ','.join([str(gpu_id) for gpu_id in gpu_ids])
            futures = []
            for worker in curr_node_workers:
                futures.append(worker.update_env_vars.remote({'CUDA_VISIBLE_DEVICES': all_ids}))
            ray.get(futures)
    return (self.workers, self.get_node_workers(self.workers))","for i in range(self.num_workers_per_host):
    remote_cls_with_options = remote_cls.options(num_cpus=self.cpus_per_worker, num_gpus=self.gpus_per_worker * int(self.use_gpu), placement_group_capture_child_tasks=False, placement_group=self.placement_group, placement_group_bundle_index=bundle_index)
    worker = remote_cls_with_options.remote(world_rank=self.num_workers_per_host * bundle_index + i, world_size=self.num_workers)
    if self.use_gpu:
        gpu_id_futures.append(worker.get_gpu_ids.remote())
    self.workers.append(worker)
    curr_node_workers.append(worker)","curr_node_workers = [self.workers.append(remote_cls.options(num_cpus=self.cpus_per_worker, num_gpus=self.gpus_per_worker * int(self.use_gpu), placement_group_capture_child_tasks=False, placement_group=self.placement_group, placement_group_bundle_index=bundle_index).remote(world_rank=self.num_workers_per_host * bundle_index + i, world_size=self.num_workers)) and (worker.get_gpu_ids.remote() if self.use_gpu else None) for i in range(self.num_workers_per_host)]",Cannot refactor,-1,,,,,robosuite,,,,,it actually cannot refactor
deluge,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/deluge/deluge/path_chooser_common.py,https://github.com/deluge-torrent/deluge/tree/master/deluge/path_chooser_common.py,,get_completion_paths$38,"def get_completion_paths(args):
    """"""
    Takes a path value and returns the available completions.
    If the path_value is a valid path, return all sub-directories.
    If the path_value is not a valid path, remove the basename from the
    path and return all sub-directories of path that start with basename.

    :param args: options
    :type args: dict
    :returns: the args argument containing the available completions for the completion_text
    :rtype: list

    """"""
    args['paths'] = []
    path_value = args['completion_text']
    hidden_files = args['show_hidden_files']

    def get_subdirs(dirname):
        try:
            if PY2:
                return os.walk(dirname).__next__[1]
            else:
                return next(os.walk(dirname))[1]
        except StopIteration:
            return []
    dirname = os.path.dirname(path_value)
    basename = os.path.basename(path_value)
    dirs = get_subdirs(dirname)
    if not dirs:
        return args
    if not basename:
        if not hidden_files:
            old_dirs = dirs
            dirs = []
            for d in old_dirs:
                if not is_hidden(os.path.join(dirname, d)):
                    dirs.append(d)
    matching_dirs = []
    for s in dirs:
        if s.startswith(basename):
            p = os.path.join(dirname, s)
            if not p.endswith(os.path.sep):
                p += os.path.sep
            matching_dirs.append(p)
    args['paths'] = sorted(matching_dirs)
    return args","for s in dirs:
    if s.startswith(basename):
        p = os.path.join(dirname, s)
        if not p.endswith(os.path.sep):
            p += os.path.sep
        matching_dirs.append(p)","matching_dirs = [os.path.join(dirname, s) + os.path.sep for s in dirs if s.startswith(basename) and (not os.path.join(dirname, s).endswith(os.path.sep))]",Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
oppia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/learner_progress_services.py,https://github.com/oppia/oppia/tree/master/core/domain/learner_progress_services.py,,get_topics_and_stories_progress$1977,"def get_topics_and_stories_progress(user_id: str) -> Tuple[learner_progress_domain.LearnerProgressInTopicsAndStories, Dict[str, int]]:
    """"""Returns the progress of the learners - the stories and learnt_topics
    completed by the user and those in progress.

    Args:
        user_id: str. The id of the learner.

    Returns:
        (LearnerProgressInTopicsAndStories, dict).
        The first return value is the learner progress in topics and stories
        domain object corresponding to the particular learner.
        The second return value is the numbers of the activities that are
        no longer present. It contains three keys:
            - partially_learnt_topics: int. The number of partially learnt
                topics no longer present.
            - completed_stories: int. The number of completed stories no
                longer present.
            - learnt_topics: int. The number of learnt topics no
                longer present.
            - topics_to_learn: int. The number of topics marked to learn.
    """"""
    activity_ids_in_learner_dashboard = get_learner_dashboard_activities(user_id)
    completed_story_ids = activity_ids_in_learner_dashboard.completed_story_ids
    learnt_topic_ids = activity_ids_in_learner_dashboard.learnt_topic_ids
    partially_learnt_topic_ids = activity_ids_in_learner_dashboard.partially_learnt_topic_ids
    topic_ids_to_learn = activity_ids_in_learner_dashboard.topic_ids_to_learn
    all_topic_ids = activity_ids_in_learner_dashboard.all_topic_ids
    untracked_topic_ids = activity_ids_in_learner_dashboard.untracked_topic_ids
    unique_topic_ids = list(set(partially_learnt_topic_ids + learnt_topic_ids + topic_ids_to_learn + all_topic_ids + untracked_topic_ids))
    activity_models = datastore_services.fetch_multiple_entities_by_ids_and_models([('TopicSummaryModel', unique_topic_ids), ('StorySummaryModel', completed_story_ids)])
    topic_id_to_model_dict: Dict[str, topic_domain.TopicSummary] = {}
    for model in activity_models[0]:
        if model is not None:
            assert isinstance(model, topic_models.TopicSummaryModel)
            topic_id_to_model_dict[model.id] = topic_fetchers.get_topic_summary_from_model(model)
    completed_story_models = activity_models[1]
    completed_story_summaries: List[Optional[story_domain.StorySummary]] = []
    for model in completed_story_models:
        if model is not None:
            assert isinstance(model, story_models.StorySummaryModel)
            completed_story_summaries.append(story_fetchers.get_story_summary_from_model(model))
        else:
            completed_story_summaries.append(None)
    partially_learnt_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in partially_learnt_topic_ids]
    learnt_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in learnt_topic_ids]
    topics_to_learn_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in topic_ids_to_learn]
    all_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in all_topic_ids]
    untracked_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in untracked_topic_ids]
    (filtered_completed_story_summaries, nonexistent_completed_story_ids, completed_to_incomplete_story_summaries) = _get_filtered_completed_story_summaries(user_id, completed_story_summaries, completed_story_ids)
    completed_to_incomplete_story_titles = []
    for story_summary in completed_to_incomplete_story_summaries:
        completed_to_incomplete_story_titles.append(story_summary.title)
    (filtered_learnt_topic_summaries, nonexistent_learnt_topic_ids, learnt_to_partially_learnt_topic_summaries) = _get_filtered_learnt_topic_summaries(user_id, learnt_topic_summaries, learnt_topic_ids)
    learnt_to_partially_learnt_topic_titles = []
    for topic_summary in learnt_to_partially_learnt_topic_summaries:
        partially_learnt_topic_summaries.append(topic_summary)
        learnt_to_partially_learnt_topic_titles.append(topic_summary.name)
        partially_learnt_topic_ids.append(topic_summary.id)
    (filtered_partially_learnt_topic_summaries, nonexistent_partially_learnt_topic_ids) = _get_filtered_partially_learnt_topic_summaries(partially_learnt_topic_summaries, partially_learnt_topic_ids)
    (filtered_topics_to_learn_summaries, nonexistent_topic_ids_to_learn) = _get_filtered_topics_to_learn_summaries(user_id, topics_to_learn_summaries, topic_ids_to_learn)
    filtered_all_topic_summaries = _get_filtered_all_topic_summaries(all_topic_summaries, all_topic_ids)
    filtered_untracked_topic_summaries = _get_filtered_untracked_topic_summaries(untracked_topic_summaries, untracked_topic_ids)
    number_of_nonexistent_topics_and_stories = {'partially_learnt_topics': len(nonexistent_partially_learnt_topic_ids), 'completed_stories': len(nonexistent_completed_story_ids), 'learnt_topics': len(nonexistent_learnt_topic_ids), 'topics_to_learn': len(nonexistent_topic_ids_to_learn)}
    _remove_activity_ids_from_incomplete_list(user_id, exploration_ids=[], collection_ids=[], partially_learnt_topic_ids=nonexistent_partially_learnt_topic_ids)
    _remove_activity_ids_from_completed_list(user_id, [], [], nonexistent_completed_story_ids, nonexistent_learnt_topic_ids)
    learner_goals_services.remove_topics_from_learn_goal(user_id, nonexistent_topic_ids_to_learn)
    learner_progress_in_topics_and_stories = learner_progress_domain.LearnerProgressInTopicsAndStories(filtered_partially_learnt_topic_summaries, filtered_completed_story_summaries, filtered_learnt_topic_summaries, filtered_topics_to_learn_summaries, filtered_all_topic_summaries, filtered_untracked_topic_summaries, completed_to_incomplete_story_titles, learnt_to_partially_learnt_topic_titles)
    return (learner_progress_in_topics_and_stories, number_of_nonexistent_topics_and_stories)","for model in completed_story_models:
    if model is not None:
        assert isinstance(model, story_models.StorySummaryModel)
        completed_story_summaries.append(story_fetchers.get_story_summary_from_model(model))
    else:
        completed_story_summaries.append(None)","completed_story_summaries = [story_fetchers.get_story_summary_from_model(model) if model is not None and isinstance(model, story_models.StorySummaryModel) else None for model in completed_story_models]",Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
oppia,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oppia/core/domain/learner_progress_services.py,https://github.com/oppia/oppia/tree/master/core/domain/learner_progress_services.py,,get_topics_and_stories_progress$1977,"def get_topics_and_stories_progress(user_id: str) -> Tuple[learner_progress_domain.LearnerProgressInTopicsAndStories, Dict[str, int]]:
    """"""Returns the progress of the learners - the stories and learnt_topics
    completed by the user and those in progress.

    Args:
        user_id: str. The id of the learner.

    Returns:
        (LearnerProgressInTopicsAndStories, dict).
        The first return value is the learner progress in topics and stories
        domain object corresponding to the particular learner.
        The second return value is the numbers of the activities that are
        no longer present. It contains three keys:
            - partially_learnt_topics: int. The number of partially learnt
                topics no longer present.
            - completed_stories: int. The number of completed stories no
                longer present.
            - learnt_topics: int. The number of learnt topics no
                longer present.
            - topics_to_learn: int. The number of topics marked to learn.
    """"""
    activity_ids_in_learner_dashboard = get_learner_dashboard_activities(user_id)
    completed_story_ids = activity_ids_in_learner_dashboard.completed_story_ids
    learnt_topic_ids = activity_ids_in_learner_dashboard.learnt_topic_ids
    partially_learnt_topic_ids = activity_ids_in_learner_dashboard.partially_learnt_topic_ids
    topic_ids_to_learn = activity_ids_in_learner_dashboard.topic_ids_to_learn
    all_topic_ids = activity_ids_in_learner_dashboard.all_topic_ids
    untracked_topic_ids = activity_ids_in_learner_dashboard.untracked_topic_ids
    unique_topic_ids = list(set(partially_learnt_topic_ids + learnt_topic_ids + topic_ids_to_learn + all_topic_ids + untracked_topic_ids))
    activity_models = datastore_services.fetch_multiple_entities_by_ids_and_models([('TopicSummaryModel', unique_topic_ids), ('StorySummaryModel', completed_story_ids)])
    topic_id_to_model_dict: Dict[str, topic_domain.TopicSummary] = {}
    for model in activity_models[0]:
        if model is not None:
            assert isinstance(model, topic_models.TopicSummaryModel)
            topic_id_to_model_dict[model.id] = topic_fetchers.get_topic_summary_from_model(model)
    completed_story_models = activity_models[1]
    completed_story_summaries: List[Optional[story_domain.StorySummary]] = []
    for model in completed_story_models:
        if model is not None:
            assert isinstance(model, story_models.StorySummaryModel)
            completed_story_summaries.append(story_fetchers.get_story_summary_from_model(model))
        else:
            completed_story_summaries.append(None)
    partially_learnt_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in partially_learnt_topic_ids]
    learnt_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in learnt_topic_ids]
    topics_to_learn_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in topic_ids_to_learn]
    all_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in all_topic_ids]
    untracked_topic_summaries = [topic_id_to_model_dict[topic_id] if topic_id in topic_id_to_model_dict else None for topic_id in untracked_topic_ids]
    (filtered_completed_story_summaries, nonexistent_completed_story_ids, completed_to_incomplete_story_summaries) = _get_filtered_completed_story_summaries(user_id, completed_story_summaries, completed_story_ids)
    completed_to_incomplete_story_titles = []
    for story_summary in completed_to_incomplete_story_summaries:
        completed_to_incomplete_story_titles.append(story_summary.title)
    (filtered_learnt_topic_summaries, nonexistent_learnt_topic_ids, learnt_to_partially_learnt_topic_summaries) = _get_filtered_learnt_topic_summaries(user_id, learnt_topic_summaries, learnt_topic_ids)
    learnt_to_partially_learnt_topic_titles = []
    for topic_summary in learnt_to_partially_learnt_topic_summaries:
        partially_learnt_topic_summaries.append(topic_summary)
        learnt_to_partially_learnt_topic_titles.append(topic_summary.name)
        partially_learnt_topic_ids.append(topic_summary.id)
    (filtered_partially_learnt_topic_summaries, nonexistent_partially_learnt_topic_ids) = _get_filtered_partially_learnt_topic_summaries(partially_learnt_topic_summaries, partially_learnt_topic_ids)
    (filtered_topics_to_learn_summaries, nonexistent_topic_ids_to_learn) = _get_filtered_topics_to_learn_summaries(user_id, topics_to_learn_summaries, topic_ids_to_learn)
    filtered_all_topic_summaries = _get_filtered_all_topic_summaries(all_topic_summaries, all_topic_ids)
    filtered_untracked_topic_summaries = _get_filtered_untracked_topic_summaries(untracked_topic_summaries, untracked_topic_ids)
    number_of_nonexistent_topics_and_stories = {'partially_learnt_topics': len(nonexistent_partially_learnt_topic_ids), 'completed_stories': len(nonexistent_completed_story_ids), 'learnt_topics': len(nonexistent_learnt_topic_ids), 'topics_to_learn': len(nonexistent_topic_ids_to_learn)}
    _remove_activity_ids_from_incomplete_list(user_id, exploration_ids=[], collection_ids=[], partially_learnt_topic_ids=nonexistent_partially_learnt_topic_ids)
    _remove_activity_ids_from_completed_list(user_id, [], [], nonexistent_completed_story_ids, nonexistent_learnt_topic_ids)
    learner_goals_services.remove_topics_from_learn_goal(user_id, nonexistent_topic_ids_to_learn)
    learner_progress_in_topics_and_stories = learner_progress_domain.LearnerProgressInTopicsAndStories(filtered_partially_learnt_topic_summaries, filtered_completed_story_summaries, filtered_learnt_topic_summaries, filtered_topics_to_learn_summaries, filtered_all_topic_summaries, filtered_untracked_topic_summaries, completed_to_incomplete_story_titles, learnt_to_partially_learnt_topic_titles)
    return (learner_progress_in_topics_and_stories, number_of_nonexistent_topics_and_stories)","for topic_summary in learnt_to_partially_learnt_topic_summaries:
    partially_learnt_topic_summaries.append(topic_summary)
    learnt_to_partially_learnt_topic_titles.append(topic_summary.name)
    partially_learnt_topic_ids.append(topic_summary.id)",partially_learnt_topic_ids.extend([topic_summary.id for topic_summary in learnt_to_partially_learnt_topic_summaries]),Cannot refactor,-1,,,,,robosuite,,,,,it actually cannot refactor
linearmodels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/linearmodels/linearmodels/iv/results.py,https://github.com/bashtage/linearmodels/tree/master/linearmodels/iv/results.py,IVModelComparison,summary$1575,"def summary(self) -> Summary:
    """"""
        Model estimation summary.

        Returns
        -------
        Summary
            Summary table of model estimation results

        Supports export to csv, html and latex  using the methods ``summary.as_csv()``,
        ``summary.as_html()`` and ``summary.as_latex()``.
        """"""
    smry = Summary()
    models = list(self._results.keys())
    title = 'Model Comparison'
    stubs = ['Dep. Variable', 'Estimator', 'No. Observations', 'Cov. Est.', 'R-squared', 'Adj. R-squared', 'F-statistic', 'P-value (F-stat)']
    dep_name: Dict[str, str] = {}
    for key in self._results:
        dep_name[key] = str(self._results[key].model.dependent.cols[0])
    dep_names = Series(dep_name)
    vals = concat([dep_names, self.estimator_method, self.nobs, self.cov_estimator, self.rsquared, self.rsquared_adj, self.f_statistic], axis=1)
    vals = [[i for i in v] for v in vals.T.values]
    vals[2] = [str(v) for v in vals[2]]
    for i in range(4, len(vals)):
        vals[i] = [_str(v) for v in vals[i]]
    params = self.params
    precision = getattr(self, self._precision)
    pvalues = asarray(self.pvalues)
    params_fmt = []
    params_stub = []
    for i in range(len(params)):
        formatted_and_starred = []
        for (v, pv) in zip(params.values[i], pvalues[i]):
            formatted_and_starred.append(add_star(_str(v), pv, self._stars))
        params_fmt.append(formatted_and_starred)
        precision_fmt = []
        for v in precision.values[i]:
            v_str = _str(v)
            v_str = '({0})'.format(v_str) if v_str.strip() else v_str
            precision_fmt.append(v_str)
        params_fmt.append(precision_fmt)
        params_stub.append(params.index[i])
        params_stub.append(' ')
    vals = table_concat((vals, params_fmt))
    stubs = stub_concat((stubs, params_stub))
    all_instr = []
    for key in self._results:
        res = self._results[key]
        all_instr.append(res.model.instruments.cols)
    ninstr = max(map(len, all_instr))
    instruments = []
    instrument_stub = ['Instruments']
    for i in range(ninstr):
        if i > 0:
            instrument_stub.append('')
        row = []
        for j in range(len(self._results)):
            instr = all_instr[j]
            if len(instr) > i:
                row.append(instr[i])
            else:
                row.append('')
        instruments.append(row)
    if instruments:
        vals = table_concat((vals, instruments))
        stubs = stub_concat((stubs, instrument_stub))
    txt_fmt = default_txt_fmt.copy()
    txt_fmt['data_aligns'] = 'r'
    txt_fmt['header_align'] = 'r'
    table = SimpleTable(vals, headers=models, title=title, stubs=stubs, txt_fmt=txt_fmt)
    smry.tables.append(table)
    prec_type = self._PRECISION_TYPES[self._precision]
    smry.add_extra_txt(['{0} reported in parentheses'.format(prec_type)])
    return smry","for i in range(len(params)):
    formatted_and_starred = []
    for (v, pv) in zip(params.values[i], pvalues[i]):
        formatted_and_starred.append(add_star(_str(v), pv, self._stars))
    params_fmt.append(formatted_and_starred)
    precision_fmt = []
    for v in precision.values[i]:
        v_str = _str(v)
        v_str = '({0})'.format(v_str) if v_str.strip() else v_str
        precision_fmt.append(v_str)
    params_fmt.append(precision_fmt)
    params_stub.append(params.index[i])
    params_stub.append(' ')",params_stub = [params.index[i] + ' ' for i in range(len(params))],Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
Mycodo,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Mycodo/mycodo/mycodo_flask/utils/utils_settings.py,https://github.com/kizniche/Mycodo/tree/master/mycodo/mycodo_flask/utils/utils_settings.py,,settings_action_import$598,"def settings_action_import(form):
    """"""
    Receive an action module file, check it for errors, add it to Mycodo controller list
    """"""
    action = '{action} {controller}'.format(action=gettext('Import'), controller=TRANSLATIONS['actions']['title'])
    error = []
    action_info = None
    try:
        install_dir = os.path.abspath(INSTALL_DIRECTORY)
        tmp_directory = os.path.join(install_dir, 'mycodo/actions/tmp_actions')
        assure_path_exists(tmp_directory)
        assure_path_exists(PATH_ACTIONS_CUSTOM)
        tmp_name = 'tmp_action_testing.py'
        full_path_tmp = os.path.join(tmp_directory, tmp_name)
        if not form.import_action_file.data:
            error.append('No file present')
        elif form.import_action_file.data.filename == '':
            error.append('No file name')
        else:
            form.import_action_file.data.save(full_path_tmp)
        try:
            (action_info, status) = load_module_from_file(full_path_tmp, 'actions')
            if not action_info or not hasattr(action_info, 'ACTION_INFORMATION'):
                error.append('Could not load ACTION_INFORMATION dictionary from the uploaded action module')
        except Exception:
            error.append('Could not load uploaded file as a python module:\n{}'.format(traceback.format_exc()))
        dict_actions = parse_action_information()
        list_actions = []
        for each_key in dict_actions.keys():
            list_actions.append(each_key.lower())
        if not error:
            if 'name_unique' not in action_info.ACTION_INFORMATION:
                error.append(""'name_unique' not found in ACTION_INFORMATION dictionary"")
            elif action_info.ACTION_INFORMATION['name_unique'] == '':
                error.append(""'name_unique' is empty"")
            elif action_info.ACTION_INFORMATION['name_unique'].lower() in list_actions:
                error.append(""'name_unique' is not unique, there is already an action with that name ({})"".format(action_info.ACTION_INFORMATION['name_unique'].lower()))
            if 'name' not in action_info.ACTION_INFORMATION:
                error.append(""'name' not found in ACTION_INFORMATION dictionary"")
            elif action_info.ACTION_INFORMATION['name'] == '':
                error.append(""'name' is empty"")
            if 'dependencies_module' in action_info.ACTION_INFORMATION:
                if not isinstance(action_info.ACTION_INFORMATION['dependencies_module'], list):
                    error.append(""'dependencies_module' must be a list of tuples"")
                else:
                    for each_dep in action_info.ACTION_INFORMATION['dependencies_module']:
                        if not isinstance(each_dep, tuple):
                            error.append(""'dependencies_module' must be a list of tuples"")
                        elif len(each_dep) != 3:
                            error.append(""'dependencies_module': tuples in list must have 3 items"")
                        elif not each_dep[0] or not each_dep[1] or (not each_dep[2]):
                            error.append(""'dependencies_module': tuples in list must not be empty"")
                        elif each_dep[0] not in ['internal', 'pip-pypi', 'apt']:
                            error.append(""'dependencies_module': first in tuple must be 'internal', 'pip-pypi', or 'apt'"")
        if not error:
            unique_name = '{}.py'.format(action_info.ACTION_INFORMATION['name_unique'].lower())
            full_path_final = os.path.join(PATH_ACTIONS_CUSTOM, unique_name)
            os.rename(full_path_tmp, full_path_final)
            cmd = '{path}/mycodo/scripts/mycodo_wrapper frontend_reload 2>&1'.format(path=install_dir)
            subprocess.Popen(cmd, shell=True)
            flash('Frontend reloaded to scan for new Action Modules', 'success')
    except Exception as err:
        logger.exception('Action Import')
        error.append('Exception: {}'.format(err))
    flash_success_errors(error, action, url_for('routes_settings.settings_action'))","for each_dep in action_info.ACTION_INFORMATION['dependencies_module']:
    if not isinstance(each_dep, tuple):
        error.append(""'dependencies_module' must be a list of tuples"")
    elif len(each_dep) != 3:
        error.append(""'dependencies_module': tuples in list must have 3 items"")
    elif not each_dep[0] or not each_dep[1] or (not each_dep[2]):
        error.append(""'dependencies_module': tuples in list must not be empty"")
    elif each_dep[0] not in ['internal', 'pip-pypi', 'apt']:
        error.append(""'dependencies_module': first in tuple must be 'internal', 'pip-pypi', or 'apt'"")","error += [""'dependencies_module' must be a list of tuples"" if not isinstance(each_dep, tuple) else ""'dependencies_module': tuples in list must have 3 items"" if len(each_dep) != 3 else ""'dependencies_module': tuples in list must not be empty"" if not each_dep[0] or not each_dep[1] or (not each_dep[2]) else ""'dependencies_module': first in tuple must be 'internal', 'pip-pypi', or 'apt'"" if each_dep[0] not in ['internal', 'pip-pypi', 'apt'] else None for each_dep in action_info.ACTION_INFORMATION['dependencies_module']]",Cannot refactor,-1,0,,,,robosuite,,,,,it actually cannot refactor
SOLO,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SOLO/tools/analyze_logs.py,https://github.com/WXinlong/SOLO/tree/master/tools/analyze_logs.py,,plot_curve$34,"def plot_curve(log_dicts, args):
    if args.backend is not None:
        plt.switch_backend(args.backend)
    sns.set_style(args.style)
    legend = args.legend
    if legend is None:
        legend = []
        for json_log in args.json_logs:
            for metric in args.keys:
                legend.append('{}_{}'.format(json_log, metric))
    assert len(legend) == len(args.json_logs) * len(args.keys)
    metrics = args.keys
    num_metrics = len(metrics)
    for (i, log_dict) in enumerate(log_dicts):
        epochs = list(log_dict.keys())
        for (j, metric) in enumerate(metrics):
            print('plot curve of {}, metric is {}'.format(args.json_logs[i], metric))
            if metric not in log_dict[epochs[0]]:
                raise KeyError('{} does not contain metric {}'.format(args.json_logs[i], metric))
            if 'mAP' in metric:
                xs = np.arange(1, max(epochs) + 1)
                ys = []
                for epoch in epochs:
                    ys += log_dict[epoch][metric]
                ax = plt.gca()
                ax.set_xticks(xs)
                plt.xlabel('epoch')
                plt.plot(xs, ys, label=legend[i * num_metrics + j], marker='o')
            else:
                xs = []
                ys = []
                num_iters_per_epoch = log_dict[epochs[0]]['iter'][-1]
                for epoch in epochs:
                    iters = log_dict[epoch]['iter']
                    if log_dict[epoch]['mode'][-1] == 'val':
                        iters = iters[:-1]
                    xs.append(np.array(iters) + (epoch - 1) * num_iters_per_epoch)
                    ys.append(np.array(log_dict[epoch][metric][:len(iters)]))
                xs = np.concatenate(xs)
                ys = np.concatenate(ys)
                plt.xlabel('iter')
                plt.plot(xs, ys, label=legend[i * num_metrics + j], linewidth=0.5)
            plt.legend()
        if args.title is not None:
            plt.title(args.title)
    if args.out is None:
        plt.show()
    else:
        print('save curve to: {}'.format(args.out))
        plt.savefig(args.out)
        plt.cla()","for epoch in epochs:
    iters = log_dict[epoch]['iter']
    if log_dict[epoch]['mode'][-1] == 'val':
        iters = iters[:-1]
    xs.append(np.array(iters) + (epoch - 1) * num_iters_per_epoch)
    ys.append(np.array(log_dict[epoch][metric][:len(iters)]))","ys += [np.array(log_dict[epoch][metric][:len(iters)]) for (epoch, iters) in zip(epochs, xs)]",Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
PaddleHub,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleHub/modules/image/text_recognition/chinese_text_detection_db_server/processor.py,https://github.com/PaddlePaddle/PaddleHub/tree/master/modules/image/text_recognition/chinese_text_detection_db_server/processor.py,DBPostProcess,__call__$177,"def __call__(self, predictions, ratio_list):
    pred = predictions[:, 0, :, :]
    segmentation = pred > self.thresh
    boxes_batch = []
    for batch_index in range(pred.shape[0]):
        (height, width) = pred.shape[-2:]
        (tmp_boxes, tmp_scores) = self.boxes_from_bitmap(pred[batch_index], segmentation[batch_index], width, height)
        boxes = []
        for k in range(len(tmp_boxes)):
            if tmp_scores[k] > self.box_thresh:
                boxes.append(tmp_boxes[k])
        if len(boxes) > 0:
            boxes = np.array(boxes)
            (ratio_h, ratio_w) = ratio_list[batch_index]
            boxes[:, :, 0] = boxes[:, :, 0] / ratio_w
            boxes[:, :, 1] = boxes[:, :, 1] / ratio_h
        boxes_batch.append(boxes)
    return boxes_batch","for batch_index in range(pred.shape[0]):
    (height, width) = pred.shape[-2:]
    (tmp_boxes, tmp_scores) = self.boxes_from_bitmap(pred[batch_index], segmentation[batch_index], width, height)
    boxes = []
    for k in range(len(tmp_boxes)):
        if tmp_scores[k] > self.box_thresh:
            boxes.append(tmp_boxes[k])
    if len(boxes) > 0:
        boxes = np.array(boxes)
        (ratio_h, ratio_w) = ratio_list[batch_index]
        boxes[:, :, 0] = boxes[:, :, 0] / ratio_w
        boxes[:, :, 1] = boxes[:, :, 1] / ratio_h
    boxes_batch.append(boxes)","boxes_batch = [np.array([box / ratio_list[batch_index][1], box / ratio_list[batch_index][0]]) for batch_index in range(pred.shape[0]) for (box, score) in zip(*self.boxes_from_bitmap(pred[batch_index], segmentation[batch_index], pred.shape[-1], pred.shape[-2])) if score > self.box_thresh]",Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
yellowbrick,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/yellowbrick/docs/api/features/pcoords_benchmark.py,https://github.com/DistrictDataLabs/yellowbrick/tree/master/docs/api/features/pcoords_benchmark.py,,plot_speedup$9,"def plot_speedup(trials=5, factors=np.arange(1, 11)):

    def pcoords_time(X, y, fast=True):
        (_, ax) = plt.subplots()
        oz = ParallelCoordinates(fast=fast, ax=ax)
        start = time.time()
        oz.fit_transform(X, y)
        delta = time.time() - start
        plt.cla()
        plt.clf()
        plt.close('all')
        return delta

    def pcoords_speedup(X, y):
        fast_time = pcoords_time(X, y, fast=True)
        slow_time = pcoords_time(X, y, fast=False)
        return slow_time / fast_time
    data = load_iris()
    speedups = []
    variance = []
    for factor in factors:
        X = np.repeat(data.data, factor, axis=0)
        y = np.repeat(data.target, factor, axis=0)
        local_speedups = []
        for trial in range(trials):
            local_speedups.append(pcoords_speedup(X, y))
        local_speedups = np.array(local_speedups)
        speedups.append(local_speedups.mean())
        variance.append(local_speedups.std())
    speedups = np.array(speedups)
    variance = np.array(variance)
    series = pd.Series(speedups, index=factors)
    (_, ax) = plt.subplots(figsize=(9, 6))
    series.plot(ax=ax, marker='o', label='speedup factor', color='b')
    ax.fill_between(factors, speedups - variance, speedups + variance, alpha=0.25, color='b')
    ax.set_ylabel('speedup factor')
    ax.set_xlabel('dataset size (number of repeats in Iris dataset)')
    ax.set_title('Speed Improvement of Fast Parallel Coordinates')
    plt.savefig('images/fast_parallel_coordinates_speedup_benchmark.png')","for factor in factors:
    X = np.repeat(data.data, factor, axis=0)
    y = np.repeat(data.target, factor, axis=0)
    local_speedups = []
    for trial in range(trials):
        local_speedups.append(pcoords_speedup(X, y))
    local_speedups = np.array(local_speedups)
    speedups.append(local_speedups.mean())
    variance.append(local_speedups.std())","variance = [np.array([pcoords_speedup(np.repeat(data.data, factor, axis=0), np.repeat(data.target, factor, axis=0)) for trial in range(trials)]).std() for factor in factors]",Cannot refactor,-1,0,,2,1,robosuite,,,,,it actually cannot refactor
new_find,,,,,,,,,,,,,,,,,,,
adapter-transformers,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/adapter-transformers/src/transformers/models/detr/feature_extraction_detr.py,https://github.com/Adapter-Hub/adapter-transformers/tree/master/src/transformers/models/detr/feature_extraction_detr.py,DetrFeatureExtractor,post_process_panoptic$973,"def post_process_panoptic(self, outputs, processed_sizes, target_sizes=None, is_thing_map=None, threshold=0.85):
    """"""
        Converts the output of [`DetrForSegmentation`] into actual panoptic predictions. Only supports PyTorch.

        Args:
            outputs ([`DetrSegmentationOutput`]):
                Raw outputs of the model.
            processed_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `List[Tuple]` of length `batch_size`):
                Torch Tensor (or list) containing the size (h, w) of each image of the batch, i.e. the size after data
                augmentation but before batching.
            target_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `List[Tuple]` of length `batch_size`, *optional*):
                Torch Tensor (or list) corresponding to the requested final size (h, w) of each prediction. If left to
                None, it will default to the `processed_sizes`.
            is_thing_map (`torch.Tensor` of shape `(batch_size, 2)`, *optional*):
                Dictionary mapping class indices to either True or False, depending on whether or not they are a thing.
                If not set, defaults to the `is_thing_map` of COCO panoptic.
            threshold (`float`, *optional*, defaults to 0.85):
                Threshold to use to filter out queries.

        Returns:
            `List[Dict]`: A list of dictionaries, each dictionary containing a PNG string and segments_info values for
            an image in the batch as predicted by the model.
        """"""
    warnings.warn('`post_process_panoptic is deprecated and will be removed in v5 of Transformers, please use `post_process_panoptic_segmentation`.', FutureWarning)
    if target_sizes is None:
        target_sizes = processed_sizes
    if len(processed_sizes) != len(target_sizes):
        raise ValueError('Make sure to pass in as many processed_sizes as target_sizes')
    if is_thing_map is None:
        is_thing_map = {i: i <= 90 for i in range(201)}
    (out_logits, raw_masks, raw_boxes) = (outputs.logits, outputs.pred_masks, outputs.pred_boxes)
    if not len(out_logits) == len(raw_masks) == len(target_sizes):
        raise ValueError('Make sure that you pass in as many target sizes as the batch dimension of the logits and masks')
    preds = []

    def to_tuple(tup):
        if isinstance(tup, tuple):
            return tup
        return tuple(tup.cpu().tolist())
    for (cur_logits, cur_masks, cur_boxes, size, target_size) in zip(out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes):
        (scores, labels) = cur_logits.softmax(-1).max(-1)
        keep = labels.ne(outputs.logits.shape[-1] - 1) & (scores > threshold)
        (cur_scores, cur_classes) = cur_logits.softmax(-1).max(-1)
        cur_scores = cur_scores[keep]
        cur_classes = cur_classes[keep]
        cur_masks = cur_masks[keep]
        cur_masks = nn.functional.interpolate(cur_masks[:, None], to_tuple(size), mode='bilinear').squeeze(1)
        cur_boxes = center_to_corners_format(cur_boxes[keep])
        (h, w) = cur_masks.shape[-2:]
        if len(cur_boxes) != len(cur_classes):
            raise ValueError('Not as many boxes as there are classes')
        cur_masks = cur_masks.flatten(1)
        stuff_equiv_classes = defaultdict(lambda : [])
        for (k, label) in enumerate(cur_classes):
            if not is_thing_map[label.item()]:
                stuff_equiv_classes[label.item()].append(k)

        def get_ids_area(masks, scores, dedup=False):
            m_id = masks.transpose(0, 1).softmax(-1)
            if m_id.shape[-1] == 0:
                m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)
            else:
                m_id = m_id.argmax(-1).view(h, w)
            if dedup:
                for equiv in stuff_equiv_classes.values():
                    if len(equiv) > 1:
                        for eq_id in equiv:
                            m_id.masked_fill_(m_id.eq(eq_id), equiv[0])
            (final_h, final_w) = to_tuple(target_size)
            seg_img = Image.fromarray(id_to_rgb(m_id.view(h, w).cpu().numpy()))
            seg_img = seg_img.resize(size=(final_w, final_h), resample=Image.NEAREST)
            np_seg_img = torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes()))
            np_seg_img = np_seg_img.view(final_h, final_w, 3)
            np_seg_img = np_seg_img.numpy()
            m_id = torch.from_numpy(rgb_to_id(np_seg_img))
            area = []
            for i in range(len(scores)):
                area.append(m_id.eq(i).sum().item())
            return (area, seg_img)
        (area, seg_img) = get_ids_area(cur_masks, cur_scores, dedup=True)
        if cur_classes.numel() > 0:
            while True:
                filtered_small = torch.as_tensor([area[i] <= 4 for (i, c) in enumerate(cur_classes)], dtype=torch.bool, device=keep.device)
                if filtered_small.any().item():
                    cur_scores = cur_scores[~filtered_small]
                    cur_classes = cur_classes[~filtered_small]
                    cur_masks = cur_masks[~filtered_small]
                    (area, seg_img) = get_ids_area(cur_masks, cur_scores)
                else:
                    break
        else:
            cur_classes = torch.ones(1, dtype=torch.long, device=cur_classes.device)
        segments_info = []
        for (i, a) in enumerate(area):
            cat = cur_classes[i].item()
            segments_info.append({'id': i, 'isthing': is_thing_map[cat], 'category_id': cat, 'area': a})
        del cur_classes
        with io.BytesIO() as out:
            seg_img.save(out, format='PNG')
            predictions = {'png_string': out.getvalue(), 'segments_info': segments_info}
        preds.append(predictions)
    return preds","for (i, a) in enumerate(area):
    cat = cur_classes[i].item()
    segments_info.append({'id': i, 'isthing': is_thing_map[cat], 'category_id': cat, 'area': a})","segments_info = [{'id': i, 'isthing': is_thing_map[cat], 'category_id': cat, 'area': a} for ((i, a), cat) in zip(enumerate(area), cur_classes)]",Cannot refactor,-1,1,,,,robosuite,,,,,
erpnext,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/erpnext/erpnext/erpnext_integrations/doctype/tally_migration/tally_migration.py,https://github.com/frappe/erpnext/tree/master/erpnext/erpnext_integrations/doctype/tally_migration/tally_migration.py,TallyMigration,_process_master_data$103,"def _process_master_data(self):

    def get_company_name(collection):
        return collection.find_all('REMOTECMPINFO.LIST')[0].REMOTECMPNAME.string.strip()

    def get_coa_customers_suppliers(collection):
        root_type_map = {'Application of Funds (Assets)': 'Asset', 'Expenses': 'Expense', 'Income': 'Income', 'Source of Funds (Liabilities)': 'Liability'}
        roots = set(root_type_map.keys())
        accounts = list(get_groups(collection.find_all('GROUP'))) + list(get_ledgers(collection.find_all('LEDGER')))
        (children, parents) = get_children_and_parent_dict(accounts)
        group_set = [acc[1] for acc in accounts if acc[2]]
        (children, customers, suppliers) = remove_parties(parents, children, group_set)
        try:
            coa = traverse({}, children, roots, roots, group_set)
        except RecursionError:
            self.log(_('Error occured while parsing Chart of Accounts: Please make sure that no two accounts have the same name'))
        for account in coa:
            coa[account]['root_type'] = root_type_map[account]
        return (coa, customers, suppliers)

    def get_groups(accounts):
        for account in accounts:
            if account['NAME'] in (self.tally_creditors_account, self.tally_debtors_account):
                yield (get_parent(account), account['NAME'], 0)
            else:
                yield (get_parent(account), account['NAME'], 1)

    def get_ledgers(accounts):
        for account in accounts:
            if account.PARENT:
                yield (account.PARENT.string.strip(), account['NAME'], 0)

    def get_parent(account):
        if account.PARENT:
            return account.PARENT.string.strip()
        return {('Yes', 'No'): 'Application of Funds (Assets)', ('Yes', 'Yes'): 'Expenses', ('No', 'Yes'): 'Income', ('No', 'No'): 'Source of Funds (Liabilities)'}[account.ISDEEMEDPOSITIVE.string.strip(), account.ISREVENUE.string.strip()]

    def get_children_and_parent_dict(accounts):
        (children, parents) = ({}, {})
        for (parent, account, is_group) in accounts:
            children.setdefault(parent, set()).add(account)
            parents.setdefault(account, set()).add(parent)
            parents[account].update(parents.get(parent, []))
        return (children, parents)

    def remove_parties(parents, children, group_set):
        (customers, suppliers) = (set(), set())
        for account in parents:
            found = False
            if self.tally_creditors_account in parents[account]:
                found = True
                if account not in group_set:
                    suppliers.add(account)
            if self.tally_debtors_account in parents[account]:
                found = True
                if account not in group_set:
                    customers.add(account)
            if found:
                children.pop(account, None)
        return (children, customers, suppliers)

    def traverse(tree, children, accounts, roots, group_set):
        for account in accounts:
            if account in group_set or account in roots:
                if account in children:
                    tree[account] = traverse({}, children, children[account], roots, group_set)
                else:
                    tree[account] = {'is_group': 1}
            else:
                tree[account] = {}
        return tree

    def get_parties_addresses(collection, customers, suppliers):
        (parties, addresses) = ([], [])
        for account in collection.find_all('LEDGER'):
            party_type = None
            links = []
            if account.NAME.string.strip() in customers:
                party_type = 'Customer'
                parties.append({'doctype': party_type, 'customer_name': account.NAME.string.strip(), 'tax_id': account.INCOMETAXNUMBER.string.strip() if account.INCOMETAXNUMBER else None, 'customer_group': 'All Customer Groups', 'territory': 'All Territories', 'customer_type': 'Individual'})
                links.append({'link_doctype': party_type, 'link_name': account['NAME']})
            if account.NAME.string.strip() in suppliers:
                party_type = 'Supplier'
                parties.append({'doctype': party_type, 'supplier_name': account.NAME.string.strip(), 'pan': account.INCOMETAXNUMBER.string.strip() if account.INCOMETAXNUMBER else None, 'supplier_group': 'All Supplier Groups', 'supplier_type': 'Individual'})
                links.append({'link_doctype': party_type, 'link_name': account['NAME']})
            if party_type:
                address = '\n'.join([a.string.strip() for a in account.find_all('ADDRESS')])
                addresses.append({'doctype': 'Address', 'address_line1': address[:140].strip(), 'address_line2': address[140:].strip(), 'country': account.COUNTRYNAME.string.strip() if account.COUNTRYNAME else None, 'state': account.LEDSTATENAME.string.strip() if account.LEDSTATENAME else None, 'gst_state': account.LEDSTATENAME.string.strip() if account.LEDSTATENAME else None, 'pin_code': account.PINCODE.string.strip() if account.PINCODE else None, 'mobile': account.LEDGERPHONE.string.strip() if account.LEDGERPHONE else None, 'phone': account.LEDGERPHONE.string.strip() if account.LEDGERPHONE else None, 'gstin': account.PARTYGSTIN.string.strip() if account.PARTYGSTIN else None, 'links': links})
        return (parties, addresses)

    def get_stock_items_uoms(collection):
        uoms = []
        for uom in collection.find_all('UNIT'):
            uoms.append({'doctype': 'UOM', 'uom_name': uom.NAME.string.strip()})
        items = []
        for item in collection.find_all('STOCKITEM'):
            stock_uom = item.BASEUNITS.string.strip() if item.BASEUNITS else self.default_uom
            items.append({'doctype': 'Item', 'item_code': item.NAME.string.strip(), 'stock_uom': stock_uom.strip(), 'is_stock_item': 0, 'item_group': 'All Item Groups', 'item_defaults': [{'company': self.erpnext_company}]})
        return (items, uoms)
    try:
        self.publish('Process Master Data', _('Reading Uploaded File'), 1, 5)
        collection = self.get_collection(self.master_data)
        company = get_company_name(collection)
        self.tally_company = company
        self.erpnext_company = company
        self.publish('Process Master Data', _('Processing Chart of Accounts and Parties'), 2, 5)
        (chart_of_accounts, customers, suppliers) = get_coa_customers_suppliers(collection)
        self.publish('Process Master Data', _('Processing Party Addresses'), 3, 5)
        (parties, addresses) = get_parties_addresses(collection, customers, suppliers)
        self.publish('Process Master Data', _('Processing Items and UOMs'), 4, 5)
        (items, uoms) = get_stock_items_uoms(collection)
        data = {'chart_of_accounts': chart_of_accounts, 'parties': parties, 'addresses': addresses, 'items': items, 'uoms': uoms}
        self.publish('Process Master Data', _('Done'), 5, 5)
        self.dump_processed_data(data)
        self.is_master_data_processed = 1
    except Exception:
        self.publish('Process Master Data', _('Process Failed'), -1, 5)
        self.log()
    finally:
        self.set_status()","for item in collection.find_all('STOCKITEM'):
    stock_uom = item.BASEUNITS.string.strip() if item.BASEUNITS else self.default_uom
    items.append({'doctype': 'Item', 'item_code': item.NAME.string.strip(), 'stock_uom': stock_uom.strip(), 'is_stock_item': 0, 'item_group': 'All Item Groups', 'item_defaults': [{'company': self.erpnext_company}]})","items = [{'doctype': 'Item', 'item_code': item.NAME.string.strip(), 'stock_uom': item.BASEUNITS.string.strip() if item.BASEUNITS else self.default_uom.strip(), 'is_stock_item': 0, 'item_group': 'All Item Groups', 'item_defaults': [{'company': self.erpnext_company}]} for item in collection.find_all('STOCKITEM')]",Cannot refactor,-1,1,,,,robosuite,,,,,
searx,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/searx/searx_extra/update/update_osm_keys_tags.py,https://github.com/searx/searx/tree/master/searx_extra/update/update_osm_keys_tags.py,,optimize_data_lang$160,"def optimize_data_lang(translations):
    language_to_delete = []
    for language in translations:
        if '-' in language:
            base_language = language.split('-')[0]
            if translations.get(base_language) == translations.get(language):
                language_to_delete.append(language)
    for language in language_to_delete:
        del translations[language]
    language_to_delete = []
    value_en = translations.get('en')
    if value_en:
        for (language, value) in translations.items():
            if language != 'en' and value == value_en:
                language_to_delete.append(language)
    for language in language_to_delete:
        del translations[language]","for language in translations:
    if '-' in language:
        base_language = language.split('-')[0]
        if translations.get(base_language) == translations.get(language):
            language_to_delete.append(language)",language_to_delete = [language for language in translations if '-' in language and translations.get(language.split('-')[0]) == translations.get(language)],Cannot refactor,-1,1,,,,robosuite,,,,,
primerpython,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/primerpython/blender_scripts/tools/tex_complex.py,https://github.com/Helpsypoo/primerpython/tree/master/blender_scripts/tools/tex_complex.py,TexComplex,add_annotation$145,"def add_annotation(self, targets=None, alignment='top', labels=None, angle=0, length=1, label_scale=0.67, gest_scale=1):
    gesture_series = []
    tex_bobj = self.tex_bobjects[targets[0]]
    for (j, target) in enumerate(targets[1]):
        bobjs = []
        path = tex_bobj.paths[target[0]]
        for i in range(target[1], target[2] + 1):
            try:
                bobjs.append(tex_bobj.imported_svg_data[path]['curves'][i])
            except:
                print(i)
                print(tex_bobj.imported_svg_data[path]['curves'])
                raise ()
        left_most = math.inf
        right_most = -math.inf
        y = 0
        for bobj in bobjs:
            cur = bobj.objects[0]
            for spline in cur.data.splines:
                for point in spline.bezier_points:
                    candidatex = cur.location[0] * cur.parent.scale[0] + cur.parent.location[0] * cur.parent.parent.scale[0] + point.co[0] * cur.scale[0]
                    if right_most < candidatex:
                        right_most = candidatex
                    if left_most > candidatex:
                        left_most = candidatex
                for point in spline.bezier_points:
                    candidatey = cur.location[1] * cur.parent.scale[1] + cur.parent.location[1] * cur.parent.parent.scale[1] + point.co[1] * cur.scale[1]
                    if alignment == 'top':
                        if y < candidatey:
                            y = candidatey
                    elif alignment == 'bottom':
                        if y > candidatey:
                            y = candidatey
        if isinstance(angle, (float, int)):
            this_angle = angle
        elif isinstance(angle, list):
            this_angle = angle[j]
        if len(target) > 3 and target[3] == None:
            if alignment == 'top':
                head = ((right_most + left_most) / 2 / gest_scale, y + length, 0)
                rot = (0, 0, 0)
            elif alignment == 'bottom':
                head = ((right_most + left_most) / 2 / gest_scale, y - length, 0)
                rot = (0, 0, math.pi)
            gesture_series.append({'type': None, 'points': {'location': head, 'rotation': rot}})
        elif len(target) > 3 and target[3] == 'bracket' or (len(target) == 3 and len(bobjs) > 1):
            if alignment == 'top':
                y += 0.2 * self.ref_obj.scale[1] * tex_bobj.ref_obj.scale[1]
                annotation_point = ((right_most + left_most) / 2 / gest_scale, y + length, 0)
                left_point = (left_most / gest_scale, y, 0)
                right_point = (right_most / gest_scale, y, 0)
            elif alignment == 'bottom':
                y -= 0.2 * self.ref_obj.scale[1] * tex_bobj.ref_obj.scale[1]
                annotation_point = ((right_most + left_most) / 2 / gest_scale, y - length, 0)
                left_point = [right_most / gest_scale, y, 0]
                right_point = [left_most / gest_scale, y, 0]
            gesture_series.append({'type': 'bracket', 'points': {'annotation_point': annotation_point, 'left_point': left_point, 'right_point': right_point}})
        elif len(target) > 3 and target[3] == 'arrow' or (len(target) == 3 and len(bobjs) == 1):
            if alignment == 'top':
                y += 0.3 * tex_bobj.ref_obj.scale[1]
                head = ((right_most + left_most) / 2 / gest_scale + math.tan(this_angle) * 0.4, y / gest_scale, 0)
                tail = ((right_most + left_most) / 2 / gest_scale + math.tan(this_angle) * length, (y + length) / gest_scale, 0)
            elif alignment == 'bottom':
                y -= 0.3 * tex_bobj.ref_obj.scale[1]
                head = ((right_most + left_most) / 2 / gest_scale - math.tan(this_angle) * 0.4, y / gest_scale, 0)
                tail = ((right_most + left_most) / 2 / gest_scale - math.tan(this_angle) * length, (y - length) / gest_scale, 0)
            gesture_series.append({'type': 'arrow', 'points': {'head': head, 'tail': tail}})
        else:
            raise Warning('Something is wrong with the gesture targets.')
    container = bobject.Bobject(name='annotation')
    gest = gesture.Gesture(gesture_series=gesture_series, color='color2', scale=gest_scale)
    container.add_subbobject(gest)
    tex_bobj.annotations.append([container, targets[1], alignment])
    self.annotations.append([container, targets[0]])
    t_bobj_count = 0
    for label in labels:
        t_bobj_count = max(len(label), t_bobj_count)
    for label in labels:
        while len(label) < t_bobj_count:
            label.append(None)
    t_bobjs = []
    for i in range(t_bobj_count):
        strings = []
        for label in labels:
            strings.append(label[i])
        t_bobj = tex_bobject.TexBobject(*strings, centered=True, color='color2')
        t_bobjs.append(t_bobj)
    if alignment == 'top':
        dy = (t_bobj_count / 2 + 1 / 2) * self.line_height
    if alignment == 'bottom':
        dy = t_bobj_count / 2 * self.line_height
    if alignment == 'top':
        for t_bobj in t_bobjs:
            if t_bobj.paths[0] == None:
                dy -= self.line_height
    label_text = TexComplex(*t_bobjs, multiline=True, centered=True, align_y='center', scale=label_scale, name='label', location=(0, dy, 0), rotation_euler=[0, 0, -gest.subbobjects[0].ref_obj.rotation_euler[2]])
    gest.subbobjects[0].add_subbobject(label_text)
    self.add_subbobject(container)","for i in range(t_bobj_count):
    strings = []
    for label in labels:
        strings.append(label[i])
    t_bobj = tex_bobject.TexBobject(*strings, centered=True, color='color2')
    t_bobjs.append(t_bobj)","t_bobjs = [tex_bobject.TexBobject(*[label[i] for label in labels], centered=True, color='color2') for i in range(t_bobj_count)]",Cannot refactor,-1,1,,,,robosuite,,,,,
nova,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nova/nova/tests/unit/scheduler/test_filters.py,https://github.com/openstack/nova/tree/master/nova/tests/unit/scheduler/test_filters.py,FiltersTestCase,test_filter_all_recursive_yields$59,"def test_filter_all_recursive_yields(self, mock_filter_one):
    filter_obj_list = ['obj1', 'obj2', 'obj3']
    spec_obj = objects.RequestSpec()
    base_filter = filters.BaseFilter()
    mock_results = []
    total_iterations = 200
    for x in range(total_iterations):
        mock_results.append(True)
    mock_results.append(False)
    for x in range(total_iterations):
        mock_results.append(True)
    mock_filter_one.side_effect = mock_results
    objs = iter(filter_obj_list)
    for x in range(total_iterations):
        objs = base_filter.filter_all(objs, spec_obj)
    self.assertTrue(inspect.isgenerator(objs))
    self.assertEqual(['obj1', 'obj3'], list(objs))","for x in range(total_iterations):
    mock_results.append(True)",mock_results += [True for x in range(total_iterations)],Cannot refactor,-1,1,,,,robosuite,,,,,
stumpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stumpy/stumpy/aamped.py,https://github.com/TDAmeritrade/stumpy/tree/master/stumpy/aamped.py,,aamped$15,"def aamped(dask_client, T_A, m, T_B=None, ignore_trivial=True):
    """"""
    Compute the non-normalized (i.e., without z-normalization) matrix profile

    This is a highly distributed implementation around the Numba JIT-compiled
    parallelized `_aamp` function which computes the non-normalized matrix profile
    according to AAMP.

    Parameters
    ----------
    dask_client : client
        A Dask Distributed client that is connected to a Dask scheduler and
        Dask workers. Setting up a Dask distributed cluster is beyond the
        scope of this library. Please refer to the Dask Distributed
        documentation.

    T_A : numpy.ndarray
        The time series or sequence for which to compute the matrix profile

    m : int
        Window size

    T_B : numpy.ndarray, default None
        The time series or sequence that will be used to annotate T_A. For every
        subsequence in T_A, its nearest neighbor in T_B will be recorded. Default is
        `None` which corresponds to a self-join.

    ignore_trivial : bool, default True
        Set to `True` if this is a self-join. Otherwise, for AB-join, set this
        to `False`. Default is `True`.

    Returns
    -------
    out : numpy.ndarray
        The first column consists of the matrix profile, the second column
        consists of the matrix profile indices.

    Notes
    -----
    `arXiv:1901.05708     <https://arxiv.org/pdf/1901.05708.pdf>`__

    See Algorithm 1

    Note that we have extended this algorithm for AB-joins as well.
    """"""
    if T_B is None:
        T_B = T_A.copy()
        ignore_trivial = True
    (T_A, T_A_subseq_isfinite) = core.preprocess_non_normalized(T_A, m)
    (T_B, T_B_subseq_isfinite) = core.preprocess_non_normalized(T_B, m)
    if T_A.ndim != 1:
        raise ValueError(f'T_A is {T_A.ndim}-dimensional and must be 1-dimensional. ')
    if T_B.ndim != 1:
        raise ValueError(f'T_B is {T_B.ndim}-dimensional and must be 1-dimensional. ')
    core.check_window_size(m, max_size=min(T_A.shape[0], T_B.shape[0]))
    if ignore_trivial is False and core.are_arrays_equal(T_A, T_B):
        logger.warning('Arrays T_A, T_B are equal, which implies a self-join.')
        logger.warning('Try setting `ignore_trivial = True`.')
    if ignore_trivial and core.are_arrays_equal(T_A, T_B) is False:
        logger.warning('Arrays T_A, T_B are not equal, which implies an AB-join.')
        logger.warning('Try setting `ignore_trivial = False`.')
    n_A = T_A.shape[0]
    n_B = T_B.shape[0]
    l = n_A - m + 1
    excl_zone = int(np.ceil(m / config.STUMPY_EXCL_ZONE_DENOM))
    out = np.empty((l, 4), dtype=object)
    hosts = list(dask_client.ncores().keys())
    nworkers = len(hosts)
    if ignore_trivial:
        diags = np.arange(excl_zone + 1, n_A - m + 1, dtype=np.int64)
    else:
        diags = np.arange(-(n_A - m + 1) + 1, n_B - m + 1, dtype=np.int64)
    ndist_counts = core._count_diagonal_ndist(diags, m, n_A, n_B)
    diags_ranges = core._get_array_ranges(ndist_counts, nworkers, False)
    diags_ranges += diags[0]
    T_A_future = dask_client.scatter(T_A, broadcast=True, hash=False)
    T_B_future = dask_client.scatter(T_B, broadcast=True, hash=False)
    T_A_subseq_isfinite_future = dask_client.scatter(T_A_subseq_isfinite, broadcast=True, hash=False)
    T_B_subseq_isfinite_future = dask_client.scatter(T_B_subseq_isfinite, broadcast=True, hash=False)
    diags_futures = []
    for (i, host) in enumerate(hosts):
        diags_future = dask_client.scatter(np.arange(diags_ranges[i, 0], diags_ranges[i, 1], dtype=np.int64), workers=[host], hash=False)
        diags_futures.append(diags_future)
    futures = []
    for i in range(len(hosts)):
        futures.append(dask_client.submit(_aamp, T_A_future, T_B_future, m, T_A_subseq_isfinite_future, T_B_subseq_isfinite_future, diags_futures[i], ignore_trivial))
    results = dask_client.gather(futures)
    (profile, indices) = results[0]
    for i in range(1, len(hosts)):
        (P, I) = results[i]
        for col in range(P.shape[1]):
            cond = P[:, col] < profile[:, col]
            profile[:, col] = np.where(cond, P[:, col], profile[:, col])
            indices[:, col] = np.where(cond, I[:, col], indices[:, col])
    out[:, 0] = profile[:, 0]
    out[:, 1:4] = indices
    dask_client.cancel(T_A_future)
    dask_client.cancel(T_B_future)
    dask_client.cancel(T_A_subseq_isfinite_future)
    dask_client.cancel(T_B_subseq_isfinite_future)
    for diags_future in diags_futures:
        dask_client.cancel(diags_future)
    for future in futures:
        dask_client.cancel(future)
    threshold = 1e-05
    if core.are_distances_too_small(out[:, 0], threshold=threshold):
        logger.warning(f'A large number of values are smaller than {threshold}.')
        logger.warning('For a self-join, try setting `ignore_trivial = True`.')
    return out","for (i, host) in enumerate(hosts):
    diags_future = dask_client.scatter(np.arange(diags_ranges[i, 0], diags_ranges[i, 1], dtype=np.int64), workers=[host], hash=False)
    diags_futures.append(diags_future)","diags_futures = [dask_client.scatter(np.arange(diags_ranges[i, 0], diags_ranges[i, 1], dtype=np.int64), workers=[host], hash=False) for (i, host) in enumerate(hosts)]",Cannot refactor,-1,1,,,,robosuite,,,,,
AutoDL-Projects,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AutoDL-Projects/exps/NATS-Bench/draw-fig2_5.py,https://github.com/D-X-Y/AutoDL-Projects/tree/master/exps/NATS-Bench/draw-fig2_5.py,,calculate_correlation$517,"def calculate_correlation(*vectors):
    matrix = []
    for (i, vectori) in enumerate(vectors):
        x = []
        for (j, vectorj) in enumerate(vectors):
            x.append(compute_kendalltau(vectori, vectorj))
        matrix.append(x)
    return np.array(matrix)","for (i, vectori) in enumerate(vectors):
    x = []
    for (j, vectorj) in enumerate(vectors):
        x.append(compute_kendalltau(vectori, vectorj))
    matrix.append(x)","matrix = [[compute_kendalltau(vectori, vectorj) for (j, vectorj) in enumerate(vectors)] for (i, vectori) in enumerate(vectors)]",Cannot refactor,-1,1,,,,robosuite,,,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/cwl/util.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/cwl/util.py,,galactic_job_json$115,"def galactic_job_json(job, test_data_directory, upload_func, collection_create_func, tool_or_workflow='workflow'):
    """"""Adapt a CWL job object to the Galaxy API.

    CWL derived tools in Galaxy can consume a job description sort of like
    CWL job objects via the API but paths need to be replaced with datasets
    and records and arrays with collection references. This function will
    stage files and modify the job description to adapt to these changes
    for Galaxy.
    """"""
    datasets = []
    dataset_collections = []

    def response_to_hda(target, upload_response):
        assert isinstance(upload_response, dict), upload_response
        assert 'outputs' in upload_response, upload_response
        assert len(upload_response['outputs']) > 0, upload_response
        dataset = upload_response['outputs'][0]
        datasets.append((dataset, target))
        dataset_id = dataset['id']
        return {'src': 'hda', 'id': dataset_id}

    def upload_file(file_path, secondary_files, **kwargs):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = FileUploadTarget(file_path, secondary_files, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_literal(contents, **kwd):
        target = FileLiteralTarget(contents, **kwd)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_tar(file_path):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = DirectoryUploadTarget(file_path)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_with_composite_data(file_path, composite_data, **kwargs):
        if file_path is not None:
            file_path = abs_path_or_uri(file_path, test_data_directory)
        composite_data_resolved = []
        for cd in composite_data:
            composite_data_resolved.append(abs_path_or_uri(cd, test_data_directory))
        target = FileUploadTarget(file_path, composite_data=composite_data_resolved, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_object(the_object):
        target = ObjectUploadTarget(the_object)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def replacement_item(value, force_to_file=False):
        is_dict = isinstance(value, dict)
        item_class = None if not is_dict else value.get('class', None)
        is_file = item_class == 'File'
        is_directory = item_class == 'Directory'
        is_collection = item_class == 'Collection'
        if force_to_file:
            if is_file:
                return replacement_file(value)
            else:
                return upload_object(value)
        if isinstance(value, list):
            return replacement_list(value)
        elif not isinstance(value, dict):
            if tool_or_workflow == 'workflow':
                return upload_object(value)
            else:
                return value
        if is_file:
            return replacement_file(value)
        elif is_directory:
            return replacement_directory(value)
        elif is_collection:
            return replacement_collection(value)
        else:
            return replacement_record(value)

    def replacement_file(value):
        if value.get('galaxy_id'):
            return {'src': 'hda', 'id': str(value['galaxy_id'])}
        file_path = value.get('location', None) or value.get('path', None)
        filetype = value.get('filetype', None) or value.get('format', None)
        composite_data_raw = value.get('composite_data', None)
        kwd = {}
        if 'tags' in value:
            kwd['tags'] = value.get('tags')
        if 'dbkey' in value:
            kwd['dbkey'] = value.get('dbkey')
        if composite_data_raw:
            composite_data = []
            for entry in composite_data_raw:
                path = None
                if isinstance(entry, dict):
                    path = entry.get('location', None) or entry.get('path', None)
                else:
                    path = entry
                composite_data.append(path)
            rval_c = upload_file_with_composite_data(None, composite_data, filetype=filetype, **kwd)
            return rval_c
        if file_path is None:
            contents = value.get('contents', None)
            if contents is not None:
                return upload_file_literal(contents, **kwd)
            return value
        secondary_files = value.get('secondaryFiles', [])
        secondary_files_tar_path = None
        if secondary_files:
            tmp = tempfile.NamedTemporaryFile(delete=False)
            tf = tarfile.open(fileobj=tmp, mode='w:')
            order: List[str] = []
            index_contents = {'order': order}
            for secondary_file in secondary_files:
                secondary_file_path = secondary_file.get('location', None) or secondary_file.get('path', None)
                assert secondary_file_path, f'Invalid secondaryFile entry found [{secondary_file}]'
                full_secondary_file_path = os.path.join(test_data_directory, secondary_file_path)
                basename = secondary_file.get('basename') or os.path.basename(secondary_file_path)
                order.append(unicodify(basename))
                tf.add(full_secondary_file_path, os.path.join(SECONDARY_FILES_EXTRA_PREFIX, basename))
            tmp_index = tempfile.NamedTemporaryFile(delete=False, mode='w')
            json.dump(index_contents, tmp_index)
            tmp_index.close()
            tf.add(tmp_index.name, SECONDARY_FILES_INDEX_PATH)
            tf.close()
            secondary_files_tar_path = tmp.name
        return upload_file(file_path, secondary_files_tar_path, filetype=filetype, **kwd)

    def replacement_directory(value):
        file_path = value.get('location', None) or value.get('path', None)
        if file_path is None:
            return value
        if not os.path.isabs(file_path):
            file_path = os.path.join(test_data_directory, file_path)
        tmp = tempfile.NamedTemporaryFile(delete=False)
        tf = tarfile.open(fileobj=tmp, mode='w:')
        tf.add(file_path, '.')
        tf.close()
        return upload_tar(tmp.name)

    def replacement_list(value):
        collection_element_identifiers = []
        for (i, item) in enumerate(value):
            dataset = replacement_item(item, force_to_file=True)
            collection_element = dataset.copy()
            collection_element['name'] = str(i)
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'list')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def to_elements(value, rank_collection_type):
        collection_element_identifiers = []
        assert 'elements' in value
        elements = value['elements']
        is_nested_collection = ':' in rank_collection_type
        for element in elements:
            if not is_nested_collection:
                dataset = replacement_item(element, force_to_file=True)
                collection_element = dataset.copy()
                collection_element['name'] = element['identifier']
                collection_element_identifiers.append(collection_element)
            else:
                sub_collection_type = rank_collection_type[rank_collection_type.find(':') + 1:]
                collection_element = {'name': element['identifier'], 'src': 'new_collection', 'collection_type': sub_collection_type, 'element_identifiers': to_elements(element, sub_collection_type)}
                collection_element_identifiers.append(collection_element)
        return collection_element_identifiers

    def replacement_collection(value):
        if value.get('galaxy_id'):
            return {'src': 'hdca', 'id': str(value['galaxy_id'])}
        assert 'collection_type' in value
        collection_type = value['collection_type']
        elements = to_elements(value, collection_type)
        collection = collection_create_func(elements, collection_type)
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def replacement_record(value):
        collection_element_identifiers = []
        for (record_key, record_value) in value.items():
            if not isinstance(record_value, dict) or record_value.get('class') != 'File':
                dataset = replacement_item(record_value, force_to_file=True)
                collection_element = dataset.copy()
            else:
                dataset = upload_file(record_value['location'], [])
                collection_element = dataset.copy()
            collection_element['name'] = record_key
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'record')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}
    replace_keys = {}
    for (key, value) in job.items():
        replace_keys[key] = replacement_item(value)
    job.update(replace_keys)
    return (job, datasets)","for element in elements:
    if not is_nested_collection:
        dataset = replacement_item(element, force_to_file=True)
        collection_element = dataset.copy()
        collection_element['name'] = element['identifier']
        collection_element_identifiers.append(collection_element)
    else:
        sub_collection_type = rank_collection_type[rank_collection_type.find(':') + 1:]
        collection_element = {'name': element['identifier'], 'src': 'new_collection', 'collection_type': sub_collection_type, 'element_identifiers': to_elements(element, sub_collection_type)}
        collection_element_identifiers.append(collection_element)","collection_element_identifiers += [{'name': element['identifier'], 'src': 'new_collection', 'collection_type': sub_collection_type, 'element_identifiers': to_elements(element, sub_collection_type)} if is_nested_collection else replacement_item(element, force_to_file=True).copy() | {'name': element['identifier']} for element in elements]",Cannot refactor,-1,1,,,,robosuite,,,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_util/cwl/util.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_util/cwl/util.py,,galactic_job_json$115,"def galactic_job_json(job, test_data_directory, upload_func, collection_create_func, tool_or_workflow='workflow'):
    """"""Adapt a CWL job object to the Galaxy API.

    CWL derived tools in Galaxy can consume a job description sort of like
    CWL job objects via the API but paths need to be replaced with datasets
    and records and arrays with collection references. This function will
    stage files and modify the job description to adapt to these changes
    for Galaxy.
    """"""
    datasets = []
    dataset_collections = []

    def response_to_hda(target, upload_response):
        assert isinstance(upload_response, dict), upload_response
        assert 'outputs' in upload_response, upload_response
        assert len(upload_response['outputs']) > 0, upload_response
        dataset = upload_response['outputs'][0]
        datasets.append((dataset, target))
        dataset_id = dataset['id']
        return {'src': 'hda', 'id': dataset_id}

    def upload_file(file_path, secondary_files, **kwargs):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = FileUploadTarget(file_path, secondary_files, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_literal(contents, **kwd):
        target = FileLiteralTarget(contents, **kwd)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_tar(file_path):
        file_path = abs_path_or_uri(file_path, test_data_directory)
        target = DirectoryUploadTarget(file_path)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_file_with_composite_data(file_path, composite_data, **kwargs):
        if file_path is not None:
            file_path = abs_path_or_uri(file_path, test_data_directory)
        composite_data_resolved = []
        for cd in composite_data:
            composite_data_resolved.append(abs_path_or_uri(cd, test_data_directory))
        target = FileUploadTarget(file_path, composite_data=composite_data_resolved, **kwargs)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def upload_object(the_object):
        target = ObjectUploadTarget(the_object)
        upload_response = upload_func(target)
        return response_to_hda(target, upload_response)

    def replacement_item(value, force_to_file=False):
        is_dict = isinstance(value, dict)
        item_class = None if not is_dict else value.get('class', None)
        is_file = item_class == 'File'
        is_directory = item_class == 'Directory'
        is_collection = item_class == 'Collection'
        if force_to_file:
            if is_file:
                return replacement_file(value)
            else:
                return upload_object(value)
        if isinstance(value, list):
            return replacement_list(value)
        elif not isinstance(value, dict):
            if tool_or_workflow == 'workflow':
                return upload_object(value)
            else:
                return value
        if is_file:
            return replacement_file(value)
        elif is_directory:
            return replacement_directory(value)
        elif is_collection:
            return replacement_collection(value)
        else:
            return replacement_record(value)

    def replacement_file(value):
        if value.get('galaxy_id'):
            return {'src': 'hda', 'id': str(value['galaxy_id'])}
        file_path = value.get('location', None) or value.get('path', None)
        filetype = value.get('filetype', None) or value.get('format', None)
        composite_data_raw = value.get('composite_data', None)
        kwd = {}
        if 'tags' in value:
            kwd['tags'] = value.get('tags')
        if 'dbkey' in value:
            kwd['dbkey'] = value.get('dbkey')
        if composite_data_raw:
            composite_data = []
            for entry in composite_data_raw:
                path = None
                if isinstance(entry, dict):
                    path = entry.get('location', None) or entry.get('path', None)
                else:
                    path = entry
                composite_data.append(path)
            rval_c = upload_file_with_composite_data(None, composite_data, filetype=filetype, **kwd)
            return rval_c
        if file_path is None:
            contents = value.get('contents', None)
            if contents is not None:
                return upload_file_literal(contents, **kwd)
            return value
        secondary_files = value.get('secondaryFiles', [])
        secondary_files_tar_path = None
        if secondary_files:
            tmp = tempfile.NamedTemporaryFile(delete=False)
            tf = tarfile.open(fileobj=tmp, mode='w:')
            order: List[str] = []
            index_contents = {'order': order}
            for secondary_file in secondary_files:
                secondary_file_path = secondary_file.get('location', None) or secondary_file.get('path', None)
                assert secondary_file_path, f'Invalid secondaryFile entry found [{secondary_file}]'
                full_secondary_file_path = os.path.join(test_data_directory, secondary_file_path)
                basename = secondary_file.get('basename') or os.path.basename(secondary_file_path)
                order.append(unicodify(basename))
                tf.add(full_secondary_file_path, os.path.join(SECONDARY_FILES_EXTRA_PREFIX, basename))
            tmp_index = tempfile.NamedTemporaryFile(delete=False, mode='w')
            json.dump(index_contents, tmp_index)
            tmp_index.close()
            tf.add(tmp_index.name, SECONDARY_FILES_INDEX_PATH)
            tf.close()
            secondary_files_tar_path = tmp.name
        return upload_file(file_path, secondary_files_tar_path, filetype=filetype, **kwd)

    def replacement_directory(value):
        file_path = value.get('location', None) or value.get('path', None)
        if file_path is None:
            return value
        if not os.path.isabs(file_path):
            file_path = os.path.join(test_data_directory, file_path)
        tmp = tempfile.NamedTemporaryFile(delete=False)
        tf = tarfile.open(fileobj=tmp, mode='w:')
        tf.add(file_path, '.')
        tf.close()
        return upload_tar(tmp.name)

    def replacement_list(value):
        collection_element_identifiers = []
        for (i, item) in enumerate(value):
            dataset = replacement_item(item, force_to_file=True)
            collection_element = dataset.copy()
            collection_element['name'] = str(i)
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'list')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def to_elements(value, rank_collection_type):
        collection_element_identifiers = []
        assert 'elements' in value
        elements = value['elements']
        is_nested_collection = ':' in rank_collection_type
        for element in elements:
            if not is_nested_collection:
                dataset = replacement_item(element, force_to_file=True)
                collection_element = dataset.copy()
                collection_element['name'] = element['identifier']
                collection_element_identifiers.append(collection_element)
            else:
                sub_collection_type = rank_collection_type[rank_collection_type.find(':') + 1:]
                collection_element = {'name': element['identifier'], 'src': 'new_collection', 'collection_type': sub_collection_type, 'element_identifiers': to_elements(element, sub_collection_type)}
                collection_element_identifiers.append(collection_element)
        return collection_element_identifiers

    def replacement_collection(value):
        if value.get('galaxy_id'):
            return {'src': 'hdca', 'id': str(value['galaxy_id'])}
        assert 'collection_type' in value
        collection_type = value['collection_type']
        elements = to_elements(value, collection_type)
        collection = collection_create_func(elements, collection_type)
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}

    def replacement_record(value):
        collection_element_identifiers = []
        for (record_key, record_value) in value.items():
            if not isinstance(record_value, dict) or record_value.get('class') != 'File':
                dataset = replacement_item(record_value, force_to_file=True)
                collection_element = dataset.copy()
            else:
                dataset = upload_file(record_value['location'], [])
                collection_element = dataset.copy()
            collection_element['name'] = record_key
            collection_element_identifiers.append(collection_element)
        collection = collection_create_func(collection_element_identifiers, 'record')
        dataset_collections.append(collection)
        hdca_id = collection['id']
        return {'src': 'hdca', 'id': hdca_id}
    replace_keys = {}
    for (key, value) in job.items():
        replace_keys[key] = replacement_item(value)
    job.update(replace_keys)
    return (job, datasets)","for entry in composite_data_raw:
    path = None
    if isinstance(entry, dict):
        path = entry.get('location', None) or entry.get('path', None)
    else:
        path = entry
    composite_data.append(path)","composite_data += [entry.get('location', None) or entry.get('path', None) if isinstance(entry, dict) else entry for entry in composite_data_raw]",Cannot refactor,-1,1,,,,robosuite,,,,,
GAM,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/GAM/src/gam/gapi/vault.py,https://github.com/jay0lee/GAM/tree/master/src/gam/gapi/vault.py,,printExports$851,"def printExports():
    v = buildGAPIObject()
    todrive = False
    csvRows = []
    initialTitles = ['matterId', 'id', 'name', 'createTime', 'status']
    titles = initialTitles[:]
    matters = []
    matterIds = []
    i = 3
    while i < len(sys.argv):
        myarg = sys.argv[i].lower().replace('_', '')
        if myarg == 'todrive':
            todrive = True
            i += 1
        elif myarg in ['matter', 'matters']:
            matters = sys.argv[i + 1].split(',')
            i += 2
        else:
            controlflow.invalid_argument_exit(myarg, 'gam print exports')
    if not matters:
        fields = 'matters(matterId),nextPageToken'
        matters_results = gapi.get_all_pages(v.matters(), 'list', 'matters', view='BASIC', state='OPEN', fields=fields)
        for matter in matters_results:
            matterIds.append(matter['matterId'])
    else:
        for matter in matters:
            matterIds.append(getMatterItem(v, matter))
    for matterId in matterIds:
        sys.stderr.write(f'Retrieving exports for matter {matterId}\n')
        exports = gapi.get_all_pages(v.matters().exports(), 'list', 'exports', matterId=matterId)
        for export in exports:
            display.add_row_titles_to_csv_file(utils.flatten_json(export, flattened={'matterId': matterId}), csvRows, titles)
    display.sort_csv_titles(initialTitles, titles)
    display.write_csv_file(csvRows, titles, 'Vault Exports', todrive)","for matter in matters:
    matterIds.append(getMatterItem(v, matter))","matterIds += [getMatterItem(v, matter) for matter in matters]",Cannot refactor,-1,1,,,,robosuite,,,,,
Det3D,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Det3D/det3d/utils/dist/dist_common.py,https://github.com/poodarchu/Det3D/tree/master/det3d/utils/dist/dist_common.py,,all_gather$48,"def all_gather(data):
    """"""
    Run all_gather on arbitrary picklable data (not necessarily tensors)
    Args:
        data: any picklable object
    Returns:
        list[data]: list of data gathered from each rank
    """"""
    world_size = get_world_size()
    if world_size == 1:
        return [data]
    buffer = pickle.dumps(data)
    storage = torch.ByteStorage.from_buffer(buffer)
    tensor = torch.ByteTensor(storage).to('cuda')
    local_size = torch.IntTensor([tensor.numel()]).to('cuda')
    size_list = [torch.IntTensor([0]).to('cuda') for _ in range(world_size)]
    dist.all_gather(size_list, local_size)
    size_list = [int(size.item()) for size in size_list]
    max_size = max(size_list)
    tensor_list = []
    for _ in size_list:
        tensor_list.append(torch.ByteTensor(size=(max_size,)).to('cuda'))
    if local_size != max_size:
        padding = torch.ByteTensor(size=(max_size - local_size,)).to('cuda')
        tensor = torch.cat((tensor, padding), dim=0)
    dist.all_gather(tensor_list, tensor)
    data_list = []
    for (size, tensor) in zip(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]
        data_list.append(pickle.loads(buffer))
    return data_list","for (size, tensor) in zip(size_list, tensor_list):
    buffer = tensor.cpu().numpy().tobytes()[:size]
    data_list.append(pickle.loads(buffer))","data_list = [pickle.loads(tensor.cpu().numpy().tobytes()[:size]) for (size, tensor) in zip(size_list, tensor_list)]",Cannot refactor,-1,1,,,,robosuite,,,,,
crnn_ctc_ocr_tf,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/crnn_ctc_ocr_tf/tools/train_crnn_ctc.py,https://github.com/bai-shang/crnn_ctc_ocr_tf/tree/master/tools/train_crnn_ctc.py,,_sparse_matrix_to_list$63,"def _sparse_matrix_to_list(sparse_matrix, char_map_dict=None):
    indices = sparse_matrix.indices
    values = sparse_matrix.values
    dense_shape = sparse_matrix.dense_shape
    if char_map_dict is None:
        char_map_dict = json.load(open(FLAGS.char_map_json_file, 'r'))
    assert isinstance(char_map_dict, dict) and 'char_map_dict is not a dict'
    dense_matrix = len(char_map_dict.keys()) * np.ones(dense_shape, dtype=np.int32)
    for (i, indice) in enumerate(indices):
        dense_matrix[indice[0], indice[1]] = values[i]
    string_list = []
    for row in dense_matrix:
        string = []
        for val in row:
            string.append(_int_to_string(val, char_map_dict))
        string_list.append(''.join((s for s in string if s != '*')))
    return string_list","for row in dense_matrix:
    string = []
    for val in row:
        string.append(_int_to_string(val, char_map_dict))
    string_list.append(''.join((s for s in string if s != '*')))","string_list = [''.join([_int_to_string(val, char_map_dict) for val in row if _int_to_string(val, char_map_dict) != '*']) for row in dense_matrix]",Cannot refactor,-1,1,,,,robosuite,,,,,
platform-espressif32,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/platform-espressif32/builder/frameworks/espidf.py,https://github.com/platformio/platform-espressif32/tree/master/builder/frameworks/espidf.py,,load_component_paths$422,"def load_component_paths(framework_components_dir, ignored_component_prefixes=None):

    def _scan_components_from_framework():
        result = []
        for component in os.listdir(framework_components_dir):
            component_path = os.path.join(framework_components_dir, component)
            if component.startswith(ignored_component_prefixes) or not os.path.isdir(component_path):
                continue
            result.append(component_path)
        return result
    components = []
    ignored_component_prefixes = ignored_component_prefixes or []
    project_description_file = os.path.join(BUILD_DIR, 'project_description.json')
    if os.path.isfile(project_description_file):
        with open(project_description_file) as fp:
            try:
                data = json.load(fp)
                for path in data.get('build_component_paths', []):
                    if not os.path.basename(path).startswith(ignored_component_prefixes):
                        components.append(path)
            except:
                print('Warning: Could not find load components from project description!\n')
    return components or _scan_components_from_framework()","for component in os.listdir(framework_components_dir):
    component_path = os.path.join(framework_components_dir, component)
    if component.startswith(ignored_component_prefixes) or not os.path.isdir(component_path):
        continue
    result.append(component_path)","result = [os.path.join(framework_components_dir, component) for component in os.listdir(framework_components_dir) if not component.startswith(ignored_component_prefixes) and os.path.isdir(os.path.join(framework_components_dir, component))]",Cannot refactor,-1,1,,,,robosuite,,,,,
you-get,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/you-get/src/you_get/extractors/vimeo.py,https://github.com/soimort/you-get/tree/master/src/you_get/extractors/vimeo.py,VimeoExtractor,extract$75,"def extract(self, **kwargs):
    for s in self.streams:
        self.streams[s]['size'] = urls_size(self.streams[s]['src'])
    master_m3u8s = []
    for m in self.master_m3u8:
        master_m3u8s.append(self.master_m3u8[m]['url'])
    master_content = None
    master_url = None
    for master_u in master_m3u8s:
        try:
            master_content = get_content(master_u).split('\n')
        except urllib.error.URLError:
            continue
        else:
            master_url = master_u
    if master_content is None:
        return
    lines = []
    for line in master_content:
        if len(line.strip()) > 0:
            lines.append(line.strip())
    pos = 0
    while pos < len(lines):
        if lines[pos].startswith('#EXT-X-STREAM-INF'):
            patt = 'RESOLUTION=(\\d+)x(\\d+)'
            hit = re.search(patt, lines[pos])
            if hit is None:
                continue
            width = hit.group(1)
            height = hit.group(2)
            if height in ('2160', '1440'):
                m3u8_url = urllib.parse.urljoin(master_url, lines[pos + 1])
                meta = dict(m3u8_url=m3u8_url, container='m3u8')
                if height == '1440':
                    meta['video_profile'] = '2560x1440'
                else:
                    meta['video_profile'] = '3840x2160'
                meta['size'] = 0
                meta['src'] = general_m3u8_extractor(m3u8_url)
                self.streams[height + 'p'] = meta
            pos += 2
        else:
            pos += 1
    self.streams_sorted = []
    for stream_type in self.stream_types:
        if stream_type['id'] in self.streams:
            item = [('id', stream_type['id'])] + list(self.streams[stream_type['id']].items())
            self.streams_sorted.append(dict(item))","for stream_type in self.stream_types:
    if stream_type['id'] in self.streams:
        item = [('id', stream_type['id'])] + list(self.streams[stream_type['id']].items())
        self.streams_sorted.append(dict(item))","self.streams_sorted = [dict([('id', stream_type['id'])] + list(self.streams[stream_type['id']].items())) for stream_type in self.stream_types if stream_type['id'] in self.streams]",Cannot refactor,-1,1,,,,robosuite,,,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/states/boto_elb.py,https://github.com/saltstack/salt/tree/master/salt/states/boto_elb.py,,_listeners_present$804,"def _listeners_present(name, listeners, region, key, keyid, profile):
    ret = {'result': True, 'comment': '', 'changes': {}}
    lb = __salt__['boto_elb.get_elb_config'](name, region, key, keyid, profile)
    if not lb:
        ret['comment'] = '{} ELB configuration could not be retrieved.'.format(name)
        ret['result'] = False
        return ret
    if not listeners:
        listeners = []
    expected_listeners_by_tuple = {}
    for l in listeners:
        l_key = __salt__['boto_elb.listener_dict_to_tuple'](l)
        expected_listeners_by_tuple[l_key] = l
    actual_listeners_by_tuple = {}
    for l in lb['listeners']:
        l_key = __salt__['boto_elb.listener_dict_to_tuple'](l)
        actual_listeners_by_tuple[l_key] = l
    to_delete = []
    to_create = []
    for (t, l) in expected_listeners_by_tuple.items():
        if t not in actual_listeners_by_tuple:
            to_create.append(l)
    for (t, l) in actual_listeners_by_tuple.items():
        if t not in expected_listeners_by_tuple:
            to_delete.append(l)
    if __opts__['test']:
        msg = []
        if to_create or to_delete:
            msg.append('ELB {} set to have listeners modified:'.format(name))
            for listener in to_create:
                msg.append('Listener {} added.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))
            for listener in to_delete:
                msg.append('Listener {} deleted.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))
            ret['result'] = None
        else:
            msg.append('Listeners already set on ELB {}.'.format(name))
        ret['comment'] = '  '.join(msg)
        return ret
    if to_delete:
        ports = [l['elb_port'] for l in to_delete]
        deleted = __salt__['boto_elb.delete_listeners'](name, ports, region, key, keyid, profile)
        if deleted:
            ret['comment'] = 'Deleted listeners on {} ELB.'.format(name)
        else:
            ret['comment'] = 'Failed to delete listeners on {} ELB.'.format(name)
            ret['result'] = False
    if to_create:
        created = __salt__['boto_elb.create_listeners'](name, to_create, region, key, keyid, profile)
        if created:
            msg = 'Created listeners on {0} ELB.'
            ret['comment'] = ' '.join([ret['comment'], msg.format(name)])
        else:
            msg = 'Failed to create listeners on {0} ELB.'
            ret['comment'] = ' '.join([ret['comment'], msg.format(name)])
            ret['result'] = False
    if to_create or to_delete:
        ret['changes']['listeners'] = {}
        ret['changes']['listeners']['old'] = lb['listeners']
        lb = __salt__['boto_elb.get_elb_config'](name, region, key, keyid, profile)
        ret['changes']['listeners']['new'] = lb['listeners']
    else:
        ret['comment'] = 'Listeners already set on ELB {}.'.format(name)
    return ret","for listener in to_create:
    msg.append('Listener {} added.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))",msg += ['Listener {} added.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)) for listener in to_create],Cannot refactor,-1,1,,,,robosuite,,,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/states/boto_elb.py,https://github.com/saltstack/salt/tree/master/salt/states/boto_elb.py,,_listeners_present$804,"def _listeners_present(name, listeners, region, key, keyid, profile):
    ret = {'result': True, 'comment': '', 'changes': {}}
    lb = __salt__['boto_elb.get_elb_config'](name, region, key, keyid, profile)
    if not lb:
        ret['comment'] = '{} ELB configuration could not be retrieved.'.format(name)
        ret['result'] = False
        return ret
    if not listeners:
        listeners = []
    expected_listeners_by_tuple = {}
    for l in listeners:
        l_key = __salt__['boto_elb.listener_dict_to_tuple'](l)
        expected_listeners_by_tuple[l_key] = l
    actual_listeners_by_tuple = {}
    for l in lb['listeners']:
        l_key = __salt__['boto_elb.listener_dict_to_tuple'](l)
        actual_listeners_by_tuple[l_key] = l
    to_delete = []
    to_create = []
    for (t, l) in expected_listeners_by_tuple.items():
        if t not in actual_listeners_by_tuple:
            to_create.append(l)
    for (t, l) in actual_listeners_by_tuple.items():
        if t not in expected_listeners_by_tuple:
            to_delete.append(l)
    if __opts__['test']:
        msg = []
        if to_create or to_delete:
            msg.append('ELB {} set to have listeners modified:'.format(name))
            for listener in to_create:
                msg.append('Listener {} added.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))
            for listener in to_delete:
                msg.append('Listener {} deleted.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))
            ret['result'] = None
        else:
            msg.append('Listeners already set on ELB {}.'.format(name))
        ret['comment'] = '  '.join(msg)
        return ret
    if to_delete:
        ports = [l['elb_port'] for l in to_delete]
        deleted = __salt__['boto_elb.delete_listeners'](name, ports, region, key, keyid, profile)
        if deleted:
            ret['comment'] = 'Deleted listeners on {} ELB.'.format(name)
        else:
            ret['comment'] = 'Failed to delete listeners on {} ELB.'.format(name)
            ret['result'] = False
    if to_create:
        created = __salt__['boto_elb.create_listeners'](name, to_create, region, key, keyid, profile)
        if created:
            msg = 'Created listeners on {0} ELB.'
            ret['comment'] = ' '.join([ret['comment'], msg.format(name)])
        else:
            msg = 'Failed to create listeners on {0} ELB.'
            ret['comment'] = ' '.join([ret['comment'], msg.format(name)])
            ret['result'] = False
    if to_create or to_delete:
        ret['changes']['listeners'] = {}
        ret['changes']['listeners']['old'] = lb['listeners']
        lb = __salt__['boto_elb.get_elb_config'](name, region, key, keyid, profile)
        ret['changes']['listeners']['new'] = lb['listeners']
    else:
        ret['comment'] = 'Listeners already set on ELB {}.'.format(name)
    return ret","for listener in to_delete:
    msg.append('Listener {} deleted.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)))",msg += ['Listener {} deleted.'.format(__salt__['boto_elb.listener_dict_to_tuple'](listener)) for listener in to_delete],Cannot refactor,-1,1,,,,robosuite,,,,,
FairMOT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FairMOT/src/lib/models/networks/pose_hrnet.py,https://github.com/ifzhang/FairMOT/tree/master/src/lib/models/networks/pose_hrnet.py,PoseHighResolutionNet,forward$462,"def forward(self, x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)
    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu(x)
    x = self.layer1(x)
    x_list = []
    for i in range(self.stage2_cfg['NUM_BRANCHES']):
        if self.transition1[i] is not None:
            x_list.append(self.transition1[i](x))
        else:
            x_list.append(x)
    y_list = self.stage2(x_list)
    x_list = []
    for i in range(self.stage3_cfg['NUM_BRANCHES']):
        if self.transition2[i] is not None:
            if i < self.stage2_cfg['NUM_BRANCHES']:
                x_list.append(self.transition2[i](y_list[i]))
            else:
                x_list.append(self.transition2[i](y_list[-1]))
        else:
            x_list.append(y_list[i])
    y_list = self.stage3(x_list)
    x_list = []
    for i in range(self.stage4_cfg['NUM_BRANCHES']):
        if self.transition3[i] is not None:
            if i < self.stage3_cfg['NUM_BRANCHES']:
                x_list.append(self.transition3[i](y_list[i]))
            else:
                x_list.append(self.transition3[i](y_list[-1]))
        else:
            x_list.append(y_list[i])
    x = self.stage4(x_list)
    (x0_h, x0_w) = (x[0].size(2), x[0].size(3))
    x1 = F.upsample(x[1], size=(x0_h, x0_w), mode='bilinear')
    x2 = F.upsample(x[2], size=(x0_h, x0_w), mode='bilinear')
    x3 = F.upsample(x[3], size=(x0_h, x0_w), mode='bilinear')
    x = torch.cat([x[0], x1, x2, x3], 1)
    z = {}
    for head in self.heads:
        z[head] = self.__getattr__(head)(x)
    return [z]","for i in range(self.stage3_cfg['NUM_BRANCHES']):
    if self.transition2[i] is not None:
        if i < self.stage2_cfg['NUM_BRANCHES']:
            x_list.append(self.transition2[i](y_list[i]))
        else:
            x_list.append(self.transition2[i](y_list[-1]))
    else:
        x_list.append(y_list[i])",x_list += [self.transition2[i](y_list[i]) if self.transition2[i] is not None and i < self.stage2_cfg['NUM_BRANCHES'] else self.transition2[i](y_list[-1]) if self.transition2[i] is not None else y_list[i] for i in range(self.stage3_cfg['NUM_BRANCHES'])],Cannot refactor,-1,1,,,,robosuite,,,,,
FairMOT,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/FairMOT/src/lib/models/networks/pose_hrnet.py,https://github.com/ifzhang/FairMOT/tree/master/src/lib/models/networks/pose_hrnet.py,PoseHighResolutionNet,forward$462,"def forward(self, x):
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)
    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu(x)
    x = self.layer1(x)
    x_list = []
    for i in range(self.stage2_cfg['NUM_BRANCHES']):
        if self.transition1[i] is not None:
            x_list.append(self.transition1[i](x))
        else:
            x_list.append(x)
    y_list = self.stage2(x_list)
    x_list = []
    for i in range(self.stage3_cfg['NUM_BRANCHES']):
        if self.transition2[i] is not None:
            if i < self.stage2_cfg['NUM_BRANCHES']:
                x_list.append(self.transition2[i](y_list[i]))
            else:
                x_list.append(self.transition2[i](y_list[-1]))
        else:
            x_list.append(y_list[i])
    y_list = self.stage3(x_list)
    x_list = []
    for i in range(self.stage4_cfg['NUM_BRANCHES']):
        if self.transition3[i] is not None:
            if i < self.stage3_cfg['NUM_BRANCHES']:
                x_list.append(self.transition3[i](y_list[i]))
            else:
                x_list.append(self.transition3[i](y_list[-1]))
        else:
            x_list.append(y_list[i])
    x = self.stage4(x_list)
    (x0_h, x0_w) = (x[0].size(2), x[0].size(3))
    x1 = F.upsample(x[1], size=(x0_h, x0_w), mode='bilinear')
    x2 = F.upsample(x[2], size=(x0_h, x0_w), mode='bilinear')
    x3 = F.upsample(x[3], size=(x0_h, x0_w), mode='bilinear')
    x = torch.cat([x[0], x1, x2, x3], 1)
    z = {}
    for head in self.heads:
        z[head] = self.__getattr__(head)(x)
    return [z]","for i in range(self.stage4_cfg['NUM_BRANCHES']):
    if self.transition3[i] is not None:
        if i < self.stage3_cfg['NUM_BRANCHES']:
            x_list.append(self.transition3[i](y_list[i]))
        else:
            x_list.append(self.transition3[i](y_list[-1]))
    else:
        x_list.append(y_list[i])",x_list += [self.transition3[i](y_list[i]) if self.transition3[i] is not None and i < self.stage3_cfg['NUM_BRANCHES'] else self.transition3[i](y_list[-1]) if self.transition3[i] is not None else y_list[i] for i in range(self.stage4_cfg['NUM_BRANCHES'])],Cannot refactor,-1,1,,,,robosuite,,,,,
extruct,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/extruct/extruct/w3cmicrodata.py,https://github.com/scrapinghub/extruct/tree/master/extruct/w3cmicrodata.py,LxmlMicrodataExtractor,_extract_item$84,"def _extract_item(self, node, items_seen, base_url, itemids):
    itemid = self.get_docid(node, itemids)
    if self.nested:
        if itemid in items_seen:
            return
        items_seen.add(itemid)
    item = {}
    if not self.nested:
        item['iid'] = itemid
    types = node.get('itemtype', '').split()
    if types:
        if not self.strict and len(types) == 1:
            item['type'] = types[0]
        else:
            item['type'] = types
        nodeid = node.get('itemid')
        if nodeid:
            item['id'] = nodeid.strip()
    properties = collections.defaultdict(list)
    for (name, value) in self._extract_properties(node, items_seen=items_seen, base_url=base_url, itemids=itemids):
        properties[name].append(value)
    refs = node.get('itemref', '').split()
    if refs:
        for refid in refs:
            for (name, value) in self._extract_property_refs(node, refid, items_seen=items_seen, base_url=base_url, itemids=itemids):
                properties[name].append(value)
    props = []
    for (name, values) in properties.items():
        if not self.strict and len(values) == 1:
            props.append((name, values[0]))
        else:
            props.append((name, values))
    if props:
        item['properties'] = dict(props)
    else:
        item['value'] = self._extract_property_value(node, force=True, items_seen=items_seen, base_url=base_url, itemids=itemids)
    if self.add_text_content:
        textContent = self._extract_textContent(node)
        if textContent:
            item['textContent'] = textContent
    if self.add_html_node:
        item['htmlNode'] = node
    return item","for refid in refs:
    for (name, value) in self._extract_property_refs(node, refid, items_seen=items_seen, base_url=base_url, itemids=itemids):
        properties[name].append(value)","properties[name] += [value for refid in refs for (name, value) in self._extract_property_refs(node, refid, items_seen=items_seen, base_url=base_url, itemids=itemids)]",Cannot refactor,-1,1,,,,robosuite,,,,,
vega,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/vega/vega/datasets/common/utils/auto_lane_codec_utils.py,https://github.com/huawei-noah/vega/tree/master/vega/datasets/common/utils/auto_lane_codec_utils.py,,delete_repeat_y$301,"def delete_repeat_y(cur_line):
    """"""Avoid same y with multi x.

    :param cur_line: the raw line
    :type cur_line:list
    :return: the deduplicated line
    :rtype:list
    """"""
    list_x = []
    list_y = []
    for pt in cur_line:
        list_x.append(pt.x)
        list_y.append(pt.y)
    sorted_y = sorted(list_y)
    sorted_x = []
    for i in range(len(sorted_y)):
        sorted_x.append(list_x[list_y.index(sorted_y[i])])
    set_sorted_y = []
    set_sorted_x = []
    index = 0
    for i in sorted_y:
        if not i in set_sorted_y:
            set_sorted_y.append(i)
            set_sorted_x.append(sorted_x[index])
        index += 1
    new_lane = []
    if len(set_sorted_y) < 2:
        return new_lane
    for i in range(len(set_sorted_y)):
        new_lane.append({'x': set_sorted_x[i], 'y': set_sorted_y[i]})
    if new_lane[0]['y'] < new_lane[1]['y']:
        new_lane = new_lane[::-1]
    return new_lane","for i in range(len(set_sorted_y)):
    new_lane.append({'x': set_sorted_x[i], 'y': set_sorted_y[i]})","new_lane += [{'x': set_sorted_x[i], 'y': set_sorted_y[i]} for i in range(len(set_sorted_y))]",Cannot refactor,-1,1,,,,robosuite,,,,,
wavegan,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wavegan/train_specgan.py,https://github.com/chrisdonahue/wavegan/tree/master//train_specgan.py,,incept$466,"def incept(args):
    incept_dir = os.path.join(args.train_dir, 'incept')
    if not os.path.isdir(incept_dir):
        os.makedirs(incept_dir)
    gan_graph = tf.Graph()
    with gan_graph.as_default():
        infer_metagraph_fp = os.path.join(args.train_dir, 'infer', 'infer.meta')
        gan_saver = tf.train.import_meta_graph(infer_metagraph_fp)
        score_saver = tf.train.Saver(max_to_keep=1)
    gan_z = gan_graph.get_tensor_by_name('z:0')
    gan_ngl = gan_graph.get_tensor_by_name('ngl:0')
    gan_G_z = gan_graph.get_tensor_by_name('G_z:0')[:, :, 0]
    gan_step = gan_graph.get_tensor_by_name('global_step:0')
    z_fp = os.path.join(incept_dir, 'z.pkl')
    if os.path.exists(z_fp):
        with open(z_fp, 'rb') as f:
            _zs = pickle.load(f)
    else:
        gan_samp_z_n = gan_graph.get_tensor_by_name('samp_z_n:0')
        gan_samp_z = gan_graph.get_tensor_by_name('samp_z:0')
        with tf.Session(graph=gan_graph) as sess:
            _zs = sess.run(gan_samp_z, {gan_samp_z_n: args.incept_n})
        with open(z_fp, 'wb') as f:
            pickle.dump(_zs, f)
    incept_graph = tf.Graph()
    with incept_graph.as_default():
        incept_saver = tf.train.import_meta_graph(args.incept_metagraph_fp)
    incept_x = incept_graph.get_tensor_by_name('x:0')
    incept_preds = incept_graph.get_tensor_by_name('scores:0')
    incept_sess = tf.Session(graph=incept_graph)
    incept_saver.restore(incept_sess, args.incept_ckpt_fp)
    summary_graph = tf.Graph()
    with summary_graph.as_default():
        incept_mean = tf.placeholder(tf.float32, [])
        incept_std = tf.placeholder(tf.float32, [])
        summaries = [tf.summary.scalar('incept_mean', incept_mean), tf.summary.scalar('incept_std', incept_std)]
        summaries = tf.summary.merge(summaries)
    summary_writer = tf.summary.FileWriter(incept_dir)
    ckpt_fp = None
    _best_score = 0.0
    while True:
        latest_ckpt_fp = tf.train.latest_checkpoint(args.train_dir)
        if latest_ckpt_fp != ckpt_fp:
            print('Incept: {}'.format(latest_ckpt_fp))
            sess = tf.Session(graph=gan_graph)
            gan_saver.restore(sess, latest_ckpt_fp)
            _step = sess.run(gan_step)
            _G_zs = []
            for i in xrange(0, args.incept_n, 100):
                _G_zs.append(sess.run(gan_G_z, {gan_z: _zs[i:i + 100], gan_ngl: args.specgan_ngl}))
            _G_zs = np.concatenate(_G_zs, axis=0)
            _preds = []
            for i in xrange(0, args.incept_n, 100):
                _preds.append(incept_sess.run(incept_preds, {incept_x: _G_zs[i:i + 100]}))
            _preds = np.concatenate(_preds, axis=0)
            _incept_scores = []
            split_size = args.incept_n // args.incept_k
            for i in xrange(args.incept_k):
                _split = _preds[i * split_size:(i + 1) * split_size]
                _kl = _split * (np.log(_split) - np.log(np.expand_dims(np.mean(_split, 0), 0)))
                _kl = np.mean(np.sum(_kl, 1))
                _incept_scores.append(np.exp(_kl))
            (_incept_mean, _incept_std) = (np.mean(_incept_scores), np.std(_incept_scores))
            with tf.Session(graph=summary_graph) as summary_sess:
                _summaries = summary_sess.run(summaries, {incept_mean: _incept_mean, incept_std: _incept_std})
            summary_writer.add_summary(_summaries, _step)
            if _incept_mean > _best_score:
                score_saver.save(sess, os.path.join(incept_dir, 'best_score'), _step)
                _best_score = _incept_mean
            sess.close()
            print('Done')
            ckpt_fp = latest_ckpt_fp
        time.sleep(1)
    incept_sess.close()","for i in xrange(args.incept_k):
    _split = _preds[i * split_size:(i + 1) * split_size]
    _kl = _split * (np.log(_split) - np.log(np.expand_dims(np.mean(_split, 0), 0)))
    _kl = np.mean(np.sum(_kl, 1))
    _incept_scores.append(np.exp(_kl))","_incept_scores = [np.exp(np.mean(np.sum(_split * (np.log(_split) - np.log(np.expand_dims(np.mean(_split, 0), 0))), 1))) for i in xrange(args.incept_k) for _split in [_preds[i * split_size:(i + 1) * split_size]]]",Cannot refactor,-1,1,,,,robosuite,,,,,
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for op in graph.get_operations():
    is_unreachable = False
    all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
    for name in all_names:
        if name not in seen_tensors:
            is_unreachable = True
    if is_unreachable:
        unreachable_ops.append(op)",unreachable_ops = [op for op in graph.get_operations() if any((name not in seen_tensors for name in [x.name for x in op.inputs] + [x.name for x in op.outputs]))],Cannot refactor,-1,1,,,,robosuite,,,,,
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for x in op.inputs:
    op_to_all[op.name].append(x.name)",op_to_all[op.name] = [x.name for x in op.inputs],Cannot refactor,-1,1,,,,robosuite,,,,,
BERT-Classification-Tutorial,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/BERT-Classification-Tutorial/modeling_test.py,https://github.com/Socialbird-AILab/BERT-Classification-Tutorial/tree/master//modeling_test.py,BertModelTest,get_unreachable_ops$192,"def get_unreachable_ops(cls, graph, outputs):
    """"""Finds all of the tensors in graph that are unreachable from outputs.""""""
    outputs = cls.flatten_recursive(outputs)
    output_to_op = collections.defaultdict(list)
    op_to_all = collections.defaultdict(list)
    assign_out_to_in = collections.defaultdict(list)
    for op in graph.get_operations():
        for x in op.inputs:
            op_to_all[op.name].append(x.name)
        for y in op.outputs:
            output_to_op[y.name].append(op.name)
            op_to_all[op.name].append(y.name)
        if str(op.type) == 'Assign':
            for y in op.outputs:
                for x in op.inputs:
                    assign_out_to_in[y.name].append(x.name)
    assign_groups = collections.defaultdict(list)
    for out_name in assign_out_to_in.keys():
        name_group = assign_out_to_in[out_name]
        for n1 in name_group:
            assign_groups[n1].append(out_name)
            for n2 in name_group:
                if n1 != n2:
                    assign_groups[n1].append(n2)
    seen_tensors = {}
    stack = [x.name for x in outputs]
    while stack:
        name = stack.pop()
        if name in seen_tensors:
            continue
        seen_tensors[name] = True
        if name in output_to_op:
            for op_name in output_to_op[name]:
                if op_name in op_to_all:
                    for input_name in op_to_all[op_name]:
                        if input_name not in stack:
                            stack.append(input_name)
        expanded_names = []
        if name in assign_groups:
            for assign_name in assign_groups[name]:
                expanded_names.append(assign_name)
        for expanded_name in expanded_names:
            if expanded_name not in stack:
                stack.append(expanded_name)
    unreachable_ops = []
    for op in graph.get_operations():
        is_unreachable = False
        all_names = [x.name for x in op.inputs] + [x.name for x in op.outputs]
        for name in all_names:
            if name not in seen_tensors:
                is_unreachable = True
        if is_unreachable:
            unreachable_ops.append(op)
    return unreachable_ops","for op_name in output_to_op[name]:
    if op_name in op_to_all:
        for input_name in op_to_all[op_name]:
            if input_name not in stack:
                stack.append(input_name)",stack += [input_name for op_name in output_to_op[name] for input_name in op_to_all[op_name] if op_name in op_to_all and input_name not in stack],Cannot refactor,-1,1,,,,robosuite,,,,,
mmocr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmocr/mmocr/datasets/pipelines/dbnet_transforms.py,https://github.com/open-mmlab/mmocr/tree/master/mmocr/datasets/pipelines/dbnet_transforms.py,ImgAug,may_augment_poly$127,"def may_augment_poly(self, aug, img_shape, polys):
    imgaug_polys = []
    for poly in polys:
        poly = poly[0]
        poly = poly.reshape(-1, 2)
        imgaug_polys.append(imgaug.Polygon(poly))
    imgaug_polys = aug.augment_polygons([imgaug.PolygonsOnImage(imgaug_polys, shape=img_shape)])[0].clip_out_of_image()
    new_polys = []
    for poly in imgaug_polys.polygons:
        new_poly = []
        for point in poly:
            new_poly.append(np.array(point, dtype=np.float32))
        new_poly = np.array(new_poly, dtype=np.float32).flatten()
        new_polys.append([new_poly])
    return new_polys","for poly in polys:
    poly = poly[0]
    poly = poly.reshape(-1, 2)
    imgaug_polys.append(imgaug.Polygon(poly))","imgaug_polys = [imgaug.Polygon(poly[0].reshape(-1, 2)) for poly in polys]",Cannot refactor,-1,1,,,,robosuite,,,,,
mmocr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmocr/mmocr/datasets/pipelines/dbnet_transforms.py,https://github.com/open-mmlab/mmocr/tree/master/mmocr/datasets/pipelines/dbnet_transforms.py,ImgAug,may_augment_poly$127,"def may_augment_poly(self, aug, img_shape, polys):
    imgaug_polys = []
    for poly in polys:
        poly = poly[0]
        poly = poly.reshape(-1, 2)
        imgaug_polys.append(imgaug.Polygon(poly))
    imgaug_polys = aug.augment_polygons([imgaug.PolygonsOnImage(imgaug_polys, shape=img_shape)])[0].clip_out_of_image()
    new_polys = []
    for poly in imgaug_polys.polygons:
        new_poly = []
        for point in poly:
            new_poly.append(np.array(point, dtype=np.float32))
        new_poly = np.array(new_poly, dtype=np.float32).flatten()
        new_polys.append([new_poly])
    return new_polys","for poly in imgaug_polys.polygons:
    new_poly = []
    for point in poly:
        new_poly.append(np.array(point, dtype=np.float32))
    new_poly = np.array(new_poly, dtype=np.float32).flatten()
    new_polys.append([new_poly])","new_polys = [[[np.array(point, dtype=np.float32) for point in poly].flatten()] for poly in imgaug_polys.polygons]",Cannot refactor,-1,1,,,,robosuite,,,,,
nlp-recipes,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nlp-recipes/utils_nlp/models/transformers/abstractive_summarization_seq2seq.py,https://github.com/microsoft/nlp-recipes/tree/master/utils_nlp/models/transformers/abstractive_summarization_seq2seq.py,S2SAbsSumProcessor,s2s_dataset_from_iterable_sum_ds$245,"def s2s_dataset_from_iterable_sum_ds(self, sum_ds, train_mode, cached_features_file=None, local_rank=-1, top_n=-1):
    """"""
        Converts IterableSummarizationDataset to S2SAbsSumDataset.

        Args:
            sum_ds (IterableSummarizationDataset): Input dataset.
            train_mode (bool): Whether the input data is for training or testing.
            cached_features_file (str, optional): Path of the cached features file.
                If provided and the file already exists, it is loaded and used.
                If provided and the file doesn't exist, processed features are
                saved to this file.
                If not provided, processed features are saved to cache_dir.
                Defaults to None.
            local_rank (int, optional): Local rank of the device in distributed
                training. Defaults to -1, which means non-distributed training.
            top_n (int, optional): The number which specifies how many examples in the
                beginning of the input dataset that will be used to create the dataset.
                Defaults to -1, which means the whole dataset should be processsed.

        Returns:
            S2SAbsSumDataset
        """"""
    examples = []
    if train_mode:
        for (source, target) in zip(sum_ds, sum_ds.get_target()):
            examples.append({'src': source, 'tgt': target})
    else:
        for source in sum_ds:
            examples.append({'src': source})
    s2s_dataset = S2SAbsSumProcessor.create_s2s_dataset(examples=examples, train_mode=train_mode, tokenizer=self.tokenizer, output_dir=self.cache_dir, local_rank=local_rank, cached_features_file=cached_features_file, top_n=top_n)
    return s2s_dataset","for source in sum_ds:
    examples.append({'src': source})",examples += [{'src': source} for source in sum_ds],Cannot refactor,-1,1,,,,robosuite,,,,,
texar-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/texar-pytorch/examples/gpt-2/gpt2_train_main.py,https://github.com/asyml/texar-pytorch/tree/master/examples/gpt-2/gpt2_train_main.py,,_test_epoch$200,"def _test_epoch():
    """"""Generates samples on the test set.
        """"""
    iterator.switch_to_dataset('test')
    model.eval()
    _all_inputs = []
    _all_samples = []
    for batch in iterator:
        input_ids = batch['text_ids']
        length = batch['length']
        start_tokens = input_ids[:, 0]
        helper = _get_helper(start_tokens)
        (output, _) = model(context=input_ids, context_sequence_length=length, max_decoding_length=max_decoding_length, helper=helper)
        sample_id = output.sample_id
        _inputs = []
        for (i, l) in zip(input_ids, length):
            _inputs.append(i[:l].tolist())
        _all_inputs.extend(_inputs)
        _samples = []
        for (s, l) in zip(sample_id, length):
            _samples.append(s[l:].tolist())
        _all_samples.extend(_samples)
    eos_token_id = tokenizer.map_token_to_id('<|endoftext|>')
    _all_input_text = []
    for i in _all_inputs:
        if i[0] == eos_token_id:
            i = i[1:]
        i_text = tokenizer.map_id_to_text(i)
        _all_input_text.append(i_text)
    _all_input_text = tx.utils.strip_eos(_all_input_text, eos_token='<|endoftext|>')
    _all_samples_text = []
    for (i, s) in zip(_all_inputs, _all_samples):
        s_text = tokenizer.map_id_to_text(s)
        s_text = s_text.replace('\n', ' ')
        _all_samples_text.append(s_text)
    _all_samples_text = tx.utils.strip_eos(_all_samples_text, eos_token='<|endoftext|>')
    output_file = os.path.join(args.output_dir, 'test_samples.tsv')
    print('Write samples to {}'.format(output_file))
    tx.utils.write_paired_text(_all_input_text, _all_samples_text, output_file)","for i in _all_inputs:
    if i[0] == eos_token_id:
        i = i[1:]
    i_text = tokenizer.map_id_to_text(i)
    _all_input_text.append(i_text)",_all_input_text = [tokenizer.map_id_to_text(i[1:]) if i[0] == eos_token_id else tokenizer.map_id_to_text(i) for i in _all_inputs],Cannot refactor,-1,1,,,,robosuite,,,,,
texar-pytorch,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/texar-pytorch/examples/gpt-2/gpt2_train_main.py,https://github.com/asyml/texar-pytorch/tree/master/examples/gpt-2/gpt2_train_main.py,,_test_epoch$200,"def _test_epoch():
    """"""Generates samples on the test set.
        """"""
    iterator.switch_to_dataset('test')
    model.eval()
    _all_inputs = []
    _all_samples = []
    for batch in iterator:
        input_ids = batch['text_ids']
        length = batch['length']
        start_tokens = input_ids[:, 0]
        helper = _get_helper(start_tokens)
        (output, _) = model(context=input_ids, context_sequence_length=length, max_decoding_length=max_decoding_length, helper=helper)
        sample_id = output.sample_id
        _inputs = []
        for (i, l) in zip(input_ids, length):
            _inputs.append(i[:l].tolist())
        _all_inputs.extend(_inputs)
        _samples = []
        for (s, l) in zip(sample_id, length):
            _samples.append(s[l:].tolist())
        _all_samples.extend(_samples)
    eos_token_id = tokenizer.map_token_to_id('<|endoftext|>')
    _all_input_text = []
    for i in _all_inputs:
        if i[0] == eos_token_id:
            i = i[1:]
        i_text = tokenizer.map_id_to_text(i)
        _all_input_text.append(i_text)
    _all_input_text = tx.utils.strip_eos(_all_input_text, eos_token='<|endoftext|>')
    _all_samples_text = []
    for (i, s) in zip(_all_inputs, _all_samples):
        s_text = tokenizer.map_id_to_text(s)
        s_text = s_text.replace('\n', ' ')
        _all_samples_text.append(s_text)
    _all_samples_text = tx.utils.strip_eos(_all_samples_text, eos_token='<|endoftext|>')
    output_file = os.path.join(args.output_dir, 'test_samples.tsv')
    print('Write samples to {}'.format(output_file))
    tx.utils.write_paired_text(_all_input_text, _all_samples_text, output_file)","for (i, s) in zip(_all_inputs, _all_samples):
    s_text = tokenizer.map_id_to_text(s)
    s_text = s_text.replace('\n', ' ')
    _all_samples_text.append(s_text)","_all_samples_text = [tokenizer.map_id_to_text(s).replace('\n', ' ') for (i, s) in zip(_all_inputs, _all_samples)]",Cannot refactor,-1,1,,,,robosuite,,,,,
nuscenes-devkit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nuscenes-devkit/python-sdk/nuscenes/eval/detection/tests/test_evaluate.py,https://github.com/nutonomy/nuscenes-devkit/tree/master/python-sdk/nuscenes/eval/detection/tests/test_evaluate.py,TestMain,_mock_submission$33,"def _mock_submission(nusc: NuScenes, split: str) -> Dict[str, dict]:
    """"""
        Creates ""reasonable"" submission (results and metadata) by looping through the mini-val set, adding 1 GT
        prediction per sample. Predictions will be permuted randomly along all axes.
        """"""

    def random_class(category_name: str) -> str:
        class_names = sorted(DETECTION_NAMES)
        tmp = category_to_detection_name(category_name)
        if tmp is not None and np.random.rand() < 0.9:
            return tmp
        else:
            return class_names[np.random.randint(0, len(class_names) - 1)]

    def random_attr(name: str) -> str:
        """"""
            This is the most straight-forward way to generate a random attribute.
            Not currently used b/c we want the test fixture to be back-wards compatible.
            """"""
        rel_attributes = detection_name_to_rel_attributes(name)
        if len(rel_attributes) == 0:
            return ''
        else:
            return rel_attributes[np.random.randint(0, len(rel_attributes))]
    mock_meta = {'use_camera': False, 'use_lidar': True, 'use_radar': False, 'use_map': False, 'use_external': False}
    mock_results = {}
    splits = create_splits_scenes()
    val_samples = []
    for sample in nusc.sample:
        if nusc.get('scene', sample['scene_token'])['name'] in splits[split]:
            val_samples.append(sample)
    for sample in tqdm(val_samples, leave=False):
        sample_res = []
        for ann_token in sample['anns']:
            ann = nusc.get('sample_annotation', ann_token)
            detection_name = random_class(ann['category_name'])
            sample_res.append({'sample_token': sample['token'], 'translation': list(np.array(ann['translation']) + 5 * (np.random.rand(3) - 0.5)), 'size': list(np.array(ann['size']) * 2 * (np.random.rand(3) + 0.5)), 'rotation': list(np.array(ann['rotation']) + (np.random.rand(4) - 0.5) * 0.1), 'velocity': list(nusc.box_velocity(ann_token)[:2] * (np.random.rand(3)[:2] + 0.5)), 'detection_name': detection_name, 'detection_score': random.random(), 'attribute_name': random_attr(detection_name)})
        mock_results[sample['token']] = sample_res
    mock_submission = {'meta': mock_meta, 'results': mock_results}
    return mock_submission","for ann_token in sample['anns']:
    ann = nusc.get('sample_annotation', ann_token)
    detection_name = random_class(ann['category_name'])
    sample_res.append({'sample_token': sample['token'], 'translation': list(np.array(ann['translation']) + 5 * (np.random.rand(3) - 0.5)), 'size': list(np.array(ann['size']) * 2 * (np.random.rand(3) + 0.5)), 'rotation': list(np.array(ann['rotation']) + (np.random.rand(4) - 0.5) * 0.1), 'velocity': list(nusc.box_velocity(ann_token)[:2] * (np.random.rand(3)[:2] + 0.5)), 'detection_name': detection_name, 'detection_score': random.random(), 'attribute_name': random_attr(detection_name)})","sample_res = [{'sample_token': sample['token'], 'translation': list(np.array(ann['translation']) + 5 * (np.random.rand(3) - 0.5)), 'size': list(np.array(ann['size']) * 2 * (np.random.rand(3) + 0.5)), 'rotation': list(np.array(ann['rotation']) + (np.random.rand(4) - 0.5) * 0.1), 'velocity': list(nusc.box_velocity(ann_token)[:2] * (np.random.rand(3)[:2] + 0.5)), 'detection_name': random_class(ann['category_name']), 'detection_score': random.random(), 'attribute_name': random_attr(random_class(ann['category_name']))} for ann_token in sample['anns'] for ann in [nusc.get('sample_annotation', ann_token)]]",Cannot refactor,-1,1,,,,robosuite,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/pdaugment/midi_preprocess/steps/track_separate.py,https://github.com/microsoft/muzic/tree/master/pdaugment/midi_preprocess/steps/track_separate.py,,remove_duplicate_tracks$54,"def remove_duplicate_tracks(features, replace=False):
    if not replace:
        features = features.copy()
    file_names = features.file_names.unique()
    duplicates = []
    for file_name in file_names:
        file_features = features[features.file_names == file_name]
        number_notes = Counter(file_features.num_notes)
        notes = []
        for ele in number_notes:
            if number_notes[ele] > 1:
                notes.append(ele)
        h_pits = []
        for note in notes:
            number_h_pit = Counter(file_features[file_features.num_notes == note].h_pit)
            for ele in number_h_pit:
                if number_h_pit[ele] > 1:
                    h_pits.append(ele)
        l_pits = []
        for h_pit in h_pits:
            number_l_pit = Counter(file_features[file_features.h_pit == h_pit].l_pit)
            for ele in number_l_pit:
                if number_l_pit[ele] > 1:
                    l_pits.append(ele)
        notes = list(set(notes))
        h_pits = list(set(h_pits))
        l_pits = list(set(l_pits))
        for note in notes:
            note_index = file_features[file_features.num_notes == note].index.values
            for h_pit in h_pits:
                h_pit_index = file_features[file_features.h_pit == h_pit].index.values
                for l_pit in l_pits:
                    l_pit_index = file_features[file_features.l_pit == l_pit].index.values
                    index_intersect = reduce(np.intersect1d, (note_index, h_pit_index, l_pit_index))
                    if len(index_intersect) > 1:
                        duplicates.append(index_intersect)
    melody_track_name = ['sing', 'vocals', 'vocal', 'melody', 'melody:']
    bass_track_name = ['bass', 'bass:']
    chord_track_name = ['chord', 'chords', 'harmony']
    for indices in duplicates:
        melody_track = False
        bass_track = False
        chord_track = False
        labels = features.loc[indices, 'trk_names']
        for label in labels:
            if label in melody_track_name:
                melody_track = True
            elif label in bass_track_name:
                bass_track = True
            elif label in chord_track_name:
                chord_track = True
            else:
                pass
        if melody_track:
            features.loc[indices, 'trk_names'] = 'melody'
        if bass_track:
            features.loc[indices, 'trk_names'] = 'bass'
        if chord_track:
            features.loc[indices, 'trk_names'] = 'chord'
        features.drop(indices[1:], inplace=True)
        print(indices[1:])
    return features","for note in notes:
    number_h_pit = Counter(file_features[file_features.num_notes == note].h_pit)
    for ele in number_h_pit:
        if number_h_pit[ele] > 1:
            h_pits.append(ele)","h_pits = [ele for note in notes for (ele, count) in Counter(file_features[file_features.num_notes == note].h_pit).items() if count > 1]",Cannot refactor,-1,1,,,,robosuite,,,,,
muzic,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/muzic/pdaugment/midi_preprocess/steps/track_separate.py,https://github.com/microsoft/muzic/tree/master/pdaugment/midi_preprocess/steps/track_separate.py,,remove_duplicate_tracks$54,"def remove_duplicate_tracks(features, replace=False):
    if not replace:
        features = features.copy()
    file_names = features.file_names.unique()
    duplicates = []
    for file_name in file_names:
        file_features = features[features.file_names == file_name]
        number_notes = Counter(file_features.num_notes)
        notes = []
        for ele in number_notes:
            if number_notes[ele] > 1:
                notes.append(ele)
        h_pits = []
        for note in notes:
            number_h_pit = Counter(file_features[file_features.num_notes == note].h_pit)
            for ele in number_h_pit:
                if number_h_pit[ele] > 1:
                    h_pits.append(ele)
        l_pits = []
        for h_pit in h_pits:
            number_l_pit = Counter(file_features[file_features.h_pit == h_pit].l_pit)
            for ele in number_l_pit:
                if number_l_pit[ele] > 1:
                    l_pits.append(ele)
        notes = list(set(notes))
        h_pits = list(set(h_pits))
        l_pits = list(set(l_pits))
        for note in notes:
            note_index = file_features[file_features.num_notes == note].index.values
            for h_pit in h_pits:
                h_pit_index = file_features[file_features.h_pit == h_pit].index.values
                for l_pit in l_pits:
                    l_pit_index = file_features[file_features.l_pit == l_pit].index.values
                    index_intersect = reduce(np.intersect1d, (note_index, h_pit_index, l_pit_index))
                    if len(index_intersect) > 1:
                        duplicates.append(index_intersect)
    melody_track_name = ['sing', 'vocals', 'vocal', 'melody', 'melody:']
    bass_track_name = ['bass', 'bass:']
    chord_track_name = ['chord', 'chords', 'harmony']
    for indices in duplicates:
        melody_track = False
        bass_track = False
        chord_track = False
        labels = features.loc[indices, 'trk_names']
        for label in labels:
            if label in melody_track_name:
                melody_track = True
            elif label in bass_track_name:
                bass_track = True
            elif label in chord_track_name:
                chord_track = True
            else:
                pass
        if melody_track:
            features.loc[indices, 'trk_names'] = 'melody'
        if bass_track:
            features.loc[indices, 'trk_names'] = 'bass'
        if chord_track:
            features.loc[indices, 'trk_names'] = 'chord'
        features.drop(indices[1:], inplace=True)
        print(indices[1:])
    return features","for h_pit in h_pits:
    number_l_pit = Counter(file_features[file_features.h_pit == h_pit].l_pit)
    for ele in number_l_pit:
        if number_l_pit[ele] > 1:
            l_pits.append(ele)","l_pits = [ele for h_pit in h_pits for (ele, count) in Counter(file_features[file_features.h_pit == h_pit].l_pit).items() if count > 1]",Cannot refactor,-1,1,,,,robosuite,,,,,
package_control,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/package_control/dev/deps.py,https://github.com/wbond/package_control/tree/master/dev/deps.py,,_pep425tags$813,"def _pep425tags():
    """"""
    :return:
        A list of 3-element tuples with unicode strings or None:
         [0] implementation tag - cp33, pp27, cp26, py2, py2.py3
         [1] abi tag - cp26m, None
         [2] arch tag - linux_x86_64, macosx_10_10_x85_64, etc
    """"""
    tags = []
    versions = []
    version_info = _pep425_version()
    major = version_info[:-1]
    for minor in range(version_info[-1], -1, -1):
        versions.append(''.join(map(str, major + (minor,))))
    impl = _pep425_implementation()
    abis = []
    abi = _pep425_get_abi()
    if abi:
        abis.append(abi)
    abi3 = _pep425_implementation() == 'cp' and sys.version_info >= (3,)
    if abi3:
        abis.append('abi3')
    abis.append('none')
    if sys.platform == 'darwin':
        plat_ver = platform.mac_ver()
        ver_parts = plat_ver[0].split('.')
        minor = int(ver_parts[1])
        arch = plat_ver[2]
        if sys.maxsize == 2147483647:
            arch = 'i386'
        arches = []
        while minor > 5:
            arches.append('macosx_10_%s_%s' % (minor, arch))
            arches.append('macosx_10_%s_intel' % (minor,))
            arches.append('macosx_10_%s_universal' % (minor,))
            minor -= 1
    elif sys.platform == 'win32':
        if 'amd64' in sys.version.lower():
            arches = ['win_amd64']
        else:
            arches = [sys.platform]
    elif hasattr(os, 'uname'):
        (plat, _, _, _, machine) = os.uname()
        plat = plat.lower().replace('/', '')
        machine.replace(' ', '_').replace('/', '_')
        if plat == 'linux' and sys.maxsize == 2147483647 and ('arm' not in machine):
            machine = 'i686'
        arch = '%s_%s' % (plat, machine)
        if _pep425_supports_manylinux():
            arches = [arch.replace('linux', 'manylinux1'), arch]
        else:
            arches = [arch]
    for abi in abis:
        for arch in arches:
            tags.append(('%s%s' % (impl, versions[0]), abi, arch))
    if abi3:
        for version in versions[1:]:
            for arch in arches:
                tags.append(('%s%s' % (impl, version), 'abi3', arch))
    for arch in arches:
        tags.append(('py%s' % versions[0][0], 'none', arch))
    tags.append(('%s%s' % (impl, versions[0]), 'none', 'any'))
    tags.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))
    for (i, version) in enumerate(versions):
        tags.append(('py%s' % (version,), 'none', 'any'))
        if i == 0:
            tags.append(('py%s' % version[0], 'none', 'any'))
    tags.append(('py2.py3', 'none', 'any'))
    return tags","for arch in arches:
    tags.append(('py%s' % versions[0][0], 'none', arch))","tags += [('py%s' % versions[0][0], 'none', arch) for arch in arches]",Cannot refactor,-1,1,,,,robosuite,,,,,
package_control,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/package_control/dev/deps.py,https://github.com/wbond/package_control/tree/master/dev/deps.py,,_pep425tags$813,"def _pep425tags():
    """"""
    :return:
        A list of 3-element tuples with unicode strings or None:
         [0] implementation tag - cp33, pp27, cp26, py2, py2.py3
         [1] abi tag - cp26m, None
         [2] arch tag - linux_x86_64, macosx_10_10_x85_64, etc
    """"""
    tags = []
    versions = []
    version_info = _pep425_version()
    major = version_info[:-1]
    for minor in range(version_info[-1], -1, -1):
        versions.append(''.join(map(str, major + (minor,))))
    impl = _pep425_implementation()
    abis = []
    abi = _pep425_get_abi()
    if abi:
        abis.append(abi)
    abi3 = _pep425_implementation() == 'cp' and sys.version_info >= (3,)
    if abi3:
        abis.append('abi3')
    abis.append('none')
    if sys.platform == 'darwin':
        plat_ver = platform.mac_ver()
        ver_parts = plat_ver[0].split('.')
        minor = int(ver_parts[1])
        arch = plat_ver[2]
        if sys.maxsize == 2147483647:
            arch = 'i386'
        arches = []
        while minor > 5:
            arches.append('macosx_10_%s_%s' % (minor, arch))
            arches.append('macosx_10_%s_intel' % (minor,))
            arches.append('macosx_10_%s_universal' % (minor,))
            minor -= 1
    elif sys.platform == 'win32':
        if 'amd64' in sys.version.lower():
            arches = ['win_amd64']
        else:
            arches = [sys.platform]
    elif hasattr(os, 'uname'):
        (plat, _, _, _, machine) = os.uname()
        plat = plat.lower().replace('/', '')
        machine.replace(' ', '_').replace('/', '_')
        if plat == 'linux' and sys.maxsize == 2147483647 and ('arm' not in machine):
            machine = 'i686'
        arch = '%s_%s' % (plat, machine)
        if _pep425_supports_manylinux():
            arches = [arch.replace('linux', 'manylinux1'), arch]
        else:
            arches = [arch]
    for abi in abis:
        for arch in arches:
            tags.append(('%s%s' % (impl, versions[0]), abi, arch))
    if abi3:
        for version in versions[1:]:
            for arch in arches:
                tags.append(('%s%s' % (impl, version), 'abi3', arch))
    for arch in arches:
        tags.append(('py%s' % versions[0][0], 'none', arch))
    tags.append(('%s%s' % (impl, versions[0]), 'none', 'any'))
    tags.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))
    for (i, version) in enumerate(versions):
        tags.append(('py%s' % (version,), 'none', 'any'))
        if i == 0:
            tags.append(('py%s' % version[0], 'none', 'any'))
    tags.append(('py2.py3', 'none', 'any'))
    return tags","for version in versions[1:]:
    for arch in arches:
        tags.append(('%s%s' % (impl, version), 'abi3', arch))","tags += [('%s%s' % (impl, version), 'abi3', arch) for version in versions[1:] for arch in arches]",Cannot refactor,-1,1,,,,robosuite,,,,,
PowerDNS-Admin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PowerDNS-Admin/powerdnsadmin/routes/dashboard.py,https://github.com/ngoduykhanh/PowerDNS-Admin/tree/master/powerdnsadmin/routes/dashboard.py,,domains_custom$46,"def domains_custom(boxId):
    if current_user.role.name in ['Administrator', 'Operator']:
        domains = Domain.query
    else:
        domains = db.session.query(Domain).outerjoin(DomainUser, Domain.id == DomainUser.domain_id).outerjoin(Account, Domain.account_id == Account.id).outerjoin(AccountUser, Account.id == AccountUser.account_id).filter(db.or_(DomainUser.user_id == current_user.id, AccountUser.user_id == current_user.id))
    template = current_app.jinja_env.get_template('dashboard_domain.html')
    render = template.make_module(vars={'current_user': current_user, 'allow_user_view_history': Setting().get('allow_user_view_history')})
    columns = [Domain.name, Domain.dnssec, Domain.type, Domain.serial, Domain.master, Domain.account_id]
    order_by = []
    for i in range(len(columns)):
        column_index = request.args.get('order[{0}][column]'.format(i))
        sort_direction = request.args.get('order[{0}][dir]'.format(i))
        if column_index is None:
            break
        if sort_direction != 'asc' and sort_direction != 'desc':
            sort_direction = 'asc'
        column = columns[int(column_index)]
        order_by.append(getattr(column, sort_direction)())
    if order_by:
        domains = domains.order_by(*order_by)
    if boxId == 'reverse':
        for boxId in customBoxes.order:
            if boxId == 'reverse':
                continue
            domains = domains.filter(not_(Domain.name.ilike(customBoxes.boxes[boxId][1])))
    else:
        domains = domains.filter(Domain.name.ilike(customBoxes.boxes[boxId][1]))
    total_count = domains.count()
    search = request.args.get('search[value]')
    if search:
        start = '' if search.startswith('^') else '%'
        end = '' if search.endswith('$') else '%'
        if current_user.role.name in ['Administrator', 'Operator']:
            domains = domains.outerjoin(Account).filter(Domain.name.ilike(start + search.strip('^$') + end) | Account.name.ilike(start + search.strip('^$') + end) | Account.description.ilike(start + search.strip('^$') + end))
        else:
            domains = domains.filter(Domain.name.ilike(start + search.strip('^$') + end))
    filtered_count = domains.count()
    start = int(request.args.get('start', 0))
    length = min(int(request.args.get('length', 0)), 100)
    if length != -1:
        domains = domains[start:start + length]
    data = []
    for domain in domains:
        data.append([render.name(domain), render.dnssec(domain), render.type(domain), render.serial(domain), render.master(domain), render.account(domain), render.actions(domain)])
    response_data = {'draw': int(request.args.get('draw', 0)), 'recordsTotal': total_count, 'recordsFiltered': filtered_count, 'data': data}
    return jsonify(response_data)","for i in range(len(columns)):
    column_index = request.args.get('order[{0}][column]'.format(i))
    sort_direction = request.args.get('order[{0}][dir]'.format(i))
    if column_index is None:
        break
    if sort_direction != 'asc' and sort_direction != 'desc':
        sort_direction = 'asc'
    column = columns[int(column_index)]
    order_by.append(getattr(column, sort_direction)())","order_by = [getattr(columns[int(request.args.get('order[{0}][column]'.format(i)))], 'asc' if request.args.get('order[{0}][dir]'.format(i)) != 'desc' and request.args.get('order[{0}][dir]'.format(i)) != 'asc' else request.args.get('order[{0}][dir]'.format(i)))() for i in range(len(columns)) if request.args.get('order[{0}][column]'.format(i)) is not None]",Cannot refactor,-1,1,,,,robosuite,,,,,
cfn-lint,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/cfn-lint/src/cfnlint/rules/parameters/Used.py,https://github.com/aws-cloudformation/cfn-lint/tree/master/src/cfnlint/rules/parameters/Used.py,Used,match$35,"def match(self, cfn):
    matches = []
    reftrees = cfn.transform_pre.get('Ref')
    subtrees = cfn.transform_pre.get('Fn::Sub')
    refs = []
    for reftree in reftrees:
        refs.append(reftree[-1])
    subs = []
    for subtree in subtrees:
        if isinstance(subtree[-1], list):
            subs.extend(cfn.get_sub_parameters(subtree[-1][0]))
        elif isinstance(subtree[-1], str):
            subs.extend(cfn.get_sub_parameters(subtree[-1]))
    for (paramname, _) in cfn.get_parameters().items():
        if paramname not in refs:
            if paramname not in subs:
                message = 'Parameter {0} not used.'
                matches.append(RuleMatch(['Parameters', paramname], message.format(paramname)))
    return matches","for (paramname, _) in cfn.get_parameters().items():
    if paramname not in refs:
        if paramname not in subs:
            message = 'Parameter {0} not used.'
            matches.append(RuleMatch(['Parameters', paramname], message.format(paramname)))","matches = [RuleMatch(['Parameters', paramname], 'Parameter {0} not used.'.format(paramname)) for (paramname, _) in cfn.get_parameters().items() if paramname not in refs and paramname not in subs]",Cannot refactor,-1,1,,,,robosuite,,,,,
AlgorithmsAndDataStructure,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/AlgorithmsAndDataStructure/Python/Algorithms/OperatingSystem/bankers_algorithm.py,https://github.com/codePerfectPlus/AlgorithmsAndDataStructure/tree/master/Python/Algorithms/OperatingSystem/bankers_algorithm.py,,isSafe$22,"def isSafe(P, R, processes, available_array, max_R, alloted):
    need = []
    for i in range(P):
        temp = []
        for j in range(R):
            temp.append(0)
        need.append(temp)
    calculateNeed(P, R, need, max_R, alloted)
    finish = [0] * P
    safeSeq = [0] * P
    work = [0] * R
    for i in range(R):
        work[i] = available_array[i]
    count = 0
    while count < P:
        found = False
        for p in range(P):
            if finish[p] == 0:
                for j in range(R):
                    if need[p][j] > work[j]:
                        break
                if j == R - 1:
                    for k in range(R):
                        work[k] += alloted[p][k]
                    safeSeq[count] = p
                    count += 1
                    finish[p] = 1
                    found = True
        if found is False:
            print('System not in safe state')
            return False
    print('System is in safe state')
    print('Safe Sequence is: ')
    print(*safeSeq)
    return True","for i in range(P):
    temp = []
    for j in range(R):
        temp.append(0)
    need.append(temp)",need = [[0 for j in range(R)] for i in range(P)],Cannot refactor,-1,1,,,,robosuite,,,,,
NLP-Tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NLP-Tutorials/pytorch/utils.py,https://github.com/MorvanZhou/NLP-Tutorials/tree/master/pytorch/utils.py,,process_w2v_data$75,"def process_w2v_data(corpus, skip_window=2, method='skip_gram'):
    all_words = [sentence.split(' ') for sentence in corpus]
    all_words = np.array(list(itertools.chain(*all_words)))
    (vocab, v_count) = np.unique(all_words, return_counts=True)
    vocab = vocab[np.argsort(v_count)[::-1]]
    print('All vocabularies are sorted by frequency in decresing oreder')
    v2i = {v: i for (i, v) in enumerate(vocab)}
    i2v = {i: v for (v, i) in v2i.items()}
    pairs = []
    js = [i for i in range(-skip_window, skip_window + 1) if i != 0]
    for c in corpus:
        words = c.split(' ')
        w_idx = [v2i[w] for w in words]
        if method == 'skip_gram':
            for i in range(len(w_idx)):
                for j in js:
                    if i + j < 0 or i + j >= len(w_idx):
                        continue
                    pairs.append((w_idx[i], w_idx[i + j]))
        elif method.lower() == 'cbow':
            for i in range(skip_window, len(w_idx) - skip_window):
                context = []
                for j in js:
                    context.append(w_idx[i + j])
                pairs.append(context + [w_idx[i]])
        else:
            raise ValueError
    pairs = np.array(pairs)
    print('5 expample pairs:\n', pairs[:5])
    if method.lower() == 'skip_gram':
        (x, y) = (pairs[:, 0], pairs[:, 1])
    elif method.lower() == 'cbow':
        (x, y) = (pairs[:, :-1], pairs[:, -1])
    else:
        raise ValueError
    return Dataset(x, y, v2i, i2v)","for i in range(len(w_idx)):
    for j in js:
        if i + j < 0 or i + j >= len(w_idx):
            continue
        pairs.append((w_idx[i], w_idx[i + j]))","pairs = [(w_idx[i], w_idx[i + j]) for i in range(len(w_idx)) for j in js if i + j >= 0 and i + j < len(w_idx)]",Cannot refactor,-1,1,,,,robosuite,,,,,
NLP-Tutorials,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/NLP-Tutorials/pytorch/utils.py,https://github.com/MorvanZhou/NLP-Tutorials/tree/master/pytorch/utils.py,,process_w2v_data$75,"def process_w2v_data(corpus, skip_window=2, method='skip_gram'):
    all_words = [sentence.split(' ') for sentence in corpus]
    all_words = np.array(list(itertools.chain(*all_words)))
    (vocab, v_count) = np.unique(all_words, return_counts=True)
    vocab = vocab[np.argsort(v_count)[::-1]]
    print('All vocabularies are sorted by frequency in decresing oreder')
    v2i = {v: i for (i, v) in enumerate(vocab)}
    i2v = {i: v for (v, i) in v2i.items()}
    pairs = []
    js = [i for i in range(-skip_window, skip_window + 1) if i != 0]
    for c in corpus:
        words = c.split(' ')
        w_idx = [v2i[w] for w in words]
        if method == 'skip_gram':
            for i in range(len(w_idx)):
                for j in js:
                    if i + j < 0 or i + j >= len(w_idx):
                        continue
                    pairs.append((w_idx[i], w_idx[i + j]))
        elif method.lower() == 'cbow':
            for i in range(skip_window, len(w_idx) - skip_window):
                context = []
                for j in js:
                    context.append(w_idx[i + j])
                pairs.append(context + [w_idx[i]])
        else:
            raise ValueError
    pairs = np.array(pairs)
    print('5 expample pairs:\n', pairs[:5])
    if method.lower() == 'skip_gram':
        (x, y) = (pairs[:, 0], pairs[:, 1])
    elif method.lower() == 'cbow':
        (x, y) = (pairs[:, :-1], pairs[:, -1])
    else:
        raise ValueError
    return Dataset(x, y, v2i, i2v)","for i in range(skip_window, len(w_idx) - skip_window):
    context = []
    for j in js:
        context.append(w_idx[i + j])
    pairs.append(context + [w_idx[i]])","pairs += [[w_idx[i + j] for j in js] + [w_idx[i]] for i in range(skip_window, len(w_idx) - skip_window)]",Cannot refactor,-1,1,,,,robosuite,,,,,
mmdetection-mini,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmdetection-mini/mmdet/cv_core/runner/hooks/logger/text.py,https://github.com/hhaAndroid/mmdetection-mini/tree/master/mmdet/cv_core/runner/hooks/logger/text.py,TextLoggerHook,_log_info$60,"def _log_info(self, log_dict, runner):
    if runner.meta is not None and 'exp_name' in runner.meta:
        if self.every_n_iters(runner, self.interval_exp_name) or (self.by_epoch and self.end_of_epoch(runner)):
            exp_info = f""Exp name: {runner.meta['exp_name']}""
            runner.logger.info(exp_info)
    if log_dict['mode'] == 'train':
        if isinstance(log_dict['lr'], dict):
            lr_str = []
            for (k, val) in log_dict['lr'].items():
                lr_str.append(f'lr_{k}: {val:.3e}')
            lr_str = ' '.join(lr_str)
        else:
            lr_str = f""lr: {log_dict['lr']:.3e}""
        if self.by_epoch:
            log_str = f""Epoch [{log_dict['epoch']}][{log_dict['iter']}/{len(runner.data_loader)}]\t""
        else:
            log_str = f""Iter [{log_dict['iter']}/{runner.max_iters}]\t""
        log_str += f'{lr_str}, '
        if 'time' in log_dict.keys():
            self.time_sec_tot += log_dict['time'] * self.interval
            time_sec_avg = self.time_sec_tot / (runner.iter - self.start_iter + 1)
            eta_sec = time_sec_avg * (runner.max_iters - runner.iter - 1)
            eta_str = str(datetime.timedelta(seconds=int(eta_sec)))
            log_str += f'eta: {eta_str}, '
            log_str += f""time: {log_dict['time']:.3f}, data_time: {log_dict['data_time']:.3f}, ""
            if torch.cuda.is_available():
                log_str += f""memory: {log_dict['memory']}, ""
    elif self.by_epoch:
        log_str = f""Epoch({log_dict['mode']}) [{log_dict['epoch']}][{log_dict['iter']}]\t""
    else:
        log_str = f""Iter({log_dict['mode']}) [{log_dict['iter']}]\t""
    log_items = []
    for (name, val) in log_dict.items():
        if name in ['mode', 'Epoch', 'iter', 'lr', 'time', 'data_time', 'memory', 'epoch']:
            continue
        if isinstance(val, float):
            val = f'{val:.4f}'
        log_items.append(f'{name}: {val}')
    log_str += ', '.join(log_items)
    runner.logger.info(log_str)","for (name, val) in log_dict.items():
    if name in ['mode', 'Epoch', 'iter', 'lr', 'time', 'data_time', 'memory', 'epoch']:
        continue
    if isinstance(val, float):
        val = f'{val:.4f}'
    log_items.append(f'{name}: {val}')","log_items = [f'{name}: {val:.4f}' if isinstance(val, float) else f'{name}: {val}' for (name, val) in log_dict.items() if name not in ['mode', 'Epoch', 'iter', 'lr', 'time', 'data_time', 'memory', 'epoch']]",Cannot refactor,-1,1,,,,robosuite,,,,,
Wudao-dict,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Wudao-dict/soup/parse_21c.py,https://github.com/ChestnutHeng/Wudao-dict/tree/master/soup/parse_21c.py,,get_text$31,"def get_text(word):
    content = get_html(word)
    word_struct = {'word': word}
    root = bs4.BeautifulSoup(content, 'lxml')
    pron = {}
    pron_fallback = False
    for pron_item in ss(root, '.pronounce'):
        pron_lang = None
        pron_phonetic = None
        for sub_item in pron_item.children:
            if isinstance(sub_item, str) and pron_lang is None:
                pron_lang = sub_item
                continue
            if isinstance(sub_item, bs4.Tag) and sub_item.name.lower() == 'span' and sub_item.has_attr('class') and ('phonetic' in sub_item.get('class')):
                pron_phonetic = sub_item
                continue
        if pron_phonetic is None:
            pron_fallback = True
            break
        if pron_lang is None:
            pron_lang = ''
        pron_lang = pron_lang.strip()
        pron_phonetic = pron_phonetic.text
        pron[pron_lang] = pron_phonetic
    if pron_fallback:
        for item in ss(root, '.phonetic'):
            if item.name.lower() == 'span':
                pron[''] = item.text
                break
    word_struct['pronunciation'] = pron
    if pron_fallback:
        d = root.select('.wordGroup.def')
        p = root.select('.wordGroup.pos')
        ds = ''
        dp = ''
        if len(d) > 0:
            ds = d[0].text.strip()
        if len(p) > 0:
            dp = p[0].text.strip()
        word_struct['paraphrase'] = (dp + ' ' + ds).strip()
    else:
        nodes = ss(root, '#phrsListTab .trans-container ul')
        basic_desc = []
        if len(nodes) != 0:
            ul = nodes[0]
            for li in ul.children:
                if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
                    continue
                basic_desc.append(li.text.strip())
        word_struct['paraphrase'] = basic_desc
    rank = ''
    nodes = ss(root, '.rank')
    if len(nodes) != 0:
        rank = nodes[0].text.strip()
    word_struct['rank'] = rank
    pattern = ''
    nodes = ss(root, '.collinsToggle .pattern')
    if len(nodes) != 0:
        pattern = multi_space_to_single(nodes[0].text.strip())
    word_struct['pattern'] = pattern
    word_struct['sentence'] = []
    for child in ss(root, '.collinsToggle .ol li'):
        p = ss(child, 'p')
        if len(p) == 0:
            continue
        p = p[0]
        desc = ''
        cx = ''
        for node in p.children:
            if isinstance(node, str):
                desc += node
            if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
                cx = node.text
        desc = multi_space_to_single(desc.strip())
        examples = []
        for el in ss(child, '.exampleLists'):
            examp = []
            for p in ss(el, '.examples p'):
                examp.append(p.text.strip())
            examples.append(examp)
        word_struct['sentence'].append([desc, cx, examples])
    if not word_struct['sentence']:
        for v in root.select('#bilingual ul li'):
            p = ss(v, 'p')
            ll = []
            for p in ss(v, 'p'):
                if len(p) == 0:
                    continue
                if 'class' not in p.attrs:
                    ll.append(p.text.strip())
            if len(ll) != 0:
                word_struct['sentence'].append(ll)
    return word_struct","for el in ss(child, '.exampleLists'):
    examp = []
    for p in ss(el, '.examples p'):
        examp.append(p.text.strip())
    examples.append(examp)","examples += [[p.text.strip() for p in ss(el, '.examples p')] for el in ss(child, '.exampleLists')]",Cannot refactor,-1,1,,,,robosuite,,,,,
Wudao-dict,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Wudao-dict/soup/parse_21c.py,https://github.com/ChestnutHeng/Wudao-dict/tree/master/soup/parse_21c.py,,get_text$31,"def get_text(word):
    content = get_html(word)
    word_struct = {'word': word}
    root = bs4.BeautifulSoup(content, 'lxml')
    pron = {}
    pron_fallback = False
    for pron_item in ss(root, '.pronounce'):
        pron_lang = None
        pron_phonetic = None
        for sub_item in pron_item.children:
            if isinstance(sub_item, str) and pron_lang is None:
                pron_lang = sub_item
                continue
            if isinstance(sub_item, bs4.Tag) and sub_item.name.lower() == 'span' and sub_item.has_attr('class') and ('phonetic' in sub_item.get('class')):
                pron_phonetic = sub_item
                continue
        if pron_phonetic is None:
            pron_fallback = True
            break
        if pron_lang is None:
            pron_lang = ''
        pron_lang = pron_lang.strip()
        pron_phonetic = pron_phonetic.text
        pron[pron_lang] = pron_phonetic
    if pron_fallback:
        for item in ss(root, '.phonetic'):
            if item.name.lower() == 'span':
                pron[''] = item.text
                break
    word_struct['pronunciation'] = pron
    if pron_fallback:
        d = root.select('.wordGroup.def')
        p = root.select('.wordGroup.pos')
        ds = ''
        dp = ''
        if len(d) > 0:
            ds = d[0].text.strip()
        if len(p) > 0:
            dp = p[0].text.strip()
        word_struct['paraphrase'] = (dp + ' ' + ds).strip()
    else:
        nodes = ss(root, '#phrsListTab .trans-container ul')
        basic_desc = []
        if len(nodes) != 0:
            ul = nodes[0]
            for li in ul.children:
                if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
                    continue
                basic_desc.append(li.text.strip())
        word_struct['paraphrase'] = basic_desc
    rank = ''
    nodes = ss(root, '.rank')
    if len(nodes) != 0:
        rank = nodes[0].text.strip()
    word_struct['rank'] = rank
    pattern = ''
    nodes = ss(root, '.collinsToggle .pattern')
    if len(nodes) != 0:
        pattern = multi_space_to_single(nodes[0].text.strip())
    word_struct['pattern'] = pattern
    word_struct['sentence'] = []
    for child in ss(root, '.collinsToggle .ol li'):
        p = ss(child, 'p')
        if len(p) == 0:
            continue
        p = p[0]
        desc = ''
        cx = ''
        for node in p.children:
            if isinstance(node, str):
                desc += node
            if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
                cx = node.text
        desc = multi_space_to_single(desc.strip())
        examples = []
        for el in ss(child, '.exampleLists'):
            examp = []
            for p in ss(el, '.examples p'):
                examp.append(p.text.strip())
            examples.append(examp)
        word_struct['sentence'].append([desc, cx, examples])
    if not word_struct['sentence']:
        for v in root.select('#bilingual ul li'):
            p = ss(v, 'p')
            ll = []
            for p in ss(v, 'p'):
                if len(p) == 0:
                    continue
                if 'class' not in p.attrs:
                    ll.append(p.text.strip())
            if len(ll) != 0:
                word_struct['sentence'].append(ll)
    return word_struct","for v in root.select('#bilingual ul li'):
    p = ss(v, 'p')
    ll = []
    for p in ss(v, 'p'):
        if len(p) == 0:
            continue
        if 'class' not in p.attrs:
            ll.append(p.text.strip())
    if len(ll) != 0:
        word_struct['sentence'].append(ll)","word_struct['sentence'] += [[p.text.strip() for p in ss(v, 'p') if len(p) != 0 and 'class' not in p.attrs] for v in root.select('#bilingual ul li')]",Cannot refactor,-1,1,,,,robosuite,,,,,
Wudao-dict,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Wudao-dict/soup/parse_21c.py,https://github.com/ChestnutHeng/Wudao-dict/tree/master/soup/parse_21c.py,,get_text$31,"def get_text(word):
    content = get_html(word)
    word_struct = {'word': word}
    root = bs4.BeautifulSoup(content, 'lxml')
    pron = {}
    pron_fallback = False
    for pron_item in ss(root, '.pronounce'):
        pron_lang = None
        pron_phonetic = None
        for sub_item in pron_item.children:
            if isinstance(sub_item, str) and pron_lang is None:
                pron_lang = sub_item
                continue
            if isinstance(sub_item, bs4.Tag) and sub_item.name.lower() == 'span' and sub_item.has_attr('class') and ('phonetic' in sub_item.get('class')):
                pron_phonetic = sub_item
                continue
        if pron_phonetic is None:
            pron_fallback = True
            break
        if pron_lang is None:
            pron_lang = ''
        pron_lang = pron_lang.strip()
        pron_phonetic = pron_phonetic.text
        pron[pron_lang] = pron_phonetic
    if pron_fallback:
        for item in ss(root, '.phonetic'):
            if item.name.lower() == 'span':
                pron[''] = item.text
                break
    word_struct['pronunciation'] = pron
    if pron_fallback:
        d = root.select('.wordGroup.def')
        p = root.select('.wordGroup.pos')
        ds = ''
        dp = ''
        if len(d) > 0:
            ds = d[0].text.strip()
        if len(p) > 0:
            dp = p[0].text.strip()
        word_struct['paraphrase'] = (dp + ' ' + ds).strip()
    else:
        nodes = ss(root, '#phrsListTab .trans-container ul')
        basic_desc = []
        if len(nodes) != 0:
            ul = nodes[0]
            for li in ul.children:
                if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
                    continue
                basic_desc.append(li.text.strip())
        word_struct['paraphrase'] = basic_desc
    rank = ''
    nodes = ss(root, '.rank')
    if len(nodes) != 0:
        rank = nodes[0].text.strip()
    word_struct['rank'] = rank
    pattern = ''
    nodes = ss(root, '.collinsToggle .pattern')
    if len(nodes) != 0:
        pattern = multi_space_to_single(nodes[0].text.strip())
    word_struct['pattern'] = pattern
    word_struct['sentence'] = []
    for child in ss(root, '.collinsToggle .ol li'):
        p = ss(child, 'p')
        if len(p) == 0:
            continue
        p = p[0]
        desc = ''
        cx = ''
        for node in p.children:
            if isinstance(node, str):
                desc += node
            if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
                cx = node.text
        desc = multi_space_to_single(desc.strip())
        examples = []
        for el in ss(child, '.exampleLists'):
            examp = []
            for p in ss(el, '.examples p'):
                examp.append(p.text.strip())
            examples.append(examp)
        word_struct['sentence'].append([desc, cx, examples])
    if not word_struct['sentence']:
        for v in root.select('#bilingual ul li'):
            p = ss(v, 'p')
            ll = []
            for p in ss(v, 'p'):
                if len(p) == 0:
                    continue
                if 'class' not in p.attrs:
                    ll.append(p.text.strip())
            if len(ll) != 0:
                word_struct['sentence'].append(ll)
    return word_struct","for li in ul.children:
    if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
        continue
    basic_desc.append(li.text.strip())","basic_desc = [li.text.strip() for li in ul.children if isinstance(li, bs4.Tag) and li.name.lower() == 'li']",Cannot refactor,-1,1,,,,robosuite,,,,,
Wudao-dict,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Wudao-dict/soup/parse_21c.py,https://github.com/ChestnutHeng/Wudao-dict/tree/master/soup/parse_21c.py,,get_text$31,"def get_text(word):
    content = get_html(word)
    word_struct = {'word': word}
    root = bs4.BeautifulSoup(content, 'lxml')
    pron = {}
    pron_fallback = False
    for pron_item in ss(root, '.pronounce'):
        pron_lang = None
        pron_phonetic = None
        for sub_item in pron_item.children:
            if isinstance(sub_item, str) and pron_lang is None:
                pron_lang = sub_item
                continue
            if isinstance(sub_item, bs4.Tag) and sub_item.name.lower() == 'span' and sub_item.has_attr('class') and ('phonetic' in sub_item.get('class')):
                pron_phonetic = sub_item
                continue
        if pron_phonetic is None:
            pron_fallback = True
            break
        if pron_lang is None:
            pron_lang = ''
        pron_lang = pron_lang.strip()
        pron_phonetic = pron_phonetic.text
        pron[pron_lang] = pron_phonetic
    if pron_fallback:
        for item in ss(root, '.phonetic'):
            if item.name.lower() == 'span':
                pron[''] = item.text
                break
    word_struct['pronunciation'] = pron
    if pron_fallback:
        d = root.select('.wordGroup.def')
        p = root.select('.wordGroup.pos')
        ds = ''
        dp = ''
        if len(d) > 0:
            ds = d[0].text.strip()
        if len(p) > 0:
            dp = p[0].text.strip()
        word_struct['paraphrase'] = (dp + ' ' + ds).strip()
    else:
        nodes = ss(root, '#phrsListTab .trans-container ul')
        basic_desc = []
        if len(nodes) != 0:
            ul = nodes[0]
            for li in ul.children:
                if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
                    continue
                basic_desc.append(li.text.strip())
        word_struct['paraphrase'] = basic_desc
    rank = ''
    nodes = ss(root, '.rank')
    if len(nodes) != 0:
        rank = nodes[0].text.strip()
    word_struct['rank'] = rank
    pattern = ''
    nodes = ss(root, '.collinsToggle .pattern')
    if len(nodes) != 0:
        pattern = multi_space_to_single(nodes[0].text.strip())
    word_struct['pattern'] = pattern
    word_struct['sentence'] = []
    for child in ss(root, '.collinsToggle .ol li'):
        p = ss(child, 'p')
        if len(p) == 0:
            continue
        p = p[0]
        desc = ''
        cx = ''
        for node in p.children:
            if isinstance(node, str):
                desc += node
            if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
                cx = node.text
        desc = multi_space_to_single(desc.strip())
        examples = []
        for el in ss(child, '.exampleLists'):
            examp = []
            for p in ss(el, '.examples p'):
                examp.append(p.text.strip())
            examples.append(examp)
        word_struct['sentence'].append([desc, cx, examples])
    if not word_struct['sentence']:
        for v in root.select('#bilingual ul li'):
            p = ss(v, 'p')
            ll = []
            for p in ss(v, 'p'):
                if len(p) == 0:
                    continue
                if 'class' not in p.attrs:
                    ll.append(p.text.strip())
            if len(ll) != 0:
                word_struct['sentence'].append(ll)
    return word_struct","for p in ss(v, 'p'):
    if len(p) == 0:
        continue
    if 'class' not in p.attrs:
        ll.append(p.text.strip())","ll = [p.text.strip() for p in ss(v, 'p') if len(p) != 0 and 'class' not in p.attrs]",Cannot refactor,-1,1,,,,robosuite,,,,,
keract,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/keract/keract/keract.py,https://github.com/philipperemy/keract/tree/master/keract/keract.py,,_convert_1d_to_2d$19,"def _convert_1d_to_2d(num_units: int):
    divisors = []
    for i in range(1, num_units + 1):
        q = num_units / i
        if int(q) == q:
            divisors.append(i)
    divisors = list(reversed(divisors))
    pairs = []
    for d in divisors:
        for e in divisors[1:]:
            if d * e == num_units:
                pairs.append((d, e))
    if len(pairs) == 0:
        return (num_units, 1)
    close_to_square_id = int(np.argmin(np.sum(np.array(pairs), axis=1)))
    return pairs[close_to_square_id]","for i in range(1, num_units + 1):
    q = num_units / i
    if int(q) == q:
        divisors.append(i)","divisors = [i for i in range(1, num_units + 1) if num_units % i == 0]",Cannot refactor,-1,1,,,,robosuite,,,,,
SMARTS,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/SMARTS/baselines/marl_benchmark/metrics/basic_metrics.py,https://github.com/huawei-noah/SMARTS/tree/master/baselines/marl_benchmark/metrics/basic_metrics.py,BehaviorMetric,_compute_agility_index$55,"def _compute_agility_index(self, agent_speed_seq):
    average_speed = []
    for agent_speed in agent_speed_seq:
        mean_speed_list = []
        for speed_list in agent_speed.values():
            mean_speed_list.append(np.mean(speed_list))
        average_speed.append(np.mean(mean_speed_list))
    self.agility = np.mean(average_speed)","for agent_speed in agent_speed_seq:
    mean_speed_list = []
    for speed_list in agent_speed.values():
        mean_speed_list.append(np.mean(speed_list))
    average_speed.append(np.mean(mean_speed_list))",average_speed = [np.mean([np.mean(speed_list) for speed_list in agent_speed.values()]) for agent_speed in agent_speed_seq],Cannot refactor,-1,1,,,,robosuite,,,,,
chemprop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chemprop/chemprop/train/molecule_fingerprint.py,https://github.com/chemprop/chemprop/tree/master/chemprop/train/molecule_fingerprint.py,,molecule_fingerprint$16,"def molecule_fingerprint(args: FingerprintArgs, smiles: List[List[str]]=None) -> List[List[Optional[float]]]:
    """"""
    Loads data and a trained model and uses the model to encode fingerprint vectors for the data.

    :param args: A :class:`~chemprop.args.PredictArgs` object containing arguments for
                 loading data and a model and making predictions.
    :param smiles: List of list of SMILES to make predictions on.
    :return: A list of fingerprint vectors (list of floats)
    """"""
    print('Loading training args')
    train_args = load_args(args.checkpoint_paths[0])
    if args.fingerprint_type == 'MPN':
        validate_feature_sources = False
    else:
        validate_feature_sources = True
    update_prediction_args(predict_args=args, train_args=train_args, validate_feature_sources=validate_feature_sources)
    args: Union[FingerprintArgs, TrainArgs]
    reset_featurization_parameters()
    if args.atom_descriptors == 'feature':
        set_extra_atom_fdim(train_args.atom_features_size)
    if args.bond_features_path is not None:
        set_extra_bond_fdim(train_args.bond_features_size)
    set_explicit_h(train_args.explicit_h)
    set_adding_hs(args.adding_h)
    if train_args.reaction:
        set_reaction(train_args.reaction, train_args.reaction_mode)
    elif train_args.reaction_solvent:
        set_reaction(True, train_args.reaction_mode)
    print('Loading data')
    if smiles is not None:
        full_data = get_data_from_smiles(smiles=smiles, skip_invalid_smiles=False, features_generator=args.features_generator)
    else:
        full_data = get_data(path=args.test_path, smiles_columns=args.smiles_columns, target_columns=[], ignore_columns=[], skip_invalid_smiles=False, args=args, store_row=True)
    print('Validating SMILES')
    full_to_valid_indices = {}
    valid_index = 0
    for full_index in range(len(full_data)):
        if all((mol is not None for mol in full_data[full_index].mol)):
            full_to_valid_indices[full_index] = valid_index
            valid_index += 1
    test_data = MoleculeDataset([full_data[i] for i in sorted(full_to_valid_indices.keys())])
    if len(test_data) == 0:
        return [None] * len(full_data)
    print(f'Test size = {len(test_data):,}')
    test_data_loader = MoleculeDataLoader(dataset=test_data, batch_size=args.batch_size, num_workers=args.num_workers)
    if args.fingerprint_type == 'MPN':
        if args.atom_descriptors == 'descriptor':
            total_fp_size = (args.hidden_size + test_data.atom_descriptors_size()) * args.number_of_molecules
        elif args.reaction_solvent:
            total_fp_size = args.hidden_size + args.hidden_size_solvent
        else:
            total_fp_size = args.hidden_size * args.number_of_molecules
        if args.features_only:
            raise ValueError('With features_only models, there is no latent MPN representation. Use last_FFN fingerprint type instead.')
    elif args.fingerprint_type == 'last_FFN':
        if args.ffn_num_layers != 1:
            total_fp_size = args.ffn_hidden_size
        else:
            raise ValueError('With a ffn_num_layers of 1, there is no latent FFN representation. Use MPN fingerprint type instead.')
    else:
        raise ValueError(f'Fingerprint type {args.fingerprint_type} not supported')
    all_fingerprints = np.zeros((len(test_data), total_fp_size, len(args.checkpoint_paths)))
    print(f'Encoding smiles into a fingerprint vector from {len(args.checkpoint_paths)} models.')
    for (index, checkpoint_path) in enumerate(tqdm(args.checkpoint_paths, total=len(args.checkpoint_paths))):
        model = load_checkpoint(checkpoint_path, device=args.device)
        (scaler, features_scaler, atom_descriptor_scaler, bond_feature_scaler) = load_scalers(args.checkpoint_paths[index])
        if args.features_scaling or train_args.atom_descriptor_scaling or train_args.bond_feature_scaling:
            test_data.reset_features_and_targets()
            if args.features_scaling:
                test_data.normalize_features(features_scaler)
            if train_args.atom_descriptor_scaling and args.atom_descriptors is not None:
                test_data.normalize_features(atom_descriptor_scaler, scale_atom_descriptors=True)
            if train_args.bond_feature_scaling and args.bond_features_size > 0:
                test_data.normalize_features(bond_feature_scaler, scale_bond_features=True)
        model_fp = model_fingerprint(model=model, data_loader=test_data_loader, fingerprint_type=args.fingerprint_type)
        if args.fingerprint_type == 'MPN' and (args.features_path is not None or args.features_generator):
            model_fp = np.array(model_fp)[:, :total_fp_size]
        all_fingerprints[:, :, index] = model_fp
    print(f'Saving predictions to {args.preds_path}')
    makedirs(args.preds_path, isfile=True)
    fingerprint_columns = []
    if args.fingerprint_type == 'MPN':
        if len(args.checkpoint_paths) == 1:
            for j in range(total_fp_size // args.number_of_molecules):
                for k in range(args.number_of_molecules):
                    fingerprint_columns.append(f'fp_{j}_mol_{k}')
        else:
            for j in range(total_fp_size // args.number_of_molecules):
                for i in range(len(args.checkpoint_paths)):
                    for k in range(args.number_of_molecules):
                        fingerprint_columns.append(f'fp_{j}_mol_{k}_model_{i}')
    elif len(args.checkpoint_paths) == 1:
        for j in range(total_fp_size):
            fingerprint_columns.append(f'fp_{j}')
    else:
        for j in range(total_fp_size):
            for i in range(len(args.checkpoint_paths)):
                fingerprint_columns.append(f'fp_{j}_model_{i}')
    for (full_index, datapoint) in enumerate(full_data):
        valid_index = full_to_valid_indices.get(full_index, None)
        preds = all_fingerprints[valid_index].reshape(len(args.checkpoint_paths) * total_fp_size) if valid_index is not None else ['Invalid SMILES'] * len(args.checkpoint_paths) * total_fp_size
        for i in range(len(fingerprint_columns)):
            datapoint.row[fingerprint_columns[i]] = preds[i]
    with open(args.preds_path, 'w') as f:
        writer = csv.DictWriter(f, fieldnames=args.smiles_columns + fingerprint_columns, extrasaction='ignore')
        writer.writeheader()
        for datapoint in full_data:
            writer.writerow(datapoint.row)
    return all_fingerprints","for j in range(total_fp_size // args.number_of_molecules):
    for i in range(len(args.checkpoint_paths)):
        for k in range(args.number_of_molecules):
            fingerprint_columns.append(f'fp_{j}_mol_{k}_model_{i}')",fingerprint_columns += [f'fp_{j}_mol_{k}_model_{i}' for j in range(total_fp_size // args.number_of_molecules) for i in range(len(args.checkpoint_paths)) for k in range(args.number_of_molecules)],Cannot refactor,-1,1,,,,robosuite,,,,,
chemprop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chemprop/chemprop/train/molecule_fingerprint.py,https://github.com/chemprop/chemprop/tree/master/chemprop/train/molecule_fingerprint.py,,molecule_fingerprint$16,"def molecule_fingerprint(args: FingerprintArgs, smiles: List[List[str]]=None) -> List[List[Optional[float]]]:
    """"""
    Loads data and a trained model and uses the model to encode fingerprint vectors for the data.

    :param args: A :class:`~chemprop.args.PredictArgs` object containing arguments for
                 loading data and a model and making predictions.
    :param smiles: List of list of SMILES to make predictions on.
    :return: A list of fingerprint vectors (list of floats)
    """"""
    print('Loading training args')
    train_args = load_args(args.checkpoint_paths[0])
    if args.fingerprint_type == 'MPN':
        validate_feature_sources = False
    else:
        validate_feature_sources = True
    update_prediction_args(predict_args=args, train_args=train_args, validate_feature_sources=validate_feature_sources)
    args: Union[FingerprintArgs, TrainArgs]
    reset_featurization_parameters()
    if args.atom_descriptors == 'feature':
        set_extra_atom_fdim(train_args.atom_features_size)
    if args.bond_features_path is not None:
        set_extra_bond_fdim(train_args.bond_features_size)
    set_explicit_h(train_args.explicit_h)
    set_adding_hs(args.adding_h)
    if train_args.reaction:
        set_reaction(train_args.reaction, train_args.reaction_mode)
    elif train_args.reaction_solvent:
        set_reaction(True, train_args.reaction_mode)
    print('Loading data')
    if smiles is not None:
        full_data = get_data_from_smiles(smiles=smiles, skip_invalid_smiles=False, features_generator=args.features_generator)
    else:
        full_data = get_data(path=args.test_path, smiles_columns=args.smiles_columns, target_columns=[], ignore_columns=[], skip_invalid_smiles=False, args=args, store_row=True)
    print('Validating SMILES')
    full_to_valid_indices = {}
    valid_index = 0
    for full_index in range(len(full_data)):
        if all((mol is not None for mol in full_data[full_index].mol)):
            full_to_valid_indices[full_index] = valid_index
            valid_index += 1
    test_data = MoleculeDataset([full_data[i] for i in sorted(full_to_valid_indices.keys())])
    if len(test_data) == 0:
        return [None] * len(full_data)
    print(f'Test size = {len(test_data):,}')
    test_data_loader = MoleculeDataLoader(dataset=test_data, batch_size=args.batch_size, num_workers=args.num_workers)
    if args.fingerprint_type == 'MPN':
        if args.atom_descriptors == 'descriptor':
            total_fp_size = (args.hidden_size + test_data.atom_descriptors_size()) * args.number_of_molecules
        elif args.reaction_solvent:
            total_fp_size = args.hidden_size + args.hidden_size_solvent
        else:
            total_fp_size = args.hidden_size * args.number_of_molecules
        if args.features_only:
            raise ValueError('With features_only models, there is no latent MPN representation. Use last_FFN fingerprint type instead.')
    elif args.fingerprint_type == 'last_FFN':
        if args.ffn_num_layers != 1:
            total_fp_size = args.ffn_hidden_size
        else:
            raise ValueError('With a ffn_num_layers of 1, there is no latent FFN representation. Use MPN fingerprint type instead.')
    else:
        raise ValueError(f'Fingerprint type {args.fingerprint_type} not supported')
    all_fingerprints = np.zeros((len(test_data), total_fp_size, len(args.checkpoint_paths)))
    print(f'Encoding smiles into a fingerprint vector from {len(args.checkpoint_paths)} models.')
    for (index, checkpoint_path) in enumerate(tqdm(args.checkpoint_paths, total=len(args.checkpoint_paths))):
        model = load_checkpoint(checkpoint_path, device=args.device)
        (scaler, features_scaler, atom_descriptor_scaler, bond_feature_scaler) = load_scalers(args.checkpoint_paths[index])
        if args.features_scaling or train_args.atom_descriptor_scaling or train_args.bond_feature_scaling:
            test_data.reset_features_and_targets()
            if args.features_scaling:
                test_data.normalize_features(features_scaler)
            if train_args.atom_descriptor_scaling and args.atom_descriptors is not None:
                test_data.normalize_features(atom_descriptor_scaler, scale_atom_descriptors=True)
            if train_args.bond_feature_scaling and args.bond_features_size > 0:
                test_data.normalize_features(bond_feature_scaler, scale_bond_features=True)
        model_fp = model_fingerprint(model=model, data_loader=test_data_loader, fingerprint_type=args.fingerprint_type)
        if args.fingerprint_type == 'MPN' and (args.features_path is not None or args.features_generator):
            model_fp = np.array(model_fp)[:, :total_fp_size]
        all_fingerprints[:, :, index] = model_fp
    print(f'Saving predictions to {args.preds_path}')
    makedirs(args.preds_path, isfile=True)
    fingerprint_columns = []
    if args.fingerprint_type == 'MPN':
        if len(args.checkpoint_paths) == 1:
            for j in range(total_fp_size // args.number_of_molecules):
                for k in range(args.number_of_molecules):
                    fingerprint_columns.append(f'fp_{j}_mol_{k}')
        else:
            for j in range(total_fp_size // args.number_of_molecules):
                for i in range(len(args.checkpoint_paths)):
                    for k in range(args.number_of_molecules):
                        fingerprint_columns.append(f'fp_{j}_mol_{k}_model_{i}')
    elif len(args.checkpoint_paths) == 1:
        for j in range(total_fp_size):
            fingerprint_columns.append(f'fp_{j}')
    else:
        for j in range(total_fp_size):
            for i in range(len(args.checkpoint_paths)):
                fingerprint_columns.append(f'fp_{j}_model_{i}')
    for (full_index, datapoint) in enumerate(full_data):
        valid_index = full_to_valid_indices.get(full_index, None)
        preds = all_fingerprints[valid_index].reshape(len(args.checkpoint_paths) * total_fp_size) if valid_index is not None else ['Invalid SMILES'] * len(args.checkpoint_paths) * total_fp_size
        for i in range(len(fingerprint_columns)):
            datapoint.row[fingerprint_columns[i]] = preds[i]
    with open(args.preds_path, 'w') as f:
        writer = csv.DictWriter(f, fieldnames=args.smiles_columns + fingerprint_columns, extrasaction='ignore')
        writer.writeheader()
        for datapoint in full_data:
            writer.writerow(datapoint.row)
    return all_fingerprints","for j in range(total_fp_size):
    fingerprint_columns.append(f'fp_{j}')",fingerprint_columns += [f'fp_{j}' for j in range(total_fp_size)],Cannot refactor,-1,1,,,,robosuite,,,,,
chemprop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/chemprop/chemprop/train/molecule_fingerprint.py,https://github.com/chemprop/chemprop/tree/master/chemprop/train/molecule_fingerprint.py,,molecule_fingerprint$16,"def molecule_fingerprint(args: FingerprintArgs, smiles: List[List[str]]=None) -> List[List[Optional[float]]]:
    """"""
    Loads data and a trained model and uses the model to encode fingerprint vectors for the data.

    :param args: A :class:`~chemprop.args.PredictArgs` object containing arguments for
                 loading data and a model and making predictions.
    :param smiles: List of list of SMILES to make predictions on.
    :return: A list of fingerprint vectors (list of floats)
    """"""
    print('Loading training args')
    train_args = load_args(args.checkpoint_paths[0])
    if args.fingerprint_type == 'MPN':
        validate_feature_sources = False
    else:
        validate_feature_sources = True
    update_prediction_args(predict_args=args, train_args=train_args, validate_feature_sources=validate_feature_sources)
    args: Union[FingerprintArgs, TrainArgs]
    reset_featurization_parameters()
    if args.atom_descriptors == 'feature':
        set_extra_atom_fdim(train_args.atom_features_size)
    if args.bond_features_path is not None:
        set_extra_bond_fdim(train_args.bond_features_size)
    set_explicit_h(train_args.explicit_h)
    set_adding_hs(args.adding_h)
    if train_args.reaction:
        set_reaction(train_args.reaction, train_args.reaction_mode)
    elif train_args.reaction_solvent:
        set_reaction(True, train_args.reaction_mode)
    print('Loading data')
    if smiles is not None:
        full_data = get_data_from_smiles(smiles=smiles, skip_invalid_smiles=False, features_generator=args.features_generator)
    else:
        full_data = get_data(path=args.test_path, smiles_columns=args.smiles_columns, target_columns=[], ignore_columns=[], skip_invalid_smiles=False, args=args, store_row=True)
    print('Validating SMILES')
    full_to_valid_indices = {}
    valid_index = 0
    for full_index in range(len(full_data)):
        if all((mol is not None for mol in full_data[full_index].mol)):
            full_to_valid_indices[full_index] = valid_index
            valid_index += 1
    test_data = MoleculeDataset([full_data[i] for i in sorted(full_to_valid_indices.keys())])
    if len(test_data) == 0:
        return [None] * len(full_data)
    print(f'Test size = {len(test_data):,}')
    test_data_loader = MoleculeDataLoader(dataset=test_data, batch_size=args.batch_size, num_workers=args.num_workers)
    if args.fingerprint_type == 'MPN':
        if args.atom_descriptors == 'descriptor':
            total_fp_size = (args.hidden_size + test_data.atom_descriptors_size()) * args.number_of_molecules
        elif args.reaction_solvent:
            total_fp_size = args.hidden_size + args.hidden_size_solvent
        else:
            total_fp_size = args.hidden_size * args.number_of_molecules
        if args.features_only:
            raise ValueError('With features_only models, there is no latent MPN representation. Use last_FFN fingerprint type instead.')
    elif args.fingerprint_type == 'last_FFN':
        if args.ffn_num_layers != 1:
            total_fp_size = args.ffn_hidden_size
        else:
            raise ValueError('With a ffn_num_layers of 1, there is no latent FFN representation. Use MPN fingerprint type instead.')
    else:
        raise ValueError(f'Fingerprint type {args.fingerprint_type} not supported')
    all_fingerprints = np.zeros((len(test_data), total_fp_size, len(args.checkpoint_paths)))
    print(f'Encoding smiles into a fingerprint vector from {len(args.checkpoint_paths)} models.')
    for (index, checkpoint_path) in enumerate(tqdm(args.checkpoint_paths, total=len(args.checkpoint_paths))):
        model = load_checkpoint(checkpoint_path, device=args.device)
        (scaler, features_scaler, atom_descriptor_scaler, bond_feature_scaler) = load_scalers(args.checkpoint_paths[index])
        if args.features_scaling or train_args.atom_descriptor_scaling or train_args.bond_feature_scaling:
            test_data.reset_features_and_targets()
            if args.features_scaling:
                test_data.normalize_features(features_scaler)
            if train_args.atom_descriptor_scaling and args.atom_descriptors is not None:
                test_data.normalize_features(atom_descriptor_scaler, scale_atom_descriptors=True)
            if train_args.bond_feature_scaling and args.bond_features_size > 0:
                test_data.normalize_features(bond_feature_scaler, scale_bond_features=True)
        model_fp = model_fingerprint(model=model, data_loader=test_data_loader, fingerprint_type=args.fingerprint_type)
        if args.fingerprint_type == 'MPN' and (args.features_path is not None or args.features_generator):
            model_fp = np.array(model_fp)[:, :total_fp_size]
        all_fingerprints[:, :, index] = model_fp
    print(f'Saving predictions to {args.preds_path}')
    makedirs(args.preds_path, isfile=True)
    fingerprint_columns = []
    if args.fingerprint_type == 'MPN':
        if len(args.checkpoint_paths) == 1:
            for j in range(total_fp_size // args.number_of_molecules):
                for k in range(args.number_of_molecules):
                    fingerprint_columns.append(f'fp_{j}_mol_{k}')
        else:
            for j in range(total_fp_size // args.number_of_molecules):
                for i in range(len(args.checkpoint_paths)):
                    for k in range(args.number_of_molecules):
                        fingerprint_columns.append(f'fp_{j}_mol_{k}_model_{i}')
    elif len(args.checkpoint_paths) == 1:
        for j in range(total_fp_size):
            fingerprint_columns.append(f'fp_{j}')
    else:
        for j in range(total_fp_size):
            for i in range(len(args.checkpoint_paths)):
                fingerprint_columns.append(f'fp_{j}_model_{i}')
    for (full_index, datapoint) in enumerate(full_data):
        valid_index = full_to_valid_indices.get(full_index, None)
        preds = all_fingerprints[valid_index].reshape(len(args.checkpoint_paths) * total_fp_size) if valid_index is not None else ['Invalid SMILES'] * len(args.checkpoint_paths) * total_fp_size
        for i in range(len(fingerprint_columns)):
            datapoint.row[fingerprint_columns[i]] = preds[i]
    with open(args.preds_path, 'w') as f:
        writer = csv.DictWriter(f, fieldnames=args.smiles_columns + fingerprint_columns, extrasaction='ignore')
        writer.writeheader()
        for datapoint in full_data:
            writer.writerow(datapoint.row)
    return all_fingerprints","for j in range(total_fp_size):
    for i in range(len(args.checkpoint_paths)):
        fingerprint_columns.append(f'fp_{j}_model_{i}')",fingerprint_columns += [f'fp_{j}_model_{i}' for j in range(total_fp_size) for i in range(len(args.checkpoint_paths))],Cannot refactor,-1,1,,,,robosuite,,,,,
oio-sds,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/oio-sds/oio/api/ec.py,https://github.com/open-io/oio-sds/tree/master/oio/api/ec.py,ECStream,_decode_segments$289,"def _decode_segments(self, fragment_iterators):
    """"""
        Reads from fragments and yield full segments
        """"""
    queues = []
    for _j in range(len(fragment_iterators)):
        queues.append(LightQueue(1))

    def put_in_queue(fragment_iterator, queue):
        """"""
            Coroutine to read the fragments from the iterator
            """"""
        try:
            for fragment in fragment_iterator:
                queue.put(fragment)
        except GreenletExit:
            pass
        except ChunkReadTimeout as err:
            self.logger.error('%s (reqid=%s)', err, self.reqid)
        except Exception:
            self.logger.exception('Exception on reading (reqid=%s)', self.reqid)
        finally:
            queue.resize(2)
            queue.put(None)
            fragment_iterator.close()
    with ContextPool(len(fragment_iterators)) as pool:
        for (fragment_iterator, queue) in zip(fragment_iterators, queues):
            pool.spawn(put_in_queue, fragment_iterator, queue)
        while True:
            data = []
            for queue in queues:
                fragment = queue.get()
                data.append(fragment)
            if not all(data):
                break
            if self.perfdata is not None:
                ec_start = monotonic_time()
            try:
                segment = self.storage_method.driver.decode(data)
            except exceptions.ECError:
                self.logger.exception('ERROR decoding fragments (reqid=%s)', self.reqid)
                raise
            finally:
                if self.perfdata is not None:
                    ec_end = monotonic_time()
                    duration = ec_end - ec_start
                    rawx_pdata = self.perfdata.setdefault('rawx', dict())
                    rawx_pdata['ec.segments'] = rawx_pdata.get('ec.segments', 0) + 1
                    rawx_pdata['ec.total'] = rawx_pdata.get('ec.total', 0.0) + duration
                    if 'ec.firstsegment' not in rawx_pdata:
                        rawx_pdata['ec.firstsegment'] = duration
            yield segment","for queue in queues:
    fragment = queue.get()
    data.append(fragment)",data = [queue.get() for queue in queues],Cannot refactor,-1,1,,,,robosuite,,,,,
shutit,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/shutit/shutit_class.py,https://github.com/ianmiell/shutit/tree/master//shutit_class.py,ShutIt,do_list_modules$3248,"def do_list_modules(self, long_output=None, sort_order=None):
    """"""Display a list of loaded modules.

		Config items:
			- shutit.list_modules['long']
			  If set, also print each module's run order value

			- shutit.list_modules['sort']
			  Select the column by which the list is ordered:
				- id: sort the list by module id
				- run_order: sort the list by module run order

		The output is also saved to ['build']['log_config_path']/module_order.txt

		Dependencies: operator
		""""""
    shutit_global.shutit_global_object.yield_to_draw()
    cfg = self.cfg
    table_list = []
    if long_output is None:
        long_output = self.list_modules['long']
    if sort_order is None:
        sort_order = self.list_modules['sort']
    if long_output:
        table_list.append(['Order', 'Module ID', 'Description', 'Run Order', 'Built', 'Compatible'])
    else:
        table_list.append(['Module ID', 'Description', 'Built', 'Compatible'])
    if sort_order == 'run_order':
        d = {}
        for m in self.shutit_modules:
            d.update({m.module_id: m.run_order})
        b = sorted(d.items(), key=operator.itemgetter(1))
        count = 0
        for pair in b:
            k = pair[0]
            for m in self.shutit_modules:
                if m.module_id == k:
                    count += 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                        cfg[m.module_id]['shutit.core.module.build'] = False
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    elif sort_order == 'id':
        l = []
        for m in self.shutit_modules:
            l.append(m.module_id)
        l.sort()
        for k in l:
            for m in self.shutit_modules:
                if m.module_id == k:
                    count = 1
                    compatible = True
                    if not cfg[m.module_id]['shutit.core.module.build']:
                        cfg[m.module_id]['shutit.core.module.build'] = True
                        compatible = self.determine_compatibility(m.module_id) == 0
                    if long_output:
                        table_list.append([str(count), m.module_id, m.description, str(m.run_order), str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
                    else:
                        table_list.append([m.module_id, m.description, str(cfg[m.module_id]['shutit.core.module.build']), str(compatible)])
    table = texttable.Texttable()
    table.add_rows(table_list)
    colwidths = []
    for item in table_list:
        for n in range(0, len(item)):
            colwidths.append(10)
        break
    for item in table_list:
        for n in range(0, len(item) - 1):
            if len(str(item[n])) > colwidths[n]:
                colwidths[n] = len(str(item[n]))
    table.set_cols_width(colwidths)
    msg = table.draw()
    shutit_global.shutit_global_object.shutit_print('\n' + msg)","for n in range(0, len(item)):
    colwidths.append(10)","colwidths = [10 for n in range(0, len(item))]",Cannot refactor,-1,1,,,,robosuite,,,,,
powerfulseal,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/powerfulseal/powerfulseal/cli/pscmd.py,https://github.com/powerfulseal/powerfulseal/tree/master/powerfulseal/cli/pscmd.py,PSCmd,completedefault$96,"def completedefault(self, text, line, begidx, endidx):
    suggestions = []
    text_lower = text.lower()
    try:
        for az in self.inventory.get_azs():
            if text_lower in az.lower():
                suggestions.append(str(az))
        for group in self.inventory.get_groups():
            if text_lower in group.lower():
                suggestions.append(str(group))
        for node in self.inventory.find_nodes():
            for attr in ['ip', 'id', 'state', 'name', 'no']:
                val = str(getattr(node, attr))
                if text_lower in val.lower():
                    suggestions.append(val)
        for extra in ('all',):
            if extra.lower().startswith(text.lower()):
                suggestions.append(extra)
    except Exception as e:
        print(e)
    return suggestions","for group in self.inventory.get_groups():
    if text_lower in group.lower():
        suggestions.append(str(group))",suggestions += [str(group) for group in self.inventory.get_groups() if text_lower in group.lower()],Cannot refactor,-1,1,,,,robosuite,,,,,
powerfulseal,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/powerfulseal/powerfulseal/cli/pscmd.py,https://github.com/powerfulseal/powerfulseal/tree/master/powerfulseal/cli/pscmd.py,PSCmd,completedefault$96,"def completedefault(self, text, line, begidx, endidx):
    suggestions = []
    text_lower = text.lower()
    try:
        for az in self.inventory.get_azs():
            if text_lower in az.lower():
                suggestions.append(str(az))
        for group in self.inventory.get_groups():
            if text_lower in group.lower():
                suggestions.append(str(group))
        for node in self.inventory.find_nodes():
            for attr in ['ip', 'id', 'state', 'name', 'no']:
                val = str(getattr(node, attr))
                if text_lower in val.lower():
                    suggestions.append(val)
        for extra in ('all',):
            if extra.lower().startswith(text.lower()):
                suggestions.append(extra)
    except Exception as e:
        print(e)
    return suggestions","for node in self.inventory.find_nodes():
    for attr in ['ip', 'id', 'state', 'name', 'no']:
        val = str(getattr(node, attr))
        if text_lower in val.lower():
            suggestions.append(val)","suggestions += [str(getattr(node, attr)) for node in self.inventory.find_nodes() for attr in ['ip', 'id', 'state', 'name', 'no'] if text_lower in str(getattr(node, attr)).lower()]",Cannot refactor,-1,1,,,,robosuite,,,,,
Ghostwriter,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Ghostwriter/ghostwriter/reporting/tests/test_models.py,https://github.com/GhostManager/Ghostwriter/tree/master/ghostwriter/reporting/tests/test_models.py,ReportFindingLinkModelTests,test_position_change_on_delete$465,"def test_position_change_on_delete(self):
    report = ReportFactory()
    num_of_findings = 5
    findings = []
    for finding_id in range(num_of_findings):
        findings.append(ReportFindingLinkFactory(report=report, severity=self.critical_severity))
    for finding_id in range(num_of_findings):
        findings.append(ReportFindingLinkFactory(report=report, severity=self.high_severity))
    with transaction.atomic():
        findings[3].delete()
        findings[5].delete()
        findings[8].delete()
    cleaned_findings = []
    for f in findings:
        try:
            f.refresh_from_db()
            cleaned_findings.append(f)
        except self.ReportFindingLink.DoesNotExist:
            pass
    self.assertEqual(cleaned_findings[0].position, 1)
    self.assertEqual(cleaned_findings[1].position, 2)
    self.assertEqual(cleaned_findings[2].position, 3)
    self.assertEqual(cleaned_findings[3].position, 4)
    self.assertEqual(cleaned_findings[4].position, 1)
    self.assertEqual(cleaned_findings[5].position, 2)
    self.assertEqual(cleaned_findings[6].position, 3)","for finding_id in range(num_of_findings):
    findings.append(ReportFindingLinkFactory(report=report, severity=self.high_severity))","findings += [ReportFindingLinkFactory(report=report, severity=self.high_severity) for finding_id in range(num_of_findings)]",Cannot refactor,-1,1,,,,robosuite,,,,,
HigherHRNet-Human-Pose-Estimation,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/HigherHRNet-Human-Pose-Estimation/lib/fp16_utils/fp16_optimizer.py,https://github.com/HRNet/HigherHRNet-Human-Pose-Estimation/tree/master/lib/fp16_utils/fp16_optimizer.py,FP16_Optimizer,_check_overflow$225,"def _check_overflow(self):
    params = []
    for group in self.fp16_groups:
        for param in group:
            params.append(param)
    for group in self.fp32_from_fp32_groups:
        for param in group:
            params.append(param)
    self.overflow = self.loss_scaler.has_overflow(params)","for group in self.fp32_from_fp32_groups:
    for param in group:
        params.append(param)",params += [param for group in self.fp32_from_fp32_groups for param in group],Cannot refactor,-1,1,,,,robosuite,,,,,
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/lib/ansible/module_utils/common/process.py,https://github.com/ansible/ansible/tree/master/lib/ansible/module_utils/common/process.py,,get_bin_path$12,"def get_bin_path(arg, opt_dirs=None, required=None):
    """"""
    Find system executable in PATH. Raises ValueError if executable is not found.
    Optional arguments:
       - required:  [Deprecated] Prior to 2.10, if executable is not found and required is true it raises an Exception.
                    In 2.10 and later, an Exception is always raised. This parameter will be removed in 2.14.
       - opt_dirs:  optional list of directories to search in addition to PATH
    If found return full path, otherwise raise ValueError.
    """"""
    opt_dirs = [] if opt_dirs is None else opt_dirs
    sbin_paths = ['/sbin', '/usr/sbin', '/usr/local/sbin']
    paths = []
    for d in opt_dirs:
        if d is not None and os.path.exists(d):
            paths.append(d)
    paths += os.environ.get('PATH', '').split(os.pathsep)
    bin_path = None
    for p in sbin_paths:
        if p not in paths and os.path.exists(p):
            paths.append(p)
    for d in paths:
        if not d:
            continue
        path = os.path.join(d, arg)
        if os.path.exists(path) and (not os.path.isdir(path)) and is_executable(path):
            bin_path = path
            break
    if bin_path is None:
        raise ValueError('Failed to find required executable ""%s"" in paths: %s' % (arg, os.pathsep.join(paths)))
    return bin_path","for p in sbin_paths:
    if p not in paths and os.path.exists(p):
        paths.append(p)",paths += [p for p in sbin_paths if p not in paths and os.path.exists(p)],Cannot refactor,-1,1,,,,robosuite,,,,,
pycorrector,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pycorrector/pycorrector/macbert/preprocess.py,https://github.com/shibing624/pycorrector/tree/master/pycorrector/macbert/preprocess.py,,proc_item$33,"def proc_item(item):
    """"""
    
    Args:
        item:
    Returns:
        list
    """"""
    root = etree.XML(item)
    passages = dict()
    mistakes = []
    for passage in root.xpath('/ESSAY/TEXT/PASSAGE'):
        passages[passage.get('id')] = traditional2simplified(passage.text)
    for mistake in root.xpath('/ESSAY/MISTAKE'):
        mistakes.append({'id': mistake.get('id'), 'location': int(mistake.get('location')) - 1, 'wrong': traditional2simplified(mistake.xpath('./WRONG/text()')[0].strip()), 'correction': traditional2simplified(mistake.xpath('./CORRECTION/text()')[0].strip())})
    rst_items = dict()

    def get_passages_by_id(pgs, _id):
        p = pgs.get(_id)
        if p:
            return p
        _id = _id[:-1] + str(int(_id[-1]) + 1)
        p = pgs.get(_id)
        if p:
            return p
        raise ValueError(f'passage not found by {_id}')
    for mistake in mistakes:
        if mistake['id'] not in rst_items.keys():
            rst_items[mistake['id']] = {'original_text': get_passages_by_id(passages, mistake['id']), 'wrong_ids': [], 'correct_text': get_passages_by_id(passages, mistake['id'])}
        ori_text = rst_items[mistake['id']]['original_text']
        cor_text = rst_items[mistake['id']]['correct_text']
        if len(ori_text) == len(cor_text):
            if ori_text[mistake['location']] in mistake['wrong']:
                rst_items[mistake['id']]['wrong_ids'].append(mistake['location'])
                wrong_char_idx = mistake['wrong'].index(ori_text[mistake['location']])
                start = mistake['location'] - wrong_char_idx
                end = start + len(mistake['wrong'])
                rst_items[mistake['id']]['correct_text'] = f""{cor_text[:start]}{mistake['correction']}{cor_text[end:]}""
        else:
            print(f""error line:\n{mistake['id']}\n{ori_text}\n{cor_text}"")
    rst = []
    for k in rst_items.keys():
        if len(rst_items[k]['correct_text']) == len(rst_items[k]['original_text']):
            rst.append({'id': k, **rst_items[k]})
        else:
            text = rst_items[k]['correct_text']
            rst.append({'id': k, 'correct_text': text, 'original_text': text, 'wrong_ids': []})
    return rst","for k in rst_items.keys():
    if len(rst_items[k]['correct_text']) == len(rst_items[k]['original_text']):
        rst.append({'id': k, **rst_items[k]})
    else:
        text = rst_items[k]['correct_text']
        rst.append({'id': k, 'correct_text': text, 'original_text': text, 'wrong_ids': []})","rst += [{'id': k, **rst_items[k]} if len(rst_items[k]['correct_text']) == len(rst_items[k]['original_text']) else {'id': k, 'correct_text': text, 'original_text': text, 'wrong_ids': []} for (k, text) in [(k, rst_items[k]['correct_text']) for k in rst_items.keys()]]",Cannot refactor,-1,1,,,,robosuite,,,,,
salt,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/salt/salt/modules/ansiblegate.py,https://github.com/saltstack/salt/tree/master/salt/modules/ansiblegate.py,,call$175,"def call(module, *args, **kwargs):
    """"""
    Call an Ansible module by invoking it.

    :param module: the name of the module.
    :param args: Arguments to pass to the module
    :param kwargs: keywords to pass to the module

    CLI Example:

    .. code-block:: bash

        salt * ansible.call ping data=foobar
    """"""
    module_args = []
    for arg in args:
        module_args.append(salt.utils.json.dumps(arg))
    _kwargs = {}
    for _kw in kwargs.get('__pub_arg', []):
        if isinstance(_kw, dict):
            _kwargs = _kw
            break
    else:
        _kwargs = {k: v for (k, v) in kwargs.items() if not k.startswith('__pub')}
    for (key, value) in _kwargs.items():
        module_args.append('{}={}'.format(key, salt.utils.json.dumps(value)))
    with NamedTemporaryFile(mode='w') as inventory:
        ansible_binary_path = salt.utils.path.which('ansible')
        log.debug('Calling ansible module %r', module)
        try:
            proc_exc = subprocess.run([ansible_binary_path, 'localhost', '--limit', '127.0.0.1', '-m', module, '-a', ' '.join(module_args), '-i', inventory.name], stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=__opts__.get('ansible_timeout', DEFAULT_TIMEOUT), universal_newlines=True, check=True, shell=False)
            original_output = proc_exc.stdout
            proc_out = original_output.splitlines()
            if proc_out[0].endswith('{'):
                proc_out[0] = '{'
                try:
                    out = salt.utils.json.loads('\n'.join(proc_out))
                except ValueError as exc:
                    out = {'Error': proc_exc.stderr or str(exc), 'Output': original_output}
                    return out
            elif proc_out[0].endswith('>>'):
                out = {'output': '\n'.join(proc_out[1:])}
            else:
                out = {'output': original_output}
        except subprocess.CalledProcessError as exc:
            out = {'Exitcode': exc.returncode, 'Error': exc.stderr or str(exc)}
            if exc.stdout:
                out['Given JSON output'] = exc.stdout
            return out
    for key in ('invocation', 'changed'):
        out.pop(key, None)
    return out","for (key, value) in _kwargs.items():
    module_args.append('{}={}'.format(key, salt.utils.json.dumps(value)))","module_args += ['{}={}'.format(key, salt.utils.json.dumps(value)) for (key, value) in _kwargs.items()]",Cannot refactor,-1,1,,,,robosuite,,,,,
coding-interview-gym,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/coding-interview-gym/leetcode.com/python/1268_Search_Suggestions_System.py,https://github.com/partho-maple/coding-interview-gym/tree/master/leetcode.com/python/1268_Search_Suggestions_System.py,Solution,suggestedProducts$5,"def suggestedProducts(self, products, searchWord):
    """"""
        :type products: List[str]
        :type searchWord: str
        :rtype: List[List[str]]
        """"""
    products.sort()
    (result, prefix, startIdx) = ([], '', 0)
    for char in searchWord:
        prefix += char
        startIdx = bisect.bisect_left(products, prefix, startIdx)
        currnetSearchRes = []
        for product in products[startIdx:startIdx + 3]:
            if product.startswith(prefix):
                currnetSearchRes.append(product)
        result.append(currnetSearchRes)
    return result","for char in searchWord:
    prefix += char
    startIdx = bisect.bisect_left(products, prefix, startIdx)
    currnetSearchRes = []
    for product in products[startIdx:startIdx + 3]:
        if product.startswith(prefix):
            currnetSearchRes.append(product)
    result.append(currnetSearchRes)","result = [[product for product in products[bisect.bisect_left(products, prefix, startIdx):bisect.bisect_left(products, prefix, startIdx) + 3] if product.startswith(prefix)] for prefix in [searchWord[:i] for i in range(1, len(searchWord) + 1)]]",Cannot refactor,-1,1,,,,robosuite,,,,,
galaxy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/galaxy/lib/galaxy/tool_shed/galaxy_install/tools/tool_panel_manager.py,https://github.com/ansible/galaxy/tree/master/lib/galaxy/tool_shed/galaxy_install/tools/tool_panel_manager.py,ToolPanelManager,add_to_shed_tool_config$25,"def add_to_shed_tool_config(self, shed_tool_conf_dict, elem_list):
    """"""
        ""A tool shed repository is being installed so change the shed_tool_conf file.  Parse the
        config file to generate the entire list of config_elems instead of using the in-memory list
        since it will be a subset of the entire list if one or more repositories have been deactivated.
        """"""
    if not elem_list:
        return
    old_toolbox = self.app.toolbox
    shed_tool_conf = shed_tool_conf_dict['config_filename']
    tool_cache_data_dir = shed_tool_conf_dict.get('tool_cache_data_dir')
    tool_path = shed_tool_conf_dict['tool_path']
    config_elems = []
    try:
        (tree, error_message) = parse_xml(shed_tool_conf, check_exists=False)
    except OSError as exc:
        if exc.errno == errno.ENOENT and shed_tool_conf_dict.get('create', None) is not None:
            log.info('Creating shed tool config with default contents: %s', shed_tool_conf)
            with open(shed_tool_conf, 'w') as fh:
                fh.write(shed_tool_conf_dict['create'])
            (tree, error_message) = parse_xml(shed_tool_conf)
        else:
            log.error('Unable to load shed tool config: %s', shed_tool_conf)
            raise
    if tree:
        root = tree.getroot()
        for elem in root:
            config_elems.append(elem)
        for elem_entry in elem_list:
            if elem_entry.tag == 'section':
                for existing_elem in config_elems:
                    if existing_elem.tag == 'section' and existing_elem.attrib.get('id', None) == elem_entry.attrib.get('id', None):
                        for child in elem_entry:
                            existing_elem.append(child)
                        break
                else:
                    config_elems.append(elem_entry)
            else:
                config_elems.append(elem_entry)
        self.config_elems_to_xml_file(config_elems, shed_tool_conf, tool_path, tool_cache_data_dir)
        self.app.wait_for_toolbox_reload(old_toolbox)
    else:
        log.error(error_message)","for child in elem_entry:
    existing_elem.append(child)",existing_elem += [child for child in elem_entry],Cannot refactor,-1,1,,,,robosuite,,,,,
PaddleX,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/PaddleX/static/paddlex/tools/x2seg.py,https://github.com/PaddlePaddle/PaddleX/tree/master/static/paddlex/tools/x2seg.py,JingLing2Seg,json2png$170,"def json2png(self, image_dir, json_dir, png_dir):
    color_map = self.get_color_map_list(256)
    for img_name in os.listdir(image_dir):
        img_name_part = osp.splitext(img_name)[0]
        json_file = osp.join(json_dir, img_name_part + '.json')
        if not osp.exists(json_file):
            os.remove(osp.join(image_dir, img_name))
            continue
        with open(json_file, mode='r', encoding=get_encoding(json_file)) as j:
            json_info = json.load(j)
            data_shapes = []
            if 'outputs' in json_info:
                for output in json_info['outputs']['object']:
                    if 'polygon' in output.keys():
                        polygon = output['polygon']
                        name = output['name']
                        points = []
                        for i in range(1, int(len(polygon) / 2) + 1):
                            points.append([polygon['x' + str(i)], polygon['y' + str(i)]])
                        shape = {'label': name, 'points': points, 'shape_type': 'polygon'}
                        data_shapes.append(shape)
            if 'size' not in json_info:
                continue
        img_shape = (json_info['size']['height'], json_info['size']['width'], json_info['size']['depth'])
        (lbl, _) = self.shapes_to_label(img_shape=img_shape, shapes=data_shapes, label_name_to_value=self.labels2ids)
        out_png_file = osp.join(png_dir, img_name_part + '.png')
        if lbl.min() >= 0 and lbl.max() <= 255:
            lbl_pil = PIL.Image.fromarray(lbl.astype(np.uint8), mode='P')
            lbl_pil.putpalette(color_map)
            lbl_pil.save(out_png_file)
        else:
            raise ValueError('[%s] Cannot save the pixel-wise class label as PNG. Please consider using the .npy format.' % out_png_file)","for output in json_info['outputs']['object']:
    if 'polygon' in output.keys():
        polygon = output['polygon']
        name = output['name']
        points = []
        for i in range(1, int(len(polygon) / 2) + 1):
            points.append([polygon['x' + str(i)], polygon['y' + str(i)]])
        shape = {'label': name, 'points': points, 'shape_type': 'polygon'}
        data_shapes.append(shape)","data_shapes = [{'label': output['name'], 'points': [[output['polygon']['x' + str(i)], output['polygon']['y' + str(i)]] for i in range(1, int(len(output['polygon']) / 2) + 1)], 'shape_type': 'polygon'} for output in json_info['outputs']['object'] if 'polygon' in output.keys()]",Cannot refactor,-1,1,,,,robosuite,,,,,
mmocr,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/mmocr/mmocr/datasets/pipelines/dbnet_transforms.py,https://github.com/open-mmlab/mmocr/tree/master/mmocr/datasets/pipelines/dbnet_transforms.py,ImgAug,may_augment_bbox$110,"def may_augment_bbox(self, aug, ori_shape, bboxes):
    imgaug_bboxes = []
    for bbox in bboxes:
        (x1, y1, x2, y2) = bbox
        imgaug_bboxes.append(imgaug.BoundingBox(x1=x1, y1=y1, x2=x2, y2=y2))
    imgaug_bboxes = aug.augment_bounding_boxes([imgaug.BoundingBoxesOnImage(imgaug_bboxes, shape=ori_shape)])[0].clip_out_of_image()
    new_bboxes = []
    for box in imgaug_bboxes.bounding_boxes:
        new_bboxes.append(np.array([box.x1, box.y1, box.x2, box.y2], dtype=np.float32))
    return new_bboxes","for bbox in bboxes:
    (x1, y1, x2, y2) = bbox
    imgaug_bboxes.append(imgaug.BoundingBox(x1=x1, y1=y1, x2=x2, y2=y2))","imgaug_bboxes = [imgaug.BoundingBox(x1=bbox[0], y1=bbox[1], x2=bbox[2], y2=bbox[3]) for bbox in bboxes]",Cannot refactor,-1,1,,,,robosuite,,,,,
audio,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/audio/test/torchaudio_unittest/example/souce_sepration/sdr_reference.py,https://github.com/pytorch/audio/tree/master/test/torchaudio_unittest/example/souce_sepration/sdr_reference.py,,batch_SDR_torch$52,"def batch_SDR_torch(estimation, origin, mask=None):
    """"""
    batch-wise SDR caculation for multiple audio files.
    estimation: (batch, nsource, nsample)
    origin: (batch, nsource, nsample)
    mask: optional, (batch, nsample), binary
    """"""
    (batch_size_est, nsource_est, nsample_est) = estimation.size()
    (batch_size_ori, nsource_ori, nsample_ori) = origin.size()
    assert batch_size_est == batch_size_ori, 'Estimation and original sources should have same shape.'
    assert nsource_est == nsource_ori, 'Estimation and original sources should have same shape.'
    assert nsample_est == nsample_ori, 'Estimation and original sources should have same shape.'
    assert nsource_est < nsample_est, 'Axis 1 should be the number of sources, and axis 2 should be the signal.'
    batch_size = batch_size_est
    nsource = nsource_est
    nsample = nsample_est
    estimation = estimation - torch.mean(estimation, 2, keepdim=True).expand_as(estimation)
    origin = origin - torch.mean(origin, 2, keepdim=True).expand_as(estimation)
    perm = list(set(permutations(np.arange(nsource))))
    SDR = torch.zeros((batch_size, nsource, nsource)).type(estimation.type())
    for i in range(nsource):
        for j in range(nsource):
            SDR[:, i, j] = calc_sdr_torch(estimation[:, i], origin[:, j], mask)
    SDR_max = []
    SDR_perm = []
    for permute in perm:
        sdr = []
        for idx in range(len(permute)):
            sdr.append(SDR[:, idx, permute[idx]].view(batch_size, -1))
        sdr = torch.sum(torch.cat(sdr, 1), 1)
        SDR_perm.append(sdr.view(batch_size, 1))
    SDR_perm = torch.cat(SDR_perm, 1)
    (SDR_max, _) = torch.max(SDR_perm, dim=1)
    return SDR_max / nsource","for permute in perm:
    sdr = []
    for idx in range(len(permute)):
        sdr.append(SDR[:, idx, permute[idx]].view(batch_size, -1))
    sdr = torch.sum(torch.cat(sdr, 1), 1)
    SDR_perm.append(sdr.view(batch_size, 1))","SDR_perm = [torch.sum(torch.cat([SDR[:, idx, permute[idx]].view(batch_size, -1) for idx in range(len(permute))], 1), 1).view(batch_size, 1) for permute in perm]",Cannot refactor,-1,1,,,,robosuite,,,,,
linearmodels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/linearmodels/linearmodels/iv/results.py,https://github.com/bashtage/linearmodels/tree/master/linearmodels/iv/results.py,IVModelComparison,summary$1575,"def summary(self) -> Summary:
    """"""
        Model estimation summary.

        Returns
        -------
        Summary
            Summary table of model estimation results

        Supports export to csv, html and latex  using the methods ``summary.as_csv()``,
        ``summary.as_html()`` and ``summary.as_latex()``.
        """"""
    smry = Summary()
    models = list(self._results.keys())
    title = 'Model Comparison'
    stubs = ['Dep. Variable', 'Estimator', 'No. Observations', 'Cov. Est.', 'R-squared', 'Adj. R-squared', 'F-statistic', 'P-value (F-stat)']
    dep_name: Dict[str, str] = {}
    for key in self._results:
        dep_name[key] = str(self._results[key].model.dependent.cols[0])
    dep_names = Series(dep_name)
    vals = concat([dep_names, self.estimator_method, self.nobs, self.cov_estimator, self.rsquared, self.rsquared_adj, self.f_statistic], axis=1)
    vals = [[i for i in v] for v in vals.T.values]
    vals[2] = [str(v) for v in vals[2]]
    for i in range(4, len(vals)):
        vals[i] = [_str(v) for v in vals[i]]
    params = self.params
    precision = getattr(self, self._precision)
    pvalues = asarray(self.pvalues)
    params_fmt = []
    params_stub = []
    for i in range(len(params)):
        formatted_and_starred = []
        for (v, pv) in zip(params.values[i], pvalues[i]):
            formatted_and_starred.append(add_star(_str(v), pv, self._stars))
        params_fmt.append(formatted_and_starred)
        precision_fmt = []
        for v in precision.values[i]:
            v_str = _str(v)
            v_str = '({0})'.format(v_str) if v_str.strip() else v_str
            precision_fmt.append(v_str)
        params_fmt.append(precision_fmt)
        params_stub.append(params.index[i])
        params_stub.append(' ')
    vals = table_concat((vals, params_fmt))
    stubs = stub_concat((stubs, params_stub))
    all_instr = []
    for key in self._results:
        res = self._results[key]
        all_instr.append(res.model.instruments.cols)
    ninstr = max(map(len, all_instr))
    instruments = []
    instrument_stub = ['Instruments']
    for i in range(ninstr):
        if i > 0:
            instrument_stub.append('')
        row = []
        for j in range(len(self._results)):
            instr = all_instr[j]
            if len(instr) > i:
                row.append(instr[i])
            else:
                row.append('')
        instruments.append(row)
    if instruments:
        vals = table_concat((vals, instruments))
        stubs = stub_concat((stubs, instrument_stub))
    txt_fmt = default_txt_fmt.copy()
    txt_fmt['data_aligns'] = 'r'
    txt_fmt['header_align'] = 'r'
    table = SimpleTable(vals, headers=models, title=title, stubs=stubs, txt_fmt=txt_fmt)
    smry.tables.append(table)
    prec_type = self._PRECISION_TYPES[self._precision]
    smry.add_extra_txt(['{0} reported in parentheses'.format(prec_type)])
    return smry","for key in self._results:
    res = self._results[key]
    all_instr.append(res.model.instruments.cols)",all_instr = [self._results[key].model.instruments.cols for key in self._results],Cannot refactor,-1,1,,,,robosuite,,,,,
linearmodels,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/linearmodels/linearmodels/iv/results.py,https://github.com/bashtage/linearmodels/tree/master/linearmodels/iv/results.py,IVModelComparison,summary$1575,"def summary(self) -> Summary:
    """"""
        Model estimation summary.

        Returns
        -------
        Summary
            Summary table of model estimation results

        Supports export to csv, html and latex  using the methods ``summary.as_csv()``,
        ``summary.as_html()`` and ``summary.as_latex()``.
        """"""
    smry = Summary()
    models = list(self._results.keys())
    title = 'Model Comparison'
    stubs = ['Dep. Variable', 'Estimator', 'No. Observations', 'Cov. Est.', 'R-squared', 'Adj. R-squared', 'F-statistic', 'P-value (F-stat)']
    dep_name: Dict[str, str] = {}
    for key in self._results:
        dep_name[key] = str(self._results[key].model.dependent.cols[0])
    dep_names = Series(dep_name)
    vals = concat([dep_names, self.estimator_method, self.nobs, self.cov_estimator, self.rsquared, self.rsquared_adj, self.f_statistic], axis=1)
    vals = [[i for i in v] for v in vals.T.values]
    vals[2] = [str(v) for v in vals[2]]
    for i in range(4, len(vals)):
        vals[i] = [_str(v) for v in vals[i]]
    params = self.params
    precision = getattr(self, self._precision)
    pvalues = asarray(self.pvalues)
    params_fmt = []
    params_stub = []
    for i in range(len(params)):
        formatted_and_starred = []
        for (v, pv) in zip(params.values[i], pvalues[i]):
            formatted_and_starred.append(add_star(_str(v), pv, self._stars))
        params_fmt.append(formatted_and_starred)
        precision_fmt = []
        for v in precision.values[i]:
            v_str = _str(v)
            v_str = '({0})'.format(v_str) if v_str.strip() else v_str
            precision_fmt.append(v_str)
        params_fmt.append(precision_fmt)
        params_stub.append(params.index[i])
        params_stub.append(' ')
    vals = table_concat((vals, params_fmt))
    stubs = stub_concat((stubs, params_stub))
    all_instr = []
    for key in self._results:
        res = self._results[key]
        all_instr.append(res.model.instruments.cols)
    ninstr = max(map(len, all_instr))
    instruments = []
    instrument_stub = ['Instruments']
    for i in range(ninstr):
        if i > 0:
            instrument_stub.append('')
        row = []
        for j in range(len(self._results)):
            instr = all_instr[j]
            if len(instr) > i:
                row.append(instr[i])
            else:
                row.append('')
        instruments.append(row)
    if instruments:
        vals = table_concat((vals, instruments))
        stubs = stub_concat((stubs, instrument_stub))
    txt_fmt = default_txt_fmt.copy()
    txt_fmt['data_aligns'] = 'r'
    txt_fmt['header_align'] = 'r'
    table = SimpleTable(vals, headers=models, title=title, stubs=stubs, txt_fmt=txt_fmt)
    smry.tables.append(table)
    prec_type = self._PRECISION_TYPES[self._precision]
    smry.add_extra_txt(['{0} reported in parentheses'.format(prec_type)])
    return smry","for v in precision.values[i]:
    v_str = _str(v)
    v_str = '({0})'.format(v_str) if v_str.strip() else v_str
    precision_fmt.append(v_str)",precision_fmt = ['({0})'.format(_str(v)) if _str(v).strip() else _str(v) for v in precision.values[i]],Cannot refactor,-1,1,,,,robosuite,,,,,
django-haystack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-haystack/haystack/backends/solr_backend.py,https://github.com/django-haystack/django-haystack/tree/master/haystack/backends/solr_backend.py,SolrSearchBackend,extract_spelling_suggestions$592,"def extract_spelling_suggestions(self, raw_results):
    collations = raw_results.spellcheck.get('collations', None)
    suggestions = raw_results.spellcheck.get('suggestions', None)
    spelling_suggestions = []
    if collations:
        if isinstance(collations, dict):
            collation_values = collations['collation']
            if isinstance(collation_values, str):
                collation_values = [collation_values]
            elif isinstance(collation_values, dict):
                collation_values = [collation_values['collationQuery']]
        elif isinstance(collations[1], dict):
            collation_values = collations
        else:
            collation_values = collations[-1:]
        for i in collation_values:
            spelling_suggestions.append(i['collationQuery'] if isinstance(i, dict) else i)
    elif suggestions:
        if isinstance(suggestions, dict):
            for i in suggestions.values():
                for j in i['suggestion']:
                    if isinstance(j, dict):
                        spelling_suggestions.append(j['word'])
                    else:
                        spelling_suggestions.append(j)
        elif isinstance(suggestions[0], str) and isinstance(suggestions[1], dict):
            for suggestion in suggestions:
                if isinstance(suggestion, dict):
                    for i in suggestion['suggestion']:
                        if isinstance(i, dict):
                            spelling_suggestions.append(i['word'])
                        else:
                            spelling_suggestions.append(i)
        else:
            spelling_suggestions.append(suggestions[-1])
    return spelling_suggestions","for i in suggestions.values():
    for j in i['suggestion']:
        if isinstance(j, dict):
            spelling_suggestions.append(j['word'])
        else:
            spelling_suggestions.append(j)","spelling_suggestions += [j['word'] if isinstance(j, dict) else j for i in suggestions.values() for j in i['suggestion']]",Cannot refactor,-1,1,,,,robosuite,,,,,
django-haystack,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/django-haystack/haystack/backends/solr_backend.py,https://github.com/django-haystack/django-haystack/tree/master/haystack/backends/solr_backend.py,SolrSearchBackend,extract_spelling_suggestions$592,"def extract_spelling_suggestions(self, raw_results):
    collations = raw_results.spellcheck.get('collations', None)
    suggestions = raw_results.spellcheck.get('suggestions', None)
    spelling_suggestions = []
    if collations:
        if isinstance(collations, dict):
            collation_values = collations['collation']
            if isinstance(collation_values, str):
                collation_values = [collation_values]
            elif isinstance(collation_values, dict):
                collation_values = [collation_values['collationQuery']]
        elif isinstance(collations[1], dict):
            collation_values = collations
        else:
            collation_values = collations[-1:]
        for i in collation_values:
            spelling_suggestions.append(i['collationQuery'] if isinstance(i, dict) else i)
    elif suggestions:
        if isinstance(suggestions, dict):
            for i in suggestions.values():
                for j in i['suggestion']:
                    if isinstance(j, dict):
                        spelling_suggestions.append(j['word'])
                    else:
                        spelling_suggestions.append(j)
        elif isinstance(suggestions[0], str) and isinstance(suggestions[1], dict):
            for suggestion in suggestions:
                if isinstance(suggestion, dict):
                    for i in suggestion['suggestion']:
                        if isinstance(i, dict):
                            spelling_suggestions.append(i['word'])
                        else:
                            spelling_suggestions.append(i)
        else:
            spelling_suggestions.append(suggestions[-1])
    return spelling_suggestions","for suggestion in suggestions:
    if isinstance(suggestion, dict):
        for i in suggestion['suggestion']:
            if isinstance(i, dict):
                spelling_suggestions.append(i['word'])
            else:
                spelling_suggestions.append(i)","spelling_suggestions += [i['word'] if isinstance(i, dict) else i for suggestion in suggestions for i in suggestion['suggestion']]",Cannot refactor,-1,1,,,,robosuite,,,,,
nova,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/nova/nova/virt/libvirt/driver.py,https://github.com/openstack/nova/tree/master/nova/virt/libvirt/driver.py,LibvirtDriver,_get_guest_config_meta$5424,"def _get_guest_config_meta(self, instance, network_info):
    """"""Get metadata config for guest.""""""
    meta = vconfig.LibvirtConfigGuestMetaNovaInstance()
    meta.package = version.version_string_with_package()
    meta.name = instance.display_name
    meta.creationTime = time.time()
    if instance.image_ref not in ('', None):
        meta.roottype = 'image'
        meta.rootid = instance.image_ref
    system_meta = instance.system_metadata
    ometa = vconfig.LibvirtConfigGuestMetaNovaOwner()
    ometa.userid = instance.user_id
    ometa.username = system_meta.get('owner_user_name', 'N/A')
    ometa.projectid = instance.project_id
    ometa.projectname = system_meta.get('owner_project_name', 'N/A')
    meta.owner = ometa
    fmeta = vconfig.LibvirtConfigGuestMetaNovaFlavor()
    flavor = instance.flavor
    fmeta.name = flavor.name
    fmeta.memory = flavor.memory_mb
    fmeta.vcpus = flavor.vcpus
    fmeta.ephemeral = flavor.ephemeral_gb
    fmeta.disk = flavor.root_gb
    fmeta.swap = flavor.swap
    meta.flavor = fmeta
    ports = []
    for vif in network_info:
        ips = []
        for subnet in vif.get('network', {}).get('subnets', []):
            for ip in subnet.get('ips', []):
                ips.append(vconfig.LibvirtConfigGuestMetaNovaIp(ip.get('type'), ip.get('address'), ip.get('version')))
        ports.append(vconfig.LibvirtConfigGuestMetaNovaPort(vif.get('id'), ips=ips))
    meta.ports = vconfig.LibvirtConfigGuestMetaNovaPorts(ports)
    return meta","for vif in network_info:
    ips = []
    for subnet in vif.get('network', {}).get('subnets', []):
        for ip in subnet.get('ips', []):
            ips.append(vconfig.LibvirtConfigGuestMetaNovaIp(ip.get('type'), ip.get('address'), ip.get('version')))
    ports.append(vconfig.LibvirtConfigGuestMetaNovaPort(vif.get('id'), ips=ips))","ports = [vconfig.LibvirtConfigGuestMetaNovaPort(vif.get('id'), ips=[vconfig.LibvirtConfigGuestMetaNovaIp(ip.get('type'), ip.get('address'), ip.get('version')) for subnet in vif.get('network', {}).get('subnets', []) for ip in subnet.get('ips', [])]) for vif in network_info]",Cannot refactor,-1,1,,,,robosuite,,,,,
leafmap,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/leafmap/leafmap/foliumap.py,https://github.com/giswqs/leafmap/tree/master/leafmap/foliumap.py,Map,add_cog_mosaic_from_file$816,"def add_cog_mosaic_from_file(self, filepath, skip_rows=0, name='Untitled', attribution='.', opacity=1.0, shown=True, titiler_endpoint='https://api.cogeo.xyz/', username='anonymous', overwrite=False, show_footprints=False, verbose=True, **kwargs):
    """"""Add a virtual mosaic of COGs to the map.

        Args:
            filepath (str): Local path or HTTP URL to the csv/txt file containing COG URLs.
            skip_rows (int, optional): The number of rows to skip in the file. Defaults to 0.
            name (str, optional): The layer name to use for the layer. Defaults to 'Untitled'.
            attribution (str, optional): The attribution to use. Defaults to '.'.
            opacity (float, optional): The opacity of the layer. Defaults to 1.
            shown (bool, optional): A flag indicating whether the layer should be on by default. Defaults to True.
            titiler_endpoint (str, optional): Titiler endpoint. Defaults to ""https://api.cogeo.xyz/"".
            username (str, optional): The username to create mosaic using the titiler endpoint. Defaults to 'anonymous'.
            overwrite (bool, optional): Whether or not to replace existing layer with the same layer name. Defaults to False.
            show_footprints (bool, optional): Whether or not to show footprints of COGs. Defaults to False.
            verbose (bool, optional): Whether or not to print descriptions. Defaults to True.
        """"""
    import urllib
    layername = name.replace(' ', '_')
    links = []
    if filepath.startswith('http'):
        data = urllib.request.urlopen(filepath)
        for line in data:
            links.append(line.decode('utf-8').strip())
    else:
        with open(filepath) as f:
            links = [line.strip() for line in f.readlines()]
    links = links[skip_rows:]
    tile = cog_mosaic(links, titiler_endpoint=titiler_endpoint, username=username, layername=layername, overwrite=overwrite, verbose=verbose)
    self.add_tile_layer(tile, name, attribution, opacity, shown)
    if show_footprints:
        if verbose:
            print(f'Generating footprints of {len(links)} COGs. This might take a while ...')
        coords = []
        for link in links:
            coord = cog_bounds(link)
            if coord is not None:
                coords.append(coord)
        fc = coords_to_geojson(coords)
        folium.GeoJson(data=fc, name=name + '_footprints').add_to(self)
        center = get_center(fc)
        if verbose:
            print('The footprint layer has been added.')
    else:
        center = cog_center(links[0], titiler_endpoint)
    self.set_center(center[0], center[1], zoom=6)","for link in links:
    coord = cog_bounds(link)
    if coord is not None:
        coords.append(coord)",coords = [coord for link in links if (coord := cog_bounds(link)) is not None],Cannot refactor,-1,1,,,,robosuite,,,,,
no_find,,,,,,,,,,,,,,,,,,,
pingouin,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pingouin/pingouin/regression.py,https://github.com/raphaelvallat/pingouin/tree/master/pingouin/regression.py,,linear_regression$17,"def linear_regression(X, y, add_intercept=True, weights=None, coef_only=False, alpha=0.05, as_dataframe=True, remove_na=False, relimp=False):
    """"""(Multiple) Linear regression.

    Parameters
    ----------
    X : array_like
        Predictor(s), of shape *(n_samples, n_features)* or *(n_samples)*.
    y : array_like
        Dependent variable, of shape *(n_samples)*.
    add_intercept : bool
        If False, assume that the data are already centered. If True, add a
        constant term to the model. In this case, the first value in the
        output dict is the intercept of the model.

        .. note:: It is generally recommended to include a constant term
            (intercept) to the model to limit the bias and force the residual
            mean to equal zero. The intercept coefficient and p-values
            are however rarely meaningful.
    weights : array_like
        An optional vector of sample weights to be used in the fitting
        process, of shape *(n_samples)*. Missing or negative weights are not
        allowed. If not null, a weighted least squares is calculated.

        .. versionadded:: 0.3.5
    coef_only : bool
        If True, return only the regression coefficients.
    alpha : float
        Alpha value used for the confidence intervals.
        :math:`\\text{CI} = [\\alpha / 2 ; 1 - \\alpha / 2]`
    as_dataframe : bool
        If True, returns a pandas DataFrame. If False, returns a dictionnary.
    remove_na : bool
        If True, apply a listwise deletion of missing values (i.e. the entire
        row is removed). Default is False, which will raise an error if missing
        values are present in either the predictor(s) or dependent
        variable.
    relimp : bool
        If True, returns the relative importance (= contribution) of
        predictors. This is irrelevant when the predictors are uncorrelated:
        the total :math:`R^2` of the model is simply the sum of each univariate
        regression :math:`R^2`-values. However, this does not apply when
        predictors are correlated. Instead, the total :math:`R^2` of the model
        is partitioned by averaging over all combinations of predictors,
        as done in the `relaimpo
        <https://cran.r-project.org/web/packages/relaimpo/relaimpo.pdf>`_
        R package (``calc.relimp(type=""lmg"")``).

        .. warning:: The computation time roughly doubles for each
            additional predictor and therefore this can be extremely slow for
            models with more than 12-15 predictors.

        .. versionadded:: 0.3.0

    Returns
    -------
    stats : :py:class:`pandas.DataFrame` or dict
        Linear regression summary:

        * ``'names'``: name of variable(s) in the model (e.g. x1, x2...)
        * ``'coef'``: regression coefficients
        * ``'se'``: standard errors
        * ``'T'``: T-values
        * ``'pval'``: p-values
        * ``'r2'``: coefficient of determination (:math:`R^2`)
        * ``'adj_r2'``: adjusted :math:`R^2`
        * ``'CI[2.5%]'``: lower confidence intervals
        * ``'CI[97.5%]'``: upper confidence intervals
        * ``'relimp'``: relative contribution of each predictor to the final                        :math:`R^2` (only if ``relimp=True``).
        * ``'relimp_perc'``: percent relative contribution

        In addition, the output dataframe comes with hidden attributes such as
        the residuals, and degrees of freedom of the model and residuals, which
        can be accessed as follow, respectively:

        >>> lm = pg.linear_regression() # doctest: +SKIP
        >>> lm.residuals_, lm.df_model_, lm.df_resid_ # doctest: +SKIP

        Note that to follow scikit-learn convention, these hidden atributes end
        with an ""_"". When ``as_dataframe=False`` however, these attributes
        are no longer hidden and can be accessed as any other keys in the
        output dictionary.

        >>> lm = pg.linear_regression() # doctest: +SKIP
        >>> lm['residuals'], lm['df_model'], lm['df_resid'] # doctest: +SKIP

        When ``as_dataframe=False`` the dictionary also contains the
        processed ``X`` and ``y`` arrays (i.e, with NaNs removed if
        ``remove_na=True``) and the model's predicted values ``pred``.

        >>> lm['X'], lm['y'], lm['pred'] # doctest: +SKIP

        For a weighted least squares fit, the weighted ``Xw`` and ``yw``
        arrays are included in the dictionary.

        >>> lm['Xw'], lm['yw'] # doctest: +SKIP

    See also
    --------
    logistic_regression, mediation_analysis, corr

    Notes
    -----
    The :math:`\\beta` coefficients are estimated using an ordinary least
    squares (OLS) regression, as implemented in the
    :py:func:`scipy.linalg.lstsq` function. The OLS method minimizes
    the sum of squared residuals, and leads to a closed-form expression for
    the estimated :math:`\\beta`:

    .. math:: \\hat{\\beta} = (X^TX)^{-1} X^Ty

    It is generally recommended to include a constant term (intercept) to the
    model to limit the bias and force the residual mean to equal zero.
    Note that intercept coefficient and p-values are however rarely meaningful.

    The standard error of the estimates is a measure of the accuracy of the
    prediction defined as:

    .. math:: \\sigma = \\sqrt{\\text{MSE} \\cdot (X^TX)^{-1}}

    where :math:`\\text{MSE}` is the mean squared error,

    .. math::

        \\text{MSE} = \\frac{SS_{\\text{resid}}}{n - p - 1}
         = \\frac{\\sum{(\\text{true} - \\text{pred})^2}}{n - p - 1}

    :math:`p` is the total number of predictor variables in the model
    (excluding the intercept) and :math:`n` is the sample size.

    Using the :math:`\\beta` coefficients and the standard errors,
    the T-values can be obtained:

    .. math:: T = \\frac{\\beta}{\\sigma}

    and the p-values approximated using a T-distribution with
    :math:`n - p - 1` degrees of freedom.

    The coefficient of determination (:math:`R^2`) is defined as:

    .. math:: R^2 = 1 - (\\frac{SS_{\\text{resid}}}{SS_{\\text{total}}})

    The adjusted :math:`R^2` is defined as:

    .. math:: \\overline{R}^2 = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1}

    The relative importance (``relimp``) column is a partitioning of the
    total :math:`R^2` of the model into individual :math:`R^2` contribution.
    This is calculated by taking the average over average contributions in
    models of different sizes. For more details, please refer to
    `Groemping et al. 2006 <http://dx.doi.org/10.18637/jss.v017.i01>`_
    and the R package `relaimpo
    <https://cran.r-project.org/web/packages/relaimpo/relaimpo.pdf>`_.

    Note that Pingouin will automatically remove any duplicate columns
    from :math:`X`, as well as any column with only one unique value
    (constant), excluding the intercept.

    Results have been compared against sklearn, R, statsmodels and JASP.

    Examples
    --------
    1. Simple linear regression using columns of a pandas dataframe

    In this first example, we'll use the tips dataset to see how well we
    can predict the waiter's tip (in dollars) based on the total bill (also
    in dollars).

    >>> import numpy as np
    >>> import pingouin as pg
    >>> df = pg.read_dataset('tips')
    >>> # Let's predict the tip ($) based on the total bill (also in $)
    >>> lm = pg.linear_regression(df['total_bill'], df['tip'])
    >>> lm.round(2)
            names  coef    se      T  pval    r2  adj_r2  CI[2.5%]  CI[97.5%]
    0   Intercept  0.92  0.16   5.76   0.0  0.46    0.45      0.61       1.23
    1  total_bill  0.11  0.01  14.26   0.0  0.46    0.45      0.09       0.12

    It comes as no surprise that total bill is indeed a significant predictor
    of the waiter's tip (T=14.26, p<0.05). The :math:`R^2` of the model is 0.46
    and the adjusted :math:`R^2` is 0.45, which means that our model roughly
    explains ~45% of the total variance in the tip amount.

    2. Multiple linear regression

    We can also have more than one predictor and run a multiple linear
    regression. Below, we add the party size as a second predictor of tip.

    >>> # We'll add a second predictor: the party size
    >>> lm = pg.linear_regression(df[['total_bill', 'size']], df['tip'])
    >>> lm.round(2)
            names  coef    se      T  pval    r2  adj_r2  CI[2.5%]  CI[97.5%]
    0   Intercept  0.67  0.19   3.46  0.00  0.47    0.46      0.29       1.05
    1  total_bill  0.09  0.01  10.17  0.00  0.47    0.46      0.07       0.11
    2        size  0.19  0.09   2.26  0.02  0.47    0.46      0.02       0.36

    The party size is also a significant predictor of tip (T=2.26, p=0.02).
    Note that adding this new predictor however only improved the :math:`R^2`
    of our model by ~1%.

    This function also works with numpy arrays:

    >>> X = df[['total_bill', 'size']].to_numpy()
    >>> y = df['tip'].to_numpy()
    >>> pg.linear_regression(X, y).round(2)
           names  coef    se      T  pval    r2  adj_r2  CI[2.5%]  CI[97.5%]
    0  Intercept  0.67  0.19   3.46  0.00  0.47    0.46      0.29       1.05
    1         x1  0.09  0.01  10.17  0.00  0.47    0.46      0.07       0.11
    2         x2  0.19  0.09   2.26  0.02  0.47    0.46      0.02       0.36

    3. Get the residuals

    >>> # For clarity, only display the first 9 values
    >>> np.round(lm.residuals_, 2)[:9]
    array([-1.62, -0.55,  0.31,  0.06, -0.11,  0.93,  0.13, -0.81, -0.49])

    Using pandas, we can show a summary of the distribution of the residuals:

    >>> import pandas as pd
    >>> pd.Series(lm.residuals_).describe().round(2)
    count    244.00
    mean      -0.00
    std        1.01
    min       -2.93
    25%       -0.55
    50%       -0.09
    75%        0.51
    max        4.04
    dtype: float64

    5. No intercept and return only the regression coefficients

    Sometimes it may be useful to remove the constant term from the regression,
    or to only return the regression coefficients without calculating the
    standard errors or p-values. This latter can potentially save you a lot of
    time if you need to calculate hundreds of regression and only care about
    the coefficients!

    >>> pg.linear_regression(X, y, add_intercept=False, coef_only=True)
    array([0.1007119 , 0.36209717])

    6. Return a dictionnary instead of a dataframe

    >>> lm_dict = pg.linear_regression(X, y, as_dataframe=False)
    >>> lm_dict.keys()
    dict_keys(['names', 'coef', 'se', 'T', 'pval', 'r2', 'adj_r2', 'CI[2.5%]',
               'CI[97.5%]', 'df_model', 'df_resid', 'residuals', 'X', 'y',
               'pred'])

    7. Remove missing values

    >>> X[4, 1] = np.nan
    >>> y[7] = np.nan
    >>> pg.linear_regression(X, y, remove_na=True, coef_only=True)
    array([0.65749955, 0.09262059, 0.19927529])

    8. Get the relative importance of predictors

    >>> lm = pg.linear_regression(X, y, remove_na=True, relimp=True)
    >>> lm[['names', 'relimp', 'relimp_perc']]
           names    relimp  relimp_perc
    0  Intercept       NaN          NaN
    1         x1  0.342503    73.045583
    2         x2  0.126386    26.954417

    The ``relimp`` column is a partitioning of the total :math:`R^2` of the
    model into individual contribution. Therefore, it sums to the :math:`R^2`
    of the full model. The ``relimp_perc`` is normalized to sum to 100%. See
    `Groemping 2006 <https://www.jstatsoft.org/article/view/v017i01>`_
    for more details.

    >>> lm[['relimp', 'relimp_perc']].sum()
    relimp           0.468889
    relimp_perc    100.000000
    dtype: float64

    9. Weighted linear regression

    >>> X = [1, 2, 3, 4, 5, 6]
    >>> y = [10, 22, 11, 13, 13, 16]
    >>> w = [1, 0.1, 1, 1, 0.5, 1]  # Array of weights. Must be >= 0.
    >>> lm = pg.linear_regression(X, y, weights=w)
    >>> lm.round(2)
           names  coef    se     T  pval    r2  adj_r2  CI[2.5%]  CI[97.5%]
    0  Intercept  9.00  2.03  4.42  0.01  0.51    0.39      3.35      14.64
    1         x1  1.04  0.50  2.06  0.11  0.51    0.39     -0.36       2.44
    """"""
    if isinstance(X, pd.DataFrame):
        names = X.keys().tolist()
    elif isinstance(X, pd.Series):
        names = [X.name]
    else:
        names = []
    X = np.asarray(X)
    y = np.asarray(y)
    assert y.ndim == 1, 'y must be one-dimensional.'
    assert 0 < alpha < 1
    if X.ndim == 1:
        X = X[..., np.newaxis]
    if remove_na:
        (X, y) = rm_na(X, y[..., np.newaxis], paired=True, axis='rows')
        y = np.squeeze(y)
    y_gd = np.isfinite(y).all()
    X_gd = np.isfinite(X).all()
    assert y_gd, 'Target (y) contains NaN or Inf. Please remove them manually or use remove_na=True.'
    assert X_gd, 'Predictors (X) contain NaN or Inf. Please remove them manually or use remove_na=True.'
    assert y.shape[0] == X.shape[0], 'X and y must have same number of samples'
    if not names:
        names = ['x' + str(i + 1) for i in range(X.shape[1])]
    if add_intercept:
        X = np.column_stack((np.ones(X.shape[0]), X))
        names.insert(0, 'Intercept')
    n_nonzero = np.count_nonzero(X, axis=0)
    idx_zero = np.flatnonzero(n_nonzero == 0)
    if len(idx_zero):
        X = np.delete(X, idx_zero, 1)
        names = np.delete(names, idx_zero)
    idx_unique = np.where(np.all(X == X[0, :], axis=0))[0]
    if len(idx_unique) > 1:
        X = np.delete(X, idx_unique[1:], 1)
        names = np.delete(names, idx_unique[1:])
    constant = 1 if len(idx_unique) > 0 else 0
    if X.shape[1] > 1:
        idx_duplicate = []
        for pair in itertools.combinations(range(X.shape[1]), 2):
            if np.array_equal(X[:, pair[0]], X[:, pair[1]]):
                idx_duplicate.append(pair[1])
        if len(idx_duplicate):
            X = np.delete(X, idx_duplicate, 1)
            names = np.delete(names, idx_duplicate)
    (n, p) = (X.shape[0], X.shape[1])
    assert n >= 3, 'At least three valid samples are required in X.'
    assert p >= 1, 'X must have at least one valid column.'
    if weights is not None:
        if relimp:
            raise ValueError('relimp = True is not supported when using weights.')
        w = np.asarray(weights)
        assert w.ndim == 1, 'weights must be a 1D array.'
        assert w.size == n, 'weights must be of shape n_samples.'
        assert not np.isnan(w).any(), 'Missing weights are not accepted.'
        assert not (w < 0).any(), 'Negative weights are not accepted.'
        n = np.count_nonzero(w)
        wts = np.diag(np.sqrt(w))
        Xw = wts @ X
        yw = wts @ y
    else:
        w = np.ones(n)
        Xw = X
        yw = y
    (coef, ss_res, rank, _) = lstsq(Xw, yw, cond=None)
    ss_res = ss_res[0] if ss_res.shape == (1,) else ss_res
    if coef_only:
        return coef
    calc_ss_res = False
    if rank < Xw.shape[1]:
        warnings.warn(f'Design matrix supplied with `X` parameter is rank deficient (rank {rank} with {Xw.shape[1]} columns). That means that one or more of the columns in `X` are a linear combination of one of more of the other columns.')
        calc_ss_res = True
    df_model = rank - constant
    df_resid = n - rank
    pred = Xw @ coef
    resid = yw - pred
    if calc_ss_res:
        ss_res = (resid ** 2).sum()
    ss_tot = yw @ yw
    ss_wtot = np.sum(w * (y - np.average(y, weights=w)) ** 2)
    if constant:
        r2 = 1 - ss_res / ss_wtot
    else:
        r2 = 1 - ss_res / ss_tot
    adj_r2 = 1 - (1 - r2) * (n - constant) / df_resid
    mse = ss_res / df_resid
    beta_var = mse * np.linalg.pinv(Xw.T @ Xw).diagonal()
    beta_se = np.sqrt(beta_var)
    T = coef / beta_se
    pval = 2 * t.sf(np.fabs(T), df_resid)
    crit = t.ppf(1 - alpha / 2, df_resid)
    marg_error = crit * beta_se
    ll = coef - marg_error
    ul = coef + marg_error
    ll_name = 'CI[%.1f%%]' % (100 * alpha / 2)
    ul_name = 'CI[%.1f%%]' % (100 * (1 - alpha / 2))
    stats = {'names': names, 'coef': coef, 'se': beta_se, 'T': T, 'pval': pval, 'r2': r2, 'adj_r2': adj_r2, ll_name: ll, ul_name: ul}
    if relimp:
        data = pd.concat([pd.DataFrame(y, columns=['y']), pd.DataFrame(X, columns=names)], sort=False, axis=1)
        if 'Intercept' in names:
            reli = _relimp(data.drop(columns=['Intercept']).cov())
            reli['names'] = ['Intercept'] + reli['names']
            reli['relimp'] = np.insert(reli['relimp'], 0, np.nan)
            reli['relimp_perc'] = np.insert(reli['relimp_perc'], 0, np.nan)
        else:
            reli = _relimp(data.cov())
        stats.update(reli)
    if as_dataframe:
        stats = _postprocess_dataframe(pd.DataFrame(stats))
        stats.df_model_ = df_model
        stats.df_resid_ = df_resid
        stats.residuals_ = 0
        stats.residuals_ = resid
    else:
        stats['df_model'] = df_model
        stats['df_resid'] = df_resid
        stats['residuals'] = resid
        stats['X'] = X
        stats['y'] = y
        stats['pred'] = pred
        if weights is not None:
            stats['yw'] = yw
            stats['Xw'] = Xw
    return stats","for pair in itertools.combinations(range(X.shape[1]), 2):
    if np.array_equal(X[:, pair[0]], X[:, pair[1]]):
        idx_duplicate.append(pair[1])","idx_duplicate = [pair[1] for pair in itertools.combinations(range(X.shape[1]), 2) if np.array_equal(X[:, pair[0]], X[:, pair[1]])]",0,,,,,,,,,,,
pixelsort,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/pixelsort/pixelsort/sorter.py,https://github.com/satyarth/pixelsort/tree/master/pixelsort/sorter.py,,sort_image$4,"def sort_image(size, image_data, mask_data, intervals, randomness, sorting_function):
    sorted_pixels = []
    for y in range(size[1]):
        row = []
        x_min = 0
        for x_max in intervals[y] + [size[0]]:
            interval = []
            for x in range(x_min, x_max):
                if mask_data[x, y]:
                    interval.append(image_data[x, y])
            if random.random() < randomness / 100:
                row += interval
            else:
                row += sort_interval(interval, sorting_function)
            x_min = x_max
        sorted_pixels.append(row)
    return sorted_pixels","for x in range(x_min, x_max):
    if mask_data[x, y]:
        interval.append(image_data[x, y])","interval = [image_data[x, y] for x in range(x_min, x_max) if mask_data[x, y]]",0,,,,,,,,,,,
Minecraft-Overviewer,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Minecraft-Overviewer/overviewer.py,https://github.com/overviewer/Minecraft-Overviewer/tree/master//overviewer.py,,main$52,"def main():
    logger.configure()
    if os.name == 'posix':
        if os.geteuid() == 0:
            logging.warning('You are running Overviewer as root. It is recommended that you never do this, as it is dangerous for your system. If you are running into permission errors, fix your file/directory permissions instead. Overviewer does not need access to critical system resources and therefore does not require root access.')
        try:
            with open('/etc/redhat-release', 'r') as release_f:
                rel_contents = release_f.read()
                try:
                    major_rel = re.search('\\d(\\.\\d+)?', rel_contents).group(0).split('.')[0]
                    if major_rel == '6':
                        logging.warning('We will be dropping support for this release of your distribution soon. Please upgrade as soon as possible, or you will not receive future Overviewer updates.')
                except AttributeError:
                    pass
        except IOError:
            pass
    try:
        cpus = multiprocessing.cpu_count()
    except NotImplementedError:
        cpus = 1
    avail_north_dirs = ['lower-left', 'upper-left', 'upper-right', 'lower-right', 'auto']
    parser = ArgumentParser(usage=helptext)
    parser.add_argument('-c', '--config', dest='config', action='store', help='Specify the config file to use.')
    parser.add_argument('-p', '--processes', dest='procs', action='store', type=int, help='The number of local worker processes to spawn. Defaults to the number of CPU cores your computer has.')
    parser.add_argument('--pid', dest='pid', action='store', help='Specify the pid file to use.')
    parser.add_argument('--rendermodes', dest='rendermodes', action='store', help=""If you're not using a config file, specify which rendermodes to render with this option. This is a comma-separated list."")
    parser.add_argument('world', nargs='?', help='Path or name of the world you want to render.')
    parser.add_argument('output', nargs='?', help='Output directory for the rendered map.')
    render_modifiers = parser.add_mutually_exclusive_group()
    render_modifiers.add_argument('--forcerender', dest='forcerender', action='store_true', help='Force re-render the entire map.')
    render_modifiers.add_argument('--check-tiles', dest='checktiles', action='store_true', help='Check each tile on disk and re-render old tiles.')
    render_modifiers.add_argument('--no-tile-checks', dest='notilechecks', action='store_true', help='Only render tiles that come from chunks that have changed since the last render (the default).')
    parser.add_argument('--check-terrain', dest='check_terrain', action='store_true', help='Try to locate the texture files. Useful for debugging texture problems.')
    parser.add_argument('-V', '--version', dest='version', help='Display version information and then exits.', action='store_true')
    parser.add_argument('--check-version', dest='checkversion', help='Fetch information about the latest version of Overviewer.', action='store_true')
    parser.add_argument('--update-web-assets', dest='update_web_assets', action='store_true', help='Update web assets. Will *not* render tiles or update overviewerConfig.js.')
    parser.add_argument('-q', '--quiet', dest='quiet', action='count', default=0, help='Print less output. You can specify this option multiple times.')
    parser.add_argument('-v', '--verbose', dest='verbose', action='count', default=0, help='Print more output. You can specify this option multiple times.')
    parser.add_argument('--simple-output', dest='simple', action='store_true', default=False, help='Use a simple output format, with no colors or progress bars.')
    exegroup = parser.add_argument_group('Other Scripts', 'These scripts may accept different arguments than the ones listed above.')
    exegroup.add_argument('--genpoi', dest='genpoi', action='store_true', help='Run the genPOI script.')
    exegroup.add_argument('--skip-scan', dest='skipscan', action='store_true', help=""When running GenPOI, don't scan for entities."")
    exegroup.add_argument('--skip-players', dest='skipplayers', action='store_true', help=""When running GenPOI, don't scan player data."")
    (args, unknowns) = parser.parse_known_args()
    if len(unknowns) > 0 and args.world and args.output:
        possible_mistakes = []
        for i in range(len(unknowns) + 1):
            possible_mistakes.append(' '.join([args.world, args.output] + unknowns[:i]))
            possible_mistakes.append(' '.join([args.output] + unknowns[:i]))
        for mistake in possible_mistakes:
            if os.path.exists(mistake):
                logging.warning('Looks like you tried to make me use {0} as an argument, but forgot to quote the argument correctly. Try using ""{0}"" instead if the spaces are part of the path.'.format(mistake))
                parser.error('Too many arguments.')
        parser.error('Too many arguments.')
    if args.genpoi:
        sys.argv.remove('--genpoi')
        g = __import__('overviewer_core.aux_files', {}, {}, ['genPOI'])
        g.genPOI.main()
        return 0
    logger.configure(logging.INFO + 10 * args.quiet - 10 * args.verbose, verbose=args.verbose > 0, simple=args.simple)
    if args.version:
        print('Minecraft Overviewer %s' % util.findGitVersion() + ' (%s)' % util.findGitHash()[:7])
        try:
            import overviewer_core.overviewer_version as overviewer_version
            print('built on %s' % overviewer_version.BUILD_DATE)
            if args.verbose > 0:
                print('Build machine: %s %s' % (overviewer_version.BUILD_PLATFORM, overviewer_version.BUILD_OS))
                print('Read version information from %r' % overviewer_version.__file__)
        except ImportError:
            print('(build info not found)')
        if args.verbose > 0:
            print('Python executable: %r' % sys.executable)
            print(sys.version)
        if not args.checkversion:
            return 0
    if args.checkversion:
        print('Currently running Minecraft Overviewer %s' % util.findGitVersion() + ' (%s)' % util.findGitHash()[:7])
        try:
            from urllib import request
            import json
            latest_ver = json.loads(request.urlopen('http://overviewer.org/download.json').read())['src']
            print('Latest version of Minecraft Overviewer %s (%s)' % (latest_ver['version'], latest_ver['commit'][:7]))
            print('See https://overviewer.org/downloads for more information.')
        except Exception:
            print('Failed to fetch latest version info.')
            if args.verbose > 0:
                import traceback
                traceback.print_exc()
            else:
                print('Re-run with --verbose for more details.')
            return 1
        return 0
    if args.pid:
        if os.path.exists(args.pid):
            try:
                with open(args.pid, 'r') as fpid:
                    pid = int(fpid.read())
                    if util.pid_exists(pid):
                        print('Overviewer is already running (pid exists) - exiting.')
                        return 0
            except (IOError, ValueError):
                pass
        with open(args.pid, 'w') as f:
            f.write(str(os.getpid()))
    if args.check_terrain and (not args.config):
        import hashlib
        from overviewer_core.textures import Textures
        tex = Textures()
        logging.info('Looking for a few common texture files...')
        try:
            f = tex.find_file('assets/minecraft/textures/block/sandstone_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/grass_block_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/diamond_ore.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/acacia_planks.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/ancient_debris_top.png', verbose=True)
        except IOError:
            logging.error('Could not find any texture files.')
            return 1
        return 0
    if not (args.world and args.output) and (not args.config):
        if util.is_bare_console():
            print('\n')
            print('The Overviewer is a console program.  Please open a Windows command prompt')
            print('first and run Overviewer from there.   Further documentation is available at')
            print('http://docs.overviewer.org/\n')
            print('\n')
            print('For a quick-start guide on Windows, visit the following URL:\n')
            print('http://docs.overviewer.org/en/latest/win_tut/windowsguide/\n')
        else:
            logging.error('You must either specify --config or give me a world directory and output directory.')
            parser.print_help()
            list_worlds()
        return 1
    if args.config and (args.world and args.output):
        print()
        print('If you specify --config, you need to specify the world to render as well as the destination in the config file, not on the command line.')
        print('Put something like this in your config file:')
        print(""worlds['myworld'] = %r"" % args[0])
        print('outputdir = %r' % (args[1] if len(args) > 1 else '/path/to/output'))
        print()
        logging.error('You cannot specify both --config AND a world + output directory on the command line.')
        parser.print_help()
        return 1
    if not args.config and (args.world or args.output) and (not (args.world and args.output)):
        logging.error('You must specify both the world directory and an output directory')
        parser.print_help()
        return 1
    mw_parser = config_parser.MultiWorldParser()
    if not args.config:
        (worldpath, destdir) = map(os.path.expanduser, [args.world, args.output])
        logging.debug('Using %r as the world directory', worldpath)
        logging.debug('Using %r as the output directory', destdir)
        mw_parser.set_config_item('worlds', {'world': worldpath})
        mw_parser.set_config_item('outputdir', destdir)
        rendermodes = ['lighting']
        if args.rendermodes:
            rendermodes = args.rendermodes.replace('-', '_').split(',')
        renders = OrderedDict()
        for rm in rendermodes:
            renders['world-' + rm] = {'world': 'world', 'title': 'Overviewer Render (%s)' % rm, 'rendermode': rm}
        mw_parser.set_config_item('renders', renders)
    else:
        if args.rendermodes:
            logging.error('You cannot specify --rendermodes if you give a config file. Configure your rendermodes in the config file instead.')
            parser.print_help()
            return 1
        try:
            mw_parser.parse(os.path.expanduser(args.config))
        except config_parser.MissingConfigException as e:
            logging.error(str(e))
            util.nice_exit(1)
    if args.procs:
        mw_parser.set_config_item('processes', args.procs)
    try:
        config = mw_parser.get_validated_config()
    except Exception as ex:
        if args.verbose:
            logging.exception('An error was encountered with your configuration. See the information below.')
        else:
            logging.error('An error was encountered with your configuration.')
            logging.error(str(ex))
        return 1
    if args.check_terrain:
        logging.info('Looking for a few common texture files...')
        for (render_name, render) in config['renders'].items():
            logging.info('Looking at render %r.', render_name)
            texopts = util.dict_subset(render, ['texturepath'])
            tex = textures.Textures(**texopts)
            f = tex.find_file('assets/minecraft/textures/block/sandstone_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/grass_block_top.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/diamond_ore.png', verbose=True)
            f = tex.find_file('assets/minecraft/textures/block/oak_planks.png', verbose=True)
        return 0
    logging.info('Welcome to Minecraft Overviewer version %s (%s)!' % (util.findGitVersion(), util.findGitHash()[:7]))
    logging.debug('Current log level: {0}.'.format(logging.getLogger().level))

    def set_renderchecks(checkname, num):
        for (name, render) in config['renders'].items():
            if render.get('renderchecks', 0) == 3:
                logging.warning(checkname + ' ignoring render ' + repr(name) + ' since it\'s marked as ""don\'t render"".')
            else:
                render['renderchecks'] = num
    if args.forcerender:
        logging.info('Forcerender mode activated. ALL tiles will be rendered.')
        set_renderchecks('forcerender', 2)
    elif args.checktiles:
        logging.info('Checking all tiles for updates manually.')
        set_renderchecks('checktiles', 1)
    elif args.notilechecks:
        logging.info('Disabling all tile mtime checks. Only rendering tiles that need updating since last render.')
        set_renderchecks('notilechecks', 0)
    if not config['renders']:
        logging.error(""You must specify at least one render in your config file. Check the documentation at http://docs.overviewer.org if you're having trouble."")
        return 1
    for (rname, render) in config['renders'].items():
        try:
            worldpath = config['worlds'][render['world']]
        except KeyError:
            logging.error(""Render %s's world is '%s', but I could not find a corresponding entry in the worlds dictionary."", rname, render['world'])
            return 1
        render['worldname_orig'] = render['world']
        render['world'] = worldpath
        if render.get('forcerender', False):
            render['renderchecks'] = 2
        if render.get('overlay', []) != []:
            for x in render.get('overlay'):
                if x != rname:
                    try:
                        renderLink = config['renders'][x]
                    except KeyError:
                        logging.error(""Render %s's overlay is '%s', but I could not find a corresponding entry in the renders dictionary."", rname, x)
                        return 1
                else:
                    logging.error(""Render %s's overlay contains itself."", rname)
                    return 1
    destdir = config['outputdir']
    if not destdir:
        logging.error('You must specify the output directory in your config file.')
        logging.error(""e.g. outputdir = '/path/to/outputdir'"")
        return 1
    if not os.path.exists(destdir):
        try:
            os.mkdir(destdir)
        except OSError:
            logging.exception('Could not create the output directory.')
            return 1
    assetMrg = assetmanager.AssetManager(destdir, config.get('customwebassets', None))
    if args.update_web_assets:
        assetMrg.output_noconfig()
        logging.info('Web assets have been updated.')
        return 0
    changelists = {}
    for render in config['renders'].values():
        if 'changelist' in render:
            path = render['changelist']
            if path not in changelists:
                out = open(path, 'w')
                logging.debug('Opening changelist %s (%s).', out, out.fileno())
                changelists[path] = out
            else:
                out = changelists[path]
            render['changelist'] = out.fileno()
    tilesets = []
    worldcache = {}
    texcache = {}
    caches = []
    caches.append(cache.LRUCache(size=100))
    renders = config['renders']
    for (render_name, render) in renders.items():
        logging.debug('Found the following render thing: %r', render)
        try:
            w = worldcache[render['world']]
        except KeyError:
            try:
                w = world.World(render['world'])
            except CorruptNBTError as e:
                logging.error('Failed to open world %r.', render['world'])
                raise e
            except world.UnsupportedVersion as e:
                for ln in str(e).split('\n'):
                    logging.error(ln)
                sys.exit(1)
            worldcache[render['world']] = w
        texopts = util.dict_subset(render, ['texturepath', 'bgcolor', 'northdirection'])
        texopts_key = tuple(texopts.items())
        if texopts_key not in texcache:
            tex = textures.Textures(**texopts)
            logging.info('Generating textures...')
            tex.generate()
            logging.debug('Finished generating textures.')
            texcache[texopts_key] = tex
        else:
            tex = texcache[texopts_key]
        try:
            logging.debug('Asking for regionset %r.' % render['dimension'][1])
            rset = w.get_regionset(render['dimension'][1])
        except IndexError:
            logging.error(""Sorry, I can't find anything to render!  Are you sure there are .mca files in the world directory of %s?"" % render['world'])
            return 1
        if rset is None:
            logging.warning(""Sorry, you requested dimension '%s' for %s, but I couldn't find it."", render['dimension'][0], render_name)
            continue
        rset = world.CachedRegionSet(rset, caches)
        if 'crop' in render:
            rsets = []
            for zone in render['crop']:
                rsets.append(world.CroppedRegionSet(rset, *zone))
        else:
            rsets = [rset]
        if render['northdirection'] > 0:
            newrsets = []
            for r in rsets:
                r = world.RotatedRegionSet(r, render['northdirection'])
                newrsets.append(r)
            rsets = newrsets
        tileset_dir = os.path.abspath(os.path.join(destdir, render_name))
        render['name'] = render_name
        tileSetOpts = util.dict_subset(render, ['name', 'imgformat', 'renderchecks', 'rerenderprob', 'bgcolor', 'defaultzoom', 'imgquality', 'imglossless', 'optimizeimg', 'rendermode', 'worldname_orig', 'title', 'dimension', 'changelist', 'showspawn', 'overlay', 'base', 'poititle', 'maxzoom', 'showlocationmarker', 'minzoom', 'center'])
        tileSetOpts.update({'spawn': w.find_true_spawn()})
        for rset in rsets:
            tset = tileset.TileSet(w, rset, assetMrg, tex, tileSetOpts, tileset_dir)
            tilesets.append(tset)
    if not tilesets:
        logging.error(""There are no tilesets to render! There's nothing to do, so exiting."")
        return 1
    logging.info('Preprocessing...')
    for ts in tilesets:
        ts.do_preprocessing()
    assetMrg.initialize(tilesets)
    if config['processes'] == 1:
        dispatch = dispatcher.Dispatcher()
    else:
        dispatch = dispatcher.MultiprocessingDispatcher(local_procs=config['processes'])
    dispatch.render_all(tilesets, config['observer'])
    dispatch.close()
    assetMrg.finalize(tilesets)
    for out in changelists.values():
        logging.debug('Closing %s (%s).', out, out.fileno())
        out.close()
    if config['processes'] == 1:
        logging.debug('Final cache stats:')
        for c in caches:
            logging.debug('\t%s: %s hits, %s misses', c.__class__.__name__, c.hits, c.misses)
    if args.pid:
        os.remove(args.pid)
    logging.info(""Your render has been written to '%s', open index.html to view it."" % destdir)
    return 0","for zone in render['crop']:
    rsets.append(world.CroppedRegionSet(rset, *zone))","rsets = [world.CroppedRegionSet(rset, *zone) for zone in render['crop']]",0,,,,,,,,,,,
investpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/investpy/investpy/currency_crosses.py,https://github.com/alvarobartt/investpy/tree/master/investpy/currency_crosses.py,,get_currency_cross_recent_data$187,"def get_currency_cross_recent_data(currency_cross, as_json=False, order='ascending', interval='Daily'):
    """"""
    This function retrieves recent historical data from the introduced `currency_cross` as indexed in Investing.com
    via Web Scraping. The resulting data can it either be stored in a :obj:`pandas.DataFrame` or in a
    :obj:`json` file, with `ascending` or `descending` order.

    Args:
        currency_cross (:obj:`str`): name of the currency_cross to retrieve recent historical data from.
        as_json (:obj:`bool`, optional):
            optional argument to determine the format of the output data (:obj:`pandas.DataFrame` or :obj:`json`).
        order (:obj:`str`, optional):
            optional argument to define the order of the retrieved data (`ascending`, `asc` or `descending`, `desc`).
        interval (:obj:`str`, optional):
            value to define the historical data interval to retrieve, by default `Daily`, but it can also be `Weekly` or `Monthly`.

    Returns:
        :obj:`pandas.DataFrame` or :obj:`json`:
            The function returns a either a :obj:`pandas.DataFrame` or a :obj:`json` file containing the retrieved
            recent data from the specified currency_cross via argument. The dataset contains the open, high, low, close,
            volume and currency values for the selected currency_cross on market days.

            The return data is in case we use default arguments will look like::

                Date || Open | High | Low | Close | Currency
                -----||------|------|-----|-------|---------
                xxxx || xxxx | xxxx | xxx | xxxxx | xxxxxxxx

            but if we define `as_json=True`, then the output will be::

                {
                    name: name,
                    recent: [
                        dd/mm/yyyy: {
                            'open': x,
                            'high': x,
                            'low': x,
                            'close': x,
                            'currency' : x
                        },
                        ...
                    ]
                }

    Raises:
        ValueError: raised if any of the introduced arguments was not valid or errored.
        IOError: raised if currency_crosses object/file not found or unable to retrieve.
        RuntimeError: raised introduced currency_cross does not match any of the indexed ones.
        ConnectionError: raised if GET request did not return 200 status code.
        IndexError: raised if currency_cross information was unavailable or not found.

    Examples:
        >>> data = investpy.get_currency_cross_recent_data(currency_cross='EUR/USD')
        >>> data.head()
                      Open    High     Low   Close Currency
        Date
        2019-08-27  1.1101  1.1116  1.1084  1.1091      USD
        2019-08-28  1.1090  1.1099  1.1072  1.1078      USD
        2019-08-29  1.1078  1.1093  1.1042  1.1057      USD
        2019-08-30  1.1058  1.1062  1.0963  1.0991      USD
        2019-09-02  1.0990  1.1000  1.0958  1.0968      USD

    """"""
    if not currency_cross:
        raise ValueError('ERR#0052: currency_cross param is mandatory and should be a str.')
    if not isinstance(currency_cross, str):
        raise ValueError('ERR#0052: currency_cross param is mandatory and should be a str.')
    if not isinstance(as_json, bool):
        raise ValueError('ERR#0002: as_json argument can just be True or False, bool type.')
    if order not in ['ascending', 'asc', 'descending', 'desc']:
        raise ValueError('ERR#0003: order argument can just be ascending (asc) or descending (desc), str type.')
    if not interval:
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    if not isinstance(interval, str):
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    interval = interval.lower()
    if interval not in ['daily', 'weekly', 'monthly']:
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    resource_package = 'investpy'
    resource_path = '/'.join(('resources', 'currency_crosses.csv'))
    if pkg_resources.resource_exists(resource_package, resource_path):
        currency_crosses = pd.read_csv(pkg_resources.resource_filename(resource_package, resource_path), keep_default_na=False)
    else:
        raise FileNotFoundError('ERR#0060: currency_crosses file not found or errored.')
    if currency_crosses is None:
        raise IOError('ERR#0050: currency_crosses not found or unable to retrieve.')
    currency_cross = unidecode(currency_cross.strip().lower())
    if currency_cross not in list(currency_crosses['name'].apply(unidecode).str.lower()):
        raise RuntimeError('ERR#0054: the introduced currency_cross ' + str(currency_cross) + ' does not exist.')
    id_ = currency_crosses.loc[(currency_crosses['name'].apply(unidecode).str.lower() == currency_cross).idxmax(), 'id']
    name = currency_crosses.loc[(currency_crosses['name'].apply(unidecode).str.lower() == currency_cross).idxmax(), 'name']
    currency = currency_crosses.loc[(currency_crosses['name'].apply(unidecode).str.lower() == currency_cross).idxmax(), 'second']
    header = name + ' Historical Data'
    params = {'curr_id': id_, 'smlID': str(randint(1000000, 99999999)), 'header': header, 'interval_sec': interval.capitalize(), 'sort_col': 'date', 'sort_ord': 'DESC', 'action': 'historical_data'}
    head = {'User-Agent': random_user_agent(), 'X-Requested-With': 'XMLHttpRequest', 'Accept': 'text/html', 'Accept-Encoding': 'gzip, deflate', 'Connection': 'keep-alive'}
    url = 'https://www.investing.com/instruments/HistoricalDataAjax'
    req = requests.post(url, headers=head, data=params)
    if req.status_code != 200:
        raise ConnectionError('ERR#0015: error ' + str(req.status_code) + ', try again later.')
    root_ = fromstring(req.text)
    path_ = root_.xpath("".//table[@id='curr_table']/tbody/tr"")
    result = list()
    if path_:
        for elements_ in path_:
            if elements_.xpath('.//td')[0].text_content() == 'No results found':
                raise IndexError('ERR#0055: currency_cross information unavailable or not found.')
            info = []
            for nested_ in elements_.xpath('.//td'):
                info.append(nested_.get('data-real-value'))
            currency_cross_date = datetime.strptime(str(datetime.fromtimestamp(int(info[0]), tz=pytz.timezone('GMT')).date()), '%Y-%m-%d')
            currency_cross_close = float(info[1].replace(',', ''))
            currency_cross_open = float(info[2].replace(',', ''))
            currency_cross_high = float(info[3].replace(',', ''))
            currency_cross_low = float(info[4].replace(',', ''))
            result.insert(len(result), Data(currency_cross_date, currency_cross_open, currency_cross_high, currency_cross_low, currency_cross_close, None, currency, None))
        if order in ['ascending', 'asc']:
            result = result[::-1]
        elif order in ['descending', 'desc']:
            result = result
        if as_json is True:
            json_ = {'name': name, 'recent': [value.currency_cross_as_json() for value in result]}
            return json.dumps(json_, sort_keys=False)
        elif as_json is False:
            df = pd.DataFrame.from_records([value.currency_cross_to_dict() for value in result])
            df.set_index('Date', inplace=True)
            return df
    else:
        raise RuntimeError('ERR#0004: data retrieval error while scraping.')","for nested_ in elements_.xpath('.//td'):
    info.append(nested_.get('data-real-value'))",info = [nested_.get('data-real-value') for nested_ in elements_.xpath('.//td')],0,,,,,,,,,,,
stacker,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/stacker/stacker/lookups/handlers/ami.py,https://github.com/cloudtools/stacker/tree/master/stacker/lookups/handlers/ami.py,AmiLookup,handle$25,"def handle(cls, value, provider, **kwargs):
    """"""Fetch the most recent AMI Id using a filter
    
        For example:
    
            ${ami [<region>@]owners:self,account,amazon name_regex:serverX-[0-9]+ architecture:x64,i386}
    
            The above fetches the most recent AMI where owner is self
            account or amazon and the ami name matches the regex described,
            the architecture will be either x64 or i386
    
            You can also optionally specify the region in which to perform the
            AMI lookup.
    
            Valid arguments:
    
            owners (comma delimited) REQUIRED ONCE:
                aws_account_id | amazon | self
    
            name_regex (a regex) REQUIRED ONCE:
                e.g. my-ubuntu-server-[0-9]+
    
            executable_users (comma delimited) OPTIONAL ONCE:
                aws_account_id | amazon | self
    
            Any other arguments specified are sent as filters to the aws api
            For example, ""architecture:x86_64"" will add a filter
        """"""
    value = read_value_from_path(value)
    if '@' in value:
        (region, value) = value.split('@', 1)
    else:
        region = provider.region
    ec2 = get_session(region).client('ec2')
    values = {}
    describe_args = {}
    matches = re.findall('([0-9a-zA-z_-]+:[^\\s$]+)', value)
    for match in matches:
        (k, v) = match.split(':', 1)
        values[k] = v
    if not values.get('owners'):
        raise Exception(""'owners' value required when using ami"")
    owners = values.pop('owners').split(',')
    describe_args['Owners'] = owners
    if not values.get('name_regex'):
        raise Exception(""'name_regex' value required when using ami"")
    name_regex = values.pop('name_regex')
    executable_users = None
    if values.get('executable_users'):
        executable_users = values.pop('executable_users').split(',')
        describe_args['ExecutableUsers'] = executable_users
    filters = []
    for (k, v) in values.items():
        filters.append({'Name': k, 'Values': v.split(',')})
    describe_args['Filters'] = filters
    result = ec2.describe_images(**describe_args)
    images = sorted(result['Images'], key=operator.itemgetter('CreationDate'), reverse=True)
    for image in images:
        if re.match('^%s$' % name_regex, image.get('Name', '')):
            return image['ImageId']
    raise ImageNotFound(value)","for (k, v) in values.items():
    filters.append({'Name': k, 'Values': v.split(',')})","filters = [{'Name': k, 'Values': v.split(',')} for (k, v) in values.items()]",0,,,,,,,,,,,
uncertainty-baselines,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/uncertainty-baselines/baselines/jft/experiments/imagenet21k_vit_base16_finetune_cifar100.py,https://github.com/google/uncertainty-baselines/tree/master/baselines/jft/experiments/imagenet21k_vit_base16_finetune_cifar100.py,,get_config$29,"def get_config():
    """"""Config for training a patch-transformer on JFT.""""""
    config = ml_collections.ConfigDict()
    config.dataset = 'cifar100'
    config.val_split = 'train[98%:]'
    config.train_split = 'train[:98%]'
    config.num_classes = 100
    BATCH_SIZE = 512
    config.batch_size = BATCH_SIZE
    config.total_steps = 10000
    INPUT_RES = 384
    pp_common = '|value_range(-1, 1)'
    pp_common += f'|onehot({config.num_classes}, key=""label"", key_result=""labels"")'
    pp_common += '|keep([""image"", ""labels""])'
    config.pp_train = f'decode|inception_crop({INPUT_RES})|flip_lr' + pp_common
    config.pp_eval = f'decode|resize({INPUT_RES})' + pp_common
    config.ood_datasets = ['cifar10', 'svhn_cropped']
    config.ood_num_classes = [10, 10]
    config.ood_split = 'test'
    config.ood_methods = ['msp', 'entropy', 'maha', 'rmaha']
    pp_eval_ood = []
    for num_classes in config.ood_num_classes:
        if num_classes > config.num_classes:
            pp_eval_ood.append(config.pp_eval.replace(f'onehot({config.num_classes}', f'onehot({num_classes}'))
        else:
            pp_eval_ood.append(config.pp_eval)
    config.pp_eval_ood = pp_eval_ood
    config.shuffle_buffer_size = 50000
    config.log_training_steps = 10
    config.log_eval_steps = 100
    config.checkpoint_steps = 1000
    config.checkpoint_timeout = 1
    config.prefetch_to_device = 2
    config.trial = 0
    config.model_init = '/path/to/pretrained_model_ckpt.npz'
    config.model = ml_collections.ConfigDict()
    config.model.patches = ml_collections.ConfigDict()
    config.model.patches.size = [16, 16]
    config.model.hidden_size = 768
    config.model.transformer = ml_collections.ConfigDict()
    config.model.transformer.attention_dropout_rate = 0.0
    config.model.transformer.dropout_rate = 0.0
    config.model.transformer.mlp_dim = 3072
    config.model.transformer.num_heads = 12
    config.model.transformer.num_layers = 12
    config.model.classifier = 'token'
    config.model.representation_size = None
    config.optim_name = 'Momentum'
    config.optim = ml_collections.ConfigDict()
    config.grad_clip_norm = 1.0
    config.weight_decay = None
    config.loss = 'softmax_xent'
    config.lr = ml_collections.ConfigDict()
    config.lr.base = 0.01
    config.lr.warmup_steps = 500
    config.lr.decay_type = 'cosine'
    return config","for num_classes in config.ood_num_classes:
    if num_classes > config.num_classes:
        pp_eval_ood.append(config.pp_eval.replace(f'onehot({config.num_classes}', f'onehot({num_classes}'))
    else:
        pp_eval_ood.append(config.pp_eval)","pp_eval_ood = [config.pp_eval.replace(f'onehot({config.num_classes}', f'onehot({num_classes}') if num_classes > config.num_classes else config.pp_eval for num_classes in config.ood_num_classes]",0,,,,,,,,,,,
tf-encrypted,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/tf-encrypted/tf_encrypted/convert/convert.py,https://github.com/tf-encrypted/tf-encrypted/tree/master/tf_encrypted/convert/convert.py,Converter,forward_function$213,"def forward_function(x):
    node_outputs = {}
    for (index, input) in enumerate(model_proto.graph.input):
        node_outputs[input.name] = x[index]
    for node in nodes:
        inputs = []
        for input in node.input:
            if input in node_outputs.keys():
                inputs.append(node_outputs[input])
        with tf.name_scope(node.name + '/forward'):
            res = tfe_nodes[node.name].forward(inputs)
        for (i, output) in enumerate(node.output):
            node_outputs[output] = res[i]
    res = []
    for output in model_proto.graph.output:
        res.append(node_outputs[output.name])
    return res","for input in node.input:
    if input in node_outputs.keys():
        inputs.append(node_outputs[input])",inputs = [node_outputs[input] for input in node.input if input in node_outputs.keys()],0,,,,,,,,,,,
Wudao-dict,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/Wudao-dict/soup/parse_21c.py,https://github.com/ChestnutHeng/Wudao-dict/tree/master/soup/parse_21c.py,,get_text$31,"def get_text(word):
    content = get_html(word)
    word_struct = {'word': word}
    root = bs4.BeautifulSoup(content, 'lxml')
    pron = {}
    pron_fallback = False
    for pron_item in ss(root, '.pronounce'):
        pron_lang = None
        pron_phonetic = None
        for sub_item in pron_item.children:
            if isinstance(sub_item, str) and pron_lang is None:
                pron_lang = sub_item
                continue
            if isinstance(sub_item, bs4.Tag) and sub_item.name.lower() == 'span' and sub_item.has_attr('class') and ('phonetic' in sub_item.get('class')):
                pron_phonetic = sub_item
                continue
        if pron_phonetic is None:
            pron_fallback = True
            break
        if pron_lang is None:
            pron_lang = ''
        pron_lang = pron_lang.strip()
        pron_phonetic = pron_phonetic.text
        pron[pron_lang] = pron_phonetic
    if pron_fallback:
        for item in ss(root, '.phonetic'):
            if item.name.lower() == 'span':
                pron[''] = item.text
                break
    word_struct['pronunciation'] = pron
    if pron_fallback:
        d = root.select('.wordGroup.def')
        p = root.select('.wordGroup.pos')
        ds = ''
        dp = ''
        if len(d) > 0:
            ds = d[0].text.strip()
        if len(p) > 0:
            dp = p[0].text.strip()
        word_struct['paraphrase'] = (dp + ' ' + ds).strip()
    else:
        nodes = ss(root, '#phrsListTab .trans-container ul')
        basic_desc = []
        if len(nodes) != 0:
            ul = nodes[0]
            for li in ul.children:
                if not (isinstance(li, bs4.Tag) and li.name.lower() == 'li'):
                    continue
                basic_desc.append(li.text.strip())
        word_struct['paraphrase'] = basic_desc
    rank = ''
    nodes = ss(root, '.rank')
    if len(nodes) != 0:
        rank = nodes[0].text.strip()
    word_struct['rank'] = rank
    pattern = ''
    nodes = ss(root, '.collinsToggle .pattern')
    if len(nodes) != 0:
        pattern = multi_space_to_single(nodes[0].text.strip())
    word_struct['pattern'] = pattern
    word_struct['sentence'] = []
    for child in ss(root, '.collinsToggle .ol li'):
        p = ss(child, 'p')
        if len(p) == 0:
            continue
        p = p[0]
        desc = ''
        cx = ''
        for node in p.children:
            if isinstance(node, str):
                desc += node
            if isinstance(node, bs4.Tag) and node.name.lower() == 'span':
                cx = node.text
        desc = multi_space_to_single(desc.strip())
        examples = []
        for el in ss(child, '.exampleLists'):
            examp = []
            for p in ss(el, '.examples p'):
                examp.append(p.text.strip())
            examples.append(examp)
        word_struct['sentence'].append([desc, cx, examples])
    if not word_struct['sentence']:
        for v in root.select('#bilingual ul li'):
            p = ss(v, 'p')
            ll = []
            for p in ss(v, 'p'):
                if len(p) == 0:
                    continue
                if 'class' not in p.attrs:
                    ll.append(p.text.strip())
            if len(ll) != 0:
                word_struct['sentence'].append(ll)
    return word_struct","for p in ss(el, '.examples p'):
    examp.append(p.text.strip())","examp = [p.text.strip() for p in ss(el, '.examples p')]",0,,,,,,,,,,,
investpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/investpy/investpy/crypto.py,https://github.com/alvarobartt/investpy/tree/master/investpy/crypto.py,,get_crypto_recent_data$116,"def get_crypto_recent_data(crypto, as_json=False, order='ascending', interval='Daily'):
    """"""
    This function retrieves recent historical data from the introduced crypto from Investing.com. So on, the recent data
    of the introduced crypto will be retrieved and returned as a :obj:`pandas.DataFrame` if the parameters are valid
    and the request to Investing.com succeeds. Note that additionally some optional parameters can be specified: as_json
    and order, which let the user decide if the data is going to be returned as a :obj:`json` or not, and if the historical
    data is going to be ordered ascending or descending (where the index is the date), respectively.

    Args:
        crypto (:obj:`str`): name of the crypto currency to retrieve data from.
        as_json (:obj:`bool`, optional):
            to determine the format of the output data, either a :obj:`pandas.DataFrame` if False and a :obj:`json` if True.
        order (:obj:`str`, optional): to define the order of the retrieved data which can either be ascending or descending.
        interval (:obj:`str`, optional):
            value to define the historical data interval to retrieve, by default `Daily`, but it can also be `Weekly` or `Monthly`.

    Returns:
        :obj:`pandas.DataFrame` or :obj:`json`:
            The function can return either a :obj:`pandas.DataFrame` or a :obj:`json` object, containing the retrieved
            recent data of the specified crypto currency. So on, the resulting dataframe contains the open, high, low,
            close and volume values for the selected crypto on market days and the currency in which those values are presented.

            The resulting recent data, in case that the default parameters were applied, will look like::

                Date || Open | High | Low | Close | Volume | Currency
                -----||------|------|-----|-------|--------|----------
                xxxx || xxxx | xxxx | xxx | xxxxx | xxxxxx | xxxxxxxx

            but in case that as_json parameter was defined as True, then the output will be::

                {
                    name: name,
                    recent: [
                        {
                            date: 'dd/mm/yyyy',
                            open: x,
                            high: x,
                            low: x,
                            close: x,
                            volume: x,
                            currency: x
                        },
                        ...
                    ]
                }

    Raises:
        ValueError: raised whenever any of the introduced arguments is not valid or errored.
        IOError: raised if cryptos object/file was not found or unable to retrieve.
        RuntimeError: raised if the introduced crypto name was not found or did not match any of the existing ones.
        ConnectionError: raised if connection to Investing.com could not be established.
        IndexError: raised if crypto recent data was unavailable or not found in Investing.com.

    Examples:
        >>> data = investpy.get_crypto_recent_data(crypto='bitcoin')
        >>> data.head()
                      Open     High     Low   Close   Volume Currency
        Date
        2019-10-25  7422.8   8697.7  7404.9  8658.3  1177632      USD
        2019-10-26  8658.4  10540.0  8061.8  9230.6  1784005      USD
        2019-10-27  9230.6   9773.2  9081.0  9529.6  1155038      USD
        2019-10-28  9530.1   9866.9  9202.5  9207.2  1039295      USD
        2019-10-29  9206.5   9531.3  9125.3  9411.3   918477      USD

    """"""
    if not crypto:
        raise ValueError('ERR#0083: crypto parameter is mandatory and must be a valid crypto name.')
    if not isinstance(crypto, str):
        raise ValueError('ERR#0084: crypto argument needs to be a str.')
    if not isinstance(as_json, bool):
        raise ValueError('ERR#0002: as_json argument can just be True or False, bool type.')
    if order not in ['ascending', 'asc', 'descending', 'desc']:
        raise ValueError('ERR#0003: order argument can just be ascending (asc) or descending (desc), str type.')
    if not interval:
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    if not isinstance(interval, str):
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    interval = interval.lower()
    if interval not in ['daily', 'weekly', 'monthly']:
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    resource_package = 'investpy'
    resource_path = '/'.join(('resources', 'cryptos.csv'))
    if pkg_resources.resource_exists(resource_package, resource_path):
        cryptos = pd.read_csv(pkg_resources.resource_filename(resource_package, resource_path), keep_default_na=False)
    else:
        raise FileNotFoundError('ERR#0081: cryptos file not found or errored.')
    if cryptos is None:
        raise IOError('ERR#0082: cryptos not found or unable to retrieve.')
    crypto = unidecode(crypto.strip().lower())
    if crypto not in list(cryptos['name'].apply(unidecode).str.lower()):
        raise RuntimeError('ERR#0085: crypto currency: ' + crypto + ', not found, check if it is correct.')
    status = cryptos.loc[(cryptos['name'].apply(unidecode).str.lower() == crypto).idxmax(), 'status']
    if status == 'unavailable':
        raise ValueError('ERR#0086: the selected crypto currency is not available for retrieval in Investing.com.')
    crypto_name = cryptos.loc[(cryptos['name'].apply(unidecode).str.lower() == crypto).idxmax(), 'name']
    crypto_id = cryptos.loc[(cryptos['name'].apply(unidecode).str.lower() == crypto).idxmax(), 'id']
    crypto_currency = cryptos.loc[(cryptos['name'].apply(unidecode).str.lower() == crypto).idxmax(), 'currency']
    header = crypto_name + ' Historical Data'
    params = {'curr_id': crypto_id, 'smlID': str(randint(1000000, 99999999)), 'header': header, 'interval_sec': interval.capitalize(), 'sort_col': 'date', 'sort_ord': 'DESC', 'action': 'historical_data'}
    head = {'User-Agent': random_user_agent(), 'X-Requested-With': 'XMLHttpRequest', 'Accept': 'text/html', 'Accept-Encoding': 'gzip, deflate', 'Connection': 'keep-alive'}
    url = 'https://www.investing.com/instruments/HistoricalDataAjax'
    req = requests.post(url, headers=head, data=params)
    if req.status_code != 200:
        raise ConnectionError('ERR#0015: error ' + str(req.status_code) + ', try again later.')
    root_ = fromstring(req.text)
    path_ = root_.xpath("".//table[@id='curr_table']/tbody/tr"")
    result = list()
    if path_:
        for elements_ in path_:
            if elements_.xpath('.//td')[0].text_content() == 'No results found':
                raise IndexError('ERR#0087: crypto information unavailable or not found.')
            info = []
            for nested_ in elements_.xpath('.//td'):
                info.append(nested_.get('data-real-value'))
            crypto_date = datetime.strptime(str(datetime.fromtimestamp(int(info[0]), tz=pytz.timezone('GMT')).date()), '%Y-%m-%d')
            crypto_close = float(info[1].replace(',', ''))
            crypto_open = float(info[2].replace(',', ''))
            crypto_high = float(info[3].replace(',', ''))
            crypto_low = float(info[4].replace(',', ''))
            crypto_volume = int(info[5])
            result.insert(len(result), Data(crypto_date, crypto_open, crypto_high, crypto_low, crypto_close, crypto_volume, crypto_currency, None))
        if order in ['ascending', 'asc']:
            result = result[::-1]
        elif order in ['descending', 'desc']:
            result = result
        if as_json is True:
            json_ = {'name': crypto_name, 'recent': [value.crypto_as_json() for value in result]}
            return json.dumps(json_, sort_keys=False)
        elif as_json is False:
            df = pd.DataFrame.from_records([value.crypto_to_dict() for value in result])
            df.set_index('Date', inplace=True)
            return df
    else:
        raise RuntimeError('ERR#0004: data retrieval error while scraping.')","for nested_ in elements_.xpath('.//td'):
    info.append(nested_.get('data-real-value'))",info = [nested_.get('data-real-value') for nested_ in elements_.xpath('.//td')],0,,,,,,,,,,,
trax,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/trax/trax/layers/research/efficient_attention.py,https://github.com/google/trax/tree/master/trax/layers/research/efficient_attention.py,LSHSelfAttention,forward_and_or_backward$2261,"def forward_and_or_backward(self, inputs, weights, state, rng, output_grad=None, compute_output=True, update_state=True):
    """"""Performs batched forward and/or backward passes.

    See `forward` for a reference implementation of what this layer does. The
    reference implementation is not very efficient, however, and this method
    provides a more performant version.

    Args:
      inputs: inputs to the attention layer
      weights: weights for the attention layer
      state: state of the attention layer
      rng: PRNG key for the layer (shared across all examples and heads)
      output_grad: gradient of the loss wrt the output of the layer, or None.
          This function performs the backward pass iff `output_grad` is not
          None.
      compute_output: bool: whether to return the output of the forward pass
          (for example, a pure backwards pass does not need to return the
          output).
      update_state: bool: whether to return an updated layer state.

    Returns:
      A tuple (output, new_state, inputs_grad, weights_grad).

      - output is not None iff compute_output is True
      - new_state is not None iff update_state is True
      - inputs_grad & weights_grad are not None iff output_grad is not None
    """"""
    have_single_input = not isinstance(inputs, (tuple, list))
    if have_single_input:
        inputs = (inputs,)
    batch_size = int(inputs[0].shape[0])
    seqlen = inputs[0].shape[-2]
    d_model = inputs[0].shape[-1]
    compute_grad = output_grad is not None
    assert compute_output or compute_grad, 'No work to perform!'
    if not self._incremental:
        forward_unbatched = functools.partial(self.forward_unbatched, rng=rng, update_state=update_state)
    else:
        if update_state:
            (inputs, state, q_start, new_mem, new_mem_end) = self._use_predict_mem(inputs, state)
        else:
            (new_mem_end, inputs, state) = state
            q_start = new_mem_end - seqlen
        forward_unbatched = functools.partial(self._incremental_forward_unbatched, q_start=fastmath.stop_gradient(q_start), q_len=fastmath.stop_gradient(seqlen), rng=rng, update_state=update_state)
    n_parallel_heads = batch_size * self._n_heads
    if self._n_parallel_heads and self._n_parallel_heads < n_parallel_heads:
        n_parallel_heads = self._n_parallel_heads

    def tree_update(tree, indices, new_values):
        return fastmath.nested_map_multiarg(lambda x, y: fastmath.index_update(x, jax.numpy.index_exp[indices], y), tree, new_values)

    def tree_add(tree, indices, new_values):
        return fastmath.nested_map_multiarg(lambda x, y: fastmath.index_add(x, jax.numpy.index_exp[indices], y), tree, new_values)
    if compute_grad:
        inputs_is_differentiable = fastmath.nested_map(lambda x: np.issubdtype(x.dtype, np.inexact), inputs)

        def split_differentiable(xs):
            differentiable_xs = fastmath.nested_map_multiarg(lambda x, is_differentiable: x if is_differentiable else None, xs, inputs_is_differentiable)
            non_differentiable_xs = fastmath.nested_map_multiarg(lambda x, is_differentiable: None if is_differentiable else x, xs, inputs_is_differentiable)
            return (differentiable_xs, non_differentiable_xs)

        def join_differentiable(differentiable_xs, non_differentiable_xs):
            """"""Reconstitute inputs pytree from differentiable/non-d. partitions.""""""
            differentiable_leaves = fastmath.tree_leaves(differentiable_xs)
            non_differentiable_leaves = fastmath.tree_leaves(non_differentiable_xs)
            leaves = []
            for is_differentiable in fastmath.tree_leaves(inputs_is_differentiable):
                if is_differentiable:
                    leaves.append(differentiable_leaves.pop(0))
                else:
                    leaves.append(non_differentiable_leaves.pop(0))
            assert not differentiable_leaves
            assert not non_differentiable_leaves
            (tree, _) = fastmath.tree_unflatten(leaves, inputs)
            return tree

        def vjp(fn, inp, *args, has_aux=False):
            (d_inp, nd_inp) = split_differentiable(inp)

            def fn_closed_over_nd_inp(d_inp, *args):
                inp = join_differentiable(d_inp, nd_inp)
                return fn(inp, *args)
            return fastmath.vjp(fn_closed_over_nd_inp, d_inp, *args, has_aux=has_aux)
    if n_parallel_heads == 1:

        def run_inner(idx, loop_val):
            """"""Runs one slice of attention (for a single head).""""""
            (o_all, s_all, i_ct_all, w_ct_all) = loop_val
            example_idx = idx // self._n_heads
            head_idx = idx % self._n_heads
            i_h = fastmath.nested_map(lambda x: x[example_idx], inputs)
            w_h = fastmath.nested_map(lambda w: w[head_idx], weights)
            s_h = fastmath.nested_map(lambda s: s[idx], state)

            def forward_fn(i_h, w_h):
                return forward_unbatched(*i_h, weights=w_h, state=fastmath.stop_gradient(s_h))
            if compute_grad:
                (o_h, backward_fn, s_h) = vjp(forward_fn, i_h, w_h, has_aux=True)
                ct_h = output_grad[example_idx]
                assert o_h.shape == ct_h.shape
                (i_ct_h, w_ct_h) = backward_fn(ct_h)
            else:
                (o_h, s_h) = forward_fn(i_h, w_h)
            if compute_output:
                o_all = fastmath.index_add(o_all, example_idx, o_h)
            if update_state:
                s_all = tree_update(s_all, idx, s_h)
            if compute_grad:
                i_ct_all = tree_add(i_ct_all, example_idx, i_ct_h)
                w_ct_all = tree_add(w_ct_all, head_idx, w_ct_h)
            return (o_all, s_all, i_ct_all, w_ct_all)
    elif n_parallel_heads < self._n_heads:
        assert self._n_heads % n_parallel_heads == 0

        def run_inner(idx, loop_val):
            """"""Runs one slice of attention (multiple heads, but one example).""""""
            (o_all, s_all, i_ct_all, w_ct_all) = loop_val
            idx = idx * self._n_parallel_heads
            example_idx = idx // self._n_heads
            head_idx_lo = idx % self._n_heads
            head_range = head_idx_lo + np.arange(n_parallel_heads, dtype=np.int32)
            state_range = idx + np.arange(n_parallel_heads, dtype=np.int32)
            i_mh = fastmath.nested_map(lambda x: x[example_idx], inputs)
            w_mh = fastmath.nested_map(lambda w: w[head_range], weights)
            s_mh = fastmath.nested_map(lambda s: s[state_range], state)

            def forward_unbatched_h(i_h, w_h, s_h):
                return forward_unbatched(*i_h, weights=w_h, state=s_h)

            def forward_fn(i_mh, w_mh):
                (o_mh, new_s_mh) = fastmath.vmap(forward_unbatched_h, in_axes=(None, 0, 0), out_axes=0)(i_mh, w_mh, s_mh)
                o_mh = np.sum(o_mh, axis=0)
                return (o_mh, new_s_mh)
            if compute_grad:
                (o_mh, backward_fn, s_mh) = vjp(forward_fn, i_mh, w_mh, has_aux=True)
                ct_mh = output_grad[example_idx]
                assert o_mh.shape == ct_mh.shape
                (i_ct_mh, w_ct_mh) = backward_fn(ct_mh)
            else:
                (o_mh, s_mh) = forward_fn(i_mh, w_mh)
            if compute_output:
                o_all = fastmath.index_add(o_all, example_idx, o_mh)
            if update_state:
                s_all = tree_update(s_all, state_range, s_mh)
            if compute_grad:
                i_ct_all = tree_add(i_ct_all, example_idx, i_ct_mh)
                w_ct_all = tree_add(w_ct_all, head_range, w_ct_mh)
            return (o_all, s_all, i_ct_all, w_ct_all)
    else:
        assert n_parallel_heads % self._n_heads == 0

        def forward_single_example(i_x, w_all, s_x):

            def forward_unbatched_h(i_h, w_h, s_h):
                return forward_unbatched(*i_h, weights=w_h, state=s_h)
            (o_x, s_x) = fastmath.vmap(forward_unbatched_h, in_axes=(None, 0, 0), out_axes=(0, 0))(i_x, w_all, s_x)
            o_x = np.sum(o_x, axis=0)
            return (o_x, s_x)

        def run_inner(idx, loop_val):
            """"""Runs one slice of attention (all heads for one or more examples).""""""
            (o_all, s_all, i_ct_all, w_ct_all) = loop_val
            idx = idx * n_parallel_heads
            example_idx_lo = idx // self._n_heads
            example_range = example_idx_lo + np.arange(n_parallel_heads // self._n_heads, dtype=np.int32)
            state_range = idx + np.arange(n_parallel_heads, dtype=np.int32)
            i_mex = fastmath.nested_map(lambda x: x[example_range], inputs)
            s_mex = fastmath.nested_map(lambda s: np.reshape(s[state_range], (-1, self._n_heads) + s.shape[1:]), state)

            def forward_fn(i_mex, w_all):
                (o_mex, new_s_mex) = fastmath.vmap(forward_single_example, in_axes=(0, None, 0), out_axes=(0, 0))(i_mex, w_all, s_mex)
                new_s_mex = fastmath.nested_map(lambda s: np.reshape(s, (n_parallel_heads,) + s.shape[2:]), new_s_mex)
                return (o_mex.astype(i_mex[0].dtype), new_s_mex)
            if compute_grad:
                (o_mex, backward_fn, s_mex) = vjp(forward_fn, i_mex, weights, has_aux=True)
                ct_mex = output_grad[example_range]
                assert o_mex.shape == ct_mex.shape, str(ct_mex.shape)
                assert o_mex.dtype == ct_mex.dtype, str(ct_mex.dtype)
                (i_ct_mex, w_ct_mex) = backward_fn(ct_mex)
            else:
                (o_mex, s_mex) = forward_fn(i_mex, weights)
            if compute_output:
                o_all = fastmath.index_add(o_all, jax.numpy.index_exp[example_range], o_mex)
            if update_state:
                s_all = tree_update(s_all, state_range, s_mex)
            if compute_grad:
                i_ct_all = tree_update(i_ct_all, example_range, i_ct_mex)
                w_ct_all = fastmath.nested_map_multiarg(lambda old_all, delta_all: old_all + delta_all, w_ct_all, w_ct_mex)
            return (o_all, s_all, i_ct_all, w_ct_all)
    o_all = s_all = i_ct_all = w_ct_all = None
    if compute_output:
        o_all = np.zeros((batch_size, seqlen, d_model), dtype=inputs[0].dtype)
    if update_state:
        s_all = state
    if compute_grad:
        i_ct_all = fastmath.nested_map(np.zeros_like, inputs)
        (i_ct_all, i_nondifferentiable_dummy_ct) = split_differentiable(i_ct_all)
        w_ct_all = fastmath.nested_map(np.zeros_like, weights)
    loop_val = (o_all, s_all, i_ct_all, w_ct_all)
    assert batch_size * self._n_heads % n_parallel_heads == 0
    loop_hi = batch_size * self._n_heads // n_parallel_heads
    if self._use_python_loop or loop_hi == 1:
        for idx in range(loop_hi):
            loop_val = run_inner(idx, loop_val)
    else:
        loop_val = fastmath.fori_loop(0, loop_hi, run_inner, loop_val)
    (o_all, s_all, i_ct_all, w_ct_all) = loop_val
    if compute_grad:
        i_ct_all = join_differentiable(i_ct_all, i_nondifferentiable_dummy_ct)
    if self._incremental and update_state:
        s_all = (new_mem_end, new_mem, s_all)
    if have_single_input and compute_grad:
        assert isinstance(i_ct_all, tuple) and len(i_ct_all) == 1
        return (o_all, s_all, i_ct_all[0], w_ct_all)
    else:
        return (o_all, s_all, i_ct_all, w_ct_all)","for is_differentiable in fastmath.tree_leaves(inputs_is_differentiable):
    if is_differentiable:
        leaves.append(differentiable_leaves.pop(0))
    else:
        leaves.append(non_differentiable_leaves.pop(0))",leaves = [differentiable_leaves.pop(0) if is_differentiable else non_differentiable_leaves.pop(0) for is_differentiable in fastmath.tree_leaves(inputs_is_differentiable)],0,,,,,,,,,,,
ludwig,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ludwig/ludwig/combiners/combiners.py,https://github.com/ludwig-ai/ludwig/tree/master/ludwig/combiners/combiners.py,ComparatorCombiner,__init__$967,"def __init__(self, input_features: Dict[str, 'InputFeature'], config: ComparatorCombinerConfig=None, **kwargs):
    super().__init__(input_features)
    self.name = 'ComparatorCombiner'
    logger.debug('Entering {}'.format(self.name))
    self.entity_1 = config.entity_1
    self.entity_2 = config.entity_2
    self.required_inputs = set(config.entity_1 + config.entity_2)
    self.fc_size = config.fc_size
    self.fc_stack = None
    fc_layers = config.fc_layers
    if fc_layers is None and config.num_fc_layers is not None:
        fc_layers = []
        for i in range(config.num_fc_layers):
            fc_layers.append({'fc_size': config.fc_size})
    if fc_layers is not None:
        logger.debug('Setting up FCStack')
        self.e1_fc_stack = FCStack(self.get_entity_shape(config.entity_1)[-1], layers=fc_layers, num_layers=config.num_fc_layers, default_fc_size=config.fc_size, default_use_bias=config.use_bias, default_weights_initializer=config.weights_initializer, default_bias_initializer=config.bias_initializer, default_norm=config.norm, default_norm_params=config.norm_params, default_activation=config.activation, default_dropout=config.dropout)
        self.e2_fc_stack = FCStack(self.get_entity_shape(config.entity_2)[-1], layers=fc_layers, num_layers=config.num_fc_layers, default_fc_size=config.fc_size, default_use_bias=config.use_bias, default_weights_initializer=config.weights_initializer, default_bias_initializer=config.bias_initializer, default_norm=config.norm, default_norm_params=config.norm_params, default_activation=config.activation, default_dropout=config.dropout)
    self.last_fc_layer_fc_size = fc_layers[-1]['fc_size']
    self.bilinear_weights = torch.randn([self.last_fc_layer_fc_size, self.last_fc_layer_fc_size], dtype=torch.float32)","for i in range(config.num_fc_layers):
    fc_layers.append({'fc_size': config.fc_size})",fc_layers = [{'fc_size': config.fc_size} for i in range(config.num_fc_layers)],0,,,,,,,,,,,
DSB2017,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/DSB2017/training/classifier/utils.py,https://github.com/lfz/DSB2017/tree/master/training/classifier/utils.py,,combine16$172,"def combine16(output, z, h, w):
    splits = []
    for i in range(len(output)):
        splits.append(output[i])
    output = np.zeros((z, h, w, splits[0].shape[3], splits[0].shape[4]), np.float32)
    z_width = z / 4
    h_width = h / 2
    w_width = w / 2
    splitzstart = splits[0].shape[0] / 2 - z_width / 2
    z_pos = [z * 3 / 8 - z_width / 2, z * 5 / 8 - z_width / 2]
    i = 0
    for (zz, zz2) in zip([[0, z_width], [z_width, z_width * 2], [z_width * 2, z_width * 3], [z_width * 3 - z, None]], [[0, z_width], [splitzstart, z_width + splitzstart], [splitzstart, z_width + splitzstart], [z_width * 3 - z, None]]):
        for hh in [[0, h_width], [h_width - h, None]]:
            for ww in [[0, w_width], [w_width - w, None]]:
                output[zz[0]:zz[1], hh[0]:hh[1], ww[0]:ww[1], :, :] = splits[i][zz2[0]:zz2[1], hh[0]:hh[1], ww[0]:ww[1], :, :]
                i = i + 1
    return output","for i in range(len(output)):
    splits.append(output[i])",splits = [output[i] for i in range(len(output))],0,,,,,,,,,,,
fairseq,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/fairseq/tests/test_dictionary.py,https://github.com/pytorch/fairseq/tree/master/tests/test_dictionary.py,TestDictionary,get_ids$53,"def get_ids(dictionary):
    ids = []
    for line in txt:
        ids.append(dictionary.encode_line(line, add_if_not_exist=False))
    return ids","for line in txt:
    ids.append(dictionary.encode_line(line, add_if_not_exist=False))","ids = [dictionary.encode_line(line, add_if_not_exist=False) for line in txt]",0,,,,,,,,,,,
beets,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/beets/beets/dbcore/db.py,https://github.com/beetbox/beets/tree/master/beets/dbcore/db.py,Database,_make_table$1028,"def _make_table(self, table, fields):
    """"""Set up the schema of the database. `fields` is a mapping
        from field names to `Type`s. Columns are added if necessary.
        """"""
    with self.transaction() as tx:
        rows = tx.query('PRAGMA table_info(%s)' % table)
    current_fields = {row[1] for row in rows}
    field_names = set(fields.keys())
    if current_fields.issuperset(field_names):
        return
    if not current_fields:
        columns = []
        for (name, typ) in fields.items():
            columns.append(f'{name} {typ.sql}')
        setup_sql = 'CREATE TABLE {} ({});\n'.format(table, ', '.join(columns))
    else:
        setup_sql = ''
        for (name, typ) in fields.items():
            if name in current_fields:
                continue
            setup_sql += 'ALTER TABLE {} ADD COLUMN {} {};\n'.format(table, name, typ.sql)
    with self.transaction() as tx:
        tx.script(setup_sql)","for (name, typ) in fields.items():
    columns.append(f'{name} {typ.sql}')","columns = [f'{name} {typ.sql}' for (name, typ) in fields.items()]",0,,,,,,,,,,,
investpy,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/investpy/investpy/etfs.py,https://github.com/alvarobartt/investpy/tree/master/investpy/etfs.py,,get_etf_historical_data$514,"def get_etf_historical_data(etf, country, from_date, to_date, stock_exchange=None, as_json=False, order='ascending', interval='Daily'):
    """"""
    This function retrieves historical data from the introduced `etf` from Investing.com via Web Scraping on the
    introduced date range. The resulting data can it either be stored in a :obj:`pandas.DataFrame` or in a
    :obj:`json` object with `ascending` or `descending` order.

    Args:
        etf (:obj:`str`): name of the etf to retrieve recent historical data from.
        country (:obj:`str`): name of the country from where the etf is.
        from_date (:obj:`str`): date as `str` formatted as `dd/mm/yyyy`, from where data is going to be retrieved.
        to_date (:obj:`str`): date as `str` formatted as `dd/mm/yyyy`, until where data is going to be retrieved.
        as_json (:obj:`bool`, optional):
            to determine the format of the output data (:obj:`pandas.DataFrame` or :obj:`json`).
        order (:obj:`str`, optional):
            optional argument to define the order of the retrieved data (`ascending`, `asc` or `descending`, `desc`).
        interval (:obj:`str`, optional):
            value to define the historical data interval to retrieve, by default `Daily`, but it can also be `Weekly` or `Monthly`.

    Returns:
        :obj:`pandas.DataFrame` or :obj:`json`:
            The function returns either a :obj:`pandas.DataFrame` or a :obj:`json` file containing the retrieved
            recent data from the specified etf via argument. The dataset contains the open, high, low and close
            values for the selected etf on market days.

            The returned data is case we use default arguments will look like::

                Date || Open | High | Low | Close | Volume | Currency | Exchange
                -----||------|------|-----|-------|--------|----------|---------
                xxxx || xxxx | xxxx | xxx | xxxxx | xxxxxx | xxxxxxxx | xxxxxxxx

            but if we define `as_json=True`, then the output will be::

                {
                    name: name,
                    historical: [
                        {
                            date: dd/mm/yyyy,
                            open: x,
                            high: x,
                            low: x,
                            close: x,
                            volume: x,
                            currency: x,
                            exchange: x
                        },
                        ...
                    ]
                }

    Raises:
        ValueError: raised whenever any of the arguments is not valid or errored.
        IOError: raised if etfs object/file not found or unable to retrieve.
        RuntimeError:raised if the introduced etf does not match any of the indexed ones.
        ConnectionError: raised if GET requests does not return 200 status code.
        IndexError: raised if etf information was unavailable or not found.

    Examples:
        >>> data = investpy.get_etf_historical_data(etf='bbva accion dj eurostoxx 50', country='spain', from_date='01/01/2010', to_date='01/01/2019')
        >>> data.head()
                     Open   High    Low  Close  Volume Currency Exchange
        Date
        2011-12-07  23.70  23.70  23.70  23.62    2000      EUR   Madrid
        2011-12-08  23.53  23.60  23.15  23.04     599      EUR   Madrid
        2011-12-09  23.36  23.60  23.36  23.62    2379      EUR   Madrid
        2011-12-12  23.15  23.26  23.00  22.88   10695      EUR   Madrid
        2011-12-13  22.88  22.88  22.88  22.80      15      EUR   Madrid

    """"""
    if not etf:
        raise ValueError('ERR#0031: etf parameter is mandatory and must be a valid etf name.')
    if not isinstance(etf, str):
        raise ValueError('ERR#0030: etf argument needs to be a str.')
    if country is None:
        raise ValueError('ERR#0039: country can not be None, it should be a str.')
    if country is not None and (not isinstance(country, str)):
        raise ValueError('ERR#0025: specified country value not valid.')
    if stock_exchange is not None and (not isinstance(stock_exchange, str)):
        raise ValueError('ERR#0125: specified stock_exchange value is not valid, it should be a str.')
    if not isinstance(as_json, bool):
        raise ValueError('ERR#0002: as_json argument can just be True or False, bool type.')
    if order not in ['ascending', 'asc', 'descending', 'desc']:
        raise ValueError('ERR#0003: order argument can just be ascending (asc) or descending (desc), str type.')
    if not interval:
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    if not isinstance(interval, str):
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    interval = interval.lower()
    if interval not in ['daily', 'weekly', 'monthly']:
        raise ValueError(""ERR#0073: interval value should be a str type and it can just be either 'Daily', 'Weekly' or 'Monthly'."")
    try:
        datetime.strptime(from_date, '%d/%m/%Y')
    except ValueError:
        raise ValueError(""ERR#0011: incorrect data format, it should be 'dd/mm/yyyy'."")
    try:
        datetime.strptime(to_date, '%d/%m/%Y')
    except ValueError:
        raise ValueError(""ERR#0011: incorrect data format, it should be 'dd/mm/yyyy'."")
    start_date = datetime.strptime(from_date, '%d/%m/%Y')
    end_date = datetime.strptime(to_date, '%d/%m/%Y')
    if start_date >= end_date:
        raise ValueError(""ERR#0032: to_date should be greater than from_date, both formatted as 'dd/mm/yyyy'."")
    date_interval = {'intervals': []}
    flag = True
    while flag is True:
        diff = end_date.year - start_date.year
        if diff > 19:
            obj = {'start': start_date.strftime('%m/%d/%Y'), 'end': start_date.replace(year=start_date.year + 19).strftime('%m/%d/%Y')}
            date_interval['intervals'].append(obj)
            start_date = start_date.replace(year=start_date.year + 19) + timedelta(days=1)
        else:
            obj = {'start': start_date.strftime('%m/%d/%Y'), 'end': end_date.strftime('%m/%d/%Y')}
            date_interval['intervals'].append(obj)
            flag = False
    interval_limit = len(date_interval['intervals'])
    interval_counter = 0
    data_flag = False
    resource_package = 'investpy'
    resource_path = '/'.join(('resources', 'etfs.csv'))
    if pkg_resources.resource_exists(resource_package, resource_path):
        etfs = pd.read_csv(pkg_resources.resource_filename(resource_package, resource_path), keep_default_na=False)
    else:
        raise FileNotFoundError('ERR#0058: etfs file not found or errored.')
    if etfs is None:
        raise IOError('ERR#0009: etfs object not found or unable to retrieve.')
    country = unidecode(country.strip().lower())
    if country not in get_etf_countries():
        raise RuntimeError('ERR#0034: country ' + country + ' not found, check if it is correct.')
    etf = unidecode(etf.strip().lower())
    def_exchange = etfs.loc[((etfs['name'].apply(unidecode).str.lower() == etf) & (etfs['def_stock_exchange'] == True)).idxmax()]
    etfs = etfs[etfs['country'].str.lower() == country]
    if etf not in list(etfs['name'].apply(unidecode).str.lower()):
        raise RuntimeError('ERR#0019: etf ' + etf + ' not found, check if it is correct.')
    etfs = etfs[etfs['name'].apply(unidecode).str.lower() == etf]
    if def_exchange['country'] != country:
        warnings.warn('Selected country does not contain the default stock exchange of the introduced ETF. ' + 'Default country is: ""' + def_exchange['country'] + '"" and default stock_exchange: ""' + def_exchange['stock_exchange'] + '"".', Warning)
        if stock_exchange:
            if stock_exchange.lower() not in etfs['stock_exchange'].str.lower():
                raise ValueError('ERR#0126: introduced stock_exchange value does not exists, leave this parameter to None to use default stock_exchange.')
            etf_exchange = etfs.loc[(etfs['stock_exchange'].str.lower() == stock_exchange.lower()).idxmax(), 'stock_exchange']
        else:
            found_etfs = etfs[etfs['name'].apply(unidecode).str.lower() == etf]
            if len(found_etfs) > 1:
                warnings.warn('Note that the displayed information can differ depending on the stock exchange. Available stock_exchange' + ' values for ""' + country + '"" are: ""' + '"", ""'.join(found_etfs['stock_exchange']) + '"".', Warning)
            del found_etfs
            etf_exchange = etfs.loc[(etfs['name'].apply(unidecode).str.lower() == etf).idxmax(), 'stock_exchange']
    elif stock_exchange:
        if stock_exchange.lower() not in etfs['stock_exchange'].str.lower():
            raise ValueError('ERR#0126: introduced stock_exchange value does not exists, leave this parameter to None to use default stock_exchange.')
        if def_exchange['stock_exchange'].lower() != stock_exchange.lower():
            warnings.warn('Selected stock_exchange is not the default one of the introduced ETF. ' + 'Default country is: ""' + def_exchange['country'] + '"" and default stock_exchange: ""' + def_exchange['stock_exchange'].lower() + '"".', Warning)
        etf_exchange = etfs.loc[(etfs['stock_exchange'].str.lower() == stock_exchange.lower()).idxmax(), 'stock_exchange']
    else:
        etf_exchange = def_exchange['stock_exchange']
    symbol = etfs.loc[((etfs['name'].apply(unidecode).str.lower() == etf) & (etfs['stock_exchange'].str.lower() == etf_exchange.lower())).idxmax(), 'symbol']
    id_ = etfs.loc[((etfs['name'].apply(unidecode).str.lower() == etf) & (etfs['stock_exchange'].str.lower() == etf_exchange.lower())).idxmax(), 'id']
    name = etfs.loc[((etfs['name'].apply(unidecode).str.lower() == etf) & (etfs['stock_exchange'].str.lower() == etf_exchange.lower())).idxmax(), 'name']
    etf_currency = etfs.loc[((etfs['name'].apply(unidecode).str.lower() == etf) & (etfs['stock_exchange'].str.lower() == etf_exchange.lower())).idxmax(), 'currency']
    final = list()
    header = symbol + ' Historical Data'
    for index in range(len(date_interval['intervals'])):
        interval_counter += 1
        params = {'curr_id': id_, 'smlID': str(randint(1000000, 99999999)), 'header': header, 'st_date': date_interval['intervals'][index]['start'], 'end_date': date_interval['intervals'][index]['end'], 'interval_sec': interval.capitalize(), 'sort_col': 'date', 'sort_ord': 'DESC', 'action': 'historical_data'}
        head = {'User-Agent': random_user_agent(), 'X-Requested-With': 'XMLHttpRequest', 'Accept': 'text/html', 'Accept-Encoding': 'gzip, deflate', 'Connection': 'keep-alive'}
        url = 'https://www.investing.com/instruments/HistoricalDataAjax'
        req = requests.post(url, headers=head, data=params)
        if req.status_code != 200:
            raise ConnectionError('ERR#0015: error ' + str(req.status_code) + ', try again later.')
        if not req.text:
            continue
        root_ = fromstring(req.text)
        path_ = root_.xpath("".//table[@id='curr_table']/tbody/tr"")
        result = list()
        if path_:
            for elements_ in path_:
                if elements_.xpath('.//td')[0].text_content() == 'No results found':
                    if interval_counter < interval_limit:
                        data_flag = False
                    else:
                        raise IndexError('ERR#0010: etf information unavailable or not found.')
                else:
                    data_flag = True
                info = []
                for nested_ in elements_.xpath('.//td'):
                    info.append(nested_.get('data-real-value'))
                if data_flag is True:
                    etf_date = datetime.strptime(str(datetime.fromtimestamp(int(info[0]), tz=pytz.timezone('GMT')).date()), '%Y-%m-%d')
                    etf_close = float(info[1].replace(',', ''))
                    etf_open = float(info[2].replace(',', ''))
                    etf_high = float(info[3].replace(',', ''))
                    etf_low = float(info[4].replace(',', ''))
                    etf_volume = int(info[5])
                    result.insert(len(result), Data(etf_date, etf_open, etf_high, etf_low, etf_close, etf_volume, etf_currency, etf_exchange))
            if data_flag is True:
                if order in ['ascending', 'asc']:
                    result = result[::-1]
                elif order in ['descending', 'desc']:
                    result = result
                if as_json is True:
                    json_list = [value.etf_as_json() for value in result]
                    final.append(json_list)
                elif as_json is False:
                    df = pd.DataFrame.from_records([value.etf_to_dict() for value in result])
                    df.set_index('Date', inplace=True)
                    final.append(df)
        else:
            raise RuntimeError('ERR#0004: data retrieval error while scraping.')
    if order in ['descending', 'desc']:
        final.reverse()
    if as_json is True:
        json_ = {'name': name, 'historical': [value for json_list in final for value in json_list]}
        return json.dumps(json_, sort_keys=False)
    elif as_json is False:
        return pd.concat(final)","for nested_ in elements_.xpath('.//td'):
    info.append(nested_.get('data-real-value'))",info = [nested_.get('data-real-value') for nested_ in elements_.xpath('.//td')],0,,,,,,,,,,,
dronekit-python,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/dronekit-python/examples/mission_import_export/mission_import_export.py,https://github.com/dronekit/dronekit-python/tree/master/examples/mission_import_export/mission_import_export.py,,download_mission$104,"def download_mission():
    """"""
    Downloads the current mission and returns it in a list.
    It is used in save_mission() to get the file information to save.
    """"""
    print(' Download mission from vehicle')
    missionlist = []
    cmds = vehicle.commands
    cmds.download()
    cmds.wait_ready()
    for cmd in cmds:
        missionlist.append(cmd)
    return missionlist","for cmd in cmds:
    missionlist.append(cmd)",missionlist = [cmd for cmd in cmds],0,,,,,,,,,,,
wukong-robot,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/wukong-robot/robot/sdk/unit.py,https://github.com/wzpan/wukong-robot/tree/master/robot/sdk/unit.py,,getSlotWords$158,"def getSlotWords(parsed, intent, name):
    """"""
    

    :param parsed: UNIT 
    :param intent: 
    :param name: 
    :returns: 
    """"""
    slots = getSlots(parsed, intent)
    words = []
    for slot in slots:
        if slot['name'] == name:
            words.append(slot['normalized_word'])
    return words","for slot in slots:
    if slot['name'] == name:
        words.append(slot['normalized_word'])",words = [slot['normalized_word'] for slot in slots if slot['name'] == name],0,,,,,,,,,,,
R-Drop,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/R-Drop/huggingface_transformer_src/src/transformers/models/dpr/tokenization_dpr.py,https://github.com/dropreg/R-Drop/tree/master/huggingface_transformer_src/src/transformers/models/dpr/tokenization_dpr.py,CustomDPRReaderTokenizerMixin,__call__$202,"def __call__(self, questions, titles: Optional[str]=None, texts: Optional[str]=None, padding: Union[bool, str]=False, truncation: Union[bool, str]=False, max_length: Optional[int]=None, return_tensors: Optional[Union[str, TensorType]]=None, return_attention_mask: Optional[bool]=None, **kwargs) -> BatchEncoding:
    if titles is None and texts is None:
        return super().__call__(questions, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)
    elif titles is None or texts is None:
        text_pair = titles if texts is None else texts
        return super().__call__(questions, text_pair, padding=padding, truncation=truncation, max_length=max_length, return_tensors=return_tensors, return_attention_mask=return_attention_mask, **kwargs)
    titles = titles if not isinstance(titles, str) else [titles]
    texts = texts if not isinstance(texts, str) else [texts]
    n_passages = len(titles)
    questions = questions if not isinstance(questions, str) else [questions] * n_passages
    assert len(titles) == len(texts), f'There should be as many titles than texts but got {len(titles)} titles and {len(texts)} texts.'
    encoded_question_and_titles = super().__call__(questions, titles, padding=False, truncation=False)['input_ids']
    encoded_texts = super().__call__(texts, add_special_tokens=False, padding=False, truncation=False)['input_ids']
    encoded_inputs = {'input_ids': [(encoded_question_and_title + encoded_text)[:max_length] if max_length is not None and truncation else encoded_question_and_title + encoded_text for (encoded_question_and_title, encoded_text) in zip(encoded_question_and_titles, encoded_texts)]}
    if return_attention_mask is not False:
        attention_mask = []
        for input_ids in encoded_inputs['input_ids']:
            attention_mask.append([int(input_id != self.pad_token_id) for input_id in input_ids])
        encoded_inputs['attention_mask'] = attention_mask
    return self.pad(encoded_inputs, padding=padding, max_length=max_length, return_tensors=return_tensors)","for input_ids in encoded_inputs['input_ids']:
    attention_mask.append([int(input_id != self.pad_token_id) for input_id in input_ids])",attention_mask = [[int(input_id != self.pad_token_id) for input_id in input_ids] for input_ids in encoded_inputs['input_ids']],0,,,,,,,,,,,
ansible,/data1/zhangzejun/mnt/zejun/smp/data/python_star_2000repo/ansible/test/support/network-integration/collections/ansible_collections/vyos/vyos/plugins/module_utils/network/vyos/utils/utils.py,https://github.com/ansible/ansible/tree/master/test/support/network-integration/collections/ansible_collections/vyos/vyos/plugins/module_utils/network/vyos/utils/utils.py,,diff_list_of_dicts$61,"def diff_list_of_dicts(want, have):
    diff = []
    set_w = set((tuple(d.items()) for d in want))
    set_h = set((tuple(d.items()) for d in have))
    difference = set_w.difference(set_h)
    for element in difference:
        diff.append(dict(((x, y) for (x, y) in element)))
    return diff","for element in difference:
    diff.append(dict(((x, y) for (x, y) in element)))","diff = [dict(((x, y) for (x, y) in element)) for element in difference]",0,,,,,,,,,,,
,19o found,37 refactor error,,536,499,462,0.925851703,,,,,,,,,,,,
